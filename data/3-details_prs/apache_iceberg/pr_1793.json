{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTI0NzA4OTI5", "number": 1793, "title": "Flink: Support streaming reader.", "bodyText": "This patch port the flink streaming reader from here. https://github.com/generic-datalake/iceberg-poc/pull/3", "createdAt": "2020-11-20T13:34:25Z", "url": "https://github.com/apache/iceberg/pull/1793", "merged": true, "mergeCommit": {"oid": "14331c4e5f61e14cdc527a567f5dafc7fd95c3e7"}, "closed": true, "closedAt": "2021-01-14T04:35:26Z", "author": {"login": "openinx"}, "timelineItems": {"totalCount": 53, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdfusgaAFqTUzNzgyNzEwNA==", "endCursor": "Y3Vyc29yOnYyOpPPAAABdv70UKgH2gAyNTI0NzA4OTI5OmVhNTMyZjRiYTI2ZjE5NjEyZWFjYjdmOWQyNjNlYTVkZjViMTE1NWY=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM3ODI3MTA0", "url": "https://github.com/apache/iceberg/pull/1793#pullrequestreview-537827104", "createdAt": "2020-11-24T19:13:07Z", "commit": {"oid": "329c60af56936dc99c784c60a1b4f3511e06abd1"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNFQxOToxMzowN1rOH5ReTg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNFQxOToxMzowN1rOH5ReTg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTgxNzE2Ng==", "bodyText": "I think returning a different builder breaks the builder pattern. Why not call these methods on this instead?", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r529817166", "createdAt": "2020-11-24T19:13:07Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/ScanContext.java", "diffHunk": "@@ -98,115 +112,203 @@ private ScanContext(boolean caseSensitive, Long snapshotId, Long startSnapshotId\n     this.splitSize = splitSize;\n     this.splitLookback = splitLookback;\n     this.splitOpenFileCost = splitOpenFileCost;\n+    this.isStreaming = isStreaming;\n+    this.monitorInterval = monitorInterval;\n+\n     this.nameMapping = nameMapping;\n     this.projectedSchema = projectedSchema;\n     this.filterExpressions = filterExpressions;\n   }\n \n-  ScanContext fromProperties(Map<String, String> properties) {\n-    Configuration config = new Configuration();\n-    properties.forEach(config::setString);\n-    return new ScanContext(config.get(CASE_SENSITIVE), config.get(SNAPSHOT_ID), config.get(START_SNAPSHOT_ID),\n-        config.get(END_SNAPSHOT_ID), config.get(AS_OF_TIMESTAMP), config.get(SPLIT_SIZE), config.get(SPLIT_LOOKBACK),\n-        config.get(SPLIT_FILE_OPEN_COST), properties.get(DEFAULT_NAME_MAPPING), projectedSchema, filterExpressions);\n-  }\n-\n   boolean caseSensitive() {\n     return caseSensitive;\n   }\n \n-  ScanContext setCaseSensitive(boolean isCaseSensitive) {\n-    return new ScanContext(isCaseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions);\n-  }\n-\n   Long snapshotId() {\n     return snapshotId;\n   }\n \n-  ScanContext useSnapshotId(Long scanSnapshotId) {\n-    return new ScanContext(caseSensitive, scanSnapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions);\n-  }\n-\n   Long startSnapshotId() {\n     return startSnapshotId;\n   }\n \n-  ScanContext startSnapshotId(Long id) {\n-    return new ScanContext(caseSensitive, snapshotId, id, endSnapshotId, asOfTimestamp, splitSize, splitLookback,\n-        splitOpenFileCost, nameMapping, projectedSchema, filterExpressions);\n-  }\n-\n   Long endSnapshotId() {\n     return endSnapshotId;\n   }\n \n-  ScanContext endSnapshotId(Long id) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, id, asOfTimestamp, splitSize, splitLookback,\n-        splitOpenFileCost, nameMapping, projectedSchema, filterExpressions);\n-  }\n-\n   Long asOfTimestamp() {\n     return asOfTimestamp;\n   }\n \n-  ScanContext asOfTimestamp(Long timestamp) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, timestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions);\n-  }\n-\n   Long splitSize() {\n     return splitSize;\n   }\n \n-  ScanContext splitSize(Long size) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, size,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions);\n-  }\n-\n   Integer splitLookback() {\n     return splitLookback;\n   }\n \n-  ScanContext splitLookback(Integer lookback) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        lookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions);\n-  }\n-\n   Long splitOpenFileCost() {\n     return splitOpenFileCost;\n   }\n \n-  ScanContext splitOpenFileCost(Long fileCost) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, fileCost, nameMapping, projectedSchema, filterExpressions);\n+  boolean isStreaming() {\n+    return isStreaming;\n   }\n \n-  String nameMapping() {\n-    return nameMapping;\n+  Duration monitorInterval() {\n+    return monitorInterval;\n   }\n \n-  ScanContext nameMapping(String mapping) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, mapping, projectedSchema, filterExpressions);\n+  String nameMapping() {\n+    return nameMapping;\n   }\n \n   Schema projectedSchema() {\n     return projectedSchema;\n   }\n \n-  ScanContext project(Schema schema) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, schema, filterExpressions);\n-  }\n-\n   List<Expression> filterExpressions() {\n     return filterExpressions;\n   }\n \n-  ScanContext filterRows(List<Expression> filters) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filters);\n+  ScanContext copyWithAppendsBetween(long newStartSnapshotId, long newEndSnapshotId) {\n+    return ScanContext.builder()\n+        .caseSensitive(caseSensitive)\n+        .useSnapshotId(null)\n+        .startSnapshotId(newStartSnapshotId)\n+        .endSnapshotId(newEndSnapshotId)\n+        .asOfTimestamp(null)\n+        .splitSize(splitSize)\n+        .splitOpenFileCost(splitOpenFileCost)\n+        .nameMapping(nameMapping)\n+        .streaming(isStreaming)\n+        .monitorInterval(monitorInterval)\n+        .build();\n+  }\n+\n+  ScanContext copyWithSnapshotId(long newSnapshotId) {\n+    return ScanContext.builder()\n+        .caseSensitive(caseSensitive)\n+        .useSnapshotId(newSnapshotId)\n+        .startSnapshotId(null)\n+        .endSnapshotId(null)\n+        .asOfTimestamp(null)\n+        .splitSize(splitSize)\n+        .splitOpenFileCost(splitOpenFileCost)\n+        .nameMapping(nameMapping)\n+        .streaming(isStreaming)\n+        .monitorInterval(monitorInterval)\n+        .build();\n+  }\n+\n+  static Builder builder() {\n+    return new Builder();\n+  }\n+\n+  static class Builder {\n+    private boolean caseSensitive;\n+    private Long snapshotId;\n+    private Long startSnapshotId;\n+    private Long endSnapshotId;\n+    private Long asOfTimestamp;\n+    private Long splitSize;\n+    private Integer splitLookback;\n+    private Long splitOpenFileCost;\n+    private boolean isStreaming;\n+    private Duration monitorInterval;\n+    private String nameMapping;\n+    private Schema projectedSchema;\n+    private List<Expression> filterExpressions;\n+\n+    private Builder() {\n+    }\n+\n+    Builder caseSensitive(boolean newCaseSensitive) {\n+      this.caseSensitive = newCaseSensitive;\n+      return this;\n+    }\n+\n+    Builder useSnapshotId(Long newSnapshotId) {\n+      this.snapshotId = newSnapshotId;\n+      return this;\n+    }\n+\n+    Builder startSnapshotId(Long newStartSnapshotId) {\n+      this.startSnapshotId = newStartSnapshotId;\n+      return this;\n+    }\n+\n+    Builder endSnapshotId(Long newEndsnapshotId) {\n+      this.endSnapshotId = newEndsnapshotId;\n+      return this;\n+    }\n+\n+    Builder asOfTimestamp(Long newAsOfTimestamp) {\n+      this.asOfTimestamp = newAsOfTimestamp;\n+      return this;\n+    }\n+\n+    Builder splitSize(Long newSplitSize) {\n+      this.splitSize = newSplitSize;\n+      return this;\n+    }\n+\n+    Builder splitLookback(Integer newSplitLookback) {\n+      this.splitLookback = newSplitLookback;\n+      return this;\n+    }\n+\n+    Builder splitOpenFileCost(Long newSplitOpenFileCost) {\n+      this.splitOpenFileCost = newSplitOpenFileCost;\n+      return this;\n+    }\n+\n+    Builder streaming(boolean streaming) {\n+      this.isStreaming = streaming;\n+      return this;\n+    }\n+\n+    Builder monitorInterval(Duration newMonitorInterval) {\n+      this.monitorInterval = newMonitorInterval;\n+      return this;\n+    }\n+\n+    Builder nameMapping(String newNameMapping) {\n+      this.nameMapping = newNameMapping;\n+      return this;\n+    }\n+\n+    Builder projectedSchema(Schema newProjectedSchema) {\n+      this.projectedSchema = newProjectedSchema;\n+      return this;\n+    }\n+\n+    Builder filterExpression(List<Expression> newFilterExpressions) {\n+      this.filterExpressions = newFilterExpressions;\n+      return this;\n+    }\n+\n+    Builder fromProperties(Map<String, String> properties) {\n+      Configuration config = new Configuration();\n+      properties.forEach(config::setString);\n+\n+      return new Builder()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "329c60af56936dc99c784c60a1b4f3511e06abd1"}, "originalPosition": 296}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM4MDEzODIz", "url": "https://github.com/apache/iceberg/pull/1793#pullrequestreview-538013823", "createdAt": "2020-11-24T22:19:26Z", "commit": {"oid": "329c60af56936dc99c784c60a1b4f3511e06abd1"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNFQyMjoxOToyNlrOH5YgiA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNFQyMjoxOToyNlrOH5YgiA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTkzMjQyNA==", "bodyText": "Shouldn't a builder supply the defaults for these? Otherwise, all of the values need to be supplied. Why not use CASE_SENSITIVE.defaultValue() and similar here?", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r529932424", "createdAt": "2020-11-24T22:19:26Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/ScanContext.java", "diffHunk": "@@ -98,115 +112,203 @@ private ScanContext(boolean caseSensitive, Long snapshotId, Long startSnapshotId\n     this.splitSize = splitSize;\n     this.splitLookback = splitLookback;\n     this.splitOpenFileCost = splitOpenFileCost;\n+    this.isStreaming = isStreaming;\n+    this.monitorInterval = monitorInterval;\n+\n     this.nameMapping = nameMapping;\n     this.projectedSchema = projectedSchema;\n     this.filterExpressions = filterExpressions;\n   }\n \n-  ScanContext fromProperties(Map<String, String> properties) {\n-    Configuration config = new Configuration();\n-    properties.forEach(config::setString);\n-    return new ScanContext(config.get(CASE_SENSITIVE), config.get(SNAPSHOT_ID), config.get(START_SNAPSHOT_ID),\n-        config.get(END_SNAPSHOT_ID), config.get(AS_OF_TIMESTAMP), config.get(SPLIT_SIZE), config.get(SPLIT_LOOKBACK),\n-        config.get(SPLIT_FILE_OPEN_COST), properties.get(DEFAULT_NAME_MAPPING), projectedSchema, filterExpressions);\n-  }\n-\n   boolean caseSensitive() {\n     return caseSensitive;\n   }\n \n-  ScanContext setCaseSensitive(boolean isCaseSensitive) {\n-    return new ScanContext(isCaseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions);\n-  }\n-\n   Long snapshotId() {\n     return snapshotId;\n   }\n \n-  ScanContext useSnapshotId(Long scanSnapshotId) {\n-    return new ScanContext(caseSensitive, scanSnapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions);\n-  }\n-\n   Long startSnapshotId() {\n     return startSnapshotId;\n   }\n \n-  ScanContext startSnapshotId(Long id) {\n-    return new ScanContext(caseSensitive, snapshotId, id, endSnapshotId, asOfTimestamp, splitSize, splitLookback,\n-        splitOpenFileCost, nameMapping, projectedSchema, filterExpressions);\n-  }\n-\n   Long endSnapshotId() {\n     return endSnapshotId;\n   }\n \n-  ScanContext endSnapshotId(Long id) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, id, asOfTimestamp, splitSize, splitLookback,\n-        splitOpenFileCost, nameMapping, projectedSchema, filterExpressions);\n-  }\n-\n   Long asOfTimestamp() {\n     return asOfTimestamp;\n   }\n \n-  ScanContext asOfTimestamp(Long timestamp) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, timestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions);\n-  }\n-\n   Long splitSize() {\n     return splitSize;\n   }\n \n-  ScanContext splitSize(Long size) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, size,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions);\n-  }\n-\n   Integer splitLookback() {\n     return splitLookback;\n   }\n \n-  ScanContext splitLookback(Integer lookback) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        lookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions);\n-  }\n-\n   Long splitOpenFileCost() {\n     return splitOpenFileCost;\n   }\n \n-  ScanContext splitOpenFileCost(Long fileCost) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, fileCost, nameMapping, projectedSchema, filterExpressions);\n+  boolean isStreaming() {\n+    return isStreaming;\n   }\n \n-  String nameMapping() {\n-    return nameMapping;\n+  Duration monitorInterval() {\n+    return monitorInterval;\n   }\n \n-  ScanContext nameMapping(String mapping) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, mapping, projectedSchema, filterExpressions);\n+  String nameMapping() {\n+    return nameMapping;\n   }\n \n   Schema projectedSchema() {\n     return projectedSchema;\n   }\n \n-  ScanContext project(Schema schema) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, schema, filterExpressions);\n-  }\n-\n   List<Expression> filterExpressions() {\n     return filterExpressions;\n   }\n \n-  ScanContext filterRows(List<Expression> filters) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filters);\n+  ScanContext copyWithAppendsBetween(long newStartSnapshotId, long newEndSnapshotId) {\n+    return ScanContext.builder()\n+        .caseSensitive(caseSensitive)\n+        .useSnapshotId(null)\n+        .startSnapshotId(newStartSnapshotId)\n+        .endSnapshotId(newEndSnapshotId)\n+        .asOfTimestamp(null)\n+        .splitSize(splitSize)\n+        .splitOpenFileCost(splitOpenFileCost)\n+        .nameMapping(nameMapping)\n+        .streaming(isStreaming)\n+        .monitorInterval(monitorInterval)\n+        .build();\n+  }\n+\n+  ScanContext copyWithSnapshotId(long newSnapshotId) {\n+    return ScanContext.builder()\n+        .caseSensitive(caseSensitive)\n+        .useSnapshotId(newSnapshotId)\n+        .startSnapshotId(null)\n+        .endSnapshotId(null)\n+        .asOfTimestamp(null)\n+        .splitSize(splitSize)\n+        .splitOpenFileCost(splitOpenFileCost)\n+        .nameMapping(nameMapping)\n+        .streaming(isStreaming)\n+        .monitorInterval(monitorInterval)\n+        .build();\n+  }\n+\n+  static Builder builder() {\n+    return new Builder();\n+  }\n+\n+  static class Builder {\n+    private boolean caseSensitive;\n+    private Long snapshotId;\n+    private Long startSnapshotId;\n+    private Long endSnapshotId;\n+    private Long asOfTimestamp;\n+    private Long splitSize;\n+    private Integer splitLookback;\n+    private Long splitOpenFileCost;\n+    private boolean isStreaming;\n+    private Duration monitorInterval;\n+    private String nameMapping;\n+    private Schema projectedSchema;\n+    private List<Expression> filterExpressions;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "329c60af56936dc99c784c60a1b4f3511e06abd1"}, "originalPosition": 222}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM4MDE2NDU4", "url": "https://github.com/apache/iceberg/pull/1793#pullrequestreview-538016458", "createdAt": "2020-11-24T22:23:50Z", "commit": {"oid": "329c60af56936dc99c784c60a1b4f3511e06abd1"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNFQyMjoyMzo1MFrOH5Yy0A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNFQyMjoyMzo1MFrOH5Yy0A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTkzNzEwNA==", "bodyText": "It would be good to include what _should _ be used to configure the stream. Looks like it should be startSnapshotId with no endSnapshotId.", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r529937104", "createdAt": "2020-11-24T22:23:50Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/StreamingMonitorFunction.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.List;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.runtime.state.FunctionInitializationContext;\n+import org.apache.flink.runtime.state.FunctionSnapshotContext;\n+import org.apache.flink.streaming.api.checkpoint.CheckpointedFunction;\n+import org.apache.flink.streaming.api.functions.source.RichSourceFunction;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.util.SnapshotUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * This is the single (non-parallel) monitoring task which takes a {@link FlinkInputFormat},\n+ * it is responsible for:\n+ *\n+ * <ol>\n+ *     <li>Monitoring snapshots of the Iceberg table.</li>\n+ *     <li>Creating the {@link FlinkInputSplit splits} corresponding to the incremental files</li>\n+ *     <li>Assigning them to downstream tasks for further processing.</li>\n+ * </ol>\n+ *\n+ * <p>The splits to be read are forwarded to the downstream {@link StreamingReaderOperator}\n+ * which can have parallelism greater than one.\n+ */\n+public class StreamingMonitorFunction extends RichSourceFunction<FlinkInputSplit> implements CheckpointedFunction {\n+\n+  private static final long serialVersionUID = 1L;\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(StreamingMonitorFunction.class);\n+\n+  private static final long DUMMY_START_SNAPSHOT_ID = -1;\n+\n+  private final TableLoader tableLoader;\n+  private final Schema projectedSchema;\n+  private final List<Expression> filterExpressions;\n+  private final ScanContext options;\n+\n+  private volatile boolean isRunning = true;\n+  private transient Object checkpointLock;\n+  private transient Table table;\n+  private transient ListState<Long> snapshotIdState;\n+  private transient long startSnapshotId;\n+\n+  public StreamingMonitorFunction(TableLoader tableLoader, Schema projectedSchema, List<Expression> filterExpressions,\n+                                  ScanContext options) {\n+    Preconditions.checkArgument(\n+        options.snapshotId() == null && options.asOfTimestamp() == null,\n+        \"The streaming reader does not support using snapshot-id or as-of-timestamp to select the table snapshot.\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "329c60af56936dc99c784c60a1b4f3511e06abd1"}, "originalPosition": 78}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM4MDE4MDc1", "url": "https://github.com/apache/iceberg/pull/1793#pullrequestreview-538018075", "createdAt": "2020-11-24T22:26:44Z", "commit": {"oid": "329c60af56936dc99c784c60a1b4f3511e06abd1"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNFQyMjoyNjo0NFrOH5Y_CQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNFQyMjoyNjo0NFrOH5Y_CQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTk0MDIzMw==", "bodyText": "Should this use newOptions?", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r529940233", "createdAt": "2020-11-24T22:26:44Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/StreamingMonitorFunction.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.List;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.runtime.state.FunctionInitializationContext;\n+import org.apache.flink.runtime.state.FunctionSnapshotContext;\n+import org.apache.flink.streaming.api.checkpoint.CheckpointedFunction;\n+import org.apache.flink.streaming.api.functions.source.RichSourceFunction;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.util.SnapshotUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * This is the single (non-parallel) monitoring task which takes a {@link FlinkInputFormat},\n+ * it is responsible for:\n+ *\n+ * <ol>\n+ *     <li>Monitoring snapshots of the Iceberg table.</li>\n+ *     <li>Creating the {@link FlinkInputSplit splits} corresponding to the incremental files</li>\n+ *     <li>Assigning them to downstream tasks for further processing.</li>\n+ * </ol>\n+ *\n+ * <p>The splits to be read are forwarded to the downstream {@link StreamingReaderOperator}\n+ * which can have parallelism greater than one.\n+ */\n+public class StreamingMonitorFunction extends RichSourceFunction<FlinkInputSplit> implements CheckpointedFunction {\n+\n+  private static final long serialVersionUID = 1L;\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(StreamingMonitorFunction.class);\n+\n+  private static final long DUMMY_START_SNAPSHOT_ID = -1;\n+\n+  private final TableLoader tableLoader;\n+  private final Schema projectedSchema;\n+  private final List<Expression> filterExpressions;\n+  private final ScanContext options;\n+\n+  private volatile boolean isRunning = true;\n+  private transient Object checkpointLock;\n+  private transient Table table;\n+  private transient ListState<Long> snapshotIdState;\n+  private transient long startSnapshotId;\n+\n+  public StreamingMonitorFunction(TableLoader tableLoader, Schema projectedSchema, List<Expression> filterExpressions,\n+                                  ScanContext options) {\n+    Preconditions.checkArgument(\n+        options.snapshotId() == null && options.asOfTimestamp() == null,\n+        \"The streaming reader does not support using snapshot-id or as-of-timestamp to select the table snapshot.\");\n+    Preconditions.checkArgument(\n+        options.endSnapshotId() == null, \"The streaming reader does not support using end snapshot id.\");\n+    this.tableLoader = tableLoader;\n+    this.projectedSchema = projectedSchema;\n+    this.filterExpressions = filterExpressions;\n+    this.options = options;\n+  }\n+\n+  @Override\n+  public void initializeState(FunctionInitializationContext context) throws Exception {\n+    snapshotIdState = context.getOperatorStateStore().getListState(\n+        new ListStateDescriptor<>(\n+            \"snapshot-id-state\",\n+            LongSerializer.INSTANCE));\n+    tableLoader.open();\n+    table = tableLoader.loadTable();\n+    if (context.isRestored()) {\n+      LOG.info(\"Restoring state for the {}.\", getClass().getSimpleName());\n+      startSnapshotId = snapshotIdState.get().iterator().next();\n+    } else {\n+      Long optionStartSnapshot = options.startSnapshotId();\n+      if (optionStartSnapshot != null) {\n+        if (!SnapshotUtil.ancestorOf(table, table.currentSnapshot().snapshotId(), optionStartSnapshot)) {\n+          throw new IllegalStateException(\"The option start-snapshot-id \" + optionStartSnapshot +\n+              \" is not an ancestor of the current snapshot\");\n+        }\n+\n+        startSnapshotId = optionStartSnapshot;\n+      } else {\n+        startSnapshotId = DUMMY_START_SNAPSHOT_ID;\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public void run(SourceContext<FlinkInputSplit> ctx) throws Exception {\n+    checkpointLock = ctx.getCheckpointLock();\n+    while (isRunning) {\n+      synchronized (checkpointLock) {\n+        monitorAndForwardSplits(ctx);\n+      }\n+      Thread.sleep(options.monitorInterval().toMillis());\n+    }\n+  }\n+\n+  private void monitorAndForwardSplits(SourceContext<FlinkInputSplit> ctx) {\n+    table.refresh();\n+    Snapshot snapshot = table.currentSnapshot();\n+    if (snapshot != null && snapshot.snapshotId() != startSnapshotId) {\n+      long snapshotId = snapshot.snapshotId();\n+      // Read current static table if startSnapshotId not set.\n+      ScanContext newOptions = startSnapshotId == DUMMY_START_SNAPSHOT_ID ?\n+          options.copyWithSnapshotId(snapshotId) :\n+          options.copyWithAppendsBetween(startSnapshotId, snapshotId);\n+\n+      FlinkInputSplit[] splits = FlinkSplitGenerator.createInputSplits(table, options);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "329c60af56936dc99c784c60a1b4f3511e06abd1"}, "originalPosition": 134}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM4MDIwMzcw", "url": "https://github.com/apache/iceberg/pull/1793#pullrequestreview-538020370", "createdAt": "2020-11-24T22:30:55Z", "commit": {"oid": "329c60af56936dc99c784c60a1b4f3511e06abd1"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNFQyMjozMDo1NVrOH5ZP7Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNFQyMjozMDo1NVrOH5ZP7Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTk0NDU1Nw==", "bodyText": "Okay, so it looks like state is always tracked at the snapshot granularity, and the checkpointLock is used to guarantee that a snapshot's data is fully emitted or not emitted at all. Is that right?", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r529944557", "createdAt": "2020-11-24T22:30:55Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/StreamingMonitorFunction.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.List;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.runtime.state.FunctionInitializationContext;\n+import org.apache.flink.runtime.state.FunctionSnapshotContext;\n+import org.apache.flink.streaming.api.checkpoint.CheckpointedFunction;\n+import org.apache.flink.streaming.api.functions.source.RichSourceFunction;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.util.SnapshotUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * This is the single (non-parallel) monitoring task which takes a {@link FlinkInputFormat},\n+ * it is responsible for:\n+ *\n+ * <ol>\n+ *     <li>Monitoring snapshots of the Iceberg table.</li>\n+ *     <li>Creating the {@link FlinkInputSplit splits} corresponding to the incremental files</li>\n+ *     <li>Assigning them to downstream tasks for further processing.</li>\n+ * </ol>\n+ *\n+ * <p>The splits to be read are forwarded to the downstream {@link StreamingReaderOperator}\n+ * which can have parallelism greater than one.\n+ */\n+public class StreamingMonitorFunction extends RichSourceFunction<FlinkInputSplit> implements CheckpointedFunction {\n+\n+  private static final long serialVersionUID = 1L;\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(StreamingMonitorFunction.class);\n+\n+  private static final long DUMMY_START_SNAPSHOT_ID = -1;\n+\n+  private final TableLoader tableLoader;\n+  private final Schema projectedSchema;\n+  private final List<Expression> filterExpressions;\n+  private final ScanContext options;\n+\n+  private volatile boolean isRunning = true;\n+  private transient Object checkpointLock;\n+  private transient Table table;\n+  private transient ListState<Long> snapshotIdState;\n+  private transient long startSnapshotId;\n+\n+  public StreamingMonitorFunction(TableLoader tableLoader, Schema projectedSchema, List<Expression> filterExpressions,\n+                                  ScanContext options) {\n+    Preconditions.checkArgument(\n+        options.snapshotId() == null && options.asOfTimestamp() == null,\n+        \"The streaming reader does not support using snapshot-id or as-of-timestamp to select the table snapshot.\");\n+    Preconditions.checkArgument(\n+        options.endSnapshotId() == null, \"The streaming reader does not support using end snapshot id.\");\n+    this.tableLoader = tableLoader;\n+    this.projectedSchema = projectedSchema;\n+    this.filterExpressions = filterExpressions;\n+    this.options = options;\n+  }\n+\n+  @Override\n+  public void initializeState(FunctionInitializationContext context) throws Exception {\n+    snapshotIdState = context.getOperatorStateStore().getListState(\n+        new ListStateDescriptor<>(\n+            \"snapshot-id-state\",\n+            LongSerializer.INSTANCE));\n+    tableLoader.open();\n+    table = tableLoader.loadTable();\n+    if (context.isRestored()) {\n+      LOG.info(\"Restoring state for the {}.\", getClass().getSimpleName());\n+      startSnapshotId = snapshotIdState.get().iterator().next();\n+    } else {\n+      Long optionStartSnapshot = options.startSnapshotId();\n+      if (optionStartSnapshot != null) {\n+        if (!SnapshotUtil.ancestorOf(table, table.currentSnapshot().snapshotId(), optionStartSnapshot)) {\n+          throw new IllegalStateException(\"The option start-snapshot-id \" + optionStartSnapshot +\n+              \" is not an ancestor of the current snapshot\");\n+        }\n+\n+        startSnapshotId = optionStartSnapshot;\n+      } else {\n+        startSnapshotId = DUMMY_START_SNAPSHOT_ID;\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public void run(SourceContext<FlinkInputSplit> ctx) throws Exception {\n+    checkpointLock = ctx.getCheckpointLock();\n+    while (isRunning) {\n+      synchronized (checkpointLock) {\n+        monitorAndForwardSplits(ctx);\n+      }\n+      Thread.sleep(options.monitorInterval().toMillis());\n+    }\n+  }\n+\n+  private void monitorAndForwardSplits(SourceContext<FlinkInputSplit> ctx) {\n+    table.refresh();\n+    Snapshot snapshot = table.currentSnapshot();\n+    if (snapshot != null && snapshot.snapshotId() != startSnapshotId) {\n+      long snapshotId = snapshot.snapshotId();\n+      // Read current static table if startSnapshotId not set.\n+      ScanContext newOptions = startSnapshotId == DUMMY_START_SNAPSHOT_ID ?\n+          options.copyWithSnapshotId(snapshotId) :\n+          options.copyWithAppendsBetween(startSnapshotId, snapshotId);\n+\n+      FlinkInputSplit[] splits = FlinkSplitGenerator.createInputSplits(table, options);\n+      for (FlinkInputSplit split : splits) {\n+        ctx.collect(split);\n+      }\n+      startSnapshotId = snapshotId;\n+    }\n+  }\n+\n+  @Override\n+  public void cancel() {\n+    // this is to cover the case where cancel() is called before the run()\n+    if (checkpointLock != null) {\n+      synchronized (checkpointLock) {\n+        isRunning = false;\n+      }\n+    } else {\n+      isRunning = false;\n+    }\n+\n+    if (tableLoader != null) {\n+      try {\n+        tableLoader.close();\n+      } catch (IOException e) {\n+        throw new UncheckedIOException(e);\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public void close() {\n+    cancel();\n+  }\n+\n+  @Override\n+  public void snapshotState(FunctionSnapshotContext context) throws Exception {\n+    snapshotIdState.clear();\n+    snapshotIdState.add(startSnapshotId);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "329c60af56936dc99c784c60a1b4f3511e06abd1"}, "originalPosition": 170}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM4MDIxNDYy", "url": "https://github.com/apache/iceberg/pull/1793#pullrequestreview-538021462", "createdAt": "2020-11-24T22:33:01Z", "commit": {"oid": "329c60af56936dc99c784c60a1b4f3511e06abd1"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNFQyMjozMzowMVrOH5ZYHg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNFQyMjozMzowMVrOH5ZYHg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTk0NjY1NA==", "bodyText": "I think this also need to check isRunning. Otherwise, this thread could check isRunning and then stop before getting the lock. Then cancel could run in another thread. Once cancel succeeds, I'm assuming that it is okay for Flink to call snapshotState to produce state with the current snapshot. But then this thread might wake up and start producing splits and update startSnapshotId.", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r529946654", "createdAt": "2020-11-24T22:33:01Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/StreamingMonitorFunction.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.List;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.runtime.state.FunctionInitializationContext;\n+import org.apache.flink.runtime.state.FunctionSnapshotContext;\n+import org.apache.flink.streaming.api.checkpoint.CheckpointedFunction;\n+import org.apache.flink.streaming.api.functions.source.RichSourceFunction;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.util.SnapshotUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * This is the single (non-parallel) monitoring task which takes a {@link FlinkInputFormat},\n+ * it is responsible for:\n+ *\n+ * <ol>\n+ *     <li>Monitoring snapshots of the Iceberg table.</li>\n+ *     <li>Creating the {@link FlinkInputSplit splits} corresponding to the incremental files</li>\n+ *     <li>Assigning them to downstream tasks for further processing.</li>\n+ * </ol>\n+ *\n+ * <p>The splits to be read are forwarded to the downstream {@link StreamingReaderOperator}\n+ * which can have parallelism greater than one.\n+ */\n+public class StreamingMonitorFunction extends RichSourceFunction<FlinkInputSplit> implements CheckpointedFunction {\n+\n+  private static final long serialVersionUID = 1L;\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(StreamingMonitorFunction.class);\n+\n+  private static final long DUMMY_START_SNAPSHOT_ID = -1;\n+\n+  private final TableLoader tableLoader;\n+  private final Schema projectedSchema;\n+  private final List<Expression> filterExpressions;\n+  private final ScanContext options;\n+\n+  private volatile boolean isRunning = true;\n+  private transient Object checkpointLock;\n+  private transient Table table;\n+  private transient ListState<Long> snapshotIdState;\n+  private transient long startSnapshotId;\n+\n+  public StreamingMonitorFunction(TableLoader tableLoader, Schema projectedSchema, List<Expression> filterExpressions,\n+                                  ScanContext options) {\n+    Preconditions.checkArgument(\n+        options.snapshotId() == null && options.asOfTimestamp() == null,\n+        \"The streaming reader does not support using snapshot-id or as-of-timestamp to select the table snapshot.\");\n+    Preconditions.checkArgument(\n+        options.endSnapshotId() == null, \"The streaming reader does not support using end snapshot id.\");\n+    this.tableLoader = tableLoader;\n+    this.projectedSchema = projectedSchema;\n+    this.filterExpressions = filterExpressions;\n+    this.options = options;\n+  }\n+\n+  @Override\n+  public void initializeState(FunctionInitializationContext context) throws Exception {\n+    snapshotIdState = context.getOperatorStateStore().getListState(\n+        new ListStateDescriptor<>(\n+            \"snapshot-id-state\",\n+            LongSerializer.INSTANCE));\n+    tableLoader.open();\n+    table = tableLoader.loadTable();\n+    if (context.isRestored()) {\n+      LOG.info(\"Restoring state for the {}.\", getClass().getSimpleName());\n+      startSnapshotId = snapshotIdState.get().iterator().next();\n+    } else {\n+      Long optionStartSnapshot = options.startSnapshotId();\n+      if (optionStartSnapshot != null) {\n+        if (!SnapshotUtil.ancestorOf(table, table.currentSnapshot().snapshotId(), optionStartSnapshot)) {\n+          throw new IllegalStateException(\"The option start-snapshot-id \" + optionStartSnapshot +\n+              \" is not an ancestor of the current snapshot\");\n+        }\n+\n+        startSnapshotId = optionStartSnapshot;\n+      } else {\n+        startSnapshotId = DUMMY_START_SNAPSHOT_ID;\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public void run(SourceContext<FlinkInputSplit> ctx) throws Exception {\n+    checkpointLock = ctx.getCheckpointLock();\n+    while (isRunning) {\n+      synchronized (checkpointLock) {\n+        monitorAndForwardSplits(ctx);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "329c60af56936dc99c784c60a1b4f3511e06abd1"}, "originalPosition": 118}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM4MDI0MTQw", "url": "https://github.com/apache/iceberg/pull/1793#pullrequestreview-538024140", "createdAt": "2020-11-24T22:37:34Z", "commit": {"oid": "329c60af56936dc99c784c60a1b4f3511e06abd1"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNFQyMjozNzozNVrOH5Zr0w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNFQyMjozNzozNVrOH5Zr0w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTk1MTY5OQ==", "bodyText": "Hm, didn't we decide not to set these because we don't actually want to guarantee Java serialization across Iceberg versions? I'm okay with this, but I think it makes it appear that we support Java serialization across versions when we probably will not.", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r529951699", "createdAt": "2020-11-24T22:37:35Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/StreamingReaderOperator.java", "diffHunk": "@@ -0,0 +1,220 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.util.Queue;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.runtime.state.JavaSerializer;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.MailboxExecutor;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.StreamOperator;\n+import org.apache.flink.streaming.api.operators.StreamOperatorParameters;\n+import org.apache.flink.streaming.api.operators.StreamSourceContexts;\n+import org.apache.flink.streaming.api.operators.YieldingOperatorFactory;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.streaming.runtime.tasks.ProcessingTimeService;\n+import org.apache.flink.streaming.runtime.tasks.mailbox.MailboxExecutorImpl;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * The operator that reads the {@link FlinkInputSplit splits} received from the preceding {@link\n+ * StreamingMonitorFunction}. Contrary to the {@link StreamingMonitorFunction} which has a parallelism of 1,\n+ * this operator can have DOP > 1.\n+ *\n+ * <p>As soon as a split descriptor is received, it is put in a queue, and use {@link MailboxExecutor}\n+ * read the actual data of the split. This architecture allows the separation of the reading thread from the one split\n+ * processing the checkpoint barriers, thus removing any potential back-pressure.\n+ */\n+public class StreamingReaderOperator extends AbstractStreamOperator<RowData>\n+    implements OneInputStreamOperator<FlinkInputSplit, RowData> {\n+\n+  private static final long serialVersionUID = 1L;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "329c60af56936dc99c784c60a1b4f3511e06abd1"}, "originalPosition": 61}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM4MDMwMTc3", "url": "https://github.com/apache/iceberg/pull/1793#pullrequestreview-538030177", "createdAt": "2020-11-24T22:48:42Z", "commit": {"oid": "329c60af56936dc99c784c60a1b4f3511e06abd1"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNFQyMjo0ODo0M1rOH5aaBQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNFQyMjo0ODo0M1rOH5aaBQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTk2MzUyNQ==", "bodyText": "Just to be sure, collect is the right method to call to emit a record? That seems oddly named to me.", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r529963525", "createdAt": "2020-11-24T22:48:43Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/StreamingReaderOperator.java", "diffHunk": "@@ -0,0 +1,220 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.util.Queue;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.runtime.state.JavaSerializer;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.MailboxExecutor;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.StreamOperator;\n+import org.apache.flink.streaming.api.operators.StreamOperatorParameters;\n+import org.apache.flink.streaming.api.operators.StreamSourceContexts;\n+import org.apache.flink.streaming.api.operators.YieldingOperatorFactory;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.streaming.runtime.tasks.ProcessingTimeService;\n+import org.apache.flink.streaming.runtime.tasks.mailbox.MailboxExecutorImpl;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * The operator that reads the {@link FlinkInputSplit splits} received from the preceding {@link\n+ * StreamingMonitorFunction}. Contrary to the {@link StreamingMonitorFunction} which has a parallelism of 1,\n+ * this operator can have DOP > 1.\n+ *\n+ * <p>As soon as a split descriptor is received, it is put in a queue, and use {@link MailboxExecutor}\n+ * read the actual data of the split. This architecture allows the separation of the reading thread from the one split\n+ * processing the checkpoint barriers, thus removing any potential back-pressure.\n+ */\n+public class StreamingReaderOperator extends AbstractStreamOperator<RowData>\n+    implements OneInputStreamOperator<FlinkInputSplit, RowData> {\n+\n+  private static final long serialVersionUID = 1L;\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(StreamingReaderOperator.class);\n+  private final MailboxExecutorImpl executor;\n+  private FlinkInputFormat format;\n+\n+  private transient SourceFunction.SourceContext<RowData> readerContext;\n+\n+  private transient ListState<FlinkInputSplit> checkpointState;\n+  private transient Queue<FlinkInputSplit> splits;\n+\n+  private StreamingReaderOperator(\n+      FlinkInputFormat format, ProcessingTimeService timeService, MailboxExecutor mailboxExecutor) {\n+    this.format = Preconditions.checkNotNull(format, \"The InputFormat should not be null.\");\n+    this.processingTimeService = timeService;\n+    this.executor = (MailboxExecutorImpl) Preconditions.checkNotNull(\n+        mailboxExecutor, \"The mailboxExecutor should not be null.\");\n+  }\n+\n+  @Override\n+  public void initializeState(StateInitializationContext context) throws Exception {\n+    super.initializeState(context);\n+\n+    checkpointState = context.getOperatorStateStore().getListState(\n+        new ListStateDescriptor<>(\"splits\", new JavaSerializer<>()));\n+\n+    int subtaskIdx = getRuntimeContext().getIndexOfThisSubtask();\n+    splits = Lists.newLinkedList();\n+    if (context.isRestored()) {\n+      LOG.info(\"Restoring state for the {} (taskIdx={}).\", getClass().getSimpleName(), subtaskIdx);\n+\n+      for (FlinkInputSplit split : checkpointState.get()) {\n+        splits.add(split);\n+      }\n+    }\n+\n+    this.readerContext = StreamSourceContexts.getSourceContext(\n+        getOperatorConfig().getTimeCharacteristic(),\n+        getProcessingTimeService(),\n+        new Object(), // no actual locking needed\n+        getContainingTask().getStreamStatusMaintainer(),\n+        output,\n+        getRuntimeContext().getExecutionConfig().getAutoWatermarkInterval(),\n+        -1);\n+\n+    if (!splits.isEmpty()) {\n+      enqueueProcessSplits();\n+    }\n+  }\n+\n+  @Override\n+  public void processElement(StreamRecord<FlinkInputSplit> element) {\n+    splits.offer(element.getValue());\n+    enqueueProcessSplits();\n+  }\n+\n+  private void enqueueProcessSplits() {\n+    executor.execute(this::processSplits, this.getClass().getSimpleName());\n+  }\n+\n+  private void processSplits() throws IOException {\n+    do {\n+      FlinkInputSplit split = splits.poll();\n+      if (split == null) {\n+        return;\n+      }\n+\n+      LOG.debug(\"load split: {}\", split);\n+      format.open(split);\n+\n+      RowData nextElement = null;\n+      while (!format.reachedEnd()) {\n+        nextElement = format.nextRecord(nextElement);\n+        if (nextElement != null) {\n+          readerContext.collect(nextElement);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "329c60af56936dc99c784c60a1b4f3511e06abd1"}, "originalPosition": 135}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM4MDMwNTcw", "url": "https://github.com/apache/iceberg/pull/1793#pullrequestreview-538030570", "createdAt": "2020-11-24T22:49:24Z", "commit": {"oid": "329c60af56936dc99c784c60a1b4f3511e06abd1"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNFQyMjo0OToyNFrOH5acpQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNFQyMjo0OToyNFrOH5acpQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTk2NDE5Nw==", "bodyText": "Why does this submit itself again?", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r529964197", "createdAt": "2020-11-24T22:49:24Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/StreamingReaderOperator.java", "diffHunk": "@@ -0,0 +1,220 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.util.Queue;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.runtime.state.JavaSerializer;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.MailboxExecutor;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.StreamOperator;\n+import org.apache.flink.streaming.api.operators.StreamOperatorParameters;\n+import org.apache.flink.streaming.api.operators.StreamSourceContexts;\n+import org.apache.flink.streaming.api.operators.YieldingOperatorFactory;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.streaming.runtime.tasks.ProcessingTimeService;\n+import org.apache.flink.streaming.runtime.tasks.mailbox.MailboxExecutorImpl;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * The operator that reads the {@link FlinkInputSplit splits} received from the preceding {@link\n+ * StreamingMonitorFunction}. Contrary to the {@link StreamingMonitorFunction} which has a parallelism of 1,\n+ * this operator can have DOP > 1.\n+ *\n+ * <p>As soon as a split descriptor is received, it is put in a queue, and use {@link MailboxExecutor}\n+ * read the actual data of the split. This architecture allows the separation of the reading thread from the one split\n+ * processing the checkpoint barriers, thus removing any potential back-pressure.\n+ */\n+public class StreamingReaderOperator extends AbstractStreamOperator<RowData>\n+    implements OneInputStreamOperator<FlinkInputSplit, RowData> {\n+\n+  private static final long serialVersionUID = 1L;\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(StreamingReaderOperator.class);\n+  private final MailboxExecutorImpl executor;\n+  private FlinkInputFormat format;\n+\n+  private transient SourceFunction.SourceContext<RowData> readerContext;\n+\n+  private transient ListState<FlinkInputSplit> checkpointState;\n+  private transient Queue<FlinkInputSplit> splits;\n+\n+  private StreamingReaderOperator(\n+      FlinkInputFormat format, ProcessingTimeService timeService, MailboxExecutor mailboxExecutor) {\n+    this.format = Preconditions.checkNotNull(format, \"The InputFormat should not be null.\");\n+    this.processingTimeService = timeService;\n+    this.executor = (MailboxExecutorImpl) Preconditions.checkNotNull(\n+        mailboxExecutor, \"The mailboxExecutor should not be null.\");\n+  }\n+\n+  @Override\n+  public void initializeState(StateInitializationContext context) throws Exception {\n+    super.initializeState(context);\n+\n+    checkpointState = context.getOperatorStateStore().getListState(\n+        new ListStateDescriptor<>(\"splits\", new JavaSerializer<>()));\n+\n+    int subtaskIdx = getRuntimeContext().getIndexOfThisSubtask();\n+    splits = Lists.newLinkedList();\n+    if (context.isRestored()) {\n+      LOG.info(\"Restoring state for the {} (taskIdx={}).\", getClass().getSimpleName(), subtaskIdx);\n+\n+      for (FlinkInputSplit split : checkpointState.get()) {\n+        splits.add(split);\n+      }\n+    }\n+\n+    this.readerContext = StreamSourceContexts.getSourceContext(\n+        getOperatorConfig().getTimeCharacteristic(),\n+        getProcessingTimeService(),\n+        new Object(), // no actual locking needed\n+        getContainingTask().getStreamStatusMaintainer(),\n+        output,\n+        getRuntimeContext().getExecutionConfig().getAutoWatermarkInterval(),\n+        -1);\n+\n+    if (!splits.isEmpty()) {\n+      enqueueProcessSplits();\n+    }\n+  }\n+\n+  @Override\n+  public void processElement(StreamRecord<FlinkInputSplit> element) {\n+    splits.offer(element.getValue());\n+    enqueueProcessSplits();\n+  }\n+\n+  private void enqueueProcessSplits() {\n+    executor.execute(this::processSplits, this.getClass().getSimpleName());\n+  }\n+\n+  private void processSplits() throws IOException {\n+    do {\n+      FlinkInputSplit split = splits.poll();\n+      if (split == null) {\n+        return;\n+      }\n+\n+      LOG.debug(\"load split: {}\", split);\n+      format.open(split);\n+\n+      RowData nextElement = null;\n+      while (!format.reachedEnd()) {\n+        nextElement = format.nextRecord(nextElement);\n+        if (nextElement != null) {\n+          readerContext.collect(nextElement);\n+        } else {\n+          break;\n+        }\n+      }\n+\n+      format.close();\n+    } while (executor.isIdle());\n+    enqueueProcessSplits();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "329c60af56936dc99c784c60a1b4f3511e06abd1"}, "originalPosition": 143}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a63e4c6d0658c8cb02e13426375ba84cfe38eda2", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/a63e4c6d0658c8cb02e13426375ba84cfe38eda2", "committedDate": "2021-01-04T13:12:37Z", "message": "Flink: Support streaming reader."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3250db974998ec3b6411fb44dcfa99e7979bff5a", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/3250db974998ec3b6411fb44dcfa99e7979bff5a", "committedDate": "2021-01-04T13:13:32Z", "message": "Minor fixes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "abf9e2b529781f2981973b3161d4ce939ce98665", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/abf9e2b529781f2981973b3161d4ce939ce98665", "committedDate": "2021-01-04T13:15:48Z", "message": "Addressing comments."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5c6a0b7ad3a8d4e88f049c7e51510edcba258eb7", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/5c6a0b7ad3a8d4e88f049c7e51510edcba258eb7", "committedDate": "2021-01-04T13:15:48Z", "message": "Minor changes."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "cd0ab3f526106da45fa53a02a7e3e89ff92043dc", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/cd0ab3f526106da45fa53a02a7e3e89ff92043dc", "committedDate": "2020-12-04T08:18:17Z", "message": "Addressing comments."}, "afterCommit": {"oid": "5c6a0b7ad3a8d4e88f049c7e51510edcba258eb7", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/5c6a0b7ad3a8d4e88f049c7e51510edcba258eb7", "committedDate": "2021-01-04T13:15:48Z", "message": "Minor changes."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "506a35f3012c8f278988795b6685474c818532e2", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/506a35f3012c8f278988795b6685474c818532e2", "committedDate": "2021-01-04T13:35:42Z", "message": "Address minor issues"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTYxMDM1NjA1", "url": "https://github.com/apache/iceberg/pull/1793#pullrequestreview-561035605", "createdAt": "2021-01-04T13:40:17Z", "commit": {"oid": "5c6a0b7ad3a8d4e88f049c7e51510edcba258eb7"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNFQxMzo0MDoxN1rOINyCZA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNFQxMzo0MDoxN1rOINyCZA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTMyMjIxMg==", "bodyText": "I think it's good to make this unit tests to extend FlinkTestBase class.", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r551322212", "createdAt": "2021-01-04T13:40:17Z", "author": {"login": "openinx"}, "path": "flink/src/test/java/org/apache/iceberg/flink/source/TestStreamScanSql.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.Iterator;\n+import java.util.List;\n+import org.apache.flink.streaming.api.environment.ExecutionCheckpointingOptions;\n+import org.apache.flink.table.api.EnvironmentSettings;\n+import org.apache.flink.table.api.TableEnvironment;\n+import org.apache.flink.table.api.TableResult;\n+import org.apache.flink.table.api.config.TableConfigOptions;\n+import org.apache.flink.test.util.AbstractTestBase;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.util.CloseableIterator;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.GenericAppenderHelper;\n+import org.apache.iceberg.data.RandomGenericData;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.Types;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import static org.apache.iceberg.types.Types.NestedField.required;\n+\n+public class TestStreamScanSql extends AbstractTestBase {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5c6a0b7ad3a8d4e88f049c7e51510edcba258eb7"}, "originalPosition": 53}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "243e3ba6c1c2829bfedf8e36b6744650d4c4d9f7", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/243e3ba6c1c2829bfedf8e36b6744650d4c4d9f7", "committedDate": "2021-01-05T10:05:11Z", "message": "Fix the broken unit tests."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1b757fd8fdd6997a4681652608bf14e1c949a3bb", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/1b757fd8fdd6997a4681652608bf14e1c949a3bb", "committedDate": "2021-01-05T10:54:45Z", "message": "Fix broken unit tests."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6ea85e37fded0c67538f3c7e1f8c33a47e033eb5", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/6ea85e37fded0c67538f3c7e1f8c33a47e033eb5", "committedDate": "2021-01-05T11:44:41Z", "message": "Fix javadoc"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTYyOTcyODYz", "url": "https://github.com/apache/iceberg/pull/1793#pullrequestreview-562972863", "createdAt": "2021-01-06T19:11:55Z", "commit": {"oid": "6ea85e37fded0c67538f3c7e1f8c33a47e033eb5"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNlQxOToxMTo1NVrOIPS__Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNlQxOToxMTo1NVrOIPS__Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjkxMDg0NQ==", "bodyText": "I have been wondering if really need to duplicate every ScanContext/Builder API in the FlinkSource. Should we just let FlinkSource builder take the ScanContext, which is constructed separately before the FlinkSource?", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r552910845", "createdAt": "2021-01-06T19:11:55Z", "author": {"login": "stevenzwu"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "diffHunk": "@@ -91,7 +89,7 @@ public Builder env(StreamExecutionEnvironment newEnv) {\n     }\n \n     public Builder filters(List<Expression> filters) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6ea85e37fded0c67538f3c7e1f8c33a47e033eb5"}, "originalPosition": 30}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTYyOTg0MDI3", "url": "https://github.com/apache/iceberg/pull/1793#pullrequestreview-562984027", "createdAt": "2021-01-06T19:29:41Z", "commit": {"oid": "6ea85e37fded0c67538f3c7e1f8c33a47e033eb5"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNlQxOToyOTo0MVrOIPTiFg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNlQxOToyOTo0MVrOIPTiFg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjkxOTU3NA==", "bodyText": "Should we introduce a start strategy config?\nhttps://github.com/stevenzwu/iceberg/blob/flip27IcebergSource/flink/src/main/java/org/apache/iceberg/flink/source/enumerator/ContinuousEnumConfig.java#L35\nI am also wondering if we should move the continuous monitoring config outside ScanContext?", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r552919574", "createdAt": "2021-01-06T19:29:41Z", "author": {"login": "stevenzwu"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/StreamingMonitorFunction.java", "diffHunk": "@@ -0,0 +1,163 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.runtime.state.FunctionInitializationContext;\n+import org.apache.flink.runtime.state.FunctionSnapshotContext;\n+import org.apache.flink.streaming.api.checkpoint.CheckpointedFunction;\n+import org.apache.flink.streaming.api.functions.source.RichSourceFunction;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.util.SnapshotUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * This is the single (non-parallel) monitoring task which takes a {@link FlinkInputFormat},\n+ * it is responsible for:\n+ *\n+ * <ol>\n+ *     <li>Monitoring snapshots of the Iceberg table.</li>\n+ *     <li>Creating the {@link FlinkInputSplit splits} corresponding to the incremental files</li>\n+ *     <li>Assigning them to downstream tasks for further processing.</li>\n+ * </ol>\n+ *\n+ * <p>The splits to be read are forwarded to the downstream {@link StreamingReaderOperator}\n+ * which can have parallelism greater than one.\n+ */\n+public class StreamingMonitorFunction extends RichSourceFunction<FlinkInputSplit> implements CheckpointedFunction {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(StreamingMonitorFunction.class);\n+\n+  private static final long DUMMY_START_SNAPSHOT_ID = -1;\n+\n+  private final TableLoader tableLoader;\n+  private final ScanContext ctxt;\n+\n+  private volatile boolean isRunning = true;\n+  private transient Object checkpointLock;\n+  private transient Table table;\n+  private transient ListState<Long> snapshotIdState;\n+  private transient long startSnapshotId;\n+\n+  public StreamingMonitorFunction(TableLoader tableLoader, ScanContext ctxt) {\n+    Preconditions.checkArgument(ctxt.snapshotId() == null && ctxt.asOfTimestamp() == null,\n+        \"The streaming reader does not support using snapshot-id or as-of-timestamp to select the table snapshot.\");\n+    Preconditions.checkArgument(ctxt.endSnapshotId() == null,\n+        \"The streaming reader does not support using end snapshot id.\");\n+    this.tableLoader = tableLoader;\n+    this.ctxt = ctxt;\n+  }\n+\n+  @Override\n+  public void initializeState(FunctionInitializationContext context) throws Exception {\n+    snapshotIdState = context.getOperatorStateStore().getListState(\n+        new ListStateDescriptor<>(\n+            \"snapshot-id-state\",\n+            LongSerializer.INSTANCE));\n+    tableLoader.open();\n+    table = tableLoader.loadTable();\n+    if (context.isRestored()) {\n+      LOG.info(\"Restoring state for the {}.\", getClass().getSimpleName());\n+      startSnapshotId = snapshotIdState.get().iterator().next();\n+    } else {\n+      Long optionStartSnapshot = ctxt.startSnapshotId();\n+      if (optionStartSnapshot != null) {\n+        if (!SnapshotUtil.ancestorOf(table, table.currentSnapshot().snapshotId(), optionStartSnapshot)) {\n+          throw new IllegalStateException(\"The option start-snapshot-id \" + optionStartSnapshot +\n+              \" is not an ancestor of the current snapshot\");\n+        }\n+\n+        startSnapshotId = optionStartSnapshot;\n+      } else {\n+        startSnapshotId = DUMMY_START_SNAPSHOT_ID;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6ea85e37fded0c67538f3c7e1f8c33a47e033eb5"}, "originalPosition": 97}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTYyOTg5OTQ1", "url": "https://github.com/apache/iceberg/pull/1793#pullrequestreview-562989945", "createdAt": "2021-01-06T19:39:26Z", "commit": {"oid": "6ea85e37fded0c67538f3c7e1f8c33a47e033eb5"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNlQxOTozOToyNlrOIPT0Hg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNlQxOTozOToyNlrOIPT0Hg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjkyNDE5MA==", "bodyText": "I opened issue-1698 a while back regarding a more stable serializer (than Java serializable) for Flink checkpointing. While Java serialization works well for batch jobs, we need a more stable serialization to support schema evolution for long-running stream jobs. We need sth like DeltaManifestsSerializer and ManifestFiles.encode for CombinedScanTask.\nIt doesn't have to be done in the initial version. So I don't think it is a blocker for this PR.", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r552924190", "createdAt": "2021-01-06T19:39:26Z", "author": {"login": "stevenzwu"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/StreamingReaderOperator.java", "diffHunk": "@@ -0,0 +1,216 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.util.Queue;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.runtime.state.JavaSerializer;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.MailboxExecutor;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.StreamOperator;\n+import org.apache.flink.streaming.api.operators.StreamOperatorParameters;\n+import org.apache.flink.streaming.api.operators.StreamSourceContexts;\n+import org.apache.flink.streaming.api.operators.YieldingOperatorFactory;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.streaming.runtime.tasks.ProcessingTimeService;\n+import org.apache.flink.streaming.runtime.tasks.mailbox.MailboxExecutorImpl;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * The operator that reads the {@link FlinkInputSplit splits} received from the preceding {@link\n+ * StreamingMonitorFunction}. Contrary to the {@link StreamingMonitorFunction} which has a parallelism of 1,\n+ * this operator can have multiple parallelism.\n+ *\n+ * <p>As soon as a split descriptor is received, it is put in a queue, and use {@link MailboxExecutor}\n+ * read the actual data of the split. This architecture allows the separation of the reading thread from the one split\n+ * processing the checkpoint barriers, thus removing any potential back-pressure.\n+ */\n+public class StreamingReaderOperator extends AbstractStreamOperator<RowData>\n+    implements OneInputStreamOperator<FlinkInputSplit, RowData> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(StreamingReaderOperator.class);\n+  private final MailboxExecutorImpl executor;\n+  private FlinkInputFormat format;\n+\n+  private transient SourceFunction.SourceContext<RowData> readerContext;\n+\n+  private transient ListState<FlinkInputSplit> checkpointState;\n+  private transient Queue<FlinkInputSplit> splits;\n+\n+  private StreamingReaderOperator(FlinkInputFormat format, ProcessingTimeService timeService,\n+                                  MailboxExecutor mailboxExecutor) {\n+    this.format = Preconditions.checkNotNull(format, \"The InputFormat should not be null.\");\n+    this.processingTimeService = timeService;\n+    this.executor = (MailboxExecutorImpl) Preconditions.checkNotNull(mailboxExecutor,\n+        \"The mailboxExecutor should not be null.\");\n+  }\n+\n+  @Override\n+  public void initializeState(StateInitializationContext context) throws Exception {\n+    super.initializeState(context);\n+\n+    checkpointState = context.getOperatorStateStore().getListState(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6ea85e37fded0c67538f3c7e1f8c33a47e033eb5"}, "originalPosition": 82}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTYzMDQ4NDU0", "url": "https://github.com/apache/iceberg/pull/1793#pullrequestreview-563048454", "createdAt": "2021-01-06T21:29:26Z", "commit": {"oid": "6ea85e37fded0c67538f3c7e1f8c33a47e033eb5"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNlQyMToyOToyN1rOIPWnXA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNlQyMToyOToyN1rOIPWnXA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mjk3MDA3Ng==", "bodyText": "should we use addAll instead, which translates to one merge call in rocksdb?", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r552970076", "createdAt": "2021-01-06T21:29:27Z", "author": {"login": "stevenzwu"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/StreamingReaderOperator.java", "diffHunk": "@@ -0,0 +1,216 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.util.Queue;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.runtime.state.JavaSerializer;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.MailboxExecutor;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.StreamOperator;\n+import org.apache.flink.streaming.api.operators.StreamOperatorParameters;\n+import org.apache.flink.streaming.api.operators.StreamSourceContexts;\n+import org.apache.flink.streaming.api.operators.YieldingOperatorFactory;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.streaming.runtime.tasks.ProcessingTimeService;\n+import org.apache.flink.streaming.runtime.tasks.mailbox.MailboxExecutorImpl;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * The operator that reads the {@link FlinkInputSplit splits} received from the preceding {@link\n+ * StreamingMonitorFunction}. Contrary to the {@link StreamingMonitorFunction} which has a parallelism of 1,\n+ * this operator can have multiple parallelism.\n+ *\n+ * <p>As soon as a split descriptor is received, it is put in a queue, and use {@link MailboxExecutor}\n+ * read the actual data of the split. This architecture allows the separation of the reading thread from the one split\n+ * processing the checkpoint barriers, thus removing any potential back-pressure.\n+ */\n+public class StreamingReaderOperator extends AbstractStreamOperator<RowData>\n+    implements OneInputStreamOperator<FlinkInputSplit, RowData> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(StreamingReaderOperator.class);\n+  private final MailboxExecutorImpl executor;\n+  private FlinkInputFormat format;\n+\n+  private transient SourceFunction.SourceContext<RowData> readerContext;\n+\n+  private transient ListState<FlinkInputSplit> checkpointState;\n+  private transient Queue<FlinkInputSplit> splits;\n+\n+  private StreamingReaderOperator(FlinkInputFormat format, ProcessingTimeService timeService,\n+                                  MailboxExecutor mailboxExecutor) {\n+    this.format = Preconditions.checkNotNull(format, \"The InputFormat should not be null.\");\n+    this.processingTimeService = timeService;\n+    this.executor = (MailboxExecutorImpl) Preconditions.checkNotNull(mailboxExecutor,\n+        \"The mailboxExecutor should not be null.\");\n+  }\n+\n+  @Override\n+  public void initializeState(StateInitializationContext context) throws Exception {\n+    super.initializeState(context);\n+\n+    checkpointState = context.getOperatorStateStore().getListState(\n+        new ListStateDescriptor<>(\"splits\", new JavaSerializer<>()));\n+\n+    int subtaskIdx = getRuntimeContext().getIndexOfThisSubtask();\n+    splits = Lists.newLinkedList();\n+    if (context.isRestored()) {\n+      LOG.info(\"Restoring state for the {} (taskIdx={}).\", getClass().getSimpleName(), subtaskIdx);\n+\n+      for (FlinkInputSplit split : checkpointState.get()) {\n+        splits.add(split);\n+      }\n+    }\n+\n+    this.readerContext = StreamSourceContexts.getSourceContext(\n+        getOperatorConfig().getTimeCharacteristic(),\n+        getProcessingTimeService(),\n+        new Object(), // no actual locking needed\n+        getContainingTask().getStreamStatusMaintainer(),\n+        output,\n+        getRuntimeContext().getExecutionConfig().getAutoWatermarkInterval(),\n+        -1);\n+\n+    if (!splits.isEmpty()) {\n+      enqueueProcessSplits();\n+    }\n+  }\n+\n+  @Override\n+  public void processElement(StreamRecord<FlinkInputSplit> element) {\n+    splits.offer(element.getValue());\n+    enqueueProcessSplits();\n+  }\n+\n+  private void enqueueProcessSplits() {\n+    executor.execute(this::processSplits, this.getClass().getSimpleName());\n+  }\n+\n+  private void processSplits() throws IOException {\n+    do {\n+      FlinkInputSplit split = splits.poll();\n+      if (split == null) {\n+        return;\n+      }\n+\n+      LOG.debug(\"load split: {}\", split);\n+      format.open(split);\n+\n+      RowData nextElement = null;\n+      while (!format.reachedEnd()) {\n+        nextElement = format.nextRecord(nextElement);\n+        if (nextElement != null) {\n+          readerContext.collect(nextElement);\n+        } else {\n+          break;\n+        }\n+      }\n+\n+      format.close();\n+    } while (executor.isIdle());\n+    enqueueProcessSplits();\n+  }\n+\n+  @Override\n+  public void processWatermark(Watermark mark) {\n+    // we do nothing because we emit our own watermarks if needed.\n+  }\n+\n+  @Override\n+  public void dispose() throws Exception {\n+    super.dispose();\n+\n+    if (format != null) {\n+      format.close();\n+      format.closeInputFormat();\n+      format = null;\n+    }\n+\n+    readerContext = null;\n+  }\n+\n+  @Override\n+  public void close() throws Exception {\n+    super.close();\n+    output.close();\n+    if (readerContext != null) {\n+      readerContext.emitWatermark(Watermark.MAX_WATERMARK);\n+      readerContext.close();\n+      readerContext = null;\n+    }\n+  }\n+\n+  @Override\n+  public void snapshotState(StateSnapshotContext context) throws Exception {\n+    super.snapshotState(context);\n+\n+    checkpointState.clear();\n+    for (FlinkInputSplit split : splits) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6ea85e37fded0c67538f3c7e1f8c33a47e033eb5"}, "originalPosition": 178}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0afaf8be1878e6daebee4779392fab4ebfc5a379", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/0afaf8be1878e6daebee4779392fab4ebfc5a379", "committedDate": "2021-01-07T10:22:38Z", "message": "Adress comments from stevenwu"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ac6febbe89682726970075a25ccc4e5f1e6f3d16", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/ac6febbe89682726970075a25ccc4e5f1e6f3d16", "committedDate": "2021-01-07T13:26:50Z", "message": "Minor changes."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTYzNjU2NTk0", "url": "https://github.com/apache/iceberg/pull/1793#pullrequestreview-563656594", "createdAt": "2021-01-07T16:59:10Z", "commit": {"oid": "ac6febbe89682726970075a25ccc4e5f1e6f3d16"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTYzOTU1OTY2", "url": "https://github.com/apache/iceberg/pull/1793#pullrequestreview-563955966", "createdAt": "2021-01-08T02:05:19Z", "commit": {"oid": "ac6febbe89682726970075a25ccc4e5f1e6f3d16"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wOFQwMjowNToxOVrOIQDHuA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wOFQwMjowNToxOVrOIQDHuA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzY5OTI1Ng==", "bodyText": "Minor: I strongly prefer full words to removing vowels from them, so contextBuilder instead of ctxtBuilder.\nWhile the other is shorter, we aren't trying to save space in code. I also find it no easier to type ctxt than context and find it is distracting when reading the code because it isn't a word so I have to think harder about saying it.", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r553699256", "createdAt": "2021-01-08T02:05:19Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "diffHunk": "@@ -101,57 +99,62 @@ public Builder project(TableSchema schema) {\n     }\n \n     public Builder limit(long newLimit) {\n-      this.limit = newLimit;\n+      ctxtBuilder.limit(newLimit);\n       return this;\n     }\n \n     public Builder properties(Map<String, String> properties) {\n-      this.context = context.fromProperties(properties);\n+      ctxtBuilder.fromProperties(properties);\n       return this;\n     }\n \n     public Builder caseSensitive(boolean caseSensitive) {\n-      this.context = context.setCaseSensitive(caseSensitive);\n+      ctxtBuilder.caseSensitive(caseSensitive);\n       return this;\n     }\n \n     public Builder snapshotId(Long snapshotId) {\n-      this.context = context.useSnapshotId(snapshotId);\n+      ctxtBuilder.useSnapshotId(snapshotId);\n       return this;\n     }\n \n     public Builder startSnapshotId(Long startSnapshotId) {\n-      this.context = context.startSnapshotId(startSnapshotId);\n+      ctxtBuilder.startSnapshotId(startSnapshotId);\n       return this;\n     }\n \n     public Builder endSnapshotId(Long endSnapshotId) {\n-      this.context = context.endSnapshotId(endSnapshotId);\n+      ctxtBuilder.endSnapshotId(endSnapshotId);\n       return this;\n     }\n \n     public Builder asOfTimestamp(Long asOfTimestamp) {\n-      this.context = context.asOfTimestamp(asOfTimestamp);\n+      ctxtBuilder.asOfTimestamp(asOfTimestamp);\n       return this;\n     }\n \n     public Builder splitSize(Long splitSize) {\n-      this.context = context.splitSize(splitSize);\n+      ctxtBuilder.splitSize(splitSize);\n       return this;\n     }\n \n     public Builder splitLookback(Integer splitLookback) {\n-      this.context = context.splitLookback(splitLookback);\n+      ctxtBuilder.splitLookback(splitLookback);\n       return this;\n     }\n \n     public Builder splitOpenFileCost(Long splitOpenFileCost) {\n-      this.context = context.splitOpenFileCost(splitOpenFileCost);\n+      ctxtBuilder.splitOpenFileCost(splitOpenFileCost);\n+      return this;\n+    }\n+\n+    public Builder streaming(boolean streaming) {\n+      ctxtBuilder.streaming(streaming);\n       return this;\n     }\n \n     public Builder nameMapping(String nameMapping) {\n-      this.context = context.nameMapping(nameMapping);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ac6febbe89682726970075a25ccc4e5f1e6f3d16"}, "originalPosition": 105}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTYzOTU2NjI1", "url": "https://github.com/apache/iceberg/pull/1793#pullrequestreview-563956625", "createdAt": "2021-01-08T02:07:35Z", "commit": {"oid": "ac6febbe89682726970075a25ccc4e5f1e6f3d16"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wOFQwMjowNzozNVrOIQDKAw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wOFQwMjowNzozNVrOIQDKAw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzY5OTg0Mw==", "bodyText": "If you're using -1 then why is this Long and not a primitive?", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r553699843", "createdAt": "2021-01-08T02:07:35Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/ScanContext.java", "diffHunk": "@@ -100,126 +96,224 @@ private ScanContext(boolean caseSensitive, Long snapshotId, Long startSnapshotId\n     this.splitSize = splitSize;\n     this.splitLookback = splitLookback;\n     this.splitOpenFileCost = splitOpenFileCost;\n+    this.isStreaming = isStreaming;\n+    this.monitorInterval = monitorInterval;\n+\n     this.nameMapping = nameMapping;\n-    this.projectedSchema = projectedSchema;\n-    this.filterExpressions = filterExpressions;\n+    this.schema = schema;\n+    this.filters = filters;\n     this.limit = limit;\n   }\n \n-  ScanContext fromProperties(Map<String, String> properties) {\n-    Configuration config = new Configuration();\n-    properties.forEach(config::setString);\n-    return new ScanContext(config.get(CASE_SENSITIVE), config.get(SNAPSHOT_ID), config.get(START_SNAPSHOT_ID),\n-        config.get(END_SNAPSHOT_ID), config.get(AS_OF_TIMESTAMP), config.get(SPLIT_SIZE), config.get(SPLIT_LOOKBACK),\n-        config.get(SPLIT_FILE_OPEN_COST), properties.get(DEFAULT_NAME_MAPPING), projectedSchema, filterExpressions,\n-        limit);\n-  }\n-\n   boolean caseSensitive() {\n     return caseSensitive;\n   }\n \n-  ScanContext setCaseSensitive(boolean isCaseSensitive) {\n-    return new ScanContext(isCaseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions, limit);\n-  }\n-\n   Long snapshotId() {\n     return snapshotId;\n   }\n \n-  ScanContext useSnapshotId(Long scanSnapshotId) {\n-    return new ScanContext(caseSensitive, scanSnapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions, limit);\n-  }\n-\n   Long startSnapshotId() {\n     return startSnapshotId;\n   }\n \n-  ScanContext startSnapshotId(Long id) {\n-    return new ScanContext(caseSensitive, snapshotId, id, endSnapshotId, asOfTimestamp, splitSize, splitLookback,\n-        splitOpenFileCost, nameMapping, projectedSchema, filterExpressions, limit);\n-  }\n-\n   Long endSnapshotId() {\n     return endSnapshotId;\n   }\n \n-  ScanContext endSnapshotId(Long id) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, id, asOfTimestamp, splitSize, splitLookback,\n-        splitOpenFileCost, nameMapping, projectedSchema, filterExpressions, limit);\n-  }\n-\n   Long asOfTimestamp() {\n     return asOfTimestamp;\n   }\n \n-  ScanContext asOfTimestamp(Long timestamp) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, timestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions, limit);\n-  }\n-\n   Long splitSize() {\n     return splitSize;\n   }\n \n-  ScanContext splitSize(Long size) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, size,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions, limit);\n-  }\n-\n   Integer splitLookback() {\n     return splitLookback;\n   }\n \n-  ScanContext splitLookback(Integer lookback) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        lookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions, limit);\n-  }\n-\n   Long splitOpenFileCost() {\n     return splitOpenFileCost;\n   }\n \n-  ScanContext splitOpenFileCost(Long fileCost) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, fileCost, nameMapping, projectedSchema, filterExpressions, limit);\n+  boolean isStreaming() {\n+    return isStreaming;\n+  }\n+\n+  Duration monitorInterval() {\n+    return monitorInterval;\n   }\n \n   String nameMapping() {\n     return nameMapping;\n   }\n \n-  ScanContext nameMapping(String mapping) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, mapping, projectedSchema, filterExpressions, limit);\n+  Schema project() {\n+    return schema;\n   }\n \n-  Schema projectedSchema() {\n-    return projectedSchema;\n+  List<Expression> filters() {\n+    return filters;\n   }\n \n-  ScanContext project(Schema schema) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, schema, filterExpressions, limit);\n+  long limit() {\n+    return limit;\n   }\n \n-  List<Expression> filterExpressions() {\n-    return filterExpressions;\n+  ScanContext copyWithAppendsBetween(long newStartSnapshotId, long newEndSnapshotId) {\n+    return ScanContext.builder()\n+        .caseSensitive(caseSensitive)\n+        .useSnapshotId(null)\n+        .startSnapshotId(newStartSnapshotId)\n+        .endSnapshotId(newEndSnapshotId)\n+        .asOfTimestamp(null)\n+        .splitSize(splitSize)\n+        .splitLookback(splitLookback)\n+        .splitOpenFileCost(splitOpenFileCost)\n+        .streaming(isStreaming)\n+        .monitorInterval(monitorInterval)\n+        .nameMapping(nameMapping)\n+        .project(schema)\n+        .filters(filters)\n+        .limit(limit)\n+        .build();\n   }\n \n-  ScanContext filterRows(List<Expression> filters) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filters, limit);\n+  ScanContext copyWithSnapshotId(long newSnapshotId) {\n+    return ScanContext.builder()\n+        .caseSensitive(caseSensitive)\n+        .useSnapshotId(newSnapshotId)\n+        .startSnapshotId(null)\n+        .endSnapshotId(null)\n+        .asOfTimestamp(null)\n+        .splitSize(splitSize)\n+        .splitLookback(splitLookback)\n+        .splitOpenFileCost(splitOpenFileCost)\n+        .streaming(isStreaming)\n+        .monitorInterval(monitorInterval)\n+        .nameMapping(nameMapping)\n+        .project(schema)\n+        .filters(filters)\n+        .limit(limit)\n+        .build();\n   }\n \n-  long limit() {\n-    return limit;\n+  static Builder builder() {\n+    return new Builder();\n   }\n \n-  ScanContext limit(Long newLimit) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions, newLimit);\n+  static class Builder {\n+    private boolean caseSensitive = CASE_SENSITIVE.defaultValue();\n+    private Long snapshotId = SNAPSHOT_ID.defaultValue();\n+    private Long startSnapshotId = START_SNAPSHOT_ID.defaultValue();\n+    private Long endSnapshotId = END_SNAPSHOT_ID.defaultValue();\n+    private Long asOfTimestamp = AS_OF_TIMESTAMP.defaultValue();\n+    private Long splitSize = SPLIT_SIZE.defaultValue();\n+    private Integer splitLookback = SPLIT_LOOKBACK.defaultValue();\n+    private Long splitOpenFileCost = SPLIT_FILE_OPEN_COST.defaultValue();\n+    private boolean isStreaming = STREAMING.defaultValue();\n+    private Duration monitorInterval = MONITOR_INTERVAL.defaultValue();\n+    private String nameMapping;\n+    private Schema projectedSchema;\n+    private List<Expression> filters;\n+    private Long limit = -1L;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ac6febbe89682726970075a25ccc4e5f1e6f3d16"}, "originalPosition": 251}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTYzOTU2NzAw", "url": "https://github.com/apache/iceberg/pull/1793#pullrequestreview-563956700", "createdAt": "2021-01-08T02:07:50Z", "commit": {"oid": "ac6febbe89682726970075a25ccc4e5f1e6f3d16"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wOFQwMjowNzo1MFrOIQDKWA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wOFQwMjowNzo1MFrOIQDKWA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzY5OTkyOA==", "bodyText": "Is there a case where it is acceptable to pass null here?", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r553699928", "createdAt": "2021-01-08T02:07:50Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/ScanContext.java", "diffHunk": "@@ -100,126 +96,224 @@ private ScanContext(boolean caseSensitive, Long snapshotId, Long startSnapshotId\n     this.splitSize = splitSize;\n     this.splitLookback = splitLookback;\n     this.splitOpenFileCost = splitOpenFileCost;\n+    this.isStreaming = isStreaming;\n+    this.monitorInterval = monitorInterval;\n+\n     this.nameMapping = nameMapping;\n-    this.projectedSchema = projectedSchema;\n-    this.filterExpressions = filterExpressions;\n+    this.schema = schema;\n+    this.filters = filters;\n     this.limit = limit;\n   }\n \n-  ScanContext fromProperties(Map<String, String> properties) {\n-    Configuration config = new Configuration();\n-    properties.forEach(config::setString);\n-    return new ScanContext(config.get(CASE_SENSITIVE), config.get(SNAPSHOT_ID), config.get(START_SNAPSHOT_ID),\n-        config.get(END_SNAPSHOT_ID), config.get(AS_OF_TIMESTAMP), config.get(SPLIT_SIZE), config.get(SPLIT_LOOKBACK),\n-        config.get(SPLIT_FILE_OPEN_COST), properties.get(DEFAULT_NAME_MAPPING), projectedSchema, filterExpressions,\n-        limit);\n-  }\n-\n   boolean caseSensitive() {\n     return caseSensitive;\n   }\n \n-  ScanContext setCaseSensitive(boolean isCaseSensitive) {\n-    return new ScanContext(isCaseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions, limit);\n-  }\n-\n   Long snapshotId() {\n     return snapshotId;\n   }\n \n-  ScanContext useSnapshotId(Long scanSnapshotId) {\n-    return new ScanContext(caseSensitive, scanSnapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions, limit);\n-  }\n-\n   Long startSnapshotId() {\n     return startSnapshotId;\n   }\n \n-  ScanContext startSnapshotId(Long id) {\n-    return new ScanContext(caseSensitive, snapshotId, id, endSnapshotId, asOfTimestamp, splitSize, splitLookback,\n-        splitOpenFileCost, nameMapping, projectedSchema, filterExpressions, limit);\n-  }\n-\n   Long endSnapshotId() {\n     return endSnapshotId;\n   }\n \n-  ScanContext endSnapshotId(Long id) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, id, asOfTimestamp, splitSize, splitLookback,\n-        splitOpenFileCost, nameMapping, projectedSchema, filterExpressions, limit);\n-  }\n-\n   Long asOfTimestamp() {\n     return asOfTimestamp;\n   }\n \n-  ScanContext asOfTimestamp(Long timestamp) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, timestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions, limit);\n-  }\n-\n   Long splitSize() {\n     return splitSize;\n   }\n \n-  ScanContext splitSize(Long size) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, size,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions, limit);\n-  }\n-\n   Integer splitLookback() {\n     return splitLookback;\n   }\n \n-  ScanContext splitLookback(Integer lookback) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        lookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions, limit);\n-  }\n-\n   Long splitOpenFileCost() {\n     return splitOpenFileCost;\n   }\n \n-  ScanContext splitOpenFileCost(Long fileCost) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, fileCost, nameMapping, projectedSchema, filterExpressions, limit);\n+  boolean isStreaming() {\n+    return isStreaming;\n+  }\n+\n+  Duration monitorInterval() {\n+    return monitorInterval;\n   }\n \n   String nameMapping() {\n     return nameMapping;\n   }\n \n-  ScanContext nameMapping(String mapping) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, mapping, projectedSchema, filterExpressions, limit);\n+  Schema project() {\n+    return schema;\n   }\n \n-  Schema projectedSchema() {\n-    return projectedSchema;\n+  List<Expression> filters() {\n+    return filters;\n   }\n \n-  ScanContext project(Schema schema) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, schema, filterExpressions, limit);\n+  long limit() {\n+    return limit;\n   }\n \n-  List<Expression> filterExpressions() {\n-    return filterExpressions;\n+  ScanContext copyWithAppendsBetween(long newStartSnapshotId, long newEndSnapshotId) {\n+    return ScanContext.builder()\n+        .caseSensitive(caseSensitive)\n+        .useSnapshotId(null)\n+        .startSnapshotId(newStartSnapshotId)\n+        .endSnapshotId(newEndSnapshotId)\n+        .asOfTimestamp(null)\n+        .splitSize(splitSize)\n+        .splitLookback(splitLookback)\n+        .splitOpenFileCost(splitOpenFileCost)\n+        .streaming(isStreaming)\n+        .monitorInterval(monitorInterval)\n+        .nameMapping(nameMapping)\n+        .project(schema)\n+        .filters(filters)\n+        .limit(limit)\n+        .build();\n   }\n \n-  ScanContext filterRows(List<Expression> filters) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filters, limit);\n+  ScanContext copyWithSnapshotId(long newSnapshotId) {\n+    return ScanContext.builder()\n+        .caseSensitive(caseSensitive)\n+        .useSnapshotId(newSnapshotId)\n+        .startSnapshotId(null)\n+        .endSnapshotId(null)\n+        .asOfTimestamp(null)\n+        .splitSize(splitSize)\n+        .splitLookback(splitLookback)\n+        .splitOpenFileCost(splitOpenFileCost)\n+        .streaming(isStreaming)\n+        .monitorInterval(monitorInterval)\n+        .nameMapping(nameMapping)\n+        .project(schema)\n+        .filters(filters)\n+        .limit(limit)\n+        .build();\n   }\n \n-  long limit() {\n-    return limit;\n+  static Builder builder() {\n+    return new Builder();\n   }\n \n-  ScanContext limit(Long newLimit) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions, newLimit);\n+  static class Builder {\n+    private boolean caseSensitive = CASE_SENSITIVE.defaultValue();\n+    private Long snapshotId = SNAPSHOT_ID.defaultValue();\n+    private Long startSnapshotId = START_SNAPSHOT_ID.defaultValue();\n+    private Long endSnapshotId = END_SNAPSHOT_ID.defaultValue();\n+    private Long asOfTimestamp = AS_OF_TIMESTAMP.defaultValue();\n+    private Long splitSize = SPLIT_SIZE.defaultValue();\n+    private Integer splitLookback = SPLIT_LOOKBACK.defaultValue();\n+    private Long splitOpenFileCost = SPLIT_FILE_OPEN_COST.defaultValue();\n+    private boolean isStreaming = STREAMING.defaultValue();\n+    private Duration monitorInterval = MONITOR_INTERVAL.defaultValue();\n+    private String nameMapping;\n+    private Schema projectedSchema;\n+    private List<Expression> filters;\n+    private Long limit = -1L;\n+\n+    private Builder() {\n+    }\n+\n+    Builder caseSensitive(boolean newCaseSensitive) {\n+      this.caseSensitive = newCaseSensitive;\n+      return this;\n+    }\n+\n+    Builder useSnapshotId(Long newSnapshotId) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ac6febbe89682726970075a25ccc4e5f1e6f3d16"}, "originalPosition": 261}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTYzOTU2ODAw", "url": "https://github.com/apache/iceberg/pull/1793#pullrequestreview-563956800", "createdAt": "2021-01-08T02:08:13Z", "commit": {"oid": "ac6febbe89682726970075a25ccc4e5f1e6f3d16"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wOFQwMjowODoxM1rOIQDKzw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wOFQwMjowODoxM1rOIQDKzw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzcwMDA0Nw==", "bodyText": "Same with these other methods. Should these be primitives?", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r553700047", "createdAt": "2021-01-08T02:08:13Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/ScanContext.java", "diffHunk": "@@ -100,126 +96,224 @@ private ScanContext(boolean caseSensitive, Long snapshotId, Long startSnapshotId\n     this.splitSize = splitSize;\n     this.splitLookback = splitLookback;\n     this.splitOpenFileCost = splitOpenFileCost;\n+    this.isStreaming = isStreaming;\n+    this.monitorInterval = monitorInterval;\n+\n     this.nameMapping = nameMapping;\n-    this.projectedSchema = projectedSchema;\n-    this.filterExpressions = filterExpressions;\n+    this.schema = schema;\n+    this.filters = filters;\n     this.limit = limit;\n   }\n \n-  ScanContext fromProperties(Map<String, String> properties) {\n-    Configuration config = new Configuration();\n-    properties.forEach(config::setString);\n-    return new ScanContext(config.get(CASE_SENSITIVE), config.get(SNAPSHOT_ID), config.get(START_SNAPSHOT_ID),\n-        config.get(END_SNAPSHOT_ID), config.get(AS_OF_TIMESTAMP), config.get(SPLIT_SIZE), config.get(SPLIT_LOOKBACK),\n-        config.get(SPLIT_FILE_OPEN_COST), properties.get(DEFAULT_NAME_MAPPING), projectedSchema, filterExpressions,\n-        limit);\n-  }\n-\n   boolean caseSensitive() {\n     return caseSensitive;\n   }\n \n-  ScanContext setCaseSensitive(boolean isCaseSensitive) {\n-    return new ScanContext(isCaseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions, limit);\n-  }\n-\n   Long snapshotId() {\n     return snapshotId;\n   }\n \n-  ScanContext useSnapshotId(Long scanSnapshotId) {\n-    return new ScanContext(caseSensitive, scanSnapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions, limit);\n-  }\n-\n   Long startSnapshotId() {\n     return startSnapshotId;\n   }\n \n-  ScanContext startSnapshotId(Long id) {\n-    return new ScanContext(caseSensitive, snapshotId, id, endSnapshotId, asOfTimestamp, splitSize, splitLookback,\n-        splitOpenFileCost, nameMapping, projectedSchema, filterExpressions, limit);\n-  }\n-\n   Long endSnapshotId() {\n     return endSnapshotId;\n   }\n \n-  ScanContext endSnapshotId(Long id) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, id, asOfTimestamp, splitSize, splitLookback,\n-        splitOpenFileCost, nameMapping, projectedSchema, filterExpressions, limit);\n-  }\n-\n   Long asOfTimestamp() {\n     return asOfTimestamp;\n   }\n \n-  ScanContext asOfTimestamp(Long timestamp) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, timestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions, limit);\n-  }\n-\n   Long splitSize() {\n     return splitSize;\n   }\n \n-  ScanContext splitSize(Long size) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, size,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions, limit);\n-  }\n-\n   Integer splitLookback() {\n     return splitLookback;\n   }\n \n-  ScanContext splitLookback(Integer lookback) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        lookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions, limit);\n-  }\n-\n   Long splitOpenFileCost() {\n     return splitOpenFileCost;\n   }\n \n-  ScanContext splitOpenFileCost(Long fileCost) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, fileCost, nameMapping, projectedSchema, filterExpressions, limit);\n+  boolean isStreaming() {\n+    return isStreaming;\n+  }\n+\n+  Duration monitorInterval() {\n+    return monitorInterval;\n   }\n \n   String nameMapping() {\n     return nameMapping;\n   }\n \n-  ScanContext nameMapping(String mapping) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, mapping, projectedSchema, filterExpressions, limit);\n+  Schema project() {\n+    return schema;\n   }\n \n-  Schema projectedSchema() {\n-    return projectedSchema;\n+  List<Expression> filters() {\n+    return filters;\n   }\n \n-  ScanContext project(Schema schema) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, schema, filterExpressions, limit);\n+  long limit() {\n+    return limit;\n   }\n \n-  List<Expression> filterExpressions() {\n-    return filterExpressions;\n+  ScanContext copyWithAppendsBetween(long newStartSnapshotId, long newEndSnapshotId) {\n+    return ScanContext.builder()\n+        .caseSensitive(caseSensitive)\n+        .useSnapshotId(null)\n+        .startSnapshotId(newStartSnapshotId)\n+        .endSnapshotId(newEndSnapshotId)\n+        .asOfTimestamp(null)\n+        .splitSize(splitSize)\n+        .splitLookback(splitLookback)\n+        .splitOpenFileCost(splitOpenFileCost)\n+        .streaming(isStreaming)\n+        .monitorInterval(monitorInterval)\n+        .nameMapping(nameMapping)\n+        .project(schema)\n+        .filters(filters)\n+        .limit(limit)\n+        .build();\n   }\n \n-  ScanContext filterRows(List<Expression> filters) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filters, limit);\n+  ScanContext copyWithSnapshotId(long newSnapshotId) {\n+    return ScanContext.builder()\n+        .caseSensitive(caseSensitive)\n+        .useSnapshotId(newSnapshotId)\n+        .startSnapshotId(null)\n+        .endSnapshotId(null)\n+        .asOfTimestamp(null)\n+        .splitSize(splitSize)\n+        .splitLookback(splitLookback)\n+        .splitOpenFileCost(splitOpenFileCost)\n+        .streaming(isStreaming)\n+        .monitorInterval(monitorInterval)\n+        .nameMapping(nameMapping)\n+        .project(schema)\n+        .filters(filters)\n+        .limit(limit)\n+        .build();\n   }\n \n-  long limit() {\n-    return limit;\n+  static Builder builder() {\n+    return new Builder();\n   }\n \n-  ScanContext limit(Long newLimit) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions, newLimit);\n+  static class Builder {\n+    private boolean caseSensitive = CASE_SENSITIVE.defaultValue();\n+    private Long snapshotId = SNAPSHOT_ID.defaultValue();\n+    private Long startSnapshotId = START_SNAPSHOT_ID.defaultValue();\n+    private Long endSnapshotId = END_SNAPSHOT_ID.defaultValue();\n+    private Long asOfTimestamp = AS_OF_TIMESTAMP.defaultValue();\n+    private Long splitSize = SPLIT_SIZE.defaultValue();\n+    private Integer splitLookback = SPLIT_LOOKBACK.defaultValue();\n+    private Long splitOpenFileCost = SPLIT_FILE_OPEN_COST.defaultValue();\n+    private boolean isStreaming = STREAMING.defaultValue();\n+    private Duration monitorInterval = MONITOR_INTERVAL.defaultValue();\n+    private String nameMapping;\n+    private Schema projectedSchema;\n+    private List<Expression> filters;\n+    private Long limit = -1L;\n+\n+    private Builder() {\n+    }\n+\n+    Builder caseSensitive(boolean newCaseSensitive) {\n+      this.caseSensitive = newCaseSensitive;\n+      return this;\n+    }\n+\n+    Builder useSnapshotId(Long newSnapshotId) {\n+      this.snapshotId = newSnapshotId;\n+      return this;\n+    }\n+\n+    Builder startSnapshotId(Long newStartSnapshotId) {\n+      this.startSnapshotId = newStartSnapshotId;\n+      return this;\n+    }\n+\n+    Builder endSnapshotId(Long newEndSnapshotId) {\n+      this.endSnapshotId = newEndSnapshotId;\n+      return this;\n+    }\n+\n+    Builder asOfTimestamp(Long newAsOfTimestamp) {\n+      this.asOfTimestamp = newAsOfTimestamp;\n+      return this;\n+    }\n+\n+    Builder splitSize(Long newSplitSize) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ac6febbe89682726970075a25ccc4e5f1e6f3d16"}, "originalPosition": 281}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTYzOTU3MzY3", "url": "https://github.com/apache/iceberg/pull/1793#pullrequestreview-563957367", "createdAt": "2021-01-08T02:10:08Z", "commit": {"oid": "ac6febbe89682726970075a25ccc4e5f1e6f3d16"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wOFQwMjoxMDowOFrOIQDMzQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wOFQwMjoxMDowOFrOIQDMzQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzcwMDU1Nw==", "bodyText": "Can get accept a default value so we don't need to use boxed objects?", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r553700557", "createdAt": "2021-01-08T02:10:08Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/ScanContext.java", "diffHunk": "@@ -100,126 +96,224 @@ private ScanContext(boolean caseSensitive, Long snapshotId, Long startSnapshotId\n     this.splitSize = splitSize;\n     this.splitLookback = splitLookback;\n     this.splitOpenFileCost = splitOpenFileCost;\n+    this.isStreaming = isStreaming;\n+    this.monitorInterval = monitorInterval;\n+\n     this.nameMapping = nameMapping;\n-    this.projectedSchema = projectedSchema;\n-    this.filterExpressions = filterExpressions;\n+    this.schema = schema;\n+    this.filters = filters;\n     this.limit = limit;\n   }\n \n-  ScanContext fromProperties(Map<String, String> properties) {\n-    Configuration config = new Configuration();\n-    properties.forEach(config::setString);\n-    return new ScanContext(config.get(CASE_SENSITIVE), config.get(SNAPSHOT_ID), config.get(START_SNAPSHOT_ID),\n-        config.get(END_SNAPSHOT_ID), config.get(AS_OF_TIMESTAMP), config.get(SPLIT_SIZE), config.get(SPLIT_LOOKBACK),\n-        config.get(SPLIT_FILE_OPEN_COST), properties.get(DEFAULT_NAME_MAPPING), projectedSchema, filterExpressions,\n-        limit);\n-  }\n-\n   boolean caseSensitive() {\n     return caseSensitive;\n   }\n \n-  ScanContext setCaseSensitive(boolean isCaseSensitive) {\n-    return new ScanContext(isCaseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions, limit);\n-  }\n-\n   Long snapshotId() {\n     return snapshotId;\n   }\n \n-  ScanContext useSnapshotId(Long scanSnapshotId) {\n-    return new ScanContext(caseSensitive, scanSnapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions, limit);\n-  }\n-\n   Long startSnapshotId() {\n     return startSnapshotId;\n   }\n \n-  ScanContext startSnapshotId(Long id) {\n-    return new ScanContext(caseSensitive, snapshotId, id, endSnapshotId, asOfTimestamp, splitSize, splitLookback,\n-        splitOpenFileCost, nameMapping, projectedSchema, filterExpressions, limit);\n-  }\n-\n   Long endSnapshotId() {\n     return endSnapshotId;\n   }\n \n-  ScanContext endSnapshotId(Long id) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, id, asOfTimestamp, splitSize, splitLookback,\n-        splitOpenFileCost, nameMapping, projectedSchema, filterExpressions, limit);\n-  }\n-\n   Long asOfTimestamp() {\n     return asOfTimestamp;\n   }\n \n-  ScanContext asOfTimestamp(Long timestamp) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, timestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions, limit);\n-  }\n-\n   Long splitSize() {\n     return splitSize;\n   }\n \n-  ScanContext splitSize(Long size) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, size,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions, limit);\n-  }\n-\n   Integer splitLookback() {\n     return splitLookback;\n   }\n \n-  ScanContext splitLookback(Integer lookback) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        lookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions, limit);\n-  }\n-\n   Long splitOpenFileCost() {\n     return splitOpenFileCost;\n   }\n \n-  ScanContext splitOpenFileCost(Long fileCost) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, fileCost, nameMapping, projectedSchema, filterExpressions, limit);\n+  boolean isStreaming() {\n+    return isStreaming;\n+  }\n+\n+  Duration monitorInterval() {\n+    return monitorInterval;\n   }\n \n   String nameMapping() {\n     return nameMapping;\n   }\n \n-  ScanContext nameMapping(String mapping) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, mapping, projectedSchema, filterExpressions, limit);\n+  Schema project() {\n+    return schema;\n   }\n \n-  Schema projectedSchema() {\n-    return projectedSchema;\n+  List<Expression> filters() {\n+    return filters;\n   }\n \n-  ScanContext project(Schema schema) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, schema, filterExpressions, limit);\n+  long limit() {\n+    return limit;\n   }\n \n-  List<Expression> filterExpressions() {\n-    return filterExpressions;\n+  ScanContext copyWithAppendsBetween(long newStartSnapshotId, long newEndSnapshotId) {\n+    return ScanContext.builder()\n+        .caseSensitive(caseSensitive)\n+        .useSnapshotId(null)\n+        .startSnapshotId(newStartSnapshotId)\n+        .endSnapshotId(newEndSnapshotId)\n+        .asOfTimestamp(null)\n+        .splitSize(splitSize)\n+        .splitLookback(splitLookback)\n+        .splitOpenFileCost(splitOpenFileCost)\n+        .streaming(isStreaming)\n+        .monitorInterval(monitorInterval)\n+        .nameMapping(nameMapping)\n+        .project(schema)\n+        .filters(filters)\n+        .limit(limit)\n+        .build();\n   }\n \n-  ScanContext filterRows(List<Expression> filters) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filters, limit);\n+  ScanContext copyWithSnapshotId(long newSnapshotId) {\n+    return ScanContext.builder()\n+        .caseSensitive(caseSensitive)\n+        .useSnapshotId(newSnapshotId)\n+        .startSnapshotId(null)\n+        .endSnapshotId(null)\n+        .asOfTimestamp(null)\n+        .splitSize(splitSize)\n+        .splitLookback(splitLookback)\n+        .splitOpenFileCost(splitOpenFileCost)\n+        .streaming(isStreaming)\n+        .monitorInterval(monitorInterval)\n+        .nameMapping(nameMapping)\n+        .project(schema)\n+        .filters(filters)\n+        .limit(limit)\n+        .build();\n   }\n \n-  long limit() {\n-    return limit;\n+  static Builder builder() {\n+    return new Builder();\n   }\n \n-  ScanContext limit(Long newLimit) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions, newLimit);\n+  static class Builder {\n+    private boolean caseSensitive = CASE_SENSITIVE.defaultValue();\n+    private Long snapshotId = SNAPSHOT_ID.defaultValue();\n+    private Long startSnapshotId = START_SNAPSHOT_ID.defaultValue();\n+    private Long endSnapshotId = END_SNAPSHOT_ID.defaultValue();\n+    private Long asOfTimestamp = AS_OF_TIMESTAMP.defaultValue();\n+    private Long splitSize = SPLIT_SIZE.defaultValue();\n+    private Integer splitLookback = SPLIT_LOOKBACK.defaultValue();\n+    private Long splitOpenFileCost = SPLIT_FILE_OPEN_COST.defaultValue();\n+    private boolean isStreaming = STREAMING.defaultValue();\n+    private Duration monitorInterval = MONITOR_INTERVAL.defaultValue();\n+    private String nameMapping;\n+    private Schema projectedSchema;\n+    private List<Expression> filters;\n+    private Long limit = -1L;\n+\n+    private Builder() {\n+    }\n+\n+    Builder caseSensitive(boolean newCaseSensitive) {\n+      this.caseSensitive = newCaseSensitive;\n+      return this;\n+    }\n+\n+    Builder useSnapshotId(Long newSnapshotId) {\n+      this.snapshotId = newSnapshotId;\n+      return this;\n+    }\n+\n+    Builder startSnapshotId(Long newStartSnapshotId) {\n+      this.startSnapshotId = newStartSnapshotId;\n+      return this;\n+    }\n+\n+    Builder endSnapshotId(Long newEndSnapshotId) {\n+      this.endSnapshotId = newEndSnapshotId;\n+      return this;\n+    }\n+\n+    Builder asOfTimestamp(Long newAsOfTimestamp) {\n+      this.asOfTimestamp = newAsOfTimestamp;\n+      return this;\n+    }\n+\n+    Builder splitSize(Long newSplitSize) {\n+      this.splitSize = newSplitSize;\n+      return this;\n+    }\n+\n+    Builder splitLookback(Integer newSplitLookback) {\n+      this.splitLookback = newSplitLookback;\n+      return this;\n+    }\n+\n+    Builder splitOpenFileCost(Long newSplitOpenFileCost) {\n+      this.splitOpenFileCost = newSplitOpenFileCost;\n+      return this;\n+    }\n+\n+    Builder streaming(boolean streaming) {\n+      this.isStreaming = streaming;\n+      return this;\n+    }\n+\n+    Builder monitorInterval(Duration newMonitorInterval) {\n+      this.monitorInterval = newMonitorInterval;\n+      return this;\n+    }\n+\n+    Builder nameMapping(String newNameMapping) {\n+      this.nameMapping = newNameMapping;\n+      return this;\n+    }\n+\n+    Builder project(Schema newProjectedSchema) {\n+      this.projectedSchema = newProjectedSchema;\n+      return this;\n+    }\n+\n+    Builder filters(List<Expression> newFilters) {\n+      this.filters = newFilters;\n+      return this;\n+    }\n+\n+    Builder limit(long newLimit) {\n+      this.limit = newLimit;\n+      return this;\n+    }\n+\n+    Builder fromProperties(Map<String, String> properties) {\n+      Configuration config = new Configuration();\n+      properties.forEach(config::setString);\n+\n+      return this.useSnapshotId(config.get(SNAPSHOT_ID))\n+          .caseSensitive(config.get(CASE_SENSITIVE))\n+          .asOfTimestamp(config.get(AS_OF_TIMESTAMP))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ac6febbe89682726970075a25ccc4e5f1e6f3d16"}, "originalPosition": 332}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTYzOTU3NzIy", "url": "https://github.com/apache/iceberg/pull/1793#pullrequestreview-563957722", "createdAt": "2021-01-08T02:11:33Z", "commit": {"oid": "ac6febbe89682726970075a25ccc4e5f1e6f3d16"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wOFQwMjoxMTozM1rOIQDOHA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wOFQwMjoxMTozM1rOIQDOHA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzcwMDg5Mg==", "bodyText": "Here as well, I think the code is more readable using context instead of ctxt.", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r553700892", "createdAt": "2021-01-08T02:11:33Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/StreamingMonitorFunction.java", "diffHunk": "@@ -0,0 +1,162 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.runtime.state.FunctionInitializationContext;\n+import org.apache.flink.runtime.state.FunctionSnapshotContext;\n+import org.apache.flink.streaming.api.checkpoint.CheckpointedFunction;\n+import org.apache.flink.streaming.api.functions.source.RichSourceFunction;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.util.SnapshotUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * This is the single (non-parallel) monitoring task which takes a {@link FlinkInputFormat},\n+ * it is responsible for:\n+ *\n+ * <ol>\n+ *     <li>Monitoring snapshots of the Iceberg table.</li>\n+ *     <li>Creating the {@link FlinkInputSplit splits} corresponding to the incremental files</li>\n+ *     <li>Assigning them to downstream tasks for further processing.</li>\n+ * </ol>\n+ *\n+ * <p>The splits to be read are forwarded to the downstream {@link StreamingReaderOperator}\n+ * which can have parallelism greater than one.\n+ */\n+public class StreamingMonitorFunction extends RichSourceFunction<FlinkInputSplit> implements CheckpointedFunction {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(StreamingMonitorFunction.class);\n+\n+  private static final long DUMMY_START_SNAPSHOT_ID = -1;\n+\n+  private final TableLoader tableLoader;\n+  private final ScanContext ctxt;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ac6febbe89682726970075a25ccc4e5f1e6f3d16"}, "originalPosition": 59}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTYzOTU4MTEy", "url": "https://github.com/apache/iceberg/pull/1793#pullrequestreview-563958112", "createdAt": "2021-01-08T02:12:57Z", "commit": {"oid": "ac6febbe89682726970075a25ccc4e5f1e6f3d16"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wOFQwMjoxMjo1N1rOIQDPYQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wOFQwMjoxMjo1N1rOIQDPYQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzcwMTIxNw==", "bodyText": "@openinx, @stevenzwu, if you have any pointers to docs that would help me understand the Flink API here, please let me know. I'm still learning the Flink side.", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r553701217", "createdAt": "2021-01-08T02:12:57Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/StreamingMonitorFunction.java", "diffHunk": "@@ -0,0 +1,162 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.runtime.state.FunctionInitializationContext;\n+import org.apache.flink.runtime.state.FunctionSnapshotContext;\n+import org.apache.flink.streaming.api.checkpoint.CheckpointedFunction;\n+import org.apache.flink.streaming.api.functions.source.RichSourceFunction;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.util.SnapshotUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * This is the single (non-parallel) monitoring task which takes a {@link FlinkInputFormat},\n+ * it is responsible for:\n+ *\n+ * <ol>\n+ *     <li>Monitoring snapshots of the Iceberg table.</li>\n+ *     <li>Creating the {@link FlinkInputSplit splits} corresponding to the incremental files</li>\n+ *     <li>Assigning them to downstream tasks for further processing.</li>\n+ * </ol>\n+ *\n+ * <p>The splits to be read are forwarded to the downstream {@link StreamingReaderOperator}\n+ * which can have parallelism greater than one.\n+ */\n+public class StreamingMonitorFunction extends RichSourceFunction<FlinkInputSplit> implements CheckpointedFunction {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(StreamingMonitorFunction.class);\n+\n+  private static final long DUMMY_START_SNAPSHOT_ID = -1;\n+\n+  private final TableLoader tableLoader;\n+  private final ScanContext ctxt;\n+\n+  private volatile boolean isRunning = true;\n+  private transient Object checkpointLock;\n+  private transient Table table;\n+  private transient ListState<Long> snapshotIdState;\n+  private transient long startSnapshotId;\n+\n+  public StreamingMonitorFunction(TableLoader tableLoader, ScanContext ctxt) {\n+    Preconditions.checkArgument(ctxt.snapshotId() == null, \"Cannot set snapshot-id option for streaming reader\");\n+    Preconditions.checkArgument(ctxt.asOfTimestamp() == null, \"Cannot set as-of-timestamp option for streaming reader\");\n+    Preconditions.checkArgument(ctxt.endSnapshotId() == null, \"Cannot set end-snapshot-id option for streaming reader\");\n+    this.tableLoader = tableLoader;\n+    this.ctxt = ctxt;\n+  }\n+\n+  @Override\n+  public void initializeState(FunctionInitializationContext context) throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ac6febbe89682726970075a25ccc4e5f1e6f3d16"}, "originalPosition": 76}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cd46095d4890809b31d7d5b8c4b3cde75522b135", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/cd46095d4890809b31d7d5b8c4b3cde75522b135", "committedDate": "2021-01-08T06:46:55Z", "message": "Addressing comments from ryan"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTY1ODYyNjQ2", "url": "https://github.com/apache/iceberg/pull/1793#pullrequestreview-565862646", "createdAt": "2021-01-12T01:10:49Z", "commit": {"oid": "cd46095d4890809b31d7d5b8c4b3cde75522b135"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xMlQwMToxMDo0OVrOIRtf8w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xMlQwMToxMDo0OVrOIRtf8w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTQ0MjE2Mw==", "bodyText": "Should this be called in dispose instead? The doc you pointed to says that dispose is when resources should be released, and that close may not be called:\n\nIn the case of a termination due to a failure or due to manual cancellation, the execution jumps directly to the dispose() and skips any intermediate phases between the phase the operator was in when the failure happened and the dispose().", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r555442163", "createdAt": "2021-01-12T01:10:49Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/StreamingMonitorFunction.java", "diffHunk": "@@ -0,0 +1,165 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.runtime.state.FunctionInitializationContext;\n+import org.apache.flink.runtime.state.FunctionSnapshotContext;\n+import org.apache.flink.streaming.api.checkpoint.CheckpointedFunction;\n+import org.apache.flink.streaming.api.functions.source.RichSourceFunction;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.util.SnapshotUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * This is the single (non-parallel) monitoring task which takes a {@link FlinkInputFormat},\n+ * it is responsible for:\n+ *\n+ * <ol>\n+ *     <li>Monitoring snapshots of the Iceberg table.</li>\n+ *     <li>Creating the {@link FlinkInputSplit splits} corresponding to the incremental files</li>\n+ *     <li>Assigning them to downstream tasks for further processing.</li>\n+ * </ol>\n+ *\n+ * <p>The splits to be read are forwarded to the downstream {@link StreamingReaderOperator}\n+ * which can have parallelism greater than one.\n+ */\n+public class StreamingMonitorFunction extends RichSourceFunction<FlinkInputSplit> implements CheckpointedFunction {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(StreamingMonitorFunction.class);\n+\n+  private static final long DUMMY_START_SNAPSHOT_ID = -1;\n+\n+  private final TableLoader tableLoader;\n+  private final ScanContext scanContext;\n+\n+  private volatile boolean isRunning = true;\n+  private transient Object checkpointLock;\n+  private transient Table table;\n+  private transient ListState<Long> snapshotIdState;\n+  private transient long startSnapshotId;\n+\n+  public StreamingMonitorFunction(TableLoader tableLoader, ScanContext scanContext) {\n+    Preconditions.checkArgument(scanContext.snapshotId() == null,\n+        \"Cannot set snapshot-id option for streaming reader\");\n+    Preconditions.checkArgument(scanContext.asOfTimestamp() == null,\n+        \"Cannot set as-of-timestamp option for streaming reader\");\n+    Preconditions.checkArgument(scanContext.endSnapshotId() == null,\n+        \"Cannot set end-snapshot-id option for streaming reader\");\n+    this.tableLoader = tableLoader;\n+    this.scanContext = scanContext;\n+  }\n+\n+  @Override\n+  public void initializeState(FunctionInitializationContext context) throws Exception {\n+    snapshotIdState = context.getOperatorStateStore().getListState(\n+        new ListStateDescriptor<>(\n+            \"snapshot-id-state\",\n+            LongSerializer.INSTANCE));\n+    tableLoader.open();\n+    table = tableLoader.loadTable();\n+    if (context.isRestored()) {\n+      LOG.info(\"Restoring state for the {}.\", getClass().getSimpleName());\n+      startSnapshotId = snapshotIdState.get().iterator().next();\n+    } else {\n+      Long optionStartSnapshot = scanContext.startSnapshotId();\n+      if (optionStartSnapshot != null) {\n+        if (!SnapshotUtil.ancestorOf(table, table.currentSnapshot().snapshotId(), optionStartSnapshot)) {\n+          throw new IllegalStateException(\"The option start-snapshot-id \" + optionStartSnapshot +\n+              \" is not an ancestor of the current snapshot\");\n+        }\n+\n+        startSnapshotId = optionStartSnapshot;\n+      } else {\n+        startSnapshotId = DUMMY_START_SNAPSHOT_ID;\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public void snapshotState(FunctionSnapshotContext context) throws Exception {\n+    snapshotIdState.clear();\n+    snapshotIdState.add(startSnapshotId);\n+  }\n+\n+  @Override\n+  public void run(SourceContext<FlinkInputSplit> ctx) throws Exception {\n+    checkpointLock = ctx.getCheckpointLock();\n+    while (isRunning) {\n+      synchronized (checkpointLock) {\n+        if (isRunning) {\n+          monitorAndForwardSplits(ctx);\n+        }\n+      }\n+      Thread.sleep(scanContext.monitorInterval().toMillis());\n+    }\n+  }\n+\n+  private void monitorAndForwardSplits(SourceContext<FlinkInputSplit> ctx) {\n+    table.refresh();\n+    Snapshot snapshot = table.currentSnapshot();\n+    if (snapshot != null && snapshot.snapshotId() != startSnapshotId) {\n+      long snapshotId = snapshot.snapshotId();\n+      // Read current static table if startSnapshotId not set.\n+      ScanContext newScanContext = startSnapshotId == DUMMY_START_SNAPSHOT_ID ?\n+          scanContext.copyWithSnapshotId(snapshotId) :\n+          scanContext.copyWithAppendsBetween(startSnapshotId, snapshotId);\n+\n+      FlinkInputSplit[] splits = FlinkSplitGenerator.createInputSplits(table, newScanContext);\n+      for (FlinkInputSplit split : splits) {\n+        ctx.collect(split);\n+      }\n+      startSnapshotId = snapshotId;\n+    }\n+  }\n+\n+  @Override\n+  public void cancel() {\n+    // this is to cover the case where cancel() is called before the run()\n+    if (checkpointLock != null) {\n+      synchronized (checkpointLock) {\n+        isRunning = false;\n+      }\n+    } else {\n+      isRunning = false;\n+    }\n+\n+    if (tableLoader != null) {\n+      try {\n+        tableLoader.close();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd46095d4890809b31d7d5b8c4b3cde75522b135"}, "originalPosition": 154}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTY1ODYzNjEz", "url": "https://github.com/apache/iceberg/pull/1793#pullrequestreview-565863613", "createdAt": "2021-01-12T01:12:14Z", "commit": {"oid": "cd46095d4890809b31d7d5b8c4b3cde75522b135"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xMlQwMToxMjoxNFrOIRtiHw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xMlQwMToxMjoxNFrOIRtiHw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTQ0MjcxOQ==", "bodyText": "This appears to be used as the last processed snapshot ID. I think it would make more sense to name it lastSnapshotId so it is clear that this snapshot has already been processed.", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r555442719", "createdAt": "2021-01-12T01:12:14Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/StreamingMonitorFunction.java", "diffHunk": "@@ -0,0 +1,165 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.runtime.state.FunctionInitializationContext;\n+import org.apache.flink.runtime.state.FunctionSnapshotContext;\n+import org.apache.flink.streaming.api.checkpoint.CheckpointedFunction;\n+import org.apache.flink.streaming.api.functions.source.RichSourceFunction;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.util.SnapshotUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * This is the single (non-parallel) monitoring task which takes a {@link FlinkInputFormat},\n+ * it is responsible for:\n+ *\n+ * <ol>\n+ *     <li>Monitoring snapshots of the Iceberg table.</li>\n+ *     <li>Creating the {@link FlinkInputSplit splits} corresponding to the incremental files</li>\n+ *     <li>Assigning them to downstream tasks for further processing.</li>\n+ * </ol>\n+ *\n+ * <p>The splits to be read are forwarded to the downstream {@link StreamingReaderOperator}\n+ * which can have parallelism greater than one.\n+ */\n+public class StreamingMonitorFunction extends RichSourceFunction<FlinkInputSplit> implements CheckpointedFunction {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(StreamingMonitorFunction.class);\n+\n+  private static final long DUMMY_START_SNAPSHOT_ID = -1;\n+\n+  private final TableLoader tableLoader;\n+  private final ScanContext scanContext;\n+\n+  private volatile boolean isRunning = true;\n+  private transient Object checkpointLock;\n+  private transient Table table;\n+  private transient ListState<Long> snapshotIdState;\n+  private transient long startSnapshotId;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd46095d4890809b31d7d5b8c4b3cde75522b135"}, "originalPosition": 65}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTY1ODY3MTEz", "url": "https://github.com/apache/iceberg/pull/1793#pullrequestreview-565867113", "createdAt": "2021-01-12T01:22:13Z", "commit": {"oid": "cd46095d4890809b31d7d5b8c4b3cde75522b135"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xMlQwMToyMjoxM1rOIRtu2Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xMlQwMToyMjoxM1rOIRtu2Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTQ0NTk3Nw==", "bodyText": "Adding an element with offer will return false if the element cannot be added. If that happens, then the split will be silently dropped because the return value is ignored. I think this should use add to throw an exception, or handle the boolean return value.", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r555445977", "createdAt": "2021-01-12T01:22:13Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/StreamingReaderOperator.java", "diffHunk": "@@ -0,0 +1,214 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.util.Queue;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.runtime.state.JavaSerializer;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.MailboxExecutor;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.StreamOperator;\n+import org.apache.flink.streaming.api.operators.StreamOperatorParameters;\n+import org.apache.flink.streaming.api.operators.StreamSourceContexts;\n+import org.apache.flink.streaming.api.operators.YieldingOperatorFactory;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.streaming.runtime.tasks.ProcessingTimeService;\n+import org.apache.flink.streaming.runtime.tasks.mailbox.MailboxExecutorImpl;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * The operator that reads the {@link FlinkInputSplit splits} received from the preceding {@link\n+ * StreamingMonitorFunction}. Contrary to the {@link StreamingMonitorFunction} which has a parallelism of 1,\n+ * this operator can have multiple parallelism.\n+ *\n+ * <p>As soon as a split descriptor is received, it is put in a queue, and use {@link MailboxExecutor}\n+ * read the actual data of the split. This architecture allows the separation of the reading thread from the one split\n+ * processing the checkpoint barriers, thus removing any potential back-pressure.\n+ */\n+public class StreamingReaderOperator extends AbstractStreamOperator<RowData>\n+    implements OneInputStreamOperator<FlinkInputSplit, RowData> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(StreamingReaderOperator.class);\n+  private final MailboxExecutorImpl executor;\n+  private FlinkInputFormat format;\n+\n+  private transient SourceFunction.SourceContext<RowData> readerContext;\n+\n+  private transient ListState<FlinkInputSplit> checkpointState;\n+  private transient Queue<FlinkInputSplit> splits;\n+\n+  private StreamingReaderOperator(FlinkInputFormat format, ProcessingTimeService timeService,\n+                                  MailboxExecutor mailboxExecutor) {\n+    this.format = Preconditions.checkNotNull(format, \"The InputFormat should not be null.\");\n+    this.processingTimeService = timeService;\n+    this.executor = (MailboxExecutorImpl) Preconditions.checkNotNull(mailboxExecutor,\n+        \"The mailboxExecutor should not be null.\");\n+  }\n+\n+  @Override\n+  public void initializeState(StateInitializationContext context) throws Exception {\n+    super.initializeState(context);\n+\n+    checkpointState = context.getOperatorStateStore().getListState(\n+        new ListStateDescriptor<>(\"splits\", new JavaSerializer<>()));\n+\n+    int subtaskIdx = getRuntimeContext().getIndexOfThisSubtask();\n+    splits = Lists.newLinkedList();\n+    if (context.isRestored()) {\n+      LOG.info(\"Restoring state for the {} (taskIdx={}).\", getClass().getSimpleName(), subtaskIdx);\n+\n+      for (FlinkInputSplit split : checkpointState.get()) {\n+        splits.add(split);\n+      }\n+    }\n+\n+    this.readerContext = StreamSourceContexts.getSourceContext(\n+        getOperatorConfig().getTimeCharacteristic(),\n+        getProcessingTimeService(),\n+        new Object(), // no actual locking needed\n+        getContainingTask().getStreamStatusMaintainer(),\n+        output,\n+        getRuntimeContext().getExecutionConfig().getAutoWatermarkInterval(),\n+        -1);\n+\n+    if (!splits.isEmpty()) {\n+      enqueueProcessSplits();\n+    }\n+  }\n+\n+  @Override\n+  public void processElement(StreamRecord<FlinkInputSplit> element) {\n+    splits.offer(element.getValue());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd46095d4890809b31d7d5b8c4b3cde75522b135"}, "originalPosition": 111}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTY1ODcyMjI0", "url": "https://github.com/apache/iceberg/pull/1793#pullrequestreview-565872224", "createdAt": "2021-01-12T01:30:26Z", "commit": {"oid": "cd46095d4890809b31d7d5b8c4b3cde75522b135"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xMlQwMTozMDoyN1rOIRt6NQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xMlQwMTozMDoyN1rOIRt6NQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTQ0ODg4NQ==", "bodyText": "In what case would the element be null, but format.reachedEnd() will not be true? That's the only case where break is needed, right? I would expect this to emit records until reachedEnd is true, regardless of whether any of them are null. This may choose to suppress null records, but I don't understand why that would change the loop.", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r555448885", "createdAt": "2021-01-12T01:30:27Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/StreamingReaderOperator.java", "diffHunk": "@@ -0,0 +1,214 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.util.Queue;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.runtime.state.JavaSerializer;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.MailboxExecutor;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.StreamOperator;\n+import org.apache.flink.streaming.api.operators.StreamOperatorParameters;\n+import org.apache.flink.streaming.api.operators.StreamSourceContexts;\n+import org.apache.flink.streaming.api.operators.YieldingOperatorFactory;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.streaming.runtime.tasks.ProcessingTimeService;\n+import org.apache.flink.streaming.runtime.tasks.mailbox.MailboxExecutorImpl;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * The operator that reads the {@link FlinkInputSplit splits} received from the preceding {@link\n+ * StreamingMonitorFunction}. Contrary to the {@link StreamingMonitorFunction} which has a parallelism of 1,\n+ * this operator can have multiple parallelism.\n+ *\n+ * <p>As soon as a split descriptor is received, it is put in a queue, and use {@link MailboxExecutor}\n+ * read the actual data of the split. This architecture allows the separation of the reading thread from the one split\n+ * processing the checkpoint barriers, thus removing any potential back-pressure.\n+ */\n+public class StreamingReaderOperator extends AbstractStreamOperator<RowData>\n+    implements OneInputStreamOperator<FlinkInputSplit, RowData> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(StreamingReaderOperator.class);\n+  private final MailboxExecutorImpl executor;\n+  private FlinkInputFormat format;\n+\n+  private transient SourceFunction.SourceContext<RowData> readerContext;\n+\n+  private transient ListState<FlinkInputSplit> checkpointState;\n+  private transient Queue<FlinkInputSplit> splits;\n+\n+  private StreamingReaderOperator(FlinkInputFormat format, ProcessingTimeService timeService,\n+                                  MailboxExecutor mailboxExecutor) {\n+    this.format = Preconditions.checkNotNull(format, \"The InputFormat should not be null.\");\n+    this.processingTimeService = timeService;\n+    this.executor = (MailboxExecutorImpl) Preconditions.checkNotNull(mailboxExecutor,\n+        \"The mailboxExecutor should not be null.\");\n+  }\n+\n+  @Override\n+  public void initializeState(StateInitializationContext context) throws Exception {\n+    super.initializeState(context);\n+\n+    checkpointState = context.getOperatorStateStore().getListState(\n+        new ListStateDescriptor<>(\"splits\", new JavaSerializer<>()));\n+\n+    int subtaskIdx = getRuntimeContext().getIndexOfThisSubtask();\n+    splits = Lists.newLinkedList();\n+    if (context.isRestored()) {\n+      LOG.info(\"Restoring state for the {} (taskIdx={}).\", getClass().getSimpleName(), subtaskIdx);\n+\n+      for (FlinkInputSplit split : checkpointState.get()) {\n+        splits.add(split);\n+      }\n+    }\n+\n+    this.readerContext = StreamSourceContexts.getSourceContext(\n+        getOperatorConfig().getTimeCharacteristic(),\n+        getProcessingTimeService(),\n+        new Object(), // no actual locking needed\n+        getContainingTask().getStreamStatusMaintainer(),\n+        output,\n+        getRuntimeContext().getExecutionConfig().getAutoWatermarkInterval(),\n+        -1);\n+\n+    if (!splits.isEmpty()) {\n+      enqueueProcessSplits();\n+    }\n+  }\n+\n+  @Override\n+  public void processElement(StreamRecord<FlinkInputSplit> element) {\n+    splits.offer(element.getValue());\n+    enqueueProcessSplits();\n+  }\n+\n+  private void enqueueProcessSplits() {\n+    executor.execute(this::processSplits, this.getClass().getSimpleName());\n+  }\n+\n+  private void processSplits() throws IOException {\n+    do {\n+      FlinkInputSplit split = splits.poll();\n+      if (split == null) {\n+        return;\n+      }\n+\n+      LOG.debug(\"load split: {}\", split);\n+      format.open(split);\n+\n+      RowData nextElement = null;\n+      while (!format.reachedEnd()) {\n+        nextElement = format.nextRecord(nextElement);\n+        if (nextElement != null) {\n+          readerContext.collect(nextElement);\n+        } else {\n+          break;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd46095d4890809b31d7d5b8c4b3cde75522b135"}, "originalPosition": 135}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTY1ODc0MjYy", "url": "https://github.com/apache/iceberg/pull/1793#pullrequestreview-565874262", "createdAt": "2021-01-12T01:36:21Z", "commit": {"oid": "cd46095d4890809b31d7d5b8c4b3cde75522b135"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xMlQwMTozNjoyMVrOIRuP1g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xMlQwMTozNjoyMVrOIRuP1g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTQ1NDQyMg==", "bodyText": "Why use the MailboxExecutorImpl class rather than the MailboxExecutor interface?", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r555454422", "createdAt": "2021-01-12T01:36:21Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/StreamingReaderOperator.java", "diffHunk": "@@ -0,0 +1,214 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.util.Queue;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.runtime.state.JavaSerializer;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.MailboxExecutor;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.StreamOperator;\n+import org.apache.flink.streaming.api.operators.StreamOperatorParameters;\n+import org.apache.flink.streaming.api.operators.StreamSourceContexts;\n+import org.apache.flink.streaming.api.operators.YieldingOperatorFactory;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.streaming.runtime.tasks.ProcessingTimeService;\n+import org.apache.flink.streaming.runtime.tasks.mailbox.MailboxExecutorImpl;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * The operator that reads the {@link FlinkInputSplit splits} received from the preceding {@link\n+ * StreamingMonitorFunction}. Contrary to the {@link StreamingMonitorFunction} which has a parallelism of 1,\n+ * this operator can have multiple parallelism.\n+ *\n+ * <p>As soon as a split descriptor is received, it is put in a queue, and use {@link MailboxExecutor}\n+ * read the actual data of the split. This architecture allows the separation of the reading thread from the one split\n+ * processing the checkpoint barriers, thus removing any potential back-pressure.\n+ */\n+public class StreamingReaderOperator extends AbstractStreamOperator<RowData>\n+    implements OneInputStreamOperator<FlinkInputSplit, RowData> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(StreamingReaderOperator.class);\n+  private final MailboxExecutorImpl executor;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd46095d4890809b31d7d5b8c4b3cde75522b135"}, "originalPosition": 62}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTY1ODgxNzMw", "url": "https://github.com/apache/iceberg/pull/1793#pullrequestreview-565881730", "createdAt": "2021-01-12T01:58:05Z", "commit": {"oid": "cd46095d4890809b31d7d5b8c4b3cde75522b135"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xMlQwMTo1ODowNVrOIRur2A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xMlQwMTo1ODowNVrOIRur2A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTQ2MTU5Mg==", "bodyText": "Shouldn't this be called in a finally block to ensure that close is called?", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r555461592", "createdAt": "2021-01-12T01:58:05Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/StreamingReaderOperator.java", "diffHunk": "@@ -0,0 +1,214 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.util.Queue;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.runtime.state.JavaSerializer;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.MailboxExecutor;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.StreamOperator;\n+import org.apache.flink.streaming.api.operators.StreamOperatorParameters;\n+import org.apache.flink.streaming.api.operators.StreamSourceContexts;\n+import org.apache.flink.streaming.api.operators.YieldingOperatorFactory;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.streaming.runtime.tasks.ProcessingTimeService;\n+import org.apache.flink.streaming.runtime.tasks.mailbox.MailboxExecutorImpl;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * The operator that reads the {@link FlinkInputSplit splits} received from the preceding {@link\n+ * StreamingMonitorFunction}. Contrary to the {@link StreamingMonitorFunction} which has a parallelism of 1,\n+ * this operator can have multiple parallelism.\n+ *\n+ * <p>As soon as a split descriptor is received, it is put in a queue, and use {@link MailboxExecutor}\n+ * read the actual data of the split. This architecture allows the separation of the reading thread from the one split\n+ * processing the checkpoint barriers, thus removing any potential back-pressure.\n+ */\n+public class StreamingReaderOperator extends AbstractStreamOperator<RowData>\n+    implements OneInputStreamOperator<FlinkInputSplit, RowData> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(StreamingReaderOperator.class);\n+  private final MailboxExecutorImpl executor;\n+  private FlinkInputFormat format;\n+\n+  private transient SourceFunction.SourceContext<RowData> readerContext;\n+\n+  private transient ListState<FlinkInputSplit> checkpointState;\n+  private transient Queue<FlinkInputSplit> splits;\n+\n+  private StreamingReaderOperator(FlinkInputFormat format, ProcessingTimeService timeService,\n+                                  MailboxExecutor mailboxExecutor) {\n+    this.format = Preconditions.checkNotNull(format, \"The InputFormat should not be null.\");\n+    this.processingTimeService = timeService;\n+    this.executor = (MailboxExecutorImpl) Preconditions.checkNotNull(mailboxExecutor,\n+        \"The mailboxExecutor should not be null.\");\n+  }\n+\n+  @Override\n+  public void initializeState(StateInitializationContext context) throws Exception {\n+    super.initializeState(context);\n+\n+    checkpointState = context.getOperatorStateStore().getListState(\n+        new ListStateDescriptor<>(\"splits\", new JavaSerializer<>()));\n+\n+    int subtaskIdx = getRuntimeContext().getIndexOfThisSubtask();\n+    splits = Lists.newLinkedList();\n+    if (context.isRestored()) {\n+      LOG.info(\"Restoring state for the {} (taskIdx={}).\", getClass().getSimpleName(), subtaskIdx);\n+\n+      for (FlinkInputSplit split : checkpointState.get()) {\n+        splits.add(split);\n+      }\n+    }\n+\n+    this.readerContext = StreamSourceContexts.getSourceContext(\n+        getOperatorConfig().getTimeCharacteristic(),\n+        getProcessingTimeService(),\n+        new Object(), // no actual locking needed\n+        getContainingTask().getStreamStatusMaintainer(),\n+        output,\n+        getRuntimeContext().getExecutionConfig().getAutoWatermarkInterval(),\n+        -1);\n+\n+    if (!splits.isEmpty()) {\n+      enqueueProcessSplits();\n+    }\n+  }\n+\n+  @Override\n+  public void processElement(StreamRecord<FlinkInputSplit> element) {\n+    splits.offer(element.getValue());\n+    enqueueProcessSplits();\n+  }\n+\n+  private void enqueueProcessSplits() {\n+    executor.execute(this::processSplits, this.getClass().getSimpleName());\n+  }\n+\n+  private void processSplits() throws IOException {\n+    do {\n+      FlinkInputSplit split = splits.poll();\n+      if (split == null) {\n+        return;\n+      }\n+\n+      LOG.debug(\"load split: {}\", split);\n+      format.open(split);\n+\n+      RowData nextElement = null;\n+      while (!format.reachedEnd()) {\n+        nextElement = format.nextRecord(nextElement);\n+        if (nextElement != null) {\n+          readerContext.collect(nextElement);\n+        } else {\n+          break;\n+        }\n+      }\n+\n+      format.close();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd46095d4890809b31d7d5b8c4b3cde75522b135"}, "originalPosition": 139}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ea3b7e118b7503a3b2d2e7208805ab997840f866", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/ea3b7e118b7503a3b2d2e7208805ab997840f866", "committedDate": "2021-01-12T09:05:46Z", "message": "Minor changes & address comments from Ryan."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "33554971c192cd6b94f5d7ae7766219cd7e7b6d9", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/33554971c192cd6b94f5d7ae7766219cd7e7b6d9", "committedDate": "2021-01-12T13:40:05Z", "message": "Execute the processSplits once per split"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "77d9aa45fa63f66f93d514501abbb79bd988bf4b", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/77d9aa45fa63f66f93d514501abbb79bd988bf4b", "committedDate": "2021-01-12T14:49:33Z", "message": "Fix the snapshot ancestor issue"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0ce3ce6e1b4a52f10313b3bf2c24762042741ef0", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/0ce3ce6e1b4a52f10313b3bf2c24762042741ef0", "committedDate": "2021-01-12T15:08:11Z", "message": "Minor changes."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTY2NTMyMDI5", "url": "https://github.com/apache/iceberg/pull/1793#pullrequestreview-566532029", "createdAt": "2021-01-12T17:59:52Z", "commit": {"oid": "0ce3ce6e1b4a52f10313b3bf2c24762042741ef0"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xMlQxNzo1OTo1MlrOISNoQw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xMlQxNzo1OTo1MlrOISNoQw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTk2ODU3OQ==", "bodyText": "Split state should be set before starting a concurrent operation right?", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r555968579", "createdAt": "2021-01-12T17:59:52Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/StreamingReaderOperator.java", "diffHunk": "@@ -0,0 +1,233 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.util.Queue;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.runtime.state.JavaSerializer;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.MailboxExecutor;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.StreamOperator;\n+import org.apache.flink.streaming.api.operators.StreamOperatorParameters;\n+import org.apache.flink.streaming.api.operators.StreamSourceContexts;\n+import org.apache.flink.streaming.api.operators.YieldingOperatorFactory;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.streaming.runtime.tasks.ProcessingTimeService;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * The operator that reads the {@link FlinkInputSplit splits} received from the preceding {@link\n+ * StreamingMonitorFunction}. Contrary to the {@link StreamingMonitorFunction} which has a parallelism of 1,\n+ * this operator can have multiple parallelism.\n+ *\n+ * <p>As soon as a split descriptor is received, it is put in a queue, and use {@link MailboxExecutor}\n+ * read the actual data of the split. This architecture allows the separation of the reading thread from the one split\n+ * processing the checkpoint barriers, thus removing any potential back-pressure.\n+ */\n+public class StreamingReaderOperator extends AbstractStreamOperator<RowData>\n+    implements OneInputStreamOperator<FlinkInputSplit, RowData> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(StreamingReaderOperator.class);\n+\n+  private final MailboxExecutor executor;\n+  private FlinkInputFormat format;\n+\n+  private transient SourceFunction.SourceContext<RowData> sourceContext;\n+\n+  private transient ListState<FlinkInputSplit> inputSplitsState;\n+  private transient Queue<FlinkInputSplit> splits;\n+\n+  // This state is used to control that only one split is occupying the executor for record reading. If there're more\n+  // splits coming, we will buffer them in flink's state. Once the executor is idle (the currentSplitState will be\n+  // marked as IDLE), it will schedule one new split from buffered splits in flink's state to executor (the\n+  // currentSplitState will be marked as RUNNING). After finished all records processing, the currentSplitState will\n+  // be marked as IDLE again.\n+  // NOTICE: all the reader and writer of this variable are the same thread, so we don't need extra synchronization.\n+  private transient SplitState currentSplitState;\n+\n+  private StreamingReaderOperator(FlinkInputFormat format, ProcessingTimeService timeService,\n+                                  MailboxExecutor mailboxExecutor) {\n+    this.format = Preconditions.checkNotNull(format, \"The InputFormat should not be null.\");\n+    this.processingTimeService = timeService;\n+    this.executor = Preconditions.checkNotNull(mailboxExecutor, \"The mailboxExecutor should not be null.\");\n+  }\n+\n+  @Override\n+  public void initializeState(StateInitializationContext context) throws Exception {\n+    super.initializeState(context);\n+\n+    // TODO Replace Java serialization with Avro approach to keep state compatibility.\n+    // See issue: https://github.com/apache/iceberg/issues/1698\n+    inputSplitsState = context.getOperatorStateStore().getListState(\n+        new ListStateDescriptor<>(\"splits\", new JavaSerializer<>()));\n+\n+    // Initialize the current split state to IDLE.\n+    currentSplitState = SplitState.IDLE;\n+\n+    // Recover splits state from flink state backend if possible.\n+    splits = Lists.newLinkedList();\n+    if (context.isRestored()) {\n+      int subtaskIdx = getRuntimeContext().getIndexOfThisSubtask();\n+      LOG.info(\"Restoring state for the {} (taskIdx: {}).\", getClass().getSimpleName(), subtaskIdx);\n+\n+      for (FlinkInputSplit split : inputSplitsState.get()) {\n+        splits.add(split);\n+      }\n+    }\n+\n+    this.sourceContext = StreamSourceContexts.getSourceContext(\n+        getOperatorConfig().getTimeCharacteristic(),\n+        getProcessingTimeService(),\n+        new Object(), // no actual locking needed\n+        getContainingTask().getStreamStatusMaintainer(),\n+        output,\n+        getRuntimeContext().getExecutionConfig().getAutoWatermarkInterval(),\n+        -1);\n+\n+    if (!splits.isEmpty()) {\n+      enqueueProcessSplits();\n+    }\n+  }\n+\n+  @Override\n+  public void processElement(StreamRecord<FlinkInputSplit> element) {\n+    splits.add(element.getValue());\n+    enqueueProcessSplits();\n+  }\n+\n+  private void enqueueProcessSplits() {\n+    if (currentSplitState == SplitState.IDLE) {\n+      executor.execute(this::processSplits, this.getClass().getSimpleName());\n+      currentSplitState = SplitState.RUNNING;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0ce3ce6e1b4a52f10313b3bf2c24762042741ef0"}, "originalPosition": 131}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTY2NTMzNzc1", "url": "https://github.com/apache/iceberg/pull/1793#pullrequestreview-566533775", "createdAt": "2021-01-12T18:02:05Z", "commit": {"oid": "0ce3ce6e1b4a52f10313b3bf2c24762042741ef0"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xMlQxODowMjowNVrOISNtRg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xMlQxODowMjowNVrOISNtRg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTk2OTg2Mg==", "bodyText": "This should be volatile if it is shared across threads.", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r555969862", "createdAt": "2021-01-12T18:02:05Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/StreamingReaderOperator.java", "diffHunk": "@@ -0,0 +1,233 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.util.Queue;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.runtime.state.JavaSerializer;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.MailboxExecutor;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.StreamOperator;\n+import org.apache.flink.streaming.api.operators.StreamOperatorParameters;\n+import org.apache.flink.streaming.api.operators.StreamSourceContexts;\n+import org.apache.flink.streaming.api.operators.YieldingOperatorFactory;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.streaming.runtime.tasks.ProcessingTimeService;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * The operator that reads the {@link FlinkInputSplit splits} received from the preceding {@link\n+ * StreamingMonitorFunction}. Contrary to the {@link StreamingMonitorFunction} which has a parallelism of 1,\n+ * this operator can have multiple parallelism.\n+ *\n+ * <p>As soon as a split descriptor is received, it is put in a queue, and use {@link MailboxExecutor}\n+ * read the actual data of the split. This architecture allows the separation of the reading thread from the one split\n+ * processing the checkpoint barriers, thus removing any potential back-pressure.\n+ */\n+public class StreamingReaderOperator extends AbstractStreamOperator<RowData>\n+    implements OneInputStreamOperator<FlinkInputSplit, RowData> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(StreamingReaderOperator.class);\n+\n+  private final MailboxExecutor executor;\n+  private FlinkInputFormat format;\n+\n+  private transient SourceFunction.SourceContext<RowData> sourceContext;\n+\n+  private transient ListState<FlinkInputSplit> inputSplitsState;\n+  private transient Queue<FlinkInputSplit> splits;\n+\n+  // This state is used to control that only one split is occupying the executor for record reading. If there're more\n+  // splits coming, we will buffer them in flink's state. Once the executor is idle (the currentSplitState will be\n+  // marked as IDLE), it will schedule one new split from buffered splits in flink's state to executor (the\n+  // currentSplitState will be marked as RUNNING). After finished all records processing, the currentSplitState will\n+  // be marked as IDLE again.\n+  // NOTICE: all the reader and writer of this variable are the same thread, so we don't need extra synchronization.\n+  private transient SplitState currentSplitState;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0ce3ce6e1b4a52f10313b3bf2c24762042741ef0"}, "originalPosition": 76}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTY2NTM1OTIx", "url": "https://github.com/apache/iceberg/pull/1793#pullrequestreview-566535921", "createdAt": "2021-01-12T18:04:54Z", "commit": {"oid": "0ce3ce6e1b4a52f10313b3bf2c24762042741ef0"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xMlQxODowNDo1NFrOISNzhQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xMlQxODowNDo1NFrOISNzhQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTk3MTQ2MQ==", "bodyText": "I don't think that using MailboxExecutor is a good idea. If I understand correctly, the mailbox queue for the executor cannot be used to hold state because that state would not be checkpointed (if, for example, it held the splits waiting to be processed). The result is that this operator has an elaborate way to keep state in a queue and continuously submit stateless mailbox tasks to keep running. But a simpler option is to create a thread directly that polls the splits queue and keeps running endlessly.", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r555971461", "createdAt": "2021-01-12T18:04:54Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/StreamingReaderOperator.java", "diffHunk": "@@ -0,0 +1,233 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.util.Queue;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.runtime.state.JavaSerializer;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.MailboxExecutor;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.StreamOperator;\n+import org.apache.flink.streaming.api.operators.StreamOperatorParameters;\n+import org.apache.flink.streaming.api.operators.StreamSourceContexts;\n+import org.apache.flink.streaming.api.operators.YieldingOperatorFactory;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.streaming.runtime.tasks.ProcessingTimeService;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * The operator that reads the {@link FlinkInputSplit splits} received from the preceding {@link\n+ * StreamingMonitorFunction}. Contrary to the {@link StreamingMonitorFunction} which has a parallelism of 1,\n+ * this operator can have multiple parallelism.\n+ *\n+ * <p>As soon as a split descriptor is received, it is put in a queue, and use {@link MailboxExecutor}\n+ * read the actual data of the split. This architecture allows the separation of the reading thread from the one split\n+ * processing the checkpoint barriers, thus removing any potential back-pressure.\n+ */\n+public class StreamingReaderOperator extends AbstractStreamOperator<RowData>\n+    implements OneInputStreamOperator<FlinkInputSplit, RowData> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(StreamingReaderOperator.class);\n+\n+  private final MailboxExecutor executor;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0ce3ce6e1b4a52f10313b3bf2c24762042741ef0"}, "originalPosition": 62}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5899a9f21329a782c54e99b56abe157128f2cf35", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/5899a9f21329a782c54e99b56abe157128f2cf35", "committedDate": "2021-01-13T10:08:17Z", "message": "More unit tests for StreamingReaderOperator."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b8e91f189c640ad90a840970555735267e06cd9e", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/b8e91f189c640ad90a840970555735267e06cd9e", "committedDate": "2021-01-13T13:10:24Z", "message": "More UT for streaming-monitor-funciton."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTY3NzcxNTU3", "url": "https://github.com/apache/iceberg/pull/1793#pullrequestreview-567771557", "createdAt": "2021-01-14T00:44:23Z", "commit": {"oid": "b8e91f189c640ad90a840970555735267e06cd9e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNFQwMDo0NDoyNFrOITK0sw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNFQwMDo0NDoyNFrOITK0sw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Njk3MTE4Nw==", "bodyText": "Minor: Should this only queue a task if splits is non-empty?\nIf splits is currently empty and this is called from processSplits, then a new task will be queued. That task will process a split if one is waiting in the mailbox queue to be processed, but often it will do nothing and set the split state back to IDLE.\nI'd probably only add a new task if splits is non-empty, or update processSplits to always submit a new task and not set IDLE in the finally block.", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r556971187", "createdAt": "2021-01-14T00:44:24Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/StreamingReaderOperator.java", "diffHunk": "@@ -0,0 +1,232 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.util.Queue;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.runtime.state.JavaSerializer;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.MailboxExecutor;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.StreamOperator;\n+import org.apache.flink.streaming.api.operators.StreamOperatorParameters;\n+import org.apache.flink.streaming.api.operators.StreamSourceContexts;\n+import org.apache.flink.streaming.api.operators.YieldingOperatorFactory;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.streaming.runtime.tasks.ProcessingTimeService;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * The operator that reads the {@link FlinkInputSplit splits} received from the preceding {@link\n+ * StreamingMonitorFunction}. Contrary to the {@link StreamingMonitorFunction} which has a parallelism of 1,\n+ * this operator can have multiple parallelism.\n+ *\n+ * <p>As soon as a split descriptor is received, it is put in a queue, and use {@link MailboxExecutor}\n+ * read the actual data of the split. This architecture allows the separation of the reading thread from the one split\n+ * processing the checkpoint barriers, thus removing any potential back-pressure.\n+ */\n+public class StreamingReaderOperator extends AbstractStreamOperator<RowData>\n+    implements OneInputStreamOperator<FlinkInputSplit, RowData> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(StreamingReaderOperator.class);\n+\n+  private final MailboxExecutor executor;\n+  private FlinkInputFormat format;\n+\n+  private transient SourceFunction.SourceContext<RowData> sourceContext;\n+\n+  private transient ListState<FlinkInputSplit> inputSplitsState;\n+  private transient Queue<FlinkInputSplit> splits;\n+\n+  // This state is used to control that only one split is occupying the executor for record reading. If there're more\n+  // splits coming, we will buffer them in flink's state. Once the executor is idle (the currentSplitState will be\n+  // marked as IDLE), it will schedule one new split from buffered splits in flink's state to executor (the\n+  // currentSplitState will be marked as RUNNING). After finished all records processing, the currentSplitState will\n+  // be marked as IDLE again.\n+  // NOTICE: all the reader and writer of this variable are the same thread, so we don't need extra synchronization.\n+  private transient SplitState currentSplitState;\n+\n+  private StreamingReaderOperator(FlinkInputFormat format, ProcessingTimeService timeService,\n+                                  MailboxExecutor mailboxExecutor) {\n+    this.format = Preconditions.checkNotNull(format, \"The InputFormat should not be null.\");\n+    this.processingTimeService = timeService;\n+    this.executor = Preconditions.checkNotNull(mailboxExecutor, \"The mailboxExecutor should not be null.\");\n+  }\n+\n+  @Override\n+  public void initializeState(StateInitializationContext context) throws Exception {\n+    super.initializeState(context);\n+\n+    // TODO Replace Java serialization with Avro approach to keep state compatibility.\n+    // See issue: https://github.com/apache/iceberg/issues/1698\n+    inputSplitsState = context.getOperatorStateStore().getListState(\n+        new ListStateDescriptor<>(\"splits\", new JavaSerializer<>()));\n+\n+    // Initialize the current split state to IDLE.\n+    currentSplitState = SplitState.IDLE;\n+\n+    // Recover splits state from flink state backend if possible.\n+    splits = Lists.newLinkedList();\n+    if (context.isRestored()) {\n+      int subtaskIdx = getRuntimeContext().getIndexOfThisSubtask();\n+      LOG.info(\"Restoring state for the {} (taskIdx: {}).\", getClass().getSimpleName(), subtaskIdx);\n+\n+      for (FlinkInputSplit split : inputSplitsState.get()) {\n+        splits.add(split);\n+      }\n+    }\n+\n+    this.sourceContext = StreamSourceContexts.getSourceContext(\n+        getOperatorConfig().getTimeCharacteristic(),\n+        getProcessingTimeService(),\n+        new Object(), // no actual locking needed\n+        getContainingTask().getStreamStatusMaintainer(),\n+        output,\n+        getRuntimeContext().getExecutionConfig().getAutoWatermarkInterval(),\n+        -1);\n+\n+    if (!splits.isEmpty()) {\n+      enqueueProcessSplits();\n+    }\n+  }\n+\n+  @Override\n+  public void snapshotState(StateSnapshotContext context) throws Exception {\n+    super.snapshotState(context);\n+\n+    inputSplitsState.clear();\n+    inputSplitsState.addAll(Lists.newArrayList(splits));\n+  }\n+\n+  @Override\n+  public void processElement(StreamRecord<FlinkInputSplit> element) {\n+    splits.add(element.getValue());\n+    enqueueProcessSplits();\n+  }\n+\n+  private void enqueueProcessSplits() {\n+    if (currentSplitState == SplitState.IDLE) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b8e91f189c640ad90a840970555735267e06cd9e"}, "originalPosition": 137}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTY3Nzc0MDQ2", "url": "https://github.com/apache/iceberg/pull/1793#pullrequestreview-567774046", "createdAt": "2021-01-14T00:51:04Z", "commit": {"oid": "b8e91f189c640ad90a840970555735267e06cd9e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNFQwMDo1MTowNFrOITK9YA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNFQwMDo1MTowNFrOITK9YA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Njk3MzQwOA==", "bodyText": "I think I would rephrase the comment here. I didn't really understand it when I read it the first time. Here's what I would suggest, assuming that I understand what's happening here:\n\nSplits are read by the same thread that calls processElement. Each read task is submitted to that thread by adding them to the executor. This state is used to ensure that only one read task is in that queue at a time, so that read tasks do not accumulate ahead of checkpoint tasks. When there is a read task in the queue, this is set to RUNNING. When there are no more files to read, this will be set to IDLE.", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r556973408", "createdAt": "2021-01-14T00:51:04Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/StreamingReaderOperator.java", "diffHunk": "@@ -0,0 +1,232 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.util.Queue;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.runtime.state.JavaSerializer;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.MailboxExecutor;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.StreamOperator;\n+import org.apache.flink.streaming.api.operators.StreamOperatorParameters;\n+import org.apache.flink.streaming.api.operators.StreamSourceContexts;\n+import org.apache.flink.streaming.api.operators.YieldingOperatorFactory;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.streaming.runtime.tasks.ProcessingTimeService;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * The operator that reads the {@link FlinkInputSplit splits} received from the preceding {@link\n+ * StreamingMonitorFunction}. Contrary to the {@link StreamingMonitorFunction} which has a parallelism of 1,\n+ * this operator can have multiple parallelism.\n+ *\n+ * <p>As soon as a split descriptor is received, it is put in a queue, and use {@link MailboxExecutor}\n+ * read the actual data of the split. This architecture allows the separation of the reading thread from the one split\n+ * processing the checkpoint barriers, thus removing any potential back-pressure.\n+ */\n+public class StreamingReaderOperator extends AbstractStreamOperator<RowData>\n+    implements OneInputStreamOperator<FlinkInputSplit, RowData> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(StreamingReaderOperator.class);\n+\n+  private final MailboxExecutor executor;\n+  private FlinkInputFormat format;\n+\n+  private transient SourceFunction.SourceContext<RowData> sourceContext;\n+\n+  private transient ListState<FlinkInputSplit> inputSplitsState;\n+  private transient Queue<FlinkInputSplit> splits;\n+\n+  // This state is used to control that only one split is occupying the executor for record reading. If there're more\n+  // splits coming, we will buffer them in flink's state. Once the executor is idle (the currentSplitState will be\n+  // marked as IDLE), it will schedule one new split from buffered splits in flink's state to executor (the\n+  // currentSplitState will be marked as RUNNING). After finished all records processing, the currentSplitState will\n+  // be marked as IDLE again.\n+  // NOTICE: all the reader and writer of this variable are the same thread, so we don't need extra synchronization.\n+  private transient SplitState currentSplitState;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b8e91f189c640ad90a840970555735267e06cd9e"}, "originalPosition": 76}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTY3Nzc3MDMy", "url": "https://github.com/apache/iceberg/pull/1793#pullrequestreview-567777032", "createdAt": "2021-01-14T00:59:04Z", "commit": {"oid": "b8e91f189c640ad90a840970555735267e06cd9e"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ea532f4ba26f19612eacb7f9d263ea5df5b1155f", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/ea532f4ba26f19612eacb7f9d263ea5df5b1155f", "committedDate": "2021-01-14T03:33:13Z", "message": "Addressing comments from Ryan."}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3374, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}