{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDA1NDI5NDAw", "number": 937, "title": "Spark: Implement an adapter to wrap Row into DataFile", "bodyText": "This PR implements an adapter to wrap Spark Rows into DataFiles so that we can write Spark Rows to a manifest.", "createdAt": "2020-04-18T02:14:49Z", "url": "https://github.com/apache/iceberg/pull/937", "merged": true, "mergeCommit": {"oid": "3160518b01a84042d27dc958b728f22746d964bf"}, "closed": true, "closedAt": "2020-04-21T00:13:55Z", "author": {"login": "aokolnychyi"}, "timelineItems": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcYsNrvAH2gAyNDA1NDI5NDAwOjMxZTdkOWE5OGY5YzMyZWNiZGM4MjcwNDA3NjdhNGE5YzQ3MjJjYjA=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcZoK7uAH2gAyNDA1NDI5NDAwOjY2OGM4M2ZjOWExZmQ3ZDQwYzc5OWY1MzgxYTNjN2FjNDZjMTRkYzU=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "31e7d9a98f9c32ecbdc827040767a4a9c4722cb0", "author": {"user": {"login": "aokolnychyi", "name": "Anton Okolnychyi"}}, "url": "https://github.com/apache/iceberg/commit/31e7d9a98f9c32ecbdc827040767a4a9c4722cb0", "committedDate": "2020-04-18T02:11:02Z", "message": "Spark: Implement an adapter to wrap Row into DataFile"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk1ODcyMTE2", "url": "https://github.com/apache/iceberg/pull/937#pullrequestreview-395872116", "createdAt": "2020-04-18T02:18:01Z", "commit": {"oid": "31e7d9a98f9c32ecbdc827040767a4a9c4722cb0"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xOFQwMjoxODowMVrOGHikcw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xOFQwMjoyMDoxM1rOGHioUw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDU1OTYwMw==", "bodyText": "As much as I don't like to use the index, referring by column name seems to incur an additional penalty as we will resolve the column name every time (based on what I see in Spark Row).", "url": "https://github.com/apache/iceberg/pull/937#discussion_r410559603", "createdAt": "2020-04-18T02:18:01Z", "author": {"login": "aokolnychyi"}, "path": "spark/src/main/java/org/apache/iceberg/spark/SparkDataFile.java", "diffHunk": "@@ -0,0 +1,134 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark;\n+\n+import java.nio.ByteBuffer;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.sql.Row;\n+\n+public class SparkDataFile implements DataFile {\n+\n+  private final Type lowerBoundsType;\n+  private final Type upperBoundsType;\n+  private final Type keyMetadataType;\n+  private final int fieldShift;\n+  private final SparkStructLike wrappedPartition;\n+  private Row wrapped;\n+\n+  public SparkDataFile(Types.StructType type) {\n+    this.lowerBoundsType = type.fieldType(\"lower_bounds\");\n+    this.upperBoundsType = type.fieldType(\"upper_bounds\");\n+    this.keyMetadataType = type.fieldType(\"key_metadata\");\n+    this.wrappedPartition = new SparkStructLike(type.fieldType(\"partition\").asStructType());\n+    // the partition field is absent for unpartitioned tables\n+    this.fieldShift = wrappedPartition.size() != 0 ? 1 : 0;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "31e7d9a98f9c32ecbdc827040767a4a9c4722cb0"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDU2MDU5NQ==", "bodyText": "I am using getAs instead of getLong as getLong and other methods specific to primitives perform a check if the value is null. The downside might be boxing/unboxing.", "url": "https://github.com/apache/iceberg/pull/937#discussion_r410560595", "createdAt": "2020-04-18T02:20:13Z", "author": {"login": "aokolnychyi"}, "path": "spark/src/main/java/org/apache/iceberg/spark/SparkDataFile.java", "diffHunk": "@@ -0,0 +1,134 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark;\n+\n+import java.nio.ByteBuffer;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.sql.Row;\n+\n+public class SparkDataFile implements DataFile {\n+\n+  private final Type lowerBoundsType;\n+  private final Type upperBoundsType;\n+  private final Type keyMetadataType;\n+  private final int fieldShift;\n+  private final SparkStructLike wrappedPartition;\n+  private Row wrapped;\n+\n+  public SparkDataFile(Types.StructType type) {\n+    this.lowerBoundsType = type.fieldType(\"lower_bounds\");\n+    this.upperBoundsType = type.fieldType(\"upper_bounds\");\n+    this.keyMetadataType = type.fieldType(\"key_metadata\");\n+    this.wrappedPartition = new SparkStructLike(type.fieldType(\"partition\").asStructType());\n+    // the partition field is absent for unpartitioned tables\n+    this.fieldShift = wrappedPartition.size() != 0 ? 1 : 0;\n+  }\n+\n+  public SparkDataFile wrap(Row row) {\n+    this.wrapped = row;\n+    if (wrappedPartition.size() > 0) {\n+      this.wrappedPartition.wrap(row.getAs(2));\n+    }\n+    return this;\n+  }\n+\n+  @Override\n+  public CharSequence path() {\n+    return wrapped.getAs(0);\n+  }\n+\n+  @Override\n+  public FileFormat format() {\n+    String formatAsString = wrapped.getString(1).toUpperCase(Locale.ROOT);\n+    return FileFormat.valueOf(formatAsString);\n+  }\n+\n+  @Override\n+  public StructLike partition() {\n+    return wrappedPartition;\n+  }\n+\n+  @Override\n+  public long recordCount() {\n+    return wrapped.getAs(fieldShift + 2);\n+  }\n+\n+  @Override\n+  public long fileSizeInBytes() {\n+    return wrapped.getAs(fieldShift + 3);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "31e7d9a98f9c32ecbdc827040767a4a9c4722cb0"}, "originalPosition": 82}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk2NzAyNDc5", "url": "https://github.com/apache/iceberg/pull/937#pullrequestreview-396702479", "createdAt": "2020-04-20T18:23:22Z", "commit": {"oid": "31e7d9a98f9c32ecbdc827040767a4a9c4722cb0"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMFQxODoyMzoyMlrOGIhuOA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMFQxODoyMzoyMlrOGIhuOA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTU5NDI5Ng==", "bodyText": "Why is this necessary? It looks like all of the code here is added to Spark.", "url": "https://github.com/apache/iceberg/pull/937#discussion_r411594296", "createdAt": "2020-04-20T18:23:22Z", "author": {"login": "rdblue"}, "path": "build.gradle", "diffHunk": "@@ -307,6 +307,7 @@ project(':iceberg-spark') {\n     compile project(':iceberg-api')\n     compile project(':iceberg-common')\n     compile project(':iceberg-core')\n+    compile project(':iceberg-data')", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "31e7d9a98f9c32ecbdc827040767a4a9c4722cb0"}, "originalPosition": 4}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk2NzA2MjM0", "url": "https://github.com/apache/iceberg/pull/937#pullrequestreview-396706234", "createdAt": "2020-04-20T18:28:29Z", "commit": {"oid": "31e7d9a98f9c32ecbdc827040767a4a9c4722cb0"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMFQxODoyODozMFrOGIh66g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMFQxODoyODozMFrOGIh66g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTU5NzU0Ng==", "bodyText": "It would be nice to have some Javadoc about this, like that values are converted to Iceberg's internal representation.", "url": "https://github.com/apache/iceberg/pull/937#discussion_r411597546", "createdAt": "2020-04-20T18:28:30Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/SparkValueConverter.java", "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark;\n+\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Maps;\n+import java.nio.ByteBuffer;\n+import java.sql.Date;\n+import java.sql.Timestamp;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils;\n+\n+public class SparkValueConverter {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "31e7d9a98f9c32ecbdc827040767a4a9c4722cb0"}, "originalPosition": 37}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2a1c840e546d41613d18b8fe07dd5adae78b001c", "author": {"user": {"login": "aokolnychyi", "name": "Anton Okolnychyi"}}, "url": "https://github.com/apache/iceberg/commit/2a1c840e546d41613d18b8fe07dd5adae78b001c", "committedDate": "2020-04-20T23:56:41Z", "message": "Switch to position array"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "668c83fc9a1fd7d40c799f5381a3c7ac46c14dc5", "author": {"user": {"login": "aokolnychyi", "name": "Anton Okolnychyi"}}, "url": "https://github.com/apache/iceberg/commit/668c83fc9a1fd7d40c799f5381a3c7ac46c14dc5", "committedDate": "2020-04-21T00:02:20Z", "message": "Fix javadoc"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4826, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}