{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzk1Nzk4MTcw", "number": 882, "reviewThreads": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMVQxNzozMjozOVrODtom4A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMVQxODoyNToyOFrODtpybA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ5MTc3ODI0OnYy", "diffSide": "RIGHT", "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestDataSourceOptions.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMVQxNzozMjozOVrOF_LQdQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMVQyMzowNDo1MVrOF_Vujg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTc4OTA0NQ==", "bodyText": "@rdblue additional 103 in writer length, can this be bug in writer factory which returns buffer size after flush?\n( no rush of merging this PR , I am trying to make sure changes are ok)", "url": "https://github.com/apache/iceberg/pull/882#discussion_r401789045", "createdAt": "2020-04-01T17:32:39Z", "author": {"login": "sudssf"}, "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestDataSourceOptions.java", "diffHunk": "@@ -201,6 +201,36 @@ public void testSplitOptionsOverridesTableProperties() throws IOException {\n     Assert.assertEquals(\"Spark partitions should match\", 2, resultDf.javaRDD().getNumPartitions());\n   }\n \n+  @Test\n+  public void testSplitOptionsOverridesTablePropertiesWithWriterLength() throws IOException {\n+    String tableLocation = temp.newFolder(\"iceberg-table\").toString();\n+\n+    HadoopTables tables = new HadoopTables(CONF);\n+    PartitionSpec spec = PartitionSpec.unpartitioned();\n+    Map<String, String> options = Maps.newHashMap();\n+    options.put(TableProperties.SPLIT_SIZE, String.valueOf(128L * 1024 * 1024)); // 128Mb\n+    tables.create(SCHEMA, spec, options, tableLocation);\n+\n+    List<SimpleRecord> expectedRecords = Lists.newArrayList(\n+        new SimpleRecord(1, \"a\"),\n+        new SimpleRecord(2, \"b\")\n+    );\n+    Dataset<Row> originalDf = spark.createDataFrame(expectedRecords, SimpleRecord.class);\n+    originalDf.select(\"id\", \"data\").write()\n+        .format(\"iceberg\")\n+        .mode(\"append\")\n+        .option(\"use-writer-length-as-file-size\", true)\n+        .save(tableLocation);\n+\n+    Dataset<Row> resultDf = spark.read()\n+        .format(\"iceberg\")\n+        .option(\"split-size\", String.valueOf(611 + 103)) // 611 bytes is the size of SimpleRecord(1,\"a\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7916d8fb9b9a421f597a6b95526ad1642ac542ab"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTgyMDc1OQ==", "bodyText": "Does this happen for all formats, or just one?", "url": "https://github.com/apache/iceberg/pull/882#discussion_r401820759", "createdAt": "2020-04-01T18:26:03Z", "author": {"login": "rdblue"}, "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestDataSourceOptions.java", "diffHunk": "@@ -201,6 +201,36 @@ public void testSplitOptionsOverridesTableProperties() throws IOException {\n     Assert.assertEquals(\"Spark partitions should match\", 2, resultDf.javaRDD().getNumPartitions());\n   }\n \n+  @Test\n+  public void testSplitOptionsOverridesTablePropertiesWithWriterLength() throws IOException {\n+    String tableLocation = temp.newFolder(\"iceberg-table\").toString();\n+\n+    HadoopTables tables = new HadoopTables(CONF);\n+    PartitionSpec spec = PartitionSpec.unpartitioned();\n+    Map<String, String> options = Maps.newHashMap();\n+    options.put(TableProperties.SPLIT_SIZE, String.valueOf(128L * 1024 * 1024)); // 128Mb\n+    tables.create(SCHEMA, spec, options, tableLocation);\n+\n+    List<SimpleRecord> expectedRecords = Lists.newArrayList(\n+        new SimpleRecord(1, \"a\"),\n+        new SimpleRecord(2, \"b\")\n+    );\n+    Dataset<Row> originalDf = spark.createDataFrame(expectedRecords, SimpleRecord.class);\n+    originalDf.select(\"id\", \"data\").write()\n+        .format(\"iceberg\")\n+        .mode(\"append\")\n+        .option(\"use-writer-length-as-file-size\", true)\n+        .save(tableLocation);\n+\n+    Dataset<Row> resultDf = spark.read()\n+        .format(\"iceberg\")\n+        .option(\"split-size\", String.valueOf(611 + 103)) // 611 bytes is the size of SimpleRecord(1,\"a\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTc4OTA0NQ=="}, "originalCommit": {"oid": "7916d8fb9b9a421f597a6b95526ad1642ac542ab"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTk1NzQ3NA==", "bodyText": "Do we know why this is happening? I would expect Parquet to return the correct size after close. We should find out what's going on.", "url": "https://github.com/apache/iceberg/pull/882#discussion_r401957474", "createdAt": "2020-04-01T22:56:09Z", "author": {"login": "rdblue"}, "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestDataSourceOptions.java", "diffHunk": "@@ -201,6 +201,36 @@ public void testSplitOptionsOverridesTableProperties() throws IOException {\n     Assert.assertEquals(\"Spark partitions should match\", 2, resultDf.javaRDD().getNumPartitions());\n   }\n \n+  @Test\n+  public void testSplitOptionsOverridesTablePropertiesWithWriterLength() throws IOException {\n+    String tableLocation = temp.newFolder(\"iceberg-table\").toString();\n+\n+    HadoopTables tables = new HadoopTables(CONF);\n+    PartitionSpec spec = PartitionSpec.unpartitioned();\n+    Map<String, String> options = Maps.newHashMap();\n+    options.put(TableProperties.SPLIT_SIZE, String.valueOf(128L * 1024 * 1024)); // 128Mb\n+    tables.create(SCHEMA, spec, options, tableLocation);\n+\n+    List<SimpleRecord> expectedRecords = Lists.newArrayList(\n+        new SimpleRecord(1, \"a\"),\n+        new SimpleRecord(2, \"b\")\n+    );\n+    Dataset<Row> originalDf = spark.createDataFrame(expectedRecords, SimpleRecord.class);\n+    originalDf.select(\"id\", \"data\").write()\n+        .format(\"iceberg\")\n+        .mode(\"append\")\n+        .option(\"use-writer-length-as-file-size\", true)\n+        .save(tableLocation);\n+\n+    Dataset<Row> resultDf = spark.read()\n+        .format(\"iceberg\")\n+        .option(\"split-size\", String.valueOf(611 + 103)) // 611 bytes is the size of SimpleRecord(1,\"a\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTc4OTA0NQ=="}, "originalCommit": {"oid": "7916d8fb9b9a421f597a6b95526ad1642ac542ab"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTk2MDU5MA==", "bodyText": "I think this happens only for parquet.\nhttps://github.com/apache/incubator-iceberg/blob/master/parquet/src/main/java/org/apache/iceberg/parquet/ParquetWriter.java#L142\nwriteStore seems to return non zero results for getBufferedSize after close.", "url": "https://github.com/apache/iceberg/pull/882#discussion_r401960590", "createdAt": "2020-04-01T23:04:51Z", "author": {"login": "sudssf"}, "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestDataSourceOptions.java", "diffHunk": "@@ -201,6 +201,36 @@ public void testSplitOptionsOverridesTableProperties() throws IOException {\n     Assert.assertEquals(\"Spark partitions should match\", 2, resultDf.javaRDD().getNumPartitions());\n   }\n \n+  @Test\n+  public void testSplitOptionsOverridesTablePropertiesWithWriterLength() throws IOException {\n+    String tableLocation = temp.newFolder(\"iceberg-table\").toString();\n+\n+    HadoopTables tables = new HadoopTables(CONF);\n+    PartitionSpec spec = PartitionSpec.unpartitioned();\n+    Map<String, String> options = Maps.newHashMap();\n+    options.put(TableProperties.SPLIT_SIZE, String.valueOf(128L * 1024 * 1024)); // 128Mb\n+    tables.create(SCHEMA, spec, options, tableLocation);\n+\n+    List<SimpleRecord> expectedRecords = Lists.newArrayList(\n+        new SimpleRecord(1, \"a\"),\n+        new SimpleRecord(2, \"b\")\n+    );\n+    Dataset<Row> originalDf = spark.createDataFrame(expectedRecords, SimpleRecord.class);\n+    originalDf.select(\"id\", \"data\").write()\n+        .format(\"iceberg\")\n+        .mode(\"append\")\n+        .option(\"use-writer-length-as-file-size\", true)\n+        .save(tableLocation);\n+\n+    Dataset<Row> resultDf = spark.read()\n+        .format(\"iceberg\")\n+        .option(\"split-size\", String.valueOf(611 + 103)) // 611 bytes is the size of SimpleRecord(1,\"a\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTc4OTA0NQ=="}, "originalCommit": {"oid": "7916d8fb9b9a421f597a6b95526ad1642ac542ab"}, "originalPosition": 27}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ5MTk3MTY0OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/source/Writer.java", "isResolved": true, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMVQxODoyNToyOVrOF_NK6g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMlQxODozMjoyMFrOF_4SWQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTgyMDM5NA==", "bodyText": "This shouldn't be a new option. Let's remove it.", "url": "https://github.com/apache/iceberg/pull/882#discussion_r401820394", "createdAt": "2020-04-01T18:25:29Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/Writer.java", "diffHunk": "@@ -114,6 +115,7 @@\n     long tableTargetFileSize = PropertyUtil.propertyAsLong(\n         table.properties(), WRITE_TARGET_FILE_SIZE_BYTES, WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n     this.targetFileSize = options.getLong(\"target-file-size-bytes\", tableTargetFileSize);\n+    this.useWriterLengthAsFileSize = options.getBoolean(\"use-writer-length-as-file-size\", false);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7916d8fb9b9a421f597a6b95526ad1642ac542ab"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTkzOTk3Ng==", "bodyText": "@rdblue  do you suggestion always using writer length, instead of calling dataFileBuilder.withEncryptedOutputFile(currentFile) which internally calls getStatus for s3a?", "url": "https://github.com/apache/iceberg/pull/882#discussion_r401939976", "createdAt": "2020-04-01T22:11:03Z", "author": {"login": "sudssf"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/Writer.java", "diffHunk": "@@ -114,6 +115,7 @@\n     long tableTargetFileSize = PropertyUtil.propertyAsLong(\n         table.properties(), WRITE_TARGET_FILE_SIZE_BYTES, WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n     this.targetFileSize = options.getLong(\"target-file-size-bytes\", tableTargetFileSize);\n+    this.useWriterLengthAsFileSize = options.getBoolean(\"use-writer-length-as-file-size\", false);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTgyMDM5NA=="}, "originalCommit": {"oid": "7916d8fb9b9a421f597a6b95526ad1642ac542ab"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTk1NzMwOA==", "bodyText": "Yes, I think it should always use the length reported by the writer.", "url": "https://github.com/apache/iceberg/pull/882#discussion_r401957308", "createdAt": "2020-04-01T22:55:45Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/Writer.java", "diffHunk": "@@ -114,6 +115,7 @@\n     long tableTargetFileSize = PropertyUtil.propertyAsLong(\n         table.properties(), WRITE_TARGET_FILE_SIZE_BYTES, WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n     this.targetFileSize = options.getLong(\"target-file-size-bytes\", tableTargetFileSize);\n+    this.useWriterLengthAsFileSize = options.getBoolean(\"use-writer-length-as-file-size\", false);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTgyMDM5NA=="}, "originalCommit": {"oid": "7916d8fb9b9a421f597a6b95526ad1642ac542ab"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTk2MDc0OQ==", "bodyText": "sounds good I will update PR.", "url": "https://github.com/apache/iceberg/pull/882#discussion_r401960749", "createdAt": "2020-04-01T23:05:21Z", "author": {"login": "sudssf"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/Writer.java", "diffHunk": "@@ -114,6 +115,7 @@\n     long tableTargetFileSize = PropertyUtil.propertyAsLong(\n         table.properties(), WRITE_TARGET_FILE_SIZE_BYTES, WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n     this.targetFileSize = options.getLong(\"target-file-size-bytes\", tableTargetFileSize);\n+    this.useWriterLengthAsFileSize = options.getBoolean(\"use-writer-length-as-file-size\", false);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTgyMDM5NA=="}, "originalCommit": {"oid": "7916d8fb9b9a421f597a6b95526ad1642ac542ab"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjUyNjgwOQ==", "bodyText": "@rdblue please take a look at updated PR at your convenience.", "url": "https://github.com/apache/iceberg/pull/882#discussion_r402526809", "createdAt": "2020-04-02T18:32:20Z", "author": {"login": "sudssf"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/Writer.java", "diffHunk": "@@ -114,6 +115,7 @@\n     long tableTargetFileSize = PropertyUtil.propertyAsLong(\n         table.properties(), WRITE_TARGET_FILE_SIZE_BYTES, WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n     this.targetFileSize = options.getLong(\"target-file-size-bytes\", tableTargetFileSize);\n+    this.useWriterLengthAsFileSize = options.getBoolean(\"use-writer-length-as-file-size\", false);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTgyMDM5NA=="}, "originalCommit": {"oid": "7916d8fb9b9a421f597a6b95526ad1642ac542ab"}, "originalPosition": 12}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2871, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}