{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTQxNDc5ODI1", "number": 1947, "title": "Spark MERGE INTO Support (copy-on-write implementation)", "bodyText": "Adds WIP support of MERGE INTO for spark leveraging the work done for DELETE by Anton.\nThis PR implements by doing copy-on-write.\n\nPlan:\n== Optimized Logical Plan ==\nReplaceData RelationV2[key1#50, value1#51] file:///..., IcebergWrite(table=file:///..., format=PARQUET)\n+- MergeInto org.apache.spark.sql.catalyst.plans.logical.MergeIntoProcessor@e1a150c, RelationV2[key1#50, value1#51] file:///...\n   +- Join FullOuter, (key1#50 = key2#65)\n      :- Project [key2#65, value2#66, true AS _source_row_present_#138]\n      :  +- RelationV2[key2#65, value2#66] file:///...\n      +- Project [key1#50, value1#51, true AS _target_row_present_#139]\n         +- DynamicFileFilter\n            :- RelationV2[key1#50, value1#51] file:///...\n            +- Aggregate [_file_name_#137], [_file_name_#137]\n               +- Project [_file_name_#137]\n                  +- Join Inner, (key1#50 = key2#65)\n                     :- Filter isnotnull(key2#65)\n                     :  +- RelationV2[key2#65] file:///...\n                     +- Project [key1#50, input_file_name() AS _file_name_#137]\n                        +- Filter isnotnull(key1#50)\n                           +- RelationV2[key1#50] file:///...", "createdAt": "2020-12-16T22:36:08Z", "url": "https://github.com/apache/iceberg/pull/1947", "merged": true, "mergeCommit": {"oid": "9cc317d85a59d68fe3445023b26add85491226fb"}, "closed": true, "closedAt": "2021-01-19T06:08:18Z", "author": {"login": "dilipbiswal"}, "timelineItems": {"totalCount": 70, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdm_u9SAFqTU1NDM5ODEwNg==", "endCursor": "Y3Vyc29yOnYyOpPPAAABdxhz8uAH2gAyNTQxNDc5ODI1OjlmYWRjMWQ4NWU2MGQ3MGY5MWIxODBiZDUzNTI5MDM5MTZjYzEzZmI=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU0Mzk4MTA2", "url": "https://github.com/apache/iceberg/pull/1947#pullrequestreview-554398106", "createdAt": "2020-12-17T09:01:40Z", "commit": {"oid": "9f021994411ac1701802ed4adcc8db4af1832d8b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QwOTowMTo0MFrOIHrSjA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QwOTowMTo0MFrOIHrSjA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDkyMDIwNA==", "bodyText": "Do we need this node? It seems we rewrite the operation into ReplaceData, no?", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r544920204", "createdAt": "2020-12-17T09:01:40Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/MergeIntoExec.scala", "diffHunk": "@@ -0,0 +1,38 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.catalyst.plans.logical.MergeIntoProcessor\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+\n+case class MergeIntoExec(mergeIntoProcessor: MergeIntoProcessor,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9f021994411ac1701802ed4adcc8db4af1832d8b"}, "originalPosition": 28}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU0NDI2MDE1", "url": "https://github.com/apache/iceberg/pull/1947#pullrequestreview-554426015", "createdAt": "2020-12-17T09:35:20Z", "commit": {"oid": "9f021994411ac1701802ed4adcc8db4af1832d8b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QwOTozNToyMFrOIHsqQQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QwOTozNToyMFrOIHsqQQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDk0MjY1Nw==", "bodyText": "nit: these vals can be private", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r544942657", "createdAt": "2020-12-17T09:35:20Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteMergeInto.scala", "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import java.util.UUID\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{AnalysisException, SparkSession}\n+import org.apache.spark.sql.catalyst.encoders.RowEncoder\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.{FullOuter, Inner, JoinType}\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.connector.catalog.Table\n+import org.apache.spark.sql.connector.iceberg.read.SupportsFileFilter\n+import org.apache.spark.sql.connector.iceberg.write.MergeBuilder\n+import org.apache.spark.sql.connector.write.{LogicalWriteInfo, LogicalWriteInfoImpl}\n+import org.apache.spark.sql.execution.datasources.v2.{DataSourceV2Relation, DataSourceV2ScanRelation}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types._\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+object RewriteMergeInto extends Rule[LogicalPlan]\n+  with PredicateHelper\n+  with Logging  {\n+  val ROW_ID_COL = \"_row_id_\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9f021994411ac1701802ed4adcc8db4af1832d8b"}, "originalPosition": 43}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU1MTE0Nzc4", "url": "https://github.com/apache/iceberg/pull/1947#pullrequestreview-555114778", "createdAt": "2020-12-18T01:36:04Z", "commit": {"oid": "9f021994411ac1701802ed4adcc8db4af1832d8b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQwMTozNjowNFrOIIPqWA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQwMTozNjowNFrOIIPqWA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTUxNjEyMA==", "bodyText": "It would be helpful to group some of these plan nodes into sections, like in RewriteDelete where methods like buildFileFilterPlan and buildScanPlan give good context for what plans are being constructed and how they will be used.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r545516120", "createdAt": "2020-12-18T01:36:04Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteMergeInto.scala", "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import java.util.UUID\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{AnalysisException, SparkSession}\n+import org.apache.spark.sql.catalyst.encoders.RowEncoder\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.{FullOuter, Inner, JoinType}\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.connector.catalog.Table\n+import org.apache.spark.sql.connector.iceberg.read.SupportsFileFilter\n+import org.apache.spark.sql.connector.iceberg.write.MergeBuilder\n+import org.apache.spark.sql.connector.write.{LogicalWriteInfo, LogicalWriteInfoImpl}\n+import org.apache.spark.sql.execution.datasources.v2.{DataSourceV2Relation, DataSourceV2ScanRelation}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types._\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+object RewriteMergeInto extends Rule[LogicalPlan]\n+  with PredicateHelper\n+  with Logging  {\n+  val ROW_ID_COL = \"_row_id_\"\n+  val FILE_NAME_COL = \"_file_name_\"\n+  val SOURCE_ROW_PRESENT_COL = \"_source_row_present_\"\n+  val TARGET_ROW_PRESENT_COL = \"_target_row_present_\"\n+\n+  import org.apache.spark.sql.execution.datasources.v2.ExtendedDataSourceV2Implicits._\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = {\n+    plan resolveOperators {\n+      // rewrite all operations that require reading the table to delete records\n+      case MergeIntoTable(target: DataSourceV2Relation,\n+                          source: LogicalPlan, cond, actions, notActions) =>\n+        // Find the files in target that matches the JOIN condition from source.\n+        val targetOutputCols = target.output\n+        val newProjectCols = target.output ++ Seq(Alias(InputFileName(), FILE_NAME_COL)())\n+        val newTargetTable = Project(newProjectCols, target)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9f021994411ac1701802ed4adcc8db4af1832d8b"}, "originalPosition": 58}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU1MTE2MDYx", "url": "https://github.com/apache/iceberg/pull/1947#pullrequestreview-555116061", "createdAt": "2020-12-18T01:40:03Z", "commit": {"oid": "9f021994411ac1701802ed4adcc8db4af1832d8b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQwMTo0MDowM1rOIIPvWA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQwMTo0MDowM1rOIIPvWA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTUxNzQwMA==", "bodyText": "As Anton noted, there needs to be validation that the assignments here (for both inserts and updates) match up with the targetOutputCols.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r545517400", "createdAt": "2020-12-18T01:40:03Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteMergeInto.scala", "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import java.util.UUID\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{AnalysisException, SparkSession}\n+import org.apache.spark.sql.catalyst.encoders.RowEncoder\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.{FullOuter, Inner, JoinType}\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.connector.catalog.Table\n+import org.apache.spark.sql.connector.iceberg.read.SupportsFileFilter\n+import org.apache.spark.sql.connector.iceberg.write.MergeBuilder\n+import org.apache.spark.sql.connector.write.{LogicalWriteInfo, LogicalWriteInfoImpl}\n+import org.apache.spark.sql.execution.datasources.v2.{DataSourceV2Relation, DataSourceV2ScanRelation}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types._\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+object RewriteMergeInto extends Rule[LogicalPlan]\n+  with PredicateHelper\n+  with Logging  {\n+  val ROW_ID_COL = \"_row_id_\"\n+  val FILE_NAME_COL = \"_file_name_\"\n+  val SOURCE_ROW_PRESENT_COL = \"_source_row_present_\"\n+  val TARGET_ROW_PRESENT_COL = \"_target_row_present_\"\n+\n+  import org.apache.spark.sql.execution.datasources.v2.ExtendedDataSourceV2Implicits._\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = {\n+    plan resolveOperators {\n+      // rewrite all operations that require reading the table to delete records\n+      case MergeIntoTable(target: DataSourceV2Relation,\n+                          source: LogicalPlan, cond, actions, notActions) =>\n+        // Find the files in target that matches the JOIN condition from source.\n+        val targetOutputCols = target.output\n+        val newProjectCols = target.output ++ Seq(Alias(InputFileName(), FILE_NAME_COL)())\n+        val newTargetTable = Project(newProjectCols, target)\n+        val prunedTargetPlan = Join(source, newTargetTable, Inner, Some(cond), JoinHint.NONE)\n+\n+        val writeInfo = newWriteInfo(target.schema)\n+        val mergeBuilder = target.table.asMergeable.newMergeBuilder(\"delete\", writeInfo)\n+        val targetTableScan =  buildScanPlan(target.table, target.output, mergeBuilder, prunedTargetPlan)\n+        val sourceTableProj = source.output ++ Seq(Alias(lit(true).expr, SOURCE_ROW_PRESENT_COL)())\n+        val targetTableProj = target.output ++ Seq(Alias(lit(true).expr, TARGET_ROW_PRESENT_COL)())\n+        val newTargetTableScan = Project(targetTableProj, targetTableScan)\n+        val newSourceTableScan = Project(sourceTableProj, source)\n+        val joinPlan = Join(newSourceTableScan, newTargetTableScan, FullOuter, Some(cond), JoinHint.NONE)\n+\n+        val mergeIntoProcessor = new MergeIntoProcessor(\n+          isSourceRowNotPresent = resolveExprs(Seq(col(SOURCE_ROW_PRESENT_COL).isNull.expr), joinPlan).head,\n+          isTargetRowNotPresent = resolveExprs(Seq(col(TARGET_ROW_PRESENT_COL).isNull.expr), joinPlan).head,\n+          matchedConditions = actions.map(resolveClauseCondition(_, joinPlan)),\n+          matchedOutputs = actions.map(actionOutput(_, targetOutputCols, joinPlan)),\n+          notMatchedConditions = notActions.map(resolveClauseCondition(_, joinPlan)),\n+          notMatchedOutputs = notActions.map(actionOutput(_, targetOutputCols, joinPlan)),\n+          targetOutput = resolveExprs(targetOutputCols :+ Literal(false), joinPlan),\n+          joinedAttributes = joinPlan.output\n+        )\n+\n+        val mergePlan = MergeInto(mergeIntoProcessor, target, joinPlan)\n+        val batchWrite = mergeBuilder.asWriteBuilder.buildForBatch()\n+        ReplaceData(target, batchWrite, mergePlan)\n+    }\n+  }\n+\n+  private def buildScanPlan(\n+      table: Table,\n+      output: Seq[AttributeReference],\n+      mergeBuilder: MergeBuilder,\n+      prunedTargetPlan: LogicalPlan): LogicalPlan = {\n+\n+    val scanBuilder = mergeBuilder.asScanBuilder\n+    val scan = scanBuilder.build()\n+    val scanRelation = DataSourceV2ScanRelation(table, scan, output)\n+\n+    scan match {\n+      case _: SupportsFileFilter =>\n+        val matchingFilePlan = buildFileFilterPlan(prunedTargetPlan)\n+        val dynamicFileFilter = DynamicFileFilter(scanRelation, matchingFilePlan)\n+        dynamicFileFilter\n+      case _ =>\n+        scanRelation\n+    }\n+  }\n+\n+  private def newWriteInfo(schema: StructType): LogicalWriteInfo = {\n+    val uuid = UUID.randomUUID()\n+    LogicalWriteInfoImpl(queryId = uuid.toString, schema, CaseInsensitiveStringMap.empty)\n+  }\n+\n+  private def buildFileFilterPlan(prunedTargetPlan: LogicalPlan): LogicalPlan = {\n+    val fileAttr = findOutputAttr(prunedTargetPlan, FILE_NAME_COL)\n+    Aggregate(Seq(fileAttr), Seq(fileAttr), prunedTargetPlan)\n+  }\n+\n+  private def findOutputAttr(plan: LogicalPlan, attrName: String): Attribute = {\n+    val resolver = SQLConf.get.resolver\n+    plan.output.find(attr => resolver(attr.name, attrName)).getOrElse {\n+      throw new AnalysisException(s\"Cannot find $attrName in ${plan.output}\")\n+    }\n+  }\n+\n+  private def resolveExprs(exprs: Seq[Expression], plan: LogicalPlan): Seq[Expression] = {\n+    val spark = SparkSession.active\n+    exprs.map { expr => resolveExpressionInternal(spark, expr, plan) }\n+  }\n+\n+  def getTargetOutputCols(target: DataSourceV2Relation): Seq[NamedExpression] = {\n+    target.schema.map { col =>\n+      target.output.find(attr => SQLConf.get.resolver(attr.name, col.name)).getOrElse {\n+        Alias(Literal(null, col.dataType), col.name)()\n+      }\n+    }\n+  }\n+\n+  def actionOutput(clause: MergeAction,\n+                   targetOutputCols: Seq[Expression],\n+                   plan: LogicalPlan): Seq[Expression] = {\n+    val exprs = clause match {\n+      case u: UpdateAction =>\n+        u.assignments.map(_.value) :+ Literal(false)\n+      case _: DeleteAction =>\n+        targetOutputCols :+ Literal(true)\n+      case i: InsertAction =>\n+        i.assignments.map(_.value) :+ Literal(false)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9f021994411ac1701802ed4adcc8db4af1832d8b"}, "originalPosition": 146}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU1MTE5MDgw", "url": "https://github.com/apache/iceberg/pull/1947#pullrequestreview-555119080", "createdAt": "2020-12-18T01:48:56Z", "commit": {"oid": "9f021994411ac1701802ed4adcc8db4af1832d8b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQwMTo0ODo1NlrOIIP7PA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQwMTo0ODo1NlrOIIP7PA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTUyMDQ0NA==", "bodyText": "This is essentially a physical plan node that is linked into both the physical plan and logical plan. I think it should be a normal physical plan node that is created in a strategy, just like other plans.\nThe main issue with the way this PR currently works is that it doesn't delegate enough to the rest of the Spark planner. All of the analysis is done during rewrite in the optimizer, for example. I think that this should be broken up into analysis rules to validate and update the MergeInto plan, the rewrite rule to build the optimizations and join, and a strategy to convert the logical plan into a MergeIntoExec. I think this should also have a validation rule that checks each action to ensure that the expressions for that action are correctly resolved.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r545520444", "createdAt": "2020-12-18T01:48:56Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/MergeInto.scala", "diffHunk": "@@ -0,0 +1,101 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.plans.logical\n+\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation\n+import org.apache.spark.sql.functions.col\n+\n+case class MergeInto(mergeIntoProcessor: MergeIntoProcessor,\n+                     targetRelation: DataSourceV2Relation,\n+                     child: LogicalPlan) extends UnaryNode {\n+  override def output: Seq[Attribute] = targetRelation.output\n+}\n+\n+class MergeIntoProcessor(isSourceRowNotPresent: Expression,\n+                         isTargetRowNotPresent: Expression,\n+                         matchedConditions: Seq[Expression],\n+                         matchedOutputs: Seq[Seq[Expression]],\n+                         notMatchedConditions: Seq[Expression],\n+                         notMatchedOutputs: Seq[Seq[Expression]],\n+                         targetOutput: Seq[Expression],\n+                         joinedAttributes: Seq[Attribute]) extends Serializable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9f021994411ac1701802ed4adcc8db4af1832d8b"}, "originalPosition": 43}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "9f021994411ac1701802ed4adcc8db4af1832d8b", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/9f021994411ac1701802ed4adcc8db4af1832d8b", "committedDate": "2020-12-16T22:31:31Z", "message": "Spark MERGE INTO Support (copy-on-write implementation"}, "afterCommit": {"oid": "76ea61f0321a7b4de250e448722f7893bd52296f", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/76ea61f0321a7b4de250e448722f7893bd52296f", "committedDate": "2021-01-04T07:36:37Z", "message": "Code review + base infrastructure"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "76ea61f0321a7b4de250e448722f7893bd52296f", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/76ea61f0321a7b4de250e448722f7893bd52296f", "committedDate": "2021-01-04T07:36:37Z", "message": "Code review + base infrastructure"}, "afterCommit": {"oid": "13889694c2fa750aae742ff207061ce40bf66504", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/13889694c2fa750aae742ff207061ce40bf66504", "committedDate": "2021-01-05T06:48:22Z", "message": "Code review + base infrastructure"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "13889694c2fa750aae742ff207061ce40bf66504", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/13889694c2fa750aae742ff207061ce40bf66504", "committedDate": "2021-01-05T06:48:22Z", "message": "Code review + base infrastructure"}, "afterCommit": {"oid": "148744edd876f823e18bf4f9ae5f2a58f4c55f65", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/148744edd876f823e18bf4f9ae5f2a58f4c55f65", "committedDate": "2021-01-11T09:00:38Z", "message": "Code review + base infrastructure"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "148744edd876f823e18bf4f9ae5f2a58f4c55f65", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/148744edd876f823e18bf4f9ae5f2a58f4c55f65", "committedDate": "2021-01-11T09:00:38Z", "message": "Code review + base infrastructure"}, "afterCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/946bbded4eb82639a9e3db7fb90172dab827508c", "committedDate": "2021-01-12T05:34:17Z", "message": "Code review + base infrastructure"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTY5NTQzNjU0", "url": "https://github.com/apache/iceberg/pull/1947#pullrequestreview-569543654", "createdAt": "2021-01-15T19:06:04Z", "commit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQxOTowNjowNFrOIUpqYw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQxOTowNjowNFrOIUpqYw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODUyNTAyNw==", "bodyText": "Shall we make these variables private?", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r558525027", "createdAt": "2021-01-15T19:06:04Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteMergeInto.scala", "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.{FullOuter, Inner}\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.catalyst.utils.PlanHelper\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.internal.SQLConf\n+\n+object RewriteMergeInto extends Rule[LogicalPlan] with PlanHelper with Logging  {\n+  val ROW_FROM_SOURCE = \"_row_from_source_\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 33}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTY5NTQ0MjQ2", "url": "https://github.com/apache/iceberg/pull/1947#pullrequestreview-569544246", "createdAt": "2021-01-15T19:07:02Z", "commit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQxOTowNzowMlrOIUptPw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQxOTowNzowMlrOIUptPw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODUyNTc1OQ==", "bodyText": "Does the comment apply? It looks like it is valid for DELETE but not really for MERGE.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r558525759", "createdAt": "2021-01-15T19:07:02Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteMergeInto.scala", "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.{FullOuter, Inner}\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.catalyst.utils.PlanHelper\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.internal.SQLConf\n+\n+object RewriteMergeInto extends Rule[LogicalPlan] with PlanHelper with Logging  {\n+  val ROW_FROM_SOURCE = \"_row_from_source_\"\n+  val ROW_FROM_TARGET = \"_row_from_target_\"\n+\n+  import org.apache.spark.sql.execution.datasources.v2.ExtendedDataSourceV2Implicits._\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = {\n+    plan resolveOperators {\n+      // rewrite all operations that require reading the table to delete records", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 40}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTY5NTQ3MDEx", "url": "https://github.com/apache/iceberg/pull/1947#pullrequestreview-569547011", "createdAt": "2021-01-15T19:11:22Z", "commit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQxOToxMToyM1rOIUp8FA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQxOToxMToyM1rOIUp8FA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODUyOTU1Ng==", "bodyText": "nit: I think matchedActions and notMatchedActions would be better names here.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r558529556", "createdAt": "2021-01-15T19:11:23Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteMergeInto.scala", "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.{FullOuter, Inner}\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.catalyst.utils.PlanHelper\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.internal.SQLConf\n+\n+object RewriteMergeInto extends Rule[LogicalPlan] with PlanHelper with Logging  {\n+  val ROW_FROM_SOURCE = \"_row_from_source_\"\n+  val ROW_FROM_TARGET = \"_row_from_target_\"\n+\n+  import org.apache.spark.sql.execution.datasources.v2.ExtendedDataSourceV2Implicits._\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = {\n+    plan resolveOperators {\n+      // rewrite all operations that require reading the table to delete records\n+      case MergeIntoTable(target: DataSourceV2Relation,\n+                          source: LogicalPlan, cond, actions, notActions) =>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 42}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTY5NTU3MTkz", "url": "https://github.com/apache/iceberg/pull/1947#pullrequestreview-569557193", "createdAt": "2021-01-15T19:24:15Z", "commit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQxOToyNDoxNVrOIUqpOg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQxOToyNDoxNVrOIUqpOg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODU0MTExNA==", "bodyText": "@dilipbiswal, do we have to add MERGE operations to PullupCorrelatedPredicatesInRowLevelOperations? Could you test the current implementation with subqueries inside the merge as well as matched/not matched conditions?", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r558541114", "createdAt": "2021-01-15T19:24:15Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/iceberg/spark/extensions/IcebergSparkSessionExtensions.scala", "diffHunk": "@@ -43,6 +43,7 @@ class IcebergSparkSessionExtensions extends (SparkSessionExtensions => Unit) {\n     // TODO: PullupCorrelatedPredicates should handle row-level operations\n     extensions.injectOptimizerRule { _ => PullupCorrelatedPredicatesInRowLevelOperations }\n     extensions.injectOptimizerRule { _ => RewriteDelete }\n+    extensions.injectOptimizerRule { _ => RewriteMergeInto }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 13}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTY5NTYxNzM0", "url": "https://github.com/apache/iceberg/pull/1947#pullrequestreview-569561734", "createdAt": "2021-01-15T19:31:19Z", "commit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQxOTozMToxOVrOIUrDMg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQxOTozMToxOVrOIUrDMg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODU0Nzc2Mg==", "bodyText": "When I was working on UPDATE, I also created a parent trait for row-level ops.\nWhat about a more specific name, like RewriteRowLevelOperation or similar?", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r558547762", "createdAt": "2021-01-15T19:31:19Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/utils/PlanHelper.scala", "diffHunk": "@@ -0,0 +1,87 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.spark.sql.catalyst.utils\n+\n+import java.util.UUID\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, AttributeReference, PredicateHelper}\n+import org.apache.spark.sql.catalyst.plans.logical.{Aggregate, DynamicFileFilter, LogicalPlan}\n+import org.apache.spark.sql.connector.catalog.Table\n+import org.apache.spark.sql.connector.iceberg.read.SupportsFileFilter\n+import org.apache.spark.sql.connector.iceberg.write.MergeBuilder\n+import org.apache.spark.sql.connector.write.{LogicalWriteInfo, LogicalWriteInfoImpl}\n+import org.apache.spark.sql.execution.datasources.v2.{DataSourceV2ScanRelation}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+trait PlanHelper extends PredicateHelper {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 34}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTY5NTYxODU0", "url": "https://github.com/apache/iceberg/pull/1947#pullrequestreview-569561854", "createdAt": "2021-01-15T19:31:30Z", "commit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQxOTozMTozMFrOIUrDxw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQxOTozMTozMFrOIUrDxw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODU0NzkxMQ==", "bodyText": "Shall we make these protected?", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r558547911", "createdAt": "2021-01-15T19:31:30Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/utils/PlanHelper.scala", "diffHunk": "@@ -0,0 +1,87 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.spark.sql.catalyst.utils\n+\n+import java.util.UUID\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, AttributeReference, PredicateHelper}\n+import org.apache.spark.sql.catalyst.plans.logical.{Aggregate, DynamicFileFilter, LogicalPlan}\n+import org.apache.spark.sql.connector.catalog.Table\n+import org.apache.spark.sql.connector.iceberg.read.SupportsFileFilter\n+import org.apache.spark.sql.connector.iceberg.write.MergeBuilder\n+import org.apache.spark.sql.connector.write.{LogicalWriteInfo, LogicalWriteInfoImpl}\n+import org.apache.spark.sql.execution.datasources.v2.{DataSourceV2ScanRelation}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+trait PlanHelper extends PredicateHelper {\n+  val FILE_NAME_COL = \"_file\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 35}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTY5NTYyMjkw", "url": "https://github.com/apache/iceberg/pull/1947#pullrequestreview-569562290", "createdAt": "2021-01-15T19:32:10Z", "commit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQxOTozMjoxMFrOIUrF-A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQxOTozMjoxMFrOIUrF-A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODU0ODQ3Mg==", "bodyText": "I think we better make methods inside this trait protected, not public.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r558548472", "createdAt": "2021-01-15T19:32:10Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/utils/PlanHelper.scala", "diffHunk": "@@ -0,0 +1,87 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.spark.sql.catalyst.utils\n+\n+import java.util.UUID\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, AttributeReference, PredicateHelper}\n+import org.apache.spark.sql.catalyst.plans.logical.{Aggregate, DynamicFileFilter, LogicalPlan}\n+import org.apache.spark.sql.connector.catalog.Table\n+import org.apache.spark.sql.connector.iceberg.read.SupportsFileFilter\n+import org.apache.spark.sql.connector.iceberg.write.MergeBuilder\n+import org.apache.spark.sql.connector.write.{LogicalWriteInfo, LogicalWriteInfoImpl}\n+import org.apache.spark.sql.execution.datasources.v2.{DataSourceV2ScanRelation}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+trait PlanHelper extends PredicateHelper {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 34}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTY5NTYyOTY1", "url": "https://github.com/apache/iceberg/pull/1947#pullrequestreview-569562965", "createdAt": "2021-01-15T19:33:16Z", "commit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQxOTozMzoxNlrOIUrJBw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQxOTozMzoxNlrOIUrJBw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODU0OTI1NQ==", "bodyText": "This does not do predicate push down as we have for DELETE.\nCan we take the implementation from RewriteDelete?", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r558549255", "createdAt": "2021-01-15T19:33:16Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/utils/PlanHelper.scala", "diffHunk": "@@ -0,0 +1,87 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.spark.sql.catalyst.utils\n+\n+import java.util.UUID\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, AttributeReference, PredicateHelper}\n+import org.apache.spark.sql.catalyst.plans.logical.{Aggregate, DynamicFileFilter, LogicalPlan}\n+import org.apache.spark.sql.connector.catalog.Table\n+import org.apache.spark.sql.connector.iceberg.read.SupportsFileFilter\n+import org.apache.spark.sql.connector.iceberg.write.MergeBuilder\n+import org.apache.spark.sql.connector.write.{LogicalWriteInfo, LogicalWriteInfoImpl}\n+import org.apache.spark.sql.execution.datasources.v2.{DataSourceV2ScanRelation}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+trait PlanHelper extends PredicateHelper {\n+  val FILE_NAME_COL = \"_file\"\n+  val ROW_POS_COL = \"_pos\"\n+\n+  def buildScanPlan(table: Table,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 38}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTY5NTY2NTg3", "url": "https://github.com/apache/iceberg/pull/1947#pullrequestreview-569566587", "createdAt": "2021-01-15T19:39:11Z", "commit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQxOTozOToxMlrOIUrULg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQxOTozOToxMlrOIUrULg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODU1MjExMA==", "bodyText": "We should probably pass it from the session like in AlignMergeIntoTable.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r558552110", "createdAt": "2021-01-15T19:39:12Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/utils/PlanHelper.scala", "diffHunk": "@@ -0,0 +1,87 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.spark.sql.catalyst.utils\n+\n+import java.util.UUID\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, AttributeReference, PredicateHelper}\n+import org.apache.spark.sql.catalyst.plans.logical.{Aggregate, DynamicFileFilter, LogicalPlan}\n+import org.apache.spark.sql.connector.catalog.Table\n+import org.apache.spark.sql.connector.iceberg.read.SupportsFileFilter\n+import org.apache.spark.sql.connector.iceberg.write.MergeBuilder\n+import org.apache.spark.sql.connector.write.{LogicalWriteInfo, LogicalWriteInfoImpl}\n+import org.apache.spark.sql.execution.datasources.v2.{DataSourceV2ScanRelation}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+trait PlanHelper extends PredicateHelper {\n+  val FILE_NAME_COL = \"_file\"\n+  val ROW_POS_COL = \"_pos\"\n+\n+  def buildScanPlan(table: Table,\n+                    output: Seq[AttributeReference],\n+                    mergeBuilder: MergeBuilder,\n+                    prunedTargetPlan: LogicalPlan): LogicalPlan = {\n+\n+    val scanBuilder = mergeBuilder.asScanBuilder\n+    val scan = scanBuilder.build()\n+    val scanRelation = DataSourceV2ScanRelation(table, scan, toOutputAttrs(scan.readSchema(), output))\n+\n+    scan match {\n+      case filterable: SupportsFileFilter =>\n+        val matchingFilePlan = buildFileFilterPlan(prunedTargetPlan)\n+        val dynamicFileFilter = DynamicFileFilter(scanRelation, matchingFilePlan, filterable)\n+        dynamicFileFilter\n+      case _ =>\n+        scanRelation\n+    }\n+  }\n+\n+  private def buildFileFilterPlan(prunedTargetPlan: LogicalPlan): LogicalPlan = {\n+    val fileAttr = findOutputAttr(prunedTargetPlan, FILE_NAME_COL)\n+    Aggregate(Seq(fileAttr), Seq(fileAttr), prunedTargetPlan)\n+  }\n+\n+  def findOutputAttr(plan: LogicalPlan, attrName: String): Attribute = {\n+    val resolver = SQLConf.get.resolver", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 63}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTY5NTg0MDk3", "url": "https://github.com/apache/iceberg/pull/1947#pullrequestreview-569584097", "createdAt": "2021-01-15T20:07:09Z", "commit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQyMDowNzowOVrOIUsLUg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQyMDowNzowOVrOIUsLUg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODU2NjIyNg==", "bodyText": "@dilipbiswal, could you move these tests to TestMerge that was introduced recently?", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r558566226", "createdAt": "2021-01-15T20:07:09Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeIntoTable.java", "diffHunk": "@@ -0,0 +1,267 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.PARQUET_VECTORIZATION_ENABLED;\n+\n+public class TestMergeIntoTable extends SparkRowLevelOperationsTestBase {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 40}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTY5NjY3Mzk1", "url": "https://github.com/apache/iceberg/pull/1947#pullrequestreview-569667395", "createdAt": "2021-01-15T22:14:56Z", "commit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQyMjoxNDo1NlrOIUvgbg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQyMjoxNDo1NlrOIUvgbg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODYyMDc4Mg==", "bodyText": "This line should use merge, instead of delete now. It should be supported.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r558620782", "createdAt": "2021-01-15T22:14:56Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteMergeInto.scala", "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.{FullOuter, Inner}\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.catalyst.utils.PlanHelper\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.internal.SQLConf\n+\n+object RewriteMergeInto extends Rule[LogicalPlan] with PlanHelper with Logging  {\n+  val ROW_FROM_SOURCE = \"_row_from_source_\"\n+  val ROW_FROM_TARGET = \"_row_from_target_\"\n+\n+  import org.apache.spark.sql.execution.datasources.v2.ExtendedDataSourceV2Implicits._\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = {\n+    plan resolveOperators {\n+      // rewrite all operations that require reading the table to delete records\n+      case MergeIntoTable(target: DataSourceV2Relation,\n+                          source: LogicalPlan, cond, actions, notActions) =>\n+        val targetOutputCols = target.output\n+        val newProjectCols = target.output ++ Seq(Alias(InputFileName(), FILE_NAME_COL)())\n+        val newTargetTable = Project(newProjectCols, target)\n+\n+        // Construct the plan to prune target based on join condition between source and\n+        // target.\n+        val prunedTargetPlan = Join(source, newTargetTable, Inner, Some(cond), JoinHint.NONE)\n+        val writeInfo = newWriteInfo(target.schema)\n+        val mergeBuilder = target.table.asMergeable.newMergeBuilder(\"delete\", writeInfo)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 51}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTY5NjkwMDYz", "url": "https://github.com/apache/iceberg/pull/1947#pullrequestreview-569690063", "createdAt": "2021-01-15T22:29:55Z", "commit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQyMjoyOTo1NVrOIUv0LQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQyMjoyOTo1NVrOIUv0LQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODYyNTgzNw==", "bodyText": "Shall we make the helper methods private?", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r558625837", "createdAt": "2021-01-15T22:29:55Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteMergeInto.scala", "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.{FullOuter, Inner}\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.catalyst.utils.PlanHelper\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.internal.SQLConf\n+\n+object RewriteMergeInto extends Rule[LogicalPlan] with PlanHelper with Logging  {\n+  val ROW_FROM_SOURCE = \"_row_from_source_\"\n+  val ROW_FROM_TARGET = \"_row_from_target_\"\n+\n+  import org.apache.spark.sql.execution.datasources.v2.ExtendedDataSourceV2Implicits._\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = {\n+    plan resolveOperators {\n+      // rewrite all operations that require reading the table to delete records\n+      case MergeIntoTable(target: DataSourceV2Relation,\n+                          source: LogicalPlan, cond, actions, notActions) =>\n+        val targetOutputCols = target.output\n+        val newProjectCols = target.output ++ Seq(Alias(InputFileName(), FILE_NAME_COL)())\n+        val newTargetTable = Project(newProjectCols, target)\n+\n+        // Construct the plan to prune target based on join condition between source and\n+        // target.\n+        val prunedTargetPlan = Join(source, newTargetTable, Inner, Some(cond), JoinHint.NONE)\n+        val writeInfo = newWriteInfo(target.schema)\n+        val mergeBuilder = target.table.asMergeable.newMergeBuilder(\"delete\", writeInfo)\n+        val targetTableScan =  buildScanPlan(target.table, target.output, mergeBuilder, prunedTargetPlan)\n+\n+        // Construct an outer join to help track changes in source and target.\n+        // TODO : Optimize this to use LEFT ANTI or RIGHT OUTER when applicable.\n+        val sourceTableProj = source.output ++ Seq(Alias(lit(true).expr, ROW_FROM_SOURCE)())\n+        val targetTableProj = target.output ++ Seq(Alias(lit(true).expr, ROW_FROM_TARGET)())\n+        val newTargetTableScan = Project(targetTableProj, targetTableScan)\n+        val newSourceTableScan = Project(sourceTableProj, source)\n+        val joinPlan = Join(newSourceTableScan, newTargetTableScan, FullOuter, Some(cond), JoinHint.NONE)\n+\n+        // Construct the plan to replace the data based on the output of `MergeInto`\n+        val mergeParams = MergeIntoParams(\n+          isSourceRowNotPresent = IsNull(findOutputAttr(joinPlan, ROW_FROM_SOURCE)),\n+          isTargetRowNotPresent = IsNull(findOutputAttr(joinPlan, ROW_FROM_TARGET)),\n+          matchedConditions = actions.map(getClauseCondition),\n+          matchedOutputs = actions.map(actionOutput(_, targetOutputCols)),\n+          notMatchedConditions = notActions.map(getClauseCondition),\n+          notMatchedOutputs = notActions.map(actionOutput(_, targetOutputCols)),\n+          targetOutput = targetOutputCols :+ Literal(false),\n+          deleteOutput = targetOutputCols :+ Literal(true),\n+          joinedAttributes = joinPlan.output\n+        )\n+        val mergePlan = MergeInto(mergeParams, target, joinPlan)\n+        val batchWrite = mergeBuilder.asWriteBuilder.buildForBatch()\n+        ReplaceData(target, batchWrite, mergePlan)\n+    }\n+  }\n+\n+  def getTargetOutputCols(target: DataSourceV2Relation): Seq[NamedExpression] = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 80}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTY5Njk0OTAx", "url": "https://github.com/apache/iceberg/pull/1947#pullrequestreview-569694901", "createdAt": "2021-01-15T22:32:57Z", "commit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQyMjozMjo1N1rOIUv4Eg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQyMjozMjo1N1rOIUv4Eg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODYyNjgzNA==", "bodyText": "It is probably better to accept SQLConf in this rule like in AlignMergeIntoTable.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r558626834", "createdAt": "2021-01-15T22:32:57Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteMergeInto.scala", "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.{FullOuter, Inner}\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.catalyst.utils.PlanHelper\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.internal.SQLConf\n+\n+object RewriteMergeInto extends Rule[LogicalPlan] with PlanHelper with Logging  {\n+  val ROW_FROM_SOURCE = \"_row_from_source_\"\n+  val ROW_FROM_TARGET = \"_row_from_target_\"\n+\n+  import org.apache.spark.sql.execution.datasources.v2.ExtendedDataSourceV2Implicits._\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = {\n+    plan resolveOperators {\n+      // rewrite all operations that require reading the table to delete records\n+      case MergeIntoTable(target: DataSourceV2Relation,\n+                          source: LogicalPlan, cond, actions, notActions) =>\n+        val targetOutputCols = target.output\n+        val newProjectCols = target.output ++ Seq(Alias(InputFileName(), FILE_NAME_COL)())\n+        val newTargetTable = Project(newProjectCols, target)\n+\n+        // Construct the plan to prune target based on join condition between source and\n+        // target.\n+        val prunedTargetPlan = Join(source, newTargetTable, Inner, Some(cond), JoinHint.NONE)\n+        val writeInfo = newWriteInfo(target.schema)\n+        val mergeBuilder = target.table.asMergeable.newMergeBuilder(\"delete\", writeInfo)\n+        val targetTableScan =  buildScanPlan(target.table, target.output, mergeBuilder, prunedTargetPlan)\n+\n+        // Construct an outer join to help track changes in source and target.\n+        // TODO : Optimize this to use LEFT ANTI or RIGHT OUTER when applicable.\n+        val sourceTableProj = source.output ++ Seq(Alias(lit(true).expr, ROW_FROM_SOURCE)())\n+        val targetTableProj = target.output ++ Seq(Alias(lit(true).expr, ROW_FROM_TARGET)())\n+        val newTargetTableScan = Project(targetTableProj, targetTableScan)\n+        val newSourceTableScan = Project(sourceTableProj, source)\n+        val joinPlan = Join(newSourceTableScan, newTargetTableScan, FullOuter, Some(cond), JoinHint.NONE)\n+\n+        // Construct the plan to replace the data based on the output of `MergeInto`\n+        val mergeParams = MergeIntoParams(\n+          isSourceRowNotPresent = IsNull(findOutputAttr(joinPlan, ROW_FROM_SOURCE)),\n+          isTargetRowNotPresent = IsNull(findOutputAttr(joinPlan, ROW_FROM_TARGET)),\n+          matchedConditions = actions.map(getClauseCondition),\n+          matchedOutputs = actions.map(actionOutput(_, targetOutputCols)),\n+          notMatchedConditions = notActions.map(getClauseCondition),\n+          notMatchedOutputs = notActions.map(actionOutput(_, targetOutputCols)),\n+          targetOutput = targetOutputCols :+ Literal(false),\n+          deleteOutput = targetOutputCols :+ Literal(true),\n+          joinedAttributes = joinPlan.output\n+        )\n+        val mergePlan = MergeInto(mergeParams, target, joinPlan)\n+        val batchWrite = mergeBuilder.asWriteBuilder.buildForBatch()\n+        ReplaceData(target, batchWrite, mergePlan)\n+    }\n+  }\n+\n+  def getTargetOutputCols(target: DataSourceV2Relation): Seq[NamedExpression] = {\n+    target.schema.map { col =>\n+      target.output.find(attr => SQLConf.get.resolver(attr.name, col.name)).getOrElse {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 82}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTY5Njk1NTMz", "url": "https://github.com/apache/iceberg/pull/1947#pullrequestreview-569695533", "createdAt": "2021-01-15T22:33:20Z", "commit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQyMjozMzoyMFrOIUv4gA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQyMjozMzoyMFrOIUv4gA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODYyNjk0NA==", "bodyText": "Is this method actually used?", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r558626944", "createdAt": "2021-01-15T22:33:20Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteMergeInto.scala", "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.{FullOuter, Inner}\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.catalyst.utils.PlanHelper\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.internal.SQLConf\n+\n+object RewriteMergeInto extends Rule[LogicalPlan] with PlanHelper with Logging  {\n+  val ROW_FROM_SOURCE = \"_row_from_source_\"\n+  val ROW_FROM_TARGET = \"_row_from_target_\"\n+\n+  import org.apache.spark.sql.execution.datasources.v2.ExtendedDataSourceV2Implicits._\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = {\n+    plan resolveOperators {\n+      // rewrite all operations that require reading the table to delete records\n+      case MergeIntoTable(target: DataSourceV2Relation,\n+                          source: LogicalPlan, cond, actions, notActions) =>\n+        val targetOutputCols = target.output\n+        val newProjectCols = target.output ++ Seq(Alias(InputFileName(), FILE_NAME_COL)())\n+        val newTargetTable = Project(newProjectCols, target)\n+\n+        // Construct the plan to prune target based on join condition between source and\n+        // target.\n+        val prunedTargetPlan = Join(source, newTargetTable, Inner, Some(cond), JoinHint.NONE)\n+        val writeInfo = newWriteInfo(target.schema)\n+        val mergeBuilder = target.table.asMergeable.newMergeBuilder(\"delete\", writeInfo)\n+        val targetTableScan =  buildScanPlan(target.table, target.output, mergeBuilder, prunedTargetPlan)\n+\n+        // Construct an outer join to help track changes in source and target.\n+        // TODO : Optimize this to use LEFT ANTI or RIGHT OUTER when applicable.\n+        val sourceTableProj = source.output ++ Seq(Alias(lit(true).expr, ROW_FROM_SOURCE)())\n+        val targetTableProj = target.output ++ Seq(Alias(lit(true).expr, ROW_FROM_TARGET)())\n+        val newTargetTableScan = Project(targetTableProj, targetTableScan)\n+        val newSourceTableScan = Project(sourceTableProj, source)\n+        val joinPlan = Join(newSourceTableScan, newTargetTableScan, FullOuter, Some(cond), JoinHint.NONE)\n+\n+        // Construct the plan to replace the data based on the output of `MergeInto`\n+        val mergeParams = MergeIntoParams(\n+          isSourceRowNotPresent = IsNull(findOutputAttr(joinPlan, ROW_FROM_SOURCE)),\n+          isTargetRowNotPresent = IsNull(findOutputAttr(joinPlan, ROW_FROM_TARGET)),\n+          matchedConditions = actions.map(getClauseCondition),\n+          matchedOutputs = actions.map(actionOutput(_, targetOutputCols)),\n+          notMatchedConditions = notActions.map(getClauseCondition),\n+          notMatchedOutputs = notActions.map(actionOutput(_, targetOutputCols)),\n+          targetOutput = targetOutputCols :+ Literal(false),\n+          deleteOutput = targetOutputCols :+ Literal(true),\n+          joinedAttributes = joinPlan.output\n+        )\n+        val mergePlan = MergeInto(mergeParams, target, joinPlan)\n+        val batchWrite = mergeBuilder.asWriteBuilder.buildForBatch()\n+        ReplaceData(target, batchWrite, mergePlan)\n+    }\n+  }\n+\n+  def getTargetOutputCols(target: DataSourceV2Relation): Seq[NamedExpression] = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 80}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTY5NzAwOTA4", "url": "https://github.com/apache/iceberg/pull/1947#pullrequestreview-569700908", "createdAt": "2021-01-15T22:36:39Z", "commit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQyMjozNjozOVrOIUv8yA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQyMjozNjozOVrOIUv8yA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODYyODA0MA==", "bodyText": "nit: I like Spark's way of formatting like this a bit more:\ncase class MergeInto(\n    mergeIntoProcessor: ...\n    targetRelation: ...\n    child: ...)", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r558628040", "createdAt": "2021-01-15T22:36:39Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/MergeInto.scala", "diffHunk": "@@ -0,0 +1,39 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.plans.logical\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation\n+\n+case class MergeInto(mergeIntoProcessor: MergeIntoParams,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 25}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTY5NzYwNjIw", "url": "https://github.com/apache/iceberg/pull/1947#pullrequestreview-569760620", "createdAt": "2021-01-15T23:18:43Z", "commit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQyMzoxODo0M1rOIUxXWQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQyMzoxODo0M1rOIUxXWQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODY1MTIyNQ==", "bodyText": "I think comments here would help people who will maintain/contribute in the future.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r558651225", "createdAt": "2021-01-15T23:18:43Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/MergeIntoExec.scala", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, BasePredicate, Expression, UnsafeProjection}\n+import org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate\n+import org.apache.spark.sql.catalyst.plans.logical.MergeIntoParams\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+\n+case class MergeIntoExec(mergeIntoProcessor: MergeIntoParams,\n+                         @transient targetRelation: DataSourceV2Relation,\n+                         override val child: SparkPlan) extends UnaryExecNode {\n+\n+  override def output: Seq[Attribute] = targetRelation.output\n+\n+  protected override def doExecute(): RDD[InternalRow] = {\n+    child.execute().mapPartitions {\n+      processPartition(mergeIntoProcessor, _)\n+    }\n+  }\n+\n+  private def generateProjection(exprs: Seq[Expression], attrs: Seq[Attribute]): UnsafeProjection = {\n+    UnsafeProjection.create(exprs, attrs)\n+  }\n+\n+  private def generatePredicate(expr: Expression, attrs: Seq[Attribute]): BasePredicate = {\n+    GeneratePredicate.generate(expr, attrs)\n+  }\n+\n+  def applyProjection(predicates: Seq[BasePredicate],\n+                      projections: Seq[UnsafeProjection],\n+                      projectTargetCols: UnsafeProjection,\n+                      projectDeleteRow: UnsafeProjection,\n+                      inputRow: InternalRow,\n+                      targetRowNotPresent: Boolean): InternalRow = {\n+    // Find the first combination where the predicate evaluates to true\n+    val pair = (predicates zip projections).find {\n+      case (predicate, _) => predicate.eval(inputRow)\n+    }\n+\n+    // Now apply the appropriate projection to either :\n+    // - Insert a row into target\n+    // - Update a row of target\n+    // - Delete a row in target. The projected row will have the deleted bit set.\n+    pair match {\n+      case Some((_, projection)) =>\n+        projection.apply(inputRow)\n+      case None =>\n+        if (targetRowNotPresent) {\n+          projectDeleteRow.apply(inputRow)\n+        } else {\n+          projectTargetCols.apply(inputRow)\n+        }\n+    }\n+  }\n+\n+  def processPartition(params: MergeIntoParams,\n+                       rowIterator: Iterator[InternalRow]): Iterator[InternalRow] = {\n+    val joinedAttrs = params.joinedAttributes\n+    val isSourceRowNotPresentPred = generatePredicate(params.isSourceRowNotPresent, joinedAttrs)\n+    val isTargetRowNotPresentPred = generatePredicate(params.isTargetRowNotPresent, joinedAttrs)\n+    val matchedPreds = params.matchedConditions.map(generatePredicate(_, joinedAttrs))\n+    val matchedProjs = params.matchedOutputs.map(generateProjection(_, joinedAttrs))\n+    val notMatchedPreds = params.notMatchedConditions.map(generatePredicate(_, joinedAttrs))\n+    val notMatchedProjs = params.notMatchedOutputs.map(generateProjection(_, joinedAttrs))\n+    val projectTargetCols = generateProjection(params.targetOutput, joinedAttrs)\n+    val projectDeletedRow = generateProjection(params.deleteOutput, joinedAttrs)\n+\n+    def shouldDeleteRow(row: InternalRow): Boolean =\n+      row.getBoolean(params.targetOutput.size - 1)\n+\n+\n+    def processRow(inputRow: InternalRow): InternalRow = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 92}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTY5NzY2MDI4", "url": "https://github.com/apache/iceberg/pull/1947#pullrequestreview-569766028", "createdAt": "2021-01-15T23:25:56Z", "commit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQyMzoyNTo1NlrOIUxyyg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQyMzoyNTo1NlrOIUxyyg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODY1ODI1MA==", "bodyText": "I think this will require further explanation. There will be up to 2 matched cases and we try to find the first one that matches? Does find guarantee the order of the traversal?", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r558658250", "createdAt": "2021-01-15T23:25:56Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/MergeIntoExec.scala", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, BasePredicate, Expression, UnsafeProjection}\n+import org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate\n+import org.apache.spark.sql.catalyst.plans.logical.MergeIntoParams\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+\n+case class MergeIntoExec(mergeIntoProcessor: MergeIntoParams,\n+                         @transient targetRelation: DataSourceV2Relation,\n+                         override val child: SparkPlan) extends UnaryExecNode {\n+\n+  override def output: Seq[Attribute] = targetRelation.output\n+\n+  protected override def doExecute(): RDD[InternalRow] = {\n+    child.execute().mapPartitions {\n+      processPartition(mergeIntoProcessor, _)\n+    }\n+  }\n+\n+  private def generateProjection(exprs: Seq[Expression], attrs: Seq[Attribute]): UnsafeProjection = {\n+    UnsafeProjection.create(exprs, attrs)\n+  }\n+\n+  private def generatePredicate(expr: Expression, attrs: Seq[Attribute]): BasePredicate = {\n+    GeneratePredicate.generate(expr, attrs)\n+  }\n+\n+  def applyProjection(predicates: Seq[BasePredicate],\n+                      projections: Seq[UnsafeProjection],\n+                      projectTargetCols: UnsafeProjection,\n+                      projectDeleteRow: UnsafeProjection,\n+                      inputRow: InternalRow,\n+                      targetRowNotPresent: Boolean): InternalRow = {\n+    // Find the first combination where the predicate evaluates to true", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 55}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTY5Nzg2MDI5", "url": "https://github.com/apache/iceberg/pull/1947#pullrequestreview-569786029", "createdAt": "2021-01-16T00:28:36Z", "commit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNlQwMDoyODozNlrOIU06Sg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNlQwMDoyODozNlrOIU06Sg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODcwOTMyMg==", "bodyText": "The operation passed to the merge builder should be merge. And we will want to add tests that the isolation level is carried through correctly.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r558709322", "createdAt": "2021-01-16T00:28:36Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteMergeInto.scala", "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.{FullOuter, Inner}\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.catalyst.utils.PlanHelper\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.internal.SQLConf\n+\n+object RewriteMergeInto extends Rule[LogicalPlan] with PlanHelper with Logging  {\n+  val ROW_FROM_SOURCE = \"_row_from_source_\"\n+  val ROW_FROM_TARGET = \"_row_from_target_\"\n+\n+  import org.apache.spark.sql.execution.datasources.v2.ExtendedDataSourceV2Implicits._\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = {\n+    plan resolveOperators {\n+      // rewrite all operations that require reading the table to delete records\n+      case MergeIntoTable(target: DataSourceV2Relation,\n+                          source: LogicalPlan, cond, actions, notActions) =>\n+        val targetOutputCols = target.output\n+        val newProjectCols = target.output ++ Seq(Alias(InputFileName(), FILE_NAME_COL)())\n+        val newTargetTable = Project(newProjectCols, target)\n+\n+        // Construct the plan to prune target based on join condition between source and\n+        // target.\n+        val prunedTargetPlan = Join(source, newTargetTable, Inner, Some(cond), JoinHint.NONE)\n+        val writeInfo = newWriteInfo(target.schema)\n+        val mergeBuilder = target.table.asMergeable.newMergeBuilder(\"delete\", writeInfo)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 51}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTY5Nzg2MjI2", "url": "https://github.com/apache/iceberg/pull/1947#pullrequestreview-569786226", "createdAt": "2021-01-16T00:29:33Z", "commit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNlQwMDoyOTozM1rOIU08XA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNlQwMDoyOTozM1rOIU08XA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODcwOTg1Mg==", "bodyText": "Is there a better name than prunedTargetPlan? What about matchedRowsPlan?", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r558709852", "createdAt": "2021-01-16T00:29:33Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteMergeInto.scala", "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.{FullOuter, Inner}\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.catalyst.utils.PlanHelper\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.internal.SQLConf\n+\n+object RewriteMergeInto extends Rule[LogicalPlan] with PlanHelper with Logging  {\n+  val ROW_FROM_SOURCE = \"_row_from_source_\"\n+  val ROW_FROM_TARGET = \"_row_from_target_\"\n+\n+  import org.apache.spark.sql.execution.datasources.v2.ExtendedDataSourceV2Implicits._\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = {\n+    plan resolveOperators {\n+      // rewrite all operations that require reading the table to delete records\n+      case MergeIntoTable(target: DataSourceV2Relation,\n+                          source: LogicalPlan, cond, actions, notActions) =>\n+        val targetOutputCols = target.output\n+        val newProjectCols = target.output ++ Seq(Alias(InputFileName(), FILE_NAME_COL)())\n+        val newTargetTable = Project(newProjectCols, target)\n+\n+        // Construct the plan to prune target based on join condition between source and\n+        // target.\n+        val prunedTargetPlan = Join(source, newTargetTable, Inner, Some(cond), JoinHint.NONE)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 49}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTY5Nzg3NzU2", "url": "https://github.com/apache/iceberg/pull/1947#pullrequestreview-569787756", "createdAt": "2021-01-16T00:37:14Z", "commit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNlQwMDozNzoxNFrOIU1NSw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNlQwMDozNzoxNFrOIU1NSw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODcxNDE4Nw==", "bodyText": "I think that this will require a reference to the target table because _file needs to come from the target table and not the source table. Right now it works because the target table is the only one with _file defined, but I think we should plan on selecting the right column if there are duplicates. If I were to define a column _file in the source data, it may not work.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r558714187", "createdAt": "2021-01-16T00:37:14Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/utils/PlanHelper.scala", "diffHunk": "@@ -0,0 +1,87 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.spark.sql.catalyst.utils\n+\n+import java.util.UUID\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, AttributeReference, PredicateHelper}\n+import org.apache.spark.sql.catalyst.plans.logical.{Aggregate, DynamicFileFilter, LogicalPlan}\n+import org.apache.spark.sql.connector.catalog.Table\n+import org.apache.spark.sql.connector.iceberg.read.SupportsFileFilter\n+import org.apache.spark.sql.connector.iceberg.write.MergeBuilder\n+import org.apache.spark.sql.connector.write.{LogicalWriteInfo, LogicalWriteInfoImpl}\n+import org.apache.spark.sql.execution.datasources.v2.{DataSourceV2ScanRelation}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+trait PlanHelper extends PredicateHelper {\n+  val FILE_NAME_COL = \"_file\"\n+  val ROW_POS_COL = \"_pos\"\n+\n+  def buildScanPlan(table: Table,\n+                    output: Seq[AttributeReference],\n+                    mergeBuilder: MergeBuilder,\n+                    prunedTargetPlan: LogicalPlan): LogicalPlan = {\n+\n+    val scanBuilder = mergeBuilder.asScanBuilder\n+    val scan = scanBuilder.build()\n+    val scanRelation = DataSourceV2ScanRelation(table, scan, toOutputAttrs(scan.readSchema(), output))\n+\n+    scan match {\n+      case filterable: SupportsFileFilter =>\n+        val matchingFilePlan = buildFileFilterPlan(prunedTargetPlan)\n+        val dynamicFileFilter = DynamicFileFilter(scanRelation, matchingFilePlan, filterable)\n+        dynamicFileFilter\n+      case _ =>\n+        scanRelation\n+    }\n+  }\n+\n+  private def buildFileFilterPlan(prunedTargetPlan: LogicalPlan): LogicalPlan = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 57}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTY5Nzg4OTgw", "url": "https://github.com/apache/iceberg/pull/1947#pullrequestreview-569788980", "createdAt": "2021-01-16T00:43:43Z", "commit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNlQwMDo0Mzo0M1rOIU1bOw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNlQwMDo0Mzo0M1rOIU1bOw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODcxNzc1NQ==", "bodyText": "We want this scan to be shared by both the file filter plan and the scan relation. I think that means that this can't accept the matchedRowsPlan / prunedTargetPlan:\n\nWe don't want to plan the scan twice\nThe scan built by MergeBuilder is going to have _file and _pos automatically added. If we want to reference those, we should use this scan.\n\nThat means we'll have to add the inner join here, or pass the scanRelation to a function that produces the inner join. I think that's what @aokolnychyi was proposing in his version of this class.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r558717755", "createdAt": "2021-01-16T00:43:43Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/utils/PlanHelper.scala", "diffHunk": "@@ -0,0 +1,87 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.spark.sql.catalyst.utils\n+\n+import java.util.UUID\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, AttributeReference, PredicateHelper}\n+import org.apache.spark.sql.catalyst.plans.logical.{Aggregate, DynamicFileFilter, LogicalPlan}\n+import org.apache.spark.sql.connector.catalog.Table\n+import org.apache.spark.sql.connector.iceberg.read.SupportsFileFilter\n+import org.apache.spark.sql.connector.iceberg.write.MergeBuilder\n+import org.apache.spark.sql.connector.write.{LogicalWriteInfo, LogicalWriteInfoImpl}\n+import org.apache.spark.sql.execution.datasources.v2.{DataSourceV2ScanRelation}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+trait PlanHelper extends PredicateHelper {\n+  val FILE_NAME_COL = \"_file\"\n+  val ROW_POS_COL = \"_pos\"\n+\n+  def buildScanPlan(table: Table,\n+                    output: Seq[AttributeReference],\n+                    mergeBuilder: MergeBuilder,\n+                    prunedTargetPlan: LogicalPlan): LogicalPlan = {\n+\n+    val scanBuilder = mergeBuilder.asScanBuilder\n+    val scan = scanBuilder.build()\n+    val scanRelation = DataSourceV2ScanRelation(table, scan, toOutputAttrs(scan.readSchema(), output))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 45}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTY5NzkwOTkx", "url": "https://github.com/apache/iceberg/pull/1947#pullrequestreview-569790991", "createdAt": "2021-01-16T00:54:09Z", "commit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNlQwMDo1NDowOVrOIU1ybA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNlQwMDo1NDowOVrOIU1ybA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODcyMzY5Mg==", "bodyText": "Could you use one import per line? That will help avoid git conflicts in the future.\nAlso, we don't use wildcard imports in Iceberg because it can cause conflicts and it isn't clear where symbols are coming from when reading PRs.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r558723692", "createdAt": "2021-01-16T00:54:09Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteMergeInto.scala", "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.expressions._", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 23}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTY5NzkxMjc2", "url": "https://github.com/apache/iceberg/pull/1947#pullrequestreview-569791276", "createdAt": "2021-01-16T00:55:48Z", "commit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNlQwMDo1NTo0OFrOIU115Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNlQwMDo1NTo0OFrOIU115Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODcyNDU4MQ==", "bodyText": "If you replace lit(true).expr with an expression, then this doesn't need to pull in sql.functions._. I added a constant:\n  private val TRUE = Literal(true, BooleanType)", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r558724581", "createdAt": "2021-01-16T00:55:48Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteMergeInto.scala", "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.{FullOuter, Inner}\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.catalyst.utils.PlanHelper\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.internal.SQLConf\n+\n+object RewriteMergeInto extends Rule[LogicalPlan] with PlanHelper with Logging  {\n+  val ROW_FROM_SOURCE = \"_row_from_source_\"\n+  val ROW_FROM_TARGET = \"_row_from_target_\"\n+\n+  import org.apache.spark.sql.execution.datasources.v2.ExtendedDataSourceV2Implicits._\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = {\n+    plan resolveOperators {\n+      // rewrite all operations that require reading the table to delete records\n+      case MergeIntoTable(target: DataSourceV2Relation,\n+                          source: LogicalPlan, cond, actions, notActions) =>\n+        val targetOutputCols = target.output\n+        val newProjectCols = target.output ++ Seq(Alias(InputFileName(), FILE_NAME_COL)())\n+        val newTargetTable = Project(newProjectCols, target)\n+\n+        // Construct the plan to prune target based on join condition between source and\n+        // target.\n+        val prunedTargetPlan = Join(source, newTargetTable, Inner, Some(cond), JoinHint.NONE)\n+        val writeInfo = newWriteInfo(target.schema)\n+        val mergeBuilder = target.table.asMergeable.newMergeBuilder(\"delete\", writeInfo)\n+        val targetTableScan =  buildScanPlan(target.table, target.output, mergeBuilder, prunedTargetPlan)\n+\n+        // Construct an outer join to help track changes in source and target.\n+        // TODO : Optimize this to use LEFT ANTI or RIGHT OUTER when applicable.\n+        val sourceTableProj = source.output ++ Seq(Alias(lit(true).expr, ROW_FROM_SOURCE)())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 56}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTY5Nzk0NDU2", "url": "https://github.com/apache/iceberg/pull/1947#pullrequestreview-569794456", "createdAt": "2021-01-16T01:14:54Z", "commit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNlQwMToxNDo1NFrOIU2dTg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNlQwMToxNDo1NFrOIU2dTg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODczNDY3MA==", "bodyText": "Nit: to make this class more readable, we've been separating cases with a blank line.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r558734670", "createdAt": "2021-01-16T01:14:54Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/ExtendedDataSourceV2Strategy.scala", "diffHunk": "@@ -75,6 +76,8 @@ case class ExtendedDataSourceV2Strategy(spark: SparkSession) extends Strategy {\n     case ReplaceData(_, batchWrite, query) =>\n       ReplaceDataExec(batchWrite, planLater(query)) :: Nil\n \n+    case MergeInto(mergeIntoProcessor, targetRelation, child) =>\n+      MergeIntoExec(mergeIntoProcessor, targetRelation, planLater(child)) :: Nil", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 13}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTY5Nzk0NTk1", "url": "https://github.com/apache/iceberg/pull/1947#pullrequestreview-569794595", "createdAt": "2021-01-16T01:15:47Z", "commit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNlQwMToxNTo0N1rOIU2fTA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNlQwMToxNTo0N1rOIU2fTA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODczNTE4MA==", "bodyText": "Looks like this variable name wasn't updated when the processor was renamed to params.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r558735180", "createdAt": "2021-01-16T01:15:47Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/MergeIntoExec.scala", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, BasePredicate, Expression, UnsafeProjection}\n+import org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate\n+import org.apache.spark.sql.catalyst.plans.logical.MergeIntoParams\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+\n+case class MergeIntoExec(mergeIntoProcessor: MergeIntoParams,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 29}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTY5Nzk1NDQ5", "url": "https://github.com/apache/iceberg/pull/1947#pullrequestreview-569795449", "createdAt": "2021-01-16T01:21:56Z", "commit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNlQwMToyMTo1N1rOIU2sRQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNlQwMToyMTo1N1rOIU2sRQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODczODUwMQ==", "bodyText": "Why use a match here and not if (isSourceRowNotPresentPred.eval(inputRow))?\nIf you did that, it would be a bit cleaner:\n      if (isSourceRowNotPresentPred.eval(inputRow)) {\n        projectTargetCols.apply(inputRow)\n      } else if (isTargetRowNotPresentPred.eval(inputRow)) {\n        applyProjection(notMatchedPreds, notMatchedProjs, projectTargetCols,\n          projectDeletedRow, inputRow, true)\n      } else {\n        applyProjection(matchedPreds, matchedProjs, projectTargetCols,\n          projectDeletedRow, inputRow, false)\n      }", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r558738501", "createdAt": "2021-01-16T01:21:57Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/MergeIntoExec.scala", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, BasePredicate, Expression, UnsafeProjection}\n+import org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate\n+import org.apache.spark.sql.catalyst.plans.logical.MergeIntoParams\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+\n+case class MergeIntoExec(mergeIntoProcessor: MergeIntoParams,\n+                         @transient targetRelation: DataSourceV2Relation,\n+                         override val child: SparkPlan) extends UnaryExecNode {\n+\n+  override def output: Seq[Attribute] = targetRelation.output\n+\n+  protected override def doExecute(): RDD[InternalRow] = {\n+    child.execute().mapPartitions {\n+      processPartition(mergeIntoProcessor, _)\n+    }\n+  }\n+\n+  private def generateProjection(exprs: Seq[Expression], attrs: Seq[Attribute]): UnsafeProjection = {\n+    UnsafeProjection.create(exprs, attrs)\n+  }\n+\n+  private def generatePredicate(expr: Expression, attrs: Seq[Attribute]): BasePredicate = {\n+    GeneratePredicate.generate(expr, attrs)\n+  }\n+\n+  def applyProjection(predicates: Seq[BasePredicate],\n+                      projections: Seq[UnsafeProjection],\n+                      projectTargetCols: UnsafeProjection,\n+                      projectDeleteRow: UnsafeProjection,\n+                      inputRow: InternalRow,\n+                      targetRowNotPresent: Boolean): InternalRow = {\n+    // Find the first combination where the predicate evaluates to true\n+    val pair = (predicates zip projections).find {\n+      case (predicate, _) => predicate.eval(inputRow)\n+    }\n+\n+    // Now apply the appropriate projection to either :\n+    // - Insert a row into target\n+    // - Update a row of target\n+    // - Delete a row in target. The projected row will have the deleted bit set.\n+    pair match {\n+      case Some((_, projection)) =>\n+        projection.apply(inputRow)\n+      case None =>\n+        if (targetRowNotPresent) {\n+          projectDeleteRow.apply(inputRow)\n+        } else {\n+          projectTargetCols.apply(inputRow)\n+        }\n+    }\n+  }\n+\n+  def processPartition(params: MergeIntoParams,\n+                       rowIterator: Iterator[InternalRow]): Iterator[InternalRow] = {\n+    val joinedAttrs = params.joinedAttributes\n+    val isSourceRowNotPresentPred = generatePredicate(params.isSourceRowNotPresent, joinedAttrs)\n+    val isTargetRowNotPresentPred = generatePredicate(params.isTargetRowNotPresent, joinedAttrs)\n+    val matchedPreds = params.matchedConditions.map(generatePredicate(_, joinedAttrs))\n+    val matchedProjs = params.matchedOutputs.map(generateProjection(_, joinedAttrs))\n+    val notMatchedPreds = params.notMatchedConditions.map(generatePredicate(_, joinedAttrs))\n+    val notMatchedProjs = params.notMatchedOutputs.map(generateProjection(_, joinedAttrs))\n+    val projectTargetCols = generateProjection(params.targetOutput, joinedAttrs)\n+    val projectDeletedRow = generateProjection(params.deleteOutput, joinedAttrs)\n+\n+    def shouldDeleteRow(row: InternalRow): Boolean =\n+      row.getBoolean(params.targetOutput.size - 1)\n+\n+\n+    def processRow(inputRow: InternalRow): InternalRow = {\n+      isSourceRowNotPresentPred.eval(inputRow) match {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 93}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTY5Nzk1ODgx", "url": "https://github.com/apache/iceberg/pull/1947#pullrequestreview-569795881", "createdAt": "2021-01-16T01:25:11Z", "commit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNlQwMToyNToxMVrOIU2y4w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNlQwMToyNToxMVrOIU2y4w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODc0MDE5NQ==", "bodyText": "It seems a bit odd to apply this projection because the target row will be deleted. It seems like we could use the same lazily-initialized row for every delete.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r558740195", "createdAt": "2021-01-16T01:25:11Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/MergeIntoExec.scala", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, BasePredicate, Expression, UnsafeProjection}\n+import org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate\n+import org.apache.spark.sql.catalyst.plans.logical.MergeIntoParams\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+\n+case class MergeIntoExec(mergeIntoProcessor: MergeIntoParams,\n+                         @transient targetRelation: DataSourceV2Relation,\n+                         override val child: SparkPlan) extends UnaryExecNode {\n+\n+  override def output: Seq[Attribute] = targetRelation.output\n+\n+  protected override def doExecute(): RDD[InternalRow] = {\n+    child.execute().mapPartitions {\n+      processPartition(mergeIntoProcessor, _)\n+    }\n+  }\n+\n+  private def generateProjection(exprs: Seq[Expression], attrs: Seq[Attribute]): UnsafeProjection = {\n+    UnsafeProjection.create(exprs, attrs)\n+  }\n+\n+  private def generatePredicate(expr: Expression, attrs: Seq[Attribute]): BasePredicate = {\n+    GeneratePredicate.generate(expr, attrs)\n+  }\n+\n+  def applyProjection(predicates: Seq[BasePredicate],\n+                      projections: Seq[UnsafeProjection],\n+                      projectTargetCols: UnsafeProjection,\n+                      projectDeleteRow: UnsafeProjection,\n+                      inputRow: InternalRow,\n+                      targetRowNotPresent: Boolean): InternalRow = {\n+    // Find the first combination where the predicate evaluates to true\n+    val pair = (predicates zip projections).find {\n+      case (predicate, _) => predicate.eval(inputRow)\n+    }\n+\n+    // Now apply the appropriate projection to either :\n+    // - Insert a row into target\n+    // - Update a row of target\n+    // - Delete a row in target. The projected row will have the deleted bit set.\n+    pair match {\n+      case Some((_, projection)) =>\n+        projection.apply(inputRow)\n+      case None =>\n+        if (targetRowNotPresent) {\n+          projectDeleteRow.apply(inputRow)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 69}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTY5Nzk2Mzk2", "url": "https://github.com/apache/iceberg/pull/1947#pullrequestreview-569796396", "createdAt": "2021-01-16T01:28:55Z", "commit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNlQwMToyODo1NVrOIU27dw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNlQwMToyODo1NVrOIU27dw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODc0MjM5MQ==", "bodyText": "These last two projections are only needed when notMatchedPreds or matchedPreds does not have a default case, i.e. lit(true).\nIn the rewrite, there is also a function, getClauseCondition, that fills in lit(true) if there is no clause condition. But I don't think that any predicates after the true condition are dropped.\nI think we could simplify the logic here and avoid extra clauses by ensuring that both matchedPreds and notMatchedPreds end with lit(true). Then this class would not need to account for the case where no predicate matches and we wouldn't have extra predicates passed through. Last, we wouldn't need the last two projections here or in MergeIntoParams because they would be added to notMatchedProjs or matchedProjs.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r558742391", "createdAt": "2021-01-16T01:28:55Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/MergeIntoExec.scala", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, BasePredicate, Expression, UnsafeProjection}\n+import org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate\n+import org.apache.spark.sql.catalyst.plans.logical.MergeIntoParams\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+\n+case class MergeIntoExec(mergeIntoProcessor: MergeIntoParams,\n+                         @transient targetRelation: DataSourceV2Relation,\n+                         override val child: SparkPlan) extends UnaryExecNode {\n+\n+  override def output: Seq[Attribute] = targetRelation.output\n+\n+  protected override def doExecute(): RDD[InternalRow] = {\n+    child.execute().mapPartitions {\n+      processPartition(mergeIntoProcessor, _)\n+    }\n+  }\n+\n+  private def generateProjection(exprs: Seq[Expression], attrs: Seq[Attribute]): UnsafeProjection = {\n+    UnsafeProjection.create(exprs, attrs)\n+  }\n+\n+  private def generatePredicate(expr: Expression, attrs: Seq[Attribute]): BasePredicate = {\n+    GeneratePredicate.generate(expr, attrs)\n+  }\n+\n+  def applyProjection(predicates: Seq[BasePredicate],\n+                      projections: Seq[UnsafeProjection],\n+                      projectTargetCols: UnsafeProjection,\n+                      projectDeleteRow: UnsafeProjection,\n+                      inputRow: InternalRow,\n+                      targetRowNotPresent: Boolean): InternalRow = {\n+    // Find the first combination where the predicate evaluates to true\n+    val pair = (predicates zip projections).find {\n+      case (predicate, _) => predicate.eval(inputRow)\n+    }\n+\n+    // Now apply the appropriate projection to either :\n+    // - Insert a row into target\n+    // - Update a row of target\n+    // - Delete a row in target. The projected row will have the deleted bit set.\n+    pair match {\n+      case Some((_, projection)) =>\n+        projection.apply(inputRow)\n+      case None =>\n+        if (targetRowNotPresent) {\n+          projectDeleteRow.apply(inputRow)\n+        } else {\n+          projectTargetCols.apply(inputRow)\n+        }\n+    }\n+  }\n+\n+  def processPartition(params: MergeIntoParams,\n+                       rowIterator: Iterator[InternalRow]): Iterator[InternalRow] = {\n+    val joinedAttrs = params.joinedAttributes\n+    val isSourceRowNotPresentPred = generatePredicate(params.isSourceRowNotPresent, joinedAttrs)\n+    val isTargetRowNotPresentPred = generatePredicate(params.isTargetRowNotPresent, joinedAttrs)\n+    val matchedPreds = params.matchedConditions.map(generatePredicate(_, joinedAttrs))\n+    val matchedProjs = params.matchedOutputs.map(generateProjection(_, joinedAttrs))\n+    val notMatchedPreds = params.notMatchedConditions.map(generatePredicate(_, joinedAttrs))\n+    val notMatchedProjs = params.notMatchedOutputs.map(generateProjection(_, joinedAttrs))\n+    val projectTargetCols = generateProjection(params.targetOutput, joinedAttrs)\n+    val projectDeletedRow = generateProjection(params.deleteOutput, joinedAttrs)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 86}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTY5Nzk3MDM3", "url": "https://github.com/apache/iceberg/pull/1947#pullrequestreview-569797037", "createdAt": "2021-01-16T01:34:05Z", "commit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNlQwMTozNDowNVrOIU3GhA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNlQwMTozNDowNVrOIU3GhA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODc0NTIyMA==", "bodyText": "I think it would be better to create these pairs just once instead of in every row. After this change and moving the projections into action cases (see comment below), this method signature would be much simpler:\n  def applyProjection(\n      actions: Seq[(BasePredicate, UnsafeProjection)],\n      inputRow: InternalRow): InternalRow = {\n    val pair = actions.find {\n      case (predicate, _) => predicate.eval(inputRow)\n    }\n    ...\n  }", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r558745220", "createdAt": "2021-01-16T01:34:05Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/MergeIntoExec.scala", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, BasePredicate, Expression, UnsafeProjection}\n+import org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate\n+import org.apache.spark.sql.catalyst.plans.logical.MergeIntoParams\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+\n+case class MergeIntoExec(mergeIntoProcessor: MergeIntoParams,\n+                         @transient targetRelation: DataSourceV2Relation,\n+                         override val child: SparkPlan) extends UnaryExecNode {\n+\n+  override def output: Seq[Attribute] = targetRelation.output\n+\n+  protected override def doExecute(): RDD[InternalRow] = {\n+    child.execute().mapPartitions {\n+      processPartition(mergeIntoProcessor, _)\n+    }\n+  }\n+\n+  private def generateProjection(exprs: Seq[Expression], attrs: Seq[Attribute]): UnsafeProjection = {\n+    UnsafeProjection.create(exprs, attrs)\n+  }\n+\n+  private def generatePredicate(expr: Expression, attrs: Seq[Attribute]): BasePredicate = {\n+    GeneratePredicate.generate(expr, attrs)\n+  }\n+\n+  def applyProjection(predicates: Seq[BasePredicate],\n+                      projections: Seq[UnsafeProjection],\n+                      projectTargetCols: UnsafeProjection,\n+                      projectDeleteRow: UnsafeProjection,\n+                      inputRow: InternalRow,\n+                      targetRowNotPresent: Boolean): InternalRow = {\n+    // Find the first combination where the predicate evaluates to true\n+    val pair = (predicates zip projections).find {\n+      case (predicate, _) => predicate.eval(inputRow)\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 58}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTY5Nzk4NzYw", "url": "https://github.com/apache/iceberg/pull/1947#pullrequestreview-569798760", "createdAt": "2021-01-16T01:38:08Z", "commit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNlQwMTozODowOFrOIU3QXg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNlQwMTozODowOFrOIU3QXg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODc0Nzc0Mg==", "bodyText": "Why is this passing an empty string?", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r558747742", "createdAt": "2021-01-16T01:38:08Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeIntoTable.java", "diffHunk": "@@ -0,0 +1,267 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.PARQUET_VECTORIZATION_ENABLED;\n+\n+public class TestMergeIntoTable extends SparkRowLevelOperationsTestBase {\n+  private final String sourceName;\n+  private final String targetName;\n+\n+  public TestMergeIntoTable(String catalogName, String implementation, Map<String, String> config,\n+                            String fileFormat, Boolean vectorized) {\n+    super(catalogName, implementation, config, fileFormat, vectorized);\n+    this.sourceName = tableName(\"source\");\n+    this.targetName = tableName(\"target\");\n+  }\n+\n+  @BeforeClass\n+  public static void setupSparkConf() {\n+    spark.conf().set(\"spark.sql.shuffle.partitions\", \"4\");\n+  }\n+\n+  protected Map<String, String> extraTableProperties() {\n+    return ImmutableMap.of(TableProperties.DELETE_MODE, \"copy-on-write\");\n+  }\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", targetName);\n+    sql(\"DROP TABLE IF EXISTS %s\", sourceName);\n+  }\n+\n+  @Test\n+  public void testEmptyTargetInsertAllNonMatchingRows() throws NoSuchTableException {\n+    createAndInitUnPartitionedTargetTable(targetName);\n+    createAndInitSourceTable(sourceName);\n+    append(sourceName, new Employee(1, \"emp-id-1\"), new Employee(2, \"emp-id-2\"), new Employee(3, \"emp-id-3\"));\n+    String sqlText = \"MERGE INTO \" + targetName + \" AS target \\n\" +\n+                     \"USING \" + sourceName + \" AS source \\n\" +\n+                     \"ON target.id = source.id \\n\" +\n+                     \"WHEN NOT MATCHED THEN INSERT * \";\n+\n+    sql(sqlText, \"\");\n+    sql(\"SELECT * FROM %s ORDER BY id, dep\", targetName);\n+    assertEquals(\"Should have expected rows\",\n+            ImmutableList.of(row(1, \"emp-id-1\"), row(2, \"emp-id-2\"), row(3, \"emp-id-3\")),\n+            sql(\"SELECT * FROM %s ORDER BY id ASC NULLS LAST\", targetName));\n+  }\n+\n+  @Test\n+  public void testEmptyTargetInsertOnlyMatchingRows() throws NoSuchTableException {\n+    createAndInitUnPartitionedTargetTable(targetName);\n+    createAndInitSourceTable(sourceName);\n+    append(sourceName, new Employee(1, \"emp-id-1\"), new Employee(2, \"emp-id-2\"), new Employee(3, \"emp-id-3\"));\n+    String sqlText = \"MERGE INTO \" + targetName + \" AS target \\n\" +\n+                     \"USING \" + sourceName + \" AS source \\n\" +\n+                     \"ON target.id = source.id \\n\" +\n+                     \"WHEN NOT MATCHED AND (source.id >= 2) THEN INSERT * \";\n+\n+    sql(sqlText, \"\");\n+    List<Object[]> res = sql(\"SELECT * FROM %s ORDER BY id, dep\", targetName);\n+    assertEquals(\"Should have expected rows\",\n+            ImmutableList.of(row(2, \"emp-id-2\"), row(3, \"emp-id-3\")),\n+            sql(\"SELECT * FROM %s ORDER BY id ASC NULLS LAST\", targetName));\n+  }\n+\n+  @Test\n+  public void testOnlyUpdate() throws NoSuchTableException {\n+    createAndInitUnPartitionedTargetTable(targetName);\n+    createAndInitSourceTable(sourceName);\n+    append(targetName, new Employee(1, \"emp-id-one\"), new Employee(6, \"emp-id-6\"));\n+    append(sourceName, new Employee(2, \"emp-id-2\"), new Employee(1, \"emp-id-1\"), new Employee(6, \"emp-id-6\"));\n+    String sqlText = \"MERGE INTO \" + targetName + \" AS target \\n\" +\n+            \"USING \" + sourceName + \" AS source \\n\" +\n+            \"ON target.id = source.id \\n\" +\n+            \"WHEN MATCHED AND target.id = 1 THEN UPDATE SET * \";\n+\n+    sql(sqlText, \"\");\n+    List<Object[]> res = sql(\"SELECT * FROM %s ORDER BY id, dep\", targetName);\n+    assertEquals(\"Should have expected rows\",\n+            ImmutableList.of(row(1, \"emp-id-1\"), row(6, \"emp-id-6\")),\n+            sql(\"SELECT * FROM %s ORDER BY id ASC NULLS LAST\", targetName));\n+  }\n+\n+  @Test\n+  public void testOnlyDelete() throws NoSuchTableException {\n+    createAndInitUnPartitionedTargetTable(targetName);\n+    createAndInitSourceTable(sourceName);\n+    append(targetName, new Employee(1, \"emp-id-one\"), new Employee(6, \"emp-id-6\"));\n+    append(sourceName, new Employee(2, \"emp-id-2\"), new Employee(1, \"emp-id-1\"), new Employee(6, \"emp-id-6\"));\n+    String sqlText = \"MERGE INTO \" + targetName + \" AS target \\n\" +\n+            \"USING \" + sourceName + \" AS source \\n\" +\n+            \"ON target.id = source.id \\n\" +\n+            \"WHEN MATCHED AND target.id = 6 THEN DELETE\";\n+\n+    sql(sqlText, \"\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 129}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTY5Nzk4ODAz", "url": "https://github.com/apache/iceberg/pull/1947#pullrequestreview-569798803", "createdAt": "2021-01-16T01:38:28Z", "commit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNlQwMTozODoyOVrOIU3RDQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNlQwMTozODoyOVrOIU3RDQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODc0NzkxNw==", "bodyText": "Why did you choose to include the newlines?", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r558747917", "createdAt": "2021-01-16T01:38:29Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeIntoTable.java", "diffHunk": "@@ -0,0 +1,267 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.PARQUET_VECTORIZATION_ENABLED;\n+\n+public class TestMergeIntoTable extends SparkRowLevelOperationsTestBase {\n+  private final String sourceName;\n+  private final String targetName;\n+\n+  public TestMergeIntoTable(String catalogName, String implementation, Map<String, String> config,\n+                            String fileFormat, Boolean vectorized) {\n+    super(catalogName, implementation, config, fileFormat, vectorized);\n+    this.sourceName = tableName(\"source\");\n+    this.targetName = tableName(\"target\");\n+  }\n+\n+  @BeforeClass\n+  public static void setupSparkConf() {\n+    spark.conf().set(\"spark.sql.shuffle.partitions\", \"4\");\n+  }\n+\n+  protected Map<String, String> extraTableProperties() {\n+    return ImmutableMap.of(TableProperties.DELETE_MODE, \"copy-on-write\");\n+  }\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", targetName);\n+    sql(\"DROP TABLE IF EXISTS %s\", sourceName);\n+  }\n+\n+  @Test\n+  public void testEmptyTargetInsertAllNonMatchingRows() throws NoSuchTableException {\n+    createAndInitUnPartitionedTargetTable(targetName);\n+    createAndInitSourceTable(sourceName);\n+    append(sourceName, new Employee(1, \"emp-id-1\"), new Employee(2, \"emp-id-2\"), new Employee(3, \"emp-id-3\"));\n+    String sqlText = \"MERGE INTO \" + targetName + \" AS target \\n\" +\n+                     \"USING \" + sourceName + \" AS source \\n\" +\n+                     \"ON target.id = source.id \\n\" +\n+                     \"WHEN NOT MATCHED THEN INSERT * \";\n+\n+    sql(sqlText, \"\");\n+    sql(\"SELECT * FROM %s ORDER BY id, dep\", targetName);\n+    assertEquals(\"Should have expected rows\",\n+            ImmutableList.of(row(1, \"emp-id-1\"), row(2, \"emp-id-2\"), row(3, \"emp-id-3\")),\n+            sql(\"SELECT * FROM %s ORDER BY id ASC NULLS LAST\", targetName));\n+  }\n+\n+  @Test\n+  public void testEmptyTargetInsertOnlyMatchingRows() throws NoSuchTableException {\n+    createAndInitUnPartitionedTargetTable(targetName);\n+    createAndInitSourceTable(sourceName);\n+    append(sourceName, new Employee(1, \"emp-id-1\"), new Employee(2, \"emp-id-2\"), new Employee(3, \"emp-id-3\"));\n+    String sqlText = \"MERGE INTO \" + targetName + \" AS target \\n\" +\n+                     \"USING \" + sourceName + \" AS source \\n\" +\n+                     \"ON target.id = source.id \\n\" +\n+                     \"WHEN NOT MATCHED AND (source.id >= 2) THEN INSERT * \";\n+\n+    sql(sqlText, \"\");\n+    List<Object[]> res = sql(\"SELECT * FROM %s ORDER BY id, dep\", targetName);\n+    assertEquals(\"Should have expected rows\",\n+            ImmutableList.of(row(2, \"emp-id-2\"), row(3, \"emp-id-3\")),\n+            sql(\"SELECT * FROM %s ORDER BY id ASC NULLS LAST\", targetName));\n+  }\n+\n+  @Test\n+  public void testOnlyUpdate() throws NoSuchTableException {\n+    createAndInitUnPartitionedTargetTable(targetName);\n+    createAndInitSourceTable(sourceName);\n+    append(targetName, new Employee(1, \"emp-id-one\"), new Employee(6, \"emp-id-6\"));\n+    append(sourceName, new Employee(2, \"emp-id-2\"), new Employee(1, \"emp-id-1\"), new Employee(6, \"emp-id-6\"));\n+    String sqlText = \"MERGE INTO \" + targetName + \" AS target \\n\" +\n+            \"USING \" + sourceName + \" AS source \\n\" +\n+            \"ON target.id = source.id \\n\" +\n+            \"WHEN MATCHED AND target.id = 1 THEN UPDATE SET * \";\n+\n+    sql(sqlText, \"\");\n+    List<Object[]> res = sql(\"SELECT * FROM %s ORDER BY id, dep\", targetName);\n+    assertEquals(\"Should have expected rows\",\n+            ImmutableList.of(row(1, \"emp-id-1\"), row(6, \"emp-id-6\")),\n+            sql(\"SELECT * FROM %s ORDER BY id ASC NULLS LAST\", targetName));\n+  }\n+\n+  @Test\n+  public void testOnlyDelete() throws NoSuchTableException {\n+    createAndInitUnPartitionedTargetTable(targetName);\n+    createAndInitSourceTable(sourceName);\n+    append(targetName, new Employee(1, \"emp-id-one\"), new Employee(6, \"emp-id-6\"));\n+    append(sourceName, new Employee(2, \"emp-id-2\"), new Employee(1, \"emp-id-1\"), new Employee(6, \"emp-id-6\"));\n+    String sqlText = \"MERGE INTO \" + targetName + \" AS target \\n\" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 124}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9db80c8f340b2e9c3e11f19a0a687d411c8dbc29", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/9db80c8f340b2e9c3e11f19a0a687d411c8dbc29", "committedDate": "2021-01-17T08:17:29Z", "message": "Spark MERGE INTO Support (copy-on-write implementation"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d82afba8da1da306791e82969c12fb6da0b2f0de", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/d82afba8da1da306791e82969c12fb6da0b2f0de", "committedDate": "2021-01-17T08:17:29Z", "message": "Rebase + Scalastyle + cleancompile"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "35f68137c102c9609754c83b8a11a462fd5a8e1f", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/35f68137c102c9609754c83b8a11a462fd5a8e1f", "committedDate": "2021-01-17T08:17:29Z", "message": "Code review + base infrastructure"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/946bbded4eb82639a9e3db7fb90172dab827508c", "committedDate": "2021-01-12T05:34:17Z", "message": "Code review + base infrastructure"}, "afterCommit": {"oid": "c92a2d8e3854ee25d02c87477f33932aa4b6401e", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/c92a2d8e3854ee25d02c87477f33932aa4b6401e", "committedDate": "2021-01-17T08:17:29Z", "message": "Code review comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9cb2e86f1962fc02b65065253f5ab3c3c18ade09", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/9cb2e86f1962fc02b65065253f5ab3c3c18ade09", "committedDate": "2021-01-17T08:46:12Z", "message": "Code review comments (Round-2)"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "c92a2d8e3854ee25d02c87477f33932aa4b6401e", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/c92a2d8e3854ee25d02c87477f33932aa4b6401e", "committedDate": "2021-01-17T08:17:29Z", "message": "Code review comments"}, "afterCommit": {"oid": "9cb2e86f1962fc02b65065253f5ab3c3c18ade09", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/9cb2e86f1962fc02b65065253f5ab3c3c18ade09", "committedDate": "2021-01-17T08:46:12Z", "message": "Code review comments (Round-2)"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTcwODEwODQy", "url": "https://github.com/apache/iceberg/pull/1947#pullrequestreview-570810842", "createdAt": "2021-01-18T22:46:52Z", "commit": {"oid": "9cb2e86f1962fc02b65065253f5ab3c3c18ade09"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQyMjo0Njo1MlrOIV4-sA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQyMjo0Njo1MlrOIV4-sA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTgyNDU2MA==", "bodyText": "@dilipbiswal, this extraction is already done in the pushFilters method that @aokolnychyi implemented for delete. That's one reason why this also passes down target.output. The filters that are pushed down are the ones that only reference those attributes:\n    val tableAttrSet = AttributeSet(tableAttrs)\n    val predicates = splitConjunctivePredicates(cond).filter(_.references.subsetOf(tableAttrSet))\n    if (predicates.nonEmpty) {\n      val normalizedPredicates = DataSourceStrategy.normalizeExprs(predicates, tableAttrs)\n      PushDownUtils.pushFilters(scanBuilder, normalizedPredicates)\n    }", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559824560", "createdAt": "2021-01-18T22:46:52Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteMergeInto.scala", "diffHunk": "@@ -0,0 +1,117 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import org.apache.spark.sql.catalyst.analysis.Resolver\n+import org.apache.spark.sql.catalyst.expressions.Alias\n+import org.apache.spark.sql.catalyst.expressions.Expression\n+import org.apache.spark.sql.catalyst.expressions.InputFileName\n+import org.apache.spark.sql.catalyst.expressions.IsNull\n+import org.apache.spark.sql.catalyst.expressions.Literal\n+import org.apache.spark.sql.catalyst.plans.FullOuter\n+import org.apache.spark.sql.catalyst.plans.Inner\n+import org.apache.spark.sql.catalyst.plans.logical.DeleteAction\n+import org.apache.spark.sql.catalyst.plans.logical.InsertAction\n+import org.apache.spark.sql.catalyst.plans.logical.Join\n+import org.apache.spark.sql.catalyst.plans.logical.JoinHint\n+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n+import org.apache.spark.sql.catalyst.plans.logical.MergeAction\n+import org.apache.spark.sql.catalyst.plans.logical.MergeInto\n+import org.apache.spark.sql.catalyst.plans.logical.MergeIntoParams\n+import org.apache.spark.sql.catalyst.plans.logical.MergeIntoTable\n+import org.apache.spark.sql.catalyst.plans.logical.Project\n+import org.apache.spark.sql.catalyst.plans.logical.ReplaceData\n+import org.apache.spark.sql.catalyst.plans.logical.UpdateAction\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.catalyst.utils.RewriteRowLevelOperationHelper\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanRelation\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.BooleanType\n+\n+case class RewriteMergeInto(conf: SQLConf) extends Rule[LogicalPlan] with RewriteRowLevelOperationHelper  {\n+  val ROW_FROM_SOURCE = \"_row_from_source_\"\n+  val ROW_FROM_TARGET = \"_row_from_target_\"\n+  private val TRUE_LITERAL = Literal(true, BooleanType)\n+  private val FALSE_LITERAL = Literal(false, BooleanType)\n+\n+  import org.apache.spark.sql.execution.datasources.v2.ExtendedDataSourceV2Implicits._\n+\n+  override def resolver: Resolver = conf.resolver\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = {\n+    plan resolveOperators {\n+      case MergeIntoTable(target: DataSourceV2Relation, source: LogicalPlan, cond, matchedActions, notMatchedActions) =>\n+        val targetOutputCols = target.output\n+        val newProjectCols = target.output ++ Seq(Alias(InputFileName(), FILE_NAME_COL)())\n+        val newTargetTable = Project(newProjectCols, target)\n+\n+        // Construct the plan to prune target based on join condition between source and\n+        // target.\n+        val writeInfo = newWriteInfo(target.schema)\n+        val mergeBuilder = target.table.asMergeable.newMergeBuilder(\"merge\", writeInfo)\n+        val matchingRowsPlanBuilder = (_: DataSourceV2ScanRelation) =>\n+          Join(source, newTargetTable, Inner, Some(cond), JoinHint.NONE)\n+        // TODO - extract the local predicates that references the target from the join condition and\n+        // pass to buildScanPlan to ensure push-down.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9cb2e86f1962fc02b65065253f5ab3c3c18ade09"}, "originalPosition": 73}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTcwODExNTYz", "url": "https://github.com/apache/iceberg/pull/1947#pullrequestreview-570811563", "createdAt": "2021-01-18T22:49:26Z", "commit": {"oid": "9cb2e86f1962fc02b65065253f5ab3c3c18ade09"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQyMjo0OToyNlrOIV5Bng==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQyMjo0OToyNlrOIV5Bng==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTgyNTMxMA==", "bodyText": "This file is no longer used, so it can be removed.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559825310", "createdAt": "2021-01-18T22:49:26Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/utils/PlanHelper.scala", "diffHunk": "@@ -0,0 +1,87 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.spark.sql.catalyst.utils\n+\n+import java.util.UUID\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, AttributeReference, PredicateHelper}\n+import org.apache.spark.sql.catalyst.plans.logical.{Aggregate, DynamicFileFilter, LogicalPlan}\n+import org.apache.spark.sql.connector.catalog.Table\n+import org.apache.spark.sql.connector.iceberg.read.SupportsFileFilter\n+import org.apache.spark.sql.connector.iceberg.write.MergeBuilder\n+import org.apache.spark.sql.connector.write.{LogicalWriteInfo, LogicalWriteInfoImpl}\n+import org.apache.spark.sql.execution.datasources.v2.{DataSourceV2ScanRelation}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+trait PlanHelper extends PredicateHelper {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9cb2e86f1962fc02b65065253f5ab3c3c18ade09"}, "originalPosition": 34}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTcwODEyMjg1", "url": "https://github.com/apache/iceberg/pull/1947#pullrequestreview-570812285", "createdAt": "2021-01-18T22:52:06Z", "commit": {"oid": "9cb2e86f1962fc02b65065253f5ab3c3c18ade09"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQyMjo1MjowNlrOIV5EPg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQyMjo1MjowNlrOIV5EPg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTgyNTk4Mg==", "bodyText": "This can use TRUE_LITERAL.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559825982", "createdAt": "2021-01-18T22:52:06Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteMergeInto.scala", "diffHunk": "@@ -0,0 +1,117 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import org.apache.spark.sql.catalyst.analysis.Resolver\n+import org.apache.spark.sql.catalyst.expressions.Alias\n+import org.apache.spark.sql.catalyst.expressions.Expression\n+import org.apache.spark.sql.catalyst.expressions.InputFileName\n+import org.apache.spark.sql.catalyst.expressions.IsNull\n+import org.apache.spark.sql.catalyst.expressions.Literal\n+import org.apache.spark.sql.catalyst.plans.FullOuter\n+import org.apache.spark.sql.catalyst.plans.Inner\n+import org.apache.spark.sql.catalyst.plans.logical.DeleteAction\n+import org.apache.spark.sql.catalyst.plans.logical.InsertAction\n+import org.apache.spark.sql.catalyst.plans.logical.Join\n+import org.apache.spark.sql.catalyst.plans.logical.JoinHint\n+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n+import org.apache.spark.sql.catalyst.plans.logical.MergeAction\n+import org.apache.spark.sql.catalyst.plans.logical.MergeInto\n+import org.apache.spark.sql.catalyst.plans.logical.MergeIntoParams\n+import org.apache.spark.sql.catalyst.plans.logical.MergeIntoTable\n+import org.apache.spark.sql.catalyst.plans.logical.Project\n+import org.apache.spark.sql.catalyst.plans.logical.ReplaceData\n+import org.apache.spark.sql.catalyst.plans.logical.UpdateAction\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.catalyst.utils.RewriteRowLevelOperationHelper\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanRelation\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.BooleanType\n+\n+case class RewriteMergeInto(conf: SQLConf) extends Rule[LogicalPlan] with RewriteRowLevelOperationHelper  {\n+  val ROW_FROM_SOURCE = \"_row_from_source_\"\n+  val ROW_FROM_TARGET = \"_row_from_target_\"\n+  private val TRUE_LITERAL = Literal(true, BooleanType)\n+  private val FALSE_LITERAL = Literal(false, BooleanType)\n+\n+  import org.apache.spark.sql.execution.datasources.v2.ExtendedDataSourceV2Implicits._\n+\n+  override def resolver: Resolver = conf.resolver\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = {\n+    plan resolveOperators {\n+      case MergeIntoTable(target: DataSourceV2Relation, source: LogicalPlan, cond, matchedActions, notMatchedActions) =>\n+        val targetOutputCols = target.output\n+        val newProjectCols = target.output ++ Seq(Alias(InputFileName(), FILE_NAME_COL)())\n+        val newTargetTable = Project(newProjectCols, target)\n+\n+        // Construct the plan to prune target based on join condition between source and\n+        // target.\n+        val writeInfo = newWriteInfo(target.schema)\n+        val mergeBuilder = target.table.asMergeable.newMergeBuilder(\"merge\", writeInfo)\n+        val matchingRowsPlanBuilder = (_: DataSourceV2ScanRelation) =>\n+          Join(source, newTargetTable, Inner, Some(cond), JoinHint.NONE)\n+        // TODO - extract the local predicates that references the target from the join condition and\n+        // pass to buildScanPlan to ensure push-down.\n+        val targetTableScan = buildScanPlan(target.table, target.output, mergeBuilder, None, matchingRowsPlanBuilder)\n+\n+        // Construct an outer join to help track changes in source and target.\n+        // TODO : Optimize this to use LEFT ANTI or RIGHT OUTER when applicable.\n+        val sourceTableProj = source.output ++ Seq(Alias(TRUE_LITERAL, ROW_FROM_SOURCE)())\n+        val targetTableProj = target.output ++ Seq(Alias(TRUE_LITERAL, ROW_FROM_TARGET)())\n+        val newTargetTableScan = Project(targetTableProj, targetTableScan)\n+        val newSourceTableScan = Project(sourceTableProj, source)\n+        val joinPlan = Join(newSourceTableScan, newTargetTableScan, FullOuter, Some(cond), JoinHint.NONE)\n+\n+        // Construct the plan to replace the data based on the output of `MergeInto`\n+        val mergeParams = MergeIntoParams(\n+          isSourceRowNotPresent = IsNull(findOutputAttr(joinPlan, ROW_FROM_SOURCE)),\n+          isTargetRowNotPresent = IsNull(findOutputAttr(joinPlan, ROW_FROM_TARGET)),\n+          matchedConditions = matchedActions.map(getClauseCondition),\n+          matchedOutputs = matchedActions.map(actionOutput(_, targetOutputCols)),\n+          notMatchedConditions = notMatchedActions.map(getClauseCondition),\n+          notMatchedOutputs = notMatchedActions.map(actionOutput(_, targetOutputCols)),\n+          targetOutput = targetOutputCols :+ FALSE_LITERAL,\n+          deleteOutput = targetOutputCols :+ TRUE_LITERAL,\n+          joinedAttributes = joinPlan.output\n+        )\n+        val mergePlan = MergeInto(mergeParams, target, joinPlan)\n+        val batchWrite = mergeBuilder.asWriteBuilder.buildForBatch()\n+        ReplaceData(target, batchWrite, mergePlan)\n+    }\n+  }\n+\n+  private def actionOutput(clause: MergeAction, targetOutputCols: Seq[Expression]): Seq[Expression] = {\n+    clause match {\n+      case u: UpdateAction =>\n+        u.assignments.map(_.value) :+ FALSE_LITERAL\n+      case _: DeleteAction =>\n+        targetOutputCols :+ TRUE_LITERAL\n+      case i: InsertAction =>\n+        i.assignments.map(_.value) :+ FALSE_LITERAL\n+    }\n+  }\n+\n+  private def getClauseCondition(clause: MergeAction): Expression = {\n+    clause.condition.getOrElse(Literal(true))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9cb2e86f1962fc02b65065253f5ab3c3c18ade09"}, "originalPosition": 114}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "227a1081dfc460840c0c611a01d6eb5fed9de15f", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/227a1081dfc460840c0c611a01d6eb5fed9de15f", "committedDate": "2021-01-18T22:58:47Z", "message": "Missed code review comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9f264b752cb6a1c81133985d139f3cdb4b42b008", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/9f264b752cb6a1c81133985d139f3cdb4b42b008", "committedDate": "2021-01-18T23:03:17Z", "message": "More review"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTcwODE3MTg3", "url": "https://github.com/apache/iceberg/pull/1947#pullrequestreview-570817187", "createdAt": "2021-01-18T23:11:02Z", "commit": {"oid": "227a1081dfc460840c0c611a01d6eb5fed9de15f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQyMzoxMTowM1rOIV5W-A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQyMzoxMTowM1rOIV5W-A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTgzMDc3Ng==", "bodyText": "This newline isn't needed. Lines up to 120 characters are allowed.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559830776", "createdAt": "2021-01-18T23:11:03Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteMergeInto.scala", "diffHunk": "@@ -0,0 +1,117 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import org.apache.spark.sql.catalyst.analysis.Resolver\n+import org.apache.spark.sql.catalyst.expressions.Alias\n+import org.apache.spark.sql.catalyst.expressions.Expression\n+import org.apache.spark.sql.catalyst.expressions.InputFileName\n+import org.apache.spark.sql.catalyst.expressions.IsNull\n+import org.apache.spark.sql.catalyst.expressions.Literal\n+import org.apache.spark.sql.catalyst.plans.FullOuter\n+import org.apache.spark.sql.catalyst.plans.Inner\n+import org.apache.spark.sql.catalyst.plans.logical.DeleteAction\n+import org.apache.spark.sql.catalyst.plans.logical.InsertAction\n+import org.apache.spark.sql.catalyst.plans.logical.Join\n+import org.apache.spark.sql.catalyst.plans.logical.JoinHint\n+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n+import org.apache.spark.sql.catalyst.plans.logical.MergeAction\n+import org.apache.spark.sql.catalyst.plans.logical.MergeInto\n+import org.apache.spark.sql.catalyst.plans.logical.MergeIntoParams\n+import org.apache.spark.sql.catalyst.plans.logical.MergeIntoTable\n+import org.apache.spark.sql.catalyst.plans.logical.Project\n+import org.apache.spark.sql.catalyst.plans.logical.ReplaceData\n+import org.apache.spark.sql.catalyst.plans.logical.UpdateAction\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.catalyst.utils.RewriteRowLevelOperationHelper\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanRelation\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.BooleanType\n+\n+case class RewriteMergeInto(conf: SQLConf) extends Rule[LogicalPlan] with RewriteRowLevelOperationHelper  {\n+  private val ROW_FROM_SOURCE = \"_row_from_source_\"\n+  private val ROW_FROM_TARGET = \"_row_from_target_\"\n+  private val TRUE_LITERAL = Literal(true, BooleanType)\n+  private val FALSE_LITERAL = Literal(false, BooleanType)\n+\n+  import org.apache.spark.sql.execution.datasources.v2.ExtendedDataSourceV2Implicits._\n+\n+  override def resolver: Resolver = conf.resolver\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = {\n+    plan resolveOperators {\n+      case MergeIntoTable(target: DataSourceV2Relation, source: LogicalPlan, cond, matchedActions, notMatchedActions) =>\n+        val targetOutputCols = target.output\n+        val newProjectCols = target.output ++ Seq(Alias(InputFileName(), FILE_NAME_COL)())\n+        val newTargetTable = Project(newProjectCols, target)\n+\n+        // Construct the plan to prune target based on join condition between source and\n+        // target.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "227a1081dfc460840c0c611a01d6eb5fed9de15f"}, "originalPosition": 67}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTcwODE3MTk5", "url": "https://github.com/apache/iceberg/pull/1947#pullrequestreview-570817199", "createdAt": "2021-01-18T23:11:06Z", "commit": {"oid": "227a1081dfc460840c0c611a01d6eb5fed9de15f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQyMzoxMTowNlrOIV5XAw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQyMzoxMTowNlrOIV5XAw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTgzMDc4Nw==", "bodyText": "This plan must use the v2 scan relation. Otherwise, the _file column is not projected. This should be:\n    val matchingRowsPlanBuilder = (rel: DataSourceV2ScanRelation) =>\n        Join(source, rel, Inner, Some(cond), JoinHint.NONE)\nThen you can also remove newProjectCols and newTargetTable.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559830787", "createdAt": "2021-01-18T23:11:06Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteMergeInto.scala", "diffHunk": "@@ -0,0 +1,117 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import org.apache.spark.sql.catalyst.analysis.Resolver\n+import org.apache.spark.sql.catalyst.expressions.Alias\n+import org.apache.spark.sql.catalyst.expressions.Expression\n+import org.apache.spark.sql.catalyst.expressions.InputFileName\n+import org.apache.spark.sql.catalyst.expressions.IsNull\n+import org.apache.spark.sql.catalyst.expressions.Literal\n+import org.apache.spark.sql.catalyst.plans.FullOuter\n+import org.apache.spark.sql.catalyst.plans.Inner\n+import org.apache.spark.sql.catalyst.plans.logical.DeleteAction\n+import org.apache.spark.sql.catalyst.plans.logical.InsertAction\n+import org.apache.spark.sql.catalyst.plans.logical.Join\n+import org.apache.spark.sql.catalyst.plans.logical.JoinHint\n+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n+import org.apache.spark.sql.catalyst.plans.logical.MergeAction\n+import org.apache.spark.sql.catalyst.plans.logical.MergeInto\n+import org.apache.spark.sql.catalyst.plans.logical.MergeIntoParams\n+import org.apache.spark.sql.catalyst.plans.logical.MergeIntoTable\n+import org.apache.spark.sql.catalyst.plans.logical.Project\n+import org.apache.spark.sql.catalyst.plans.logical.ReplaceData\n+import org.apache.spark.sql.catalyst.plans.logical.UpdateAction\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.catalyst.utils.RewriteRowLevelOperationHelper\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanRelation\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.BooleanType\n+\n+case class RewriteMergeInto(conf: SQLConf) extends Rule[LogicalPlan] with RewriteRowLevelOperationHelper  {\n+  private val ROW_FROM_SOURCE = \"_row_from_source_\"\n+  private val ROW_FROM_TARGET = \"_row_from_target_\"\n+  private val TRUE_LITERAL = Literal(true, BooleanType)\n+  private val FALSE_LITERAL = Literal(false, BooleanType)\n+\n+  import org.apache.spark.sql.execution.datasources.v2.ExtendedDataSourceV2Implicits._\n+\n+  override def resolver: Resolver = conf.resolver\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = {\n+    plan resolveOperators {\n+      case MergeIntoTable(target: DataSourceV2Relation, source: LogicalPlan, cond, matchedActions, notMatchedActions) =>\n+        val targetOutputCols = target.output\n+        val newProjectCols = target.output ++ Seq(Alias(InputFileName(), FILE_NAME_COL)())\n+        val newTargetTable = Project(newProjectCols, target)\n+\n+        // Construct the plan to prune target based on join condition between source and\n+        // target.\n+        val writeInfo = newWriteInfo(target.schema)\n+        val mergeBuilder = target.table.asMergeable.newMergeBuilder(\"merge\", writeInfo)\n+        val matchingRowsPlanBuilder = (_: DataSourceV2ScanRelation) =>\n+          Join(source, newTargetTable, Inner, Some(cond), JoinHint.NONE)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "227a1081dfc460840c0c611a01d6eb5fed9de15f"}, "originalPosition": 71}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTcwODE4MDU3", "url": "https://github.com/apache/iceberg/pull/1947#pullrequestreview-570818057", "createdAt": "2021-01-18T23:14:48Z", "commit": {"oid": "227a1081dfc460840c0c611a01d6eb5fed9de15f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQyMzoxNDo0OVrOIV5Zxw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQyMzoxNDo0OVrOIV5Zxw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTgzMTQ5NQ==", "bodyText": "You can solve this problem by passing the target table attrs from the DataSourceV2ScanRelation:\n    val matchingFilePlan = buildFileFilterPlan(scanRelation.output, matchingRowsPlanBuilder(scanRelation))\n  ...\n\n  private def buildFileFilterPlan(tableAttrs: Seq[AttributeReference], matchingRowsPlan: LogicalPlan): LogicalPlan = {\n    val fileAttr = findOutputAttr(tableAttrs, FILE_NAME_COL)\n    val agg = Aggregate(Seq(fileAttr), Seq(fileAttr), matchingRowsPlan)\n    Project(Seq(findOutputAttr(agg.output, FILE_NAME_COL)), agg)\n  }\n\n  protected def findOutputAttr(attrs: Seq[Attribute], attrName: String): Attribute = {\n    attrs.find(attr => resolver(attr.name, attrName)).getOrElse {\n      throw new AnalysisException(s\"Cannot find $attrName in $attrs\")\n    }\n  }", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559831495", "createdAt": "2021-01-18T23:14:49Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/utils/RewriteRowLevelOperationHelper.scala", "diffHunk": "@@ -103,6 +103,7 @@ trait RewriteRowLevelOperationHelper extends PredicateHelper with Logging {\n   }\n \n   private def buildFileFilterPlan(matchingRowsPlan: LogicalPlan): LogicalPlan = {\n+    // TODO: For merge-into make sure _file is resolved only from target table.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "227a1081dfc460840c0c611a01d6eb5fed9de15f"}, "originalPosition": 19}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTcwODE4MTgw", "url": "https://github.com/apache/iceberg/pull/1947#pullrequestreview-570818180", "createdAt": "2021-01-18T23:15:18Z", "commit": {"oid": "227a1081dfc460840c0c611a01d6eb5fed9de15f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQyMzoxNToxOFrOIV5aPw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQyMzoxNToxOFrOIV5aPw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTgzMTYxNQ==", "bodyText": "I don't think this change is needed because Anton's update already extracts the correct filters from cond.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559831615", "createdAt": "2021-01-18T23:15:18Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/utils/RewriteRowLevelOperationHelper.scala", "diffHunk": "@@ -54,12 +54,12 @@ trait RewriteRowLevelOperationHelper extends PredicateHelper with Logging {\n       table: Table,\n       tableAttrs: Seq[AttributeReference],\n       mergeBuilder: MergeBuilder,\n-      cond: Expression,\n+      cond: Option[Expression] = None,\n       matchingRowsPlanBuilder: DataSourceV2ScanRelation => LogicalPlan): LogicalPlan = {\n \n     val scanBuilder = mergeBuilder.asScanBuilder\n \n-    pushFilters(scanBuilder, cond, tableAttrs)\n+    cond.map(pushFilters(scanBuilder, _, tableAttrs))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "227a1081dfc460840c0c611a01d6eb5fed9de15f"}, "originalPosition": 11}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTcwODE4NTU3", "url": "https://github.com/apache/iceberg/pull/1947#pullrequestreview-570818557", "createdAt": "2021-01-18T23:16:47Z", "commit": {"oid": "227a1081dfc460840c0c611a01d6eb5fed9de15f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQyMzoxNjo0OFrOIV5bsw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQyMzoxNjo0OFrOIV5bsw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTgzMTk4Nw==", "bodyText": "Style: this is a doc comment because it starts with /**. Usually, multi-line comments in code would use //   on each line.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559831987", "createdAt": "2021-01-18T23:16:48Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/MergeIntoExec.scala", "diffHunk": "@@ -0,0 +1,139 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.catalyst.expressions.BasePredicate\n+import org.apache.spark.sql.catalyst.expressions.Expression\n+import org.apache.spark.sql.catalyst.expressions.UnsafeProjection\n+import org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate\n+import org.apache.spark.sql.catalyst.plans.logical.MergeIntoParams\n+import org.apache.spark.sql.execution.SparkPlan\n+import org.apache.spark.sql.execution.UnaryExecNode\n+\n+case class MergeIntoExec(\n+    mergeIntoParams: MergeIntoParams,\n+    @transient targetRelation: DataSourceV2Relation,\n+    override val child: SparkPlan) extends UnaryExecNode {\n+\n+  override def output: Seq[Attribute] = targetRelation.output\n+\n+  protected override def doExecute(): RDD[InternalRow] = {\n+    child.execute().mapPartitions {\n+      processPartition(mergeIntoParams, _)\n+    }\n+  }\n+\n+  private def generateProjection(exprs: Seq[Expression], attrs: Seq[Attribute]): UnsafeProjection = {\n+    UnsafeProjection.create(exprs, attrs)\n+  }\n+\n+  private def generatePredicate(expr: Expression, attrs: Seq[Attribute]): BasePredicate = {\n+    GeneratePredicate.generate(expr, attrs)\n+  }\n+\n+  def applyProjection(\n+     actions: Seq[(BasePredicate, UnsafeProjection)],\n+     projectTargetCols: UnsafeProjection,\n+     projectDeleteRow: UnsafeProjection,\n+     inputRow: InternalRow,\n+     targetRowNotPresent: Boolean): InternalRow = {\n+\n+    /**\n+     * Find the first combination where the predicate evaluates to true.\n+     * In case when there are overlapping condition in the MATCHED\n+     * clauses, for the first one that satisfies the predicate, the\n+     * corresponding action is applied. For example:\n+     *\n+     * WHEN MATCHED AND id > 1 AND id < 10 UPDATE *\n+     * WHEN MATCHED AND id = 5 OR id = 21 DELETE\n+     *\n+     * In above case, when id = 5, it applies both that matched predicates. In this\n+     * case the first one we see is applied.\n+     */", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "227a1081dfc460840c0c611a01d6eb5fed9de15f"}, "originalPosition": 72}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTcwODIwNjAx", "url": "https://github.com/apache/iceberg/pull/1947#pullrequestreview-570820601", "createdAt": "2021-01-18T23:25:32Z", "commit": {"oid": "227a1081dfc460840c0c611a01d6eb5fed9de15f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQyMzoyNTozMlrOIV5jbw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQyMzoyNTozMlrOIV5jbw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTgzMzk2Nw==", "bodyText": "Nit: could use filterNot(shouldDeleteRow) instead.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559833967", "createdAt": "2021-01-18T23:25:32Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/MergeIntoExec.scala", "diffHunk": "@@ -0,0 +1,139 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.catalyst.expressions.BasePredicate\n+import org.apache.spark.sql.catalyst.expressions.Expression\n+import org.apache.spark.sql.catalyst.expressions.UnsafeProjection\n+import org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate\n+import org.apache.spark.sql.catalyst.plans.logical.MergeIntoParams\n+import org.apache.spark.sql.execution.SparkPlan\n+import org.apache.spark.sql.execution.UnaryExecNode\n+\n+case class MergeIntoExec(\n+    mergeIntoParams: MergeIntoParams,\n+    @transient targetRelation: DataSourceV2Relation,\n+    override val child: SparkPlan) extends UnaryExecNode {\n+\n+  override def output: Seq[Attribute] = targetRelation.output\n+\n+  protected override def doExecute(): RDD[InternalRow] = {\n+    child.execute().mapPartitions {\n+      processPartition(mergeIntoParams, _)\n+    }\n+  }\n+\n+  private def generateProjection(exprs: Seq[Expression], attrs: Seq[Attribute]): UnsafeProjection = {\n+    UnsafeProjection.create(exprs, attrs)\n+  }\n+\n+  private def generatePredicate(expr: Expression, attrs: Seq[Attribute]): BasePredicate = {\n+    GeneratePredicate.generate(expr, attrs)\n+  }\n+\n+  def applyProjection(\n+     actions: Seq[(BasePredicate, UnsafeProjection)],\n+     projectTargetCols: UnsafeProjection,\n+     projectDeleteRow: UnsafeProjection,\n+     inputRow: InternalRow,\n+     targetRowNotPresent: Boolean): InternalRow = {\n+\n+    /**\n+     * Find the first combination where the predicate evaluates to true.\n+     * In case when there are overlapping condition in the MATCHED\n+     * clauses, for the first one that satisfies the predicate, the\n+     * corresponding action is applied. For example:\n+     *\n+     * WHEN MATCHED AND id > 1 AND id < 10 UPDATE *\n+     * WHEN MATCHED AND id = 5 OR id = 21 DELETE\n+     *\n+     * In above case, when id = 5, it applies both that matched predicates. In this\n+     * case the first one we see is applied.\n+     */\n+\n+    val pair = actions.find {\n+      case (predicate, _) => predicate.eval(inputRow)\n+    }\n+\n+    // Now apply the appropriate projection to either :\n+    // - Insert a row into target\n+    // - Update a row of target\n+    // - Delete a row in target. The projected row will have the deleted bit set.\n+    pair match {\n+      case Some((_, projection)) =>\n+        projection.apply(inputRow)\n+      case None =>\n+        if (targetRowNotPresent) {\n+          projectDeleteRow.apply(inputRow)\n+        } else {\n+          projectTargetCols.apply(inputRow)\n+        }\n+    }\n+  }\n+\n+  def processPartition(\n+     params: MergeIntoParams,\n+     rowIterator: Iterator[InternalRow]): Iterator[InternalRow] = {\n+\n+    val joinedAttrs = params.joinedAttributes\n+    val isSourceRowNotPresentPred = generatePredicate(params.isSourceRowNotPresent, joinedAttrs)\n+    val isTargetRowNotPresentPred = generatePredicate(params.isTargetRowNotPresent, joinedAttrs)\n+    val matchedPreds = params.matchedConditions.map(generatePredicate(_, joinedAttrs))\n+    val matchedProjs = params.matchedOutputs.map(generateProjection(_, joinedAttrs))\n+    val notMatchedPreds = params.notMatchedConditions.map(generatePredicate(_, joinedAttrs))\n+    val notMatchedProjs = params.notMatchedOutputs.map(generateProjection(_, joinedAttrs))\n+    val projectTargetCols = generateProjection(params.targetOutput, joinedAttrs)\n+    val projectDeletedRow = generateProjection(params.deleteOutput, joinedAttrs)\n+    val nonMatchedPairs =   notMatchedPreds zip notMatchedProjs\n+    val matchedPairs = matchedPreds zip matchedProjs\n+\n+    def shouldDeleteRow(row: InternalRow): Boolean =\n+      row.getBoolean(params.targetOutput.size - 1)\n+\n+\n+    /**\n+     * This method is responsible for processing a input row to emit the resultant row with an\n+     * additional column that indicates whether the row is going to be included in the final\n+     * output of merge or not.\n+     * 1. If there is a target row for which there is no corresponding source row (join condition not met)\n+     *    - Only project the target columns with deleted flag set to false.\n+     * 2. If there is a source row for which there is no corresponding target row (join condition not met)\n+     *    - Apply the not matched actions (i.e INSERT actions) if non match conditions are met.\n+     * 3. If there is a source row for which there is a corresponding target row (join condition met)\n+     *    - Apply the matched actions (i.e DELETE or UPDATE actions) if match conditions are met.\n+     */\n+    def processRow(inputRow: InternalRow): InternalRow = {\n+      if (isSourceRowNotPresentPred.eval(inputRow)) {\n+        projectTargetCols.apply(inputRow)\n+      } else if (isTargetRowNotPresentPred.eval(inputRow)) {\n+        applyProjection(nonMatchedPairs, projectTargetCols, projectDeletedRow, inputRow, true)\n+      } else {\n+        applyProjection(matchedPairs, projectTargetCols, projectDeletedRow, inputRow, false)\n+      }\n+    }\n+\n+    rowIterator\n+      .map(processRow)\n+      .filter(!shouldDeleteRow(_))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "227a1081dfc460840c0c611a01d6eb5fed9de15f"}, "originalPosition": 137}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTcwODIxMDM4", "url": "https://github.com/apache/iceberg/pull/1947#pullrequestreview-570821038", "createdAt": "2021-01-18T23:27:26Z", "commit": {"oid": "227a1081dfc460840c0c611a01d6eb5fed9de15f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQyMzoyNzoyNlrOIV5lVQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQyMzoyNzoyNlrOIV5lVQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTgzNDQ1Mw==", "bodyText": "This should be the merge equivalent.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559834453", "createdAt": "2021-01-18T23:27:26Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeIntoTable.java", "diffHunk": "@@ -0,0 +1,267 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.PARQUET_VECTORIZATION_ENABLED;\n+\n+public class TestMergeIntoTable extends SparkRowLevelOperationsTestBase {\n+  private final String sourceName;\n+  private final String targetName;\n+\n+  public TestMergeIntoTable(String catalogName, String implementation, Map<String, String> config,\n+                            String fileFormat, Boolean vectorized) {\n+    super(catalogName, implementation, config, fileFormat, vectorized);\n+    this.sourceName = tableName(\"source\");\n+    this.targetName = tableName(\"target\");\n+  }\n+\n+  @BeforeClass\n+  public static void setupSparkConf() {\n+    spark.conf().set(\"spark.sql.shuffle.partitions\", \"4\");\n+  }\n+\n+  protected Map<String, String> extraTableProperties() {\n+    return ImmutableMap.of(TableProperties.DELETE_MODE, \"copy-on-write\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "227a1081dfc460840c0c611a01d6eb5fed9de15f"}, "originalPosition": 57}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6f293356f830d1fe05056bfd820cc470b1de6110", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/6f293356f830d1fe05056bfd820cc470b1de6110", "committedDate": "2021-01-18T23:38:24Z", "message": "Review - contd."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTcwODIzOTcx", "url": "https://github.com/apache/iceberg/pull/1947#pullrequestreview-570823971", "createdAt": "2021-01-18T23:40:27Z", "commit": {"oid": "227a1081dfc460840c0c611a01d6eb5fed9de15f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQyMzo0MDoyN1rOIV5wzA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQyMzo0MDoyN1rOIV5wzA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTgzNzM4OA==", "bodyText": "I don't think that there is a need to test this with both Hive and Hadoop catalogs or with all 3 formats, since the main thing that needs to be tested is conversion and Spark behavior. Also, this doesn't test partitioned tables at all. To fix those, I think this should customize parameters:\n  @Parameterized.Parameters(\n      name = \"catalogName = {0}, implementation = {1}, config = {2}, format = {3}, vectorized = {4}, partitioned = {5}\")\n  public static Object[][] parameters() {\n    return new Object[][] {\n        { \"testhive\", SparkCatalog.class.getName(),\n            ImmutableMap.of(\n                \"type\", \"hive\",\n                \"default-namespace\", \"default\"\n            ),\n            \"parquet\",\n            true,\n            false\n        },\n        { \"spark_catalog\", SparkSessionCatalog.class.getName(),\n            ImmutableMap.of(\n                \"type\", \"hive\",\n                \"default-namespace\", \"default\",\n                \"clients\", \"1\",\n                \"parquet-enabled\", \"false\",\n                \"cache-enabled\", \"false\" // Spark will delete tables using v1, leaving the cache out of sync\n            ),\n            \"parquet\",\n            false,\n            true\n        }\n    };\n  }\n\n  public TestMergeIntoTable(String catalogName, String implementation, Map<String, String> config,\n                            String fileFormat, Boolean vectorized, Boolean partitioned) {\n    super(catalogName, implementation, config, fileFormat, vectorized);\n    this.partitioned = partitioned;\n    this.sourceName = tableName(\"source\");\n    this.targetName = tableName(\"target\");\n  }\nI also added a partitioned boolean and moved all of the createAndInit calls into a @Before:\n  @Before\n  public void createTables() {\n    if (partitioned) {\n      createAndInitPartitionedTargetTable(targetName);\n    } else {\n      createAndInitUnPartitionedTargetTable(targetName);\n    }\n    createAndInitSourceTable(sourceName);\n  }\nWith those changes, tests run faster and cover partitioned tables.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559837388", "createdAt": "2021-01-18T23:40:27Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeIntoTable.java", "diffHunk": "@@ -0,0 +1,267 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.PARQUET_VECTORIZATION_ENABLED;\n+\n+public class TestMergeIntoTable extends SparkRowLevelOperationsTestBase {\n+  private final String sourceName;\n+  private final String targetName;\n+\n+  public TestMergeIntoTable(String catalogName, String implementation, Map<String, String> config,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "227a1081dfc460840c0c611a01d6eb5fed9de15f"}, "originalPosition": 44}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTcwODI1MTU2", "url": "https://github.com/apache/iceberg/pull/1947#pullrequestreview-570825156", "createdAt": "2021-01-18T23:45:54Z", "commit": {"oid": "227a1081dfc460840c0c611a01d6eb5fed9de15f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQyMzo0NTo1NVrOIV51JQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQyMzo0NTo1NVrOIV51JQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTgzODUwMQ==", "bodyText": "Rather than embedding the names directly, you can pass them to the sql method, like removeTables does:\nsql(\"MERGE INTO %s AS target USING %s AS source ...\", targetName, sourceName);", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559838501", "createdAt": "2021-01-18T23:45:55Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeIntoTable.java", "diffHunk": "@@ -0,0 +1,267 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.PARQUET_VECTORIZATION_ENABLED;\n+\n+public class TestMergeIntoTable extends SparkRowLevelOperationsTestBase {\n+  private final String sourceName;\n+  private final String targetName;\n+\n+  public TestMergeIntoTable(String catalogName, String implementation, Map<String, String> config,\n+                            String fileFormat, Boolean vectorized) {\n+    super(catalogName, implementation, config, fileFormat, vectorized);\n+    this.sourceName = tableName(\"source\");\n+    this.targetName = tableName(\"target\");\n+  }\n+\n+  @BeforeClass\n+  public static void setupSparkConf() {\n+    spark.conf().set(\"spark.sql.shuffle.partitions\", \"4\");\n+  }\n+\n+  protected Map<String, String> extraTableProperties() {\n+    return ImmutableMap.of(TableProperties.DELETE_MODE, \"copy-on-write\");\n+  }\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", targetName);\n+    sql(\"DROP TABLE IF EXISTS %s\", sourceName);\n+  }\n+\n+  @Test\n+  public void testEmptyTargetInsertAllNonMatchingRows() throws NoSuchTableException {\n+    createAndInitUnPartitionedTargetTable(targetName);\n+    createAndInitSourceTable(sourceName);\n+    append(sourceName, new Employee(1, \"emp-id-1\"), new Employee(2, \"emp-id-2\"), new Employee(3, \"emp-id-3\"));\n+    String sqlText = \"MERGE INTO \" + targetName + \" AS target \" +\n+                     \"USING \" + sourceName + \" AS source \" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "227a1081dfc460840c0c611a01d6eb5fed9de15f"}, "originalPosition": 72}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTcwODI1Mzg1", "url": "https://github.com/apache/iceberg/pull/1947#pullrequestreview-570825385", "createdAt": "2021-01-18T23:46:58Z", "commit": {"oid": "227a1081dfc460840c0c611a01d6eb5fed9de15f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQyMzo0Njo1OFrOIV51-w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQyMzo0Njo1OFrOIV51-w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTgzODcxNQ==", "bodyText": "This select has no effect, can you remove it?", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559838715", "createdAt": "2021-01-18T23:46:58Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeIntoTable.java", "diffHunk": "@@ -0,0 +1,267 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.PARQUET_VECTORIZATION_ENABLED;\n+\n+public class TestMergeIntoTable extends SparkRowLevelOperationsTestBase {\n+  private final String sourceName;\n+  private final String targetName;\n+\n+  public TestMergeIntoTable(String catalogName, String implementation, Map<String, String> config,\n+                            String fileFormat, Boolean vectorized) {\n+    super(catalogName, implementation, config, fileFormat, vectorized);\n+    this.sourceName = tableName(\"source\");\n+    this.targetName = tableName(\"target\");\n+  }\n+\n+  @BeforeClass\n+  public static void setupSparkConf() {\n+    spark.conf().set(\"spark.sql.shuffle.partitions\", \"4\");\n+  }\n+\n+  protected Map<String, String> extraTableProperties() {\n+    return ImmutableMap.of(TableProperties.DELETE_MODE, \"copy-on-write\");\n+  }\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", targetName);\n+    sql(\"DROP TABLE IF EXISTS %s\", sourceName);\n+  }\n+\n+  @Test\n+  public void testEmptyTargetInsertAllNonMatchingRows() throws NoSuchTableException {\n+    createAndInitUnPartitionedTargetTable(targetName);\n+    createAndInitSourceTable(sourceName);\n+    append(sourceName, new Employee(1, \"emp-id-1\"), new Employee(2, \"emp-id-2\"), new Employee(3, \"emp-id-3\"));\n+    String sqlText = \"MERGE INTO \" + targetName + \" AS target \" +\n+                     \"USING \" + sourceName + \" AS source \" +\n+                     \"ON target.id = source.id \" +\n+                     \"WHEN NOT MATCHED THEN INSERT * \";\n+\n+    sql(sqlText);\n+    sql(\"SELECT * FROM %s ORDER BY id, dep\", targetName);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "227a1081dfc460840c0c611a01d6eb5fed9de15f"}, "originalPosition": 77}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTcwODI1NzAw", "url": "https://github.com/apache/iceberg/pull/1947#pullrequestreview-570825700", "createdAt": "2021-01-18T23:48:26Z", "commit": {"oid": "227a1081dfc460840c0c611a01d6eb5fed9de15f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQyMzo0ODoyNlrOIV53Zw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQyMzo0ODoyNlrOIV53Zw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTgzOTA3OQ==", "bodyText": "res is not used in this test or others. Can you remove this line?", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559839079", "createdAt": "2021-01-18T23:48:26Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeIntoTable.java", "diffHunk": "@@ -0,0 +1,267 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.PARQUET_VECTORIZATION_ENABLED;\n+\n+public class TestMergeIntoTable extends SparkRowLevelOperationsTestBase {\n+  private final String sourceName;\n+  private final String targetName;\n+\n+  public TestMergeIntoTable(String catalogName, String implementation, Map<String, String> config,\n+                            String fileFormat, Boolean vectorized) {\n+    super(catalogName, implementation, config, fileFormat, vectorized);\n+    this.sourceName = tableName(\"source\");\n+    this.targetName = tableName(\"target\");\n+  }\n+\n+  @BeforeClass\n+  public static void setupSparkConf() {\n+    spark.conf().set(\"spark.sql.shuffle.partitions\", \"4\");\n+  }\n+\n+  protected Map<String, String> extraTableProperties() {\n+    return ImmutableMap.of(TableProperties.DELETE_MODE, \"copy-on-write\");\n+  }\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", targetName);\n+    sql(\"DROP TABLE IF EXISTS %s\", sourceName);\n+  }\n+\n+  @Test\n+  public void testEmptyTargetInsertAllNonMatchingRows() throws NoSuchTableException {\n+    createAndInitUnPartitionedTargetTable(targetName);\n+    createAndInitSourceTable(sourceName);\n+    append(sourceName, new Employee(1, \"emp-id-1\"), new Employee(2, \"emp-id-2\"), new Employee(3, \"emp-id-3\"));\n+    String sqlText = \"MERGE INTO \" + targetName + \" AS target \" +\n+                     \"USING \" + sourceName + \" AS source \" +\n+                     \"ON target.id = source.id \" +\n+                     \"WHEN NOT MATCHED THEN INSERT * \";\n+\n+    sql(sqlText);\n+    sql(\"SELECT * FROM %s ORDER BY id, dep\", targetName);\n+    assertEquals(\"Should have expected rows\",\n+            ImmutableList.of(row(1, \"emp-id-1\"), row(2, \"emp-id-2\"), row(3, \"emp-id-3\")),\n+            sql(\"SELECT * FROM %s ORDER BY id ASC NULLS LAST\", targetName));\n+  }\n+\n+  @Test\n+  public void testEmptyTargetInsertOnlyMatchingRows() throws NoSuchTableException {\n+    createAndInitUnPartitionedTargetTable(targetName);\n+    createAndInitSourceTable(sourceName);\n+    append(sourceName, new Employee(1, \"emp-id-1\"), new Employee(2, \"emp-id-2\"), new Employee(3, \"emp-id-3\"));\n+    String sqlText = \"MERGE INTO \" + targetName + \" AS target \" +\n+                     \"USING \" + sourceName + \" AS source \" +\n+                     \"ON target.id = source.id \" +\n+                     \"WHEN NOT MATCHED AND (source.id >= 2) THEN INSERT * \";\n+\n+    sql(sqlText);\n+    List<Object[]> res = sql(\"SELECT * FROM %s ORDER BY id, dep\", targetName);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "227a1081dfc460840c0c611a01d6eb5fed9de15f"}, "originalPosition": 94}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTcwODI4MjYx", "url": "https://github.com/apache/iceberg/pull/1947#pullrequestreview-570828261", "createdAt": "2021-01-19T00:00:44Z", "commit": {"oid": "227a1081dfc460840c0c611a01d6eb5fed9de15f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOVQwMDowMDo0NFrOIV6BPQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOVQwMDowMDo0NFrOIV6BPQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTg0MTU5Nw==", "bodyText": "The employee dep is identical for both records with id 6, so the assertion can't distinguish between the case where employee 6 is replaced or not. Could you update the original target data to emp-id-six and assert that it is unchanged because of the target.id = 1 requirement?", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559841597", "createdAt": "2021-01-19T00:00:44Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeIntoTable.java", "diffHunk": "@@ -0,0 +1,267 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.PARQUET_VECTORIZATION_ENABLED;\n+\n+public class TestMergeIntoTable extends SparkRowLevelOperationsTestBase {\n+  private final String sourceName;\n+  private final String targetName;\n+\n+  public TestMergeIntoTable(String catalogName, String implementation, Map<String, String> config,\n+                            String fileFormat, Boolean vectorized) {\n+    super(catalogName, implementation, config, fileFormat, vectorized);\n+    this.sourceName = tableName(\"source\");\n+    this.targetName = tableName(\"target\");\n+  }\n+\n+  @BeforeClass\n+  public static void setupSparkConf() {\n+    spark.conf().set(\"spark.sql.shuffle.partitions\", \"4\");\n+  }\n+\n+  protected Map<String, String> extraTableProperties() {\n+    return ImmutableMap.of(TableProperties.DELETE_MODE, \"copy-on-write\");\n+  }\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", targetName);\n+    sql(\"DROP TABLE IF EXISTS %s\", sourceName);\n+  }\n+\n+  @Test\n+  public void testEmptyTargetInsertAllNonMatchingRows() throws NoSuchTableException {\n+    createAndInitUnPartitionedTargetTable(targetName);\n+    createAndInitSourceTable(sourceName);\n+    append(sourceName, new Employee(1, \"emp-id-1\"), new Employee(2, \"emp-id-2\"), new Employee(3, \"emp-id-3\"));\n+    String sqlText = \"MERGE INTO \" + targetName + \" AS target \" +\n+                     \"USING \" + sourceName + \" AS source \" +\n+                     \"ON target.id = source.id \" +\n+                     \"WHEN NOT MATCHED THEN INSERT * \";\n+\n+    sql(sqlText);\n+    sql(\"SELECT * FROM %s ORDER BY id, dep\", targetName);\n+    assertEquals(\"Should have expected rows\",\n+            ImmutableList.of(row(1, \"emp-id-1\"), row(2, \"emp-id-2\"), row(3, \"emp-id-3\")),\n+            sql(\"SELECT * FROM %s ORDER BY id ASC NULLS LAST\", targetName));\n+  }\n+\n+  @Test\n+  public void testEmptyTargetInsertOnlyMatchingRows() throws NoSuchTableException {\n+    createAndInitUnPartitionedTargetTable(targetName);\n+    createAndInitSourceTable(sourceName);\n+    append(sourceName, new Employee(1, \"emp-id-1\"), new Employee(2, \"emp-id-2\"), new Employee(3, \"emp-id-3\"));\n+    String sqlText = \"MERGE INTO \" + targetName + \" AS target \" +\n+                     \"USING \" + sourceName + \" AS source \" +\n+                     \"ON target.id = source.id \" +\n+                     \"WHEN NOT MATCHED AND (source.id >= 2) THEN INSERT * \";\n+\n+    sql(sqlText);\n+    List<Object[]> res = sql(\"SELECT * FROM %s ORDER BY id, dep\", targetName);\n+    assertEquals(\"Should have expected rows\",\n+            ImmutableList.of(row(2, \"emp-id-2\"), row(3, \"emp-id-3\")),\n+            sql(\"SELECT * FROM %s ORDER BY id ASC NULLS LAST\", targetName));\n+  }\n+\n+  @Test\n+  public void testOnlyUpdate() throws NoSuchTableException {\n+    createAndInitUnPartitionedTargetTable(targetName);\n+    createAndInitSourceTable(sourceName);\n+    append(targetName, new Employee(1, \"emp-id-one\"), new Employee(6, \"emp-id-6\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "227a1081dfc460840c0c611a01d6eb5fed9de15f"}, "originalPosition": 104}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTcwODI5NTg3", "url": "https://github.com/apache/iceberg/pull/1947#pullrequestreview-570829587", "createdAt": "2021-01-19T00:06:45Z", "commit": {"oid": "6f293356f830d1fe05056bfd820cc470b1de6110"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOVQwMDowNjo0NVrOIV6FtA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOVQwMDowNjo0NVrOIV6FtA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTg0Mjc0MA==", "bodyText": "Nit: extra newline.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559842740", "createdAt": "2021-01-19T00:06:45Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/MergeIntoExec.scala", "diffHunk": "@@ -0,0 +1,139 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.catalyst.expressions.BasePredicate\n+import org.apache.spark.sql.catalyst.expressions.Expression\n+import org.apache.spark.sql.catalyst.expressions.UnsafeProjection\n+import org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate\n+import org.apache.spark.sql.catalyst.plans.logical.MergeIntoParams\n+import org.apache.spark.sql.execution.SparkPlan\n+import org.apache.spark.sql.execution.UnaryExecNode\n+\n+case class MergeIntoExec(\n+    mergeIntoParams: MergeIntoParams,\n+    @transient targetRelation: DataSourceV2Relation,\n+    override val child: SparkPlan) extends UnaryExecNode {\n+\n+  override def output: Seq[Attribute] = targetRelation.output\n+\n+  protected override def doExecute(): RDD[InternalRow] = {\n+    child.execute().mapPartitions {\n+      processPartition(mergeIntoParams, _)\n+    }\n+  }\n+\n+  private def generateProjection(exprs: Seq[Expression], attrs: Seq[Attribute]): UnsafeProjection = {\n+    UnsafeProjection.create(exprs, attrs)\n+  }\n+\n+  private def generatePredicate(expr: Expression, attrs: Seq[Attribute]): BasePredicate = {\n+    GeneratePredicate.generate(expr, attrs)\n+  }\n+\n+  def applyProjection(\n+     actions: Seq[(BasePredicate, UnsafeProjection)],\n+     projectTargetCols: UnsafeProjection,\n+     projectDeleteRow: UnsafeProjection,\n+     inputRow: InternalRow,\n+     targetRowNotPresent: Boolean): InternalRow = {\n+\n+\n+    // Find the first combination where the predicate evaluates to true.\n+    // In case when there are overlapping condition in the MATCHED\n+    // clauses, for the first one that satisfies the predicate, the\n+    // corresponding action is applied. For example:\n+    //\n+    // WHEN MATCHED AND id > 1 AND id < 10 UPDATE *\n+    // WHEN MATCHED AND id = 5 OR id = 21 DELETE\n+    //\n+    // In above case, when id = 5, it applies both that matched predicates. In this\n+    // case the first one we see is applied.\n+    //\n+\n+    val pair = actions.find {\n+      case (predicate, _) => predicate.eval(inputRow)\n+    }\n+\n+    // Now apply the appropriate projection to either :\n+    // - Insert a row into target\n+    // - Update a row of target\n+    // - Delete a row in target. The projected row will have the deleted bit set.\n+    pair match {\n+      case Some((_, projection)) =>\n+        projection.apply(inputRow)\n+      case None =>\n+        if (targetRowNotPresent) {\n+          projectDeleteRow.apply(inputRow)\n+        } else {\n+          projectTargetCols.apply(inputRow)\n+        }\n+    }\n+  }\n+\n+  def processPartition(\n+     params: MergeIntoParams,\n+     rowIterator: Iterator[InternalRow]): Iterator[InternalRow] = {\n+\n+    val joinedAttrs = params.joinedAttributes\n+    val isSourceRowNotPresentPred = generatePredicate(params.isSourceRowNotPresent, joinedAttrs)\n+    val isTargetRowNotPresentPred = generatePredicate(params.isTargetRowNotPresent, joinedAttrs)\n+    val matchedPreds = params.matchedConditions.map(generatePredicate(_, joinedAttrs))\n+    val matchedProjs = params.matchedOutputs.map(generateProjection(_, joinedAttrs))\n+    val notMatchedPreds = params.notMatchedConditions.map(generatePredicate(_, joinedAttrs))\n+    val notMatchedProjs = params.notMatchedOutputs.map(generateProjection(_, joinedAttrs))\n+    val projectTargetCols = generateProjection(params.targetOutput, joinedAttrs)\n+    val projectDeletedRow = generateProjection(params.deleteOutput, joinedAttrs)\n+    val nonMatchedPairs =   notMatchedPreds zip notMatchedProjs\n+    val matchedPairs = matchedPreds zip matchedProjs\n+\n+    def shouldDeleteRow(row: InternalRow): Boolean =\n+      row.getBoolean(params.targetOutput.size - 1)\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f293356f830d1fe05056bfd820cc470b1de6110"}, "originalPosition": 112}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d7caece5de862437092fbbd58702f41fc66d8fdd", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/d7caece5de862437092fbbd58702f41fc66d8fdd", "committedDate": "2021-01-19T01:00:46Z", "message": "Code review"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTcwODQ5OTI3", "url": "https://github.com/apache/iceberg/pull/1947#pullrequestreview-570849927", "createdAt": "2021-01-19T01:34:46Z", "commit": {"oid": "d7caece5de862437092fbbd58702f41fc66d8fdd"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOVQwMTozNDo0NlrOIV7SBw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOVQwMTozNDo0NlrOIV7SBw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTg2MjI3OQ==", "bodyText": "@dilipbiswal, this can be reverted as well.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559862279", "createdAt": "2021-01-19T01:34:46Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteDelete.scala", "diffHunk": "@@ -56,7 +56,7 @@ case class RewriteDelete(conf: SQLConf) extends Rule[LogicalPlan] with RewriteRo\n       d\n \n     // rewrite all operations that require reading the table to delete records\n-    case DeleteFromTable(r: DataSourceV2Relation, Some(cond)) =>\n+    case DeleteFromTable(r: DataSourceV2Relation, optionalCond @ Some(cond)) =>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d7caece5de862437092fbbd58702f41fc66d8fdd"}, "originalPosition": 5}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTcwODUwNDYy", "url": "https://github.com/apache/iceberg/pull/1947#pullrequestreview-570850462", "createdAt": "2021-01-19T01:36:52Z", "commit": {"oid": "d7caece5de862437092fbbd58702f41fc66d8fdd"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOVQwMTozNjo1MlrOIV7UBA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOVQwMTozNjo1MlrOIV7UBA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTg2Mjc4OA==", "bodyText": "Nit: no need for an empty comment and an empty line.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559862788", "createdAt": "2021-01-19T01:36:52Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/MergeIntoExec.scala", "diffHunk": "@@ -0,0 +1,138 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.catalyst.expressions.BasePredicate\n+import org.apache.spark.sql.catalyst.expressions.Expression\n+import org.apache.spark.sql.catalyst.expressions.UnsafeProjection\n+import org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate\n+import org.apache.spark.sql.catalyst.plans.logical.MergeIntoParams\n+import org.apache.spark.sql.execution.SparkPlan\n+import org.apache.spark.sql.execution.UnaryExecNode\n+\n+case class MergeIntoExec(\n+    mergeIntoParams: MergeIntoParams,\n+    @transient targetRelation: DataSourceV2Relation,\n+    override val child: SparkPlan) extends UnaryExecNode {\n+\n+  override def output: Seq[Attribute] = targetRelation.output\n+\n+  protected override def doExecute(): RDD[InternalRow] = {\n+    child.execute().mapPartitions {\n+      processPartition(mergeIntoParams, _)\n+    }\n+  }\n+\n+  private def generateProjection(exprs: Seq[Expression], attrs: Seq[Attribute]): UnsafeProjection = {\n+    UnsafeProjection.create(exprs, attrs)\n+  }\n+\n+  private def generatePredicate(expr: Expression, attrs: Seq[Attribute]): BasePredicate = {\n+    GeneratePredicate.generate(expr, attrs)\n+  }\n+\n+  def applyProjection(\n+     actions: Seq[(BasePredicate, UnsafeProjection)],\n+     projectTargetCols: UnsafeProjection,\n+     projectDeleteRow: UnsafeProjection,\n+     inputRow: InternalRow,\n+     targetRowNotPresent: Boolean): InternalRow = {\n+\n+\n+    // Find the first combination where the predicate evaluates to true.\n+    // In case when there are overlapping condition in the MATCHED\n+    // clauses, for the first one that satisfies the predicate, the\n+    // corresponding action is applied. For example:\n+    //\n+    // WHEN MATCHED AND id > 1 AND id < 10 UPDATE *\n+    // WHEN MATCHED AND id = 5 OR id = 21 DELETE\n+    //\n+    // In above case, when id = 5, it applies both that matched predicates. In this\n+    // case the first one we see is applied.\n+    //\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d7caece5de862437092fbbd58702f41fc66d8fdd"}, "originalPosition": 73}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTcwODUxMDAx", "url": "https://github.com/apache/iceberg/pull/1947#pullrequestreview-570851001", "createdAt": "2021-01-19T01:39:07Z", "commit": {"oid": "d7caece5de862437092fbbd58702f41fc66d8fdd"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOVQwMTozOTowN1rOIV7Wrg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOVQwMTozOTowN1rOIV7Wrg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTg2MzQ3MA==", "bodyText": "Nit: it looks like there are unnecessary string literals. \" \" + \"MERGE ...\" can be updated to \" MERGE ...\".", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559863470", "createdAt": "2021-01-19T01:39:07Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeIntoTable.java", "diffHunk": "@@ -0,0 +1,275 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+import org.junit.runners.Parameterized;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.PARQUET_VECTORIZATION_ENABLED;\n+\n+public class TestMergeIntoTable extends SparkRowLevelOperationsTestBase {\n+  private final String sourceName;\n+  private final String targetName;\n+\n+  @Parameterized.Parameters(\n+      name = \"catalogName = {0}, implementation = {1}, config = {2}, format = {3}, vectorized = {4}\")\n+  public static Object[][] parameters() {\n+    return new Object[][] {\n+        { \"testhive\", SparkCatalog.class.getName(),\n+            ImmutableMap.of(\n+                \"type\", \"hive\",\n+                \"default-namespace\", \"default\"\n+            ),\n+            \"parquet\",\n+            true\n+        },\n+        { \"spark_catalog\", SparkSessionCatalog.class.getName(),\n+            ImmutableMap.of(\n+                \"type\", \"hive\",\n+                \"default-namespace\", \"default\",\n+                \"clients\", \"1\",\n+                \"parquet-enabled\", \"false\",\n+                \"cache-enabled\", \"false\" // Spark will delete tables using v1, leaving the cache out of sync\n+            ),\n+            \"parquet\",\n+            false\n+        }\n+    };\n+  }\n+\n+  public TestMergeIntoTable(String catalogName, String implementation, Map<String, String> config,\n+                            String fileFormat, Boolean vectorized) {\n+    super(catalogName, implementation, config, fileFormat, vectorized);\n+    this.sourceName = tableName(\"source\");\n+    this.targetName = tableName(\"target\");\n+  }\n+\n+  @BeforeClass\n+  public static void setupSparkConf() {\n+    spark.conf().set(\"spark.sql.shuffle.partitions\", \"4\");\n+  }\n+\n+  protected Map<String, String> extraTableProperties() {\n+    return ImmutableMap.of(TableProperties.MERGE_MODE, TableProperties.MERGE_MODE_DEFAULT);\n+  }\n+\n+  @Before\n+  public void createTables() {\n+    createAndInitUnPartitionedTargetTable(targetName);\n+    createAndInitSourceTable(sourceName);\n+  }\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", targetName);\n+    sql(\"DROP TABLE IF EXISTS %s\", sourceName);\n+  }\n+\n+  @Test\n+  public void testEmptyTargetInsertAllNonMatchingRows() throws NoSuchTableException {\n+    append(sourceName, new Employee(1, \"emp-id-1\"), new Employee(2, \"emp-id-2\"), new Employee(3, \"emp-id-3\"));\n+    String sqlText = \"MERGE INTO %s AS target \" +\n+                     \"USING %s AS source \" +\n+                     \"ON target.id = source.id \" +\n+                     \"WHEN NOT MATCHED THEN INSERT * \";\n+\n+    sql(sqlText, targetName, sourceName);\n+    assertEquals(\"Should have expected rows\",\n+            ImmutableList.of(row(1, \"emp-id-1\"), row(2, \"emp-id-2\"), row(3, \"emp-id-3\")),\n+            sql(\"SELECT * FROM %s ORDER BY id ASC NULLS LAST\", targetName));\n+  }\n+\n+  @Test\n+  public void testEmptyTargetInsertOnlyMatchingRows() throws NoSuchTableException {\n+    append(sourceName, new Employee(1, \"emp-id-1\"), new Employee(2, \"emp-id-2\"), new Employee(3, \"emp-id-3\"));\n+    String sqlText = \"MERGE INTO %s AS target \" +\n+                     \"USING %s AS source \" +\n+                     \"ON target.id = source.id \" +\n+                     \"WHEN NOT MATCHED AND (source.id >= 2) THEN INSERT * \";\n+\n+    sql(sqlText, targetName, sourceName);\n+    assertEquals(\"Should have expected rows\",\n+            ImmutableList.of(row(2, \"emp-id-2\"), row(3, \"emp-id-3\")),\n+            sql(\"SELECT * FROM %s ORDER BY id ASC NULLS LAST\", targetName));\n+  }\n+\n+  @Test\n+  public void testOnlyUpdate() throws NoSuchTableException {\n+    append(targetName, new Employee(1, \"emp-id-one\"), new Employee(6, \"emp-id-six\"));\n+    append(sourceName, new Employee(2, \"emp-id-2\"), new Employee(1, \"emp-id-1\"), new Employee(6, \"emp-id-6\"));\n+    String sqlText = \"MERGE INTO %s AS target \" +\n+            \"USING %s AS source \" +\n+            \"ON target.id = source.id \" +\n+            \"WHEN MATCHED AND target.id = 1 THEN UPDATE SET * \";\n+\n+    sql(sqlText, targetName, sourceName);\n+    assertEquals(\"Should have expected rows\",\n+            ImmutableList.of(row(1, \"emp-id-1\"), row(6, \"emp-id-six\")),\n+            sql(\"SELECT * FROM %s ORDER BY id ASC NULLS LAST\", targetName));\n+  }\n+\n+  @Test\n+  public void testOnlyDelete() throws NoSuchTableException {\n+    append(targetName, new Employee(1, \"emp-id-one\"), new Employee(6, \"emp-id-6\"));\n+    append(sourceName, new Employee(2, \"emp-id-2\"), new Employee(1, \"emp-id-1\"), new Employee(6, \"emp-id-6\"));\n+    String sqlText = \"MERGE INTO %s AS target \" +\n+            \"USING %s AS source \" +\n+            \"ON target.id = source.id \" +\n+            \"WHEN MATCHED AND target.id = 6 THEN DELETE\";\n+\n+    sql(sqlText, targetName, sourceName);\n+    assertEquals(\"Should have expected rows\",\n+            ImmutableList.of(row(1, \"emp-id-one\")),\n+            sql(\"SELECT * FROM %s ORDER BY id ASC NULLS LAST\", targetName));\n+  }\n+\n+  @Test\n+  public void testAllCauses() throws NoSuchTableException {\n+    append(targetName, new Employee(1, \"emp-id-one\"), new Employee(6, \"emp-id-6\"));\n+    append(sourceName, new Employee(2, \"emp-id-2\"), new Employee(1, \"emp-id-1\"), new Employee(6, \"emp-id-6\"));\n+    String sqlText = \"MERGE INTO %s AS target \" +\n+                     \"USING %s AS source \" +\n+                     \"ON target.id = source.id \" +\n+                     \"WHEN MATCHED AND target.id = 1 THEN UPDATE SET * \" +\n+                     \"WHEN MATCHED AND target.id = 6 THEN DELETE \" +\n+                     \"WHEN NOT MATCHED AND source.id = 2 THEN INSERT * \";\n+\n+    sql(sqlText, targetName, sourceName);\n+    assertEquals(\"Should have expected rows\",\n+            ImmutableList.of(row(1, \"emp-id-1\"), row(2, \"emp-id-2\")),\n+            sql(\"SELECT * FROM %s ORDER BY id ASC NULLS LAST\", targetName));\n+  }\n+\n+  @Test\n+  public void testAllCausesWithExplicitColumnSpecification() throws NoSuchTableException {\n+    append(targetName, new Employee(1, \"emp-id-one\"), new Employee(6, \"emp-id-6\"));\n+    append(sourceName, new Employee(2, \"emp-id-2\"), new Employee(1, \"emp-id-1\"), new Employee(6, \"emp-id-6\"));\n+    String sqlText = \"MERGE INTO %s AS target \" +\n+            \"USING %s AS source \" +\n+            \"ON target.id = source.id \" +\n+            \"WHEN MATCHED AND target.id = 1 THEN UPDATE SET target.id = source.id, target.dep = source.dep \" +\n+            \"WHEN MATCHED AND target.id = 6 THEN DELETE \" +\n+            \"WHEN NOT MATCHED AND source.id = 2 THEN INSERT (target.id, target.dep) VALUES (source.id, source.dep) \";\n+\n+    sql(sqlText, targetName, sourceName);\n+    assertEquals(\"Should have expected rows\",\n+            ImmutableList.of(row(1, \"emp-id-1\"), row(2, \"emp-id-2\")),\n+            sql(\"SELECT * FROM %s ORDER BY id ASC NULLS LAST\", targetName));\n+  }\n+\n+  @Test\n+  public void testSourceCTE() throws NoSuchTableException {\n+    Assume.assumeFalse(catalogName.equalsIgnoreCase(\"testhadoop\"));\n+    Assume.assumeFalse(catalogName.equalsIgnoreCase(\"testhive\"));\n+\n+    append(targetName, new Employee(2, \"emp-id-two\"), new Employee(6, \"emp-id-6\"));\n+    append(sourceName, new Employee(2, \"emp-id-3\"), new Employee(1, \"emp-id-2\"), new Employee(5, \"emp-id-6\"));\n+    String sourceCTE = \"WITH cte1 AS (SELECT id + 1 AS id, dep FROM source)\";\n+    String sqlText = sourceCTE + \" \" + \"MERGE INTO %s AS target \" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d7caece5de862437092fbbd58702f41fc66d8fdd"}, "originalPosition": 202}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9fadc1d85e60d70f91b180bd5352903916cc13fb", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/9fadc1d85e60d70f91b180bd5352903916cc13fb", "committedDate": "2021-01-19T02:23:08Z", "message": "More review"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3283, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}