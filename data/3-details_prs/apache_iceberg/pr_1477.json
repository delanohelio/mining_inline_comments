{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDg5MjI0MDUz", "number": 1477, "title": "Flink: maintain the complete data files into manifest before checkpoint finished.", "bodyText": "This patch addressed the comment from here.", "createdAt": "2020-09-18T10:24:54Z", "url": "https://github.com/apache/iceberg/pull/1477", "merged": true, "mergeCommit": {"oid": "b9634c9511c8a028074e8e5bdc54f0db47058668"}, "closed": true, "closedAt": "2020-10-28T04:06:54Z", "author": {"login": "openinx"}, "timelineItems": {"totalCount": 34, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdLkGmyAFqTQ5Mzk5NDQ1Mg==", "endCursor": "Y3Vyc29yOnYyOpPPAAABdW1J0qAFqTUxODMwMjIzNQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDkzOTk0NDUy", "url": "https://github.com/apache/iceberg/pull/1477#pullrequestreview-493994452", "createdAt": "2020-09-23T02:54:59Z", "commit": {"oid": "ac9a036b07d9404010e337d5c030959df920dc71"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yM1QwMjo1NDo1OVrOHWUh5A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yM1QwMzoxNzo0OVrOHWU3hA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzE2NzA3Ng==", "bodyText": "Do we need this stack? Why not print the message?", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r493167076", "createdAt": "2020-09-23T02:54:59Z", "author": {"login": "chenjunjiedada"}, "path": "core/src/main/java/org/apache/iceberg/avro/GenericAvroReader.java", "diffHunk": "@@ -83,6 +83,7 @@ private ReadBuilder(ClassLoader loader) {\n         return ValueReaders.record(fields, record);\n \n       } catch (ClassNotFoundException e) {\n+        e.printStackTrace();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ac9a036b07d9404010e337d5c030959df920dc71"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzE2ODExMw==", "bodyText": "Why use format v2? Can we use the version from the table as constructor parameter? That could keep consistency, I think.", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r493168113", "createdAt": "2020-09-23T02:59:09Z", "author": {"login": "chenjunjiedada"}, "path": "flink/src/main/java/org/apache/iceberg/flink/sink/FlinkManifest.java", "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.ManifestFile;\n+import org.apache.iceberg.ManifestFiles;\n+import org.apache.iceberg.ManifestWriter;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.LocationProvider;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.io.OutputFileFactory;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+\n+class FlinkManifest {\n+  private static final int ICEBERG_FORMAT_VERSION = 2;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ac9a036b07d9404010e337d5c030959df920dc71"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzE3MjEzNQ==", "bodyText": "I see the createFactory definition is createFactory(Table table, String flinkJobId, int partitionId, long taskId) . Do we need to make parameters as same as here?", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r493172135", "createdAt": "2020-09-23T03:15:44Z", "author": {"login": "chenjunjiedada"}, "path": "flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java", "diffHunk": "@@ -117,6 +120,10 @@ public void initializeState(StateInitializationContext context) throws Exception\n     // Open the table loader and load the table.\n     this.tableLoader.open(hadoopConf.get());\n     this.table = tableLoader.loadTable();\n+\n+    int subTaskId = getRuntimeContext().getIndexOfThisSubtask();\n+    int attemptId = getRuntimeContext().getAttemptNumber();\n+    this.flinkManifestFactory = FlinkManifest.createFactory(table, flinkJobId, subTaskId, attemptId);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ac9a036b07d9404010e337d5c030959df920dc71"}, "originalPosition": 72}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzE3MjYxMg==", "bodyText": "Since we are now writing manifest to state, do we need to rename datafiles related variables to manifest context?", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r493172612", "createdAt": "2020-09-23T03:17:49Z", "author": {"login": "chenjunjiedada"}, "path": "flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java", "diffHunk": "@@ -149,7 +156,7 @@ public void snapshotState(StateSnapshotContext context) throws Exception {\n     LOG.info(\"Start to flush snapshot state to state backend, table: {}, checkpointId: {}\", table, checkpointId);\n \n     // Update the checkpoint state.\n-    dataFilesPerCheckpoint.put(checkpointId, ImmutableList.copyOf(dataFilesOfCurrentCheckpoint));\n+    dataFilesPerCheckpoint.put(checkpointId, writeToManifest(checkpointId));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ac9a036b07d9404010e337d5c030959df920dc71"}, "originalPosition": 90}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk3MTYxMTE4", "url": "https://github.com/apache/iceberg/pull/1477#pullrequestreview-497161118", "createdAt": "2020-09-28T03:05:41Z", "commit": {"oid": "ac9a036b07d9404010e337d5c030959df920dc71"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQwMzowNTo0MVrOHYtIBA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQwMzowNTo0MVrOHYtIBA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTY2NzIwNA==", "bodyText": "hmm. I am not sure if we should tie Flink state serialization to Iceberg serialization", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r495667204", "createdAt": "2020-09-28T03:05:41Z", "author": {"login": "stevenzwu"}, "path": "core/src/main/java/org/apache/iceberg/ManifestFiles.java", "diffHunk": "@@ -149,6 +160,33 @@ private ManifestFiles() {\n     throw new UnsupportedOperationException(\"Cannot write manifest for table version: \" + formatVersion);\n   }\n \n+  private static org.apache.avro.Schema getManifestAvroSchema() {\n+    return AvroSchemaUtil.convert(ManifestFile.schema(), ImmutableMap.of(\n+        ManifestFile.schema().asStruct(), GenericManifestFile.class.getName(),\n+        ManifestFile.PARTITION_SUMMARY_TYPE, GenericPartitionFieldSummary.class.getName()\n+    ));\n+  }\n+\n+  public static byte[] encode(ManifestFile manifestFile) throws IOException {\n+    try (ByteArrayOutputStream out = new ByteArrayOutputStream()) {\n+      BinaryEncoder encoder = EncoderFactory.get().binaryEncoder(out, null);\n+      DatumWriter<GenericManifestFile> writer = new GenericAvroWriter<>(getManifestAvroSchema());\n+      writer.write((GenericManifestFile) manifestFile, encoder);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ac9a036b07d9404010e337d5c030959df920dc71"}, "originalPosition": 43}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk3MTYxNzkz", "url": "https://github.com/apache/iceberg/pull/1477#pullrequestreview-497161793", "createdAt": "2020-09-28T03:08:56Z", "commit": {"oid": "ac9a036b07d9404010e337d5c030959df920dc71"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQwMzowODo1NlrOHYtKNg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQwMzowODo1NlrOHYtKNg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTY2Nzc2Ng==", "bodyText": "what is partitionId here?", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r495667766", "createdAt": "2020-09-28T03:08:56Z", "author": {"login": "stevenzwu"}, "path": "flink/src/main/java/org/apache/iceberg/flink/sink/FlinkManifest.java", "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.ManifestFile;\n+import org.apache.iceberg.ManifestFiles;\n+import org.apache.iceberg.ManifestWriter;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.LocationProvider;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.io.OutputFileFactory;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+\n+class FlinkManifest {\n+  private static final int ICEBERG_FORMAT_VERSION = 2;\n+  private static final Long DUMMY_SNAPSHOT_ID = 0L;\n+\n+  private final OutputFile outputFile;\n+  private final PartitionSpec spec;\n+\n+  private FlinkManifest(OutputFile outputFile, PartitionSpec spec) {\n+    this.outputFile = outputFile;\n+    this.spec = spec;\n+  }\n+\n+  ManifestFile write(List<DataFile> dataFiles) throws IOException {\n+    ManifestWriter<DataFile> writer = ManifestFiles.write(ICEBERG_FORMAT_VERSION, spec, outputFile, DUMMY_SNAPSHOT_ID);\n+    try (ManifestWriter<DataFile> closeableWriter = writer) {\n+      closeableWriter.addAll(dataFiles);\n+    }\n+    return writer.toManifestFile();\n+  }\n+\n+  static List<DataFile> read(ManifestFile manifestFile, FileIO io) throws IOException {\n+    try (CloseableIterable<DataFile> dataFiles = ManifestFiles.read(manifestFile, io).project(ManifestFile.schema())) {\n+      return Lists.newArrayList(dataFiles);\n+    }\n+  }\n+\n+  static FlinkManifestFactory createFactory(Table table, String flinkJobId, int partitionId, long taskId) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ac9a036b07d9404010e337d5c030959df920dc71"}, "originalPosition": 66}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk3MTY1MDUx", "url": "https://github.com/apache/iceberg/pull/1477#pullrequestreview-497165051", "createdAt": "2020-09-28T03:23:26Z", "commit": {"oid": "ac9a036b07d9404010e337d5c030959df920dc71"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQwMzoyMzoyNlrOHYtVPQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQwMzoyMzoyNlrOHYtVPQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTY3MDU4OQ==", "bodyText": "instead of a Byte[] value type, should we just define the a Flink ManifestFile state type? maybe an Avro record or use SimpleVersionedSerializer that Jingsong suggested before? In addition, we can have the state type include the metadata (like jobId). '\n{\n    \"type\": \"record\",\n    \"name\": \"ManifestFileState\",\n    \"namespace\": \"org.apache.iceberg....\",\n    \"fields\": [\n        {\"name\":\"path\", \"type\":\"string\"},\n        {\"name\":\"length\", \"type\":\"long\"},\n        {\"name\":\"specId\", \"type\":\"int\"},\n        { \"name\":\"dataFileCount\", \"type\": \"long\"},\n        { \"name\":\"recordCount\", \"type\": \"long\"},\n////////////// Flink specific fields\n        {\"name\":\"jobId\", \"type\":\"string\"},\n        { \"name\":\"checkpointId\", \"type\": \"long\"},\n        { \"name\":\"checkpointTimestamp\", \"type\": \"long\"},\n        { \"name\":\"byteCount\", \"type\": \"long\"}\n    ]\n}\n\nThen we also have FlinkManifestFile interface extends from ManifestFile interface and can convert from/to ManifestFileState\npublic interface FlinkManifestFile extends ManifestFile {\n  String jobId();\n  long checkpointId();\n  long checkpointTimestamp();\n  long byteCount();\n  ManifestFileState toState();\n}", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r495670589", "createdAt": "2020-09-28T03:23:26Z", "author": {"login": "stevenzwu"}, "path": "flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java", "diffHunk": "@@ -100,8 +103,8 @@\n       \"iceberg-flink-job-id\", BasicTypeInfo.STRING_TYPE_INFO);\n   private transient ListState<String> jobIdState;\n   // All pending checkpoints states for this function.\n-  private static final ListStateDescriptor<SortedMap<Long, List<DataFile>>> STATE_DESCRIPTOR = buildStateDescriptor();\n-  private transient ListState<SortedMap<Long, List<DataFile>>> checkpointsState;\n+  private static final ListStateDescriptor<SortedMap<Long, Byte[]>> STATE_DESCRIPTOR = buildStateDescriptor();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ac9a036b07d9404010e337d5c030959df920dc71"}, "originalPosition": 60}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk3MTY2ODE0", "url": "https://github.com/apache/iceberg/pull/1477#pullrequestreview-497166814", "createdAt": "2020-09-28T03:31:24Z", "commit": {"oid": "ac9a036b07d9404010e337d5c030959df920dc71"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQwMzozMToyNVrOHYtbOw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQwMzozMToyNVrOHYtbOw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTY3MjEyMw==", "bodyText": "just a note for future improvement, we probably need to allow users to supply the manifest file base path. E.g. we run the streaming Flink jobs in 3 different regions, while Iceberg table's base path is only in one region. We don't want to write the manifiest file to the table home/base path. Instead, we want to write it to a local region (S3 bucket). I haven't thought about too much on how we can extend from the open source implementation yet. We probably can discuss it in a more systematic/thorough way maybe when we integrating Iceberg sink with the new FLIP-143 sink interface.", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r495672123", "createdAt": "2020-09-28T03:31:25Z", "author": {"login": "stevenzwu"}, "path": "flink/src/main/java/org/apache/iceberg/flink/sink/FlinkManifest.java", "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.ManifestFile;\n+import org.apache.iceberg.ManifestFiles;\n+import org.apache.iceberg.ManifestWriter;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.LocationProvider;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.io.OutputFileFactory;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+\n+class FlinkManifest {\n+  private static final int ICEBERG_FORMAT_VERSION = 2;\n+  private static final Long DUMMY_SNAPSHOT_ID = 0L;\n+\n+  private final OutputFile outputFile;\n+  private final PartitionSpec spec;\n+\n+  private FlinkManifest(OutputFile outputFile, PartitionSpec spec) {\n+    this.outputFile = outputFile;\n+    this.spec = spec;\n+  }\n+\n+  ManifestFile write(List<DataFile> dataFiles) throws IOException {\n+    ManifestWriter<DataFile> writer = ManifestFiles.write(ICEBERG_FORMAT_VERSION, spec, outputFile, DUMMY_SNAPSHOT_ID);\n+    try (ManifestWriter<DataFile> closeableWriter = writer) {\n+      closeableWriter.addAll(dataFiles);\n+    }\n+    return writer.toManifestFile();\n+  }\n+\n+  static List<DataFile> read(ManifestFile manifestFile, FileIO io) throws IOException {\n+    try (CloseableIterable<DataFile> dataFiles = ManifestFiles.read(manifestFile, io).project(ManifestFile.schema())) {\n+      return Lists.newArrayList(dataFiles);\n+    }\n+  }\n+\n+  static FlinkManifestFactory createFactory(Table table, String flinkJobId, int partitionId, long taskId) {\n+    return new FlinkManifestFactory(table.location(), table.spec(), FileFormat.AVRO, table.locationProvider(),\n+        table.io(), table.encryption(), flinkJobId, partitionId, taskId);\n+  }\n+\n+  static class FlinkManifestFactory {\n+    private final String tableLocation;\n+    private final PartitionSpec spec;\n+    private final FileFormat format;\n+    private final OutputFileFactory outputFileFactory;\n+    private final int partitionId;\n+    private final long taskId;\n+    private final String flinkJobId;\n+    private final AtomicInteger fileCount = new AtomicInteger(0);\n+\n+    FlinkManifestFactory(String tableLocation, PartitionSpec spec, FileFormat format, LocationProvider locations,\n+                         FileIO io, EncryptionManager encryptionManager, String flinkJobId, int partitionId,\n+                         long taskId) {\n+      this.tableLocation = tableLocation;\n+      this.spec = spec;\n+      this.format = format;\n+      this.flinkJobId = flinkJobId;\n+      this.partitionId = partitionId;\n+      this.taskId = taskId;\n+      this.outputFileFactory = new OutputFileFactory(spec, format, locations, io,\n+          encryptionManager, partitionId, taskId);\n+    }\n+\n+    private String generateRelativeFilePath(long checkpointId) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ac9a036b07d9404010e337d5c030959df920dc71"}, "originalPosition": 94}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk3OTk1MTI4", "url": "https://github.com/apache/iceberg/pull/1477#pullrequestreview-497995128", "createdAt": "2020-09-29T01:07:52Z", "commit": {"oid": "ac9a036b07d9404010e337d5c030959df920dc71"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwMTowNzo1MlrOHZUzEQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwMTowNzo1MlrOHZUzEQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjMxNzIwMQ==", "bodyText": "In Avro, the write schema -- that was used to encode a record -- is required in order to correctly read the record. Because this doesn't encode anything about the write schema that was used, the bytes will no longer be readable if the schema changes.\nThe checkpoint state will need to store the ManifestFile Avro schema string and this would need a way to pass the Avro schema string into decode.", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r496317201", "createdAt": "2020-09-29T01:07:52Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/ManifestFiles.java", "diffHunk": "@@ -149,6 +160,33 @@ private ManifestFiles() {\n     throw new UnsupportedOperationException(\"Cannot write manifest for table version: \" + formatVersion);\n   }\n \n+  private static org.apache.avro.Schema getManifestAvroSchema() {\n+    return AvroSchemaUtil.convert(ManifestFile.schema(), ImmutableMap.of(\n+        ManifestFile.schema().asStruct(), GenericManifestFile.class.getName(),\n+        ManifestFile.PARTITION_SUMMARY_TYPE, GenericPartitionFieldSummary.class.getName()\n+    ));\n+  }\n+\n+  public static byte[] encode(ManifestFile manifestFile) throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ac9a036b07d9404010e337d5c030959df920dc71"}, "originalPosition": 39}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk3OTk1MzA5", "url": "https://github.com/apache/iceberg/pull/1477#pullrequestreview-497995309", "createdAt": "2020-09-29T01:08:32Z", "commit": {"oid": "ac9a036b07d9404010e337d5c030959df920dc71"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwMTowODozMlrOHZUztg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwMTowODozMlrOHZUztg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjMxNzM2Ng==", "bodyText": "I would rather not leak an Avro class from this API. Can we put these methods in a utility in the Avro package?", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r496317366", "createdAt": "2020-09-29T01:08:32Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/ManifestFiles.java", "diffHunk": "@@ -149,6 +160,33 @@ private ManifestFiles() {\n     throw new UnsupportedOperationException(\"Cannot write manifest for table version: \" + formatVersion);\n   }\n \n+  private static org.apache.avro.Schema getManifestAvroSchema() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ac9a036b07d9404010e337d5c030959df920dc71"}, "originalPosition": 32}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk3OTk1NjAw", "url": "https://github.com/apache/iceberg/pull/1477#pullrequestreview-497995600", "createdAt": "2020-09-29T01:09:37Z", "commit": {"oid": "ac9a036b07d9404010e337d5c030959df920dc71"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwMTowOTozN1rOHZU0wQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwMTowOTozN1rOHZU0wQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjMxNzYzMw==", "bodyText": "Adding the utility methods to the Avro package would avoid needing to expose this class (and the corresponding writer) publicly. I think that would be good.", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r496317633", "createdAt": "2020-09-29T01:09:37Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/avro/GenericAvroReader.java", "diffHunk": "@@ -30,14 +30,14 @@\n import org.apache.iceberg.common.DynClasses;\n import org.apache.iceberg.data.avro.DecoderResolver;\n \n-class GenericAvroReader<T> implements DatumReader<T> {\n+public class GenericAvroReader<T> implements DatumReader<T> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ac9a036b07d9404010e337d5c030959df920dc71"}, "originalPosition": 5}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk3OTk3MTQ2", "url": "https://github.com/apache/iceberg/pull/1477#pullrequestreview-497997146", "createdAt": "2020-09-29T01:14:49Z", "commit": {"oid": "ac9a036b07d9404010e337d5c030959df920dc71"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwMToxNDo0OVrOHZU6Ig==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwMToxNDo0OVrOHZU6Ig==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjMxOTAxMA==", "bodyText": "This should use TableOperations.metadataFileLocation(filename) to create the full location, and should use FileIO directly to create the output file.\nThe output file factory is used for data files and it creates paths using the location provider. The location provider is only used for data files. For metadata files, TableOperations.metadataFileLocation is used.\nAlso, there is no key metadata tracking for metadata files, so using the encryption manager would produce a file that is either plaintext or unreadable because the key metadata is lost.", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r496319010", "createdAt": "2020-09-29T01:14:49Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/sink/FlinkManifest.java", "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.ManifestFile;\n+import org.apache.iceberg.ManifestFiles;\n+import org.apache.iceberg.ManifestWriter;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.LocationProvider;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.io.OutputFileFactory;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+\n+class FlinkManifest {\n+  private static final int ICEBERG_FORMAT_VERSION = 2;\n+  private static final Long DUMMY_SNAPSHOT_ID = 0L;\n+\n+  private final OutputFile outputFile;\n+  private final PartitionSpec spec;\n+\n+  private FlinkManifest(OutputFile outputFile, PartitionSpec spec) {\n+    this.outputFile = outputFile;\n+    this.spec = spec;\n+  }\n+\n+  ManifestFile write(List<DataFile> dataFiles) throws IOException {\n+    ManifestWriter<DataFile> writer = ManifestFiles.write(ICEBERG_FORMAT_VERSION, spec, outputFile, DUMMY_SNAPSHOT_ID);\n+    try (ManifestWriter<DataFile> closeableWriter = writer) {\n+      closeableWriter.addAll(dataFiles);\n+    }\n+    return writer.toManifestFile();\n+  }\n+\n+  static List<DataFile> read(ManifestFile manifestFile, FileIO io) throws IOException {\n+    try (CloseableIterable<DataFile> dataFiles = ManifestFiles.read(manifestFile, io).project(ManifestFile.schema())) {\n+      return Lists.newArrayList(dataFiles);\n+    }\n+  }\n+\n+  static FlinkManifestFactory createFactory(Table table, String flinkJobId, int partitionId, long taskId) {\n+    return new FlinkManifestFactory(table.location(), table.spec(), FileFormat.AVRO, table.locationProvider(),\n+        table.io(), table.encryption(), flinkJobId, partitionId, taskId);\n+  }\n+\n+  static class FlinkManifestFactory {\n+    private final String tableLocation;\n+    private final PartitionSpec spec;\n+    private final FileFormat format;\n+    private final OutputFileFactory outputFileFactory;\n+    private final int partitionId;\n+    private final long taskId;\n+    private final String flinkJobId;\n+    private final AtomicInteger fileCount = new AtomicInteger(0);\n+\n+    FlinkManifestFactory(String tableLocation, PartitionSpec spec, FileFormat format, LocationProvider locations,\n+                         FileIO io, EncryptionManager encryptionManager, String flinkJobId, int partitionId,\n+                         long taskId) {\n+      this.tableLocation = tableLocation;\n+      this.spec = spec;\n+      this.format = format;\n+      this.flinkJobId = flinkJobId;\n+      this.partitionId = partitionId;\n+      this.taskId = taskId;\n+      this.outputFileFactory = new OutputFileFactory(spec, format, locations, io,\n+          encryptionManager, partitionId, taskId);\n+    }\n+\n+    private String generateRelativeFilePath(long checkpointId) {\n+      return format.addExtension(\n+          String.format(\"%s/flink-manifest/%s-%05d-%d-%d-%05d\", tableLocation, flinkJobId, partitionId, taskId,\n+              checkpointId, fileCount.incrementAndGet()));\n+    }\n+\n+    FlinkManifest create(long checkpointId) {\n+      String relativeFilePath = generateRelativeFilePath(checkpointId);\n+      return new FlinkManifest(outputFileFactory.newOutputFile(relativeFilePath).encryptingOutputFile(), spec);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ac9a036b07d9404010e337d5c030959df920dc71"}, "originalPosition": 102}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk3OTk3ODg4", "url": "https://github.com/apache/iceberg/pull/1477#pullrequestreview-497997888", "createdAt": "2020-09-29T01:17:13Z", "commit": {"oid": "ac9a036b07d9404010e337d5c030959df920dc71"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwMToxNzoxM1rOHZU89A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwMToxNzoxM1rOHZU89A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjMxOTczMg==", "bodyText": "This class needs to create full paths. It should not create an output file for a relative path like this. That's why the location provider is used in the other cases.\nAlso, as I noted below, metadata files should not use the encryption manager and should also use TableOperations.metadataFileLocation to create a full path from a manifest file name. So I don't think we will need to add a method here.\nThat is, unless you wanted to rename this to newMetadataOutputFile, call ops.metadataFileLocation to create the full path, and bypass the encryption manager.", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r496319732", "createdAt": "2020-09-29T01:17:13Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/io/OutputFileFactory.java", "diffHunk": "@@ -57,20 +57,26 @@ private String generateFilename() {\n         String.format(\"%05d-%d-%s-%05d\", partitionId, taskId, uuid, fileCount.incrementAndGet()));\n   }\n \n+  /**\n+   * Generates EncryptedOutputFile with relative path under iceberg table location.\n+   */\n+  public EncryptedOutputFile newOutputFile(String relativePath) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ac9a036b07d9404010e337d5c030959df920dc71"}, "originalPosition": 7}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk3OTk5NDg1", "url": "https://github.com/apache/iceberg/pull/1477#pullrequestreview-497999485", "createdAt": "2020-09-29T01:22:22Z", "commit": {"oid": "ac9a036b07d9404010e337d5c030959df920dc71"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwMToyMjoyMlrOHZVCiA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwMToyMjoyMlrOHZVCiA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjMyMTE2MA==", "bodyText": "I think it is okay to do this, but the append operation can append whole manifest files and either copy the manifest contents into an appropriate path for you, or take ownership of the manifest and manage it so it doesn't need to be rewritten. You might consider using AppendFiles.appendManifest instead.\nIf you do choose to append manifests, then we would want to know whether the manifests are written into the table's metadata space (path came from TableOperations.metadataFilePath) or whether the manifest was written to a temp location, like @stevenzwu's suggestion. In the latter case, we would want to request that Iceberg rewrite the manifests in the commit operation.", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r496321160", "createdAt": "2020-09-29T01:22:22Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java", "diffHunk": "@@ -178,14 +185,20 @@ public void notifyCheckpointComplete(long checkpointId) throws Exception {\n     }\n   }\n \n-  private void commitUpToCheckpoint(NavigableMap<Long, List<DataFile>> dataFilesMap,\n+  private void commitUpToCheckpoint(NavigableMap<Long, Byte[]> manifestsMap,\n                                     String newFlinkJobId,\n-                                    long checkpointId) {\n-    NavigableMap<Long, List<DataFile>> pendingFileMap = dataFilesMap.headMap(checkpointId, true);\n+                                    long checkpointId) throws IOException {\n+    NavigableMap<Long, Byte[]> pendingManifestMap = manifestsMap.headMap(checkpointId, true);\n+\n+    List<ManifestFile> manifestFiles = Lists.newArrayList();\n+    for (Byte[] manifestData : pendingManifestMap.values()) {\n+      ManifestFile manifestFile = ManifestFiles.decode(ArrayUtils.toPrimitive(manifestData));\n+      manifestFiles.add(manifestFile);\n+    }\n \n     List<DataFile> pendingDataFiles = Lists.newArrayList();\n-    for (List<DataFile> dataFiles : pendingFileMap.values()) {\n-      pendingDataFiles.addAll(dataFiles);\n+    for (ManifestFile manifestFile : manifestFiles) {\n+      pendingDataFiles.addAll(FlinkManifest.read(manifestFile, table.io()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ac9a036b07d9404010e337d5c030959df920dc71"}, "originalPosition": 116}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk3OTk5NzM4", "url": "https://github.com/apache/iceberg/pull/1477#pullrequestreview-497999738", "createdAt": "2020-09-29T01:23:12Z", "commit": {"oid": "ac9a036b07d9404010e337d5c030959df920dc71"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwMToyMzoxMlrOHZVDQQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwMToyMzoxMlrOHZVDQQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjMyMTM0NQ==", "bodyText": "Why is this Byte[] instead of byte[]?", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r496321345", "createdAt": "2020-09-29T01:23:12Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java", "diffHunk": "@@ -240,12 +256,22 @@ public void processElement(StreamRecord<DataFile> element) {\n   }\n \n   @Override\n-  public void endInput() {\n+  public void endInput() throws IOException {\n     // Flush the buffered data files into 'dataFilesPerCheckpoint' firstly.\n-    dataFilesPerCheckpoint.put(Long.MAX_VALUE, ImmutableList.copyOf(dataFilesOfCurrentCheckpoint));\n+    long currentCheckpointId = Long.MAX_VALUE;\n+    dataFilesPerCheckpoint.put(currentCheckpointId, writeToManifest(currentCheckpointId));\n     dataFilesOfCurrentCheckpoint.clear();\n \n-    commitUpToCheckpoint(dataFilesPerCheckpoint, flinkJobId, Long.MAX_VALUE);\n+    commitUpToCheckpoint(dataFilesPerCheckpoint, flinkJobId, currentCheckpointId);\n+  }\n+\n+  /**\n+   * Write all the complete data files to a newly created manifest file and return the manifest's avro serialized bytes.\n+   */\n+  private Byte[] writeToManifest(long checkpointId) throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ac9a036b07d9404010e337d5c030959df920dc71"}, "originalPosition": 153}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "32f59e43de04bdee047d40a2e229725027f4e868", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/32f59e43de04bdee047d40a2e229725027f4e868", "committedDate": "2020-10-12T07:51:06Z", "message": "Add more unit tests and javadoc."}, "afterCommit": {"oid": "3b7043d00879585ee8bb633a71f8267a6e9ba1f5", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/3b7043d00879585ee8bb633a71f8267a6e9ba1f5", "committedDate": "2020-10-13T08:22:15Z", "message": "Add more unit tests and javadoc."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA3NjE5ODcw", "url": "https://github.com/apache/iceberg/pull/1477#pullrequestreview-507619870", "createdAt": "2020-10-13T16:25:18Z", "commit": {"oid": "3b7043d00879585ee8bb633a71f8267a6e9ba1f5"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QxNjoyNToxOFrOHgvQPg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QxNjoyNToxOFrOHgvQPg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDA5MDY4Ng==", "bodyText": "curious why don't we use DataFileWriter provided by Avro?", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r504090686", "createdAt": "2020-10-13T16:25:18Z", "author": {"login": "stevenzwu"}, "path": "core/src/main/java/org/apache/iceberg/avro/AvroEncoderUtil.java", "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.avro;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import org.apache.avro.LogicalTypes;\n+import org.apache.avro.Schema;\n+import org.apache.avro.io.BinaryDecoder;\n+import org.apache.avro.io.BinaryEncoder;\n+import org.apache.avro.io.DatumReader;\n+import org.apache.avro.io.DatumWriter;\n+import org.apache.avro.io.DecoderFactory;\n+import org.apache.avro.io.EncoderFactory;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+\n+public class AvroEncoderUtil {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3b7043d00879585ee8bb633a71f8267a6e9ba1f5"}, "originalPosition": 38}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA4NTc4NTg5", "url": "https://github.com/apache/iceberg/pull/1477#pullrequestreview-508578589", "createdAt": "2020-10-14T17:34:43Z", "commit": {"oid": "3b7043d00879585ee8bb633a71f8267a6e9ba1f5"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQxNzozNDo0M1rOHhd2LQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQxNzozNDo0M1rOHhd2LQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDg1NDA2MQ==", "bodyText": "nit: I thought assertEquals will produce an error msg like coded here. maybe redundant.", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r504854061", "createdAt": "2020-10-14T17:34:43Z", "author": {"login": "stevenzwu"}, "path": "flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergFilesCommitter.java", "diffHunk": "@@ -473,12 +535,65 @@ public void testBoundedStream() throws Exception {\n       harness.processElement(dataFile, 1);\n       ((BoundedOneInput) harness.getOneInputOperator()).endInput();\n \n+      assertFlinkManifests(0);\n       SimpleDataUtil.assertTableRows(tablePath, tableRows);\n       assertSnapshotSize(1);\n       assertMaxCommittedCheckpointId(jobId, Long.MAX_VALUE);\n     }\n   }\n \n+  @Test\n+  public void testFlinkManifests() throws Exception {\n+    long timestamp = 0;\n+    final long checkpoint = 10;\n+\n+    JobID jobId = new JobID();\n+    try (OneInputStreamOperatorTestHarness<DataFile, Void> harness = createStreamSink(jobId)) {\n+      harness.setup();\n+      harness.open();\n+\n+      assertMaxCommittedCheckpointId(jobId, -1L);\n+\n+      RowData row1 = SimpleDataUtil.createRowData(1, \"hello\");\n+      DataFile dataFile1 = writeDataFile(\"data-1\", ImmutableList.of(row1));\n+\n+      harness.processElement(dataFile1, ++timestamp);\n+      assertMaxCommittedCheckpointId(jobId, -1L);\n+\n+      // 1. snapshotState for checkpoint#1\n+      harness.snapshot(checkpoint, ++timestamp);\n+      List<Path> manifestPaths = assertFlinkManifests(1);\n+      Path manifestPath = manifestPaths.get(0);\n+      Assert.assertEquals(\"File name should have the expected pattern.\",\n+          String.format(\"%s-%05d-%d-%d-%05d.avro\", jobId, 0, 0, checkpoint, 1), manifestPath.getFileName().toString());\n+\n+      // 2. Read the data files from manifests and assert.\n+      List<DataFile> dataFiles = FlinkManifest.read(createTestingManifestFile(manifestPath), table.io());\n+      Assert.assertEquals(1, dataFiles.size());\n+      TestFlinkManifest.checkDataFile(dataFile1, dataFiles.get(0));\n+\n+      // 3. notifyCheckpointComplete for checkpoint#1\n+      harness.notifyOfCompletedCheckpoint(checkpoint);\n+      SimpleDataUtil.assertTableRows(tablePath, ImmutableList.of(row1));\n+      assertMaxCommittedCheckpointId(jobId, checkpoint);\n+      assertFlinkManifests(0);\n+    }\n+  }\n+\n+  private ManifestFile createTestingManifestFile(Path manifestPath) {\n+    return new GenericManifestFile(manifestPath.toAbsolutePath().toString(), manifestPath.toFile().length(), 0,\n+        ManifestContent.DATA, 0, 0, 0L, 0, 0, 0, 0, 0, 0, null);\n+  }\n+\n+  private List<Path> assertFlinkManifests(int expectedCount) throws IOException {\n+    List<Path> manifests = Files.list(flinkManifestFolder.toPath())\n+        .filter(p -> !p.toString().endsWith(\".crc\"))\n+        .collect(Collectors.toList());\n+    Assert.assertEquals(String.format(\"Expected %s flink manifests, but the list is: %s\", expectedCount, manifests),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3b7043d00879585ee8bb633a71f8267a6e9ba1f5"}, "originalPosition": 325}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE2MDc1NzAy", "url": "https://github.com/apache/iceberg/pull/1477#pullrequestreview-516075702", "createdAt": "2020-10-23T23:22:42Z", "commit": {"oid": "caf9803d96cef23ef97ddcfc4605517781631541"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QyMzoyMjo0MlrOHng3bg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QyMzoyMjo0MlrOHng3bg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTE5NDk5MA==", "bodyText": "If the snapshot is not known, then this should pass null.", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r511194990", "createdAt": "2020-10-23T23:22:42Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/sink/FlinkManifest.java", "diffHunk": "@@ -0,0 +1,123 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.HasTableOperations;\n+import org.apache.iceberg.ManifestFile;\n+import org.apache.iceberg.ManifestFiles;\n+import org.apache.iceberg.ManifestWriter;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableOperations;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+\n+class FlinkManifest {\n+  private static final int ICEBERG_FORMAT_VERSION = 2;\n+  private static final Long DUMMY_SNAPSHOT_ID = 0L;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "caf9803d96cef23ef97ddcfc4605517781631541"}, "originalPosition": 42}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE2MDc2ODc3", "url": "https://github.com/apache/iceberg/pull/1477#pullrequestreview-516076877", "createdAt": "2020-10-23T23:23:41Z", "commit": {"oid": "caf9803d96cef23ef97ddcfc4605517781631541"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QyMzoyMzo0MVrOHng4Hg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QyMzoyMzo0MVrOHng4Hg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTE5NTE2Ng==", "bodyText": "Why not just use ManifestFiles.write(spec, outputFile)? That is intended for cases like this.", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r511195166", "createdAt": "2020-10-23T23:23:41Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/sink/FlinkManifest.java", "diffHunk": "@@ -0,0 +1,123 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.HasTableOperations;\n+import org.apache.iceberg.ManifestFile;\n+import org.apache.iceberg.ManifestFiles;\n+import org.apache.iceberg.ManifestWriter;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableOperations;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+\n+class FlinkManifest {\n+  private static final int ICEBERG_FORMAT_VERSION = 2;\n+  private static final Long DUMMY_SNAPSHOT_ID = 0L;\n+\n+  private final OutputFile outputFile;\n+  private final PartitionSpec spec;\n+\n+  private FlinkManifest(OutputFile outputFile, PartitionSpec spec) {\n+    this.outputFile = outputFile;\n+    this.spec = spec;\n+  }\n+\n+  ManifestFile write(List<DataFile> dataFiles) throws IOException {\n+    ManifestWriter<DataFile> writer = ManifestFiles.write(ICEBERG_FORMAT_VERSION, spec, outputFile, DUMMY_SNAPSHOT_ID);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "caf9803d96cef23ef97ddcfc4605517781631541"}, "originalPosition": 53}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE2MDc5ODg4", "url": "https://github.com/apache/iceberg/pull/1477#pullrequestreview-516079888", "createdAt": "2020-10-23T23:27:41Z", "commit": {"oid": "caf9803d96cef23ef97ddcfc4605517781631541"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QyMzoyNzo0MlrOHng7OA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QyMzoyNzo0MlrOHng7OA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTE5NTk2MA==", "bodyText": "Nit: we avoid using get (it doesn't add context or value) and typically use the field name instead.", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r511195960", "createdAt": "2020-10-23T23:27:42Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/sink/FlinkManifestSerializer.java", "diffHunk": "@@ -0,0 +1,71 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.io.IOException;\n+import org.apache.flink.core.io.SimpleVersionedSerializer;\n+import org.apache.flink.core.memory.DataInputDeserializer;\n+import org.apache.flink.core.memory.DataOutputSerializer;\n+import org.apache.iceberg.ManifestFile;\n+import org.apache.iceberg.ManifestFiles;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+\n+class FlinkManifestSerializer implements SimpleVersionedSerializer<ManifestFile> {\n+  private static final int VERSION_NUM = 1;\n+  static final FlinkManifestSerializer INSTANCE = new FlinkManifestSerializer();\n+\n+  @Override\n+  public int getVersion() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "caf9803d96cef23ef97ddcfc4605517781631541"}, "originalPosition": 35}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE2MDk4NDYz", "url": "https://github.com/apache/iceberg/pull/1477#pullrequestreview-516098463", "createdAt": "2020-10-23T23:38:19Z", "commit": {"oid": "caf9803d96cef23ef97ddcfc4605517781631541"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QyMzozODoxOVrOHnhDCQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QyMzozODoxOVrOHnhDCQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTE5Nzk2MQ==", "bodyText": "Can we move this so that we don't need to make it public? It works, but it basically defines its own format for a single message. I'd prefer to use Avro's single-message encoding if we can but this is good for this PR.\nThe more important thing is that we should not expose utility methods that we may need to support that serialize single objects with Avro. I think we should move this and the ManifestFiles methods to package-private to support the Flink ManifestFileSerializer.", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r511197961", "createdAt": "2020-10-23T23:38:19Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/avro/AvroEncoderUtil.java", "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.avro;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import org.apache.avro.LogicalTypes;\n+import org.apache.avro.Schema;\n+import org.apache.avro.io.BinaryDecoder;\n+import org.apache.avro.io.BinaryEncoder;\n+import org.apache.avro.io.DatumReader;\n+import org.apache.avro.io.DatumWriter;\n+import org.apache.avro.io.DecoderFactory;\n+import org.apache.avro.io.EncoderFactory;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+\n+public class AvroEncoderUtil {\n+\n+  private AvroEncoderUtil() {\n+  }\n+\n+  static {\n+    LogicalTypes.register(LogicalMap.NAME, schema -> LogicalMap.get());\n+  }\n+\n+  private static final int VERSION = 1;\n+  private static final byte[] MAGIC_BYTES = new byte[] {'a', 'V', 'R', VERSION};\n+\n+  private static byte[] encodeInt(int value) {\n+    return ByteBuffer.allocate(4).putInt(value).array();\n+  }\n+\n+  private static int decodeInt(byte[] value) {\n+    return ByteBuffer.wrap(value).getInt();\n+  }\n+\n+  public static <T> byte[] encode(T datum, Schema avroSchema) throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "caf9803d96cef23ef97ddcfc4605517781631541"}, "originalPosition": 58}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE2NDMxNDgy", "url": "https://github.com/apache/iceberg/pull/1477#pullrequestreview-516431482", "createdAt": "2020-10-26T02:08:06Z", "commit": {"oid": "caf9803d96cef23ef97ddcfc4605517781631541"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQwMjowODowNlrOHn-5JA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQwMjoyNTowMFrOHn_GBA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTY4Njk0OA==", "bodyText": "You should remove version things in this AvroEncoderUtil since SimpleVersionedSerializer already handle versions.", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r511686948", "createdAt": "2020-10-26T02:08:06Z", "author": {"login": "JingsongLi"}, "path": "core/src/main/java/org/apache/iceberg/avro/AvroEncoderUtil.java", "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.avro;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import org.apache.avro.LogicalTypes;\n+import org.apache.avro.Schema;\n+import org.apache.avro.io.BinaryDecoder;\n+import org.apache.avro.io.BinaryEncoder;\n+import org.apache.avro.io.DatumReader;\n+import org.apache.avro.io.DatumWriter;\n+import org.apache.avro.io.DecoderFactory;\n+import org.apache.avro.io.EncoderFactory;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+\n+public class AvroEncoderUtil {\n+\n+  private AvroEncoderUtil() {\n+  }\n+\n+  static {\n+    LogicalTypes.register(LogicalMap.NAME, schema -> LogicalMap.get());\n+  }\n+\n+  private static final int VERSION = 1;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "caf9803d96cef23ef97ddcfc4605517781631541"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTY4NzMxNw==", "bodyText": "Why write VERSION_NUM again? Should this be a bug?", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r511687317", "createdAt": "2020-10-26T02:10:05Z", "author": {"login": "JingsongLi"}, "path": "flink/src/main/java/org/apache/iceberg/flink/sink/FlinkManifestSerializer.java", "diffHunk": "@@ -0,0 +1,71 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.io.IOException;\n+import org.apache.flink.core.io.SimpleVersionedSerializer;\n+import org.apache.flink.core.memory.DataInputDeserializer;\n+import org.apache.flink.core.memory.DataOutputSerializer;\n+import org.apache.iceberg.ManifestFile;\n+import org.apache.iceberg.ManifestFiles;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+\n+class FlinkManifestSerializer implements SimpleVersionedSerializer<ManifestFile> {\n+  private static final int VERSION_NUM = 1;\n+  static final FlinkManifestSerializer INSTANCE = new FlinkManifestSerializer();\n+\n+  @Override\n+  public int getVersion() {\n+    return VERSION_NUM;\n+  }\n+\n+  @Override\n+  public byte[] serialize(ManifestFile manifestFile) throws IOException {\n+    Preconditions.checkNotNull(manifestFile, \"ManifestFile to be serialized should not be null\");\n+\n+    DataOutputSerializer out = new DataOutputSerializer(256);\n+    out.writeInt(VERSION_NUM);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "caf9803d96cef23ef97ddcfc4605517781631541"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTY4NzczNQ==", "bodyText": "You should use SimpleVersionedSerialization.readVersionAndDeSerialize", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r511687735", "createdAt": "2020-10-26T02:12:08Z", "author": {"login": "JingsongLi"}, "path": "flink/src/main/java/org/apache/iceberg/flink/sink/FlinkManifestSerializer.java", "diffHunk": "@@ -0,0 +1,71 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.io.IOException;\n+import org.apache.flink.core.io.SimpleVersionedSerializer;\n+import org.apache.flink.core.memory.DataInputDeserializer;\n+import org.apache.flink.core.memory.DataOutputSerializer;\n+import org.apache.iceberg.ManifestFile;\n+import org.apache.iceberg.ManifestFiles;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+\n+class FlinkManifestSerializer implements SimpleVersionedSerializer<ManifestFile> {\n+  private static final int VERSION_NUM = 1;\n+  static final FlinkManifestSerializer INSTANCE = new FlinkManifestSerializer();\n+\n+  @Override\n+  public int getVersion() {\n+    return VERSION_NUM;\n+  }\n+\n+  @Override\n+  public byte[] serialize(ManifestFile manifestFile) throws IOException {\n+    Preconditions.checkNotNull(manifestFile, \"ManifestFile to be serialized should not be null\");\n+\n+    DataOutputSerializer out = new DataOutputSerializer(256);\n+    out.writeInt(VERSION_NUM);\n+\n+    byte[] serialized = ManifestFiles.encode(manifestFile);\n+    out.writeInt(serialized.length);\n+    out.write(serialized);\n+\n+    return out.getCopyOfBuffer();\n+  }\n+\n+  @Override\n+  public ManifestFile deserialize(int version, byte[] serialized) throws IOException {\n+    if (version == VERSION_NUM) {\n+      return ManifestFiles.decode(serialized);\n+    } else {\n+      throw new IOException(\"Unrecognized version or corrupt state: \" + version);\n+    }\n+  }\n+\n+  static ManifestFile readVersionAndDeserialize(byte[] versionedSerialized) throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "caf9803d96cef23ef97ddcfc4605517781631541"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTY4ODk1OA==", "bodyText": "Can we use DataInputStream and DataOutputStream to avoid implementing these parser logic?", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r511688958", "createdAt": "2020-10-26T02:18:41Z", "author": {"login": "JingsongLi"}, "path": "core/src/main/java/org/apache/iceberg/avro/AvroEncoderUtil.java", "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.avro;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import org.apache.avro.LogicalTypes;\n+import org.apache.avro.Schema;\n+import org.apache.avro.io.BinaryDecoder;\n+import org.apache.avro.io.BinaryEncoder;\n+import org.apache.avro.io.DatumReader;\n+import org.apache.avro.io.DatumWriter;\n+import org.apache.avro.io.DecoderFactory;\n+import org.apache.avro.io.EncoderFactory;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+\n+public class AvroEncoderUtil {\n+\n+  private AvroEncoderUtil() {\n+  }\n+\n+  static {\n+    LogicalTypes.register(LogicalMap.NAME, schema -> LogicalMap.get());\n+  }\n+\n+  private static final int VERSION = 1;\n+  private static final byte[] MAGIC_BYTES = new byte[] {'a', 'V', 'R', VERSION};\n+\n+  private static byte[] encodeInt(int value) {\n+    return ByteBuffer.allocate(4).putInt(value).array();\n+  }\n+\n+  private static int decodeInt(byte[] value) {\n+    return ByteBuffer.wrap(value).getInt();\n+  }\n+\n+  public static <T> byte[] encode(T datum, Schema avroSchema) throws IOException {\n+    try (ByteArrayOutputStream out = new ByteArrayOutputStream()) {\n+      // Write the magic bytes\n+      out.write(MAGIC_BYTES);\n+\n+      // Write the length of avro schema string.\n+      byte[] avroSchemaBytes = avroSchema.toString().getBytes(StandardCharsets.UTF_8);\n+      out.write(encodeInt(avroSchemaBytes.length));\n+\n+      // Write the avro schema string.\n+      out.write(avroSchemaBytes);\n+\n+      // Encode the datum with avro schema.\n+      BinaryEncoder encoder = EncoderFactory.get().binaryEncoder(out, null);\n+      DatumWriter<T> writer = new GenericAvroWriter<>(avroSchema);\n+      writer.write(datum, encoder);\n+      encoder.flush();\n+      return out.toByteArray();\n+    }\n+  }\n+\n+  public static <T> T decode(byte[] data) throws IOException {\n+    byte[] buffer4 = new byte[4];\n+    try (ByteArrayInputStream in = new ByteArrayInputStream(data, 0, data.length)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "caf9803d96cef23ef97ddcfc4605517781631541"}, "originalPosition": 81}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTY4OTQxNw==", "bodyText": "Can you extract two methods encodeSchema(Schema schema, OutputStream out) and decodeSchema(InputStream in)?", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r511689417", "createdAt": "2020-10-26T02:20:58Z", "author": {"login": "JingsongLi"}, "path": "core/src/main/java/org/apache/iceberg/avro/AvroEncoderUtil.java", "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.avro;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import org.apache.avro.LogicalTypes;\n+import org.apache.avro.Schema;\n+import org.apache.avro.io.BinaryDecoder;\n+import org.apache.avro.io.BinaryEncoder;\n+import org.apache.avro.io.DatumReader;\n+import org.apache.avro.io.DatumWriter;\n+import org.apache.avro.io.DecoderFactory;\n+import org.apache.avro.io.EncoderFactory;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+\n+public class AvroEncoderUtil {\n+\n+  private AvroEncoderUtil() {\n+  }\n+\n+  static {\n+    LogicalTypes.register(LogicalMap.NAME, schema -> LogicalMap.get());\n+  }\n+\n+  private static final int VERSION = 1;\n+  private static final byte[] MAGIC_BYTES = new byte[] {'a', 'V', 'R', VERSION};\n+\n+  private static byte[] encodeInt(int value) {\n+    return ByteBuffer.allocate(4).putInt(value).array();\n+  }\n+\n+  private static int decodeInt(byte[] value) {\n+    return ByteBuffer.wrap(value).getInt();\n+  }\n+\n+  public static <T> byte[] encode(T datum, Schema avroSchema) throws IOException {\n+    try (ByteArrayOutputStream out = new ByteArrayOutputStream()) {\n+      // Write the magic bytes\n+      out.write(MAGIC_BYTES);\n+\n+      // Write the length of avro schema string.\n+      byte[] avroSchemaBytes = avroSchema.toString().getBytes(StandardCharsets.UTF_8);\n+      out.write(encodeInt(avroSchemaBytes.length));\n+\n+      // Write the avro schema string.\n+      out.write(avroSchemaBytes);\n+\n+      // Encode the datum with avro schema.\n+      BinaryEncoder encoder = EncoderFactory.get().binaryEncoder(out, null);\n+      DatumWriter<T> writer = new GenericAvroWriter<>(avroSchema);\n+      writer.write(datum, encoder);\n+      encoder.flush();\n+      return out.toByteArray();\n+    }\n+  }\n+\n+  public static <T> T decode(byte[] data) throws IOException {\n+    byte[] buffer4 = new byte[4];\n+    try (ByteArrayInputStream in = new ByteArrayInputStream(data, 0, data.length)) {\n+      // Read the magic bytes\n+      Preconditions.checkState(in.read(buffer4) == 4, \"Size of magic bytes isn't 4.\");\n+      Preconditions.checkState(Arrays.equals(MAGIC_BYTES, buffer4), \"Magic bytes mismatched.\");\n+\n+      // Read the length of avro schema string.\n+      Preconditions.checkState(in.read(buffer4) == 4, \"Could not read an integer from input stream.\");\n+      int avroSchemaLength = decodeInt(buffer4);\n+      Preconditions.checkState(avroSchemaLength > 0, \"Length of avro schema string should be positive\");\n+\n+      // Read the avro schema string.\n+      byte[] avroSchemaBytes = new byte[avroSchemaLength];", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "caf9803d96cef23ef97ddcfc4605517781631541"}, "originalPosition": 92}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTY5MDI0NA==", "bodyText": "I don't quite understand this class. It looks like a manifest writer and reader?", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r511690244", "createdAt": "2020-10-26T02:25:00Z", "author": {"login": "JingsongLi"}, "path": "flink/src/main/java/org/apache/iceberg/flink/sink/FlinkManifest.java", "diffHunk": "@@ -0,0 +1,123 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.HasTableOperations;\n+import org.apache.iceberg.ManifestFile;\n+import org.apache.iceberg.ManifestFiles;\n+import org.apache.iceberg.ManifestWriter;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableOperations;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+\n+class FlinkManifest {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "caf9803d96cef23ef97ddcfc4605517781631541"}, "originalPosition": 40}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b1c905dc63937d8aa5a60fe7e6ba3b39b499f386", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/b1c905dc63937d8aa5a60fe7e6ba3b39b499f386", "committedDate": "2020-10-26T02:58:28Z", "message": "Flink: maintain the complete data files into manifest before checkpoint finished."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "42d1435b9708f09e962e3ea14e34375d2ff4c38d", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/42d1435b9708f09e962e3ea14e34375d2ff4c38d", "committedDate": "2020-10-26T02:58:28Z", "message": "Update according to the comments from PR"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ad241e12eda9680f96393e21dad2160d7196f746", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/ad241e12eda9680f96393e21dad2160d7196f746", "committedDate": "2020-10-26T02:58:28Z", "message": "Change from Byte[] to byte[] in flink state"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "923ff1bcd44c4f6fb0fcc47c98acc11285dab45c", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/923ff1bcd44c4f6fb0fcc47c98acc11285dab45c", "committedDate": "2020-10-26T02:58:28Z", "message": "Fix the broken unit tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e6fc423adf5b670ddf7af65975b3240c1137eb23", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/e6fc423adf5b670ddf7af65975b3240c1137eb23", "committedDate": "2020-10-26T02:58:28Z", "message": "Add basic unit tests for AvroEncoderUtil"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "151a19a88f747d857e41412e079e4bb85bf7f71d", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/151a19a88f747d857e41412e079e4bb85bf7f71d", "committedDate": "2020-10-26T02:58:29Z", "message": "More unit tests."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "07761e2ff879fe36edecfd695a1d70b68bfdce50", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/07761e2ff879fe36edecfd695a1d70b68bfdce50", "committedDate": "2020-10-26T02:58:29Z", "message": "Add more unit tests and javadoc."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cfcce849a054f006f824910248a837791fbc0384", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/cfcce849a054f006f824910248a837791fbc0384", "committedDate": "2020-10-26T02:58:29Z", "message": "Use SimpleVersionedSerializer to serilize/deserilize the flink state"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0da62b8b95fcbb735d45b23dd901811721d30c65", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/0da62b8b95fcbb735d45b23dd901811721d30c65", "committedDate": "2020-10-26T08:26:30Z", "message": "Use SimpleVersionedSerialization to serialize & deserialize the flink manifest."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "96f31d1a058e3dcdcd5e0d9846c3b97422356033", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/96f31d1a058e3dcdcd5e0d9846c3b97422356033", "committedDate": "2020-10-26T08:51:18Z", "message": "Extract the encodeSchema & decodeSchema."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4b6cae3b5c2d4021f8bbcb0c88196bb398d8c9af", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/4b6cae3b5c2d4021f8bbcb0c88196bb398d8c9af", "committedDate": "2020-10-26T09:49:19Z", "message": "Refactor the FlinkManifest to make it more clear"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "caf9803d96cef23ef97ddcfc4605517781631541", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/caf9803d96cef23ef97ddcfc4605517781631541", "committedDate": "2020-10-16T02:53:34Z", "message": "Use SimpleVersionedSerializer to serilize/deserilize the flink state"}, "afterCommit": {"oid": "4b6cae3b5c2d4021f8bbcb0c88196bb398d8c9af", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/4b6cae3b5c2d4021f8bbcb0c88196bb398d8c9af", "committedDate": "2020-10-26T09:49:19Z", "message": "Refactor the FlinkManifest to make it more clear"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE4MjM4MzIy", "url": "https://github.com/apache/iceberg/pull/1477#pullrequestreview-518238322", "createdAt": "2020-10-28T00:09:53Z", "commit": {"oid": "4b6cae3b5c2d4021f8bbcb0c88196bb398d8c9af"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE4MzAyMjM1", "url": "https://github.com/apache/iceberg/pull/1477#pullrequestreview-518302235", "createdAt": "2020-10-28T03:39:16Z", "commit": {"oid": "4b6cae3b5c2d4021f8bbcb0c88196bb398d8c9af"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3801, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}