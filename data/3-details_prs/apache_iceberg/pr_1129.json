{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDM3MTA3MDE1", "number": 1129, "title": "Turn on name mapping for avro", "bodyText": "", "createdAt": "2020-06-19T13:37:48Z", "url": "https://github.com/apache/iceberg/pull/1129", "merged": true, "mergeCommit": {"oid": "33f1825b72efe05c7d83feb68fa1669eb00bb129"}, "closed": true, "closedAt": "2020-06-23T17:28:42Z", "author": {"login": "chenjunjiedada"}, "timelineItems": {"totalCount": 13, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABctr6ncgBqjM0NjcwMDU3MzY=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcuI340gFqTQzNTk5OTA5MQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "6ec197740ad62d24aa91e91bf7b23361c81693b7", "author": {"user": {"login": "chenjunjiedada", "name": "Chen, Junjie"}}, "url": "https://github.com/apache/iceberg/commit/6ec197740ad62d24aa91e91bf7b23361c81693b7", "committedDate": "2020-06-22T07:26:05Z", "message": "Add a unit test"}, "afterCommit": {"oid": "ddf536dbfb75098000b9866b96edfed0202816f6", "author": {"user": {"login": "chenjunjiedada", "name": "Chen, Junjie"}}, "url": "https://github.com/apache/iceberg/commit/ddf536dbfb75098000b9866b96edfed0202816f6", "committedDate": "2020-06-22T07:40:22Z", "message": "Add a unit test"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM1MTAzNjM4", "url": "https://github.com/apache/iceberg/pull/1129#pullrequestreview-435103638", "createdAt": "2020-06-22T16:47:39Z", "commit": {"oid": "ddf536dbfb75098000b9866b96edfed0202816f6"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yMlQxNjo0Nzo0MFrOGnI9YQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yMlQxNjo0Nzo0MFrOGnI9YQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzY5NDQzMw==", "bodyText": "FYI, this isn't necessary because the files added to a table don't have to match the format.", "url": "https://github.com/apache/iceberg/pull/1129#discussion_r443694433", "createdAt": "2020-06-22T16:47:40Z", "author": {"login": "rdblue"}, "path": "spark2/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java", "diffHunk": "@@ -298,4 +309,69 @@ public void testImportWithNameMappingForVectorizedParquetReader() throws Excepti\n \n     Assert.assertEquals(expected.stream().map(SimpleRecord::getData).collect(Collectors.toList()), actual);\n   }\n+\n+  @Test\n+  public void testAvroReaderWithNameMapping() throws IOException {\n+    File avroFile = temp.newFile();\n+    org.apache.avro.Schema avroSchema = SchemaBuilder.record(\"TestRecord\")\n+        .namespace(\"org.apache.iceberg.spark.data\")\n+        .fields()\n+        .requiredInt(\"id\")\n+        .requiredString(\"name\")\n+        .endRecord();\n+\n+    GenericRecord record1 = new GenericData.Record(avroSchema);\n+    record1.put(\"id\", 1);\n+    record1.put(\"name\", \"Bob\");\n+\n+    GenericRecord record2 = new GenericData.Record(avroSchema);\n+    record2.put(\"id\", 2);\n+    record2.put(\"name\", \"Alice\");\n+\n+    DatumWriter<GenericRecord> datumWriter = new GenericDatumWriter<>(avroSchema);\n+    DataFileWriter<GenericRecord> dataFileWriter = new DataFileWriter<>(datumWriter);\n+\n+    dataFileWriter.create(avroSchema, avroFile);\n+    dataFileWriter.append(record1);\n+    dataFileWriter.append(record2);\n+    dataFileWriter.close();\n+\n+    DataFile avroDataFile = DataFiles.builder(PartitionSpec.unpartitioned())\n+        .withFormat(\"avro\")\n+        .withFileSizeInBytes(avroFile.length())\n+        .withPath(avroFile.getAbsolutePath())\n+        .withRecordCount(2)\n+        .build();\n+\n+    Schema filteredSchema = new Schema(\n+        required(1, \"name\", Types.StringType.get())\n+    );\n+\n+    NameMapping nameMapping = MappingUtil.create(filteredSchema);\n+\n+    Table table = catalog.createTable(\n+        org.apache.iceberg.catalog.TableIdentifier.of(DB_NAME, \"avro_table\"),\n+        filteredSchema,\n+        PartitionSpec.unpartitioned());\n+\n+    table.updateProperties()\n+        .set(DEFAULT_NAME_MAPPING, NameMappingParser.toJson(nameMapping))\n+        .set(DEFAULT_FILE_FORMAT, \"avro\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ddf536dbfb75098000b9866b96edfed0202816f6"}, "originalPosition": 82}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM1MTA1MzMw", "url": "https://github.com/apache/iceberg/pull/1129#pullrequestreview-435105330", "createdAt": "2020-06-22T16:49:56Z", "commit": {"oid": "ddf536dbfb75098000b9866b96edfed0202816f6"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yMlQxNjo0OTo1N1rOGnJCXg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yMlQxNjo0OTo1N1rOGnJCXg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzY5NTcxMA==", "bodyText": "This should not project because ID is being ignored.", "url": "https://github.com/apache/iceberg/pull/1129#discussion_r443695710", "createdAt": "2020-06-22T16:49:57Z", "author": {"login": "rdblue"}, "path": "spark2/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java", "diffHunk": "@@ -298,4 +309,69 @@ public void testImportWithNameMappingForVectorizedParquetReader() throws Excepti\n \n     Assert.assertEquals(expected.stream().map(SimpleRecord::getData).collect(Collectors.toList()), actual);\n   }\n+\n+  @Test\n+  public void testAvroReaderWithNameMapping() throws IOException {\n+    File avroFile = temp.newFile();\n+    org.apache.avro.Schema avroSchema = SchemaBuilder.record(\"TestRecord\")\n+        .namespace(\"org.apache.iceberg.spark.data\")\n+        .fields()\n+        .requiredInt(\"id\")\n+        .requiredString(\"name\")\n+        .endRecord();\n+\n+    GenericRecord record1 = new GenericData.Record(avroSchema);\n+    record1.put(\"id\", 1);\n+    record1.put(\"name\", \"Bob\");\n+\n+    GenericRecord record2 = new GenericData.Record(avroSchema);\n+    record2.put(\"id\", 2);\n+    record2.put(\"name\", \"Alice\");\n+\n+    DatumWriter<GenericRecord> datumWriter = new GenericDatumWriter<>(avroSchema);\n+    DataFileWriter<GenericRecord> dataFileWriter = new DataFileWriter<>(datumWriter);\n+\n+    dataFileWriter.create(avroSchema, avroFile);\n+    dataFileWriter.append(record1);\n+    dataFileWriter.append(record2);\n+    dataFileWriter.close();\n+\n+    DataFile avroDataFile = DataFiles.builder(PartitionSpec.unpartitioned())\n+        .withFormat(\"avro\")\n+        .withFileSizeInBytes(avroFile.length())\n+        .withPath(avroFile.getAbsolutePath())\n+        .withRecordCount(2)\n+        .build();\n+\n+    Schema filteredSchema = new Schema(\n+        required(1, \"name\", Types.StringType.get())\n+    );\n+\n+    NameMapping nameMapping = MappingUtil.create(filteredSchema);\n+\n+    Table table = catalog.createTable(\n+        org.apache.iceberg.catalog.TableIdentifier.of(DB_NAME, \"avro_table\"),\n+        filteredSchema,\n+        PartitionSpec.unpartitioned());\n+\n+    table.updateProperties()\n+        .set(DEFAULT_NAME_MAPPING, NameMappingParser.toJson(nameMapping))\n+        .set(DEFAULT_FILE_FORMAT, \"avro\")\n+        .commit();\n+\n+    table.newFastAppend().appendFile(avroDataFile).commit();\n+\n+    List<String> actual = spark.read().format(\"iceberg\")\n+        .load(DB_NAME + \".avro_table\")\n+        .select(\"name\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ddf536dbfb75098000b9866b96edfed0202816f6"}, "originalPosition": 89}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM1MTA2Mzk5", "url": "https://github.com/apache/iceberg/pull/1129#pullrequestreview-435106399", "createdAt": "2020-06-22T16:51:16Z", "commit": {"oid": "ddf536dbfb75098000b9866b96edfed0202816f6"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yMlQxNjo1MToxNlrOGnJFhA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yMlQxNjo1MToxNlrOGnJFhA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzY5NjUxNg==", "bodyText": "I like that filteredSchema is used to create the mapping. Could you use a full schema for the table (1: name, 2: id) so that we can verify that the ID is projected as null?", "url": "https://github.com/apache/iceberg/pull/1129#discussion_r443696516", "createdAt": "2020-06-22T16:51:16Z", "author": {"login": "rdblue"}, "path": "spark2/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java", "diffHunk": "@@ -298,4 +309,69 @@ public void testImportWithNameMappingForVectorizedParquetReader() throws Excepti\n \n     Assert.assertEquals(expected.stream().map(SimpleRecord::getData).collect(Collectors.toList()), actual);\n   }\n+\n+  @Test\n+  public void testAvroReaderWithNameMapping() throws IOException {\n+    File avroFile = temp.newFile();\n+    org.apache.avro.Schema avroSchema = SchemaBuilder.record(\"TestRecord\")\n+        .namespace(\"org.apache.iceberg.spark.data\")\n+        .fields()\n+        .requiredInt(\"id\")\n+        .requiredString(\"name\")\n+        .endRecord();\n+\n+    GenericRecord record1 = new GenericData.Record(avroSchema);\n+    record1.put(\"id\", 1);\n+    record1.put(\"name\", \"Bob\");\n+\n+    GenericRecord record2 = new GenericData.Record(avroSchema);\n+    record2.put(\"id\", 2);\n+    record2.put(\"name\", \"Alice\");\n+\n+    DatumWriter<GenericRecord> datumWriter = new GenericDatumWriter<>(avroSchema);\n+    DataFileWriter<GenericRecord> dataFileWriter = new DataFileWriter<>(datumWriter);\n+\n+    dataFileWriter.create(avroSchema, avroFile);\n+    dataFileWriter.append(record1);\n+    dataFileWriter.append(record2);\n+    dataFileWriter.close();\n+\n+    DataFile avroDataFile = DataFiles.builder(PartitionSpec.unpartitioned())\n+        .withFormat(\"avro\")\n+        .withFileSizeInBytes(avroFile.length())\n+        .withPath(avroFile.getAbsolutePath())\n+        .withRecordCount(2)\n+        .build();\n+\n+    Schema filteredSchema = new Schema(\n+        required(1, \"name\", Types.StringType.get())\n+    );\n+\n+    NameMapping nameMapping = MappingUtil.create(filteredSchema);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ddf536dbfb75098000b9866b96edfed0202816f6"}, "originalPosition": 73}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM1MTA3OTQ0", "url": "https://github.com/apache/iceberg/pull/1129#pullrequestreview-435107944", "createdAt": "2020-06-22T16:53:14Z", "commit": {"oid": "ddf536dbfb75098000b9866b96edfed0202816f6"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yMlQxNjo1MzoxNFrOGnJKKA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yMlQxNjo1MzoxNFrOGnJKKA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzY5NzcwNA==", "bodyText": "I think this validation looks strange. There's no need to create a list with just record2, and no need to transform that list. It is easier to read and maintain tests with simple assertions, like assertEquals(\"Should project 1 record\", 1, actual.size());. And each field should have its own assertion since you're hard-coding the filter for Alice and we know that id will be null.", "url": "https://github.com/apache/iceberg/pull/1129#discussion_r443697704", "createdAt": "2020-06-22T16:53:14Z", "author": {"login": "rdblue"}, "path": "spark2/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java", "diffHunk": "@@ -298,4 +309,69 @@ public void testImportWithNameMappingForVectorizedParquetReader() throws Excepti\n \n     Assert.assertEquals(expected.stream().map(SimpleRecord::getData).collect(Collectors.toList()), actual);\n   }\n+\n+  @Test\n+  public void testAvroReaderWithNameMapping() throws IOException {\n+    File avroFile = temp.newFile();\n+    org.apache.avro.Schema avroSchema = SchemaBuilder.record(\"TestRecord\")\n+        .namespace(\"org.apache.iceberg.spark.data\")\n+        .fields()\n+        .requiredInt(\"id\")\n+        .requiredString(\"name\")\n+        .endRecord();\n+\n+    GenericRecord record1 = new GenericData.Record(avroSchema);\n+    record1.put(\"id\", 1);\n+    record1.put(\"name\", \"Bob\");\n+\n+    GenericRecord record2 = new GenericData.Record(avroSchema);\n+    record2.put(\"id\", 2);\n+    record2.put(\"name\", \"Alice\");\n+\n+    DatumWriter<GenericRecord> datumWriter = new GenericDatumWriter<>(avroSchema);\n+    DataFileWriter<GenericRecord> dataFileWriter = new DataFileWriter<>(datumWriter);\n+\n+    dataFileWriter.create(avroSchema, avroFile);\n+    dataFileWriter.append(record1);\n+    dataFileWriter.append(record2);\n+    dataFileWriter.close();\n+\n+    DataFile avroDataFile = DataFiles.builder(PartitionSpec.unpartitioned())\n+        .withFormat(\"avro\")\n+        .withFileSizeInBytes(avroFile.length())\n+        .withPath(avroFile.getAbsolutePath())\n+        .withRecordCount(2)\n+        .build();\n+\n+    Schema filteredSchema = new Schema(\n+        required(1, \"name\", Types.StringType.get())\n+    );\n+\n+    NameMapping nameMapping = MappingUtil.create(filteredSchema);\n+\n+    Table table = catalog.createTable(\n+        org.apache.iceberg.catalog.TableIdentifier.of(DB_NAME, \"avro_table\"),\n+        filteredSchema,\n+        PartitionSpec.unpartitioned());\n+\n+    table.updateProperties()\n+        .set(DEFAULT_NAME_MAPPING, NameMappingParser.toJson(nameMapping))\n+        .set(DEFAULT_FILE_FORMAT, \"avro\")\n+        .commit();\n+\n+    table.newFastAppend().appendFile(avroDataFile).commit();\n+\n+    List<String> actual = spark.read().format(\"iceberg\")\n+        .load(DB_NAME + \".avro_table\")\n+        .select(\"name\")\n+        .filter(\"name='Alice'\")\n+        .collectAsList()\n+        .stream()\n+        .map(r -> r.getString(0))\n+        .collect(Collectors.toList());\n+\n+    List<GenericRecord> expected = Lists.newArrayList(record2);\n+\n+    Assert.assertEquals(expected.stream().map(r -> r.get(\"name\")).collect(Collectors.toList()), actual);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ddf536dbfb75098000b9866b96edfed0202816f6"}, "originalPosition": 98}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM1MTA4NjQz", "url": "https://github.com/apache/iceberg/pull/1129#pullrequestreview-435108643", "createdAt": "2020-06-22T16:54:08Z", "commit": {"oid": "ddf536dbfb75098000b9866b96edfed0202816f6"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yMlQxNjo1NDowOFrOGnJMIA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yMlQxNjo1NDowOFrOGnJMIA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzY5ODIwOA==", "bodyText": "Why was this added to SparkTableUtil? It doesn't use SparkTableUtil, so it is not related.\nCould you start a test suite, TestNameMappingProjection?", "url": "https://github.com/apache/iceberg/pull/1129#discussion_r443698208", "createdAt": "2020-06-22T16:54:08Z", "author": {"login": "rdblue"}, "path": "spark2/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java", "diffHunk": "@@ -298,4 +309,69 @@ public void testImportWithNameMappingForVectorizedParquetReader() throws Excepti\n \n     Assert.assertEquals(expected.stream().map(SimpleRecord::getData).collect(Collectors.toList()), actual);\n   }\n+\n+  @Test\n+  public void testAvroReaderWithNameMapping() throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ddf536dbfb75098000b9866b96edfed0202816f6"}, "originalPosition": 37}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2456a86fd382fce43891a88887cfa1b777c69a99", "author": {"user": {"login": "chenjunjiedada", "name": "Chen, Junjie"}}, "url": "https://github.com/apache/iceberg/commit/2456a86fd382fce43891a88887cfa1b777c69a99", "committedDate": "2020-06-23T07:32:03Z", "message": "Turn on name mapping for avro"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5fe0e6f1971eb2f3d0e6d030847a8865d8461201", "author": {"user": {"login": "chenjunjiedada", "name": "Chen, Junjie"}}, "url": "https://github.com/apache/iceberg/commit/5fe0e6f1971eb2f3d0e6d030847a8865d8461201", "committedDate": "2020-06-23T07:32:03Z", "message": "Add a unit test"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "ddf536dbfb75098000b9866b96edfed0202816f6", "author": {"user": {"login": "chenjunjiedada", "name": "Chen, Junjie"}}, "url": "https://github.com/apache/iceberg/commit/ddf536dbfb75098000b9866b96edfed0202816f6", "committedDate": "2020-06-22T07:40:22Z", "message": "Add a unit test"}, "afterCommit": {"oid": "8f104a34bd90b42f224cf2f4bb1d8a1127633336", "author": {"user": {"login": "chenjunjiedada", "name": "Chen, Junjie"}}, "url": "https://github.com/apache/iceberg/commit/8f104a34bd90b42f224cf2f4bb1d8a1127633336", "committedDate": "2020-06-23T08:06:35Z", "message": "address comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e89ec2d544510dd252bbcdda9fa08b78dd517f51", "author": {"user": {"login": "chenjunjiedada", "name": "Chen, Junjie"}}, "url": "https://github.com/apache/iceberg/commit/e89ec2d544510dd252bbcdda9fa08b78dd517f51", "committedDate": "2020-06-23T08:37:25Z", "message": "address comments"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "8f104a34bd90b42f224cf2f4bb1d8a1127633336", "author": {"user": {"login": "chenjunjiedada", "name": "Chen, Junjie"}}, "url": "https://github.com/apache/iceberg/commit/8f104a34bd90b42f224cf2f4bb1d8a1127633336", "committedDate": "2020-06-23T08:06:35Z", "message": "address comments"}, "afterCommit": {"oid": "e89ec2d544510dd252bbcdda9fa08b78dd517f51", "author": {"user": {"login": "chenjunjiedada", "name": "Chen, Junjie"}}, "url": "https://github.com/apache/iceberg/commit/e89ec2d544510dd252bbcdda9fa08b78dd517f51", "committedDate": "2020-06-23T08:37:25Z", "message": "address comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM1OTk4Nzcz", "url": "https://github.com/apache/iceberg/pull/1129#pullrequestreview-435998773", "createdAt": "2020-06-23T17:26:27Z", "commit": {"oid": "e89ec2d544510dd252bbcdda9fa08b78dd517f51"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNzoyNjoyN1rOGnzUbg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNzoyNjoyN1rOGnzUbg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDM4ODQ2Mg==", "bodyText": "This is the line that I would change, so that the table also has an ID column.", "url": "https://github.com/apache/iceberg/pull/1129#discussion_r444388462", "createdAt": "2020-06-23T17:26:27Z", "author": {"login": "rdblue"}, "path": "spark2/src/test/java/org/apache/iceberg/spark/source/TestNameMappingProjection.java", "diffHunk": "@@ -0,0 +1,144 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.List;\n+import org.apache.avro.SchemaBuilder;\n+import org.apache.avro.file.DataFileWriter;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.avro.generic.GenericDatumWriter;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.io.DatumWriter;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.avro.RemoveIds;\n+import org.apache.iceberg.hive.HiveTableBaseTest;\n+import org.apache.iceberg.mapping.MappingUtil;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.iceberg.mapping.NameMappingParser;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.BeforeClass;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_NAME_MAPPING;\n+import static org.apache.iceberg.types.Types.NestedField.required;\n+\n+public class TestNameMappingProjection extends HiveTableBaseTest {\n+  private static final Configuration CONF = HiveTableBaseTest.hiveConf;\n+  private static SparkSession spark = null;\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  @BeforeClass\n+  public static void startSpark() {\n+    String metastoreURI = CONF.get(HiveConf.ConfVars.METASTOREURIS.varname);\n+\n+    // Create a spark session.\n+    TestNameMappingProjection.spark = SparkSession.builder().master(\"local[2]\")\n+        .enableHiveSupport()\n+        .config(\"spark.hadoop.hive.metastore.uris\", metastoreURI)\n+        .config(\"hive.exec.dynamic.partition\", \"true\")\n+        .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")\n+        .config(\"spark.sql.legacy.allowCreatingManagedTableUsingNonemptyLocation\", \"true\")\n+        .getOrCreate();\n+  }\n+\n+  @AfterClass\n+  public static void stopSpark() {\n+    SparkSession currentSpark = TestNameMappingProjection.spark;\n+    // Stop the spark session.\n+    TestNameMappingProjection.spark = null;\n+    currentSpark.stop();\n+  }\n+\n+  @Test\n+  public void testAvroReaderWithNameMapping() throws IOException {\n+    File avroFile = temp.newFile();\n+    org.apache.avro.Schema avroSchema = SchemaBuilder.record(\"TestRecord\")\n+        .namespace(\"org.apache.iceberg.spark.data\")\n+        .fields()\n+        .requiredInt(\"id\")\n+        .requiredString(\"name\")\n+        .endRecord();\n+\n+    org.apache.avro.Schema avroSchemaWithoutIds = RemoveIds.removeIds(avroSchema);\n+\n+    GenericRecord record1 = new GenericData.Record(avroSchemaWithoutIds);\n+    record1.put(\"id\", 1);\n+    record1.put(\"name\", \"Bob\");\n+\n+    GenericRecord record2 = new GenericData.Record(avroSchemaWithoutIds);\n+    record2.put(\"id\", 2);\n+    record2.put(\"name\", \"Alice\");\n+\n+    DatumWriter<GenericRecord> datumWriter = new GenericDatumWriter<>(avroSchemaWithoutIds);\n+    DataFileWriter<GenericRecord> dataFileWriter = new DataFileWriter<>(datumWriter);\n+\n+    dataFileWriter.create(avroSchemaWithoutIds, avroFile);\n+    dataFileWriter.append(record1);\n+    dataFileWriter.append(record2);\n+    dataFileWriter.close();\n+\n+    DataFile avroDataFile = DataFiles.builder(PartitionSpec.unpartitioned())\n+        .withFormat(\"avro\")\n+        .withFileSizeInBytes(avroFile.length())\n+        .withPath(avroFile.getAbsolutePath())\n+        .withRecordCount(2)\n+        .build();\n+\n+    Schema filteredSchema = new Schema(\n+        required(1, \"name\", Types.StringType.get())\n+    );\n+    NameMapping nameMapping = MappingUtil.create(filteredSchema);\n+\n+    Table table = catalog.createTable(\n+        org.apache.iceberg.catalog.TableIdentifier.of(DB_NAME, \"avro_table\"),\n+        filteredSchema,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e89ec2d544510dd252bbcdda9fa08b78dd517f51"}, "originalPosition": 127}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM1OTk5MDkx", "url": "https://github.com/apache/iceberg/pull/1129#pullrequestreview-435999091", "createdAt": "2020-06-23T17:26:52Z", "commit": {"oid": "e89ec2d544510dd252bbcdda9fa08b78dd517f51"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNzoyNjo1MlrOGnzVXQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNzoyNjo1MlrOGnzVXQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDM4ODcwMQ==", "bodyText": "And we would need a second assertion here that the id value is null.", "url": "https://github.com/apache/iceberg/pull/1129#discussion_r444388701", "createdAt": "2020-06-23T17:26:52Z", "author": {"login": "rdblue"}, "path": "spark2/src/test/java/org/apache/iceberg/spark/source/TestNameMappingProjection.java", "diffHunk": "@@ -0,0 +1,144 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.List;\n+import org.apache.avro.SchemaBuilder;\n+import org.apache.avro.file.DataFileWriter;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.avro.generic.GenericDatumWriter;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.io.DatumWriter;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.avro.RemoveIds;\n+import org.apache.iceberg.hive.HiveTableBaseTest;\n+import org.apache.iceberg.mapping.MappingUtil;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.iceberg.mapping.NameMappingParser;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.BeforeClass;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_NAME_MAPPING;\n+import static org.apache.iceberg.types.Types.NestedField.required;\n+\n+public class TestNameMappingProjection extends HiveTableBaseTest {\n+  private static final Configuration CONF = HiveTableBaseTest.hiveConf;\n+  private static SparkSession spark = null;\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  @BeforeClass\n+  public static void startSpark() {\n+    String metastoreURI = CONF.get(HiveConf.ConfVars.METASTOREURIS.varname);\n+\n+    // Create a spark session.\n+    TestNameMappingProjection.spark = SparkSession.builder().master(\"local[2]\")\n+        .enableHiveSupport()\n+        .config(\"spark.hadoop.hive.metastore.uris\", metastoreURI)\n+        .config(\"hive.exec.dynamic.partition\", \"true\")\n+        .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")\n+        .config(\"spark.sql.legacy.allowCreatingManagedTableUsingNonemptyLocation\", \"true\")\n+        .getOrCreate();\n+  }\n+\n+  @AfterClass\n+  public static void stopSpark() {\n+    SparkSession currentSpark = TestNameMappingProjection.spark;\n+    // Stop the spark session.\n+    TestNameMappingProjection.spark = null;\n+    currentSpark.stop();\n+  }\n+\n+  @Test\n+  public void testAvroReaderWithNameMapping() throws IOException {\n+    File avroFile = temp.newFile();\n+    org.apache.avro.Schema avroSchema = SchemaBuilder.record(\"TestRecord\")\n+        .namespace(\"org.apache.iceberg.spark.data\")\n+        .fields()\n+        .requiredInt(\"id\")\n+        .requiredString(\"name\")\n+        .endRecord();\n+\n+    org.apache.avro.Schema avroSchemaWithoutIds = RemoveIds.removeIds(avroSchema);\n+\n+    GenericRecord record1 = new GenericData.Record(avroSchemaWithoutIds);\n+    record1.put(\"id\", 1);\n+    record1.put(\"name\", \"Bob\");\n+\n+    GenericRecord record2 = new GenericData.Record(avroSchemaWithoutIds);\n+    record2.put(\"id\", 2);\n+    record2.put(\"name\", \"Alice\");\n+\n+    DatumWriter<GenericRecord> datumWriter = new GenericDatumWriter<>(avroSchemaWithoutIds);\n+    DataFileWriter<GenericRecord> dataFileWriter = new DataFileWriter<>(datumWriter);\n+\n+    dataFileWriter.create(avroSchemaWithoutIds, avroFile);\n+    dataFileWriter.append(record1);\n+    dataFileWriter.append(record2);\n+    dataFileWriter.close();\n+\n+    DataFile avroDataFile = DataFiles.builder(PartitionSpec.unpartitioned())\n+        .withFormat(\"avro\")\n+        .withFileSizeInBytes(avroFile.length())\n+        .withPath(avroFile.getAbsolutePath())\n+        .withRecordCount(2)\n+        .build();\n+\n+    Schema filteredSchema = new Schema(\n+        required(1, \"name\", Types.StringType.get())\n+    );\n+    NameMapping nameMapping = MappingUtil.create(filteredSchema);\n+\n+    Table table = catalog.createTable(\n+        org.apache.iceberg.catalog.TableIdentifier.of(DB_NAME, \"avro_table\"),\n+        filteredSchema,\n+        PartitionSpec.unpartitioned());\n+\n+    table.updateProperties()\n+        .set(DEFAULT_NAME_MAPPING, NameMappingParser.toJson(nameMapping))\n+        .commit();\n+\n+    table.newFastAppend().appendFile(avroDataFile).commit();\n+\n+    List<Row> actual = spark.read().format(\"iceberg\")\n+        .load(DB_NAME + \".avro_table\")\n+        .filter(\"name='Alice'\")\n+        .collectAsList();\n+\n+    Assert.assertEquals(\"Should project 1 record\", 1, actual.size());\n+    Assert.assertEquals(\"Should equal to 'Alice'\", \"Alice\", actual.get(0).getString(0));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e89ec2d544510dd252bbcdda9fa08b78dd517f51"}, "originalPosition": 142}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4550, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}