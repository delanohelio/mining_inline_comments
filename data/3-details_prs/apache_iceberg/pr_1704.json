{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTEzODg5Nzk2", "number": 1704, "title": "Flink : Fix Repeated Rewrite for RewriteDataFilesAction", "bodyText": "the pr fix #1661\nIf DataFile cannot be  combined to CombinedScanTask with other DataFiles, the size of CombinedScanTask list size is 1, so we remove these CombinedScanTasks to avoid compressed repeatedly.", "createdAt": "2020-11-02T08:28:09Z", "url": "https://github.com/apache/iceberg/pull/1704", "merged": true, "mergeCommit": {"oid": "e3088300388512380aeb753ae391d05151de6866"}, "closed": true, "closedAt": "2020-12-31T09:53:53Z", "author": {"login": "zhangjun0x01"}, "timelineItems": {"totalCount": 28, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdZTvX8gFqTUyMzcwODU3MA==", "endCursor": "Y3Vyc29yOnYyOpPPAAABdrdor-ABqjQxNTg5OTk3MDk=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIzNzA4NTcw", "url": "https://github.com/apache/iceberg/pull/1704#pullrequestreview-523708570", "createdAt": "2020-11-04T20:25:17Z", "commit": {"oid": "31f66b872b5a2ee00ec0385376861eb4047f4747"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQyMDoyNToxN1rOHtocVA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQyMDoyNToxN1rOHtocVA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzYxMDU4MA==", "bodyText": "Typo in variable name", "url": "https://github.com/apache/iceberg/pull/1704#discussion_r517610580", "createdAt": "2020-11-04T20:25:17Z", "author": {"login": "RussellSpitzer"}, "path": "core/src/main/java/org/apache/iceberg/actions/BaseRewriteDataFilesAction.java", "diffHunk": "@@ -228,9 +228,18 @@ public RewriteDataFilesActionResult execute() {\n         .flatMap(Streams::stream)\n         .collect(Collectors.toList());\n \n+    // add a filter  to the CombinedScanTask list to avoid repeated rewrite datafile\n+    List<CombinedScanTask> fileterCombinedScanTasks =", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "31f66b872b5a2ee00ec0385376861eb4047f4747"}, "originalPosition": 5}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIzNzA5OTUz", "url": "https://github.com/apache/iceberg/pull/1704#pullrequestreview-523709953", "createdAt": "2020-11-04T20:27:22Z", "commit": {"oid": "31f66b872b5a2ee00ec0385376861eb4047f4747"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQyMDoyNzoyMlrOHtoggw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQyMDoyNzoyMlrOHtoggw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzYxMTY1MQ==", "bodyText": "Maybe more clear as !(task.files.nonempty)", "url": "https://github.com/apache/iceberg/pull/1704#discussion_r517611651", "createdAt": "2020-11-04T20:27:22Z", "author": {"login": "RussellSpitzer"}, "path": "core/src/main/java/org/apache/iceberg/actions/BaseRewriteDataFilesAction.java", "diffHunk": "@@ -228,9 +228,18 @@ public RewriteDataFilesActionResult execute() {\n         .flatMap(Streams::stream)\n         .collect(Collectors.toList());\n \n+    // add a filter  to the CombinedScanTask list to avoid repeated rewrite datafile\n+    List<CombinedScanTask> fileterCombinedScanTasks =\n+        combinedScanTasks.stream().filter(task -> task.files().size() > 1).collect(Collectors.toList());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "31f66b872b5a2ee00ec0385376861eb4047f4747"}, "originalPosition": 6}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIzNzExOTE1", "url": "https://github.com/apache/iceberg/pull/1704#pullrequestreview-523711915", "createdAt": "2020-11-04T20:30:12Z", "commit": {"oid": "31f66b872b5a2ee00ec0385376861eb4047f4747"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQyMDozMDoxM1rOHtom_g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQyMDozMDoxM1rOHtom_g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzYxMzMxMA==", "bodyText": "I think the flatmap here is unecessary since we can flatMap within the map operation instead,\n    filteredCombinedScanTasks.stream()\n        .flatMap(task -> task.files().stream().map(FileScanTask::file))\n        .collect(Collectors.toList())\n\nThis should skip at least one List allocation :)", "url": "https://github.com/apache/iceberg/pull/1704#discussion_r517613310", "createdAt": "2020-11-04T20:30:13Z", "author": {"login": "RussellSpitzer"}, "path": "core/src/main/java/org/apache/iceberg/actions/BaseRewriteDataFilesAction.java", "diffHunk": "@@ -228,9 +228,18 @@ public RewriteDataFilesActionResult execute() {\n         .flatMap(Streams::stream)\n         .collect(Collectors.toList());\n \n+    // add a filter  to the CombinedScanTask list to avoid repeated rewrite datafile\n+    List<CombinedScanTask> fileterCombinedScanTasks =\n+        combinedScanTasks.stream().filter(task -> task.files().size() > 1).collect(Collectors.toList());\n+\n+    if (fileterCombinedScanTasks.isEmpty()) {\n+      return RewriteDataFilesActionResult.empty();\n+    }\n+\n     List<DataFile> addedDataFiles = rewriteDataForTasks(combinedScanTasks);\n-    List<DataFile> currentDataFiles = filteredGroupedTasks.values().stream()\n-        .flatMap(tasks -> tasks.stream().map(FileScanTask::file))\n+    List<DataFile> currentDataFiles = fileterCombinedScanTasks.stream()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "31f66b872b5a2ee00ec0385376861eb4047f4747"}, "originalPosition": 15}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "31f66b872b5a2ee00ec0385376861eb4047f4747", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/31f66b872b5a2ee00ec0385376861eb4047f4747", "committedDate": "2020-11-02T07:28:16Z", "message": "FixRepeatedRewrite"}, "afterCommit": {"oid": "d925f9eacd41bd54708f3d451b045d0a3abe6490", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/d925f9eacd41bd54708f3d451b045d0a3abe6490", "committedDate": "2020-11-05T00:58:17Z", "message": "FixRepeatedRewrite"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "5f397d30eb670663b678ed526eab55803eb248ec", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/5f397d30eb670663b678ed526eab55803eb248ec", "committedDate": "2020-11-05T01:26:19Z", "message": "FixRepeatedRewrite"}, "afterCommit": {"oid": "edc890acd5bb90bbfeda3cce07270dbc3d75d899", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/edc890acd5bb90bbfeda3cce07270dbc3d75d899", "committedDate": "2020-11-10T05:29:05Z", "message": "FixRepeatedRewrite"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "edc890acd5bb90bbfeda3cce07270dbc3d75d899", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/edc890acd5bb90bbfeda3cce07270dbc3d75d899", "committedDate": "2020-11-10T05:29:05Z", "message": "FixRepeatedRewrite"}, "afterCommit": {"oid": "8927787217fde3ce34403ed4e51fb6f85d1e4f1a", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/8927787217fde3ce34403ed4e51fb6f85d1e4f1a", "committedDate": "2020-11-12T06:15:23Z", "message": "add UT"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "8927787217fde3ce34403ed4e51fb6f85d1e4f1a", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/8927787217fde3ce34403ed4e51fb6f85d1e4f1a", "committedDate": "2020-11-12T06:15:23Z", "message": "add UT"}, "afterCommit": {"oid": "9653e59c5f9e83bf1f811082b12c7278a1074e0a", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/9653e59c5f9e83bf1f811082b12c7278a1074e0a", "committedDate": "2020-11-12T06:23:38Z", "message": "add UT"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "9653e59c5f9e83bf1f811082b12c7278a1074e0a", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/9653e59c5f9e83bf1f811082b12c7278a1074e0a", "committedDate": "2020-11-12T06:23:38Z", "message": "add UT"}, "afterCommit": {"oid": "6a7c9a2ed3895d60b712d809662fb76c4997de53", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/6a7c9a2ed3895d60b712d809662fb76c4997de53", "committedDate": "2020-11-12T06:15:23Z", "message": "FixRepeatedRewrite"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "6a7c9a2ed3895d60b712d809662fb76c4997de53", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/6a7c9a2ed3895d60b712d809662fb76c4997de53", "committedDate": "2020-11-12T06:15:23Z", "message": "FixRepeatedRewrite"}, "afterCommit": {"oid": "e6af44271a45fcd93868edc0e103d4b539f22093", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/e6af44271a45fcd93868edc0e103d4b539f22093", "committedDate": "2020-11-13T01:28:25Z", "message": "add UT testRewriteAvoidRepeateCompress"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "e6af44271a45fcd93868edc0e103d4b539f22093", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/e6af44271a45fcd93868edc0e103d4b539f22093", "committedDate": "2020-11-13T01:28:25Z", "message": "add UT testRewriteAvoidRepeateCompress"}, "afterCommit": {"oid": "50347a4e1e2588385236f322e568e081549d5dc1", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/50347a4e1e2588385236f322e568e081549d5dc1", "committedDate": "2020-11-13T01:37:59Z", "message": "add UT testRewriteAvoidRepeateCompress"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI5NjgwNDM0", "url": "https://github.com/apache/iceberg/pull/1704#pullrequestreview-529680434", "createdAt": "2020-11-13T03:29:32Z", "commit": {"oid": "50347a4e1e2588385236f322e568e081549d5dc1"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QwMzoyOTozM1rOHyYb6w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QwMzoyOTozM1rOHyYb6w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU5MTIxMQ==", "bodyText": "Is this change correct  ?   If we have two file1 (200MB),  file2 (1MB), and the target file size is 128MB, then it will be planned into three combined tasks:  128MB, 72MB, 1MB.  Finally the file1 will occur twice in the currentDataFiles  ?", "url": "https://github.com/apache/iceberg/pull/1704#discussion_r522591211", "createdAt": "2020-11-13T03:29:33Z", "author": {"login": "openinx"}, "path": "core/src/main/java/org/apache/iceberg/actions/BaseRewriteDataFilesAction.java", "diffHunk": "@@ -226,11 +226,16 @@ public RewriteDataFilesActionResult execute() {\n           return TableScanUtil.planTasks(splitTasks, targetSizeInBytes, splitLookback, splitOpenFileCost);\n         })\n         .flatMap(Streams::stream)\n+        .filter(task -> task.files().size() > 1)\n         .collect(Collectors.toList());\n \n+    if (combinedScanTasks.isEmpty()) {\n+      return RewriteDataFilesActionResult.empty();\n+    }\n+\n     List<DataFile> addedDataFiles = rewriteDataForTasks(combinedScanTasks);\n-    List<DataFile> currentDataFiles = filteredGroupedTasks.values().stream()\n-        .flatMap(tasks -> tasks.stream().map(FileScanTask::file))\n+    List<DataFile> currentDataFiles = combinedScanTasks.stream()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "50347a4e1e2588385236f322e568e081549d5dc1"}, "originalPosition": 14}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI5NjgyMzg3", "url": "https://github.com/apache/iceberg/pull/1704#pullrequestreview-529682387", "createdAt": "2020-11-13T03:36:30Z", "commit": {"oid": "50347a4e1e2588385236f322e568e081549d5dc1"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QwMzozNjozMFrOHyYiQg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QwMzozNjozMFrOHyYiQg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU5MjgzNA==", "bodyText": "nit:  do we need to concat such a complex string for the data  ?  It doesn't look elegant.", "url": "https://github.com/apache/iceberg/pull/1704#discussion_r522592834", "createdAt": "2020-11-13T03:36:30Z", "author": {"login": "openinx"}, "path": "flink/src/test/java/org/apache/iceberg/flink/actions/TestRewriteDataFilesAction.java", "diffHunk": "@@ -280,4 +280,91 @@ public void testRewriteLargeTableHasResiduals() throws IOException {\n     // Assert the table records as expected.\n     SimpleDataUtil.assertTableRecords(icebergTableUnPartitioned, expected);\n   }\n+\n+  /**\n+   * a test case to test avoid repeate compress\n+   * <p>\n+   * If datafile cannot be combined to CombinedScanTask with other DataFiles, the size of the CombinedScanTask list size\n+   * is 1, so we remove these CombinedScanTasks to avoid compressed repeatedly.\n+   * <p>\n+   * In this test case,we generated 3 data files and set targetSizeInBytes greater than the largest file size so that\n+   * it cannot be  combined a CombinedScanTask with other datafiles. The datafile with the largest file size will not be\n+   * compressed.\n+   * <p>\n+   * For the same data, the file sizes of different formats are different. The file sizes of different formats generated\n+   * by the data in the current test case are as follows:\n+   * <p>\n+   *   avro :\n+   *  size  file\n+   *  408 00000-0-5a218337-1742-4ed1-83d8-55e301da49b8-00001.avro\n+   * 2390 00000-0-8f431924-ec8d-4957-a238-b8fe2b136210-00001.avro\n+   *  408 00000-0-9c75bcc4-49f0-4722-9528-c1d5faa50fa7-00001.avro\n+   *\n+   * orc :\n+   * size  file\n+   * 1626 00000-0-260d42d1-f00f-4c5f-9628-5f41f6395093-00001.orc\n+   *  331 00000-0-942fd38b-d7af-4ad2-a985-0e6ccdb4d8d3-00001.orc\n+   *  333 00000-0-ad8f2c34-6cf7-43fe-990f-f8f6389d198e-00001.orc\n+   *\n+   * parquet :\n+   * size  file\n+   *  611 00000-0-84e1fd63-a840-4a23-983f-5247e9218cbe-00001.parquet\n+   *  611 00000-0-91b070f0-7d17-487c-97ec-de0f0b09aa31-00001.parquet\n+   * 2691 00000-0-e09c969d-d6ee-4a41-9e42-9dcbf42bc4e1-00001.parquet\n+   *\n+   * @throws IOException IOException\n+   */\n+  @Test\n+  public void testRewriteAvoidRepeateCompress() throws IOException {\n+    List<String> records = Lists.newArrayList();\n+    List<Record> expected = Lists.newArrayList();\n+    for (int i = 0; i < 500; i++) {\n+      String data = String.valueOf(i) + \"hello iceberg,hello flink\";\n+      records.add(\"(\" + i + \",'\" + data + \"')\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "50347a4e1e2588385236f322e568e081549d5dc1"}, "originalPosition": 44}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "50347a4e1e2588385236f322e568e081549d5dc1", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/50347a4e1e2588385236f322e568e081549d5dc1", "committedDate": "2020-11-13T01:37:59Z", "message": "add UT testRewriteAvoidRepeateCompress"}, "afterCommit": {"oid": "3eeb5879b8a48f0306e2177082e6e9bf599b1933", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/3eeb5879b8a48f0306e2177082e6e9bf599b1933", "committedDate": "2020-11-17T02:00:25Z", "message": "generate file by FileAppender"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "3eeb5879b8a48f0306e2177082e6e9bf599b1933", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/3eeb5879b8a48f0306e2177082e6e9bf599b1933", "committedDate": "2020-11-17T02:00:25Z", "message": "generate file by FileAppender"}, "afterCommit": {"oid": "a09f0adb6a0cd3327f75eb479ba3507b7629b283", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/a09f0adb6a0cd3327f75eb479ba3507b7629b283", "committedDate": "2020-11-27T03:51:56Z", "message": "generate file by FileAppender"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQxNzQyNzUx", "url": "https://github.com/apache/iceberg/pull/1704#pullrequestreview-541742751", "createdAt": "2020-12-01T10:08:50Z", "commit": {"oid": "a09f0adb6a0cd3327f75eb479ba3507b7629b283"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMDowODo1MFrOH8kH7w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMDowODo1MFrOH8kH7w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzI2ODQ2Mw==", "bodyText": "Use Assume.assumeFalse(\"ORC does not support getting length when file is opening\", format.equals(FileFormat.ORC));", "url": "https://github.com/apache/iceberg/pull/1704#discussion_r533268463", "createdAt": "2020-12-01T10:08:50Z", "author": {"login": "openinx"}, "path": "flink/src/test/java/org/apache/iceberg/flink/actions/TestRewriteDataFilesAction.java", "diffHunk": "@@ -280,4 +290,79 @@ public void testRewriteLargeTableHasResiduals() throws IOException {\n     // Assert the table records as expected.\n     SimpleDataUtil.assertTableRecords(icebergTableUnPartitioned, expected);\n   }\n+\n+  /**\n+   * a test case to test avoid repeate compress\n+   * <p>\n+   * If datafile cannot be combined to CombinedScanTask with other DataFiles, the size of the CombinedScanTask list size\n+   * is 1, so we remove these CombinedScanTasks to avoid compressed repeatedly.\n+   * <p>\n+   * In this test case,we generated 3 data files and set targetSizeInBytes greater than the largest file size so that it\n+   * cannot be  combined a CombinedScanTask with other datafiles. The datafile with the largest file size will not be\n+   * compressed.\n+   *\n+   * @throws IOException IOException\n+   */\n+  @Test\n+  public void testRewriteAvoidRepeateCompress() throws IOException {\n+    if (!format.equals(FileFormat.ORC)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a09f0adb6a0cd3327f75eb479ba3507b7629b283"}, "originalPosition": 65}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQxNzQ4ODI4", "url": "https://github.com/apache/iceberg/pull/1704#pullrequestreview-541748828", "createdAt": "2020-12-01T10:16:32Z", "commit": {"oid": "a09f0adb6a0cd3327f75eb479ba3507b7629b283"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMDoxNjozM1rOH8kq8A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMDoxNjozM1rOH8kq8A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzI3NzQyNA==", "bodyText": "Use the try(...){} to close the fileAppender like this:\n      File file = temp.newFile();\n\n      int fileSize = 2000;\n      try (FileAppender<Record> fileAppender = genericAppenderFactory.newAppender(Files.localOutput(file), format)) {\n        for (int idx = 0; fileAppender.length() < fileSize; idx++) {\n          Record record = RECORD.copy();\n          record.setField(\"id\", idx);\n          record.setField(\"data\", \"iceberg\");\n          fileAppender.add(record);\n          expected.add(record);\n        }\n      }\n\n      DataFile dataFile = DataFiles.builder(icebergTableUnPartitioned.spec())\n          .withPath(file.getAbsolutePath())\n          .withFileSizeInBytes(file.length())\n          .withFormat(format)\n          .withRecordCount(expected.size())\n          .build();", "url": "https://github.com/apache/iceberg/pull/1704#discussion_r533277424", "createdAt": "2020-12-01T10:16:33Z", "author": {"login": "openinx"}, "path": "flink/src/test/java/org/apache/iceberg/flink/actions/TestRewriteDataFilesAction.java", "diffHunk": "@@ -280,4 +290,79 @@ public void testRewriteLargeTableHasResiduals() throws IOException {\n     // Assert the table records as expected.\n     SimpleDataUtil.assertTableRecords(icebergTableUnPartitioned, expected);\n   }\n+\n+  /**\n+   * a test case to test avoid repeate compress\n+   * <p>\n+   * If datafile cannot be combined to CombinedScanTask with other DataFiles, the size of the CombinedScanTask list size\n+   * is 1, so we remove these CombinedScanTasks to avoid compressed repeatedly.\n+   * <p>\n+   * In this test case,we generated 3 data files and set targetSizeInBytes greater than the largest file size so that it\n+   * cannot be  combined a CombinedScanTask with other datafiles. The datafile with the largest file size will not be\n+   * compressed.\n+   *\n+   * @throws IOException IOException\n+   */\n+  @Test\n+  public void testRewriteAvoidRepeateCompress() throws IOException {\n+    if (!format.equals(FileFormat.ORC)) {\n+      List<Record> expected = Lists.newArrayList();\n+      Schema schema = icebergTableUnPartitioned.schema();\n+      GenericAppenderFactory genericAppenderFactory = new GenericAppenderFactory(schema);\n+      File file = temp.newFile();\n+      FileAppender<Record> fileAppender = genericAppenderFactory.newAppender(Files.localOutput(file), format);\n+      long filesize = 20000;\n+      int count = 0;\n+      for (; fileAppender.length() < filesize; count++) {\n+        Record record = RECORD.copy();\n+        record.setField(\"id\", count);\n+        record.setField(\"data\", \"iceberg\");\n+        fileAppender.add(record);\n+        expected.add(record);\n+      }\n+      fileAppender.close();\n+\n+      DataFile dataFile = DataFiles.builder(icebergTableUnPartitioned.spec())\n+          .withPath(file.getAbsolutePath())\n+          .withFileSizeInBytes(file.length())\n+          .withFormat(format)\n+          .withRecordCount(count)\n+          .build();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a09f0adb6a0cd3327f75eb479ba3507b7629b283"}, "originalPosition": 87}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQxNzUwNTk5", "url": "https://github.com/apache/iceberg/pull/1704#pullrequestreview-541750599", "createdAt": "2020-12-01T10:18:37Z", "commit": {"oid": "a09f0adb6a0cd3327f75eb479ba3507b7629b283"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMDoxODozN1rOH8k08g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMDoxODozN1rOH8k08g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzI3OTk4Ng==", "bodyText": "I think we'd better to check that the compaction did not compaction the biggest file with one of the two small files ?     That means the dataFiles1  should have included the dataFile ?", "url": "https://github.com/apache/iceberg/pull/1704#discussion_r533279986", "createdAt": "2020-12-01T10:18:37Z", "author": {"login": "openinx"}, "path": "flink/src/test/java/org/apache/iceberg/flink/actions/TestRewriteDataFilesAction.java", "diffHunk": "@@ -280,4 +290,79 @@ public void testRewriteLargeTableHasResiduals() throws IOException {\n     // Assert the table records as expected.\n     SimpleDataUtil.assertTableRecords(icebergTableUnPartitioned, expected);\n   }\n+\n+  /**\n+   * a test case to test avoid repeate compress\n+   * <p>\n+   * If datafile cannot be combined to CombinedScanTask with other DataFiles, the size of the CombinedScanTask list size\n+   * is 1, so we remove these CombinedScanTasks to avoid compressed repeatedly.\n+   * <p>\n+   * In this test case,we generated 3 data files and set targetSizeInBytes greater than the largest file size so that it\n+   * cannot be  combined a CombinedScanTask with other datafiles. The datafile with the largest file size will not be\n+   * compressed.\n+   *\n+   * @throws IOException IOException\n+   */\n+  @Test\n+  public void testRewriteAvoidRepeateCompress() throws IOException {\n+    if (!format.equals(FileFormat.ORC)) {\n+      List<Record> expected = Lists.newArrayList();\n+      Schema schema = icebergTableUnPartitioned.schema();\n+      GenericAppenderFactory genericAppenderFactory = new GenericAppenderFactory(schema);\n+      File file = temp.newFile();\n+      FileAppender<Record> fileAppender = genericAppenderFactory.newAppender(Files.localOutput(file), format);\n+      long filesize = 20000;\n+      int count = 0;\n+      for (; fileAppender.length() < filesize; count++) {\n+        Record record = RECORD.copy();\n+        record.setField(\"id\", count);\n+        record.setField(\"data\", \"iceberg\");\n+        fileAppender.add(record);\n+        expected.add(record);\n+      }\n+      fileAppender.close();\n+\n+      DataFile dataFile = DataFiles.builder(icebergTableUnPartitioned.spec())\n+          .withPath(file.getAbsolutePath())\n+          .withFileSizeInBytes(file.length())\n+          .withFormat(format)\n+          .withRecordCount(count)\n+          .build();\n+\n+      icebergTableUnPartitioned.newAppend()\n+          .appendFile(dataFile)\n+          .commit();\n+\n+      sql(\"INSERT INTO %s SELECT 1,'a' \", TABLE_NAME_UNPARTITIONED);\n+      sql(\"INSERT INTO %s SELECT 2,'b' \", TABLE_NAME_UNPARTITIONED);\n+\n+      icebergTableUnPartitioned.refresh();\n+\n+      CloseableIterable<FileScanTask> tasks = icebergTableUnPartitioned.newScan().planFiles();\n+      List<DataFile> dataFiles = Lists.newArrayList(CloseableIterable.transform(tasks, FileScanTask::file));\n+      Assert.assertEquals(\"Should have 3 data files before rewrite\", 3, dataFiles.size());\n+\n+      Actions actions = Actions.forTable(icebergTableUnPartitioned);\n+\n+      long targetSizeInBytes = file.length() + 10;\n+      RewriteDataFilesActionResult result = actions\n+          .rewriteDataFiles()\n+          .targetSizeInBytes(targetSizeInBytes)\n+          .splitOpenFileCost(1)\n+          .execute();\n+      Assert.assertEquals(\"Action should rewrite 2 data files\", 2, result.deletedDataFiles().size());\n+      Assert.assertEquals(\"Action should add 1 data file\", 1, result.addedDataFiles().size());\n+\n+      icebergTableUnPartitioned.refresh();\n+\n+      CloseableIterable<FileScanTask> tasks1 = icebergTableUnPartitioned.newScan().planFiles();\n+      List<DataFile> dataFiles1 = Lists.newArrayList(CloseableIterable.transform(tasks1, FileScanTask::file));\n+      Assert.assertEquals(\"Should have 2 data files after rewrite\", 2, dataFiles1.size());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a09f0adb6a0cd3327f75eb479ba3507b7629b283"}, "originalPosition": 117}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "a09f0adb6a0cd3327f75eb479ba3507b7629b283", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/a09f0adb6a0cd3327f75eb479ba3507b7629b283", "committedDate": "2020-11-27T03:51:56Z", "message": "generate file by FileAppender"}, "afterCommit": {"oid": "df3918aea6408c36cb119399ab2cf36f457acc0c", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/df3918aea6408c36cb119399ab2cf36f457acc0c", "committedDate": "2020-12-04T09:19:43Z", "message": "fix test case\n\nfix test case"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU5NDUwNzkz", "url": "https://github.com/apache/iceberg/pull/1704#pullrequestreview-559450793", "createdAt": "2020-12-29T09:36:36Z", "commit": {"oid": "df3918aea6408c36cb119399ab2cf36f457acc0c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwOTozNjozNlrOIMLH9A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwOTozNjozNlrOIMLH9A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTYzNjA4NA==", "bodyText": "nit: we could just use the the:\nRecord record = SimpleDataUtil.createRecord(count, \"iceberg\");", "url": "https://github.com/apache/iceberg/pull/1704#discussion_r549636084", "createdAt": "2020-12-29T09:36:36Z", "author": {"login": "openinx"}, "path": "flink/src/test/java/org/apache/iceberg/flink/actions/TestRewriteDataFilesAction.java", "diffHunk": "@@ -280,4 +292,82 @@ public void testRewriteLargeTableHasResiduals() throws IOException {\n     // Assert the table records as expected.\n     SimpleDataUtil.assertTableRecords(icebergTableUnPartitioned, expected);\n   }\n+\n+  /**\n+   * a test case to test avoid repeate compress\n+   * <p>\n+   * If datafile cannot be combined to CombinedScanTask with other DataFiles, the size of the CombinedScanTask list size\n+   * is 1, so we remove these CombinedScanTasks to avoid compressed repeatedly.\n+   * <p>\n+   * In this test case,we generated 3 data files and set targetSizeInBytes greater than the largest file size so that it\n+   * cannot be  combined a CombinedScanTask with other datafiles. The datafile with the largest file size will not be\n+   * compressed.\n+   *\n+   * @throws IOException IOException\n+   */\n+  @Test\n+  public void testRewriteAvoidRepeateCompress() throws IOException {\n+    Assume.assumeFalse(\"ORC does not support getting length when file is opening\", format.equals(FileFormat.ORC));\n+    List<Record> expected = Lists.newArrayList();\n+    Schema schema = icebergTableUnPartitioned.schema();\n+    GenericAppenderFactory genericAppenderFactory = new GenericAppenderFactory(schema);\n+    File file = temp.newFile();\n+    int count = 0;\n+    try (FileAppender<Record> fileAppender = genericAppenderFactory.newAppender(Files.localOutput(file), format)) {\n+      long filesize = 20000;\n+      for (; fileAppender.length() < filesize; count++) {\n+        Record record = RECORD.copy();\n+        record.setField(\"id\", count);\n+        record.setField(\"data\", \"iceberg\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "df3918aea6408c36cb119399ab2cf36f457acc0c"}, "originalPosition": 78}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU5NDUzMDY5", "url": "https://github.com/apache/iceberg/pull/1704#pullrequestreview-559453069", "createdAt": "2020-12-29T09:42:50Z", "commit": {"oid": "df3918aea6408c36cb119399ab2cf36f457acc0c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwOTo0Mjo1MFrOIMLPQA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwOTo0Mjo1MFrOIMLPQA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTYzNzk1Mg==", "bodyText": "nit:  Rewrited -> Rewrote,  or we could just name it newDataFiles.", "url": "https://github.com/apache/iceberg/pull/1704#discussion_r549637952", "createdAt": "2020-12-29T09:42:50Z", "author": {"login": "openinx"}, "path": "flink/src/test/java/org/apache/iceberg/flink/actions/TestRewriteDataFilesAction.java", "diffHunk": "@@ -280,4 +292,82 @@ public void testRewriteLargeTableHasResiduals() throws IOException {\n     // Assert the table records as expected.\n     SimpleDataUtil.assertTableRecords(icebergTableUnPartitioned, expected);\n   }\n+\n+  /**\n+   * a test case to test avoid repeate compress\n+   * <p>\n+   * If datafile cannot be combined to CombinedScanTask with other DataFiles, the size of the CombinedScanTask list size\n+   * is 1, so we remove these CombinedScanTasks to avoid compressed repeatedly.\n+   * <p>\n+   * In this test case,we generated 3 data files and set targetSizeInBytes greater than the largest file size so that it\n+   * cannot be  combined a CombinedScanTask with other datafiles. The datafile with the largest file size will not be\n+   * compressed.\n+   *\n+   * @throws IOException IOException\n+   */\n+  @Test\n+  public void testRewriteAvoidRepeateCompress() throws IOException {\n+    Assume.assumeFalse(\"ORC does not support getting length when file is opening\", format.equals(FileFormat.ORC));\n+    List<Record> expected = Lists.newArrayList();\n+    Schema schema = icebergTableUnPartitioned.schema();\n+    GenericAppenderFactory genericAppenderFactory = new GenericAppenderFactory(schema);\n+    File file = temp.newFile();\n+    int count = 0;\n+    try (FileAppender<Record> fileAppender = genericAppenderFactory.newAppender(Files.localOutput(file), format)) {\n+      long filesize = 20000;\n+      for (; fileAppender.length() < filesize; count++) {\n+        Record record = RECORD.copy();\n+        record.setField(\"id\", count);\n+        record.setField(\"data\", \"iceberg\");\n+        fileAppender.add(record);\n+        expected.add(record);\n+      }\n+    }\n+\n+    DataFile dataFile = DataFiles.builder(icebergTableUnPartitioned.spec())\n+        .withPath(file.getAbsolutePath())\n+        .withFileSizeInBytes(file.length())\n+        .withFormat(format)\n+        .withRecordCount(count)\n+        .build();\n+\n+    icebergTableUnPartitioned.newAppend()\n+        .appendFile(dataFile)\n+        .commit();\n+\n+    sql(\"INSERT INTO %s SELECT 1,'a' \", TABLE_NAME_UNPARTITIONED);\n+    sql(\"INSERT INTO %s SELECT 2,'b' \", TABLE_NAME_UNPARTITIONED);\n+\n+    icebergTableUnPartitioned.refresh();\n+\n+    CloseableIterable<FileScanTask> tasks = icebergTableUnPartitioned.newScan().planFiles();\n+    List<DataFile> dataFiles = Lists.newArrayList(CloseableIterable.transform(tasks, FileScanTask::file));\n+    Assert.assertEquals(\"Should have 3 data files before rewrite\", 3, dataFiles.size());\n+\n+    Actions actions = Actions.forTable(icebergTableUnPartitioned);\n+\n+    long targetSizeInBytes = file.length() + 10;\n+    RewriteDataFilesActionResult result = actions\n+        .rewriteDataFiles()\n+        .targetSizeInBytes(targetSizeInBytes)\n+        .splitOpenFileCost(1)\n+        .execute();\n+    Assert.assertEquals(\"Action should rewrite 2 data files\", 2, result.deletedDataFiles().size());\n+    Assert.assertEquals(\"Action should add 1 data file\", 1, result.addedDataFiles().size());\n+\n+    icebergTableUnPartitioned.refresh();\n+\n+    CloseableIterable<FileScanTask> tasks1 = icebergTableUnPartitioned.newScan().planFiles();\n+    List<DataFile> dataFilesRewrited = Lists.newArrayList(CloseableIterable.transform(tasks1, FileScanTask::file));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "df3918aea6408c36cb119399ab2cf36f457acc0c"}, "originalPosition": 118}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU5NDU0MTg3", "url": "https://github.com/apache/iceberg/pull/1704#pullrequestreview-559454187", "createdAt": "2020-12-29T09:45:51Z", "commit": {"oid": "df3918aea6408c36cb119399ab2cf36f457acc0c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwOTo0NTo1MVrOIMLTHQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwOTo0NTo1MVrOIMLTHQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTYzODk0MQ==", "bodyText": "nit:  could use the method references here.\n        // The biggest file do not be rewrote.\n    List<CharSequence> newPaths = newDataFiles.stream().map(ContentFile::path).collect(Collectors.toList());\n    Assert.assertTrue(newPaths.contains(file.getAbsolutePath()));", "url": "https://github.com/apache/iceberg/pull/1704#discussion_r549638941", "createdAt": "2020-12-29T09:45:51Z", "author": {"login": "openinx"}, "path": "flink/src/test/java/org/apache/iceberg/flink/actions/TestRewriteDataFilesAction.java", "diffHunk": "@@ -280,4 +292,82 @@ public void testRewriteLargeTableHasResiduals() throws IOException {\n     // Assert the table records as expected.\n     SimpleDataUtil.assertTableRecords(icebergTableUnPartitioned, expected);\n   }\n+\n+  /**\n+   * a test case to test avoid repeate compress\n+   * <p>\n+   * If datafile cannot be combined to CombinedScanTask with other DataFiles, the size of the CombinedScanTask list size\n+   * is 1, so we remove these CombinedScanTasks to avoid compressed repeatedly.\n+   * <p>\n+   * In this test case,we generated 3 data files and set targetSizeInBytes greater than the largest file size so that it\n+   * cannot be  combined a CombinedScanTask with other datafiles. The datafile with the largest file size will not be\n+   * compressed.\n+   *\n+   * @throws IOException IOException\n+   */\n+  @Test\n+  public void testRewriteAvoidRepeateCompress() throws IOException {\n+    Assume.assumeFalse(\"ORC does not support getting length when file is opening\", format.equals(FileFormat.ORC));\n+    List<Record> expected = Lists.newArrayList();\n+    Schema schema = icebergTableUnPartitioned.schema();\n+    GenericAppenderFactory genericAppenderFactory = new GenericAppenderFactory(schema);\n+    File file = temp.newFile();\n+    int count = 0;\n+    try (FileAppender<Record> fileAppender = genericAppenderFactory.newAppender(Files.localOutput(file), format)) {\n+      long filesize = 20000;\n+      for (; fileAppender.length() < filesize; count++) {\n+        Record record = RECORD.copy();\n+        record.setField(\"id\", count);\n+        record.setField(\"data\", \"iceberg\");\n+        fileAppender.add(record);\n+        expected.add(record);\n+      }\n+    }\n+\n+    DataFile dataFile = DataFiles.builder(icebergTableUnPartitioned.spec())\n+        .withPath(file.getAbsolutePath())\n+        .withFileSizeInBytes(file.length())\n+        .withFormat(format)\n+        .withRecordCount(count)\n+        .build();\n+\n+    icebergTableUnPartitioned.newAppend()\n+        .appendFile(dataFile)\n+        .commit();\n+\n+    sql(\"INSERT INTO %s SELECT 1,'a' \", TABLE_NAME_UNPARTITIONED);\n+    sql(\"INSERT INTO %s SELECT 2,'b' \", TABLE_NAME_UNPARTITIONED);\n+\n+    icebergTableUnPartitioned.refresh();\n+\n+    CloseableIterable<FileScanTask> tasks = icebergTableUnPartitioned.newScan().planFiles();\n+    List<DataFile> dataFiles = Lists.newArrayList(CloseableIterable.transform(tasks, FileScanTask::file));\n+    Assert.assertEquals(\"Should have 3 data files before rewrite\", 3, dataFiles.size());\n+\n+    Actions actions = Actions.forTable(icebergTableUnPartitioned);\n+\n+    long targetSizeInBytes = file.length() + 10;\n+    RewriteDataFilesActionResult result = actions\n+        .rewriteDataFiles()\n+        .targetSizeInBytes(targetSizeInBytes)\n+        .splitOpenFileCost(1)\n+        .execute();\n+    Assert.assertEquals(\"Action should rewrite 2 data files\", 2, result.deletedDataFiles().size());\n+    Assert.assertEquals(\"Action should add 1 data file\", 1, result.addedDataFiles().size());\n+\n+    icebergTableUnPartitioned.refresh();\n+\n+    CloseableIterable<FileScanTask> tasks1 = icebergTableUnPartitioned.newScan().planFiles();\n+    List<DataFile> dataFilesRewrited = Lists.newArrayList(CloseableIterable.transform(tasks1, FileScanTask::file));\n+    Assert.assertEquals(\"Should have 2 data files after rewrite\", 2, dataFilesRewrited.size());\n+\n+    // the biggest file do not be rewrited\n+    List rewritedDataFileNames = dataFilesRewrited.stream().map(df -> df.path()).collect(Collectors.toList());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "df3918aea6408c36cb119399ab2cf36f457acc0c"}, "originalPosition": 122}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "df3918aea6408c36cb119399ab2cf36f457acc0c", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/df3918aea6408c36cb119399ab2cf36f457acc0c", "committedDate": "2020-12-04T09:19:43Z", "message": "fix test case\n\nfix test case"}, "afterCommit": {"oid": "88f5b331e7e0d2c9fb2df6b4914beeca4026aaf3", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/88f5b331e7e0d2c9fb2df6b4914beeca4026aaf3", "committedDate": "2020-12-30T02:00:47Z", "message": "fix typo"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fa106b398b69dc080a3e0d84128cd6d6235c4556", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/fa106b398b69dc080a3e0d84128cd6d6235c4556", "committedDate": "2020-12-31T06:07:14Z", "message": "FixRepeatedRewrite"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9e7bdbb0a29fe19519b569a1ffd7a232932cac26", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/9e7bdbb0a29fe19519b569a1ffd7a232932cac26", "committedDate": "2020-12-31T06:07:15Z", "message": "add UT testRewriteAvoidRepeateCompress"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "faa9e0d6c5dea3b4dc010b1a2f11715e852586c6", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/faa9e0d6c5dea3b4dc010b1a2f11715e852586c6", "committedDate": "2020-12-31T06:07:15Z", "message": "generate file by FileAppender"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cd1fdbd08a2fae6be19b3f2e87797a31f0018aa8", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/cd1fdbd08a2fae6be19b3f2e87797a31f0018aa8", "committedDate": "2020-12-31T06:07:15Z", "message": "fix test case\n\nfix test case"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "03a2236f91234cd57ce7641d91b252a4be7edcf4", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/03a2236f91234cd57ce7641d91b252a4be7edcf4", "committedDate": "2020-12-31T06:07:15Z", "message": "fix typo"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "88f5b331e7e0d2c9fb2df6b4914beeca4026aaf3", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/88f5b331e7e0d2c9fb2df6b4914beeca4026aaf3", "committedDate": "2020-12-30T02:00:47Z", "message": "fix typo"}, "afterCommit": {"oid": "03a2236f91234cd57ce7641d91b252a4be7edcf4", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/03a2236f91234cd57ce7641d91b252a4be7edcf4", "committedDate": "2020-12-31T06:07:15Z", "message": "fix typo"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3660, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}