{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDU3NDczNDM2", "number": 1261, "title": "Spark: [DOC] guide about structured streaming sink for Iceberg", "bodyText": "This patch adds the guide about using structured streaming sink for Iceberg, which is not documented yet.\nThe usage itself is pretty simple, but there're some points end users would like to know in prior to write the table with streaming query, so the patch also adds some known guides on maintaining the table.\nSnapshot follows:", "createdAt": "2020-07-28T00:39:36Z", "url": "https://github.com/apache/iceberg/pull/1261", "merged": true, "mergeCommit": {"oid": "ad60ba5756af69e04c9baf1bdd171a9c5afd7004"}, "closed": true, "closedAt": "2020-08-26T00:47:29Z", "author": {"login": "HeartSaVioR"}, "timelineItems": {"totalCount": 23, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABc5LXY0AH2gAyNDU3NDczNDM2OjUyMzVlMzc4YjQyOTIwYzcwMjM5ZTgyM2QzYjU0ODU0NmJjNGMxMTA=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdCgkxwgH2gAyNDU3NDczNDM2OmQ5ZjMwZjZlMDRjZGQwZmM5ZjBiZGI3YzgwNWViOThiYjQ2MTRkZWY=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "5235e378b42920c70239e823d3b548546bc4c110", "author": {"user": {"login": "HeartSaVioR", "name": "Jungtaek Lim"}}, "url": "https://github.com/apache/iceberg/commit/5235e378b42920c70239e823d3b548546bc4c110", "committedDate": "2020-07-28T00:34:16Z", "message": "Spark: [DOC] guide about structured streaming sink for Iceberg"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cba54a086110bc17cc18751ef21b0418d383b0f6", "author": {"user": {"login": "HeartSaVioR", "name": "Jungtaek Lim"}}, "url": "https://github.com/apache/iceberg/commit/cba54a086110bc17cc18751ef21b0418d383b0f6", "committedDate": "2020-07-28T03:28:32Z", "message": "Mention supported modes"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU3MDU0Mzc2", "url": "https://github.com/apache/iceberg/pull/1261#pullrequestreview-457054376", "createdAt": "2020-07-28T21:51:10Z", "commit": {"oid": "cba54a086110bc17cc18751ef21b0418d383b0f6"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQyMTo1MToxMVrOG4gayg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQyMTo1MToxMVrOG4gayg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTkwNDU4Ng==", "bodyText": "This looks specific to 2.4. Should we have a 3.0 example and a separate 2.4 example like the other sections?\nAn alternative is to create a new page for Spark Streaming and add the docs there. Then we could have a table like the one at the top of the Spark page that explains what is supported in different versions.", "url": "https://github.com/apache/iceberg/pull/1261#discussion_r461904586", "createdAt": "2020-07-28T21:51:11Z", "author": {"login": "rdblue"}, "path": "site/docs/spark.md", "diffHunk": "@@ -520,6 +520,28 @@ data.writeTo(\"prod.db.table\")\n     .createOrReplace()\n ```\n \n+### Writing from streaming query (Structured Streaming)\n+\n+To write values from streaming query to Iceberg table, use `writeStream`:\n+\n+```scala\n+data.writeStream\n+    .format(\"iceberg\")\n+    .outputMode(\"append\")\n+    .option(\"path\", pathToTable)\n+    .option(\"checkpointLocation\", checkpointPath)\n+    .start()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cba54a086110bc17cc18751ef21b0418d383b0f6"}, "originalPosition": 14}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU3MDU5NTQz", "url": "https://github.com/apache/iceberg/pull/1261#pullrequestreview-457059543", "createdAt": "2020-07-28T22:00:41Z", "commit": {"oid": "cba54a086110bc17cc18751ef21b0418d383b0f6"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQyMjowMDo0MVrOG4g4nA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQyMjowMDo0MVrOG4g4nA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTkxMjIyMA==", "bodyText": "I think this is worth a section, not just a note.\n\nStreaming queries can create new table versions quickly, which creates lots of table metadata to track those versions. Maintaining metadata by tuning the rate of commits, expiring old snapshots, and automatically cleaning up metadata files is highly recommended.\n\nThen you could give an overview of those options and links to further docs, like the table property docs for delete-after-commit.", "url": "https://github.com/apache/iceberg/pull/1261#discussion_r461912220", "createdAt": "2020-07-28T22:00:41Z", "author": {"login": "rdblue"}, "path": "site/docs/spark.md", "diffHunk": "@@ -520,6 +520,28 @@ data.writeTo(\"prod.db.table\")\n     .createOrReplace()\n ```\n \n+### Writing from streaming query (Structured Streaming)\n+\n+To write values from streaming query to Iceberg table, use `writeStream`:\n+\n+```scala\n+data.writeStream\n+    .format(\"iceberg\")\n+    .outputMode(\"append\")\n+    .option(\"path\", pathToTable)\n+    .option(\"checkpointLocation\", checkpointPath)\n+    .start()\n+```\n+\n+`append` and `complete` modes are supported. The table should be created in prior to start the streaming query.\n+ \n+!!! Note\n+    To avoid metadata growing too huge, there're several guides you may want to follow: ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cba54a086110bc17cc18751ef21b0418d383b0f6"}, "originalPosition": 20}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "4cff8c95ddecab04fb2080c9d3f70506f7e67277", "author": {"user": {"login": "HeartSaVioR", "name": "Jungtaek Lim"}}, "url": "https://github.com/apache/iceberg/commit/4cff8c95ddecab04fb2080c9d3f70506f7e67277", "committedDate": "2020-07-29T12:59:15Z", "message": "Separate doc for Spark Structured Streaming"}, "afterCommit": {"oid": "7f6f21a8d306df21fec1a925d16f2e6497b933f7", "author": {"user": {"login": "HeartSaVioR", "name": "Jungtaek Lim"}}, "url": "https://github.com/apache/iceberg/commit/7f6f21a8d306df21fec1a925d16f2e6497b933f7", "committedDate": "2020-07-29T13:02:01Z", "message": "Separate doc for Spark Structured Streaming"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ca720ec2cec315f9e766a7b634f7cd92e119d514", "author": {"user": {"login": "HeartSaVioR", "name": "Jungtaek Lim"}}, "url": "https://github.com/apache/iceberg/commit/ca720ec2cec315f9e766a7b634f7cd92e119d514", "committedDate": "2020-07-29T13:04:22Z", "message": "Separate doc for Spark Structured Streaming"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "7f6f21a8d306df21fec1a925d16f2e6497b933f7", "author": {"user": {"login": "HeartSaVioR", "name": "Jungtaek Lim"}}, "url": "https://github.com/apache/iceberg/commit/7f6f21a8d306df21fec1a925d16f2e6497b933f7", "committedDate": "2020-07-29T13:02:01Z", "message": "Separate doc for Spark Structured Streaming"}, "afterCommit": {"oid": "ca720ec2cec315f9e766a7b634f7cd92e119d514", "author": {"user": {"login": "HeartSaVioR", "name": "Jungtaek Lim"}}, "url": "https://github.com/apache/iceberg/commit/ca720ec2cec315f9e766a7b634f7cd92e119d514", "committedDate": "2020-07-29T13:04:22Z", "message": "Separate doc for Spark Structured Streaming"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "88ea5b437b51ed16e73e8a40bf6039dc59825fc4", "author": {"user": {"login": "HeartSaVioR", "name": "Jungtaek Lim"}}, "url": "https://github.com/apache/iceberg/commit/88ea5b437b51ed16e73e8a40bf6039dc59825fc4", "committedDate": "2020-07-30T05:09:51Z", "message": "Enriching explanations on rewriting manifests/data files, add section on removing orphan files"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU4NDMyOTk2", "url": "https://github.com/apache/iceberg/pull/1261#pullrequestreview-458432996", "createdAt": "2020-07-30T14:04:35Z", "commit": {"oid": "88ea5b437b51ed16e73e8a40bf6039dc59825fc4"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxNDowNDozNVrOG5kiMg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxNDowNDozNVrOG5kiMg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzAyMDU5NA==", "bodyText": "I just realized I should provide table identifier instead of path in HiveCatalog. I'll update the same.", "url": "https://github.com/apache/iceberg/pull/1261#discussion_r463020594", "createdAt": "2020-07-30T14:04:35Z", "author": {"login": "HeartSaVioR"}, "path": "site/docs/spark-structured-streaming.md", "diffHunk": "@@ -0,0 +1,184 @@\n+<!--\n+ - Licensed to the Apache Software Foundation (ASF) under one or more\n+ - contributor license agreements.  See the NOTICE file distributed with\n+ - this work for additional information regarding copyright ownership.\n+ - The ASF licenses this file to You under the Apache License, Version 2.0\n+ - (the \"License\"); you may not use this file except in compliance with\n+ - the License.  You may obtain a copy of the License at\n+ -\n+ -   http://www.apache.org/licenses/LICENSE-2.0\n+ -\n+ - Unless required by applicable law or agreed to in writing, software\n+ - distributed under the License is distributed on an \"AS IS\" BASIS,\n+ - WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ - See the License for the specific language governing permissions and\n+ - limitations under the License.\n+ -->\n+\n+# Spark Structured Streaming\n+\n+Iceberg uses Apache Spark's DataSourceV2 API for data source and catalog implementations. Spark DSv2 is an evolving API\n+with different levels of support in Spark versions.\n+\n+As of Spark 3.0, the new API on reading/writing table on table identifier is not yet added on streaming query.\n+\n+| Feature support                                  | Spark 3.0| Spark 2.4  | Notes                                          |\n+|--------------------------------------------------|----------|------------|------------------------------------------------|\n+| [DataFrame write](#writing-with-streaming-query) | \u2714        | \u2714          |                                                |\n+\n+## Writing with streaming query\n+\n+To write values from streaming query to Iceberg table, use `DataStreamWriter`:\n+\n+```scala\n+data.writeStream\n+    .format(\"iceberg\")\n+    .outputMode(\"append\")\n+    .trigger(Trigger.ProcessingTime(1, TimeUnit.MINUTES))\n+    .option(\"path\", pathToTable)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "88ea5b437b51ed16e73e8a40bf6039dc59825fc4"}, "originalPosition": 38}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU4ODQxMTg3", "url": "https://github.com/apache/iceberg/pull/1261#pullrequestreview-458841187", "createdAt": "2020-07-31T00:06:32Z", "commit": {"oid": "88ea5b437b51ed16e73e8a40bf6039dc59825fc4"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMDowNjozMlrOG5314Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMDowNjozMlrOG5314Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzMzNjkyOQ==", "bodyText": "It would be great to document what these do:\n\nappend - appends the output of every micro-batch to the table\ncomplete - replaces the table contents every micro-batch", "url": "https://github.com/apache/iceberg/pull/1261#discussion_r463336929", "createdAt": "2020-07-31T00:06:32Z", "author": {"login": "rdblue"}, "path": "site/docs/spark-structured-streaming.md", "diffHunk": "@@ -0,0 +1,184 @@\n+<!--\n+ - Licensed to the Apache Software Foundation (ASF) under one or more\n+ - contributor license agreements.  See the NOTICE file distributed with\n+ - this work for additional information regarding copyright ownership.\n+ - The ASF licenses this file to You under the Apache License, Version 2.0\n+ - (the \"License\"); you may not use this file except in compliance with\n+ - the License.  You may obtain a copy of the License at\n+ -\n+ -   http://www.apache.org/licenses/LICENSE-2.0\n+ -\n+ - Unless required by applicable law or agreed to in writing, software\n+ - distributed under the License is distributed on an \"AS IS\" BASIS,\n+ - WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ - See the License for the specific language governing permissions and\n+ - limitations under the License.\n+ -->\n+\n+# Spark Structured Streaming\n+\n+Iceberg uses Apache Spark's DataSourceV2 API for data source and catalog implementations. Spark DSv2 is an evolving API\n+with different levels of support in Spark versions.\n+\n+As of Spark 3.0, the new API on reading/writing table on table identifier is not yet added on streaming query.\n+\n+| Feature support                                  | Spark 3.0| Spark 2.4  | Notes                                          |\n+|--------------------------------------------------|----------|------------|------------------------------------------------|\n+| [DataFrame write](#writing-with-streaming-query) | \u2714        | \u2714          |                                                |\n+\n+## Writing with streaming query\n+\n+To write values from streaming query to Iceberg table, use `DataStreamWriter`:\n+\n+```scala\n+data.writeStream\n+    .format(\"iceberg\")\n+    .outputMode(\"append\")\n+    .trigger(Trigger.ProcessingTime(1, TimeUnit.MINUTES))\n+    .option(\"path\", pathToTable)\n+    .option(\"checkpointLocation\", checkpointPath)\n+    .start()\n+```\n+\n+Iceberg supports below output modes:\n+\n+* append\n+* complete", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "88ea5b437b51ed16e73e8a40bf6039dc59825fc4"}, "originalPosition": 46}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU4ODQxMjU1", "url": "https://github.com/apache/iceberg/pull/1261#pullrequestreview-458841255", "createdAt": "2020-07-31T00:06:47Z", "commit": {"oid": "88ea5b437b51ed16e73e8a40bf6039dc59825fc4"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMDowNjo0N1rOG532KA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMDowNjo0N1rOG532KA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzMzNzAwMA==", "bodyText": "Should this link to the CREATE TABLE docs on the Spark page?", "url": "https://github.com/apache/iceberg/pull/1261#discussion_r463337000", "createdAt": "2020-07-31T00:06:47Z", "author": {"login": "rdblue"}, "path": "site/docs/spark-structured-streaming.md", "diffHunk": "@@ -0,0 +1,184 @@\n+<!--\n+ - Licensed to the Apache Software Foundation (ASF) under one or more\n+ - contributor license agreements.  See the NOTICE file distributed with\n+ - this work for additional information regarding copyright ownership.\n+ - The ASF licenses this file to You under the Apache License, Version 2.0\n+ - (the \"License\"); you may not use this file except in compliance with\n+ - the License.  You may obtain a copy of the License at\n+ -\n+ -   http://www.apache.org/licenses/LICENSE-2.0\n+ -\n+ - Unless required by applicable law or agreed to in writing, software\n+ - distributed under the License is distributed on an \"AS IS\" BASIS,\n+ - WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ - See the License for the specific language governing permissions and\n+ - limitations under the License.\n+ -->\n+\n+# Spark Structured Streaming\n+\n+Iceberg uses Apache Spark's DataSourceV2 API for data source and catalog implementations. Spark DSv2 is an evolving API\n+with different levels of support in Spark versions.\n+\n+As of Spark 3.0, the new API on reading/writing table on table identifier is not yet added on streaming query.\n+\n+| Feature support                                  | Spark 3.0| Spark 2.4  | Notes                                          |\n+|--------------------------------------------------|----------|------------|------------------------------------------------|\n+| [DataFrame write](#writing-with-streaming-query) | \u2714        | \u2714          |                                                |\n+\n+## Writing with streaming query\n+\n+To write values from streaming query to Iceberg table, use `DataStreamWriter`:\n+\n+```scala\n+data.writeStream\n+    .format(\"iceberg\")\n+    .outputMode(\"append\")\n+    .trigger(Trigger.ProcessingTime(1, TimeUnit.MINUTES))\n+    .option(\"path\", pathToTable)\n+    .option(\"checkpointLocation\", checkpointPath)\n+    .start()\n+```\n+\n+Iceberg supports below output modes:\n+\n+* append\n+* complete\n+\n+The table should be created in prior to start the streaming query.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "88ea5b437b51ed16e73e8a40bf6039dc59825fc4"}, "originalPosition": 48}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU4ODQxNjIx", "url": "https://github.com/apache/iceberg/pull/1261#pullrequestreview-458841621", "createdAt": "2020-07-31T00:07:58Z", "commit": {"oid": "88ea5b437b51ed16e73e8a40bf6039dc59825fc4"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMDowNzo1OFrOG533Uw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMDowNzo1OFrOG533Uw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzMzNzI5OQ==", "bodyText": "How is this configured? A link to the relevant Spark docs and a quick summary would be really useful.", "url": "https://github.com/apache/iceberg/pull/1261#discussion_r463337299", "createdAt": "2020-07-31T00:07:58Z", "author": {"login": "rdblue"}, "path": "site/docs/spark-structured-streaming.md", "diffHunk": "@@ -0,0 +1,184 @@\n+<!--\n+ - Licensed to the Apache Software Foundation (ASF) under one or more\n+ - contributor license agreements.  See the NOTICE file distributed with\n+ - this work for additional information regarding copyright ownership.\n+ - The ASF licenses this file to You under the Apache License, Version 2.0\n+ - (the \"License\"); you may not use this file except in compliance with\n+ - the License.  You may obtain a copy of the License at\n+ -\n+ -   http://www.apache.org/licenses/LICENSE-2.0\n+ -\n+ - Unless required by applicable law or agreed to in writing, software\n+ - distributed under the License is distributed on an \"AS IS\" BASIS,\n+ - WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ - See the License for the specific language governing permissions and\n+ - limitations under the License.\n+ -->\n+\n+# Spark Structured Streaming\n+\n+Iceberg uses Apache Spark's DataSourceV2 API for data source and catalog implementations. Spark DSv2 is an evolving API\n+with different levels of support in Spark versions.\n+\n+As of Spark 3.0, the new API on reading/writing table on table identifier is not yet added on streaming query.\n+\n+| Feature support                                  | Spark 3.0| Spark 2.4  | Notes                                          |\n+|--------------------------------------------------|----------|------------|------------------------------------------------|\n+| [DataFrame write](#writing-with-streaming-query) | \u2714        | \u2714          |                                                |\n+\n+## Writing with streaming query\n+\n+To write values from streaming query to Iceberg table, use `DataStreamWriter`:\n+\n+```scala\n+data.writeStream\n+    .format(\"iceberg\")\n+    .outputMode(\"append\")\n+    .trigger(Trigger.ProcessingTime(1, TimeUnit.MINUTES))\n+    .option(\"path\", pathToTable)\n+    .option(\"checkpointLocation\", checkpointPath)\n+    .start()\n+```\n+\n+Iceberg supports below output modes:\n+\n+* append\n+* complete\n+\n+The table should be created in prior to start the streaming query.\n+\n+## Maintenance\n+\n+Streaming queries can create new table versions quickly, which creates lots of table metadata to track those versions.\n+Maintaining metadata by tuning the rate of commits, expiring old snapshots, and automatically cleaning up metadata files\n+is highly recommended.\n+\n+### Tune the rate of commits\n+\n+Having high rate of commits would produce lots of data files, manifests, and snapshots which leads the table hard\n+to maintain. We encourage having trigger interval 1 minute at minimum, and increase the interval if you encounter\n+issues.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "88ea5b437b51ed16e73e8a40bf6039dc59825fc4"}, "originalPosition": 60}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU4ODQxNzM0", "url": "https://github.com/apache/iceberg/pull/1261#pullrequestreview-458841734", "createdAt": "2020-07-31T00:08:20Z", "commit": {"oid": "88ea5b437b51ed16e73e8a40bf6039dc59825fc4"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMDowODoyMFrOG533sw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMDowODoyMFrOG533sw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzMzNzM5NQ==", "bodyText": "This applies to all catalogs, not just Hadoop. I think you can simply remove that clause.", "url": "https://github.com/apache/iceberg/pull/1261#discussion_r463337395", "createdAt": "2020-07-31T00:08:20Z", "author": {"login": "rdblue"}, "path": "site/docs/spark-structured-streaming.md", "diffHunk": "@@ -0,0 +1,184 @@\n+<!--\n+ - Licensed to the Apache Software Foundation (ASF) under one or more\n+ - contributor license agreements.  See the NOTICE file distributed with\n+ - this work for additional information regarding copyright ownership.\n+ - The ASF licenses this file to You under the Apache License, Version 2.0\n+ - (the \"License\"); you may not use this file except in compliance with\n+ - the License.  You may obtain a copy of the License at\n+ -\n+ -   http://www.apache.org/licenses/LICENSE-2.0\n+ -\n+ - Unless required by applicable law or agreed to in writing, software\n+ - distributed under the License is distributed on an \"AS IS\" BASIS,\n+ - WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ - See the License for the specific language governing permissions and\n+ - limitations under the License.\n+ -->\n+\n+# Spark Structured Streaming\n+\n+Iceberg uses Apache Spark's DataSourceV2 API for data source and catalog implementations. Spark DSv2 is an evolving API\n+with different levels of support in Spark versions.\n+\n+As of Spark 3.0, the new API on reading/writing table on table identifier is not yet added on streaming query.\n+\n+| Feature support                                  | Spark 3.0| Spark 2.4  | Notes                                          |\n+|--------------------------------------------------|----------|------------|------------------------------------------------|\n+| [DataFrame write](#writing-with-streaming-query) | \u2714        | \u2714          |                                                |\n+\n+## Writing with streaming query\n+\n+To write values from streaming query to Iceberg table, use `DataStreamWriter`:\n+\n+```scala\n+data.writeStream\n+    .format(\"iceberg\")\n+    .outputMode(\"append\")\n+    .trigger(Trigger.ProcessingTime(1, TimeUnit.MINUTES))\n+    .option(\"path\", pathToTable)\n+    .option(\"checkpointLocation\", checkpointPath)\n+    .start()\n+```\n+\n+Iceberg supports below output modes:\n+\n+* append\n+* complete\n+\n+The table should be created in prior to start the streaming query.\n+\n+## Maintenance\n+\n+Streaming queries can create new table versions quickly, which creates lots of table metadata to track those versions.\n+Maintaining metadata by tuning the rate of commits, expiring old snapshots, and automatically cleaning up metadata files\n+is highly recommended.\n+\n+### Tune the rate of commits\n+\n+Having high rate of commits would produce lots of data files, manifests, and snapshots which leads the table hard\n+to maintain. We encourage having trigger interval 1 minute at minimum, and increase the interval if you encounter\n+issues.\n+\n+### Retain recent metadata files in Hadoop catalog\n+\n+If you are using HadoopCatalog, you may want to enable `write.metadata.delete-after-commit.enabled` in the table", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "88ea5b437b51ed16e73e8a40bf6039dc59825fc4"}, "originalPosition": 64}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU4ODQyODk5", "url": "https://github.com/apache/iceberg/pull/1261#pullrequestreview-458842899", "createdAt": "2020-07-31T00:12:16Z", "commit": {"oid": "88ea5b437b51ed16e73e8a40bf6039dc59825fc4"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMDoxMjoxN1rOG5377g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMDoxMjoxN1rOG5377g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzMzODQ3OA==", "bodyText": "How about \"Removed old metadata files\"? That matches the wording used for \"Expire old snapshots\".\nAlso, we should probably make these recommendations in order of importance, which would mean putting the expire snapshots section first. Those keep a lot more metadata and affect table performance; these don't affect table performance and are smaller if you clean up snapshots.", "url": "https://github.com/apache/iceberg/pull/1261#discussion_r463338478", "createdAt": "2020-07-31T00:12:17Z", "author": {"login": "rdblue"}, "path": "site/docs/spark-structured-streaming.md", "diffHunk": "@@ -0,0 +1,184 @@\n+<!--\n+ - Licensed to the Apache Software Foundation (ASF) under one or more\n+ - contributor license agreements.  See the NOTICE file distributed with\n+ - this work for additional information regarding copyright ownership.\n+ - The ASF licenses this file to You under the Apache License, Version 2.0\n+ - (the \"License\"); you may not use this file except in compliance with\n+ - the License.  You may obtain a copy of the License at\n+ -\n+ -   http://www.apache.org/licenses/LICENSE-2.0\n+ -\n+ - Unless required by applicable law or agreed to in writing, software\n+ - distributed under the License is distributed on an \"AS IS\" BASIS,\n+ - WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ - See the License for the specific language governing permissions and\n+ - limitations under the License.\n+ -->\n+\n+# Spark Structured Streaming\n+\n+Iceberg uses Apache Spark's DataSourceV2 API for data source and catalog implementations. Spark DSv2 is an evolving API\n+with different levels of support in Spark versions.\n+\n+As of Spark 3.0, the new API on reading/writing table on table identifier is not yet added on streaming query.\n+\n+| Feature support                                  | Spark 3.0| Spark 2.4  | Notes                                          |\n+|--------------------------------------------------|----------|------------|------------------------------------------------|\n+| [DataFrame write](#writing-with-streaming-query) | \u2714        | \u2714          |                                                |\n+\n+## Writing with streaming query\n+\n+To write values from streaming query to Iceberg table, use `DataStreamWriter`:\n+\n+```scala\n+data.writeStream\n+    .format(\"iceberg\")\n+    .outputMode(\"append\")\n+    .trigger(Trigger.ProcessingTime(1, TimeUnit.MINUTES))\n+    .option(\"path\", pathToTable)\n+    .option(\"checkpointLocation\", checkpointPath)\n+    .start()\n+```\n+\n+Iceberg supports below output modes:\n+\n+* append\n+* complete\n+\n+The table should be created in prior to start the streaming query.\n+\n+## Maintenance\n+\n+Streaming queries can create new table versions quickly, which creates lots of table metadata to track those versions.\n+Maintaining metadata by tuning the rate of commits, expiring old snapshots, and automatically cleaning up metadata files\n+is highly recommended.\n+\n+### Tune the rate of commits\n+\n+Having high rate of commits would produce lots of data files, manifests, and snapshots which leads the table hard\n+to maintain. We encourage having trigger interval 1 minute at minimum, and increase the interval if you encounter\n+issues.\n+\n+### Retain recent metadata files in Hadoop catalog", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "88ea5b437b51ed16e73e8a40bf6039dc59825fc4"}, "originalPosition": 62}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU4ODQzODYy", "url": "https://github.com/apache/iceberg/pull/1261#pullrequestreview-458843862", "createdAt": "2020-07-31T00:15:37Z", "commit": {"oid": "88ea5b437b51ed16e73e8a40bf6039dc59825fc4"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMDoxNTozN1rOG53_TQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMDoxNTozN1rOG53_TQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzMzOTM0MQ==", "bodyText": "How about \"Run expireSnapshots regularly to prune . . .\"?\nThat's more direct and avoids the natural question \"When may I not want to do this?\" Since the answer is you always want to clean snapshots, being direct is more clear.", "url": "https://github.com/apache/iceberg/pull/1261#discussion_r463339341", "createdAt": "2020-07-31T00:15:37Z", "author": {"login": "rdblue"}, "path": "site/docs/spark-structured-streaming.md", "diffHunk": "@@ -0,0 +1,184 @@\n+<!--\n+ - Licensed to the Apache Software Foundation (ASF) under one or more\n+ - contributor license agreements.  See the NOTICE file distributed with\n+ - this work for additional information regarding copyright ownership.\n+ - The ASF licenses this file to You under the Apache License, Version 2.0\n+ - (the \"License\"); you may not use this file except in compliance with\n+ - the License.  You may obtain a copy of the License at\n+ -\n+ -   http://www.apache.org/licenses/LICENSE-2.0\n+ -\n+ - Unless required by applicable law or agreed to in writing, software\n+ - distributed under the License is distributed on an \"AS IS\" BASIS,\n+ - WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ - See the License for the specific language governing permissions and\n+ - limitations under the License.\n+ -->\n+\n+# Spark Structured Streaming\n+\n+Iceberg uses Apache Spark's DataSourceV2 API for data source and catalog implementations. Spark DSv2 is an evolving API\n+with different levels of support in Spark versions.\n+\n+As of Spark 3.0, the new API on reading/writing table on table identifier is not yet added on streaming query.\n+\n+| Feature support                                  | Spark 3.0| Spark 2.4  | Notes                                          |\n+|--------------------------------------------------|----------|------------|------------------------------------------------|\n+| [DataFrame write](#writing-with-streaming-query) | \u2714        | \u2714          |                                                |\n+\n+## Writing with streaming query\n+\n+To write values from streaming query to Iceberg table, use `DataStreamWriter`:\n+\n+```scala\n+data.writeStream\n+    .format(\"iceberg\")\n+    .outputMode(\"append\")\n+    .trigger(Trigger.ProcessingTime(1, TimeUnit.MINUTES))\n+    .option(\"path\", pathToTable)\n+    .option(\"checkpointLocation\", checkpointPath)\n+    .start()\n+```\n+\n+Iceberg supports below output modes:\n+\n+* append\n+* complete\n+\n+The table should be created in prior to start the streaming query.\n+\n+## Maintenance\n+\n+Streaming queries can create new table versions quickly, which creates lots of table metadata to track those versions.\n+Maintaining metadata by tuning the rate of commits, expiring old snapshots, and automatically cleaning up metadata files\n+is highly recommended.\n+\n+### Tune the rate of commits\n+\n+Having high rate of commits would produce lots of data files, manifests, and snapshots which leads the table hard\n+to maintain. We encourage having trigger interval 1 minute at minimum, and increase the interval if you encounter\n+issues.\n+\n+### Retain recent metadata files in Hadoop catalog\n+\n+If you are using HadoopCatalog, you may want to enable `write.metadata.delete-after-commit.enabled` in the table\n+properties, and reduce `write.metadata.previous-versions-max` as well (if necessary) to retain only specific number of\n+metadata files.\n+\n+Please refer the [table write properties](/configuration/#write-properties) for more details.\n+\n+### Expire old snapshots\n+\n+You may want to run [expireSnapshots()](/javadoc/master/org/apache/iceberg/Table.html#expireSnapshots--) periodically", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "88ea5b437b51ed16e73e8a40bf6039dc59825fc4"}, "originalPosition": 72}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7c4ad10dcbbba129c3f72bfa0529fb28673cbbbf", "author": {"user": {"login": "HeartSaVioR", "name": "Jungtaek Lim"}}, "url": "https://github.com/apache/iceberg/commit/7c4ad10dcbbba129c3f72bfa0529fb28673cbbbf", "committedDate": "2020-07-31T01:14:07Z", "message": "Reflect review comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ffb2ca64b5e7b8339abeb1691c816c5180eb4693", "author": {"user": {"login": "HeartSaVioR", "name": "Jungtaek Lim"}}, "url": "https://github.com/apache/iceberg/commit/ffb2ca64b5e7b8339abeb1691c816c5180eb4693", "committedDate": "2020-07-31T04:47:39Z", "message": "Fix for Hive catalog"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bccbd62cfe3649da5a1b73a4c0b04690d7206121", "author": {"user": {"login": "HeartSaVioR", "name": "Jungtaek Lim"}}, "url": "https://github.com/apache/iceberg/commit/bccbd62cfe3649da5a1b73a4c0b04690d7206121", "committedDate": "2020-08-02T11:08:07Z", "message": "Fix"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "396530d97fc4f38592f66c307957ce95da374ba7", "author": {"user": {"login": "HeartSaVioR", "name": "Jungtaek Lim"}}, "url": "https://github.com/apache/iceberg/commit/396530d97fc4f38592f66c307957ce95da374ba7", "committedDate": "2020-08-06T00:55:00Z", "message": "Reflect review comment"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1da21c58a52d74e57d79295cab4303c6c473e57a", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/1da21c58a52d74e57d79295cab4303c6c473e57a", "committedDate": "2020-08-25T22:40:11Z", "message": "Separate maintenance content into a maintenance doc."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "39ac0225430a64bb229b96b01bc7013375c694cb", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/39ac0225430a64bb229b96b01bc7013375c694cb", "committedDate": "2020-08-25T23:51:02Z", "message": "Add missing file."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8a6ae5dd8abaf5cfcbced40871f0859dfe7f41d9", "author": {"user": {"login": "HeartSaVioR", "name": "Jungtaek Lim"}}, "url": "https://github.com/apache/iceberg/commit/8a6ae5dd8abaf5cfcbced40871f0859dfe7f41d9", "committedDate": "2020-08-26T00:07:39Z", "message": "Merge pull request #1 from rdblue/pr-1261-structured-streaming-docs\n\nSeparate maintenance content into a maintenance doc."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d9f30f6e04cdd0fc9f0bdb7c805eb98bb4614def", "author": {"user": {"login": "HeartSaVioR", "name": "Jungtaek Lim"}}, "url": "https://github.com/apache/iceberg/commit/d9f30f6e04cdd0fc9f0bdb7c805eb98bb4614def", "committedDate": "2020-08-26T00:22:13Z", "message": "fix nit"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4346, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}