{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDk3MDAzOTk0", "number": 1545, "reviewThreads": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMlQxNjoxNDoxM1rOEp3vTg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMlQxNjoxNDoxM1rOEp3vTg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyMzQwMzAyOnYy", "diffSide": "RIGHT", "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestSparkDataWrite.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMlQxNjoxNDoxM1rOHbziGQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMlQxNjoxNDoxM1rOHbziGQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODkxNzkxMw==", "bodyText": "This was failing for tables loaded through IcebergSource in Spark 3. Now, it matches Spark 2.", "url": "https://github.com/apache/iceberg/pull/1545#discussion_r498917913", "createdAt": "2020-10-02T16:14:13Z", "author": {"login": "aokolnychyi"}, "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestSparkDataWrite.java", "diffHunk": "@@ -458,4 +458,55 @@ public void testWriteProjectionWithMiddle() throws IOException {\n     Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n     Assert.assertEquals(\"Result rows should match\", expected, actual);\n   }\n+\n+  @Test\n+  public void testViewsReturnRecentResults() throws IOException {\n+    File parent = temp.newFolder(format.toString());\n+    File location = new File(parent, \"test\");\n+\n+    HadoopTables tables = new HadoopTables(CONF);\n+    PartitionSpec spec = PartitionSpec.builderFor(SCHEMA).identity(\"data\").build();\n+    tables.create(SCHEMA, spec, location.toString());\n+\n+    List<SimpleRecord> records = Lists.newArrayList(\n+        new SimpleRecord(1, \"a\"),\n+        new SimpleRecord(2, \"b\"),\n+        new SimpleRecord(3, \"c\")\n+    );\n+\n+    Dataset<Row> df = spark.createDataFrame(records, SimpleRecord.class);\n+\n+    df.select(\"id\", \"data\").write()\n+        .format(\"iceberg\")\n+        .option(\"write-format\", format.toString())\n+        .mode(\"append\")\n+        .save(location.toString());\n+\n+    Dataset<Row> query = spark.read()\n+        .format(\"iceberg\")\n+        .load(location.toString())\n+        .where(\"id = 1\");\n+    query.createOrReplaceTempView(\"tmp\");\n+\n+    List<SimpleRecord> actual1 = spark.table(\"tmp\").as(Encoders.bean(SimpleRecord.class)).collectAsList();\n+    List<SimpleRecord> expected1 = Lists.newArrayList(\n+        new SimpleRecord(1, \"a\")\n+    );\n+    Assert.assertEquals(\"Number of rows should match\", expected1.size(), actual1.size());\n+    Assert.assertEquals(\"Result rows should match\", expected1, actual1);\n+\n+    df.select(\"id\", \"data\").write()\n+        .format(\"iceberg\")\n+        .option(\"write-format\", format.toString())\n+        .mode(\"append\")\n+        .save(location.toString());\n+\n+    List<SimpleRecord> actual2 = spark.table(\"tmp\").as(Encoders.bean(SimpleRecord.class)).collectAsList();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4be560495400b1b10c5368e3706cbc3a60c0f010"}, "originalPosition": 47}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3567, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}