{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDc0NDYwNzM0", "number": 1393, "title": "Flink: Support creating table and altering table in Flink SQL", "bodyText": "Fixes #1392\n\nCreate table: Only support identity partition, wait for Flink SQL support hidden partition, but can load all Iceberg tables.\nAlter table:\n\nCurrently, Flink SQL only support altering table properties, not support altering schema and partition.\nAlso support location, current-snapshot-id and cherry-pick-snapshot-id like Spark.", "createdAt": "2020-08-27T05:40:19Z", "url": "https://github.com/apache/iceberg/pull/1393", "merged": true, "mergeCommit": {"oid": "d4dee8e667368945d00ad6f0051e2f492c9d59a5"}, "closed": true, "closedAt": "2020-09-02T22:18:18Z", "author": {"login": "JingsongLi"}, "timelineItems": {"totalCount": 19, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdC5tPKgH2gAyNDc0NDYwNzM0OmIwMDNiMjk1MTNjOGVmOWNiOTU4NDBkYjAyOTFmNWJkNDVhZWIxZGU=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdEi6PRgH2gAyNDc0NDYwNzM0OmFjMDZlNTI0ZjMxOTFmYzMwMzM0MGVjMjNiYzIxNDljOGY5M2RjZjI=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "b003b29513c8ef9cb95840db0291f5bd45aeb1de", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/iceberg/commit/b003b29513c8ef9cb95840db0291f5bd45aeb1de", "committedDate": "2020-08-27T05:39:05Z", "message": "Flink: Support creating table and altering table in Flink SQL"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "87f8c79f679aef109f0fce0fe55745e880948dca", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/iceberg/commit/87f8c79f679aef109f0fce0fe55745e880948dca", "committedDate": "2020-08-27T05:57:52Z", "message": "Fix case"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc2NTI1OTY0", "url": "https://github.com/apache/iceberg/pull/1393#pullrequestreview-476525964", "createdAt": "2020-08-27T09:04:57Z", "commit": {"oid": "87f8c79f679aef109f0fce0fe55745e880948dca"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yN1QwOTowNDo1OFrOHIHNcA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yN1QwOToxMDoxMFrOHIHZRg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODI2ODc4NA==", "bodyText": "nit: better to use ImmutableMap ?", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r478268784", "createdAt": "2020-08-27T09:04:58Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java", "diffHunk": "@@ -320,19 +338,158 @@ public void renameTable(ObjectPath tablePath, String newTableName, boolean ignor\n     }\n   }\n \n-  /**\n-   * TODO Add partitioning to the Flink DDL parser.\n-   */\n   @Override\n   public void createTable(ObjectPath tablePath, CatalogBaseTable table, boolean ignoreIfExists)\n-      throws CatalogException {\n-    throw new UnsupportedOperationException(\"Not support createTable now.\");\n+      throws CatalogException, TableAlreadyExistException {\n+    validateFlinkTable(table);\n+\n+    Schema icebergSchema = FlinkSchemaUtil.convert(table.getSchema());\n+    PartitionSpec spec = toPartitionSpec(((CatalogTable) table).getPartitionKeys(), icebergSchema);\n+    Map<String, String> options = Maps.newHashMap(table.getOptions());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "87f8c79f679aef109f0fce0fe55745e880948dca"}, "originalPosition": 92}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODI3MTgxNA==", "bodyText": "What's the reason that we could not support adding /removing/renaming column ?", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r478271814", "createdAt": "2020-08-27T09:10:10Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java", "diffHunk": "@@ -320,19 +338,158 @@ public void renameTable(ObjectPath tablePath, String newTableName, boolean ignor\n     }\n   }\n \n-  /**\n-   * TODO Add partitioning to the Flink DDL parser.\n-   */\n   @Override\n   public void createTable(ObjectPath tablePath, CatalogBaseTable table, boolean ignoreIfExists)\n-      throws CatalogException {\n-    throw new UnsupportedOperationException(\"Not support createTable now.\");\n+      throws CatalogException, TableAlreadyExistException {\n+    validateFlinkTable(table);\n+\n+    Schema icebergSchema = FlinkSchemaUtil.convert(table.getSchema());\n+    PartitionSpec spec = toPartitionSpec(((CatalogTable) table).getPartitionKeys(), icebergSchema);\n+    Map<String, String> options = Maps.newHashMap(table.getOptions());\n+\n+    try {\n+      icebergCatalog.createTable(\n+          toIdentifier(tablePath),\n+          icebergSchema,\n+          spec,\n+          options.get(\"location\"),\n+          options);\n+    } catch (AlreadyExistsException e) {\n+      throw new TableAlreadyExistException(getName(), tablePath, e);\n+    }\n   }\n \n   @Override\n   public void alterTable(ObjectPath tablePath, CatalogBaseTable newTable, boolean ignoreIfNotExists)\n-      throws CatalogException {\n-    throw new UnsupportedOperationException(\"Not support alterTable now.\");\n+      throws CatalogException, TableNotExistException {\n+    validateFlinkTable(newTable);\n+    Table icebergTable = getIcebergTable(tablePath);\n+    CatalogTable table = toCatalogTable(icebergTable);\n+\n+    // Currently, Flink SQL only support altering table properties.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "87f8c79f679aef109f0fce0fe55745e880948dca"}, "originalPosition": 115}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc2NTQ2ODgx", "url": "https://github.com/apache/iceberg/pull/1393#pullrequestreview-476546881", "createdAt": "2020-08-27T09:32:18Z", "commit": {"oid": "87f8c79f679aef109f0fce0fe55745e880948dca"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yN1QwOTozMjoxOFrOHIIMvg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yN1QwOTozMjoxOFrOHIIMvg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODI4NDk5MA==", "bodyText": "Q:  Does this align with the flink sql semantics ?\nI saw the document said: \"Set one or more properties in the specified table. If a particular property is already set in the table, override the old value with the new one.\"\nALTER TABLE [catalog_name.][db_name.]table_name SET (key1=val1, key2=val2, ...)\nFor the existing key-values  (in old table ) which don't appear in the new table,  should we remove them from old table ?  ( The document did not describe this case clearly,  just for confirmation).", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r478284990", "createdAt": "2020-08-27T09:32:18Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java", "diffHunk": "@@ -320,19 +338,158 @@ public void renameTable(ObjectPath tablePath, String newTableName, boolean ignor\n     }\n   }\n \n-  /**\n-   * TODO Add partitioning to the Flink DDL parser.\n-   */\n   @Override\n   public void createTable(ObjectPath tablePath, CatalogBaseTable table, boolean ignoreIfExists)\n-      throws CatalogException {\n-    throw new UnsupportedOperationException(\"Not support createTable now.\");\n+      throws CatalogException, TableAlreadyExistException {\n+    validateFlinkTable(table);\n+\n+    Schema icebergSchema = FlinkSchemaUtil.convert(table.getSchema());\n+    PartitionSpec spec = toPartitionSpec(((CatalogTable) table).getPartitionKeys(), icebergSchema);\n+    Map<String, String> options = Maps.newHashMap(table.getOptions());\n+\n+    try {\n+      icebergCatalog.createTable(\n+          toIdentifier(tablePath),\n+          icebergSchema,\n+          spec,\n+          options.get(\"location\"),\n+          options);\n+    } catch (AlreadyExistsException e) {\n+      throw new TableAlreadyExistException(getName(), tablePath, e);\n+    }\n   }\n \n   @Override\n   public void alterTable(ObjectPath tablePath, CatalogBaseTable newTable, boolean ignoreIfNotExists)\n-      throws CatalogException {\n-    throw new UnsupportedOperationException(\"Not support alterTable now.\");\n+      throws CatalogException, TableNotExistException {\n+    validateFlinkTable(newTable);\n+    Table icebergTable = getIcebergTable(tablePath);\n+    CatalogTable table = toCatalogTable(icebergTable);\n+\n+    // Currently, Flink SQL only support altering table properties.\n+\n+    if (!table.getSchema().equals(newTable.getSchema())) {\n+      throw new UnsupportedOperationException(\"Altering schema is not supported yet.\");\n+    }\n+\n+    if (!table.getPartitionKeys().equals(((CatalogTable) newTable).getPartitionKeys())) {\n+      throw new UnsupportedOperationException(\"Altering partition keys is not supported yet.\");\n+    }\n+\n+    Map<String, String> oldOptions = table.getOptions();\n+    Map<String, String> setProperties = Maps.newHashMap();\n+\n+    String setLocation = null;\n+    String setSnapshotId = null;\n+    String pickSnapshotId = null;\n+\n+    for (Map.Entry<String, String> entry : newTable.getOptions().entrySet()) {\n+      String key = entry.getKey();\n+      String value = entry.getValue();\n+\n+      if (Objects.equals(value, oldOptions.get(key))) {\n+        continue;\n+      }\n+\n+      if (\"location\".equalsIgnoreCase(key)) {\n+        setLocation = value;\n+      } else if (\"current-snapshot-id\".equalsIgnoreCase(key)) {\n+        setSnapshotId = value;\n+      } else if (\"cherry-pick-snapshot-id\".equalsIgnoreCase(key)) {\n+        pickSnapshotId = value;\n+      } else {\n+        setProperties.put(key, value);\n+      }\n+    }\n+\n+    oldOptions.keySet().forEach(k -> {\n+      if (!newTable.getOptions().containsKey(k)) {\n+        setProperties.put(k, null);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "87f8c79f679aef109f0fce0fe55745e880948dca"}, "originalPosition": 153}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc2NTY3Njky", "url": "https://github.com/apache/iceberg/pull/1393#pullrequestreview-476567692", "createdAt": "2020-08-27T09:59:42Z", "commit": {"oid": "87f8c79f679aef109f0fce0fe55745e880948dca"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yN1QwOTo1OTo0MlrOHIJL2A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yN1QwOTo1OTo0MlrOHIJL2A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODMwMTE0NA==", "bodyText": "nit:  we may need to change this comment ? Actually, I did not found any class named IcebergCatalogTable,  or I misunderstood something ?", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r478301144", "createdAt": "2020-08-27T09:59:42Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java", "diffHunk": "@@ -277,20 +286,29 @@ public void alterDatabase(String name, CatalogDatabase newDatabase, boolean igno\n   }\n \n   @Override\n-  public CatalogBaseTable getTable(ObjectPath tablePath) throws TableNotExistException, CatalogException {\n-    try {\n-      Table table = icebergCatalog.loadTable(toIdentifier(tablePath));\n-      TableSchema tableSchema = FlinkSchemaUtil.toSchema(FlinkSchemaUtil.convert(table.schema()));\n+  public CatalogTable getTable(ObjectPath tablePath) throws TableNotExistException, CatalogException {\n+    Table table = getIcebergTable(tablePath);\n+    return toCatalogTable(table);\n+  }\n \n-      // NOTE: We can not create a IcebergCatalogTable, because Flink optimizer may use CatalogTableImpl to copy a new\n-      // catalog table.\n-      // Let's re-loading table from Iceberg catalog when creating source/sink operators.\n-      return new CatalogTableImpl(tableSchema, table.properties(), null);\n+  private Table getIcebergTable(ObjectPath tablePath) throws TableNotExistException {\n+    try {\n+      return icebergCatalog.loadTable(toIdentifier(tablePath));\n     } catch (org.apache.iceberg.exceptions.NoSuchTableException e) {\n       throw new TableNotExistException(getName(), tablePath, e);\n     }\n   }\n \n+  private CatalogTable toCatalogTable(Table table) {\n+    TableSchema schema = FlinkSchemaUtil.toSchema(FlinkSchemaUtil.convert(table.schema()));\n+    List<String> partitionKeys = toPartitionKeys(table.spec(), table.schema());\n+\n+    // NOTE: We can not create a IcebergCatalogTable, because Flink optimizer may use CatalogTableImpl to copy a new", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "87f8c79f679aef109f0fce0fe55745e880948dca"}, "originalPosition": 67}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5adac5c64d27e3a538c9741f9e9b37da60940ff5", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/iceberg/commit/5adac5c64d27e3a538c9741f9e9b37da60940ff5", "committedDate": "2020-08-28T08:23:34Z", "message": "Address comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2d1aed64d2681f41d27c7d945e009a72e51e05d2", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/iceberg/commit/2d1aed64d2681f41d27c7d945e009a72e51e05d2", "committedDate": "2020-08-28T09:15:55Z", "message": "checkstyle"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc4MDU5Mzc1", "url": "https://github.com/apache/iceberg/pull/1393#pullrequestreview-478059375", "createdAt": "2020-08-28T22:40:04Z", "commit": {"oid": "2d1aed64d2681f41d27c7d945e009a72e51e05d2"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQyMjo0MDowNFrOHJWMOw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQyMjo0MDowNFrOHJWMOw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU2MjgxMQ==", "bodyText": "What is null? Could you add a comment?", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r479562811", "createdAt": "2020-08-28T22:40:04Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java", "diffHunk": "@@ -277,20 +287,29 @@ public void alterDatabase(String name, CatalogDatabase newDatabase, boolean igno\n   }\n \n   @Override\n-  public CatalogBaseTable getTable(ObjectPath tablePath) throws TableNotExistException, CatalogException {\n-    try {\n-      Table table = icebergCatalog.loadTable(toIdentifier(tablePath));\n-      TableSchema tableSchema = FlinkSchemaUtil.toSchema(FlinkSchemaUtil.convert(table.schema()));\n+  public CatalogTable getTable(ObjectPath tablePath) throws TableNotExistException, CatalogException {\n+    Table table = getIcebergTable(tablePath);\n+    return toCatalogTable(table);\n+  }\n \n-      // NOTE: We can not create a IcebergCatalogTable, because Flink optimizer may use CatalogTableImpl to copy a new\n-      // catalog table.\n-      // Let's re-loading table from Iceberg catalog when creating source/sink operators.\n-      return new CatalogTableImpl(tableSchema, table.properties(), null);\n+  private Table getIcebergTable(ObjectPath tablePath) throws TableNotExistException {\n+    try {\n+      return icebergCatalog.loadTable(toIdentifier(tablePath));\n     } catch (org.apache.iceberg.exceptions.NoSuchTableException e) {\n       throw new TableNotExistException(getName(), tablePath, e);\n     }\n   }\n \n+  private CatalogTable toCatalogTable(Table table) {\n+    TableSchema schema = FlinkSchemaUtil.toSchema(FlinkSchemaUtil.convert(table.schema()));\n+    List<String> partitionKeys = toPartitionKeys(table.spec(), table.schema());\n+\n+    // NOTE: We can not create a IcebergCatalogTable, because Flink optimizer may use CatalogTableImpl to copy a new\n+    // catalog table.\n+    // Let's re-loading table from Iceberg catalog when creating source/sink operators.\n+    return new CatalogTableImpl(schema, partitionKeys, table.properties(), null);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2d1aed64d2681f41d27c7d945e009a72e51e05d2"}, "originalPosition": 71}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc4MDYwNjc4", "url": "https://github.com/apache/iceberg/pull/1393#pullrequestreview-478060678", "createdAt": "2020-08-28T22:44:59Z", "commit": {"oid": "2d1aed64d2681f41d27c7d945e009a72e51e05d2"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQyMjo0NDo1OVrOHJWQ0g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQyMjo0NDo1OVrOHJWQ0g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU2Mzk4Ng==", "bodyText": "get is not a very specific verb. I usually prefer load for cases like this because it more accurately describes what is happening.", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r479563986", "createdAt": "2020-08-28T22:44:59Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java", "diffHunk": "@@ -277,20 +287,29 @@ public void alterDatabase(String name, CatalogDatabase newDatabase, boolean igno\n   }\n \n   @Override\n-  public CatalogBaseTable getTable(ObjectPath tablePath) throws TableNotExistException, CatalogException {\n-    try {\n-      Table table = icebergCatalog.loadTable(toIdentifier(tablePath));\n-      TableSchema tableSchema = FlinkSchemaUtil.toSchema(FlinkSchemaUtil.convert(table.schema()));\n+  public CatalogTable getTable(ObjectPath tablePath) throws TableNotExistException, CatalogException {\n+    Table table = getIcebergTable(tablePath);\n+    return toCatalogTable(table);\n+  }\n \n-      // NOTE: We can not create a IcebergCatalogTable, because Flink optimizer may use CatalogTableImpl to copy a new\n-      // catalog table.\n-      // Let's re-loading table from Iceberg catalog when creating source/sink operators.\n-      return new CatalogTableImpl(tableSchema, table.properties(), null);\n+  private Table getIcebergTable(ObjectPath tablePath) throws TableNotExistException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2d1aed64d2681f41d27c7d945e009a72e51e05d2"}, "originalPosition": 56}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc4MDYzMzg0", "url": "https://github.com/apache/iceberg/pull/1393#pullrequestreview-478063384", "createdAt": "2020-08-28T22:55:32Z", "commit": {"oid": "2d1aed64d2681f41d27c7d945e009a72e51e05d2"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQyMjo1NTozMlrOHJWacQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQyMjo1NTozMlrOHJWacQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU2NjQ0OQ==", "bodyText": "Is there a case where CatalogBaseTable doesn't implement CatalogTable?", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r479566449", "createdAt": "2020-08-28T22:55:32Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java", "diffHunk": "@@ -320,19 +339,167 @@ public void renameTable(ObjectPath tablePath, String newTableName, boolean ignor\n     }\n   }\n \n-  /**\n-   * TODO Add partitioning to the Flink DDL parser.\n-   */\n   @Override\n   public void createTable(ObjectPath tablePath, CatalogBaseTable table, boolean ignoreIfExists)\n-      throws CatalogException {\n-    throw new UnsupportedOperationException(\"Not support createTable now.\");\n+      throws CatalogException, TableAlreadyExistException {\n+    validateFlinkTable(table);\n+\n+    Schema icebergSchema = FlinkSchemaUtil.convert(table.getSchema());\n+    PartitionSpec spec = toPartitionSpec(((CatalogTable) table).getPartitionKeys(), icebergSchema);\n+\n+    ImmutableMap.Builder<String, String> properties = ImmutableMap.builder();\n+    String location = null;\n+    for (Map.Entry<String, String> entry : table.getOptions().entrySet()) {\n+      if (\"location\".equalsIgnoreCase(entry.getKey())) {\n+        location = entry.getValue();\n+      } else {\n+        properties.put(entry.getKey(), entry.getValue());\n+      }\n+    }\n+\n+    try {\n+      icebergCatalog.createTable(\n+          toIdentifier(tablePath),\n+          icebergSchema,\n+          spec,\n+          location,\n+          properties.build());\n+    } catch (AlreadyExistsException e) {\n+      throw new TableAlreadyExistException(getName(), tablePath, e);\n+    }\n   }\n \n   @Override\n   public void alterTable(ObjectPath tablePath, CatalogBaseTable newTable, boolean ignoreIfNotExists)\n-      throws CatalogException {\n-    throw new UnsupportedOperationException(\"Not support alterTable now.\");\n+      throws CatalogException, TableNotExistException {\n+    validateFlinkTable(newTable);\n+    Table icebergTable = getIcebergTable(tablePath);\n+    CatalogTable table = toCatalogTable(icebergTable);\n+\n+    // Currently, Flink SQL only support altering table properties.\n+\n+    if (!table.getSchema().equals(newTable.getSchema())) {\n+      throw new UnsupportedOperationException(\"Altering schema is not supported yet.\");\n+    }\n+\n+    if (!table.getPartitionKeys().equals(((CatalogTable) newTable).getPartitionKeys())) {\n+      throw new UnsupportedOperationException(\"Altering partition keys is not supported yet.\");\n+    }\n+\n+    Map<String, String> oldOptions = table.getOptions();\n+    Map<String, String> setProperties = Maps.newHashMap();\n+\n+    String setLocation = null;\n+    String setSnapshotId = null;\n+    String pickSnapshotId = null;\n+\n+    for (Map.Entry<String, String> entry : newTable.getOptions().entrySet()) {\n+      String key = entry.getKey();\n+      String value = entry.getValue();\n+\n+      if (Objects.equals(value, oldOptions.get(key))) {\n+        continue;\n+      }\n+\n+      if (\"location\".equalsIgnoreCase(key)) {\n+        setLocation = value;\n+      } else if (\"current-snapshot-id\".equalsIgnoreCase(key)) {\n+        setSnapshotId = value;\n+      } else if (\"cherry-pick-snapshot-id\".equalsIgnoreCase(key)) {\n+        pickSnapshotId = value;\n+      } else {\n+        setProperties.put(key, value);\n+      }\n+    }\n+\n+    oldOptions.keySet().forEach(k -> {\n+      if (!newTable.getOptions().containsKey(k)) {\n+        setProperties.put(k, null);\n+      }\n+    });\n+\n+    commitChanges(icebergTable, setLocation, setSnapshotId, pickSnapshotId, setProperties);\n+  }\n+\n+  private static void validateFlinkTable(CatalogBaseTable table) {\n+    Preconditions.checkArgument(table instanceof CatalogTable, \"The Table should be a CatalogTable.\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2d1aed64d2681f41d27c7d945e009a72e51e05d2"}, "originalPosition": 171}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc4MDYzNTkw", "url": "https://github.com/apache/iceberg/pull/1393#pullrequestreview-478063590", "createdAt": "2020-08-28T22:56:23Z", "commit": {"oid": "2d1aed64d2681f41d27c7d945e009a72e51e05d2"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQyMjo1NjoyM1rOHJWbJA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQyMjo1NjoyM1rOHJWbJA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU2NjYyOA==", "bodyText": "Is this something we should add to Iceberg for Flink use cases? What does Flink use the primary key for?", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r479566628", "createdAt": "2020-08-28T22:56:23Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java", "diffHunk": "@@ -320,19 +339,167 @@ public void renameTable(ObjectPath tablePath, String newTableName, boolean ignor\n     }\n   }\n \n-  /**\n-   * TODO Add partitioning to the Flink DDL parser.\n-   */\n   @Override\n   public void createTable(ObjectPath tablePath, CatalogBaseTable table, boolean ignoreIfExists)\n-      throws CatalogException {\n-    throw new UnsupportedOperationException(\"Not support createTable now.\");\n+      throws CatalogException, TableAlreadyExistException {\n+    validateFlinkTable(table);\n+\n+    Schema icebergSchema = FlinkSchemaUtil.convert(table.getSchema());\n+    PartitionSpec spec = toPartitionSpec(((CatalogTable) table).getPartitionKeys(), icebergSchema);\n+\n+    ImmutableMap.Builder<String, String> properties = ImmutableMap.builder();\n+    String location = null;\n+    for (Map.Entry<String, String> entry : table.getOptions().entrySet()) {\n+      if (\"location\".equalsIgnoreCase(entry.getKey())) {\n+        location = entry.getValue();\n+      } else {\n+        properties.put(entry.getKey(), entry.getValue());\n+      }\n+    }\n+\n+    try {\n+      icebergCatalog.createTable(\n+          toIdentifier(tablePath),\n+          icebergSchema,\n+          spec,\n+          location,\n+          properties.build());\n+    } catch (AlreadyExistsException e) {\n+      throw new TableAlreadyExistException(getName(), tablePath, e);\n+    }\n   }\n \n   @Override\n   public void alterTable(ObjectPath tablePath, CatalogBaseTable newTable, boolean ignoreIfNotExists)\n-      throws CatalogException {\n-    throw new UnsupportedOperationException(\"Not support alterTable now.\");\n+      throws CatalogException, TableNotExistException {\n+    validateFlinkTable(newTable);\n+    Table icebergTable = getIcebergTable(tablePath);\n+    CatalogTable table = toCatalogTable(icebergTable);\n+\n+    // Currently, Flink SQL only support altering table properties.\n+\n+    if (!table.getSchema().equals(newTable.getSchema())) {\n+      throw new UnsupportedOperationException(\"Altering schema is not supported yet.\");\n+    }\n+\n+    if (!table.getPartitionKeys().equals(((CatalogTable) newTable).getPartitionKeys())) {\n+      throw new UnsupportedOperationException(\"Altering partition keys is not supported yet.\");\n+    }\n+\n+    Map<String, String> oldOptions = table.getOptions();\n+    Map<String, String> setProperties = Maps.newHashMap();\n+\n+    String setLocation = null;\n+    String setSnapshotId = null;\n+    String pickSnapshotId = null;\n+\n+    for (Map.Entry<String, String> entry : newTable.getOptions().entrySet()) {\n+      String key = entry.getKey();\n+      String value = entry.getValue();\n+\n+      if (Objects.equals(value, oldOptions.get(key))) {\n+        continue;\n+      }\n+\n+      if (\"location\".equalsIgnoreCase(key)) {\n+        setLocation = value;\n+      } else if (\"current-snapshot-id\".equalsIgnoreCase(key)) {\n+        setSnapshotId = value;\n+      } else if (\"cherry-pick-snapshot-id\".equalsIgnoreCase(key)) {\n+        pickSnapshotId = value;\n+      } else {\n+        setProperties.put(key, value);\n+      }\n+    }\n+\n+    oldOptions.keySet().forEach(k -> {\n+      if (!newTable.getOptions().containsKey(k)) {\n+        setProperties.put(k, null);\n+      }\n+    });\n+\n+    commitChanges(icebergTable, setLocation, setSnapshotId, pickSnapshotId, setProperties);\n+  }\n+\n+  private static void validateFlinkTable(CatalogBaseTable table) {\n+    Preconditions.checkArgument(table instanceof CatalogTable, \"The Table should be a CatalogTable.\");\n+\n+    TableSchema schema = table.getSchema();\n+    schema.getTableColumns().forEach(column -> {\n+      if (column.isGenerated()) {\n+        throw new UnsupportedOperationException(\"Creating table with computed columns is not supported yet.\");\n+      }\n+    });\n+\n+    if (!schema.getWatermarkSpecs().isEmpty()) {\n+      throw new UnsupportedOperationException(\"Creating table with watermark specs is not supported yet.\");\n+    }\n+\n+    if (schema.getPrimaryKey().isPresent()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2d1aed64d2681f41d27c7d945e009a72e51e05d2"}, "originalPosition": 184}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc4MDY0MDM5", "url": "https://github.com/apache/iceberg/pull/1393#pullrequestreview-478064039", "createdAt": "2020-08-28T22:58:06Z", "commit": {"oid": "2d1aed64d2681f41d27c7d945e009a72e51e05d2"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQyMjo1ODowNlrOHJWcrA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQyMjo1ODowNlrOHJWcrA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU2NzAyMA==", "bodyText": "All tables with any partition transform other than identity appear to be unpartitioned? Why not return all of the identity fields at least?", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r479567020", "createdAt": "2020-08-28T22:58:06Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java", "diffHunk": "@@ -320,19 +339,167 @@ public void renameTable(ObjectPath tablePath, String newTableName, boolean ignor\n     }\n   }\n \n-  /**\n-   * TODO Add partitioning to the Flink DDL parser.\n-   */\n   @Override\n   public void createTable(ObjectPath tablePath, CatalogBaseTable table, boolean ignoreIfExists)\n-      throws CatalogException {\n-    throw new UnsupportedOperationException(\"Not support createTable now.\");\n+      throws CatalogException, TableAlreadyExistException {\n+    validateFlinkTable(table);\n+\n+    Schema icebergSchema = FlinkSchemaUtil.convert(table.getSchema());\n+    PartitionSpec spec = toPartitionSpec(((CatalogTable) table).getPartitionKeys(), icebergSchema);\n+\n+    ImmutableMap.Builder<String, String> properties = ImmutableMap.builder();\n+    String location = null;\n+    for (Map.Entry<String, String> entry : table.getOptions().entrySet()) {\n+      if (\"location\".equalsIgnoreCase(entry.getKey())) {\n+        location = entry.getValue();\n+      } else {\n+        properties.put(entry.getKey(), entry.getValue());\n+      }\n+    }\n+\n+    try {\n+      icebergCatalog.createTable(\n+          toIdentifier(tablePath),\n+          icebergSchema,\n+          spec,\n+          location,\n+          properties.build());\n+    } catch (AlreadyExistsException e) {\n+      throw new TableAlreadyExistException(getName(), tablePath, e);\n+    }\n   }\n \n   @Override\n   public void alterTable(ObjectPath tablePath, CatalogBaseTable newTable, boolean ignoreIfNotExists)\n-      throws CatalogException {\n-    throw new UnsupportedOperationException(\"Not support alterTable now.\");\n+      throws CatalogException, TableNotExistException {\n+    validateFlinkTable(newTable);\n+    Table icebergTable = getIcebergTable(tablePath);\n+    CatalogTable table = toCatalogTable(icebergTable);\n+\n+    // Currently, Flink SQL only support altering table properties.\n+\n+    if (!table.getSchema().equals(newTable.getSchema())) {\n+      throw new UnsupportedOperationException(\"Altering schema is not supported yet.\");\n+    }\n+\n+    if (!table.getPartitionKeys().equals(((CatalogTable) newTable).getPartitionKeys())) {\n+      throw new UnsupportedOperationException(\"Altering partition keys is not supported yet.\");\n+    }\n+\n+    Map<String, String> oldOptions = table.getOptions();\n+    Map<String, String> setProperties = Maps.newHashMap();\n+\n+    String setLocation = null;\n+    String setSnapshotId = null;\n+    String pickSnapshotId = null;\n+\n+    for (Map.Entry<String, String> entry : newTable.getOptions().entrySet()) {\n+      String key = entry.getKey();\n+      String value = entry.getValue();\n+\n+      if (Objects.equals(value, oldOptions.get(key))) {\n+        continue;\n+      }\n+\n+      if (\"location\".equalsIgnoreCase(key)) {\n+        setLocation = value;\n+      } else if (\"current-snapshot-id\".equalsIgnoreCase(key)) {\n+        setSnapshotId = value;\n+      } else if (\"cherry-pick-snapshot-id\".equalsIgnoreCase(key)) {\n+        pickSnapshotId = value;\n+      } else {\n+        setProperties.put(key, value);\n+      }\n+    }\n+\n+    oldOptions.keySet().forEach(k -> {\n+      if (!newTable.getOptions().containsKey(k)) {\n+        setProperties.put(k, null);\n+      }\n+    });\n+\n+    commitChanges(icebergTable, setLocation, setSnapshotId, pickSnapshotId, setProperties);\n+  }\n+\n+  private static void validateFlinkTable(CatalogBaseTable table) {\n+    Preconditions.checkArgument(table instanceof CatalogTable, \"The Table should be a CatalogTable.\");\n+\n+    TableSchema schema = table.getSchema();\n+    schema.getTableColumns().forEach(column -> {\n+      if (column.isGenerated()) {\n+        throw new UnsupportedOperationException(\"Creating table with computed columns is not supported yet.\");\n+      }\n+    });\n+\n+    if (!schema.getWatermarkSpecs().isEmpty()) {\n+      throw new UnsupportedOperationException(\"Creating table with watermark specs is not supported yet.\");\n+    }\n+\n+    if (schema.getPrimaryKey().isPresent()) {\n+      throw new UnsupportedOperationException(\"Creating table with primary key is not supported yet.\");\n+    }\n+  }\n+\n+  private static PartitionSpec toPartitionSpec(List<String> partitionKeys, Schema icebergSchema) {\n+    PartitionSpec.Builder builder = PartitionSpec.builderFor(icebergSchema);\n+    partitionKeys.forEach(builder::identity);\n+    return builder.build();\n+  }\n+\n+  private static List<String> toPartitionKeys(PartitionSpec spec, Schema icebergSchema) {\n+    List<String> partitionKeys = Lists.newArrayList();\n+    for (PartitionField field : spec.fields()) {\n+      if (field.transform().isIdentity()) {\n+        partitionKeys.add(icebergSchema.findColumnName(field.sourceId()));\n+      } else {\n+        // Not created by Flink SQL.\n+        // For compatibility with iceberg tables, return empty.\n+        // TODO modify this after Flink support partition transform.\n+        return Collections.emptyList();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2d1aed64d2681f41d27c7d945e009a72e51e05d2"}, "originalPosition": 204}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc4MDY0Mzkz", "url": "https://github.com/apache/iceberg/pull/1393#pullrequestreview-478064393", "createdAt": "2020-08-28T22:59:32Z", "commit": {"oid": "2d1aed64d2681f41d27c7d945e009a72e51e05d2"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQyMjo1OTozM1rOHJWd8Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQyMjo1OTozM1rOHJWd8Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU2NzM0NQ==", "bodyText": "Should we do the operations above this point in the transaction as well? That seems reasonable to me. I'm not sure why we don't in other places.", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r479567345", "createdAt": "2020-08-28T22:59:33Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java", "diffHunk": "@@ -320,19 +339,167 @@ public void renameTable(ObjectPath tablePath, String newTableName, boolean ignor\n     }\n   }\n \n-  /**\n-   * TODO Add partitioning to the Flink DDL parser.\n-   */\n   @Override\n   public void createTable(ObjectPath tablePath, CatalogBaseTable table, boolean ignoreIfExists)\n-      throws CatalogException {\n-    throw new UnsupportedOperationException(\"Not support createTable now.\");\n+      throws CatalogException, TableAlreadyExistException {\n+    validateFlinkTable(table);\n+\n+    Schema icebergSchema = FlinkSchemaUtil.convert(table.getSchema());\n+    PartitionSpec spec = toPartitionSpec(((CatalogTable) table).getPartitionKeys(), icebergSchema);\n+\n+    ImmutableMap.Builder<String, String> properties = ImmutableMap.builder();\n+    String location = null;\n+    for (Map.Entry<String, String> entry : table.getOptions().entrySet()) {\n+      if (\"location\".equalsIgnoreCase(entry.getKey())) {\n+        location = entry.getValue();\n+      } else {\n+        properties.put(entry.getKey(), entry.getValue());\n+      }\n+    }\n+\n+    try {\n+      icebergCatalog.createTable(\n+          toIdentifier(tablePath),\n+          icebergSchema,\n+          spec,\n+          location,\n+          properties.build());\n+    } catch (AlreadyExistsException e) {\n+      throw new TableAlreadyExistException(getName(), tablePath, e);\n+    }\n   }\n \n   @Override\n   public void alterTable(ObjectPath tablePath, CatalogBaseTable newTable, boolean ignoreIfNotExists)\n-      throws CatalogException {\n-    throw new UnsupportedOperationException(\"Not support alterTable now.\");\n+      throws CatalogException, TableNotExistException {\n+    validateFlinkTable(newTable);\n+    Table icebergTable = getIcebergTable(tablePath);\n+    CatalogTable table = toCatalogTable(icebergTable);\n+\n+    // Currently, Flink SQL only support altering table properties.\n+\n+    if (!table.getSchema().equals(newTable.getSchema())) {\n+      throw new UnsupportedOperationException(\"Altering schema is not supported yet.\");\n+    }\n+\n+    if (!table.getPartitionKeys().equals(((CatalogTable) newTable).getPartitionKeys())) {\n+      throw new UnsupportedOperationException(\"Altering partition keys is not supported yet.\");\n+    }\n+\n+    Map<String, String> oldOptions = table.getOptions();\n+    Map<String, String> setProperties = Maps.newHashMap();\n+\n+    String setLocation = null;\n+    String setSnapshotId = null;\n+    String pickSnapshotId = null;\n+\n+    for (Map.Entry<String, String> entry : newTable.getOptions().entrySet()) {\n+      String key = entry.getKey();\n+      String value = entry.getValue();\n+\n+      if (Objects.equals(value, oldOptions.get(key))) {\n+        continue;\n+      }\n+\n+      if (\"location\".equalsIgnoreCase(key)) {\n+        setLocation = value;\n+      } else if (\"current-snapshot-id\".equalsIgnoreCase(key)) {\n+        setSnapshotId = value;\n+      } else if (\"cherry-pick-snapshot-id\".equalsIgnoreCase(key)) {\n+        pickSnapshotId = value;\n+      } else {\n+        setProperties.put(key, value);\n+      }\n+    }\n+\n+    oldOptions.keySet().forEach(k -> {\n+      if (!newTable.getOptions().containsKey(k)) {\n+        setProperties.put(k, null);\n+      }\n+    });\n+\n+    commitChanges(icebergTable, setLocation, setSnapshotId, pickSnapshotId, setProperties);\n+  }\n+\n+  private static void validateFlinkTable(CatalogBaseTable table) {\n+    Preconditions.checkArgument(table instanceof CatalogTable, \"The Table should be a CatalogTable.\");\n+\n+    TableSchema schema = table.getSchema();\n+    schema.getTableColumns().forEach(column -> {\n+      if (column.isGenerated()) {\n+        throw new UnsupportedOperationException(\"Creating table with computed columns is not supported yet.\");\n+      }\n+    });\n+\n+    if (!schema.getWatermarkSpecs().isEmpty()) {\n+      throw new UnsupportedOperationException(\"Creating table with watermark specs is not supported yet.\");\n+    }\n+\n+    if (schema.getPrimaryKey().isPresent()) {\n+      throw new UnsupportedOperationException(\"Creating table with primary key is not supported yet.\");\n+    }\n+  }\n+\n+  private static PartitionSpec toPartitionSpec(List<String> partitionKeys, Schema icebergSchema) {\n+    PartitionSpec.Builder builder = PartitionSpec.builderFor(icebergSchema);\n+    partitionKeys.forEach(builder::identity);\n+    return builder.build();\n+  }\n+\n+  private static List<String> toPartitionKeys(PartitionSpec spec, Schema icebergSchema) {\n+    List<String> partitionKeys = Lists.newArrayList();\n+    for (PartitionField field : spec.fields()) {\n+      if (field.transform().isIdentity()) {\n+        partitionKeys.add(icebergSchema.findColumnName(field.sourceId()));\n+      } else {\n+        // Not created by Flink SQL.\n+        // For compatibility with iceberg tables, return empty.\n+        // TODO modify this after Flink support partition transform.\n+        return Collections.emptyList();\n+      }\n+    }\n+    return partitionKeys;\n+  }\n+\n+  private static void commitChanges(Table table, String setLocation, String setSnapshotId,\n+                                    String pickSnapshotId, Map<String, String> setProperties) {\n+    // don't allow setting the snapshot and picking a commit at the same time because order is ambiguous and choosing\n+    // one order leads to different results\n+    Preconditions.checkArgument(setSnapshotId == null || pickSnapshotId == null,\n+        \"Cannot set the current the current snapshot ID and cherry-pick snapshot changes\");\n+\n+    if (setSnapshotId != null) {\n+      long newSnapshotId = Long.parseLong(setSnapshotId);\n+      table.manageSnapshots().setCurrentSnapshot(newSnapshotId).commit();\n+    }\n+\n+    // if updating the table snapshot, perform that update first in case it fails\n+    if (pickSnapshotId != null) {\n+      long newSnapshotId = Long.parseLong(pickSnapshotId);\n+      table.manageSnapshots().cherrypick(newSnapshotId).commit();\n+    }\n+\n+    Transaction transaction = table.newTransaction();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2d1aed64d2681f41d27c7d945e009a72e51e05d2"}, "originalPosition": 228}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc4MDY0Njg2", "url": "https://github.com/apache/iceberg/pull/1393#pullrequestreview-478064686", "createdAt": "2020-08-28T23:00:43Z", "commit": {"oid": "2d1aed64d2681f41d27c7d945e009a72e51e05d2"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQyMzowMDo0NFrOHJWfFw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQyMzowMDo0NFrOHJWfFw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU2NzYzOQ==", "bodyText": "Does Flink support the LOCATION clause, or just the table property?", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r479567639", "createdAt": "2020-08-28T23:00:44Z", "author": {"login": "rdblue"}, "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkCatalogTable.java", "diffHunk": "@@ -88,4 +105,172 @@ public void testRenameTable() {\n         Collections.singletonList(TableColumn.of(\"id\", DataTypes.BIGINT())),\n         tEnv.from(\"tl2\").getSchema().getTableColumns());\n   }\n+\n+  @Test\n+  public void testCreateTable() throws TableNotExistException {\n+    tEnv.executeSql(\"CREATE TABLE tl(id BIGINT)\");\n+\n+    Table table = table(\"tl\");\n+    Assert.assertEquals(\n+        new Schema(Types.NestedField.optional(1, \"id\", Types.LongType.get())).asStruct(),\n+        table.schema().asStruct());\n+    Assert.assertEquals(Maps.newHashMap(), table.properties());\n+\n+    CatalogTable catalogTable = catalogTable(\"tl\");\n+    Assert.assertEquals(TableSchema.builder().field(\"id\", DataTypes.BIGINT()).build(), catalogTable.getSchema());\n+    Assert.assertEquals(Maps.newHashMap(), catalogTable.getOptions());\n+  }\n+\n+  @Test\n+  public void testCreateTableLocation() {\n+    Assume.assumeFalse(\"HadoopCatalog does not support creating table with location\", isHadoopCatalog);\n+\n+    tEnv.executeSql(\"CREATE TABLE tl(id BIGINT) WITH ('location'='/tmp/location')\");\n+\n+    Table table = table(\"tl\");\n+    Assert.assertEquals(\n+        new Schema(Types.NestedField.optional(1, \"id\", Types.LongType.get())).asStruct(),\n+        table.schema().asStruct());\n+    Assert.assertEquals(\"/tmp/location\", table.location());\n+    Assert.assertEquals(Maps.newHashMap(), table.properties());\n+  }\n+\n+  @Test\n+  public void testCreatePartitionTable() throws TableNotExistException {\n+    tEnv.executeSql(\"CREATE TABLE tl(id BIGINT, dt STRING) PARTITIONED BY(dt)\");\n+\n+    Table table = table(\"tl\");\n+    Assert.assertEquals(\n+        new Schema(\n+            Types.NestedField.optional(1, \"id\", Types.LongType.get()),\n+            Types.NestedField.optional(2, \"dt\", Types.StringType.get())).asStruct(),\n+        table.schema().asStruct());\n+    Assert.assertEquals(PartitionSpec.builderFor(table.schema()).identity(\"dt\").build(), table.spec());\n+    Assert.assertEquals(Maps.newHashMap(), table.properties());\n+\n+    CatalogTable catalogTable = catalogTable(\"tl\");\n+    Assert.assertEquals(\n+        TableSchema.builder().field(\"id\", DataTypes.BIGINT()).field(\"dt\", DataTypes.STRING()).build(),\n+        catalogTable.getSchema());\n+    Assert.assertEquals(Maps.newHashMap(), catalogTable.getOptions());\n+    Assert.assertEquals(Collections.singletonList(\"dt\"), catalogTable.getPartitionKeys());\n+  }\n+\n+  @Test\n+  public void testLoadTransformPartitionTable() throws TableNotExistException {\n+    Schema schema = new Schema(Types.NestedField.optional(0, \"id\", Types.LongType.get()));\n+    validationCatalog.createTable(\n+        TableIdentifier.of(icebergNamespace, \"tl\"), schema,\n+        PartitionSpec.builderFor(schema).bucket(\"id\", 100).build());\n+\n+    CatalogTable catalogTable = catalogTable(\"tl\");\n+    Assert.assertEquals(\n+        TableSchema.builder().field(\"id\", DataTypes.BIGINT()).build(),\n+        catalogTable.getSchema());\n+    Assert.assertEquals(Maps.newHashMap(), catalogTable.getOptions());\n+    Assert.assertEquals(Collections.emptyList(), catalogTable.getPartitionKeys());\n+  }\n+\n+  @Test\n+  public void testAlterTable() throws TableNotExistException {\n+    tEnv.executeSql(\"CREATE TABLE tl(id BIGINT) WITH ('oldK'='oldV')\");\n+    Map<String, String> properties = Maps.newHashMap();\n+    properties.put(\"oldK\", \"oldV\");\n+\n+    // new\n+    tEnv.executeSql(\"ALTER TABLE tl SET('newK'='newV')\");\n+    properties.put(\"newK\", \"newV\");\n+    Assert.assertEquals(properties, table(\"tl\").properties());\n+\n+    // update old\n+    tEnv.executeSql(\"ALTER TABLE tl SET('oldK'='oldV2')\");\n+    properties.put(\"oldK\", \"oldV2\");\n+    Assert.assertEquals(properties, table(\"tl\").properties());\n+\n+    // remove property\n+    CatalogTable catalogTable = catalogTable(\"tl\");\n+    properties.remove(\"oldK\");\n+    tEnv.getCatalog(tEnv.getCurrentCatalog()).get().alterTable(\n+        new ObjectPath(DATABASE, \"tl\"), catalogTable.copy(properties), false);\n+    Assert.assertEquals(properties, table(\"tl\").properties());\n+  }\n+\n+  @Test\n+  public void testRelocateTable() {\n+    Assume.assumeFalse(\"HadoopCatalog does not support relocate table\", isHadoopCatalog);\n+\n+    tEnv.executeSql(\"CREATE TABLE tl(id BIGINT)\");\n+    tEnv.executeSql(\"ALTER TABLE tl SET('location'='/tmp/location')\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2d1aed64d2681f41d27c7d945e009a72e51e05d2"}, "originalPosition": 130}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc4MTY1NjM0", "url": "https://github.com/apache/iceberg/pull/1393#pullrequestreview-478165634", "createdAt": "2020-08-30T02:00:39Z", "commit": {"oid": "2d1aed64d2681f41d27c7d945e009a72e51e05d2"}, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0zMFQwMjowMDozOVrOHJfMZQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0zMFQwMjoyNzozMlrOHJfTNQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTcxMDMwOQ==", "bodyText": "Should location still be placed in the table properties or will that cause some kind of conflict / error?", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r479710309", "createdAt": "2020-08-30T02:00:39Z", "author": {"login": "kbendick"}, "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java", "diffHunk": "@@ -320,19 +339,167 @@ public void renameTable(ObjectPath tablePath, String newTableName, boolean ignor\n     }\n   }\n \n-  /**\n-   * TODO Add partitioning to the Flink DDL parser.\n-   */\n   @Override\n   public void createTable(ObjectPath tablePath, CatalogBaseTable table, boolean ignoreIfExists)\n-      throws CatalogException {\n-    throw new UnsupportedOperationException(\"Not support createTable now.\");\n+      throws CatalogException, TableAlreadyExistException {\n+    validateFlinkTable(table);\n+\n+    Schema icebergSchema = FlinkSchemaUtil.convert(table.getSchema());\n+    PartitionSpec spec = toPartitionSpec(((CatalogTable) table).getPartitionKeys(), icebergSchema);\n+\n+    ImmutableMap.Builder<String, String> properties = ImmutableMap.builder();\n+    String location = null;\n+    for (Map.Entry<String, String> entry : table.getOptions().entrySet()) {\n+      if (\"location\".equalsIgnoreCase(entry.getKey())) {\n+        location = entry.getValue();\n+      } else {\n+        properties.put(entry.getKey(), entry.getValue());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2d1aed64d2681f41d27c7d945e009a72e51e05d2"}, "originalPosition": 100}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTcxMTA3MA==", "bodyText": "I found this which might answer your question: https://cwiki.apache.org/confluence/display/FLINK/FLIP+87%3A+Primary+key+constraints+in+Table+API\nIn particular, here are the proposed changes:\nProposed Changes\nWe suggest to introduce the concept of primary key constraint as a hint for FLINK to leverage for optimizations.\n\nPrimary key constraints tell that a column or a set of columns of a table or a view are unique and they do not contain null.\nNeither of columns in a primary can be nullable.\nPrimary key therefore uniquely identify a row in a table.\n\nSo it sounds just like an RDBMS primary key.\nNote however, that even in the FLIP (which is just the proposal and not necessarily the finished product), it does state that there's no planned enforcement on the PK. It's up to the user to ensure that the PK is non-null and unique.\nPrimary key validity checks\nSQL standard specifies that a constraint can either be ENFORCED or NOT ENFORCED.\nThis controls if the constraint checks are performed on the incoming/outgoing data.\nFlink does not own the data therefore the only mode we want to support is the NOT ENFORCED mode.\nIts up to the user to ensure that the query enforces key integrity.\n\nSo I agree here that throwing might be the most useful option and that there's likely nothing on the iceberg side to be added to enforce this as Flink doesn't enforce it either. In an entirely streaming setting, ensuring unique keys would be rather difficult and so to me it somewhat sounds like the PK is just more metadata that could very well be in TBLPROPERTIES.\nBut a more experienced Flink SQL user than myself might have more to say on the matter. I've never attempted to enforce a PK when using Flink SQL. Sounds like the work to do so would involve custom operators etc.\nTLDR: The Primary Key is just a constraint, which is currently part of Flink's Table spec but goes unenforced and is up to the user. It does not appear as though the PK info is supported in any UpsertSinks etc, though that may be discussed / planned in the future. Support in the DDL for Primary Key constraints is relatively new (Flink 1.11 / current, with support in the API coming in at Flink 1.10).", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r479711070", "createdAt": "2020-08-30T02:12:33Z", "author": {"login": "kbendick"}, "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java", "diffHunk": "@@ -320,19 +339,167 @@ public void renameTable(ObjectPath tablePath, String newTableName, boolean ignor\n     }\n   }\n \n-  /**\n-   * TODO Add partitioning to the Flink DDL parser.\n-   */\n   @Override\n   public void createTable(ObjectPath tablePath, CatalogBaseTable table, boolean ignoreIfExists)\n-      throws CatalogException {\n-    throw new UnsupportedOperationException(\"Not support createTable now.\");\n+      throws CatalogException, TableAlreadyExistException {\n+    validateFlinkTable(table);\n+\n+    Schema icebergSchema = FlinkSchemaUtil.convert(table.getSchema());\n+    PartitionSpec spec = toPartitionSpec(((CatalogTable) table).getPartitionKeys(), icebergSchema);\n+\n+    ImmutableMap.Builder<String, String> properties = ImmutableMap.builder();\n+    String location = null;\n+    for (Map.Entry<String, String> entry : table.getOptions().entrySet()) {\n+      if (\"location\".equalsIgnoreCase(entry.getKey())) {\n+        location = entry.getValue();\n+      } else {\n+        properties.put(entry.getKey(), entry.getValue());\n+      }\n+    }\n+\n+    try {\n+      icebergCatalog.createTable(\n+          toIdentifier(tablePath),\n+          icebergSchema,\n+          spec,\n+          location,\n+          properties.build());\n+    } catch (AlreadyExistsException e) {\n+      throw new TableAlreadyExistException(getName(), tablePath, e);\n+    }\n   }\n \n   @Override\n   public void alterTable(ObjectPath tablePath, CatalogBaseTable newTable, boolean ignoreIfNotExists)\n-      throws CatalogException {\n-    throw new UnsupportedOperationException(\"Not support alterTable now.\");\n+      throws CatalogException, TableNotExistException {\n+    validateFlinkTable(newTable);\n+    Table icebergTable = getIcebergTable(tablePath);\n+    CatalogTable table = toCatalogTable(icebergTable);\n+\n+    // Currently, Flink SQL only support altering table properties.\n+\n+    if (!table.getSchema().equals(newTable.getSchema())) {\n+      throw new UnsupportedOperationException(\"Altering schema is not supported yet.\");\n+    }\n+\n+    if (!table.getPartitionKeys().equals(((CatalogTable) newTable).getPartitionKeys())) {\n+      throw new UnsupportedOperationException(\"Altering partition keys is not supported yet.\");\n+    }\n+\n+    Map<String, String> oldOptions = table.getOptions();\n+    Map<String, String> setProperties = Maps.newHashMap();\n+\n+    String setLocation = null;\n+    String setSnapshotId = null;\n+    String pickSnapshotId = null;\n+\n+    for (Map.Entry<String, String> entry : newTable.getOptions().entrySet()) {\n+      String key = entry.getKey();\n+      String value = entry.getValue();\n+\n+      if (Objects.equals(value, oldOptions.get(key))) {\n+        continue;\n+      }\n+\n+      if (\"location\".equalsIgnoreCase(key)) {\n+        setLocation = value;\n+      } else if (\"current-snapshot-id\".equalsIgnoreCase(key)) {\n+        setSnapshotId = value;\n+      } else if (\"cherry-pick-snapshot-id\".equalsIgnoreCase(key)) {\n+        pickSnapshotId = value;\n+      } else {\n+        setProperties.put(key, value);\n+      }\n+    }\n+\n+    oldOptions.keySet().forEach(k -> {\n+      if (!newTable.getOptions().containsKey(k)) {\n+        setProperties.put(k, null);\n+      }\n+    });\n+\n+    commitChanges(icebergTable, setLocation, setSnapshotId, pickSnapshotId, setProperties);\n+  }\n+\n+  private static void validateFlinkTable(CatalogBaseTable table) {\n+    Preconditions.checkArgument(table instanceof CatalogTable, \"The Table should be a CatalogTable.\");\n+\n+    TableSchema schema = table.getSchema();\n+    schema.getTableColumns().forEach(column -> {\n+      if (column.isGenerated()) {\n+        throw new UnsupportedOperationException(\"Creating table with computed columns is not supported yet.\");\n+      }\n+    });\n+\n+    if (!schema.getWatermarkSpecs().isEmpty()) {\n+      throw new UnsupportedOperationException(\"Creating table with watermark specs is not supported yet.\");\n+    }\n+\n+    if (schema.getPrimaryKey().isPresent()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU2NjYyOA=="}, "originalCommit": {"oid": "2d1aed64d2681f41d27c7d945e009a72e51e05d2"}, "originalPosition": 184}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTcxMTgwNw==", "bodyText": "CatalogTable is a subinterface that inherits from CatalogBaseTable. So definitely, yes.\nSee the java docs on the current CatalogBaseTable in Flink:\nhttps://ci.apache.org/projects/flink/flink-docs-release-1.11/api/java/org/apache/flink/table/catalog/CatalogBaseTable.html", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r479711807", "createdAt": "2020-08-30T02:23:58Z", "author": {"login": "kbendick"}, "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java", "diffHunk": "@@ -320,19 +339,167 @@ public void renameTable(ObjectPath tablePath, String newTableName, boolean ignor\n     }\n   }\n \n-  /**\n-   * TODO Add partitioning to the Flink DDL parser.\n-   */\n   @Override\n   public void createTable(ObjectPath tablePath, CatalogBaseTable table, boolean ignoreIfExists)\n-      throws CatalogException {\n-    throw new UnsupportedOperationException(\"Not support createTable now.\");\n+      throws CatalogException, TableAlreadyExistException {\n+    validateFlinkTable(table);\n+\n+    Schema icebergSchema = FlinkSchemaUtil.convert(table.getSchema());\n+    PartitionSpec spec = toPartitionSpec(((CatalogTable) table).getPartitionKeys(), icebergSchema);\n+\n+    ImmutableMap.Builder<String, String> properties = ImmutableMap.builder();\n+    String location = null;\n+    for (Map.Entry<String, String> entry : table.getOptions().entrySet()) {\n+      if (\"location\".equalsIgnoreCase(entry.getKey())) {\n+        location = entry.getValue();\n+      } else {\n+        properties.put(entry.getKey(), entry.getValue());\n+      }\n+    }\n+\n+    try {\n+      icebergCatalog.createTable(\n+          toIdentifier(tablePath),\n+          icebergSchema,\n+          spec,\n+          location,\n+          properties.build());\n+    } catch (AlreadyExistsException e) {\n+      throw new TableAlreadyExistException(getName(), tablePath, e);\n+    }\n   }\n \n   @Override\n   public void alterTable(ObjectPath tablePath, CatalogBaseTable newTable, boolean ignoreIfNotExists)\n-      throws CatalogException {\n-    throw new UnsupportedOperationException(\"Not support alterTable now.\");\n+      throws CatalogException, TableNotExistException {\n+    validateFlinkTable(newTable);\n+    Table icebergTable = getIcebergTable(tablePath);\n+    CatalogTable table = toCatalogTable(icebergTable);\n+\n+    // Currently, Flink SQL only support altering table properties.\n+\n+    if (!table.getSchema().equals(newTable.getSchema())) {\n+      throw new UnsupportedOperationException(\"Altering schema is not supported yet.\");\n+    }\n+\n+    if (!table.getPartitionKeys().equals(((CatalogTable) newTable).getPartitionKeys())) {\n+      throw new UnsupportedOperationException(\"Altering partition keys is not supported yet.\");\n+    }\n+\n+    Map<String, String> oldOptions = table.getOptions();\n+    Map<String, String> setProperties = Maps.newHashMap();\n+\n+    String setLocation = null;\n+    String setSnapshotId = null;\n+    String pickSnapshotId = null;\n+\n+    for (Map.Entry<String, String> entry : newTable.getOptions().entrySet()) {\n+      String key = entry.getKey();\n+      String value = entry.getValue();\n+\n+      if (Objects.equals(value, oldOptions.get(key))) {\n+        continue;\n+      }\n+\n+      if (\"location\".equalsIgnoreCase(key)) {\n+        setLocation = value;\n+      } else if (\"current-snapshot-id\".equalsIgnoreCase(key)) {\n+        setSnapshotId = value;\n+      } else if (\"cherry-pick-snapshot-id\".equalsIgnoreCase(key)) {\n+        pickSnapshotId = value;\n+      } else {\n+        setProperties.put(key, value);\n+      }\n+    }\n+\n+    oldOptions.keySet().forEach(k -> {\n+      if (!newTable.getOptions().containsKey(k)) {\n+        setProperties.put(k, null);\n+      }\n+    });\n+\n+    commitChanges(icebergTable, setLocation, setSnapshotId, pickSnapshotId, setProperties);\n+  }\n+\n+  private static void validateFlinkTable(CatalogBaseTable table) {\n+    Preconditions.checkArgument(table instanceof CatalogTable, \"The Table should be a CatalogTable.\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU2NjQ0OQ=="}, "originalCommit": {"oid": "2d1aed64d2681f41d27c7d945e009a72e51e05d2"}, "originalPosition": 171}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTcxMTk3Nw==", "bodyText": "To me, it seems like adding all of the identity fields (but not the transformed fields) would likely be incorrect. Although returning an empty list when the table is partitioned seems like a possible correctness bug to me too.\nShould we consider throwing an exception in this case instead until such a time that Flink supports partition transforms?", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r479711977", "createdAt": "2020-08-30T02:26:27Z", "author": {"login": "kbendick"}, "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java", "diffHunk": "@@ -320,19 +339,167 @@ public void renameTable(ObjectPath tablePath, String newTableName, boolean ignor\n     }\n   }\n \n-  /**\n-   * TODO Add partitioning to the Flink DDL parser.\n-   */\n   @Override\n   public void createTable(ObjectPath tablePath, CatalogBaseTable table, boolean ignoreIfExists)\n-      throws CatalogException {\n-    throw new UnsupportedOperationException(\"Not support createTable now.\");\n+      throws CatalogException, TableAlreadyExistException {\n+    validateFlinkTable(table);\n+\n+    Schema icebergSchema = FlinkSchemaUtil.convert(table.getSchema());\n+    PartitionSpec spec = toPartitionSpec(((CatalogTable) table).getPartitionKeys(), icebergSchema);\n+\n+    ImmutableMap.Builder<String, String> properties = ImmutableMap.builder();\n+    String location = null;\n+    for (Map.Entry<String, String> entry : table.getOptions().entrySet()) {\n+      if (\"location\".equalsIgnoreCase(entry.getKey())) {\n+        location = entry.getValue();\n+      } else {\n+        properties.put(entry.getKey(), entry.getValue());\n+      }\n+    }\n+\n+    try {\n+      icebergCatalog.createTable(\n+          toIdentifier(tablePath),\n+          icebergSchema,\n+          spec,\n+          location,\n+          properties.build());\n+    } catch (AlreadyExistsException e) {\n+      throw new TableAlreadyExistException(getName(), tablePath, e);\n+    }\n   }\n \n   @Override\n   public void alterTable(ObjectPath tablePath, CatalogBaseTable newTable, boolean ignoreIfNotExists)\n-      throws CatalogException {\n-    throw new UnsupportedOperationException(\"Not support alterTable now.\");\n+      throws CatalogException, TableNotExistException {\n+    validateFlinkTable(newTable);\n+    Table icebergTable = getIcebergTable(tablePath);\n+    CatalogTable table = toCatalogTable(icebergTable);\n+\n+    // Currently, Flink SQL only support altering table properties.\n+\n+    if (!table.getSchema().equals(newTable.getSchema())) {\n+      throw new UnsupportedOperationException(\"Altering schema is not supported yet.\");\n+    }\n+\n+    if (!table.getPartitionKeys().equals(((CatalogTable) newTable).getPartitionKeys())) {\n+      throw new UnsupportedOperationException(\"Altering partition keys is not supported yet.\");\n+    }\n+\n+    Map<String, String> oldOptions = table.getOptions();\n+    Map<String, String> setProperties = Maps.newHashMap();\n+\n+    String setLocation = null;\n+    String setSnapshotId = null;\n+    String pickSnapshotId = null;\n+\n+    for (Map.Entry<String, String> entry : newTable.getOptions().entrySet()) {\n+      String key = entry.getKey();\n+      String value = entry.getValue();\n+\n+      if (Objects.equals(value, oldOptions.get(key))) {\n+        continue;\n+      }\n+\n+      if (\"location\".equalsIgnoreCase(key)) {\n+        setLocation = value;\n+      } else if (\"current-snapshot-id\".equalsIgnoreCase(key)) {\n+        setSnapshotId = value;\n+      } else if (\"cherry-pick-snapshot-id\".equalsIgnoreCase(key)) {\n+        pickSnapshotId = value;\n+      } else {\n+        setProperties.put(key, value);\n+      }\n+    }\n+\n+    oldOptions.keySet().forEach(k -> {\n+      if (!newTable.getOptions().containsKey(k)) {\n+        setProperties.put(k, null);\n+      }\n+    });\n+\n+    commitChanges(icebergTable, setLocation, setSnapshotId, pickSnapshotId, setProperties);\n+  }\n+\n+  private static void validateFlinkTable(CatalogBaseTable table) {\n+    Preconditions.checkArgument(table instanceof CatalogTable, \"The Table should be a CatalogTable.\");\n+\n+    TableSchema schema = table.getSchema();\n+    schema.getTableColumns().forEach(column -> {\n+      if (column.isGenerated()) {\n+        throw new UnsupportedOperationException(\"Creating table with computed columns is not supported yet.\");\n+      }\n+    });\n+\n+    if (!schema.getWatermarkSpecs().isEmpty()) {\n+      throw new UnsupportedOperationException(\"Creating table with watermark specs is not supported yet.\");\n+    }\n+\n+    if (schema.getPrimaryKey().isPresent()) {\n+      throw new UnsupportedOperationException(\"Creating table with primary key is not supported yet.\");\n+    }\n+  }\n+\n+  private static PartitionSpec toPartitionSpec(List<String> partitionKeys, Schema icebergSchema) {\n+    PartitionSpec.Builder builder = PartitionSpec.builderFor(icebergSchema);\n+    partitionKeys.forEach(builder::identity);\n+    return builder.build();\n+  }\n+\n+  private static List<String> toPartitionKeys(PartitionSpec spec, Schema icebergSchema) {\n+    List<String> partitionKeys = Lists.newArrayList();\n+    for (PartitionField field : spec.fields()) {\n+      if (field.transform().isIdentity()) {\n+        partitionKeys.add(icebergSchema.findColumnName(field.sourceId()));\n+      } else {\n+        // Not created by Flink SQL.\n+        // For compatibility with iceberg tables, return empty.\n+        // TODO modify this after Flink support partition transform.\n+        return Collections.emptyList();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU2NzAyMA=="}, "originalCommit": {"oid": "2d1aed64d2681f41d27c7d945e009a72e51e05d2"}, "originalPosition": 204}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTcxMjA1Mw==", "bodyText": "Nit: Duplication of the words the current in the Preconditions string. It currently reads Cannot set the current the current.", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r479712053", "createdAt": "2020-08-30T02:27:32Z", "author": {"login": "kbendick"}, "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java", "diffHunk": "@@ -320,19 +339,167 @@ public void renameTable(ObjectPath tablePath, String newTableName, boolean ignor\n     }\n   }\n \n-  /**\n-   * TODO Add partitioning to the Flink DDL parser.\n-   */\n   @Override\n   public void createTable(ObjectPath tablePath, CatalogBaseTable table, boolean ignoreIfExists)\n-      throws CatalogException {\n-    throw new UnsupportedOperationException(\"Not support createTable now.\");\n+      throws CatalogException, TableAlreadyExistException {\n+    validateFlinkTable(table);\n+\n+    Schema icebergSchema = FlinkSchemaUtil.convert(table.getSchema());\n+    PartitionSpec spec = toPartitionSpec(((CatalogTable) table).getPartitionKeys(), icebergSchema);\n+\n+    ImmutableMap.Builder<String, String> properties = ImmutableMap.builder();\n+    String location = null;\n+    for (Map.Entry<String, String> entry : table.getOptions().entrySet()) {\n+      if (\"location\".equalsIgnoreCase(entry.getKey())) {\n+        location = entry.getValue();\n+      } else {\n+        properties.put(entry.getKey(), entry.getValue());\n+      }\n+    }\n+\n+    try {\n+      icebergCatalog.createTable(\n+          toIdentifier(tablePath),\n+          icebergSchema,\n+          spec,\n+          location,\n+          properties.build());\n+    } catch (AlreadyExistsException e) {\n+      throw new TableAlreadyExistException(getName(), tablePath, e);\n+    }\n   }\n \n   @Override\n   public void alterTable(ObjectPath tablePath, CatalogBaseTable newTable, boolean ignoreIfNotExists)\n-      throws CatalogException {\n-    throw new UnsupportedOperationException(\"Not support alterTable now.\");\n+      throws CatalogException, TableNotExistException {\n+    validateFlinkTable(newTable);\n+    Table icebergTable = getIcebergTable(tablePath);\n+    CatalogTable table = toCatalogTable(icebergTable);\n+\n+    // Currently, Flink SQL only support altering table properties.\n+\n+    if (!table.getSchema().equals(newTable.getSchema())) {\n+      throw new UnsupportedOperationException(\"Altering schema is not supported yet.\");\n+    }\n+\n+    if (!table.getPartitionKeys().equals(((CatalogTable) newTable).getPartitionKeys())) {\n+      throw new UnsupportedOperationException(\"Altering partition keys is not supported yet.\");\n+    }\n+\n+    Map<String, String> oldOptions = table.getOptions();\n+    Map<String, String> setProperties = Maps.newHashMap();\n+\n+    String setLocation = null;\n+    String setSnapshotId = null;\n+    String pickSnapshotId = null;\n+\n+    for (Map.Entry<String, String> entry : newTable.getOptions().entrySet()) {\n+      String key = entry.getKey();\n+      String value = entry.getValue();\n+\n+      if (Objects.equals(value, oldOptions.get(key))) {\n+        continue;\n+      }\n+\n+      if (\"location\".equalsIgnoreCase(key)) {\n+        setLocation = value;\n+      } else if (\"current-snapshot-id\".equalsIgnoreCase(key)) {\n+        setSnapshotId = value;\n+      } else if (\"cherry-pick-snapshot-id\".equalsIgnoreCase(key)) {\n+        pickSnapshotId = value;\n+      } else {\n+        setProperties.put(key, value);\n+      }\n+    }\n+\n+    oldOptions.keySet().forEach(k -> {\n+      if (!newTable.getOptions().containsKey(k)) {\n+        setProperties.put(k, null);\n+      }\n+    });\n+\n+    commitChanges(icebergTable, setLocation, setSnapshotId, pickSnapshotId, setProperties);\n+  }\n+\n+  private static void validateFlinkTable(CatalogBaseTable table) {\n+    Preconditions.checkArgument(table instanceof CatalogTable, \"The Table should be a CatalogTable.\");\n+\n+    TableSchema schema = table.getSchema();\n+    schema.getTableColumns().forEach(column -> {\n+      if (column.isGenerated()) {\n+        throw new UnsupportedOperationException(\"Creating table with computed columns is not supported yet.\");\n+      }\n+    });\n+\n+    if (!schema.getWatermarkSpecs().isEmpty()) {\n+      throw new UnsupportedOperationException(\"Creating table with watermark specs is not supported yet.\");\n+    }\n+\n+    if (schema.getPrimaryKey().isPresent()) {\n+      throw new UnsupportedOperationException(\"Creating table with primary key is not supported yet.\");\n+    }\n+  }\n+\n+  private static PartitionSpec toPartitionSpec(List<String> partitionKeys, Schema icebergSchema) {\n+    PartitionSpec.Builder builder = PartitionSpec.builderFor(icebergSchema);\n+    partitionKeys.forEach(builder::identity);\n+    return builder.build();\n+  }\n+\n+  private static List<String> toPartitionKeys(PartitionSpec spec, Schema icebergSchema) {\n+    List<String> partitionKeys = Lists.newArrayList();\n+    for (PartitionField field : spec.fields()) {\n+      if (field.transform().isIdentity()) {\n+        partitionKeys.add(icebergSchema.findColumnName(field.sourceId()));\n+      } else {\n+        // Not created by Flink SQL.\n+        // For compatibility with iceberg tables, return empty.\n+        // TODO modify this after Flink support partition transform.\n+        return Collections.emptyList();\n+      }\n+    }\n+    return partitionKeys;\n+  }\n+\n+  private static void commitChanges(Table table, String setLocation, String setSnapshotId,\n+                                    String pickSnapshotId, Map<String, String> setProperties) {\n+    // don't allow setting the snapshot and picking a commit at the same time because order is ambiguous and choosing\n+    // one order leads to different results\n+    Preconditions.checkArgument(setSnapshotId == null || pickSnapshotId == null,\n+        \"Cannot set the current the current snapshot ID and cherry-pick snapshot changes\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2d1aed64d2681f41d27c7d945e009a72e51e05d2"}, "originalPosition": 215}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6eb6a5cf0cb4ded46f54aa457d499a835fdf9208", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/iceberg/commit/6eb6a5cf0cb4ded46f54aa457d499a835fdf9208", "committedDate": "2020-08-31T03:31:15Z", "message": "Address comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc4NDI4NzY0", "url": "https://github.com/apache/iceberg/pull/1393#pullrequestreview-478428764", "createdAt": "2020-08-31T08:45:02Z", "commit": {"oid": "6eb6a5cf0cb4ded46f54aa457d499a835fdf9208"}, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0zMVQwODo0NTowMlrOHJv-ig==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0zMVQwOTowNTo1OFrOHJwpcA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTk4NTI5MA==", "bodyText": "nit: how about making the iceberg schema -> flink TableSchema conversion to be a static method inside FlinkSchemaUtil ?  The table sink pr #1348 will also depend on this static method (https://github.com/apache/iceberg/pull/1348/files#diff-0ad7dfff9cfa32fbb760796d976fd650R50).", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r479985290", "createdAt": "2020-08-31T08:45:02Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java", "diffHunk": "@@ -277,20 +287,30 @@ public void alterDatabase(String name, CatalogDatabase newDatabase, boolean igno\n   }\n \n   @Override\n-  public CatalogBaseTable getTable(ObjectPath tablePath) throws TableNotExistException, CatalogException {\n-    try {\n-      Table table = icebergCatalog.loadTable(toIdentifier(tablePath));\n-      TableSchema tableSchema = FlinkSchemaUtil.toSchema(FlinkSchemaUtil.convert(table.schema()));\n+  public CatalogTable getTable(ObjectPath tablePath) throws TableNotExistException, CatalogException {\n+    Table table = loadIcebergTable(tablePath);\n+    return toCatalogTable(table);\n+  }\n \n-      // NOTE: We can not create a IcebergCatalogTable, because Flink optimizer may use CatalogTableImpl to copy a new\n-      // catalog table.\n-      // Let's re-loading table from Iceberg catalog when creating source/sink operators.\n-      return new CatalogTableImpl(tableSchema, table.properties(), null);\n+  private Table loadIcebergTable(ObjectPath tablePath) throws TableNotExistException {\n+    try {\n+      return icebergCatalog.loadTable(toIdentifier(tablePath));\n     } catch (org.apache.iceberg.exceptions.NoSuchTableException e) {\n       throw new TableNotExistException(getName(), tablePath, e);\n     }\n   }\n \n+  private CatalogTable toCatalogTable(Table table) {\n+    TableSchema schema = FlinkSchemaUtil.toSchema(FlinkSchemaUtil.convert(table.schema()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6eb6a5cf0cb4ded46f54aa457d499a835fdf9208"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTk4NjQ2MA==", "bodyText": "Got it,  maybe could write this comment more clear.", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r479986460", "createdAt": "2020-08-31T08:47:20Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java", "diffHunk": "@@ -277,20 +286,29 @@ public void alterDatabase(String name, CatalogDatabase newDatabase, boolean igno\n   }\n \n   @Override\n-  public CatalogBaseTable getTable(ObjectPath tablePath) throws TableNotExistException, CatalogException {\n-    try {\n-      Table table = icebergCatalog.loadTable(toIdentifier(tablePath));\n-      TableSchema tableSchema = FlinkSchemaUtil.toSchema(FlinkSchemaUtil.convert(table.schema()));\n+  public CatalogTable getTable(ObjectPath tablePath) throws TableNotExistException, CatalogException {\n+    Table table = getIcebergTable(tablePath);\n+    return toCatalogTable(table);\n+  }\n \n-      // NOTE: We can not create a IcebergCatalogTable, because Flink optimizer may use CatalogTableImpl to copy a new\n-      // catalog table.\n-      // Let's re-loading table from Iceberg catalog when creating source/sink operators.\n-      return new CatalogTableImpl(tableSchema, table.properties(), null);\n+  private Table getIcebergTable(ObjectPath tablePath) throws TableNotExistException {\n+    try {\n+      return icebergCatalog.loadTable(toIdentifier(tablePath));\n     } catch (org.apache.iceberg.exceptions.NoSuchTableException e) {\n       throw new TableNotExistException(getName(), tablePath, e);\n     }\n   }\n \n+  private CatalogTable toCatalogTable(Table table) {\n+    TableSchema schema = FlinkSchemaUtil.toSchema(FlinkSchemaUtil.convert(table.schema()));\n+    List<String> partitionKeys = toPartitionKeys(table.spec(), table.schema());\n+\n+    // NOTE: We can not create a IcebergCatalogTable, because Flink optimizer may use CatalogTableImpl to copy a new", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODMwMTE0NA=="}, "originalCommit": {"oid": "87f8c79f679aef109f0fce0fe55745e880948dca"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTk4OTYzNg==", "bodyText": "Do we need to add a TODO indicating that we flink only support identity partition now but will support hidden column future  ?", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r479989636", "createdAt": "2020-08-31T08:53:36Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java", "diffHunk": "@@ -320,19 +340,169 @@ public void renameTable(ObjectPath tablePath, String newTableName, boolean ignor\n     }\n   }\n \n-  /**\n-   * TODO Add partitioning to the Flink DDL parser.\n-   */\n   @Override\n   public void createTable(ObjectPath tablePath, CatalogBaseTable table, boolean ignoreIfExists)\n-      throws CatalogException {\n-    throw new UnsupportedOperationException(\"Not support createTable now.\");\n+      throws CatalogException, TableAlreadyExistException {\n+    validateFlinkTable(table);\n+\n+    Schema icebergSchema = FlinkSchemaUtil.convert(table.getSchema());\n+    PartitionSpec spec = toPartitionSpec(((CatalogTable) table).getPartitionKeys(), icebergSchema);\n+\n+    ImmutableMap.Builder<String, String> properties = ImmutableMap.builder();\n+    String location = null;\n+    for (Map.Entry<String, String> entry : table.getOptions().entrySet()) {\n+      if (\"location\".equalsIgnoreCase(entry.getKey())) {\n+        location = entry.getValue();\n+      } else {\n+        properties.put(entry.getKey(), entry.getValue());\n+      }\n+    }\n+\n+    try {\n+      icebergCatalog.createTable(\n+          toIdentifier(tablePath),\n+          icebergSchema,\n+          spec,\n+          location,\n+          properties.build());\n+    } catch (AlreadyExistsException e) {\n+      throw new TableAlreadyExistException(getName(), tablePath, e);\n+    }\n   }\n \n   @Override\n   public void alterTable(ObjectPath tablePath, CatalogBaseTable newTable, boolean ignoreIfNotExists)\n-      throws CatalogException {\n-    throw new UnsupportedOperationException(\"Not support alterTable now.\");\n+      throws CatalogException, TableNotExistException {\n+    validateFlinkTable(newTable);\n+    Table icebergTable = loadIcebergTable(tablePath);\n+    CatalogTable table = toCatalogTable(icebergTable);\n+\n+    // Currently, Flink SQL only support altering table properties.\n+\n+    // For current Flink Catalog API, support for adding/removing/renaming columns cannot be done by comparing\n+    // CatalogTable instances, unless the Flink schema contains Iceberg column IDs.\n+    if (!table.getSchema().equals(newTable.getSchema())) {\n+      throw new UnsupportedOperationException(\"Altering schema is not supported yet.\");\n+    }\n+\n+    if (!table.getPartitionKeys().equals(((CatalogTable) newTable).getPartitionKeys())) {\n+      throw new UnsupportedOperationException(\"Altering partition keys is not supported yet.\");\n+    }\n+\n+    Map<String, String> oldOptions = table.getOptions();\n+    Map<String, String> setProperties = Maps.newHashMap();\n+\n+    String setLocation = null;\n+    String setSnapshotId = null;\n+    String pickSnapshotId = null;\n+\n+    for (Map.Entry<String, String> entry : newTable.getOptions().entrySet()) {\n+      String key = entry.getKey();\n+      String value = entry.getValue();\n+\n+      if (Objects.equals(value, oldOptions.get(key))) {\n+        continue;\n+      }\n+\n+      if (\"location\".equalsIgnoreCase(key)) {\n+        setLocation = value;\n+      } else if (\"current-snapshot-id\".equalsIgnoreCase(key)) {\n+        setSnapshotId = value;\n+      } else if (\"cherry-pick-snapshot-id\".equalsIgnoreCase(key)) {\n+        pickSnapshotId = value;\n+      } else {\n+        setProperties.put(key, value);\n+      }\n+    }\n+\n+    oldOptions.keySet().forEach(k -> {\n+      if (!newTable.getOptions().containsKey(k)) {\n+        setProperties.put(k, null);\n+      }\n+    });\n+\n+    commitChanges(icebergTable, setLocation, setSnapshotId, pickSnapshotId, setProperties);\n+  }\n+\n+  private static void validateFlinkTable(CatalogBaseTable table) {\n+    Preconditions.checkArgument(table instanceof CatalogTable, \"The Table should be a CatalogTable.\");\n+\n+    TableSchema schema = table.getSchema();\n+    schema.getTableColumns().forEach(column -> {\n+      if (column.isGenerated()) {\n+        throw new UnsupportedOperationException(\"Creating table with computed columns is not supported yet.\");\n+      }\n+    });\n+\n+    if (!schema.getWatermarkSpecs().isEmpty()) {\n+      throw new UnsupportedOperationException(\"Creating table with watermark specs is not supported yet.\");\n+    }\n+\n+    if (schema.getPrimaryKey().isPresent()) {\n+      throw new UnsupportedOperationException(\"Creating table with primary key is not supported yet.\");\n+    }\n+  }\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6eb6a5cf0cb4ded46f54aa457d499a835fdf9208"}, "originalPosition": 191}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTk5NTQzMw==", "bodyText": "setProperties.put(k, null) ?  The javadoc from Map said :\n * @throws NullPointerException if the specified key or value is null\n     *         and this map does not permit null keys or values\n     * @throws IllegalArgumentException if some property of the specified key\n     *         or value prevents it from being stored in this map\n     */\n    V put(K key, V value);", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r479995433", "createdAt": "2020-08-31T09:04:26Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java", "diffHunk": "@@ -320,19 +338,158 @@ public void renameTable(ObjectPath tablePath, String newTableName, boolean ignor\n     }\n   }\n \n-  /**\n-   * TODO Add partitioning to the Flink DDL parser.\n-   */\n   @Override\n   public void createTable(ObjectPath tablePath, CatalogBaseTable table, boolean ignoreIfExists)\n-      throws CatalogException {\n-    throw new UnsupportedOperationException(\"Not support createTable now.\");\n+      throws CatalogException, TableAlreadyExistException {\n+    validateFlinkTable(table);\n+\n+    Schema icebergSchema = FlinkSchemaUtil.convert(table.getSchema());\n+    PartitionSpec spec = toPartitionSpec(((CatalogTable) table).getPartitionKeys(), icebergSchema);\n+    Map<String, String> options = Maps.newHashMap(table.getOptions());\n+\n+    try {\n+      icebergCatalog.createTable(\n+          toIdentifier(tablePath),\n+          icebergSchema,\n+          spec,\n+          options.get(\"location\"),\n+          options);\n+    } catch (AlreadyExistsException e) {\n+      throw new TableAlreadyExistException(getName(), tablePath, e);\n+    }\n   }\n \n   @Override\n   public void alterTable(ObjectPath tablePath, CatalogBaseTable newTable, boolean ignoreIfNotExists)\n-      throws CatalogException {\n-    throw new UnsupportedOperationException(\"Not support alterTable now.\");\n+      throws CatalogException, TableNotExistException {\n+    validateFlinkTable(newTable);\n+    Table icebergTable = getIcebergTable(tablePath);\n+    CatalogTable table = toCatalogTable(icebergTable);\n+\n+    // Currently, Flink SQL only support altering table properties.\n+\n+    if (!table.getSchema().equals(newTable.getSchema())) {\n+      throw new UnsupportedOperationException(\"Altering schema is not supported yet.\");\n+    }\n+\n+    if (!table.getPartitionKeys().equals(((CatalogTable) newTable).getPartitionKeys())) {\n+      throw new UnsupportedOperationException(\"Altering partition keys is not supported yet.\");\n+    }\n+\n+    Map<String, String> oldOptions = table.getOptions();\n+    Map<String, String> setProperties = Maps.newHashMap();\n+\n+    String setLocation = null;\n+    String setSnapshotId = null;\n+    String pickSnapshotId = null;\n+\n+    for (Map.Entry<String, String> entry : newTable.getOptions().entrySet()) {\n+      String key = entry.getKey();\n+      String value = entry.getValue();\n+\n+      if (Objects.equals(value, oldOptions.get(key))) {\n+        continue;\n+      }\n+\n+      if (\"location\".equalsIgnoreCase(key)) {\n+        setLocation = value;\n+      } else if (\"current-snapshot-id\".equalsIgnoreCase(key)) {\n+        setSnapshotId = value;\n+      } else if (\"cherry-pick-snapshot-id\".equalsIgnoreCase(key)) {\n+        pickSnapshotId = value;\n+      } else {\n+        setProperties.put(key, value);\n+      }\n+    }\n+\n+    oldOptions.keySet().forEach(k -> {\n+      if (!newTable.getOptions().containsKey(k)) {\n+        setProperties.put(k, null);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODI4NDk5MA=="}, "originalCommit": {"oid": "87f8c79f679aef109f0fce0fe55745e880948dca"}, "originalPosition": 153}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTk5NjI3Mg==", "bodyText": "The v should never be null in HashMap ?", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r479996272", "createdAt": "2020-08-31T09:05:58Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java", "diffHunk": "@@ -320,19 +340,169 @@ public void renameTable(ObjectPath tablePath, String newTableName, boolean ignor\n     }\n   }\n \n-  /**\n-   * TODO Add partitioning to the Flink DDL parser.\n-   */\n   @Override\n   public void createTable(ObjectPath tablePath, CatalogBaseTable table, boolean ignoreIfExists)\n-      throws CatalogException {\n-    throw new UnsupportedOperationException(\"Not support createTable now.\");\n+      throws CatalogException, TableAlreadyExistException {\n+    validateFlinkTable(table);\n+\n+    Schema icebergSchema = FlinkSchemaUtil.convert(table.getSchema());\n+    PartitionSpec spec = toPartitionSpec(((CatalogTable) table).getPartitionKeys(), icebergSchema);\n+\n+    ImmutableMap.Builder<String, String> properties = ImmutableMap.builder();\n+    String location = null;\n+    for (Map.Entry<String, String> entry : table.getOptions().entrySet()) {\n+      if (\"location\".equalsIgnoreCase(entry.getKey())) {\n+        location = entry.getValue();\n+      } else {\n+        properties.put(entry.getKey(), entry.getValue());\n+      }\n+    }\n+\n+    try {\n+      icebergCatalog.createTable(\n+          toIdentifier(tablePath),\n+          icebergSchema,\n+          spec,\n+          location,\n+          properties.build());\n+    } catch (AlreadyExistsException e) {\n+      throw new TableAlreadyExistException(getName(), tablePath, e);\n+    }\n   }\n \n   @Override\n   public void alterTable(ObjectPath tablePath, CatalogBaseTable newTable, boolean ignoreIfNotExists)\n-      throws CatalogException {\n-    throw new UnsupportedOperationException(\"Not support alterTable now.\");\n+      throws CatalogException, TableNotExistException {\n+    validateFlinkTable(newTable);\n+    Table icebergTable = loadIcebergTable(tablePath);\n+    CatalogTable table = toCatalogTable(icebergTable);\n+\n+    // Currently, Flink SQL only support altering table properties.\n+\n+    // For current Flink Catalog API, support for adding/removing/renaming columns cannot be done by comparing\n+    // CatalogTable instances, unless the Flink schema contains Iceberg column IDs.\n+    if (!table.getSchema().equals(newTable.getSchema())) {\n+      throw new UnsupportedOperationException(\"Altering schema is not supported yet.\");\n+    }\n+\n+    if (!table.getPartitionKeys().equals(((CatalogTable) newTable).getPartitionKeys())) {\n+      throw new UnsupportedOperationException(\"Altering partition keys is not supported yet.\");\n+    }\n+\n+    Map<String, String> oldOptions = table.getOptions();\n+    Map<String, String> setProperties = Maps.newHashMap();\n+\n+    String setLocation = null;\n+    String setSnapshotId = null;\n+    String pickSnapshotId = null;\n+\n+    for (Map.Entry<String, String> entry : newTable.getOptions().entrySet()) {\n+      String key = entry.getKey();\n+      String value = entry.getValue();\n+\n+      if (Objects.equals(value, oldOptions.get(key))) {\n+        continue;\n+      }\n+\n+      if (\"location\".equalsIgnoreCase(key)) {\n+        setLocation = value;\n+      } else if (\"current-snapshot-id\".equalsIgnoreCase(key)) {\n+        setSnapshotId = value;\n+      } else if (\"cherry-pick-snapshot-id\".equalsIgnoreCase(key)) {\n+        pickSnapshotId = value;\n+      } else {\n+        setProperties.put(key, value);\n+      }\n+    }\n+\n+    oldOptions.keySet().forEach(k -> {\n+      if (!newTable.getOptions().containsKey(k)) {\n+        setProperties.put(k, null);\n+      }\n+    });\n+\n+    commitChanges(icebergTable, setLocation, setSnapshotId, pickSnapshotId, setProperties);\n+  }\n+\n+  private static void validateFlinkTable(CatalogBaseTable table) {\n+    Preconditions.checkArgument(table instanceof CatalogTable, \"The Table should be a CatalogTable.\");\n+\n+    TableSchema schema = table.getSchema();\n+    schema.getTableColumns().forEach(column -> {\n+      if (column.isGenerated()) {\n+        throw new UnsupportedOperationException(\"Creating table with computed columns is not supported yet.\");\n+      }\n+    });\n+\n+    if (!schema.getWatermarkSpecs().isEmpty()) {\n+      throw new UnsupportedOperationException(\"Creating table with watermark specs is not supported yet.\");\n+    }\n+\n+    if (schema.getPrimaryKey().isPresent()) {\n+      throw new UnsupportedOperationException(\"Creating table with primary key is not supported yet.\");\n+    }\n+  }\n+\n+  private static PartitionSpec toPartitionSpec(List<String> partitionKeys, Schema icebergSchema) {\n+    PartitionSpec.Builder builder = PartitionSpec.builderFor(icebergSchema);\n+    partitionKeys.forEach(builder::identity);\n+    return builder.build();\n+  }\n+\n+  private static List<String> toPartitionKeys(PartitionSpec spec, Schema icebergSchema) {\n+    List<String> partitionKeys = Lists.newArrayList();\n+    for (PartitionField field : spec.fields()) {\n+      if (field.transform().isIdentity()) {\n+        partitionKeys.add(icebergSchema.findColumnName(field.sourceId()));\n+      } else {\n+        // Not created by Flink SQL.\n+        // For compatibility with iceberg tables, return empty.\n+        // TODO modify this after Flink support partition transform.\n+        return Collections.emptyList();\n+      }\n+    }\n+    return partitionKeys;\n+  }\n+\n+  private static void commitChanges(Table table, String setLocation, String setSnapshotId,\n+                                    String pickSnapshotId, Map<String, String> setProperties) {\n+    // don't allow setting the snapshot and picking a commit at the same time because order is ambiguous and choosing\n+    // one order leads to different results\n+    Preconditions.checkArgument(setSnapshotId == null || pickSnapshotId == null,\n+        \"Cannot set the current snapshot ID and cherry-pick snapshot changes\");\n+\n+    if (setSnapshotId != null) {\n+      long newSnapshotId = Long.parseLong(setSnapshotId);\n+      table.manageSnapshots().setCurrentSnapshot(newSnapshotId).commit();\n+    }\n+\n+    // if updating the table snapshot, perform that update first in case it fails\n+    if (pickSnapshotId != null) {\n+      long newSnapshotId = Long.parseLong(pickSnapshotId);\n+      table.manageSnapshots().cherrypick(newSnapshotId).commit();\n+    }\n+\n+    Transaction transaction = table.newTransaction();\n+\n+    if (setLocation != null) {\n+      transaction.updateLocation()\n+          .setLocation(setLocation)\n+          .commit();\n+    }\n+\n+    if (!setProperties.isEmpty()) {\n+      UpdateProperties updateProperties = transaction.updateProperties();\n+      setProperties.forEach((k, v) -> {\n+        if (v == null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6eb6a5cf0cb4ded46f54aa457d499a835fdf9208"}, "originalPosition": 242}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e1dcb7f35d773e69749180ed337f7711c817c713", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/iceberg/commit/e1dcb7f35d773e69749180ed337f7711c817c713", "committedDate": "2020-08-31T09:27:08Z", "message": "Address comment"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ac06e524f3191fc303340ec23bc2149c8f93dcf2", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/iceberg/commit/ac06e524f3191fc303340ec23bc2149c8f93dcf2", "committedDate": "2020-09-01T08:13:19Z", "message": "checkstyles"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4128, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}