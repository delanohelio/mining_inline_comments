{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDY3MzY2MjA1", "number": 1334, "reviewThreads": {"totalCount": 14, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxNTowNzoxOFrOEYGN6g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOVQyMzozNDo1OVrOEeA3hg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNzAzMTQ2OnYy", "diffSide": "RIGHT", "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "isResolved": true, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxNTowNzoxOVrOHAP6iA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QxNzozMjoxOVrOHBzODQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDAyMjc5Mg==", "bodyText": "I believe the write can only come from org.apache.orc.storage.ql.exec.vector.BytesColumnVector, which would be heap array backed. Unless there is another implementation I don't know about.\nSee\nhttps://orc.apache.org/api/hive-storage-api/index.html?org/apache/hadoop/hive/ql/exec/vector/BytesColumnVector.html\nand\nhttps://github.com/apache/hive/blob/master/storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/BytesColumnVector.java", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r470022792", "createdAt": "2020-08-13T15:07:19Z", "author": {"login": "RussellSpitzer"}, "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "diffHunk": "@@ -231,7 +231,24 @@ public void nonNullWrite(int rowId, String data, ColumnVector output) {\n \n     @Override\n     public void nonNullWrite(int rowId, ByteBuffer data, ColumnVector output) {\n-      ((BytesColumnVector) output).setRef(rowId, data.array(), 0, data.array().length);\n+      // We technically can't be sure if the ByteBuffer coming in is on or off\n+      // heap so we cannot safely call `.array()` on it without first checking\n+      // via the method ByteBuffer.hasArray().\n+      // See: https://errorprone.info/bugpattern/ByteBufferBackingArray\n+      //\n+      // When there is a backing heap based byte array, we avoided the overhead of\n+      // copying, which is especially important for small byte buffers.\n+      //\n+      // TODO - This copy slows it down, perhap unnecessarily. Is there any other way to tell, or no?", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "37b7b618a2ab4dd3042529656f9f2aa618bc1332"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDc2MDE1Nw==", "bodyText": "Isn't this writing into a BytesColumnVector? If so, I don't think that the values are coming from one.\nI think it is correct to updating the handling here.\nIt would be great to get an opinion from @shardulm94 and @edgarRd as well.", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r470760157", "createdAt": "2020-08-14T17:31:33Z", "author": {"login": "rdblue"}, "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "diffHunk": "@@ -231,7 +231,24 @@ public void nonNullWrite(int rowId, String data, ColumnVector output) {\n \n     @Override\n     public void nonNullWrite(int rowId, ByteBuffer data, ColumnVector output) {\n-      ((BytesColumnVector) output).setRef(rowId, data.array(), 0, data.array().length);\n+      // We technically can't be sure if the ByteBuffer coming in is on or off\n+      // heap so we cannot safely call `.array()` on it without first checking\n+      // via the method ByteBuffer.hasArray().\n+      // See: https://errorprone.info/bugpattern/ByteBufferBackingArray\n+      //\n+      // When there is a backing heap based byte array, we avoided the overhead of\n+      // copying, which is especially important for small byte buffers.\n+      //\n+      // TODO - This copy slows it down, perhap unnecessarily. Is there any other way to tell, or no?", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDAyMjc5Mg=="}, "originalCommit": {"oid": "37b7b618a2ab4dd3042529656f9f2aa618bc1332"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDc2MDUxOQ==", "bodyText": "Ah that is true, I was thinking about it the wrong direction", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r470760519", "createdAt": "2020-08-14T17:32:17Z", "author": {"login": "RussellSpitzer"}, "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "diffHunk": "@@ -231,7 +231,24 @@ public void nonNullWrite(int rowId, String data, ColumnVector output) {\n \n     @Override\n     public void nonNullWrite(int rowId, ByteBuffer data, ColumnVector output) {\n-      ((BytesColumnVector) output).setRef(rowId, data.array(), 0, data.array().length);\n+      // We technically can't be sure if the ByteBuffer coming in is on or off\n+      // heap so we cannot safely call `.array()` on it without first checking\n+      // via the method ByteBuffer.hasArray().\n+      // See: https://errorprone.info/bugpattern/ByteBufferBackingArray\n+      //\n+      // When there is a backing heap based byte array, we avoided the overhead of\n+      // copying, which is especially important for small byte buffers.\n+      //\n+      // TODO - This copy slows it down, perhap unnecessarily. Is there any other way to tell, or no?", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDAyMjc5Mg=="}, "originalCommit": {"oid": "37b7b618a2ab4dd3042529656f9f2aa618bc1332"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkyMjg4MQ==", "bodyText": "I went looking into spark's implementation of off-heap byte arrays (for the spark-native version) and it somewhat lead me to believe that it could be reading in values as off-heap byte buffers when using spark.memory.offHeap.enabled=true and some value for  --conf spark.memory.offHeap.size.\nI'm working on a demo to see if that's true, but of note when I started up a spark-shell with those parameters specified (4G for the off heap size), I got the following error which leads me to believe that ByteBuffers were being created in an off-heap way.\nWARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/Cellar/apache-spark/3.0.0/libexec/jars/spark-unsafe_2.12-3.0.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n\nI am trying to work on a demo to test writing specifically off heap byte arrays to format(\"orc\") and then try reading it in and see what happens, but I'm not 100% sure when I'll have the time.\nBut if the ByteBuffers are off heap and we pass them to this function without \"sanitizing\" them, we need the check. The check is pretty inexpensive as well fwiw, it just checks if a primitive array element if equal to null or not.", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r470922881", "createdAt": "2020-08-15T01:44:45Z", "author": {"login": "kbendick"}, "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "diffHunk": "@@ -231,7 +231,24 @@ public void nonNullWrite(int rowId, String data, ColumnVector output) {\n \n     @Override\n     public void nonNullWrite(int rowId, ByteBuffer data, ColumnVector output) {\n-      ((BytesColumnVector) output).setRef(rowId, data.array(), 0, data.array().length);\n+      // We technically can't be sure if the ByteBuffer coming in is on or off\n+      // heap so we cannot safely call `.array()` on it without first checking\n+      // via the method ByteBuffer.hasArray().\n+      // See: https://errorprone.info/bugpattern/ByteBufferBackingArray\n+      //\n+      // When there is a backing heap based byte array, we avoided the overhead of\n+      // copying, which is especially important for small byte buffers.\n+      //\n+      // TODO - This copy slows it down, perhap unnecessarily. Is there any other way to tell, or no?", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDAyMjc5Mg=="}, "originalCommit": {"oid": "37b7b618a2ab4dd3042529656f9f2aa618bc1332"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTY0OTgwNQ==", "bodyText": "Right, I agree that when writing this ByteBuffer to a BytesColumnVector there was an assumption that it was backed by an array, but to support any ByteBuffer we need this handling here.", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r471649805", "createdAt": "2020-08-17T17:32:19Z", "author": {"login": "edgarRd"}, "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "diffHunk": "@@ -231,7 +231,24 @@ public void nonNullWrite(int rowId, String data, ColumnVector output) {\n \n     @Override\n     public void nonNullWrite(int rowId, ByteBuffer data, ColumnVector output) {\n-      ((BytesColumnVector) output).setRef(rowId, data.array(), 0, data.array().length);\n+      // We technically can't be sure if the ByteBuffer coming in is on or off\n+      // heap so we cannot safely call `.array()` on it without first checking\n+      // via the method ByteBuffer.hasArray().\n+      // See: https://errorprone.info/bugpattern/ByteBufferBackingArray\n+      //\n+      // When there is a backing heap based byte array, we avoided the overhead of\n+      // copying, which is especially important for small byte buffers.\n+      //\n+      // TODO - This copy slows it down, perhap unnecessarily. Is there any other way to tell, or no?", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDAyMjc5Mg=="}, "originalCommit": {"oid": "37b7b618a2ab4dd3042529656f9f2aa618bc1332"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0MTgxMjM5OnYy", "diffSide": "RIGHT", "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "isResolved": true, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQxNzoyODo1MFrOHA81Sw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QxODo1Nzo0NlrOHB3D4w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDc1ODczMQ==", "bodyText": "This is not a correct use of ByteBuffer because it doesn't use arrayOffset or remaining. I think the current version must work because Spark returns new arrays that we wrap in ByteBuffer, but if the goal here is to make this accept any ByteBuffer then we should account for cases where the buffer is not simply the entire backing array.", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r470758731", "createdAt": "2020-08-14T17:28:50Z", "author": {"login": "rdblue"}, "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "diffHunk": "@@ -231,7 +231,24 @@ public void nonNullWrite(int rowId, String data, ColumnVector output) {\n \n     @Override\n     public void nonNullWrite(int rowId, ByteBuffer data, ColumnVector output) {\n-      ((BytesColumnVector) output).setRef(rowId, data.array(), 0, data.array().length);\n+      // We technically can't be sure if the ByteBuffer coming in is on or off\n+      // heap so we cannot safely call `.array()` on it without first checking\n+      // via the method ByteBuffer.hasArray().\n+      // See: https://errorprone.info/bugpattern/ByteBufferBackingArray\n+      //\n+      // When there is a backing heap based byte array, we avoided the overhead of\n+      // copying, which is especially important for small byte buffers.\n+      //\n+      // TODO - This copy slows it down, perhap unnecessarily. Is there any other way to tell, or no?\n+      //        My guess is no, if I consider things like VectorizedOrcReaders on Spark.\n+      if (data.hasArray()) {\n+        ((BytesColumnVector) output).setRef(rowId, data.array(), 0, data.array().length);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "37b7b618a2ab4dd3042529656f9f2aa618bc1332"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkyMzA2OA==", "bodyText": "I'd like to point out that this is the original version of nonNullWrite, unless you're referring to the else block lines 246-251 just below your comment, in which case that's my addition.  https://github.com/apache/iceberg/pull/1334/files#diff-b1b07b15f036000a3f2bed76fdd9f961R246-R251\nI believe my addition (in the else block) is correct as it does call remaining. If we use hasArray, I just left it as the original function. I'd be happy to update anything that needs updating though.", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r470923068", "createdAt": "2020-08-15T01:46:26Z", "author": {"login": "kbendick"}, "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "diffHunk": "@@ -231,7 +231,24 @@ public void nonNullWrite(int rowId, String data, ColumnVector output) {\n \n     @Override\n     public void nonNullWrite(int rowId, ByteBuffer data, ColumnVector output) {\n-      ((BytesColumnVector) output).setRef(rowId, data.array(), 0, data.array().length);\n+      // We technically can't be sure if the ByteBuffer coming in is on or off\n+      // heap so we cannot safely call `.array()` on it without first checking\n+      // via the method ByteBuffer.hasArray().\n+      // See: https://errorprone.info/bugpattern/ByteBufferBackingArray\n+      //\n+      // When there is a backing heap based byte array, we avoided the overhead of\n+      // copying, which is especially important for small byte buffers.\n+      //\n+      // TODO - This copy slows it down, perhap unnecessarily. Is there any other way to tell, or no?\n+      //        My guess is no, if I consider things like VectorizedOrcReaders on Spark.\n+      if (data.hasArray()) {\n+        ((BytesColumnVector) output).setRef(rowId, data.array(), 0, data.array().length);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDc1ODczMQ=="}, "originalCommit": {"oid": "37b7b618a2ab4dd3042529656f9f2aa618bc1332"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA1NTA5Mg==", "bodyText": "Ok I see what you're referring to now. I will handle the fix in this PR as well. \ud83d\udc4d", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r471055092", "createdAt": "2020-08-16T02:20:21Z", "author": {"login": "kbendick"}, "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "diffHunk": "@@ -231,7 +231,24 @@ public void nonNullWrite(int rowId, String data, ColumnVector output) {\n \n     @Override\n     public void nonNullWrite(int rowId, ByteBuffer data, ColumnVector output) {\n-      ((BytesColumnVector) output).setRef(rowId, data.array(), 0, data.array().length);\n+      // We technically can't be sure if the ByteBuffer coming in is on or off\n+      // heap so we cannot safely call `.array()` on it without first checking\n+      // via the method ByteBuffer.hasArray().\n+      // See: https://errorprone.info/bugpattern/ByteBufferBackingArray\n+      //\n+      // When there is a backing heap based byte array, we avoided the overhead of\n+      // copying, which is especially important for small byte buffers.\n+      //\n+      // TODO - This copy slows it down, perhap unnecessarily. Is there any other way to tell, or no?\n+      //        My guess is no, if I consider things like VectorizedOrcReaders on Spark.\n+      if (data.hasArray()) {\n+        ((BytesColumnVector) output).setRef(rowId, data.array(), 0, data.array().length);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDc1ODczMQ=="}, "originalCommit": {"oid": "37b7b618a2ab4dd3042529656f9f2aa618bc1332"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA2MzIxMg==", "bodyText": "Should we consume the buffer up until that point or reset it to its original position?", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r471063212", "createdAt": "2020-08-16T04:21:11Z", "author": {"login": "kbendick"}, "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "diffHunk": "@@ -231,7 +231,24 @@ public void nonNullWrite(int rowId, String data, ColumnVector output) {\n \n     @Override\n     public void nonNullWrite(int rowId, ByteBuffer data, ColumnVector output) {\n-      ((BytesColumnVector) output).setRef(rowId, data.array(), 0, data.array().length);\n+      // We technically can't be sure if the ByteBuffer coming in is on or off\n+      // heap so we cannot safely call `.array()` on it without first checking\n+      // via the method ByteBuffer.hasArray().\n+      // See: https://errorprone.info/bugpattern/ByteBufferBackingArray\n+      //\n+      // When there is a backing heap based byte array, we avoided the overhead of\n+      // copying, which is especially important for small byte buffers.\n+      //\n+      // TODO - This copy slows it down, perhap unnecessarily. Is there any other way to tell, or no?\n+      //        My guess is no, if I consider things like VectorizedOrcReaders on Spark.\n+      if (data.hasArray()) {\n+        ((BytesColumnVector) output).setRef(rowId, data.array(), 0, data.array().length);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDc1ODczMQ=="}, "originalCommit": {"oid": "37b7b618a2ab4dd3042529656f9f2aa618bc1332"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTcxMjczOQ==", "bodyText": "If we are accessing the backing array, there is no need to worry about the state of the ByteBuffer. But the actual starting offset in the array is data.arrayOffset() + data.position() and the length is data.remaining(). Have a look at the copy methods in our ByteBuffers class to see examples.", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r471712739", "createdAt": "2020-08-17T18:57:46Z", "author": {"login": "rdblue"}, "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "diffHunk": "@@ -231,7 +231,24 @@ public void nonNullWrite(int rowId, String data, ColumnVector output) {\n \n     @Override\n     public void nonNullWrite(int rowId, ByteBuffer data, ColumnVector output) {\n-      ((BytesColumnVector) output).setRef(rowId, data.array(), 0, data.array().length);\n+      // We technically can't be sure if the ByteBuffer coming in is on or off\n+      // heap so we cannot safely call `.array()` on it without first checking\n+      // via the method ByteBuffer.hasArray().\n+      // See: https://errorprone.info/bugpattern/ByteBufferBackingArray\n+      //\n+      // When there is a backing heap based byte array, we avoided the overhead of\n+      // copying, which is especially important for small byte buffers.\n+      //\n+      // TODO - This copy slows it down, perhap unnecessarily. Is there any other way to tell, or no?\n+      //        My guess is no, if I consider things like VectorizedOrcReaders on Spark.\n+      if (data.hasArray()) {\n+        ((BytesColumnVector) output).setRef(rowId, data.array(), 0, data.array().length);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDc1ODczMQ=="}, "originalCommit": {"oid": "37b7b618a2ab4dd3042529656f9f2aa618bc1332"}, "originalPosition": 16}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0MTgzNzMxOnYy", "diffSide": "RIGHT", "path": "pig/src/main/java/org/apache/iceberg/pig/IcebergPigInputFormat.java", "isResolved": true, "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQxNzozNjo1NlrOHA9Etg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQwNzoxNDo1MVrOHC47fw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDc2MjY3OA==", "bodyText": "I don't think this is correct. This calls buffer.get(new byte[...]), which returns the ByteBuffer that get was called on. It is no different than returning new DataByteArray(buffer.array()). Calling get shows that the intent was to read the bytes into a new array and pass that to create a DataByteArray.\nThe correct implementation is this:\nByteBuffer buffer = (ByteBuffer) value;\nbyte[] bytes = new byte[buffer.remaining()];\nbuffer.get(bytes);\nreturn new DataByteArray(bytes);", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r470762678", "createdAt": "2020-08-14T17:36:56Z", "author": {"login": "rdblue"}, "path": "pig/src/main/java/org/apache/iceberg/pig/IcebergPigInputFormat.java", "diffHunk": "@@ -243,6 +243,7 @@ private boolean advance() throws IOException {\n       return true;\n     }\n \n+    @SuppressWarnings(\"ByteBufferBackingArray\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "37b7b618a2ab4dd3042529656f9f2aa618bc1332"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkxODk1NQ==", "bodyText": "Ok. Thanks for the insight. This was one admittedly that I suppressed but wasn't sure about later on. I'm in agreement though, due to the same arguments I've given elsewhere. I know much less about Pig, but I think it's better to be safe than sorry here if we don't know about the exact behavior of the input source, not or in the future.\nShould I update it in this PR or another PR?", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r470918955", "createdAt": "2020-08-15T01:07:41Z", "author": {"login": "kbendick"}, "path": "pig/src/main/java/org/apache/iceberg/pig/IcebergPigInputFormat.java", "diffHunk": "@@ -243,6 +243,7 @@ private boolean advance() throws IOException {\n       return true;\n     }\n \n+    @SuppressWarnings(\"ByteBufferBackingArray\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDc2MjY3OA=="}, "originalCommit": {"oid": "37b7b618a2ab4dd3042529656f9f2aa618bc1332"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkyNDA5Nw==", "bodyText": "And do I need to restore the buffer position as well? In my implementation above that uses remaining above in GenericOrcWriters that we're also discussing, I returned the original buffer (value here) to its original position. IIUC, they'd be backed by the same data as the casted buffer.", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r470924097", "createdAt": "2020-08-15T01:58:17Z", "author": {"login": "kbendick"}, "path": "pig/src/main/java/org/apache/iceberg/pig/IcebergPigInputFormat.java", "diffHunk": "@@ -243,6 +243,7 @@ private boolean advance() throws IOException {\n       return true;\n     }\n \n+    @SuppressWarnings(\"ByteBufferBackingArray\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDc2MjY3OA=="}, "originalCommit": {"oid": "37b7b618a2ab4dd3042529656f9f2aa618bc1332"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkyNDQzNA==", "bodyText": "It's worth noting that the java doc of ByteBuffer states that the underlying buffer does get moved forward.\n     * Unrelated stuff about BufferUnderflowException....\n     *\n     * <p> Otherwise, this method copies <tt>length</tt> bytes from this\n     * buffer into the given array, starting at the current position of this\n     * buffer and at the given offset in the array.  The position of this\n     * buffer is then incremented by <tt>length</tt>.\n     * ....", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r470924434", "createdAt": "2020-08-15T02:02:07Z", "author": {"login": "kbendick"}, "path": "pig/src/main/java/org/apache/iceberg/pig/IcebergPigInputFormat.java", "diffHunk": "@@ -243,6 +243,7 @@ private boolean advance() throws IOException {\n       return true;\n     }\n \n+    @SuppressWarnings(\"ByteBufferBackingArray\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDc2MjY3OA=="}, "originalCommit": {"oid": "37b7b618a2ab4dd3042529656f9f2aa618bc1332"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA2ODkyMQ==", "bodyText": "I updated this. I also moved the position of value (or buffer as it were) back to remaining before returning the copy. Is that necessary / will it lead to issues? I'm not sure what's needed in this case.", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r471068921", "createdAt": "2020-08-16T05:43:48Z", "author": {"login": "kbendick"}, "path": "pig/src/main/java/org/apache/iceberg/pig/IcebergPigInputFormat.java", "diffHunk": "@@ -243,6 +243,7 @@ private boolean advance() throws IOException {\n       return true;\n     }\n \n+    @SuppressWarnings(\"ByteBufferBackingArray\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDc2MjY3OA=="}, "originalCommit": {"oid": "37b7b618a2ab4dd3042529656f9f2aa618bc1332"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTcxMzMwNw==", "bodyText": "We usually duplicate the buffer before calling get to avoid changing the original buffer's state.\nAnd this can be updated in this PR, since the scope here is to fix the errors and suppress any other warnings.", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r471713307", "createdAt": "2020-08-17T18:58:50Z", "author": {"login": "rdblue"}, "path": "pig/src/main/java/org/apache/iceberg/pig/IcebergPigInputFormat.java", "diffHunk": "@@ -243,6 +243,7 @@ private boolean advance() throws IOException {\n       return true;\n     }\n \n+    @SuppressWarnings(\"ByteBufferBackingArray\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDc2MjY3OA=="}, "originalCommit": {"oid": "37b7b618a2ab4dd3042529656f9f2aa618bc1332"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mjc5MTkzNQ==", "bodyText": "I see. That makes sense. Thanks for helping me to understand better. And you're right, fixing the issues is definitely in scope. That's why I started this endeavor \ud83d\ude05", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r472791935", "createdAt": "2020-08-19T07:14:51Z", "author": {"login": "kbendick"}, "path": "pig/src/main/java/org/apache/iceberg/pig/IcebergPigInputFormat.java", "diffHunk": "@@ -243,6 +243,7 @@ private boolean advance() throws IOException {\n       return true;\n     }\n \n+    @SuppressWarnings(\"ByteBufferBackingArray\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDc2MjY3OA=="}, "originalCommit": {"oid": "37b7b618a2ab4dd3042529656f9f2aa618bc1332"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0NDA2MTU4OnYy", "diffSide": "RIGHT", "path": "pig/src/main/java/org/apache/iceberg/pig/IcebergPigInputFormat.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNlQwNTowMTowNFrOHBPkzA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQwMzo1ODozMVrOHCCdYQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA2NTgwNA==", "bodyText": "I updated this one as you requested @rdblue, however I wasn't sure if I should return the input value to its original position (vs consuming the remainder of the byte buffer and moving the position to the end).", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r471065804", "createdAt": "2020-08-16T05:01:04Z", "author": {"login": "kbendick"}, "path": "pig/src/main/java/org/apache/iceberg/pig/IcebergPigInputFormat.java", "diffHunk": "@@ -246,7 +246,11 @@ private boolean advance() throws IOException {\n     private Object convertPartitionValue(Type type, Object value) {\n       if (type.typeId() == Types.BinaryType.get().typeId()) {\n         ByteBuffer buffer = (ByteBuffer) value;\n-        return new DataByteArray(buffer.get(new byte[buffer.remaining()]).array());\n+        byte[] bytes = new byte[buffer.remaining()];\n+        buffer.get(bytes);\n+        // Return the input buffer back to its original position.\n+        buffer.position(buffer.position() - bytes.length);\n+        return new DataByteArray(bytes);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8fe4ecdb2a522c419791c57567c6ae0cbf6ecf23"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTcxNTg5OQ==", "bodyText": "Looks good, but we should use slice or duplicate to read to avoid needing to restore the original buffer's state.", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r471715899", "createdAt": "2020-08-17T19:03:57Z", "author": {"login": "rdblue"}, "path": "pig/src/main/java/org/apache/iceberg/pig/IcebergPigInputFormat.java", "diffHunk": "@@ -246,7 +246,11 @@ private boolean advance() throws IOException {\n     private Object convertPartitionValue(Type type, Object value) {\n       if (type.typeId() == Types.BinaryType.get().typeId()) {\n         ByteBuffer buffer = (ByteBuffer) value;\n-        return new DataByteArray(buffer.get(new byte[buffer.remaining()]).array());\n+        byte[] bytes = new byte[buffer.remaining()];\n+        buffer.get(bytes);\n+        // Return the input buffer back to its original position.\n+        buffer.position(buffer.position() - bytes.length);\n+        return new DataByteArray(bytes);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA2NTgwNA=="}, "originalCommit": {"oid": "8fe4ecdb2a522c419791c57567c6ae0cbf6ecf23"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTg5OTQ4OQ==", "bodyText": "Ok. I will update this one to use slice to avoid having to restore the original buffer's state.", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r471899489", "createdAt": "2020-08-18T03:58:31Z", "author": {"login": "kbendick"}, "path": "pig/src/main/java/org/apache/iceberg/pig/IcebergPigInputFormat.java", "diffHunk": "@@ -246,7 +246,11 @@ private boolean advance() throws IOException {\n     private Object convertPartitionValue(Type type, Object value) {\n       if (type.typeId() == Types.BinaryType.get().typeId()) {\n         ByteBuffer buffer = (ByteBuffer) value;\n-        return new DataByteArray(buffer.get(new byte[buffer.remaining()]).array());\n+        byte[] bytes = new byte[buffer.remaining()];\n+        buffer.get(bytes);\n+        // Return the input buffer back to its original position.\n+        buffer.position(buffer.position() - bytes.length);\n+        return new DataByteArray(bytes);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA2NTgwNA=="}, "originalCommit": {"oid": "8fe4ecdb2a522c419791c57567c6ae0cbf6ecf23"}, "originalPosition": 9}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0NDA4NzM4OnYy", "diffSide": "RIGHT", "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNlQwNTo0MjowMFrOHBPwaw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQwNzoxNjozM1rOHC5BXQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA2ODc3OQ==", "bodyText": "I updated this line to not assume that the entire backing array of data is what we're reading in. Via the use of slice, a shallow copy takes place (which mimics the previous shallow copy). Does this resolve your previous concern?\nI can clean up the comments etc once I know if this resolves any on going issues or not @rdblue . Thanks so much for your thorough review!", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r471068779", "createdAt": "2020-08-16T05:42:00Z", "author": {"login": "kbendick"}, "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "diffHunk": "@@ -231,7 +231,23 @@ public void nonNullWrite(int rowId, String data, ColumnVector output) {\n \n     @Override\n     public void nonNullWrite(int rowId, ByteBuffer data, ColumnVector output) {\n-      ((BytesColumnVector) output).setRef(rowId, data.array(), 0, data.array().length);\n+      // When there is a backing heap based byte array, we avoid the overhead of\n+      // copying it.\n+      if (data.hasArray()) {\n+        // Don't assume the we're reading in the entire backing array.\n+        // Using slice as any mutations to the data at rowId of output\n+        // would also be visible in `data`.\n+        ByteBuffer slice = data.slice();\n+        ((BytesColumnVector) output).setRef(rowId, slice.array(), 0, slice.array().length);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "00f9e46044b270aca0d29179e2405a9c312455e6"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTE3Mjc0OA==", "bodyText": "Actually, my solution with slice causes a ByteBufferBackingArray warning, which could be suppressed as we called .hasArray before. But at this point suppression seems silly given the work that went in to correcting this.\nIf anybody has any solutions which doesn't involve a deep copy, I'm open to suggestions.", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r471172748", "createdAt": "2020-08-16T23:28:16Z", "author": {"login": "kbendick"}, "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "diffHunk": "@@ -231,7 +231,23 @@ public void nonNullWrite(int rowId, String data, ColumnVector output) {\n \n     @Override\n     public void nonNullWrite(int rowId, ByteBuffer data, ColumnVector output) {\n-      ((BytesColumnVector) output).setRef(rowId, data.array(), 0, data.array().length);\n+      // When there is a backing heap based byte array, we avoid the overhead of\n+      // copying it.\n+      if (data.hasArray()) {\n+        // Don't assume the we're reading in the entire backing array.\n+        // Using slice as any mutations to the data at rowId of output\n+        // would also be visible in `data`.\n+        ByteBuffer slice = data.slice();\n+        ((BytesColumnVector) output).setRef(rowId, slice.array(), 0, slice.array().length);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA2ODc3OQ=="}, "originalCommit": {"oid": "00f9e46044b270aca0d29179e2405a9c312455e6"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTcxNTM1NQ==", "bodyText": "This isn't a correct use of slice. Slice is like duplicate, except that it guarantees that position will be 0 and capacity is equal to limit. It still uses the same backing bytes and will not copy, which we would not want even if it did. Because this still uses the same backing array, we still need to use arrayOffset.\nThere isn't any benefit to using slice here since accessing the backing byte array means that we don't modify the original buffer.", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r471715355", "createdAt": "2020-08-17T19:02:44Z", "author": {"login": "rdblue"}, "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "diffHunk": "@@ -231,7 +231,23 @@ public void nonNullWrite(int rowId, String data, ColumnVector output) {\n \n     @Override\n     public void nonNullWrite(int rowId, ByteBuffer data, ColumnVector output) {\n-      ((BytesColumnVector) output).setRef(rowId, data.array(), 0, data.array().length);\n+      // When there is a backing heap based byte array, we avoid the overhead of\n+      // copying it.\n+      if (data.hasArray()) {\n+        // Don't assume the we're reading in the entire backing array.\n+        // Using slice as any mutations to the data at rowId of output\n+        // would also be visible in `data`.\n+        ByteBuffer slice = data.slice();\n+        ((BytesColumnVector) output).setRef(rowId, slice.array(), 0, slice.array().length);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA2ODc3OQ=="}, "originalCommit": {"oid": "00f9e46044b270aca0d29179e2405a9c312455e6"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mjc5MzQzNw==", "bodyText": "I've factored out a duplicate and then reused that in both portions, on heap and off, and changed the position to be arrayOffset when on heap. Thanks for your review.", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r472793437", "createdAt": "2020-08-19T07:16:33Z", "author": {"login": "kbendick"}, "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "diffHunk": "@@ -231,7 +231,23 @@ public void nonNullWrite(int rowId, String data, ColumnVector output) {\n \n     @Override\n     public void nonNullWrite(int rowId, ByteBuffer data, ColumnVector output) {\n-      ((BytesColumnVector) output).setRef(rowId, data.array(), 0, data.array().length);\n+      // When there is a backing heap based byte array, we avoid the overhead of\n+      // copying it.\n+      if (data.hasArray()) {\n+        // Don't assume the we're reading in the entire backing array.\n+        // Using slice as any mutations to the data at rowId of output\n+        // would also be visible in `data`.\n+        ByteBuffer slice = data.slice();\n+        ((BytesColumnVector) output).setRef(rowId, slice.array(), 0, slice.array().length);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA2ODc3OQ=="}, "originalCommit": {"oid": "00f9e46044b270aca0d29179e2405a9c312455e6"}, "originalPosition": 12}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0ODA2MTM4OnYy", "diffSide": "RIGHT", "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QxNzozNToyMlrOHBzUWA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQwNzoxMjowNlrOHC4y0w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTY1MTQxNg==", "bodyText": "Could you avoid this by using slice too? Maybe factoring the slice out of the if...else?", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r471651416", "createdAt": "2020-08-17T17:35:22Z", "author": {"login": "edgarRd"}, "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "diffHunk": "@@ -231,7 +231,23 @@ public void nonNullWrite(int rowId, String data, ColumnVector output) {\n \n     @Override\n     public void nonNullWrite(int rowId, ByteBuffer data, ColumnVector output) {\n-      ((BytesColumnVector) output).setRef(rowId, data.array(), 0, data.array().length);\n+      // When there is a backing heap based byte array, we avoid the overhead of\n+      // copying it.\n+      if (data.hasArray()) {\n+        // Don't assume the we're reading in the entire backing array.\n+        // Using slice as any mutations to the data at rowId of output\n+        // would also be visible in `data`.\n+        ByteBuffer slice = data.slice();\n+        ((BytesColumnVector) output).setRef(rowId, slice.array(), 0, slice.array().length);\n+      } else {\n+        // Consume the remaining contents of the input data\n+        byte[] bytes = new byte[data.remaining()];\n+        data.get(bytes);\n+        // Restores the buffer position\n+        // TODO - Is this necessary?\n+        data.position(data.position() - bytes.length);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "00f9e46044b270aca0d29179e2405a9c312455e6"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTcxNTYyNA==", "bodyText": "I agree. By using either slice or duplicate, there is no need to restore the state of the incoming buffer.", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r471715624", "createdAt": "2020-08-17T19:03:20Z", "author": {"login": "rdblue"}, "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "diffHunk": "@@ -231,7 +231,23 @@ public void nonNullWrite(int rowId, String data, ColumnVector output) {\n \n     @Override\n     public void nonNullWrite(int rowId, ByteBuffer data, ColumnVector output) {\n-      ((BytesColumnVector) output).setRef(rowId, data.array(), 0, data.array().length);\n+      // When there is a backing heap based byte array, we avoid the overhead of\n+      // copying it.\n+      if (data.hasArray()) {\n+        // Don't assume the we're reading in the entire backing array.\n+        // Using slice as any mutations to the data at rowId of output\n+        // would also be visible in `data`.\n+        ByteBuffer slice = data.slice();\n+        ((BytesColumnVector) output).setRef(rowId, slice.array(), 0, slice.array().length);\n+      } else {\n+        // Consume the remaining contents of the input data\n+        byte[] bytes = new byte[data.remaining()];\n+        data.get(bytes);\n+        // Restores the buffer position\n+        // TODO - Is this necessary?\n+        data.position(data.position() - bytes.length);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTY1MTQxNg=="}, "originalCommit": {"oid": "00f9e46044b270aca0d29179e2405a9c312455e6"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mjc3OTEyOQ==", "bodyText": "Ok. I will prefer duplicate here. I'll still need to use a Suppression, but it will be the correct solution if I understand correctly.", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r472779129", "createdAt": "2020-08-19T06:58:36Z", "author": {"login": "kbendick"}, "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "diffHunk": "@@ -231,7 +231,23 @@ public void nonNullWrite(int rowId, String data, ColumnVector output) {\n \n     @Override\n     public void nonNullWrite(int rowId, ByteBuffer data, ColumnVector output) {\n-      ((BytesColumnVector) output).setRef(rowId, data.array(), 0, data.array().length);\n+      // When there is a backing heap based byte array, we avoid the overhead of\n+      // copying it.\n+      if (data.hasArray()) {\n+        // Don't assume the we're reading in the entire backing array.\n+        // Using slice as any mutations to the data at rowId of output\n+        // would also be visible in `data`.\n+        ByteBuffer slice = data.slice();\n+        ((BytesColumnVector) output).setRef(rowId, slice.array(), 0, slice.array().length);\n+      } else {\n+        // Consume the remaining contents of the input data\n+        byte[] bytes = new byte[data.remaining()];\n+        data.get(bytes);\n+        // Restores the buffer position\n+        // TODO - Is this necessary?\n+        data.position(data.position() - bytes.length);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTY1MTQxNg=="}, "originalCommit": {"oid": "00f9e46044b270aca0d29179e2405a9c312455e6"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mjc4OTcxNQ==", "bodyText": "I actually didn't have to use the suppression once I realized why I needed to use arrayOffset in addition to hasArray. Thanks guys! Let me know if this solution won't do.", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r472789715", "createdAt": "2020-08-19T07:12:06Z", "author": {"login": "kbendick"}, "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "diffHunk": "@@ -231,7 +231,23 @@ public void nonNullWrite(int rowId, String data, ColumnVector output) {\n \n     @Override\n     public void nonNullWrite(int rowId, ByteBuffer data, ColumnVector output) {\n-      ((BytesColumnVector) output).setRef(rowId, data.array(), 0, data.array().length);\n+      // When there is a backing heap based byte array, we avoid the overhead of\n+      // copying it.\n+      if (data.hasArray()) {\n+        // Don't assume the we're reading in the entire backing array.\n+        // Using slice as any mutations to the data at rowId of output\n+        // would also be visible in `data`.\n+        ByteBuffer slice = data.slice();\n+        ((BytesColumnVector) output).setRef(rowId, slice.array(), 0, slice.array().length);\n+      } else {\n+        // Consume the remaining contents of the input data\n+        byte[] bytes = new byte[data.remaining()];\n+        data.get(bytes);\n+        // Restores the buffer position\n+        // TODO - Is this necessary?\n+        data.position(data.position() - bytes.length);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTY1MTQxNg=="}, "originalCommit": {"oid": "00f9e46044b270aca0d29179e2405a9c312455e6"}, "originalPosition": 19}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0OTYzNDY1OnYy", "diffSide": "RIGHT", "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQwMzo0MDo1OFrOHCCNTQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQwNzoxODoxMFrOHC5GoQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTg5NTM3Mw==", "bodyText": "I took the logic for copying from following org.apache.iceberg.io.ParquetValueWriters into org.apache.parquet.io.api.Binary, specifically the need to update both the limit and the position in order to return the original ByteBuffer, data, back to its original position as ByteBuffer offers no API to do that.\nSince I was no longer going to use any optimizations if the buffer is on heap or not (such as the usage of .slice() or .duplicate() or .array() I chose to skip entirely the check for data.hasArray().\nI somewhat copied emulated the logic to copy and reset the original byte buffer data's position (which could be invalidated by moving the position passed the limit), as demonstrated in org.apache.parquet.io.api.Binary", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r471895373", "createdAt": "2020-08-18T03:40:58Z", "author": {"login": "kbendick"}, "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "diffHunk": "@@ -231,7 +231,33 @@ public void nonNullWrite(int rowId, String data, ColumnVector output) {\n \n     @Override\n     public void nonNullWrite(int rowId, ByteBuffer data, ColumnVector output) {\n-      ((BytesColumnVector) output).setRef(rowId, data.array(), 0, data.array().length);\n+      // Don't assume the we're reading in the entire backing array.\n+      //\n+      // We use the same logic for on heap vs off heap buffers as we don't assume\n+      // that the position of the byte buffer received for the write is at the beginning\n+      // position, such an assumption is needed to make use of methods like `.slice()`\n+      // or any methods that would be useful here if we checked if there is an on-heap\n+      // backing array and that the buffer is at the beginning position. (aka we don't check\n+      // that one can use if `data.hasArray()` is true as we couldn't make any optimizations\n+      // in that case).\n+      int position = data.position();\n+      int limit = data.limit();\n+      int curIndex = data.arrayOffset() + data.position();\n+      int endIndex = curIndex + data.remaining();\n+\n+      // Prep for copy into bytes\n+      byte[] bytes = new byte[data.remaining()];\n+      data.limit(curIndex + limit);\n+      data.position(curIndex);\n+\n+      // Perform copy into bytes of remainder of byte buffer.\n+      data.get(bytes, curIndex, endIndex - curIndex);\n+\n+      // Reset the byte buffer.\n+      data.limit(limit);\n+      data.position(position);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9c6c190f1cc07bbc4f498a1d5967f284deaac34f"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mjc5NDc4NQ==", "bodyText": "This wasn't needed and I rolled back", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r472794785", "createdAt": "2020-08-19T07:18:10Z", "author": {"login": "kbendick"}, "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "diffHunk": "@@ -231,7 +231,33 @@ public void nonNullWrite(int rowId, String data, ColumnVector output) {\n \n     @Override\n     public void nonNullWrite(int rowId, ByteBuffer data, ColumnVector output) {\n-      ((BytesColumnVector) output).setRef(rowId, data.array(), 0, data.array().length);\n+      // Don't assume the we're reading in the entire backing array.\n+      //\n+      // We use the same logic for on heap vs off heap buffers as we don't assume\n+      // that the position of the byte buffer received for the write is at the beginning\n+      // position, such an assumption is needed to make use of methods like `.slice()`\n+      // or any methods that would be useful here if we checked if there is an on-heap\n+      // backing array and that the buffer is at the beginning position. (aka we don't check\n+      // that one can use if `data.hasArray()` is true as we couldn't make any optimizations\n+      // in that case).\n+      int position = data.position();\n+      int limit = data.limit();\n+      int curIndex = data.arrayOffset() + data.position();\n+      int endIndex = curIndex + data.remaining();\n+\n+      // Prep for copy into bytes\n+      byte[] bytes = new byte[data.remaining()];\n+      data.limit(curIndex + limit);\n+      data.position(curIndex);\n+\n+      // Perform copy into bytes of remainder of byte buffer.\n+      data.get(bytes, curIndex, endIndex - curIndex);\n+\n+      // Reset the byte buffer.\n+      data.limit(limit);\n+      data.position(position);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTg5NTM3Mw=="}, "originalCommit": {"oid": "9c6c190f1cc07bbc4f498a1d5967f284deaac34f"}, "originalPosition": 29}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk1NzM0OTY1OnYy", "diffSide": "RIGHT", "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxNTozODoyNlrOHDNNGw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOVQwNzo1Nzo1NlrOHJZ32Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzEyNDEyMw==", "bodyText": "The offset should be dupe.arrayOffset() + dupe.position().", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r473124123", "createdAt": "2020-08-19T15:38:26Z", "author": {"login": "rdblue"}, "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "diffHunk": "@@ -231,7 +231,14 @@ public void nonNullWrite(int rowId, String data, ColumnVector output) {\n \n     @Override\n     public void nonNullWrite(int rowId, ByteBuffer data, ColumnVector output) {\n-      ((BytesColumnVector) output).setRef(rowId, data.array(), 0, data.array().length);\n+      ByteBuffer dupe = data.duplicate();\n+      if (dupe.hasArray()) {\n+        ((BytesColumnVector) output).setRef(rowId, dupe.array(), dupe.arrayOffset(), dupe.remaining());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60d3ef03e6905257ede7a7d7caf6d07bce5a3134"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTYyMzEyOQ==", "bodyText": "Ok. I have updated \ud83d\udc4d", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r479623129", "createdAt": "2020-08-29T07:57:56Z", "author": {"login": "kbendick"}, "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "diffHunk": "@@ -231,7 +231,14 @@ public void nonNullWrite(int rowId, String data, ColumnVector output) {\n \n     @Override\n     public void nonNullWrite(int rowId, ByteBuffer data, ColumnVector output) {\n-      ((BytesColumnVector) output).setRef(rowId, data.array(), 0, data.array().length);\n+      ByteBuffer dupe = data.duplicate();\n+      if (dupe.hasArray()) {\n+        ((BytesColumnVector) output).setRef(rowId, dupe.array(), dupe.arrayOffset(), dupe.remaining());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzEyNDEyMw=="}, "originalCommit": {"oid": "60d3ef03e6905257ede7a7d7caf6d07bce5a3134"}, "originalPosition": 7}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk1NzM1MjY5OnYy", "diffSide": "RIGHT", "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxNTozOTowN1rOHDNPBg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxNTozOTowN1rOHDNPBg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzEyNDYxNA==", "bodyText": "There is no need to duplicate the ByteBuffer when accessing the backing on-heap byte array, so this should only be done in the case where we read from the ByteBuffer using get.", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r473124614", "createdAt": "2020-08-19T15:39:07Z", "author": {"login": "rdblue"}, "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "diffHunk": "@@ -231,7 +231,14 @@ public void nonNullWrite(int rowId, String data, ColumnVector output) {\n \n     @Override\n     public void nonNullWrite(int rowId, ByteBuffer data, ColumnVector output) {\n-      ((BytesColumnVector) output).setRef(rowId, data.array(), 0, data.array().length);\n+      ByteBuffer dupe = data.duplicate();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60d3ef03e6905257ede7a7d7caf6d07bce5a3134"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk1NzM2NjQxOnYy", "diffSide": "RIGHT", "path": "pig/src/main/java/org/apache/iceberg/pig/IcebergPigInputFormat.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxNTo0MjoxN1rOHDNXkQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOVQwNzo0Mzo1MVrOHJZzjQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzEyNjgwMQ==", "bodyText": "I don't think that we need to check hasArray here. I think the reason why this didn't previously check hasArray is that the array passed to DataByteArray must start at offset 0 and be valid through the array length, so a copy was needed in almost every case.\nIt may be simpler to change this to use ByteBuffers.toByteArray and pass the result to create DataByteArray.", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r473126801", "createdAt": "2020-08-19T15:42:17Z", "author": {"login": "rdblue"}, "path": "pig/src/main/java/org/apache/iceberg/pig/IcebergPigInputFormat.java", "diffHunk": "@@ -245,8 +245,14 @@ private boolean advance() throws IOException {\n \n     private Object convertPartitionValue(Type type, Object value) {\n       if (type.typeId() == Types.BinaryType.get().typeId()) {\n-        ByteBuffer buffer = (ByteBuffer) value;\n-        return new DataByteArray(buffer.get(new byte[buffer.remaining()]).array());\n+        ByteBuffer dupe = ((ByteBuffer) value).duplicate();\n+        if (dupe.hasArray()) {\n+          return new DataByteArray(dupe.array());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60d3ef03e6905257ede7a7d7caf6d07bce5a3134"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTYyMjAyOQ==", "bodyText": "Let me know if I used this wrong but this is a very useful utility function!", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r479622029", "createdAt": "2020-08-29T07:43:51Z", "author": {"login": "kbendick"}, "path": "pig/src/main/java/org/apache/iceberg/pig/IcebergPigInputFormat.java", "diffHunk": "@@ -245,8 +245,14 @@ private boolean advance() throws IOException {\n \n     private Object convertPartitionValue(Type type, Object value) {\n       if (type.typeId() == Types.BinaryType.get().typeId()) {\n-        ByteBuffer buffer = (ByteBuffer) value;\n-        return new DataByteArray(buffer.get(new byte[buffer.remaining()]).array());\n+        ByteBuffer dupe = ((ByteBuffer) value).duplicate();\n+        if (dupe.hasArray()) {\n+          return new DataByteArray(dupe.array());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzEyNjgwMQ=="}, "originalCommit": {"oid": "60d3ef03e6905257ede7a7d7caf6d07bce5a3134"}, "originalPosition": 8}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk5ODQyMDMyOnYy", "diffSide": "RIGHT", "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOVQwNzo1OToyMVrOHJZ4NA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOVQyMjo1NToxN1rOHJecRw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTYyMzIyMA==", "bodyText": "@rdblue please let me know if this is an appropriate use of ByteBuffers.toByteArray (since we know this is off-heap, I figure this brings it on heap in the way I was somewhat before).\nAnd is the null check necessary? I saw that toByteArray returned null if null was passed in, and I didn't see any @NonNull kind of tags so I wasn't sure.", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r479623220", "createdAt": "2020-08-29T07:59:21Z", "author": {"login": "kbendick"}, "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "diffHunk": "@@ -231,7 +234,13 @@ public void nonNullWrite(int rowId, String data, ColumnVector output) {\n \n     @Override\n     public void nonNullWrite(int rowId, ByteBuffer data, ColumnVector output) {\n-      ((BytesColumnVector) output).setRef(rowId, data.array(), 0, data.array().length);\n+      if (data != null && data.hasArray()) {\n+        ((BytesColumnVector) output).setRef(rowId, data.array(), data.arrayOffset() + data.position(), data.remaining());\n+      } else {\n+        byte[] rawData = ByteBuffers.toByteArray(data);\n+        int length = rawData == null ? 0 : rawData.length;\n+        ((BytesColumnVector) output).setRef(rowId, rawData, 0, length);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fbd6021bdf0a5f71b7756786d993dd2a7d2c29e4"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY3MzY1NA==", "bodyText": "The null check isn't needed because this is nonNullWrite so nulls have already been handled at this point.", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r479673654", "createdAt": "2020-08-29T17:53:09Z", "author": {"login": "rdblue"}, "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "diffHunk": "@@ -231,7 +234,13 @@ public void nonNullWrite(int rowId, String data, ColumnVector output) {\n \n     @Override\n     public void nonNullWrite(int rowId, ByteBuffer data, ColumnVector output) {\n-      ((BytesColumnVector) output).setRef(rowId, data.array(), 0, data.array().length);\n+      if (data != null && data.hasArray()) {\n+        ((BytesColumnVector) output).setRef(rowId, data.array(), data.arrayOffset() + data.position(), data.remaining());\n+      } else {\n+        byte[] rawData = ByteBuffers.toByteArray(data);\n+        int length = rawData == null ? 0 : rawData.length;\n+        ((BytesColumnVector) output).setRef(rowId, rawData, 0, length);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTYyMzIyMA=="}, "originalCommit": {"oid": "fbd6021bdf0a5f71b7756786d993dd2a7d2c29e4"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY5Nzk5MQ==", "bodyText": "Haha didn't realize that. That's what I get for not looking at this PR for over a week \ud83d\ude04", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r479697991", "createdAt": "2020-08-29T22:55:17Z", "author": {"login": "kbendick"}, "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "diffHunk": "@@ -231,7 +234,13 @@ public void nonNullWrite(int rowId, String data, ColumnVector output) {\n \n     @Override\n     public void nonNullWrite(int rowId, ByteBuffer data, ColumnVector output) {\n-      ((BytesColumnVector) output).setRef(rowId, data.array(), 0, data.array().length);\n+      if (data != null && data.hasArray()) {\n+        ((BytesColumnVector) output).setRef(rowId, data.array(), data.arrayOffset() + data.position(), data.remaining());\n+      } else {\n+        byte[] rawData = ByteBuffers.toByteArray(data);\n+        int length = rawData == null ? 0 : rawData.length;\n+        ((BytesColumnVector) output).setRef(rowId, rawData, 0, length);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTYyMzIyMA=="}, "originalCommit": {"oid": "fbd6021bdf0a5f71b7756786d993dd2a7d2c29e4"}, "originalPosition": 27}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk5ODgzOTg3OnYy", "diffSide": "RIGHT", "path": "pig/src/main/java/org/apache/iceberg/pig/IcebergPigInputFormat.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOVQxNzo1MzoyNVrOHJc9Rw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOVQyMzozMzoxNlrOHJemRg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY3MzY3MQ==", "bodyText": "Looks good.", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r479673671", "createdAt": "2020-08-29T17:53:25Z", "author": {"login": "rdblue"}, "path": "pig/src/main/java/org/apache/iceberg/pig/IcebergPigInputFormat.java", "diffHunk": "@@ -245,8 +246,7 @@ private boolean advance() throws IOException {\n \n     private Object convertPartitionValue(Type type, Object value) {\n       if (type.typeId() == Types.BinaryType.get().typeId()) {\n-        ByteBuffer buffer = (ByteBuffer) value;\n-        return new DataByteArray(buffer.get(new byte[buffer.remaining()]).array());\n+        return new DataByteArray(ByteBuffers.toByteArray(((ByteBuffer) value));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fbd6021bdf0a5f71b7756786d993dd2a7d2c29e4"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTcwMDU1MA==", "bodyText": "Fixed a linter error. Wasn't aware we had any breaking linting in the repo. Would love to help enable more of that!", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r479700550", "createdAt": "2020-08-29T23:33:16Z", "author": {"login": "kbendick"}, "path": "pig/src/main/java/org/apache/iceberg/pig/IcebergPigInputFormat.java", "diffHunk": "@@ -245,8 +246,7 @@ private boolean advance() throws IOException {\n \n     private Object convertPartitionValue(Type type, Object value) {\n       if (type.typeId() == Types.BinaryType.get().typeId()) {\n-        ByteBuffer buffer = (ByteBuffer) value;\n-        return new DataByteArray(buffer.get(new byte[buffer.remaining()]).array());\n+        return new DataByteArray(ByteBuffers.toByteArray(((ByteBuffer) value));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY3MzY3MQ=="}, "originalCommit": {"oid": "fbd6021bdf0a5f71b7756786d993dd2a7d2c29e4"}, "originalPosition": 14}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk5ODgzOTkzOnYy", "diffSide": "RIGHT", "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOVQxNzo1MzozNlrOHJc9UA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOVQyMjo1ODozNlrOHJedKw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY3MzY4MA==", "bodyText": "Looks good.", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r479673680", "createdAt": "2020-08-29T17:53:36Z", "author": {"login": "rdblue"}, "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "diffHunk": "@@ -231,7 +234,13 @@ public void nonNullWrite(int rowId, String data, ColumnVector output) {\n \n     @Override\n     public void nonNullWrite(int rowId, ByteBuffer data, ColumnVector output) {\n-      ((BytesColumnVector) output).setRef(rowId, data.array(), 0, data.array().length);\n+      if (data != null && data.hasArray()) {\n+        ((BytesColumnVector) output).setRef(rowId, data.array(), data.arrayOffset() + data.position(), data.remaining());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fbd6021bdf0a5f71b7756786d993dd2a7d2c29e4"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY5ODIxOQ==", "bodyText": "I removed the null check above since as mentioned we're in nonNullWrite \ud83d\ude05", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r479698219", "createdAt": "2020-08-29T22:58:36Z", "author": {"login": "kbendick"}, "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "diffHunk": "@@ -231,7 +234,13 @@ public void nonNullWrite(int rowId, String data, ColumnVector output) {\n \n     @Override\n     public void nonNullWrite(int rowId, ByteBuffer data, ColumnVector output) {\n-      ((BytesColumnVector) output).setRef(rowId, data.array(), 0, data.array().length);\n+      if (data != null && data.hasArray()) {\n+        ((BytesColumnVector) output).setRef(rowId, data.array(), data.arrayOffset() + data.position(), data.remaining());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY3MzY4MA=="}, "originalCommit": {"oid": "fbd6021bdf0a5f71b7756786d993dd2a7d2c29e4"}, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk5OTA2OTUwOnYy", "diffSide": "RIGHT", "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOVQyMzozNDo1OVrOHJemtQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOVQyMzozNDo1OVrOHJemtQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTcwMDY2MQ==", "bodyText": "There was a line length breaking linter error here. Not sure what the expected way to break up the argument list is, so I chose something that seemed to make sense and that passed the linter.", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r479700661", "createdAt": "2020-08-29T23:34:59Z", "author": {"login": "kbendick"}, "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "diffHunk": "@@ -231,7 +232,13 @@ public void nonNullWrite(int rowId, String data, ColumnVector output) {\n \n     @Override\n     public void nonNullWrite(int rowId, ByteBuffer data, ColumnVector output) {\n-      ((BytesColumnVector) output).setRef(rowId, data.array(), 0, data.array().length);\n+      if (data.hasArray()) {\n+        ((BytesColumnVector) output).setRef(rowId, data.array(),\n+                data.arrayOffset() + data.position(), data.remaining());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "737b8afa1fd84d1156c8aedd7f9ae1e41430ff1f"}, "originalPosition": 15}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3640, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}