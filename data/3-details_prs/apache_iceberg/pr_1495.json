{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDkxNzIzOTg4", "number": 1495, "title": "HiveMetaHook implementation to enable CREATE TABLE and DROP TABLE from Hive queries", "bodyText": "This patch enables the following commands from Hive:\n\nTable creation (with HadoopTables LOCATION should be provided too):\n\nCREATE EXTERNAL TABLE customers \nSTORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' \nTBLPROPERTIES ('iceberg.mr.table.schema'='{\"type\":\"struct\",\"fields\":[{\"id\":1,\"name\":\"customer_id\",\"required\":true,\"type\":\"long\"},{\"id\":2,\"name\":\"first_name\",\"required\":true,\"type\":\"string\"}]}');\nor\nCREATE EXTERNAL TABLE customers \nSTORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler'\nLOCATION 's3://some_bucket/some_path/table_a'\nTBLPROPERTIES ('iceberg.mr.table.schema'='{\"type\":\"struct\",\"fields\":[{\"id\":1,\"name\":\"customer_id\",\"required\":true,\"type\":\"long\"},{\"id\":2,\"name\":\"first_name\",\"required\":true,\"type\":\"string\"}]}');\n\nTable drop:\n\nDROP TABLE customers;\nThe backing Iceberg table is created/dropped automatically using the Catalogs new interface methods created by (#1481).\nWith the help of #1407 this patch will enable CREATE/INSERT/DROP path using Hive queries and backed by Iceberg tables.\nThe patch consists of the following main changes:\n\nThe first commit is just the squashed commits of #1481 since they are needed here\nHiveTableOperations / BaseMetastoreCatalog changes to allow initializing already created HMS tables by adding the appropriate Iceberg related data - The exception handling might merit a second look here, opted for the less disrupting change for now\nCatalogs.canWorkWithoutHive method - not too proud about it but needed to throw an exception if the user wants to keep the backing Iceberg table but trying to drop the Hive table without purging. Any better ideas would be welcome \ud83d\ude04\nImplementation of the HiveIcebergMetaHook which does the actual job\nSmall change in the HiveIcebergSerDe to handle Schema definition even if the table is not yet created\nTests for positive and negative cases", "createdAt": "2020-09-23T11:59:23Z", "url": "https://github.com/apache/iceberg/pull/1495", "merged": true, "mergeCommit": {"oid": "80ea771f761e3f5398f40d7bef1e6b8513170564"}, "closed": true, "closedAt": "2020-10-13T16:08:25Z", "author": {"login": "pvary"}, "timelineItems": {"totalCount": 53, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdL2e0IAFqTQ5NTE1MTc5Nw==", "endCursor": "Y3Vyc29yOnYyOpPPAAABdSDVjqABqjM4Njk3MDYzNDA=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk1MTUxNzk3", "url": "https://github.com/apache/iceberg/pull/1495#pullrequestreview-495151797", "createdAt": "2020-09-24T00:58:56Z", "commit": {"oid": "dc7c1c61197c93bb26215fd945f8e7c83749bd29"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwMDo1ODo1NlrOHXGH5A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwMDo1ODo1NlrOHXGH5A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk3OTYyMA==", "bodyText": "Is this telling Iceberg to overwrite the Hive table? I think this property should probably not be persisted to Hive in table properties.", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r493979620", "createdAt": "2020-09-24T00:58:56Z", "author": {"login": "rdblue"}, "path": "hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java", "diffHunk": "@@ -139,6 +140,8 @@ protected void doRefresh() {\n \n   @Override\n   protected void doCommit(TableMetadata base, TableMetadata metadata) {\n+    boolean updateTable = base != null || metadata.propertyAsBoolean(TABLE_FROM_HIVE, false);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc7c1c61197c93bb26215fd945f8e7c83749bd29"}, "originalPosition": 12}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk1MTUyNTc5", "url": "https://github.com/apache/iceberg/pull/1495#pullrequestreview-495152579", "createdAt": "2020-09-24T01:01:39Z", "commit": {"oid": "dc7c1c61197c93bb26215fd945f8e7c83749bd29"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwMTowMTozOVrOHXGKow==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwMTowMTozOVrOHXGKow==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk4MDMyMw==", "bodyText": "It would be helpful to have some context for this. Why did this change? Could you add some comments to make it more clear what's happening in the code?", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r493980323", "createdAt": "2020-09-24T01:01:39Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergSerDe.java", "diffHunk": "@@ -27,21 +27,32 @@\n import org.apache.hadoop.hive.serde2.SerDeStats;\n import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n import org.apache.hadoop.io.Writable;\n-import org.apache.iceberg.Table;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.exceptions.NoSuchTableException;\n import org.apache.iceberg.mr.Catalogs;\n+import org.apache.iceberg.mr.InputFormatConfig;\n import org.apache.iceberg.mr.hive.serde.objectinspector.IcebergObjectInspector;\n import org.apache.iceberg.mr.mapred.Container;\n \n public class HiveIcebergSerDe extends AbstractSerDe {\n-\n+  private Schema schema;\n   private ObjectInspector inspector;\n \n   @Override\n   public void initialize(@Nullable Configuration configuration, Properties serDeProperties) throws SerDeException {\n-    Table table = Catalogs.loadTable(configuration, serDeProperties);\n-\n     try {\n-      this.inspector = IcebergObjectInspector.create(table.schema());\n+      String schemaString = (String) serDeProperties.get(InputFormatConfig.TABLE_SCHEMA);\n+      if (schemaString != null) {\n+        schema = SchemaParser.fromJson(schemaString);\n+      } else {\n+        try {\n+          schema = Catalogs.loadTable(configuration, serDeProperties).schema();\n+        } catch (NoSuchTableException nte) {\n+          throw new SerDeException(\"Please provide an existing table or a valid schema\", nte);\n+        }\n+      }\n+      inspector = IcebergObjectInspector.create(schema);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc7c1c61197c93bb26215fd945f8e7c83749bd29"}, "originalPosition": 34}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk1MTU0Mzgz", "url": "https://github.com/apache/iceberg/pull/1495#pullrequestreview-495154383", "createdAt": "2020-09-24T01:08:16Z", "commit": {"oid": "dc7c1c61197c93bb26215fd945f8e7c83749bd29"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwMTowODoxNlrOHXGQrQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwMTowODoxNlrOHXGQrQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk4MTg2OQ==", "bodyText": "When setting instance fields, we prefix the field with this. to make it obvious it isn't a local variable.", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r493981869", "createdAt": "2020-09-24T01:08:16Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java", "diffHunk": "@@ -0,0 +1,161 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.util.HashSet;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.metastore.HiveMetaHook;\n+import org.apache.hadoop.hive.metastore.api.MetaException;\n+import org.apache.iceberg.BaseMetastoreTableOperations;\n+import org.apache.iceberg.PartitionSpecParser;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.NoSuchTableException;\n+import org.apache.iceberg.hive.HiveTableOperations;\n+import org.apache.iceberg.mr.Catalogs;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class HiveIcebergMetaHook implements HiveMetaHook {\n+  private static final Logger LOG = LoggerFactory.getLogger(HiveIcebergMetaHook.class);\n+  private static final Set<String> PARAMETERS_TO_REMOVE = Stream\n+      .of(InputFormatConfig.TABLE_SCHEMA, InputFormatConfig.PARTITION_SPEC, Catalogs.LOCATION, Catalogs.NAME)\n+      .collect(Collectors.toCollection(HashSet::new));\n+  private static final Set<String> PROPERTIES_TO_REMOVE = Stream\n+      .of(InputFormatConfig.HIVE_DELETE_BACKING_TABLE, \"storage_handler\", \"EXTERNAL\")\n+      .collect(Collectors.toCollection(HashSet::new));\n+\n+  private final Configuration conf;\n+  private Table icebergTable = null;\n+  private Properties catalogProperties;\n+  private boolean deleteIcebergTable;\n+\n+  public HiveIcebergMetaHook(Configuration conf) {\n+    this.conf = conf;\n+  }\n+\n+  @Override\n+  public void preCreateTable(org.apache.hadoop.hive.metastore.api.Table hmsTable) {\n+    catalogProperties = getCatalogProperties(hmsTable);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc7c1c61197c93bb26215fd945f8e7c83749bd29"}, "originalPosition": 63}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk1MTU0NTg5", "url": "https://github.com/apache/iceberg/pull/1495#pullrequestreview-495154589", "createdAt": "2020-09-24T01:08:54Z", "commit": {"oid": "dc7c1c61197c93bb26215fd945f8e7c83749bd29"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwMTowODo1NFrOHXGRRg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwMTowODo1NFrOHXGRRg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk4MjAyMg==", "bodyText": "Isn't this already logged in the catalog implementation?", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r493982022", "createdAt": "2020-09-24T01:08:54Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java", "diffHunk": "@@ -0,0 +1,161 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.util.HashSet;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.metastore.HiveMetaHook;\n+import org.apache.hadoop.hive.metastore.api.MetaException;\n+import org.apache.iceberg.BaseMetastoreTableOperations;\n+import org.apache.iceberg.PartitionSpecParser;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.NoSuchTableException;\n+import org.apache.iceberg.hive.HiveTableOperations;\n+import org.apache.iceberg.mr.Catalogs;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class HiveIcebergMetaHook implements HiveMetaHook {\n+  private static final Logger LOG = LoggerFactory.getLogger(HiveIcebergMetaHook.class);\n+  private static final Set<String> PARAMETERS_TO_REMOVE = Stream\n+      .of(InputFormatConfig.TABLE_SCHEMA, InputFormatConfig.PARTITION_SPEC, Catalogs.LOCATION, Catalogs.NAME)\n+      .collect(Collectors.toCollection(HashSet::new));\n+  private static final Set<String> PROPERTIES_TO_REMOVE = Stream\n+      .of(InputFormatConfig.HIVE_DELETE_BACKING_TABLE, \"storage_handler\", \"EXTERNAL\")\n+      .collect(Collectors.toCollection(HashSet::new));\n+\n+  private final Configuration conf;\n+  private Table icebergTable = null;\n+  private Properties catalogProperties;\n+  private boolean deleteIcebergTable;\n+\n+  public HiveIcebergMetaHook(Configuration conf) {\n+    this.conf = conf;\n+  }\n+\n+  @Override\n+  public void preCreateTable(org.apache.hadoop.hive.metastore.api.Table hmsTable) {\n+    catalogProperties = getCatalogProperties(hmsTable);\n+    try {\n+      icebergTable = Catalogs.loadTable(conf, catalogProperties);\n+\n+      Preconditions.checkArgument(catalogProperties.getProperty(InputFormatConfig.TABLE_SCHEMA) == null,\n+          \"Iceberg table already created - can not use provided schema\");\n+      Preconditions.checkArgument(catalogProperties.getProperty(InputFormatConfig.PARTITION_SPEC) == null,\n+          \"Iceberg table already created - can not use provided partition specification\");\n+\n+      LOG.info(\"Iceberg table already exists {}\", icebergTable);\n+    } catch (NoSuchTableException nte) {\n+      String schemaString = catalogProperties.getProperty(InputFormatConfig.TABLE_SCHEMA);\n+      Preconditions.checkNotNull(schemaString, \"Please provide a table schema\");\n+      // Just check if it is parsable, and later use for partition specification parsing\n+      Schema schema = SchemaParser.fromJson(schemaString);\n+\n+      String specString = catalogProperties.getProperty(InputFormatConfig.PARTITION_SPEC);\n+      if (specString != null) {\n+        // Just check if it is parsable\n+        PartitionSpecParser.fromJson(schema, schemaString);\n+      }\n+\n+      // Allow purging table data if the table is created now and not set otherwise\n+      if (hmsTable.getParameters().get(InputFormatConfig.HIVE_DELETE_BACKING_TABLE) == null) {\n+        hmsTable.getParameters().put(InputFormatConfig.HIVE_DELETE_BACKING_TABLE, \"TRUE\");\n+      }\n+\n+      // Set the table type even for non HiveCatalog based tables\n+      hmsTable.getParameters().put(BaseMetastoreTableOperations.TABLE_TYPE_PROP,\n+          BaseMetastoreTableOperations.ICEBERG_TABLE_TYPE_VALUE.toUpperCase());\n+\n+      // Remove creation related properties\n+      PARAMETERS_TO_REMOVE.forEach(hmsTable.getParameters()::remove);\n+    }\n+  }\n+\n+  @Override\n+  public void rollbackCreateTable(org.apache.hadoop.hive.metastore.api.Table hmsTable) {\n+    // do nothing\n+  }\n+\n+  @Override\n+  public void commitCreateTable(org.apache.hadoop.hive.metastore.api.Table hmsTable) {\n+    if (icebergTable == null) {\n+      catalogProperties.put(HiveTableOperations.TABLE_FROM_HIVE, true);\n+      LOG.info(\"Iceberg table creation with the following properties {}\", catalogProperties.keySet());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc7c1c61197c93bb26215fd945f8e7c83749bd29"}, "originalPosition": 108}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk1MTU0Nzk1", "url": "https://github.com/apache/iceberg/pull/1495#pullrequestreview-495154795", "createdAt": "2020-09-24T01:09:37Z", "commit": {"oid": "dc7c1c61197c93bb26215fd945f8e7c83749bd29"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwMTowOTozOFrOHXGR9g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwMTowOTozOFrOHXGR9g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk4MjE5OA==", "bodyText": "Can you explain this option?", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r493982198", "createdAt": "2020-09-24T01:09:38Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java", "diffHunk": "@@ -0,0 +1,161 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.util.HashSet;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.metastore.HiveMetaHook;\n+import org.apache.hadoop.hive.metastore.api.MetaException;\n+import org.apache.iceberg.BaseMetastoreTableOperations;\n+import org.apache.iceberg.PartitionSpecParser;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.NoSuchTableException;\n+import org.apache.iceberg.hive.HiveTableOperations;\n+import org.apache.iceberg.mr.Catalogs;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class HiveIcebergMetaHook implements HiveMetaHook {\n+  private static final Logger LOG = LoggerFactory.getLogger(HiveIcebergMetaHook.class);\n+  private static final Set<String> PARAMETERS_TO_REMOVE = Stream\n+      .of(InputFormatConfig.TABLE_SCHEMA, InputFormatConfig.PARTITION_SPEC, Catalogs.LOCATION, Catalogs.NAME)\n+      .collect(Collectors.toCollection(HashSet::new));\n+  private static final Set<String> PROPERTIES_TO_REMOVE = Stream\n+      .of(InputFormatConfig.HIVE_DELETE_BACKING_TABLE, \"storage_handler\", \"EXTERNAL\")\n+      .collect(Collectors.toCollection(HashSet::new));\n+\n+  private final Configuration conf;\n+  private Table icebergTable = null;\n+  private Properties catalogProperties;\n+  private boolean deleteIcebergTable;\n+\n+  public HiveIcebergMetaHook(Configuration conf) {\n+    this.conf = conf;\n+  }\n+\n+  @Override\n+  public void preCreateTable(org.apache.hadoop.hive.metastore.api.Table hmsTable) {\n+    catalogProperties = getCatalogProperties(hmsTable);\n+    try {\n+      icebergTable = Catalogs.loadTable(conf, catalogProperties);\n+\n+      Preconditions.checkArgument(catalogProperties.getProperty(InputFormatConfig.TABLE_SCHEMA) == null,\n+          \"Iceberg table already created - can not use provided schema\");\n+      Preconditions.checkArgument(catalogProperties.getProperty(InputFormatConfig.PARTITION_SPEC) == null,\n+          \"Iceberg table already created - can not use provided partition specification\");\n+\n+      LOG.info(\"Iceberg table already exists {}\", icebergTable);\n+    } catch (NoSuchTableException nte) {\n+      String schemaString = catalogProperties.getProperty(InputFormatConfig.TABLE_SCHEMA);\n+      Preconditions.checkNotNull(schemaString, \"Please provide a table schema\");\n+      // Just check if it is parsable, and later use for partition specification parsing\n+      Schema schema = SchemaParser.fromJson(schemaString);\n+\n+      String specString = catalogProperties.getProperty(InputFormatConfig.PARTITION_SPEC);\n+      if (specString != null) {\n+        // Just check if it is parsable\n+        PartitionSpecParser.fromJson(schema, schemaString);\n+      }\n+\n+      // Allow purging table data if the table is created now and not set otherwise\n+      if (hmsTable.getParameters().get(InputFormatConfig.HIVE_DELETE_BACKING_TABLE) == null) {\n+        hmsTable.getParameters().put(InputFormatConfig.HIVE_DELETE_BACKING_TABLE, \"TRUE\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc7c1c61197c93bb26215fd945f8e7c83749bd29"}, "originalPosition": 87}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk1MTU1MTQw", "url": "https://github.com/apache/iceberg/pull/1495#pullrequestreview-495155140", "createdAt": "2020-09-24T01:10:51Z", "commit": {"oid": "dc7c1c61197c93bb26215fd945f8e7c83749bd29"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwMToxMDo1MVrOHXGTLQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwMToxMDo1MVrOHXGTLQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk4MjUwOQ==", "bodyText": "I think what is happening here should be documented clearly in the code.", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r493982509", "createdAt": "2020-09-24T01:10:51Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java", "diffHunk": "@@ -0,0 +1,161 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.util.HashSet;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.metastore.HiveMetaHook;\n+import org.apache.hadoop.hive.metastore.api.MetaException;\n+import org.apache.iceberg.BaseMetastoreTableOperations;\n+import org.apache.iceberg.PartitionSpecParser;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.NoSuchTableException;\n+import org.apache.iceberg.hive.HiveTableOperations;\n+import org.apache.iceberg.mr.Catalogs;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class HiveIcebergMetaHook implements HiveMetaHook {\n+  private static final Logger LOG = LoggerFactory.getLogger(HiveIcebergMetaHook.class);\n+  private static final Set<String> PARAMETERS_TO_REMOVE = Stream\n+      .of(InputFormatConfig.TABLE_SCHEMA, InputFormatConfig.PARTITION_SPEC, Catalogs.LOCATION, Catalogs.NAME)\n+      .collect(Collectors.toCollection(HashSet::new));\n+  private static final Set<String> PROPERTIES_TO_REMOVE = Stream\n+      .of(InputFormatConfig.HIVE_DELETE_BACKING_TABLE, \"storage_handler\", \"EXTERNAL\")\n+      .collect(Collectors.toCollection(HashSet::new));\n+\n+  private final Configuration conf;\n+  private Table icebergTable = null;\n+  private Properties catalogProperties;\n+  private boolean deleteIcebergTable;\n+\n+  public HiveIcebergMetaHook(Configuration conf) {\n+    this.conf = conf;\n+  }\n+\n+  @Override\n+  public void preCreateTable(org.apache.hadoop.hive.metastore.api.Table hmsTable) {\n+    catalogProperties = getCatalogProperties(hmsTable);\n+    try {\n+      icebergTable = Catalogs.loadTable(conf, catalogProperties);\n+\n+      Preconditions.checkArgument(catalogProperties.getProperty(InputFormatConfig.TABLE_SCHEMA) == null,\n+          \"Iceberg table already created - can not use provided schema\");\n+      Preconditions.checkArgument(catalogProperties.getProperty(InputFormatConfig.PARTITION_SPEC) == null,\n+          \"Iceberg table already created - can not use provided partition specification\");\n+\n+      LOG.info(\"Iceberg table already exists {}\", icebergTable);\n+    } catch (NoSuchTableException nte) {\n+      String schemaString = catalogProperties.getProperty(InputFormatConfig.TABLE_SCHEMA);\n+      Preconditions.checkNotNull(schemaString, \"Please provide a table schema\");\n+      // Just check if it is parsable, and later use for partition specification parsing\n+      Schema schema = SchemaParser.fromJson(schemaString);\n+\n+      String specString = catalogProperties.getProperty(InputFormatConfig.PARTITION_SPEC);\n+      if (specString != null) {\n+        // Just check if it is parsable\n+        PartitionSpecParser.fromJson(schema, schemaString);\n+      }\n+\n+      // Allow purging table data if the table is created now and not set otherwise\n+      if (hmsTable.getParameters().get(InputFormatConfig.HIVE_DELETE_BACKING_TABLE) == null) {\n+        hmsTable.getParameters().put(InputFormatConfig.HIVE_DELETE_BACKING_TABLE, \"TRUE\");\n+      }\n+\n+      // Set the table type even for non HiveCatalog based tables\n+      hmsTable.getParameters().put(BaseMetastoreTableOperations.TABLE_TYPE_PROP,\n+          BaseMetastoreTableOperations.ICEBERG_TABLE_TYPE_VALUE.toUpperCase());\n+\n+      // Remove creation related properties\n+      PARAMETERS_TO_REMOVE.forEach(hmsTable.getParameters()::remove);\n+    }\n+  }\n+\n+  @Override\n+  public void rollbackCreateTable(org.apache.hadoop.hive.metastore.api.Table hmsTable) {\n+    // do nothing\n+  }\n+\n+  @Override\n+  public void commitCreateTable(org.apache.hadoop.hive.metastore.api.Table hmsTable) {\n+    if (icebergTable == null) {\n+      catalogProperties.put(HiveTableOperations.TABLE_FROM_HIVE, true);\n+      LOG.info(\"Iceberg table creation with the following properties {}\", catalogProperties.keySet());\n+      Catalogs.createTable(conf, catalogProperties);\n+    }\n+  }\n+\n+  @Override\n+  public void preDropTable(org.apache.hadoop.hive.metastore.api.Table hmsTable) throws MetaException {\n+    catalogProperties = getCatalogProperties(hmsTable);\n+    deleteIcebergTable = hmsTable.getParameters() != null &&\n+        \"TRUE\".equalsIgnoreCase(hmsTable.getParameters().get(InputFormatConfig.HIVE_DELETE_BACKING_TABLE));\n+\n+    if (!deleteIcebergTable) {\n+      if (!Catalogs.canWorkWithoutHive(conf)) {\n+        // This should happen only if someone were manually removing this property from the table, or\n+        // added the table from outside of Hive\n+        throw new MetaException(\"Can not drop Hive table and keep Iceberg table data when using HiveCatalog. \" +\n+            \"Please add \" + InputFormatConfig.HIVE_DELETE_BACKING_TABLE + \"='TRUE' to TBLPROPERTIES \" +\n+            \"of the Hive table to enable dropping\");\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public void rollbackDropTable(org.apache.hadoop.hive.metastore.api.Table hmsTable) {\n+    // do nothing\n+  }\n+\n+  @Override\n+  public void commitDropTable(org.apache.hadoop.hive.metastore.api.Table hmsTable, boolean deleteData) {\n+    if (deleteData && deleteIcebergTable) {\n+      LOG.info(\"Dropping with purge all the data for table {}.{}\", hmsTable.getDbName(), hmsTable.getTableName());\n+      Catalogs.dropTable(conf, catalogProperties);\n+    }\n+  }\n+\n+  private Properties getCatalogProperties(org.apache.hadoop.hive.metastore.api.Table hmsTable) {\n+    Properties properties = new Properties();\n+    properties.putAll(hmsTable.getParameters());\n+\n+    if (properties.get(Catalogs.LOCATION) == null &&\n+        hmsTable.getSd() != null && hmsTable.getSd().getLocation() != null) {\n+      properties.put(Catalogs.LOCATION, hmsTable.getSd().getLocation());\n+    }\n+\n+    if (properties.get(Catalogs.NAME) == null) {\n+      properties.put(Catalogs.NAME, hmsTable.getDbName() + \".\" + hmsTable.getTableName());\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc7c1c61197c93bb26215fd945f8e7c83749bd29"}, "originalPosition": 154}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "dc7c1c61197c93bb26215fd945f8e7c83749bd29", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/dc7c1c61197c93bb26215fd945f8e7c83749bd29", "committedDate": "2020-09-23T11:37:32Z", "message": "HiveMetaHook implementation to enable CREATE TABLE and DROP TABLE from Hive queries"}, "afterCommit": {"oid": "bc13a60e8987e53828b1f5c6cd3dd1f2654c1e57", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/bc13a60e8987e53828b1f5c6cd3dd1f2654c1e57", "committedDate": "2020-09-24T12:07:38Z", "message": "Addressed review comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk3MDYyNjY0", "url": "https://github.com/apache/iceberg/pull/1495#pullrequestreview-497062664", "createdAt": "2020-09-27T08:26:01Z", "commit": {"oid": "bc13a60e8987e53828b1f5c6cd3dd1f2654c1e57"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yN1QwODoyNjowMVrOHYltdw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yN1QwODoyNjowMVrOHYltdw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTU0NTcxOQ==", "bodyText": "PartitionSpecParser.fromJson(schema, schemaString)\n\nshould be replaced with\n\nPartitionSpecParser.fromJson(schema, specString)\n\nRight?", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r495545719", "createdAt": "2020-09-27T08:26:01Z", "author": {"login": "qphien"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java", "diffHunk": "@@ -0,0 +1,169 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.util.Properties;\n+import java.util.Set;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.metastore.HiveMetaHook;\n+import org.apache.hadoop.hive.metastore.api.MetaException;\n+import org.apache.hadoop.hive.metastore.api.hive_metastoreConstants;\n+import org.apache.iceberg.BaseMetastoreTableOperations;\n+import org.apache.iceberg.PartitionSpecParser;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.exceptions.NoSuchTableException;\n+import org.apache.iceberg.hive.HiveTableOperations;\n+import org.apache.iceberg.mr.Catalogs;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class HiveIcebergMetaHook implements HiveMetaHook {\n+  private static final Logger LOG = LoggerFactory.getLogger(HiveIcebergMetaHook.class);\n+  private static final Set<String> PARAMETERS_TO_REMOVE = ImmutableSet\n+      .of(InputFormatConfig.TABLE_SCHEMA, InputFormatConfig.PARTITION_SPEC, Catalogs.LOCATION, Catalogs.NAME);\n+  private static final Set<String> PROPERTIES_TO_REMOVE = ImmutableSet\n+      .of(InputFormatConfig.HIVE_DELETE_BACKING_TABLE, hive_metastoreConstants.META_TABLE_STORAGE, \"EXTERNAL\");\n+\n+  private final Configuration conf;\n+  private Table icebergTable = null;\n+  private Properties catalogProperties;\n+  private boolean deleteIcebergTable;\n+\n+  public HiveIcebergMetaHook(Configuration conf) {\n+    this.conf = conf;\n+  }\n+\n+  @Override\n+  public void preCreateTable(org.apache.hadoop.hive.metastore.api.Table hmsTable) {\n+    this.catalogProperties = getCatalogProperties(hmsTable);\n+    try {\n+      this.icebergTable = Catalogs.loadTable(conf, catalogProperties);\n+\n+      Preconditions.checkArgument(catalogProperties.getProperty(InputFormatConfig.TABLE_SCHEMA) == null,\n+          \"Iceberg table already created - can not use provided schema\");\n+      Preconditions.checkArgument(catalogProperties.getProperty(InputFormatConfig.PARTITION_SPEC) == null,\n+          \"Iceberg table already created - can not use provided partition specification\");\n+\n+      LOG.info(\"Iceberg table already exists {}\", icebergTable);\n+    } catch (NoSuchTableException nte) {\n+      String schemaString = catalogProperties.getProperty(InputFormatConfig.TABLE_SCHEMA);\n+      Preconditions.checkNotNull(schemaString, \"Please provide a table schema\");\n+      // Just check if it is parsable, and later use for partition specification parsing\n+      Schema schema = SchemaParser.fromJson(schemaString);\n+\n+      String specString = catalogProperties.getProperty(InputFormatConfig.PARTITION_SPEC);\n+      if (specString != null) {\n+        // Just check if it is parsable\n+        PartitionSpecParser.fromJson(schema, schemaString);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bc13a60e8987e53828b1f5c6cd3dd1f2654c1e57"}, "originalPosition": 80}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "bc13a60e8987e53828b1f5c6cd3dd1f2654c1e57", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/bc13a60e8987e53828b1f5c6cd3dd1f2654c1e57", "committedDate": "2020-09-24T12:07:38Z", "message": "Addressed review comments"}, "afterCommit": {"oid": "794b8390c3d9adbe1d3a8358a57b84e22fd3c0c7", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/794b8390c3d9adbe1d3a8358a57b84e22fd3c0c7", "committedDate": "2020-09-28T18:31:09Z", "message": "Fix for partitionspec handling and tests for it"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk4NzEzNjk0", "url": "https://github.com/apache/iceberg/pull/1495#pullrequestreview-498713694", "createdAt": "2020-09-29T17:21:02Z", "commit": {"oid": "794b8390c3d9adbe1d3a8358a57b84e22fd3c0c7"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQxNzoyMTowMlrOHZ4_Pg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQxNzoyMTowMlrOHZ4_Pg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjkxMDE0Mg==", "bodyText": "Is this correct?\nIf this happens, then the table exists and is not an Iceberg table. That is expected if we are creating the table from the Hive hook, but in that case I think that we will have injected metadata in the hook that signals to the TableOperations implementation that the table was pre-created by Hive and should be replaced (so update will be used for the initial commit). If we detect that in TableOperations then we should be able to avoid throwing this exception and return null like before.\nIf the operation is not happening inside of the Hive hooks, then we do want to let the exception propagate because there is an existing Hive table.", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r496910142", "createdAt": "2020-09-29T17:21:02Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/BaseMetastoreCatalog.java", "diffHunk": "@@ -215,8 +216,15 @@ public TableBuilder withProperty(String key, String value) {\n     @Override\n     public Table create() {\n       TableOperations ops = newTableOps(identifier);\n-      if (ops.current() != null) {\n-        throw new AlreadyExistsException(\"Table already exists: %s\", identifier);\n+      try {\n+        if (ops.current() != null) {\n+          throw new AlreadyExistsException(\"Table already exists: %s\", identifier);\n+        }\n+      } catch (NoSuchIcebergTableException ne) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "794b8390c3d9adbe1d3a8358a57b84e22fd3c0c7"}, "originalPosition": 18}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk4NzI2MTQz", "url": "https://github.com/apache/iceberg/pull/1495#pullrequestreview-498726143", "createdAt": "2020-09-29T17:36:48Z", "commit": {"oid": "794b8390c3d9adbe1d3a8358a57b84e22fd3c0c7"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQxNzozNjo0OVrOHZ5mKQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQxNzozNjo0OVrOHZ5mKQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjkyMDEwNQ==", "bodyText": "Are we sure that it is okay for this class to have state? What is the lifecycle of hooks in Hive? I would assume that there is one instance of this class that is used for all operations. But the instance fields that hold a table that is being created seems to assume that there is one instance of the class per operation.", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r496920105", "createdAt": "2020-09-29T17:36:49Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java", "diffHunk": "@@ -0,0 +1,169 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.util.Properties;\n+import java.util.Set;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.metastore.HiveMetaHook;\n+import org.apache.hadoop.hive.metastore.api.MetaException;\n+import org.apache.hadoop.hive.metastore.api.hive_metastoreConstants;\n+import org.apache.iceberg.BaseMetastoreTableOperations;\n+import org.apache.iceberg.PartitionSpecParser;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.exceptions.NoSuchTableException;\n+import org.apache.iceberg.hive.HiveTableOperations;\n+import org.apache.iceberg.mr.Catalogs;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class HiveIcebergMetaHook implements HiveMetaHook {\n+  private static final Logger LOG = LoggerFactory.getLogger(HiveIcebergMetaHook.class);\n+  private static final Set<String> PARAMETERS_TO_REMOVE = ImmutableSet\n+      .of(InputFormatConfig.TABLE_SCHEMA, InputFormatConfig.PARTITION_SPEC, Catalogs.LOCATION, Catalogs.NAME);\n+  private static final Set<String> PROPERTIES_TO_REMOVE = ImmutableSet\n+      .of(InputFormatConfig.HIVE_DELETE_BACKING_TABLE, hive_metastoreConstants.META_TABLE_STORAGE, \"EXTERNAL\");\n+\n+  private final Configuration conf;\n+  private Table icebergTable = null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "794b8390c3d9adbe1d3a8358a57b84e22fd3c0c7"}, "originalPosition": 51}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk4ODc3MzY4", "url": "https://github.com/apache/iceberg/pull/1495#pullrequestreview-498877368", "createdAt": "2020-09-29T20:43:24Z", "commit": {"oid": "2c18a3d580ac5b2f9d6bc4a01ca18a64d9f9062b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQyMDo0MzoyNFrOHaAu2g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQyMDo0MzoyNFrOHaAu2g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzAzNzAxOA==", "bodyText": "Can we remove this? If the debug log shows that this path was taken, then it is redundant.", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r497037018", "createdAt": "2020-09-29T20:43:24Z", "author": {"login": "rdblue"}, "path": "hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java", "diffHunk": "@@ -142,16 +146,22 @@ protected void doCommit(TableMetadata base, TableMetadata metadata) {\n     String newMetadataLocation = writeNewMetadata(metadata, currentVersion() + 1);\n \n     boolean threw = true;\n+    boolean updateHiveTable = false;\n     Optional<Long> lockId = Optional.empty();\n     try {\n       lockId = Optional.of(acquireLock());\n       // TODO add lock heart beating for cases where default lock timeout is too low.\n       Table tbl;\n-      if (base != null) {\n-        LOG.debug(\"Committing existing table: {}\", fullName);\n+      try {\n         tbl = metaClients.run(client -> client.getTable(database, tableName));\n+        if (base == null && tbl.getParameters().get(TABLE_CREATION_FROM_HIVE) == null) {\n+          throw new AlreadyExistsException(\"Table already exists: %s.%s\", database, tableName);\n+        }\n+        updateHiveTable = true;\n+        LOG.debug(\"Committing existing table: {}\", fullName);\n         tbl.setSd(storageDescriptor(metadata)); // set to pickup any schema changes\n-      } else {\n+      } catch (NoSuchObjectException nte) {\n+        LOG.trace(\"Table not found {}\", fullName, nte);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2c18a3d580ac5b2f9d6bc4a01ca18a64d9f9062b"}, "originalPosition": 47}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk4ODc4MjI0", "url": "https://github.com/apache/iceberg/pull/1495#pullrequestreview-498878224", "createdAt": "2020-09-29T20:44:40Z", "commit": {"oid": "2c18a3d580ac5b2f9d6bc4a01ca18a64d9f9062b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQyMDo0NDo0MFrOHaAz6A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQyMDo0NDo0MFrOHaAz6A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzAzODMxMg==", "bodyText": "Why rename the variable here?", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r497038312", "createdAt": "2020-09-29T20:44:40Z", "author": {"login": "rdblue"}, "path": "hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java", "diffHunk": "@@ -179,17 +189,18 @@ protected void doCommit(TableMetadata base, TableMetadata metadata) {\n \n       setParameters(newMetadataLocation, tbl);\n \n-      if (base != null) {\n+      Table newTable = tbl;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2c18a3d580ac5b2f9d6bc4a01ca18a64d9f9062b"}, "originalPosition": 56}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk4ODg2ODkx", "url": "https://github.com/apache/iceberg/pull/1495#pullrequestreview-498886891", "createdAt": "2020-09-29T20:57:19Z", "commit": {"oid": "2c18a3d580ac5b2f9d6bc4a01ca18a64d9f9062b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQyMDo1NzoxOVrOHaBmog==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQyMDo1NzoxOVrOHaBmog==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzA1MTI5OA==", "bodyText": "Is this needed? TABLE_TYPE_PROP is set in the pre-commit hook. In fact, can we just use that instead of TABLE_CREATION_FROM_HIVE?", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r497051298", "createdAt": "2020-09-29T20:57:19Z", "author": {"login": "rdblue"}, "path": "hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java", "diffHunk": "@@ -313,6 +327,11 @@ protected void doUnlock(long lockId) throws TException, InterruptedException {\n   }\n \n   static void validateTableIsIceberg(Table table, String fullName) {\n+    if (\"TRUE\".equalsIgnoreCase(table.getParameters().get(TABLE_CREATION_FROM_HIVE))) {\n+      // No check is needed. The table has been created from Hive (HiveIcebergMetaHook), and we are about to initialize\n+      // the Iceberg metadata\n+      return;\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2c18a3d580ac5b2f9d6bc4a01ca18a64d9f9062b"}, "originalPosition": 91}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk5Nzk3ODg1", "url": "https://github.com/apache/iceberg/pull/1495#pullrequestreview-499797885", "createdAt": "2020-09-30T20:45:43Z", "commit": {"oid": "10127f34eaf2780ad44a385a045be8a656fc1421"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQyMDo0NTo0NFrOHauqdQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQyMDo0NTo0NFrOHauqdQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzc4OTU1Nw==", "bodyText": "SchemaParser has a cache, so that should help some.", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r497789557", "createdAt": "2020-09-30T20:45:44Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergSerDe.java", "diffHunk": "@@ -27,21 +27,41 @@\n import org.apache.hadoop.hive.serde2.SerDeStats;\n import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n import org.apache.hadoop.io.Writable;\n-import org.apache.iceberg.Table;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.exceptions.NoSuchTableException;\n import org.apache.iceberg.mr.Catalogs;\n+import org.apache.iceberg.mr.InputFormatConfig;\n import org.apache.iceberg.mr.hive.serde.objectinspector.IcebergObjectInspector;\n import org.apache.iceberg.mr.mapred.Container;\n \n public class HiveIcebergSerDe extends AbstractSerDe {\n-\n+  private Schema schema;\n   private ObjectInspector inspector;\n \n   @Override\n   public void initialize(@Nullable Configuration configuration, Properties serDeProperties) throws SerDeException {\n-    Table table = Catalogs.loadTable(configuration, serDeProperties);\n-\n     try {\n-      this.inspector = IcebergObjectInspector.create(table.schema());\n+      // HiveIcebergSerDe.initialize is called multiple places in Hive code:\n+      // - When we are trying to create a table - HiveDDL data is stored at the serDeProperties, but no Iceberg table\n+      // is created yet.\n+      // - When we are compiling the Hive query on HiveServer2 side - We only have table information (location/name),\n+      // and we have to read the schema using the table data. This is called multiple times so there is room for\n+      // optimizing here.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "10127f34eaf2780ad44a385a045be8a656fc1421"}, "originalPosition": 29}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk5ODAzMjkz", "url": "https://github.com/apache/iceberg/pull/1495#pullrequestreview-499803293", "createdAt": "2020-09-30T20:53:35Z", "commit": {"oid": "10127f34eaf2780ad44a385a045be8a656fc1421"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQyMDo1MzozNVrOHau7Og==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQyMDo1MzozNVrOHau7Og==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzc5Mzg1MA==", "bodyText": "Just thinking about how this interacts with #1505: This PR relies on setting the storage handler here and it is stored in parameters. Right now, nothing will remove it, but I think the right thing to do in #1505 is to remove this if Hive is not enabled for the table.\nIf we remove the property when Hive isn't enabled, then this test would break. To fix it, I think we should always set the engine.hive.enabled property to true in the hook. That could be done either in this PR or the other one if this one is merged first.", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r497793850", "createdAt": "2020-09-30T20:53:35Z", "author": {"login": "rdblue"}, "path": "mr/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandlerBaseTest.java", "diffHunk": "@@ -157,6 +177,282 @@ public void testJoinTables() throws IOException {\n     Assert.assertArrayEquals(new Object[] {1L, \"Bob\", 102L, 33.33d}, rows.get(2));\n   }\n \n+  @Test\n+  public void testCreateDropTable() throws TException, IOException {\n+    // We need the location for HadoopTable based tests only\n+    String location = locationForCreateTable(temp.getRoot().getPath(), \"customers\");\n+    shell.executeStatement(\"CREATE EXTERNAL TABLE customers \" +\n+        \"STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' \" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "10127f34eaf2780ad44a385a045be8a656fc1421"}, "originalPosition": 56}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk5ODA0MTI1", "url": "https://github.com/apache/iceberg/pull/1495#pullrequestreview-499804125", "createdAt": "2020-09-30T20:54:48Z", "commit": {"oid": "10127f34eaf2780ad44a385a045be8a656fc1421"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQyMDo1NDo0OFrOHau9rw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQyMDo1NDo0OFrOHau9rw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzc5NDQ3OQ==", "bodyText": "You should be able to use PartitionSpec in the assertEquals because its equals method is implemented.", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r497794479", "createdAt": "2020-09-30T20:54:48Z", "author": {"login": "rdblue"}, "path": "mr/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandlerBaseTest.java", "diffHunk": "@@ -157,6 +177,282 @@ public void testJoinTables() throws IOException {\n     Assert.assertArrayEquals(new Object[] {1L, \"Bob\", 102L, 33.33d}, rows.get(2));\n   }\n \n+  @Test\n+  public void testCreateDropTable() throws TException, IOException {\n+    // We need the location for HadoopTable based tests only\n+    String location = locationForCreateTable(temp.getRoot().getPath(), \"customers\");\n+    shell.executeStatement(\"CREATE EXTERNAL TABLE customers \" +\n+        \"STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' \" +\n+        (location != null ? \"LOCATION '\" + location + \"' \" : \"\") +\n+        \"TBLPROPERTIES ('\" + InputFormatConfig.TABLE_SCHEMA + \"'='\" + SchemaParser.toJson(CUSTOMER_SCHEMA) + \"', \" +\n+        \"'\" + InputFormatConfig.PARTITION_SPEC + \"'='\" + PartitionSpecParser.toJson(IDENTITY_SPEC) + \"', \" +\n+        \"'dummy'='test')\");\n+\n+    Properties properties = new Properties();\n+    properties.put(Catalogs.NAME, TableIdentifier.of(\"default\", \"customers\").toString());\n+    if (location != null) {\n+      properties.put(Catalogs.LOCATION, location);\n+    }\n+\n+    // Check the Iceberg table data\n+    org.apache.iceberg.Table icebergTable = Catalogs.loadTable(shell.getHiveConf(), properties);\n+    Assert.assertEquals(SchemaParser.toJson(CUSTOMER_SCHEMA), SchemaParser.toJson(icebergTable.schema()));\n+    Assert.assertEquals(PartitionSpecParser.toJson(IDENTITY_SPEC), PartitionSpecParser.toJson(icebergTable.spec()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "10127f34eaf2780ad44a385a045be8a656fc1421"}, "originalPosition": 71}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk5ODA0NzI0", "url": "https://github.com/apache/iceberg/pull/1495#pullrequestreview-499804724", "createdAt": "2020-09-30T20:55:39Z", "commit": {"oid": "10127f34eaf2780ad44a385a045be8a656fc1421"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQyMDo1NTozOVrOHau_fw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQyMDo1NTozOVrOHau_fw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzc5NDk0Mw==", "bodyText": "Instead of comparing the strings, it would be better to compare icebergTable.schema().asStruct(). StructType implements equals, but Schema doesn't because it may have extra metadata (like aliases).", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r497794943", "createdAt": "2020-09-30T20:55:39Z", "author": {"login": "rdblue"}, "path": "mr/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandlerBaseTest.java", "diffHunk": "@@ -157,6 +177,282 @@ public void testJoinTables() throws IOException {\n     Assert.assertArrayEquals(new Object[] {1L, \"Bob\", 102L, 33.33d}, rows.get(2));\n   }\n \n+  @Test\n+  public void testCreateDropTable() throws TException, IOException {\n+    // We need the location for HadoopTable based tests only\n+    String location = locationForCreateTable(temp.getRoot().getPath(), \"customers\");\n+    shell.executeStatement(\"CREATE EXTERNAL TABLE customers \" +\n+        \"STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' \" +\n+        (location != null ? \"LOCATION '\" + location + \"' \" : \"\") +\n+        \"TBLPROPERTIES ('\" + InputFormatConfig.TABLE_SCHEMA + \"'='\" + SchemaParser.toJson(CUSTOMER_SCHEMA) + \"', \" +\n+        \"'\" + InputFormatConfig.PARTITION_SPEC + \"'='\" + PartitionSpecParser.toJson(IDENTITY_SPEC) + \"', \" +\n+        \"'dummy'='test')\");\n+\n+    Properties properties = new Properties();\n+    properties.put(Catalogs.NAME, TableIdentifier.of(\"default\", \"customers\").toString());\n+    if (location != null) {\n+      properties.put(Catalogs.LOCATION, location);\n+    }\n+\n+    // Check the Iceberg table data\n+    org.apache.iceberg.Table icebergTable = Catalogs.loadTable(shell.getHiveConf(), properties);\n+    Assert.assertEquals(SchemaParser.toJson(CUSTOMER_SCHEMA), SchemaParser.toJson(icebergTable.schema()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "10127f34eaf2780ad44a385a045be8a656fc1421"}, "originalPosition": 70}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk5ODU5OTA0", "url": "https://github.com/apache/iceberg/pull/1495#pullrequestreview-499859904", "createdAt": "2020-09-30T22:37:55Z", "commit": {"oid": "10127f34eaf2780ad44a385a045be8a656fc1421"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQyMjozNzo1NVrOHaxt_A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQyMjozNzo1NVrOHaxt_A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzgzOTYxMg==", "bodyText": "We might want to use a shared HiveClientPool for this. We've had problems in the past where too many clients led to the HMS becoming unresponsive in tests. It's really annoying and makes tests flaky. Sharing a pool across all tests fixes the problem, and makes us more confident that if we hit a connection issue, it is probably in prod code and not test.\nI think it would also make the test cases smaller because you wouldn't need try/finally blocks.", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r497839612", "createdAt": "2020-09-30T22:37:55Z", "author": {"login": "rdblue"}, "path": "mr/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandlerBaseTest.java", "diffHunk": "@@ -157,6 +177,282 @@ public void testJoinTables() throws IOException {\n     Assert.assertArrayEquals(new Object[] {1L, \"Bob\", 102L, 33.33d}, rows.get(2));\n   }\n \n+  @Test\n+  public void testCreateDropTable() throws TException, IOException {\n+    // We need the location for HadoopTable based tests only\n+    String location = locationForCreateTable(temp.getRoot().getPath(), \"customers\");\n+    shell.executeStatement(\"CREATE EXTERNAL TABLE customers \" +\n+        \"STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' \" +\n+        (location != null ? \"LOCATION '\" + location + \"' \" : \"\") +\n+        \"TBLPROPERTIES ('\" + InputFormatConfig.TABLE_SCHEMA + \"'='\" + SchemaParser.toJson(CUSTOMER_SCHEMA) + \"', \" +\n+        \"'\" + InputFormatConfig.PARTITION_SPEC + \"'='\" + PartitionSpecParser.toJson(IDENTITY_SPEC) + \"', \" +\n+        \"'dummy'='test')\");\n+\n+    Properties properties = new Properties();\n+    properties.put(Catalogs.NAME, TableIdentifier.of(\"default\", \"customers\").toString());\n+    if (location != null) {\n+      properties.put(Catalogs.LOCATION, location);\n+    }\n+\n+    // Check the Iceberg table data\n+    org.apache.iceberg.Table icebergTable = Catalogs.loadTable(shell.getHiveConf(), properties);\n+    Assert.assertEquals(SchemaParser.toJson(CUSTOMER_SCHEMA), SchemaParser.toJson(icebergTable.schema()));\n+    Assert.assertEquals(PartitionSpecParser.toJson(IDENTITY_SPEC), PartitionSpecParser.toJson(icebergTable.spec()));\n+    Assert.assertEquals(Collections.singletonMap(\"dummy\", \"test\"), icebergTable.properties());\n+\n+    // Check the HMS table parameters\n+    IMetaStoreClient client = null;\n+    org.apache.hadoop.hive.metastore.api.Table hmsTable;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "10127f34eaf2780ad44a385a045be8a656fc1421"}, "originalPosition": 76}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk5ODYxMjgz", "url": "https://github.com/apache/iceberg/pull/1495#pullrequestreview-499861283", "createdAt": "2020-09-30T22:41:12Z", "commit": {"oid": "10127f34eaf2780ad44a385a045be8a656fc1421"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQyMjo0MToxMlrOHaxydQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQyMjo0MToxMlrOHaxydQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzg0MDc1Nw==", "bodyText": "Should this be done in the case where the table can work without Hive? The if statement seems backward to me.", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r497840757", "createdAt": "2020-09-30T22:41:12Z", "author": {"login": "rdblue"}, "path": "mr/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandlerBaseTest.java", "diffHunk": "@@ -157,6 +177,282 @@ public void testJoinTables() throws IOException {\n     Assert.assertArrayEquals(new Object[] {1L, \"Bob\", 102L, 33.33d}, rows.get(2));\n   }\n \n+  @Test\n+  public void testCreateDropTable() throws TException, IOException {\n+    // We need the location for HadoopTable based tests only\n+    String location = locationForCreateTable(temp.getRoot().getPath(), \"customers\");\n+    shell.executeStatement(\"CREATE EXTERNAL TABLE customers \" +\n+        \"STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' \" +\n+        (location != null ? \"LOCATION '\" + location + \"' \" : \"\") +\n+        \"TBLPROPERTIES ('\" + InputFormatConfig.TABLE_SCHEMA + \"'='\" + SchemaParser.toJson(CUSTOMER_SCHEMA) + \"', \" +\n+        \"'\" + InputFormatConfig.PARTITION_SPEC + \"'='\" + PartitionSpecParser.toJson(IDENTITY_SPEC) + \"', \" +\n+        \"'dummy'='test')\");\n+\n+    Properties properties = new Properties();\n+    properties.put(Catalogs.NAME, TableIdentifier.of(\"default\", \"customers\").toString());\n+    if (location != null) {\n+      properties.put(Catalogs.LOCATION, location);\n+    }\n+\n+    // Check the Iceberg table data\n+    org.apache.iceberg.Table icebergTable = Catalogs.loadTable(shell.getHiveConf(), properties);\n+    Assert.assertEquals(SchemaParser.toJson(CUSTOMER_SCHEMA), SchemaParser.toJson(icebergTable.schema()));\n+    Assert.assertEquals(PartitionSpecParser.toJson(IDENTITY_SPEC), PartitionSpecParser.toJson(icebergTable.spec()));\n+    Assert.assertEquals(Collections.singletonMap(\"dummy\", \"test\"), icebergTable.properties());\n+\n+    // Check the HMS table parameters\n+    IMetaStoreClient client = null;\n+    org.apache.hadoop.hive.metastore.api.Table hmsTable;\n+    try {\n+      client = new HiveMetaStoreClient(metastore.hiveConf());\n+      hmsTable = client.getTable(\"default\", \"customers\");\n+    } finally {\n+      if (client != null) {\n+        client.close();\n+      }\n+    }\n+\n+    Map<String, String> hmsParams = hmsTable.getParameters();\n+\n+    // This is only set for HiveCatalog based tables. Check the value, then remove it so the other checks can be general\n+    if (needToCheckSnapshotLocation()) {\n+      Assert.assertTrue(hmsParams.get(BaseMetastoreTableOperations.METADATA_LOCATION_PROP)\n+          .startsWith(icebergTable.location()));\n+      hmsParams.remove(BaseMetastoreTableOperations.METADATA_LOCATION_PROP);\n+    }\n+\n+    // General metadata checks\n+    Assert.assertEquals(6, hmsParams.size());\n+    Assert.assertEquals(\"test\", hmsParams.get(\"dummy\"));\n+    Assert.assertEquals(\"TRUE\", hmsParams.get(InputFormatConfig.EXTERNAL_TABLE_PURGE));\n+    Assert.assertEquals(\"TRUE\", hmsParams.get(\"EXTERNAL\"));\n+    Assert.assertNotNull(hmsParams.get(hive_metastoreConstants.DDL_TIME));\n+    Assert.assertEquals(HiveIcebergStorageHandler.class.getName(),\n+        hmsTable.getParameters().get(hive_metastoreConstants.META_TABLE_STORAGE));\n+    Assert.assertEquals(BaseMetastoreTableOperations.ICEBERG_TABLE_TYPE_VALUE.toUpperCase(),\n+        hmsTable.getParameters().get(BaseMetastoreTableOperations.TABLE_TYPE_PROP));\n+\n+    if (Catalogs.canWorkWithoutHive(shell.getHiveConf())) {\n+      shell.executeStatement(\"DROP TABLE customers\");\n+\n+      // Check if the table was really dropped even from the Catalog\n+      AssertHelpers.assertThrows(\"should throw exception\", NoSuchTableException.class,\n+          \"Table does not exist\", () -> {\n+            Catalogs.loadTable(shell.getHiveConf(), properties);\n+          }\n+      );\n+    } else {\n+      // Check the HMS table parameters\n+      Path hmsTableLocation;\n+      try {\n+        client = new HiveMetaStoreClient(metastore.hiveConf());\n+        hmsTableLocation = new Path(client.getTable(\"default\", \"customers\").getSd().getLocation());\n+      } finally {\n+        if (client != null) {\n+          client.close();\n+        }\n+      }\n+\n+      // Drop the table\n+      shell.executeStatement(\"DROP TABLE customers\");\n+\n+      // Check if we drop an exception when trying to drop the table\n+      AssertHelpers.assertThrows(\"should throw exception\", NoSuchTableException.class,\n+          \"Table does not exist\", () -> {\n+            Catalogs.loadTable(shell.getHiveConf(), properties);\n+          }\n+      );\n+\n+      // Check if the files are kept\n+      FileSystem fs = Util.getFs(hmsTableLocation, shell.getHiveConf());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "10127f34eaf2780ad44a385a045be8a656fc1421"}, "originalPosition": 138}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk5ODYxNTA1", "url": "https://github.com/apache/iceberg/pull/1495#pullrequestreview-499861505", "createdAt": "2020-09-30T22:41:46Z", "commit": {"oid": "10127f34eaf2780ad44a385a045be8a656fc1421"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQyMjo0MTo0NlrOHaxzKg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQyMjo0MTo0NlrOHaxzKg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzg0MDkzOA==", "bodyText": "Should this be done with DROP TABLE IF EXISTS in a @Before method?", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r497840938", "createdAt": "2020-09-30T22:41:46Z", "author": {"login": "rdblue"}, "path": "mr/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandlerBaseTest.java", "diffHunk": "@@ -157,6 +177,282 @@ public void testJoinTables() throws IOException {\n     Assert.assertArrayEquals(new Object[] {1L, \"Bob\", 102L, 33.33d}, rows.get(2));\n   }\n \n+  @Test\n+  public void testCreateDropTable() throws TException, IOException {\n+    // We need the location for HadoopTable based tests only\n+    String location = locationForCreateTable(temp.getRoot().getPath(), \"customers\");\n+    shell.executeStatement(\"CREATE EXTERNAL TABLE customers \" +\n+        \"STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' \" +\n+        (location != null ? \"LOCATION '\" + location + \"' \" : \"\") +\n+        \"TBLPROPERTIES ('\" + InputFormatConfig.TABLE_SCHEMA + \"'='\" + SchemaParser.toJson(CUSTOMER_SCHEMA) + \"', \" +\n+        \"'\" + InputFormatConfig.PARTITION_SPEC + \"'='\" + PartitionSpecParser.toJson(IDENTITY_SPEC) + \"', \" +\n+        \"'dummy'='test')\");\n+\n+    Properties properties = new Properties();\n+    properties.put(Catalogs.NAME, TableIdentifier.of(\"default\", \"customers\").toString());\n+    if (location != null) {\n+      properties.put(Catalogs.LOCATION, location);\n+    }\n+\n+    // Check the Iceberg table data\n+    org.apache.iceberg.Table icebergTable = Catalogs.loadTable(shell.getHiveConf(), properties);\n+    Assert.assertEquals(SchemaParser.toJson(CUSTOMER_SCHEMA), SchemaParser.toJson(icebergTable.schema()));\n+    Assert.assertEquals(PartitionSpecParser.toJson(IDENTITY_SPEC), PartitionSpecParser.toJson(icebergTable.spec()));\n+    Assert.assertEquals(Collections.singletonMap(\"dummy\", \"test\"), icebergTable.properties());\n+\n+    // Check the HMS table parameters\n+    IMetaStoreClient client = null;\n+    org.apache.hadoop.hive.metastore.api.Table hmsTable;\n+    try {\n+      client = new HiveMetaStoreClient(metastore.hiveConf());\n+      hmsTable = client.getTable(\"default\", \"customers\");\n+    } finally {\n+      if (client != null) {\n+        client.close();\n+      }\n+    }\n+\n+    Map<String, String> hmsParams = hmsTable.getParameters();\n+\n+    // This is only set for HiveCatalog based tables. Check the value, then remove it so the other checks can be general\n+    if (needToCheckSnapshotLocation()) {\n+      Assert.assertTrue(hmsParams.get(BaseMetastoreTableOperations.METADATA_LOCATION_PROP)\n+          .startsWith(icebergTable.location()));\n+      hmsParams.remove(BaseMetastoreTableOperations.METADATA_LOCATION_PROP);\n+    }\n+\n+    // General metadata checks\n+    Assert.assertEquals(6, hmsParams.size());\n+    Assert.assertEquals(\"test\", hmsParams.get(\"dummy\"));\n+    Assert.assertEquals(\"TRUE\", hmsParams.get(InputFormatConfig.EXTERNAL_TABLE_PURGE));\n+    Assert.assertEquals(\"TRUE\", hmsParams.get(\"EXTERNAL\"));\n+    Assert.assertNotNull(hmsParams.get(hive_metastoreConstants.DDL_TIME));\n+    Assert.assertEquals(HiveIcebergStorageHandler.class.getName(),\n+        hmsTable.getParameters().get(hive_metastoreConstants.META_TABLE_STORAGE));\n+    Assert.assertEquals(BaseMetastoreTableOperations.ICEBERG_TABLE_TYPE_VALUE.toUpperCase(),\n+        hmsTable.getParameters().get(BaseMetastoreTableOperations.TABLE_TYPE_PROP));\n+\n+    if (Catalogs.canWorkWithoutHive(shell.getHiveConf())) {\n+      shell.executeStatement(\"DROP TABLE customers\");\n+\n+      // Check if the table was really dropped even from the Catalog\n+      AssertHelpers.assertThrows(\"should throw exception\", NoSuchTableException.class,\n+          \"Table does not exist\", () -> {\n+            Catalogs.loadTable(shell.getHiveConf(), properties);\n+          }\n+      );\n+    } else {\n+      // Check the HMS table parameters\n+      Path hmsTableLocation;\n+      try {\n+        client = new HiveMetaStoreClient(metastore.hiveConf());\n+        hmsTableLocation = new Path(client.getTable(\"default\", \"customers\").getSd().getLocation());\n+      } finally {\n+        if (client != null) {\n+          client.close();\n+        }\n+      }\n+\n+      // Drop the table\n+      shell.executeStatement(\"DROP TABLE customers\");\n+\n+      // Check if we drop an exception when trying to drop the table\n+      AssertHelpers.assertThrows(\"should throw exception\", NoSuchTableException.class,\n+          \"Table does not exist\", () -> {\n+            Catalogs.loadTable(shell.getHiveConf(), properties);\n+          }\n+      );\n+\n+      // Check if the files are kept\n+      FileSystem fs = Util.getFs(hmsTableLocation, shell.getHiveConf());\n+      // TODO: files should be deleted\n+      // Assert.assertEquals(0, fs.listStatus(hmsTableLocation).length);\n+    }\n+  }\n+\n+  @Test\n+  public void testCreateTableWithoutSpec() throws TException {\n+    // We need the location for HadoopTable based tests only\n+    String location = locationForCreateTable(temp.getRoot().getPath(), \"customers\");\n+    shell.executeStatement(\"CREATE EXTERNAL TABLE customers \" +\n+        \"STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' \" +\n+        (location != null ? \"LOCATION '\" + location + \"' \" : \"\") +\n+        \"TBLPROPERTIES ('\" + InputFormatConfig.TABLE_SCHEMA + \"'='\" + SchemaParser.toJson(CUSTOMER_SCHEMA) + \"')\");\n+\n+    Properties properties = new Properties();\n+    properties.put(Catalogs.NAME, TableIdentifier.of(\"default\", \"customers\").toString());\n+    if (location != null) {\n+      properties.put(Catalogs.LOCATION, location);\n+    }\n+\n+    // Check the Iceberg table partition data\n+    org.apache.iceberg.Table icebergTable = Catalogs.loadTable(shell.getHiveConf(), properties);\n+    Assert.assertEquals(PartitionSpecParser.toJson(SPEC), PartitionSpecParser.toJson(icebergTable.spec()));\n+\n+    // Check the HMS table parameters\n+    IMetaStoreClient client = null;\n+    org.apache.hadoop.hive.metastore.api.Table hmsTable;\n+    try {\n+      client = new HiveMetaStoreClient(metastore.hiveConf());\n+      hmsTable = client.getTable(\"default\", \"customers\");\n+    } finally {\n+      if (client != null) {\n+        client.close();\n+      }\n+    }\n+\n+    Map<String, String> hmsParams = hmsTable.getParameters();\n+\n+    // Just check that the PartitionSpec is not set in the metadata\n+    Assert.assertNull(hmsParams.get(InputFormatConfig.PARTITION_SPEC));\n+\n+    if (needToCheckSnapshotLocation()) {\n+      Assert.assertEquals(6, hmsParams.size());\n+    } else {\n+      Assert.assertEquals(5, hmsParams.size());\n+    }\n+\n+    shell.executeStatement(\"DROP TABLE customers\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "10127f34eaf2780ad44a385a045be8a656fc1421"}, "originalPosition": 186}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "9989b81e685fc14a5bd34d4b422c14aa6671edb3", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/9989b81e685fc14a5bd34d4b422c14aa6671edb3", "committedDate": "2020-10-01T17:43:39Z", "message": "Moved drop tables to the @After method"}, "afterCommit": {"oid": "059c9f5335e25206d0ddbdcac5438b6eae6bf3ac", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/059c9f5335e25206d0ddbdcac5438b6eae6bf3ac", "committedDate": "2020-10-06T21:09:09Z", "message": "Enable hive engine for tests for storageHandler tests too"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "1776812e3d9e2443d44130caa794ea74aaf67690", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/1776812e3d9e2443d44130caa794ea74aaf67690", "committedDate": "2020-10-07T06:27:18Z", "message": "Enable hive engine for tables created by the Hook"}, "afterCommit": {"oid": "20367c90e3ac1f9e4ecfc4870074aad2b3f35972", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/20367c90e3ac1f9e4ecfc4870074aad2b3f35972", "committedDate": "2020-10-07T10:44:04Z", "message": "Cleaned up code after too many local fixes\n- Production code change: For HiveCatalog do not try to load the table in preCreateTable - it should not exist anyway\n- Test code:\n    - Remove stat related table properties when checking\n    - Remove needToCheckSnapshotLocation() use Catalogs.hiveCatalog instead\n    - Add new test to check Hive table creation above existing Iceberg table\n    - locationForCreateTable for HadoopTable should use the same as createIcebergTable method"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA0MDcyMDAy", "url": "https://github.com/apache/iceberg/pull/1495#pullrequestreview-504072002", "createdAt": "2020-10-07T16:42:02Z", "commit": {"oid": "20367c90e3ac1f9e4ecfc4870074aad2b3f35972"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QxNjo0MjowMlrOHd8P4g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QxNjo0MjowMlrOHd8P4g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTE1Nzg1OA==", "bodyText": "This is the fix for flaky tests? What was the heap size before?", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r501157858", "createdAt": "2020-10-07T16:42:02Z", "author": {"login": "rdblue"}, "path": "build.gradle", "diffHunk": "@@ -470,6 +470,11 @@ project(':iceberg-mr') {\n       exclude group: 'org.apache.calcite.avatica'\n     }\n   }\n+\n+  test {\n+    // testJoinTables / testScanTable\n+    maxHeapSize '1500m'", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "20367c90e3ac1f9e4ecfc4870074aad2b3f35972"}, "originalPosition": 7}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA0MDc2NjA1", "url": "https://github.com/apache/iceberg/pull/1495#pullrequestreview-504076605", "createdAt": "2020-10-07T16:47:45Z", "commit": {"oid": "20367c90e3ac1f9e4ecfc4870074aad2b3f35972"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QxNjo0Nzo0NVrOHd8eGw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QxNjo0Nzo0NVrOHd8eGw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTE2MTQ5OQ==", "bodyText": "I don't think this is correct. Looks like this is happening because we always try to load the table, so the create path (the previous else case after checking base != null) is not taken.\nI think the logic should be: if the table exists, then check whether we are trying to create it (base is null). If we are trying to create it, validate that it is in a state that can be created: the table should have type \"iceberg\", but should not have a metadata location. If it has a metadata location, then the table already exists and it should throw the original AlreadyExistsException.", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r501161499", "createdAt": "2020-10-07T16:47:45Z", "author": {"login": "rdblue"}, "path": "hive-metastore/src/test/java/org/apache/iceberg/hive/HiveCreateReplaceTableTest.java", "diffHunk": "@@ -104,8 +105,8 @@ public void testCreateTableTxnTableCreatedConcurrently() {\n \n     AssertHelpers.assertThrows(\n         \"Create table txn should fail\",\n-        AlreadyExistsException.class,\n-        \"Table already exists: hivedb.tbl\",\n+        CommitFailedException.class,\n+        \"is not same as the current table metadata\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "20367c90e3ac1f9e4ecfc4870074aad2b3f35972"}, "originalPosition": 15}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "20367c90e3ac1f9e4ecfc4870074aad2b3f35972", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/20367c90e3ac1f9e4ecfc4870074aad2b3f35972", "committedDate": "2020-10-07T10:44:04Z", "message": "Cleaned up code after too many local fixes\n- Production code change: For HiveCatalog do not try to load the table in preCreateTable - it should not exist anyway\n- Test code:\n    - Remove stat related table properties when checking\n    - Remove needToCheckSnapshotLocation() use Catalogs.hiveCatalog instead\n    - Add new test to check Hive table creation above existing Iceberg table\n    - locationForCreateTable for HadoopTable should use the same as createIcebergTable method"}, "afterCommit": {"oid": "d90bcd752cb9aca8516dff5b057fe86fe7fe6bbc", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/d90bcd752cb9aca8516dff5b057fe86fe7fe6bbc", "committedDate": "2020-10-08T17:59:08Z", "message": "Reverted to create the HMS client anyway, since we use it to clean the HMS after every test"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "d90bcd752cb9aca8516dff5b057fe86fe7fe6bbc", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/d90bcd752cb9aca8516dff5b057fe86fe7fe6bbc", "committedDate": "2020-10-08T17:59:08Z", "message": "Reverted to create the HMS client anyway, since we use it to clean the HMS after every test"}, "afterCommit": {"oid": "6115ad1c676b644f786382ec9b852c5350433261", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/6115ad1c676b644f786382ec9b852c5350433261", "committedDate": "2020-10-08T19:47:50Z", "message": "Rebased again"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA1MTkzNzEw", "url": "https://github.com/apache/iceberg/pull/1495#pullrequestreview-505193710", "createdAt": "2020-10-08T21:55:30Z", "commit": {"oid": "6115ad1c676b644f786382ec9b852c5350433261"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQyMTo1NTozMFrOHexxHw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQyMTo1NTozMFrOHexxHw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjAzNDcxOQ==", "bodyText": "This file was probably added by mistake?", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r502034719", "createdAt": "2020-10-08T21:55:30Z", "author": {"login": "rdblue"}, "path": "mr/hs_err_pid63322.log", "diffHunk": "@@ -0,0 +1,811 @@\n+#", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6115ad1c676b644f786382ec9b852c5350433261"}, "originalPosition": 1}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA1MTk0NDMy", "url": "https://github.com/apache/iceberg/pull/1495#pullrequestreview-505194432", "createdAt": "2020-10-08T21:56:50Z", "commit": {"oid": "6115ad1c676b644f786382ec9b852c5350433261"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQyMTo1Njo1MFrOHexzQw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQyMTo1Njo1MFrOHexzQw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjAzNTI2Nw==", "bodyText": "I'd recommend taking a look at other classes based on this and copying their use of a client pool, rather than creating a client here.", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r502035267", "createdAt": "2020-10-08T21:56:50Z", "author": {"login": "rdblue"}, "path": "hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveMetastore.java", "diffHunk": "@@ -66,26 +75,31 @@\n   private ExecutorService executorService;\n   private TServer server;\n   private HiveMetaStore.HMSHandler baseHandler;\n+  private IMetaStoreClient client = null;\n \n   public void start() {\n     try {\n-      hiveLocalDir = createTempDirectory(\"hive\", asFileAttribute(fromString(\"rwxrwxrwx\"))).toFile();\n+      this.hiveLocalDir = createTempDirectory(\"hive\", asFileAttribute(fromString(\"rwxrwxrwx\"))).toFile();\n       File derbyLogFile = new File(hiveLocalDir, \"derby.log\");\n       System.setProperty(\"derby.stream.error.file\", derbyLogFile.getAbsolutePath());\n       setupMetastoreDB(\"jdbc:derby:\" + getDerbyPath() + \";create=true\");\n \n       TServerSocket socket = new TServerSocket(0);\n       int port = socket.getServerSocket().getLocalPort();\n-      hiveConf = newHiveConf(port);\n-      server = newThriftServer(socket, hiveConf);\n-      executorService = Executors.newSingleThreadExecutor();\n-      executorService.submit(() -> server.serve());\n+      this.hiveConf = newHiveConf(port);\n+      this.server = newThriftServer(socket, hiveConf);\n+      this.executorService = Executors.newSingleThreadExecutor();\n+      this.executorService.submit(() -> server.serve());\n+      this.client = new HiveMetaStoreClient(hiveConf);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6115ad1c676b644f786382ec9b852c5350433261"}, "originalPosition": 54}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "6115ad1c676b644f786382ec9b852c5350433261", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/6115ad1c676b644f786382ec9b852c5350433261", "committedDate": "2020-10-08T19:47:50Z", "message": "Rebased again"}, "afterCommit": {"oid": "6f798bed7101bf2b4eceb079c4b6dc96ab87a77d", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/6f798bed7101bf2b4eceb079c4b6dc96ab87a77d", "committedDate": "2020-10-12T11:55:46Z", "message": "Fixing rebase errors"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA2ODY3NjY2", "url": "https://github.com/apache/iceberg/pull/1495#pullrequestreview-506867666", "createdAt": "2020-10-12T19:35:08Z", "commit": {"oid": "6f798bed7101bf2b4eceb079c4b6dc96ab87a77d"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQxOTozNTowOVrOHgKzEg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQxOTozNTowOVrOHgKzEg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzQ5MzM5NA==", "bodyText": "Why is this needed? It may be nice to use this for applications, but for testing it seems like this allows us to pass the configuration incorrectly and still have tests pass. If it isn't needed, I'd rather remove it so that we must pass configuration properly.", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r503493394", "createdAt": "2020-10-12T19:35:09Z", "author": {"login": "rdblue"}, "path": "hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveMetastore.java", "diffHunk": "@@ -66,26 +72,35 @@\n   private ExecutorService executorService;\n   private TServer server;\n   private HiveMetaStore.HMSHandler baseHandler;\n+  private HiveClientPool clientPool;\n \n   public void start() {\n     try {\n-      hiveLocalDir = createTempDirectory(\"hive\", asFileAttribute(fromString(\"rwxrwxrwx\"))).toFile();\n+      this.hiveLocalDir = createTempDirectory(\"hive\", asFileAttribute(fromString(\"rwxrwxrwx\"))).toFile();\n       File derbyLogFile = new File(hiveLocalDir, \"derby.log\");\n       System.setProperty(\"derby.stream.error.file\", derbyLogFile.getAbsolutePath());\n       setupMetastoreDB(\"jdbc:derby:\" + getDerbyPath() + \";create=true\");\n \n       TServerSocket socket = new TServerSocket(0);\n       int port = socket.getServerSocket().getLocalPort();\n-      hiveConf = newHiveConf(port);\n-      server = newThriftServer(socket, hiveConf);\n-      executorService = Executors.newSingleThreadExecutor();\n-      executorService.submit(() -> server.serve());\n+      this.hiveConf = newHiveConf(port);\n+      this.server = newThriftServer(socket, hiveConf);\n+      this.executorService = Executors.newSingleThreadExecutor();\n+      this.executorService.submit(() -> server.serve());\n+\n+      // in Hive3, setting this as a system prop ensures that it will be picked up whenever a new HiveConf is created\n+      System.setProperty(HiveConf.ConfVars.METASTOREURIS.varname, hiveConf.getVar(HiveConf.ConfVars.METASTOREURIS));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f798bed7101bf2b4eceb079c4b6dc96ab87a77d"}, "originalPosition": 53}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA2ODY4NjA2", "url": "https://github.com/apache/iceberg/pull/1495#pullrequestreview-506868606", "createdAt": "2020-10-12T19:37:01Z", "commit": {"oid": "6f798bed7101bf2b4eceb079c4b6dc96ab87a77d"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQxOTozNzowMVrOHgK2Jw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQxOTozNzowMVrOHgK2Jw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzQ5NDE4Mw==", "bodyText": "Minor: this seems like a good candidate for else if (serdeProperties...).", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r503494183", "createdAt": "2020-10-12T19:37:01Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergSerDe.java", "diffHunk": "@@ -29,25 +29,42 @@\n import org.apache.hadoop.io.Writable;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.SchemaParser;\n-import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.NoSuchTableException;\n import org.apache.iceberg.mr.Catalogs;\n import org.apache.iceberg.mr.InputFormatConfig;\n import org.apache.iceberg.mr.hive.serde.objectinspector.IcebergObjectInspector;\n import org.apache.iceberg.mr.mapred.Container;\n \n public class HiveIcebergSerDe extends AbstractSerDe {\n-\n   private ObjectInspector inspector;\n \n   @Override\n   public void initialize(@Nullable Configuration configuration, Properties serDeProperties) throws SerDeException {\n+    // HiveIcebergSerDe.initialize is called multiple places in Hive code:\n+    // - When we are trying to create a table - HiveDDL data is stored at the serDeProperties, but no Iceberg table\n+    // is created yet.\n+    // - When we are compiling the Hive query on HiveServer2 side - We only have table information (location/name),\n+    // and we have to read the schema using the table data. This is called multiple times so there is room for\n+    // optimizing here.\n+    // - When we are executing the Hive query in the execution engine - We do not want to load the table data on every\n+    // executor, but serDeProperties are populated by HiveIcebergStorageHandler.configureInputJobProperties() and\n+    // the resulting properties are serialized and distributed to the executors\n+\n     Schema tableSchema;\n     if (configuration.get(InputFormatConfig.TABLE_SCHEMA) != null) {\n       tableSchema = SchemaParser.fromJson(configuration.get(InputFormatConfig.TABLE_SCHEMA));\n     } else {\n-      Table table = Catalogs.loadTable(configuration, serDeProperties);\n-      tableSchema = table.schema();\n+      if (serDeProperties.get(InputFormatConfig.TABLE_SCHEMA) != null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f798bed7101bf2b4eceb079c4b6dc96ab87a77d"}, "originalPosition": 33}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c787816b029d10ab9f7882b8ca7edf8795b05c71", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/c787816b029d10ab9f7882b8ca7edf8795b05c71", "committedDate": "2020-10-13T07:09:34Z", "message": "HiveMetaHook implementation to enable CREATE TABLE and DROP TABLE from Hive queries\n\n(cherry picked from commit c242616c90fce03e0074c3b9cb218bc77fe66469)"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fcd54062732fbc5a9d84eac9fd15daa9c98d197b", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/fcd54062732fbc5a9d84eac9fd15daa9c98d197b", "committedDate": "2020-10-13T07:09:34Z", "message": "Addressed review comments\n\n(cherry picked from commit bc13a60e8987e53828b1f5c6cd3dd1f2654c1e57)"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "734fc5deedead33e1f65a09ff9dc0281f846c687", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/734fc5deedead33e1f65a09ff9dc0281f846c687", "committedDate": "2020-10-13T07:09:34Z", "message": "Fix for partitionspec handling and tests for it"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ffc108bb023644ac34c293564b90245a15be8a74", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/ffc108bb023644ac34c293564b90245a15be8a74", "committedDate": "2020-10-13T07:09:34Z", "message": "Store and remove TABLE_CREATION_FROM_HIVE property in HMS"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f57c5a18efcacdb91f4ebfab401372baa62dea76", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/f57c5a18efcacdb91f4ebfab401372baa62dea76", "committedDate": "2020-10-13T07:09:34Z", "message": "Fix checkstyle"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2756a51f6e35c6bf5e58096ad488310be6685353", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/2756a51f6e35c6bf5e58096ad488310be6685353", "committedDate": "2020-10-13T07:09:34Z", "message": "Handling external.table.purge"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c0dcfb2bc49626823851d0cd3c32c7e7dd57fcea", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/c0dcfb2bc49626823851d0cd3c32c7e7dd57fcea", "committedDate": "2020-10-13T07:09:34Z", "message": "Reworked createTable from Hive"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b9e88d54c0f6df53714b46cca5bc969209a4f4b9", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/b9e88d54c0f6df53714b46cca5bc969209a4f4b9", "committedDate": "2020-10-13T07:09:34Z", "message": "Fixed drop table issue with HiveCatalog"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "274c72c2735e8df010c484941a610106dd442f38", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/274c72c2735e8df010c484941a610106dd442f38", "committedDate": "2020-10-13T07:09:34Z", "message": "Removed unused imports"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6aaaee4d48d1bfb5a384242bfc20305dd627922d", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/6aaaee4d48d1bfb5a384242bfc20305dd627922d", "committedDate": "2020-10-13T07:09:34Z", "message": "Moved drop tables to the @After method"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "947620e6d6ae9e6f09aec1a4d3c9a791cb2483d3", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/947620e6d6ae9e6f09aec1a4d3c9a791cb2483d3", "committedDate": "2020-10-13T07:09:35Z", "message": "Enable hive engine for tests for storageHandler tests too"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "48477598ef2fad16c7acb0c6681d05d564091929", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/48477598ef2fad16c7acb0c6681d05d564091929", "committedDate": "2020-10-13T07:09:35Z", "message": "Enable hive engine for tables created by the Hook"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0e26dd231f6e1c1cf0e3fd390fb8a51099f19e0b", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/0e26dd231f6e1c1cf0e3fd390fb8a51099f19e0b", "committedDate": "2020-10-13T07:09:35Z", "message": "Cleaned up code after too many local fixes\n- Production code change: For HiveCatalog do not try to load the table in preCreateTable - it should not exist anyway\n- Test code:\n    - Remove stat related table properties when checking\n    - Remove needToCheckSnapshotLocation() use Catalogs.hiveCatalog instead\n    - Add new test to check Hive table creation above existing Iceberg table\n    - locationForCreateTable for HadoopTable should use the same as createIcebergTable method"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4fd871d7d0332057bb2f0d5a046c440da7522cb8", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/4fd871d7d0332057bb2f0d5a046c440da7522cb8", "committedDate": "2020-10-13T07:09:35Z", "message": "Using the new metastore in tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "085ffb9fbc19b4edb92e1945ba5804e8b87c6444", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/085ffb9fbc19b4edb92e1945ba5804e8b87c6444", "committedDate": "2020-10-13T07:09:35Z", "message": "Revert to AlreadyExistsException for concurrent commits to the table"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5ca165efbe9430e16c2d7713ee42ff435bc25752", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/5ca165efbe9430e16c2d7713ee42ff435bc25752", "committedDate": "2020-10-13T07:09:35Z", "message": "Reverted to create the HMS client anyway, since we use it to clean the HMS after every test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f3bf7f3ed9e409e79ebf6a2add30d0be92198a4e", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/f3bf7f3ed9e409e79ebf6a2add30d0be92198a4e", "committedDate": "2020-10-13T07:09:35Z", "message": "Move to client-pool"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1471f2a39f30aa475a4913bed20af450d6da70b4", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/1471f2a39f30aa475a4913bed20af450d6da70b4", "committedDate": "2020-10-13T07:09:35Z", "message": "Fixed Hive3 test failures"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4296e4566652b982daeb4c58f1474141c7fb55e9", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/4296e4566652b982daeb4c58f1474141c7fb55e9", "committedDate": "2020-10-13T07:09:35Z", "message": "Fixing rebase errors"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2a500084bf7e32b6528e8cc78d2c85ff103df067", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/2a500084bf7e32b6528e8cc78d2c85ff103df067", "committedDate": "2020-10-13T07:20:40Z", "message": "Else if formatting"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "6f798bed7101bf2b4eceb079c4b6dc96ab87a77d", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/6f798bed7101bf2b4eceb079c4b6dc96ab87a77d", "committedDate": "2020-10-12T11:55:46Z", "message": "Fixing rebase errors"}, "afterCommit": {"oid": "2a500084bf7e32b6528e8cc78d2c85ff103df067", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/2a500084bf7e32b6528e8cc78d2c85ff103df067", "committedDate": "2020-10-13T07:20:40Z", "message": "Else if formatting"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3828, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}