{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDQ2NzI2ODA5", "number": 1185, "title": "Flink: Add the iceberg files committer to collect data files and commit to iceberg table.", "bodyText": "As we discussed in the pull request #856,  the flink sink connector actually are composed by two stateful flink operator:\n\nFlinkStreamWriter is used for receiving the records and writing them to DataFile(s),  when a checkpoint happen then it will emit the completed DataFile to downstream operator.\nFlinkFilesCommitter: it will collect the DataFile  from FlinkStreamWriter  and commit the files to iceberg in a transaction, it's a single parallelism operator because we don't expect transaction conflict happen when transaction committing.\n\nThis patch mainly implemented the FlinkFilesCommitter and provided several end to end integrate tests to address the correctness.  the dependencies of this one are:  #1180, #1145, #1175", "createdAt": "2020-07-09T09:09:59Z", "url": "https://github.com/apache/iceberg/pull/1185", "merged": true, "mergeCommit": {"oid": "f950a3e63c98e88b1f8905faacf33a1d5d31e4c0"}, "closed": true, "closedAt": "2020-08-28T17:19:34Z", "author": {"login": "openinx"}, "timelineItems": {"totalCount": 63, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABc2-H7bAFqTQ1MjEwOTkxMQ==", "endCursor": "Y3Vyc29yOnYyOpPPAAABdDfLtHgFqTQ3ODA4NjAzOQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDUyMTA5OTEx", "url": "https://github.com/apache/iceberg/pull/1185#pullrequestreview-452109911", "createdAt": "2020-07-21T04:00:46Z", "commit": {"oid": "4f404245be05c497da450d78d85c3b015f5965e7"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU1MjU5MjEx", "url": "https://github.com/apache/iceberg/pull/1185#pullrequestreview-455259211", "createdAt": "2020-07-25T04:07:28Z", "commit": {"oid": "b5b518b4ddcb36d9219c7e53cf40704f3a2e0e4e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNVQwNDowNzoyOFrOG3CPEw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNVQwNDowNzoyOFrOG3CPEw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM2MTQ5MQ==", "bodyText": "As discussed in https://github.com/apache/iceberg/pull/1145/files#diff-22e0fba47887f8c06bc56a654fd3ee9fL363, we were considering avoiding the usage of de.javakaffee for additional kryo serializers.\nCould we create custom Flink serializers, or possibly use POJOs or Flink's built in support for Avro serializers. There's a recently posted blog post on the discussion of serializer choices in Flink here: https://flink.apache.org/news/2020/04/15/flink-serialization-tuning-vol-1.html\nI think Avro makes the most sense given that it's been native to Flink for a long time and because Avro is so widely used in Iceberg already.\nAs an added bonus, Avro supports full schema evolution of state (such that changes to the Avro schema of types stored in stateful operators don't cause said operators to lose their state on upgrade to the new schema, provided they meet certain specifications which are relatively normal rules for type migrations, especially with Avro).", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r460361491", "createdAt": "2020-07-25T04:07:28Z", "author": {"login": "kbendick"}, "path": "build.gradle", "diffHunk": "@@ -253,6 +255,9 @@ project(':iceberg-flink') {\n     compileOnly(\"org.apache.hadoop:hadoop-minicluster\") {\n       exclude group: 'org.apache.avro', module: 'avro'\n     }\n+    compileOnly(\"de.javakaffee:kryo-serializers:0.45\") {\n+      exclude group: 'com.esotericsoftware', module: 'kryo'\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b5b518b4ddcb36d9219c7e53cf40704f3a2e0e4e"}, "originalPosition": 22}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU1MjU5MzQy", "url": "https://github.com/apache/iceberg/pull/1185#pullrequestreview-455259342", "createdAt": "2020-07-25T04:09:57Z", "commit": {"oid": "b5b518b4ddcb36d9219c7e53cf40704f3a2e0e4e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNVQwNDowOTo1N1rOG3CPxw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNVQwNDowOTo1N1rOG3CPxw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM2MTY3MQ==", "bodyText": "Relatively nooby question unrelated to this PR, but does Iceberg only support Flink projects running on Scala 2.12? And is this the same for Spark? For Spark, I would imagine so as Spark 3.0 drops support for Scala 2.11 entirely.", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r460361671", "createdAt": "2020-07-25T04:09:57Z", "author": {"login": "kbendick"}, "path": "build.gradle", "diffHunk": "@@ -253,6 +255,9 @@ project(':iceberg-flink') {\n     compileOnly(\"org.apache.hadoop:hadoop-minicluster\") {\n       exclude group: 'org.apache.avro', module: 'avro'\n     }\n+    compileOnly(\"de.javakaffee:kryo-serializers:0.45\") {\n+      exclude group: 'com.esotericsoftware', module: 'kryo'\n+    }\n \n     testCompile \"org.apache.flink:flink-core\"\n     testCompile \"org.apache.flink:flink-runtime_2.12\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b5b518b4ddcb36d9219c7e53cf40704f3a2e0e4e"}, "originalPosition": 25}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU1MjU5NTgy", "url": "https://github.com/apache/iceberg/pull/1185#pullrequestreview-455259582", "createdAt": "2020-07-25T04:14:38Z", "commit": {"oid": "b5b518b4ddcb36d9219c7e53cf40704f3a2e0e4e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNVQwNDoxNDozOVrOG3CREQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNVQwNDoxNDozOVrOG3CREQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM2MjAwMQ==", "bodyText": "Setting the uid to be a randomly generated number is not a good practice. Operator uid's should be stable, such that between intentional redeployments or simply on restarts due to lost task managers, Flink will know which operators each state belongs to.\nIf the uid is changed between restarts, state could be lost.\nYou can find more information about the use of operator uid's with flink's savepoints (intentional restarts) and why stateful operators should be assigned a stable uid here: https://ci.apache.org/projects/flink/flink-docs-stable/ops/upgrading.html#matching-operator-state", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r460362001", "createdAt": "2020-07-25T04:14:39Z", "author": {"login": "kbendick"}, "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergDataStream.java", "diffHunk": "@@ -0,0 +1,109 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.util.UUID;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.DataFile;\n+\n+public class IcebergDataStream {\n+\n+  private static final String ICEBERG_STREAM_WRITER = \"Iceberg-Stream-Writer\";\n+  private static final String ICEBERG_FILES_COMMITTER = \"Iceberg-Files-Committer\";\n+\n+  private final DataStream<Row> dataStream;\n+  private final String path;\n+  private final Configuration conf;\n+  private final TableSchema tableSchema;\n+  private final Integer parallelism;\n+\n+  private IcebergDataStream(DataStream<Row> dataStream, String path,\n+                            Configuration conf, TableSchema tableSchema,\n+                            Integer parallelism) {\n+    this.dataStream = dataStream;\n+    this.path = path;\n+    this.conf = conf;\n+    this.tableSchema = tableSchema;\n+    this.parallelism = parallelism;\n+  }\n+\n+  public static Builder buildFor(DataStream<Row> dataStream) {\n+    return new Builder().dataStream(dataStream);\n+  }\n+\n+  public static class Builder {\n+    private DataStream<Row> dataStream;\n+    private String path;\n+    private Configuration conf;\n+    private TableSchema tableSchema;\n+    private Integer parallelism;\n+\n+    private Builder dataStream(DataStream<Row> newDataStream) {\n+      this.dataStream = newDataStream;\n+      return this;\n+    }\n+\n+    public Builder path(String newPath) {\n+      this.path = newPath;\n+      return this;\n+    }\n+\n+    public Builder config(Configuration newConfig) {\n+      this.conf = newConfig;\n+      return this;\n+    }\n+\n+    public Builder tableSchema(TableSchema newTableSchema) {\n+      this.tableSchema = newTableSchema;\n+      return this;\n+    }\n+\n+    public Builder parallelism(int newParallelism) {\n+      this.parallelism = newParallelism;\n+      return this;\n+    }\n+\n+    public IcebergDataStream build() {\n+      return new IcebergDataStream(dataStream, path, conf, tableSchema, parallelism);\n+    }\n+  }\n+\n+  public void append() {\n+    IcebergStreamWriter streamWriter = IcebergStreamWriter.createStreamWriter(path, tableSchema, conf);\n+    IcebergFilesCommitter filesCommitter = new IcebergFilesCommitter(path, conf);\n+\n+    SingleOutputStreamOperator<DataFile> operator = dataStream\n+        .transform(ICEBERG_STREAM_WRITER, DataFileTypeInfo.TYPE_INFO, streamWriter)\n+        .uid(UUID.randomUUID().toString());\n+\n+    if (parallelism != null && parallelism > 0) {\n+      operator.setParallelism(parallelism);\n+    }\n+\n+    operator.addSink(filesCommitter)\n+        .name(ICEBERG_FILES_COMMITTER)\n+        .uid(UUID.randomUUID().toString())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b5b518b4ddcb36d9219c7e53cf40704f3a2e0e4e"}, "originalPosition": 106}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYwNTMyMjMz", "url": "https://github.com/apache/iceberg/pull/1185#pullrequestreview-460532233", "createdAt": "2020-08-04T06:36:21Z", "commit": {"oid": "b5b518b4ddcb36d9219c7e53cf40704f3a2e0e4e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQwNjozNjoyMVrOG7S_Zg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQwNjozNjoyMVrOG7S_Zg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgzMDMxMA==", "bodyText": "pos[i] It seemed the fields of flink ddl must be the same sequence with the fields of iceberg schema . what if flink ddl misses some fields? I think partition keys' indices will be different from in iceberg schema", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r464830310", "createdAt": "2020-08-04T06:36:21Z", "author": {"login": "YesOrNo828"}, "path": "flink/src/main/java/org/apache/iceberg/flink/PartitionKey.java", "diffHunk": "@@ -0,0 +1,126 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.flink.types.Row;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.transforms.Transform;\n+import org.apache.iceberg.types.Types;\n+\n+public class PartitionKey implements StructLike {\n+\n+  private final Object[] partitionTuple;\n+\n+  private PartitionKey(Object[] partitionTuple) {\n+    this.partitionTuple = partitionTuple;\n+  }\n+\n+  @Override\n+  public boolean equals(Object o) {\n+    if (this == o) {\n+      return true;\n+    }\n+    if (!(o instanceof PartitionKey)) {\n+      return false;\n+    }\n+\n+    PartitionKey that = (PartitionKey) o;\n+    return Arrays.equals(partitionTuple, that.partitionTuple);\n+  }\n+\n+  @Override\n+  public int hashCode() {\n+    return Arrays.hashCode(partitionTuple);\n+  }\n+\n+  @Override\n+  public int size() {\n+    return partitionTuple.length;\n+  }\n+\n+  @Override\n+  public <T> T get(int pos, Class<T> javaClass) {\n+    return javaClass.cast(partitionTuple[pos]);\n+  }\n+\n+  public Object[] getPartitionTuple() {\n+    return partitionTuple;\n+  }\n+\n+  @Override\n+  public <T> void set(int pos, T value) {\n+    partitionTuple[pos] = value;\n+  }\n+\n+  private static Map<Integer, Integer> buildFieldId2PosMap(Schema schema) {\n+    Map<Integer, Integer> fieldId2Position = Maps.newHashMap();\n+    List<Types.NestedField> nestedFields = schema.asStruct().fields();\n+    for (int i = 0; i < nestedFields.size(); i++) {\n+      fieldId2Position.put(nestedFields.get(i).fieldId(), i);\n+    }\n+    return fieldId2Position;\n+  }\n+\n+  public static Builder builder(PartitionSpec spec) {\n+    return new Builder(spec);\n+  }\n+\n+  public static class Builder {\n+    private final int size;\n+\n+    private final int[] pos;\n+    private final Transform[] transforms;\n+\n+    private Builder(PartitionSpec spec) {\n+      List<PartitionField> fields = spec.fields();\n+      this.size = fields.size();\n+      this.pos = new int[size];\n+      this.transforms = new Transform[size];\n+\n+      Map<Integer, Integer> fieldId2Pos = buildFieldId2PosMap(spec.schema());\n+\n+      for (int i = 0; i < size; i += 1) {\n+        PartitionField field = fields.get(i);\n+        Integer position = fieldId2Pos.get(field.sourceId());\n+        Preconditions.checkArgument(position != null,\n+            \"Field source id from PartitionSpec MUST exist in the original schema\");\n+        this.pos[i] = position;\n+        this.transforms[i] = field.transform();\n+      }\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    public PartitionKey build(Row row) {\n+      Object[] partitionTuple = new Object[size];\n+      for (int i = 0; i < partitionTuple.length; i += 1) {\n+        partitionTuple[i] = transforms[i].apply(row.getField(pos[i]));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b5b518b4ddcb36d9219c7e53cf40704f3a2e0e4e"}, "originalPosition": 121}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY0MTQ3NTc3", "url": "https://github.com/apache/iceberg/pull/1185#pullrequestreview-464147577", "createdAt": "2020-08-10T11:25:30Z", "commit": {"oid": "b5b518b4ddcb36d9219c7e53cf40704f3a2e0e4e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQxMToyNTozMVrOG-KsKw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQxMToyNTozMVrOG-KsKw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Nzg0MDA0Mw==", "bodyText": "I think this method parseMaxCommittedCheckpointId should be invoked when this job is stored.", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r467840043", "createdAt": "2020-08-10T11:25:31Z", "author": {"login": "YesOrNo828"}, "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergFilesCommitter.java", "diffHunk": "@@ -0,0 +1,158 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.ObjectInputStream;\n+import java.io.ObjectOutputStream;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NavigableMap;\n+import org.apache.commons.compress.utils.Lists;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer;\n+import org.apache.flink.runtime.state.CheckpointListener;\n+import org.apache.flink.runtime.state.FunctionInitializationContext;\n+import org.apache.flink.runtime.state.FunctionSnapshotContext;\n+import org.apache.flink.streaming.api.checkpoint.CheckpointedFunction;\n+import org.apache.flink.streaming.api.functions.sink.RichSinkFunction;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+class IcebergFilesCommitter extends RichSinkFunction<DataFile> implements\n+    CheckpointListener, CheckpointedFunction {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergFilesCommitter.class);\n+  private static final String MAX_COMMITTED_CHECKPOINT_ID = \"flink.max-committed.checkpoint.id\";\n+\n+  private final String path;\n+  private final SerializableConfiguration conf;\n+\n+  private transient long maxCommittedCheckpointId;\n+  private transient NavigableMap<Long, List<DataFile>> dataFilesPerCheckpoint;\n+  private transient List<DataFile> dataFilesOfCurrentCheckpoint;\n+  private transient Table table;\n+\n+  // State for all checkpoints;\n+  private static final ListStateDescriptor<byte[]> STATE_DESCRIPTOR =\n+      new ListStateDescriptor<>(\"checkpoints-state\", BytePrimitiveArraySerializer.INSTANCE);\n+  private transient ListState<byte[]> checkpointsState;\n+\n+  IcebergFilesCommitter(String path, Configuration newConf) {\n+    this.path = path;\n+    this.conf = new SerializableConfiguration(newConf);\n+  }\n+\n+  @Override\n+  public void initializeState(FunctionInitializationContext context) throws Exception {\n+    table = TableUtil.findTable(path, conf.get());\n+    maxCommittedCheckpointId = parseMaxCommittedCheckpointId(table.currentSnapshot());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b5b518b4ddcb36d9219c7e53cf40704f3a2e0e4e"}, "originalPosition": 78}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "b5b518b4ddcb36d9219c7e53cf40704f3a2e0e4e", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/b5b518b4ddcb36d9219c7e53cf40704f3a2e0e4e", "committedDate": "2020-07-21T12:00:54Z", "message": "Define a customized serializer to serialize DataFile."}, "afterCommit": {"oid": "65d7347af2d1cdba405a2b81d204f0148be75141", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/65d7347af2d1cdba405a2b81d204f0148be75141", "committedDate": "2020-08-13T08:53:33Z", "message": "Flink: Add the iceberg files committer to collect data files and commit to iceberg table."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY2NTgwNzg1", "url": "https://github.com/apache/iceberg/pull/1185#pullrequestreview-466580785", "createdAt": "2020-08-13T09:08:58Z", "commit": {"oid": "65d7347af2d1cdba405a2b81d204f0148be75141"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QwOTowODo1OFrOHAC30Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QwOToxNDo0NFrOHADGBQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTgwOTEwNQ==", "bodyText": "Can we use Flink MapTypeInfo, and addAll to sorted map?", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r469809105", "createdAt": "2020-08-13T09:08:58Z", "author": {"login": "JingsongLi"}, "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergFilesCommitter.java", "diffHunk": "@@ -0,0 +1,166 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.ObjectInputStream;\n+import java.io.ObjectOutputStream;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NavigableMap;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer;\n+import org.apache.flink.runtime.state.CheckpointListener;\n+import org.apache.flink.runtime.state.FunctionInitializationContext;\n+import org.apache.flink.runtime.state.FunctionSnapshotContext;\n+import org.apache.flink.streaming.api.checkpoint.CheckpointedFunction;\n+import org.apache.flink.streaming.api.functions.sink.RichSinkFunction;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+class IcebergFilesCommitter extends RichSinkFunction<DataFile> implements\n+    CheckpointListener, CheckpointedFunction {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergFilesCommitter.class);\n+  private static final String MAX_COMMITTED_CHECKPOINT_ID = \"flink.max-committed.checkpoint.id\";\n+\n+  private static final FlinkCatalogFactory CATALOG_FACTORY = new FlinkCatalogFactory();\n+\n+  private final String path;\n+  private final SerializableConfiguration conf;\n+  private final ImmutableMap<String, String> options;\n+\n+  private transient long maxCommittedCheckpointId;\n+  private transient NavigableMap<Long, List<DataFile>> dataFilesPerCheckpoint;\n+  private transient List<DataFile> dataFilesOfCurrentCheckpoint;\n+  private transient Table table;\n+\n+  // State for all checkpoints;\n+  private static final ListStateDescriptor<byte[]> STATE_DESCRIPTOR =\n+      new ListStateDescriptor<>(\"checkpoints-state\", BytePrimitiveArraySerializer.INSTANCE);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "65d7347af2d1cdba405a2b81d204f0148be75141"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTgxMjc0MQ==", "bodyText": "Why exclude maxCommittedCheckpointId and checkpointId? Just dataFilesPerCheckpoint.tailMap(checkpointId, true)? I don't understand why we need store previous checkpoint id.", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r469812741", "createdAt": "2020-08-13T09:14:44Z", "author": {"login": "JingsongLi"}, "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergFilesCommitter.java", "diffHunk": "@@ -0,0 +1,166 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.ObjectInputStream;\n+import java.io.ObjectOutputStream;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NavigableMap;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer;\n+import org.apache.flink.runtime.state.CheckpointListener;\n+import org.apache.flink.runtime.state.FunctionInitializationContext;\n+import org.apache.flink.runtime.state.FunctionSnapshotContext;\n+import org.apache.flink.streaming.api.checkpoint.CheckpointedFunction;\n+import org.apache.flink.streaming.api.functions.sink.RichSinkFunction;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+class IcebergFilesCommitter extends RichSinkFunction<DataFile> implements\n+    CheckpointListener, CheckpointedFunction {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergFilesCommitter.class);\n+  private static final String MAX_COMMITTED_CHECKPOINT_ID = \"flink.max-committed.checkpoint.id\";\n+\n+  private static final FlinkCatalogFactory CATALOG_FACTORY = new FlinkCatalogFactory();\n+\n+  private final String path;\n+  private final SerializableConfiguration conf;\n+  private final ImmutableMap<String, String> options;\n+\n+  private transient long maxCommittedCheckpointId;\n+  private transient NavigableMap<Long, List<DataFile>> dataFilesPerCheckpoint;\n+  private transient List<DataFile> dataFilesOfCurrentCheckpoint;\n+  private transient Table table;\n+\n+  // State for all checkpoints;\n+  private static final ListStateDescriptor<byte[]> STATE_DESCRIPTOR =\n+      new ListStateDescriptor<>(\"checkpoints-state\", BytePrimitiveArraySerializer.INSTANCE);\n+  private transient ListState<byte[]> checkpointsState;\n+\n+  IcebergFilesCommitter(String path, Map<String, String> options, Configuration conf) {\n+    this.path = path;\n+    this.options = ImmutableMap.copyOf(options);\n+    this.conf = new SerializableConfiguration(conf);\n+  }\n+\n+  @Override\n+  public void initializeState(FunctionInitializationContext context) throws Exception {\n+    Catalog icebergCatalog = CATALOG_FACTORY.buildIcebergCatalog(path, options, conf.get());\n+    table = icebergCatalog.loadTable(TableIdentifier.parse(path));\n+    maxCommittedCheckpointId = parseMaxCommittedCheckpointId(table.currentSnapshot());\n+\n+    dataFilesPerCheckpoint = Maps.newTreeMap();\n+    dataFilesOfCurrentCheckpoint = Lists.newArrayList();\n+\n+    checkpointsState = context.getOperatorStateStore().getListState(STATE_DESCRIPTOR);\n+    if (context.isRestored()) {\n+      dataFilesPerCheckpoint = deserializeState(checkpointsState.get().iterator().next());\n+    }\n+  }\n+\n+  @Override\n+  public void snapshotState(FunctionSnapshotContext context) throws Exception {\n+    long checkpointId = context.getCheckpointId();\n+    LOG.info(\"Start to flush snapshot state to state backend, table: {}, checkpointId: {}\", table, checkpointId);\n+\n+    // Update the checkpoint state.\n+    dataFilesPerCheckpoint.put(checkpointId, ImmutableList.copyOf(dataFilesOfCurrentCheckpoint));\n+\n+    // Reset the snapshot state to the latest state.\n+    checkpointsState.clear();\n+    checkpointsState.addAll(ImmutableList.of(serializeState(dataFilesPerCheckpoint)));\n+\n+    // Clear the local buffer for current checkpoint.\n+    dataFilesOfCurrentCheckpoint.clear();\n+  }\n+\n+  @Override\n+  public void notifyCheckpointComplete(long checkpointId) {\n+    NavigableMap<Long, List<DataFile>> pendingFileMap = dataFilesPerCheckpoint.tailMap(maxCommittedCheckpointId, false);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "65d7347af2d1cdba405a2b81d204f0148be75141"}, "originalPosition": 115}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDcxMzQ1NzIz", "url": "https://github.com/apache/iceberg/pull/1185#pullrequestreview-471345723", "createdAt": "2020-08-20T07:47:34Z", "commit": {"oid": "cbb6ea209b2a63c91d7b179878d7841a2b56086f"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 15, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yMFQwNzo0NzozNFrOHDxTsg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yMFQwODoxMzo0NFrOHDzK-Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzcxNTYzNA==", "bodyText": "Please add parquet support, and also add tests.", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r473715634", "createdAt": "2020-08-20T07:47:34Z", "author": {"login": "JingsongLi"}, "path": "flink/src/main/java/org/apache/iceberg/flink/RowDataTaskWriterFactory.java", "diffHunk": "@@ -113,7 +114,7 @@ protected PartitionKey partition(RowData row) {\n     }\n   }\n \n-  private static class FlinkFileAppenderFactory implements FileAppenderFactory<RowData> {\n+  private static class FlinkFileAppenderFactory implements FileAppenderFactory<RowData>, Serializable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cbb6ea209b2a63c91d7b179878d7841a2b56086f"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzcxNjk1Mg==", "bodyText": "Maybe you can let FileAppenderFactory extends Serializable", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r473716952", "createdAt": "2020-08-20T07:48:38Z", "author": {"login": "JingsongLi"}, "path": "flink/src/main/java/org/apache/iceberg/flink/RowDataTaskWriterFactory.java", "diffHunk": "@@ -113,7 +114,7 @@ protected PartitionKey partition(RowData row) {\n     }\n   }\n \n-  private static class FlinkFileAppenderFactory implements FileAppenderFactory<RowData> {\n+  private static class FlinkFileAppenderFactory implements FileAppenderFactory<RowData>, Serializable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cbb6ea209b2a63c91d7b179878d7841a2b56086f"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzcxNzExOQ==", "bodyText": "Why?", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r473717119", "createdAt": "2020-08-20T07:48:45Z", "author": {"login": "JingsongLi"}, "path": "core/src/main/java/org/apache/iceberg/io/OutputFileFactory.java", "diffHunk": "@@ -27,7 +28,7 @@\n import org.apache.iceberg.encryption.EncryptedOutputFile;\n import org.apache.iceberg.encryption.EncryptionManager;\n \n-public class OutputFileFactory {\n+public class OutputFileFactory implements Serializable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cbb6ea209b2a63c91d7b179878d7841a2b56086f"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzcxOTE4MQ==", "bodyText": "If this is a user API, I think a class FlinkSink (Instead of IcebergSink?) is more suitable.\nAnd do you think we can let it be a builder kind class?", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r473719181", "createdAt": "2020-08-20T07:50:16Z", "author": {"login": "JingsongLi"}, "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergSinkUtil.java", "diffHunk": "@@ -37,9 +45,32 @@\n import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT;\n \n class IcebergSinkUtil {\n+\n   private IcebergSinkUtil() {\n   }\n \n+  @SuppressWarnings(\"unchecked\")\n+  static DataStreamSink<RowData> write(DataStream<RowData> inputStream,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cbb6ea209b2a63c91d7b179878d7841a2b56086f"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzcyNTc2Nw==", "bodyText": "Actually, I think it is just flinkSchema.", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r473725767", "createdAt": "2020-08-20T07:55:07Z", "author": {"login": "JingsongLi"}, "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergSinkUtil.java", "diffHunk": "@@ -37,9 +45,32 @@\n import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT;\n \n class IcebergSinkUtil {\n+\n   private IcebergSinkUtil() {\n   }\n \n+  @SuppressWarnings(\"unchecked\")\n+  static DataStreamSink<RowData> write(DataStream<RowData> inputStream,\n+                                       Map<String, String> options,\n+                                       Configuration conf,\n+                                       String fullTableName,\n+                                       Table table,\n+                                       TableSchema requestedSchema) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cbb6ea209b2a63c91d7b179878d7841a2b56086f"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzcyODQxMg==", "bodyText": "Why not use jobId?", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r473728412", "createdAt": "2020-08-20T07:57:09Z", "author": {"login": "JingsongLi"}, "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergFilesCommitter.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NavigableMap;\n+import java.util.SortedMap;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.BasicTypeInfo;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.ListTypeInfo;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.BoundedOneInput;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.runtime.typeutils.SortedMapTypeInfo;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.types.Comparators;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+class IcebergFilesCommitter extends AbstractStreamOperator<Void>\n+    implements OneInputStreamOperator<DataFile, Void>, BoundedOneInput {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final long INITIAL_CHECKPOINT_ID = -1L;\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergFilesCommitter.class);\n+  private static final String GLOBAL_FILES_COMMITTER_UID = \"flink.files-committer.uid\";\n+  private static final String MAX_COMMITTED_CHECKPOINT_ID = \"flink.max-committed.checkpoint.id\";\n+\n+  private static final FlinkCatalogFactory CATALOG_FACTORY = new FlinkCatalogFactory();\n+\n+  // It will have an unique identifier for one job.\n+  private final String filesCommitterUid;\n+  private final String fullTableName;\n+  private final SerializableConfiguration conf;\n+  private final ImmutableMap<String, String> options;\n+\n+  // The max checkpoint id we've committed to iceberg table. As the flink's checkpoint is always increasing, so we could\n+  // correctly commit all the data files whose checkpoint id is greater than the max committed one to iceberg table, for\n+  // avoiding committing the same data files twice. This id will be attached to iceberg's meta when committing the\n+  // iceberg transaction.\n+  private transient long maxCommittedCheckpointId;\n+\n+  // A sorted map to maintain the completed data files for each pending checkpointId (which have not been committed\n+  // to iceberg table). We need a sorted map here because there's possible that few checkpoints snapshot failed, for\n+  // example: the 1st checkpoint have 2 data files <1, <file0, file1>>, the 2st checkpoint have 1 data files\n+  // <2, <file3>>. Snapshot for checkpoint#1 interrupted because of network/disk failure etc, while we don't expect\n+  // any data loss in iceberg table. So we keep the finished files <1, <file0, file1>> in memory and retry to commit\n+  // iceberg table when the next checkpoint happen.\n+  private final NavigableMap<Long, List<DataFile>> dataFilesPerCheckpoint = Maps.newTreeMap();\n+\n+  // The data files cache for current checkpoint. Once the snapshot barrier received, it will be flushed to the\n+  // 'dataFilesPerCheckpoint'.\n+  private final List<DataFile> dataFilesOfCurrentCheckpoint = Lists.newArrayList();\n+  private transient Table table;\n+\n+  // All pending checkpoints states for this function.\n+  private static final ListStateDescriptor<SortedMap<Long, List<DataFile>>> STATE_DESCRIPTOR = buildStateDescriptor();\n+\n+  private transient ListState<SortedMap<Long, List<DataFile>>> checkpointsState;\n+\n+  IcebergFilesCommitter(String filesCommitterUid, String fullTableName,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cbb6ea209b2a63c91d7b179878d7841a2b56086f"}, "originalPosition": 99}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzczMDg2NA==", "bodyText": "maxCommittedCheckpointId can be a local field?", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r473730864", "createdAt": "2020-08-20T07:58:58Z", "author": {"login": "JingsongLi"}, "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergFilesCommitter.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NavigableMap;\n+import java.util.SortedMap;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.BasicTypeInfo;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.ListTypeInfo;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.BoundedOneInput;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.runtime.typeutils.SortedMapTypeInfo;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.types.Comparators;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+class IcebergFilesCommitter extends AbstractStreamOperator<Void>\n+    implements OneInputStreamOperator<DataFile, Void>, BoundedOneInput {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final long INITIAL_CHECKPOINT_ID = -1L;\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergFilesCommitter.class);\n+  private static final String GLOBAL_FILES_COMMITTER_UID = \"flink.files-committer.uid\";\n+  private static final String MAX_COMMITTED_CHECKPOINT_ID = \"flink.max-committed.checkpoint.id\";\n+\n+  private static final FlinkCatalogFactory CATALOG_FACTORY = new FlinkCatalogFactory();\n+\n+  // It will have an unique identifier for one job.\n+  private final String filesCommitterUid;\n+  private final String fullTableName;\n+  private final SerializableConfiguration conf;\n+  private final ImmutableMap<String, String> options;\n+\n+  // The max checkpoint id we've committed to iceberg table. As the flink's checkpoint is always increasing, so we could\n+  // correctly commit all the data files whose checkpoint id is greater than the max committed one to iceberg table, for\n+  // avoiding committing the same data files twice. This id will be attached to iceberg's meta when committing the\n+  // iceberg transaction.\n+  private transient long maxCommittedCheckpointId;\n+\n+  // A sorted map to maintain the completed data files for each pending checkpointId (which have not been committed\n+  // to iceberg table). We need a sorted map here because there's possible that few checkpoints snapshot failed, for\n+  // example: the 1st checkpoint have 2 data files <1, <file0, file1>>, the 2st checkpoint have 1 data files\n+  // <2, <file3>>. Snapshot for checkpoint#1 interrupted because of network/disk failure etc, while we don't expect\n+  // any data loss in iceberg table. So we keep the finished files <1, <file0, file1>> in memory and retry to commit\n+  // iceberg table when the next checkpoint happen.\n+  private final NavigableMap<Long, List<DataFile>> dataFilesPerCheckpoint = Maps.newTreeMap();\n+\n+  // The data files cache for current checkpoint. Once the snapshot barrier received, it will be flushed to the\n+  // 'dataFilesPerCheckpoint'.\n+  private final List<DataFile> dataFilesOfCurrentCheckpoint = Lists.newArrayList();\n+  private transient Table table;\n+\n+  // All pending checkpoints states for this function.\n+  private static final ListStateDescriptor<SortedMap<Long, List<DataFile>>> STATE_DESCRIPTOR = buildStateDescriptor();\n+\n+  private transient ListState<SortedMap<Long, List<DataFile>>> checkpointsState;\n+\n+  IcebergFilesCommitter(String filesCommitterUid, String fullTableName,\n+                        Map<String, String> options, Configuration conf) {\n+    this.filesCommitterUid = filesCommitterUid;\n+    this.fullTableName = fullTableName;\n+    this.options = ImmutableMap.copyOf(options);\n+    this.conf = new SerializableConfiguration(conf);\n+  }\n+\n+  @Override\n+  public void initializeState(StateInitializationContext context) throws Exception {\n+    super.initializeState(context);\n+    Catalog icebergCatalog = CATALOG_FACTORY.buildIcebergCatalog(fullTableName, options, conf.get());\n+\n+    table = icebergCatalog.loadTable(TableIdentifier.parse(fullTableName));\n+    maxCommittedCheckpointId = INITIAL_CHECKPOINT_ID;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cbb6ea209b2a63c91d7b179878d7841a2b56086f"}, "originalPosition": 113}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzczMzI5NA==", "bodyText": "Maybe just maxCommittedCheckpointId != INIT_ID?", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r473733294", "createdAt": "2020-08-20T08:00:52Z", "author": {"login": "JingsongLi"}, "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergFilesCommitter.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NavigableMap;\n+import java.util.SortedMap;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.BasicTypeInfo;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.ListTypeInfo;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.BoundedOneInput;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.runtime.typeutils.SortedMapTypeInfo;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.types.Comparators;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+class IcebergFilesCommitter extends AbstractStreamOperator<Void>\n+    implements OneInputStreamOperator<DataFile, Void>, BoundedOneInput {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final long INITIAL_CHECKPOINT_ID = -1L;\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergFilesCommitter.class);\n+  private static final String GLOBAL_FILES_COMMITTER_UID = \"flink.files-committer.uid\";\n+  private static final String MAX_COMMITTED_CHECKPOINT_ID = \"flink.max-committed.checkpoint.id\";\n+\n+  private static final FlinkCatalogFactory CATALOG_FACTORY = new FlinkCatalogFactory();\n+\n+  // It will have an unique identifier for one job.\n+  private final String filesCommitterUid;\n+  private final String fullTableName;\n+  private final SerializableConfiguration conf;\n+  private final ImmutableMap<String, String> options;\n+\n+  // The max checkpoint id we've committed to iceberg table. As the flink's checkpoint is always increasing, so we could\n+  // correctly commit all the data files whose checkpoint id is greater than the max committed one to iceberg table, for\n+  // avoiding committing the same data files twice. This id will be attached to iceberg's meta when committing the\n+  // iceberg transaction.\n+  private transient long maxCommittedCheckpointId;\n+\n+  // A sorted map to maintain the completed data files for each pending checkpointId (which have not been committed\n+  // to iceberg table). We need a sorted map here because there's possible that few checkpoints snapshot failed, for\n+  // example: the 1st checkpoint have 2 data files <1, <file0, file1>>, the 2st checkpoint have 1 data files\n+  // <2, <file3>>. Snapshot for checkpoint#1 interrupted because of network/disk failure etc, while we don't expect\n+  // any data loss in iceberg table. So we keep the finished files <1, <file0, file1>> in memory and retry to commit\n+  // iceberg table when the next checkpoint happen.\n+  private final NavigableMap<Long, List<DataFile>> dataFilesPerCheckpoint = Maps.newTreeMap();\n+\n+  // The data files cache for current checkpoint. Once the snapshot barrier received, it will be flushed to the\n+  // 'dataFilesPerCheckpoint'.\n+  private final List<DataFile> dataFilesOfCurrentCheckpoint = Lists.newArrayList();\n+  private transient Table table;\n+\n+  // All pending checkpoints states for this function.\n+  private static final ListStateDescriptor<SortedMap<Long, List<DataFile>>> STATE_DESCRIPTOR = buildStateDescriptor();\n+\n+  private transient ListState<SortedMap<Long, List<DataFile>>> checkpointsState;\n+\n+  IcebergFilesCommitter(String filesCommitterUid, String fullTableName,\n+                        Map<String, String> options, Configuration conf) {\n+    this.filesCommitterUid = filesCommitterUid;\n+    this.fullTableName = fullTableName;\n+    this.options = ImmutableMap.copyOf(options);\n+    this.conf = new SerializableConfiguration(conf);\n+  }\n+\n+  @Override\n+  public void initializeState(StateInitializationContext context) throws Exception {\n+    super.initializeState(context);\n+    Catalog icebergCatalog = CATALOG_FACTORY.buildIcebergCatalog(fullTableName, options, conf.get());\n+\n+    table = icebergCatalog.loadTable(TableIdentifier.parse(fullTableName));\n+    maxCommittedCheckpointId = INITIAL_CHECKPOINT_ID;\n+\n+    checkpointsState = context.getOperatorStateStore().getListState(STATE_DESCRIPTOR);\n+    if (context.isRestored()) {\n+      maxCommittedCheckpointId = getMaxCommittedCheckpointId(table, filesCommitterUid);\n+      // In the restoring path, it should have one valid snapshot for current flink job at least, so the max committed\n+      // checkpoint id should be positive. If it's not positive, that means someone might have removed or expired the\n+      // iceberg snapshot, in that case we should throw an exception in case of committing duplicated data files into\n+      // the iceberg table.\n+      Preconditions.checkArgument(maxCommittedCheckpointId > 0,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cbb6ea209b2a63c91d7b179878d7841a2b56086f"}, "originalPosition": 122}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzczNjc0MA==", "bodyText": "checkpointsState.add(dataFilesPerCheckpoint);", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r473736740", "createdAt": "2020-08-20T08:03:42Z", "author": {"login": "JingsongLi"}, "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergFilesCommitter.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NavigableMap;\n+import java.util.SortedMap;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.BasicTypeInfo;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.ListTypeInfo;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.BoundedOneInput;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.runtime.typeutils.SortedMapTypeInfo;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.types.Comparators;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+class IcebergFilesCommitter extends AbstractStreamOperator<Void>\n+    implements OneInputStreamOperator<DataFile, Void>, BoundedOneInput {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final long INITIAL_CHECKPOINT_ID = -1L;\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergFilesCommitter.class);\n+  private static final String GLOBAL_FILES_COMMITTER_UID = \"flink.files-committer.uid\";\n+  private static final String MAX_COMMITTED_CHECKPOINT_ID = \"flink.max-committed.checkpoint.id\";\n+\n+  private static final FlinkCatalogFactory CATALOG_FACTORY = new FlinkCatalogFactory();\n+\n+  // It will have an unique identifier for one job.\n+  private final String filesCommitterUid;\n+  private final String fullTableName;\n+  private final SerializableConfiguration conf;\n+  private final ImmutableMap<String, String> options;\n+\n+  // The max checkpoint id we've committed to iceberg table. As the flink's checkpoint is always increasing, so we could\n+  // correctly commit all the data files whose checkpoint id is greater than the max committed one to iceberg table, for\n+  // avoiding committing the same data files twice. This id will be attached to iceberg's meta when committing the\n+  // iceberg transaction.\n+  private transient long maxCommittedCheckpointId;\n+\n+  // A sorted map to maintain the completed data files for each pending checkpointId (which have not been committed\n+  // to iceberg table). We need a sorted map here because there's possible that few checkpoints snapshot failed, for\n+  // example: the 1st checkpoint have 2 data files <1, <file0, file1>>, the 2st checkpoint have 1 data files\n+  // <2, <file3>>. Snapshot for checkpoint#1 interrupted because of network/disk failure etc, while we don't expect\n+  // any data loss in iceberg table. So we keep the finished files <1, <file0, file1>> in memory and retry to commit\n+  // iceberg table when the next checkpoint happen.\n+  private final NavigableMap<Long, List<DataFile>> dataFilesPerCheckpoint = Maps.newTreeMap();\n+\n+  // The data files cache for current checkpoint. Once the snapshot barrier received, it will be flushed to the\n+  // 'dataFilesPerCheckpoint'.\n+  private final List<DataFile> dataFilesOfCurrentCheckpoint = Lists.newArrayList();\n+  private transient Table table;\n+\n+  // All pending checkpoints states for this function.\n+  private static final ListStateDescriptor<SortedMap<Long, List<DataFile>>> STATE_DESCRIPTOR = buildStateDescriptor();\n+\n+  private transient ListState<SortedMap<Long, List<DataFile>>> checkpointsState;\n+\n+  IcebergFilesCommitter(String filesCommitterUid, String fullTableName,\n+                        Map<String, String> options, Configuration conf) {\n+    this.filesCommitterUid = filesCommitterUid;\n+    this.fullTableName = fullTableName;\n+    this.options = ImmutableMap.copyOf(options);\n+    this.conf = new SerializableConfiguration(conf);\n+  }\n+\n+  @Override\n+  public void initializeState(StateInitializationContext context) throws Exception {\n+    super.initializeState(context);\n+    Catalog icebergCatalog = CATALOG_FACTORY.buildIcebergCatalog(fullTableName, options, conf.get());\n+\n+    table = icebergCatalog.loadTable(TableIdentifier.parse(fullTableName));\n+    maxCommittedCheckpointId = INITIAL_CHECKPOINT_ID;\n+\n+    checkpointsState = context.getOperatorStateStore().getListState(STATE_DESCRIPTOR);\n+    if (context.isRestored()) {\n+      maxCommittedCheckpointId = getMaxCommittedCheckpointId(table, filesCommitterUid);\n+      // In the restoring path, it should have one valid snapshot for current flink job at least, so the max committed\n+      // checkpoint id should be positive. If it's not positive, that means someone might have removed or expired the\n+      // iceberg snapshot, in that case we should throw an exception in case of committing duplicated data files into\n+      // the iceberg table.\n+      Preconditions.checkArgument(maxCommittedCheckpointId > 0,\n+          \"There should be an existing iceberg snapshot for current flink job: %s\", filesCommitterUid);\n+\n+      SortedMap<Long, List<DataFile>> restoredDataFiles = checkpointsState.get().iterator().next();\n+      // Only keep the uncommitted data files in the cache.\n+      dataFilesPerCheckpoint.putAll(restoredDataFiles.tailMap(maxCommittedCheckpointId + 1));\n+    }\n+  }\n+\n+  @Override\n+  public void snapshotState(StateSnapshotContext context) throws Exception {\n+    super.snapshotState(context);\n+    long checkpointId = context.getCheckpointId();\n+    LOG.info(\"Start to flush snapshot state to state backend, table: {}, checkpointId: {}\", table, checkpointId);\n+\n+    // Update the checkpoint state.\n+    dataFilesPerCheckpoint.put(checkpointId, ImmutableList.copyOf(dataFilesOfCurrentCheckpoint));\n+\n+    // Reset the snapshot state to the latest state.\n+    checkpointsState.clear();\n+    checkpointsState.addAll(ImmutableList.of(dataFilesPerCheckpoint));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cbb6ea209b2a63c91d7b179878d7841a2b56086f"}, "originalPosition": 142}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mzc0MDQ3Mg==", "bodyText": "Maybe we should commit even pendingDataFiles is empty for MAX_COMMITTED_CHECKPOINT_ID.", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r473740472", "createdAt": "2020-08-20T08:06:51Z", "author": {"login": "JingsongLi"}, "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergFilesCommitter.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NavigableMap;\n+import java.util.SortedMap;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.BasicTypeInfo;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.ListTypeInfo;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.BoundedOneInput;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.runtime.typeutils.SortedMapTypeInfo;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.types.Comparators;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+class IcebergFilesCommitter extends AbstractStreamOperator<Void>\n+    implements OneInputStreamOperator<DataFile, Void>, BoundedOneInput {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final long INITIAL_CHECKPOINT_ID = -1L;\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergFilesCommitter.class);\n+  private static final String GLOBAL_FILES_COMMITTER_UID = \"flink.files-committer.uid\";\n+  private static final String MAX_COMMITTED_CHECKPOINT_ID = \"flink.max-committed.checkpoint.id\";\n+\n+  private static final FlinkCatalogFactory CATALOG_FACTORY = new FlinkCatalogFactory();\n+\n+  // It will have an unique identifier for one job.\n+  private final String filesCommitterUid;\n+  private final String fullTableName;\n+  private final SerializableConfiguration conf;\n+  private final ImmutableMap<String, String> options;\n+\n+  // The max checkpoint id we've committed to iceberg table. As the flink's checkpoint is always increasing, so we could\n+  // correctly commit all the data files whose checkpoint id is greater than the max committed one to iceberg table, for\n+  // avoiding committing the same data files twice. This id will be attached to iceberg's meta when committing the\n+  // iceberg transaction.\n+  private transient long maxCommittedCheckpointId;\n+\n+  // A sorted map to maintain the completed data files for each pending checkpointId (which have not been committed\n+  // to iceberg table). We need a sorted map here because there's possible that few checkpoints snapshot failed, for\n+  // example: the 1st checkpoint have 2 data files <1, <file0, file1>>, the 2st checkpoint have 1 data files\n+  // <2, <file3>>. Snapshot for checkpoint#1 interrupted because of network/disk failure etc, while we don't expect\n+  // any data loss in iceberg table. So we keep the finished files <1, <file0, file1>> in memory and retry to commit\n+  // iceberg table when the next checkpoint happen.\n+  private final NavigableMap<Long, List<DataFile>> dataFilesPerCheckpoint = Maps.newTreeMap();\n+\n+  // The data files cache for current checkpoint. Once the snapshot barrier received, it will be flushed to the\n+  // 'dataFilesPerCheckpoint'.\n+  private final List<DataFile> dataFilesOfCurrentCheckpoint = Lists.newArrayList();\n+  private transient Table table;\n+\n+  // All pending checkpoints states for this function.\n+  private static final ListStateDescriptor<SortedMap<Long, List<DataFile>>> STATE_DESCRIPTOR = buildStateDescriptor();\n+\n+  private transient ListState<SortedMap<Long, List<DataFile>>> checkpointsState;\n+\n+  IcebergFilesCommitter(String filesCommitterUid, String fullTableName,\n+                        Map<String, String> options, Configuration conf) {\n+    this.filesCommitterUid = filesCommitterUid;\n+    this.fullTableName = fullTableName;\n+    this.options = ImmutableMap.copyOf(options);\n+    this.conf = new SerializableConfiguration(conf);\n+  }\n+\n+  @Override\n+  public void initializeState(StateInitializationContext context) throws Exception {\n+    super.initializeState(context);\n+    Catalog icebergCatalog = CATALOG_FACTORY.buildIcebergCatalog(fullTableName, options, conf.get());\n+\n+    table = icebergCatalog.loadTable(TableIdentifier.parse(fullTableName));\n+    maxCommittedCheckpointId = INITIAL_CHECKPOINT_ID;\n+\n+    checkpointsState = context.getOperatorStateStore().getListState(STATE_DESCRIPTOR);\n+    if (context.isRestored()) {\n+      maxCommittedCheckpointId = getMaxCommittedCheckpointId(table, filesCommitterUid);\n+      // In the restoring path, it should have one valid snapshot for current flink job at least, so the max committed\n+      // checkpoint id should be positive. If it's not positive, that means someone might have removed or expired the\n+      // iceberg snapshot, in that case we should throw an exception in case of committing duplicated data files into\n+      // the iceberg table.\n+      Preconditions.checkArgument(maxCommittedCheckpointId > 0,\n+          \"There should be an existing iceberg snapshot for current flink job: %s\", filesCommitterUid);\n+\n+      SortedMap<Long, List<DataFile>> restoredDataFiles = checkpointsState.get().iterator().next();\n+      // Only keep the uncommitted data files in the cache.\n+      dataFilesPerCheckpoint.putAll(restoredDataFiles.tailMap(maxCommittedCheckpointId + 1));\n+    }\n+  }\n+\n+  @Override\n+  public void snapshotState(StateSnapshotContext context) throws Exception {\n+    super.snapshotState(context);\n+    long checkpointId = context.getCheckpointId();\n+    LOG.info(\"Start to flush snapshot state to state backend, table: {}, checkpointId: {}\", table, checkpointId);\n+\n+    // Update the checkpoint state.\n+    dataFilesPerCheckpoint.put(checkpointId, ImmutableList.copyOf(dataFilesOfCurrentCheckpoint));\n+\n+    // Reset the snapshot state to the latest state.\n+    checkpointsState.clear();\n+    checkpointsState.addAll(ImmutableList.of(dataFilesPerCheckpoint));\n+\n+    // Clear the local buffer for current checkpoint.\n+    dataFilesOfCurrentCheckpoint.clear();\n+  }\n+\n+  @Override\n+  public void notifyCheckpointComplete(long checkpointId) throws Exception {\n+    super.notifyCheckpointComplete(checkpointId);\n+    commitUpToCheckpoint(checkpointId);\n+  }\n+\n+  private void commitUpToCheckpoint(long checkpointId) {\n+    NavigableMap<Long, List<DataFile>> pendingFileMap = dataFilesPerCheckpoint.headMap(checkpointId, true);\n+\n+    List<DataFile> pendingDataFiles = Lists.newArrayList();\n+    for (List<DataFile> dataFiles : pendingFileMap.values()) {\n+      pendingDataFiles.addAll(dataFiles);\n+    }\n+\n+    if (!pendingDataFiles.isEmpty()) {\n+      AppendFiles appendFiles = table.newAppend();\n+      pendingDataFiles.forEach(appendFiles::appendFile);\n+      appendFiles.set(MAX_COMMITTED_CHECKPOINT_ID, Long.toString(checkpointId));\n+      appendFiles.set(GLOBAL_FILES_COMMITTER_UID, filesCommitterUid);\n+      appendFiles.commit();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cbb6ea209b2a63c91d7b179878d7841a2b56086f"}, "originalPosition": 167}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mzc0MTQ3OA==", "bodyText": "Maybe should be checkState.", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r473741478", "createdAt": "2020-08-20T08:08:04Z", "author": {"login": "JingsongLi"}, "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergFilesCommitter.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NavigableMap;\n+import java.util.SortedMap;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.BasicTypeInfo;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.ListTypeInfo;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.BoundedOneInput;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.runtime.typeutils.SortedMapTypeInfo;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.types.Comparators;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+class IcebergFilesCommitter extends AbstractStreamOperator<Void>\n+    implements OneInputStreamOperator<DataFile, Void>, BoundedOneInput {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final long INITIAL_CHECKPOINT_ID = -1L;\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergFilesCommitter.class);\n+  private static final String GLOBAL_FILES_COMMITTER_UID = \"flink.files-committer.uid\";\n+  private static final String MAX_COMMITTED_CHECKPOINT_ID = \"flink.max-committed.checkpoint.id\";\n+\n+  private static final FlinkCatalogFactory CATALOG_FACTORY = new FlinkCatalogFactory();\n+\n+  // It will have an unique identifier for one job.\n+  private final String filesCommitterUid;\n+  private final String fullTableName;\n+  private final SerializableConfiguration conf;\n+  private final ImmutableMap<String, String> options;\n+\n+  // The max checkpoint id we've committed to iceberg table. As the flink's checkpoint is always increasing, so we could\n+  // correctly commit all the data files whose checkpoint id is greater than the max committed one to iceberg table, for\n+  // avoiding committing the same data files twice. This id will be attached to iceberg's meta when committing the\n+  // iceberg transaction.\n+  private transient long maxCommittedCheckpointId;\n+\n+  // A sorted map to maintain the completed data files for each pending checkpointId (which have not been committed\n+  // to iceberg table). We need a sorted map here because there's possible that few checkpoints snapshot failed, for\n+  // example: the 1st checkpoint have 2 data files <1, <file0, file1>>, the 2st checkpoint have 1 data files\n+  // <2, <file3>>. Snapshot for checkpoint#1 interrupted because of network/disk failure etc, while we don't expect\n+  // any data loss in iceberg table. So we keep the finished files <1, <file0, file1>> in memory and retry to commit\n+  // iceberg table when the next checkpoint happen.\n+  private final NavigableMap<Long, List<DataFile>> dataFilesPerCheckpoint = Maps.newTreeMap();\n+\n+  // The data files cache for current checkpoint. Once the snapshot barrier received, it will be flushed to the\n+  // 'dataFilesPerCheckpoint'.\n+  private final List<DataFile> dataFilesOfCurrentCheckpoint = Lists.newArrayList();\n+  private transient Table table;\n+\n+  // All pending checkpoints states for this function.\n+  private static final ListStateDescriptor<SortedMap<Long, List<DataFile>>> STATE_DESCRIPTOR = buildStateDescriptor();\n+\n+  private transient ListState<SortedMap<Long, List<DataFile>>> checkpointsState;\n+\n+  IcebergFilesCommitter(String filesCommitterUid, String fullTableName,\n+                        Map<String, String> options, Configuration conf) {\n+    this.filesCommitterUid = filesCommitterUid;\n+    this.fullTableName = fullTableName;\n+    this.options = ImmutableMap.copyOf(options);\n+    this.conf = new SerializableConfiguration(conf);\n+  }\n+\n+  @Override\n+  public void initializeState(StateInitializationContext context) throws Exception {\n+    super.initializeState(context);\n+    Catalog icebergCatalog = CATALOG_FACTORY.buildIcebergCatalog(fullTableName, options, conf.get());\n+\n+    table = icebergCatalog.loadTable(TableIdentifier.parse(fullTableName));\n+    maxCommittedCheckpointId = INITIAL_CHECKPOINT_ID;\n+\n+    checkpointsState = context.getOperatorStateStore().getListState(STATE_DESCRIPTOR);\n+    if (context.isRestored()) {\n+      maxCommittedCheckpointId = getMaxCommittedCheckpointId(table, filesCommitterUid);\n+      // In the restoring path, it should have one valid snapshot for current flink job at least, so the max committed\n+      // checkpoint id should be positive. If it's not positive, that means someone might have removed or expired the\n+      // iceberg snapshot, in that case we should throw an exception in case of committing duplicated data files into\n+      // the iceberg table.\n+      Preconditions.checkArgument(maxCommittedCheckpointId > 0,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cbb6ea209b2a63c91d7b179878d7841a2b56086f"}, "originalPosition": 122}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mzc0MzQxNA==", "bodyText": "checkpoint-id?", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r473743414", "createdAt": "2020-08-20T08:10:26Z", "author": {"login": "JingsongLi"}, "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergFilesCommitter.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NavigableMap;\n+import java.util.SortedMap;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.BasicTypeInfo;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.ListTypeInfo;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.BoundedOneInput;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.runtime.typeutils.SortedMapTypeInfo;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.types.Comparators;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+class IcebergFilesCommitter extends AbstractStreamOperator<Void>\n+    implements OneInputStreamOperator<DataFile, Void>, BoundedOneInput {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final long INITIAL_CHECKPOINT_ID = -1L;\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergFilesCommitter.class);\n+  private static final String GLOBAL_FILES_COMMITTER_UID = \"flink.files-committer.uid\";\n+  private static final String MAX_COMMITTED_CHECKPOINT_ID = \"flink.max-committed.checkpoint.id\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cbb6ea209b2a63c91d7b179878d7841a2b56086f"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mzc0MzU2NA==", "bodyText": "uid -> job-id?", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r473743564", "createdAt": "2020-08-20T08:10:37Z", "author": {"login": "JingsongLi"}, "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergFilesCommitter.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NavigableMap;\n+import java.util.SortedMap;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.BasicTypeInfo;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.ListTypeInfo;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.BoundedOneInput;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.runtime.typeutils.SortedMapTypeInfo;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.types.Comparators;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+class IcebergFilesCommitter extends AbstractStreamOperator<Void>\n+    implements OneInputStreamOperator<DataFile, Void>, BoundedOneInput {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final long INITIAL_CHECKPOINT_ID = -1L;\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergFilesCommitter.class);\n+  private static final String GLOBAL_FILES_COMMITTER_UID = \"flink.files-committer.uid\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cbb6ea209b2a63c91d7b179878d7841a2b56086f"}, "originalPosition": 64}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mzc0NDM1NQ==", "bodyText": "Please name the sink.", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r473744355", "createdAt": "2020-08-20T08:11:38Z", "author": {"login": "JingsongLi"}, "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergSinkUtil.java", "diffHunk": "@@ -37,9 +45,32 @@\n import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT;\n \n class IcebergSinkUtil {\n+\n   private IcebergSinkUtil() {\n   }\n \n+  @SuppressWarnings(\"unchecked\")\n+  static DataStreamSink<RowData> write(DataStream<RowData> inputStream,\n+                                       Map<String, String> options,\n+                                       Configuration conf,\n+                                       String fullTableName,\n+                                       Table table,\n+                                       TableSchema requestedSchema) {\n+    IcebergStreamWriter<RowData> streamWriter = createStreamWriter(table, requestedSchema);\n+\n+    String filesCommitterUID = String.format(\"IcebergFilesCommitter-%s\", UUID.randomUUID().toString());\n+    IcebergFilesCommitter filesCommitter = new IcebergFilesCommitter(filesCommitterUID, fullTableName, options, conf);\n+\n+    DataStream<Void> returnStream = inputStream\n+        .transform(IcebergStreamWriter.class.getSimpleName(), TypeInformation.of(DataFile.class), streamWriter)\n+        .setParallelism(inputStream.getParallelism())\n+        .transform(IcebergFilesCommitter.class.getSimpleName(), Types.VOID, filesCommitter)\n+        .setParallelism(1)\n+        .setMaxParallelism(1);\n+\n+    return returnStream.addSink(new DiscardingSink()).setParallelism(1);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cbb6ea209b2a63c91d7b179878d7841a2b56086f"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mzc0NjE2OQ==", "bodyText": "I just notice IcebergStreamWriter does not set chaining strategy. Should add in the constructor of IcebergStreamWriter: setChainingStrategy(ChainingStrategy.ALWAYS);", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r473746169", "createdAt": "2020-08-20T08:13:44Z", "author": {"login": "JingsongLi"}, "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergSinkUtil.java", "diffHunk": "@@ -37,9 +45,32 @@\n import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT;\n \n class IcebergSinkUtil {\n+\n   private IcebergSinkUtil() {\n   }\n \n+  @SuppressWarnings(\"unchecked\")\n+  static DataStreamSink<RowData> write(DataStream<RowData> inputStream,\n+                                       Map<String, String> options,\n+                                       Configuration conf,\n+                                       String fullTableName,\n+                                       Table table,\n+                                       TableSchema requestedSchema) {\n+    IcebergStreamWriter<RowData> streamWriter = createStreamWriter(table, requestedSchema);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cbb6ea209b2a63c91d7b179878d7841a2b56086f"}, "originalPosition": 33}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "2399de899b52e1beb5cf91cc51598cd79a3ab80c", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/2399de899b52e1beb5cf91cc51598cd79a3ab80c", "committedDate": "2020-08-20T12:27:23Z", "message": "Minor fixes"}, "afterCommit": {"oid": "20196f8100589c3d3f4a2cce0a740e4374608437", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/20196f8100589c3d3f4a2cce0a740e4374608437", "committedDate": "2020-08-21T03:06:02Z", "message": "Use the TableLoader to load table lazily inside icebergFilesCommitter"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDczMTA1MDUy", "url": "https://github.com/apache/iceberg/pull/1185#pullrequestreview-473105052", "createdAt": "2020-08-24T03:58:43Z", "commit": {"oid": "3fdffd7f08f236b5fc36278dd79bbdc92b05ccdf"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNFQwMzo1ODo0M1rOHFTzfw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNFQwNDowOTo0NVrOHFT82A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTMyOTQwNw==", "bodyText": "NIT:  I think these fields not have to be static.", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r475329407", "createdAt": "2020-08-24T03:58:43Z", "author": {"login": "JingsongLi"}, "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkSink.java", "diffHunk": "@@ -0,0 +1,191 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeinfo.Types;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.DataStreamSink;\n+import org.apache.flink.streaming.api.functions.sink.DiscardingSink;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.util.DataFormatConverters;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.utils.TypeConversions;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.PropertyUtil;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT_DEFAULT;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT;\n+\n+public class FlinkSink {\n+\n+  private static final TypeInformation<DataFile> DATA_FILE_TYPE_INFO = TypeInformation.of(DataFile.class);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3fdffd7f08f236b5fc36278dd79bbdc92b05ccdf"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTMzMDY0OA==", "bodyText": "Actually, the default value of maxCommittedCheckpointId can be null instead of INITIAL_CHECKPOINT_ID.", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r475330648", "createdAt": "2020-08-24T04:04:34Z", "author": {"login": "JingsongLi"}, "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergFilesCommitter.java", "diffHunk": "@@ -0,0 +1,218 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NavigableMap;\n+import java.util.SortedMap;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.BasicTypeInfo;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.ListTypeInfo;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.BoundedOneInput;\n+import org.apache.flink.streaming.api.operators.ChainingStrategy;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.runtime.typeutils.SortedMapTypeInfo;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.types.Comparators;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+class IcebergFilesCommitter extends AbstractStreamOperator<Void>\n+    implements OneInputStreamOperator<DataFile, Void>, BoundedOneInput {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final long INITIAL_CHECKPOINT_ID = -1L;\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergFilesCommitter.class);\n+  private static final String FLINK_JOB_ID = \"flink.job-id\";\n+\n+  // The max checkpoint id we've committed to iceberg table. As the flink's checkpoint is always increasing, so we could\n+  // correctly commit all the data files whose checkpoint id is greater than the max committed one to iceberg table, for\n+  // avoiding committing the same data files twice. This id will be attached to iceberg's meta when committing the\n+  // iceberg transaction.\n+  private static final String MAX_COMMITTED_CHECKPOINT_ID = \"flink.max-committed-checkpoint-id\";\n+\n+  // TableLoader to load iceberg table lazily.\n+  private final TableLoader tableLoader;\n+  private final SerializableConfiguration hadoopConf;\n+\n+  // A sorted map to maintain the completed data files for each pending checkpointId (which have not been committed\n+  // to iceberg table). We need a sorted map here because there's possible that few checkpoints snapshot failed, for\n+  // example: the 1st checkpoint have 2 data files <1, <file0, file1>>, the 2st checkpoint have 1 data files\n+  // <2, <file3>>. Snapshot for checkpoint#1 interrupted because of network/disk failure etc, while we don't expect\n+  // any data loss in iceberg table. So we keep the finished files <1, <file0, file1>> in memory and retry to commit\n+  // iceberg table when the next checkpoint happen.\n+  private final NavigableMap<Long, List<DataFile>> dataFilesPerCheckpoint = Maps.newTreeMap();\n+\n+  // The data files cache for current checkpoint. Once the snapshot barrier received, it will be flushed to the\n+  // 'dataFilesPerCheckpoint'.\n+  private final List<DataFile> dataFilesOfCurrentCheckpoint = Lists.newArrayList();\n+\n+  // It will have an unique identifier for one job.\n+  private transient String flinkJobId;\n+  private transient Table table;\n+  private transient long maxCommittedCheckpointId;\n+\n+  // All pending checkpoints states for this function.\n+  private static final ListStateDescriptor<SortedMap<Long, List<DataFile>>> STATE_DESCRIPTOR = buildStateDescriptor();\n+  private transient ListState<SortedMap<Long, List<DataFile>>> checkpointsState;\n+\n+  IcebergFilesCommitter(TableLoader tableLoader, Configuration hadoopConf) {\n+    this.tableLoader = tableLoader;\n+    this.hadoopConf = new SerializableConfiguration(hadoopConf);\n+    setChainingStrategy(ChainingStrategy.ALWAYS);\n+  }\n+\n+  @Override\n+  public void initializeState(StateInitializationContext context) throws Exception {\n+    super.initializeState(context);\n+    flinkJobId = getContainingTask().getEnvironment().getJobID().toString();\n+\n+    // Open the table loader and load the table.\n+    tableLoader.open(hadoopConf.get());\n+    table = tableLoader.loadTable();\n+\n+    checkpointsState = context.getOperatorStateStore().getListState(STATE_DESCRIPTOR);\n+    if (context.isRestored()) {\n+      maxCommittedCheckpointId = getMaxCommittedCheckpointId(table, flinkJobId);\n+      // In the restoring path, it should have one valid snapshot for current flink job at least, so the max committed\n+      // checkpoint id should be positive. If it's not positive, that means someone might have removed or expired the\n+      // iceberg snapshot, in that case we should throw an exception in case of committing duplicated data files into\n+      // the iceberg table.\n+      Preconditions.checkState(maxCommittedCheckpointId != INITIAL_CHECKPOINT_ID,\n+          \"There should be an existing iceberg snapshot for current flink job: %s\", flinkJobId);\n+\n+      SortedMap<Long, List<DataFile>> restoredDataFiles = checkpointsState.get().iterator().next();\n+      // Only keep the uncommitted data files in the cache.\n+      dataFilesPerCheckpoint.putAll(restoredDataFiles.tailMap(maxCommittedCheckpointId + 1));\n+    }\n+  }\n+\n+  @Override\n+  public void snapshotState(StateSnapshotContext context) throws Exception {\n+    super.snapshotState(context);\n+    long checkpointId = context.getCheckpointId();\n+    LOG.info(\"Start to flush snapshot state to state backend, table: {}, checkpointId: {}\", table, checkpointId);\n+\n+    // Update the checkpoint state.\n+    dataFilesPerCheckpoint.put(checkpointId, ImmutableList.copyOf(dataFilesOfCurrentCheckpoint));\n+\n+    // Reset the snapshot state to the latest state.\n+    checkpointsState.clear();\n+    checkpointsState.add(dataFilesPerCheckpoint);\n+\n+    // Clear the local buffer for current checkpoint.\n+    dataFilesOfCurrentCheckpoint.clear();\n+  }\n+\n+  @Override\n+  public void notifyCheckpointComplete(long checkpointId) throws Exception {\n+    super.notifyCheckpointComplete(checkpointId);\n+    // It's possible that we have the following events:\n+    //   1. snapshotState(ckpId);\n+    //   2. snapshotState(ckpId+1);\n+    //   3. notifyCheckpointComplete(ckpId+1);\n+    //   4. notifyCheckpointComplete(ckpId);\n+    // For step#4, we don't need to commit iceberg table again because in step#3 we've committed all the files,\n+    // Besides, we need to maintain the max-committed-checkpoint-id to be increasing.\n+    if (checkpointId > maxCommittedCheckpointId) {\n+      commitUpToCheckpoint(checkpointId);\n+      maxCommittedCheckpointId = checkpointId;\n+    }\n+  }\n+\n+  private void commitUpToCheckpoint(long checkpointId) {\n+    NavigableMap<Long, List<DataFile>> pendingFileMap = dataFilesPerCheckpoint.headMap(checkpointId, true);\n+\n+    List<DataFile> pendingDataFiles = Lists.newArrayList();\n+    for (List<DataFile> dataFiles : pendingFileMap.values()) {\n+      pendingDataFiles.addAll(dataFiles);\n+    }\n+\n+    AppendFiles appendFiles = table.newAppend();\n+    pendingDataFiles.forEach(appendFiles::appendFile);\n+    appendFiles.set(MAX_COMMITTED_CHECKPOINT_ID, Long.toString(checkpointId));\n+    appendFiles.set(FLINK_JOB_ID, flinkJobId);\n+    appendFiles.commit();\n+\n+    // Clear the committed data files from dataFilesPerCheckpoint.\n+    pendingFileMap.clear();\n+  }\n+\n+  @Override\n+  public void processElement(StreamRecord<DataFile> element) {\n+    this.dataFilesOfCurrentCheckpoint.add(element.getValue());\n+  }\n+\n+  @Override\n+  public void endInput() {\n+    commitUpToCheckpoint(Long.MAX_VALUE);\n+  }\n+\n+  private static ListStateDescriptor<SortedMap<Long, List<DataFile>>> buildStateDescriptor() {\n+    Comparator<Long> longComparator = Comparators.forType(Types.LongType.get());\n+    // Construct a ListTypeInfo.\n+    ListTypeInfo<DataFile> dataFileListTypeInfo = new ListTypeInfo<>(TypeInformation.of(DataFile.class));\n+    // Construct a SortedMapTypeInfo.\n+    SortedMapTypeInfo<Long, List<DataFile>> sortedMapTypeInfo = new SortedMapTypeInfo<>(\n+        BasicTypeInfo.LONG_TYPE_INFO, dataFileListTypeInfo, longComparator\n+    );\n+    return new ListStateDescriptor<>(\"iceberg-files-committer-state\", sortedMapTypeInfo);\n+  }\n+\n+  static Long getMaxCommittedCheckpointId(Table table, String flinkJobId) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3fdffd7f08f236b5fc36278dd79bbdc92b05ccdf"}, "originalPosition": 198}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTMzMTYwNQ==", "bodyText": "We can let committer be a single node.", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r475331605", "createdAt": "2020-08-24T04:08:58Z", "author": {"login": "JingsongLi"}, "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergFilesCommitter.java", "diffHunk": "@@ -0,0 +1,218 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NavigableMap;\n+import java.util.SortedMap;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.BasicTypeInfo;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.ListTypeInfo;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.BoundedOneInput;\n+import org.apache.flink.streaming.api.operators.ChainingStrategy;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.runtime.typeutils.SortedMapTypeInfo;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.types.Comparators;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+class IcebergFilesCommitter extends AbstractStreamOperator<Void>\n+    implements OneInputStreamOperator<DataFile, Void>, BoundedOneInput {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final long INITIAL_CHECKPOINT_ID = -1L;\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergFilesCommitter.class);\n+  private static final String FLINK_JOB_ID = \"flink.job-id\";\n+\n+  // The max checkpoint id we've committed to iceberg table. As the flink's checkpoint is always increasing, so we could\n+  // correctly commit all the data files whose checkpoint id is greater than the max committed one to iceberg table, for\n+  // avoiding committing the same data files twice. This id will be attached to iceberg's meta when committing the\n+  // iceberg transaction.\n+  private static final String MAX_COMMITTED_CHECKPOINT_ID = \"flink.max-committed-checkpoint-id\";\n+\n+  // TableLoader to load iceberg table lazily.\n+  private final TableLoader tableLoader;\n+  private final SerializableConfiguration hadoopConf;\n+\n+  // A sorted map to maintain the completed data files for each pending checkpointId (which have not been committed\n+  // to iceberg table). We need a sorted map here because there's possible that few checkpoints snapshot failed, for\n+  // example: the 1st checkpoint have 2 data files <1, <file0, file1>>, the 2st checkpoint have 1 data files\n+  // <2, <file3>>. Snapshot for checkpoint#1 interrupted because of network/disk failure etc, while we don't expect\n+  // any data loss in iceberg table. So we keep the finished files <1, <file0, file1>> in memory and retry to commit\n+  // iceberg table when the next checkpoint happen.\n+  private final NavigableMap<Long, List<DataFile>> dataFilesPerCheckpoint = Maps.newTreeMap();\n+\n+  // The data files cache for current checkpoint. Once the snapshot barrier received, it will be flushed to the\n+  // 'dataFilesPerCheckpoint'.\n+  private final List<DataFile> dataFilesOfCurrentCheckpoint = Lists.newArrayList();\n+\n+  // It will have an unique identifier for one job.\n+  private transient String flinkJobId;\n+  private transient Table table;\n+  private transient long maxCommittedCheckpointId;\n+\n+  // All pending checkpoints states for this function.\n+  private static final ListStateDescriptor<SortedMap<Long, List<DataFile>>> STATE_DESCRIPTOR = buildStateDescriptor();\n+  private transient ListState<SortedMap<Long, List<DataFile>>> checkpointsState;\n+\n+  IcebergFilesCommitter(TableLoader tableLoader, Configuration hadoopConf) {\n+    this.tableLoader = tableLoader;\n+    this.hadoopConf = new SerializableConfiguration(hadoopConf);\n+    setChainingStrategy(ChainingStrategy.ALWAYS);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3fdffd7f08f236b5fc36278dd79bbdc92b05ccdf"}, "originalPosition": 98}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTMzMTgwMA==", "bodyText": "Better to close this tableLoader in the dispose.", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r475331800", "createdAt": "2020-08-24T04:09:45Z", "author": {"login": "JingsongLi"}, "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergFilesCommitter.java", "diffHunk": "@@ -0,0 +1,218 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NavigableMap;\n+import java.util.SortedMap;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.BasicTypeInfo;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.ListTypeInfo;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.BoundedOneInput;\n+import org.apache.flink.streaming.api.operators.ChainingStrategy;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.runtime.typeutils.SortedMapTypeInfo;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.types.Comparators;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+class IcebergFilesCommitter extends AbstractStreamOperator<Void>\n+    implements OneInputStreamOperator<DataFile, Void>, BoundedOneInput {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final long INITIAL_CHECKPOINT_ID = -1L;\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergFilesCommitter.class);\n+  private static final String FLINK_JOB_ID = \"flink.job-id\";\n+\n+  // The max checkpoint id we've committed to iceberg table. As the flink's checkpoint is always increasing, so we could\n+  // correctly commit all the data files whose checkpoint id is greater than the max committed one to iceberg table, for\n+  // avoiding committing the same data files twice. This id will be attached to iceberg's meta when committing the\n+  // iceberg transaction.\n+  private static final String MAX_COMMITTED_CHECKPOINT_ID = \"flink.max-committed-checkpoint-id\";\n+\n+  // TableLoader to load iceberg table lazily.\n+  private final TableLoader tableLoader;\n+  private final SerializableConfiguration hadoopConf;\n+\n+  // A sorted map to maintain the completed data files for each pending checkpointId (which have not been committed\n+  // to iceberg table). We need a sorted map here because there's possible that few checkpoints snapshot failed, for\n+  // example: the 1st checkpoint have 2 data files <1, <file0, file1>>, the 2st checkpoint have 1 data files\n+  // <2, <file3>>. Snapshot for checkpoint#1 interrupted because of network/disk failure etc, while we don't expect\n+  // any data loss in iceberg table. So we keep the finished files <1, <file0, file1>> in memory and retry to commit\n+  // iceberg table when the next checkpoint happen.\n+  private final NavigableMap<Long, List<DataFile>> dataFilesPerCheckpoint = Maps.newTreeMap();\n+\n+  // The data files cache for current checkpoint. Once the snapshot barrier received, it will be flushed to the\n+  // 'dataFilesPerCheckpoint'.\n+  private final List<DataFile> dataFilesOfCurrentCheckpoint = Lists.newArrayList();\n+\n+  // It will have an unique identifier for one job.\n+  private transient String flinkJobId;\n+  private transient Table table;\n+  private transient long maxCommittedCheckpointId;\n+\n+  // All pending checkpoints states for this function.\n+  private static final ListStateDescriptor<SortedMap<Long, List<DataFile>>> STATE_DESCRIPTOR = buildStateDescriptor();\n+  private transient ListState<SortedMap<Long, List<DataFile>>> checkpointsState;\n+\n+  IcebergFilesCommitter(TableLoader tableLoader, Configuration hadoopConf) {\n+    this.tableLoader = tableLoader;\n+    this.hadoopConf = new SerializableConfiguration(hadoopConf);\n+    setChainingStrategy(ChainingStrategy.ALWAYS);\n+  }\n+\n+  @Override\n+  public void initializeState(StateInitializationContext context) throws Exception {\n+    super.initializeState(context);\n+    flinkJobId = getContainingTask().getEnvironment().getJobID().toString();\n+\n+    // Open the table loader and load the table.\n+    tableLoader.open(hadoopConf.get());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3fdffd7f08f236b5fc36278dd79bbdc92b05ccdf"}, "originalPosition": 107}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "3afd033630d4729aa7f7fb1b177332db4aa9e198", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/3afd033630d4729aa7f7fb1b177332db4aa9e198", "committedDate": "2020-08-24T06:40:54Z", "message": "Addressing the comments"}, "afterCommit": {"oid": "8d20dfe2774cd09d95b5edc7d48c3f528d06d408", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/8d20dfe2774cd09d95b5edc7d48c3f528d06d408", "committedDate": "2020-08-25T03:37:59Z", "message": "Add parquet to the unit tests."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc0Mjg3OTAw", "url": "https://github.com/apache/iceberg/pull/1185#pullrequestreview-474287900", "createdAt": "2020-08-25T09:09:21Z", "commit": {"oid": "8d20dfe2774cd09d95b5edc7d48c3f528d06d408"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNVQwOTowOToyMlrOHGO_wg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNVQwOTowOToyMlrOHGO_wg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjI5OTIwMg==", "bodyText": "Since we already have TableLoader, will it be easier for user if we load table here ?", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r476299202", "createdAt": "2020-08-25T09:09:22Z", "author": {"login": "jrthe42"}, "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkSink.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeinfo.Types;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.DataStreamSink;\n+import org.apache.flink.streaming.api.functions.sink.DiscardingSink;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.util.DataFormatConverters;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.utils.TypeConversions;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.PropertyUtil;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT_DEFAULT;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT;\n+\n+public class FlinkSink {\n+\n+  private static final String ICEBERG_STREAM_WRITER_NAME = IcebergStreamWriter.class.getSimpleName();\n+  private static final String ICEBERG_FILES_COMMITTER_NAME = IcebergFilesCommitter.class.getSimpleName();\n+\n+  private FlinkSink() {\n+  }\n+\n+  public static Builder forRow(DataStream<Row> input) {\n+    return new Builder().forRow(input);\n+  }\n+\n+  public static Builder forRowData(DataStream<RowData> input) {\n+    return new Builder().forRowData(input);\n+  }\n+\n+  public static class Builder {\n+    private DataStream<Row> rowInput = null;\n+    private DataStream<RowData> rowDataInput = null;\n+    private TableLoader tableLoader;\n+    private Configuration hadoopConf;\n+    private Table table;\n+    private TableSchema tableSchema;\n+\n+    private Builder forRow(DataStream<Row> newRowInput) {\n+      this.rowInput = newRowInput;\n+      return this;\n+    }\n+\n+    private Builder forRowData(DataStream<RowData> newRowDataInput) {\n+      this.rowDataInput = newRowDataInput;\n+      return this;\n+    }\n+\n+    public Builder table(Table newTable) {\n+      this.table = newTable;\n+      return this;\n+    }\n+\n+    public Builder tableLoader(TableLoader newTableLoader) {\n+      this.tableLoader = newTableLoader;\n+      return this;\n+    }\n+\n+    public Builder hadoopConf(Configuration newHadoopConf) {\n+      this.hadoopConf = newHadoopConf;\n+      return this;\n+    }\n+\n+    public Builder tableSchema(TableSchema newTableSchema) {\n+      this.tableSchema = newTableSchema;\n+      return this;\n+    }\n+\n+    private DataStream<RowData> convert() {\n+      Preconditions.checkArgument(rowInput != null, \"The DataStream<Row> to convert shouldn't be null\");\n+\n+      RowType rowType;\n+      DataType[] fieldDataTypes;\n+      if (tableSchema != null) {\n+        rowType = (RowType) tableSchema.toRowDataType().getLogicalType();\n+        fieldDataTypes = tableSchema.getFieldDataTypes();\n+      } else {\n+        rowType = FlinkSchemaUtil.convert(table.schema());\n+        fieldDataTypes = TypeConversions.fromLogicalToDataType(rowType.getChildren().toArray(new LogicalType[0]));\n+      }\n+\n+      DataFormatConverters.RowConverter rowConverter = new DataFormatConverters.RowConverter(fieldDataTypes);\n+\n+      return rowInput.map(rowConverter::toInternal, RowDataTypeInfo.of(rowType));\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    public DataStreamSink<RowData> build() {\n+      Preconditions.checkArgument(rowInput != null || rowDataInput != null,\n+          \"Should initialize input DataStream first with DataStream<Row> or DataStream<RowData>\");\n+      Preconditions.checkArgument(rowInput == null || rowDataInput == null,\n+          \"Could only initialize input DataStream with either DataStream<Row> or DataStream<RowData>\");\n+      Preconditions.checkNotNull(table, \"Table shouldn't be null\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8d20dfe2774cd09d95b5edc7d48c3f528d06d408"}, "originalPosition": 130}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9f238d699c0c9976b7003f561cfaabd0e8e1f266", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/9f238d699c0c9976b7003f561cfaabd0e8e1f266", "committedDate": "2020-08-27T02:09:35Z", "message": "Flink: Add the iceberg files committer to collect data files and commit to iceberg table."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "47db75fb329d35b4f360613c60ee4cab144a3347", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/47db75fb329d35b4f360613c60ee4cab144a3347", "committedDate": "2020-08-27T02:09:35Z", "message": "Add comments for the critical members"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0fcf8c4374777fed5ac75a167832bb971105c5dc", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/0fcf8c4374777fed5ac75a167832bb971105c5dc", "committedDate": "2020-08-27T02:09:35Z", "message": "Read the max-committed-checkpoint-id only when restoring the job."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2ca3e45201eb0dc2c0e623abf5fcc18859382fa1", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/2ca3e45201eb0dc2c0e623abf5fcc18859382fa1", "committedDate": "2020-08-27T02:09:35Z", "message": "Fix the broken unit tests."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "02ece7e607ba77f4d75a1556f5182291bc531db9", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/02ece7e607ba77f4d75a1556f5182291bc531db9", "committedDate": "2020-08-27T02:09:35Z", "message": "Refactor to use the SortedMapTypeInfo"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "24c9e80cfcc3344385ef272b2a232d3556999e08", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/24c9e80cfcc3344385ef272b2a232d3556999e08", "committedDate": "2020-08-27T02:09:35Z", "message": "Handle the cases that two different jobs write the the same table."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ad83367c3759de9fd17bfa8fce4fb5b4a810f57d", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/ad83367c3759de9fd17bfa8fce4fb5b4a810f57d", "committedDate": "2020-08-27T02:09:35Z", "message": "Minor fix"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6f5307eb20afac386d46b96bc10f1d01783a7d87", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/6f5307eb20afac386d46b96bc10f1d01783a7d87", "committedDate": "2020-08-27T02:09:35Z", "message": "Make IcebergFilesCommitter extends from AbstractStreamOperator for committing txn for bounded stream"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "527ea4c0aa45063266b444dcb7869ae5b89bee71", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/527ea4c0aa45063266b444dcb7869ae5b89bee71", "committedDate": "2020-08-27T02:09:35Z", "message": "Add the unit tests for ordered and disordered events between two continues checkpoints"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8542b703382ed346a60d45ac18c9b9d78650a9e0", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/8542b703382ed346a60d45ac18c9b9d78650a9e0", "committedDate": "2020-08-27T02:09:35Z", "message": "Fix the checkstyle"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d14aa089b5e9efa346f54bdcd52b405c1a3a5792", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/d14aa089b5e9efa346f54bdcd52b405c1a3a5792", "committedDate": "2020-08-27T02:09:35Z", "message": "Add a check to validate the parsed maxCommittedCheckpointId."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2cb965828d05efae43acd837f39c8465f0a382a9", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/2cb965828d05efae43acd837f39c8465f0a382a9", "committedDate": "2020-08-27T02:09:35Z", "message": "Improve the flink data stream unit tests."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7c532604481f1f13b6b8fd0dcd4c83e91db49463", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/7c532604481f1f13b6b8fd0dcd4c83e91db49463", "committedDate": "2020-08-27T02:09:35Z", "message": "Addressing comments."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "951a0ceccb57f8666e7ff99e67fdb886111bc074", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/951a0ceccb57f8666e7ff99e67fdb886111bc074", "committedDate": "2020-08-27T02:09:35Z", "message": "Minor fixes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "65c192165655fc7557db8914bf852b8b4bd13ca6", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/65c192165655fc7557db8914bf852b8b4bd13ca6", "committedDate": "2020-08-27T02:09:35Z", "message": "Introduce orc to the filesCommitter unit tests."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "71585830938bf70ab867db249321be1dbddba3ca", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/71585830938bf70ab867db249321be1dbddba3ca", "committedDate": "2020-08-27T02:09:35Z", "message": "Use the TableLoader to load table lazily inside icebergFilesCommitter"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fbc30fcfe9ffbbf2699d33913533c7d68c0cfb88", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/fbc30fcfe9ffbbf2699d33913533c7d68c0cfb88", "committedDate": "2020-08-27T02:09:35Z", "message": "Advancing the max-committed-checkpoint-id when there's no data files to commit."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "35fa7d3852e91641719e10ca36f9316be566820f", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/35fa7d3852e91641719e10ca36f9316be566820f", "committedDate": "2020-08-27T02:09:35Z", "message": "Fix the broken unit tests after ORC was introduced"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3e3fa49d8b1447d7885fb8e3364452c6024abfb8", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/3e3fa49d8b1447d7885fb8e3364452c6024abfb8", "committedDate": "2020-08-27T02:09:35Z", "message": "Keep the increased max-committed-checkpoint-id"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ac9d1e24a9f304edf6307b8ec22411a2fc94f363", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/ac9d1e24a9f304edf6307b8ec22411a2fc94f363", "committedDate": "2020-08-27T02:09:35Z", "message": "Refactor the FlinkSink builder to support both DataStream<Row> dan DataStream<RowData>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "56d0ed6c6b8da245fe61647aac2d3ec30d508e6f", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/56d0ed6c6b8da245fe61647aac2d3ec30d508e6f", "committedDate": "2020-08-27T02:09:35Z", "message": "Close the table loader once we disposed in iceberg files committer"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d7b42c3647c0a9e8b7dab97bd49ff7612e6091c2", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/d7b42c3647c0a9e8b7dab97bd49ff7612e6091c2", "committedDate": "2020-08-27T02:09:35Z", "message": "Addressing the comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "31d84365a4adf399551b7e45c5da4c1380a4d913", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/31d84365a4adf399551b7e45c5da4c1380a4d913", "committedDate": "2020-08-27T02:09:35Z", "message": "Add parquet to the unit tests."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "64f05738fd42237973a3937c67c88ec656527f57", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/64f05738fd42237973a3937c67c88ec656527f57", "committedDate": "2020-08-27T02:09:35Z", "message": "Add unit tests for bounded stream"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "826ef560fc8ad47cbc0bb991480481c720b3d1a1", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/826ef560fc8ad47cbc0bb991480481c720b3d1a1", "committedDate": "2020-08-27T02:22:34Z", "message": "Rebase to master and fix the complie error"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "bf2d6909b832def35748846667cff9780d020a9f", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/bf2d6909b832def35748846667cff9780d020a9f", "committedDate": "2020-08-26T13:10:23Z", "message": "Add unit tests for bounded stream"}, "afterCommit": {"oid": "826ef560fc8ad47cbc0bb991480481c720b3d1a1", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/826ef560fc8ad47cbc0bb991480481c720b3d1a1", "committedDate": "2020-08-27T02:22:34Z", "message": "Rebase to master and fix the complie error"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fd6a22e9d233952fac0bf838d22cf97470c469e9", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/fd6a22e9d233952fac0bf838d22cf97470c469e9", "committedDate": "2020-08-27T10:23:36Z", "message": "Move the writer classes to sink package"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc3MTY4Nzcz", "url": "https://github.com/apache/iceberg/pull/1185#pullrequestreview-477168773", "createdAt": "2020-08-28T00:34:18Z", "commit": {"oid": "fd6a22e9d233952fac0bf838d22cf97470c469e9"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQwMDozNDoxOFrOHIlolQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQwMDozNDoxOFrOHIlolQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODc2NzI1Mw==", "bodyText": "This isn't required for Spark. Could we add this to the FlinkFileAppenderFactory instead?", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r478767253", "createdAt": "2020-08-28T00:34:18Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/io/FileAppenderFactory.java", "diffHunk": "@@ -19,14 +19,15 @@\n \n package org.apache.iceberg.io;\n \n+import java.io.Serializable;\n import org.apache.iceberg.FileFormat;\n \n /**\n  * Factory to create a new {@link FileAppender} to write records.\n  *\n  * @param <T> data type of the rows to append.\n  */\n-public interface FileAppenderFactory<T> {\n+public interface FileAppenderFactory<T> extends Serializable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fd6a22e9d233952fac0bf838d22cf97470c469e9"}, "originalPosition": 13}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc3MTcxMjY4", "url": "https://github.com/apache/iceberg/pull/1185#pullrequestreview-477171268", "createdAt": "2020-08-28T00:43:50Z", "commit": {"oid": "fd6a22e9d233952fac0bf838d22cf97470c469e9"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQwMDo0Mzo1MFrOHIlx4A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQwMDo0Mzo1MFrOHIlx4A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODc2OTYzMg==", "bodyText": "Curious: are there any plans to support other object models? I know the original sink used Avro, which is supported by Avro and Parquet file formats.\nMaybe for that we need a better way to configure file formats with object models, though.", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r478769632", "createdAt": "2020-08-28T00:43:50Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java", "diffHunk": "@@ -0,0 +1,192 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeinfo.Types;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.DataStreamSink;\n+import org.apache.flink.streaming.api.functions.sink.DiscardingSink;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.util.DataFormatConverters;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.utils.TypeConversions;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.PropertyUtil;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT_DEFAULT;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT;\n+\n+public class FlinkSink {\n+\n+  private static final String ICEBERG_STREAM_WRITER_NAME = IcebergStreamWriter.class.getSimpleName();\n+  private static final String ICEBERG_FILES_COMMITTER_NAME = IcebergFilesCommitter.class.getSimpleName();\n+\n+  private FlinkSink() {\n+  }\n+\n+  public static Builder forRow(DataStream<Row> input) {\n+    return new Builder().forRow(input);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fd6a22e9d233952fac0bf838d22cf97470c469e9"}, "originalPosition": 63}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc3MTcxNzk4", "url": "https://github.com/apache/iceberg/pull/1185#pullrequestreview-477171798", "createdAt": "2020-08-28T00:45:50Z", "commit": {"oid": "fd6a22e9d233952fac0bf838d22cf97470c469e9"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQwMDo0NTo1MFrOHIlz6w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQwMDo0NTo1MFrOHIlz6w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODc3MDE1NQ==", "bodyText": "I don't think these messages are very helpful. The problem is that neither forRow or forRowData is called. Being specific about how to fix it (call one of those methods) would be a more helpful error.", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r478770155", "createdAt": "2020-08-28T00:45:50Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java", "diffHunk": "@@ -0,0 +1,192 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeinfo.Types;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.DataStreamSink;\n+import org.apache.flink.streaming.api.functions.sink.DiscardingSink;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.util.DataFormatConverters;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.utils.TypeConversions;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.PropertyUtil;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT_DEFAULT;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT;\n+\n+public class FlinkSink {\n+\n+  private static final String ICEBERG_STREAM_WRITER_NAME = IcebergStreamWriter.class.getSimpleName();\n+  private static final String ICEBERG_FILES_COMMITTER_NAME = IcebergFilesCommitter.class.getSimpleName();\n+\n+  private FlinkSink() {\n+  }\n+\n+  public static Builder forRow(DataStream<Row> input) {\n+    return new Builder().forRow(input);\n+  }\n+\n+  public static Builder forRowData(DataStream<RowData> input) {\n+    return new Builder().forRowData(input);\n+  }\n+\n+  public static class Builder {\n+    private DataStream<Row> rowInput = null;\n+    private DataStream<RowData> rowDataInput = null;\n+    private TableLoader tableLoader;\n+    private Configuration hadoopConf;\n+    private Table table;\n+    private TableSchema tableSchema;\n+\n+    private Builder forRow(DataStream<Row> newRowInput) {\n+      this.rowInput = newRowInput;\n+      return this;\n+    }\n+\n+    private Builder forRowData(DataStream<RowData> newRowDataInput) {\n+      this.rowDataInput = newRowDataInput;\n+      return this;\n+    }\n+\n+    public Builder table(Table newTable) {\n+      this.table = newTable;\n+      return this;\n+    }\n+\n+    public Builder tableLoader(TableLoader newTableLoader) {\n+      this.tableLoader = newTableLoader;\n+      return this;\n+    }\n+\n+    public Builder hadoopConf(Configuration newHadoopConf) {\n+      this.hadoopConf = newHadoopConf;\n+      return this;\n+    }\n+\n+    public Builder tableSchema(TableSchema newTableSchema) {\n+      this.tableSchema = newTableSchema;\n+      return this;\n+    }\n+\n+    private DataStream<RowData> convert() {\n+      Preconditions.checkArgument(rowInput != null, \"The DataStream<Row> to convert shouldn't be null\");\n+\n+      RowType rowType;\n+      DataType[] fieldDataTypes;\n+      if (tableSchema != null) {\n+        rowType = (RowType) tableSchema.toRowDataType().getLogicalType();\n+        fieldDataTypes = tableSchema.getFieldDataTypes();\n+      } else {\n+        rowType = FlinkSchemaUtil.convert(table.schema());\n+        fieldDataTypes = TypeConversions.fromLogicalToDataType(rowType.getChildren().toArray(new LogicalType[0]));\n+      }\n+\n+      DataFormatConverters.RowConverter rowConverter = new DataFormatConverters.RowConverter(fieldDataTypes);\n+\n+      return rowInput.map(rowConverter::toInternal, RowDataTypeInfo.of(rowType));\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    public DataStreamSink<RowData> build() {\n+      Preconditions.checkArgument(rowInput != null || rowDataInput != null,\n+          \"Should initialize input DataStream first with DataStream<Row> or DataStream<RowData>\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fd6a22e9d233952fac0bf838d22cf97470c469e9"}, "originalPosition": 129}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc3MTcyMDA1", "url": "https://github.com/apache/iceberg/pull/1185#pullrequestreview-477172005", "createdAt": "2020-08-28T00:46:42Z", "commit": {"oid": "fd6a22e9d233952fac0bf838d22cf97470c469e9"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQwMDo0Njo0MlrOHIl0vA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQwMDo0Njo0MlrOHIl0vA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODc3MDM2NA==", "bodyText": "The preconditions in build ensure this is never the case. I'm fine keeping the check (up to you) but it seems odd to have it in a private method. Also, this relies on the check that table is not null when tableSchema is null.", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r478770364", "createdAt": "2020-08-28T00:46:42Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java", "diffHunk": "@@ -0,0 +1,192 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeinfo.Types;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.DataStreamSink;\n+import org.apache.flink.streaming.api.functions.sink.DiscardingSink;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.util.DataFormatConverters;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.utils.TypeConversions;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.PropertyUtil;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT_DEFAULT;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT;\n+\n+public class FlinkSink {\n+\n+  private static final String ICEBERG_STREAM_WRITER_NAME = IcebergStreamWriter.class.getSimpleName();\n+  private static final String ICEBERG_FILES_COMMITTER_NAME = IcebergFilesCommitter.class.getSimpleName();\n+\n+  private FlinkSink() {\n+  }\n+\n+  public static Builder forRow(DataStream<Row> input) {\n+    return new Builder().forRow(input);\n+  }\n+\n+  public static Builder forRowData(DataStream<RowData> input) {\n+    return new Builder().forRowData(input);\n+  }\n+\n+  public static class Builder {\n+    private DataStream<Row> rowInput = null;\n+    private DataStream<RowData> rowDataInput = null;\n+    private TableLoader tableLoader;\n+    private Configuration hadoopConf;\n+    private Table table;\n+    private TableSchema tableSchema;\n+\n+    private Builder forRow(DataStream<Row> newRowInput) {\n+      this.rowInput = newRowInput;\n+      return this;\n+    }\n+\n+    private Builder forRowData(DataStream<RowData> newRowDataInput) {\n+      this.rowDataInput = newRowDataInput;\n+      return this;\n+    }\n+\n+    public Builder table(Table newTable) {\n+      this.table = newTable;\n+      return this;\n+    }\n+\n+    public Builder tableLoader(TableLoader newTableLoader) {\n+      this.tableLoader = newTableLoader;\n+      return this;\n+    }\n+\n+    public Builder hadoopConf(Configuration newHadoopConf) {\n+      this.hadoopConf = newHadoopConf;\n+      return this;\n+    }\n+\n+    public Builder tableSchema(TableSchema newTableSchema) {\n+      this.tableSchema = newTableSchema;\n+      return this;\n+    }\n+\n+    private DataStream<RowData> convert() {\n+      Preconditions.checkArgument(rowInput != null, \"The DataStream<Row> to convert shouldn't be null\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fd6a22e9d233952fac0bf838d22cf97470c469e9"}, "originalPosition": 109}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc3MTc0NDk1", "url": "https://github.com/apache/iceberg/pull/1185#pullrequestreview-477174495", "createdAt": "2020-08-28T00:55:01Z", "commit": {"oid": "fd6a22e9d233952fac0bf838d22cf97470c469e9"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQwMDo1NTowMVrOHIl8vA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQwMDo1NTowMVrOHIl8vA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODc3MjQxMg==", "bodyText": "I think in the Netflix version, we track old checkpoint state by writing a manifest file and appending that manifest to the table. The advantage is that state is really small so the job can be blocked from committing for a long time (days) and can recover when connectivity to commit has been restored.\nWe use this to handle resilience when AWS regions can't talk to one another and our metastore is in a single region.", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r478772412", "createdAt": "2020-08-28T00:55:01Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java", "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NavigableMap;\n+import java.util.SortedMap;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.BasicTypeInfo;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.ListTypeInfo;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.BoundedOneInput;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.runtime.typeutils.SortedMapTypeInfo;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.types.Comparators;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+class IcebergFilesCommitter extends AbstractStreamOperator<Void>\n+    implements OneInputStreamOperator<DataFile, Void>, BoundedOneInput {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final long INITIAL_CHECKPOINT_ID = -1L;\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergFilesCommitter.class);\n+  private static final String FLINK_JOB_ID = \"flink.job-id\";\n+\n+  // The max checkpoint id we've committed to iceberg table. As the flink's checkpoint is always increasing, so we could\n+  // correctly commit all the data files whose checkpoint id is greater than the max committed one to iceberg table, for\n+  // avoiding committing the same data files twice. This id will be attached to iceberg's meta when committing the\n+  // iceberg transaction.\n+  private static final String MAX_COMMITTED_CHECKPOINT_ID = \"flink.max-committed-checkpoint-id\";\n+\n+  // TableLoader to load iceberg table lazily.\n+  private final TableLoader tableLoader;\n+  private final SerializableConfiguration hadoopConf;\n+\n+  // A sorted map to maintain the completed data files for each pending checkpointId (which have not been committed\n+  // to iceberg table). We need a sorted map here because there's possible that few checkpoints snapshot failed, for\n+  // example: the 1st checkpoint have 2 data files <1, <file0, file1>>, the 2st checkpoint have 1 data files\n+  // <2, <file3>>. Snapshot for checkpoint#1 interrupted because of network/disk failure etc, while we don't expect\n+  // any data loss in iceberg table. So we keep the finished files <1, <file0, file1>> in memory and retry to commit\n+  // iceberg table when the next checkpoint happen.\n+  private final NavigableMap<Long, List<DataFile>> dataFilesPerCheckpoint = Maps.newTreeMap();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fd6a22e9d233952fac0bf838d22cf97470c469e9"}, "originalPosition": 80}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc3MTc1NzY5", "url": "https://github.com/apache/iceberg/pull/1185#pullrequestreview-477175769", "createdAt": "2020-08-28T00:57:41Z", "commit": {"oid": "fd6a22e9d233952fac0bf838d22cf97470c469e9"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQwMDo1Nzo0MVrOHImAmQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQwMDo1Nzo0MVrOHImAmQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODc3MzQwMQ==", "bodyText": "Does this need to be closed?\nhttps://github.com/apache/iceberg/pull/1346/files#r474605890", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r478773401", "createdAt": "2020-08-28T00:57:41Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java", "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NavigableMap;\n+import java.util.SortedMap;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.BasicTypeInfo;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.ListTypeInfo;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.BoundedOneInput;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.runtime.typeutils.SortedMapTypeInfo;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.types.Comparators;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+class IcebergFilesCommitter extends AbstractStreamOperator<Void>\n+    implements OneInputStreamOperator<DataFile, Void>, BoundedOneInput {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final long INITIAL_CHECKPOINT_ID = -1L;\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergFilesCommitter.class);\n+  private static final String FLINK_JOB_ID = \"flink.job-id\";\n+\n+  // The max checkpoint id we've committed to iceberg table. As the flink's checkpoint is always increasing, so we could\n+  // correctly commit all the data files whose checkpoint id is greater than the max committed one to iceberg table, for\n+  // avoiding committing the same data files twice. This id will be attached to iceberg's meta when committing the\n+  // iceberg transaction.\n+  private static final String MAX_COMMITTED_CHECKPOINT_ID = \"flink.max-committed-checkpoint-id\";\n+\n+  // TableLoader to load iceberg table lazily.\n+  private final TableLoader tableLoader;\n+  private final SerializableConfiguration hadoopConf;\n+\n+  // A sorted map to maintain the completed data files for each pending checkpointId (which have not been committed\n+  // to iceberg table). We need a sorted map here because there's possible that few checkpoints snapshot failed, for\n+  // example: the 1st checkpoint have 2 data files <1, <file0, file1>>, the 2st checkpoint have 1 data files\n+  // <2, <file3>>. Snapshot for checkpoint#1 interrupted because of network/disk failure etc, while we don't expect\n+  // any data loss in iceberg table. So we keep the finished files <1, <file0, file1>> in memory and retry to commit\n+  // iceberg table when the next checkpoint happen.\n+  private final NavigableMap<Long, List<DataFile>> dataFilesPerCheckpoint = Maps.newTreeMap();\n+\n+  // The data files cache for current checkpoint. Once the snapshot barrier received, it will be flushed to the\n+  // 'dataFilesPerCheckpoint'.\n+  private final List<DataFile> dataFilesOfCurrentCheckpoint = Lists.newArrayList();\n+\n+  // It will have an unique identifier for one job.\n+  private transient String flinkJobId;\n+  private transient Table table;\n+  private transient long maxCommittedCheckpointId;\n+\n+  // All pending checkpoints states for this function.\n+  private static final ListStateDescriptor<SortedMap<Long, List<DataFile>>> STATE_DESCRIPTOR = buildStateDescriptor();\n+  private transient ListState<SortedMap<Long, List<DataFile>>> checkpointsState;\n+\n+  IcebergFilesCommitter(TableLoader tableLoader, Configuration hadoopConf) {\n+    this.tableLoader = tableLoader;\n+    this.hadoopConf = new SerializableConfiguration(hadoopConf);\n+  }\n+\n+  @Override\n+  public void initializeState(StateInitializationContext context) throws Exception {\n+    super.initializeState(context);\n+    flinkJobId = getContainingTask().getEnvironment().getJobID().toString();\n+\n+    // Open the table loader and load the table.\n+    tableLoader.open(hadoopConf.get());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fd6a22e9d233952fac0bf838d22cf97470c469e9"}, "originalPosition": 106}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc3MTc2NDQ0", "url": "https://github.com/apache/iceberg/pull/1185#pullrequestreview-477176444", "createdAt": "2020-08-28T00:59:55Z", "commit": {"oid": "fd6a22e9d233952fac0bf838d22cf97470c469e9"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQwMDo1OTo1NVrOHImDPw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQwMDo1OTo1NVrOHImDPw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODc3NDA3OQ==", "bodyText": "Minor: I prefer to prefix field names with this. when setting instance fields, so it is obvious that it is not a local variable.", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r478774079", "createdAt": "2020-08-28T00:59:55Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java", "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NavigableMap;\n+import java.util.SortedMap;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.BasicTypeInfo;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.ListTypeInfo;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.BoundedOneInput;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.runtime.typeutils.SortedMapTypeInfo;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.types.Comparators;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+class IcebergFilesCommitter extends AbstractStreamOperator<Void>\n+    implements OneInputStreamOperator<DataFile, Void>, BoundedOneInput {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final long INITIAL_CHECKPOINT_ID = -1L;\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergFilesCommitter.class);\n+  private static final String FLINK_JOB_ID = \"flink.job-id\";\n+\n+  // The max checkpoint id we've committed to iceberg table. As the flink's checkpoint is always increasing, so we could\n+  // correctly commit all the data files whose checkpoint id is greater than the max committed one to iceberg table, for\n+  // avoiding committing the same data files twice. This id will be attached to iceberg's meta when committing the\n+  // iceberg transaction.\n+  private static final String MAX_COMMITTED_CHECKPOINT_ID = \"flink.max-committed-checkpoint-id\";\n+\n+  // TableLoader to load iceberg table lazily.\n+  private final TableLoader tableLoader;\n+  private final SerializableConfiguration hadoopConf;\n+\n+  // A sorted map to maintain the completed data files for each pending checkpointId (which have not been committed\n+  // to iceberg table). We need a sorted map here because there's possible that few checkpoints snapshot failed, for\n+  // example: the 1st checkpoint have 2 data files <1, <file0, file1>>, the 2st checkpoint have 1 data files\n+  // <2, <file3>>. Snapshot for checkpoint#1 interrupted because of network/disk failure etc, while we don't expect\n+  // any data loss in iceberg table. So we keep the finished files <1, <file0, file1>> in memory and retry to commit\n+  // iceberg table when the next checkpoint happen.\n+  private final NavigableMap<Long, List<DataFile>> dataFilesPerCheckpoint = Maps.newTreeMap();\n+\n+  // The data files cache for current checkpoint. Once the snapshot barrier received, it will be flushed to the\n+  // 'dataFilesPerCheckpoint'.\n+  private final List<DataFile> dataFilesOfCurrentCheckpoint = Lists.newArrayList();\n+\n+  // It will have an unique identifier for one job.\n+  private transient String flinkJobId;\n+  private transient Table table;\n+  private transient long maxCommittedCheckpointId;\n+\n+  // All pending checkpoints states for this function.\n+  private static final ListStateDescriptor<SortedMap<Long, List<DataFile>>> STATE_DESCRIPTOR = buildStateDescriptor();\n+  private transient ListState<SortedMap<Long, List<DataFile>>> checkpointsState;\n+\n+  IcebergFilesCommitter(TableLoader tableLoader, Configuration hadoopConf) {\n+    this.tableLoader = tableLoader;\n+    this.hadoopConf = new SerializableConfiguration(hadoopConf);\n+  }\n+\n+  @Override\n+  public void initializeState(StateInitializationContext context) throws Exception {\n+    super.initializeState(context);\n+    flinkJobId = getContainingTask().getEnvironment().getJobID().toString();\n+\n+    // Open the table loader and load the table.\n+    tableLoader.open(hadoopConf.get());\n+    table = tableLoader.loadTable();\n+    maxCommittedCheckpointId = INITIAL_CHECKPOINT_ID;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fd6a22e9d233952fac0bf838d22cf97470c469e9"}, "originalPosition": 108}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc3MTc4MDY5", "url": "https://github.com/apache/iceberg/pull/1185#pullrequestreview-477178069", "createdAt": "2020-08-28T01:05:37Z", "commit": {"oid": "fd6a22e9d233952fac0bf838d22cf97470c469e9"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQwMTowNTozOFrOHImIxA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQwMTowNTozOFrOHImIxA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODc3NTQ5Mg==", "bodyText": "What does this do?", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r478775492", "createdAt": "2020-08-28T01:05:38Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/sink/IcebergStreamWriter.java", "diffHunk": "@@ -34,15 +35,16 @@\n   private static final long serialVersionUID = 1L;\n \n   private final String fullTableName;\n+  private final TaskWriterFactory<T> taskWriterFactory;\n \n-  private transient TaskWriterFactory<T> taskWriterFactory;\n   private transient TaskWriter<T> writer;\n   private transient int subTaskId;\n   private transient int attemptId;\n \n   IcebergStreamWriter(String fullTableName, TaskWriterFactory<T> taskWriterFactory) {\n     this.fullTableName = fullTableName;\n     this.taskWriterFactory = taskWriterFactory;\n+    setChainingStrategy(ChainingStrategy.ALWAYS);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fd6a22e9d233952fac0bf838d22cf97470c469e9"}, "originalPosition": 28}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "78a0d362a21faf912496b1bb846ea5d435deb3a5", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/78a0d362a21faf912496b1bb846ea5d435deb3a5", "committedDate": "2020-08-28T02:47:22Z", "message": "Addressing comments from Ryan"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "65076d72d65ebadce31c5ac920460c9e0cd1ea64", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/65076d72d65ebadce31c5ac920460c9e0cd1ea64", "committedDate": "2020-08-28T06:28:42Z", "message": "Minor changes: removing stale comments etc."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "675977b47d3702d40bc716a520f8cac49465e73c", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/675977b47d3702d40bc716a520f8cac49465e73c", "committedDate": "2020-08-28T07:11:21Z", "message": "Refactor the FlinkSink API and provide detailed javadoc."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e674ed7182289f3c6132c0ca2e76f0fd2d0b6871", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/e674ed7182289f3c6132c0ca2e76f0fd2d0b6871", "committedDate": "2020-08-28T10:02:30Z", "message": "Fix the broken unit tests"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc3ODk5MzAw", "url": "https://github.com/apache/iceberg/pull/1185#pullrequestreview-477899300", "createdAt": "2020-08-28T17:17:14Z", "commit": {"oid": "e674ed7182289f3c6132c0ca2e76f0fd2d0b6871"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQxNzoxNzoxNFrOHJOciQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQxNzoxNzoxNFrOHJOciQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTQzNTkxMw==", "bodyText": "Minor: it seems odd to provide the loader and the table. Couldn't the loader be called in the sink builder?", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r479435913", "createdAt": "2020-08-28T17:17:14Z", "author": {"login": "rdblue"}, "path": "flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSink.java", "diffHunk": "@@ -0,0 +1,178 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.RowTypeInfo;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.util.FiniteTestSource;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.util.DataFormatConverters;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.test.util.AbstractTestBase;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.flink.SimpleDataUtil;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+@RunWith(Parameterized.class)\n+public class TestFlinkIcebergSink extends AbstractTestBase {\n+  private static final Configuration CONF = new Configuration();\n+  private static final TypeInformation<Row> ROW_TYPE_INFO = new RowTypeInfo(\n+      SimpleDataUtil.FLINK_SCHEMA.getFieldTypes());\n+  private static final DataFormatConverters.RowConverter CONVERTER = new DataFormatConverters.RowConverter(\n+      SimpleDataUtil.FLINK_SCHEMA.getFieldDataTypes());\n+\n+  @Rule\n+  public TemporaryFolder tempFolder = new TemporaryFolder();\n+\n+  private String tablePath;\n+  private Table table;\n+  private StreamExecutionEnvironment env;\n+  private TableLoader tableLoader;\n+\n+  private final FileFormat format;\n+  private final int parallelism;\n+  private final boolean partitioned;\n+\n+  @Parameterized.Parameters(name = \"format={0}, parallelism = {1}, partitioned = {2}\")\n+  public static Object[][] parameters() {\n+    return new Object[][] {\n+        new Object[] {\"avro\", 1, true},\n+        new Object[] {\"avro\", 1, false},\n+        new Object[] {\"avro\", 2, true},\n+        new Object[] {\"avro\", 2, false},\n+        new Object[] {\"orc\", 1, true},\n+        new Object[] {\"orc\", 1, false},\n+        new Object[] {\"orc\", 2, true},\n+        new Object[] {\"orc\", 2, false},\n+        new Object[] {\"parquet\", 1, true},\n+        new Object[] {\"parquet\", 1, false},\n+        new Object[] {\"parquet\", 2, true},\n+        new Object[] {\"parquet\", 2, false}\n+    };\n+  }\n+\n+  public TestFlinkIcebergSink(String format, int parallelism, boolean partitioned) {\n+    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));\n+    this.parallelism = parallelism;\n+    this.partitioned = partitioned;\n+  }\n+\n+  @Before\n+  public void before() throws IOException {\n+    File folder = tempFolder.newFolder();\n+    String warehouse = folder.getAbsolutePath();\n+\n+    tablePath = warehouse.concat(\"/test\");\n+    Assert.assertTrue(\"Should create the table path correctly.\", new File(tablePath).mkdir());\n+\n+    Map<String, String> props = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format.name());\n+    table = SimpleDataUtil.createTable(tablePath, props, partitioned);\n+\n+    env = StreamExecutionEnvironment.getExecutionEnvironment()\n+        .enableCheckpointing(100)\n+        .setParallelism(parallelism)\n+        .setMaxParallelism(parallelism);\n+\n+    tableLoader = TableLoader.fromHadoopTable(tablePath);\n+  }\n+\n+  private List<RowData> convertToRowData(List<Row> rows) {\n+    return rows.stream().map(CONVERTER::toInternal).collect(Collectors.toList());\n+  }\n+\n+  @Test\n+  public void testWriteRowData() throws Exception {\n+    List<Row> rows = Lists.newArrayList(\n+        Row.of(1, \"hello\"),\n+        Row.of(2, \"world\"),\n+        Row.of(3, \"foo\")\n+    );\n+    DataStream<RowData> dataStream = env.addSource(new FiniteTestSource<>(rows), ROW_TYPE_INFO)\n+        .map(CONVERTER::toInternal, RowDataTypeInfo.of(SimpleDataUtil.ROW_TYPE));\n+\n+    FlinkSink.forRowData(dataStream)\n+        .table(table)\n+        .tableLoader(tableLoader)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e674ed7182289f3c6132c0ca2e76f0fd2d0b6871"}, "originalPosition": 135}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc3OTE1ODM4", "url": "https://github.com/apache/iceberg/pull/1185#pullrequestreview-477915838", "createdAt": "2020-08-28T17:44:15Z", "commit": {"oid": "e674ed7182289f3c6132c0ca2e76f0fd2d0b6871"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQxNzo0NDoxNVrOHJPPYA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQxNzo0NDoxNVrOHJPPYA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTQ0ODkyOA==", "bodyText": "there can be multiple Iceberg sinks in the same job. we probably should add the table identifier string suffix to make operator name and id unique. We have a unique sinkName within a job and we add the sinkName suffix for operator name.", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r479448928", "createdAt": "2020-08-28T17:44:15Z", "author": {"login": "stevenzwu"}, "path": "flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java", "diffHunk": "@@ -0,0 +1,220 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.flink.api.common.functions.MapFunction;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeinfo.Types;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.DataStreamSink;\n+import org.apache.flink.streaming.api.functions.sink.DiscardingSink;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.util.DataFormatConverters;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.PropertyUtil;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT_DEFAULT;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT;\n+\n+public class FlinkSink {\n+\n+  private static final String ICEBERG_STREAM_WRITER_NAME = IcebergStreamWriter.class.getSimpleName();\n+  private static final String ICEBERG_FILES_COMMITTER_NAME = IcebergFilesCommitter.class.getSimpleName();\n+\n+  private FlinkSink() {\n+  }\n+\n+  /**\n+   * Initialize a {@link Builder} to export the data from generic input data stream into iceberg table. We use\n+   * {@link RowData} inside the sink connector, so users need to provide a mapper function and a\n+   * {@link TypeInformation} to convert those generic records to a RowData DataStream.\n+   *\n+   * @param input      the generic source input data stream.\n+   * @param mapper     function to convert the generic data to {@link RowData}\n+   * @param outputType to define the {@link TypeInformation} for the input data.\n+   * @param <T>        the data type of records.\n+   * @return {@link Builder} to connect the iceberg table.\n+   */\n+  public static <T> Builder builderFor(DataStream<T> input,\n+                                       MapFunction<T, RowData> mapper,\n+                                       TypeInformation<RowData> outputType) {\n+    DataStream<RowData> dataStream = input.map(mapper, outputType);\n+    return forRowData(dataStream);\n+  }\n+\n+  /**\n+   * Initialize a {@link Builder} to export the data from input data stream with {@link Row}s into iceberg table. We use\n+   * {@link RowData} inside the sink connector, so users need to provide a {@link TableSchema} for builder to convert\n+   * those {@link Row}s to a {@link RowData} DataStream.\n+   *\n+   * @param input       the source input data stream with {@link Row}s.\n+   * @param tableSchema defines the {@link TypeInformation} for input data.\n+   * @return {@link Builder} to connect the iceberg table.\n+   */\n+  public static Builder forRow(DataStream<Row> input, TableSchema tableSchema) {\n+    RowType rowType = (RowType) tableSchema.toRowDataType().getLogicalType();\n+    DataType[] fieldDataTypes = tableSchema.getFieldDataTypes();\n+\n+    DataFormatConverters.RowConverter rowConverter = new DataFormatConverters.RowConverter(fieldDataTypes);\n+    return builderFor(input, rowConverter::toInternal, RowDataTypeInfo.of(rowType))\n+        .tableSchema(tableSchema);\n+  }\n+\n+  /**\n+   * Initialize a {@link Builder} to export the data from input data stream with {@link RowData}s into iceberg table.\n+   *\n+   * @param input the source input data stream with {@link RowData}s.\n+   * @return {@link Builder} to connect the iceberg table.\n+   */\n+  public static Builder forRowData(DataStream<RowData> input) {\n+    return new Builder().forRowData(input);\n+  }\n+\n+  public static class Builder {\n+    private DataStream<RowData> rowDataInput = null;\n+    private TableLoader tableLoader;\n+    private Configuration hadoopConf;\n+    private Table table;\n+    private TableSchema tableSchema;\n+\n+    private Builder() {\n+    }\n+\n+    private Builder forRowData(DataStream<RowData> newRowDataInput) {\n+      this.rowDataInput = newRowDataInput;\n+      return this;\n+    }\n+\n+    /**\n+     * This iceberg {@link Table} instance is used for initializing {@link IcebergStreamWriter} which will write all\n+     * the records into {@link DataFile}s and emit them to downstream operator. Providing a table would avoid so many\n+     * table loading from each separate task.\n+     *\n+     * @param newTable the loaded iceberg table instance.\n+     * @return {@link Builder} to connect the iceberg table.\n+     */\n+    public Builder table(Table newTable) {\n+      this.table = newTable;\n+      return this;\n+    }\n+\n+    /**\n+     * The table loader is used for loading tables in {@link IcebergFilesCommitter} lazily, we need this loader because\n+     * {@link Table} is not serializable and could not just use the loaded table from Builder#table in the remote task\n+     * manager.\n+     *\n+     * @param newTableLoader to load iceberg table inside tasks.\n+     * @return {@link Builder} to connect the iceberg table.\n+     */\n+    public Builder tableLoader(TableLoader newTableLoader) {\n+      this.tableLoader = newTableLoader;\n+      return this;\n+    }\n+\n+    public Builder hadoopConf(Configuration newHadoopConf) {\n+      this.hadoopConf = newHadoopConf;\n+      return this;\n+    }\n+\n+    public Builder tableSchema(TableSchema newTableSchema) {\n+      this.tableSchema = newTableSchema;\n+      return this;\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    public DataStreamSink<RowData> build() {\n+      Preconditions.checkArgument(rowDataInput != null,\n+          \"Please use forRowData() to initialize the input DataStream.\");\n+      Preconditions.checkNotNull(table, \"Table shouldn't be null\");\n+      Preconditions.checkNotNull(tableLoader, \"Table loader shouldn't be null\");\n+      Preconditions.checkNotNull(hadoopConf, \"Hadoop configuration shouldn't be null\");\n+\n+      IcebergStreamWriter<RowData> streamWriter = createStreamWriter(table, tableSchema);\n+      IcebergFilesCommitter filesCommitter = new IcebergFilesCommitter(tableLoader, hadoopConf);\n+\n+      DataStream<Void> returnStream = rowDataInput\n+          .transform(ICEBERG_STREAM_WRITER_NAME, TypeInformation.of(DataFile.class), streamWriter)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e674ed7182289f3c6132c0ca2e76f0fd2d0b6871"}, "originalPosition": 170}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc3OTE2NjU1", "url": "https://github.com/apache/iceberg/pull/1185#pullrequestreview-477916655", "createdAt": "2020-08-28T17:45:37Z", "commit": {"oid": "e674ed7182289f3c6132c0ca2e76f0fd2d0b6871"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQxNzo0NTozN1rOHJPR9Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQxNzo0NTozN1rOHJPR9Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTQ0OTU4OQ==", "bodyText": "committer is a stateful operator, we should probably explicitly set uid.", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r479449589", "createdAt": "2020-08-28T17:45:37Z", "author": {"login": "stevenzwu"}, "path": "flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java", "diffHunk": "@@ -0,0 +1,220 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.flink.api.common.functions.MapFunction;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeinfo.Types;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.DataStreamSink;\n+import org.apache.flink.streaming.api.functions.sink.DiscardingSink;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.util.DataFormatConverters;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.PropertyUtil;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT_DEFAULT;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT;\n+\n+public class FlinkSink {\n+\n+  private static final String ICEBERG_STREAM_WRITER_NAME = IcebergStreamWriter.class.getSimpleName();\n+  private static final String ICEBERG_FILES_COMMITTER_NAME = IcebergFilesCommitter.class.getSimpleName();\n+\n+  private FlinkSink() {\n+  }\n+\n+  /**\n+   * Initialize a {@link Builder} to export the data from generic input data stream into iceberg table. We use\n+   * {@link RowData} inside the sink connector, so users need to provide a mapper function and a\n+   * {@link TypeInformation} to convert those generic records to a RowData DataStream.\n+   *\n+   * @param input      the generic source input data stream.\n+   * @param mapper     function to convert the generic data to {@link RowData}\n+   * @param outputType to define the {@link TypeInformation} for the input data.\n+   * @param <T>        the data type of records.\n+   * @return {@link Builder} to connect the iceberg table.\n+   */\n+  public static <T> Builder builderFor(DataStream<T> input,\n+                                       MapFunction<T, RowData> mapper,\n+                                       TypeInformation<RowData> outputType) {\n+    DataStream<RowData> dataStream = input.map(mapper, outputType);\n+    return forRowData(dataStream);\n+  }\n+\n+  /**\n+   * Initialize a {@link Builder} to export the data from input data stream with {@link Row}s into iceberg table. We use\n+   * {@link RowData} inside the sink connector, so users need to provide a {@link TableSchema} for builder to convert\n+   * those {@link Row}s to a {@link RowData} DataStream.\n+   *\n+   * @param input       the source input data stream with {@link Row}s.\n+   * @param tableSchema defines the {@link TypeInformation} for input data.\n+   * @return {@link Builder} to connect the iceberg table.\n+   */\n+  public static Builder forRow(DataStream<Row> input, TableSchema tableSchema) {\n+    RowType rowType = (RowType) tableSchema.toRowDataType().getLogicalType();\n+    DataType[] fieldDataTypes = tableSchema.getFieldDataTypes();\n+\n+    DataFormatConverters.RowConverter rowConverter = new DataFormatConverters.RowConverter(fieldDataTypes);\n+    return builderFor(input, rowConverter::toInternal, RowDataTypeInfo.of(rowType))\n+        .tableSchema(tableSchema);\n+  }\n+\n+  /**\n+   * Initialize a {@link Builder} to export the data from input data stream with {@link RowData}s into iceberg table.\n+   *\n+   * @param input the source input data stream with {@link RowData}s.\n+   * @return {@link Builder} to connect the iceberg table.\n+   */\n+  public static Builder forRowData(DataStream<RowData> input) {\n+    return new Builder().forRowData(input);\n+  }\n+\n+  public static class Builder {\n+    private DataStream<RowData> rowDataInput = null;\n+    private TableLoader tableLoader;\n+    private Configuration hadoopConf;\n+    private Table table;\n+    private TableSchema tableSchema;\n+\n+    private Builder() {\n+    }\n+\n+    private Builder forRowData(DataStream<RowData> newRowDataInput) {\n+      this.rowDataInput = newRowDataInput;\n+      return this;\n+    }\n+\n+    /**\n+     * This iceberg {@link Table} instance is used for initializing {@link IcebergStreamWriter} which will write all\n+     * the records into {@link DataFile}s and emit them to downstream operator. Providing a table would avoid so many\n+     * table loading from each separate task.\n+     *\n+     * @param newTable the loaded iceberg table instance.\n+     * @return {@link Builder} to connect the iceberg table.\n+     */\n+    public Builder table(Table newTable) {\n+      this.table = newTable;\n+      return this;\n+    }\n+\n+    /**\n+     * The table loader is used for loading tables in {@link IcebergFilesCommitter} lazily, we need this loader because\n+     * {@link Table} is not serializable and could not just use the loaded table from Builder#table in the remote task\n+     * manager.\n+     *\n+     * @param newTableLoader to load iceberg table inside tasks.\n+     * @return {@link Builder} to connect the iceberg table.\n+     */\n+    public Builder tableLoader(TableLoader newTableLoader) {\n+      this.tableLoader = newTableLoader;\n+      return this;\n+    }\n+\n+    public Builder hadoopConf(Configuration newHadoopConf) {\n+      this.hadoopConf = newHadoopConf;\n+      return this;\n+    }\n+\n+    public Builder tableSchema(TableSchema newTableSchema) {\n+      this.tableSchema = newTableSchema;\n+      return this;\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    public DataStreamSink<RowData> build() {\n+      Preconditions.checkArgument(rowDataInput != null,\n+          \"Please use forRowData() to initialize the input DataStream.\");\n+      Preconditions.checkNotNull(table, \"Table shouldn't be null\");\n+      Preconditions.checkNotNull(tableLoader, \"Table loader shouldn't be null\");\n+      Preconditions.checkNotNull(hadoopConf, \"Hadoop configuration shouldn't be null\");\n+\n+      IcebergStreamWriter<RowData> streamWriter = createStreamWriter(table, tableSchema);\n+      IcebergFilesCommitter filesCommitter = new IcebergFilesCommitter(tableLoader, hadoopConf);\n+\n+      DataStream<Void> returnStream = rowDataInput\n+          .transform(ICEBERG_STREAM_WRITER_NAME, TypeInformation.of(DataFile.class), streamWriter)\n+          .setParallelism(rowDataInput.getParallelism())\n+          .transform(ICEBERG_FILES_COMMITTER_NAME, Types.VOID, filesCommitter)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e674ed7182289f3c6132c0ca2e76f0fd2d0b6871"}, "originalPosition": 172}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc3OTE3NzAx", "url": "https://github.com/apache/iceberg/pull/1185#pullrequestreview-477917701", "createdAt": "2020-08-28T17:47:25Z", "commit": {"oid": "e674ed7182289f3c6132c0ca2e76f0fd2d0b6871"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQxNzo0NzoyNVrOHJPVMA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQxNzo0NzoyNVrOHJPVMA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTQ1MDQxNg==", "bodyText": "curious about the reason why don't we make the committer a sink function and instead add a dummy DiscardingSink.\nConceptually, this writer-committer combo is the reverse/mirror of split enumerator-reader FLIP-27 source interface. It will be nice to run committer on jobmanager (similar to enumerator). This way, Iceberg sink won't change the nature of the embarrassingly-parallel DAG.", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r479450416", "createdAt": "2020-08-28T17:47:25Z", "author": {"login": "stevenzwu"}, "path": "flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java", "diffHunk": "@@ -0,0 +1,220 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.flink.api.common.functions.MapFunction;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeinfo.Types;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.DataStreamSink;\n+import org.apache.flink.streaming.api.functions.sink.DiscardingSink;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.util.DataFormatConverters;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.PropertyUtil;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT_DEFAULT;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT;\n+\n+public class FlinkSink {\n+\n+  private static final String ICEBERG_STREAM_WRITER_NAME = IcebergStreamWriter.class.getSimpleName();\n+  private static final String ICEBERG_FILES_COMMITTER_NAME = IcebergFilesCommitter.class.getSimpleName();\n+\n+  private FlinkSink() {\n+  }\n+\n+  /**\n+   * Initialize a {@link Builder} to export the data from generic input data stream into iceberg table. We use\n+   * {@link RowData} inside the sink connector, so users need to provide a mapper function and a\n+   * {@link TypeInformation} to convert those generic records to a RowData DataStream.\n+   *\n+   * @param input      the generic source input data stream.\n+   * @param mapper     function to convert the generic data to {@link RowData}\n+   * @param outputType to define the {@link TypeInformation} for the input data.\n+   * @param <T>        the data type of records.\n+   * @return {@link Builder} to connect the iceberg table.\n+   */\n+  public static <T> Builder builderFor(DataStream<T> input,\n+                                       MapFunction<T, RowData> mapper,\n+                                       TypeInformation<RowData> outputType) {\n+    DataStream<RowData> dataStream = input.map(mapper, outputType);\n+    return forRowData(dataStream);\n+  }\n+\n+  /**\n+   * Initialize a {@link Builder} to export the data from input data stream with {@link Row}s into iceberg table. We use\n+   * {@link RowData} inside the sink connector, so users need to provide a {@link TableSchema} for builder to convert\n+   * those {@link Row}s to a {@link RowData} DataStream.\n+   *\n+   * @param input       the source input data stream with {@link Row}s.\n+   * @param tableSchema defines the {@link TypeInformation} for input data.\n+   * @return {@link Builder} to connect the iceberg table.\n+   */\n+  public static Builder forRow(DataStream<Row> input, TableSchema tableSchema) {\n+    RowType rowType = (RowType) tableSchema.toRowDataType().getLogicalType();\n+    DataType[] fieldDataTypes = tableSchema.getFieldDataTypes();\n+\n+    DataFormatConverters.RowConverter rowConverter = new DataFormatConverters.RowConverter(fieldDataTypes);\n+    return builderFor(input, rowConverter::toInternal, RowDataTypeInfo.of(rowType))\n+        .tableSchema(tableSchema);\n+  }\n+\n+  /**\n+   * Initialize a {@link Builder} to export the data from input data stream with {@link RowData}s into iceberg table.\n+   *\n+   * @param input the source input data stream with {@link RowData}s.\n+   * @return {@link Builder} to connect the iceberg table.\n+   */\n+  public static Builder forRowData(DataStream<RowData> input) {\n+    return new Builder().forRowData(input);\n+  }\n+\n+  public static class Builder {\n+    private DataStream<RowData> rowDataInput = null;\n+    private TableLoader tableLoader;\n+    private Configuration hadoopConf;\n+    private Table table;\n+    private TableSchema tableSchema;\n+\n+    private Builder() {\n+    }\n+\n+    private Builder forRowData(DataStream<RowData> newRowDataInput) {\n+      this.rowDataInput = newRowDataInput;\n+      return this;\n+    }\n+\n+    /**\n+     * This iceberg {@link Table} instance is used for initializing {@link IcebergStreamWriter} which will write all\n+     * the records into {@link DataFile}s and emit them to downstream operator. Providing a table would avoid so many\n+     * table loading from each separate task.\n+     *\n+     * @param newTable the loaded iceberg table instance.\n+     * @return {@link Builder} to connect the iceberg table.\n+     */\n+    public Builder table(Table newTable) {\n+      this.table = newTable;\n+      return this;\n+    }\n+\n+    /**\n+     * The table loader is used for loading tables in {@link IcebergFilesCommitter} lazily, we need this loader because\n+     * {@link Table} is not serializable and could not just use the loaded table from Builder#table in the remote task\n+     * manager.\n+     *\n+     * @param newTableLoader to load iceberg table inside tasks.\n+     * @return {@link Builder} to connect the iceberg table.\n+     */\n+    public Builder tableLoader(TableLoader newTableLoader) {\n+      this.tableLoader = newTableLoader;\n+      return this;\n+    }\n+\n+    public Builder hadoopConf(Configuration newHadoopConf) {\n+      this.hadoopConf = newHadoopConf;\n+      return this;\n+    }\n+\n+    public Builder tableSchema(TableSchema newTableSchema) {\n+      this.tableSchema = newTableSchema;\n+      return this;\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    public DataStreamSink<RowData> build() {\n+      Preconditions.checkArgument(rowDataInput != null,\n+          \"Please use forRowData() to initialize the input DataStream.\");\n+      Preconditions.checkNotNull(table, \"Table shouldn't be null\");\n+      Preconditions.checkNotNull(tableLoader, \"Table loader shouldn't be null\");\n+      Preconditions.checkNotNull(hadoopConf, \"Hadoop configuration shouldn't be null\");\n+\n+      IcebergStreamWriter<RowData> streamWriter = createStreamWriter(table, tableSchema);\n+      IcebergFilesCommitter filesCommitter = new IcebergFilesCommitter(tableLoader, hadoopConf);\n+\n+      DataStream<Void> returnStream = rowDataInput\n+          .transform(ICEBERG_STREAM_WRITER_NAME, TypeInformation.of(DataFile.class), streamWriter)\n+          .setParallelism(rowDataInput.getParallelism())\n+          .transform(ICEBERG_FILES_COMMITTER_NAME, Types.VOID, filesCommitter)\n+          .setParallelism(1)\n+          .setMaxParallelism(1);\n+\n+      return returnStream.addSink(new DiscardingSink())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e674ed7182289f3c6132c0ca2e76f0fd2d0b6871"}, "originalPosition": 176}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc3OTE4OTE0", "url": "https://github.com/apache/iceberg/pull/1185#pullrequestreview-477918914", "createdAt": "2020-08-28T17:49:25Z", "commit": {"oid": "e674ed7182289f3c6132c0ca2e76f0fd2d0b6871"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQxNzo0OToyNVrOHJPYww==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQxNzo0OToyNVrOHJPYww==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTQ1MTMzMQ==", "bodyText": "this is a good default value for writer parallelism. we have users who want to explicitly control the writer parallelism to control the number of written files. in the future, we may want to allow user to set parallelism in the builder.", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r479451331", "createdAt": "2020-08-28T17:49:25Z", "author": {"login": "stevenzwu"}, "path": "flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java", "diffHunk": "@@ -0,0 +1,220 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.flink.api.common.functions.MapFunction;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeinfo.Types;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.DataStreamSink;\n+import org.apache.flink.streaming.api.functions.sink.DiscardingSink;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.util.DataFormatConverters;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.PropertyUtil;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT_DEFAULT;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT;\n+\n+public class FlinkSink {\n+\n+  private static final String ICEBERG_STREAM_WRITER_NAME = IcebergStreamWriter.class.getSimpleName();\n+  private static final String ICEBERG_FILES_COMMITTER_NAME = IcebergFilesCommitter.class.getSimpleName();\n+\n+  private FlinkSink() {\n+  }\n+\n+  /**\n+   * Initialize a {@link Builder} to export the data from generic input data stream into iceberg table. We use\n+   * {@link RowData} inside the sink connector, so users need to provide a mapper function and a\n+   * {@link TypeInformation} to convert those generic records to a RowData DataStream.\n+   *\n+   * @param input      the generic source input data stream.\n+   * @param mapper     function to convert the generic data to {@link RowData}\n+   * @param outputType to define the {@link TypeInformation} for the input data.\n+   * @param <T>        the data type of records.\n+   * @return {@link Builder} to connect the iceberg table.\n+   */\n+  public static <T> Builder builderFor(DataStream<T> input,\n+                                       MapFunction<T, RowData> mapper,\n+                                       TypeInformation<RowData> outputType) {\n+    DataStream<RowData> dataStream = input.map(mapper, outputType);\n+    return forRowData(dataStream);\n+  }\n+\n+  /**\n+   * Initialize a {@link Builder} to export the data from input data stream with {@link Row}s into iceberg table. We use\n+   * {@link RowData} inside the sink connector, so users need to provide a {@link TableSchema} for builder to convert\n+   * those {@link Row}s to a {@link RowData} DataStream.\n+   *\n+   * @param input       the source input data stream with {@link Row}s.\n+   * @param tableSchema defines the {@link TypeInformation} for input data.\n+   * @return {@link Builder} to connect the iceberg table.\n+   */\n+  public static Builder forRow(DataStream<Row> input, TableSchema tableSchema) {\n+    RowType rowType = (RowType) tableSchema.toRowDataType().getLogicalType();\n+    DataType[] fieldDataTypes = tableSchema.getFieldDataTypes();\n+\n+    DataFormatConverters.RowConverter rowConverter = new DataFormatConverters.RowConverter(fieldDataTypes);\n+    return builderFor(input, rowConverter::toInternal, RowDataTypeInfo.of(rowType))\n+        .tableSchema(tableSchema);\n+  }\n+\n+  /**\n+   * Initialize a {@link Builder} to export the data from input data stream with {@link RowData}s into iceberg table.\n+   *\n+   * @param input the source input data stream with {@link RowData}s.\n+   * @return {@link Builder} to connect the iceberg table.\n+   */\n+  public static Builder forRowData(DataStream<RowData> input) {\n+    return new Builder().forRowData(input);\n+  }\n+\n+  public static class Builder {\n+    private DataStream<RowData> rowDataInput = null;\n+    private TableLoader tableLoader;\n+    private Configuration hadoopConf;\n+    private Table table;\n+    private TableSchema tableSchema;\n+\n+    private Builder() {\n+    }\n+\n+    private Builder forRowData(DataStream<RowData> newRowDataInput) {\n+      this.rowDataInput = newRowDataInput;\n+      return this;\n+    }\n+\n+    /**\n+     * This iceberg {@link Table} instance is used for initializing {@link IcebergStreamWriter} which will write all\n+     * the records into {@link DataFile}s and emit them to downstream operator. Providing a table would avoid so many\n+     * table loading from each separate task.\n+     *\n+     * @param newTable the loaded iceberg table instance.\n+     * @return {@link Builder} to connect the iceberg table.\n+     */\n+    public Builder table(Table newTable) {\n+      this.table = newTable;\n+      return this;\n+    }\n+\n+    /**\n+     * The table loader is used for loading tables in {@link IcebergFilesCommitter} lazily, we need this loader because\n+     * {@link Table} is not serializable and could not just use the loaded table from Builder#table in the remote task\n+     * manager.\n+     *\n+     * @param newTableLoader to load iceberg table inside tasks.\n+     * @return {@link Builder} to connect the iceberg table.\n+     */\n+    public Builder tableLoader(TableLoader newTableLoader) {\n+      this.tableLoader = newTableLoader;\n+      return this;\n+    }\n+\n+    public Builder hadoopConf(Configuration newHadoopConf) {\n+      this.hadoopConf = newHadoopConf;\n+      return this;\n+    }\n+\n+    public Builder tableSchema(TableSchema newTableSchema) {\n+      this.tableSchema = newTableSchema;\n+      return this;\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    public DataStreamSink<RowData> build() {\n+      Preconditions.checkArgument(rowDataInput != null,\n+          \"Please use forRowData() to initialize the input DataStream.\");\n+      Preconditions.checkNotNull(table, \"Table shouldn't be null\");\n+      Preconditions.checkNotNull(tableLoader, \"Table loader shouldn't be null\");\n+      Preconditions.checkNotNull(hadoopConf, \"Hadoop configuration shouldn't be null\");\n+\n+      IcebergStreamWriter<RowData> streamWriter = createStreamWriter(table, tableSchema);\n+      IcebergFilesCommitter filesCommitter = new IcebergFilesCommitter(tableLoader, hadoopConf);\n+\n+      DataStream<Void> returnStream = rowDataInput\n+          .transform(ICEBERG_STREAM_WRITER_NAME, TypeInformation.of(DataFile.class), streamWriter)\n+          .setParallelism(rowDataInput.getParallelism())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e674ed7182289f3c6132c0ca2e76f0fd2d0b6871"}, "originalPosition": 171}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc4MDAxNTc2", "url": "https://github.com/apache/iceberg/pull/1185#pullrequestreview-478001576", "createdAt": "2020-08-28T20:16:48Z", "commit": {"oid": "e674ed7182289f3c6132c0ca2e76f0fd2d0b6871"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQyMDoxNjo0OFrOHJTTLg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQyMDoxNjo0OFrOHJTTLg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUxNTQzOA==", "bodyText": "nice. this is the reverse iteration I was looking for. we were just walking through the table.snapshots() iterable.", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r479515438", "createdAt": "2020-08-28T20:16:48Z", "author": {"login": "stevenzwu"}, "path": "flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java", "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NavigableMap;\n+import java.util.SortedMap;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.BasicTypeInfo;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.ListTypeInfo;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.BoundedOneInput;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.runtime.typeutils.SortedMapTypeInfo;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.types.Comparators;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+class IcebergFilesCommitter extends AbstractStreamOperator<Void>\n+    implements OneInputStreamOperator<DataFile, Void>, BoundedOneInput {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final long INITIAL_CHECKPOINT_ID = -1L;\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergFilesCommitter.class);\n+  private static final String FLINK_JOB_ID = \"flink.job-id\";\n+\n+  // The max checkpoint id we've committed to iceberg table. As the flink's checkpoint is always increasing, so we could\n+  // correctly commit all the data files whose checkpoint id is greater than the max committed one to iceberg table, for\n+  // avoiding committing the same data files twice. This id will be attached to iceberg's meta when committing the\n+  // iceberg transaction.\n+  private static final String MAX_COMMITTED_CHECKPOINT_ID = \"flink.max-committed-checkpoint-id\";\n+\n+  // TableLoader to load iceberg table lazily.\n+  private final TableLoader tableLoader;\n+  private final SerializableConfiguration hadoopConf;\n+\n+  // A sorted map to maintain the completed data files for each pending checkpointId (which have not been committed\n+  // to iceberg table). We need a sorted map here because there's possible that few checkpoints snapshot failed, for\n+  // example: the 1st checkpoint have 2 data files <1, <file0, file1>>, the 2st checkpoint have 1 data files\n+  // <2, <file3>>. Snapshot for checkpoint#1 interrupted because of network/disk failure etc, while we don't expect\n+  // any data loss in iceberg table. So we keep the finished files <1, <file0, file1>> in memory and retry to commit\n+  // iceberg table when the next checkpoint happen.\n+  private final NavigableMap<Long, List<DataFile>> dataFilesPerCheckpoint = Maps.newTreeMap();\n+\n+  // The data files cache for current checkpoint. Once the snapshot barrier received, it will be flushed to the\n+  // 'dataFilesPerCheckpoint'.\n+  private final List<DataFile> dataFilesOfCurrentCheckpoint = Lists.newArrayList();\n+\n+  // It will have an unique identifier for one job.\n+  private transient String flinkJobId;\n+  private transient Table table;\n+  private transient long maxCommittedCheckpointId;\n+\n+  // All pending checkpoints states for this function.\n+  private static final ListStateDescriptor<SortedMap<Long, List<DataFile>>> STATE_DESCRIPTOR = buildStateDescriptor();\n+  private transient ListState<SortedMap<Long, List<DataFile>>> checkpointsState;\n+\n+  IcebergFilesCommitter(TableLoader tableLoader, Configuration hadoopConf) {\n+    this.tableLoader = tableLoader;\n+    this.hadoopConf = new SerializableConfiguration(hadoopConf);\n+  }\n+\n+  @Override\n+  public void initializeState(StateInitializationContext context) throws Exception {\n+    super.initializeState(context);\n+    this.flinkJobId = getContainingTask().getEnvironment().getJobID().toString();\n+\n+    // Open the table loader and load the table.\n+    this.tableLoader.open(hadoopConf.get());\n+    this.table = tableLoader.loadTable();\n+    this.maxCommittedCheckpointId = INITIAL_CHECKPOINT_ID;\n+\n+    this.checkpointsState = context.getOperatorStateStore().getListState(STATE_DESCRIPTOR);\n+    if (context.isRestored()) {\n+      this.maxCommittedCheckpointId = getMaxCommittedCheckpointId(table, flinkJobId);\n+      // In the restoring path, it should have one valid snapshot for current flink job at least, so the max committed\n+      // checkpoint id should be positive. If it's not positive, that means someone might have removed or expired the\n+      // iceberg snapshot, in that case we should throw an exception in case of committing duplicated data files into\n+      // the iceberg table.\n+      Preconditions.checkState(maxCommittedCheckpointId != INITIAL_CHECKPOINT_ID,\n+          \"There should be an existing iceberg snapshot for current flink job: %s\", flinkJobId);\n+\n+      SortedMap<Long, List<DataFile>> restoredDataFiles = checkpointsState.get().iterator().next();\n+      // Only keep the uncommitted data files in the cache.\n+      this.dataFilesPerCheckpoint.putAll(restoredDataFiles.tailMap(maxCommittedCheckpointId + 1));\n+    }\n+  }\n+\n+  @Override\n+  public void snapshotState(StateSnapshotContext context) throws Exception {\n+    super.snapshotState(context);\n+    long checkpointId = context.getCheckpointId();\n+    LOG.info(\"Start to flush snapshot state to state backend, table: {}, checkpointId: {}\", table, checkpointId);\n+\n+    // Update the checkpoint state.\n+    dataFilesPerCheckpoint.put(checkpointId, ImmutableList.copyOf(dataFilesOfCurrentCheckpoint));\n+\n+    // Reset the snapshot state to the latest state.\n+    checkpointsState.clear();\n+    checkpointsState.add(dataFilesPerCheckpoint);\n+\n+    // Clear the local buffer for current checkpoint.\n+    dataFilesOfCurrentCheckpoint.clear();\n+  }\n+\n+  @Override\n+  public void notifyCheckpointComplete(long checkpointId) throws Exception {\n+    super.notifyCheckpointComplete(checkpointId);\n+    // It's possible that we have the following events:\n+    //   1. snapshotState(ckpId);\n+    //   2. snapshotState(ckpId+1);\n+    //   3. notifyCheckpointComplete(ckpId+1);\n+    //   4. notifyCheckpointComplete(ckpId);\n+    // For step#4, we don't need to commit iceberg table again because in step#3 we've committed all the files,\n+    // Besides, we need to maintain the max-committed-checkpoint-id to be increasing.\n+    if (checkpointId > maxCommittedCheckpointId) {\n+      commitUpToCheckpoint(checkpointId);\n+      this.maxCommittedCheckpointId = checkpointId;\n+    }\n+  }\n+\n+  private void commitUpToCheckpoint(long checkpointId) {\n+    NavigableMap<Long, List<DataFile>> pendingFileMap = dataFilesPerCheckpoint.headMap(checkpointId, true);\n+\n+    List<DataFile> pendingDataFiles = Lists.newArrayList();\n+    for (List<DataFile> dataFiles : pendingFileMap.values()) {\n+      pendingDataFiles.addAll(dataFiles);\n+    }\n+\n+    AppendFiles appendFiles = table.newAppend();\n+    pendingDataFiles.forEach(appendFiles::appendFile);\n+    appendFiles.set(MAX_COMMITTED_CHECKPOINT_ID, Long.toString(checkpointId));\n+    appendFiles.set(FLINK_JOB_ID, flinkJobId);\n+    appendFiles.commit();\n+\n+    // Clear the committed data files from dataFilesPerCheckpoint.\n+    pendingFileMap.clear();\n+  }\n+\n+  @Override\n+  public void processElement(StreamRecord<DataFile> element) {\n+    this.dataFilesOfCurrentCheckpoint.add(element.getValue());\n+  }\n+\n+  @Override\n+  public void endInput() {\n+    // Flush the buffered data files into 'dataFilesPerCheckpoint' firstly.\n+    dataFilesPerCheckpoint.put(Long.MAX_VALUE, ImmutableList.copyOf(dataFilesOfCurrentCheckpoint));\n+    dataFilesOfCurrentCheckpoint.clear();\n+\n+    commitUpToCheckpoint(Long.MAX_VALUE);\n+  }\n+\n+  @Override\n+  public void dispose() throws Exception {\n+    if (tableLoader != null) {\n+      tableLoader.close();\n+    }\n+  }\n+\n+  private static ListStateDescriptor<SortedMap<Long, List<DataFile>>> buildStateDescriptor() {\n+    Comparator<Long> longComparator = Comparators.forType(Types.LongType.get());\n+    // Construct a ListTypeInfo.\n+    ListTypeInfo<DataFile> dataFileListTypeInfo = new ListTypeInfo<>(TypeInformation.of(DataFile.class));\n+    // Construct a SortedMapTypeInfo.\n+    SortedMapTypeInfo<Long, List<DataFile>> sortedMapTypeInfo = new SortedMapTypeInfo<>(\n+        BasicTypeInfo.LONG_TYPE_INFO, dataFileListTypeInfo, longComparator\n+    );\n+    return new ListStateDescriptor<>(\"iceberg-files-committer-state\", sortedMapTypeInfo);\n+  }\n+\n+  static long getMaxCommittedCheckpointId(Table table, String flinkJobId) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e674ed7182289f3c6132c0ca2e76f0fd2d0b6871"}, "originalPosition": 209}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc4MDM0MTky", "url": "https://github.com/apache/iceberg/pull/1185#pullrequestreview-478034192", "createdAt": "2020-08-28T21:25:19Z", "commit": {"oid": "e674ed7182289f3c6132c0ca2e76f0fd2d0b6871"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQyMToyNToxOVrOHJU4ZQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQyMToyNToxOVrOHJU4ZQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU0MTM0OQ==", "bodyText": "I am wondering if Flink guarantees the serialized execution for notifyCheckpointComplete?", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r479541349", "createdAt": "2020-08-28T21:25:19Z", "author": {"login": "stevenzwu"}, "path": "flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java", "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NavigableMap;\n+import java.util.SortedMap;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.BasicTypeInfo;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.ListTypeInfo;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.BoundedOneInput;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.runtime.typeutils.SortedMapTypeInfo;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.types.Comparators;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+class IcebergFilesCommitter extends AbstractStreamOperator<Void>\n+    implements OneInputStreamOperator<DataFile, Void>, BoundedOneInput {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final long INITIAL_CHECKPOINT_ID = -1L;\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergFilesCommitter.class);\n+  private static final String FLINK_JOB_ID = \"flink.job-id\";\n+\n+  // The max checkpoint id we've committed to iceberg table. As the flink's checkpoint is always increasing, so we could\n+  // correctly commit all the data files whose checkpoint id is greater than the max committed one to iceberg table, for\n+  // avoiding committing the same data files twice. This id will be attached to iceberg's meta when committing the\n+  // iceberg transaction.\n+  private static final String MAX_COMMITTED_CHECKPOINT_ID = \"flink.max-committed-checkpoint-id\";\n+\n+  // TableLoader to load iceberg table lazily.\n+  private final TableLoader tableLoader;\n+  private final SerializableConfiguration hadoopConf;\n+\n+  // A sorted map to maintain the completed data files for each pending checkpointId (which have not been committed\n+  // to iceberg table). We need a sorted map here because there's possible that few checkpoints snapshot failed, for\n+  // example: the 1st checkpoint have 2 data files <1, <file0, file1>>, the 2st checkpoint have 1 data files\n+  // <2, <file3>>. Snapshot for checkpoint#1 interrupted because of network/disk failure etc, while we don't expect\n+  // any data loss in iceberg table. So we keep the finished files <1, <file0, file1>> in memory and retry to commit\n+  // iceberg table when the next checkpoint happen.\n+  private final NavigableMap<Long, List<DataFile>> dataFilesPerCheckpoint = Maps.newTreeMap();\n+\n+  // The data files cache for current checkpoint. Once the snapshot barrier received, it will be flushed to the\n+  // 'dataFilesPerCheckpoint'.\n+  private final List<DataFile> dataFilesOfCurrentCheckpoint = Lists.newArrayList();\n+\n+  // It will have an unique identifier for one job.\n+  private transient String flinkJobId;\n+  private transient Table table;\n+  private transient long maxCommittedCheckpointId;\n+\n+  // All pending checkpoints states for this function.\n+  private static final ListStateDescriptor<SortedMap<Long, List<DataFile>>> STATE_DESCRIPTOR = buildStateDescriptor();\n+  private transient ListState<SortedMap<Long, List<DataFile>>> checkpointsState;\n+\n+  IcebergFilesCommitter(TableLoader tableLoader, Configuration hadoopConf) {\n+    this.tableLoader = tableLoader;\n+    this.hadoopConf = new SerializableConfiguration(hadoopConf);\n+  }\n+\n+  @Override\n+  public void initializeState(StateInitializationContext context) throws Exception {\n+    super.initializeState(context);\n+    this.flinkJobId = getContainingTask().getEnvironment().getJobID().toString();\n+\n+    // Open the table loader and load the table.\n+    this.tableLoader.open(hadoopConf.get());\n+    this.table = tableLoader.loadTable();\n+    this.maxCommittedCheckpointId = INITIAL_CHECKPOINT_ID;\n+\n+    this.checkpointsState = context.getOperatorStateStore().getListState(STATE_DESCRIPTOR);\n+    if (context.isRestored()) {\n+      this.maxCommittedCheckpointId = getMaxCommittedCheckpointId(table, flinkJobId);\n+      // In the restoring path, it should have one valid snapshot for current flink job at least, so the max committed\n+      // checkpoint id should be positive. If it's not positive, that means someone might have removed or expired the\n+      // iceberg snapshot, in that case we should throw an exception in case of committing duplicated data files into\n+      // the iceberg table.\n+      Preconditions.checkState(maxCommittedCheckpointId != INITIAL_CHECKPOINT_ID,\n+          \"There should be an existing iceberg snapshot for current flink job: %s\", flinkJobId);\n+\n+      SortedMap<Long, List<DataFile>> restoredDataFiles = checkpointsState.get().iterator().next();\n+      // Only keep the uncommitted data files in the cache.\n+      this.dataFilesPerCheckpoint.putAll(restoredDataFiles.tailMap(maxCommittedCheckpointId + 1));\n+    }\n+  }\n+\n+  @Override\n+  public void snapshotState(StateSnapshotContext context) throws Exception {\n+    super.snapshotState(context);\n+    long checkpointId = context.getCheckpointId();\n+    LOG.info(\"Start to flush snapshot state to state backend, table: {}, checkpointId: {}\", table, checkpointId);\n+\n+    // Update the checkpoint state.\n+    dataFilesPerCheckpoint.put(checkpointId, ImmutableList.copyOf(dataFilesOfCurrentCheckpoint));\n+\n+    // Reset the snapshot state to the latest state.\n+    checkpointsState.clear();\n+    checkpointsState.add(dataFilesPerCheckpoint);\n+\n+    // Clear the local buffer for current checkpoint.\n+    dataFilesOfCurrentCheckpoint.clear();\n+  }\n+\n+  @Override\n+  public void notifyCheckpointComplete(long checkpointId) throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e674ed7182289f3c6132c0ca2e76f0fd2d0b6871"}, "originalPosition": 144}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc4MDM5NjEw", "url": "https://github.com/apache/iceberg/pull/1185#pullrequestreview-478039610", "createdAt": "2020-08-28T21:39:11Z", "commit": {"oid": "e674ed7182289f3c6132c0ca2e76f0fd2d0b6871"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQyMTozOToxMVrOHJVKew==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQyMTozOToxMVrOHJVKew==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU0NTk3OQ==", "bodyText": "We might have a problem if redeploying the Flink job from external checkpoint. It is a new flinkJobId in this case. maxCommittedCheckpointId will be -1.  As a result, we can commit those committed files again later.\nThe way we do de-dup is to generate a hash for the manifest file path and store the hash in snapshot summary. During restore, we use the hash to de-dup if the manifest file was committed or not.\n        List<String> hashes = new ArrayList<>(flinkManifestFiles.size());\n        AppendFiles appendFiles = transaction.newAppend();\n        for (FlinkManifestFile flinkManifestFile : flinkManifestFiles) {\n          appendFiles.appendManifest(flinkManifestFile);\n          hashes.add(flinkManifestFile.hash());\n        }\n        appendFiles.set(\n            COMMIT_MANIFEST_HASHES_KEY, FlinkManifestFileUtil.hashesListToString(hashes));", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r479545979", "createdAt": "2020-08-28T21:39:11Z", "author": {"login": "stevenzwu"}, "path": "flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java", "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NavigableMap;\n+import java.util.SortedMap;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.BasicTypeInfo;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.ListTypeInfo;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.BoundedOneInput;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.runtime.typeutils.SortedMapTypeInfo;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.types.Comparators;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+class IcebergFilesCommitter extends AbstractStreamOperator<Void>\n+    implements OneInputStreamOperator<DataFile, Void>, BoundedOneInput {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final long INITIAL_CHECKPOINT_ID = -1L;\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergFilesCommitter.class);\n+  private static final String FLINK_JOB_ID = \"flink.job-id\";\n+\n+  // The max checkpoint id we've committed to iceberg table. As the flink's checkpoint is always increasing, so we could\n+  // correctly commit all the data files whose checkpoint id is greater than the max committed one to iceberg table, for\n+  // avoiding committing the same data files twice. This id will be attached to iceberg's meta when committing the\n+  // iceberg transaction.\n+  private static final String MAX_COMMITTED_CHECKPOINT_ID = \"flink.max-committed-checkpoint-id\";\n+\n+  // TableLoader to load iceberg table lazily.\n+  private final TableLoader tableLoader;\n+  private final SerializableConfiguration hadoopConf;\n+\n+  // A sorted map to maintain the completed data files for each pending checkpointId (which have not been committed\n+  // to iceberg table). We need a sorted map here because there's possible that few checkpoints snapshot failed, for\n+  // example: the 1st checkpoint have 2 data files <1, <file0, file1>>, the 2st checkpoint have 1 data files\n+  // <2, <file3>>. Snapshot for checkpoint#1 interrupted because of network/disk failure etc, while we don't expect\n+  // any data loss in iceberg table. So we keep the finished files <1, <file0, file1>> in memory and retry to commit\n+  // iceberg table when the next checkpoint happen.\n+  private final NavigableMap<Long, List<DataFile>> dataFilesPerCheckpoint = Maps.newTreeMap();\n+\n+  // The data files cache for current checkpoint. Once the snapshot barrier received, it will be flushed to the\n+  // 'dataFilesPerCheckpoint'.\n+  private final List<DataFile> dataFilesOfCurrentCheckpoint = Lists.newArrayList();\n+\n+  // It will have an unique identifier for one job.\n+  private transient String flinkJobId;\n+  private transient Table table;\n+  private transient long maxCommittedCheckpointId;\n+\n+  // All pending checkpoints states for this function.\n+  private static final ListStateDescriptor<SortedMap<Long, List<DataFile>>> STATE_DESCRIPTOR = buildStateDescriptor();\n+  private transient ListState<SortedMap<Long, List<DataFile>>> checkpointsState;\n+\n+  IcebergFilesCommitter(TableLoader tableLoader, Configuration hadoopConf) {\n+    this.tableLoader = tableLoader;\n+    this.hadoopConf = new SerializableConfiguration(hadoopConf);\n+  }\n+\n+  @Override\n+  public void initializeState(StateInitializationContext context) throws Exception {\n+    super.initializeState(context);\n+    this.flinkJobId = getContainingTask().getEnvironment().getJobID().toString();\n+\n+    // Open the table loader and load the table.\n+    this.tableLoader.open(hadoopConf.get());\n+    this.table = tableLoader.loadTable();\n+    this.maxCommittedCheckpointId = INITIAL_CHECKPOINT_ID;\n+\n+    this.checkpointsState = context.getOperatorStateStore().getListState(STATE_DESCRIPTOR);\n+    if (context.isRestored()) {\n+      this.maxCommittedCheckpointId = getMaxCommittedCheckpointId(table, flinkJobId);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e674ed7182289f3c6132c0ca2e76f0fd2d0b6871"}, "originalPosition": 112}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc4MDgyOTI2", "url": "https://github.com/apache/iceberg/pull/1185#pullrequestreview-478082926", "createdAt": "2020-08-29T00:45:38Z", "commit": {"oid": "e674ed7182289f3c6132c0ca2e76f0fd2d0b6871"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOVQwMDo0NTozOVrOHJXmYA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOVQwMDo0NTozOVrOHJXmYA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU4NTg4OA==", "bodyText": "In our implementation, we immediately commit any uncommitted files upon restore to avoid waiting for another checkpoint cycle.", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r479585888", "createdAt": "2020-08-29T00:45:39Z", "author": {"login": "stevenzwu"}, "path": "flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java", "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NavigableMap;\n+import java.util.SortedMap;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.BasicTypeInfo;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.ListTypeInfo;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.BoundedOneInput;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.runtime.typeutils.SortedMapTypeInfo;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.types.Comparators;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+class IcebergFilesCommitter extends AbstractStreamOperator<Void>\n+    implements OneInputStreamOperator<DataFile, Void>, BoundedOneInput {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final long INITIAL_CHECKPOINT_ID = -1L;\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergFilesCommitter.class);\n+  private static final String FLINK_JOB_ID = \"flink.job-id\";\n+\n+  // The max checkpoint id we've committed to iceberg table. As the flink's checkpoint is always increasing, so we could\n+  // correctly commit all the data files whose checkpoint id is greater than the max committed one to iceberg table, for\n+  // avoiding committing the same data files twice. This id will be attached to iceberg's meta when committing the\n+  // iceberg transaction.\n+  private static final String MAX_COMMITTED_CHECKPOINT_ID = \"flink.max-committed-checkpoint-id\";\n+\n+  // TableLoader to load iceberg table lazily.\n+  private final TableLoader tableLoader;\n+  private final SerializableConfiguration hadoopConf;\n+\n+  // A sorted map to maintain the completed data files for each pending checkpointId (which have not been committed\n+  // to iceberg table). We need a sorted map here because there's possible that few checkpoints snapshot failed, for\n+  // example: the 1st checkpoint have 2 data files <1, <file0, file1>>, the 2st checkpoint have 1 data files\n+  // <2, <file3>>. Snapshot for checkpoint#1 interrupted because of network/disk failure etc, while we don't expect\n+  // any data loss in iceberg table. So we keep the finished files <1, <file0, file1>> in memory and retry to commit\n+  // iceberg table when the next checkpoint happen.\n+  private final NavigableMap<Long, List<DataFile>> dataFilesPerCheckpoint = Maps.newTreeMap();\n+\n+  // The data files cache for current checkpoint. Once the snapshot barrier received, it will be flushed to the\n+  // 'dataFilesPerCheckpoint'.\n+  private final List<DataFile> dataFilesOfCurrentCheckpoint = Lists.newArrayList();\n+\n+  // It will have an unique identifier for one job.\n+  private transient String flinkJobId;\n+  private transient Table table;\n+  private transient long maxCommittedCheckpointId;\n+\n+  // All pending checkpoints states for this function.\n+  private static final ListStateDescriptor<SortedMap<Long, List<DataFile>>> STATE_DESCRIPTOR = buildStateDescriptor();\n+  private transient ListState<SortedMap<Long, List<DataFile>>> checkpointsState;\n+\n+  IcebergFilesCommitter(TableLoader tableLoader, Configuration hadoopConf) {\n+    this.tableLoader = tableLoader;\n+    this.hadoopConf = new SerializableConfiguration(hadoopConf);\n+  }\n+\n+  @Override\n+  public void initializeState(StateInitializationContext context) throws Exception {\n+    super.initializeState(context);\n+    this.flinkJobId = getContainingTask().getEnvironment().getJobID().toString();\n+\n+    // Open the table loader and load the table.\n+    this.tableLoader.open(hadoopConf.get());\n+    this.table = tableLoader.loadTable();\n+    this.maxCommittedCheckpointId = INITIAL_CHECKPOINT_ID;\n+\n+    this.checkpointsState = context.getOperatorStateStore().getListState(STATE_DESCRIPTOR);\n+    if (context.isRestored()) {\n+      this.maxCommittedCheckpointId = getMaxCommittedCheckpointId(table, flinkJobId);\n+      // In the restoring path, it should have one valid snapshot for current flink job at least, so the max committed\n+      // checkpoint id should be positive. If it's not positive, that means someone might have removed or expired the\n+      // iceberg snapshot, in that case we should throw an exception in case of committing duplicated data files into\n+      // the iceberg table.\n+      Preconditions.checkState(maxCommittedCheckpointId != INITIAL_CHECKPOINT_ID,\n+          \"There should be an existing iceberg snapshot for current flink job: %s\", flinkJobId);\n+\n+      SortedMap<Long, List<DataFile>> restoredDataFiles = checkpointsState.get().iterator().next();\n+      // Only keep the uncommitted data files in the cache.\n+      this.dataFilesPerCheckpoint.putAll(restoredDataFiles.tailMap(maxCommittedCheckpointId + 1));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e674ed7182289f3c6132c0ca2e76f0fd2d0b6871"}, "originalPosition": 122}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc4MDg1Mjcz", "url": "https://github.com/apache/iceberg/pull/1185#pullrequestreview-478085273", "createdAt": "2020-08-29T01:09:53Z", "commit": {"oid": "e674ed7182289f3c6132c0ca2e76f0fd2d0b6871"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOVQwMTowOTo1M1rOHJXxNw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOVQwMTowOTo1M1rOHJXxNw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU4ODY2Mw==", "bodyText": "Just want to point out that this will commit if there is zero pending files. For us, we actually still want to commit in this case mainly to update the region watermark info.", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r479588663", "createdAt": "2020-08-29T01:09:53Z", "author": {"login": "stevenzwu"}, "path": "flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java", "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NavigableMap;\n+import java.util.SortedMap;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.BasicTypeInfo;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.ListTypeInfo;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.BoundedOneInput;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.runtime.typeutils.SortedMapTypeInfo;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.types.Comparators;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+class IcebergFilesCommitter extends AbstractStreamOperator<Void>\n+    implements OneInputStreamOperator<DataFile, Void>, BoundedOneInput {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final long INITIAL_CHECKPOINT_ID = -1L;\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergFilesCommitter.class);\n+  private static final String FLINK_JOB_ID = \"flink.job-id\";\n+\n+  // The max checkpoint id we've committed to iceberg table. As the flink's checkpoint is always increasing, so we could\n+  // correctly commit all the data files whose checkpoint id is greater than the max committed one to iceberg table, for\n+  // avoiding committing the same data files twice. This id will be attached to iceberg's meta when committing the\n+  // iceberg transaction.\n+  private static final String MAX_COMMITTED_CHECKPOINT_ID = \"flink.max-committed-checkpoint-id\";\n+\n+  // TableLoader to load iceberg table lazily.\n+  private final TableLoader tableLoader;\n+  private final SerializableConfiguration hadoopConf;\n+\n+  // A sorted map to maintain the completed data files for each pending checkpointId (which have not been committed\n+  // to iceberg table). We need a sorted map here because there's possible that few checkpoints snapshot failed, for\n+  // example: the 1st checkpoint have 2 data files <1, <file0, file1>>, the 2st checkpoint have 1 data files\n+  // <2, <file3>>. Snapshot for checkpoint#1 interrupted because of network/disk failure etc, while we don't expect\n+  // any data loss in iceberg table. So we keep the finished files <1, <file0, file1>> in memory and retry to commit\n+  // iceberg table when the next checkpoint happen.\n+  private final NavigableMap<Long, List<DataFile>> dataFilesPerCheckpoint = Maps.newTreeMap();\n+\n+  // The data files cache for current checkpoint. Once the snapshot barrier received, it will be flushed to the\n+  // 'dataFilesPerCheckpoint'.\n+  private final List<DataFile> dataFilesOfCurrentCheckpoint = Lists.newArrayList();\n+\n+  // It will have an unique identifier for one job.\n+  private transient String flinkJobId;\n+  private transient Table table;\n+  private transient long maxCommittedCheckpointId;\n+\n+  // All pending checkpoints states for this function.\n+  private static final ListStateDescriptor<SortedMap<Long, List<DataFile>>> STATE_DESCRIPTOR = buildStateDescriptor();\n+  private transient ListState<SortedMap<Long, List<DataFile>>> checkpointsState;\n+\n+  IcebergFilesCommitter(TableLoader tableLoader, Configuration hadoopConf) {\n+    this.tableLoader = tableLoader;\n+    this.hadoopConf = new SerializableConfiguration(hadoopConf);\n+  }\n+\n+  @Override\n+  public void initializeState(StateInitializationContext context) throws Exception {\n+    super.initializeState(context);\n+    this.flinkJobId = getContainingTask().getEnvironment().getJobID().toString();\n+\n+    // Open the table loader and load the table.\n+    this.tableLoader.open(hadoopConf.get());\n+    this.table = tableLoader.loadTable();\n+    this.maxCommittedCheckpointId = INITIAL_CHECKPOINT_ID;\n+\n+    this.checkpointsState = context.getOperatorStateStore().getListState(STATE_DESCRIPTOR);\n+    if (context.isRestored()) {\n+      this.maxCommittedCheckpointId = getMaxCommittedCheckpointId(table, flinkJobId);\n+      // In the restoring path, it should have one valid snapshot for current flink job at least, so the max committed\n+      // checkpoint id should be positive. If it's not positive, that means someone might have removed or expired the\n+      // iceberg snapshot, in that case we should throw an exception in case of committing duplicated data files into\n+      // the iceberg table.\n+      Preconditions.checkState(maxCommittedCheckpointId != INITIAL_CHECKPOINT_ID,\n+          \"There should be an existing iceberg snapshot for current flink job: %s\", flinkJobId);\n+\n+      SortedMap<Long, List<DataFile>> restoredDataFiles = checkpointsState.get().iterator().next();\n+      // Only keep the uncommitted data files in the cache.\n+      this.dataFilesPerCheckpoint.putAll(restoredDataFiles.tailMap(maxCommittedCheckpointId + 1));\n+    }\n+  }\n+\n+  @Override\n+  public void snapshotState(StateSnapshotContext context) throws Exception {\n+    super.snapshotState(context);\n+    long checkpointId = context.getCheckpointId();\n+    LOG.info(\"Start to flush snapshot state to state backend, table: {}, checkpointId: {}\", table, checkpointId);\n+\n+    // Update the checkpoint state.\n+    dataFilesPerCheckpoint.put(checkpointId, ImmutableList.copyOf(dataFilesOfCurrentCheckpoint));\n+\n+    // Reset the snapshot state to the latest state.\n+    checkpointsState.clear();\n+    checkpointsState.add(dataFilesPerCheckpoint);\n+\n+    // Clear the local buffer for current checkpoint.\n+    dataFilesOfCurrentCheckpoint.clear();\n+  }\n+\n+  @Override\n+  public void notifyCheckpointComplete(long checkpointId) throws Exception {\n+    super.notifyCheckpointComplete(checkpointId);\n+    // It's possible that we have the following events:\n+    //   1. snapshotState(ckpId);\n+    //   2. snapshotState(ckpId+1);\n+    //   3. notifyCheckpointComplete(ckpId+1);\n+    //   4. notifyCheckpointComplete(ckpId);\n+    // For step#4, we don't need to commit iceberg table again because in step#3 we've committed all the files,\n+    // Besides, we need to maintain the max-committed-checkpoint-id to be increasing.\n+    if (checkpointId > maxCommittedCheckpointId) {\n+      commitUpToCheckpoint(checkpointId);\n+      this.maxCommittedCheckpointId = checkpointId;\n+    }\n+  }\n+\n+  private void commitUpToCheckpoint(long checkpointId) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e674ed7182289f3c6132c0ca2e76f0fd2d0b6871"}, "originalPosition": 159}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc4MDg2MDM5", "url": "https://github.com/apache/iceberg/pull/1185#pullrequestreview-478086039", "createdAt": "2020-08-29T01:18:51Z", "commit": {"oid": "e674ed7182289f3c6132c0ca2e76f0fd2d0b6871"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOVQwMToxODo1MVrOHJX1Hw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOVQwMToxODo1MVrOHJX1Hw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU4OTY2Mw==", "bodyText": "this probably gets complicated when we allow concurrent checkpoints. Committer can receive files from both checkpoint N and N+1. We probably need add the checkpointId in the DataFile. It might make sense to provide FlinkDataFile wrapper so that we can add Flink additional metadata.\nWe have FlinkDataFile in our implementation for transmitting low and high timestamps. Now thinking about this issue. Maybe we can include checkpointId too so that committer can distinguish data files from different checkpoints.\npublic class FlinkDataFile implements Serializable {\n  private final long lowWatermark;\n  private final long highWatermark;\n  private final DataFile dataFile;\n\nThis does imposes additional requirement on the writer. It needs to know the last/next checkpointId.\n\nfor job started without checkpoint, the last checkpointId is 0\nfor job started with checkpoint, now IcebergWriter needs to know the last checkpointId retored.\n\nI couldn't find the checkpointId from the restored context in the initializeState(context) method for either AbstractStreamOperator or RichSinkFunction. It will be nice if it can be exposed.\nAlternatively, we can store the nextCheckpointId in the operator state. However, they also have some problems.\n\noperator list state can't deal with rescale as new subtasks won't get state\noperator union list state is not scalable. Kafka source is suffering the scalability issue with union state.\n\nNote that we may flush file before checkpoint barrier comes. We have two use cases to need the rollover by time and file size.\n\nAs I mentioned in another comment, we have Flink streaming jobs running in 3 regions in AWS and data warehouses live only in us-east-1. There is a backend service monitor files in two other remote regions and lift/copy them back to the us-east-1 home region. S3 cross-region file copy has 5 GB limit. For that reason, we flushes files if the size reaches 4 GB.\nWhen we implement rough ordering for Iceberg source, we need event time alignment. In these cases, we use Kafka broker time as event time since our Iceberg source tries to emulate the rough ordering for Kafka source. One possible solution is to roll over the file when the min and max timestamp reached certain threshold (e.g. 5 minutes).", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r479589663", "createdAt": "2020-08-29T01:18:51Z", "author": {"login": "stevenzwu"}, "path": "flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java", "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NavigableMap;\n+import java.util.SortedMap;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.BasicTypeInfo;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.ListTypeInfo;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.BoundedOneInput;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.runtime.typeutils.SortedMapTypeInfo;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.types.Comparators;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+class IcebergFilesCommitter extends AbstractStreamOperator<Void>\n+    implements OneInputStreamOperator<DataFile, Void>, BoundedOneInput {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final long INITIAL_CHECKPOINT_ID = -1L;\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergFilesCommitter.class);\n+  private static final String FLINK_JOB_ID = \"flink.job-id\";\n+\n+  // The max checkpoint id we've committed to iceberg table. As the flink's checkpoint is always increasing, so we could\n+  // correctly commit all the data files whose checkpoint id is greater than the max committed one to iceberg table, for\n+  // avoiding committing the same data files twice. This id will be attached to iceberg's meta when committing the\n+  // iceberg transaction.\n+  private static final String MAX_COMMITTED_CHECKPOINT_ID = \"flink.max-committed-checkpoint-id\";\n+\n+  // TableLoader to load iceberg table lazily.\n+  private final TableLoader tableLoader;\n+  private final SerializableConfiguration hadoopConf;\n+\n+  // A sorted map to maintain the completed data files for each pending checkpointId (which have not been committed\n+  // to iceberg table). We need a sorted map here because there's possible that few checkpoints snapshot failed, for\n+  // example: the 1st checkpoint have 2 data files <1, <file0, file1>>, the 2st checkpoint have 1 data files\n+  // <2, <file3>>. Snapshot for checkpoint#1 interrupted because of network/disk failure etc, while we don't expect\n+  // any data loss in iceberg table. So we keep the finished files <1, <file0, file1>> in memory and retry to commit\n+  // iceberg table when the next checkpoint happen.\n+  private final NavigableMap<Long, List<DataFile>> dataFilesPerCheckpoint = Maps.newTreeMap();\n+\n+  // The data files cache for current checkpoint. Once the snapshot barrier received, it will be flushed to the\n+  // 'dataFilesPerCheckpoint'.\n+  private final List<DataFile> dataFilesOfCurrentCheckpoint = Lists.newArrayList();\n+\n+  // It will have an unique identifier for one job.\n+  private transient String flinkJobId;\n+  private transient Table table;\n+  private transient long maxCommittedCheckpointId;\n+\n+  // All pending checkpoints states for this function.\n+  private static final ListStateDescriptor<SortedMap<Long, List<DataFile>>> STATE_DESCRIPTOR = buildStateDescriptor();\n+  private transient ListState<SortedMap<Long, List<DataFile>>> checkpointsState;\n+\n+  IcebergFilesCommitter(TableLoader tableLoader, Configuration hadoopConf) {\n+    this.tableLoader = tableLoader;\n+    this.hadoopConf = new SerializableConfiguration(hadoopConf);\n+  }\n+\n+  @Override\n+  public void initializeState(StateInitializationContext context) throws Exception {\n+    super.initializeState(context);\n+    this.flinkJobId = getContainingTask().getEnvironment().getJobID().toString();\n+\n+    // Open the table loader and load the table.\n+    this.tableLoader.open(hadoopConf.get());\n+    this.table = tableLoader.loadTable();\n+    this.maxCommittedCheckpointId = INITIAL_CHECKPOINT_ID;\n+\n+    this.checkpointsState = context.getOperatorStateStore().getListState(STATE_DESCRIPTOR);\n+    if (context.isRestored()) {\n+      this.maxCommittedCheckpointId = getMaxCommittedCheckpointId(table, flinkJobId);\n+      // In the restoring path, it should have one valid snapshot for current flink job at least, so the max committed\n+      // checkpoint id should be positive. If it's not positive, that means someone might have removed or expired the\n+      // iceberg snapshot, in that case we should throw an exception in case of committing duplicated data files into\n+      // the iceberg table.\n+      Preconditions.checkState(maxCommittedCheckpointId != INITIAL_CHECKPOINT_ID,\n+          \"There should be an existing iceberg snapshot for current flink job: %s\", flinkJobId);\n+\n+      SortedMap<Long, List<DataFile>> restoredDataFiles = checkpointsState.get().iterator().next();\n+      // Only keep the uncommitted data files in the cache.\n+      this.dataFilesPerCheckpoint.putAll(restoredDataFiles.tailMap(maxCommittedCheckpointId + 1));\n+    }\n+  }\n+\n+  @Override\n+  public void snapshotState(StateSnapshotContext context) throws Exception {\n+    super.snapshotState(context);\n+    long checkpointId = context.getCheckpointId();\n+    LOG.info(\"Start to flush snapshot state to state backend, table: {}, checkpointId: {}\", table, checkpointId);\n+\n+    // Update the checkpoint state.\n+    dataFilesPerCheckpoint.put(checkpointId, ImmutableList.copyOf(dataFilesOfCurrentCheckpoint));\n+\n+    // Reset the snapshot state to the latest state.\n+    checkpointsState.clear();\n+    checkpointsState.add(dataFilesPerCheckpoint);\n+\n+    // Clear the local buffer for current checkpoint.\n+    dataFilesOfCurrentCheckpoint.clear();\n+  }\n+\n+  @Override\n+  public void notifyCheckpointComplete(long checkpointId) throws Exception {\n+    super.notifyCheckpointComplete(checkpointId);\n+    // It's possible that we have the following events:\n+    //   1. snapshotState(ckpId);\n+    //   2. snapshotState(ckpId+1);\n+    //   3. notifyCheckpointComplete(ckpId+1);\n+    //   4. notifyCheckpointComplete(ckpId);\n+    // For step#4, we don't need to commit iceberg table again because in step#3 we've committed all the files,\n+    // Besides, we need to maintain the max-committed-checkpoint-id to be increasing.\n+    if (checkpointId > maxCommittedCheckpointId) {\n+      commitUpToCheckpoint(checkpointId);\n+      this.maxCommittedCheckpointId = checkpointId;\n+    }\n+  }\n+\n+  private void commitUpToCheckpoint(long checkpointId) {\n+    NavigableMap<Long, List<DataFile>> pendingFileMap = dataFilesPerCheckpoint.headMap(checkpointId, true);\n+\n+    List<DataFile> pendingDataFiles = Lists.newArrayList();\n+    for (List<DataFile> dataFiles : pendingFileMap.values()) {\n+      pendingDataFiles.addAll(dataFiles);\n+    }\n+\n+    AppendFiles appendFiles = table.newAppend();\n+    pendingDataFiles.forEach(appendFiles::appendFile);\n+    appendFiles.set(MAX_COMMITTED_CHECKPOINT_ID, Long.toString(checkpointId));\n+    appendFiles.set(FLINK_JOB_ID, flinkJobId);\n+    appendFiles.commit();\n+\n+    // Clear the committed data files from dataFilesPerCheckpoint.\n+    pendingFileMap.clear();\n+  }\n+\n+  @Override\n+  public void processElement(StreamRecord<DataFile> element) {\n+    this.dataFilesOfCurrentCheckpoint.add(element.getValue());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e674ed7182289f3c6132c0ca2e76f0fd2d0b6871"}, "originalPosition": 179}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4239, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}