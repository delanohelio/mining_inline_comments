{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTM0MzM2NzIx", "number": 1888, "title": "Core: Add BaseDeltaWriter", "bodyText": "This is a separate PR to add the BaseDeltaWriter in BaseTaskWriter (https://github.com/apache/iceberg/pull/1818/files#diff-fc9a9fd84d24c607fd85e053b08a559f56dd2dd2a46f1341c528e7a0269f873cR92).   The DeltaWriter could accept both insert and equality deletes.\nFor the CDC case and upsert case,  compute engine such as flink and spark could write the streaming records (INSERT or DELETE) by this delta writer to apache iceberg table.", "createdAt": "2020-12-08T10:08:23Z", "url": "https://github.com/apache/iceberg/pull/1888", "merged": true, "mergeCommit": {"oid": "47b6b0532a1be28b061164aac3855fadc2c30114"}, "closed": true, "closedAt": "2020-12-11T17:59:50Z", "author": {"login": "openinx"}, "timelineItems": {"totalCount": 19, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdkEbn1AH2gAyNTM0MzM2NzIxOmFmZTJhNTllYWVlZDVlNmM5ODg1M2ZhNmU5ZTY0OGFlNTJjNjlhOGI=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdlL1Q3AFqTU1MDM4NDAwMg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "afe2a59eaeed5e6c98853fa6e9e648ae52c69a8b", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/afe2a59eaeed5e6c98853fa6e9e648ae52c69a8b", "committedDate": "2020-12-08T06:48:18Z", "message": "Core: Add BaseDeltaWriter."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "49028eb7c3432b72d920aaa070a4ddf22047abae", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/49028eb7c3432b72d920aaa070a4ddf22047abae", "committedDate": "2020-12-08T09:59:07Z", "message": "More unit tests."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c1946455b880e3112bd43aaaae41113ead7ee2ed", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/c1946455b880e3112bd43aaaae41113ead7ee2ed", "committedDate": "2020-12-08T11:20:35Z", "message": "Minor fixes"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ3NjE3MzM0", "url": "https://github.com/apache/iceberg/pull/1888#pullrequestreview-547617334", "createdAt": "2020-12-08T20:49:31Z", "commit": {"oid": "c1946455b880e3112bd43aaaae41113ead7ee2ed"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQyMDo0OTozMlrOIB1eJQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQyMDo0OTozMlrOIB1eJQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc5NTU1Nw==", "bodyText": "Nit: the error message is slightly misleading because it uses \"could\", which implies that there is some case where it could be null. How about \"Equality delete schema cannot be null\"?", "url": "https://github.com/apache/iceberg/pull/1888#discussion_r538795557", "createdAt": "2020-12-08T20:49:32Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/io/BaseTaskWriter.java", "diffHunk": "@@ -75,6 +79,113 @@ public WriteResult complete() throws IOException {\n         .build();\n   }\n \n+  /**\n+   * Base delta writer to write both insert records and equality-deletes.\n+   */\n+  protected abstract class BaseDeltaWriter implements Closeable {\n+    private RollingFileWriter dataWriter;\n+    private RollingEqDeleteWriter eqDeleteWriter;\n+    private SortedPosDeleteWriter<T> posDeleteWriter;\n+    private StructLikeMap<PathOffset> insertedRowMap;\n+\n+    public BaseDeltaWriter(PartitionKey partition, Schema eqDeleteSchema) {\n+      Preconditions.checkNotNull(eqDeleteSchema, \"equality-delete schema could not be null.\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c1946455b880e3112bd43aaaae41113ead7ee2ed"}, "originalPosition": 30}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ3NjE5NzA5", "url": "https://github.com/apache/iceberg/pull/1888#pullrequestreview-547619709", "createdAt": "2020-12-08T20:51:11Z", "commit": {"oid": "c1946455b880e3112bd43aaaae41113ead7ee2ed"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQyMDo1MToxMVrOIB1h9A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQyMDo1MToxMVrOIB1h9A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc5NjUzMg==", "bodyText": "How about BaseEqualityDeltaWriter?\nI think Spark MERGE INTO will likely use a delta writer that doesn't create the equality writer or use the SortedPosDeleteWriter because it will request that the rows are already ordered.", "url": "https://github.com/apache/iceberg/pull/1888#discussion_r538796532", "createdAt": "2020-12-08T20:51:11Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/io/BaseTaskWriter.java", "diffHunk": "@@ -75,6 +79,113 @@ public WriteResult complete() throws IOException {\n         .build();\n   }\n \n+  /**\n+   * Base delta writer to write both insert records and equality-deletes.\n+   */\n+  protected abstract class BaseDeltaWriter implements Closeable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c1946455b880e3112bd43aaaae41113ead7ee2ed"}, "originalPosition": 23}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ3NjI1MjI1", "url": "https://github.com/apache/iceberg/pull/1888#pullrequestreview-547625225", "createdAt": "2020-12-08T20:57:15Z", "commit": {"oid": "c1946455b880e3112bd43aaaae41113ead7ee2ed"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQyMDo1NzoxNVrOIB13Zg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQyMDo1NzoxNVrOIB13Zg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODgwMjAyMg==", "bodyText": "Why pass a key here instead of a row?\nI think it would be easier to assume that this is a row, so that the write and delete methods accept the same data. That also provides a way to write the row to the delete file, or just the key based on configuration. The way it is here, there is no way to write the whole row in the delete.", "url": "https://github.com/apache/iceberg/pull/1888#discussion_r538802022", "createdAt": "2020-12-08T20:57:15Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/io/BaseTaskWriter.java", "diffHunk": "@@ -75,6 +79,113 @@ public WriteResult complete() throws IOException {\n         .build();\n   }\n \n+  /**\n+   * Base delta writer to write both insert records and equality-deletes.\n+   */\n+  protected abstract class BaseDeltaWriter implements Closeable {\n+    private RollingFileWriter dataWriter;\n+    private RollingEqDeleteWriter eqDeleteWriter;\n+    private SortedPosDeleteWriter<T> posDeleteWriter;\n+    private StructLikeMap<PathOffset> insertedRowMap;\n+\n+    public BaseDeltaWriter(PartitionKey partition, Schema eqDeleteSchema) {\n+      Preconditions.checkNotNull(eqDeleteSchema, \"equality-delete schema could not be null.\");\n+\n+      this.dataWriter = new RollingFileWriter(partition);\n+\n+      this.eqDeleteWriter = new RollingEqDeleteWriter(partition);\n+      this.insertedRowMap = StructLikeMap.create(eqDeleteSchema.asStruct());\n+\n+      this.posDeleteWriter = new SortedPosDeleteWriter<>(appenderFactory, fileFactory, format, partition);\n+    }\n+\n+    /**\n+     * Make the generic data could be read as a {@link StructLike}.\n+     */\n+    protected abstract StructLike asStructLike(T data);\n+\n+    protected abstract StructLike asCopiedKey(T row);\n+\n+    public void write(T row) throws IOException {\n+      PathOffset pathOffset = PathOffset.of(dataWriter.currentPath(), dataWriter.currentRows());\n+\n+      StructLike copiedKey = asCopiedKey(row);\n+      // Adding a pos-delete to replace the old filePos.\n+      PathOffset previous = insertedRowMap.put(copiedKey, pathOffset);\n+      if (previous != null) {\n+        // TODO attach the previous row if has a positional-delete row schema in appender factory.\n+        posDeleteWriter.delete(previous.path, previous.rowOffset, null);\n+      }\n+\n+      dataWriter.write(row);\n+    }\n+\n+    /**\n+     * Delete the rows with the given key.\n+     *\n+     * @param key is the projected values which could be write to eq-delete file directly.\n+     */\n+    public void delete(T key) throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c1946455b880e3112bd43aaaae41113ead7ee2ed"}, "originalPosition": 66}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ3NjI5MDc2", "url": "https://github.com/apache/iceberg/pull/1888#pullrequestreview-547629076", "createdAt": "2020-12-08T21:02:13Z", "commit": {"oid": "c1946455b880e3112bd43aaaae41113ead7ee2ed"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQyMTowMjoxM1rOIB2DoQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQyMTowMjoxM1rOIB2DoQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODgwNTE1Mw==", "bodyText": "commitTransaction(result)?", "url": "https://github.com/apache/iceberg/pull/1888#discussion_r538805153", "createdAt": "2020-12-08T21:02:13Z", "author": {"login": "rdblue"}, "path": "data/src/test/java/org/apache/iceberg/io/TestTaskDeltaWriter.java", "diffHunk": "@@ -0,0 +1,417 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.io;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.function.Function;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DeleteFile;\n+import org.apache.iceberg.FileContent;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Files;\n+import org.apache.iceberg.PartitionKey;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RowDelta;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.TableTestBase;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.data.GenericAppenderFactory;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.IcebergGenerics;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.util.ArrayUtil;\n+import org.apache.iceberg.util.StructLikeSet;\n+import org.apache.iceberg.util.StructProjection;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+@RunWith(Parameterized.class)\n+public class TestTaskDeltaWriter extends TableTestBase {\n+  private static final int FORMAT_V2 = 2;\n+  private static final long TARGET_FILE_SIZE = 128 * 1024 * 1024L;\n+\n+  private final FileFormat format;\n+  private final GenericRecord gRecord = GenericRecord.create(SCHEMA);\n+  private final GenericRecord posRecord = GenericRecord.create(DeleteSchemaUtil.pathPosSchema());\n+\n+  private OutputFileFactory fileFactory = null;\n+  private int idFieldId;\n+  private int dataFieldId;\n+\n+  @Parameterized.Parameters(name = \"FileFormat = {0}\")\n+  public static Object[][] parameters() {\n+    return new Object[][] {\n+        {\"avro\"},\n+        {\"parquet\"}\n+    };\n+  }\n+\n+  public TestTaskDeltaWriter(String fileFormat) {\n+    super(FORMAT_V2);\n+    this.format = FileFormat.valueOf(fileFormat.toUpperCase(Locale.ENGLISH));\n+  }\n+\n+  @Before\n+  public void setupTable() throws IOException {\n+    this.tableDir = temp.newFolder();\n+    Assert.assertTrue(tableDir.delete()); // created by table create\n+\n+    this.metadataDir = new File(tableDir, \"metadata\");\n+\n+    this.table = create(SCHEMA, PartitionSpec.unpartitioned());\n+    this.fileFactory = new OutputFileFactory(table.spec(), format, table.locationProvider(), table.io(),\n+        table.encryption(), 1, 1);\n+\n+    this.idFieldId = table.schema().findField(\"id\").fieldId();\n+    this.dataFieldId = table.schema().findField(\"data\").fieldId();\n+\n+    table.updateProperties()\n+        .defaultFormat(format)\n+        .commit();\n+  }\n+\n+  private Record createRecord(Integer id, String data) {\n+    return gRecord.copy(\"id\", id, \"data\", data);\n+  }\n+\n+  @Test\n+  public void testPureInsert() throws IOException {\n+    List<Integer> eqDeleteFieldIds = Lists.newArrayList(idFieldId, dataFieldId);\n+    Schema eqDeleteSchema = table.schema();\n+\n+    GenericTaskDeltaWriter deltaWriter = createTaskWriter(eqDeleteFieldIds, eqDeleteSchema);\n+    List<Record> expected = Lists.newArrayList();\n+    for (int i = 0; i < 20; i++) {\n+      Record record = createRecord(i, String.format(\"val-%d\", i));\n+      expected.add(record);\n+\n+      deltaWriter.write(record);\n+    }\n+\n+    WriteResult result = deltaWriter.complete();\n+    Assert.assertEquals(\"Should only have a data file.\", 1, result.dataFiles().length);\n+    Assert.assertEquals(\"Should have no delete file\", 0, result.deleteFiles().length);\n+    commitTransaction(result);\n+    Assert.assertEquals(\"Should have expected records\", expectedRowSet(expected), actualRowSet(\"*\"));\n+\n+    deltaWriter = createTaskWriter(eqDeleteFieldIds, eqDeleteSchema);\n+    for (int i = 20; i < 30; i++) {\n+      Record record = createRecord(i, String.format(\"val-%d\", i));\n+      expected.add(record);\n+\n+      deltaWriter.write(record);\n+    }\n+    result = deltaWriter.complete();\n+    Assert.assertEquals(\"Should only have a data file.\", 1, result.dataFiles().length);\n+    Assert.assertEquals(\"Should have no delete file\", 0, result.deleteFiles().length);\n+    commitTransaction(deltaWriter.complete());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c1946455b880e3112bd43aaaae41113ead7ee2ed"}, "originalPosition": 137}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ3NjMyNTkw", "url": "https://github.com/apache/iceberg/pull/1888#pullrequestreview-547632590", "createdAt": "2020-12-08T21:07:28Z", "commit": {"oid": "c1946455b880e3112bd43aaaae41113ead7ee2ed"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQyMTowNzoyOVrOIB2POQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQyMTowNzoyOVrOIB2POQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODgwODEyMQ==", "bodyText": "Is it necessary to encode the record twice? What about detecting already deleted keys and omitting the second delete? That might be over-complicating this for a very small optimization though.", "url": "https://github.com/apache/iceberg/pull/1888#discussion_r538808121", "createdAt": "2020-12-08T21:07:29Z", "author": {"login": "rdblue"}, "path": "data/src/test/java/org/apache/iceberg/io/TestTaskDeltaWriter.java", "diffHunk": "@@ -0,0 +1,417 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.io;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.function.Function;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DeleteFile;\n+import org.apache.iceberg.FileContent;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Files;\n+import org.apache.iceberg.PartitionKey;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RowDelta;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.TableTestBase;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.data.GenericAppenderFactory;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.IcebergGenerics;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.util.ArrayUtil;\n+import org.apache.iceberg.util.StructLikeSet;\n+import org.apache.iceberg.util.StructProjection;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+@RunWith(Parameterized.class)\n+public class TestTaskDeltaWriter extends TableTestBase {\n+  private static final int FORMAT_V2 = 2;\n+  private static final long TARGET_FILE_SIZE = 128 * 1024 * 1024L;\n+\n+  private final FileFormat format;\n+  private final GenericRecord gRecord = GenericRecord.create(SCHEMA);\n+  private final GenericRecord posRecord = GenericRecord.create(DeleteSchemaUtil.pathPosSchema());\n+\n+  private OutputFileFactory fileFactory = null;\n+  private int idFieldId;\n+  private int dataFieldId;\n+\n+  @Parameterized.Parameters(name = \"FileFormat = {0}\")\n+  public static Object[][] parameters() {\n+    return new Object[][] {\n+        {\"avro\"},\n+        {\"parquet\"}\n+    };\n+  }\n+\n+  public TestTaskDeltaWriter(String fileFormat) {\n+    super(FORMAT_V2);\n+    this.format = FileFormat.valueOf(fileFormat.toUpperCase(Locale.ENGLISH));\n+  }\n+\n+  @Before\n+  public void setupTable() throws IOException {\n+    this.tableDir = temp.newFolder();\n+    Assert.assertTrue(tableDir.delete()); // created by table create\n+\n+    this.metadataDir = new File(tableDir, \"metadata\");\n+\n+    this.table = create(SCHEMA, PartitionSpec.unpartitioned());\n+    this.fileFactory = new OutputFileFactory(table.spec(), format, table.locationProvider(), table.io(),\n+        table.encryption(), 1, 1);\n+\n+    this.idFieldId = table.schema().findField(\"id\").fieldId();\n+    this.dataFieldId = table.schema().findField(\"data\").fieldId();\n+\n+    table.updateProperties()\n+        .defaultFormat(format)\n+        .commit();\n+  }\n+\n+  private Record createRecord(Integer id, String data) {\n+    return gRecord.copy(\"id\", id, \"data\", data);\n+  }\n+\n+  @Test\n+  public void testPureInsert() throws IOException {\n+    List<Integer> eqDeleteFieldIds = Lists.newArrayList(idFieldId, dataFieldId);\n+    Schema eqDeleteSchema = table.schema();\n+\n+    GenericTaskDeltaWriter deltaWriter = createTaskWriter(eqDeleteFieldIds, eqDeleteSchema);\n+    List<Record> expected = Lists.newArrayList();\n+    for (int i = 0; i < 20; i++) {\n+      Record record = createRecord(i, String.format(\"val-%d\", i));\n+      expected.add(record);\n+\n+      deltaWriter.write(record);\n+    }\n+\n+    WriteResult result = deltaWriter.complete();\n+    Assert.assertEquals(\"Should only have a data file.\", 1, result.dataFiles().length);\n+    Assert.assertEquals(\"Should have no delete file\", 0, result.deleteFiles().length);\n+    commitTransaction(result);\n+    Assert.assertEquals(\"Should have expected records\", expectedRowSet(expected), actualRowSet(\"*\"));\n+\n+    deltaWriter = createTaskWriter(eqDeleteFieldIds, eqDeleteSchema);\n+    for (int i = 20; i < 30; i++) {\n+      Record record = createRecord(i, String.format(\"val-%d\", i));\n+      expected.add(record);\n+\n+      deltaWriter.write(record);\n+    }\n+    result = deltaWriter.complete();\n+    Assert.assertEquals(\"Should only have a data file.\", 1, result.dataFiles().length);\n+    Assert.assertEquals(\"Should have no delete file\", 0, result.deleteFiles().length);\n+    commitTransaction(deltaWriter.complete());\n+    Assert.assertEquals(\"Should have expected records\", expectedRowSet(expected), actualRowSet(\"*\"));\n+  }\n+\n+  @Test\n+  public void testInsertDuplicatedKey() throws IOException {\n+    List<Integer> equalityFieldIds = Lists.newArrayList(idFieldId);\n+    Schema eqDeleteSchema = table.schema().select(\"id\");\n+\n+    GenericTaskDeltaWriter deltaWriter = createTaskWriter(equalityFieldIds, eqDeleteSchema);\n+    deltaWriter.write(createRecord(1, \"aaa\"));\n+    deltaWriter.write(createRecord(2, \"bbb\"));\n+    deltaWriter.write(createRecord(3, \"ccc\"));\n+    deltaWriter.write(createRecord(4, \"ddd\"));\n+    deltaWriter.write(createRecord(4, \"eee\"));\n+    deltaWriter.write(createRecord(3, \"fff\"));\n+    deltaWriter.write(createRecord(2, \"ggg\"));\n+    deltaWriter.write(createRecord(1, \"hhh\"));\n+\n+    WriteResult result = deltaWriter.complete();\n+    commitTransaction(result);\n+\n+    Assert.assertEquals(\"Should have a data file.\", 1, result.dataFiles().length);\n+    Assert.assertEquals(\"Should have a pos-delete file\", 1, result.deleteFiles().length);\n+    DeleteFile posDeleteFile = result.deleteFiles()[0];\n+    Assert.assertEquals(\"Should be a pos-delete file\", FileContent.POSITION_DELETES, posDeleteFile.content());\n+    Assert.assertEquals(\"Should have expected records\", expectedRowSet(ImmutableList.of(\n+        createRecord(4, \"eee\"),\n+        createRecord(3, \"fff\"),\n+        createRecord(2, \"ggg\"),\n+        createRecord(1, \"hhh\")\n+    )), actualRowSet(\"*\"));\n+\n+    // Check records in the data file.\n+    DataFile dataFile = result.dataFiles()[0];\n+    Assert.assertEquals(ImmutableList.of(\n+        createRecord(1, \"aaa\"),\n+        createRecord(2, \"bbb\"),\n+        createRecord(3, \"ccc\"),\n+        createRecord(4, \"ddd\"),\n+        createRecord(4, \"eee\"),\n+        createRecord(3, \"fff\"),\n+        createRecord(2, \"ggg\"),\n+        createRecord(1, \"hhh\")\n+    ), readRecordsAsList(table.schema(), dataFile.path()));\n+\n+    // Check records in the pos-delete file.\n+    Schema posDeleteSchema = DeleteSchemaUtil.pathPosSchema();\n+    Assert.assertEquals(ImmutableList.of(\n+        posRecord.copy(\"file_path\", dataFile.path(), \"pos\", 0L),\n+        posRecord.copy(\"file_path\", dataFile.path(), \"pos\", 1L),\n+        posRecord.copy(\"file_path\", dataFile.path(), \"pos\", 2L),\n+        posRecord.copy(\"file_path\", dataFile.path(), \"pos\", 3L)\n+    ), readRecordsAsList(posDeleteSchema, posDeleteFile.path()));\n+  }\n+\n+  @Test\n+  public void testUpsertSameRow() throws IOException {\n+    List<Integer> eqDeleteFieldIds = Lists.newArrayList(idFieldId, dataFieldId);\n+    Schema eqDeleteSchema = table.schema();\n+\n+    GenericTaskDeltaWriter deltaWriter = createTaskWriter(eqDeleteFieldIds, eqDeleteSchema);\n+\n+    Record record = createRecord(1, \"aaa\");\n+    deltaWriter.write(record);\n+    deltaWriter.delete(record);\n+    deltaWriter.write(record);\n+    deltaWriter.delete(record);\n+    deltaWriter.write(record);\n+\n+    WriteResult result = deltaWriter.complete();\n+    Assert.assertEquals(\"Should have a data file.\", 1, result.dataFiles().length);\n+    Assert.assertEquals(\"Should have a pos-delete file and an eq-delete file\", 2, result.deleteFiles().length);\n+    commitTransaction(result);\n+    Assert.assertEquals(\"Should have an expected record\", expectedRowSet(ImmutableList.of(record)), actualRowSet(\"*\"));\n+\n+    // Check records in the data file.\n+    DataFile dataFile = result.dataFiles()[0];\n+    Assert.assertEquals(ImmutableList.of(record, record, record), readRecordsAsList(table.schema(), dataFile.path()));\n+\n+    // Check records in the eq-delete file.\n+    DeleteFile eqDeleteFile = result.deleteFiles()[0];\n+    Assert.assertEquals(ImmutableList.of(record, record), readRecordsAsList(eqDeleteSchema, eqDeleteFile.path()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c1946455b880e3112bd43aaaae41113ead7ee2ed"}, "originalPosition": 219}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ3NjMzNTg4", "url": "https://github.com/apache/iceberg/pull/1888#pullrequestreview-547633588", "createdAt": "2020-12-08T21:08:58Z", "commit": {"oid": "c1946455b880e3112bd43aaaae41113ead7ee2ed"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQyMTowODo1OFrOIB2SYw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQyMTowODo1OFrOIB2SYw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODgwODkzMQ==", "bodyText": "Nit: it would normally be \"2nd\"", "url": "https://github.com/apache/iceberg/pull/1888#discussion_r538808931", "createdAt": "2020-12-08T21:08:58Z", "author": {"login": "rdblue"}, "path": "data/src/test/java/org/apache/iceberg/io/TestTaskDeltaWriter.java", "diffHunk": "@@ -0,0 +1,417 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.io;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.function.Function;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DeleteFile;\n+import org.apache.iceberg.FileContent;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Files;\n+import org.apache.iceberg.PartitionKey;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RowDelta;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.TableTestBase;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.data.GenericAppenderFactory;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.IcebergGenerics;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.util.ArrayUtil;\n+import org.apache.iceberg.util.StructLikeSet;\n+import org.apache.iceberg.util.StructProjection;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+@RunWith(Parameterized.class)\n+public class TestTaskDeltaWriter extends TableTestBase {\n+  private static final int FORMAT_V2 = 2;\n+  private static final long TARGET_FILE_SIZE = 128 * 1024 * 1024L;\n+\n+  private final FileFormat format;\n+  private final GenericRecord gRecord = GenericRecord.create(SCHEMA);\n+  private final GenericRecord posRecord = GenericRecord.create(DeleteSchemaUtil.pathPosSchema());\n+\n+  private OutputFileFactory fileFactory = null;\n+  private int idFieldId;\n+  private int dataFieldId;\n+\n+  @Parameterized.Parameters(name = \"FileFormat = {0}\")\n+  public static Object[][] parameters() {\n+    return new Object[][] {\n+        {\"avro\"},\n+        {\"parquet\"}\n+    };\n+  }\n+\n+  public TestTaskDeltaWriter(String fileFormat) {\n+    super(FORMAT_V2);\n+    this.format = FileFormat.valueOf(fileFormat.toUpperCase(Locale.ENGLISH));\n+  }\n+\n+  @Before\n+  public void setupTable() throws IOException {\n+    this.tableDir = temp.newFolder();\n+    Assert.assertTrue(tableDir.delete()); // created by table create\n+\n+    this.metadataDir = new File(tableDir, \"metadata\");\n+\n+    this.table = create(SCHEMA, PartitionSpec.unpartitioned());\n+    this.fileFactory = new OutputFileFactory(table.spec(), format, table.locationProvider(), table.io(),\n+        table.encryption(), 1, 1);\n+\n+    this.idFieldId = table.schema().findField(\"id\").fieldId();\n+    this.dataFieldId = table.schema().findField(\"data\").fieldId();\n+\n+    table.updateProperties()\n+        .defaultFormat(format)\n+        .commit();\n+  }\n+\n+  private Record createRecord(Integer id, String data) {\n+    return gRecord.copy(\"id\", id, \"data\", data);\n+  }\n+\n+  @Test\n+  public void testPureInsert() throws IOException {\n+    List<Integer> eqDeleteFieldIds = Lists.newArrayList(idFieldId, dataFieldId);\n+    Schema eqDeleteSchema = table.schema();\n+\n+    GenericTaskDeltaWriter deltaWriter = createTaskWriter(eqDeleteFieldIds, eqDeleteSchema);\n+    List<Record> expected = Lists.newArrayList();\n+    for (int i = 0; i < 20; i++) {\n+      Record record = createRecord(i, String.format(\"val-%d\", i));\n+      expected.add(record);\n+\n+      deltaWriter.write(record);\n+    }\n+\n+    WriteResult result = deltaWriter.complete();\n+    Assert.assertEquals(\"Should only have a data file.\", 1, result.dataFiles().length);\n+    Assert.assertEquals(\"Should have no delete file\", 0, result.deleteFiles().length);\n+    commitTransaction(result);\n+    Assert.assertEquals(\"Should have expected records\", expectedRowSet(expected), actualRowSet(\"*\"));\n+\n+    deltaWriter = createTaskWriter(eqDeleteFieldIds, eqDeleteSchema);\n+    for (int i = 20; i < 30; i++) {\n+      Record record = createRecord(i, String.format(\"val-%d\", i));\n+      expected.add(record);\n+\n+      deltaWriter.write(record);\n+    }\n+    result = deltaWriter.complete();\n+    Assert.assertEquals(\"Should only have a data file.\", 1, result.dataFiles().length);\n+    Assert.assertEquals(\"Should have no delete file\", 0, result.deleteFiles().length);\n+    commitTransaction(deltaWriter.complete());\n+    Assert.assertEquals(\"Should have expected records\", expectedRowSet(expected), actualRowSet(\"*\"));\n+  }\n+\n+  @Test\n+  public void testInsertDuplicatedKey() throws IOException {\n+    List<Integer> equalityFieldIds = Lists.newArrayList(idFieldId);\n+    Schema eqDeleteSchema = table.schema().select(\"id\");\n+\n+    GenericTaskDeltaWriter deltaWriter = createTaskWriter(equalityFieldIds, eqDeleteSchema);\n+    deltaWriter.write(createRecord(1, \"aaa\"));\n+    deltaWriter.write(createRecord(2, \"bbb\"));\n+    deltaWriter.write(createRecord(3, \"ccc\"));\n+    deltaWriter.write(createRecord(4, \"ddd\"));\n+    deltaWriter.write(createRecord(4, \"eee\"));\n+    deltaWriter.write(createRecord(3, \"fff\"));\n+    deltaWriter.write(createRecord(2, \"ggg\"));\n+    deltaWriter.write(createRecord(1, \"hhh\"));\n+\n+    WriteResult result = deltaWriter.complete();\n+    commitTransaction(result);\n+\n+    Assert.assertEquals(\"Should have a data file.\", 1, result.dataFiles().length);\n+    Assert.assertEquals(\"Should have a pos-delete file\", 1, result.deleteFiles().length);\n+    DeleteFile posDeleteFile = result.deleteFiles()[0];\n+    Assert.assertEquals(\"Should be a pos-delete file\", FileContent.POSITION_DELETES, posDeleteFile.content());\n+    Assert.assertEquals(\"Should have expected records\", expectedRowSet(ImmutableList.of(\n+        createRecord(4, \"eee\"),\n+        createRecord(3, \"fff\"),\n+        createRecord(2, \"ggg\"),\n+        createRecord(1, \"hhh\")\n+    )), actualRowSet(\"*\"));\n+\n+    // Check records in the data file.\n+    DataFile dataFile = result.dataFiles()[0];\n+    Assert.assertEquals(ImmutableList.of(\n+        createRecord(1, \"aaa\"),\n+        createRecord(2, \"bbb\"),\n+        createRecord(3, \"ccc\"),\n+        createRecord(4, \"ddd\"),\n+        createRecord(4, \"eee\"),\n+        createRecord(3, \"fff\"),\n+        createRecord(2, \"ggg\"),\n+        createRecord(1, \"hhh\")\n+    ), readRecordsAsList(table.schema(), dataFile.path()));\n+\n+    // Check records in the pos-delete file.\n+    Schema posDeleteSchema = DeleteSchemaUtil.pathPosSchema();\n+    Assert.assertEquals(ImmutableList.of(\n+        posRecord.copy(\"file_path\", dataFile.path(), \"pos\", 0L),\n+        posRecord.copy(\"file_path\", dataFile.path(), \"pos\", 1L),\n+        posRecord.copy(\"file_path\", dataFile.path(), \"pos\", 2L),\n+        posRecord.copy(\"file_path\", dataFile.path(), \"pos\", 3L)\n+    ), readRecordsAsList(posDeleteSchema, posDeleteFile.path()));\n+  }\n+\n+  @Test\n+  public void testUpsertSameRow() throws IOException {\n+    List<Integer> eqDeleteFieldIds = Lists.newArrayList(idFieldId, dataFieldId);\n+    Schema eqDeleteSchema = table.schema();\n+\n+    GenericTaskDeltaWriter deltaWriter = createTaskWriter(eqDeleteFieldIds, eqDeleteSchema);\n+\n+    Record record = createRecord(1, \"aaa\");\n+    deltaWriter.write(record);\n+    deltaWriter.delete(record);\n+    deltaWriter.write(record);\n+    deltaWriter.delete(record);\n+    deltaWriter.write(record);\n+\n+    WriteResult result = deltaWriter.complete();\n+    Assert.assertEquals(\"Should have a data file.\", 1, result.dataFiles().length);\n+    Assert.assertEquals(\"Should have a pos-delete file and an eq-delete file\", 2, result.deleteFiles().length);\n+    commitTransaction(result);\n+    Assert.assertEquals(\"Should have an expected record\", expectedRowSet(ImmutableList.of(record)), actualRowSet(\"*\"));\n+\n+    // Check records in the data file.\n+    DataFile dataFile = result.dataFiles()[0];\n+    Assert.assertEquals(ImmutableList.of(record, record, record), readRecordsAsList(table.schema(), dataFile.path()));\n+\n+    // Check records in the eq-delete file.\n+    DeleteFile eqDeleteFile = result.deleteFiles()[0];\n+    Assert.assertEquals(ImmutableList.of(record, record), readRecordsAsList(eqDeleteSchema, eqDeleteFile.path()));\n+\n+    // Check records in the pos-delete file.\n+    DeleteFile posDeleteFile = result.deleteFiles()[1];\n+    Assert.assertEquals(ImmutableList.of(\n+        posRecord.copy(\"file_path\", dataFile.path(), \"pos\", 0L),\n+        posRecord.copy(\"file_path\", dataFile.path(), \"pos\", 1L)\n+    ), readRecordsAsList(DeleteSchemaUtil.pathPosSchema(), posDeleteFile.path()));\n+\n+    deltaWriter = createTaskWriter(eqDeleteFieldIds, eqDeleteSchema);\n+    deltaWriter.delete(record);\n+    result = deltaWriter.complete();\n+    Assert.assertEquals(\"Should have 0 data file.\", 0, result.dataFiles().length);\n+    Assert.assertEquals(\"Should have 1 eq-delete file\", 1, result.deleteFiles().length);\n+    commitTransaction(result);\n+    Assert.assertEquals(\"Should have no record\", expectedRowSet(ImmutableList.of()), actualRowSet(\"*\"));\n+  }\n+\n+  @Test\n+  public void testUpsertByDataField() throws IOException {\n+    List<Integer> eqDeleteFieldIds = Lists.newArrayList(dataFieldId);\n+    Schema eqDeleteSchema = table.schema().select(\"data\");\n+\n+    GenericTaskDeltaWriter deltaWriter = createTaskWriter(eqDeleteFieldIds, eqDeleteSchema);\n+    deltaWriter.write(createRecord(1, \"aaa\"));\n+    deltaWriter.write(createRecord(2, \"bbb\"));\n+    deltaWriter.write(createRecord(3, \"aaa\"));\n+    deltaWriter.write(createRecord(3, \"ccc\"));\n+    deltaWriter.write(createRecord(4, \"ccc\"));\n+\n+    // Commit the 1th transaction.\n+    WriteResult result = deltaWriter.complete();\n+    Assert.assertEquals(\"Should have a data file\", 1, result.dataFiles().length);\n+    Assert.assertEquals(\"Should have a pos-delete file for deduplication purpose\", 1, result.deleteFiles().length);\n+    Assert.assertEquals(\"Should be pos-delete file\", FileContent.POSITION_DELETES, result.deleteFiles()[0].content());\n+    commitTransaction(result);\n+\n+    Assert.assertEquals(\"Should have expected records\", expectedRowSet(ImmutableList.of(\n+        createRecord(2, \"bbb\"),\n+        createRecord(3, \"aaa\"),\n+        createRecord(4, \"ccc\")\n+    )), actualRowSet(\"*\"));\n+\n+    // Start the 2th transaction.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c1946455b880e3112bd43aaaae41113ead7ee2ed"}, "originalPosition": 262}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ3NjM4MzY1", "url": "https://github.com/apache/iceberg/pull/1888#pullrequestreview-547638365", "createdAt": "2020-12-08T21:15:56Z", "commit": {"oid": "c1946455b880e3112bd43aaaae41113ead7ee2ed"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQyMToxNTo1NlrOIB2iXg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQyMToxNTo1NlrOIB2iXg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODgxMzAyMg==", "bodyText": "I think this could be removed from the API because this already has asStructLike. Rather than relying on the implementation to create the projection and copy it, this could be implemented here like this:\n  protected StructLike asCopiedKey(T row) {\n    return structProjection.copy().wrap(asStructLike(row));\n  }\nWe would need to create StructProjection in this class, but it would be fairly easy.", "url": "https://github.com/apache/iceberg/pull/1888#discussion_r538813022", "createdAt": "2020-12-08T21:15:56Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/io/BaseTaskWriter.java", "diffHunk": "@@ -75,6 +79,113 @@ public WriteResult complete() throws IOException {\n         .build();\n   }\n \n+  /**\n+   * Base delta writer to write both insert records and equality-deletes.\n+   */\n+  protected abstract class BaseDeltaWriter implements Closeable {\n+    private RollingFileWriter dataWriter;\n+    private RollingEqDeleteWriter eqDeleteWriter;\n+    private SortedPosDeleteWriter<T> posDeleteWriter;\n+    private StructLikeMap<PathOffset> insertedRowMap;\n+\n+    public BaseDeltaWriter(PartitionKey partition, Schema eqDeleteSchema) {\n+      Preconditions.checkNotNull(eqDeleteSchema, \"equality-delete schema could not be null.\");\n+\n+      this.dataWriter = new RollingFileWriter(partition);\n+\n+      this.eqDeleteWriter = new RollingEqDeleteWriter(partition);\n+      this.insertedRowMap = StructLikeMap.create(eqDeleteSchema.asStruct());\n+\n+      this.posDeleteWriter = new SortedPosDeleteWriter<>(appenderFactory, fileFactory, format, partition);\n+    }\n+\n+    /**\n+     * Make the generic data could be read as a {@link StructLike}.\n+     */\n+    protected abstract StructLike asStructLike(T data);\n+\n+    protected abstract StructLike asCopiedKey(T row);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c1946455b880e3112bd43aaaae41113ead7ee2ed"}, "originalPosition": 45}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4a6c0321802d7e236a9b8c4ed31bc1b7fbc5eb39", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/4a6c0321802d7e236a9b8c4ed31bc1b7fbc5eb39", "committedDate": "2020-12-09T08:36:04Z", "message": "Address comments."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "46212bc8af5e3f8b6ca7d824210d8cc8649c5936", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/46212bc8af5e3f8b6ca7d824210d8cc8649c5936", "committedDate": "2020-12-09T08:57:39Z", "message": "Minor changes."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ4NTk5NDMz", "url": "https://github.com/apache/iceberg/pull/1888#pullrequestreview-548599433", "createdAt": "2020-12-09T20:53:27Z", "commit": {"oid": "46212bc8af5e3f8b6ca7d824210d8cc8649c5936"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOVQyMDo1MzoyN1rOICo_IQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOVQyMDo1MzoyN1rOICo_IQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTYzOTU4NQ==", "bodyText": "Doesn't this have the problem you were talking about with using only wrappers? If asStructLike uses a wrapper that is reused, then even if the key is a new instance, the underlying data in the StructLike wrapper returned will change on the next call to write.\nI think instead of copying the projection, this needs to copy the result of the projection into a new struct and add that to the map.", "url": "https://github.com/apache/iceberg/pull/1888#discussion_r539639585", "createdAt": "2020-12-09T20:53:27Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/io/BaseTaskWriter.java", "diffHunk": "@@ -75,6 +81,135 @@ public WriteResult complete() throws IOException {\n         .build();\n   }\n \n+  /**\n+   * Base equality delta writer to write both insert records and equality-deletes.\n+   */\n+  protected abstract class BaseEqualityDeltaWriter implements Closeable {\n+    private final StructProjection structProjection;\n+    private RollingFileWriter dataWriter;\n+    private RollingEqDeleteWriter eqDeleteWriter;\n+    private SortedPosDeleteWriter<T> posDeleteWriter;\n+    private Map<StructLike, PathOffset> insertedRowMap;\n+\n+    public BaseEqualityDeltaWriter(PartitionKey partition, Schema schema, Schema deleteSchema) {\n+      Preconditions.checkNotNull(schema, \"Iceberg table schema cannot be null.\");\n+      Preconditions.checkNotNull(deleteSchema, \"Equality-delete schema cannot be null.\");\n+      this.structProjection = StructProjection.create(schema, deleteSchema);\n+\n+      this.dataWriter = new RollingFileWriter(partition);\n+\n+      this.eqDeleteWriter = new RollingEqDeleteWriter(partition);\n+      this.insertedRowMap = StructLikeMap.create(deleteSchema.asStruct());\n+\n+      this.posDeleteWriter = new SortedPosDeleteWriter<>(appenderFactory, fileFactory, format, partition);\n+    }\n+\n+    /**\n+     * Make the generic data could be read as a {@link StructLike}.\n+     */\n+    protected abstract StructLike asStructLike(T data);\n+\n+    public void write(T row) throws IOException {\n+      PathOffset pathOffset = PathOffset.of(dataWriter.currentPath(), dataWriter.currentRows());\n+\n+      StructLike copiedKey = structProjection.copy().wrap(asStructLike(row));\n+      // Adding a pos-delete to replace the old path-offset.\n+      PathOffset previous = insertedRowMap.put(copiedKey, pathOffset);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "46212bc8af5e3f8b6ca7d824210d8cc8649c5936"}, "originalPosition": 60}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8e5cb38e25189b544eca614d5031ab6396f3e006", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/8e5cb38e25189b544eca614d5031ab6396f3e006", "committedDate": "2020-12-10T02:32:39Z", "message": "Introduce asCopiedStructLike"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "dfe0401b1f7fc637129f25631e3b87030d3e214f", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/dfe0401b1f7fc637129f25631e3b87030d3e214f", "committedDate": "2020-12-10T08:52:12Z", "message": "Validate the referenced data files."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5NzE2MTI5", "url": "https://github.com/apache/iceberg/pull/1888#pullrequestreview-549716129", "createdAt": "2020-12-11T00:24:46Z", "commit": {"oid": "dfe0401b1f7fc637129f25631e3b87030d3e214f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwMDoyNDo0NlrOIDjkdQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwMDoyNDo0NlrOIDjkdQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDU5OTQxMw==", "bodyText": "Good catch. Should this set be part of the WriteResult instead of separate? I think that tasks are going to need to pass the set back to the commit for validation, so adding it to the WriteResult seems like the right way to handle it.", "url": "https://github.com/apache/iceberg/pull/1888#discussion_r540599413", "createdAt": "2020-12-11T00:24:46Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/io/BaseTaskWriter.java", "diffHunk": "@@ -54,6 +64,10 @@ protected BaseTaskWriter(PartitionSpec spec, FileFormat format, FileAppenderFact\n     this.targetFileSize = targetFileSize;\n   }\n \n+  public Set<CharSequence> referencedDataFiles() {\n+    return referencedDataFiles;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dfe0401b1f7fc637129f25631e3b87030d3e214f"}, "originalPosition": 37}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "dfeed41fd0ef72c28a414689a08a4b10617bdf4d", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/dfeed41fd0ef72c28a414689a08a4b10617bdf4d", "committedDate": "2020-12-11T06:39:53Z", "message": "Addressing the comments."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b9bc7557f9426a348d0e9e070aabcfe5e0b822ce", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/b9bc7557f9426a348d0e9e070aabcfe5e0b822ce", "committedDate": "2020-12-11T07:21:02Z", "message": "Minor changes."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTUwMzg0MDAy", "url": "https://github.com/apache/iceberg/pull/1888#pullrequestreview-550384002", "createdAt": "2020-12-11T17:59:34Z", "commit": {"oid": "b9bc7557f9426a348d0e9e070aabcfe5e0b822ce"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3541, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}