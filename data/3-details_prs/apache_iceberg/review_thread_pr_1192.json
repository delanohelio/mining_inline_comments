{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDQ3MTc0OTAz", "number": 1192, "reviewThreads": {"totalCount": 14, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMVQwMDo0MzoxMFrOENc4PQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQyMzoxMDoxOVrOEQ1DGw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyNTQwMDkzOnYy", "diffSide": "RIGHT", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergSplit.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMVQwMDo0MzoxMFrOGwJCmA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNFQxOToxNzowOFrOGxhuxA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEzMjk1Mg==", "bodyText": "Looks like what's happening is the table location is used as the split's path so that Hive associates all splits with the same PartitionDesc that contains a TableDesc. Is that correct? If so, I think it would be better to add that as the comment. It's difficult to read the Hive code and figure out what's going on using just the pointers here.", "url": "https://github.com/apache/iceberg/pull/1192#discussion_r453132952", "createdAt": "2020-07-11T00:43:10Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergSplit.java", "diffHunk": "@@ -0,0 +1,91 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.mapred.FileSplit;\n+import org.apache.iceberg.mr.SerializationUtil;\n+import org.apache.iceberg.mr.mapreduce.IcebergSplit;\n+import org.apache.iceberg.mr.mapreduce.IcebergSplitContainer;\n+\n+// Hive requires file formats to return splits that are instances of `FileSplit`.\n+public class HiveIcebergSplit extends FileSplit implements IcebergSplitContainer {\n+\n+  private IcebergSplit innerSplit;\n+\n+  // The table location of the split allows Hive to map a split to a table and/or partition.\n+  // See calls to `getPartitionDescFromPathRecursively` in `CombineHiveInputFormat` or `HiveInputFormat`.\n+  private String tableLocation;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f46b38b081a8d2a3b6b439ab6368c29447808e1c"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDU4NjA1Mg==", "bodyText": "Hive uses the path name of the split to map it back to a PartitionDesc or TableDesc, which specify the relevant input format for reading the files belonging to that partition or table. That way, HiveInputFormat and CombineHiveInputFormat can read files with different input formats in the same MR job and combine compatible splits together.\nI'll update the comment.", "url": "https://github.com/apache/iceberg/pull/1192#discussion_r454586052", "createdAt": "2020-07-14T19:17:08Z", "author": {"login": "guilload"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergSplit.java", "diffHunk": "@@ -0,0 +1,91 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.mapred.FileSplit;\n+import org.apache.iceberg.mr.SerializationUtil;\n+import org.apache.iceberg.mr.mapreduce.IcebergSplit;\n+import org.apache.iceberg.mr.mapreduce.IcebergSplitContainer;\n+\n+// Hive requires file formats to return splits that are instances of `FileSplit`.\n+public class HiveIcebergSplit extends FileSplit implements IcebergSplitContainer {\n+\n+  private IcebergSplit innerSplit;\n+\n+  // The table location of the split allows Hive to map a split to a table and/or partition.\n+  // See calls to `getPartitionDescFromPathRecursively` in `CombineHiveInputFormat` or `HiveInputFormat`.\n+  private String tableLocation;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEzMjk1Mg=="}, "originalCommit": {"oid": "f46b38b081a8d2a3b6b439ab6368c29447808e1c"}, "originalPosition": 38}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyNTQwMTU2OnYy", "diffSide": "RIGHT", "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/Container.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMVQwMDo0NDowM1rOGwJC9Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMVQwMDo0NDowM1rOGwJC9Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEzMzA0NQ==", "bodyText": "I don't think these last two functions need to change?", "url": "https://github.com/apache/iceberg/pull/1192#discussion_r453133045", "createdAt": "2020-07-11T00:44:03Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/Container.java", "diffHunk": "@@ -21,48 +21,33 @@\n \n import java.io.DataInput;\n import java.io.DataOutput;\n+import java.io.IOException;\n import org.apache.hadoop.io.Writable;\n-import org.apache.iceberg.Schema;\n-import org.apache.iceberg.data.Record;\n \n /**\n- * Wraps an Iceberg Record in a Writable which Hive can use in the SerDe.\n+ * A simple container of objects that you can get and set.\n+ *\n+ * @param <T> the Java type of the object held by this container\n  */\n-public class IcebergWritable implements Writable {\n-\n-  private Record record;\n-  private Schema schema;\n-\n-  public IcebergWritable(Record record, Schema schema) {\n-    this.record = record;\n-    this.schema = schema;\n-  }\n+public class Container<T> implements Writable {\n \n-  @SuppressWarnings(\"checkstyle:HiddenField\")\n-  public void wrapRecord(Record record) {\n-    this.record = record;\n-  }\n-\n-  public Record record() {\n-    return record;\n-  }\n+  private T value;\n \n-  public Schema schema() {\n-    return schema;\n+  public T get() {\n+    return value;\n   }\n \n-  @SuppressWarnings(\"checkstyle:HiddenField\")\n-  public void wrapSchema(Schema schema) {\n-    this.schema = schema;\n+  public void set(T newValue) {\n+    this.value = newValue;\n   }\n \n   @Override\n-  public void write(DataOutput dataOutput) {\n-    throw new UnsupportedOperationException(\"write is not supported.\");\n+  public void readFields(DataInput in) throws IOException {\n+    throw new UnsupportedOperationException(\"readFields is not supported\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f46b38b081a8d2a3b6b439ab6368c29447808e1c"}, "originalPosition": 53}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyNTQwNDY2OnYy", "diffSide": "RIGHT", "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/MapredIcebergInputFormat.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMVQwMDo0Njo1MFrOGwJEkg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMVQwMDo0Njo1MFrOGwJEkg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEzMzQ1OA==", "bodyText": "Could you update this to Java class of records constructed by Iceberg; default is {@link Record}?\nIt is odd that this currently states that T could be either A or B, but defaults to C.", "url": "https://github.com/apache/iceberg/pull/1192#discussion_r453133458", "createdAt": "2020-07-11T00:46:50Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/MapredIcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.io.IOException;\n+import java.util.Optional;\n+import org.apache.hadoop.mapred.InputFormat;\n+import org.apache.hadoop.mapred.InputSplit;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.RecordReader;\n+import org.apache.hadoop.mapred.Reporter;\n+import org.apache.hadoop.mapred.TaskAttemptContextImpl;\n+import org.apache.hadoop.mapred.TaskAttemptID;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.mr.mapreduce.IcebergSplit;\n+import org.apache.iceberg.mr.mapreduce.IcebergSplitContainer;\n+\n+/**\n+ * Generic Mrv1 InputFormat API for Iceberg.\n+ *\n+ * @param <T> T is the in memory data model which can either be Pig tuples, Hive rows. Default is Iceberg records", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f46b38b081a8d2a3b6b439ab6368c29447808e1c"}, "originalPosition": 39}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyNTQwNzU1OnYy", "diffSide": "RIGHT", "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/MapredIcebergInputFormat.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMVQwMDo0OTo0OFrOGwJGGA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNlQyMjo1MToyMFrOGy_ceQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEzMzg0OA==", "bodyText": "When would mapred.task.id be null? Should we throw an exception in that case?", "url": "https://github.com/apache/iceberg/pull/1192#discussion_r453133848", "createdAt": "2020-07-11T00:49:48Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/MapredIcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.io.IOException;\n+import java.util.Optional;\n+import org.apache.hadoop.mapred.InputFormat;\n+import org.apache.hadoop.mapred.InputSplit;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.RecordReader;\n+import org.apache.hadoop.mapred.Reporter;\n+import org.apache.hadoop.mapred.TaskAttemptContextImpl;\n+import org.apache.hadoop.mapred.TaskAttemptID;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.mr.mapreduce.IcebergSplit;\n+import org.apache.iceberg.mr.mapreduce.IcebergSplitContainer;\n+\n+/**\n+ * Generic Mrv1 InputFormat API for Iceberg.\n+ *\n+ * @param <T> T is the in memory data model which can either be Pig tuples, Hive rows. Default is Iceberg records\n+ */\n+public class MapredIcebergInputFormat<T> implements InputFormat<Void, Container<T>> {\n+\n+  private final org.apache.iceberg.mr.mapreduce.IcebergInputFormat<T> innerInputFormat;\n+\n+  public MapredIcebergInputFormat() {\n+    this.innerInputFormat = new org.apache.iceberg.mr.mapreduce.IcebergInputFormat<>();\n+  }\n+\n+  /**\n+   * Configures the {@code JobConf} to use the {@code MapredIcebergInputFormat} and\n+   * returns a helper to add further configuration.\n+   *\n+   * @param job the {@code JobConf} to configure\n+   */\n+  public static InputFormatConfig.ConfigBuilder configure(JobConf job) {\n+    job.setInputFormat(MapredIcebergInputFormat.class);\n+    return new InputFormatConfig.ConfigBuilder(job);\n+  }\n+\n+  @Override\n+  public InputSplit[] getSplits(JobConf job, int numSplits) throws IOException {\n+    return innerInputFormat.getSplits(getTaskAttemptContext(job))\n+                           .stream()\n+                           .map(InputSplit.class::cast)\n+                           .toArray(InputSplit[]::new);\n+  }\n+\n+  @Override\n+  public RecordReader<Void, Container<T>> getRecordReader(InputSplit split, JobConf job,\n+                                                          Reporter reporter) throws IOException {\n+    IcebergSplit icebergSplit = ((IcebergSplitContainer) split).icebergSplit();\n+    return new MapredIcebergRecordReader<>(innerInputFormat, icebergSplit, job, reporter);\n+  }\n+\n+  static TaskAttemptContext getTaskAttemptContext(JobConf job) {\n+    TaskAttemptID taskAttemptID = Optional.ofNullable(TaskAttemptID.forName(job.get(\"mapred.task.id\")))\n+                                          .orElse(new TaskAttemptID());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f46b38b081a8d2a3b6b439ab6368c29447808e1c"}, "originalPosition": 77}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEzNDkxMQ==", "bodyText": "I'm guessing that this is because an attempt context is passed as a JobContext. Let's fix that problem and then we won't need to do this. The helpers I mentioned also demonstrate how to create a TaskAttemptContext from specific values.", "url": "https://github.com/apache/iceberg/pull/1192#discussion_r453134911", "createdAt": "2020-07-11T00:58:08Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/MapredIcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.io.IOException;\n+import java.util.Optional;\n+import org.apache.hadoop.mapred.InputFormat;\n+import org.apache.hadoop.mapred.InputSplit;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.RecordReader;\n+import org.apache.hadoop.mapred.Reporter;\n+import org.apache.hadoop.mapred.TaskAttemptContextImpl;\n+import org.apache.hadoop.mapred.TaskAttemptID;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.mr.mapreduce.IcebergSplit;\n+import org.apache.iceberg.mr.mapreduce.IcebergSplitContainer;\n+\n+/**\n+ * Generic Mrv1 InputFormat API for Iceberg.\n+ *\n+ * @param <T> T is the in memory data model which can either be Pig tuples, Hive rows. Default is Iceberg records\n+ */\n+public class MapredIcebergInputFormat<T> implements InputFormat<Void, Container<T>> {\n+\n+  private final org.apache.iceberg.mr.mapreduce.IcebergInputFormat<T> innerInputFormat;\n+\n+  public MapredIcebergInputFormat() {\n+    this.innerInputFormat = new org.apache.iceberg.mr.mapreduce.IcebergInputFormat<>();\n+  }\n+\n+  /**\n+   * Configures the {@code JobConf} to use the {@code MapredIcebergInputFormat} and\n+   * returns a helper to add further configuration.\n+   *\n+   * @param job the {@code JobConf} to configure\n+   */\n+  public static InputFormatConfig.ConfigBuilder configure(JobConf job) {\n+    job.setInputFormat(MapredIcebergInputFormat.class);\n+    return new InputFormatConfig.ConfigBuilder(job);\n+  }\n+\n+  @Override\n+  public InputSplit[] getSplits(JobConf job, int numSplits) throws IOException {\n+    return innerInputFormat.getSplits(getTaskAttemptContext(job))\n+                           .stream()\n+                           .map(InputSplit.class::cast)\n+                           .toArray(InputSplit[]::new);\n+  }\n+\n+  @Override\n+  public RecordReader<Void, Container<T>> getRecordReader(InputSplit split, JobConf job,\n+                                                          Reporter reporter) throws IOException {\n+    IcebergSplit icebergSplit = ((IcebergSplitContainer) split).icebergSplit();\n+    return new MapredIcebergRecordReader<>(innerInputFormat, icebergSplit, job, reporter);\n+  }\n+\n+  static TaskAttemptContext getTaskAttemptContext(JobConf job) {\n+    TaskAttemptID taskAttemptID = Optional.ofNullable(TaskAttemptID.forName(job.get(\"mapred.task.id\")))\n+                                          .orElse(new TaskAttemptID());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEzMzg0OA=="}, "originalCommit": {"oid": "f46b38b081a8d2a3b6b439ab6368c29447808e1c"}, "originalPosition": 77}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjEyMTQ2NQ==", "bodyText": "I looked into this and Hive (when spawning a local MR job) or YARN populates the mapreduce.task.id and mapreduce.task.attempt.id\" (among many other properties). So I changed this line to: TaskAttemptID.forName(job.get(\"mapreduce.task.attempt.id\"). Several input formats in the Hive codebase do the same.\nI believe it is not the responsibility of the input format to create a TaskAttemptID from scratch and setting the mapred{uce}* properties. The framework using the input format are responsible for that.\nDuring my tests, I've seen that the task attempt it is not set only when Hive uses the the file input format outside of a MR job (single fetch task). This is when we need to fallback to the default constructor.\nThe same logic applies for the JobContext object.", "url": "https://github.com/apache/iceberg/pull/1192#discussion_r456121465", "createdAt": "2020-07-16T22:51:20Z", "author": {"login": "guilload"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/MapredIcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.io.IOException;\n+import java.util.Optional;\n+import org.apache.hadoop.mapred.InputFormat;\n+import org.apache.hadoop.mapred.InputSplit;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.RecordReader;\n+import org.apache.hadoop.mapred.Reporter;\n+import org.apache.hadoop.mapred.TaskAttemptContextImpl;\n+import org.apache.hadoop.mapred.TaskAttemptID;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.mr.mapreduce.IcebergSplit;\n+import org.apache.iceberg.mr.mapreduce.IcebergSplitContainer;\n+\n+/**\n+ * Generic Mrv1 InputFormat API for Iceberg.\n+ *\n+ * @param <T> T is the in memory data model which can either be Pig tuples, Hive rows. Default is Iceberg records\n+ */\n+public class MapredIcebergInputFormat<T> implements InputFormat<Void, Container<T>> {\n+\n+  private final org.apache.iceberg.mr.mapreduce.IcebergInputFormat<T> innerInputFormat;\n+\n+  public MapredIcebergInputFormat() {\n+    this.innerInputFormat = new org.apache.iceberg.mr.mapreduce.IcebergInputFormat<>();\n+  }\n+\n+  /**\n+   * Configures the {@code JobConf} to use the {@code MapredIcebergInputFormat} and\n+   * returns a helper to add further configuration.\n+   *\n+   * @param job the {@code JobConf} to configure\n+   */\n+  public static InputFormatConfig.ConfigBuilder configure(JobConf job) {\n+    job.setInputFormat(MapredIcebergInputFormat.class);\n+    return new InputFormatConfig.ConfigBuilder(job);\n+  }\n+\n+  @Override\n+  public InputSplit[] getSplits(JobConf job, int numSplits) throws IOException {\n+    return innerInputFormat.getSplits(getTaskAttemptContext(job))\n+                           .stream()\n+                           .map(InputSplit.class::cast)\n+                           .toArray(InputSplit[]::new);\n+  }\n+\n+  @Override\n+  public RecordReader<Void, Container<T>> getRecordReader(InputSplit split, JobConf job,\n+                                                          Reporter reporter) throws IOException {\n+    IcebergSplit icebergSplit = ((IcebergSplitContainer) split).icebergSplit();\n+    return new MapredIcebergRecordReader<>(innerInputFormat, icebergSplit, job, reporter);\n+  }\n+\n+  static TaskAttemptContext getTaskAttemptContext(JobConf job) {\n+    TaskAttemptID taskAttemptID = Optional.ofNullable(TaskAttemptID.forName(job.get(\"mapred.task.id\")))\n+                                          .orElse(new TaskAttemptID());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEzMzg0OA=="}, "originalCommit": {"oid": "f46b38b081a8d2a3b6b439ab6368c29447808e1c"}, "originalPosition": 77}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyNTQxMzc0OnYy", "diffSide": "RIGHT", "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/MapredIcebergInputFormat.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMVQwMDo1NjoxOFrOGwJJXA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMVQwMDo1NjoxOFrOGwJJXA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEzNDY4NA==", "bodyText": "getSplits accepts a JobContext and I think it makes sense to pass objects that are as close as possible to what the mapreduce framework would use. We have some helper methods in our branch for reading Hive tables from Spark's DSv2 that you might want to check out: https://github.com/Netflix/iceberg/blob/netflix-spark-2.4/metacat/src/main/java/com/netflix/iceberg/batch/MapReduceUtil.java.\nThose can help you create mapreduce objects after inspecting the mapred objects.", "url": "https://github.com/apache/iceberg/pull/1192#discussion_r453134684", "createdAt": "2020-07-11T00:56:18Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/MapredIcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.io.IOException;\n+import java.util.Optional;\n+import org.apache.hadoop.mapred.InputFormat;\n+import org.apache.hadoop.mapred.InputSplit;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.RecordReader;\n+import org.apache.hadoop.mapred.Reporter;\n+import org.apache.hadoop.mapred.TaskAttemptContextImpl;\n+import org.apache.hadoop.mapred.TaskAttemptID;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.mr.mapreduce.IcebergSplit;\n+import org.apache.iceberg.mr.mapreduce.IcebergSplitContainer;\n+\n+/**\n+ * Generic Mrv1 InputFormat API for Iceberg.\n+ *\n+ * @param <T> T is the in memory data model which can either be Pig tuples, Hive rows. Default is Iceberg records\n+ */\n+public class MapredIcebergInputFormat<T> implements InputFormat<Void, Container<T>> {\n+\n+  private final org.apache.iceberg.mr.mapreduce.IcebergInputFormat<T> innerInputFormat;\n+\n+  public MapredIcebergInputFormat() {\n+    this.innerInputFormat = new org.apache.iceberg.mr.mapreduce.IcebergInputFormat<>();\n+  }\n+\n+  /**\n+   * Configures the {@code JobConf} to use the {@code MapredIcebergInputFormat} and\n+   * returns a helper to add further configuration.\n+   *\n+   * @param job the {@code JobConf} to configure\n+   */\n+  public static InputFormatConfig.ConfigBuilder configure(JobConf job) {\n+    job.setInputFormat(MapredIcebergInputFormat.class);\n+    return new InputFormatConfig.ConfigBuilder(job);\n+  }\n+\n+  @Override\n+  public InputSplit[] getSplits(JobConf job, int numSplits) throws IOException {\n+    return innerInputFormat.getSplits(getTaskAttemptContext(job))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f46b38b081a8d2a3b6b439ab6368c29447808e1c"}, "originalPosition": 62}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgzMzk1MzU2OnYy", "diffSide": "RIGHT", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergInputFormat.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNFQxMzo0OTo0M1rOGxUjvw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNlQyMTozMDoyMlrOGy9ePg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDM3MDIzOQ==", "bodyText": "Can these properties ever be null?", "url": "https://github.com/apache/iceberg/pull/1192#discussion_r454370239", "createdAt": "2020-07-14T13:49:43Z", "author": {"login": "rdsr"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;\n+import org.apache.hadoop.hive.serde2.ColumnProjectionUtils;\n+import org.apache.hadoop.mapred.InputSplit;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.RecordReader;\n+import org.apache.hadoop.mapred.Reporter;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.mr.mapred.Container;\n+import org.apache.iceberg.mr.mapred.MapredIcebergInputFormat;\n+import org.apache.iceberg.mr.mapred.TableResolver;\n+import org.apache.iceberg.mr.mapreduce.IcebergSplit;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+\n+public class HiveIcebergInputFormat extends MapredIcebergInputFormat<Record>\n+                                    implements CombineHiveInputFormat.AvoidSplitCombination {\n+\n+  private transient Table table;\n+  private transient Schema schema;\n+  private transient Schema projection;\n+\n+  @Override\n+  public InputSplit[] getSplits(JobConf job, int numSplits) throws IOException {\n+    table = TableResolver.resolveTableFromConfiguration(job);\n+    schema = table.schema();\n+\n+    List<String> projectedColumns = parseProjectedColumns(job);\n+    projection = projectedColumns.isEmpty() ? schema : projection.select(projectedColumns);\n+\n+    forwardConfigSettings(job);\n+\n+    return Arrays.stream(super.getSplits(job, numSplits))\n+                 .map(split -> new HiveIcebergSplit((IcebergSplit) split, table.location()))\n+                 .toArray(InputSplit[]::new);\n+  }\n+\n+  @Override\n+  public RecordReader<Void, Container<Record>> getRecordReader(InputSplit split, JobConf job,\n+                                                               Reporter reporter) throws IOException {\n+    // Since Hive passes a copy of `job` in `getSplits`, we need to forward the conf settings again.\n+    forwardConfigSettings(job);\n+    return super.getRecordReader(split, job, reporter);\n+  }\n+\n+  @Override\n+  public boolean shouldSkipCombine(Path path, Configuration conf) {\n+    return true;\n+  }\n+\n+  /**\n+   * Forward configuration settings to the underlying MR input format.\n+   */\n+  private void forwardConfigSettings(JobConf job) {\n+    Preconditions.checkNotNull(table, \"Table cannot be null\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f3928c77ea2498fa363257ddec86ff5090288712"}, "originalPosition": 84}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjA4OTE1MA==", "bodyText": "Only if getRecordReader is called before getSplits. Happy to remove those checks if we judge them overkill.", "url": "https://github.com/apache/iceberg/pull/1192#discussion_r456089150", "createdAt": "2020-07-16T21:30:22Z", "author": {"login": "guilload"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;\n+import org.apache.hadoop.hive.serde2.ColumnProjectionUtils;\n+import org.apache.hadoop.mapred.InputSplit;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.RecordReader;\n+import org.apache.hadoop.mapred.Reporter;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.mr.mapred.Container;\n+import org.apache.iceberg.mr.mapred.MapredIcebergInputFormat;\n+import org.apache.iceberg.mr.mapred.TableResolver;\n+import org.apache.iceberg.mr.mapreduce.IcebergSplit;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+\n+public class HiveIcebergInputFormat extends MapredIcebergInputFormat<Record>\n+                                    implements CombineHiveInputFormat.AvoidSplitCombination {\n+\n+  private transient Table table;\n+  private transient Schema schema;\n+  private transient Schema projection;\n+\n+  @Override\n+  public InputSplit[] getSplits(JobConf job, int numSplits) throws IOException {\n+    table = TableResolver.resolveTableFromConfiguration(job);\n+    schema = table.schema();\n+\n+    List<String> projectedColumns = parseProjectedColumns(job);\n+    projection = projectedColumns.isEmpty() ? schema : projection.select(projectedColumns);\n+\n+    forwardConfigSettings(job);\n+\n+    return Arrays.stream(super.getSplits(job, numSplits))\n+                 .map(split -> new HiveIcebergSplit((IcebergSplit) split, table.location()))\n+                 .toArray(InputSplit[]::new);\n+  }\n+\n+  @Override\n+  public RecordReader<Void, Container<Record>> getRecordReader(InputSplit split, JobConf job,\n+                                                               Reporter reporter) throws IOException {\n+    // Since Hive passes a copy of `job` in `getSplits`, we need to forward the conf settings again.\n+    forwardConfigSettings(job);\n+    return super.getRecordReader(split, job, reporter);\n+  }\n+\n+  @Override\n+  public boolean shouldSkipCombine(Path path, Configuration conf) {\n+    return true;\n+  }\n+\n+  /**\n+   * Forward configuration settings to the underlying MR input format.\n+   */\n+  private void forwardConfigSettings(JobConf job) {\n+    Preconditions.checkNotNull(table, \"Table cannot be null\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDM3MDIzOQ=="}, "originalCommit": {"oid": "f3928c77ea2498fa363257ddec86ff5090288712"}, "originalPosition": 84}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgzNzgwMDAwOnYy", "diffSide": "RIGHT", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergInputFormat.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQxMDo1ODo0MlrOGx4_tQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQxMDo1ODo0MlrOGx4_tQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDk2NzIyMQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                projection = projectedColumns.isEmpty() ? schema : projection.select(projectedColumns);\n          \n          \n            \n                projection = projectedColumns.isEmpty() ? schema : schema.select(projectedColumns);\n          \n      \n    \n    \n  \n\nShouldn't the code be like above? If not I get a NPE like this when running a HiveRunner test:\n       Caused by:\n                java.lang.NullPointerException\n                    at org.apache.iceberg.mr.hive.HiveIcebergInputFormat.getSplits(HiveIcebergInputFormat.java:58)\n                    at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextSplits(FetchOperator.java:372)\n                    at org.apache.hadoop.hive.ql.exec.FetchOperator.getRecordReader(FetchOperator.java:304)\n                    at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:459)\n                    ... 13 more", "url": "https://github.com/apache/iceberg/pull/1192#discussion_r454967221", "createdAt": "2020-07-15T10:58:42Z", "author": {"login": "massdosage"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;\n+import org.apache.hadoop.hive.serde2.ColumnProjectionUtils;\n+import org.apache.hadoop.mapred.InputSplit;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.RecordReader;\n+import org.apache.hadoop.mapred.Reporter;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.mr.mapred.Container;\n+import org.apache.iceberg.mr.mapred.MapredIcebergInputFormat;\n+import org.apache.iceberg.mr.mapred.TableResolver;\n+import org.apache.iceberg.mr.mapreduce.IcebergSplit;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+\n+public class HiveIcebergInputFormat extends MapredIcebergInputFormat<Record>\n+                                    implements CombineHiveInputFormat.AvoidSplitCombination {\n+\n+  private transient Table table;\n+  private transient Schema schema;\n+  private transient Schema projection;\n+\n+  @Override\n+  public InputSplit[] getSplits(JobConf job, int numSplits) throws IOException {\n+    table = TableResolver.resolveTableFromConfiguration(job);\n+    schema = table.schema();\n+\n+    List<String> projectedColumns = parseProjectedColumns(job);\n+    projection = projectedColumns.isEmpty() ? schema : projection.select(projectedColumns);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "23ad535e8206efb4bbb1bef1a9504051aa4ec80c"}, "originalPosition": 58}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgzOTgwMDIxOnYy", "diffSide": "RIGHT", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergInputFormat.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQxOToyMDoxNFrOGyMcUw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQxOToyMDoxNFrOGyMcUw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTI4NTg0Mw==", "bodyText": "seems like conf  will not be null", "url": "https://github.com/apache/iceberg/pull/1192#discussion_r455285843", "createdAt": "2020-07-15T19:20:14Z", "author": {"login": "rdsr"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;\n+import org.apache.hadoop.hive.serde2.ColumnProjectionUtils;\n+import org.apache.hadoop.mapred.InputSplit;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.RecordReader;\n+import org.apache.hadoop.mapred.Reporter;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.mr.mapred.Container;\n+import org.apache.iceberg.mr.mapred.MapredIcebergInputFormat;\n+import org.apache.iceberg.mr.mapred.TableResolver;\n+import org.apache.iceberg.mr.mapreduce.IcebergSplit;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+\n+public class HiveIcebergInputFormat extends MapredIcebergInputFormat<Record>\n+                                    implements CombineHiveInputFormat.AvoidSplitCombination {\n+\n+  private transient Table table;\n+  private transient Schema schema;\n+  private transient Schema projection;\n+\n+  @Override\n+  public InputSplit[] getSplits(JobConf job, int numSplits) throws IOException {\n+    table = TableResolver.resolveTableFromConfiguration(job);\n+    schema = table.schema();\n+\n+    List<String> projectedColumns = parseProjectedColumns(job);\n+    projection = projectedColumns.isEmpty() ? schema : schema.select(projectedColumns);\n+\n+    forwardConfigSettings(job);\n+\n+    return Arrays.stream(super.getSplits(job, numSplits))\n+                 .map(split -> new HiveIcebergSplit((IcebergSplit) split, table.location()))\n+                 .toArray(InputSplit[]::new);\n+  }\n+\n+  @Override\n+  public RecordReader<Void, Container<Record>> getRecordReader(InputSplit split, JobConf job,\n+                                                               Reporter reporter) throws IOException {\n+    // Since Hive passes a copy of `job` in `getSplits`, we need to forward the conf settings again.\n+    forwardConfigSettings(job);\n+    return super.getRecordReader(split, job, reporter);\n+  }\n+\n+  @Override\n+  public boolean shouldSkipCombine(Path path, Configuration conf) {\n+    return true;\n+  }\n+\n+  /**\n+   * Forward configuration settings to the underlying MR input format.\n+   */\n+  private void forwardConfigSettings(JobConf job) {\n+    Preconditions.checkNotNull(table, \"Table cannot be null\");\n+    Preconditions.checkNotNull(schema, \"Schema cannot be null\");\n+    Preconditions.checkNotNull(projection, \"Projection cannot be null\");\n+\n+    // Once mapred.TableResolver and mapreduce.TableResolver use the same property for the location of the table\n+    // (TABLE_LOCATION vs. TABLE_PATH), this line can be removed: see https://github.com/apache/iceberg/issues/1155.\n+    job.set(InputFormatConfig.TABLE_PATH, table.location());\n+    job.set(InputFormatConfig.TABLE_SCHEMA, SchemaParser.toJson(schema));\n+    job.set(InputFormatConfig.READ_SCHEMA, SchemaParser.toJson(projection));\n+  }\n+\n+  private static List<String> parseProjectedColumns(Configuration conf) {\n+    if (conf == null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b373262cf50391eb1b4d44d49bc3dcbc14b1e80a"}, "originalPosition": 96}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgzOTgxMDg5OnYy", "diffSide": "RIGHT", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergInputFormat.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQxOToyMzoxOVrOGyMi5A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNlQyMToyOTowNFrOGy9bzA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTI4NzUyNA==", "bodyText": "Who is setting these configurations. IcebergStorageHandler?", "url": "https://github.com/apache/iceberg/pull/1192#discussion_r455287524", "createdAt": "2020-07-15T19:23:19Z", "author": {"login": "rdsr"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;\n+import org.apache.hadoop.hive.serde2.ColumnProjectionUtils;\n+import org.apache.hadoop.mapred.InputSplit;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.RecordReader;\n+import org.apache.hadoop.mapred.Reporter;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.mr.mapred.Container;\n+import org.apache.iceberg.mr.mapred.MapredIcebergInputFormat;\n+import org.apache.iceberg.mr.mapred.TableResolver;\n+import org.apache.iceberg.mr.mapreduce.IcebergSplit;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+\n+public class HiveIcebergInputFormat extends MapredIcebergInputFormat<Record>\n+                                    implements CombineHiveInputFormat.AvoidSplitCombination {\n+\n+  private transient Table table;\n+  private transient Schema schema;\n+  private transient Schema projection;\n+\n+  @Override\n+  public InputSplit[] getSplits(JobConf job, int numSplits) throws IOException {\n+    table = TableResolver.resolveTableFromConfiguration(job);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b373262cf50391eb1b4d44d49bc3dcbc14b1e80a"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjA4ODUyNA==", "bodyText": "Currently, mapred.TableResolver copies the SerDe properties in the main configuration when called in IcebergSerDe. Down the road, IcebergStorageHandler should probably handle this so we can clean this up.", "url": "https://github.com/apache/iceberg/pull/1192#discussion_r456088524", "createdAt": "2020-07-16T21:29:04Z", "author": {"login": "guilload"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;\n+import org.apache.hadoop.hive.serde2.ColumnProjectionUtils;\n+import org.apache.hadoop.mapred.InputSplit;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.RecordReader;\n+import org.apache.hadoop.mapred.Reporter;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.mr.mapred.Container;\n+import org.apache.iceberg.mr.mapred.MapredIcebergInputFormat;\n+import org.apache.iceberg.mr.mapred.TableResolver;\n+import org.apache.iceberg.mr.mapreduce.IcebergSplit;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+\n+public class HiveIcebergInputFormat extends MapredIcebergInputFormat<Record>\n+                                    implements CombineHiveInputFormat.AvoidSplitCombination {\n+\n+  private transient Table table;\n+  private transient Schema schema;\n+  private transient Schema projection;\n+\n+  @Override\n+  public InputSplit[] getSplits(JobConf job, int numSplits) throws IOException {\n+    table = TableResolver.resolveTableFromConfiguration(job);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTI4NzUyNA=="}, "originalCommit": {"oid": "b373262cf50391eb1b4d44d49bc3dcbc14b1e80a"}, "originalPosition": 54}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgzOTg0NjY1OnYy", "diffSide": "RIGHT", "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/MapredIcebergInputFormat.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQxOTozMzozNVrOGyM5Sw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNlQyMTo0MDoyNlrOGy9wXA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTI5MzI1OQ==", "bodyText": "is this null check required?", "url": "https://github.com/apache/iceberg/pull/1192#discussion_r455293259", "createdAt": "2020-07-15T19:33:35Z", "author": {"login": "rdsr"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/MapredIcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,191 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.io.IOException;\n+import java.util.Optional;\n+import org.apache.hadoop.mapred.InputFormat;\n+import org.apache.hadoop.mapred.InputSplit;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.RecordReader;\n+import org.apache.hadoop.mapred.Reporter;\n+import org.apache.hadoop.mapred.TaskAttemptContextImpl;\n+import org.apache.hadoop.mapred.TaskAttemptID;\n+import org.apache.hadoop.mapreduce.Counter;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.JobID;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.task.JobContextImpl;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.mr.mapreduce.IcebergSplit;\n+import org.apache.iceberg.mr.mapreduce.IcebergSplitContainer;\n+\n+/**\n+ * Generic MR v1 InputFormat API for Iceberg.\n+ *\n+ * @param <T> Java class of records constructed by Iceberg; default is {@link Record}\n+ */\n+public class MapredIcebergInputFormat<T> implements InputFormat<Void, Container<T>> {\n+\n+  private final org.apache.iceberg.mr.mapreduce.IcebergInputFormat<T> innerInputFormat;\n+\n+  public MapredIcebergInputFormat() {\n+    this.innerInputFormat = new org.apache.iceberg.mr.mapreduce.IcebergInputFormat<>();\n+  }\n+\n+  /**\n+   * Configures the {@code JobConf} to use the {@code MapredIcebergInputFormat} and\n+   * returns a helper to add further configuration.\n+   *\n+   * @param job the {@code JobConf} to configure\n+   */\n+  public static InputFormatConfig.ConfigBuilder configure(JobConf job) {\n+    job.setInputFormat(MapredIcebergInputFormat.class);\n+    return new InputFormatConfig.ConfigBuilder(job);\n+  }\n+\n+  @Override\n+  public InputSplit[] getSplits(JobConf job, int numSplits) throws IOException {\n+    return innerInputFormat.getSplits(newJobContext(job))\n+                           .stream()\n+                           .map(InputSplit.class::cast)\n+                           .toArray(InputSplit[]::new);\n+  }\n+\n+  @Override\n+  public RecordReader<Void, Container<T>> getRecordReader(InputSplit split, JobConf job,\n+                                                          Reporter reporter) throws IOException {\n+    IcebergSplit icebergSplit = ((IcebergSplitContainer) split).icebergSplit();\n+    return new MapredIcebergRecordReader<>(innerInputFormat, icebergSplit, job, reporter);\n+  }\n+\n+  private static final class MapredIcebergRecordReader<T> implements RecordReader<Void, Container<T>> {\n+\n+    private final org.apache.hadoop.mapreduce.RecordReader<Void, T> innerReader;\n+    private final long splitLength; // for getPos()\n+\n+    MapredIcebergRecordReader(org.apache.iceberg.mr.mapreduce.IcebergInputFormat<T> mapreduceInputFormat,\n+                              IcebergSplit split, JobConf job, Reporter reporter) throws IOException {\n+      TaskAttemptContext context = newTaskAttemptContext(job, reporter);\n+\n+      try {\n+        innerReader = mapreduceInputFormat.createRecordReader(split, context);\n+        innerReader.initialize(split, context);\n+      } catch (InterruptedException e) {\n+        Thread.currentThread().interrupt();\n+        throw new RuntimeException(e);\n+      }\n+\n+      splitLength = split.getLength();\n+    }\n+\n+    @Override\n+    public boolean next(Void key, Container<T> value) throws IOException {\n+      try {\n+        if (innerReader.nextKeyValue()) {\n+          value.set(innerReader.getCurrentValue());\n+          return true;\n+        }\n+      } catch (InterruptedException ie) {\n+        Thread.currentThread().interrupt();\n+        throw new RuntimeException(ie);\n+      }\n+\n+      return false;\n+    }\n+\n+    @Override\n+    public Void createKey() {\n+      return null;\n+    }\n+\n+    @Override\n+    public Container<T> createValue() {\n+      return new Container<>();\n+    }\n+\n+    @Override\n+    public long getPos() throws IOException {\n+      return (long) (splitLength * getProgress());\n+    }\n+\n+    @Override\n+    public float getProgress() throws IOException {\n+      if (innerReader == null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b373262cf50391eb1b4d44d49bc3dcbc14b1e80a"}, "originalPosition": 132}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjA5Mzc4OA==", "bodyText": "Probably not, will remove.", "url": "https://github.com/apache/iceberg/pull/1192#discussion_r456093788", "createdAt": "2020-07-16T21:40:26Z", "author": {"login": "guilload"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/MapredIcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,191 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.io.IOException;\n+import java.util.Optional;\n+import org.apache.hadoop.mapred.InputFormat;\n+import org.apache.hadoop.mapred.InputSplit;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.RecordReader;\n+import org.apache.hadoop.mapred.Reporter;\n+import org.apache.hadoop.mapred.TaskAttemptContextImpl;\n+import org.apache.hadoop.mapred.TaskAttemptID;\n+import org.apache.hadoop.mapreduce.Counter;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.JobID;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.task.JobContextImpl;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.mr.mapreduce.IcebergSplit;\n+import org.apache.iceberg.mr.mapreduce.IcebergSplitContainer;\n+\n+/**\n+ * Generic MR v1 InputFormat API for Iceberg.\n+ *\n+ * @param <T> Java class of records constructed by Iceberg; default is {@link Record}\n+ */\n+public class MapredIcebergInputFormat<T> implements InputFormat<Void, Container<T>> {\n+\n+  private final org.apache.iceberg.mr.mapreduce.IcebergInputFormat<T> innerInputFormat;\n+\n+  public MapredIcebergInputFormat() {\n+    this.innerInputFormat = new org.apache.iceberg.mr.mapreduce.IcebergInputFormat<>();\n+  }\n+\n+  /**\n+   * Configures the {@code JobConf} to use the {@code MapredIcebergInputFormat} and\n+   * returns a helper to add further configuration.\n+   *\n+   * @param job the {@code JobConf} to configure\n+   */\n+  public static InputFormatConfig.ConfigBuilder configure(JobConf job) {\n+    job.setInputFormat(MapredIcebergInputFormat.class);\n+    return new InputFormatConfig.ConfigBuilder(job);\n+  }\n+\n+  @Override\n+  public InputSplit[] getSplits(JobConf job, int numSplits) throws IOException {\n+    return innerInputFormat.getSplits(newJobContext(job))\n+                           .stream()\n+                           .map(InputSplit.class::cast)\n+                           .toArray(InputSplit[]::new);\n+  }\n+\n+  @Override\n+  public RecordReader<Void, Container<T>> getRecordReader(InputSplit split, JobConf job,\n+                                                          Reporter reporter) throws IOException {\n+    IcebergSplit icebergSplit = ((IcebergSplitContainer) split).icebergSplit();\n+    return new MapredIcebergRecordReader<>(innerInputFormat, icebergSplit, job, reporter);\n+  }\n+\n+  private static final class MapredIcebergRecordReader<T> implements RecordReader<Void, Container<T>> {\n+\n+    private final org.apache.hadoop.mapreduce.RecordReader<Void, T> innerReader;\n+    private final long splitLength; // for getPos()\n+\n+    MapredIcebergRecordReader(org.apache.iceberg.mr.mapreduce.IcebergInputFormat<T> mapreduceInputFormat,\n+                              IcebergSplit split, JobConf job, Reporter reporter) throws IOException {\n+      TaskAttemptContext context = newTaskAttemptContext(job, reporter);\n+\n+      try {\n+        innerReader = mapreduceInputFormat.createRecordReader(split, context);\n+        innerReader.initialize(split, context);\n+      } catch (InterruptedException e) {\n+        Thread.currentThread().interrupt();\n+        throw new RuntimeException(e);\n+      }\n+\n+      splitLength = split.getLength();\n+    }\n+\n+    @Override\n+    public boolean next(Void key, Container<T> value) throws IOException {\n+      try {\n+        if (innerReader.nextKeyValue()) {\n+          value.set(innerReader.getCurrentValue());\n+          return true;\n+        }\n+      } catch (InterruptedException ie) {\n+        Thread.currentThread().interrupt();\n+        throw new RuntimeException(ie);\n+      }\n+\n+      return false;\n+    }\n+\n+    @Override\n+    public Void createKey() {\n+      return null;\n+    }\n+\n+    @Override\n+    public Container<T> createValue() {\n+      return new Container<>();\n+    }\n+\n+    @Override\n+    public long getPos() throws IOException {\n+      return (long) (splitLength * getProgress());\n+    }\n+\n+    @Override\n+    public float getProgress() throws IOException {\n+      if (innerReader == null) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTI5MzI1OQ=="}, "originalCommit": {"oid": "b373262cf50391eb1b4d44d49bc3dcbc14b1e80a"}, "originalPosition": 132}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgzOTg1NDA0OnYy", "diffSide": "RIGHT", "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/MapredIcebergInputFormat.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQxOTozNTozOVrOGyM9ng==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xOFQxNjozNTo0OFrOGzpO9w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTI5NDM2Ng==", "bodyText": "is JobContext.ID which maps to mapreduce.job.id an MRv2 setting and will not be set for Mrv1 jobs?", "url": "https://github.com/apache/iceberg/pull/1192#discussion_r455294366", "createdAt": "2020-07-15T19:35:39Z", "author": {"login": "rdsr"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/MapredIcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,191 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.io.IOException;\n+import java.util.Optional;\n+import org.apache.hadoop.mapred.InputFormat;\n+import org.apache.hadoop.mapred.InputSplit;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.RecordReader;\n+import org.apache.hadoop.mapred.Reporter;\n+import org.apache.hadoop.mapred.TaskAttemptContextImpl;\n+import org.apache.hadoop.mapred.TaskAttemptID;\n+import org.apache.hadoop.mapreduce.Counter;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.JobID;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.task.JobContextImpl;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.mr.mapreduce.IcebergSplit;\n+import org.apache.iceberg.mr.mapreduce.IcebergSplitContainer;\n+\n+/**\n+ * Generic MR v1 InputFormat API for Iceberg.\n+ *\n+ * @param <T> Java class of records constructed by Iceberg; default is {@link Record}\n+ */\n+public class MapredIcebergInputFormat<T> implements InputFormat<Void, Container<T>> {\n+\n+  private final org.apache.iceberg.mr.mapreduce.IcebergInputFormat<T> innerInputFormat;\n+\n+  public MapredIcebergInputFormat() {\n+    this.innerInputFormat = new org.apache.iceberg.mr.mapreduce.IcebergInputFormat<>();\n+  }\n+\n+  /**\n+   * Configures the {@code JobConf} to use the {@code MapredIcebergInputFormat} and\n+   * returns a helper to add further configuration.\n+   *\n+   * @param job the {@code JobConf} to configure\n+   */\n+  public static InputFormatConfig.ConfigBuilder configure(JobConf job) {\n+    job.setInputFormat(MapredIcebergInputFormat.class);\n+    return new InputFormatConfig.ConfigBuilder(job);\n+  }\n+\n+  @Override\n+  public InputSplit[] getSplits(JobConf job, int numSplits) throws IOException {\n+    return innerInputFormat.getSplits(newJobContext(job))\n+                           .stream()\n+                           .map(InputSplit.class::cast)\n+                           .toArray(InputSplit[]::new);\n+  }\n+\n+  @Override\n+  public RecordReader<Void, Container<T>> getRecordReader(InputSplit split, JobConf job,\n+                                                          Reporter reporter) throws IOException {\n+    IcebergSplit icebergSplit = ((IcebergSplitContainer) split).icebergSplit();\n+    return new MapredIcebergRecordReader<>(innerInputFormat, icebergSplit, job, reporter);\n+  }\n+\n+  private static final class MapredIcebergRecordReader<T> implements RecordReader<Void, Container<T>> {\n+\n+    private final org.apache.hadoop.mapreduce.RecordReader<Void, T> innerReader;\n+    private final long splitLength; // for getPos()\n+\n+    MapredIcebergRecordReader(org.apache.iceberg.mr.mapreduce.IcebergInputFormat<T> mapreduceInputFormat,\n+                              IcebergSplit split, JobConf job, Reporter reporter) throws IOException {\n+      TaskAttemptContext context = newTaskAttemptContext(job, reporter);\n+\n+      try {\n+        innerReader = mapreduceInputFormat.createRecordReader(split, context);\n+        innerReader.initialize(split, context);\n+      } catch (InterruptedException e) {\n+        Thread.currentThread().interrupt();\n+        throw new RuntimeException(e);\n+      }\n+\n+      splitLength = split.getLength();\n+    }\n+\n+    @Override\n+    public boolean next(Void key, Container<T> value) throws IOException {\n+      try {\n+        if (innerReader.nextKeyValue()) {\n+          value.set(innerReader.getCurrentValue());\n+          return true;\n+        }\n+      } catch (InterruptedException ie) {\n+        Thread.currentThread().interrupt();\n+        throw new RuntimeException(ie);\n+      }\n+\n+      return false;\n+    }\n+\n+    @Override\n+    public Void createKey() {\n+      return null;\n+    }\n+\n+    @Override\n+    public Container<T> createValue() {\n+      return new Container<>();\n+    }\n+\n+    @Override\n+    public long getPos() throws IOException {\n+      return (long) (splitLength * getProgress());\n+    }\n+\n+    @Override\n+    public float getProgress() throws IOException {\n+      if (innerReader == null) {\n+        return 0;\n+      }\n+\n+      try {\n+        return innerReader.getProgress();\n+      } catch (InterruptedException e) {\n+        Thread.currentThread().interrupt();\n+        throw new RuntimeException(e);\n+      }\n+    }\n+\n+    @Override\n+    public void close() throws IOException {\n+      if (innerReader != null) {\n+        innerReader.close();\n+      }\n+    }\n+  }\n+\n+  private static JobContext newJobContext(JobConf job) {\n+    JobID jobID = Optional.ofNullable(JobID.forName(job.get(JobContext.ID)))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b373262cf50391eb1b4d44d49bc3dcbc14b1e80a"}, "originalPosition": 153}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjEwMDIzOQ==", "bodyText": "From what I understand, the mapred.job.id and mapreduce.job.id properties are set by the callers. In practice, I've seen that Hive (when launching a MR job in local-mode) and YARN set the MRv2 properties only.\nI guess the MRv1 properties will be set only if the job is run as a MRv1 job, which should be very rare, right? Do we want to cover this case?", "url": "https://github.com/apache/iceberg/pull/1192#discussion_r456100239", "createdAt": "2020-07-16T21:54:22Z", "author": {"login": "guilload"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/MapredIcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,191 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.io.IOException;\n+import java.util.Optional;\n+import org.apache.hadoop.mapred.InputFormat;\n+import org.apache.hadoop.mapred.InputSplit;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.RecordReader;\n+import org.apache.hadoop.mapred.Reporter;\n+import org.apache.hadoop.mapred.TaskAttemptContextImpl;\n+import org.apache.hadoop.mapred.TaskAttemptID;\n+import org.apache.hadoop.mapreduce.Counter;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.JobID;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.task.JobContextImpl;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.mr.mapreduce.IcebergSplit;\n+import org.apache.iceberg.mr.mapreduce.IcebergSplitContainer;\n+\n+/**\n+ * Generic MR v1 InputFormat API for Iceberg.\n+ *\n+ * @param <T> Java class of records constructed by Iceberg; default is {@link Record}\n+ */\n+public class MapredIcebergInputFormat<T> implements InputFormat<Void, Container<T>> {\n+\n+  private final org.apache.iceberg.mr.mapreduce.IcebergInputFormat<T> innerInputFormat;\n+\n+  public MapredIcebergInputFormat() {\n+    this.innerInputFormat = new org.apache.iceberg.mr.mapreduce.IcebergInputFormat<>();\n+  }\n+\n+  /**\n+   * Configures the {@code JobConf} to use the {@code MapredIcebergInputFormat} and\n+   * returns a helper to add further configuration.\n+   *\n+   * @param job the {@code JobConf} to configure\n+   */\n+  public static InputFormatConfig.ConfigBuilder configure(JobConf job) {\n+    job.setInputFormat(MapredIcebergInputFormat.class);\n+    return new InputFormatConfig.ConfigBuilder(job);\n+  }\n+\n+  @Override\n+  public InputSplit[] getSplits(JobConf job, int numSplits) throws IOException {\n+    return innerInputFormat.getSplits(newJobContext(job))\n+                           .stream()\n+                           .map(InputSplit.class::cast)\n+                           .toArray(InputSplit[]::new);\n+  }\n+\n+  @Override\n+  public RecordReader<Void, Container<T>> getRecordReader(InputSplit split, JobConf job,\n+                                                          Reporter reporter) throws IOException {\n+    IcebergSplit icebergSplit = ((IcebergSplitContainer) split).icebergSplit();\n+    return new MapredIcebergRecordReader<>(innerInputFormat, icebergSplit, job, reporter);\n+  }\n+\n+  private static final class MapredIcebergRecordReader<T> implements RecordReader<Void, Container<T>> {\n+\n+    private final org.apache.hadoop.mapreduce.RecordReader<Void, T> innerReader;\n+    private final long splitLength; // for getPos()\n+\n+    MapredIcebergRecordReader(org.apache.iceberg.mr.mapreduce.IcebergInputFormat<T> mapreduceInputFormat,\n+                              IcebergSplit split, JobConf job, Reporter reporter) throws IOException {\n+      TaskAttemptContext context = newTaskAttemptContext(job, reporter);\n+\n+      try {\n+        innerReader = mapreduceInputFormat.createRecordReader(split, context);\n+        innerReader.initialize(split, context);\n+      } catch (InterruptedException e) {\n+        Thread.currentThread().interrupt();\n+        throw new RuntimeException(e);\n+      }\n+\n+      splitLength = split.getLength();\n+    }\n+\n+    @Override\n+    public boolean next(Void key, Container<T> value) throws IOException {\n+      try {\n+        if (innerReader.nextKeyValue()) {\n+          value.set(innerReader.getCurrentValue());\n+          return true;\n+        }\n+      } catch (InterruptedException ie) {\n+        Thread.currentThread().interrupt();\n+        throw new RuntimeException(ie);\n+      }\n+\n+      return false;\n+    }\n+\n+    @Override\n+    public Void createKey() {\n+      return null;\n+    }\n+\n+    @Override\n+    public Container<T> createValue() {\n+      return new Container<>();\n+    }\n+\n+    @Override\n+    public long getPos() throws IOException {\n+      return (long) (splitLength * getProgress());\n+    }\n+\n+    @Override\n+    public float getProgress() throws IOException {\n+      if (innerReader == null) {\n+        return 0;\n+      }\n+\n+      try {\n+        return innerReader.getProgress();\n+      } catch (InterruptedException e) {\n+        Thread.currentThread().interrupt();\n+        throw new RuntimeException(e);\n+      }\n+    }\n+\n+    @Override\n+    public void close() throws IOException {\n+      if (innerReader != null) {\n+        innerReader.close();\n+      }\n+    }\n+  }\n+\n+  private static JobContext newJobContext(JobConf job) {\n+    JobID jobID = Optional.ofNullable(JobID.forName(job.get(JobContext.ID)))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTI5NDM2Ng=="}, "originalCommit": {"oid": "b373262cf50391eb1b4d44d49bc3dcbc14b1e80a"}, "originalPosition": 153}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjgwNjEzNQ==", "bodyText": "If JobContext.ID is set correctly for Hive then it should be fine!", "url": "https://github.com/apache/iceberg/pull/1192#discussion_r456806135", "createdAt": "2020-07-18T16:35:48Z", "author": {"login": "rdsr"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/MapredIcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,191 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.io.IOException;\n+import java.util.Optional;\n+import org.apache.hadoop.mapred.InputFormat;\n+import org.apache.hadoop.mapred.InputSplit;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.RecordReader;\n+import org.apache.hadoop.mapred.Reporter;\n+import org.apache.hadoop.mapred.TaskAttemptContextImpl;\n+import org.apache.hadoop.mapred.TaskAttemptID;\n+import org.apache.hadoop.mapreduce.Counter;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.JobID;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.task.JobContextImpl;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.mr.mapreduce.IcebergSplit;\n+import org.apache.iceberg.mr.mapreduce.IcebergSplitContainer;\n+\n+/**\n+ * Generic MR v1 InputFormat API for Iceberg.\n+ *\n+ * @param <T> Java class of records constructed by Iceberg; default is {@link Record}\n+ */\n+public class MapredIcebergInputFormat<T> implements InputFormat<Void, Container<T>> {\n+\n+  private final org.apache.iceberg.mr.mapreduce.IcebergInputFormat<T> innerInputFormat;\n+\n+  public MapredIcebergInputFormat() {\n+    this.innerInputFormat = new org.apache.iceberg.mr.mapreduce.IcebergInputFormat<>();\n+  }\n+\n+  /**\n+   * Configures the {@code JobConf} to use the {@code MapredIcebergInputFormat} and\n+   * returns a helper to add further configuration.\n+   *\n+   * @param job the {@code JobConf} to configure\n+   */\n+  public static InputFormatConfig.ConfigBuilder configure(JobConf job) {\n+    job.setInputFormat(MapredIcebergInputFormat.class);\n+    return new InputFormatConfig.ConfigBuilder(job);\n+  }\n+\n+  @Override\n+  public InputSplit[] getSplits(JobConf job, int numSplits) throws IOException {\n+    return innerInputFormat.getSplits(newJobContext(job))\n+                           .stream()\n+                           .map(InputSplit.class::cast)\n+                           .toArray(InputSplit[]::new);\n+  }\n+\n+  @Override\n+  public RecordReader<Void, Container<T>> getRecordReader(InputSplit split, JobConf job,\n+                                                          Reporter reporter) throws IOException {\n+    IcebergSplit icebergSplit = ((IcebergSplitContainer) split).icebergSplit();\n+    return new MapredIcebergRecordReader<>(innerInputFormat, icebergSplit, job, reporter);\n+  }\n+\n+  private static final class MapredIcebergRecordReader<T> implements RecordReader<Void, Container<T>> {\n+\n+    private final org.apache.hadoop.mapreduce.RecordReader<Void, T> innerReader;\n+    private final long splitLength; // for getPos()\n+\n+    MapredIcebergRecordReader(org.apache.iceberg.mr.mapreduce.IcebergInputFormat<T> mapreduceInputFormat,\n+                              IcebergSplit split, JobConf job, Reporter reporter) throws IOException {\n+      TaskAttemptContext context = newTaskAttemptContext(job, reporter);\n+\n+      try {\n+        innerReader = mapreduceInputFormat.createRecordReader(split, context);\n+        innerReader.initialize(split, context);\n+      } catch (InterruptedException e) {\n+        Thread.currentThread().interrupt();\n+        throw new RuntimeException(e);\n+      }\n+\n+      splitLength = split.getLength();\n+    }\n+\n+    @Override\n+    public boolean next(Void key, Container<T> value) throws IOException {\n+      try {\n+        if (innerReader.nextKeyValue()) {\n+          value.set(innerReader.getCurrentValue());\n+          return true;\n+        }\n+      } catch (InterruptedException ie) {\n+        Thread.currentThread().interrupt();\n+        throw new RuntimeException(ie);\n+      }\n+\n+      return false;\n+    }\n+\n+    @Override\n+    public Void createKey() {\n+      return null;\n+    }\n+\n+    @Override\n+    public Container<T> createValue() {\n+      return new Container<>();\n+    }\n+\n+    @Override\n+    public long getPos() throws IOException {\n+      return (long) (splitLength * getProgress());\n+    }\n+\n+    @Override\n+    public float getProgress() throws IOException {\n+      if (innerReader == null) {\n+        return 0;\n+      }\n+\n+      try {\n+        return innerReader.getProgress();\n+      } catch (InterruptedException e) {\n+        Thread.currentThread().interrupt();\n+        throw new RuntimeException(e);\n+      }\n+    }\n+\n+    @Override\n+    public void close() throws IOException {\n+      if (innerReader != null) {\n+        innerReader.close();\n+      }\n+    }\n+  }\n+\n+  private static JobContext newJobContext(JobConf job) {\n+    JobID jobID = Optional.ofNullable(JobID.forName(job.get(JobContext.ID)))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTI5NDM2Ng=="}, "originalCommit": {"oid": "b373262cf50391eb1b4d44d49bc3dcbc14b1e80a"}, "originalPosition": 153}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgzOTkzNDU3OnYy", "diffSide": "RIGHT", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergSplit.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQxOTo1OTo1M1rOGyNv4g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQxNjo1MjoyNVrOG1rdyw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTMwNzIzNA==", "bodyText": "Not sure if this is a big win, but can we have the Container<T> as an interface which can then be implemented by Mrv1Value and HiveIcebergSplit", "url": "https://github.com/apache/iceberg/pull/1192#discussion_r455307234", "createdAt": "2020-07-15T19:59:53Z", "author": {"login": "rdsr"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergSplit.java", "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.mapred.FileSplit;\n+import org.apache.iceberg.mr.SerializationUtil;\n+import org.apache.iceberg.mr.mapreduce.IcebergSplit;\n+import org.apache.iceberg.mr.mapreduce.IcebergSplitContainer;\n+\n+// Hive requires file formats to return splits that are instances of `FileSplit`.\n+public class HiveIcebergSplit extends FileSplit implements IcebergSplitContainer {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b373262cf50391eb1b4d44d49bc3dcbc14b1e80a"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjA5MzUxMQ==", "bodyText": "Container being implemented as a class allows to return a generic Container<T> in MapredIcebergInputFormat.createValue() and express MapredIcebergInputFormat<T> as a generic InputFormat<Void, Container<T>>.  I used DeprecatedParquetInputFormat as a reference.\nCan we do that with an interface?", "url": "https://github.com/apache/iceberg/pull/1192#discussion_r456093511", "createdAt": "2020-07-16T21:39:51Z", "author": {"login": "guilload"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergSplit.java", "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.mapred.FileSplit;\n+import org.apache.iceberg.mr.SerializationUtil;\n+import org.apache.iceberg.mr.mapreduce.IcebergSplit;\n+import org.apache.iceberg.mr.mapreduce.IcebergSplitContainer;\n+\n+// Hive requires file formats to return splits that are instances of `FileSplit`.\n+public class HiveIcebergSplit extends FileSplit implements IcebergSplitContainer {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTMwNzIzNA=="}, "originalCommit": {"oid": "b373262cf50391eb1b4d44d49bc3dcbc14b1e80a"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODkzOTg1MQ==", "bodyText": "I was thinking Mrv1Value would implement Container and HiveIcebergSplit would also implement Container, but that I guess seems not worthwhile for now.", "url": "https://github.com/apache/iceberg/pull/1192#discussion_r458939851", "createdAt": "2020-07-22T16:52:25Z", "author": {"login": "rdsr"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergSplit.java", "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.mapred.FileSplit;\n+import org.apache.iceberg.mr.SerializationUtil;\n+import org.apache.iceberg.mr.mapreduce.IcebergSplit;\n+import org.apache.iceberg.mr.mapreduce.IcebergSplitContainer;\n+\n+// Hive requires file formats to return splits that are instances of `FileSplit`.\n+public class HiveIcebergSplit extends FileSplit implements IcebergSplitContainer {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTMwNzIzNA=="}, "originalCommit": {"oid": "b373262cf50391eb1b4d44d49bc3dcbc14b1e80a"}, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0NDk2MzYzOnYy", "diffSide": "LEFT", "path": "mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNlQyMTo0Njo1OFrOGy97ew==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNlQyMTo0Njo1OFrOGy97ew==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjA5NjYzNQ==", "bodyText": "Hive keeps a cache of input format instance arounds (see HiveInputFormat) that breaks this logic so I've chosen to remove it for now.\nWe can re-implement this later but the logic will have to be a bit more robust.", "url": "https://github.com/apache/iceberg/pull/1192#discussion_r456096635", "createdAt": "2020-07-16T21:46:58Z", "author": {"login": "guilload"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java", "diffHunk": "@@ -100,11 +94,6 @@\n \n   @Override\n   public List<InputSplit> getSplits(JobContext context) {\n-    if (splits != null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "61859ccf41e42ce6e18a0d281115e04d2bb03273"}, "originalPosition": 38}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2MDgxODE5OnYy", "diffSide": "RIGHT", "path": "mr/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergInputFormat.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQyMzoxMDoxOVrOG1M_gg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQyMzoxMDoxOVrOG1M_gg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQ0MDU3OA==", "bodyText": "This should be Assert.assertEquals(3, descRows.size());", "url": "https://github.com/apache/iceberg/pull/1192#discussion_r458440578", "createdAt": "2020-07-21T23:10:19Z", "author": {"login": "HotSushi"}, "path": "mr/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,169 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import com.klarna.hiverunner.HiveShell;\n+import com.klarna.hiverunner.StandaloneHiveRunner;\n+import com.klarna.hiverunner.annotations.HiveSQL;\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.List;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.mr.TestHelper;\n+import org.apache.iceberg.mr.mapred.IcebergSerDe;\n+import org.apache.iceberg.types.Types;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runner.RunWith;\n+\n+import static org.apache.iceberg.types.Types.NestedField.required;\n+\n+@RunWith(StandaloneHiveRunner.class)\n+public class TestHiveIcebergInputFormat {\n+\n+  @HiveSQL(files = {}, autoStart = true)\n+  private HiveShell shell;\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  private static final Schema CUSTOMER_SCHEMA = new Schema(\n+          required(1, \"customer_id\", Types.LongType.get()),\n+          required(2, \"first_name\", Types.StringType.get())\n+  );\n+\n+  private static final List<Record> CUSTOMER_RECORDS = TestHelper.RecordsBuilder.newInstance(CUSTOMER_SCHEMA)\n+          .add(0L, \"Alice\")\n+          .add(1L, \"Bob\")\n+          .add(2L, \"Trudy\")\n+          .build();\n+\n+  private static final Schema ORDER_SCHEMA = new Schema(\n+          required(1, \"order_id\", Types.LongType.get()),\n+          required(2, \"customer_id\", Types.LongType.get()),\n+          required(3, \"total\", Types.DoubleType.get()));\n+\n+  private static final List<Record> ORDER_RECORDS = TestHelper.RecordsBuilder.newInstance(ORDER_SCHEMA)\n+          .add(100L, 0L, 11.11d)\n+          .add(101L, 0L, 22.22d)\n+          .add(102L, 1L, 33.33d)\n+          .build();\n+\n+  // before variables\n+  private HadoopTables tables;\n+  private Table customerTable;\n+  private Table orderTable;\n+\n+  @Before\n+  public void before() throws IOException {\n+    Configuration conf = new Configuration();\n+    tables = new HadoopTables(conf);\n+\n+    File customerLocation = temp.newFolder(\"customers\");\n+    Assert.assertTrue(customerLocation.delete());\n+\n+    TestHelper customerHelper = new TestHelper(\n+            conf, tables, CUSTOMER_SCHEMA, PartitionSpec.unpartitioned(), FileFormat.PARQUET, temp, customerLocation);\n+\n+    customerTable = customerHelper.createUnpartitionedTable();\n+    customerHelper.appendToTable(customerHelper.writeFile(null, CUSTOMER_RECORDS));\n+\n+    File orderLocation = temp.newFolder(\"orders\");\n+    Assert.assertTrue(orderLocation.delete());\n+\n+    TestHelper orderHelper = new TestHelper(\n+            conf, tables, ORDER_SCHEMA, PartitionSpec.unpartitioned(), FileFormat.PARQUET, temp, orderLocation);\n+\n+    orderTable = orderHelper.createUnpartitionedTable();\n+    orderHelper.appendToTable(orderHelper.writeFile(null, ORDER_RECORDS));\n+  }\n+\n+  @Test\n+  public void testScanEmptyTable() throws IOException {\n+    File emptyLocation = temp.newFolder(\"empty\");\n+    Assert.assertTrue(emptyLocation.delete());\n+\n+    Schema emptySchema = new Schema(required(1, \"empty\", Types.StringType.get()));\n+    Table emptyTable = tables.create(\n+            emptySchema, PartitionSpec.unpartitioned(), Collections.emptyMap(), emptyLocation.toString());\n+    createHiveTable(\"empty\", emptyTable.location());\n+\n+    List<Object[]> rows = shell.executeStatement(\"SELECT * FROM default.empty\");\n+    Assert.assertEquals(0, rows.size());\n+  }\n+\n+  @Test\n+  public void testScanTable() {\n+    createHiveTable(\"customers\", customerTable.location());\n+\n+    // Single fetch task: no MR job.\n+    List<Object[]> rows = shell.executeStatement(\"SELECT * FROM default.customers\");\n+\n+    Assert.assertEquals(3, rows.size());\n+    Assert.assertArrayEquals(new Object[] {0L, \"Alice\"}, rows.get(0));\n+    Assert.assertArrayEquals(new Object[] {1L, \"Bob\"}, rows.get(1));\n+    Assert.assertArrayEquals(new Object[] {2L, \"Trudy\"}, rows.get(2));\n+\n+    // Adding the ORDER BY clause will cause Hive to spawn a local MR job this time.\n+    List<Object[]> descRows = shell.executeStatement(\"SELECT * FROM default.customers ORDER BY customer_id DESC\");\n+\n+    Assert.assertEquals(3, rows.size());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f535af0e18b322082e83e33dc31a44e13719ef83"}, "originalPosition": 137}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3760, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}