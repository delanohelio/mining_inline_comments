{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTI4MTg5ODMw", "number": 1837, "reviewThreads": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNlQxNzowMjowMFrOE9tfNw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQyMTo0NzoyNFrOE-2sUA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMzMTQzODYzOnYy", "diffSide": "RIGHT", "path": "site/docs/hive.md", "isResolved": false, "comments": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNlQxNzowMjowMFrOH6i3QQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yN1QxNDo1ODoxNVrOH7BMvw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTE1MDY1Nw==", "bodyText": "@pvary So for the above, if I create the Hive table but don't set iceberg.mr.catalog=hadoop or then I can actually query it successfully. However if I do set these (which the code at https://github.com/apache/iceberg/blob/master/mr/src/test/java/org/apache/iceberg/mr/hive/TestTables.java#L133 suggests one should) then I can't query the table. Instead I get this stack trace:\n2020-11-26T16:46:36,531 INFO  [f94f328b-fe24-4cbc-8723-423fb6ffea6e main([])]: mr.Catalogs (Catalogs.java:loadCatalog(212)) - Loaded Hadoop catalog HadoopCatalog{name=hadoop, location=/tmp/iceberg-test-hadoop-tables}\n2020-11-26T16:46:36,533 ERROR [f94f328b-fe24-4cbc-8723-423fb6ffea6e main([])]: hive.log (MetaStoreUtils.java:getDeserializer(456)) - error in initSerDe: org.apache.hadoop.hive.serde2.SerDeException Please provide an existing table or a valid schema\norg.apache.hadoop.hive.serde2.SerDeException: Please provide an existing table or a valid schema\n        at org.apache.iceberg.mr.hive.HiveIcebergSerDe.initialize(HiveIcebergSerDe.java:62) ~[?:?]\n        at org.apache.hadoop.hive.serde2.AbstractSerDe.initialize(AbstractSerDe.java:54) ~[hive-exec-2.3.6-amzn-2.jar:2.3.6-amzn-2]\n        at org.apache.hadoop.hive.serde2.SerDeUtils.initializeSerDe(SerDeUtils.java:533) ~[hive-exec-2.3.6-amzn-2.jar:2.3.6-amzn-2]\n        at org.apache.hadoop.hive.metastore.MetaStoreUtils.getDeserializer(MetaStoreUtils.java:450) ~[hive-exec-2.3.6-amzn-2.jar:2.3.6-amzn-2]\n        at org.apache.hadoop.hive.metastore.MetaStoreUtils.getDeserializer(MetaStoreUtils.java:437) ~[hive-exec-2.3.6-amzn-2.jar:2.3.6-amzn-2]\n        at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:281) ~[hive-exec-2.3.6-amzn-2.jar:2.3.6-amzn-2]\n        at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:263) ~[hive-exec-2.3.6-amzn-2.jar:2.3.6-amzn-2]\n        at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:838) ~[hive-exec-2.3.6-amzn-2.jar:2.3.6-amzn-2]\n        at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:872) ~[hive-exec-2.3.6-amzn-2.jar:2.3.6-amzn-2]\n        at org.apache.hadoop.hive.ql.exec.DDLTask.createTable(DDLTask.java:4356) ~[hive-exec-2.3.6-amzn-2.jar:2.3.6-amzn-2]\n        at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:354) ~[hive-exec-2.3.6-amzn-2.jar:2.3.6-amzn-2]\n        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:199) ~[hive-exec-2.3.6-amzn-2.jar:2.3.6-amzn-2]\n        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100) ~[hive-exec-2.3.6-amzn-2.jar:2.3.6-amzn-2]\n        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2183) ~[hive-exec-2.3.6-amzn-2.jar:2.3.6-amzn-2]\n        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1839) ~[hive-exec-2.3.6-amzn-2.jar:2.3.6-amzn-2]\n        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1526) ~[hive-exec-2.3.6-amzn-2.jar:2.3.6-amzn-2]\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237) ~[hive-exec-2.3.6-amzn-2.jar:2.3.6-amzn-2]\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1227) ~[hive-exec-2.3.6-amzn-2.jar:2.3.6-amzn-2]\n        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:233) ~[hive-cli-2.3.6-amzn-2.jar:2.3.6-amzn-2]\n        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:184) ~[hive-cli-2.3.6-amzn-2.jar:2.3.6-amzn-2]\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:403) ~[hive-cli-2.3.6-amzn-2.jar:2.3.6-amzn-2]\n        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:821) ~[hive-cli-2.3.6-amzn-2.jar:2.3.6-amzn-2]\n        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:759) ~[hive-cli-2.3.6-amzn-2.jar:2.3.6-amzn-2]\n        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:686) ~[hive-cli-2.3.6-amzn-2.jar:2.3.6-amzn-2]\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_252]\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_252]\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_252]\n        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_252]\n        at org.apache.hadoop.util.RunJar.run(RunJar.java:239) ~[hadoop-common-2.8.5-amzn-6.jar:?]\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:153) ~[hadoop-common-2.8.5-amzn-6.jar:?]\nCaused by: org.apache.iceberg.exceptions.NoSuchTableException: Table does not exist: test.iceberg_table_from_hadoop_catalog\n        at org.apache.iceberg.BaseMetastoreCatalog.loadTable(BaseMetastoreCatalog.java:108) ~[?:?]\n        at org.apache.iceberg.mr.Catalogs.loadTable(Catalogs.java:100) ~[?:?]\n        at org.apache.iceberg.mr.Catalogs.loadTable(Catalogs.java:92) ~[?:?]\n        at org.apache.iceberg.mr.hive.HiveIcebergSerDe.initialize(HiveIcebergSerDe.java:60) ~[?:?]\n        ... 29 more\n2020-11-26T16:46:36,533 ERROR [f94f328b-fe24-4cbc-8723-423fb6ffea6e main([])]: exec.DDLTask (DDLTask.java:failed(639)) - org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: MetaException(message:org.apache.hadoop.hive.serde2.SerDeException Please provide an existing table or a valid schema)\n        at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:867)\n        at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:872)\n        at org.apache.hadoop.hive.ql.exec.DDLTask.createTable(DDLTask.java:4356)\n        at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:354)\n        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:199)\n        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)\n        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2183)\n        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1839)\n        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1526)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1227)\n        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:233)\n        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:184)\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:403)\n        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:821)\n        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:759)\n        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:686)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:498)\n        at org.apache.hadoop.util.RunJar.run(RunJar.java:239)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:153)\nCaused by: java.lang.RuntimeException: MetaException(message:org.apache.hadoop.hive.serde2.SerDeException Please provide an existing table or a valid schema)\n        at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:283)\n        at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:263)\n        at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:838)\n        ... 22 more\nCaused by: MetaException(message:org.apache.hadoop.hive.serde2.SerDeException Please provide an existing table or a valid schema)\n        at org.apache.hadoop.hive.metastore.MetaStoreUtils.getDeserializer(MetaStoreUtils.java:458)\n        at org.apache.hadoop.hive.metastore.MetaStoreUtils.getDeserializer(MetaStoreUtils.java:437)\n        at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:281)\n        ... 24 more\n\nAny ideas what's going on? I've got a feeling I'm missing something obvious, it's been a long day ;) I'll try debug it tomorrow, just thought you might have some ideas :)", "url": "https://github.com/apache/iceberg/pull/1837#discussion_r531150657", "createdAt": "2020-11-26T17:02:00Z", "author": {"login": "massdosage"}, "path": "site/docs/hive.md", "diffHunk": "@@ -84,7 +84,32 @@ You should now be able to issue Hive SQL `SELECT` queries using the above table\n SELECT * from table_b;\n ```\n \n+#### Using Hadoop Catalog\n+Iceberg tables created using `HadoopCatalog` are stored entirely in a directory in a filesytem like HDFS. \n+\n+##### Create an Iceberg table\n+The first step is to create an Iceberg table using the Spark/Java/Python API and `HadoopCatalog`. For the purposes of this documentation we will assume that the table is called `database_a.table_c` and that the table location is `hdfs://some_path/database_a/table_c`.\n+\n+##### Create a Hive table\n+Now overlay a Hive table on top of this Iceberg table by issuing Hive DDL like so:\n+```sql\n+CREATE EXTERNAL TABLE table_a \n+STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' \n+LOCATION 'hdfs://some_bucket/some_path/database_a/table_c';\n+```\n+\n+#### Query the Iceberg table via Hive\n+TODO: why does below work if no config settings are set in Hive but fails if we add `set iceberg.mr.catalog=hadoop` like the code suggests we need to do?", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "21231dac75581eaccb4e338c9a3a814d19f2cce4"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTE2OTYzNQ==", "bodyText": "Shouldn't the catalog to use also be specified when creating the table? It seems odd that the consumer has to be aware of the catalog used to store the table. Also what if we need to read multiple tables stored in different underlying catalogs?", "url": "https://github.com/apache/iceberg/pull/1837#discussion_r531169635", "createdAt": "2020-11-26T17:49:33Z", "author": {"login": "shardulm94"}, "path": "site/docs/hive.md", "diffHunk": "@@ -84,7 +84,32 @@ You should now be able to issue Hive SQL `SELECT` queries using the above table\n SELECT * from table_b;\n ```\n \n+#### Using Hadoop Catalog\n+Iceberg tables created using `HadoopCatalog` are stored entirely in a directory in a filesytem like HDFS. \n+\n+##### Create an Iceberg table\n+The first step is to create an Iceberg table using the Spark/Java/Python API and `HadoopCatalog`. For the purposes of this documentation we will assume that the table is called `database_a.table_c` and that the table location is `hdfs://some_path/database_a/table_c`.\n+\n+##### Create a Hive table\n+Now overlay a Hive table on top of this Iceberg table by issuing Hive DDL like so:\n+```sql\n+CREATE EXTERNAL TABLE table_a \n+STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' \n+LOCATION 'hdfs://some_bucket/some_path/database_a/table_c';\n+```\n+\n+#### Query the Iceberg table via Hive\n+TODO: why does below work if no config settings are set in Hive but fails if we add `set iceberg.mr.catalog=hadoop` like the code suggests we need to do?", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTE1MDY1Nw=="}, "originalCommit": {"oid": "21231dac75581eaccb4e338c9a3a814d19f2cce4"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTE5MTc3MQ==", "bodyText": "Currently we expect that the catalog is set in the global config (for example: hive-site.xml). Every Iceberg table will use that Catalog. I do not see the exact commands Adrian has executed, but I think Shardul is right and the global config does not contain the catalog settings, and the required values were set between the table creation and the query. Adrian, could you please try:\nset catalog;\ncreate table;\n-- insert data from spark\nselect from table\n\nAs for Shardul's question: @lcspinter is started a conversation on the dev list about enabling table level catalog configuration.\nThanks, Peter", "url": "https://github.com/apache/iceberg/pull/1837#discussion_r531191771", "createdAt": "2020-11-26T19:00:53Z", "author": {"login": "pvary"}, "path": "site/docs/hive.md", "diffHunk": "@@ -84,7 +84,32 @@ You should now be able to issue Hive SQL `SELECT` queries using the above table\n SELECT * from table_b;\n ```\n \n+#### Using Hadoop Catalog\n+Iceberg tables created using `HadoopCatalog` are stored entirely in a directory in a filesytem like HDFS. \n+\n+##### Create an Iceberg table\n+The first step is to create an Iceberg table using the Spark/Java/Python API and `HadoopCatalog`. For the purposes of this documentation we will assume that the table is called `database_a.table_c` and that the table location is `hdfs://some_path/database_a/table_c`.\n+\n+##### Create a Hive table\n+Now overlay a Hive table on top of this Iceberg table by issuing Hive DDL like so:\n+```sql\n+CREATE EXTERNAL TABLE table_a \n+STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' \n+LOCATION 'hdfs://some_bucket/some_path/database_a/table_c';\n+```\n+\n+#### Query the Iceberg table via Hive\n+TODO: why does below work if no config settings are set in Hive but fails if we add `set iceberg.mr.catalog=hadoop` like the code suggests we need to do?", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTE1MDY1Nw=="}, "originalCommit": {"oid": "21231dac75581eaccb4e338c9a3a814d19f2cce4"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTYyNDQxNw==", "bodyText": "Thanks @pvary, so what I am doing this is...\nFirst I create the table using the Spark API something along the lines of this:\nHadoopCatalog catalog = new HadoopCatalog(conf, warehousePath);\nTableIdentifier tableId = TableIdentifier.of(\"hadoop_catalog_db\", \"iceberg_table_from_hadoop_catalog\");\ncatalog.createTable(tableId, schema, spec);\n...\nDataset<Row> ds = spark.createDataFrame(hotels, Hotel.class);\nds.write().format(\"iceberg\").mode(\"append\").save(table.location());\n\nThis then creates the Iceberg table at hdfs://host/tmp/iceberg-test-hadoop-tables/hadoop_catalog_db/iceberg_table_from_hadoop_catalog/ - I can see the data and metadata folder on HDFS. So far so good. Then I go to a Hive shell, add the iceberg-hive-runtime.jar to the path and then try create a Hive table on top of the above like so:\nCREATE EXTERNAL TABLE test.iceberg_table_from_hadoop_catalog STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' LOCATION 'hdfs://host/tmp/iceberg-test-hadoop-tables/hadoop_catalog_db/iceberg_table_from_hadoop_catalog/';\n\nThis looks like its created OK, I check by doing:\nshow create table test.iceberg_table_from_hadoop_catalog;\nOK\nCREATE EXTERNAL TABLE `test.iceberg_table_from_hadoop_catalog`(\n  `id` bigint COMMENT 'from deserializer', \n  `name` string COMMENT 'from deserializer')\nROW FORMAT SERDE \n  'org.apache.iceberg.mr.hive.HiveIcebergSerDe' \nSTORED BY \n  'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' \nWITH SERDEPROPERTIES ( \n  'serialization.format'='1')\nLOCATION\n  'hdfs://host/tmp/iceberg-test-hadoop-tables/hadoop_catalog_db/iceberg_table_from_hadoop_catalog'\nTBLPROPERTIES (\n  'table_type'='ICEBERG', \n  'transient_lastDdlTime'='1606485921')\n\nAt this point I can successfully perform SELECT statements on the table without setting any conf properties, which surprised me, it seems that this is going via the \"Hadoop Tables\" path since there is a location on the table and it actually works! Then I tried doing the equivalent of what I did in order to read Hive Catalog tables which is to set the value for iceberg.mr.catalog:\nset iceberg.mr.catalog=hadoop;\n\nI would expect this to now activate the \"Hadoop Catalog\" path and the SELECT statement to succeed but instead it fails with the error above. Can you think of anything I'm doing wrong?", "url": "https://github.com/apache/iceberg/pull/1837#discussion_r531624417", "createdAt": "2020-11-27T14:12:14Z", "author": {"login": "massdosage"}, "path": "site/docs/hive.md", "diffHunk": "@@ -84,7 +84,32 @@ You should now be able to issue Hive SQL `SELECT` queries using the above table\n SELECT * from table_b;\n ```\n \n+#### Using Hadoop Catalog\n+Iceberg tables created using `HadoopCatalog` are stored entirely in a directory in a filesytem like HDFS. \n+\n+##### Create an Iceberg table\n+The first step is to create an Iceberg table using the Spark/Java/Python API and `HadoopCatalog`. For the purposes of this documentation we will assume that the table is called `database_a.table_c` and that the table location is `hdfs://some_path/database_a/table_c`.\n+\n+##### Create a Hive table\n+Now overlay a Hive table on top of this Iceberg table by issuing Hive DDL like so:\n+```sql\n+CREATE EXTERNAL TABLE table_a \n+STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' \n+LOCATION 'hdfs://some_bucket/some_path/database_a/table_c';\n+```\n+\n+#### Query the Iceberg table via Hive\n+TODO: why does below work if no config settings are set in Hive but fails if we add `set iceberg.mr.catalog=hadoop` like the code suggests we need to do?", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTE1MDY1Nw=="}, "originalCommit": {"oid": "21231dac75581eaccb4e338c9a3a814d19f2cce4"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTYyNTk4NA==", "bodyText": "@shardulm94 I agree with you, at the moment I'm just documenting how it works currently but the idea is for us all to take a step back from the implementation details and look at it through the lens of an end user and make changes. In our earlier implementation we did store the catalog type as a table property so it didn't have to be specified by the end user, I'm not sure how this got lost along the way. I'll test shortly whether this still works or not. I also think we should rename the property to not have mr in it. As @pvary says there is some discussion on the mailing list about this and that should lead to us agreeing to make some changes to how this implemented and then updating these docs accordingly.", "url": "https://github.com/apache/iceberg/pull/1837#discussion_r531625984", "createdAt": "2020-11-27T14:15:22Z", "author": {"login": "massdosage"}, "path": "site/docs/hive.md", "diffHunk": "@@ -84,7 +84,32 @@ You should now be able to issue Hive SQL `SELECT` queries using the above table\n SELECT * from table_b;\n ```\n \n+#### Using Hadoop Catalog\n+Iceberg tables created using `HadoopCatalog` are stored entirely in a directory in a filesytem like HDFS. \n+\n+##### Create an Iceberg table\n+The first step is to create an Iceberg table using the Spark/Java/Python API and `HadoopCatalog`. For the purposes of this documentation we will assume that the table is called `database_a.table_c` and that the table location is `hdfs://some_path/database_a/table_c`.\n+\n+##### Create a Hive table\n+Now overlay a Hive table on top of this Iceberg table by issuing Hive DDL like so:\n+```sql\n+CREATE EXTERNAL TABLE table_a \n+STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' \n+LOCATION 'hdfs://some_bucket/some_path/database_a/table_c';\n+```\n+\n+#### Query the Iceberg table via Hive\n+TODO: why does below work if no config settings are set in Hive but fails if we add `set iceberg.mr.catalog=hadoop` like the code suggests we need to do?", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTE1MDY1Nw=="}, "originalCommit": {"oid": "21231dac75581eaccb4e338c9a3a814d19f2cce4"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTYzMzIzMQ==", "bodyText": "@massdosage: Would it be a problem with the warehouse path for the HadoopCatalog: \"location=/tmp/iceberg-test-hadoop-tables\"? My guess is that this should be \"hdfs://host/tmp/iceberg-test-hadoop-tables\", and so the HadoopCatalog is not able to read the table data.", "url": "https://github.com/apache/iceberg/pull/1837#discussion_r531633231", "createdAt": "2020-11-27T14:29:39Z", "author": {"login": "pvary"}, "path": "site/docs/hive.md", "diffHunk": "@@ -84,7 +84,32 @@ You should now be able to issue Hive SQL `SELECT` queries using the above table\n SELECT * from table_b;\n ```\n \n+#### Using Hadoop Catalog\n+Iceberg tables created using `HadoopCatalog` are stored entirely in a directory in a filesytem like HDFS. \n+\n+##### Create an Iceberg table\n+The first step is to create an Iceberg table using the Spark/Java/Python API and `HadoopCatalog`. For the purposes of this documentation we will assume that the table is called `database_a.table_c` and that the table location is `hdfs://some_path/database_a/table_c`.\n+\n+##### Create a Hive table\n+Now overlay a Hive table on top of this Iceberg table by issuing Hive DDL like so:\n+```sql\n+CREATE EXTERNAL TABLE table_a \n+STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' \n+LOCATION 'hdfs://some_bucket/some_path/database_a/table_c';\n+```\n+\n+#### Query the Iceberg table via Hive\n+TODO: why does below work if no config settings are set in Hive but fails if we add `set iceberg.mr.catalog=hadoop` like the code suggests we need to do?", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTE1MDY1Nw=="}, "originalCommit": {"oid": "21231dac75581eaccb4e338c9a3a814d19f2cce4"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTYzNDI5OQ==", "bodyText": "OK, I think we can put the iceberg.mr.catalog as a table property and this will be used (so it doesn't have to be done at the Hive conf level). I tried this on the HadoopCatalog table above and get the same error when trying to SELECT as when I set the environment variable (which doesn't solve my problem but at least is consistent :) ). @pvary I see the tables automatically created by HiveCatalog don't have that as a table property but perhaps they should if the \"hive execution enabled\" flag is set to true?", "url": "https://github.com/apache/iceberg/pull/1837#discussion_r531634299", "createdAt": "2020-11-27T14:31:49Z", "author": {"login": "massdosage"}, "path": "site/docs/hive.md", "diffHunk": "@@ -84,7 +84,32 @@ You should now be able to issue Hive SQL `SELECT` queries using the above table\n SELECT * from table_b;\n ```\n \n+#### Using Hadoop Catalog\n+Iceberg tables created using `HadoopCatalog` are stored entirely in a directory in a filesytem like HDFS. \n+\n+##### Create an Iceberg table\n+The first step is to create an Iceberg table using the Spark/Java/Python API and `HadoopCatalog`. For the purposes of this documentation we will assume that the table is called `database_a.table_c` and that the table location is `hdfs://some_path/database_a/table_c`.\n+\n+##### Create a Hive table\n+Now overlay a Hive table on top of this Iceberg table by issuing Hive DDL like so:\n+```sql\n+CREATE EXTERNAL TABLE table_a \n+STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' \n+LOCATION 'hdfs://some_bucket/some_path/database_a/table_c';\n+```\n+\n+#### Query the Iceberg table via Hive\n+TODO: why does below work if no config settings are set in Hive but fails if we add `set iceberg.mr.catalog=hadoop` like the code suggests we need to do?", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTE1MDY1Nw=="}, "originalCommit": {"oid": "21231dac75581eaccb4e338c9a3a814d19f2cce4"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTY0NzY3OQ==", "bodyText": "@massdosage: Would it be a problem with the warehouse path for the HadoopCatalog: \"location=/tmp/iceberg-test-hadoop-tables\"? My guess is that this should be \"hdfs://host/tmp/iceberg-test-hadoop-tables\", and so the HadoopCatalog is not able to read the table data.\n\nOK, this jogged me into realising that I also need to set \"iceberg.mr.catalog.hadoop.warehouse.location\". If I do that then it works! Interestingly enough it works with the table location set like you suggested and also with the full path like I had. Anyway, I'll update the document accordingly to show at least how it can be done currently. Thanks!", "url": "https://github.com/apache/iceberg/pull/1837#discussion_r531647679", "createdAt": "2020-11-27T14:58:15Z", "author": {"login": "massdosage"}, "path": "site/docs/hive.md", "diffHunk": "@@ -84,7 +84,32 @@ You should now be able to issue Hive SQL `SELECT` queries using the above table\n SELECT * from table_b;\n ```\n \n+#### Using Hadoop Catalog\n+Iceberg tables created using `HadoopCatalog` are stored entirely in a directory in a filesytem like HDFS. \n+\n+##### Create an Iceberg table\n+The first step is to create an Iceberg table using the Spark/Java/Python API and `HadoopCatalog`. For the purposes of this documentation we will assume that the table is called `database_a.table_c` and that the table location is `hdfs://some_path/database_a/table_c`.\n+\n+##### Create a Hive table\n+Now overlay a Hive table on top of this Iceberg table by issuing Hive DDL like so:\n+```sql\n+CREATE EXTERNAL TABLE table_a \n+STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' \n+LOCATION 'hdfs://some_bucket/some_path/database_a/table_c';\n+```\n+\n+#### Query the Iceberg table via Hive\n+TODO: why does below work if no config settings are set in Hive but fails if we add `set iceberg.mr.catalog=hadoop` like the code suggests we need to do?", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTE1MDY1Nw=="}, "originalCommit": {"oid": "21231dac75581eaccb4e338c9a3a814d19f2cce4"}, "originalPosition": 19}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMzMTQ0MTU1OnYy", "diffSide": "RIGHT", "path": "site/docs/hive.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNlQxNzowMjo1NlrOH6i42w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNlQxNzowMjo1NlrOH6i42w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTE1MTA2Nw==", "bodyText": "This isn't related to the HadoopCatalog but since #1417 was just merged I thought I might as well just put it here.", "url": "https://github.com/apache/iceberg/pull/1837#discussion_r531151067", "createdAt": "2020-11-26T17:02:56Z", "author": {"login": "massdosage"}, "path": "site/docs/hive.md", "diffHunk": "@@ -84,7 +84,32 @@ You should now be able to issue Hive SQL `SELECT` queries using the above table\n SELECT * from table_b;\n ```\n \n+#### Using Hadoop Catalog\n+Iceberg tables created using `HadoopCatalog` are stored entirely in a directory in a filesytem like HDFS. \n+\n+##### Create an Iceberg table\n+The first step is to create an Iceberg table using the Spark/Java/Python API and `HadoopCatalog`. For the purposes of this documentation we will assume that the table is called `database_a.table_c` and that the table location is `hdfs://some_path/database_a/table_c`.\n+\n+##### Create a Hive table\n+Now overlay a Hive table on top of this Iceberg table by issuing Hive DDL like so:\n+```sql\n+CREATE EXTERNAL TABLE table_a \n+STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' \n+LOCATION 'hdfs://some_bucket/some_path/database_a/table_c';\n+```\n+\n+#### Query the Iceberg table via Hive\n+TODO: why does below work if no config settings are set in Hive but fails if we add `set iceberg.mr.catalog=hadoop` like the code suggests we need to do?\n+\n+You should now be able to issue Hive SQL `SELECT` queries using the above table and see the results returned from the underlying Iceberg table. Both the Map Reduce and Tez query execution engines are supported.\n+```sql\n+SELECT * from table_c;\n+```\n+\n ### Features\n \n #### Predicate pushdown\n Pushdown of the Hive SQL `WHERE` clause has been implemented so that these filters are used at the Iceberg TableScan level as well as by the Parquet and ORC Readers.\n+\n+#### Column Projection", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "21231dac75581eaccb4e338c9a3a814d19f2cce4"}, "originalPosition": 31}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0MTU3ODU5OnYy", "diffSide": "LEFT", "path": "site/docs/hive.md", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxNDoyNjo1OFrOH79fJQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxNDozNjo1MFrOH796xw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjYzNTQyOQ==", "bodyText": "Why did we remove that both MR and Tez is supported?", "url": "https://github.com/apache/iceberg/pull/1837#discussion_r532635429", "createdAt": "2020-11-30T14:26:58Z", "author": {"login": "pvary"}, "path": "site/docs/hive.md", "diffHunk": "@@ -79,12 +79,45 @@ In order to query a Hive table created by either of the HiveCatalog methods desc\n ```sql\n SET iceberg.mr.catalog=hive;\n ```\n-You should now be able to issue Hive SQL `SELECT` queries using the above table and see the results returned from the underlying Iceberg table. Both the Map Reduce and Tez query execution engines are supported.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1542d2662ea12c942d021294aaf2269d37612f0c"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjYzNjUxNg==", "bodyText": "Why did we remove that both MR and Tez is supported?\n\nI moved it to the bottom under \"Features\" so I didn't have to keep repeating it :)", "url": "https://github.com/apache/iceberg/pull/1837#discussion_r532636516", "createdAt": "2020-11-30T14:28:31Z", "author": {"login": "massdosage"}, "path": "site/docs/hive.md", "diffHunk": "@@ -79,12 +79,45 @@ In order to query a Hive table created by either of the HiveCatalog methods desc\n ```sql\n SET iceberg.mr.catalog=hive;\n ```\n-You should now be able to issue Hive SQL `SELECT` queries using the above table and see the results returned from the underlying Iceberg table. Both the Map Reduce and Tez query execution engines are supported.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjYzNTQyOQ=="}, "originalCommit": {"oid": "1542d2662ea12c942d021294aaf2269d37612f0c"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjYzNjk4MQ==", "bodyText": "It has been moved down here: https://github.com/apache/iceberg/pull/1837/files#diff-0270f04c6c1a4be5da895415fff2797103da7ded6ec97c303f2f7e218e99ac26R122 to avoid duplication :)", "url": "https://github.com/apache/iceberg/pull/1837#discussion_r532636981", "createdAt": "2020-11-30T14:29:13Z", "author": {"login": "marton-bod"}, "path": "site/docs/hive.md", "diffHunk": "@@ -79,12 +79,45 @@ In order to query a Hive table created by either of the HiveCatalog methods desc\n ```sql\n SET iceberg.mr.catalog=hive;\n ```\n-You should now be able to issue Hive SQL `SELECT` queries using the above table and see the results returned from the underlying Iceberg table. Both the Map Reduce and Tez query execution engines are supported.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjYzNTQyOQ=="}, "originalCommit": {"oid": "1542d2662ea12c942d021294aaf2269d37612f0c"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjY0MjUwMw==", "bodyText": "Ok guys, just do not pile on me please \ud83d\ude04\nI am reading stuff sequentially \ud83d\ude04", "url": "https://github.com/apache/iceberg/pull/1837#discussion_r532642503", "createdAt": "2020-11-30T14:36:50Z", "author": {"login": "pvary"}, "path": "site/docs/hive.md", "diffHunk": "@@ -79,12 +79,45 @@ In order to query a Hive table created by either of the HiveCatalog methods desc\n ```sql\n SET iceberg.mr.catalog=hive;\n ```\n-You should now be able to issue Hive SQL `SELECT` queries using the above table and see the results returned from the underlying Iceberg table. Both the Map Reduce and Tez query execution engines are supported.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjYzNTQyOQ=="}, "originalCommit": {"oid": "1542d2662ea12c942d021294aaf2269d37612f0c"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0MTU4NDQ2OnYy", "diffSide": "RIGHT", "path": "site/docs/hive.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxNDoyODoxOFrOH79izg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxNDoyODoxOFrOH79izg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjYzNjM2Ng==", "bodyText": "It would be good to remove this restriction later", "url": "https://github.com/apache/iceberg/pull/1837#discussion_r532636366", "createdAt": "2020-11-30T14:28:18Z", "author": {"login": "pvary"}, "path": "site/docs/hive.md", "diffHunk": "@@ -79,12 +79,45 @@ In order to query a Hive table created by either of the HiveCatalog methods desc\n ```sql\n SET iceberg.mr.catalog=hive;\n ```\n-You should now be able to issue Hive SQL `SELECT` queries using the above table and see the results returned from the underlying Iceberg table. Both the Map Reduce and Tez query execution engines are supported.\n+You should now be able to issue Hive SQL `SELECT` queries using the above table and see the results returned from the underlying Iceberg table.\n ```sql\n SELECT * from table_b;\n ```\n \n+#### Using Hadoop Catalog\n+Iceberg tables created using `HadoopCatalog` are stored entirely in a directory in a filesytem like HDFS. \n+\n+##### Create an Iceberg table\n+The first step is to create an Iceberg table using the Spark/Java/Python API and `HadoopCatalog`. For the purposes of this documentation we will assume that the fully qualified table identifier is `database_a.table_c` and that the Hadoop Catalog warehouse location is `hdfs://some_bucket/path_to_hadoop_warehouse`. Iceberg will therefore create the table at the location `hdfs://some_bucket/path_to_hadoop_warehouse/database_a/table_c`.\n+\n+##### Create a Hive table\n+Now overlay a Hive table on top of this Iceberg table by issuing Hive DDL like so:\n+```sql\n+CREATE EXTERNAL TABLE database_a.table_c \n+STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' \n+LOCATION 'hdfs://some_bucket/path_to_hadoop_warehouse/database_a/table_c'\n+TBLPROPERTIES (\n+  'iceberg.mr.catalog'='hadoop', \n+  'iceberg.mr.catalog.hadoop.warehouse.location'='hdfs://some_bucket/path_to_hadoop_warehouse')\n+;\n+```\n+Note that the Hive database and table name *must* match the values used in the Iceberg `TableIdentifier` when the table was created. ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1542d2662ea12c942d021294aaf2269d37612f0c"}, "originalPosition": 36}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0MTU5MzgwOnYy", "diffSide": "RIGHT", "path": "site/docs/hive.md", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxNDozMDoyM1rOH79omw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxNjo1MTozOFrOH82S2Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjYzNzg1MQ==", "bodyText": "After the latest changes now it is possible to use Hive CREATE TABLE command to create the Hive table and the Iceberg table at the same time. Do we want to document it here, or do we want to do it in a different PR?", "url": "https://github.com/apache/iceberg/pull/1837#discussion_r532637851", "createdAt": "2020-11-30T14:30:23Z", "author": {"login": "pvary"}, "path": "site/docs/hive.md", "diffHunk": "@@ -79,12 +79,45 @@ In order to query a Hive table created by either of the HiveCatalog methods desc\n ```sql\n SET iceberg.mr.catalog=hive;\n ```\n-You should now be able to issue Hive SQL `SELECT` queries using the above table and see the results returned from the underlying Iceberg table. Both the Map Reduce and Tez query execution engines are supported.\n+You should now be able to issue Hive SQL `SELECT` queries using the above table and see the results returned from the underlying Iceberg table.\n ```sql\n SELECT * from table_b;\n ```\n \n+#### Using Hadoop Catalog\n+Iceberg tables created using `HadoopCatalog` are stored entirely in a directory in a filesytem like HDFS. \n+\n+##### Create an Iceberg table\n+The first step is to create an Iceberg table using the Spark/Java/Python API and `HadoopCatalog`. For the purposes of this documentation we will assume that the fully qualified table identifier is `database_a.table_c` and that the Hadoop Catalog warehouse location is `hdfs://some_bucket/path_to_hadoop_warehouse`. Iceberg will therefore create the table at the location `hdfs://some_bucket/path_to_hadoop_warehouse/database_a/table_c`.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1542d2662ea12c942d021294aaf2269d37612f0c"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjkyNTY4NQ==", "bodyText": "I think we should add this, but in a separate PR because this one is ready to merge.", "url": "https://github.com/apache/iceberg/pull/1837#discussion_r532925685", "createdAt": "2020-11-30T21:49:23Z", "author": {"login": "rdblue"}, "path": "site/docs/hive.md", "diffHunk": "@@ -79,12 +79,45 @@ In order to query a Hive table created by either of the HiveCatalog methods desc\n ```sql\n SET iceberg.mr.catalog=hive;\n ```\n-You should now be able to issue Hive SQL `SELECT` queries using the above table and see the results returned from the underlying Iceberg table. Both the Map Reduce and Tez query execution engines are supported.\n+You should now be able to issue Hive SQL `SELECT` queries using the above table and see the results returned from the underlying Iceberg table.\n ```sql\n SELECT * from table_b;\n ```\n \n+#### Using Hadoop Catalog\n+Iceberg tables created using `HadoopCatalog` are stored entirely in a directory in a filesytem like HDFS. \n+\n+##### Create an Iceberg table\n+The first step is to create an Iceberg table using the Spark/Java/Python API and `HadoopCatalog`. For the purposes of this documentation we will assume that the fully qualified table identifier is `database_a.table_c` and that the Hadoop Catalog warehouse location is `hdfs://some_bucket/path_to_hadoop_warehouse`. Iceberg will therefore create the table at the location `hdfs://some_bucket/path_to_hadoop_warehouse/database_a/table_c`.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjYzNzg1MQ=="}, "originalCommit": {"oid": "1542d2662ea12c942d021294aaf2269d37612f0c"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzI5Mjc3Nw==", "bodyText": "@pvary - sounds good. What I've done for everything I've documented here is first I've added code to an integration test suite I have that uses the Spark and Hive runtimes on a distributed cluster to really try test everything as an end user would. Can you give me the high level steps needed for what you're talking about above and I'll try reproduce it and can then raise a separate PR to document it? I assume this is related to the first parts of the output format that were merged?", "url": "https://github.com/apache/iceberg/pull/1837#discussion_r533292777", "createdAt": "2020-12-01T10:29:11Z", "author": {"login": "massdosage"}, "path": "site/docs/hive.md", "diffHunk": "@@ -79,12 +79,45 @@ In order to query a Hive table created by either of the HiveCatalog methods desc\n ```sql\n SET iceberg.mr.catalog=hive;\n ```\n-You should now be able to issue Hive SQL `SELECT` queries using the above table and see the results returned from the underlying Iceberg table. Both the Map Reduce and Tez query execution engines are supported.\n+You should now be able to issue Hive SQL `SELECT` queries using the above table and see the results returned from the underlying Iceberg table.\n ```sql\n SELECT * from table_b;\n ```\n \n+#### Using Hadoop Catalog\n+Iceberg tables created using `HadoopCatalog` are stored entirely in a directory in a filesytem like HDFS. \n+\n+##### Create an Iceberg table\n+The first step is to create an Iceberg table using the Spark/Java/Python API and `HadoopCatalog`. For the purposes of this documentation we will assume that the fully qualified table identifier is `database_a.table_c` and that the Hadoop Catalog warehouse location is `hdfs://some_bucket/path_to_hadoop_warehouse`. Iceberg will therefore create the table at the location `hdfs://some_bucket/path_to_hadoop_warehouse/database_a/table_c`.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjYzNzg1MQ=="}, "originalCommit": {"oid": "1542d2662ea12c942d021294aaf2269d37612f0c"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzU2NjE2OQ==", "bodyText": "This should work now for HiveCatalog:\nCREATE EXTERNAL TABLE customers (customer_id BIGINT, first_name STRING, last_name STRING)\nSTORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler'\n\nThis should work for other Catalogs:\nCREATE EXTERNAL TABLE customers (customer_id BIGINT, first_name STRING, last_name STRING)\nSTORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler'\nLOCATION '<LOCATION>'\n\nThe location and the table identifier should be the same ATM.", "url": "https://github.com/apache/iceberg/pull/1837#discussion_r533566169", "createdAt": "2020-12-01T16:51:38Z", "author": {"login": "pvary"}, "path": "site/docs/hive.md", "diffHunk": "@@ -79,12 +79,45 @@ In order to query a Hive table created by either of the HiveCatalog methods desc\n ```sql\n SET iceberg.mr.catalog=hive;\n ```\n-You should now be able to issue Hive SQL `SELECT` queries using the above table and see the results returned from the underlying Iceberg table. Both the Map Reduce and Tez query execution engines are supported.\n+You should now be able to issue Hive SQL `SELECT` queries using the above table and see the results returned from the underlying Iceberg table.\n ```sql\n SELECT * from table_b;\n ```\n \n+#### Using Hadoop Catalog\n+Iceberg tables created using `HadoopCatalog` are stored entirely in a directory in a filesytem like HDFS. \n+\n+##### Create an Iceberg table\n+The first step is to create an Iceberg table using the Spark/Java/Python API and `HadoopCatalog`. For the purposes of this documentation we will assume that the fully qualified table identifier is `database_a.table_c` and that the Hadoop Catalog warehouse location is `hdfs://some_bucket/path_to_hadoop_warehouse`. Iceberg will therefore create the table at the location `hdfs://some_bucket/path_to_hadoop_warehouse/database_a/table_c`.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjYzNzg1MQ=="}, "originalCommit": {"oid": "1542d2662ea12c942d021294aaf2269d37612f0c"}, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0MzQzMjQ4OnYy", "diffSide": "RIGHT", "path": "site/docs/hive.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQyMTo0NzoyNFrOH8PI6w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMDoyNTo0OVrOH8lW-A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjkyNDY1MQ==", "bodyText": "Does it work to set iceberg.mr.catalog in table properties for a Hive catalog table as well? If so, we should probably note that a Hive table can be created for an Iceberg table this way as well.", "url": "https://github.com/apache/iceberg/pull/1837#discussion_r532924651", "createdAt": "2020-11-30T21:47:24Z", "author": {"login": "rdblue"}, "path": "site/docs/hive.md", "diffHunk": "@@ -79,12 +79,45 @@ In order to query a Hive table created by either of the HiveCatalog methods desc\n ```sql\n SET iceberg.mr.catalog=hive;\n ```\n-You should now be able to issue Hive SQL `SELECT` queries using the above table and see the results returned from the underlying Iceberg table. Both the Map Reduce and Tez query execution engines are supported.\n+You should now be able to issue Hive SQL `SELECT` queries using the above table and see the results returned from the underlying Iceberg table.\n ```sql\n SELECT * from table_b;\n ```\n \n+#### Using Hadoop Catalog\n+Iceberg tables created using `HadoopCatalog` are stored entirely in a directory in a filesytem like HDFS. \n+\n+##### Create an Iceberg table\n+The first step is to create an Iceberg table using the Spark/Java/Python API and `HadoopCatalog`. For the purposes of this documentation we will assume that the fully qualified table identifier is `database_a.table_c` and that the Hadoop Catalog warehouse location is `hdfs://some_bucket/path_to_hadoop_warehouse`. Iceberg will therefore create the table at the location `hdfs://some_bucket/path_to_hadoop_warehouse/database_a/table_c`.\n+\n+##### Create a Hive table\n+Now overlay a Hive table on top of this Iceberg table by issuing Hive DDL like so:\n+```sql\n+CREATE EXTERNAL TABLE database_a.table_c \n+STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' \n+LOCATION 'hdfs://some_bucket/path_to_hadoop_warehouse/database_a/table_c'\n+TBLPROPERTIES (\n+  'iceberg.mr.catalog'='hadoop', ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1542d2662ea12c942d021294aaf2269d37612f0c"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzI4ODY5Ng==", "bodyText": "I think so, I'm not sure why we don't set that when we automatically create those tables when the Hive exec engine is enabled. I'll make a note and take a look and will update the documentation accordingly (and perhaps also add the code to set that property).", "url": "https://github.com/apache/iceberg/pull/1837#discussion_r533288696", "createdAt": "2020-12-01T10:25:49Z", "author": {"login": "massdosage"}, "path": "site/docs/hive.md", "diffHunk": "@@ -79,12 +79,45 @@ In order to query a Hive table created by either of the HiveCatalog methods desc\n ```sql\n SET iceberg.mr.catalog=hive;\n ```\n-You should now be able to issue Hive SQL `SELECT` queries using the above table and see the results returned from the underlying Iceberg table. Both the Map Reduce and Tez query execution engines are supported.\n+You should now be able to issue Hive SQL `SELECT` queries using the above table and see the results returned from the underlying Iceberg table.\n ```sql\n SELECT * from table_b;\n ```\n \n+#### Using Hadoop Catalog\n+Iceberg tables created using `HadoopCatalog` are stored entirely in a directory in a filesytem like HDFS. \n+\n+##### Create an Iceberg table\n+The first step is to create an Iceberg table using the Spark/Java/Python API and `HadoopCatalog`. For the purposes of this documentation we will assume that the fully qualified table identifier is `database_a.table_c` and that the Hadoop Catalog warehouse location is `hdfs://some_bucket/path_to_hadoop_warehouse`. Iceberg will therefore create the table at the location `hdfs://some_bucket/path_to_hadoop_warehouse/database_a/table_c`.\n+\n+##### Create a Hive table\n+Now overlay a Hive table on top of this Iceberg table by issuing Hive DDL like so:\n+```sql\n+CREATE EXTERNAL TABLE database_a.table_c \n+STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' \n+LOCATION 'hdfs://some_bucket/path_to_hadoop_warehouse/database_a/table_c'\n+TBLPROPERTIES (\n+  'iceberg.mr.catalog'='hadoop', ", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjkyNDY1MQ=="}, "originalCommit": {"oid": "1542d2662ea12c942d021294aaf2269d37612f0c"}, "originalPosition": 32}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3179, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}