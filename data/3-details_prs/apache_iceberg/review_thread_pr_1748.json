{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTE4NDU1OTAz", "number": 1748, "reviewThreads": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMFQxMjozNDo1N1rOE3NKlQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQxNzo1NDo1NFrOE4R7Ag==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2MzIyODM3OnYy", "diffSide": "RIGHT", "path": "site/docs/hive.md", "isResolved": false, "comments": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMFQxMjozNDo1N1rOHwagyg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQwODozNDo1NVrOHxv2ZA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDUyODA3NA==", "bodyText": "@pvary What does one need to do in order to get the table set up properly for the Hive read path in this case? What I have tried to do so far is this, first create an Iceberg table using the HiveCatalog like so:\n PartitionSpec spec = PartitionSpec.unpartitioned();\n Schema schema = new Schema(optional(1, \"id\", Types.LongType.get()), optional(2, \"name\", Types.StringType.get()));\n SparkSession spark = SparkSession.builder().appName(\"IcebergTest\").getOrCreate();\n Configuration hadoopConfiguration = spark.sparkContext().hadoopConfiguration();\n Catalog catalog = new HiveCatalog(hadoopConfiguration);\n TableIdentifier tableId = TableIdentifier.of(\"test\", \"iceberg_table_from_hive_catalog\");\n catalog.createTable(tableId, schema, spec);\n\nThe table created in Hive by the above has DDL like so:\nCREATE EXTERNAL TABLE `iceberg_table_from_hive_catalog`(\n  `id` bigint COMMENT '', \n  `name` string COMMENT '')\nROW FORMAT SERDE \n  'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe' \nSTORED AS INPUTFORMAT \n  'org.apache.hadoop.mapred.FileInputFormat' \nOUTPUTFORMAT \n  'org.apache.hadoop.mapred.FileOutputFormat'\nLOCATION\n  's3://REDACTED/iceberg_table_from_hive_catalog'\nTBLPROPERTIES (\n  'metadata_location'='s3://REDACTED/iceberg_table_from_hive_catalog/metadata/00000-7addbbf2-1836-4973-86af-0511ae7577fb.metadata.json', \n  'table_type'='ICEBERG', \n  'transient_lastDdlTime'='1605007216')\n\nWhich is obviously incorrect as the StorageHandler hasn't been set etc.. I know you worked on a PR that set this all up properly as long as some config/setup was performed at table creation time. Can you please let me know what I need to do and I'll then document it accordingly once I test it working?", "url": "https://github.com/apache/iceberg/pull/1748#discussion_r520528074", "createdAt": "2020-11-10T12:34:57Z", "author": {"login": "massdosage"}, "path": "site/docs/hive.md", "diffHunk": "@@ -50,6 +50,19 @@ You should now be able to issue Hive SQL `SELECT` queries using the above table\n SELECT * from table_a;\n ```\n \n+#### Using Hive Catalog\n+Iceberg tables created using `HiveCatalog` are automatically registered with Hive. \n+\n+##### Create an Iceberg table\n+The first step is to create an Iceberg table using the Spark/Java/Python API and `HiveCatalog`. For the purposes of this documentation we will assume that the table is called `table_b` and that the table location is `s3://some_path/table_b`.\n+TODO: what do we need to set up when we create this table programatically for everything to be registered correctly for read usage in Hive?", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "625226ee6fb6d167e15b0cae3d5dbedc2fb50e1c"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDU2NzIyMw==", "bodyText": "I think setting engine.hive.enabled=true should help.", "url": "https://github.com/apache/iceberg/pull/1748#discussion_r520567223", "createdAt": "2020-11-10T13:38:23Z", "author": {"login": "pvary"}, "path": "site/docs/hive.md", "diffHunk": "@@ -50,6 +50,19 @@ You should now be able to issue Hive SQL `SELECT` queries using the above table\n SELECT * from table_a;\n ```\n \n+#### Using Hive Catalog\n+Iceberg tables created using `HiveCatalog` are automatically registered with Hive. \n+\n+##### Create an Iceberg table\n+The first step is to create an Iceberg table using the Spark/Java/Python API and `HiveCatalog`. For the purposes of this documentation we will assume that the table is called `table_b` and that the table location is `s3://some_path/table_b`.\n+TODO: what do we need to set up when we create this table programatically for everything to be registered correctly for read usage in Hive?", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDUyODA3NA=="}, "originalCommit": {"oid": "625226ee6fb6d167e15b0cae3d5dbedc2fb50e1c"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDY2MDkwNQ==", "bodyText": "OK, so I tried adding this:\nhadoopConfiguration.set(\"engine.hive.enabled\", \"true\");\n\nbefore passing the hadoopConfiguration on to the\nHiveCatalog catalog = new HiveCatalog(hadoopConfiguration);\n\nbut it doesn't seem to make any difference to the DDL of the table that gets subsequently created.", "url": "https://github.com/apache/iceberg/pull/1748#discussion_r520660905", "createdAt": "2020-11-10T15:40:29Z", "author": {"login": "massdosage"}, "path": "site/docs/hive.md", "diffHunk": "@@ -50,6 +50,19 @@ You should now be able to issue Hive SQL `SELECT` queries using the above table\n SELECT * from table_a;\n ```\n \n+#### Using Hive Catalog\n+Iceberg tables created using `HiveCatalog` are automatically registered with Hive. \n+\n+##### Create an Iceberg table\n+The first step is to create an Iceberg table using the Spark/Java/Python API and `HiveCatalog`. For the purposes of this documentation we will assume that the table is called `table_b` and that the table location is `s3://some_path/table_b`.\n+TODO: what do we need to set up when we create this table programatically for everything to be registered correctly for read usage in Hive?", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDUyODA3NA=="}, "originalCommit": {"oid": "625226ee6fb6d167e15b0cae3d5dbedc2fb50e1c"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDY4MjIzNg==", "bodyText": "Oh wait, iceberg.engine.hive.enabled is what it should be, let me try that...", "url": "https://github.com/apache/iceberg/pull/1748#discussion_r520682236", "createdAt": "2020-11-10T16:07:43Z", "author": {"login": "massdosage"}, "path": "site/docs/hive.md", "diffHunk": "@@ -50,6 +50,19 @@ You should now be able to issue Hive SQL `SELECT` queries using the above table\n SELECT * from table_a;\n ```\n \n+#### Using Hive Catalog\n+Iceberg tables created using `HiveCatalog` are automatically registered with Hive. \n+\n+##### Create an Iceberg table\n+The first step is to create an Iceberg table using the Spark/Java/Python API and `HiveCatalog`. For the purposes of this documentation we will assume that the table is called `table_b` and that the table location is `s3://some_path/table_b`.\n+TODO: what do we need to set up when we create this table programatically for everything to be registered correctly for read usage in Hive?", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDUyODA3NA=="}, "originalCommit": {"oid": "625226ee6fb6d167e15b0cae3d5dbedc2fb50e1c"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDY5MjY4MQ==", "bodyText": "OK, so, with setting that property the DDL looks better:\nCREATE EXTERNAL TABLE `iceberg_table_from_hive_catalog`(\n)\nROW FORMAT SERDE \n  'org.apache.iceberg.mr.hive.HiveIcebergSerDe' \nSTORED BY \n  'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' \n\nLOCATION\n  's3://REDACTED/iceberg_table_from_hive_catalog'\nTBLPROPERTIES (\n  'metadata_location'='s3://REDACTED/iceberg_table_from_hive_catalog/metadata/00000-95297465-a383-4f0b-b546-dffe7c4ff778.metadata.json', \n  'table_type'='ICEBERG', \n  'transient_lastDdlTime'='1605024789')\n\nNot sure whether it's an issue that the schema information is missing? I also can't successfully query this table from Hive so I'll look into that next and see what the issue is. Thanks for helping me over the first hurdle though, I'll update this PR shortly with that required config setting.", "url": "https://github.com/apache/iceberg/pull/1748#discussion_r520692681", "createdAt": "2020-11-10T16:21:23Z", "author": {"login": "massdosage"}, "path": "site/docs/hive.md", "diffHunk": "@@ -50,6 +50,19 @@ You should now be able to issue Hive SQL `SELECT` queries using the above table\n SELECT * from table_a;\n ```\n \n+#### Using Hive Catalog\n+Iceberg tables created using `HiveCatalog` are automatically registered with Hive. \n+\n+##### Create an Iceberg table\n+The first step is to create an Iceberg table using the Spark/Java/Python API and `HiveCatalog`. For the purposes of this documentation we will assume that the table is called `table_b` and that the table location is `s3://some_path/table_b`.\n+TODO: what do we need to set up when we create this table programatically for everything to be registered correctly for read usage in Hive?", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDUyODA3NA=="}, "originalCommit": {"oid": "625226ee6fb6d167e15b0cae3d5dbedc2fb50e1c"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTU2Njc0Ng==", "bodyText": "I got this working when I tested 0.10.0-rc4. In Hive, I had to set iceberg.mr.catalog=hive so that the correct catalog was used. Unfortunately, that broke the Hadoop table.\nI think we might want to add support for Hadoop tables in our catalogs using an identifier that contains a path to fix this.", "url": "https://github.com/apache/iceberg/pull/1748#discussion_r521566746", "createdAt": "2020-11-11T18:47:05Z", "author": {"login": "rdblue"}, "path": "site/docs/hive.md", "diffHunk": "@@ -50,6 +50,19 @@ You should now be able to issue Hive SQL `SELECT` queries using the above table\n SELECT * from table_a;\n ```\n \n+#### Using Hive Catalog\n+Iceberg tables created using `HiveCatalog` are automatically registered with Hive. \n+\n+##### Create an Iceberg table\n+The first step is to create an Iceberg table using the Spark/Java/Python API and `HiveCatalog`. For the purposes of this documentation we will assume that the table is called `table_b` and that the table location is `s3://some_path/table_b`.\n+TODO: what do we need to set up when we create this table programatically for everything to be registered correctly for read usage in Hive?", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDUyODA3NA=="}, "originalCommit": {"oid": "625226ee6fb6d167e15b0cae3d5dbedc2fb50e1c"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTU2NzI1MQ==", "bodyText": "Also, engine.hive.enabled is the table property. When a table has that property, it will be created with the storage handler. I set the property on my table from Spark and it worked.", "url": "https://github.com/apache/iceberg/pull/1748#discussion_r521567251", "createdAt": "2020-11-11T18:48:02Z", "author": {"login": "rdblue"}, "path": "site/docs/hive.md", "diffHunk": "@@ -50,6 +50,19 @@ You should now be able to issue Hive SQL `SELECT` queries using the above table\n SELECT * from table_a;\n ```\n \n+#### Using Hive Catalog\n+Iceberg tables created using `HiveCatalog` are automatically registered with Hive. \n+\n+##### Create an Iceberg table\n+The first step is to create an Iceberg table using the Spark/Java/Python API and `HiveCatalog`. For the purposes of this documentation we will assume that the table is called `table_b` and that the table location is `s3://some_path/table_b`.\n+TODO: what do we need to set up when we create this table programatically for everything to be registered correctly for read usage in Hive?", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDUyODA3NA=="}, "originalCommit": {"oid": "625226ee6fb6d167e15b0cae3d5dbedc2fb50e1c"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTU3MDEzOA==", "bodyText": "Also, engine.hive.enabled is the table property. When a table has that property, it will be created with the storage handler. I set the property on my table from Spark and it worked.\n\nReally? From what I can see the code uses the value from here \n  \n    \n      iceberg/core/src/main/java/org/apache/iceberg/hadoop/ConfigProperties.java\n    \n    \n         Line 27\n      in\n      b403009\n    \n    \n    \n    \n\n        \n          \n           public static final String ENGINE_HIVE_ENABLED = \"iceberg.engine.hive.enabled\"; \n        \n    \n  \n\n which is prefixed with iceberg as I said above. I tried it both ways and only this way worked but the schema in the Hive table definition is empty so Hive seems to be throwing an error about that when it tries to query it. I haven't tried setting iceberg.mr.catalog=hive in Hive when I do the query, I will try that next and see if it makes any difference and what impact that has on the HadoopTables version of it.", "url": "https://github.com/apache/iceberg/pull/1748#discussion_r521570138", "createdAt": "2020-11-11T18:53:42Z", "author": {"login": "massdosage"}, "path": "site/docs/hive.md", "diffHunk": "@@ -50,6 +50,19 @@ You should now be able to issue Hive SQL `SELECT` queries using the above table\n SELECT * from table_a;\n ```\n \n+#### Using Hive Catalog\n+Iceberg tables created using `HiveCatalog` are automatically registered with Hive. \n+\n+##### Create an Iceberg table\n+The first step is to create an Iceberg table using the Spark/Java/Python API and `HiveCatalog`. For the purposes of this documentation we will assume that the table is called `table_b` and that the table location is `s3://some_path/table_b`.\n+TODO: what do we need to set up when we create this table programatically for everything to be registered correctly for read usage in Hive?", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDUyODA3NA=="}, "originalCommit": {"oid": "625226ee6fb6d167e15b0cae3d5dbedc2fb50e1c"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTYzNzM0MQ==", "bodyText": "The logic for setting up the storage handler is here: https://github.com/apache/iceberg/blob/master/hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java#L377-L384\nIt uses both the environment config and the table property.", "url": "https://github.com/apache/iceberg/pull/1748#discussion_r521637341", "createdAt": "2020-11-11T21:04:08Z", "author": {"login": "rdblue"}, "path": "site/docs/hive.md", "diffHunk": "@@ -50,6 +50,19 @@ You should now be able to issue Hive SQL `SELECT` queries using the above table\n SELECT * from table_a;\n ```\n \n+#### Using Hive Catalog\n+Iceberg tables created using `HiveCatalog` are automatically registered with Hive. \n+\n+##### Create an Iceberg table\n+The first step is to create an Iceberg table using the Spark/Java/Python API and `HiveCatalog`. For the purposes of this documentation we will assume that the table is called `table_b` and that the table location is `s3://some_path/table_b`.\n+TODO: what do we need to set up when we create this table programatically for everything to be registered correctly for read usage in Hive?", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDUyODA3NA=="}, "originalCommit": {"oid": "625226ee6fb6d167e15b0cae3d5dbedc2fb50e1c"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTkyMjg3Nw==", "bodyText": "Yes, exactly, so in this case when I'm creating a new table it doesn't find the value in the table metadata so it drops through to line 383 and looks in the Configuration using the ConfigProperties key I mentioned above which is iceberg.engine.hive.enabled. I stepped through this code to figure out why it wasn't working with engine.hive.enabled and saw the property it was looking for in the conf is actually iceberg.engine.hive.enabled. So I guess there are two options here:\n\nyou explicitly create the table with metadata and set engine.hive.enabled as a table property.\n\nor\n\nyou set the value iceberg.engine.hive.enabled explicitly in the conf object (like I'm doing in my example code above) or let this be automatically set by adding it to hive-site.xml.\n\nIf we agree on this I can document the above here ^. I\nI'll then look into what's necessary in order to query the table and add it.", "url": "https://github.com/apache/iceberg/pull/1748#discussion_r521922877", "createdAt": "2020-11-12T08:29:14Z", "author": {"login": "massdosage"}, "path": "site/docs/hive.md", "diffHunk": "@@ -50,6 +50,19 @@ You should now be able to issue Hive SQL `SELECT` queries using the above table\n SELECT * from table_a;\n ```\n \n+#### Using Hive Catalog\n+Iceberg tables created using `HiveCatalog` are automatically registered with Hive. \n+\n+##### Create an Iceberg table\n+The first step is to create an Iceberg table using the Spark/Java/Python API and `HiveCatalog`. For the purposes of this documentation we will assume that the table is called `table_b` and that the table location is `s3://some_path/table_b`.\n+TODO: what do we need to set up when we create this table programatically for everything to be registered correctly for read usage in Hive?", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDUyODA3NA=="}, "originalCommit": {"oid": "625226ee6fb6d167e15b0cae3d5dbedc2fb50e1c"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTkyNjI0NA==", "bodyText": "CC @lcspinter as he is working on coming up a generic way to handle properties / configuration values wrt Hive.\nWe would like to come up with solution where the user could easily understand how the different properties are used:\n\nIceberg table properties\nHMS table properties\nHMS serde properties\nHiveConf\nDefault values\n\nAlso we should understand how these properties are used when:\n\nReading Hive / Iceberg tables\nWriting Hive / Iceberg tables", "url": "https://github.com/apache/iceberg/pull/1748#discussion_r521926244", "createdAt": "2020-11-12T08:34:55Z", "author": {"login": "pvary"}, "path": "site/docs/hive.md", "diffHunk": "@@ -50,6 +50,19 @@ You should now be able to issue Hive SQL `SELECT` queries using the above table\n SELECT * from table_a;\n ```\n \n+#### Using Hive Catalog\n+Iceberg tables created using `HiveCatalog` are automatically registered with Hive. \n+\n+##### Create an Iceberg table\n+The first step is to create an Iceberg table using the Spark/Java/Python API and `HiveCatalog`. For the purposes of this documentation we will assume that the table is called `table_b` and that the table location is `s3://some_path/table_b`.\n+TODO: what do we need to set up when we create this table programatically for everything to be registered correctly for read usage in Hive?", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDUyODA3NA=="}, "originalCommit": {"oid": "625226ee6fb6d167e15b0cae3d5dbedc2fb50e1c"}, "originalPosition": 9}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI3NDQ5MDE1OnYy", "diffSide": "RIGHT", "path": "site/docs/hive.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQxNzo1NDowOVrOHyGzvA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQxOToxNTozOVrOHyJ82w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjMwMjM5Ng==", "bodyText": "This seems to me contradictory to the following lines. Is this TODO still valid?", "url": "https://github.com/apache/iceberg/pull/1748#discussion_r522302396", "createdAt": "2020-11-12T17:54:09Z", "author": {"login": "rdsr"}, "path": "site/docs/hive.md", "diffHunk": "@@ -50,6 +50,38 @@ You should now be able to issue Hive SQL `SELECT` queries using the above table\n SELECT * from table_a;\n ```\n \n+#### Using Hive Catalog\n+Iceberg tables created using `HiveCatalog` are automatically registered with Hive.\n+\n+##### Create an Iceberg table\n+The first step is to create an Iceberg table using the Spark/Java/Python API and `HiveCatalog`. For the purposes of this documentation we will assume that the table is called `table_b` and that the table location is `s3://some_path/table_b`. In order for Iceberg to correctly set up the Hive table for querying some configuration values need to be set, the two options for this are described below - you can use either or the other depending on your use case.\n+\n+##### Hive Configuration\n+The value `iceberg.engine.hive.enabled` needs to be set to `true` and added to the Hive configuration file on the classpath of the application creating the table. This can be done by modifying the relevant `hive-site.xml`. Alternatively this can done programatically like so:\n+```java\n+Configuration hadoopConfiguration = spark.sparkContext().hadoopConfiguration();\n+hadoopConfiguration.set(ConfigProperties.ENGINE_HIVE_ENABLED, \"true\"); //iceberg.engine.hive.enabled=true\n+HiveCatalog catalog = new HiveCatalog(hadoopConfiguration);\n+...\n+catalog.createTable(tableId, schema, spec);\n+```\n+\n+##### Table Property Configuration\n+The property `engine.hive.enabled` needs to be set to `true` and added to the table properties when creating the Iceberg table. This can be done like so:\n+```java\n+    Map<String, String> tableProperties = new HashMap<String, String>();\n+    tableProperties.put(TableProperties.ENGINE_HIVE_ENABLED, \"true\"); ////engine.hive.enabled=true\n+    catalog.createTable(tableId, schema, spec, tableProperties);\n+```\n+\n+#### Query the Iceberg table via Hive\n+TODO: tables created by the above can't just be read \"as is\", need to document steps needed in order to be able to query them here.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0ce135ca4ceb3cd683893575b61c3cdd8d193f5e"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjM1Mzg4Mw==", "bodyText": "Yes, I'm currently unable to query the tables created by the above from Hive and need to figure out what I need to do in order to get it working. There's either an issue with the generated DDL or some properties need to be set at read time. I haven't had time to look into it properly yet.", "url": "https://github.com/apache/iceberg/pull/1748#discussion_r522353883", "createdAt": "2020-11-12T19:15:39Z", "author": {"login": "massdosage"}, "path": "site/docs/hive.md", "diffHunk": "@@ -50,6 +50,38 @@ You should now be able to issue Hive SQL `SELECT` queries using the above table\n SELECT * from table_a;\n ```\n \n+#### Using Hive Catalog\n+Iceberg tables created using `HiveCatalog` are automatically registered with Hive.\n+\n+##### Create an Iceberg table\n+The first step is to create an Iceberg table using the Spark/Java/Python API and `HiveCatalog`. For the purposes of this documentation we will assume that the table is called `table_b` and that the table location is `s3://some_path/table_b`. In order for Iceberg to correctly set up the Hive table for querying some configuration values need to be set, the two options for this are described below - you can use either or the other depending on your use case.\n+\n+##### Hive Configuration\n+The value `iceberg.engine.hive.enabled` needs to be set to `true` and added to the Hive configuration file on the classpath of the application creating the table. This can be done by modifying the relevant `hive-site.xml`. Alternatively this can done programatically like so:\n+```java\n+Configuration hadoopConfiguration = spark.sparkContext().hadoopConfiguration();\n+hadoopConfiguration.set(ConfigProperties.ENGINE_HIVE_ENABLED, \"true\"); //iceberg.engine.hive.enabled=true\n+HiveCatalog catalog = new HiveCatalog(hadoopConfiguration);\n+...\n+catalog.createTable(tableId, schema, spec);\n+```\n+\n+##### Table Property Configuration\n+The property `engine.hive.enabled` needs to be set to `true` and added to the table properties when creating the Iceberg table. This can be done like so:\n+```java\n+    Map<String, String> tableProperties = new HashMap<String, String>();\n+    tableProperties.put(TableProperties.ENGINE_HIVE_ENABLED, \"true\"); ////engine.hive.enabled=true\n+    catalog.createTable(tableId, schema, spec, tableProperties);\n+```\n+\n+#### Query the Iceberg table via Hive\n+TODO: tables created by the above can't just be read \"as is\", need to document steps needed in order to be able to query them here.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjMwMjM5Ng=="}, "originalCommit": {"oid": "0ce135ca4ceb3cd683893575b61c3cdd8d193f5e"}, "originalPosition": 29}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI3NDQ5MzQ2OnYy", "diffSide": "RIGHT", "path": "site/docs/hive.md", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQxNzo1NDo1NFrOHyG1yA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQxMTo1NDoyOVrOH2Z5OA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjMwMjkyMA==", "bodyText": "I assume we can query existing Iceberg tables also through Hive by setting the iceberg.engine.hive.enabled flag? Do you think it worth calling that out?", "url": "https://github.com/apache/iceberg/pull/1748#discussion_r522302920", "createdAt": "2020-11-12T17:54:54Z", "author": {"login": "rdsr"}, "path": "site/docs/hive.md", "diffHunk": "@@ -50,6 +50,38 @@ You should now be able to issue Hive SQL `SELECT` queries using the above table\n SELECT * from table_a;\n ```\n \n+#### Using Hive Catalog\n+Iceberg tables created using `HiveCatalog` are automatically registered with Hive.\n+\n+##### Create an Iceberg table\n+The first step is to create an Iceberg table using the Spark/Java/Python API and `HiveCatalog`. For the purposes of this documentation we will assume that the table is called `table_b` and that the table location is `s3://some_path/table_b`. In order for Iceberg to correctly set up the Hive table for querying some configuration values need to be set, the two options for this are described below - you can use either or the other depending on your use case.\n+\n+##### Hive Configuration\n+The value `iceberg.engine.hive.enabled` needs to be set to `true` and added to the Hive configuration file on the classpath of the application creating the table. This can be done by modifying the relevant `hive-site.xml`. Alternatively this can done programatically like so:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0ce135ca4ceb3cd683893575b61c3cdd8d193f5e"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjM1NDIyNw==", "bodyText": "I'll let you know if and when I get it working ;) and yes, that will need to be added to the documentation.", "url": "https://github.com/apache/iceberg/pull/1748#discussion_r522354227", "createdAt": "2020-11-12T19:16:18Z", "author": {"login": "massdosage"}, "path": "site/docs/hive.md", "diffHunk": "@@ -50,6 +50,38 @@ You should now be able to issue Hive SQL `SELECT` queries using the above table\n SELECT * from table_a;\n ```\n \n+#### Using Hive Catalog\n+Iceberg tables created using `HiveCatalog` are automatically registered with Hive.\n+\n+##### Create an Iceberg table\n+The first step is to create an Iceberg table using the Spark/Java/Python API and `HiveCatalog`. For the purposes of this documentation we will assume that the table is called `table_b` and that the table location is `s3://some_path/table_b`. In order for Iceberg to correctly set up the Hive table for querying some configuration values need to be set, the two options for this are described below - you can use either or the other depending on your use case.\n+\n+##### Hive Configuration\n+The value `iceberg.engine.hive.enabled` needs to be set to `true` and added to the Hive configuration file on the classpath of the application creating the table. This can be done by modifying the relevant `hive-site.xml`. Alternatively this can done programatically like so:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjMwMjkyMA=="}, "originalCommit": {"oid": "0ce135ca4ceb3cd683893575b61c3cdd8d193f5e"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjgwOTQwMA==", "bodyText": "Updated docs to describe the property you need to set for Hive read is set iceberg.mr.catalog=hive", "url": "https://github.com/apache/iceberg/pull/1748#discussion_r526809400", "createdAt": "2020-11-19T11:54:29Z", "author": {"login": "massdosage"}, "path": "site/docs/hive.md", "diffHunk": "@@ -50,6 +50,38 @@ You should now be able to issue Hive SQL `SELECT` queries using the above table\n SELECT * from table_a;\n ```\n \n+#### Using Hive Catalog\n+Iceberg tables created using `HiveCatalog` are automatically registered with Hive.\n+\n+##### Create an Iceberg table\n+The first step is to create an Iceberg table using the Spark/Java/Python API and `HiveCatalog`. For the purposes of this documentation we will assume that the table is called `table_b` and that the table location is `s3://some_path/table_b`. In order for Iceberg to correctly set up the Hive table for querying some configuration values need to be set, the two options for this are described below - you can use either or the other depending on your use case.\n+\n+##### Hive Configuration\n+The value `iceberg.engine.hive.enabled` needs to be set to `true` and added to the Hive configuration file on the classpath of the application creating the table. This can be done by modifying the relevant `hive-site.xml`. Alternatively this can done programatically like so:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjMwMjkyMA=="}, "originalCommit": {"oid": "0ce135ca4ceb3cd683893575b61c3cdd8d193f5e"}, "originalPosition": 11}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3431, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}