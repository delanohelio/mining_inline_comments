{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTQyMjI4MjA3", "number": 1955, "title": "Spark: Sort retained rows in DELETE FROM by file and position", "bodyText": "This updates Spark's DELETE FROM command to sort the retained rows by original file and position to ensure that the original data clustering is preserved by the command.\nBecause Spark does not yet support metadata columns, this exposes _file and _pos by adding them automatically to all merge scans. Projecting both columns was mostly supported, with only minor changes needed to project _file using the constants map supported by Avro, Parquet, and ORC.\nThis also required refactoring DynamicFileFilter. When projecting _file and _pos but only using file, the optimizer would throw an exception that the node could not be copied because the optimizer was attempting to rewrite the node with a projection to remove the unused _pos_. The fix is to update DynamicFileFilter so that the SupportsFileFilter is passed separately. Then the scan can be passed as a logical plan that can be rewritten by the planner. This also required updating conversion to physical plan because the scan plan may be more complicated than a single scan node. This ensures that the scan is converted to an extended scan by using a new logical plan wrapper so that planLater can be used in conversion like normal.", "createdAt": "2020-12-18T01:16:52Z", "url": "https://github.com/apache/iceberg/pull/1955", "merged": true, "mergeCommit": {"oid": "bafda6168b533286bb57bea71e8060f660d0f4dc"}, "closed": true, "closedAt": "2020-12-21T21:19:15Z", "author": {"login": "rdblue"}, "timelineItems": {"totalCount": 22, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdnNsb5gFqTU1NTEwODU5Ng==", "endCursor": "Y3Vyc29yOnYyOpPPAAABdobq1pAFqTU1NjYzNDYxNQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU1MTA4NTk2", "url": "https://github.com/apache/iceberg/pull/1955#pullrequestreview-555108596", "createdAt": "2020-12-18T01:17:34Z", "commit": {"oid": "934a375adbf9efca155936d63f4003f94adb070b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQwMToxNzozNVrOIIPTWg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQwMToxNzozNVrOIIPTWg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTUxMDIzNA==", "bodyText": "Needed to allow projecting _file even though it isn't in the data file.", "url": "https://github.com/apache/iceberg/pull/1955#discussion_r545510234", "createdAt": "2020-12-18T01:17:35Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/avro/BuildAvroProjection.java", "diffHunk": "@@ -96,7 +96,7 @@ public Schema record(Schema record, List<String> names, Iterable<Schema.Field> s\n \n       } else {\n         Preconditions.checkArgument(\n-            field.isOptional() || field.fieldId() == MetadataColumns.ROW_POSITION.fieldId(),\n+            field.isOptional() || MetadataColumns.metadataFieldIds().contains(field.fieldId()),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "934a375adbf9efca155936d63f4003f94adb070b"}, "originalPosition": 5}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU1MTA4NzU1", "url": "https://github.com/apache/iceberg/pull/1955#pullrequestreview-555108755", "createdAt": "2020-12-18T01:18:01Z", "commit": {"oid": "934a375adbf9efca155936d63f4003f94adb070b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQwMToxODowMlrOIIPT6A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQwMToxODowMlrOIIPT6A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTUxMDM3Ng==", "bodyText": "This adds _file to the constants map so it is set in records like a partition value.", "url": "https://github.com/apache/iceberg/pull/1955#discussion_r545510376", "createdAt": "2020-12-18T01:18:02Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/util/PartitionUtil.java", "diffHunk": "@@ -39,13 +40,17 @@ private PartitionUtil() {\n   }\n \n   public static Map<Integer, ?> constantsMap(FileScanTask task, BiFunction<Type, Object, Object> convertConstant) {\n-    return constantsMap(task.spec(), task.file().partition(), convertConstant);\n-  }\n+    PartitionSpec spec = task.spec();\n+    StructLike partitionData = task.file().partition();\n \n-  private static Map<Integer, ?> constantsMap(PartitionSpec spec, StructLike partitionData,\n-                                              BiFunction<Type, Object, Object> convertConstant) {\n     // use java.util.HashMap because partition data may contain null values\n     Map<Integer, Object> idToConstant = new HashMap<>();\n+\n+    // add _file\n+    idToConstant.put(\n+        MetadataColumns.FILE_PATH.fieldId(),\n+        convertConstant.apply(Types.StringType.get(), task.file().path()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "934a375adbf9efca155936d63f4003f94adb070b"}, "originalPosition": 25}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU1MTA5NDk1", "url": "https://github.com/apache/iceberg/pull/1955#pullrequestreview-555109495", "createdAt": "2020-12-18T01:20:08Z", "commit": {"oid": "934a375adbf9efca155936d63f4003f94adb070b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQwMToyMDowOFrOIIPWlA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQwMToyMDowOFrOIIPWlA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTUxMTA2MA==", "bodyText": "Spark's contract is that the scan's schema is the one that should be used, not the original table schema. This allows the merge scan to return the extra _file and _pos columns and matches the behavior of normal scans that are configured with PushDownUtils.pruneColumns.", "url": "https://github.com/apache/iceberg/pull/1955#discussion_r545511060", "createdAt": "2020-12-18T01:20:08Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteDelete.scala", "diffHunk": "@@ -77,33 +101,30 @@ object RewriteDelete extends Rule[LogicalPlan] with PredicateHelper with Logging\n     PushDownUtils.pushFilters(scanBuilder, normalizedPredicates)\n \n     val scan = scanBuilder.build()\n-    val scanRelation = DataSourceV2ScanRelation(table, scan, output)\n+    val scanRelation = DataSourceV2ScanRelation(table, scan, toOutputAttrs(scan.readSchema(), output))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "934a375adbf9efca155936d63f4003f94adb070b"}, "originalPosition": 62}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU1MzE5ODg4", "url": "https://github.com/apache/iceberg/pull/1955#pullrequestreview-555319888", "createdAt": "2020-12-18T09:15:57Z", "commit": {"oid": "934a375adbf9efca155936d63f4003f94adb070b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQwOToxNTo1N1rOIIajhw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQwOToxNTo1N1rOIIajhw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTY5NDU5OQ==", "bodyText": "nit: should it be isNonMetadataColumn to indicate it is a boolean flag?", "url": "https://github.com/apache/iceberg/pull/1955#discussion_r545694599", "createdAt": "2020-12-18T09:15:57Z", "author": {"login": "aokolnychyi"}, "path": "core/src/main/java/org/apache/iceberg/MetadataColumns.java", "diffHunk": "@@ -55,4 +55,16 @@ private MetadataColumns() {\n   public static Set<Integer> metadataFieldIds() {\n     return META_IDS;\n   }\n+\n+  public static NestedField get(String name) {\n+    return META_COLUMNS.get(name);\n+  }\n+\n+  public static boolean isMetadataColumn(String name) {\n+    return META_COLUMNS.containsKey(name);\n+  }\n+\n+  public static boolean nonMetadataColumn(String name) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "934a375adbf9efca155936d63f4003f94adb070b"}, "originalPosition": 13}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU1MzI1MzYx", "url": "https://github.com/apache/iceberg/pull/1955#pullrequestreview-555325361", "createdAt": "2020-12-18T09:24:14Z", "commit": {"oid": "934a375adbf9efca155936d63f4003f94adb070b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQwOToyNDoxNFrOIIbB7g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQwOToyNDoxNFrOIIbB7g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTcwMjM4Mg==", "bodyText": "+1", "url": "https://github.com/apache/iceberg/pull/1955#discussion_r545702382", "createdAt": "2020-12-18T09:24:14Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteDelete.scala", "diffHunk": "@@ -77,33 +101,30 @@ object RewriteDelete extends Rule[LogicalPlan] with PredicateHelper with Logging\n     PushDownUtils.pushFilters(scanBuilder, normalizedPredicates)\n \n     val scan = scanBuilder.build()\n-    val scanRelation = DataSourceV2ScanRelation(table, scan, output)\n+    val scanRelation = DataSourceV2ScanRelation(table, scan, toOutputAttrs(scan.readSchema(), output))\n \n     val scanPlan = scan match {\n-      case _: SupportsFileFilter =>\n+      case filterable: SupportsFileFilter =>\n         val matchingFilePlan = buildFileFilterPlan(cond, scanRelation)\n-        val dynamicFileFilter = DynamicFileFilter(scanRelation, matchingFilePlan)\n+        val dynamicFileFilter = DynamicFileFilter(ExtendedScanRelation(scanRelation), matchingFilePlan, filterable)\n         dynamicFileFilter\n       case _ =>\n         scanRelation\n     }\n \n-    // include file name so that we can group data back\n-    val fileNameExpr = Alias(InputFileName(), FILE_NAME_COL)()\n-    Project(scanPlan.output :+ fileNameExpr, scanPlan)\n+    scanPlan\n   }\n \n   private def buildWritePlan(\n       remainingRowsPlan: LogicalPlan,\n       output: Seq[AttributeReference]): LogicalPlan = {\n \n-    // TODO: sort by _pos to keep the original ordering of rows\n-    // TODO: consider setting a file size limit\n-\n     val fileNameCol = findOutputAttr(remainingRowsPlan, FILE_NAME_COL)\n+    val rowPosCol = findOutputAttr(remainingRowsPlan, ROW_POS_COL)\n+    val order = Seq(SortOrder(fileNameCol, Ascending), SortOrder(rowPosCol, Ascending))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "934a375adbf9efca155936d63f4003f94adb070b"}, "originalPosition": 90}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU1MzMxMDc1", "url": "https://github.com/apache/iceberg/pull/1955#pullrequestreview-555331075", "createdAt": "2020-12-18T09:31:06Z", "commit": {"oid": "934a375adbf9efca155936d63f4003f94adb070b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQwOTozMTowNlrOIIbc3w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQwOTozMTowNlrOIIbc3w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTcwOTI3OQ==", "bodyText": "Are there any cases when pruneColumns is going to be called multiple times? Should we worry about it at all?", "url": "https://github.com/apache/iceberg/pull/1955#discussion_r545709279", "createdAt": "2020-12-18T09:31:06Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/source/SparkScanBuilder.java", "diffHunk": "@@ -131,28 +138,52 @@ public SparkScanBuilder caseSensitive(boolean isCaseSensitive) {\n \n   @Override\n   public void pruneColumns(StructType requestedSchema) {\n-    this.requestedProjection = requestedSchema;\n+    this.requestedProjection = new StructType(Stream.of(requestedSchema.fields())\n+        .filter(field -> MetadataColumns.nonMetadataColumn(field.name()))\n+        .toArray(StructField[]::new));\n+\n+    Stream.of(requestedSchema.fields())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "934a375adbf9efca155936d63f4003f94adb070b"}, "originalPosition": 44}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU1NDU4MTIz", "url": "https://github.com/apache/iceberg/pull/1955#pullrequestreview-555458123", "createdAt": "2020-12-18T12:49:04Z", "commit": {"oid": "934a375adbf9efca155936d63f4003f94adb070b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQxMjo0OTowNFrOIIhnEw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQxMjo0OTowNFrOIIhnEw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTgxMDE5NQ==", "bodyText": "I am a bit worried about adding this class as I am not sure we want to maintain it in Spark later. There is another idea how to solve the rewrite rule: we can simply disable column pruning for DynamicFileFilter nodes. I think it should be sufficient to extend the node references to also cover all output attributes of the scan.\nAttributeSet(scanRelation.output ++ fileFilterPlan.output)\n\nI've submitted a PR with this idea to your branch, @rdblue. Feel free to discard/modify as needed.", "url": "https://github.com/apache/iceberg/pull/1955#discussion_r545810195", "createdAt": "2020-12-18T12:49:04Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/ExtendedScanRelation.scala", "diffHunk": "@@ -0,0 +1,29 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.plans.logical\n+\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanRelation\n+\n+case class ExtendedScanRelation(relation: DataSourceV2ScanRelation) extends LogicalPlan {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "934a375adbf9efca155936d63f4003f94adb070b"}, "originalPosition": 25}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU1NzU3OTE4", "url": "https://github.com/apache/iceberg/pull/1955#pullrequestreview-555757918", "createdAt": "2020-12-18T19:24:55Z", "commit": {"oid": "934a375adbf9efca155936d63f4003f94adb070b"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "934a375adbf9efca155936d63f4003f94adb070b", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/934a375adbf9efca155936d63f4003f94adb070b", "committedDate": "2020-12-18T01:09:18Z", "message": "Spark: Sort retained rows in DELETE FROM by file and position."}, "afterCommit": {"oid": "7a87a438309f57eb83ee21178c01f8bae336b89f", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/7a87a438309f57eb83ee21178c01f8bae336b89f", "committedDate": "2020-12-19T00:13:02Z", "message": "Fix vectorized Parquet _file and _pos."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU1ODkwNDYz", "url": "https://github.com/apache/iceberg/pull/1955#pullrequestreview-555890463", "createdAt": "2020-12-19T00:14:55Z", "commit": {"oid": "7a87a438309f57eb83ee21178c01f8bae336b89f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOVQwMDoxNDo1NVrOII20Hw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOVQwMDoxNDo1NVrOII20Hw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjE1NzU5OQ==", "bodyText": "This is needed for cases where Arrow checks the validity buffer.", "url": "https://github.com/apache/iceberg/pull/1955#discussion_r546157599", "createdAt": "2020-12-19T00:14:55Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorizedArrowReader.java", "diffHunk": "@@ -381,9 +382,13 @@ public VectorHolder read(VectorHolder reuse, int numValsToRead) {\n         for (int i = 0; i < numValsToRead; i += 1) {\n           vec.getDataBuffer().setLong(i * Long.BYTES, rowStart + i);\n         }\n+        for (int i = 0; i < numValsToRead; i += 1) {\n+          BitVectorHelper.setValidityBitToOne(vec.getValidityBuffer(), i);\n+        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7a87a438309f57eb83ee21178c01f8bae336b89f"}, "originalPosition": 14}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU1ODkwNTM3", "url": "https://github.com/apache/iceberg/pull/1955#pullrequestreview-555890537", "createdAt": "2020-12-19T00:15:14Z", "commit": {"oid": "7a87a438309f57eb83ee21178c01f8bae336b89f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOVQwMDoxNToxNFrOII20YA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOVQwMDoxNToxNFrOII20YA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjE1NzY2NA==", "bodyText": "Looks like this was an oversight in the original PR. FYI @chenjunjiedada.", "url": "https://github.com/apache/iceberg/pull/1955#discussion_r546157664", "createdAt": "2020-12-19T00:15:14Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorizedArrowReader.java", "diffHunk": "@@ -381,9 +382,13 @@ public VectorHolder read(VectorHolder reuse, int numValsToRead) {\n         for (int i = 0; i < numValsToRead; i += 1) {\n           vec.getDataBuffer().setLong(i * Long.BYTES, rowStart + i);\n         }\n+        for (int i = 0; i < numValsToRead; i += 1) {\n+          BitVectorHelper.setValidityBitToOne(vec.getValidityBuffer(), i);\n+        }\n         nulls = new NullabilityHolder(numValsToRead);\n       }\n \n+      rowStart += numValsToRead;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7a87a438309f57eb83ee21178c01f8bae336b89f"}, "originalPosition": 18}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU1ODkwODgy", "url": "https://github.com/apache/iceberg/pull/1955#pullrequestreview-555890882", "createdAt": "2020-12-19T00:16:50Z", "commit": {"oid": "7a87a438309f57eb83ee21178c01f8bae336b89f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOVQwMDoxNjo1MFrOII21xw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOVQwMDoxNjo1MFrOII21xw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjE1ODAyMw==", "bodyText": "It isn't necessary to check whether there are projected ID columns. The code is shorter if the values are available by default, even if they aren't used. This fixes the problem where there are constants to add (like _file) but no identity partition values are projected.", "url": "https://github.com/apache/iceberg/pull/1955#discussion_r546158023", "createdAt": "2020-12-19T00:16:50Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/BatchDataReader.java", "diffHunk": "@@ -68,18 +68,7 @@\n     // update the current file for Spark's filename() function\n     InputFileBlockHolder.set(file.path().toString(), task.start(), task.length());\n \n-    // schema or rows returned by readers\n-    PartitionSpec spec = task.spec();\n-    Set<Integer> idColumns = spec.identitySourceIds();\n-    Schema partitionSchema = TypeUtil.select(expectedSchema, idColumns);\n-    boolean projectsIdentityPartitionColumns = !partitionSchema.columns().isEmpty();\n-\n-    Map<Integer, ?> idToConstant;\n-    if (projectsIdentityPartitionColumns) {\n-      idToConstant = PartitionUtil.constantsMap(task, BatchDataReader::convertConstant);\n-    } else {\n-      idToConstant = ImmutableMap.of();\n-    }\n+    Map<Integer, ?> idToConstant = PartitionUtil.constantsMap(task, BatchDataReader::convertConstant);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7a87a438309f57eb83ee21178c01f8bae336b89f"}, "originalPosition": 16}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU2Mjg2Njky", "url": "https://github.com/apache/iceberg/pull/1955#pullrequestreview-556286692", "createdAt": "2020-12-21T10:42:10Z", "commit": {"oid": "d9213621701f7061dde785e821d588f0ab9020c9"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMVQxMDo0MjoxMFrOIJT4vQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMVQxMDo0MjoxMFrOIJT4vQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjYzMzkxNw==", "bodyText": "I think this block can be simplified a bit.\n    scan match {\n      case filterable: SupportsFileFilter =>\n        val matchingFilePlan = buildFileFilterPlan(cond, scanRelation)\n        DynamicFileFilter(scanRelation, matchingFilePlan, filterable)\n      case _ =>\n        scanRelation\n    }", "url": "https://github.com/apache/iceberg/pull/1955#discussion_r546633917", "createdAt": "2020-12-21T10:42:10Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteDelete.scala", "diffHunk": "@@ -77,33 +101,30 @@ object RewriteDelete extends Rule[LogicalPlan] with PredicateHelper with Logging\n     PushDownUtils.pushFilters(scanBuilder, normalizedPredicates)\n \n     val scan = scanBuilder.build()\n-    val scanRelation = DataSourceV2ScanRelation(table, scan, output)\n+    val scanRelation = DataSourceV2ScanRelation(table, scan, toOutputAttrs(scan.readSchema(), output))\n \n     val scanPlan = scan match {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d9213621701f7061dde785e821d588f0ab9020c9"}, "originalPosition": 64}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU2Mjg4NTIx", "url": "https://github.com/apache/iceberg/pull/1955#pullrequestreview-556288521", "createdAt": "2020-12-21T10:45:18Z", "commit": {"oid": "d9213621701f7061dde785e821d588f0ab9020c9"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMVQxMDo0NToxOFrOIJT-tg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMVQxMDo0NToxOFrOIJT-tg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjYzNTQ0Ng==", "bodyText": "Used for local testing?", "url": "https://github.com/apache/iceberg/pull/1955#discussion_r546635446", "createdAt": "2020-12-21T10:45:18Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/SparkRowLevelOperationsTestBase.java", "diffHunk": "@@ -48,32 +48,32 @@ public SparkRowLevelOperationsTestBase(String catalogName, String implementation\n   @Parameters(name = \"catalogName = {0}, implementation = {1}, config = {2}, format = {3}, vectorized = {4}\")\n   public static Object[][] parameters() {\n     return new Object[][] {\n-        { \"testhive\", SparkCatalog.class.getName(),\n-            ImmutableMap.of(\n-                \"type\", \"hive\",\n-                \"default-namespace\", \"default\"\n-            ),\n-            \"orc\",\n-            true\n-        },\n+//        { \"testhive\", SparkCatalog.class.getName(),\n+//            ImmutableMap.of(\n+//                \"type\", \"hive\",\n+//                \"default-namespace\", \"default\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d9213621701f7061dde785e821d588f0ab9020c9"}, "originalPosition": 15}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6ae2a917c7cd905420db715be5fc43e4eb1f057e", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/6ae2a917c7cd905420db715be5fc43e4eb1f057e", "committedDate": "2020-12-21T18:10:26Z", "message": "Spark: Sort retained rows in DELETE FROM by file and position."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2bb4199772e19967f58c7b86b2d95f34c67c05c1", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/2bb4199772e19967f58c7b86b2d95f34c67c05c1", "committedDate": "2020-12-21T18:10:27Z", "message": "Fix vectorized Parquet _file and _pos."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "545f5db1d3c7d4a0fbcb58ec5ba5f804a79dbab1", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/545f5db1d3c7d4a0fbcb58ec5ba5f804a79dbab1", "committedDate": "2020-12-21T18:10:27Z", "message": "Fix checkstyle."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "eb90c18be19583c5fbfb219c9b54cc28a7074a17", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/eb90c18be19583c5fbfb219c9b54cc28a7074a17", "committedDate": "2020-12-21T18:10:27Z", "message": "Fix test parameters from debugging."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5b3b8d83141bd637a48b2499519a2db1ad819ef0", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/5b3b8d83141bd637a48b2499519a2db1ad819ef0", "committedDate": "2020-12-21T18:10:27Z", "message": "Simplify statement in buildScanPlan."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "ffee55c574f390f684df30363ca4a4f777b9b886", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/ffee55c574f390f684df30363ca4a4f777b9b886", "committedDate": "2020-12-21T18:07:54Z", "message": "Fix test parameters from debugging."}, "afterCommit": {"oid": "5b3b8d83141bd637a48b2499519a2db1ad819ef0", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/5b3b8d83141bd637a48b2499519a2db1ad819ef0", "committedDate": "2020-12-21T18:10:27Z", "message": "Simplify statement in buildScanPlan."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ab4505c3aee81d005d2f48e5ef7b689ba80a834a", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/ab4505c3aee81d005d2f48e5ef7b689ba80a834a", "committedDate": "2020-12-21T19:28:50Z", "message": "Remove ExtendedScanRelation node."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU2NjM0NjE1", "url": "https://github.com/apache/iceberg/pull/1955#pullrequestreview-556634615", "createdAt": "2020-12-21T20:08:26Z", "commit": {"oid": "ab4505c3aee81d005d2f48e5ef7b689ba80a834a"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3289, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}