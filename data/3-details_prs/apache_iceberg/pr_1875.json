{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTMyNTM4OTQ4", "number": 1875, "title": "Allow Spark2 DataFrame to use a custom catalog", "bodyText": "This is the spark 2 equivalent of #1783 and enables Spark2 data frames to use a custom catalog by setting reader/writer options or via Spark conf.\nThe spark conf are in keeping with Spark3 catalog params and are prefixed by spark.sql.catalog.iceberg.. These parameters correspond to the settings needed for a custom catalog named iceberg in Spark3", "createdAt": "2020-12-04T13:36:05Z", "url": "https://github.com/apache/iceberg/pull/1875", "merged": true, "mergeCommit": {"oid": "6731211eaf746c6a4abfe71386ef53172a3fc137"}, "closed": true, "closedAt": "2020-12-28T23:05:35Z", "author": {"login": "rymurr"}, "timelineItems": {"totalCount": 40, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdi4tBNgBqjQwNzI5NjU3MTI=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdqp3k6ABqjQxNTI2NzMyNjE=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "545ec69fde7db0638031cc5a05344c19fbb1a2c9", "author": {"user": {"login": "rymurr", "name": "Ryan Murray"}}, "url": "https://github.com/apache/iceberg/commit/545ec69fde7db0638031cc5a05344c19fbb1a2c9", "committedDate": "2020-12-04T13:33:08Z", "message": "Allow Spark2 DataFrame to use a custom catalog"}, "afterCommit": {"oid": "b864ffb8a4bba4438bc2ae87b35b748509f1cb00", "author": {"user": {"login": "rymurr", "name": "Ryan Murray"}}, "url": "https://github.com/apache/iceberg/commit/b864ffb8a4bba4438bc2ae87b35b748509f1cb00", "committedDate": "2020-12-04T14:34:20Z", "message": "Allow Spark2 DataFrame to use a custom catalog"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTUwNTY3NjUx", "url": "https://github.com/apache/iceberg/pull/1875#pullrequestreview-550567651", "createdAt": "2020-12-11T20:48:55Z", "commit": {"oid": "6009447f49c5cbb3a389529f68f30db587bee75b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQyMDo0ODo1NVrOIEMukg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQyMDo0ODo1NVrOIEMukg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTI3Mzc0Ng==", "bodyText": "New paragraphs in Javadoc require <p>.", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r541273746", "createdAt": "2020-12-11T20:48:55Z", "author": {"login": "rdblue"}, "path": "spark2/src/main/java/org/apache/iceberg/spark/source/CustomCatalogs.java", "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import com.github.benmanes.caffeine.cache.Cache;\n+import com.github.benmanes.caffeine.cache.Caffeine;\n+import java.util.Arrays;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.iceberg.CatalogProperties;\n+import org.apache.iceberg.CatalogUtil;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.hive.HiveCatalogs;\n+import org.apache.iceberg.relocated.com.google.common.annotations.VisibleForTesting;\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.sql.SparkSession;\n+\n+public final class CustomCatalogs {\n+  private static final Cache<String, Catalog> CATALOG_CACHE = Caffeine.newBuilder().softValues().build();\n+\n+  public static final String ICEBERG_CATALOG_PREFIX = \"spark.sql.catalog.iceberg.\";\n+  public static final String ICEBERG_CATALOG_TYPE = \"type\";\n+  public static final String ICEBERG_CATALOG_TYPE_HADOOP = \"hadoop\";\n+  public static final String ICEBERG_CATALOG_TYPE_HIVE = \"hive\";\n+\n+  private CustomCatalogs() {\n+  }\n+\n+  /**\n+   * Build an Iceberg {@link Catalog} to be used by this Spark source adapter.\n+   *", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6009447f49c5cbb3a389529f68f30db587bee75b"}, "originalPosition": 52}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTUwNTY5MDEz", "url": "https://github.com/apache/iceberg/pull/1875#pullrequestreview-550569013", "createdAt": "2020-12-11T20:51:20Z", "commit": {"oid": "6009447f49c5cbb3a389529f68f30db587bee75b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQyMDo1MToyMFrOIEM3aw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQyMDo1MToyMFrOIEM3aw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTI3NjAxMQ==", "bodyText": "It looks like the options here are the read options. I don't think that it is necessary to pass any of the read options to create a catalog. In Spark 3, the catalogs exist and are configured using Spark config and the read options are independent.", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r541276011", "createdAt": "2020-12-11T20:51:20Z", "author": {"login": "rdblue"}, "path": "spark2/src/main/java/org/apache/iceberg/spark/source/CustomCatalogs.java", "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import com.github.benmanes.caffeine.cache.Cache;\n+import com.github.benmanes.caffeine.cache.Caffeine;\n+import java.util.Arrays;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.iceberg.CatalogProperties;\n+import org.apache.iceberg.CatalogUtil;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.hive.HiveCatalogs;\n+import org.apache.iceberg.relocated.com.google.common.annotations.VisibleForTesting;\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.sql.SparkSession;\n+\n+public final class CustomCatalogs {\n+  private static final Cache<String, Catalog> CATALOG_CACHE = Caffeine.newBuilder().softValues().build();\n+\n+  public static final String ICEBERG_CATALOG_PREFIX = \"spark.sql.catalog.iceberg.\";\n+  public static final String ICEBERG_CATALOG_TYPE = \"type\";\n+  public static final String ICEBERG_CATALOG_TYPE_HADOOP = \"hadoop\";\n+  public static final String ICEBERG_CATALOG_TYPE_HIVE = \"hive\";\n+\n+  private CustomCatalogs() {\n+  }\n+\n+  /**\n+   * Build an Iceberg {@link Catalog} to be used by this Spark source adapter.\n+   *\n+   * The cache is to facilitate reuse of catalogs, especially if wrapped in CachingCatalog. For non-Hive catalogs all\n+   * custom parameters passed to the catalog are considered in the cache key. Hive catalogs only cache based on\n+   * the Metastore URIs as per previous behaviour.\n+   *\n+   * @param options options from Spark\n+   * @return an Iceberg catalog\n+   */\n+  public static Catalog buildIcebergCatalog(Map<String, String> options) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6009447f49c5cbb3a389529f68f30db587bee75b"}, "originalPosition": 60}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTUwNTcwNTQz", "url": "https://github.com/apache/iceberg/pull/1875#pullrequestreview-550570543", "createdAt": "2020-12-11T20:53:54Z", "commit": {"oid": "6009447f49c5cbb3a389529f68f30db587bee75b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQyMDo1Mzo1NFrOIENAmw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQyMDo1Mzo1NFrOIENAmw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTI3ODM2Mw==", "bodyText": "I would expect the cache to delegate to buildIcebergCatalog, no the other way around. What about using a simple getter:\npublic static Catalog loadCatalog(SparkSession spark, String name) {\n  return CATALOG_CACHE.get(Pair.of(spark, name));\n}\nThen this just needs to build the named catalog for a particular Spark session. And that could be done by moving the existing SparkCatalog.buildIcebergCatalog into CatalogUtil and calling it with the right name and config from the session.", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r541278363", "createdAt": "2020-12-11T20:53:54Z", "author": {"login": "rdblue"}, "path": "spark2/src/main/java/org/apache/iceberg/spark/source/CustomCatalogs.java", "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import com.github.benmanes.caffeine.cache.Cache;\n+import com.github.benmanes.caffeine.cache.Caffeine;\n+import java.util.Arrays;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.iceberg.CatalogProperties;\n+import org.apache.iceberg.CatalogUtil;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.hive.HiveCatalogs;\n+import org.apache.iceberg.relocated.com.google.common.annotations.VisibleForTesting;\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.sql.SparkSession;\n+\n+public final class CustomCatalogs {\n+  private static final Cache<String, Catalog> CATALOG_CACHE = Caffeine.newBuilder().softValues().build();\n+\n+  public static final String ICEBERG_CATALOG_PREFIX = \"spark.sql.catalog.iceberg.\";\n+  public static final String ICEBERG_CATALOG_TYPE = \"type\";\n+  public static final String ICEBERG_CATALOG_TYPE_HADOOP = \"hadoop\";\n+  public static final String ICEBERG_CATALOG_TYPE_HIVE = \"hive\";\n+\n+  private CustomCatalogs() {\n+  }\n+\n+  /**\n+   * Build an Iceberg {@link Catalog} to be used by this Spark source adapter.\n+   *\n+   * The cache is to facilitate reuse of catalogs, especially if wrapped in CachingCatalog. For non-Hive catalogs all\n+   * custom parameters passed to the catalog are considered in the cache key. Hive catalogs only cache based on\n+   * the Metastore URIs as per previous behaviour.\n+   *\n+   * @param options options from Spark\n+   * @return an Iceberg catalog\n+   */\n+  public static Catalog buildIcebergCatalog(Map<String, String> options) {\n+    String name = \"spark_source\";\n+    SparkConf sparkConf = SparkSession.active().sparkContext().getConf();\n+    Map<String, String> sparkMap = Arrays.stream(sparkConf.getAllWithPrefix(ICEBERG_CATALOG_PREFIX))\n+        .collect(Collectors.toMap(x -> x._1, x -> x._2));\n+    sparkMap.putAll(options);\n+    Configuration conf = SparkSession.active().sessionState().newHadoopConf();\n+\n+    String catalogImpl = sparkMap.get(CatalogProperties.CATALOG_IMPL);\n+    if (catalogImpl != null) {\n+      String cacheKey = options.entrySet()\n+          .stream().map(x -> String.format(\"%s:%s\", x.getKey(), x.getValue())).collect(Collectors.joining(\";\"));\n+      return CATALOG_CACHE.get(cacheKey, x -> CatalogUtil.loadCatalog(catalogImpl, name, sparkMap, conf));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6009447f49c5cbb3a389529f68f30db587bee75b"}, "originalPosition": 72}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "7351747b74281b8d2f98c7a848e6c066602245b6", "author": {"user": {"login": "rymurr", "name": "Ryan Murray"}}, "url": "https://github.com/apache/iceberg/commit/7351747b74281b8d2f98c7a848e6c066602245b6", "committedDate": "2020-12-14T18:57:11Z", "message": "address code review comments and more code reuse"}, "afterCommit": {"oid": "8a267310cb01236ce0335c4c82d8d22add1f3335", "author": {"user": {"login": "rymurr", "name": "Ryan Murray"}}, "url": "https://github.com/apache/iceberg/commit/8a267310cb01236ce0335c4c82d8d22add1f3335", "committedDate": "2020-12-15T16:27:17Z", "message": "clarify catalog util and fix tests"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTUyNjcxMTE3", "url": "https://github.com/apache/iceberg/pull/1875#pullrequestreview-552671117", "createdAt": "2020-12-15T16:59:53Z", "commit": {"oid": "8a267310cb01236ce0335c4c82d8d22add1f3335"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxNjo1OTo1M1rOIGV1UA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxNjo1OTo1M1rOIGV1UA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzUyMDA4MA==", "bodyText": "This works, but I think it would be better to make it so that the Hive catalog can be loaded using the normal no-arg constructor followed by initialize and setConf.", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r543520080", "createdAt": "2020-12-15T16:59:53Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/CatalogUtil.java", "diffHunk": "@@ -169,6 +174,40 @@ public static Catalog loadCatalog(\n     return catalog;\n   }\n \n+  public static Catalog buildIcebergCatalog(String name, Map<String, String> options, Configuration conf) {\n+\n+    String catalogImpl = options.get(CatalogProperties.CATALOG_IMPL);\n+    if (catalogImpl != null) {\n+      return CatalogUtil.loadCatalog(catalogImpl, name, options, conf);\n+    }\n+\n+    String catalogType = options.getOrDefault(ICEBERG_CATALOG_TYPE, ICEBERG_CATALOG_TYPE_HIVE);\n+    switch (catalogType.toLowerCase(Locale.ENGLISH)) {\n+      case ICEBERG_CATALOG_TYPE_HIVE:\n+        String clientPoolSize = options.getOrDefault(CatalogProperties.HIVE_CLIENT_POOL_SIZE,\n+            Integer.toString(CatalogProperties.HIVE_CLIENT_POOL_SIZE_DEFAULT));\n+        String uri = options.get(CatalogProperties.HIVE_URI);\n+        return buildHiveCatalog(name, uri, Integer.parseInt(clientPoolSize), conf);\n+      case ICEBERG_CATALOG_TYPE_HADOOP:\n+        String warehouseLocation = options.get(CatalogProperties.WAREHOUSE_LOCATION);\n+        return new HadoopCatalog(name, conf, warehouseLocation, options);\n+\n+      default:\n+        throw new UnsupportedOperationException(\"Unknown catalog type: \" + catalogType);\n+    }\n+  }\n+\n+  private static Catalog buildHiveCatalog(String name, String uri, int clientPoolSize, Configuration conf) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8a267310cb01236ce0335c4c82d8d22add1f3335"}, "originalPosition": 55}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTUyNjcyMDI2", "url": "https://github.com/apache/iceberg/pull/1875#pullrequestreview-552672026", "createdAt": "2020-12-15T17:00:47Z", "commit": {"oid": "8a267310cb01236ce0335c4c82d8d22add1f3335"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxNzowMDo0OFrOIGV4RA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxNzowMDo0OFrOIGV4RA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzUyMDgzNg==", "bodyText": "Nit: indentation.", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r543520836", "createdAt": "2020-12-15T17:00:48Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java", "diffHunk": "@@ -61,4 +65,40 @@ public static void validatePartitionTransforms(PartitionSpec spec) {\n           String.format(\"Cannot write using unsupported transforms: %s\", unsupported));\n     }\n   }\n+\n+  /**\n+   * A modified version of Spark's LookupCatalog.CatalogAndIdentifier.unapply\n+   * Attempts to find the catalog and identifier a multipart identifier represents\n+   * @param nameParts Multipart identifier representing a table\n+   * @return The CatalogPlugin and Identifier for the table\n+   */\n+  public static <C, T> Pair<C,T> catalogAndIdentifier(List<String> nameParts,\n+                                                       Function<String, C> catalog,\n+                                                       IdentiferFunction<T> identifer,\n+                                                       String[] currentNamespace) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8a267310cb01236ce0335c4c82d8d22add1f3335"}, "originalPosition": 33}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTUyNjc5NTQ2", "url": "https://github.com/apache/iceberg/pull/1875#pullrequestreview-552679546", "createdAt": "2020-12-15T17:08:44Z", "commit": {"oid": "8a267310cb01236ce0335c4c82d8d22add1f3335"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxNzowODo0NFrOIGWQSw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxNzowODo0NFrOIGWQSw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzUyNjk4Nw==", "bodyText": "This seems like a concern for CatalogUtil.buildIcebergCatalog, not here. This should create the map and pass it on to allow that method to reject the options. That way, the user gets a more specific error, like \"Missing catalog implementation class or type\". That can also create a default catalog if we choose to later -- though I'm skeptical we would -- without needing to change all of the callers.", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r543526987", "createdAt": "2020-12-15T17:08:44Z", "author": {"login": "rdblue"}, "path": "spark2/src/main/java/org/apache/iceberg/spark/source/CustomCatalogs.java", "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import com.github.benmanes.caffeine.cache.Cache;\n+import com.github.benmanes.caffeine.cache.Caffeine;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.CatalogUtil;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.Namespace;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.relocated.com.google.common.annotations.VisibleForTesting;\n+import org.apache.iceberg.relocated.com.google.common.base.Splitter;\n+import org.apache.iceberg.spark.SparkUtil;\n+import org.apache.iceberg.util.Pair;\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.sql.SparkSession;\n+\n+public final class CustomCatalogs {\n+  private static final Cache<Pair<SparkSession, String>, Catalog> CATALOG_CACHE = Caffeine.newBuilder().softValues().build();\n+\n+  public static final String ICEBERG_DEFAULT_CATALOG = \"default_catalog\";\n+  public static final String ICEBERG_CATALOG_PREFIX = \"spark.sql.catalog\";\n+\n+  private CustomCatalogs() {\n+  }\n+\n+  /**\n+   * Build an Iceberg {@link Catalog} to be used by this Spark source adapter.\n+   *\n+   * <p>\n+   *   The cache is to facilitate reuse of catalogs, especially if wrapped in CachingCatalog. For non-Hive catalogs all\n+   *   custom parameters passed to the catalog are considered in the cache key. Hive catalogs only cache based on\n+   *   the Metastore URIs as per previous behaviour.\n+   * </p>\n+   *\n+   * @param spark Spark Session\n+   * @param name Catalog Name\n+   * @return an Iceberg catalog\n+   */\n+  public static Catalog buildIcebergCatalog(SparkSession spark, String name) {\n+    return CATALOG_CACHE.get(Pair.of(spark, name), CustomCatalogs::build);\n+  }\n+\n+  private static Catalog build(Pair<SparkSession, String> sparkAndName) {\n+    SparkSession spark = sparkAndName.first();\n+    String name =  sparkAndName.second() == null ? ICEBERG_DEFAULT_CATALOG : sparkAndName.second();\n+    SparkConf sparkConf = spark.sparkContext().getConf();\n+    Configuration conf = spark.sessionState().newHadoopConf();\n+\n+    String catalogPrefix = String.format(\"%s.%s.\", ICEBERG_CATALOG_PREFIX, name);\n+    Map<String, String> options = Arrays.stream(sparkConf.getAllWithPrefix(catalogPrefix))\n+        .collect(Collectors.toMap(x -> x._1, x -> x._2));\n+\n+    if (options.isEmpty() && !name.equals(ICEBERG_DEFAULT_CATALOG)) {\n+      throw new IllegalArgumentException(String.format(\"Cannot instantiate catalog %s. Incorrect Parameters\", name));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8a267310cb01236ce0335c4c82d8d22add1f3335"}, "originalPosition": 78}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTUyNjgwNzc4", "url": "https://github.com/apache/iceberg/pull/1875#pullrequestreview-552680778", "createdAt": "2020-12-15T17:10:07Z", "commit": {"oid": "8a267310cb01236ce0335c4c82d8d22add1f3335"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxNzoxMDowN1rOIGWUQg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxNzoxMDowN1rOIGWUQg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzUyODAwMg==", "bodyText": "This isn't really a \"build\" method any more since it wraps the cache, it is more of a \"load\" I think.", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r543528002", "createdAt": "2020-12-15T17:10:07Z", "author": {"login": "rdblue"}, "path": "spark2/src/main/java/org/apache/iceberg/spark/source/CustomCatalogs.java", "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import com.github.benmanes.caffeine.cache.Cache;\n+import com.github.benmanes.caffeine.cache.Caffeine;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.CatalogUtil;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.Namespace;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.relocated.com.google.common.annotations.VisibleForTesting;\n+import org.apache.iceberg.relocated.com.google.common.base.Splitter;\n+import org.apache.iceberg.spark.SparkUtil;\n+import org.apache.iceberg.util.Pair;\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.sql.SparkSession;\n+\n+public final class CustomCatalogs {\n+  private static final Cache<Pair<SparkSession, String>, Catalog> CATALOG_CACHE = Caffeine.newBuilder().softValues().build();\n+\n+  public static final String ICEBERG_DEFAULT_CATALOG = \"default_catalog\";\n+  public static final String ICEBERG_CATALOG_PREFIX = \"spark.sql.catalog\";\n+\n+  private CustomCatalogs() {\n+  }\n+\n+  /**\n+   * Build an Iceberg {@link Catalog} to be used by this Spark source adapter.\n+   *\n+   * <p>\n+   *   The cache is to facilitate reuse of catalogs, especially if wrapped in CachingCatalog. For non-Hive catalogs all\n+   *   custom parameters passed to the catalog are considered in the cache key. Hive catalogs only cache based on\n+   *   the Metastore URIs as per previous behaviour.\n+   * </p>\n+   *\n+   * @param spark Spark Session\n+   * @param name Catalog Name\n+   * @return an Iceberg catalog\n+   */\n+  public static Catalog buildIcebergCatalog(SparkSession spark, String name) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8a267310cb01236ce0335c4c82d8d22add1f3335"}, "originalPosition": 63}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTUyNjgxODgw", "url": "https://github.com/apache/iceberg/pull/1875#pullrequestreview-552681880", "createdAt": "2020-12-15T17:11:19Z", "commit": {"oid": "8a267310cb01236ce0335c4c82d8d22add1f3335"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxNzoxMToxOVrOIGWX3A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxNzoxMToxOVrOIGWX3A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzUyODkyNA==", "bodyText": "Looks like some imports might be stale.", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r543528924", "createdAt": "2020-12-15T17:11:19Z", "author": {"login": "rdblue"}, "path": "spark2/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java", "diffHunk": "@@ -24,16 +24,16 @@\n import org.apache.hadoop.conf.Configuration;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.Catalog;\n import org.apache.iceberg.catalog.TableIdentifier;\n import org.apache.iceberg.encryption.EncryptionManager;\n import org.apache.iceberg.hadoop.HadoopTables;\n-import org.apache.iceberg.hive.HiveCatalog;\n-import org.apache.iceberg.hive.HiveCatalogs;\n import org.apache.iceberg.io.FileIO;\n import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n import org.apache.iceberg.spark.SparkSchemaUtil;\n import org.apache.iceberg.spark.SparkUtil;\n import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.Pair;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8a267310cb01236ce0335c4c82d8d22add1f3335"}, "originalPosition": 15}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "41e4890b48c870abaa5dcebe07d2eba9fc39b53b", "author": {"user": {"login": "rymurr", "name": "Ryan Murray"}}, "url": "https://github.com/apache/iceberg/commit/41e4890b48c870abaa5dcebe07d2eba9fc39b53b", "committedDate": "2020-12-15T20:50:34Z", "message": "refactor and clean-up"}, "afterCommit": {"oid": "dc8baca0a6e3919360a5eefc517080967f13e04b", "author": {"user": {"login": "rymurr", "name": "Ryan Murray"}}, "url": "https://github.com/apache/iceberg/commit/dc8baca0a6e3919360a5eefc517080967f13e04b", "committedDate": "2020-12-15T20:52:21Z", "message": "refactor and clean-up"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU3Mjg0MzU2", "url": "https://github.com/apache/iceberg/pull/1875#pullrequestreview-557284356", "createdAt": "2020-12-22T18:32:34Z", "commit": {"oid": "25b2f918c96611af21de42c5a47cf8fa3b824ae2"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxODozMjozNFrOIKFAEQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxODozMjozNFrOIKFAEQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQzODYwOQ==", "bodyText": "This adds properties to the map twice?\nWe usually prefer Maps.newHashMap(), too.", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r547438609", "createdAt": "2020-12-22T18:32:34Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/hadoop/HadoopCatalog.java", "diffHunk": "@@ -68,45 +70,62 @@\n  *\n  * Note: The HadoopCatalog requires that the underlying file system supports atomic rename.\n  */\n-public class HadoopCatalog extends BaseMetastoreCatalog implements Closeable, SupportsNamespaces {\n+public class HadoopCatalog extends BaseMetastoreCatalog implements Closeable, SupportsNamespaces, Configurable {\n \n   private static final String ICEBERG_HADOOP_WAREHOUSE_BASE = \"iceberg/warehouse\";\n   private static final String TABLE_METADATA_FILE_EXTENSION = \".metadata.json\";\n   private static final Joiner SLASH = Joiner.on(\"/\");\n   private static final PathFilter TABLE_FILTER = path -> path.getName().endsWith(TABLE_METADATA_FILE_EXTENSION);\n \n-  private final String catalogName;\n-  private final Configuration conf;\n-  private final String warehouseLocation;\n-  private final FileSystem fs;\n-  private final FileIO fileIO;\n+  private String catalogName;\n+  private Configuration conf;\n+  private String warehouseLocation;\n+  private FileSystem fs;\n+  private FileIO fileIO;\n+\n+  public HadoopCatalog(){\n+  }\n \n   /**\n    * The constructor of the HadoopCatalog. It uses the passed location as its warehouse directory.\n    *\n+   * @deprecated please use the no-arg constructor, setConf and initialize to construct the catalog\n    * @param name The catalog name\n    * @param conf The Hadoop configuration\n    * @param warehouseLocation The location used as warehouse directory\n    */\n+  @Deprecated\n   public HadoopCatalog(String name, Configuration conf, String warehouseLocation) {\n     this(name, conf, warehouseLocation, Maps.newHashMap());\n   }\n \n   /**\n    * The all-arg constructor of the HadoopCatalog.\n    *\n+   * @deprecated please use the no-arg constructor, setConf and initialize to construct the catalog\n    * @param name The catalog name\n    * @param conf The Hadoop configuration\n    * @param warehouseLocation The location used as warehouse directory\n    * @param properties catalog properties\n    */\n+  @Deprecated\n   public HadoopCatalog(String name, Configuration conf, String warehouseLocation, Map<String, String> properties) {\n     Preconditions.checkArgument(warehouseLocation != null && !warehouseLocation.equals(\"\"),\n         \"no location provided for warehouse\");\n+    setConf(conf);\n+    Map<String, String> props = new HashMap<>(properties);\n+    props.putAll(properties);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "25b2f918c96611af21de42c5a47cf8fa3b824ae2"}, "originalPosition": 68}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU3Mjg1NjYx", "url": "https://github.com/apache/iceberg/pull/1875#pullrequestreview-557285661", "createdAt": "2020-12-22T18:34:53Z", "commit": {"oid": "25b2f918c96611af21de42c5a47cf8fa3b824ae2"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxODozNDo1M1rOIKFEIQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxODozNDo1M1rOIKFEIQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQzOTY0OQ==", "bodyText": "Nit: Error messages should use sentence case, with the first word capitalized. I'd also make it more clear that warehouse is the configuration key to set, like \"Cannot create Hadoop catalog without 'warehouse' location\".", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r547439649", "createdAt": "2020-12-22T18:34:53Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/hadoop/HadoopCatalog.java", "diffHunk": "@@ -68,45 +70,62 @@\n  *\n  * Note: The HadoopCatalog requires that the underlying file system supports atomic rename.\n  */\n-public class HadoopCatalog extends BaseMetastoreCatalog implements Closeable, SupportsNamespaces {\n+public class HadoopCatalog extends BaseMetastoreCatalog implements Closeable, SupportsNamespaces, Configurable {\n \n   private static final String ICEBERG_HADOOP_WAREHOUSE_BASE = \"iceberg/warehouse\";\n   private static final String TABLE_METADATA_FILE_EXTENSION = \".metadata.json\";\n   private static final Joiner SLASH = Joiner.on(\"/\");\n   private static final PathFilter TABLE_FILTER = path -> path.getName().endsWith(TABLE_METADATA_FILE_EXTENSION);\n \n-  private final String catalogName;\n-  private final Configuration conf;\n-  private final String warehouseLocation;\n-  private final FileSystem fs;\n-  private final FileIO fileIO;\n+  private String catalogName;\n+  private Configuration conf;\n+  private String warehouseLocation;\n+  private FileSystem fs;\n+  private FileIO fileIO;\n+\n+  public HadoopCatalog(){\n+  }\n \n   /**\n    * The constructor of the HadoopCatalog. It uses the passed location as its warehouse directory.\n    *\n+   * @deprecated please use the no-arg constructor, setConf and initialize to construct the catalog\n    * @param name The catalog name\n    * @param conf The Hadoop configuration\n    * @param warehouseLocation The location used as warehouse directory\n    */\n+  @Deprecated\n   public HadoopCatalog(String name, Configuration conf, String warehouseLocation) {\n     this(name, conf, warehouseLocation, Maps.newHashMap());\n   }\n \n   /**\n    * The all-arg constructor of the HadoopCatalog.\n    *\n+   * @deprecated please use the no-arg constructor, setConf and initialize to construct the catalog\n    * @param name The catalog name\n    * @param conf The Hadoop configuration\n    * @param warehouseLocation The location used as warehouse directory\n    * @param properties catalog properties\n    */\n+  @Deprecated\n   public HadoopCatalog(String name, Configuration conf, String warehouseLocation, Map<String, String> properties) {\n     Preconditions.checkArgument(warehouseLocation != null && !warehouseLocation.equals(\"\"),\n         \"no location provided for warehouse\");\n+    setConf(conf);\n+    Map<String, String> props = new HashMap<>(properties);\n+    props.putAll(properties);\n+    props.put(CatalogProperties.WAREHOUSE_LOCATION, warehouseLocation);\n+    initialize(name, props);\n+  }\n \n+  @Override\n+  public void initialize(String name, Map<String, String> properties) {\n+    String inputWarehouseLocation = properties.get(CatalogProperties.WAREHOUSE_LOCATION);\n+    Preconditions.checkArgument(inputWarehouseLocation != null && !inputWarehouseLocation.equals(\"\"),\n+        \"no location provided for warehouse\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "25b2f918c96611af21de42c5a47cf8fa3b824ae2"}, "originalPosition": 77}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU3Mjg2MjA5", "url": "https://github.com/apache/iceberg/pull/1875#pullrequestreview-557286209", "createdAt": "2020-12-22T18:35:58Z", "commit": {"oid": "25b2f918c96611af21de42c5a47cf8fa3b824ae2"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxODozNTo1OFrOIKFF8g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxODozNTo1OFrOIKFF8g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQ0MDExNA==", "bodyText": "Can you call this last? I think it is typically set after calling initialize. The important thing is to try to use the same order.", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r547440114", "createdAt": "2020-12-22T18:35:58Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/hadoop/HadoopCatalog.java", "diffHunk": "@@ -68,45 +70,62 @@\n  *\n  * Note: The HadoopCatalog requires that the underlying file system supports atomic rename.\n  */\n-public class HadoopCatalog extends BaseMetastoreCatalog implements Closeable, SupportsNamespaces {\n+public class HadoopCatalog extends BaseMetastoreCatalog implements Closeable, SupportsNamespaces, Configurable {\n \n   private static final String ICEBERG_HADOOP_WAREHOUSE_BASE = \"iceberg/warehouse\";\n   private static final String TABLE_METADATA_FILE_EXTENSION = \".metadata.json\";\n   private static final Joiner SLASH = Joiner.on(\"/\");\n   private static final PathFilter TABLE_FILTER = path -> path.getName().endsWith(TABLE_METADATA_FILE_EXTENSION);\n \n-  private final String catalogName;\n-  private final Configuration conf;\n-  private final String warehouseLocation;\n-  private final FileSystem fs;\n-  private final FileIO fileIO;\n+  private String catalogName;\n+  private Configuration conf;\n+  private String warehouseLocation;\n+  private FileSystem fs;\n+  private FileIO fileIO;\n+\n+  public HadoopCatalog(){\n+  }\n \n   /**\n    * The constructor of the HadoopCatalog. It uses the passed location as its warehouse directory.\n    *\n+   * @deprecated please use the no-arg constructor, setConf and initialize to construct the catalog\n    * @param name The catalog name\n    * @param conf The Hadoop configuration\n    * @param warehouseLocation The location used as warehouse directory\n    */\n+  @Deprecated\n   public HadoopCatalog(String name, Configuration conf, String warehouseLocation) {\n     this(name, conf, warehouseLocation, Maps.newHashMap());\n   }\n \n   /**\n    * The all-arg constructor of the HadoopCatalog.\n    *\n+   * @deprecated please use the no-arg constructor, setConf and initialize to construct the catalog\n    * @param name The catalog name\n    * @param conf The Hadoop configuration\n    * @param warehouseLocation The location used as warehouse directory\n    * @param properties catalog properties\n    */\n+  @Deprecated\n   public HadoopCatalog(String name, Configuration conf, String warehouseLocation, Map<String, String> properties) {\n     Preconditions.checkArgument(warehouseLocation != null && !warehouseLocation.equals(\"\"),\n         \"no location provided for warehouse\");\n+    setConf(conf);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "25b2f918c96611af21de42c5a47cf8fa3b824ae2"}, "originalPosition": 66}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU3Mjg2NDkw", "url": "https://github.com/apache/iceberg/pull/1875#pullrequestreview-557286490", "createdAt": "2020-12-22T18:36:29Z", "commit": {"oid": "25b2f918c96611af21de42c5a47cf8fa3b824ae2"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxODozNjoyOVrOIKFG3Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxODozNjoyOVrOIKFG3Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQ0MDM0OQ==", "bodyText": "Can you also add when this will be removed? Like \"will be removed in 0.12.0\".", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r547440349", "createdAt": "2020-12-22T18:36:29Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/hadoop/HadoopCatalog.java", "diffHunk": "@@ -68,45 +70,62 @@\n  *\n  * Note: The HadoopCatalog requires that the underlying file system supports atomic rename.\n  */\n-public class HadoopCatalog extends BaseMetastoreCatalog implements Closeable, SupportsNamespaces {\n+public class HadoopCatalog extends BaseMetastoreCatalog implements Closeable, SupportsNamespaces, Configurable {\n \n   private static final String ICEBERG_HADOOP_WAREHOUSE_BASE = \"iceberg/warehouse\";\n   private static final String TABLE_METADATA_FILE_EXTENSION = \".metadata.json\";\n   private static final Joiner SLASH = Joiner.on(\"/\");\n   private static final PathFilter TABLE_FILTER = path -> path.getName().endsWith(TABLE_METADATA_FILE_EXTENSION);\n \n-  private final String catalogName;\n-  private final Configuration conf;\n-  private final String warehouseLocation;\n-  private final FileSystem fs;\n-  private final FileIO fileIO;\n+  private String catalogName;\n+  private Configuration conf;\n+  private String warehouseLocation;\n+  private FileSystem fs;\n+  private FileIO fileIO;\n+\n+  public HadoopCatalog(){\n+  }\n \n   /**\n    * The constructor of the HadoopCatalog. It uses the passed location as its warehouse directory.\n    *\n+   * @deprecated please use the no-arg constructor, setConf and initialize to construct the catalog\n    * @param name The catalog name\n    * @param conf The Hadoop configuration\n    * @param warehouseLocation The location used as warehouse directory\n    */\n+  @Deprecated\n   public HadoopCatalog(String name, Configuration conf, String warehouseLocation) {\n     this(name, conf, warehouseLocation, Maps.newHashMap());\n   }\n \n   /**\n    * The all-arg constructor of the HadoopCatalog.\n    *\n+   * @deprecated please use the no-arg constructor, setConf and initialize to construct the catalog", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "25b2f918c96611af21de42c5a47cf8fa3b824ae2"}, "originalPosition": 56}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU3MjkyMjA1", "url": "https://github.com/apache/iceberg/pull/1875#pullrequestreview-557292205", "createdAt": "2020-12-22T18:47:10Z", "commit": {"oid": "25b2f918c96611af21de42c5a47cf8fa3b824ae2"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxODo0NzoxMFrOIKFZXQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxODo0NzoxMFrOIKFZXQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQ0NTA4NQ==", "bodyText": "Looks like this newline should be above the return to separate it from the control flow.", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r547445085", "createdAt": "2020-12-22T18:47:10Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/CatalogUtil.java", "diffHunk": "@@ -169,6 +175,26 @@ public static Catalog loadCatalog(\n     return catalog;\n   }\n \n+  public static Catalog buildIcebergCatalog(String name, Map<String, String> options, Configuration conf) {\n+\n+    String catalogImpl = options.get(CatalogProperties.CATALOG_IMPL);\n+    if (catalogImpl == null) {\n+      String catalogType = options.getOrDefault(ICEBERG_CATALOG_TYPE, ICEBERG_CATALOG_TYPE_HIVE);\n+      switch (catalogType.toLowerCase(Locale.ENGLISH)) {\n+        case ICEBERG_CATALOG_TYPE_HIVE:\n+          catalogImpl = ICEBERG_CATALOG_HIVE;\n+          break;\n+        case ICEBERG_CATALOG_TYPE_HADOOP:\n+          catalogImpl = ICEBERG_CATALOG_HADOOP;\n+          break;\n+        default:\n+          throw new UnsupportedOperationException(\"Unknown catalog type: \" + catalogType);\n+      }\n+    }\n+    return CatalogUtil.loadCatalog(catalogImpl, name, options, conf);\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "25b2f918c96611af21de42c5a47cf8fa3b824ae2"}, "originalPosition": 41}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU3MjkyMjc0", "url": "https://github.com/apache/iceberg/pull/1875#pullrequestreview-557292274", "createdAt": "2020-12-22T18:47:17Z", "commit": {"oid": "25b2f918c96611af21de42c5a47cf8fa3b824ae2"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxODo0NzoxOFrOIKFZiQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxODo0NzoxOFrOIKFZiQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQ0NTEyOQ==", "bodyText": "No need to start methods with a newline.", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r547445129", "createdAt": "2020-12-22T18:47:18Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/CatalogUtil.java", "diffHunk": "@@ -169,6 +175,26 @@ public static Catalog loadCatalog(\n     return catalog;\n   }\n \n+  public static Catalog buildIcebergCatalog(String name, Map<String, String> options, Configuration conf) {\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "25b2f918c96611af21de42c5a47cf8fa3b824ae2"}, "originalPosition": 25}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU3MjkzNDY1", "url": "https://github.com/apache/iceberg/pull/1875#pullrequestreview-557293465", "createdAt": "2020-12-22T18:49:28Z", "commit": {"oid": "25b2f918c96611af21de42c5a47cf8fa3b824ae2"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxODo0OToyOFrOIKFdcA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxODo0OToyOFrOIKFdcA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQ0NjEyOA==", "bodyText": "Rather than try/catch, I think this should check whether catalog.apply returns null. If the result is null, then the catalog does not exist and it should not be set in the pair (set null). Then the caller can fill in the default catalog.", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r547446128", "createdAt": "2020-12-22T18:49:28Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java", "diffHunk": "@@ -61,4 +65,40 @@ public static void validatePartitionTransforms(PartitionSpec spec) {\n           String.format(\"Cannot write using unsupported transforms: %s\", unsupported));\n     }\n   }\n+\n+  /**\n+   * A modified version of Spark's LookupCatalog.CatalogAndIdentifier.unapply\n+   * Attempts to find the catalog and identifier a multipart identifier represents\n+   * @param nameParts Multipart identifier representing a table\n+   * @return The CatalogPlugin and Identifier for the table\n+   */\n+  public static <C, T> Pair<C, T> catalogAndIdentifier(List<String> nameParts,\n+                                                       Function<String, C> catalog,\n+                                                       IdentiferFunction<T> identifer,\n+                                                       String[] currentNamespace) {\n+    Preconditions.checkArgument(!nameParts.isEmpty(),\n+        \"Cannot determine catalog and Identifier from empty name parts\");\n+\n+    int lastElementIndex = nameParts.size() - 1;\n+    String name = nameParts.get(lastElementIndex);\n+\n+    if (nameParts.size() == 1) {\n+      // Only a single element, use current catalog and namespace\n+      return Pair.of(catalog.apply(null), identifer.of(currentNamespace, name));\n+    } else {\n+      try {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "25b2f918c96611af21de42c5a47cf8fa3b824ae2"}, "originalPosition": 44}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU3Mjk1NDk4", "url": "https://github.com/apache/iceberg/pull/1875#pullrequestreview-557295498", "createdAt": "2020-12-22T18:53:07Z", "commit": {"oid": "25b2f918c96611af21de42c5a47cf8fa3b824ae2"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxODo1MzowN1rOIKFj3g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxODo1MzowN1rOIKFj3g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQ0Nzc3NA==", "bodyText": "I think that the logic here should be identical to the Spark 3 case, but with the catalog load function and identifier construction replaced. That doesn't appear to be what is done because catalog.apply is used when the catalog is not set (1 part name).", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r547447774", "createdAt": "2020-12-22T18:53:07Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java", "diffHunk": "@@ -61,4 +65,40 @@ public static void validatePartitionTransforms(PartitionSpec spec) {\n           String.format(\"Cannot write using unsupported transforms: %s\", unsupported));\n     }\n   }\n+\n+  /**\n+   * A modified version of Spark's LookupCatalog.CatalogAndIdentifier.unapply\n+   * Attempts to find the catalog and identifier a multipart identifier represents\n+   * @param nameParts Multipart identifier representing a table\n+   * @return The CatalogPlugin and Identifier for the table\n+   */\n+  public static <C, T> Pair<C, T> catalogAndIdentifier(List<String> nameParts,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "25b2f918c96611af21de42c5a47cf8fa3b824ae2"}, "originalPosition": 30}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU3Mjk2ODYx", "url": "https://github.com/apache/iceberg/pull/1875#pullrequestreview-557296861", "createdAt": "2020-12-22T18:55:39Z", "commit": {"oid": "25b2f918c96611af21de42c5a47cf8fa3b824ae2"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxODo1NTo0MFrOIKFoMQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxODo1NTo0MFrOIKFoMQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQ0ODg4MQ==", "bodyText": "Can you use the variant of this that checks the exception message?", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r547448881", "createdAt": "2020-12-22T18:55:40Z", "author": {"login": "rdblue"}, "path": "spark2/src/test/java/org/apache/iceberg/spark/source/TestCustomCatalog.java", "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import java.io.File;\n+import java.util.List;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.CatalogProperties;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.junit.After;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+\n+public class TestCustomCatalog {\n+  private static final String CATALOG_IMPL = String.format(\"%s.%s.%s\", CustomCatalogs.ICEBERG_CATALOG_PREFIX,\n+      CustomCatalogs.ICEBERG_DEFAULT_CATALOG, CatalogProperties.CATALOG_IMPL);\n+  private static final String WAREHOUSE = String.format(\"%s.%s.%s\", CustomCatalogs.ICEBERG_CATALOG_PREFIX,\n+      CustomCatalogs.ICEBERG_DEFAULT_CATALOG, CatalogProperties.WAREHOUSE_LOCATION);\n+  private static final String URI_KEY = String.format(\"%s.%s.%s\", CustomCatalogs.ICEBERG_CATALOG_PREFIX,\n+      CustomCatalogs.ICEBERG_DEFAULT_CATALOG, CatalogProperties.HIVE_URI);\n+  private static final String URI_VAL = \"thrift://localhost:12345\"; // dummy uri\n+  private static final String CATALOG_VAL = \"org.apache.iceberg.spark.source.TestCatalog\";\n+  private static final TableIdentifier TABLE = TableIdentifier.of(\"default\", \"table\");\n+  private static final Schema SCHEMA = new Schema(\n+      optional(1, \"id\", Types.IntegerType.get()),\n+      optional(2, \"data\", Types.StringType.get())\n+  );\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  File tableDir = null;\n+  String tableLocation = null;\n+  HadoopTables tables;\n+\n+  protected static SparkSession spark = null;\n+\n+  @BeforeClass\n+  public static void startMetastoreAndSpark() {\n+    spark = SparkSession.builder().master(\"local[2]\").getOrCreate();\n+  }\n+\n+  @AfterClass\n+  public static void stopMetastoreAndSpark() {\n+    spark.stop();\n+    spark = null;\n+  }\n+\n+  @Before\n+  public void setupTable() throws Exception {\n+    this.tables = new HadoopTables(spark.sessionState().newHadoopConf());\n+    this.tableDir = temp.newFolder();\n+    tableDir.delete(); // created by table create\n+    this.tableLocation = tableDir.toURI().toString();\n+    tables.create(SCHEMA, PartitionSpec.unpartitioned(), String.format(\"%s/%s\", tableLocation, TABLE.name()));\n+  }\n+\n+  @After\n+  public void removeTable() {\n+    SparkConf sparkConf = spark.sparkContext().conf();\n+    sparkConf.remove(CATALOG_IMPL);\n+    sparkConf.remove(WAREHOUSE);\n+    sparkConf.remove(URI_KEY);\n+    tables.dropTable(String.format(\"%s/%s\", tableLocation, TABLE.name()));\n+    tableDir.delete();\n+    CustomCatalogs.clearCache();\n+  }\n+\n+  @Test\n+  public void withSparkOptions() {\n+\n+    SparkConf sparkConf = spark.sparkContext().conf();\n+    sparkConf.set(CATALOG_IMPL, CATALOG_VAL);\n+    sparkConf.set(URI_KEY, URI_VAL);\n+\n+    List<SimpleRecord> expected = Lists.newArrayList(\n+        new SimpleRecord(1, \"a\"),\n+        new SimpleRecord(2, \"b\"),\n+        new SimpleRecord(3, \"c\")\n+    );\n+\n+    Dataset<Row> df = spark.createDataFrame(expected, SimpleRecord.class);\n+    AssertHelpers.assertThrows(\"We have not set all properties\", IllegalArgumentException.class, () ->", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "25b2f918c96611af21de42c5a47cf8fa3b824ae2"}, "originalPosition": 117}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU3Mjk3ODMz", "url": "https://github.com/apache/iceberg/pull/1875#pullrequestreview-557297833", "createdAt": "2020-12-22T18:57:21Z", "commit": {"oid": "25b2f918c96611af21de42c5a47cf8fa3b824ae2"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxODo1NzoyMVrOIKFreA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxODo1NzoyMVrOIKFreA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQ0OTcyMA==", "bodyText": "This makes config properties case sensitive. Should we convert to a Java case insensitive map?", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r547449720", "createdAt": "2020-12-22T18:57:21Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/SparkCatalog.java", "diffHunk": "@@ -101,27 +94,7 @@\n    */\n   protected Catalog buildIcebergCatalog(String name, CaseInsensitiveStringMap options) {\n     Configuration conf = SparkSession.active().sessionState().newHadoopConf();\n-\n-    String catalogImpl = options.get(CatalogProperties.CATALOG_IMPL);\n-    if (catalogImpl != null) {\n-      return CatalogUtil.loadCatalog(catalogImpl, name, options, conf);\n-    }\n-\n-    String catalogType = options.getOrDefault(ICEBERG_CATALOG_TYPE, ICEBERG_CATALOG_TYPE_HIVE);\n-    switch (catalogType.toLowerCase(Locale.ENGLISH)) {\n-      case ICEBERG_CATALOG_TYPE_HIVE:\n-        int clientPoolSize = options.getInt(CatalogProperties.HIVE_CLIENT_POOL_SIZE,\n-            CatalogProperties.HIVE_CLIENT_POOL_SIZE_DEFAULT);\n-        String uri = options.get(CatalogProperties.HIVE_URI);\n-        return new HiveCatalog(name, uri, clientPoolSize, conf);\n-\n-      case ICEBERG_CATALOG_TYPE_HADOOP:\n-        String warehouseLocation = options.get(CatalogProperties.WAREHOUSE_LOCATION);\n-        return new HadoopCatalog(name, conf, warehouseLocation, options.asCaseSensitiveMap());\n-\n-      default:\n-        throw new UnsupportedOperationException(\"Unknown catalog type: \" + catalogType);\n-    }\n+    return CatalogUtil.buildIcebergCatalog(name, options.asCaseSensitiveMap(), conf);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "25b2f918c96611af21de42c5a47cf8fa3b824ae2"}, "originalPosition": 57}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "25b2f918c96611af21de42c5a47cf8fa3b824ae2", "author": {"user": {"login": "rymurr", "name": "Ryan Murray"}}, "url": "https://github.com/apache/iceberg/commit/25b2f918c96611af21de42c5a47cf8fa3b824ae2", "committedDate": "2020-12-15T21:19:53Z", "message": "fix checkstyle"}, "afterCommit": {"oid": "a92b06cf32a5583cea9e8228d2a1a56fe35d6352", "author": {"user": {"login": "rymurr", "name": "Ryan Murray"}}, "url": "https://github.com/apache/iceberg/commit/a92b06cf32a5583cea9e8228d2a1a56fe35d6352", "committedDate": "2020-12-22T22:42:16Z", "message": "address code review"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU3NDExNzA4", "url": "https://github.com/apache/iceberg/pull/1875#pullrequestreview-557411708", "createdAt": "2020-12-22T22:52:14Z", "commit": {"oid": "a92b06cf32a5583cea9e8228d2a1a56fe35d6352"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQyMjo1MjoxNFrOIKLgQg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQyMjo1MjoxNFrOIKLgQg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU0NTE1NA==", "bodyText": "Current catalog? I think it would be current catalog and current namespace.", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r547545154", "createdAt": "2020-12-22T22:52:14Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java", "diffHunk": "@@ -61,4 +65,42 @@ public static void validatePartitionTransforms(PartitionSpec spec) {\n           String.format(\"Cannot write using unsupported transforms: %s\", unsupported));\n     }\n   }\n+\n+  /**\n+   * A modified version of Spark's LookupCatalog.CatalogAndIdentifier.unapply\n+   * Attempts to find the catalog and identifier a multipart identifier represents\n+   * @param nameParts Multipart identifier representing a table\n+   * @return The CatalogPlugin and Identifier for the table\n+   */\n+  public static <C, T> Pair<C, T> catalogAndIdentifier(List<String> nameParts,\n+                                                       Function<String, C> catalogProvider,\n+                                                       IdentiferFunction<T> identiferProvider,\n+                                                       C defaultCatalog,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a92b06cf32a5583cea9e8228d2a1a56fe35d6352"}, "originalPosition": 33}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU3NDEyMTEx", "url": "https://github.com/apache/iceberg/pull/1875#pullrequestreview-557412111", "createdAt": "2020-12-22T22:53:23Z", "commit": {"oid": "a92b06cf32a5583cea9e8228d2a1a56fe35d6352"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQyMjo1MzoyM1rOIKLhjQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQyMjo1MzoyM1rOIKLhjQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU0NTQ4NQ==", "bodyText": "\"name parts\" is an internal thing. What about dropping \"parts\" and just referring to it as \"name\"?\nAlso, no need to capitalize identifier.", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r547545485", "createdAt": "2020-12-22T22:53:23Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java", "diffHunk": "@@ -61,4 +65,42 @@ public static void validatePartitionTransforms(PartitionSpec spec) {\n           String.format(\"Cannot write using unsupported transforms: %s\", unsupported));\n     }\n   }\n+\n+  /**\n+   * A modified version of Spark's LookupCatalog.CatalogAndIdentifier.unapply\n+   * Attempts to find the catalog and identifier a multipart identifier represents\n+   * @param nameParts Multipart identifier representing a table\n+   * @return The CatalogPlugin and Identifier for the table\n+   */\n+  public static <C, T> Pair<C, T> catalogAndIdentifier(List<String> nameParts,\n+                                                       Function<String, C> catalogProvider,\n+                                                       IdentiferFunction<T> identiferProvider,\n+                                                       C defaultCatalog,\n+                                                       String[] currentNamespace) {\n+    Preconditions.checkArgument(!nameParts.isEmpty(),\n+        \"Cannot determine catalog and Identifier from empty name parts\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a92b06cf32a5583cea9e8228d2a1a56fe35d6352"}, "originalPosition": 36}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU3NDEyNjA4", "url": "https://github.com/apache/iceberg/pull/1875#pullrequestreview-557412608", "createdAt": "2020-12-22T22:54:47Z", "commit": {"oid": "a92b06cf32a5583cea9e8228d2a1a56fe35d6352"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQyMjo1NDo0N1rOIKLjWw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQyMjo1NDo0N1rOIKLjWw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU0NTk0Nw==", "bodyText": "Does this need subList if it is returning the entire list?", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r547545947", "createdAt": "2020-12-22T22:54:47Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java", "diffHunk": "@@ -61,4 +65,42 @@ public static void validatePartitionTransforms(PartitionSpec spec) {\n           String.format(\"Cannot write using unsupported transforms: %s\", unsupported));\n     }\n   }\n+\n+  /**\n+   * A modified version of Spark's LookupCatalog.CatalogAndIdentifier.unapply\n+   * Attempts to find the catalog and identifier a multipart identifier represents\n+   * @param nameParts Multipart identifier representing a table\n+   * @return The CatalogPlugin and Identifier for the table\n+   */\n+  public static <C, T> Pair<C, T> catalogAndIdentifier(List<String> nameParts,\n+                                                       Function<String, C> catalogProvider,\n+                                                       IdentiferFunction<T> identiferProvider,\n+                                                       C defaultCatalog,\n+                                                       String[] currentNamespace) {\n+    Preconditions.checkArgument(!nameParts.isEmpty(),\n+        \"Cannot determine catalog and Identifier from empty name parts\");\n+\n+    int lastElementIndex = nameParts.size() - 1;\n+    String name = nameParts.get(lastElementIndex);\n+\n+    if (nameParts.size() == 1) {\n+      // Only a single element, use current catalog and namespace\n+      return Pair.of(defaultCatalog, identiferProvider.of(currentNamespace, name));\n+    } else {\n+      C catalog = catalogProvider.apply(nameParts.get(0));\n+      if (catalog == null) {\n+        // The first element was not a valid catalog, treat it like part of the namespace\n+        String[] namespace =  nameParts.subList(0, lastElementIndex).toArray(new String[0]);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a92b06cf32a5583cea9e8228d2a1a56fe35d6352"}, "originalPosition": 48}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU3NDEyOTkx", "url": "https://github.com/apache/iceberg/pull/1875#pullrequestreview-557412991", "createdAt": "2020-12-22T22:55:47Z", "commit": {"oid": "a92b06cf32a5583cea9e8228d2a1a56fe35d6352"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQyMjo1NTo0N1rOIKLkpA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQyMjo1NTo0N1rOIKLkpA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU0NjI3Ng==", "bodyText": "This could be BiFunction<String[], String, T>. Then you wouldn't need a separate interface for it.", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r547546276", "createdAt": "2020-12-22T22:55:47Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java", "diffHunk": "@@ -61,4 +65,42 @@ public static void validatePartitionTransforms(PartitionSpec spec) {\n           String.format(\"Cannot write using unsupported transforms: %s\", unsupported));\n     }\n   }\n+\n+  /**\n+   * A modified version of Spark's LookupCatalog.CatalogAndIdentifier.unapply\n+   * Attempts to find the catalog and identifier a multipart identifier represents\n+   * @param nameParts Multipart identifier representing a table\n+   * @return The CatalogPlugin and Identifier for the table\n+   */\n+  public static <C, T> Pair<C, T> catalogAndIdentifier(List<String> nameParts,\n+                                                       Function<String, C> catalogProvider,\n+                                                       IdentiferFunction<T> identiferProvider,\n+                                                       C defaultCatalog,\n+                                                       String[] currentNamespace) {\n+    Preconditions.checkArgument(!nameParts.isEmpty(),\n+        \"Cannot determine catalog and Identifier from empty name parts\");\n+\n+    int lastElementIndex = nameParts.size() - 1;\n+    String name = nameParts.get(lastElementIndex);\n+\n+    if (nameParts.size() == 1) {\n+      // Only a single element, use current catalog and namespace\n+      return Pair.of(defaultCatalog, identiferProvider.of(currentNamespace, name));\n+    } else {\n+      C catalog = catalogProvider.apply(nameParts.get(0));\n+      if (catalog == null) {\n+        // The first element was not a valid catalog, treat it like part of the namespace\n+        String[] namespace =  nameParts.subList(0, lastElementIndex).toArray(new String[0]);\n+        return Pair.of(defaultCatalog, identiferProvider.of(namespace, name));\n+      } else {\n+        // Assume the first element is a valid catalog\n+        String[] namespace = nameParts.subList(1, lastElementIndex).toArray(new String[0]);\n+        return Pair.of(catalog, identiferProvider.of(namespace, name));\n+      }\n+    }\n+  }\n+\n+  public interface IdentiferFunction<T> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a92b06cf32a5583cea9e8228d2a1a56fe35d6352"}, "originalPosition": 58}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU3NDE0MDIy", "url": "https://github.com/apache/iceberg/pull/1875#pullrequestreview-557414022", "createdAt": "2020-12-22T22:58:51Z", "commit": {"oid": "a92b06cf32a5583cea9e8228d2a1a56fe35d6352"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQyMjo1ODo1MVrOIKLoHQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQyMjo1ODo1MVrOIKLoHQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU0NzE2NQ==", "bodyText": "When I suggested using SparkCatalog earlier, I didn't think about how it isn't defined for 2.4. Instead of this check, let's just check whether the catalog property is defined at all. As long as catalogName is non-null, it is a catalog for the purposes of this config. If the catalog doesn't have a valid type or implementation class then loading it will fail.\nAlso, it would be catalogImpl not name because name is the catalog name.", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r547547165", "createdAt": "2020-12-22T22:58:51Z", "author": {"login": "rdblue"}, "path": "spark2/src/main/java/org/apache/iceberg/spark/source/CustomCatalogs.java", "diffHunk": "@@ -0,0 +1,108 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import com.github.benmanes.caffeine.cache.Cache;\n+import com.github.benmanes.caffeine.cache.Caffeine;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.CatalogUtil;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.Namespace;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.relocated.com.google.common.annotations.VisibleForTesting;\n+import org.apache.iceberg.relocated.com.google.common.base.Splitter;\n+import org.apache.iceberg.spark.SparkUtil;\n+import org.apache.iceberg.util.Pair;\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.sql.SparkSession;\n+\n+public final class CustomCatalogs {\n+  private static final Cache<Pair<SparkSession, String>, Catalog> CATALOG_CACHE = Caffeine.newBuilder()\n+      .softValues().build();\n+\n+  public static final String ICEBERG_DEFAULT_CATALOG = \"default_catalog\";\n+  public static final String ICEBERG_CATALOG_PREFIX = \"spark.sql.catalog\";\n+\n+  private CustomCatalogs() {\n+  }\n+\n+  /**\n+   * Build an Iceberg {@link Catalog} to be used by this Spark source adapter.\n+   *\n+   * <p>\n+   * The cache is to facilitate reuse of catalogs, especially if wrapped in CachingCatalog. For non-Hive catalogs all\n+   * custom parameters passed to the catalog are considered in the cache key. Hive catalogs only cache based on\n+   * the Metastore URIs as per previous behaviour.\n+   *\n+   *\n+   * @param spark Spark Session\n+   * @param name Catalog Name\n+   * @return an Iceberg catalog\n+   */\n+  public static Catalog buildIcebergCatalog(SparkSession spark, String name) {\n+    return CATALOG_CACHE.get(Pair.of(spark, name), CustomCatalogs::load);\n+  }\n+\n+  private static Catalog load(Pair<SparkSession, String> sparkAndName) {\n+    SparkSession spark = sparkAndName.first();\n+    String name = sparkAndName.second();\n+    SparkConf sparkConf = spark.sparkContext().getConf();\n+    Configuration conf = spark.sessionState().newHadoopConf();\n+\n+    String catalogPrefix = String.format(\"%s.%s\", ICEBERG_CATALOG_PREFIX, name);\n+    String catalogName = sparkConf.get(catalogPrefix, null);\n+    if (!name.equals(ICEBERG_DEFAULT_CATALOG) &&\n+        !org.apache.spark.sql.catalog.Catalog.class.getName().equals(catalogName)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a92b06cf32a5583cea9e8228d2a1a56fe35d6352"}, "originalPosition": 77}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU3NDE1MDQw", "url": "https://github.com/apache/iceberg/pull/1875#pullrequestreview-557415040", "createdAt": "2020-12-22T23:02:04Z", "commit": {"oid": "a92b06cf32a5583cea9e8228d2a1a56fe35d6352"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQyMzowMjowNFrOIKLrPw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQyMzowMjowNFrOIKLrPw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU0Nzk2Nw==", "bodyText": "Can you also add a test for a catalog.db.table name?", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r547547967", "createdAt": "2020-12-22T23:02:04Z", "author": {"login": "rdblue"}, "path": "spark2/src/test/java/org/apache/iceberg/spark/source/TestCustomCatalog.java", "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import java.io.File;\n+import java.util.List;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.CatalogProperties;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalog.Catalog;\n+import org.junit.After;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+\n+public class TestCustomCatalog {\n+  private static final String CATALOG_IMPL = String.format(\"%s.%s.%s\", CustomCatalogs.ICEBERG_CATALOG_PREFIX,\n+      CustomCatalogs.ICEBERG_DEFAULT_CATALOG, CatalogProperties.CATALOG_IMPL);\n+  private static final String WAREHOUSE = String.format(\"%s.%s.%s\", CustomCatalogs.ICEBERG_CATALOG_PREFIX,\n+      CustomCatalogs.ICEBERG_DEFAULT_CATALOG, CatalogProperties.WAREHOUSE_LOCATION);\n+  private static final String URI_KEY = String.format(\"%s.%s.%s\", CustomCatalogs.ICEBERG_CATALOG_PREFIX,\n+      CustomCatalogs.ICEBERG_DEFAULT_CATALOG, CatalogProperties.HIVE_URI);\n+  private static final String URI_VAL = \"thrift://localhost:12345\"; // dummy uri\n+  private static final String CATALOG_VAL = \"org.apache.iceberg.spark.source.TestCatalog\";\n+  private static final TableIdentifier TABLE = TableIdentifier.of(\"default\", \"table\");\n+  private static final Schema SCHEMA = new Schema(\n+      optional(1, \"id\", Types.IntegerType.get()),\n+      optional(2, \"data\", Types.StringType.get())\n+  );\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  File tableDir = null;\n+  String tableLocation = null;\n+  HadoopTables tables;\n+\n+  protected static SparkSession spark = null;\n+\n+  @BeforeClass\n+  public static void startMetastoreAndSpark() {\n+    spark = SparkSession.builder().master(\"local[2]\").getOrCreate();\n+  }\n+\n+  @AfterClass\n+  public static void stopMetastoreAndSpark() {\n+    spark.stop();\n+    spark = null;\n+  }\n+\n+  @Before\n+  public void setupTable() throws Exception {\n+    SparkConf sparkConf = spark.sparkContext().conf();\n+    sparkConf.set(\n+        String.format(\"%s.%s\", CustomCatalogs.ICEBERG_CATALOG_PREFIX, CustomCatalogs.ICEBERG_DEFAULT_CATALOG),\n+        Catalog.class.getName());\n+    this.tables = new HadoopTables(spark.sessionState().newHadoopConf());\n+    this.tableDir = temp.newFolder();\n+    tableDir.delete(); // created by table create\n+    this.tableLocation = tableDir.toURI().toString();\n+    tables.create(SCHEMA, PartitionSpec.unpartitioned(), String.format(\"%s/%s\", tableLocation, TABLE.name()));\n+  }\n+\n+  @After\n+  public void removeTable() {\n+    SparkConf sparkConf = spark.sparkContext().conf();\n+    sparkConf.remove(CATALOG_IMPL);\n+    sparkConf.remove(WAREHOUSE);\n+    sparkConf.remove(URI_KEY);\n+    tables.dropTable(String.format(\"%s/%s\", tableLocation, TABLE.name()));\n+    tableDir.delete();\n+    CustomCatalogs.clearCache();\n+  }\n+\n+  @Test\n+  public void withSparkOptions() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a92b06cf32a5583cea9e8228d2a1a56fe35d6352"}, "originalPosition": 109}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7a557b064050b844d9aeab033ace3a72c2dd9904", "author": {"user": {"login": "rymurr", "name": "Ryan Murray"}}, "url": "https://github.com/apache/iceberg/commit/7a557b064050b844d9aeab033ace3a72c2dd9904", "committedDate": "2020-12-28T17:48:33Z", "message": "Allow Spark2 DataFrame to use a custom catalog"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8ae16f205223966256c79db2f0efa05dd1a3ccf7", "author": {"user": {"login": "rymurr", "name": "Ryan Murray"}}, "url": "https://github.com/apache/iceberg/commit/8ae16f205223966256c79db2f0efa05dd1a3ccf7", "committedDate": "2020-12-28T17:48:34Z", "message": "skip cache for Hive"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f907ccd1d5e2683ef35ae42c68459571981deb3d", "author": {"user": {"login": "rymurr", "name": "Ryan Murray"}}, "url": "https://github.com/apache/iceberg/commit/f907ccd1d5e2683ef35ae42c68459571981deb3d", "committedDate": "2020-12-28T17:48:35Z", "message": "update cache behaviour and javadoc"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "915dda23583a4e07767acee2d5b51e1a8eb9a636", "author": {"user": {"login": "rymurr", "name": "Ryan Murray"}}, "url": "https://github.com/apache/iceberg/commit/915dda23583a4e07767acee2d5b51e1a8eb9a636", "committedDate": "2020-12-28T17:48:36Z", "message": "address code review comments and more code reuse"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8ef997be848a107c0a3cb1b45d9ed0ac8d309294", "author": {"user": {"login": "rymurr", "name": "Ryan Murray"}}, "url": "https://github.com/apache/iceberg/commit/8ef997be848a107c0a3cb1b45d9ed0ac8d309294", "committedDate": "2020-12-28T17:48:36Z", "message": "clarify catalog util and fix tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9d1548a58f6a8395b5ca9b0558dae01c9ca9a446", "author": {"user": {"login": "rymurr", "name": "Ryan Murray"}}, "url": "https://github.com/apache/iceberg/commit/9d1548a58f6a8395b5ca9b0558dae01c9ca9a446", "committedDate": "2020-12-28T17:48:37Z", "message": "style fixes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "50a2b7dfa9d8992dceed67f0572130f7b42cece6", "author": {"user": {"login": "rymurr", "name": "Ryan Murray"}}, "url": "https://github.com/apache/iceberg/commit/50a2b7dfa9d8992dceed67f0572130f7b42cece6", "committedDate": "2020-12-28T17:48:38Z", "message": "refactor and clean-up"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "83586650320af4a5551681362ad09bdbe4f32e41", "author": {"user": {"login": "rymurr", "name": "Ryan Murray"}}, "url": "https://github.com/apache/iceberg/commit/83586650320af4a5551681362ad09bdbe4f32e41", "committedDate": "2020-12-28T17:48:39Z", "message": "fix checkstyle"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "49b591097f49fcb9854decba8d84086e34b64310", "author": {"user": {"login": "rymurr", "name": "Ryan Murray"}}, "url": "https://github.com/apache/iceberg/commit/49b591097f49fcb9854decba8d84086e34b64310", "committedDate": "2020-12-28T17:48:40Z", "message": "address code review"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "56e115ba8e721297d717d90ffb690c244e49fdcf", "author": {"user": {"login": "rymurr", "name": "Ryan Murray"}}, "url": "https://github.com/apache/iceberg/commit/56e115ba8e721297d717d90ffb690c244e49fdcf", "committedDate": "2020-12-28T17:48:41Z", "message": "clean up and address code review comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d97f48c6d14046f00aac02f3e8cff7fd50dc027d", "author": {"user": {"login": "rymurr", "name": "Ryan Murray"}}, "url": "https://github.com/apache/iceberg/commit/d97f48c6d14046f00aac02f3e8cff7fd50dc027d", "committedDate": "2020-12-28T17:48:42Z", "message": "add test for different catalog and catalog in the identifier"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "3aa9291549388f72b5b51f544874ddce965f68ca", "author": {"user": {"login": "rymurr", "name": "Ryan Murray"}}, "url": "https://github.com/apache/iceberg/commit/3aa9291549388f72b5b51f544874ddce965f68ca", "committedDate": "2020-12-23T17:02:00Z", "message": "clean up and address code review comments"}, "afterCommit": {"oid": "d97f48c6d14046f00aac02f3e8cff7fd50dc027d", "author": {"user": {"login": "rymurr", "name": "Ryan Murray"}}, "url": "https://github.com/apache/iceberg/commit/d97f48c6d14046f00aac02f3e8cff7fd50dc027d", "committedDate": "2020-12-28T17:48:42Z", "message": "add test for different catalog and catalog in the identifier"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3529, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}