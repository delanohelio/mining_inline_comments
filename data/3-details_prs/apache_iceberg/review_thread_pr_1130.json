{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDM3MzMxNDIx", "number": 1130, "reviewThreads": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQyMzozNToyN1rOEHQjPg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQyMzozNToyN1rOEHQjPg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2MDQ2NjU0OnYy", "diffSide": "RIGHT", "path": "mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java", "isResolved": true, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQyMzozNToyN1rOGmjUfw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yMlQxODoxMzoxNVrOGnLyjw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA3Nzc1OQ==", "bodyText": "I was hesitant to use the converter from TableScanIterable as it seems it might only supports Avro. Also we might require different maps for Hive and Pig", "url": "https://github.com/apache/iceberg/pull/1130#discussion_r443077759", "createdAt": "2020-06-19T23:35:27Z", "author": {"login": "rdsr"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java", "diffHunk": "@@ -348,49 +342,38 @@ public void close() throws IOException {\n       currentIterator.close();\n     }\n \n-    private static Map<String, Integer> buildNameToPos(Schema expectedSchema) {\n-      Map<String, Integer> nameToPos = Maps.newHashMap();\n-      for (int pos = 0; pos < expectedSchema.asStruct().fields().size(); pos++) {\n-        Types.NestedField field = expectedSchema.asStruct().fields().get(pos);\n-        nameToPos.put(field.name(), pos);\n-      }\n-      return nameToPos;\n-    }\n-\n-    private CloseableIterator<T> open(FileScanTask currentTask) {\n-      DataFile file = currentTask.file();\n-      // schema of rows returned by readers\n-      PartitionSpec spec = currentTask.spec();\n-      Set<Integer> idColumns =  Sets.intersection(spec.identitySourceIds(), TypeUtil.getProjectedIds(expectedSchema));\n-      boolean hasJoinedPartitionColumns = !idColumns.isEmpty();\n+    private CloseableIterator<T> open(FileScanTask task) {\n+      PartitionSpec spec = task.spec();\n+      Set<Integer> idColumns = spec.identitySourceIds();\n+      Schema partitionSchema = TypeUtil.select(expectedSchema, idColumns);\n+      boolean projectsIdentityPartitionColumns = !partitionSchema.columns().isEmpty();\n \n       CloseableIterable<T> iterable;\n-      if (hasJoinedPartitionColumns) {\n-        Schema readDataSchema = TypeUtil.selectNot(expectedSchema, idColumns);\n-        Schema identityPartitionSchema = TypeUtil.select(expectedSchema, idColumns);\n-        iterable = CloseableIterable.transform(open(currentTask, readDataSchema),\n-            row -> withIdentityPartitionColumns(row, identityPartitionSchema, spec, file.partition()));\n+      if (projectsIdentityPartitionColumns) {\n+        //TODO: seems like we have to specify a converter to convert the partition values", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc548deb2be122339ab5e2e1ea3382e902a83a15"}, "originalPosition": 85}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzcwMTQ3Nw==", "bodyText": "Yes, we should use the one from TableScanIterable. That conversion is from internal representations (e.g., long for timestamps) to Iceberg generics (LocalDateTime or OffsetDateTime for timestamps) so it is the right one to use here. When we add support for Pig or Hive objects, we would need to convert differently for those.", "url": "https://github.com/apache/iceberg/pull/1130#discussion_r443701477", "createdAt": "2020-06-22T16:59:49Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java", "diffHunk": "@@ -348,49 +342,38 @@ public void close() throws IOException {\n       currentIterator.close();\n     }\n \n-    private static Map<String, Integer> buildNameToPos(Schema expectedSchema) {\n-      Map<String, Integer> nameToPos = Maps.newHashMap();\n-      for (int pos = 0; pos < expectedSchema.asStruct().fields().size(); pos++) {\n-        Types.NestedField field = expectedSchema.asStruct().fields().get(pos);\n-        nameToPos.put(field.name(), pos);\n-      }\n-      return nameToPos;\n-    }\n-\n-    private CloseableIterator<T> open(FileScanTask currentTask) {\n-      DataFile file = currentTask.file();\n-      // schema of rows returned by readers\n-      PartitionSpec spec = currentTask.spec();\n-      Set<Integer> idColumns =  Sets.intersection(spec.identitySourceIds(), TypeUtil.getProjectedIds(expectedSchema));\n-      boolean hasJoinedPartitionColumns = !idColumns.isEmpty();\n+    private CloseableIterator<T> open(FileScanTask task) {\n+      PartitionSpec spec = task.spec();\n+      Set<Integer> idColumns = spec.identitySourceIds();\n+      Schema partitionSchema = TypeUtil.select(expectedSchema, idColumns);\n+      boolean projectsIdentityPartitionColumns = !partitionSchema.columns().isEmpty();\n \n       CloseableIterable<T> iterable;\n-      if (hasJoinedPartitionColumns) {\n-        Schema readDataSchema = TypeUtil.selectNot(expectedSchema, idColumns);\n-        Schema identityPartitionSchema = TypeUtil.select(expectedSchema, idColumns);\n-        iterable = CloseableIterable.transform(open(currentTask, readDataSchema),\n-            row -> withIdentityPartitionColumns(row, identityPartitionSchema, spec, file.partition()));\n+      if (projectsIdentityPartitionColumns) {\n+        //TODO: seems like we have to specify a converter to convert the partition values", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA3Nzc1OQ=="}, "originalCommit": {"oid": "dc548deb2be122339ab5e2e1ea3382e902a83a15"}, "originalPosition": 85}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzcwMzYyNQ==", "bodyText": "Any thoughts on which is the right place to put the converter? Or maybe I can just make the one in TableScanIterable public and use that?", "url": "https://github.com/apache/iceberg/pull/1130#discussion_r443703625", "createdAt": "2020-06-22T17:03:55Z", "author": {"login": "rdsr"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java", "diffHunk": "@@ -348,49 +342,38 @@ public void close() throws IOException {\n       currentIterator.close();\n     }\n \n-    private static Map<String, Integer> buildNameToPos(Schema expectedSchema) {\n-      Map<String, Integer> nameToPos = Maps.newHashMap();\n-      for (int pos = 0; pos < expectedSchema.asStruct().fields().size(); pos++) {\n-        Types.NestedField field = expectedSchema.asStruct().fields().get(pos);\n-        nameToPos.put(field.name(), pos);\n-      }\n-      return nameToPos;\n-    }\n-\n-    private CloseableIterator<T> open(FileScanTask currentTask) {\n-      DataFile file = currentTask.file();\n-      // schema of rows returned by readers\n-      PartitionSpec spec = currentTask.spec();\n-      Set<Integer> idColumns =  Sets.intersection(spec.identitySourceIds(), TypeUtil.getProjectedIds(expectedSchema));\n-      boolean hasJoinedPartitionColumns = !idColumns.isEmpty();\n+    private CloseableIterator<T> open(FileScanTask task) {\n+      PartitionSpec spec = task.spec();\n+      Set<Integer> idColumns = spec.identitySourceIds();\n+      Schema partitionSchema = TypeUtil.select(expectedSchema, idColumns);\n+      boolean projectsIdentityPartitionColumns = !partitionSchema.columns().isEmpty();\n \n       CloseableIterable<T> iterable;\n-      if (hasJoinedPartitionColumns) {\n-        Schema readDataSchema = TypeUtil.selectNot(expectedSchema, idColumns);\n-        Schema identityPartitionSchema = TypeUtil.select(expectedSchema, idColumns);\n-        iterable = CloseableIterable.transform(open(currentTask, readDataSchema),\n-            row -> withIdentityPartitionColumns(row, identityPartitionSchema, spec, file.partition()));\n+      if (projectsIdentityPartitionColumns) {\n+        //TODO: seems like we have to specify a converter to convert the partition values", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA3Nzc1OQ=="}, "originalCommit": {"oid": "dc548deb2be122339ab5e2e1ea3382e902a83a15"}, "originalPosition": 85}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzcyNTQ5Mg==", "bodyText": "Now that the generics for Avro are in core, we can put them in the org.apache.iceberg.data package in iceberg-core. How about a new Converters class?", "url": "https://github.com/apache/iceberg/pull/1130#discussion_r443725492", "createdAt": "2020-06-22T17:45:02Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java", "diffHunk": "@@ -348,49 +342,38 @@ public void close() throws IOException {\n       currentIterator.close();\n     }\n \n-    private static Map<String, Integer> buildNameToPos(Schema expectedSchema) {\n-      Map<String, Integer> nameToPos = Maps.newHashMap();\n-      for (int pos = 0; pos < expectedSchema.asStruct().fields().size(); pos++) {\n-        Types.NestedField field = expectedSchema.asStruct().fields().get(pos);\n-        nameToPos.put(field.name(), pos);\n-      }\n-      return nameToPos;\n-    }\n-\n-    private CloseableIterator<T> open(FileScanTask currentTask) {\n-      DataFile file = currentTask.file();\n-      // schema of rows returned by readers\n-      PartitionSpec spec = currentTask.spec();\n-      Set<Integer> idColumns =  Sets.intersection(spec.identitySourceIds(), TypeUtil.getProjectedIds(expectedSchema));\n-      boolean hasJoinedPartitionColumns = !idColumns.isEmpty();\n+    private CloseableIterator<T> open(FileScanTask task) {\n+      PartitionSpec spec = task.spec();\n+      Set<Integer> idColumns = spec.identitySourceIds();\n+      Schema partitionSchema = TypeUtil.select(expectedSchema, idColumns);\n+      boolean projectsIdentityPartitionColumns = !partitionSchema.columns().isEmpty();\n \n       CloseableIterable<T> iterable;\n-      if (hasJoinedPartitionColumns) {\n-        Schema readDataSchema = TypeUtil.selectNot(expectedSchema, idColumns);\n-        Schema identityPartitionSchema = TypeUtil.select(expectedSchema, idColumns);\n-        iterable = CloseableIterable.transform(open(currentTask, readDataSchema),\n-            row -> withIdentityPartitionColumns(row, identityPartitionSchema, spec, file.partition()));\n+      if (projectsIdentityPartitionColumns) {\n+        //TODO: seems like we have to specify a converter to convert the partition values", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA3Nzc1OQ=="}, "originalCommit": {"oid": "dc548deb2be122339ab5e2e1ea3382e902a83a15"}, "originalPosition": 85}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzc0MDgxNQ==", "bodyText": "Fixed", "url": "https://github.com/apache/iceberg/pull/1130#discussion_r443740815", "createdAt": "2020-06-22T18:13:15Z", "author": {"login": "rdsr"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java", "diffHunk": "@@ -348,49 +342,38 @@ public void close() throws IOException {\n       currentIterator.close();\n     }\n \n-    private static Map<String, Integer> buildNameToPos(Schema expectedSchema) {\n-      Map<String, Integer> nameToPos = Maps.newHashMap();\n-      for (int pos = 0; pos < expectedSchema.asStruct().fields().size(); pos++) {\n-        Types.NestedField field = expectedSchema.asStruct().fields().get(pos);\n-        nameToPos.put(field.name(), pos);\n-      }\n-      return nameToPos;\n-    }\n-\n-    private CloseableIterator<T> open(FileScanTask currentTask) {\n-      DataFile file = currentTask.file();\n-      // schema of rows returned by readers\n-      PartitionSpec spec = currentTask.spec();\n-      Set<Integer> idColumns =  Sets.intersection(spec.identitySourceIds(), TypeUtil.getProjectedIds(expectedSchema));\n-      boolean hasJoinedPartitionColumns = !idColumns.isEmpty();\n+    private CloseableIterator<T> open(FileScanTask task) {\n+      PartitionSpec spec = task.spec();\n+      Set<Integer> idColumns = spec.identitySourceIds();\n+      Schema partitionSchema = TypeUtil.select(expectedSchema, idColumns);\n+      boolean projectsIdentityPartitionColumns = !partitionSchema.columns().isEmpty();\n \n       CloseableIterable<T> iterable;\n-      if (hasJoinedPartitionColumns) {\n-        Schema readDataSchema = TypeUtil.selectNot(expectedSchema, idColumns);\n-        Schema identityPartitionSchema = TypeUtil.select(expectedSchema, idColumns);\n-        iterable = CloseableIterable.transform(open(currentTask, readDataSchema),\n-            row -> withIdentityPartitionColumns(row, identityPartitionSchema, spec, file.partition()));\n+      if (projectsIdentityPartitionColumns) {\n+        //TODO: seems like we have to specify a converter to convert the partition values", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA3Nzc1OQ=="}, "originalCommit": {"oid": "dc548deb2be122339ab5e2e1ea3382e902a83a15"}, "originalPosition": 85}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3916, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}