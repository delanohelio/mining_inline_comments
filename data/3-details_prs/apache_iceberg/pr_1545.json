{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDk3MDAzOTk0", "number": 1545, "title": "Spark3: Make views that reference Iceberg tables show current data", "bodyText": "This PR makes views that reference Iceberg tables show up-to-date data with and without catalog cache.\nSee #1485 for more info.", "createdAt": "2020-10-02T16:12:41Z", "url": "https://github.com/apache/iceberg/pull/1545", "merged": true, "mergeCommit": {"oid": "83157819fec6f5b3a428f6800684bd5215edbb41"}, "closed": true, "closedAt": "2020-10-06T21:29:59Z", "author": {"login": "aokolnychyi"}, "timelineItems": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdOoHQNAH2gAyNDk3MDAzOTk0OjRiZTU2MDQ5NTQwMGIxYjEwYzUzNjhlMzcwNmNiYzNhNjBjMGYwMTA=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdOoXqAgFqTUwMTI2OTcxOQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "4be560495400b1b10c5368e3706cbc3a60c0f010", "author": {"user": {"login": "aokolnychyi", "name": "Anton Okolnychyi"}}, "url": "https://github.com/apache/iceberg/commit/4be560495400b1b10c5368e3706cbc3a60c0f010", "committedDate": "2020-10-02T15:56:18Z", "message": "Spark3: Make views that reference Iceberg tables show current data"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAxMjY5NzE5", "url": "https://github.com/apache/iceberg/pull/1545#pullrequestreview-501269719", "createdAt": "2020-10-02T16:14:12Z", "commit": {"oid": "4be560495400b1b10c5368e3706cbc3a60c0f010"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMlQxNjoxNDoxM1rOHbziGQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMlQxNjoxNDoxM1rOHbziGQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODkxNzkxMw==", "bodyText": "This was failing for tables loaded through IcebergSource in Spark 3. Now, it matches Spark 2.", "url": "https://github.com/apache/iceberg/pull/1545#discussion_r498917913", "createdAt": "2020-10-02T16:14:13Z", "author": {"login": "aokolnychyi"}, "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestSparkDataWrite.java", "diffHunk": "@@ -458,4 +458,55 @@ public void testWriteProjectionWithMiddle() throws IOException {\n     Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n     Assert.assertEquals(\"Result rows should match\", expected, actual);\n   }\n+\n+  @Test\n+  public void testViewsReturnRecentResults() throws IOException {\n+    File parent = temp.newFolder(format.toString());\n+    File location = new File(parent, \"test\");\n+\n+    HadoopTables tables = new HadoopTables(CONF);\n+    PartitionSpec spec = PartitionSpec.builderFor(SCHEMA).identity(\"data\").build();\n+    tables.create(SCHEMA, spec, location.toString());\n+\n+    List<SimpleRecord> records = Lists.newArrayList(\n+        new SimpleRecord(1, \"a\"),\n+        new SimpleRecord(2, \"b\"),\n+        new SimpleRecord(3, \"c\")\n+    );\n+\n+    Dataset<Row> df = spark.createDataFrame(records, SimpleRecord.class);\n+\n+    df.select(\"id\", \"data\").write()\n+        .format(\"iceberg\")\n+        .option(\"write-format\", format.toString())\n+        .mode(\"append\")\n+        .save(location.toString());\n+\n+    Dataset<Row> query = spark.read()\n+        .format(\"iceberg\")\n+        .load(location.toString())\n+        .where(\"id = 1\");\n+    query.createOrReplaceTempView(\"tmp\");\n+\n+    List<SimpleRecord> actual1 = spark.table(\"tmp\").as(Encoders.bean(SimpleRecord.class)).collectAsList();\n+    List<SimpleRecord> expected1 = Lists.newArrayList(\n+        new SimpleRecord(1, \"a\")\n+    );\n+    Assert.assertEquals(\"Number of rows should match\", expected1.size(), actual1.size());\n+    Assert.assertEquals(\"Result rows should match\", expected1, actual1);\n+\n+    df.select(\"id\", \"data\").write()\n+        .format(\"iceberg\")\n+        .option(\"write-format\", format.toString())\n+        .mode(\"append\")\n+        .save(location.toString());\n+\n+    List<SimpleRecord> actual2 = spark.table(\"tmp\").as(Encoders.bean(SimpleRecord.class)).collectAsList();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4be560495400b1b10c5368e3706cbc3a60c0f010"}, "originalPosition": 47}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3894, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}