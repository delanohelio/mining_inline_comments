{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDUwMTk4MjUy", "number": 1213, "title": "Abstract the generic task writers for sharing the common codes between spark and flink", "bodyText": "When I implement the PR #1145, I found that the flink TaskWriter share most of the codes with spark. So I did some abstraction to move the common logics in the iceberg-core module, so that both of them could share it.\nFYI @rdblue .", "createdAt": "2020-07-16T13:31:04Z", "url": "https://github.com/apache/iceberg/pull/1213", "merged": true, "mergeCommit": {"oid": "28e7a1b72cd28f3c7dd1fd2704bcab6963466be2"}, "closed": true, "closedAt": "2020-07-31T19:38:52Z", "author": {"login": "openinx"}, "timelineItems": {"totalCount": 43, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABc3JoKkAFqTQ1MjY3NTAzOA==", "endCursor": "Y3Vyc29yOnYyOpPPAAABc6YrN8gH2gAyNDUwMTk4MjUyOmU2MTUyZDE5NDUzZDE0NGJmOGE3YmFkZTkwZWRlOTk3NTM3YzVlZGE=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDUyNjc1MDM4", "url": "https://github.com/apache/iceberg/pull/1213#pullrequestreview-452675038", "createdAt": "2020-07-21T17:24:55Z", "commit": {"oid": "a539fb89f4c9e0728d3f14b55b45e95bf2954788"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQxNzoyNDo1NVrOG1CTKQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQxNzoyNDo1NVrOG1CTKQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODI2NTM4NQ==", "bodyText": "This will need Javadoc.", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r458265385", "createdAt": "2020-07-21T17:24:55Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/taskio/FileAppenderFactory.java", "diffHunk": "@@ -0,0 +1,29 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.taskio;\n+\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.io.OutputFile;\n+\n+public interface FileAppenderFactory<T> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a539fb89f4c9e0728d3f14b55b45e95bf2954788"}, "originalPosition": 26}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDUzNzY1MTg2", "url": "https://github.com/apache/iceberg/pull/1213#pullrequestreview-453765186", "createdAt": "2020-07-23T00:08:50Z", "commit": {"oid": "37e378577d53567afae212a531be6df9c998ad33"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwMDowODo1MFrOG14X7w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwMDowODo1MFrOG14X7w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE1MTM0Mw==", "bodyText": "If this is a refactor, I'd prefer to keep the existing structure the way it was, with openCurent, closeCurrent, and writeInternal. That way we aren't introducing additional changes in this PR. If we want to refactor how writers work, we can do that separately.", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r459151343", "createdAt": "2020-07-23T00:08:50Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/taskio/BaseTaskWriter.java", "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.taskio;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.function.Supplier;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.PartitionKey;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.encryption.EncryptedOutputFile;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.util.Tasks;\n+\n+abstract class BaseTaskWriter<T> implements TaskWriter<T> {\n+  protected static final int ROWS_DIVISOR = 1000;\n+\n+  private final List<DataFile> completedFiles = Lists.newArrayList();\n+  private final PartitionSpec spec;\n+  private final FileFormat format;\n+  private final FileAppenderFactory<T> appenderFactory;\n+  private final OutputFileFactory fileFactory;\n+  private final FileIO io;\n+  private final long targetFileSize;\n+\n+  protected BaseTaskWriter(PartitionSpec spec, FileFormat format, FileAppenderFactory<T> appenderFactory,\n+                           OutputFileFactory fileFactory, FileIO io, long targetFileSize) {\n+    this.spec = spec;\n+    this.format = format;\n+    this.appenderFactory = appenderFactory;\n+    this.fileFactory = fileFactory;\n+    this.io = io;\n+    this.targetFileSize = targetFileSize;\n+  }\n+\n+  @Override\n+  public void abort() throws IOException {\n+    close();\n+\n+    // clean up files created by this writer\n+    Tasks.foreach(completedFiles)\n+        .throwFailureWhenFinished()\n+        .noRetry()\n+        .run(file -> io.deleteFile(file.path().toString()));\n+  }\n+\n+  @Override\n+  public List<DataFile> pollCompleteFiles() {\n+    if (completedFiles.size() > 0) {\n+      List<DataFile> dataFiles = ImmutableList.copyOf(completedFiles);\n+      completedFiles.clear();\n+      return dataFiles;\n+    } else {\n+      return Collections.emptyList();\n+    }\n+  }\n+\n+  protected OutputFileFactory outputFileFactory() {\n+    return this.fileFactory;\n+  }\n+\n+  WrappedFileAppender createWrappedFileAppender(PartitionKey partitionKey,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "37e378577d53567afae212a531be6df9c998ad33"}, "originalPosition": 86}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDUzNzY2NzU1", "url": "https://github.com/apache/iceberg/pull/1213#pullrequestreview-453766755", "createdAt": "2020-07-23T00:14:19Z", "commit": {"oid": "37e378577d53567afae212a531be6df9c998ad33"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwMDoxNDoxOVrOG14dvg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwMDoxNDoxOVrOG14dvg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE1MjgzMA==", "bodyText": "I don't think it's a good idea to have a poll method like this one because it leaks critical state (completedFiles) and creates an opportunity for threading issues between write and pollCompleteFiles.\nInstead, I think the base implementation should use a push model, where each file is released as it is closed.\n  /**\n   * Called when a data file is completed and no longer needed by the writer.\n   */\n  protected abstract void completedFile(DataFile file);\nThen closeCurrent would call completedFile(dataFile) and the implementation of completedFile would handle it from there.", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r459152830", "createdAt": "2020-07-23T00:14:19Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/taskio/BaseTaskWriter.java", "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.taskio;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.function.Supplier;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.PartitionKey;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.encryption.EncryptedOutputFile;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.util.Tasks;\n+\n+abstract class BaseTaskWriter<T> implements TaskWriter<T> {\n+  protected static final int ROWS_DIVISOR = 1000;\n+\n+  private final List<DataFile> completedFiles = Lists.newArrayList();\n+  private final PartitionSpec spec;\n+  private final FileFormat format;\n+  private final FileAppenderFactory<T> appenderFactory;\n+  private final OutputFileFactory fileFactory;\n+  private final FileIO io;\n+  private final long targetFileSize;\n+\n+  protected BaseTaskWriter(PartitionSpec spec, FileFormat format, FileAppenderFactory<T> appenderFactory,\n+                           OutputFileFactory fileFactory, FileIO io, long targetFileSize) {\n+    this.spec = spec;\n+    this.format = format;\n+    this.appenderFactory = appenderFactory;\n+    this.fileFactory = fileFactory;\n+    this.io = io;\n+    this.targetFileSize = targetFileSize;\n+  }\n+\n+  @Override\n+  public void abort() throws IOException {\n+    close();\n+\n+    // clean up files created by this writer\n+    Tasks.foreach(completedFiles)\n+        .throwFailureWhenFinished()\n+        .noRetry()\n+        .run(file -> io.deleteFile(file.path().toString()));\n+  }\n+\n+  @Override\n+  public List<DataFile> pollCompleteFiles() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "37e378577d53567afae212a531be6df9c998ad33"}, "originalPosition": 72}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDUzNzY4NzYy", "url": "https://github.com/apache/iceberg/pull/1213#pullrequestreview-453768762", "createdAt": "2020-07-23T00:21:19Z", "commit": {"oid": "37e378577d53567afae212a531be6df9c998ad33"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwMDoyMToxOVrOG14kuw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwMDoyMToxOVrOG14kuw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE1NDYxOQ==", "bodyText": "Minor: I think the Javadoc for arguments should describe the argument's purpose, like an OutputFile used to create an output stream. If the purpose is clear from the expected type, then keeping it simple is fine, like an OutputFile.", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r459154619", "createdAt": "2020-07-23T00:21:19Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/taskio/FileAppenderFactory.java", "diffHunk": "@@ -0,0 +1,41 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.taskio;\n+\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.io.OutputFile;\n+\n+/**\n+ * Factory to create a new {@link FileAppender} to write records.\n+ *\n+ * @param <T> data type of the rows to append.\n+ */\n+public interface FileAppenderFactory<T> {\n+\n+  /**\n+   * Create a new {@link FileAppender}.\n+   *\n+   * @param outputFile indicate the file location to write.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "37e378577d53567afae212a531be6df9c998ad33"}, "originalPosition": 36}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDUzNzY5MTU2", "url": "https://github.com/apache/iceberg/pull/1213#pullrequestreview-453769156", "createdAt": "2020-07-23T00:22:38Z", "commit": {"oid": "37e378577d53567afae212a531be6df9c998ad33"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwMDoyMjozOFrOG14mJQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwMDoyMjozOFrOG14mJQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE1NDk4MQ==", "bodyText": "Why not just use the existing io package? That, or maybe a tasks package.", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r459154981", "createdAt": "2020-07-23T00:22:38Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/taskio/OutputFileFactory.java", "diffHunk": "@@ -17,7 +17,7 @@\n  * under the License.\n  */\n \n-package org.apache.iceberg.spark.source;\n+package org.apache.iceberg.taskio;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "37e378577d53567afae212a531be6df9c998ad33"}, "originalPosition": 5}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDUzNzc3MDgz", "url": "https://github.com/apache/iceberg/pull/1213#pullrequestreview-453777083", "createdAt": "2020-07-23T00:51:53Z", "commit": {"oid": "37e378577d53567afae212a531be6df9c998ad33"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwMDo1MTo1M1rOG15CsQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwMDo1MTo1M1rOG15CsQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE2MjI4OQ==", "bodyText": "I don't see much value in this class. Its primary use is to keep track of whether a file is large enough to release, but it doesn't actually have any of the logic to do that. As a consequence, the code is now split across multiple places.\nThis also has the logic for closing an appender and converting it to a DataFile, but that could just as easily be done in a DataFile closeAppender(FileAppender appender) method.\nIt would make sense to keep this class if it completely encapsulated the logic of rolling new files. That would require some refactoring so that it could create new files using the file and appender factories. It would also require passing a Consumer<DataFile> so that it can release closed files. Otherwise, I think we should remove this class.", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r459162289", "createdAt": "2020-07-23T00:51:53Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/taskio/BaseTaskWriter.java", "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.taskio;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.function.Supplier;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.PartitionKey;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.encryption.EncryptedOutputFile;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.util.Tasks;\n+\n+abstract class BaseTaskWriter<T> implements TaskWriter<T> {\n+  protected static final int ROWS_DIVISOR = 1000;\n+\n+  private final List<DataFile> completedFiles = Lists.newArrayList();\n+  private final PartitionSpec spec;\n+  private final FileFormat format;\n+  private final FileAppenderFactory<T> appenderFactory;\n+  private final OutputFileFactory fileFactory;\n+  private final FileIO io;\n+  private final long targetFileSize;\n+\n+  protected BaseTaskWriter(PartitionSpec spec, FileFormat format, FileAppenderFactory<T> appenderFactory,\n+                           OutputFileFactory fileFactory, FileIO io, long targetFileSize) {\n+    this.spec = spec;\n+    this.format = format;\n+    this.appenderFactory = appenderFactory;\n+    this.fileFactory = fileFactory;\n+    this.io = io;\n+    this.targetFileSize = targetFileSize;\n+  }\n+\n+  @Override\n+  public void abort() throws IOException {\n+    close();\n+\n+    // clean up files created by this writer\n+    Tasks.foreach(completedFiles)\n+        .throwFailureWhenFinished()\n+        .noRetry()\n+        .run(file -> io.deleteFile(file.path().toString()));\n+  }\n+\n+  @Override\n+  public List<DataFile> pollCompleteFiles() {\n+    if (completedFiles.size() > 0) {\n+      List<DataFile> dataFiles = ImmutableList.copyOf(completedFiles);\n+      completedFiles.clear();\n+      return dataFiles;\n+    } else {\n+      return Collections.emptyList();\n+    }\n+  }\n+\n+  protected OutputFileFactory outputFileFactory() {\n+    return this.fileFactory;\n+  }\n+\n+  WrappedFileAppender createWrappedFileAppender(PartitionKey partitionKey,\n+                                                Supplier<EncryptedOutputFile> outputFileSupplier) {\n+    EncryptedOutputFile outputFile = outputFileSupplier.get();\n+    FileAppender<T> appender = appenderFactory.newAppender(outputFile.encryptingOutputFile(), format);\n+    return new WrappedFileAppender(partitionKey, outputFile, appender);\n+  }\n+\n+  class WrappedFileAppender {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "37e378577d53567afae212a531be6df9c998ad33"}, "originalPosition": 93}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDUzNzc3NTMy", "url": "https://github.com/apache/iceberg/pull/1213#pullrequestreview-453777532", "createdAt": "2020-07-23T00:53:52Z", "commit": {"oid": "37e378577d53567afae212a531be6df9c998ad33"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwMDo1Mzo1MlrOG15EWw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwMDo1Mzo1MlrOG15EWw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE2MjcxNQ==", "bodyText": "We should consider changing the ORC appender to simply return 0 if the file isn't finished. That way this check is still valid, but the file will never be rolled.", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r459162715", "createdAt": "2020-07-23T00:53:52Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/taskio/BaseTaskWriter.java", "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.taskio;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.function.Supplier;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.PartitionKey;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.encryption.EncryptedOutputFile;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.util.Tasks;\n+\n+abstract class BaseTaskWriter<T> implements TaskWriter<T> {\n+  protected static final int ROWS_DIVISOR = 1000;\n+\n+  private final List<DataFile> completedFiles = Lists.newArrayList();\n+  private final PartitionSpec spec;\n+  private final FileFormat format;\n+  private final FileAppenderFactory<T> appenderFactory;\n+  private final OutputFileFactory fileFactory;\n+  private final FileIO io;\n+  private final long targetFileSize;\n+\n+  protected BaseTaskWriter(PartitionSpec spec, FileFormat format, FileAppenderFactory<T> appenderFactory,\n+                           OutputFileFactory fileFactory, FileIO io, long targetFileSize) {\n+    this.spec = spec;\n+    this.format = format;\n+    this.appenderFactory = appenderFactory;\n+    this.fileFactory = fileFactory;\n+    this.io = io;\n+    this.targetFileSize = targetFileSize;\n+  }\n+\n+  @Override\n+  public void abort() throws IOException {\n+    close();\n+\n+    // clean up files created by this writer\n+    Tasks.foreach(completedFiles)\n+        .throwFailureWhenFinished()\n+        .noRetry()\n+        .run(file -> io.deleteFile(file.path().toString()));\n+  }\n+\n+  @Override\n+  public List<DataFile> pollCompleteFiles() {\n+    if (completedFiles.size() > 0) {\n+      List<DataFile> dataFiles = ImmutableList.copyOf(completedFiles);\n+      completedFiles.clear();\n+      return dataFiles;\n+    } else {\n+      return Collections.emptyList();\n+    }\n+  }\n+\n+  protected OutputFileFactory outputFileFactory() {\n+    return this.fileFactory;\n+  }\n+\n+  WrappedFileAppender createWrappedFileAppender(PartitionKey partitionKey,\n+                                                Supplier<EncryptedOutputFile> outputFileSupplier) {\n+    EncryptedOutputFile outputFile = outputFileSupplier.get();\n+    FileAppender<T> appender = appenderFactory.newAppender(outputFile.encryptingOutputFile(), format);\n+    return new WrappedFileAppender(partitionKey, outputFile, appender);\n+  }\n+\n+  class WrappedFileAppender {\n+    private final PartitionKey partitionKey;\n+    private final EncryptedOutputFile encryptedOutputFile;\n+    private final FileAppender<T> appender;\n+\n+    private boolean closed = false;\n+    private long currentRows = 0;\n+\n+    WrappedFileAppender(PartitionKey partitionKey, EncryptedOutputFile encryptedOutputFile, FileAppender<T> appender) {\n+      this.partitionKey = partitionKey;\n+      this.encryptedOutputFile = encryptedOutputFile;\n+      this.appender = appender;\n+    }\n+\n+    void add(T record) {\n+      this.appender.add(record);\n+      this.currentRows++;\n+    }\n+\n+    boolean shouldRollToNewFile() {\n+      //TODO: ORC file now not support target file size before closed", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "37e378577d53567afae212a531be6df9c998ad33"}, "originalPosition": 113}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDUzNzc5OTI0", "url": "https://github.com/apache/iceberg/pull/1213#pullrequestreview-453779924", "createdAt": "2020-07-23T01:03:28Z", "commit": {"oid": "37e378577d53567afae212a531be6df9c998ad33"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwMTowMzoyOFrOG15Nfg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwMTowMzoyOFrOG15Nfg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE2NTA1NA==", "bodyText": "Instead of passing a function, I think this should be an abstract method:\n  /**\n   * Create a PartitionKey from the values in row.\n   * <p>\n   * Any PartitionKey returned by this method can be reused by the implementation.\n   *\n   * @param row a data row\n   */\n  protected abstract PartitionKey partition(T row);\nPassing a function is good if we need to inject behavior that might need to be customized, but here the only customization that would be required is to partition the objects that this class is already parameterized by. So it will be easier just to add a method for subclasses to implement. And that puts the responsibility on the implementation instead of on the code that constructs the writer.", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r459165054", "createdAt": "2020-07-23T01:03:28Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/taskio/PartitionedFanoutWriter.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.taskio;\n+\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.function.Function;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionKey;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+\n+public class PartitionedFanoutWriter<T> extends BaseTaskWriter<T> {\n+  private final Function<T, PartitionKey> keyGetter;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "37e378577d53567afae212a531be6df9c998ad33"}, "originalPosition": 33}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDUzNzgwMjQw", "url": "https://github.com/apache/iceberg/pull/1213#pullrequestreview-453780240", "createdAt": "2020-07-23T01:04:40Z", "commit": {"oid": "37e378577d53567afae212a531be6df9c998ad33"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwMTowNDo0MFrOG15OnQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwMTowNDo0MFrOG15OnQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE2NTM0MQ==", "bodyText": "Like the other partitioned writer, I think this should use an abstract method to be implemented by subclasses.", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r459165341", "createdAt": "2020-07-23T01:04:40Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/taskio/PartitionedWriter.java", "diffHunk": "@@ -17,41 +17,40 @@\n  * under the License.\n  */\n \n-package org.apache.iceberg.spark.source;\n+package org.apache.iceberg.taskio;\n \n import java.io.IOException;\n import java.util.Set;\n+import java.util.function.Function;\n import org.apache.iceberg.FileFormat;\n import org.apache.iceberg.PartitionKey;\n import org.apache.iceberg.PartitionSpec;\n-import org.apache.iceberg.Schema;\n import org.apache.iceberg.io.FileIO;\n import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n-import org.apache.iceberg.spark.SparkSchemaUtil;\n-import org.apache.spark.sql.catalyst.InternalRow;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-class PartitionedWriter extends BaseWriter {\n+public class PartitionedWriter<T> extends BaseTaskWriter<T> {\n   private static final Logger LOG = LoggerFactory.getLogger(PartitionedWriter.class);\n \n-  private final PartitionKey key;\n-  private final InternalRowWrapper wrapper;\n+  private final Function<T, PartitionKey> keyGetter;\n   private final Set<PartitionKey> completedPartitions = Sets.newHashSet();\n \n-  PartitionedWriter(PartitionSpec spec, FileFormat format, SparkAppenderFactory appenderFactory,\n-                    OutputFileFactory fileFactory, FileIO io, long targetFileSize, Schema writeSchema) {\n+  private PartitionKey currentKey = null;\n+  private WrappedFileAppender currentAppender = null;\n+\n+  public PartitionedWriter(PartitionSpec spec, FileFormat format, FileAppenderFactory<T> appenderFactory,\n+                           OutputFileFactory fileFactory, FileIO io, long targetFileSize,\n+                           Function<T, PartitionKey> keyGetter) {\n     super(spec, format, appenderFactory, fileFactory, io, targetFileSize);\n-    this.key = new PartitionKey(spec, writeSchema);\n-    this.wrapper = new InternalRowWrapper(SparkSchemaUtil.convert(writeSchema));\n+    this.keyGetter = keyGetter;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "37e378577d53567afae212a531be6df9c998ad33"}, "originalPosition": 42}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU1Mzk3OTM1", "url": "https://github.com/apache/iceberg/pull/1213#pullrequestreview-455397935", "createdAt": "2020-07-26T21:46:39Z", "commit": {"oid": "9732fcd651878e3ccf84ce8bde1fa8cdeff8120c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNlQyMTo0NjozOVrOG3PaBw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNlQyMTo0NjozOVrOG3PaBw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDU3NzI4Nw==", "bodyText": "This code here is handling the case where we've not seen this partition key yet. This is especially likely to happen when users did not keyBy or otherwise pre-shuffle the data according to the partition key.\nIs pre-shuffling something that the users should be doing before writing to the table (either keyBy or ORDER BY in Flink SQL)? I understand that this is specifically a PartitionedFanoutWriter, and so it makes sense that keys might not always come together (and even in the case where users did keyBy the partition key, if the number of TaskManager slots that are writing does not equal the cardinality of the partition key you'll still wind up with multiple RollingFileAppenders in a single Flink writing task and thus fanout). However, for long running streaming queries, it's possible that this TaskManager doesn't see this partition key again for days or even weeks (especially at a high enough volume to emit a complete file of the given target file size).\nI guess my concern is that users wind up with a very high cardinality of keys on a single TaskManager. Either because they didn't pre-shuffle their data or perhaps they have an imbalance between the cardinality on the partition key and the parallelism at the write stage such that records might not naturally group together enough to emit an entire file. Or,  as another edge case, one partition key value is simply not common enough to emit an entire file from this PartitionedFanoutWriter.\nIIUC, if the PartitionedFanoutWriter does not see this partition key enough times in this TaskManager again to emit a full file for quite some time, a file containing this data won't be written until close is called. For very long running streaming jobs, this could be days or even weeks in my experience. This could also lead to small files upon close. Is this a concern that Iceberg should take into consideration or is this left to the users in their Flink query to determine when tuning their queries?\nI imagine with S3, data locality of a file written much later than its timestamp of when the data was received is not a major concern, as the manifest file will tell whatever query engine reads this table which keys in their S3 bucket to grab and the locality issue is relatively abstracted away from the user, but what about if the user is using HDFS? Could this lead to performance issues (or even correctness issues) on read if records with relatively similar timestamps at their RollingFileAppender are scattered across a potentially large number of files?\nI suppose this amounts to three concerns (and forgive me if these are non-issues as I am still new to the project, but not new to Flink so partially this is for helping me understand, as well as reviewing my concerns when reading this code):\n\nShould we be concerned that a writer won't emit a file until a streaming query is closed due to the previously mentioned case? Possibly tracking the time that each writer has existed and then emitting a file if it has been far too long (however that could be determined).\nIf a record comes in at some time, and then the file containing that record isn't written for a much greater period of time (on the order of days or weeks), could this lead to correctness problems or very large performance problems when any query engine reads this table?\nWould it be beneficial to at least emit a warning or info level log to the user that it might be beneficial to pre-partition their data according to the partition key spec if perhaps the number of unique RollingFileAppender writers gets too high for one given Flink writer slot / TaskManager? Admittedly, it might be difficult to determine a heuristic of when this might be a problem vs just the natural difference in the parallelism of writing task slots vs the cardinality of the partition key.", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r460577287", "createdAt": "2020-07-26T21:46:39Z", "author": {"login": "kbendick"}, "path": "core/src/main/java/org/apache/iceberg/io/PartitionedFanoutWriter.java", "diffHunk": "@@ -0,0 +1,73 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.io;\n+\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.Map;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionKey;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+\n+public abstract class PartitionedFanoutWriter<T> extends BaseTaskWriter<T> {\n+  private final Map<PartitionKey, RollingFileAppender> writers = Maps.newHashMap();\n+\n+  public PartitionedFanoutWriter(PartitionSpec spec, FileFormat format, FileAppenderFactory<T> appenderFactory,\n+                                 OutputFileFactory fileFactory, FileIO io, long targetFileSize) {\n+    super(spec, format, appenderFactory, fileFactory, io, targetFileSize);\n+  }\n+\n+  /**\n+   * Create a PartitionKey from the values in row.\n+   * <p>\n+   * Any PartitionKey returned by this method can be reused by the implementation.\n+   *\n+   * @param row a data row\n+   */\n+  protected abstract PartitionKey partition(T row);\n+\n+  @Override\n+  public void write(T row) throws IOException {\n+    PartitionKey partitionKey = partition(row);\n+\n+    RollingFileAppender writer = writers.get(partitionKey);\n+    if (writer == null) {\n+      // NOTICE: we need to copy a new partition key here, in case of messing up the keys in writers.\n+      PartitionKey copiedKey = partitionKey.copy();\n+      writer = new RollingFileAppender(copiedKey);\n+      writers.put(copiedKey, writer);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9732fcd651878e3ccf84ce8bde1fa8cdeff8120c"}, "originalPosition": 56}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU1NDAxNDA4", "url": "https://github.com/apache/iceberg/pull/1213#pullrequestreview-455401408", "createdAt": "2020-07-26T22:42:02Z", "commit": {"oid": "9732fcd651878e3ccf84ce8bde1fa8cdeff8120c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNlQyMjo0MjowM1rOG3PuhA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNlQyMjo0MjowM1rOG3PuhA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDU4MjUzMg==", "bodyText": "Minor: This doesn't implement FileAppender, so maybe RollingFileWriter would make more sense?", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r460582532", "createdAt": "2020-07-26T22:42:03Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/io/BaseTaskWriter.java", "diffHunk": "@@ -0,0 +1,154 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.io;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.List;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.PartitionKey;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.encryption.EncryptedOutputFile;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.util.Tasks;\n+\n+abstract class BaseTaskWriter<T> implements TaskWriter<T> {\n+  private final List<DataFile> completedFiles = Lists.newArrayList();\n+  private final PartitionSpec spec;\n+  private final FileFormat format;\n+  private final FileAppenderFactory<T> appenderFactory;\n+  private final OutputFileFactory fileFactory;\n+  private final FileIO io;\n+  private final long targetFileSize;\n+\n+  protected BaseTaskWriter(PartitionSpec spec, FileFormat format, FileAppenderFactory<T> appenderFactory,\n+                           OutputFileFactory fileFactory, FileIO io, long targetFileSize) {\n+    this.spec = spec;\n+    this.format = format;\n+    this.appenderFactory = appenderFactory;\n+    this.fileFactory = fileFactory;\n+    this.io = io;\n+    this.targetFileSize = targetFileSize;\n+  }\n+\n+  @Override\n+  public void abort() throws IOException {\n+    close();\n+\n+    // clean up files created by this writer\n+    Tasks.foreach(completedFiles)\n+        .throwFailureWhenFinished()\n+        .noRetry()\n+        .run(file -> io.deleteFile(file.path().toString()));\n+  }\n+\n+  @Override\n+  public List<DataFile> complete() throws IOException {\n+    close();\n+\n+    if (completedFiles.size() > 0) {\n+      return ImmutableList.copyOf(completedFiles);\n+    } else {\n+      return Collections.emptyList();\n+    }\n+  }\n+\n+  class RollingFileAppender implements Closeable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9732fcd651878e3ccf84ce8bde1fa8cdeff8120c"}, "originalPosition": 78}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU1NDAxNzYz", "url": "https://github.com/apache/iceberg/pull/1213#pullrequestreview-455401763", "createdAt": "2020-07-26T22:48:20Z", "commit": {"oid": "9732fcd651878e3ccf84ce8bde1fa8cdeff8120c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNlQyMjo0ODoyMFrOG3Pwgw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNlQyMjo0ODoyMFrOG3Pwgw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDU4MzA0Mw==", "bodyText": "It would be nice to not change the logic for opening an appender. Before, this was part of the flow of changing partitions and I don't see any value in moving it.", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r460583043", "createdAt": "2020-07-26T22:48:20Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/io/PartitionedWriter.java", "diffHunk": "@@ -63,10 +66,29 @@ public void write(InternalRow row) throws IOException {\n         throw new IllegalStateException(\"Already closed files for partition: \" + key.toPath());\n       }\n \n-      setCurrentKey(key.copy());\n-      openCurrent();\n+      currentKey = key.copy();\n+    }\n+\n+    if (currentAppender == null) {\n+      currentAppender = new RollingFileAppender(currentKey);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9732fcd651878e3ccf84ce8bde1fa8cdeff8120c"}, "originalPosition": 70}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU1NDAxODk5", "url": "https://github.com/apache/iceberg/pull/1213#pullrequestreview-455401899", "createdAt": "2020-07-26T22:50:38Z", "commit": {"oid": "9732fcd651878e3ccf84ce8bde1fa8cdeff8120c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNlQyMjo1MDozOFrOG3PxRQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNlQyMjo1MDozOFrOG3PxRQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDU4MzIzNw==", "bodyText": "Now that the current key is null, we will need a check before adding it to completedPartitions in the write method:\nif (!key.equals(currentKey)) {\n  closeCurrent();\n  if (currentKey != null) {\n    // if the key is null, there was no previous current key\n    completedPartitions.add(currentKey);\n  }\n  ...\n}", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r460583237", "createdAt": "2020-07-26T22:50:38Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/io/PartitionedWriter.java", "diffHunk": "@@ -17,41 +17,44 @@\n  * under the License.\n  */\n \n-package org.apache.iceberg.spark.source;\n+package org.apache.iceberg.io;\n \n import java.io.IOException;\n import java.util.Set;\n import org.apache.iceberg.FileFormat;\n import org.apache.iceberg.PartitionKey;\n import org.apache.iceberg.PartitionSpec;\n-import org.apache.iceberg.Schema;\n-import org.apache.iceberg.io.FileIO;\n import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n-import org.apache.iceberg.spark.SparkSchemaUtil;\n-import org.apache.spark.sql.catalyst.InternalRow;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-class PartitionedWriter extends BaseWriter {\n+public abstract class PartitionedWriter<T> extends BaseTaskWriter<T> {\n   private static final Logger LOG = LoggerFactory.getLogger(PartitionedWriter.class);\n \n-  private final PartitionKey key;\n-  private final InternalRowWrapper wrapper;\n   private final Set<PartitionKey> completedPartitions = Sets.newHashSet();\n \n-  PartitionedWriter(PartitionSpec spec, FileFormat format, SparkAppenderFactory appenderFactory,\n-                    OutputFileFactory fileFactory, FileIO io, long targetFileSize, Schema writeSchema) {\n+  private PartitionKey currentKey = null;\n+  private RollingFileAppender currentAppender = null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9732fcd651878e3ccf84ce8bde1fa8cdeff8120c"}, "originalPosition": 32}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU1NDAyMDEz", "url": "https://github.com/apache/iceberg/pull/1213#pullrequestreview-455402013", "createdAt": "2020-07-26T22:52:37Z", "commit": {"oid": "9732fcd651878e3ccf84ce8bde1fa8cdeff8120c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNlQyMjo1MjozOFrOG3Px1Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNlQyMjo1MjozOFrOG3Px1Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDU4MzM4MQ==", "bodyText": "Why not initialize currentAppender in the constructor? Then we don't need an additional null check in write, which is called in a tight loop.", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r460583381", "createdAt": "2020-07-26T22:52:38Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/io/UnpartitionedWriter.java", "diffHunk": "@@ -0,0 +1,58 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.io;\n+\n+import java.io.IOException;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+\n+public class UnpartitionedWriter<T> extends BaseTaskWriter<T> {\n+\n+  private RollingFileAppender currentAppender = null;\n+\n+  public UnpartitionedWriter(PartitionSpec spec, FileFormat format, FileAppenderFactory<T> appenderFactory,\n+                             OutputFileFactory fileFactory, FileIO io, long targetFileSize) {\n+    super(spec, format, appenderFactory, fileFactory, io, targetFileSize);\n+  }\n+\n+  @Override\n+  public void write(T record) throws IOException {\n+    if (currentAppender == null) {\n+      currentAppender = new RollingFileAppender(null);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9732fcd651878e3ccf84ce8bde1fa8cdeff8120c"}, "originalPosition": 38}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU1NDAyMTMw", "url": "https://github.com/apache/iceberg/pull/1213#pullrequestreview-455402130", "createdAt": "2020-07-26T22:54:23Z", "commit": {"oid": "9732fcd651878e3ccf84ce8bde1fa8cdeff8120c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNlQyMjo1NDoyM1rOG3PyaQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNlQyMjo1NDoyM1rOG3PyaQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDU4MzUyOQ==", "bodyText": "Is this change needed? It looks non-functional.", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r460583529", "createdAt": "2020-07-26T22:54:23Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/RowDataRewriter.java", "diffHunk": "@@ -92,16 +96,17 @@ private TaskResult rewriteDataForTask(CombinedScanTask task) throws Exception {\n     RowDataReader dataReader = new RowDataReader(\n         task, schema, schema, nameMapping, io.value(), encryptionManager.value(), caseSensitive);\n \n-    SparkAppenderFactory appenderFactory = new SparkAppenderFactory(\n-        properties, schema, SparkSchemaUtil.convert(schema));\n+    StructType structType = SparkSchemaUtil.convert(schema);\n+    SparkAppenderFactory appenderFactory = new SparkAppenderFactory(properties, schema, structType);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9732fcd651878e3ccf84ce8bde1fa8cdeff8120c"}, "originalPosition": 23}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU1NDAyMzAz", "url": "https://github.com/apache/iceberg/pull/1213#pullrequestreview-455402303", "createdAt": "2020-07-26T22:57:05Z", "commit": {"oid": "9732fcd651878e3ccf84ce8bde1fa8cdeff8120c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNlQyMjo1NzowNVrOG3PzhQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNlQyMjo1NzowNVrOG3PzhQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDU4MzgxMw==", "bodyText": "If complete doesn't produce TaskResult, then I'm not sure that we need it at all anymore. Could we just construct TaskCommit directly?", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r460583813", "createdAt": "2020-07-26T22:57:05Z", "author": {"login": "rdblue"}, "path": "spark2/src/main/java/org/apache/iceberg/spark/source/Writer.java", "diffHunk": "@@ -250,33 +253,42 @@ public String toString() {\n       if (spec.fields().isEmpty()) {\n         return new Unpartitioned24Writer(spec, format, appenderFactory, fileFactory, io.value(), targetFileSize);\n       } else {\n-        return new Partitioned24Writer(\n-            spec, format, appenderFactory, fileFactory, io.value(), targetFileSize, writeSchema);\n+        return new Partitioned24Writer(spec, format, appenderFactory, fileFactory, io.value(),\n+            targetFileSize, writeSchema, dsSchema);\n       }\n     }\n   }\n \n-  private static class Unpartitioned24Writer extends UnpartitionedWriter implements DataWriter<InternalRow> {\n+  private static class Unpartitioned24Writer extends UnpartitionedWriter<InternalRow>\n+      implements DataWriter<InternalRow> {\n     Unpartitioned24Writer(PartitionSpec spec, FileFormat format, SparkAppenderFactory appenderFactory,\n                           OutputFileFactory fileFactory, FileIO fileIo, long targetFileSize) {\n       super(spec, format, appenderFactory, fileFactory, fileIo, targetFileSize);\n     }\n \n     @Override\n     public WriterCommitMessage commit() throws IOException {\n-      return new TaskCommit(complete());\n+      this.close();\n+\n+      List<DataFile> dataFiles = complete();\n+      return new TaskCommit(new TaskResult(dataFiles));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9732fcd651878e3ccf84ce8bde1fa8cdeff8120c"}, "originalPosition": 43}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU1NDAyNjI0", "url": "https://github.com/apache/iceberg/pull/1213#pullrequestreview-455402624", "createdAt": "2020-07-26T23:01:23Z", "commit": {"oid": "9732fcd651878e3ccf84ce8bde1fa8cdeff8120c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNlQyMzowMToyM1rOG3P1Bw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNlQyMzowMToyM1rOG3P1Bw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDU4NDE5OQ==", "bodyText": "#1232 and #1237 rebuild the Avro and Parquet writers to use RowData instead of Row. To deconflict, do you think it makes sense to get the base classes and Spark refactor in this PR and separate out the Flink side? I'm fine either way, whatever you think is going to be easier.", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r460584199", "createdAt": "2020-07-26T23:01:23Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/TaskWriterFactory.java", "diffHunk": "@@ -0,0 +1,122 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.Map;\n+import org.apache.flink.types.Row;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.MetricsConfig;\n+import org.apache.iceberg.PartitionKey;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.flink.data.FlinkAvroWriter;\n+import org.apache.iceberg.flink.data.FlinkParquetWriters;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.io.FileAppenderFactory;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.io.OutputFileFactory;\n+import org.apache.iceberg.io.PartitionedFanoutWriter;\n+import org.apache.iceberg.io.TaskWriter;\n+import org.apache.iceberg.io.UnpartitionedWriter;\n+import org.apache.iceberg.parquet.Parquet;\n+\n+class TaskWriterFactory {\n+  private TaskWriterFactory() {\n+  }\n+\n+  static TaskWriter<Row> createTaskWriter(Schema schema,\n+                                          PartitionSpec spec,\n+                                          FileFormat format,\n+                                          FileAppenderFactory<Row> appenderFactory,\n+                                          OutputFileFactory fileFactory,\n+                                          FileIO io,\n+                                          long targetFileSizeBytes) {\n+    if (spec.fields().isEmpty()) {\n+      return new UnpartitionedWriter<>(spec, format, appenderFactory, fileFactory, io, targetFileSizeBytes);\n+    } else {\n+      return new RowPartitionedFanoutWriter(spec, format, appenderFactory, fileFactory,\n+          io, targetFileSizeBytes, schema);\n+    }\n+  }\n+\n+  private static class RowPartitionedFanoutWriter extends PartitionedFanoutWriter<Row> {\n+\n+    private final PartitionKey partitionKey;\n+    private final RowWrapper rowWrapper;\n+\n+    RowPartitionedFanoutWriter(PartitionSpec spec, FileFormat format, FileAppenderFactory<Row> appenderFactory,\n+                               OutputFileFactory fileFactory, FileIO io, long targetFileSize, Schema schema) {\n+      super(spec, format, appenderFactory, fileFactory, io, targetFileSize);\n+      this.partitionKey = new PartitionKey(spec, schema);\n+      this.rowWrapper = new RowWrapper(schema.asStruct());\n+    }\n+\n+    @Override\n+    protected PartitionKey partition(Row row) {\n+      partitionKey.partition(rowWrapper.wrap(row));\n+      return partitionKey;\n+    }\n+  }\n+\n+  static class FlinkFileAppenderFactory implements FileAppenderFactory<Row> {\n+    private final Schema schema;\n+    private final Map<String, String> props;\n+\n+    FlinkFileAppenderFactory(Schema schema, Map<String, String> props) {\n+      this.schema = schema;\n+      this.props = props;\n+    }\n+\n+    @Override\n+    public FileAppender<Row> newAppender(OutputFile outputFile, FileFormat format) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9732fcd651878e3ccf84ce8bde1fa8cdeff8120c"}, "originalPosition": 92}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU1NDAzMjgy", "url": "https://github.com/apache/iceberg/pull/1213#pullrequestreview-455403282", "createdAt": "2020-07-26T23:11:18Z", "commit": {"oid": "9732fcd651878e3ccf84ce8bde1fa8cdeff8120c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNlQyMzoxMToxOFrOG3P4SQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNlQyMzoxMToxOFrOG3P4SQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDU4NTAzMw==", "bodyText": "Many iterator classes don't implement remove. What about iterating over the key set separately instead?\n  if (!writers.isEmpty()) {\n    for (PartitionKey key : writers.keySet()) {\n      RollingFileAppender writer = writers.remove(key);\n      writer.close();\n    }\n  }", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r460585033", "createdAt": "2020-07-26T23:11:18Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/io/PartitionedFanoutWriter.java", "diffHunk": "@@ -0,0 +1,73 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.io;\n+\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.Map;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionKey;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+\n+public abstract class PartitionedFanoutWriter<T> extends BaseTaskWriter<T> {\n+  private final Map<PartitionKey, RollingFileAppender> writers = Maps.newHashMap();\n+\n+  public PartitionedFanoutWriter(PartitionSpec spec, FileFormat format, FileAppenderFactory<T> appenderFactory,\n+                                 OutputFileFactory fileFactory, FileIO io, long targetFileSize) {\n+    super(spec, format, appenderFactory, fileFactory, io, targetFileSize);\n+  }\n+\n+  /**\n+   * Create a PartitionKey from the values in row.\n+   * <p>\n+   * Any PartitionKey returned by this method can be reused by the implementation.\n+   *\n+   * @param row a data row\n+   */\n+  protected abstract PartitionKey partition(T row);\n+\n+  @Override\n+  public void write(T row) throws IOException {\n+    PartitionKey partitionKey = partition(row);\n+\n+    RollingFileAppender writer = writers.get(partitionKey);\n+    if (writer == null) {\n+      // NOTICE: we need to copy a new partition key here, in case of messing up the keys in writers.\n+      PartitionKey copiedKey = partitionKey.copy();\n+      writer = new RollingFileAppender(copiedKey);\n+      writers.put(copiedKey, writer);\n+    }\n+\n+    writer.add(row);\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    if (!writers.isEmpty()) {\n+      Iterator<RollingFileAppender> iterator = writers.values().iterator();\n+      while (iterator.hasNext()) {\n+        iterator.next().close();\n+        // Remove from the writers after closed.\n+        iterator.remove();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9732fcd651878e3ccf84ce8bde1fa8cdeff8120c"}, "originalPosition": 69}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU1NDA0MzE2", "url": "https://github.com/apache/iceberg/pull/1213#pullrequestreview-455404316", "createdAt": "2020-07-26T23:24:19Z", "commit": {"oid": "9732fcd651878e3ccf84ce8bde1fa8cdeff8120c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNlQyMzoyNDoxOVrOG3P9cw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNlQyMzoyNDoxOVrOG3P9cw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDU4NjM1NQ==", "bodyText": "I think this is fine, but you might want to move this into Flink and combine it with the Flink-specific writer. There are a lot of concerns that might need to change for this class, like using a LRU cache for writers, incrementally releasing files, etc. Since this is only used by Flink, we might just want to iterate on it there instead of trying to maintain this as an independent class. We can always bring it back out when we have an additional use case.", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r460586355", "createdAt": "2020-07-26T23:24:19Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/io/PartitionedFanoutWriter.java", "diffHunk": "@@ -0,0 +1,73 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.io;\n+\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.Map;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionKey;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+\n+public abstract class PartitionedFanoutWriter<T> extends BaseTaskWriter<T> {\n+  private final Map<PartitionKey, RollingFileAppender> writers = Maps.newHashMap();\n+\n+  public PartitionedFanoutWriter(PartitionSpec spec, FileFormat format, FileAppenderFactory<T> appenderFactory,\n+                                 OutputFileFactory fileFactory, FileIO io, long targetFileSize) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9732fcd651878e3ccf84ce8bde1fa8cdeff8120c"}, "originalPosition": 34}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU4ODQ5MDI2", "url": "https://github.com/apache/iceberg/pull/1213#pullrequestreview-458849026", "createdAt": "2020-07-31T00:33:29Z", "commit": {"oid": "7135c10f2caff4dc5b0de95c9928f68cdfa94f75"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMDozMzoyOVrOG54RtA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMDozMzoyOVrOG54RtA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM0NDA1Mg==", "bodyText": "The Javadoc for a release should not be modified. I think this is probably a search and replace error.", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r463344052", "createdAt": "2020-07-31T00:33:29Z", "author": {"login": "rdblue"}, "path": "site/docs/javadoc/0.9.0/serialized-form.html", "diffHunk": "@@ -1031,7 +1031,7 @@ <h4>spec</h4>\n <li class=\"blockList\"><a id=\"org.apache.iceberg.spark.source.SparkBatchWrite.TaskCommit\">\n <!--   -->\n </a>\n-<h3>Class org.apache.iceberg.spark.source.SparkBatchWrite.TaskCommit extends org.apache.iceberg.spark.source.TaskResult implements Serializable</h3>\n+<h3>Class org.apache.iceberg.spark.source.SparkBatchWrite.TaskCommit implements Serializable</h3>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7135c10f2caff4dc5b0de95c9928f68cdfa94f75"}, "originalPosition": 5}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU4ODUxMDAz", "url": "https://github.com/apache/iceberg/pull/1213#pullrequestreview-458851003", "createdAt": "2020-07-31T00:41:11Z", "commit": {"oid": "7135c10f2caff4dc5b0de95c9928f68cdfa94f75"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMDo0MToxMVrOG54ZEQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMDo0MToxMVrOG54ZEQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM0NTkzNw==", "bodyText": "Unpartitioned writers pass a null partition key. Would it make more sense to use that instead of using spec?", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r463345937", "createdAt": "2020-07-31T00:41:11Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/io/BaseTaskWriter.java", "diffHunk": "@@ -0,0 +1,154 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.io;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.List;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.PartitionKey;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.encryption.EncryptedOutputFile;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.util.Tasks;\n+\n+public abstract class BaseTaskWriter<T> implements TaskWriter<T> {\n+  private final List<DataFile> completedFiles = Lists.newArrayList();\n+  private final PartitionSpec spec;\n+  private final FileFormat format;\n+  private final FileAppenderFactory<T> appenderFactory;\n+  private final OutputFileFactory fileFactory;\n+  private final FileIO io;\n+  private final long targetFileSize;\n+\n+  protected BaseTaskWriter(PartitionSpec spec, FileFormat format, FileAppenderFactory<T> appenderFactory,\n+                           OutputFileFactory fileFactory, FileIO io, long targetFileSize) {\n+    this.spec = spec;\n+    this.format = format;\n+    this.appenderFactory = appenderFactory;\n+    this.fileFactory = fileFactory;\n+    this.io = io;\n+    this.targetFileSize = targetFileSize;\n+  }\n+\n+  @Override\n+  public void abort() throws IOException {\n+    close();\n+\n+    // clean up files created by this writer\n+    Tasks.foreach(completedFiles)\n+        .throwFailureWhenFinished()\n+        .noRetry()\n+        .run(file -> io.deleteFile(file.path().toString()));\n+  }\n+\n+  @Override\n+  public List<DataFile> complete() throws IOException {\n+    close();\n+\n+    if (completedFiles.size() > 0) {\n+      return ImmutableList.copyOf(completedFiles);\n+    } else {\n+      return Collections.emptyList();\n+    }\n+  }\n+\n+  protected class RollingFileWriter implements Closeable {\n+    private static final int ROWS_DIVISOR = 1000;\n+    private final PartitionKey partitionKey;\n+\n+    private EncryptedOutputFile currentFile = null;\n+    private FileAppender<T> currentAppender = null;\n+    private long currentRows = 0;\n+\n+    public RollingFileWriter(PartitionKey partitionKey) {\n+      this.partitionKey = partitionKey;\n+    }\n+\n+    public void add(T record) throws IOException {\n+      if (currentAppender == null) {\n+        openCurrent();\n+      }\n+\n+      this.currentAppender.add(record);\n+      this.currentRows++;\n+\n+      if (shouldRollToNewFile()) {\n+        closeCurrent();\n+      }\n+    }\n+\n+    private void openCurrent() {\n+      if (spec.fields().size() == 0) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7135c10f2caff4dc5b0de95c9928f68cdfa94f75"}, "originalPosition": 104}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU4ODUxODg0", "url": "https://github.com/apache/iceberg/pull/1213#pullrequestreview-458851884", "createdAt": "2020-07-31T00:44:30Z", "commit": {"oid": "7135c10f2caff4dc5b0de95c9928f68cdfa94f75"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMDo0NDozMFrOG54cOA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMDo0NDozMFrOG54cOA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM0Njc0NA==", "bodyText": "This should be an Array, like it was when this class was based on TaskResult. Arrays are easier to handle when working with Serializable classes because we don't have to worry about bugs caused by List implementations (like the recent Kryo bug with unmodifiable lists).", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r463346744", "createdAt": "2020-07-31T00:44:30Z", "author": {"login": "rdblue"}, "path": "spark2/src/main/java/org/apache/iceberg/spark/source/Writer.java", "diffHunk": "@@ -209,9 +212,15 @@ public String toString() {\n     return String.format(\"IcebergWrite(table=%s, format=%s)\", table, format);\n   }\n \n-  private static class TaskCommit extends TaskResult implements WriterCommitMessage {\n-    TaskCommit(TaskResult toCopy) {\n-      super(toCopy.files());\n+  private static class TaskCommit implements WriterCommitMessage {\n+    private final List<DataFile> taskFiles;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7135c10f2caff4dc5b0de95c9928f68cdfa94f75"}, "originalPosition": 34}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU4ODUyMDE2", "url": "https://github.com/apache/iceberg/pull/1213#pullrequestreview-458852016", "createdAt": "2020-07-31T00:44:59Z", "commit": {"oid": "7135c10f2caff4dc5b0de95c9928f68cdfa94f75"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMDo0NDo1OVrOG54csg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMDo0NDo1OVrOG54csg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM0Njg2Ng==", "bodyText": "No need to use the prefix this for close calls, is there?", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r463346866", "createdAt": "2020-07-31T00:44:59Z", "author": {"login": "rdblue"}, "path": "spark2/src/main/java/org/apache/iceberg/spark/source/Writer.java", "diffHunk": "@@ -250,32 +259,39 @@ public String toString() {\n       if (spec.fields().isEmpty()) {\n         return new Unpartitioned24Writer(spec, format, appenderFactory, fileFactory, io.value(), targetFileSize);\n       } else {\n-        return new Partitioned24Writer(\n-            spec, format, appenderFactory, fileFactory, io.value(), targetFileSize, writeSchema);\n+        return new Partitioned24Writer(spec, format, appenderFactory, fileFactory, io.value(),\n+            targetFileSize, writeSchema, dsSchema);\n       }\n     }\n   }\n \n-  private static class Unpartitioned24Writer extends UnpartitionedWriter implements DataWriter<InternalRow> {\n+  private static class Unpartitioned24Writer extends UnpartitionedWriter<InternalRow>\n+      implements DataWriter<InternalRow> {\n     Unpartitioned24Writer(PartitionSpec spec, FileFormat format, SparkAppenderFactory appenderFactory,\n                           OutputFileFactory fileFactory, FileIO fileIo, long targetFileSize) {\n       super(spec, format, appenderFactory, fileFactory, fileIo, targetFileSize);\n     }\n \n     @Override\n     public WriterCommitMessage commit() throws IOException {\n+      this.close();\n+\n       return new TaskCommit(complete());\n     }\n   }\n \n-  private static class Partitioned24Writer extends PartitionedWriter implements DataWriter<InternalRow> {\n+  private static class Partitioned24Writer extends SparkPartitionedWriter implements DataWriter<InternalRow> {\n+\n     Partitioned24Writer(PartitionSpec spec, FileFormat format, SparkAppenderFactory appenderFactory,\n-                               OutputFileFactory fileFactory, FileIO fileIo, long targetFileSize, Schema writeSchema) {\n-      super(spec, format, appenderFactory, fileFactory, fileIo, targetFileSize, writeSchema);\n+                        OutputFileFactory fileFactory, FileIO fileIo, long targetFileSize,\n+                        Schema schema, StructType sparkSchema) {\n+      super(spec, format, appenderFactory, fileFactory, fileIo, targetFileSize, schema, sparkSchema);\n     }\n \n     @Override\n     public WriterCommitMessage commit() throws IOException {\n+      this.close();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7135c10f2caff4dc5b0de95c9928f68cdfa94f75"}, "originalPosition": 86}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU4ODUyMDg0", "url": "https://github.com/apache/iceberg/pull/1213#pullrequestreview-458852084", "createdAt": "2020-07-31T00:45:16Z", "commit": {"oid": "7135c10f2caff4dc5b0de95c9928f68cdfa94f75"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMDo0NToxNlrOG54dAw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMDo0NToxNlrOG54dAw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM0Njk0Nw==", "bodyText": "Same here, this class should use an Array of data files.", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r463346947", "createdAt": "2020-07-31T00:45:16Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/source/SparkBatchWrite.java", "diffHunk": "@@ -226,9 +229,15 @@ public String toString() {\n     return String.format(\"IcebergWrite(table=%s, format=%s)\", table, format);\n   }\n \n-  public static class TaskCommit extends TaskResult implements WriterCommitMessage {\n-    TaskCommit(TaskResult result) {\n-      super(result.files());\n+  public static class TaskCommit implements WriterCommitMessage {\n+    private final List<DataFile> taskFiles;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7135c10f2caff4dc5b0de95c9928f68cdfa94f75"}, "originalPosition": 25}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU4ODUzMTIw", "url": "https://github.com/apache/iceberg/pull/1213#pullrequestreview-458853120", "createdAt": "2020-07-31T00:48:22Z", "commit": {"oid": "7135c10f2caff4dc5b0de95c9928f68cdfa94f75"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMDo0ODoyMlrOG54gxw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMDo0ODoyMlrOG54gxw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM0NzkxMQ==", "bodyText": "Is this method needed? Why not merge it with close?", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r463347911", "createdAt": "2020-07-31T00:48:22Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/io/UnpartitionedWriter.java", "diffHunk": "@@ -17,24 +17,42 @@\n  * under the License.\n  */\n \n-package org.apache.iceberg.spark.source;\n+package org.apache.iceberg.io;\n \n import java.io.IOException;\n import org.apache.iceberg.FileFormat;\n import org.apache.iceberg.PartitionSpec;\n-import org.apache.iceberg.io.FileIO;\n-import org.apache.spark.sql.catalyst.InternalRow;\n \n-class UnpartitionedWriter extends BaseWriter {\n-  UnpartitionedWriter(PartitionSpec spec, FileFormat format, SparkAppenderFactory appenderFactory,\n-                      OutputFileFactory fileFactory, FileIO io, long targetFileSize) {\n+public class UnpartitionedWriter<T> extends BaseTaskWriter<T> {\n+\n+  private RollingFileWriter currentWriter = null;\n+\n+  public UnpartitionedWriter(PartitionSpec spec, FileFormat format, FileAppenderFactory<T> appenderFactory,\n+                             OutputFileFactory fileFactory, FileIO io, long targetFileSize) {\n     super(spec, format, appenderFactory, fileFactory, io, targetFileSize);\n+  }\n \n-    openCurrent();\n+  @Override\n+  public void write(T record) throws IOException {\n+    if (currentWriter == null) {\n+      currentWriter = new RollingFileWriter(null);\n+    }\n+    currentWriter.add(record);\n   }\n \n   @Override\n-  public void write(InternalRow row) throws IOException {\n-    writeInternal(row);\n+  public void close() throws IOException {\n+    closeCurrent();\n+  }\n+\n+  private void closeCurrent() throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7135c10f2caff4dc5b0de95c9928f68cdfa94f75"}, "originalPosition": 41}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU4ODU3NTU4", "url": "https://github.com/apache/iceberg/pull/1213#pullrequestreview-458857558", "createdAt": "2020-07-31T01:03:12Z", "commit": {"oid": "7135c10f2caff4dc5b0de95c9928f68cdfa94f75"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMTowMzoxMlrOG54wtA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMTowMzoxMlrOG54wtA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM1MTk4OA==", "bodyText": "I think this PR should not change writers to be lazily created.\nFirst, it changes the assumptions in the writers, which doesn't make sense to include in what is primarily a refactor.\nSecond, I think those assumptions were a better structure for these classes. Opening the file in the constructor and relying on it always being there avoids a null check in write, which is called in a tight loop. The main benefit of this is to avoid a delete in close when no records were written, but that check is still present in RollingFileWriter. And I think that check should be there because it is another helpful invariant: if a 0-record file is produced by any writer wrapped by RollingFileWriter, then it should be discarded. That helps avoid the problem in future implementations, which may not consider the case.\nThis is fairly minor, but since there are other changes needed (in particular, the array fix for task commit messages), I'd like to change at least the Spark writers back to eagerly creating output files instead of lazily checking for null in write.", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r463351988", "createdAt": "2020-07-31T01:03:12Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/io/UnpartitionedWriter.java", "diffHunk": "@@ -17,24 +17,42 @@\n  * under the License.\n  */\n \n-package org.apache.iceberg.spark.source;\n+package org.apache.iceberg.io;\n \n import java.io.IOException;\n import org.apache.iceberg.FileFormat;\n import org.apache.iceberg.PartitionSpec;\n-import org.apache.iceberg.io.FileIO;\n-import org.apache.spark.sql.catalyst.InternalRow;\n \n-class UnpartitionedWriter extends BaseWriter {\n-  UnpartitionedWriter(PartitionSpec spec, FileFormat format, SparkAppenderFactory appenderFactory,\n-                      OutputFileFactory fileFactory, FileIO io, long targetFileSize) {\n+public class UnpartitionedWriter<T> extends BaseTaskWriter<T> {\n+\n+  private RollingFileWriter currentWriter = null;\n+\n+  public UnpartitionedWriter(PartitionSpec spec, FileFormat format, FileAppenderFactory<T> appenderFactory,\n+                             OutputFileFactory fileFactory, FileIO io, long targetFileSize) {\n     super(spec, format, appenderFactory, fileFactory, io, targetFileSize);\n+  }\n \n-    openCurrent();\n+  @Override\n+  public void write(T record) throws IOException {\n+    if (currentWriter == null) {\n+      currentWriter = new RollingFileWriter(null);\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7135c10f2caff4dc5b0de95c9928f68cdfa94f75"}, "originalPosition": 30}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e7bc4e6dd913cdcfdd43bcb445b0875509bc23a2", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/e7bc4e6dd913cdcfdd43bcb445b0875509bc23a2", "committedDate": "2020-07-31T06:43:35Z", "message": "Abstract the generic task writers for sharing the common codes between spark and flink."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5f1b29ee019e7379e95671099c41209aab14dd2a", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/5f1b29ee019e7379e95671099c41209aab14dd2a", "committedDate": "2020-07-31T06:43:35Z", "message": "Minor fixes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "14e4dc5ef381213d417c2a68273584e68f51a931", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/14e4dc5ef381213d417c2a68273584e68f51a931", "committedDate": "2020-07-31T06:43:35Z", "message": "Add flink task writers and unit tests."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f1ccdfd691eb3062ba66da6ea21470eb37f65c16", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/f1ccdfd691eb3062ba66da6ea21470eb37f65c16", "committedDate": "2020-07-31T06:43:35Z", "message": "Adjust the unit tests."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8fe27c2104d32886978b004d112927c69936eefe", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/8fe27c2104d32886978b004d112927c69936eefe", "committedDate": "2020-07-31T06:43:35Z", "message": "Addressing the failure unit tests."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a586c5112f348fae3de210dcc2b6e6a8ce47e6e3", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/a586c5112f348fae3de210dcc2b6e6a8ce47e6e3", "committedDate": "2020-07-31T06:43:35Z", "message": "Add unit test and more javadoc"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "93a9318e9f53849fa8095c331434c3cbfabde583", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/93a9318e9f53849fa8095c331434c3cbfabde583", "committedDate": "2020-07-31T06:43:35Z", "message": "Fix the broken TestRewriteDataFilesAction"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bb69950dfd0d4456ca74b78426f5aa05d4cf2dc3", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/bb69950dfd0d4456ca74b78426f5aa05d4cf2dc3", "committedDate": "2020-07-31T06:43:35Z", "message": "Add javadoc for FileAppenderFactory"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "abb0b5d6a7608cc9cd5ed8e9e1ad289c5cdacd06", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/abb0b5d6a7608cc9cd5ed8e9e1ad289c5cdacd06", "committedDate": "2020-07-31T06:43:35Z", "message": "More unit tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9f1fa70d55eb563199395ebc0274a290d5713c39", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/9f1fa70d55eb563199395ebc0274a290d5713c39", "committedDate": "2020-07-31T06:44:20Z", "message": "Addressing the comment."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "499a48f30667b9f3b00fea9de01421bbba78a4a0", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/499a48f30667b9f3b00fea9de01421bbba78a4a0", "committedDate": "2020-07-31T06:45:05Z", "message": "Remove the keyGetter and use the abstract partitioned(..) method."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "21be53a686aab6c6be54334a7598377d148b4856", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/21be53a686aab6c6be54334a7598377d148b4856", "committedDate": "2020-07-31T06:45:05Z", "message": "Make the few public classes/methods to be package-access"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "61dde9b99619e1de92de96358ae39239c5416179", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/61dde9b99619e1de92de96358ae39239c5416179", "committedDate": "2020-07-31T06:45:05Z", "message": "Addressing the comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8a0920796a1958c9d92e8f564b60d171c5007786", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/8a0920796a1958c9d92e8f564b60d171c5007786", "committedDate": "2020-07-31T06:46:07Z", "message": "Remove the public modifiers from PartitionedFanoutWriter"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0c7423aed5800862f2830d290b204e099926b1f8", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/0c7423aed5800862f2830d290b204e099926b1f8", "committedDate": "2020-07-31T08:04:02Z", "message": "Addressing the lastest comment from Ryan Blue."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "7135c10f2caff4dc5b0de95c9928f68cdfa94f75", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/7135c10f2caff4dc5b0de95c9928f68cdfa94f75", "committedDate": "2020-07-27T07:34:33Z", "message": "Remove the public modifiers from PartitionedFanoutWriter"}, "afterCommit": {"oid": "0c7423aed5800862f2830d290b204e099926b1f8", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/0c7423aed5800862f2830d290b204e099926b1f8", "committedDate": "2020-07-31T08:04:02Z", "message": "Addressing the lastest comment from Ryan Blue."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e6152d19453d144bf8a7bade90ede997537c5eda", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/e6152d19453d144bf8a7bade90ede997537c5eda", "committedDate": "2020-07-31T18:38:37Z", "message": "Fix NullPointerException in PartitionedWriter"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4276, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}