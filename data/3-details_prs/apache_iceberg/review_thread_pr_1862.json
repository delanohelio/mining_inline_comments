{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTMxMTQ0ODUx", "number": 1862, "reviewThreads": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQxNjo0MTo0N1rOE_tYDA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wM1QxMDowNDowMFrOFAIiZA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM1MjM5MTgwOnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/iceberg/TableProperties.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQxNjo0MTo0N1rOH9kDKQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQxNDoyMToxNlrOH_S9ew==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDMxNTgxNw==", "bodyText": "Well, I wanted it to be write.delete.isolation-level but we have no way to find out which operation is being performed in the merge builder.", "url": "https://github.com/apache/iceberg/pull/1862#discussion_r534315817", "createdAt": "2020-12-02T16:41:47Z", "author": {"login": "aokolnychyi"}, "path": "core/src/main/java/org/apache/iceberg/TableProperties.java", "diffHunk": "@@ -140,4 +140,10 @@ private TableProperties() {\n \n   public static final String GC_ENABLED = \"gc.enabled\";\n   public static final boolean GC_ENABLED_DEFAULT = true;\n+\n+  public static final String WRITE_ISOLATION_LEVEL = \"write.isolation-level\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "225167896ff7c3b7c4c59ff7998306673dbf9904"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTQyMTMyMA==", "bodyText": "It might be better to use write.row-level.isolation-level so it matches the mode property. This doesn't apply to INSERT OVERWRITE so it would be nice to be clear about it.\nNot sure if we can make that property name better. Seems odd to have row-level and isolation-level next to one another since \"row-level\" is specifying the granularity of the changes and \"isolation-level\" is actually choosing an option.", "url": "https://github.com/apache/iceberg/pull/1862#discussion_r535421320", "createdAt": "2020-12-03T17:07:40Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/TableProperties.java", "diffHunk": "@@ -140,4 +140,10 @@ private TableProperties() {\n \n   public static final String GC_ENABLED = \"gc.enabled\";\n   public static final boolean GC_ENABLED_DEFAULT = true;\n+\n+  public static final String WRITE_ISOLATION_LEVEL = \"write.isolation-level\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDMxNTgxNw=="}, "originalCommit": {"oid": "225167896ff7c3b7c4c59ff7998306673dbf9904"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjEzMjk4Nw==", "bodyText": "Fixed.", "url": "https://github.com/apache/iceberg/pull/1862#discussion_r536132987", "createdAt": "2020-12-04T14:21:16Z", "author": {"login": "aokolnychyi"}, "path": "core/src/main/java/org/apache/iceberg/TableProperties.java", "diffHunk": "@@ -140,4 +140,10 @@ private TableProperties() {\n \n   public static final String GC_ENABLED = \"gc.enabled\";\n   public static final boolean GC_ENABLED_DEFAULT = true;\n+\n+  public static final String WRITE_ISOLATION_LEVEL = \"write.isolation-level\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDMxNTgxNw=="}, "originalCommit": {"oid": "225167896ff7c3b7c4c59ff7998306673dbf9904"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM1MjM5Njk0OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/iceberg/TableProperties.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQxNjo0Mjo1MlrOH9kGPA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQxNjo0Mjo1MlrOH9kGPA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDMxNjYwNA==", "bodyText": "Same here. I wanted it to be write.delete.mode.", "url": "https://github.com/apache/iceberg/pull/1862#discussion_r534316604", "createdAt": "2020-12-02T16:42:52Z", "author": {"login": "aokolnychyi"}, "path": "core/src/main/java/org/apache/iceberg/TableProperties.java", "diffHunk": "@@ -140,4 +140,10 @@ private TableProperties() {\n \n   public static final String GC_ENABLED = \"gc.enabled\";\n   public static final boolean GC_ENABLED_DEFAULT = true;\n+\n+  public static final String WRITE_ISOLATION_LEVEL = \"write.isolation-level\";\n+  public static final String WRITE_ISOLATION_LEVEL_DEFAULT = \"serializable\";\n+\n+  public static final String WRITE_ROW_LEVEL_MODE = \"write.row-level.mode\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8ecea59d1035bcc73931cb923314bf7db4392b9c"}, "originalPosition": 8}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM1MzQ3NjUzOnYy", "diffSide": "RIGHT", "path": "api/src/main/java/org/apache/iceberg/IsolationLevel.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQyMToxNToyMVrOH9ubzQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQxNDoyMTozMFrOH_S-OQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDQ4NTk2NQ==", "bodyText": "Does this need to be in api? I think it is only used to parse a string and hold the result. For now, I would move it to core to avoid exposing it too widely.\nFYI @jacques-n: isolation level for a single table.", "url": "https://github.com/apache/iceberg/pull/1862#discussion_r534485965", "createdAt": "2020-12-02T21:15:21Z", "author": {"login": "rdblue"}, "path": "api/src/main/java/org/apache/iceberg/IsolationLevel.java", "diffHunk": "@@ -0,0 +1,39 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+/**\n+ * An isolation level in a table.\n+ * <p>\n+ * Two isolation levels are supported: serializable and snapshot isolation. Both of them provide\n+ * a read consistent view of the table to all operations and allow readers to see only already\n+ * committed data. While serializable is the strongest isolation level in databases,\n+ * snapshot isolation is beneficial for environments with many concurrent writers.\n+ * <p>\n+ * The serializable isolation level guarantees that an ongoing UPDATE/DELETE/MERGE operation\n+ * fails if a concurrent transaction commits a new file that might contain rows matching\n+ * the condition used in UPDATE/DELETE/MERGE. For example, if there is an ongoing update\n+ * on a subset of rows and a concurrent transaction adds a new file with records\n+ * that potentially match the update condition, the update operation must fail under\n+ * the serializable isolation but can still commit under the snapshot isolation.\n+ */\n+public enum IsolationLevel {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8ecea59d1035bcc73931cb923314bf7db4392b9c"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTA0MzExNg==", "bodyText": "Sound good to me.", "url": "https://github.com/apache/iceberg/pull/1862#discussion_r535043116", "createdAt": "2020-12-03T10:05:44Z", "author": {"login": "aokolnychyi"}, "path": "api/src/main/java/org/apache/iceberg/IsolationLevel.java", "diffHunk": "@@ -0,0 +1,39 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+/**\n+ * An isolation level in a table.\n+ * <p>\n+ * Two isolation levels are supported: serializable and snapshot isolation. Both of them provide\n+ * a read consistent view of the table to all operations and allow readers to see only already\n+ * committed data. While serializable is the strongest isolation level in databases,\n+ * snapshot isolation is beneficial for environments with many concurrent writers.\n+ * <p>\n+ * The serializable isolation level guarantees that an ongoing UPDATE/DELETE/MERGE operation\n+ * fails if a concurrent transaction commits a new file that might contain rows matching\n+ * the condition used in UPDATE/DELETE/MERGE. For example, if there is an ongoing update\n+ * on a subset of rows and a concurrent transaction adds a new file with records\n+ * that potentially match the update condition, the update operation must fail under\n+ * the serializable isolation but can still commit under the snapshot isolation.\n+ */\n+public enum IsolationLevel {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDQ4NTk2NQ=="}, "originalCommit": {"oid": "8ecea59d1035bcc73931cb923314bf7db4392b9c"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjEzMzE3Nw==", "bodyText": "Moved.", "url": "https://github.com/apache/iceberg/pull/1862#discussion_r536133177", "createdAt": "2020-12-04T14:21:30Z", "author": {"login": "aokolnychyi"}, "path": "api/src/main/java/org/apache/iceberg/IsolationLevel.java", "diffHunk": "@@ -0,0 +1,39 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+/**\n+ * An isolation level in a table.\n+ * <p>\n+ * Two isolation levels are supported: serializable and snapshot isolation. Both of them provide\n+ * a read consistent view of the table to all operations and allow readers to see only already\n+ * committed data. While serializable is the strongest isolation level in databases,\n+ * snapshot isolation is beneficial for environments with many concurrent writers.\n+ * <p>\n+ * The serializable isolation level guarantees that an ongoing UPDATE/DELETE/MERGE operation\n+ * fails if a concurrent transaction commits a new file that might contain rows matching\n+ * the condition used in UPDATE/DELETE/MERGE. For example, if there is an ongoing update\n+ * on a subset of rows and a concurrent transaction adds a new file with records\n+ * that potentially match the update condition, the update operation must fail under\n+ * the serializable isolation but can still commit under the snapshot isolation.\n+ */\n+public enum IsolationLevel {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDQ4NTk2NQ=="}, "originalCommit": {"oid": "8ecea59d1035bcc73931cb923314bf7db4392b9c"}, "originalPosition": 37}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM1NDQ1ODI4OnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/SparkMergeBuilder.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wM1QwMzowNzoyNlrOH93dRA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQxNDoyMTozOFrOH_S-oA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDYzMzc5Ng==", "bodyText": "Shouldn't this return writeInfo.options()?", "url": "https://github.com/apache/iceberg/pull/1862#discussion_r534633796", "createdAt": "2020-12-03T03:07:26Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/source/SparkMergeBuilder.java", "diffHunk": "@@ -0,0 +1,110 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import java.util.HashMap;\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.arrow.util.Preconditions;\n+import org.apache.iceberg.IsolationLevel;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.connector.iceberg.write.MergeBuilder;\n+import org.apache.spark.sql.connector.read.Scan;\n+import org.apache.spark.sql.connector.read.ScanBuilder;\n+import org.apache.spark.sql.connector.write.LogicalWriteInfo;\n+import org.apache.spark.sql.connector.write.WriteBuilder;\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap;\n+\n+class SparkMergeBuilder implements MergeBuilder {\n+\n+  private final SparkSession spark;\n+  private final Table table;\n+  private final LogicalWriteInfo writeInfo;\n+  private final IsolationLevel isolationLevel;\n+\n+  // lazy vars\n+  private ScanBuilder lazyScanBuilder;\n+  private Scan configuredScan;\n+  private WriteBuilder lazyWriteBuilder;\n+\n+  SparkMergeBuilder(SparkSession spark, Table table, LogicalWriteInfo writeInfo) {\n+    this.spark = spark;\n+    this.table = table;\n+    this.writeInfo = writeInfo;\n+\n+    String isolationLevelAsString = table.properties().getOrDefault(\n+        TableProperties.WRITE_ISOLATION_LEVEL,\n+        TableProperties.WRITE_ISOLATION_LEVEL_DEFAULT\n+    ).toUpperCase(Locale.ROOT);\n+    this.isolationLevel = IsolationLevel.valueOf(isolationLevelAsString);\n+  }\n+\n+  @Override\n+  public ScanBuilder asScanBuilder() {\n+    return scanBuilder();\n+  }\n+\n+  private ScanBuilder scanBuilder() {\n+    if (lazyScanBuilder == null) {\n+      SparkScanBuilder scanBuilder = new SparkScanBuilder(spark, table, scanOptions()) {\n+        public Scan build() {\n+          Scan scan = super.buildMergeScan();\n+          SparkMergeBuilder.this.configuredScan = scan;\n+          return scan;\n+        }\n+      };\n+      // ignore residuals to ensure we read full files\n+      lazyScanBuilder = scanBuilder.ignoreResiduals();\n+    }\n+\n+    return lazyScanBuilder;\n+  }\n+\n+  private CaseInsensitiveStringMap scanOptions() {\n+    Snapshot currentSnapshot = table.currentSnapshot();\n+\n+    if (currentSnapshot == null) {\n+      return CaseInsensitiveStringMap.empty();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8ecea59d1035bcc73931cb923314bf7db4392b9c"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTA0NDA3Mw==", "bodyText": "It should, plus we need to validate no snapshot-id is set in options. Good catch.", "url": "https://github.com/apache/iceberg/pull/1862#discussion_r535044073", "createdAt": "2020-12-03T10:06:39Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/source/SparkMergeBuilder.java", "diffHunk": "@@ -0,0 +1,110 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import java.util.HashMap;\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.arrow.util.Preconditions;\n+import org.apache.iceberg.IsolationLevel;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.connector.iceberg.write.MergeBuilder;\n+import org.apache.spark.sql.connector.read.Scan;\n+import org.apache.spark.sql.connector.read.ScanBuilder;\n+import org.apache.spark.sql.connector.write.LogicalWriteInfo;\n+import org.apache.spark.sql.connector.write.WriteBuilder;\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap;\n+\n+class SparkMergeBuilder implements MergeBuilder {\n+\n+  private final SparkSession spark;\n+  private final Table table;\n+  private final LogicalWriteInfo writeInfo;\n+  private final IsolationLevel isolationLevel;\n+\n+  // lazy vars\n+  private ScanBuilder lazyScanBuilder;\n+  private Scan configuredScan;\n+  private WriteBuilder lazyWriteBuilder;\n+\n+  SparkMergeBuilder(SparkSession spark, Table table, LogicalWriteInfo writeInfo) {\n+    this.spark = spark;\n+    this.table = table;\n+    this.writeInfo = writeInfo;\n+\n+    String isolationLevelAsString = table.properties().getOrDefault(\n+        TableProperties.WRITE_ISOLATION_LEVEL,\n+        TableProperties.WRITE_ISOLATION_LEVEL_DEFAULT\n+    ).toUpperCase(Locale.ROOT);\n+    this.isolationLevel = IsolationLevel.valueOf(isolationLevelAsString);\n+  }\n+\n+  @Override\n+  public ScanBuilder asScanBuilder() {\n+    return scanBuilder();\n+  }\n+\n+  private ScanBuilder scanBuilder() {\n+    if (lazyScanBuilder == null) {\n+      SparkScanBuilder scanBuilder = new SparkScanBuilder(spark, table, scanOptions()) {\n+        public Scan build() {\n+          Scan scan = super.buildMergeScan();\n+          SparkMergeBuilder.this.configuredScan = scan;\n+          return scan;\n+        }\n+      };\n+      // ignore residuals to ensure we read full files\n+      lazyScanBuilder = scanBuilder.ignoreResiduals();\n+    }\n+\n+    return lazyScanBuilder;\n+  }\n+\n+  private CaseInsensitiveStringMap scanOptions() {\n+    Snapshot currentSnapshot = table.currentSnapshot();\n+\n+    if (currentSnapshot == null) {\n+      return CaseInsensitiveStringMap.empty();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDYzMzc5Ng=="}, "originalCommit": {"oid": "8ecea59d1035bcc73931cb923314bf7db4392b9c"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTU0MDg3MQ==", "bodyText": "Now that I'm thinking about this more, why choose a snapshot here and pass it to the scan? The scan could determine the snapshot itself without modifying these options. The writer gets the snapshot ID from the scan anyway, so it doesn't matter whether it is determined here or in the scan.", "url": "https://github.com/apache/iceberg/pull/1862#discussion_r535540871", "createdAt": "2020-12-03T19:53:31Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/source/SparkMergeBuilder.java", "diffHunk": "@@ -0,0 +1,110 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import java.util.HashMap;\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.arrow.util.Preconditions;\n+import org.apache.iceberg.IsolationLevel;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.connector.iceberg.write.MergeBuilder;\n+import org.apache.spark.sql.connector.read.Scan;\n+import org.apache.spark.sql.connector.read.ScanBuilder;\n+import org.apache.spark.sql.connector.write.LogicalWriteInfo;\n+import org.apache.spark.sql.connector.write.WriteBuilder;\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap;\n+\n+class SparkMergeBuilder implements MergeBuilder {\n+\n+  private final SparkSession spark;\n+  private final Table table;\n+  private final LogicalWriteInfo writeInfo;\n+  private final IsolationLevel isolationLevel;\n+\n+  // lazy vars\n+  private ScanBuilder lazyScanBuilder;\n+  private Scan configuredScan;\n+  private WriteBuilder lazyWriteBuilder;\n+\n+  SparkMergeBuilder(SparkSession spark, Table table, LogicalWriteInfo writeInfo) {\n+    this.spark = spark;\n+    this.table = table;\n+    this.writeInfo = writeInfo;\n+\n+    String isolationLevelAsString = table.properties().getOrDefault(\n+        TableProperties.WRITE_ISOLATION_LEVEL,\n+        TableProperties.WRITE_ISOLATION_LEVEL_DEFAULT\n+    ).toUpperCase(Locale.ROOT);\n+    this.isolationLevel = IsolationLevel.valueOf(isolationLevelAsString);\n+  }\n+\n+  @Override\n+  public ScanBuilder asScanBuilder() {\n+    return scanBuilder();\n+  }\n+\n+  private ScanBuilder scanBuilder() {\n+    if (lazyScanBuilder == null) {\n+      SparkScanBuilder scanBuilder = new SparkScanBuilder(spark, table, scanOptions()) {\n+        public Scan build() {\n+          Scan scan = super.buildMergeScan();\n+          SparkMergeBuilder.this.configuredScan = scan;\n+          return scan;\n+        }\n+      };\n+      // ignore residuals to ensure we read full files\n+      lazyScanBuilder = scanBuilder.ignoreResiduals();\n+    }\n+\n+    return lazyScanBuilder;\n+  }\n+\n+  private CaseInsensitiveStringMap scanOptions() {\n+    Snapshot currentSnapshot = table.currentSnapshot();\n+\n+    if (currentSnapshot == null) {\n+      return CaseInsensitiveStringMap.empty();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDYzMzc5Ng=="}, "originalCommit": {"oid": "8ecea59d1035bcc73931cb923314bf7db4392b9c"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjEzMzI4MA==", "bodyText": "Done.", "url": "https://github.com/apache/iceberg/pull/1862#discussion_r536133280", "createdAt": "2020-12-04T14:21:38Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/source/SparkMergeBuilder.java", "diffHunk": "@@ -0,0 +1,110 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import java.util.HashMap;\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.arrow.util.Preconditions;\n+import org.apache.iceberg.IsolationLevel;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.connector.iceberg.write.MergeBuilder;\n+import org.apache.spark.sql.connector.read.Scan;\n+import org.apache.spark.sql.connector.read.ScanBuilder;\n+import org.apache.spark.sql.connector.write.LogicalWriteInfo;\n+import org.apache.spark.sql.connector.write.WriteBuilder;\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap;\n+\n+class SparkMergeBuilder implements MergeBuilder {\n+\n+  private final SparkSession spark;\n+  private final Table table;\n+  private final LogicalWriteInfo writeInfo;\n+  private final IsolationLevel isolationLevel;\n+\n+  // lazy vars\n+  private ScanBuilder lazyScanBuilder;\n+  private Scan configuredScan;\n+  private WriteBuilder lazyWriteBuilder;\n+\n+  SparkMergeBuilder(SparkSession spark, Table table, LogicalWriteInfo writeInfo) {\n+    this.spark = spark;\n+    this.table = table;\n+    this.writeInfo = writeInfo;\n+\n+    String isolationLevelAsString = table.properties().getOrDefault(\n+        TableProperties.WRITE_ISOLATION_LEVEL,\n+        TableProperties.WRITE_ISOLATION_LEVEL_DEFAULT\n+    ).toUpperCase(Locale.ROOT);\n+    this.isolationLevel = IsolationLevel.valueOf(isolationLevelAsString);\n+  }\n+\n+  @Override\n+  public ScanBuilder asScanBuilder() {\n+    return scanBuilder();\n+  }\n+\n+  private ScanBuilder scanBuilder() {\n+    if (lazyScanBuilder == null) {\n+      SparkScanBuilder scanBuilder = new SparkScanBuilder(spark, table, scanOptions()) {\n+        public Scan build() {\n+          Scan scan = super.buildMergeScan();\n+          SparkMergeBuilder.this.configuredScan = scan;\n+          return scan;\n+        }\n+      };\n+      // ignore residuals to ensure we read full files\n+      lazyScanBuilder = scanBuilder.ignoreResiduals();\n+    }\n+\n+    return lazyScanBuilder;\n+  }\n+\n+  private CaseInsensitiveStringMap scanOptions() {\n+    Snapshot currentSnapshot = table.currentSnapshot();\n+\n+    if (currentSnapshot == null) {\n+      return CaseInsensitiveStringMap.empty();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDYzMzc5Ng=="}, "originalCommit": {"oid": "8ecea59d1035bcc73931cb923314bf7db4392b9c"}, "originalPosition": 87}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM1NDQ2NjI3OnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/SparkTable.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wM1QwMzoxMTowNVrOH93h3w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wM1QxMDowODo1NlrOH-Qp3w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDYzNDk3NQ==", "bodyText": "Are v2 references committed in Spark?", "url": "https://github.com/apache/iceberg/pull/1862#discussion_r534634975", "createdAt": "2020-12-03T03:11:05Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/source/SparkTable.java", "diffHunk": "@@ -160,6 +167,43 @@ public WriteBuilder newWriteBuilder(LogicalWriteInfo info) {\n     return new SparkWriteBuilder(sparkSession(), icebergTable, info);\n   }\n \n+  @Override\n+  public MergeBuilder newMergeBuilder(LogicalWriteInfo info) {\n+    String mode = icebergTable.properties().getOrDefault(WRITE_ROW_LEVEL_MODE, WRITE_ROW_LEVEL_MODE_DEFAULT);\n+    ValidationException.check(mode.equals(\"copy-on-write\"), \"Unsupported row operations mode: %s\", mode);\n+    return new SparkMergeBuilder(sparkSession(), icebergTable, info);\n+  }\n+\n+  @Override\n+  public boolean canDeleteWhere(Filter[] filters) {\n+    if (table().specs().size() > 1) {\n+      // cannot guarantee a metadata delete will be successful if we have multiple specs\n+      return false;\n+    }\n+\n+    Set<Integer> identitySourceIds = table().spec().identitySourceIds();\n+    Schema schema = table().schema();\n+\n+    for (Filter filter : filters) {\n+      // return false if the filter requires rewrite or if we cannot translate the filter\n+      if (requiresRewrite(filter, schema, identitySourceIds) || SparkFilters.convert(filter) == null) {\n+        return false;\n+      }\n+    }\n+\n+    return true;\n+  }\n+\n+  private boolean requiresRewrite(Filter filter, Schema schema, Set<Integer> identitySourceIds) {\n+    // TODO: handle dots correctly via v2references", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8ecea59d1035bcc73931cb923314bf7db4392b9c"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTA0NjYyMw==", "bodyText": "Yeah, they are already supported.", "url": "https://github.com/apache/iceberg/pull/1862#discussion_r535046623", "createdAt": "2020-12-03T10:08:56Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/source/SparkTable.java", "diffHunk": "@@ -160,6 +167,43 @@ public WriteBuilder newWriteBuilder(LogicalWriteInfo info) {\n     return new SparkWriteBuilder(sparkSession(), icebergTable, info);\n   }\n \n+  @Override\n+  public MergeBuilder newMergeBuilder(LogicalWriteInfo info) {\n+    String mode = icebergTable.properties().getOrDefault(WRITE_ROW_LEVEL_MODE, WRITE_ROW_LEVEL_MODE_DEFAULT);\n+    ValidationException.check(mode.equals(\"copy-on-write\"), \"Unsupported row operations mode: %s\", mode);\n+    return new SparkMergeBuilder(sparkSession(), icebergTable, info);\n+  }\n+\n+  @Override\n+  public boolean canDeleteWhere(Filter[] filters) {\n+    if (table().specs().size() > 1) {\n+      // cannot guarantee a metadata delete will be successful if we have multiple specs\n+      return false;\n+    }\n+\n+    Set<Integer> identitySourceIds = table().spec().identitySourceIds();\n+    Schema schema = table().schema();\n+\n+    for (Filter filter : filters) {\n+      // return false if the filter requires rewrite or if we cannot translate the filter\n+      if (requiresRewrite(filter, schema, identitySourceIds) || SparkFilters.convert(filter) == null) {\n+        return false;\n+      }\n+    }\n+\n+    return true;\n+  }\n+\n+  private boolean requiresRewrite(Filter filter, Schema schema, Set<Integer> identitySourceIds) {\n+    // TODO: handle dots correctly via v2references", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDYzNDk3NQ=="}, "originalCommit": {"oid": "8ecea59d1035bcc73931cb923314bf7db4392b9c"}, "originalPosition": 70}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM1NDQ3NTY5OnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/SparkTable.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wM1QwMzoxNToxNlrOH93nLw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQxNDoyMjowN1rOH_S_zg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDYzNjMzNQ==", "bodyText": "We may want to add another TODO to detect more cases that don't require rewrites.", "url": "https://github.com/apache/iceberg/pull/1862#discussion_r534636335", "createdAt": "2020-12-03T03:15:16Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/source/SparkTable.java", "diffHunk": "@@ -160,6 +167,43 @@ public WriteBuilder newWriteBuilder(LogicalWriteInfo info) {\n     return new SparkWriteBuilder(sparkSession(), icebergTable, info);\n   }\n \n+  @Override\n+  public MergeBuilder newMergeBuilder(LogicalWriteInfo info) {\n+    String mode = icebergTable.properties().getOrDefault(WRITE_ROW_LEVEL_MODE, WRITE_ROW_LEVEL_MODE_DEFAULT);\n+    ValidationException.check(mode.equals(\"copy-on-write\"), \"Unsupported row operations mode: %s\", mode);\n+    return new SparkMergeBuilder(sparkSession(), icebergTable, info);\n+  }\n+\n+  @Override\n+  public boolean canDeleteWhere(Filter[] filters) {\n+    if (table().specs().size() > 1) {\n+      // cannot guarantee a metadata delete will be successful if we have multiple specs\n+      return false;\n+    }\n+\n+    Set<Integer> identitySourceIds = table().spec().identitySourceIds();\n+    Schema schema = table().schema();\n+\n+    for (Filter filter : filters) {\n+      // return false if the filter requires rewrite or if we cannot translate the filter\n+      if (requiresRewrite(filter, schema, identitySourceIds) || SparkFilters.convert(filter) == null) {\n+        return false;\n+      }\n+    }\n+\n+    return true;\n+  }\n+\n+  private boolean requiresRewrite(Filter filter, Schema schema, Set<Integer> identitySourceIds) {\n+    // TODO: handle dots correctly via v2references", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8ecea59d1035bcc73931cb923314bf7db4392b9c"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTA0NTA1Nw==", "bodyText": "Will do.", "url": "https://github.com/apache/iceberg/pull/1862#discussion_r535045057", "createdAt": "2020-12-03T10:07:31Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/source/SparkTable.java", "diffHunk": "@@ -160,6 +167,43 @@ public WriteBuilder newWriteBuilder(LogicalWriteInfo info) {\n     return new SparkWriteBuilder(sparkSession(), icebergTable, info);\n   }\n \n+  @Override\n+  public MergeBuilder newMergeBuilder(LogicalWriteInfo info) {\n+    String mode = icebergTable.properties().getOrDefault(WRITE_ROW_LEVEL_MODE, WRITE_ROW_LEVEL_MODE_DEFAULT);\n+    ValidationException.check(mode.equals(\"copy-on-write\"), \"Unsupported row operations mode: %s\", mode);\n+    return new SparkMergeBuilder(sparkSession(), icebergTable, info);\n+  }\n+\n+  @Override\n+  public boolean canDeleteWhere(Filter[] filters) {\n+    if (table().specs().size() > 1) {\n+      // cannot guarantee a metadata delete will be successful if we have multiple specs\n+      return false;\n+    }\n+\n+    Set<Integer> identitySourceIds = table().spec().identitySourceIds();\n+    Schema schema = table().schema();\n+\n+    for (Filter filter : filters) {\n+      // return false if the filter requires rewrite or if we cannot translate the filter\n+      if (requiresRewrite(filter, schema, identitySourceIds) || SparkFilters.convert(filter) == null) {\n+        return false;\n+      }\n+    }\n+\n+    return true;\n+  }\n+\n+  private boolean requiresRewrite(Filter filter, Schema schema, Set<Integer> identitySourceIds) {\n+    // TODO: handle dots correctly via v2references", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDYzNjMzNQ=="}, "originalCommit": {"oid": "8ecea59d1035bcc73931cb923314bf7db4392b9c"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjEzMzU4Mg==", "bodyText": "Done.", "url": "https://github.com/apache/iceberg/pull/1862#discussion_r536133582", "createdAt": "2020-12-04T14:22:07Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/source/SparkTable.java", "diffHunk": "@@ -160,6 +167,43 @@ public WriteBuilder newWriteBuilder(LogicalWriteInfo info) {\n     return new SparkWriteBuilder(sparkSession(), icebergTable, info);\n   }\n \n+  @Override\n+  public MergeBuilder newMergeBuilder(LogicalWriteInfo info) {\n+    String mode = icebergTable.properties().getOrDefault(WRITE_ROW_LEVEL_MODE, WRITE_ROW_LEVEL_MODE_DEFAULT);\n+    ValidationException.check(mode.equals(\"copy-on-write\"), \"Unsupported row operations mode: %s\", mode);\n+    return new SparkMergeBuilder(sparkSession(), icebergTable, info);\n+  }\n+\n+  @Override\n+  public boolean canDeleteWhere(Filter[] filters) {\n+    if (table().specs().size() > 1) {\n+      // cannot guarantee a metadata delete will be successful if we have multiple specs\n+      return false;\n+    }\n+\n+    Set<Integer> identitySourceIds = table().spec().identitySourceIds();\n+    Schema schema = table().schema();\n+\n+    for (Filter filter : filters) {\n+      // return false if the filter requires rewrite or if we cannot translate the filter\n+      if (requiresRewrite(filter, schema, identitySourceIds) || SparkFilters.convert(filter) == null) {\n+        return false;\n+      }\n+    }\n+\n+    return true;\n+  }\n+\n+  private boolean requiresRewrite(Filter filter, Schema schema, Set<Integer> identitySourceIds) {\n+    // TODO: handle dots correctly via v2references", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDYzNjMzNQ=="}, "originalCommit": {"oid": "8ecea59d1035bcc73931cb923314bf7db4392b9c"}, "originalPosition": 70}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM1Njg0MTk2OnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/SparkTable.java", "isResolved": true, "comments": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wM1QxMDowNDowMFrOH-QVqg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQxNDoyMjoyOVrOH_TA5Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTA0MTQ1MA==", "bodyText": "We could set operation inside info.options() in RewriteDelete and pass it here so that we can support table properties like write.delete.mode instead of write.row-level.mode. Thoughts, @rdblue?", "url": "https://github.com/apache/iceberg/pull/1862#discussion_r535041450", "createdAt": "2020-12-03T10:04:00Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/source/SparkTable.java", "diffHunk": "@@ -160,6 +167,43 @@ public WriteBuilder newWriteBuilder(LogicalWriteInfo info) {\n     return new SparkWriteBuilder(sparkSession(), icebergTable, info);\n   }\n \n+  @Override\n+  public MergeBuilder newMergeBuilder(LogicalWriteInfo info) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8ecea59d1035bcc73931cb923314bf7db4392b9c"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTQ0NTQ3MA==", "bodyText": "I'm not a fan of passing things through Spark's options. If we have the information in the writer, can we just call a setter to set it on the reader?", "url": "https://github.com/apache/iceberg/pull/1862#discussion_r535445470", "createdAt": "2020-12-03T17:41:09Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/source/SparkTable.java", "diffHunk": "@@ -160,6 +167,43 @@ public WriteBuilder newWriteBuilder(LogicalWriteInfo info) {\n     return new SparkWriteBuilder(sparkSession(), icebergTable, info);\n   }\n \n+  @Override\n+  public MergeBuilder newMergeBuilder(LogicalWriteInfo info) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTA0MTQ1MA=="}, "originalCommit": {"oid": "8ecea59d1035bcc73931cb923314bf7db4392b9c"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTQ3OTA0NA==", "bodyText": "We have this info only in RewriteDelete rule that calls newMergeBuilder. After that, we have no way to know whether it is a delete or merge.", "url": "https://github.com/apache/iceberg/pull/1862#discussion_r535479044", "createdAt": "2020-12-03T18:26:46Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/source/SparkTable.java", "diffHunk": "@@ -160,6 +167,43 @@ public WriteBuilder newWriteBuilder(LogicalWriteInfo info) {\n     return new SparkWriteBuilder(sparkSession(), icebergTable, info);\n   }\n \n+  @Override\n+  public MergeBuilder newMergeBuilder(LogicalWriteInfo info) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTA0MTQ1MA=="}, "originalCommit": {"oid": "8ecea59d1035bcc73931cb923314bf7db4392b9c"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTQ3OTgzMw==", "bodyText": "So if we pass something that tells us what logical operation it is, we can have write.delete.isolation-level.", "url": "https://github.com/apache/iceberg/pull/1862#discussion_r535479833", "createdAt": "2020-12-03T18:27:56Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/source/SparkTable.java", "diffHunk": "@@ -160,6 +167,43 @@ public WriteBuilder newWriteBuilder(LogicalWriteInfo info) {\n     return new SparkWriteBuilder(sparkSession(), icebergTable, info);\n   }\n \n+  @Override\n+  public MergeBuilder newMergeBuilder(LogicalWriteInfo info) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTA0MTQ1MA=="}, "originalCommit": {"oid": "8ecea59d1035bcc73931cb923314bf7db4392b9c"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTQ4MDUwMw==", "bodyText": "Unless we want to have newMergeBuilder, newDeleteBuilder, etc.", "url": "https://github.com/apache/iceberg/pull/1862#discussion_r535480503", "createdAt": "2020-12-03T18:29:07Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/source/SparkTable.java", "diffHunk": "@@ -160,6 +167,43 @@ public WriteBuilder newWriteBuilder(LogicalWriteInfo info) {\n     return new SparkWriteBuilder(sparkSession(), icebergTable, info);\n   }\n \n+  @Override\n+  public MergeBuilder newMergeBuilder(LogicalWriteInfo info) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTA0MTQ1MA=="}, "originalCommit": {"oid": "8ecea59d1035bcc73931cb923314bf7db4392b9c"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTUzOTQwNA==", "bodyText": "I like that this is used for both MERGE and DELETE without needing to customize, so I'm reluctant to do too much here. It is close, but I think I would opt to have a single config property.\nOperation is something that we could pass through LogicalWriteInfo in the future for better logging and purposes like this one. For now, we could add the operation as a String passed to newMergeBuilder, but there is no guarantee that we would add this to LogicalWriteInfo later. I think that means that we should go for a single property and we can customize later if we do get the operation when the builder is created.", "url": "https://github.com/apache/iceberg/pull/1862#discussion_r535539404", "createdAt": "2020-12-03T19:51:25Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/source/SparkTable.java", "diffHunk": "@@ -160,6 +167,43 @@ public WriteBuilder newWriteBuilder(LogicalWriteInfo info) {\n     return new SparkWriteBuilder(sparkSession(), icebergTable, info);\n   }\n \n+  @Override\n+  public MergeBuilder newMergeBuilder(LogicalWriteInfo info) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTA0MTQ1MA=="}, "originalCommit": {"oid": "8ecea59d1035bcc73931cb923314bf7db4392b9c"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTU1MzI4Nw==", "bodyText": "After discussing this directly, I think the right way to go is to add the operation name. That fixes the isolation level and mode property problem. And it is reasonable to add the operation to LogicalWriteInfo.", "url": "https://github.com/apache/iceberg/pull/1862#discussion_r535553287", "createdAt": "2020-12-03T20:11:15Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/source/SparkTable.java", "diffHunk": "@@ -160,6 +167,43 @@ public WriteBuilder newWriteBuilder(LogicalWriteInfo info) {\n     return new SparkWriteBuilder(sparkSession(), icebergTable, info);\n   }\n \n+  @Override\n+  public MergeBuilder newMergeBuilder(LogicalWriteInfo info) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTA0MTQ1MA=="}, "originalCommit": {"oid": "8ecea59d1035bcc73931cb923314bf7db4392b9c"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjEzMzg2MQ==", "bodyText": "Done.", "url": "https://github.com/apache/iceberg/pull/1862#discussion_r536133861", "createdAt": "2020-12-04T14:22:29Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/source/SparkTable.java", "diffHunk": "@@ -160,6 +167,43 @@ public WriteBuilder newWriteBuilder(LogicalWriteInfo info) {\n     return new SparkWriteBuilder(sparkSession(), icebergTable, info);\n   }\n \n+  @Override\n+  public MergeBuilder newMergeBuilder(LogicalWriteInfo info) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTA0MTQ1MA=="}, "originalCommit": {"oid": "8ecea59d1035bcc73931cb923314bf7db4392b9c"}, "originalPosition": 43}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3224, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}