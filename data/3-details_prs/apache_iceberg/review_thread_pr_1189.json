{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDQ3MTU1NTM3", "number": 1189, "reviewThreads": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMFQyMzoyODo1OFrOENcUqg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMFQyMzo1NTo1NlrOENci6g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyNTMwOTg2OnYy", "diffSide": "RIGHT", "path": "orc/src/main/java/org/apache/iceberg/orc/ORC.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMFQyMzoyODo1OFrOGwIQWA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMVQwMjoxOTowNFrOGwJq3Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEyMDA4OA==", "bodyText": "This should validate that createReaderFunc hasn't also been called. And that function should validate that this one hasn't been called. That way there is no ambiguous behavior and the user gets an error when they try to set up both batch and row readers.", "url": "https://github.com/apache/iceberg/pull/1189#discussion_r453120088", "createdAt": "2020-07-10T23:28:58Z", "author": {"login": "rdblue"}, "path": "orc/src/main/java/org/apache/iceberg/orc/ORC.java", "diffHunk": "@@ -177,9 +179,20 @@ public ReadBuilder filter(Expression newFilter) {\n       return this;\n     }\n \n+    public ReadBuilder createBatchedReaderFunc(Function<TypeDescription, OrcBatchReader<?>> batchReaderFunction) {\n+      this.batchedReaderFunc = batchReaderFunction;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6a5e8c53d3950922b8f6bd753329b7d943ac423d"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzE0MzI2MQ==", "bodyText": "Fixed in d13c366", "url": "https://github.com/apache/iceberg/pull/1189#discussion_r453143261", "createdAt": "2020-07-11T02:19:04Z", "author": {"login": "shardulm94"}, "path": "orc/src/main/java/org/apache/iceberg/orc/ORC.java", "diffHunk": "@@ -177,9 +179,20 @@ public ReadBuilder filter(Expression newFilter) {\n       return this;\n     }\n \n+    public ReadBuilder createBatchedReaderFunc(Function<TypeDescription, OrcBatchReader<?>> batchReaderFunction) {\n+      this.batchedReaderFunc = batchReaderFunction;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEyMDA4OA=="}, "originalCommit": {"oid": "6a5e8c53d3950922b8f6bd753329b7d943ac423d"}, "originalPosition": 14}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyNTMxNTA4OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcValueReaders.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMFQyMzozMjoyM1rOGwITZw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMVQwMDowMzoxNlrOGwIqaQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEyMDg3MQ==", "bodyText": "I like the changes here and in SparkOrcReader, but they aren't really needed for this PR and increase the number of files that are touched. In general, I prefer keeping changes like this separate to avoid unnecessary conflicts, and so that we can revert commits without removing unrelated clean-up.", "url": "https://github.com/apache/iceberg/pull/1189#discussion_r453120871", "createdAt": "2020-07-10T23:32:23Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcValueReaders.java", "diffHunk": "@@ -42,19 +42,26 @@\n import org.apache.spark.sql.types.Decimal;\n import org.apache.spark.unsafe.types.UTF8String;\n \n-\n-class SparkOrcValueReaders {\n+public class SparkOrcValueReaders {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6a5e8c53d3950922b8f6bd753329b7d943ac423d"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEyNjUyNg==", "bodyText": "I wanted to avoid duplicating the if (precision < 18) then Decimal18Reader else Decimal38Reader logic, since I am reusing it in VectorizedSparkOrcReaders. Similarly I made utf8string and timestampTzs methods public for reuse in the vectorized package. Let me know if you would like me to revert the Decimal change.", "url": "https://github.com/apache/iceberg/pull/1189#discussion_r453126526", "createdAt": "2020-07-11T00:01:57Z", "author": {"login": "shardulm94"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcValueReaders.java", "diffHunk": "@@ -42,19 +42,26 @@\n import org.apache.spark.sql.types.Decimal;\n import org.apache.spark.unsafe.types.UTF8String;\n \n-\n-class SparkOrcValueReaders {\n+public class SparkOrcValueReaders {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEyMDg3MQ=="}, "originalCommit": {"oid": "6a5e8c53d3950922b8f6bd753329b7d943ac423d"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEyNjc2MQ==", "bodyText": "Okay, I think that's a good justification for including this. Thanks!", "url": "https://github.com/apache/iceberg/pull/1189#discussion_r453126761", "createdAt": "2020-07-11T00:03:16Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcValueReaders.java", "diffHunk": "@@ -42,19 +42,26 @@\n import org.apache.spark.sql.types.Decimal;\n import org.apache.spark.unsafe.types.UTF8String;\n \n-\n-class SparkOrcValueReaders {\n+public class SparkOrcValueReaders {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEyMDg3MQ=="}, "originalCommit": {"oid": "6a5e8c53d3950922b8f6bd753329b7d943ac423d"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyNTMyNTE2OnYy", "diffSide": "RIGHT", "path": "orc/src/main/java/org/apache/iceberg/orc/OrcIterable.java", "isResolved": true, "comments": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMFQyMzozOTo0OVrOGwIZOQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wOS0xN1QwNzo1NDozN1rOKl5-sQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEyMjM2MQ==", "bodyText": "This needs to be a CloseableIterator to avoid leaking an open file. I'm also wondering if we could replace this implementation with a method like CloseableIterable.transform for CloseableIterator. All this is doing is calling a method on the result of another Iterator.", "url": "https://github.com/apache/iceberg/pull/1189#discussion_r453122361", "createdAt": "2020-07-10T23:39:49Z", "author": {"login": "rdblue"}, "path": "orc/src/main/java/org/apache/iceberg/orc/OrcIterable.java", "diffHunk": "@@ -130,4 +140,25 @@ public T next() {\n     }\n   }\n \n+  private static class OrcBatchIterator<T> implements Iterator<T> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6a5e8c53d3950922b8f6bd753329b7d943ac423d"}, "originalPosition": 79}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEzNDEyMQ==", "bodyText": "This Iterator is converted to a CloseableIterator at \n  \n    \n      iceberg/orc/src/main/java/org/apache/iceberg/orc/OrcIterable.java\n    \n    \n         Line 89\n      in\n      6a5e8c5\n    \n    \n    \n    \n\n        \n          \n           return CloseableIterator.withClose(iterator); \n        \n    \n  \n\n\nI too want to avoid creating the OrcBatchIterator class, however I am unable to make Iterators.transform work here because of the capture type of batchReaderFunction.\n\nI am also not able to cast the batchReaderFunction to the generic type used by OrcIterable.", "url": "https://github.com/apache/iceberg/pull/1189#discussion_r453134121", "createdAt": "2020-07-11T00:51:41Z", "author": {"login": "shardulm94"}, "path": "orc/src/main/java/org/apache/iceberg/orc/OrcIterable.java", "diffHunk": "@@ -130,4 +140,25 @@ public T next() {\n     }\n   }\n \n+  private static class OrcBatchIterator<T> implements Iterator<T> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEyMjM2MQ=="}, "originalCommit": {"oid": "6a5e8c53d3950922b8f6bd753329b7d943ac423d"}, "originalPosition": 79}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEzNjQ4MA==", "bodyText": "What about Iterators.transform(rowBatchIterator, batchReaderFunction.apply(readOrcSchema)::read)? That should allow the compiler to convert it to a Function.", "url": "https://github.com/apache/iceberg/pull/1189#discussion_r453136480", "createdAt": "2020-07-11T01:11:28Z", "author": {"login": "rdblue"}, "path": "orc/src/main/java/org/apache/iceberg/orc/OrcIterable.java", "diffHunk": "@@ -130,4 +140,25 @@ public T next() {\n     }\n   }\n \n+  private static class OrcBatchIterator<T> implements Iterator<T> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEyMjM2MQ=="}, "originalCommit": {"oid": "6a5e8c53d3950922b8f6bd753329b7d943ac423d"}, "originalPosition": 79}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEzNzA5Ng==", "bodyText": "This Iterator is converted to a CloseableIterator at\n\nYes, it is converted to a CloseableIterator, but that iterator's close method won't do anything unless this is Closeable. Here's where it delegates:\n      @Override\n      public void close() throws IOException {\n        if (iterator instanceof Closeable) {\n          ((Closeable) iterator).close();\n        }\n      }\nSo if this Iterator doesn't implement Close to pass the close to batchIter, then batchIter won't get closed.\nAlternatively, you could add a transform factory method to CloseableIterator:\n  static <S, T> CloseableIterator<T> transform(CloseableIterator<S> iter, Function<S, T> transform) {\n    return new CloseableIterator<T>() {\n      @Override\n      public void close() throws IOException {\n        iter.close();\n      }\n\n      @Override\n      public boolean hasNext() {\n        return iter.hasNext();\n      }\n\n      @Override\n      public T next() {\n        return transform.apply(iter.next());\n      }\n    };\n  }", "url": "https://github.com/apache/iceberg/pull/1189#discussion_r453137096", "createdAt": "2020-07-11T01:16:35Z", "author": {"login": "rdblue"}, "path": "orc/src/main/java/org/apache/iceberg/orc/OrcIterable.java", "diffHunk": "@@ -130,4 +140,25 @@ public T next() {\n     }\n   }\n \n+  private static class OrcBatchIterator<T> implements Iterator<T> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEyMjM2MQ=="}, "originalCommit": {"oid": "6a5e8c53d3950922b8f6bd753329b7d943ac423d"}, "originalPosition": 79}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEzNzE5MQ==", "bodyText": "I remember trying this too. OrcBatchReader is a functional interface, so it should not have issues converting the reader to a Function. The issue seems to be the capture type.", "url": "https://github.com/apache/iceberg/pull/1189#discussion_r453137191", "createdAt": "2020-07-11T01:17:17Z", "author": {"login": "shardulm94"}, "path": "orc/src/main/java/org/apache/iceberg/orc/OrcIterable.java", "diffHunk": "@@ -130,4 +140,25 @@ public T next() {\n     }\n   }\n \n+  private static class OrcBatchIterator<T> implements Iterator<T> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEyMjM2MQ=="}, "originalCommit": {"oid": "6a5e8c53d3950922b8f6bd753329b7d943ac423d"}, "originalPosition": 79}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEzODAyMQ==", "bodyText": "Okay, I am able to make it work if I assign the transformed iterator to Iterator instead of Iterator<T>.", "url": "https://github.com/apache/iceberg/pull/1189#discussion_r453138021", "createdAt": "2020-07-11T01:25:06Z", "author": {"login": "shardulm94"}, "path": "orc/src/main/java/org/apache/iceberg/orc/OrcIterable.java", "diffHunk": "@@ -130,4 +140,25 @@ public T next() {\n     }\n   }\n \n+  private static class OrcBatchIterator<T> implements Iterator<T> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEyMjM2MQ=="}, "originalCommit": {"oid": "6a5e8c53d3950922b8f6bd753329b7d943ac423d"}, "originalPosition": 79}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzE0MzUzMA==", "bodyText": "Fixed in 7d0cf1c\nAdded CloseableIterator.transform, updated VectorizedRowBatchIterator to use CloseableIterator, and also fixed the issue with incorrect use of CloseableIterator.withClose.", "url": "https://github.com/apache/iceberg/pull/1189#discussion_r453143530", "createdAt": "2020-07-11T02:21:46Z", "author": {"login": "shardulm94"}, "path": "orc/src/main/java/org/apache/iceberg/orc/OrcIterable.java", "diffHunk": "@@ -130,4 +140,25 @@ public T next() {\n     }\n   }\n \n+  private static class OrcBatchIterator<T> implements Iterator<T> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEyMjM2MQ=="}, "originalCommit": {"oid": "6a5e8c53d3950922b8f6bd753329b7d943ac423d"}, "originalPosition": 79}, {"id": "PRRC_kwDOCW7NX84qXn6x", "bodyText": "PaymentFlowInfoObject \u90a3\u908a\uff5e", "url": "https://github.com/apache/iceberg/pull/1189#discussion_r710835889", "createdAt": "2021-09-17T07:54:37Z", "author": {"login": "megshao"}, "path": "orc/src/main/java/org/apache/iceberg/orc/OrcIterable.java", "diffHunk": "@@ -130,4 +140,25 @@ public T next() {\n     }\n   }\n \n+  private static class OrcBatchIterator<T> implements Iterator<T> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEyMjM2MQ=="}, "originalCommit": {"oid": "6a5e8c53d3950922b8f6bd753329b7d943ac423d"}, "originalPosition": 79}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyNTMzNDUwOnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/VectorizedSparkOrcReaders.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMFQyMzo0NjozNFrOGwIedg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMVQwMjoxODo1MFrOGwJqzg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEyMzcwMg==", "bodyText": "Nit: we would normally align this with the start of Schema on the previous line.", "url": "https://github.com/apache/iceberg/pull/1189#discussion_r453123702", "createdAt": "2020-07-10T23:46:34Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/VectorizedSparkOrcReaders.java", "diffHunk": "@@ -0,0 +1,415 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.orc.OrcBatchReader;\n+import org.apache.iceberg.orc.OrcSchemaWithTypeVisitor;\n+import org.apache.iceberg.orc.OrcValueReader;\n+import org.apache.iceberg.orc.OrcValueReaders;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.spark.data.SparkOrcValueReaders;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.storage.ql.exec.vector.ListColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.MapColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.StructColumnVector;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarArray;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+import org.apache.spark.sql.vectorized.ColumnarMap;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+public class VectorizedSparkOrcReaders {\n+\n+  private VectorizedSparkOrcReaders() {\n+  }\n+\n+  public static OrcBatchReader<ColumnarBatch> buildReader(Schema expectedSchema, TypeDescription fileSchema,\n+      Map<Integer, ?> idToConstant) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6a5e8c53d3950922b8f6bd753329b7d943ac423d"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzE0MzI0Ng==", "bodyText": "Fixed in faeffa3", "url": "https://github.com/apache/iceberg/pull/1189#discussion_r453143246", "createdAt": "2020-07-11T02:18:50Z", "author": {"login": "shardulm94"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/VectorizedSparkOrcReaders.java", "diffHunk": "@@ -0,0 +1,415 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.orc.OrcBatchReader;\n+import org.apache.iceberg.orc.OrcSchemaWithTypeVisitor;\n+import org.apache.iceberg.orc.OrcValueReader;\n+import org.apache.iceberg.orc.OrcValueReaders;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.spark.data.SparkOrcValueReaders;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.storage.ql.exec.vector.ListColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.MapColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.StructColumnVector;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarArray;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+import org.apache.spark.sql.vectorized.ColumnarMap;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+public class VectorizedSparkOrcReaders {\n+\n+  private VectorizedSparkOrcReaders() {\n+  }\n+\n+  public static OrcBatchReader<ColumnarBatch> buildReader(Schema expectedSchema, TypeDescription fileSchema,\n+      Map<Integer, ?> idToConstant) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEyMzcwMg=="}, "originalCommit": {"oid": "6a5e8c53d3950922b8f6bd753329b7d943ac423d"}, "originalPosition": 52}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyNTM0MDE5OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/source/BatchDataReader.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMFQyMzo1MDo1MVrOGwIhkA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMVQwMjoxOTo0MFrOGwJrEQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEyNDQ5Ng==", "bodyText": "I will change this to not project constant columns to avoid materializing a ColumnVector similar to what we did in #1191", "url": "https://github.com/apache/iceberg/pull/1189#discussion_r453124496", "createdAt": "2020-07-10T23:50:51Z", "author": {"login": "shardulm94"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/BatchDataReader.java", "diffHunk": "@@ -75,6 +103,16 @@\n       }\n \n       iter = builder.build();\n+    } else if (task.file().format() == FileFormat.ORC) {\n+      iter = ORC.read(location)\n+          .project(expectedSchema)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6a5e8c53d3950922b8f6bd753329b7d943ac423d"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzE0MzMxMw==", "bodyText": "Fixed in 9c2af8d", "url": "https://github.com/apache/iceberg/pull/1189#discussion_r453143313", "createdAt": "2020-07-11T02:19:40Z", "author": {"login": "shardulm94"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/BatchDataReader.java", "diffHunk": "@@ -75,6 +103,16 @@\n       }\n \n       iter = builder.build();\n+    } else if (task.file().format() == FileFormat.ORC) {\n+      iter = ORC.read(location)\n+          .project(expectedSchema)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEyNDQ5Ng=="}, "originalCommit": {"oid": "6a5e8c53d3950922b8f6bd753329b7d943ac423d"}, "originalPosition": 62}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyNTM0NjM0OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/VectorizedSparkOrcReaders.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMFQyMzo1NTo1NlrOGwIk_w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QxNjo1MDowNVrOGwxFAA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEyNTM3NQ==", "bodyText": "Why this extra check just to add a default value? If it is okay to return a default value, then getInt should never be called for a rowId where the value is null. And if this is never called when the value is null, then I'd rather directly cast. That way, a NullPointerException is thrown if the method contract is violated instead of silently returning the wrong value.", "url": "https://github.com/apache/iceberg/pull/1189#discussion_r453125375", "createdAt": "2020-07-10T23:55:56Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/VectorizedSparkOrcReaders.java", "diffHunk": "@@ -0,0 +1,415 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.orc.OrcBatchReader;\n+import org.apache.iceberg.orc.OrcSchemaWithTypeVisitor;\n+import org.apache.iceberg.orc.OrcValueReader;\n+import org.apache.iceberg.orc.OrcValueReaders;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.spark.data.SparkOrcValueReaders;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.storage.ql.exec.vector.ListColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.MapColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.StructColumnVector;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarArray;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+import org.apache.spark.sql.vectorized.ColumnarMap;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+public class VectorizedSparkOrcReaders {\n+\n+  private VectorizedSparkOrcReaders() {\n+  }\n+\n+  public static OrcBatchReader<ColumnarBatch> buildReader(Schema expectedSchema, TypeDescription fileSchema,\n+      Map<Integer, ?> idToConstant) {\n+    Converter converter = OrcSchemaWithTypeVisitor.visit(expectedSchema, fileSchema, new ReadBuilder(idToConstant));\n+\n+    return batch -> {\n+      BaseOrcColumnVector cv = (BaseOrcColumnVector) converter.convert(new StructColumnVector(batch.size, batch.cols),\n+          batch.size);\n+      ColumnarBatch columnarBatch = new ColumnarBatch(IntStream.range(0, expectedSchema.columns().size())\n+          .mapToObj(cv::getChild)\n+          .toArray(ColumnVector[]::new));\n+      columnarBatch.setNumRows(batch.size);\n+      return columnarBatch;\n+    };\n+  }\n+\n+  private interface Converter {\n+    ColumnVector convert(org.apache.orc.storage.ql.exec.vector.ColumnVector columnVector, int batchSize);\n+  }\n+\n+  private static class ReadBuilder extends OrcSchemaWithTypeVisitor<Converter> {\n+    private final Map<Integer, ?> idToConstant;\n+\n+    private ReadBuilder(Map<Integer, ?> idToConstant) {\n+      this.idToConstant = idToConstant;\n+    }\n+\n+    @Override\n+    public Converter record(Types.StructType iStruct, TypeDescription record, List<String> names,\n+        List<Converter> fields) {\n+      return new StructConverter(iStruct, fields, idToConstant);\n+    }\n+\n+    @Override\n+    public Converter list(Types.ListType iList, TypeDescription array, Converter element) {\n+      return new ArrayConverter(iList, element);\n+    }\n+\n+    @Override\n+    public Converter map(Types.MapType iMap, TypeDescription map, Converter key, Converter value) {\n+      return new MapConverter(iMap, key, value);\n+    }\n+\n+    @Override\n+    public Converter primitive(Type.PrimitiveType iPrimitive, TypeDescription primitive) {\n+      final OrcValueReader<?> primitiveValueReader;\n+      switch (primitive.getCategory()) {\n+        case BOOLEAN:\n+          primitiveValueReader = OrcValueReaders.booleans();\n+          break;\n+        case BYTE:\n+          // Iceberg does not have a byte type. Use int\n+        case SHORT:\n+          // Iceberg does not have a short type. Use int\n+        case DATE:\n+        case INT:\n+          primitiveValueReader = OrcValueReaders.ints();\n+          break;\n+        case LONG:\n+          primitiveValueReader = OrcValueReaders.longs();\n+          break;\n+        case FLOAT:\n+          primitiveValueReader = OrcValueReaders.floats();\n+          break;\n+        case DOUBLE:\n+          primitiveValueReader = OrcValueReaders.doubles();\n+          break;\n+        case TIMESTAMP_INSTANT:\n+          primitiveValueReader = SparkOrcValueReaders.timestampTzs();\n+          break;\n+        case DECIMAL:\n+          primitiveValueReader = SparkOrcValueReaders.decimals(primitive.getPrecision(), primitive.getScale());\n+          break;\n+        case CHAR:\n+        case VARCHAR:\n+        case STRING:\n+          primitiveValueReader = SparkOrcValueReaders.utf8String();\n+          break;\n+        case BINARY:\n+          primitiveValueReader = OrcValueReaders.bytes();\n+          break;\n+        default:\n+          throw new IllegalArgumentException(\"Unhandled type \" + primitive);\n+      }\n+      return (columnVector, batchSize) ->\n+          new PrimitiveOrcColumnVector(iPrimitive, batchSize, columnVector, primitiveValueReader);\n+    }\n+  }\n+\n+  private abstract static class BaseOrcColumnVector extends ColumnVector {\n+    private final org.apache.orc.storage.ql.exec.vector.ColumnVector vector;\n+    private final int batchSize;\n+    private Integer numNulls;\n+\n+    BaseOrcColumnVector(Type type, int batchSize, org.apache.orc.storage.ql.exec.vector.ColumnVector vector) {\n+      super(SparkSchemaUtil.convert(type));\n+      this.vector = vector;\n+      this.batchSize = batchSize;\n+    }\n+\n+    @Override\n+    public void close() {\n+    }\n+\n+    @Override\n+    public boolean hasNull() {\n+      return !vector.noNulls;\n+    }\n+\n+    @Override\n+    public int numNulls() {\n+      if (numNulls == null) {\n+        numNulls = numNullsHelper();\n+      }\n+      return numNulls;\n+    }\n+\n+    private int numNullsHelper() {\n+      if (vector.isRepeating) {\n+        if (vector.isNull[0]) {\n+          return batchSize;\n+        } else {\n+          return 0;\n+        }\n+      } else if (vector.noNulls) {\n+        return 0;\n+      } else {\n+        int count = 0;\n+        for (int i = 0; i < batchSize; i++) {\n+          if (vector.isNull[i]) {\n+            count++;\n+          }\n+        }\n+        return count;\n+      }\n+    }\n+\n+    protected int getRowIndex(int rowId) {\n+      return vector.isRepeating ? 0 : rowId;\n+    }\n+\n+    @Override\n+    public boolean isNullAt(int rowId) {\n+      return vector.isNull[getRowIndex(rowId)];\n+    }\n+\n+    @Override\n+    public boolean getBoolean(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public byte getByte(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public short getShort(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public int getInt(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public long getLong(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public float getFloat(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public double getDouble(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public Decimal getDecimal(int rowId, int precision, int scale) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public UTF8String getUTF8String(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public byte[] getBinary(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public ColumnarArray getArray(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public ColumnarMap getMap(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public ColumnVector getChild(int ordinal) {\n+      throw new UnsupportedOperationException();\n+    }\n+  }\n+\n+  private static class PrimitiveOrcColumnVector extends BaseOrcColumnVector {\n+    private final org.apache.orc.storage.ql.exec.vector.ColumnVector vector;\n+    private final OrcValueReader<?> primitiveValueReader;\n+\n+    PrimitiveOrcColumnVector(Type type, int batchSize, org.apache.orc.storage.ql.exec.vector.ColumnVector vector,\n+        OrcValueReader<?> primitiveValueReader) {\n+      super(type, batchSize, vector);\n+      this.vector = vector;\n+      this.primitiveValueReader = primitiveValueReader;\n+    }\n+\n+    @Override\n+    public boolean getBoolean(int rowId) {\n+      Boolean value = (Boolean) primitiveValueReader.read(vector, rowId);\n+      return value != null ? value : false;\n+    }\n+\n+    @Override\n+    public int getInt(int rowId) {\n+      Integer value = (Integer) primitiveValueReader.read(vector, rowId);\n+      return value != null ? value : 0;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6a5e8c53d3950922b8f6bd753329b7d943ac423d"}, "originalPosition": 282}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzE0MzE2OA==", "bodyText": "Yep! That makes sense. If the reader is not going to check for null, then returning a default value would be erroneous. One issue I see is that Spark's own ColumnVector code seems to violate this at https://github.com/apache/spark/blob/d5b903e38556ee3e8e1eb8f71a08e232afa4e36a/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnVector.java#L92 and similar methods within the same class.\nIn Spark 2.4 these methods are not actually used, the methods are used at https://github.com/apache/spark/blob/d5b903e38556ee3e8e1eb8f71a08e232afa4e36a/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnarArray.java#L53 but those are not used anywhere.\nIn Spark 3.0, they are used at https://github.com/apache/spark/blob/3b0aee3f9557ae1666b63665b08d899fe7682852/sql/catalyst/src/main/java/org/apache/spark/sql/vectorized/ColumnarArray.java#L55, but this seems incorrect for the reasons you mentioned above. I am unsure when the copy method would actually be triggered though.", "url": "https://github.com/apache/iceberg/pull/1189#discussion_r453143168", "createdAt": "2020-07-11T02:17:39Z", "author": {"login": "shardulm94"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/VectorizedSparkOrcReaders.java", "diffHunk": "@@ -0,0 +1,415 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.orc.OrcBatchReader;\n+import org.apache.iceberg.orc.OrcSchemaWithTypeVisitor;\n+import org.apache.iceberg.orc.OrcValueReader;\n+import org.apache.iceberg.orc.OrcValueReaders;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.spark.data.SparkOrcValueReaders;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.storage.ql.exec.vector.ListColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.MapColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.StructColumnVector;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarArray;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+import org.apache.spark.sql.vectorized.ColumnarMap;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+public class VectorizedSparkOrcReaders {\n+\n+  private VectorizedSparkOrcReaders() {\n+  }\n+\n+  public static OrcBatchReader<ColumnarBatch> buildReader(Schema expectedSchema, TypeDescription fileSchema,\n+      Map<Integer, ?> idToConstant) {\n+    Converter converter = OrcSchemaWithTypeVisitor.visit(expectedSchema, fileSchema, new ReadBuilder(idToConstant));\n+\n+    return batch -> {\n+      BaseOrcColumnVector cv = (BaseOrcColumnVector) converter.convert(new StructColumnVector(batch.size, batch.cols),\n+          batch.size);\n+      ColumnarBatch columnarBatch = new ColumnarBatch(IntStream.range(0, expectedSchema.columns().size())\n+          .mapToObj(cv::getChild)\n+          .toArray(ColumnVector[]::new));\n+      columnarBatch.setNumRows(batch.size);\n+      return columnarBatch;\n+    };\n+  }\n+\n+  private interface Converter {\n+    ColumnVector convert(org.apache.orc.storage.ql.exec.vector.ColumnVector columnVector, int batchSize);\n+  }\n+\n+  private static class ReadBuilder extends OrcSchemaWithTypeVisitor<Converter> {\n+    private final Map<Integer, ?> idToConstant;\n+\n+    private ReadBuilder(Map<Integer, ?> idToConstant) {\n+      this.idToConstant = idToConstant;\n+    }\n+\n+    @Override\n+    public Converter record(Types.StructType iStruct, TypeDescription record, List<String> names,\n+        List<Converter> fields) {\n+      return new StructConverter(iStruct, fields, idToConstant);\n+    }\n+\n+    @Override\n+    public Converter list(Types.ListType iList, TypeDescription array, Converter element) {\n+      return new ArrayConverter(iList, element);\n+    }\n+\n+    @Override\n+    public Converter map(Types.MapType iMap, TypeDescription map, Converter key, Converter value) {\n+      return new MapConverter(iMap, key, value);\n+    }\n+\n+    @Override\n+    public Converter primitive(Type.PrimitiveType iPrimitive, TypeDescription primitive) {\n+      final OrcValueReader<?> primitiveValueReader;\n+      switch (primitive.getCategory()) {\n+        case BOOLEAN:\n+          primitiveValueReader = OrcValueReaders.booleans();\n+          break;\n+        case BYTE:\n+          // Iceberg does not have a byte type. Use int\n+        case SHORT:\n+          // Iceberg does not have a short type. Use int\n+        case DATE:\n+        case INT:\n+          primitiveValueReader = OrcValueReaders.ints();\n+          break;\n+        case LONG:\n+          primitiveValueReader = OrcValueReaders.longs();\n+          break;\n+        case FLOAT:\n+          primitiveValueReader = OrcValueReaders.floats();\n+          break;\n+        case DOUBLE:\n+          primitiveValueReader = OrcValueReaders.doubles();\n+          break;\n+        case TIMESTAMP_INSTANT:\n+          primitiveValueReader = SparkOrcValueReaders.timestampTzs();\n+          break;\n+        case DECIMAL:\n+          primitiveValueReader = SparkOrcValueReaders.decimals(primitive.getPrecision(), primitive.getScale());\n+          break;\n+        case CHAR:\n+        case VARCHAR:\n+        case STRING:\n+          primitiveValueReader = SparkOrcValueReaders.utf8String();\n+          break;\n+        case BINARY:\n+          primitiveValueReader = OrcValueReaders.bytes();\n+          break;\n+        default:\n+          throw new IllegalArgumentException(\"Unhandled type \" + primitive);\n+      }\n+      return (columnVector, batchSize) ->\n+          new PrimitiveOrcColumnVector(iPrimitive, batchSize, columnVector, primitiveValueReader);\n+    }\n+  }\n+\n+  private abstract static class BaseOrcColumnVector extends ColumnVector {\n+    private final org.apache.orc.storage.ql.exec.vector.ColumnVector vector;\n+    private final int batchSize;\n+    private Integer numNulls;\n+\n+    BaseOrcColumnVector(Type type, int batchSize, org.apache.orc.storage.ql.exec.vector.ColumnVector vector) {\n+      super(SparkSchemaUtil.convert(type));\n+      this.vector = vector;\n+      this.batchSize = batchSize;\n+    }\n+\n+    @Override\n+    public void close() {\n+    }\n+\n+    @Override\n+    public boolean hasNull() {\n+      return !vector.noNulls;\n+    }\n+\n+    @Override\n+    public int numNulls() {\n+      if (numNulls == null) {\n+        numNulls = numNullsHelper();\n+      }\n+      return numNulls;\n+    }\n+\n+    private int numNullsHelper() {\n+      if (vector.isRepeating) {\n+        if (vector.isNull[0]) {\n+          return batchSize;\n+        } else {\n+          return 0;\n+        }\n+      } else if (vector.noNulls) {\n+        return 0;\n+      } else {\n+        int count = 0;\n+        for (int i = 0; i < batchSize; i++) {\n+          if (vector.isNull[i]) {\n+            count++;\n+          }\n+        }\n+        return count;\n+      }\n+    }\n+\n+    protected int getRowIndex(int rowId) {\n+      return vector.isRepeating ? 0 : rowId;\n+    }\n+\n+    @Override\n+    public boolean isNullAt(int rowId) {\n+      return vector.isNull[getRowIndex(rowId)];\n+    }\n+\n+    @Override\n+    public boolean getBoolean(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public byte getByte(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public short getShort(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public int getInt(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public long getLong(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public float getFloat(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public double getDouble(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public Decimal getDecimal(int rowId, int precision, int scale) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public UTF8String getUTF8String(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public byte[] getBinary(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public ColumnarArray getArray(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public ColumnarMap getMap(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public ColumnVector getChild(int ordinal) {\n+      throw new UnsupportedOperationException();\n+    }\n+  }\n+\n+  private static class PrimitiveOrcColumnVector extends BaseOrcColumnVector {\n+    private final org.apache.orc.storage.ql.exec.vector.ColumnVector vector;\n+    private final OrcValueReader<?> primitiveValueReader;\n+\n+    PrimitiveOrcColumnVector(Type type, int batchSize, org.apache.orc.storage.ql.exec.vector.ColumnVector vector,\n+        OrcValueReader<?> primitiveValueReader) {\n+      super(type, batchSize, vector);\n+      this.vector = vector;\n+      this.primitiveValueReader = primitiveValueReader;\n+    }\n+\n+    @Override\n+    public boolean getBoolean(int rowId) {\n+      Boolean value = (Boolean) primitiveValueReader.read(vector, rowId);\n+      return value != null ? value : false;\n+    }\n+\n+    @Override\n+    public int getInt(int rowId) {\n+      Integer value = (Integer) primitiveValueReader.read(vector, rowId);\n+      return value != null ? value : 0;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEyNTM3NQ=="}, "originalCommit": {"oid": "6a5e8c53d3950922b8f6bd753329b7d943ac423d"}, "originalPosition": 282}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc4ODkyOA==", "bodyText": "Looks like other implementations in Spark just return from the underlying vector, without checking. So these methods would always return something. Arrow is an exception: if null checking is enabled (the default) then it will check and throw IllegalStateException.\nI think it still makes sense to throw the NullPointerException, even if copy in Spark 3 would use it. I don't see any uses of ArrayData.copy in Spark 3, and any problem would only affect arrays, so the impact is limited. Plus, other sources (Arrow) break in this case and it appears to be overlooked. I think it makes sense to throw an exception to prevent a copy that corrupts the data by dropping null values.", "url": "https://github.com/apache/iceberg/pull/1189#discussion_r453788928", "createdAt": "2020-07-13T16:50:05Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/VectorizedSparkOrcReaders.java", "diffHunk": "@@ -0,0 +1,415 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.orc.OrcBatchReader;\n+import org.apache.iceberg.orc.OrcSchemaWithTypeVisitor;\n+import org.apache.iceberg.orc.OrcValueReader;\n+import org.apache.iceberg.orc.OrcValueReaders;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.spark.data.SparkOrcValueReaders;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.storage.ql.exec.vector.ListColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.MapColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.StructColumnVector;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarArray;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+import org.apache.spark.sql.vectorized.ColumnarMap;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+public class VectorizedSparkOrcReaders {\n+\n+  private VectorizedSparkOrcReaders() {\n+  }\n+\n+  public static OrcBatchReader<ColumnarBatch> buildReader(Schema expectedSchema, TypeDescription fileSchema,\n+      Map<Integer, ?> idToConstant) {\n+    Converter converter = OrcSchemaWithTypeVisitor.visit(expectedSchema, fileSchema, new ReadBuilder(idToConstant));\n+\n+    return batch -> {\n+      BaseOrcColumnVector cv = (BaseOrcColumnVector) converter.convert(new StructColumnVector(batch.size, batch.cols),\n+          batch.size);\n+      ColumnarBatch columnarBatch = new ColumnarBatch(IntStream.range(0, expectedSchema.columns().size())\n+          .mapToObj(cv::getChild)\n+          .toArray(ColumnVector[]::new));\n+      columnarBatch.setNumRows(batch.size);\n+      return columnarBatch;\n+    };\n+  }\n+\n+  private interface Converter {\n+    ColumnVector convert(org.apache.orc.storage.ql.exec.vector.ColumnVector columnVector, int batchSize);\n+  }\n+\n+  private static class ReadBuilder extends OrcSchemaWithTypeVisitor<Converter> {\n+    private final Map<Integer, ?> idToConstant;\n+\n+    private ReadBuilder(Map<Integer, ?> idToConstant) {\n+      this.idToConstant = idToConstant;\n+    }\n+\n+    @Override\n+    public Converter record(Types.StructType iStruct, TypeDescription record, List<String> names,\n+        List<Converter> fields) {\n+      return new StructConverter(iStruct, fields, idToConstant);\n+    }\n+\n+    @Override\n+    public Converter list(Types.ListType iList, TypeDescription array, Converter element) {\n+      return new ArrayConverter(iList, element);\n+    }\n+\n+    @Override\n+    public Converter map(Types.MapType iMap, TypeDescription map, Converter key, Converter value) {\n+      return new MapConverter(iMap, key, value);\n+    }\n+\n+    @Override\n+    public Converter primitive(Type.PrimitiveType iPrimitive, TypeDescription primitive) {\n+      final OrcValueReader<?> primitiveValueReader;\n+      switch (primitive.getCategory()) {\n+        case BOOLEAN:\n+          primitiveValueReader = OrcValueReaders.booleans();\n+          break;\n+        case BYTE:\n+          // Iceberg does not have a byte type. Use int\n+        case SHORT:\n+          // Iceberg does not have a short type. Use int\n+        case DATE:\n+        case INT:\n+          primitiveValueReader = OrcValueReaders.ints();\n+          break;\n+        case LONG:\n+          primitiveValueReader = OrcValueReaders.longs();\n+          break;\n+        case FLOAT:\n+          primitiveValueReader = OrcValueReaders.floats();\n+          break;\n+        case DOUBLE:\n+          primitiveValueReader = OrcValueReaders.doubles();\n+          break;\n+        case TIMESTAMP_INSTANT:\n+          primitiveValueReader = SparkOrcValueReaders.timestampTzs();\n+          break;\n+        case DECIMAL:\n+          primitiveValueReader = SparkOrcValueReaders.decimals(primitive.getPrecision(), primitive.getScale());\n+          break;\n+        case CHAR:\n+        case VARCHAR:\n+        case STRING:\n+          primitiveValueReader = SparkOrcValueReaders.utf8String();\n+          break;\n+        case BINARY:\n+          primitiveValueReader = OrcValueReaders.bytes();\n+          break;\n+        default:\n+          throw new IllegalArgumentException(\"Unhandled type \" + primitive);\n+      }\n+      return (columnVector, batchSize) ->\n+          new PrimitiveOrcColumnVector(iPrimitive, batchSize, columnVector, primitiveValueReader);\n+    }\n+  }\n+\n+  private abstract static class BaseOrcColumnVector extends ColumnVector {\n+    private final org.apache.orc.storage.ql.exec.vector.ColumnVector vector;\n+    private final int batchSize;\n+    private Integer numNulls;\n+\n+    BaseOrcColumnVector(Type type, int batchSize, org.apache.orc.storage.ql.exec.vector.ColumnVector vector) {\n+      super(SparkSchemaUtil.convert(type));\n+      this.vector = vector;\n+      this.batchSize = batchSize;\n+    }\n+\n+    @Override\n+    public void close() {\n+    }\n+\n+    @Override\n+    public boolean hasNull() {\n+      return !vector.noNulls;\n+    }\n+\n+    @Override\n+    public int numNulls() {\n+      if (numNulls == null) {\n+        numNulls = numNullsHelper();\n+      }\n+      return numNulls;\n+    }\n+\n+    private int numNullsHelper() {\n+      if (vector.isRepeating) {\n+        if (vector.isNull[0]) {\n+          return batchSize;\n+        } else {\n+          return 0;\n+        }\n+      } else if (vector.noNulls) {\n+        return 0;\n+      } else {\n+        int count = 0;\n+        for (int i = 0; i < batchSize; i++) {\n+          if (vector.isNull[i]) {\n+            count++;\n+          }\n+        }\n+        return count;\n+      }\n+    }\n+\n+    protected int getRowIndex(int rowId) {\n+      return vector.isRepeating ? 0 : rowId;\n+    }\n+\n+    @Override\n+    public boolean isNullAt(int rowId) {\n+      return vector.isNull[getRowIndex(rowId)];\n+    }\n+\n+    @Override\n+    public boolean getBoolean(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public byte getByte(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public short getShort(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public int getInt(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public long getLong(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public float getFloat(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public double getDouble(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public Decimal getDecimal(int rowId, int precision, int scale) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public UTF8String getUTF8String(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public byte[] getBinary(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public ColumnarArray getArray(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public ColumnarMap getMap(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public ColumnVector getChild(int ordinal) {\n+      throw new UnsupportedOperationException();\n+    }\n+  }\n+\n+  private static class PrimitiveOrcColumnVector extends BaseOrcColumnVector {\n+    private final org.apache.orc.storage.ql.exec.vector.ColumnVector vector;\n+    private final OrcValueReader<?> primitiveValueReader;\n+\n+    PrimitiveOrcColumnVector(Type type, int batchSize, org.apache.orc.storage.ql.exec.vector.ColumnVector vector,\n+        OrcValueReader<?> primitiveValueReader) {\n+      super(type, batchSize, vector);\n+      this.vector = vector;\n+      this.primitiveValueReader = primitiveValueReader;\n+    }\n+\n+    @Override\n+    public boolean getBoolean(int rowId) {\n+      Boolean value = (Boolean) primitiveValueReader.read(vector, rowId);\n+      return value != null ? value : false;\n+    }\n+\n+    @Override\n+    public int getInt(int rowId) {\n+      Integer value = (Integer) primitiveValueReader.read(vector, rowId);\n+      return value != null ? value : 0;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEyNTM3NQ=="}, "originalCommit": {"oid": "6a5e8c53d3950922b8f6bd753329b7d943ac423d"}, "originalPosition": 282}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3755, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}