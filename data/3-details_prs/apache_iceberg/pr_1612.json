{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTAzMjk4Mjc3", "number": 1612, "title": "Hive: Using Hive schema to create tables and partition specification", "bodyText": "As discussed on #1495 we should create the table specification from the columns in the table creation command.\nThis PR does this.\nHere are the changes:\n\nCreate the Iceberg schema using the serDeProperties\nCreate the Iceberg partitioning specification using the partition columns defined in the CREATE TABLE command\nAdded tests which are reading the tables after creating them.\n\nChanges which are worth to double check:\n\nIf we are creating a Hive table with CRATE TABLE ... PARTITIONED BY command, then the resulting Iceberg table will be partitioned with identity partitions, but the Hive table itself will not be partitioned. This was needed since the read path is working with partitioned tables, and I do not see any good way to solve this since Hive wants to read the partitions one-by-one\nThe HadoopCatalog prevented setting the location when creating a new Iceberg table. Changed to allow calling withLocation if the provided location is set to defaultLocation, so I do not have to branch the code in Catalogs.\nOnly HiveCatalog will be using the default table location in Hive. When using other Catalogs the LOCATION should be provided in the CREATE TABLE command.", "createdAt": "2020-10-14T11:16:50Z", "url": "https://github.com/apache/iceberg/pull/1612", "merged": true, "mergeCommit": {"oid": "e69e52146d27956221ea4df4ad0baf2af7c827cd"}, "closed": true, "closedAt": "2020-11-26T00:34:05Z", "author": {"login": "pvary"}, "timelineItems": {"totalCount": 43, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdVcIAgAFqTUxNTk3Mjg2NA==", "endCursor": "Y3Vyc29yOnYyOpPPAAABdgH1rtgFqTUzODkyMDQwMg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE1OTcyODY0", "url": "https://github.com/apache/iceberg/pull/1612#pullrequestreview-515972864", "createdAt": "2020-10-23T19:55:43Z", "commit": {"oid": "047357045604654c0dab6bf50f55d01b77e4f88c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QxOTo1NTo0M1rOHnctbQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QxOTo1NTo0M1rOHnctbQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTEyNjg5Mw==", "bodyText": "What about disallowing a schema in Hive DDL when the table already exists? Then we could always fill in the schema from the Iceberg table.", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r511126893", "createdAt": "2020-10-23T19:55:43Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java", "diffHunk": "@@ -79,6 +83,7 @@ public void preCreateTable(org.apache.hadoop.hive.metastore.api.Table hmsTable)\n             \"Iceberg table already created - can not use provided schema\");\n         Preconditions.checkArgument(catalogProperties.getProperty(InputFormatConfig.PARTITION_SPEC) == null,\n             \"Iceberg table already created - can not use provided partition specification\");\n+        // TODO: Check type compatibility between this.icebergTable and hmsTable.getSd().getCols()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "047357045604654c0dab6bf50f55d01b77e4f88c"}, "originalPosition": 22}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE1OTc1MDA5", "url": "https://github.com/apache/iceberg/pull/1612#pullrequestreview-515975009", "createdAt": "2020-10-23T19:59:38Z", "commit": {"oid": "047357045604654c0dab6bf50f55d01b77e4f88c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QxOTo1OTozOFrOHnczmg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QxOTo1OTozOFrOHnczmg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTEyODQ3NA==", "bodyText": "I think it would be safer to detect the Hadoop catalog, right? There could be other catalogs that are not Hive but also don't require a location.", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r511128474", "createdAt": "2020-10-23T19:59:38Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java", "diffHunk": "@@ -89,22 +94,39 @@ public void preCreateTable(org.apache.hadoop.hive.metastore.api.Table hmsTable)\n     }\n \n     // If the table does not exist collect data for table creation\n-    String schemaString = catalogProperties.getProperty(InputFormatConfig.TABLE_SCHEMA);\n-    Preconditions.checkNotNull(schemaString, \"Please provide a table schema\");\n-    // Just check if it is parsable, and later use for partition specification parsing\n-    Schema schema = SchemaParser.fromJson(schemaString);\n-\n-    String specString = catalogProperties.getProperty(InputFormatConfig.PARTITION_SPEC);\n-    if (specString != null) {\n-      // Just check if it is parsable\n-      PartitionSpecParser.fromJson(schema, specString);\n+    // - InputFormatConfig.TABLE_SCHEMA, InputFormatConfig.PARTITION_SPEC takes precedence so the user can override the\n+    // Iceberg schema and specification generated by the code\n+    // - Partitioned Hive tables are converted to non-partitioned Hive tables and the Iceberg partition specification\n+    // is generated automatically based on the provided columns. If the MetaStore table contains partitioning\n+    // information then:\n+    //     - Merging the normal and partitioned columns for the table we are creating\n+    //     - Removing partition columns for the table we are creating\n+    //     - Creating Iceberg partitioning specification using the partition columns\n+\n+    Schema schema = schema(catalogProperties, hmsTable);\n+    PartitionSpec spec = spec(schema, catalogProperties, hmsTable);\n+\n+    catalogProperties.put(InputFormatConfig.TABLE_SCHEMA, SchemaParser.toJson(schema));\n+    catalogProperties.put(InputFormatConfig.PARTITION_SPEC, PartitionSpecParser.toJson(spec));\n+\n+    // Merging partition columns to the normal columns, since Hive table reads are working only on non-partitioned\n+    // tables\n+    if (hmsTable.getPartitionKeys() != null && !hmsTable.getPartitionKeys().isEmpty()) {\n+      hmsTable.getSd().getCols().addAll(hmsTable.getPartitionKeys());\n+      hmsTable.setPartitionKeysIsSet(false);\n     }\n \n     // Allow purging table data if the table is created now and not set otherwise\n     if (hmsTable.getParameters().get(InputFormatConfig.EXTERNAL_TABLE_PURGE) == null) {\n       hmsTable.getParameters().put(InputFormatConfig.EXTERNAL_TABLE_PURGE, \"TRUE\");\n     }\n \n+    // If the table is not managed by Hive catalog then the location should be set\n+    if (!Catalogs.hiveCatalog(conf)) {\n+      Preconditions.checkArgument(hmsTable.getSd() != null && hmsTable.getSd().getLocation() != null,\n+          \"Table location not set\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "047357045604654c0dab6bf50f55d01b77e4f88c"}, "originalPosition": 69}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE1OTk0MjEw", "url": "https://github.com/apache/iceberg/pull/1612#pullrequestreview-515994210", "createdAt": "2020-10-23T20:34:49Z", "commit": {"oid": "047357045604654c0dab6bf50f55d01b77e4f88c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QyMDozNDo1MFrOHndsFA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QyMDozNDo1MFrOHndsFA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTE0MjkzMg==", "bodyText": "I think CHAR should be mapped to String. Why map it to fixed? While the length is fixed, the behavior should be identical to string because the padding characters are ignored. CHAR is completely up to the engine to enforce, and otherwise behaves as a String.", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r511142932", "createdAt": "2020-10-23T20:34:50Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveSchemaUtil.java", "diffHunk": "@@ -0,0 +1,171 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.serde2.typeinfo.CharTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.ListTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.MapTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.types.Types;\n+\n+public class HiveSchemaUtil {\n+  private HiveSchemaUtil() {\n+  }\n+\n+  /**\n+   * Converts the list of Hive FieldSchemas to an Iceberg schema.\n+   * <p>\n+   * The list should contain the columns and the partition columns as well.\n+   * @param fieldSchemas The list of the columns\n+   * @return An equivalent Iceberg Schema\n+   */\n+  public static Schema schema(List<FieldSchema> fieldSchemas) {\n+    List<String> names = new ArrayList<>(fieldSchemas.size());\n+    List<TypeInfo> typeInfos = new ArrayList<>(fieldSchemas.size());\n+\n+    for (FieldSchema col : fieldSchemas) {\n+      names.add(col.getName());\n+      typeInfos.add(TypeInfoUtils.getTypeInfoFromTypeString(col.getType()));\n+    }\n+\n+    return convert(names, typeInfos);\n+  }\n+\n+  /**\n+   * Converts the Hive properties defining the columns to an Iceberg schema.\n+   * @param columnNames The property containing the column names\n+   * @param columnTypes The property containing the column types\n+   * @param columnNameDelimiter The name delimiter\n+   * @return The Iceberg schema\n+   */\n+  public static Schema schema(String columnNames, String columnTypes, String columnNameDelimiter) {\n+    // Parse the configuration parameters\n+    List<String> names = new ArrayList<>();\n+    Collections.addAll(names, columnNames.split(columnNameDelimiter));\n+\n+    return HiveSchemaUtil.convert(names, TypeInfoUtils.getTypeInfosFromTypeString(columnTypes));\n+  }\n+\n+  /**\n+   * Converts the Hive partition columns to Iceberg identity partition specification.\n+   * @param schema The Iceberg schema\n+   * @param fieldSchemas The partition column specification\n+   * @return The Iceberg partition specification\n+   */\n+  public static PartitionSpec spec(Schema schema, List<FieldSchema> fieldSchemas) {\n+    PartitionSpec.Builder builder = PartitionSpec.builderFor(schema);\n+    fieldSchemas.forEach(fieldSchema -> builder.identity(fieldSchema.getName()));\n+    return builder.build();\n+  }\n+\n+  private static Schema convert(List<String> names, List<TypeInfo> typeInfos) {\n+    HiveSchemaVisitor visitor = new HiveSchemaVisitor();\n+    return new Schema(visitor.visit(names, typeInfos));\n+  }\n+\n+  private static class HiveSchemaVisitor {\n+    private int id;\n+\n+    private HiveSchemaVisitor() {\n+      id = 0;\n+    }\n+\n+    private List<Types.NestedField> visit(List<String> names, List<TypeInfo> typeInfos) {\n+      List<Types.NestedField> result = new ArrayList<>(names.size());\n+      for (int i = 0; i < names.size(); ++i) {\n+        result.add(visit(names.get(i), typeInfos.get(i)));\n+      }\n+\n+      return result;\n+    }\n+\n+    private Types.NestedField visit(String name, TypeInfo typeInfo) {\n+      switch (typeInfo.getCategory()) {\n+        case PRIMITIVE:\n+          switch (((PrimitiveTypeInfo) typeInfo).getPrimitiveCategory()) {\n+            case FLOAT:\n+              return Types.NestedField.optional(id++, name, Types.FloatType.get());\n+            case DOUBLE:\n+              return Types.NestedField.optional(id++, name, Types.DoubleType.get());\n+            case BOOLEAN:\n+              return Types.NestedField.optional(id++, name, Types.BooleanType.get());\n+            case BYTE:\n+            case SHORT:\n+            case INT:\n+              return Types.NestedField.optional(id++, name, Types.IntegerType.get());\n+            case LONG:\n+              return Types.NestedField.optional(id++, name, Types.LongType.get());\n+            case BINARY:\n+              return Types.NestedField.optional(id++, name, Types.BinaryType.get());\n+            case STRING:\n+            case VARCHAR:\n+              return Types.NestedField.optional(id++, name, Types.StringType.get());\n+            case CHAR:\n+              Types.FixedType fixedType = Types.FixedType.ofLength(((CharTypeInfo) typeInfo).getLength());\n+              return Types.NestedField.optional(id++, name, fixedType);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "047357045604654c0dab6bf50f55d01b77e4f88c"}, "originalPosition": 132}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE1OTk0NzM0", "url": "https://github.com/apache/iceberg/pull/1612#pullrequestreview-515994734", "createdAt": "2020-10-23T20:35:46Z", "commit": {"oid": "047357045604654c0dab6bf50f55d01b77e4f88c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QyMDozNTo0N1rOHndtoQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QyMDozNTo0N1rOHndtoQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTE0MzMyOQ==", "bodyText": "Does Hive support both TIMESTAMP WITH ZONE and WITHOUT ZONE? I thought that the default was WITHOUT ZONE, so I'm surprised to see the default here to with zone.", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r511143329", "createdAt": "2020-10-23T20:35:47Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveSchemaUtil.java", "diffHunk": "@@ -0,0 +1,171 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.serde2.typeinfo.CharTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.ListTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.MapTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.types.Types;\n+\n+public class HiveSchemaUtil {\n+  private HiveSchemaUtil() {\n+  }\n+\n+  /**\n+   * Converts the list of Hive FieldSchemas to an Iceberg schema.\n+   * <p>\n+   * The list should contain the columns and the partition columns as well.\n+   * @param fieldSchemas The list of the columns\n+   * @return An equivalent Iceberg Schema\n+   */\n+  public static Schema schema(List<FieldSchema> fieldSchemas) {\n+    List<String> names = new ArrayList<>(fieldSchemas.size());\n+    List<TypeInfo> typeInfos = new ArrayList<>(fieldSchemas.size());\n+\n+    for (FieldSchema col : fieldSchemas) {\n+      names.add(col.getName());\n+      typeInfos.add(TypeInfoUtils.getTypeInfoFromTypeString(col.getType()));\n+    }\n+\n+    return convert(names, typeInfos);\n+  }\n+\n+  /**\n+   * Converts the Hive properties defining the columns to an Iceberg schema.\n+   * @param columnNames The property containing the column names\n+   * @param columnTypes The property containing the column types\n+   * @param columnNameDelimiter The name delimiter\n+   * @return The Iceberg schema\n+   */\n+  public static Schema schema(String columnNames, String columnTypes, String columnNameDelimiter) {\n+    // Parse the configuration parameters\n+    List<String> names = new ArrayList<>();\n+    Collections.addAll(names, columnNames.split(columnNameDelimiter));\n+\n+    return HiveSchemaUtil.convert(names, TypeInfoUtils.getTypeInfosFromTypeString(columnTypes));\n+  }\n+\n+  /**\n+   * Converts the Hive partition columns to Iceberg identity partition specification.\n+   * @param schema The Iceberg schema\n+   * @param fieldSchemas The partition column specification\n+   * @return The Iceberg partition specification\n+   */\n+  public static PartitionSpec spec(Schema schema, List<FieldSchema> fieldSchemas) {\n+    PartitionSpec.Builder builder = PartitionSpec.builderFor(schema);\n+    fieldSchemas.forEach(fieldSchema -> builder.identity(fieldSchema.getName()));\n+    return builder.build();\n+  }\n+\n+  private static Schema convert(List<String> names, List<TypeInfo> typeInfos) {\n+    HiveSchemaVisitor visitor = new HiveSchemaVisitor();\n+    return new Schema(visitor.visit(names, typeInfos));\n+  }\n+\n+  private static class HiveSchemaVisitor {\n+    private int id;\n+\n+    private HiveSchemaVisitor() {\n+      id = 0;\n+    }\n+\n+    private List<Types.NestedField> visit(List<String> names, List<TypeInfo> typeInfos) {\n+      List<Types.NestedField> result = new ArrayList<>(names.size());\n+      for (int i = 0; i < names.size(); ++i) {\n+        result.add(visit(names.get(i), typeInfos.get(i)));\n+      }\n+\n+      return result;\n+    }\n+\n+    private Types.NestedField visit(String name, TypeInfo typeInfo) {\n+      switch (typeInfo.getCategory()) {\n+        case PRIMITIVE:\n+          switch (((PrimitiveTypeInfo) typeInfo).getPrimitiveCategory()) {\n+            case FLOAT:\n+              return Types.NestedField.optional(id++, name, Types.FloatType.get());\n+            case DOUBLE:\n+              return Types.NestedField.optional(id++, name, Types.DoubleType.get());\n+            case BOOLEAN:\n+              return Types.NestedField.optional(id++, name, Types.BooleanType.get());\n+            case BYTE:\n+            case SHORT:\n+            case INT:\n+              return Types.NestedField.optional(id++, name, Types.IntegerType.get());\n+            case LONG:\n+              return Types.NestedField.optional(id++, name, Types.LongType.get());\n+            case BINARY:\n+              return Types.NestedField.optional(id++, name, Types.BinaryType.get());\n+            case STRING:\n+            case VARCHAR:\n+              return Types.NestedField.optional(id++, name, Types.StringType.get());\n+            case CHAR:\n+              Types.FixedType fixedType = Types.FixedType.ofLength(((CharTypeInfo) typeInfo).getLength());\n+              return Types.NestedField.optional(id++, name, fixedType);\n+            case TIMESTAMP:\n+              return Types.NestedField.optional(id++, name, Types.TimestampType.withZone());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "047357045604654c0dab6bf50f55d01b77e4f88c"}, "originalPosition": 134}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE1OTk2MDMx", "url": "https://github.com/apache/iceberg/pull/1612#pullrequestreview-515996031", "createdAt": "2020-10-23T20:38:18Z", "commit": {"oid": "047357045604654c0dab6bf50f55d01b77e4f88c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QyMDozODoxOFrOHndxfQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QyMDozODoxOFrOHndxfQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTE0NDMxNw==", "bodyText": "Would it make sense to have a method for TypeInfo so you don't have to create an artificial name and then discard the field information that isn't data type?", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r511144317", "createdAt": "2020-10-23T20:38:18Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveSchemaUtil.java", "diffHunk": "@@ -0,0 +1,171 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.serde2.typeinfo.CharTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.ListTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.MapTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.types.Types;\n+\n+public class HiveSchemaUtil {\n+  private HiveSchemaUtil() {\n+  }\n+\n+  /**\n+   * Converts the list of Hive FieldSchemas to an Iceberg schema.\n+   * <p>\n+   * The list should contain the columns and the partition columns as well.\n+   * @param fieldSchemas The list of the columns\n+   * @return An equivalent Iceberg Schema\n+   */\n+  public static Schema schema(List<FieldSchema> fieldSchemas) {\n+    List<String> names = new ArrayList<>(fieldSchemas.size());\n+    List<TypeInfo> typeInfos = new ArrayList<>(fieldSchemas.size());\n+\n+    for (FieldSchema col : fieldSchemas) {\n+      names.add(col.getName());\n+      typeInfos.add(TypeInfoUtils.getTypeInfoFromTypeString(col.getType()));\n+    }\n+\n+    return convert(names, typeInfos);\n+  }\n+\n+  /**\n+   * Converts the Hive properties defining the columns to an Iceberg schema.\n+   * @param columnNames The property containing the column names\n+   * @param columnTypes The property containing the column types\n+   * @param columnNameDelimiter The name delimiter\n+   * @return The Iceberg schema\n+   */\n+  public static Schema schema(String columnNames, String columnTypes, String columnNameDelimiter) {\n+    // Parse the configuration parameters\n+    List<String> names = new ArrayList<>();\n+    Collections.addAll(names, columnNames.split(columnNameDelimiter));\n+\n+    return HiveSchemaUtil.convert(names, TypeInfoUtils.getTypeInfosFromTypeString(columnTypes));\n+  }\n+\n+  /**\n+   * Converts the Hive partition columns to Iceberg identity partition specification.\n+   * @param schema The Iceberg schema\n+   * @param fieldSchemas The partition column specification\n+   * @return The Iceberg partition specification\n+   */\n+  public static PartitionSpec spec(Schema schema, List<FieldSchema> fieldSchemas) {\n+    PartitionSpec.Builder builder = PartitionSpec.builderFor(schema);\n+    fieldSchemas.forEach(fieldSchema -> builder.identity(fieldSchema.getName()));\n+    return builder.build();\n+  }\n+\n+  private static Schema convert(List<String> names, List<TypeInfo> typeInfos) {\n+    HiveSchemaVisitor visitor = new HiveSchemaVisitor();\n+    return new Schema(visitor.visit(names, typeInfos));\n+  }\n+\n+  private static class HiveSchemaVisitor {\n+    private int id;\n+\n+    private HiveSchemaVisitor() {\n+      id = 0;\n+    }\n+\n+    private List<Types.NestedField> visit(List<String> names, List<TypeInfo> typeInfos) {\n+      List<Types.NestedField> result = new ArrayList<>(names.size());\n+      for (int i = 0; i < names.size(); ++i) {\n+        result.add(visit(names.get(i), typeInfos.get(i)));\n+      }\n+\n+      return result;\n+    }\n+\n+    private Types.NestedField visit(String name, TypeInfo typeInfo) {\n+      switch (typeInfo.getCategory()) {\n+        case PRIMITIVE:\n+          switch (((PrimitiveTypeInfo) typeInfo).getPrimitiveCategory()) {\n+            case FLOAT:\n+              return Types.NestedField.optional(id++, name, Types.FloatType.get());\n+            case DOUBLE:\n+              return Types.NestedField.optional(id++, name, Types.DoubleType.get());\n+            case BOOLEAN:\n+              return Types.NestedField.optional(id++, name, Types.BooleanType.get());\n+            case BYTE:\n+            case SHORT:\n+            case INT:\n+              return Types.NestedField.optional(id++, name, Types.IntegerType.get());\n+            case LONG:\n+              return Types.NestedField.optional(id++, name, Types.LongType.get());\n+            case BINARY:\n+              return Types.NestedField.optional(id++, name, Types.BinaryType.get());\n+            case STRING:\n+            case VARCHAR:\n+              return Types.NestedField.optional(id++, name, Types.StringType.get());\n+            case CHAR:\n+              Types.FixedType fixedType = Types.FixedType.ofLength(((CharTypeInfo) typeInfo).getLength());\n+              return Types.NestedField.optional(id++, name, fixedType);\n+            case TIMESTAMP:\n+              return Types.NestedField.optional(id++, name, Types.TimestampType.withZone());\n+            case DATE:\n+              return Types.NestedField.optional(id++, name, Types.DateType.get());\n+            case DECIMAL:\n+              DecimalTypeInfo decimalTypeInfo = (DecimalTypeInfo) typeInfo;\n+              Types.DecimalType decimalType =\n+                  Types.DecimalType.of(decimalTypeInfo.precision(), decimalTypeInfo.scale());\n+              return Types.NestedField.optional(id++, name, decimalType);\n+            // TODO: In Hive3 we have TIMESTAMPLOCALTZ\n+            default:\n+              throw new IllegalArgumentException(\"Unknown primitive type \" +\n+                  ((PrimitiveTypeInfo) typeInfo).getPrimitiveCategory());\n+          }\n+        case STRUCT:\n+          StructTypeInfo structTypeInfo = (StructTypeInfo) typeInfo;\n+          List<Types.NestedField> fields =\n+              visit(structTypeInfo.getAllStructFieldNames(), structTypeInfo.getAllStructFieldTypeInfos());\n+          Types.StructType structType = Types.StructType.of(fields);\n+          return Types.NestedField.optional(id++, name, structType);\n+        case MAP:\n+          MapTypeInfo mapTypeInfo = (MapTypeInfo) typeInfo;\n+          Types.NestedField keyField = visit(name + \"_key\", mapTypeInfo.getMapKeyTypeInfo());\n+          Types.NestedField valueField = visit(name + \"_value\", mapTypeInfo.getMapValueTypeInfo());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "047357045604654c0dab6bf50f55d01b77e4f88c"}, "originalPosition": 156}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE1OTk2NTAw", "url": "https://github.com/apache/iceberg/pull/1612#pullrequestreview-515996500", "createdAt": "2020-10-23T20:39:15Z", "commit": {"oid": "047357045604654c0dab6bf50f55d01b77e4f88c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QyMDozOToxNVrOHndy7w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QyMDozOToxNVrOHndy7w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTE0NDY4Nw==", "bodyText": "Empty string defaults are very suspicious to me. Why not use null here?", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r511144687", "createdAt": "2020-10-23T20:39:15Z", "author": {"login": "rdblue"}, "path": "mr/src/test/java/org/apache/iceberg/mr/hive/TestTables.java", "diffHunk": "@@ -172,5 +221,18 @@ public String identifier(String tableIdentifier) {\n     public Map<String, String> properties() {\n       return ImmutableMap.of(InputFormatConfig.CATALOG, \"hive\");\n     }\n+\n+    @Override\n+    public String locationForCreateTableSQL(TableIdentifier identifier) {\n+      return \"\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "047357045604654c0dab6bf50f55d01b77e4f88c"}, "originalPosition": 118}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "047357045604654c0dab6bf50f55d01b77e4f88c", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/047357045604654c0dab6bf50f55d01b77e4f88c", "committedDate": "2020-10-16T10:36:12Z", "message": "Moved table specific location generation to TestTables instead of the specific catalog test classes"}, "afterCommit": {"oid": "d1940bb6ff27e961f5e18039772519676d86776d", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/d1940bb6ff27e961f5e18039772519676d86776d", "committedDate": "2020-11-02T17:05:33Z", "message": "Some review comment changes:\n- CHAR\n- TypeInfo generation instead of NestedField in conversion"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "73d331bf60ad47bbd2d969fe6a3290ef7949d27c", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/73d331bf60ad47bbd2d969fe6a3290ef7949d27c", "committedDate": "2020-11-02T18:44:15Z", "message": "Checkstyle"}, "afterCommit": {"oid": "ba014580c536aa8d8aec85b0a90e0f48f7d96c55", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/ba014580c536aa8d8aec85b0a90e0f48f7d96c55", "committedDate": "2020-11-03T14:09:18Z", "message": "Different Hive2/Hive3 timestamp handling"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI0OTM0OTE3", "url": "https://github.com/apache/iceberg/pull/1612#pullrequestreview-524934917", "createdAt": "2020-11-06T08:16:16Z", "commit": {"oid": "ba014580c536aa8d8aec85b0a90e0f48f7d96c55"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQwODoxNjoxNlrOHukCnw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQwODoxNjoxNlrOHukCnw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODU4NzAzOQ==", "bodyText": "Returning Types.TimestampType.withoutZone() will use IcebergTimestampObjectInspector.INSTANCE_WITHOUT_ZONE instance to get local time and then throw ClassCastException", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r518587039", "createdAt": "2020-11-06T08:16:16Z", "author": {"login": "qphien"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveSchemaVisitor.java", "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.ListTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.MapTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+\n+public class HiveSchemaVisitor {\n+  private int id;\n+\n+  public HiveSchemaVisitor() {\n+    id = 0;\n+  }\n+\n+  List<Types.NestedField> visit(List<String> names, List<TypeInfo> typeInfos) {\n+    List<Types.NestedField> result = new ArrayList<>(names.size());\n+    for (int i = 0; i < names.size(); ++i) {\n+      result.add(Types.NestedField.optional(id++, names.get(i), visit(typeInfos.get(i))));\n+    }\n+\n+    return result;\n+  }\n+\n+  Type visit(TypeInfo typeInfo) {\n+    switch (typeInfo.getCategory()) {\n+      case PRIMITIVE:\n+        switch (((PrimitiveTypeInfo) typeInfo).getPrimitiveCategory()) {\n+          case FLOAT:\n+            return Types.FloatType.get();\n+          case DOUBLE:\n+            return Types.DoubleType.get();\n+          case BOOLEAN:\n+            return Types.BooleanType.get();\n+          case BYTE:\n+          case SHORT:\n+          case INT:\n+            return Types.IntegerType.get();\n+          case LONG:\n+            return Types.LongType.get();\n+          case BINARY:\n+            return Types.BinaryType.get();\n+          case STRING:\n+          case CHAR:\n+          case VARCHAR:\n+            return Types.StringType.get();\n+          case TIMESTAMP:\n+            return Types.TimestampType.withoutZone();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ba014580c536aa8d8aec85b0a90e0f48f7d96c55"}, "originalPosition": 72}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "ba014580c536aa8d8aec85b0a90e0f48f7d96c55", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/ba014580c536aa8d8aec85b0a90e0f48f7d96c55", "committedDate": "2020-11-03T14:09:18Z", "message": "Different Hive2/Hive3 timestamp handling"}, "afterCommit": {"oid": "4102aba5abe38f18c0b92cf9d15c5ddf0d36a147", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/4102aba5abe38f18c0b92cf9d15c5ddf0d36a147", "committedDate": "2020-11-12T12:48:04Z", "message": "Handling Timestamp / Timestamp with local timezone and friends.\nAlso 2 small fixes"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "4102aba5abe38f18c0b92cf9d15c5ddf0d36a147", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/4102aba5abe38f18c0b92cf9d15c5ddf0d36a147", "committedDate": "2020-11-12T12:48:04Z", "message": "Handling Timestamp / Timestamp with local timezone and friends.\nAlso 2 small fixes"}, "afterCommit": {"oid": "2c1630f2492ce25481989f869e788ef2bd3ae8f0", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/2c1630f2492ce25481989f869e788ef2bd3ae8f0", "committedDate": "2020-11-12T12:56:49Z", "message": "Handling Timestamp / Timestamp with local timezone and friends.\nAlso 2 small fixes"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI5NjQxNzI2", "url": "https://github.com/apache/iceberg/pull/1612#pullrequestreview-529641726", "createdAt": "2020-11-13T01:41:33Z", "commit": {"oid": "2c1630f2492ce25481989f869e788ef2bd3ae8f0"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QwMTo0MTozNFrOHyWP4Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QwMTo0MTozNFrOHyWP4Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU1NTM2MQ==", "bodyText": "Why is this specific to Hive 2?", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r522555361", "createdAt": "2020-11-13T01:41:34Z", "author": {"login": "rdblue"}, "path": "build.gradle", "diffHunk": "@@ -511,6 +511,7 @@ if (jdkVersion == '8') {\n \n     // exclude these Hive2-specific tests from iceberg-mr\n     test {\n+      exclude '**/TestHiveSchemaUtil.class'", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2c1630f2492ce25481989f869e788ef2bd3ae8f0"}, "originalPosition": 4}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI5NjQzMjM3", "url": "https://github.com/apache/iceberg/pull/1612#pullrequestreview-529643237", "createdAt": "2020-11-13T01:46:09Z", "commit": {"oid": "2c1630f2492ce25481989f869e788ef2bd3ae8f0"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QwMTo0NjowOVrOHyWVCA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QwMTo0NjowOVrOHyWVCA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU1NjY4MA==", "bodyText": "It looks like this visitor is a converter. We may want to fix the name.", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r522556680", "createdAt": "2020-11-13T01:46:09Z", "author": {"login": "rdblue"}, "path": "hive3/src/main/java/org/apache/iceberg/mr/hive/Hive3SchemaVisitor.java", "diffHunk": "@@ -0,0 +1,43 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+\n+public class Hive3SchemaVisitor extends HiveSchemaVisitor {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2c1630f2492ce25481989f869e788ef2bd3ae8f0"}, "originalPosition": 27}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI5NjQzNTg5", "url": "https://github.com/apache/iceberg/pull/1612#pullrequestreview-529643589", "createdAt": "2020-11-13T01:47:09Z", "commit": {"oid": "2c1630f2492ce25481989f869e788ef2bd3ae8f0"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QwMTo0NzowOVrOHyWWWQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QwMTo0NzowOVrOHyWWWQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU1NzAxNw==", "bodyText": "Looks like a lot of this is handling that is specific to Hive 3. Is it possible to exclude those tests from Hive 3 for now and review just the base changes? Then we can add Hive 3 later and enable the shared tests.", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r522557017", "createdAt": "2020-11-13T01:47:09Z", "author": {"login": "rdblue"}, "path": "hive3/src/main/java/org/apache/iceberg/mr/hive/serde/objectinspector/IcebergTimestampLocalTZObjectInspectorHive3.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive.serde.objectinspector;\n+\n+import java.time.Instant;\n+import java.time.OffsetDateTime;\n+import java.time.ZoneOffset;\n+import java.time.ZonedDateTime;\n+import org.apache.hadoop.hive.common.type.TimestampTZ;\n+import org.apache.hadoop.hive.serde2.io.TimestampLocalTZWritable;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.AbstractPrimitiveJavaObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.TimestampLocalTZObjectInspector;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n+\n+public class IcebergTimestampLocalTZObjectInspectorHive3 extends AbstractPrimitiveJavaObjectInspector", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2c1630f2492ce25481989f869e788ef2bd3ae8f0"}, "originalPosition": 32}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI5NjQ1MjMz", "url": "https://github.com/apache/iceberg/pull/1612#pullrequestreview-529645233", "createdAt": "2020-11-13T01:52:09Z", "commit": {"oid": "2c1630f2492ce25481989f869e788ef2bd3ae8f0"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QwMTo1MjowOVrOHyWb9g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QwMTo1MjowOVrOHyWb9g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU1ODQ1NA==", "bodyText": "To make sure tests don't break across zones, we create date/time values using Instant or millis. Looks like you're trying to add micros to this test, which I think is a good idea. But we should avoid using these factory methods because the underlying values are not reliable.", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r522558454", "createdAt": "2020-11-13T01:52:09Z", "author": {"login": "rdblue"}, "path": "hive3/src/test/java/org/apache/iceberg/mr/hive/serde/objectinspector/TestIcebergTimestampObjectInspectorHive3.java", "diffHunk": "@@ -52,9 +54,8 @@ public void testIcebergTimestampObjectInspector() {\n     Assert.assertNull(oi.getPrimitiveJavaObject(null));\n     Assert.assertNull(oi.getPrimitiveWritableObject(null));\n \n-    long epochMilli = 1601471970000L;\n-    LocalDateTime local = LocalDateTime.ofInstant(Instant.ofEpochMilli(epochMilli), ZoneId.of(\"UTC\"));\n-    Timestamp ts = Timestamp.ofEpochMilli(epochMilli);\n+    LocalDateTime local = LocalDateTime.of(2020, 11, 22, 8, 11, 16, 123456789);\n+    Timestamp ts = Timestamp.valueOf(\"2020-11-22 8:11:16.123456789\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2c1630f2492ce25481989f869e788ef2bd3ae8f0"}, "originalPosition": 37}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI5NjQ2NDYy", "url": "https://github.com/apache/iceberg/pull/1612#pullrequestreview-529646462", "createdAt": "2020-11-13T01:55:54Z", "commit": {"oid": "2c1630f2492ce25481989f869e788ef2bd3ae8f0"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QwMTo1NTo1NFrOHyWgPw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QwMTo1NTo1NFrOHyWgPw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU1OTU1MQ==", "bodyText": "What about rejecting the use of a schema if the table already exists in a different catalog? I think that would make more sense than allowing it but checking compatibility.", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r522559551", "createdAt": "2020-11-13T01:55:54Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java", "diffHunk": "@@ -80,6 +81,10 @@ public void preCreateTable(org.apache.hadoop.hive.metastore.api.Table hmsTable)\n         Preconditions.checkArgument(catalogProperties.getProperty(InputFormatConfig.PARTITION_SPEC) == null,\n             \"Iceberg table already created - can not use provided partition specification\");\n \n+        Schema hmsSchema = HiveSchemaUtil.schema(hmsTable.getSd().getCols());\n+        Preconditions.checkArgument(HiveSchemaUtil.compatible(hmsSchema, icebergTable.schema()),\n+            \"Iceberg table already created - with different specification\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2c1630f2492ce25481989f869e788ef2bd3ae8f0"}, "originalPosition": 14}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI5NjQ2ODMw", "url": "https://github.com/apache/iceberg/pull/1612#pullrequestreview-529646830", "createdAt": "2020-11-13T01:57:04Z", "commit": {"oid": "2c1630f2492ce25481989f869e788ef2bd3ae8f0"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QwMTo1NzowNVrOHyWhig==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QwMTo1NzowNVrOHyWhig==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU1OTg4Mg==", "bodyText": "In this case, I think we also need to check the hmsTable schema and validate that it is either null or compatible with the one set in the table property. I'd prefer requiring it to be null.", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r522559882", "createdAt": "2020-11-13T01:57:05Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java", "diffHunk": "@@ -185,4 +198,20 @@ private Properties getCatalogProperties(org.apache.hadoop.hive.metastore.api.Tab\n \n     return properties;\n   }\n+\n+  private Schema schema(Properties properties, org.apache.hadoop.hive.metastore.api.Table hmsTable) {\n+    if (properties.getProperty(InputFormatConfig.TABLE_SCHEMA) != null) {\n+      return SchemaParser.fromJson(properties.getProperty(InputFormatConfig.TABLE_SCHEMA));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2c1630f2492ce25481989f869e788ef2bd3ae8f0"}, "originalPosition": 67}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI5NjQ3MTQ3", "url": "https://github.com/apache/iceberg/pull/1612#pullrequestreview-529647147", "createdAt": "2020-11-13T01:58:04Z", "commit": {"oid": "2c1630f2492ce25481989f869e788ef2bd3ae8f0"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QwMTo1ODowNFrOHyWikg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QwMTo1ODowNFrOHyWikg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU2MDE0Ng==", "bodyText": "I think this check should be in spec, like validation that the table schema is not present.", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r522560146", "createdAt": "2020-11-13T01:58:04Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java", "diffHunk": "@@ -89,22 +94,30 @@ public void preCreateTable(org.apache.hadoop.hive.metastore.api.Table hmsTable)\n     }\n \n     // If the table does not exist collect data for table creation\n-    String schemaString = catalogProperties.getProperty(InputFormatConfig.TABLE_SCHEMA);\n-    Preconditions.checkNotNull(schemaString, \"Please provide a table schema\");\n-    // Just check if it is parsable, and later use for partition specification parsing\n-    Schema schema = SchemaParser.fromJson(schemaString);\n-\n-    String specString = catalogProperties.getProperty(InputFormatConfig.PARTITION_SPEC);\n-    if (specString != null) {\n-      // Just check if it is parsable\n-      PartitionSpecParser.fromJson(schema, specString);\n-    }\n+    // - InputFormatConfig.TABLE_SCHEMA, InputFormatConfig.PARTITION_SPEC takes precedence so the user can override the\n+    // Iceberg schema and specification generated by the code\n+    // - Partitioned Hive tables are currently not allowed\n+\n+    Schema schema = schema(catalogProperties, hmsTable);\n+    PartitionSpec spec = spec(schema, catalogProperties);\n+\n+    catalogProperties.put(InputFormatConfig.TABLE_SCHEMA, SchemaParser.toJson(schema));\n+    catalogProperties.put(InputFormatConfig.PARTITION_SPEC, PartitionSpecParser.toJson(spec));\n+\n+    Preconditions.checkArgument(hmsTable.getPartitionKeys() == null || hmsTable.getPartitionKeys().isEmpty(),\n+        \"Partitioned Hive tables are currently not supported\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2c1630f2492ce25481989f869e788ef2bd3ae8f0"}, "originalPosition": 44}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMzMzI5MzQ1", "url": "https://github.com/apache/iceberg/pull/1612#pullrequestreview-533329345", "createdAt": "2020-11-18T10:59:38Z", "commit": {"oid": "99f8bad169c4c0859909981b068072141df05b6d"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQxMDo1OTozOFrOH1oSOg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQxMDo1OTozOFrOH1oSOg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTk5NjYwMg==", "bodyText": "It might make sense to throw an exception if the user wants to use these Hive types (byte, short, char, varchar, etc.) which do not exist in Iceberg. E.g. \"unsupported Hive type (byte) for Iceberg tables. Consider using type (int) instead.\" - or something along the lines. Silently using a different Iceberg type than the one specified in the Hive DDL (e.g. Hive short -> Iceberg int) under the hood can violate the expectations of the user, for example regarding storage footprint, so it might be better to force a one-to-one type mapping.", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r525996602", "createdAt": "2020-11-18T10:59:38Z", "author": {"login": "marton-bod"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveSchemaConverter.java", "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.ListTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.MapTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+\n+class HiveSchemaConverter {\n+  private int id;\n+\n+  HiveSchemaConverter() {\n+    id = 0;\n+  }\n+\n+  List<Types.NestedField> convert(List<String> names, List<TypeInfo> typeInfos) {\n+    List<Types.NestedField> result = new ArrayList<>(names.size());\n+    for (int i = 0; i < names.size(); ++i) {\n+      result.add(Types.NestedField.optional(id++, names.get(i), convert(typeInfos.get(i))));\n+    }\n+\n+    return result;\n+  }\n+\n+  Type convert(TypeInfo typeInfo) {\n+    switch (typeInfo.getCategory()) {\n+      case PRIMITIVE:\n+        switch (((PrimitiveTypeInfo) typeInfo).getPrimitiveCategory()) {\n+          case FLOAT:\n+            return Types.FloatType.get();\n+          case DOUBLE:\n+            return Types.DoubleType.get();\n+          case BOOLEAN:\n+            return Types.BooleanType.get();\n+          case BYTE:\n+          case SHORT:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "99f8bad169c4c0859909981b068072141df05b6d"}, "originalPosition": 60}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM0MDAyMzU0", "url": "https://github.com/apache/iceberg/pull/1612#pullrequestreview-534002354", "createdAt": "2020-11-19T01:02:15Z", "commit": {"oid": "35324a864e676e89ca3e4a999e0bf0c48e4c51d0"}, "state": "COMMENTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQwMTowMjoxNVrOH2IdYg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQwMzozOTozMVrOH2LhdA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjUyMzc0Ng==", "bodyText": "Should we make this a static method? Seems like .convert() only makes sense to be called once per HiveSchemaConverter object, else the id counter will be already incremented at the beginning.", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r526523746", "createdAt": "2020-11-19T01:02:15Z", "author": {"login": "shardulm94"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveSchemaConverter.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.ListTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.MapTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+\n+class HiveSchemaConverter {\n+  private int id;\n+\n+  HiveSchemaConverter() {\n+    id = 0;\n+  }\n+\n+  List<Types.NestedField> convert(List<String> names, List<TypeInfo> typeInfos) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "35324a864e676e89ca3e4a999e0bf0c48e4c51d0"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjU2NDE4Mg==", "bodyText": "I think what @rdblue means here is that it should be in the spec() method below", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r526564182", "createdAt": "2020-11-19T03:10:48Z", "author": {"login": "shardulm94"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java", "diffHunk": "@@ -89,22 +94,30 @@ public void preCreateTable(org.apache.hadoop.hive.metastore.api.Table hmsTable)\n     }\n \n     // If the table does not exist collect data for table creation\n-    String schemaString = catalogProperties.getProperty(InputFormatConfig.TABLE_SCHEMA);\n-    Preconditions.checkNotNull(schemaString, \"Please provide a table schema\");\n-    // Just check if it is parsable, and later use for partition specification parsing\n-    Schema schema = SchemaParser.fromJson(schemaString);\n-\n-    String specString = catalogProperties.getProperty(InputFormatConfig.PARTITION_SPEC);\n-    if (specString != null) {\n-      // Just check if it is parsable\n-      PartitionSpecParser.fromJson(schema, specString);\n-    }\n+    // - InputFormatConfig.TABLE_SCHEMA, InputFormatConfig.PARTITION_SPEC takes precedence so the user can override the\n+    // Iceberg schema and specification generated by the code\n+    // - Partitioned Hive tables are currently not allowed\n+\n+    Schema schema = schema(catalogProperties, hmsTable);\n+    PartitionSpec spec = spec(schema, catalogProperties);\n+\n+    catalogProperties.put(InputFormatConfig.TABLE_SCHEMA, SchemaParser.toJson(schema));\n+    catalogProperties.put(InputFormatConfig.PARTITION_SPEC, PartitionSpecParser.toJson(spec));\n+\n+    Preconditions.checkArgument(hmsTable.getPartitionKeys() == null || hmsTable.getPartitionKeys().isEmpty(),\n+        \"Partitioned Hive tables are currently not supported\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU2MDE0Ng=="}, "originalCommit": {"oid": "2c1630f2492ce25481989f869e788ef2bd3ae8f0"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjU3MDI4MA==", "bodyText": "Not sure I understand the usecases mentioned here. Why would the user need to specify a Hive schema when creating the Hive table even for the UUID type lets say? Shouldn't the deserializer be responsible for returning the Hive compatible schema?\nE.g. for Avro tables, users do not need to specify a \"string\" column for enum types, the AvroSerDe maps it to string correctly.", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r526570280", "createdAt": "2020-11-19T03:26:07Z", "author": {"login": "shardulm94"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java", "diffHunk": "@@ -80,6 +81,10 @@ public void preCreateTable(org.apache.hadoop.hive.metastore.api.Table hmsTable)\n         Preconditions.checkArgument(catalogProperties.getProperty(InputFormatConfig.PARTITION_SPEC) == null,\n             \"Iceberg table already created - can not use provided partition specification\");\n \n+        Schema hmsSchema = HiveSchemaUtil.schema(hmsTable.getSd().getCols());\n+        Preconditions.checkArgument(HiveSchemaUtil.compatible(hmsSchema, icebergTable.schema()),\n+            \"Iceberg table already created - with different specification\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU1OTU1MQ=="}, "originalCommit": {"oid": "2c1630f2492ce25481989f869e788ef2bd3ae8f0"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjU3MjE2MA==", "bodyText": "Are these properties set on table as properties, or are they generated dynamically during reads for column pruning?\nIf they are set as table properties, what happens if they go stale with respect to the iceberg table? e.g. column renames, type promotions, new top level fields added, etc.", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r526572160", "createdAt": "2020-11-19T03:32:57Z", "author": {"login": "shardulm94"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergSerDe.java", "diffHunk": "@@ -56,10 +62,22 @@ public void initialize(@Nullable Configuration configuration, Properties serDePr\n     } else if (serDeProperties.get(InputFormatConfig.TABLE_SCHEMA) != null) {\n       tableSchema = SchemaParser.fromJson((String) serDeProperties.get(InputFormatConfig.TABLE_SCHEMA));\n     } else {\n-      try {\n-        tableSchema = Catalogs.loadTable(configuration, serDeProperties).schema();\n-      } catch (NoSuchTableException nte) {\n-        throw new SerDeException(\"Please provide an existing table or a valid schema\", nte);\n+      // Read the configuration parameters\n+      String columnNames = serDeProperties.getProperty(serdeConstants.LIST_COLUMNS);\n+      String columnTypes = serDeProperties.getProperty(serdeConstants.LIST_COLUMN_TYPES);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "35324a864e676e89ca3e4a999e0bf0c48e4c51d0"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjU3MzU0NA==", "bodyText": "Does this preserve field name casing or will the return schema have all lowercase field names?", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r526573544", "createdAt": "2020-11-19T03:37:56Z", "author": {"login": "shardulm94"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergSerDe.java", "diffHunk": "@@ -56,10 +62,22 @@ public void initialize(@Nullable Configuration configuration, Properties serDePr\n     } else if (serDeProperties.get(InputFormatConfig.TABLE_SCHEMA) != null) {\n       tableSchema = SchemaParser.fromJson((String) serDeProperties.get(InputFormatConfig.TABLE_SCHEMA));\n     } else {\n-      try {\n-        tableSchema = Catalogs.loadTable(configuration, serDeProperties).schema();\n-      } catch (NoSuchTableException nte) {\n-        throw new SerDeException(\"Please provide an existing table or a valid schema\", nte);\n+      // Read the configuration parameters\n+      String columnNames = serDeProperties.getProperty(serdeConstants.LIST_COLUMNS);\n+      String columnTypes = serDeProperties.getProperty(serdeConstants.LIST_COLUMN_TYPES);\n+      String columnNameDelimiter = serDeProperties.containsKey(serdeConstants.COLUMN_NAME_DELIMITER) ?\n+          serDeProperties.getProperty(serdeConstants.COLUMN_NAME_DELIMITER) : String.valueOf(SerDeUtils.COMMA);\n+      if (columnNames != null && columnTypes != null && columnNameDelimiter != null &&\n+          !columnNames.isEmpty() && !columnTypes.isEmpty() && !columnNameDelimiter.isEmpty()) {\n+        tableSchema = HiveSchemaUtil.schema(columnNames, columnTypes, columnNameDelimiter);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "35324a864e676e89ca3e4a999e0bf0c48e4c51d0"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjU3Mzk0MA==", "bodyText": "Nit: Typo shake -> sake", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r526573940", "createdAt": "2020-11-19T03:39:31Z", "author": {"login": "shardulm94"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveSchemaUtil.java", "diffHunk": "@@ -0,0 +1,167 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.MapObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.StructField;\n+import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.TimestampObjectInspector;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.mr.hive.serde.objectinspector.IcebergObjectInspector;\n+import org.apache.iceberg.mr.hive.serde.objectinspector.IcebergRecordObjectInspector;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class HiveSchemaUtil {\n+  private static final Logger LOG = LoggerFactory.getLogger(HiveSchemaUtil.class);\n+\n+  private HiveSchemaUtil() {\n+  }\n+\n+  /**\n+   * Converts the list of Hive FieldSchemas to an Iceberg schema.\n+   * <p>\n+   * The list should contain the columns and the partition columns as well.\n+   * @param fieldSchemas The list of the columns\n+   * @return An equivalent Iceberg Schema\n+   */\n+  public static Schema schema(List<FieldSchema> fieldSchemas) {\n+    List<String> names = new ArrayList<>(fieldSchemas.size());\n+    List<TypeInfo> typeInfos = new ArrayList<>(fieldSchemas.size());\n+\n+    for (FieldSchema col : fieldSchemas) {\n+      names.add(col.getName());\n+      typeInfos.add(TypeInfoUtils.getTypeInfoFromTypeString(col.getType()));\n+    }\n+\n+    return convert(names, typeInfos);\n+  }\n+\n+  /**\n+   * Converts the Hive properties defining the columns to an Iceberg schema.\n+   * @param columnNames The property containing the column names\n+   * @param columnTypes The property containing the column types\n+   * @param columnNameDelimiter The name delimiter\n+   * @return The Iceberg schema\n+   */\n+  public static Schema schema(String columnNames, String columnTypes, String columnNameDelimiter) {\n+    // Parse the configuration parameters\n+    List<String> names = new ArrayList<>();\n+    Collections.addAll(names, columnNames.split(columnNameDelimiter));\n+\n+    return HiveSchemaUtil.convert(names, TypeInfoUtils.getTypeInfosFromTypeString(columnTypes));\n+  }\n+\n+  /**\n+   * Checks if the ObjectInspectors generated by the two schema definitions are compatible.\n+   * <p>\n+   * Currently only allows the same column names and column types. Later we might want allow compatible column types as\n+   * well.\n+   * TODO: We might want to allow compatible conversions\n+   * @param schema First schema\n+   * @param other Second schema\n+   * @return True if the two schema is compatible\n+   */\n+  public static boolean compatible(Schema schema, Schema other) {\n+    ObjectInspector inspector = IcebergObjectInspector.create(schema);\n+    ObjectInspector otherInspector = IcebergObjectInspector.create(other);\n+\n+    if (!(inspector instanceof IcebergRecordObjectInspector) ||\n+        !(otherInspector instanceof IcebergRecordObjectInspector)) {\n+      return false;\n+    }\n+\n+    return compatible(inspector, otherInspector);\n+  }\n+\n+  private static boolean compatible(ObjectInspector inspector, ObjectInspector other) {\n+    if (inspector == null && other == null) {\n+      // We do not expect this type of calls, but for completeness shake", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "35324a864e676e89ca3e4a999e0bf0c48e4c51d0"}, "originalPosition": 104}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "83ffcba8aee292bbc9ec14dfa36bc6eefe30421c", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/83ffcba8aee292bbc9ec14dfa36bc6eefe30421c", "committedDate": "2020-11-19T09:47:09Z", "message": "Addressed review comments"}, "afterCommit": {"oid": "1cb69d297c4c3f44f181d2477b0253902fb59143", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/1cb69d297c4c3f44f181d2477b0253902fb59143", "committedDate": "2020-11-24T10:52:24Z", "message": "Removing schema comparison"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "356f723adcb022d701b3c7ab0d9c03a69934e9a9", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/356f723adcb022d701b3c7ab0d9c03a69934e9a9", "committedDate": "2020-11-24T12:13:23Z", "message": "Using Hive schema to create tables and partition specification"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4ff16a8c309456d71dec8f75263f465316a3a7f6", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/4ff16a8c309456d71dec8f75263f465316a3a7f6", "committedDate": "2020-11-24T12:13:28Z", "message": "Fixing Custom catalog tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a40b6c27b14c34e72a292e6d3f35a18b618fb500", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/a40b6c27b14c34e72a292e6d3f35a18b618fb500", "committedDate": "2020-11-24T12:14:15Z", "message": "Moved table specific location generation to TestTables instead of the specific catalog test classes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e3b3905ae8fa59ae00a8daeb0eade6f4bead460c", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/e3b3905ae8fa59ae00a8daeb0eade6f4bead460c", "committedDate": "2020-11-24T12:14:18Z", "message": "Some review comment changes:\n- CHAR\n- TypeInfo generation instead of NestedField in conversion"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1d71fee3599bf47ebe77802f0a9d1a4f86bf7759", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/1d71fee3599bf47ebe77802f0a9d1a4f86bf7759", "committedDate": "2020-11-24T12:14:19Z", "message": "Throw an exception when PARTITONED BY is used in the CREATE TABLE command"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "133f1882f0ca6188ed46ed6d22971990a4068942", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/133f1882f0ca6188ed46ed6d22971990a4068942", "committedDate": "2020-11-24T12:14:19Z", "message": "Checkstyle"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "87202bf7a3dc823d1d30f02dea28631fefeacb77", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/87202bf7a3dc823d1d30f02dea28631fefeacb77", "committedDate": "2020-11-24T12:14:19Z", "message": "First version of compatibility checks"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0fecd948af6f974fa3cdea3161b531d635561f60", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/0fecd948af6f974fa3cdea3161b531d635561f60", "committedDate": "2020-11-24T12:14:19Z", "message": "Different Hive2/Hive3 timestamp handling"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "aefbca0aa6b134aa4d7f8a37fb9ab2b9d10501e5", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/aefbca0aa6b134aa4d7f8a37fb9ab2b9d10501e5", "committedDate": "2020-11-24T12:14:19Z", "message": "Handling Timestamp / Timestamp with local timezone and friends.\nAlso 2 small fixes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3c3f1bed72b1200621f95e3b3293448b80713309", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/3c3f1bed72b1200621f95e3b3293448b80713309", "committedDate": "2020-11-24T12:14:19Z", "message": "Reverted Timestamp related changes\nAddressed review comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ae751a6ef46283f3c0f1b973a3daae690951cc0d", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/ae751a6ef46283f3c0f1b973a3daae690951cc0d", "committedDate": "2020-11-24T12:14:19Z", "message": "Renamed wrong method name"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b4b985e0d31ca46f37db8516c02f73bc6c9232b5", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/b4b985e0d31ca46f37db8516c02f73bc6c9232b5", "committedDate": "2020-11-24T12:14:19Z", "message": "Prevent creating columns with Hive types which does not have an exact Iceberg interpretation."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1a3b6101f72c13c60db8b80f00189c3d9e4364a8", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/1a3b6101f72c13c60db8b80f00189c3d9e4364a8", "committedDate": "2020-11-24T12:14:19Z", "message": "Addressed review comments"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "1cb69d297c4c3f44f181d2477b0253902fb59143", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/1cb69d297c4c3f44f181d2477b0253902fb59143", "committedDate": "2020-11-24T10:52:24Z", "message": "Removing schema comparison"}, "afterCommit": {"oid": "a6622db349cda45960338428fe3a440ba1a8c279", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/a6622db349cda45960338428fe3a440ba1a8c279", "committedDate": "2020-11-24T12:14:19Z", "message": "Removing schema comparison"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a6792e2d815fb8a5674746f6db34d6e548b4ab78", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/a6792e2d815fb8a5674746f6db34d6e548b4ab78", "committedDate": "2020-11-24T14:33:28Z", "message": "Removing schema comparison"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "a6622db349cda45960338428fe3a440ba1a8c279", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/a6622db349cda45960338428fe3a440ba1a8c279", "committedDate": "2020-11-24T12:14:19Z", "message": "Removing schema comparison"}, "afterCommit": {"oid": "a6792e2d815fb8a5674746f6db34d6e548b4ab78", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/a6792e2d815fb8a5674746f6db34d6e548b4ab78", "committedDate": "2020-11-24T14:33:28Z", "message": "Removing schema comparison"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM4MTQwMzky", "url": "https://github.com/apache/iceberg/pull/1612#pullrequestreview-538140392", "createdAt": "2020-11-25T04:00:32Z", "commit": {"oid": "a6792e2d815fb8a5674746f6db34d6e548b4ab78"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNVQwNDowMDozMlrOH5iVcQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNVQwNDowMToxMFrOH5iWMA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDA5MzQyNQ==", "bodyText": "Can we extract this out in a common function to reuse above?", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r530093425", "createdAt": "2020-11-25T04:00:32Z", "author": {"login": "shardulm94"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergSerDe.java", "diffHunk": "@@ -56,10 +61,28 @@ public void initialize(@Nullable Configuration configuration, Properties serDePr\n     } else if (serDeProperties.get(InputFormatConfig.TABLE_SCHEMA) != null) {\n       tableSchema = SchemaParser.fromJson((String) serDeProperties.get(InputFormatConfig.TABLE_SCHEMA));\n     } else {\n-      try {\n-        tableSchema = Catalogs.loadTable(configuration, serDeProperties).schema();\n-      } catch (NoSuchTableException nte) {\n-        throw new SerDeException(\"Please provide an existing table or a valid schema\", nte);\n+      if (Catalogs.hiveCatalog(configuration)) {\n+        tableSchema = hiveSchema(serDeProperties);\n+        if (tableSchema == null) {\n+          throw new SerDeException(\"Please provide a valid schema\");\n+        } else {\n+          LOG.info(\"Using hive schema {}\", SchemaParser.toJson(tableSchema));\n+        }\n+      } else {\n+        try {\n+          // always prefer the original table schema if there is one\n+          tableSchema = Catalogs.loadTable(configuration, serDeProperties).schema();\n+          LOG.info(\"Using schema from existing table {}\", SchemaParser.toJson(tableSchema));\n+        } catch (Exception e) {\n+          // If we can not load the table try the provided hive schema\n+          tableSchema = hiveSchema(serDeProperties);\n+          if (tableSchema == null) {\n+            throw new SerDeException(\"Please provide an existing table or a valid schema\", e);\n+          } else {\n+            LOG.info(\"Using schema from column specification {} since table load is failed\",\n+                SchemaParser.toJson(tableSchema), e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a6792e2d815fb8a5674746f6db34d6e548b4ab78"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDA5MzYxNg==", "bodyText": "What is the reason behind handling HiveCatalogs separately?", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r530093616", "createdAt": "2020-11-25T04:01:10Z", "author": {"login": "shardulm94"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergSerDe.java", "diffHunk": "@@ -56,10 +61,28 @@ public void initialize(@Nullable Configuration configuration, Properties serDePr\n     } else if (serDeProperties.get(InputFormatConfig.TABLE_SCHEMA) != null) {\n       tableSchema = SchemaParser.fromJson((String) serDeProperties.get(InputFormatConfig.TABLE_SCHEMA));\n     } else {\n-      try {\n-        tableSchema = Catalogs.loadTable(configuration, serDeProperties).schema();\n-      } catch (NoSuchTableException nte) {\n-        throw new SerDeException(\"Please provide an existing table or a valid schema\", nte);\n+      if (Catalogs.hiveCatalog(configuration)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a6792e2d815fb8a5674746f6db34d6e548b4ab78"}, "originalPosition": 35}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "49569703b2cd1ab26a63674796dacfe287b7944d", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/49569703b2cd1ab26a63674796dacfe287b7944d", "committedDate": "2020-11-25T08:48:22Z", "message": "Updated hiveSchema parsing"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6f881bb80609b0ca13de963d3c919bf07c505f42", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/6f881bb80609b0ca13de963d3c919bf07c505f42", "committedDate": "2020-11-25T09:35:33Z", "message": "Handle HiveCatalog the same way as the other catalogs. This is suboptimal, but later we need that anyway"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ad53a0ae5d4a74ca79a3547695e4d35af480d09e", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/ad53a0ae5d4a74ca79a3547695e4d35af480d09e", "committedDate": "2020-11-25T11:56:31Z", "message": "Timestamp changens are not needed if we stick to the Iceberg schema\nAlso reverting some formatting only changes"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM4ODM2MjM2", "url": "https://github.com/apache/iceberg/pull/1612#pullrequestreview-538836236", "createdAt": "2020-11-25T20:47:04Z", "commit": {"oid": "ad53a0ae5d4a74ca79a3547695e4d35af480d09e"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM4OTIwNDAy", "url": "https://github.com/apache/iceberg/pull/1612#pullrequestreview-538920402", "createdAt": "2020-11-26T00:30:46Z", "commit": {"oid": "ad53a0ae5d4a74ca79a3547695e4d35af480d09e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNlQwMDozMDo0NlrOH6H0Kg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNlQwMDozMDo0NlrOH6H0Kg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDcwNzQ5OA==", "bodyText": "I would be fine mapping these to string.", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r530707498", "createdAt": "2020-11-26T00:30:46Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveSchemaConverter.java", "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.ListTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.MapTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+\n+class HiveSchemaConverter {\n+  private int id;\n+\n+  private HiveSchemaConverter() {\n+    id = 0;\n+  }\n+\n+  static Schema convert(List<String> names, List<TypeInfo> typeInfos) {\n+    HiveSchemaConverter converter = new HiveSchemaConverter();\n+    return new Schema(converter.convertInternal(names, typeInfos));\n+  }\n+\n+  List<Types.NestedField> convertInternal(List<String> names, List<TypeInfo> typeInfos) {\n+    List<Types.NestedField> result = new ArrayList<>(names.size());\n+    for (int i = 0; i < names.size(); ++i) {\n+      result.add(Types.NestedField.optional(id++, names.get(i), convert(typeInfos.get(i))));\n+    }\n+\n+    return result;\n+  }\n+\n+  Type convert(TypeInfo typeInfo) {\n+    switch (typeInfo.getCategory()) {\n+      case PRIMITIVE:\n+        switch (((PrimitiveTypeInfo) typeInfo).getPrimitiveCategory()) {\n+          case FLOAT:\n+            return Types.FloatType.get();\n+          case DOUBLE:\n+            return Types.DoubleType.get();\n+          case BOOLEAN:\n+            return Types.BooleanType.get();\n+          case BYTE:\n+          case SHORT:\n+            throw new IllegalArgumentException(\"Unsupported Hive type (\" +\n+                ((PrimitiveTypeInfo) typeInfo).getPrimitiveCategory() +\n+                \") for Iceberg tables. Consider using INT/INTEGER type instead.\");\n+          case INT:\n+            return Types.IntegerType.get();\n+          case LONG:\n+            return Types.LongType.get();\n+          case BINARY:\n+            return Types.BinaryType.get();\n+          case CHAR:\n+          case VARCHAR:\n+            throw new IllegalArgumentException(\"Unsupported Hive type (\" +\n+                ((PrimitiveTypeInfo) typeInfo).getPrimitiveCategory() +\n+                \") for Iceberg tables. Consider using STRING type instead.\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ad53a0ae5d4a74ca79a3547695e4d35af480d09e"}, "originalPosition": 80}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3958, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}