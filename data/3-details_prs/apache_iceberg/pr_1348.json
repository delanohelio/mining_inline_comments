{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDY4ODE1NTM4", "number": 1348, "title": "Flink: Support table sink.", "bodyText": "This patch will wrap the flink's DataStream  as a StreamTable,  which could allow user to use SQL to insert records to iceberg table,  it will try to provide the similar experience with spark sql. Currently, this patch is depending on #1185.", "createdAt": "2020-08-17T12:58:06Z", "url": "https://github.com/apache/iceberg/pull/1348", "merged": true, "mergeCommit": {"oid": "f153349f536818ecc686c4b097fc6b7ca70aaade"}, "closed": true, "closedAt": "2020-09-03T15:15:12Z", "author": {"login": "openinx"}, "timelineItems": {"totalCount": 33, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdAwb8YABqjM2NzUzMjg2ODU=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdFI4ddABqjM3MjI4MjE0OTM=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "cf9d490ce657bbf52fbdc020cef26af61e86332c", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/cf9d490ce657bbf52fbdc020cef26af61e86332c", "committedDate": "2020-08-17T12:51:46Z", "message": "Flink: Support table sink."}, "afterCommit": {"oid": "3fb0ca0c9237ede8824851e7fd3900f0a33cb0d7", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/3fb0ca0c9237ede8824851e7fd3900f0a33cb0d7", "committedDate": "2020-08-20T13:41:51Z", "message": "Introduce the catalog name."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "3fb0ca0c9237ede8824851e7fd3900f0a33cb0d7", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/3fb0ca0c9237ede8824851e7fd3900f0a33cb0d7", "committedDate": "2020-08-20T13:41:51Z", "message": "Introduce the catalog name."}, "afterCommit": {"oid": "948ea352e1e828f5f265b9a0ae602b73c62808c2", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/948ea352e1e828f5f265b9a0ae602b73c62808c2", "committedDate": "2020-08-20T14:24:28Z", "message": "Introduce the catalog name."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "e22a71bbd47f2ebe625c1dfac10c8d34fb9d4d3d", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/e22a71bbd47f2ebe625c1dfac10c8d34fb9d4d3d", "committedDate": "2020-08-21T01:53:50Z", "message": "Fix the broken unit tests."}, "afterCommit": {"oid": "4072210b430aad79a91de2103da516c9dc616912", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/4072210b430aad79a91de2103da516c9dc616912", "committedDate": "2020-08-25T10:35:16Z", "message": "Rebase to IcebergFilesCommitter"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "f07608c7e96a6784624b486c165961ee8cca0e5c", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/f07608c7e96a6784624b486c165961ee8cca0e5c", "committedDate": "2020-08-26T07:41:25Z", "message": "Remove access from SPI."}, "afterCommit": {"oid": "0345cd4d15679b3aea6afd4f8645fcd0aa12b498", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/0345cd4d15679b3aea6afd4f8645fcd0aa12b498", "committedDate": "2020-08-26T09:27:46Z", "message": "Rebase to master"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "49c5a29023aa43f1e62ef36a5fb9b2e3f82b1c3f", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/49c5a29023aa43f1e62ef36a5fb9b2e3f82b1c3f", "committedDate": "2020-08-26T12:54:20Z", "message": "Add batch unit tests."}, "afterCommit": {"oid": "0caf973eaf4e66dbf9a185cc98dfcb3053fdd870", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/0caf973eaf4e66dbf9a185cc98dfcb3053fdd870", "committedDate": "2020-08-27T12:19:35Z", "message": "Rebase to flink-committer"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc4MjQ2NjM5", "url": "https://github.com/apache/iceberg/pull/1348#pullrequestreview-478246639", "createdAt": "2020-08-30T23:54:03Z", "commit": {"oid": "0caf973eaf4e66dbf9a185cc98dfcb3053fdd870"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0zMFQyMzo1NDowM1rOHJmoeQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0zMFQyMzo1NDowM1rOHJmoeQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTgzMjE4NQ==", "bodyText": "Since we are already returning the DataStream, would it make sense to avoid the discarding sink and possibly let people stream the iceberg commit files instead? Like what if I wanted to also feed them into kafka?", "url": "https://github.com/apache/iceberg/pull/1348#discussion_r479832185", "createdAt": "2020-08-30T23:54:03Z", "author": {"login": "kbendick"}, "path": "flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java", "diffHunk": "@@ -0,0 +1,192 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeinfo.Types;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.DataStreamSink;\n+import org.apache.flink.streaming.api.functions.sink.DiscardingSink;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.util.DataFormatConverters;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.utils.TypeConversions;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.PropertyUtil;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT_DEFAULT;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT;\n+\n+public class FlinkSink {\n+\n+  private static final String ICEBERG_STREAM_WRITER_NAME = IcebergStreamWriter.class.getSimpleName();\n+  private static final String ICEBERG_FILES_COMMITTER_NAME = IcebergFilesCommitter.class.getSimpleName();\n+\n+  private FlinkSink() {\n+  }\n+\n+  public static Builder forRow(DataStream<Row> input) {\n+    return new Builder().forRow(input);\n+  }\n+\n+  public static Builder forRowData(DataStream<RowData> input) {\n+    return new Builder().forRowData(input);\n+  }\n+\n+  public static class Builder {\n+    private DataStream<Row> rowInput = null;\n+    private DataStream<RowData> rowDataInput = null;\n+    private TableLoader tableLoader;\n+    private Configuration hadoopConf;\n+    private Table table;\n+    private TableSchema tableSchema;\n+\n+    private Builder forRow(DataStream<Row> newRowInput) {\n+      this.rowInput = newRowInput;\n+      return this;\n+    }\n+\n+    private Builder forRowData(DataStream<RowData> newRowDataInput) {\n+      this.rowDataInput = newRowDataInput;\n+      return this;\n+    }\n+\n+    public Builder table(Table newTable) {\n+      this.table = newTable;\n+      return this;\n+    }\n+\n+    public Builder tableLoader(TableLoader newTableLoader) {\n+      this.tableLoader = newTableLoader;\n+      return this;\n+    }\n+\n+    public Builder hadoopConf(Configuration newHadoopConf) {\n+      this.hadoopConf = newHadoopConf;\n+      return this;\n+    }\n+\n+    public Builder tableSchema(TableSchema newTableSchema) {\n+      this.tableSchema = newTableSchema;\n+      return this;\n+    }\n+\n+    private DataStream<RowData> convert() {\n+      Preconditions.checkArgument(rowInput != null, \"The DataStream<Row> to convert shouldn't be null\");\n+\n+      RowType rowType;\n+      DataType[] fieldDataTypes;\n+      if (tableSchema != null) {\n+        rowType = (RowType) tableSchema.toRowDataType().getLogicalType();\n+        fieldDataTypes = tableSchema.getFieldDataTypes();\n+      } else {\n+        rowType = FlinkSchemaUtil.convert(table.schema());\n+        fieldDataTypes = TypeConversions.fromLogicalToDataType(rowType.getChildren().toArray(new LogicalType[0]));\n+      }\n+\n+      DataFormatConverters.RowConverter rowConverter = new DataFormatConverters.RowConverter(fieldDataTypes);\n+\n+      return rowInput.map(rowConverter::toInternal, RowDataTypeInfo.of(rowType));\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    public DataStreamSink<RowData> build() {\n+      Preconditions.checkArgument(rowInput != null || rowDataInput != null,\n+          \"Should initialize input DataStream first with DataStream<Row> or DataStream<RowData>\");\n+      Preconditions.checkArgument(rowInput == null || rowDataInput == null,\n+          \"Could only initialize input DataStream with either DataStream<Row> or DataStream<RowData>\");\n+      Preconditions.checkNotNull(table, \"Table shouldn't be null\");\n+      Preconditions.checkNotNull(tableLoader, \"Table loader shouldn't be null\");\n+      Preconditions.checkNotNull(hadoopConf, \"Hadoop configuration shouldn't be null\");\n+\n+      DataStream<RowData> inputStream = rowInput != null ? convert() : rowDataInput;\n+\n+      IcebergStreamWriter<RowData> streamWriter = createStreamWriter(table, tableSchema);\n+      IcebergFilesCommitter filesCommitter = new IcebergFilesCommitter(tableLoader, hadoopConf);\n+\n+      DataStream<Void> returnStream = inputStream\n+          .transform(ICEBERG_STREAM_WRITER_NAME, TypeInformation.of(DataFile.class), streamWriter)\n+          .setParallelism(inputStream.getParallelism())\n+          .transform(ICEBERG_FILES_COMMITTER_NAME, Types.VOID, filesCommitter)\n+          .setParallelism(1)\n+          .setMaxParallelism(1);\n+\n+      return returnStream.addSink(new DiscardingSink())\n+          .name(String.format(\"IcebergSink %s\", table.toString()))\n+          .setParallelism(1);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0caf973eaf4e66dbf9a185cc98dfcb3053fdd870"}, "originalPosition": 150}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "0caf973eaf4e66dbf9a185cc98dfcb3053fdd870", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/0caf973eaf4e66dbf9a185cc98dfcb3053fdd870", "committedDate": "2020-08-27T12:19:35Z", "message": "Rebase to flink-committer"}, "afterCommit": {"oid": "afb913e94ca33c3cedea0a1fe0d7dc979748414d", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/afb913e94ca33c3cedea0a1fe0d7dc979748414d", "committedDate": "2020-08-31T01:58:13Z", "message": "Flink: Support table sink."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "262a1144833126261acffd139c0d9ffcb0a74a44", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/262a1144833126261acffd139c0d9ffcb0a74a44", "committedDate": "2020-08-31T10:28:02Z", "message": "Minior fixes"}, "afterCommit": {"oid": "4712393d6d4b862cf801875d8f206399c6662ec9", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/4712393d6d4b862cf801875d8f206399c6662ec9", "committedDate": "2020-08-31T11:23:56Z", "message": "Minior fixes"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDgwNDYyMjA2", "url": "https://github.com/apache/iceberg/pull/1348#pullrequestreview-480462206", "createdAt": "2020-09-02T03:47:30Z", "commit": {"oid": "82f798700f2cb4c0b11ced1f6ec6e4d7fa5af141"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQwMzo0NzozMFrOHLUAyQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQwMzo0NzozMFrOHLUAyQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTYyNDI2NQ==", "bodyText": "I think it is better to just pass a table loader to sink, source and sink can reuse this loader creation function, just like in:\nhttps://github.com/apache/iceberg/pull/1293/files#diff-0ad7dfff9cfa32fbb760796d976fd650R61\nWhat do you think?", "url": "https://github.com/apache/iceberg/pull/1348#discussion_r481624265", "createdAt": "2020-09-02T03:47:30Z", "author": {"login": "JingsongLi"}, "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkTableFactory.java", "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.flink.table.api.ValidationException;\n+import org.apache.flink.table.catalog.ObjectIdentifier;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.catalog.exceptions.TableNotExistException;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.factories.StreamTableSinkFactory;\n+import org.apache.flink.table.sinks.StreamTableSink;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+\n+public class FlinkTableFactory implements StreamTableSinkFactory<RowData> {\n+  private final FlinkCatalog catalog;\n+\n+  public FlinkTableFactory(FlinkCatalog catalog) {\n+    this.catalog = catalog;\n+  }\n+\n+  @Override\n+  public StreamTableSink<RowData> createTableSink(Context context) {\n+    ObjectIdentifier identifier = context.getObjectIdentifier();\n+    ObjectPath objectPath = new ObjectPath(identifier.getDatabaseName(), identifier.getObjectName());\n+    TableIdentifier icebergIdentifier = catalog.toIdentifier(objectPath);\n+    try {\n+      Table table = catalog.loadIcebergTable(objectPath);\n+      return new IcebergTableSink(icebergIdentifier, table,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "82f798700f2cb4c0b11ced1f6ec6e4d7fa5af141"}, "originalPosition": 48}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDgwNDYyNjQw", "url": "https://github.com/apache/iceberg/pull/1348#pullrequestreview-480462640", "createdAt": "2020-09-02T03:49:10Z", "commit": {"oid": "82f798700f2cb4c0b11ced1f6ec6e4d7fa5af141"}, "state": "COMMENTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQwMzo0OToxMFrOHLUHDA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQwMzo1OTo1M1rOHLUwIg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTYyNTg2OA==", "bodyText": "This is a deprecated method, no one will call it, you can just return this.", "url": "https://github.com/apache/iceberg/pull/1348#discussion_r481625868", "createdAt": "2020-09-02T03:49:10Z", "author": {"login": "JingsongLi"}, "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergTableSink.java", "diffHunk": "@@ -0,0 +1,90 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.util.Arrays;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.DataStreamSink;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.api.ValidationException;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.sinks.AppendStreamTableSink;\n+import org.apache.flink.table.sinks.TableSink;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.flink.sink.FlinkSink;\n+\n+public class IcebergTableSink implements AppendStreamTableSink<RowData> {\n+  private final TableIdentifier tableIdentifier;\n+  private final Table table;\n+  private final CatalogLoader catalogLoader;\n+  private final TableSchema tableSchema;\n+  private final Configuration hadoopConf;\n+\n+  public IcebergTableSink(TableIdentifier tableIdentifier, Table table,\n+                          CatalogLoader catalogLoader, Configuration hadoopConf,\n+                          TableSchema tableSchema) {\n+    this.tableIdentifier = tableIdentifier;\n+    this.table = table;\n+    this.catalogLoader = catalogLoader;\n+    this.hadoopConf = hadoopConf;\n+    this.tableSchema = tableSchema;\n+  }\n+\n+  @Override\n+  public DataStreamSink<?> consumeDataStream(DataStream<RowData> dataStream) {\n+    return FlinkSink.forRowData(dataStream)\n+        .table(table)\n+        .tableLoader(TableLoader.fromCatalog(catalogLoader, tableIdentifier))\n+        .hadoopConf(hadoopConf)\n+        .tableSchema(tableSchema)\n+        .build();\n+  }\n+\n+  @Override\n+  public DataType getConsumedDataType() {\n+    return tableSchema.toRowDataType().bridgedTo(RowData.class);\n+  }\n+\n+  @Override\n+  public TableSchema getTableSchema() {\n+    return this.tableSchema;\n+  }\n+\n+  @Override\n+  public TableSink<RowData> configure(String[] fieldNames, TypeInformation<?>[] fieldTypes) {\n+    if (!Arrays.equals(tableSchema.getFieldNames(), fieldNames)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "82f798700f2cb4c0b11ced1f6ec6e4d7fa5af141"}, "originalPosition": 76}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTYyODA3Nw==", "bodyText": "Can we use Parameterized for batch too?", "url": "https://github.com/apache/iceberg/pull/1348#discussion_r481628077", "createdAt": "2020-09-02T03:51:29Z", "author": {"login": "JingsongLi"}, "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java", "diffHunk": "@@ -0,0 +1,215 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.concurrent.ExecutionException;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.RowTypeInfo;\n+import org.apache.flink.streaming.api.TimeCharacteristic;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.util.FiniteTestSource;\n+import org.apache.flink.table.api.EnvironmentSettings;\n+import org.apache.flink.table.api.TableEnvironment;\n+import org.apache.flink.table.api.TableResult;\n+import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;\n+import org.apache.flink.table.api.config.TableConfigOptions;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.util.DataFormatConverters;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.test.util.AbstractTestBase;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.util.Pair;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import static org.apache.flink.table.api.Expressions.$;\n+\n+@RunWith(Parameterized.class)\n+public class TestFlinkTableSink extends AbstractTestBase {\n+  private static final Configuration CONF = new Configuration();\n+  private static final DataFormatConverters.RowConverter CONVERTER = new DataFormatConverters.RowConverter(\n+      SimpleDataUtil.FLINK_SCHEMA.getFieldDataTypes());\n+\n+  private static final String TABLE_NAME = \"flink_table\";\n+\n+  @Rule\n+  public TemporaryFolder tempFolder = new TemporaryFolder();\n+  private String tablePath;\n+  private String warehouse;\n+  private Map<String, String> properties;\n+  private Catalog catalog;\n+  private StreamExecutionEnvironment env;\n+  private StreamTableEnvironment tEnv;\n+\n+  private final FileFormat format;\n+  private final int parallelism;\n+\n+  @Parameterized.Parameters(name = \"{index}: format={0}, parallelism={2}\")\n+  public static Iterable<Object[]> data() {\n+    return Arrays.asList(\n+        new Object[] {\"avro\", 1},\n+        new Object[] {\"avro\", 2},\n+        new Object[] {\"orc\", 1},\n+        new Object[] {\"orc\", 2},\n+        new Object[] {\"parquet\", 1},\n+        new Object[] {\"parquet\", 2}\n+    );\n+  }\n+\n+  public TestFlinkTableSink(String format, int parallelism) {\n+    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));\n+    this.parallelism = parallelism;\n+  }\n+\n+  @Before\n+  public void before() throws IOException {\n+    File folder = tempFolder.newFolder();\n+    warehouse = folder.getAbsolutePath();\n+\n+    tablePath = warehouse.concat(\"/default/\").concat(TABLE_NAME);\n+    Assert.assertTrue(\"Should create the table path correctly.\", new File(tablePath).mkdirs());\n+\n+    properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format.name());\n+    catalog = new HadoopCatalog(CONF, warehouse);\n+\n+    env = StreamExecutionEnvironment.getExecutionEnvironment();\n+    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n+    env.enableCheckpointing(400);\n+    env.setParallelism(parallelism);\n+\n+    EnvironmentSettings settings = EnvironmentSettings\n+        .newInstance()\n+        .useBlinkPlanner()\n+        .inStreamingMode()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "82f798700f2cb4c0b11ced1f6ec6e4d7fa5af141"}, "originalPosition": 122}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTYyODg2NA==", "bodyText": "Looks like there is no dynamic table options. (Table hints)", "url": "https://github.com/apache/iceberg/pull/1348#discussion_r481628864", "createdAt": "2020-09-02T03:52:11Z", "author": {"login": "JingsongLi"}, "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java", "diffHunk": "@@ -0,0 +1,215 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.concurrent.ExecutionException;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.RowTypeInfo;\n+import org.apache.flink.streaming.api.TimeCharacteristic;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.util.FiniteTestSource;\n+import org.apache.flink.table.api.EnvironmentSettings;\n+import org.apache.flink.table.api.TableEnvironment;\n+import org.apache.flink.table.api.TableResult;\n+import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;\n+import org.apache.flink.table.api.config.TableConfigOptions;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.util.DataFormatConverters;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.test.util.AbstractTestBase;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.util.Pair;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import static org.apache.flink.table.api.Expressions.$;\n+\n+@RunWith(Parameterized.class)\n+public class TestFlinkTableSink extends AbstractTestBase {\n+  private static final Configuration CONF = new Configuration();\n+  private static final DataFormatConverters.RowConverter CONVERTER = new DataFormatConverters.RowConverter(\n+      SimpleDataUtil.FLINK_SCHEMA.getFieldDataTypes());\n+\n+  private static final String TABLE_NAME = \"flink_table\";\n+\n+  @Rule\n+  public TemporaryFolder tempFolder = new TemporaryFolder();\n+  private String tablePath;\n+  private String warehouse;\n+  private Map<String, String> properties;\n+  private Catalog catalog;\n+  private StreamExecutionEnvironment env;\n+  private StreamTableEnvironment tEnv;\n+\n+  private final FileFormat format;\n+  private final int parallelism;\n+\n+  @Parameterized.Parameters(name = \"{index}: format={0}, parallelism={2}\")\n+  public static Iterable<Object[]> data() {\n+    return Arrays.asList(\n+        new Object[] {\"avro\", 1},\n+        new Object[] {\"avro\", 2},\n+        new Object[] {\"orc\", 1},\n+        new Object[] {\"orc\", 2},\n+        new Object[] {\"parquet\", 1},\n+        new Object[] {\"parquet\", 2}\n+    );\n+  }\n+\n+  public TestFlinkTableSink(String format, int parallelism) {\n+    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));\n+    this.parallelism = parallelism;\n+  }\n+\n+  @Before\n+  public void before() throws IOException {\n+    File folder = tempFolder.newFolder();\n+    warehouse = folder.getAbsolutePath();\n+\n+    tablePath = warehouse.concat(\"/default/\").concat(TABLE_NAME);\n+    Assert.assertTrue(\"Should create the table path correctly.\", new File(tablePath).mkdirs());\n+\n+    properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format.name());\n+    catalog = new HadoopCatalog(CONF, warehouse);\n+\n+    env = StreamExecutionEnvironment.getExecutionEnvironment();\n+    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n+    env.enableCheckpointing(400);\n+    env.setParallelism(parallelism);\n+\n+    EnvironmentSettings settings = EnvironmentSettings\n+        .newInstance()\n+        .useBlinkPlanner()\n+        .inStreamingMode()\n+        .build();\n+    tEnv = StreamTableEnvironment.create(env, settings);\n+    tEnv.executeSql(String.format(\"create catalog iceberg_catalog with (\" +\n+        \"'type'='iceberg', 'catalog-type'='hadoop', 'warehouse'='%s')\", warehouse));\n+    tEnv.executeSql(\"use catalog iceberg_catalog\");\n+    tEnv.getConfig().getConfiguration().set(TableConfigOptions.TABLE_DYNAMIC_TABLE_OPTIONS_ENABLED, true);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "82f798700f2cb4c0b11ced1f6ec6e4d7fa5af141"}, "originalPosition": 128}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTYyOTc3NA==", "bodyText": "Can we use TableEnvironment.fromValues?", "url": "https://github.com/apache/iceberg/pull/1348#discussion_r481629774", "createdAt": "2020-09-02T03:53:07Z", "author": {"login": "JingsongLi"}, "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java", "diffHunk": "@@ -0,0 +1,215 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.concurrent.ExecutionException;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.RowTypeInfo;\n+import org.apache.flink.streaming.api.TimeCharacteristic;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.util.FiniteTestSource;\n+import org.apache.flink.table.api.EnvironmentSettings;\n+import org.apache.flink.table.api.TableEnvironment;\n+import org.apache.flink.table.api.TableResult;\n+import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;\n+import org.apache.flink.table.api.config.TableConfigOptions;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.util.DataFormatConverters;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.test.util.AbstractTestBase;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.util.Pair;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import static org.apache.flink.table.api.Expressions.$;\n+\n+@RunWith(Parameterized.class)\n+public class TestFlinkTableSink extends AbstractTestBase {\n+  private static final Configuration CONF = new Configuration();\n+  private static final DataFormatConverters.RowConverter CONVERTER = new DataFormatConverters.RowConverter(\n+      SimpleDataUtil.FLINK_SCHEMA.getFieldDataTypes());\n+\n+  private static final String TABLE_NAME = \"flink_table\";\n+\n+  @Rule\n+  public TemporaryFolder tempFolder = new TemporaryFolder();\n+  private String tablePath;\n+  private String warehouse;\n+  private Map<String, String> properties;\n+  private Catalog catalog;\n+  private StreamExecutionEnvironment env;\n+  private StreamTableEnvironment tEnv;\n+\n+  private final FileFormat format;\n+  private final int parallelism;\n+\n+  @Parameterized.Parameters(name = \"{index}: format={0}, parallelism={2}\")\n+  public static Iterable<Object[]> data() {\n+    return Arrays.asList(\n+        new Object[] {\"avro\", 1},\n+        new Object[] {\"avro\", 2},\n+        new Object[] {\"orc\", 1},\n+        new Object[] {\"orc\", 2},\n+        new Object[] {\"parquet\", 1},\n+        new Object[] {\"parquet\", 2}\n+    );\n+  }\n+\n+  public TestFlinkTableSink(String format, int parallelism) {\n+    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));\n+    this.parallelism = parallelism;\n+  }\n+\n+  @Before\n+  public void before() throws IOException {\n+    File folder = tempFolder.newFolder();\n+    warehouse = folder.getAbsolutePath();\n+\n+    tablePath = warehouse.concat(\"/default/\").concat(TABLE_NAME);\n+    Assert.assertTrue(\"Should create the table path correctly.\", new File(tablePath).mkdirs());\n+\n+    properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format.name());\n+    catalog = new HadoopCatalog(CONF, warehouse);\n+\n+    env = StreamExecutionEnvironment.getExecutionEnvironment();\n+    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n+    env.enableCheckpointing(400);\n+    env.setParallelism(parallelism);\n+\n+    EnvironmentSettings settings = EnvironmentSettings\n+        .newInstance()\n+        .useBlinkPlanner()\n+        .inStreamingMode()\n+        .build();\n+    tEnv = StreamTableEnvironment.create(env, settings);\n+    tEnv.executeSql(String.format(\"create catalog iceberg_catalog with (\" +\n+        \"'type'='iceberg', 'catalog-type'='hadoop', 'warehouse'='%s')\", warehouse));\n+    tEnv.executeSql(\"use catalog iceberg_catalog\");\n+    tEnv.getConfig().getConfiguration().set(TableConfigOptions.TABLE_DYNAMIC_TABLE_OPTIONS_ENABLED, true);\n+\n+    catalog.createTable(TableIdentifier.parse(\"default.\" + TABLE_NAME),\n+        SimpleDataUtil.SCHEMA,\n+        PartitionSpec.unpartitioned(),\n+        properties);\n+  }\n+\n+  private DataStream<RowData> generateInputStream(List<Row> rows) {\n+    TypeInformation<Row> typeInformation = new RowTypeInfo(SimpleDataUtil.FLINK_SCHEMA.getFieldTypes());\n+    return env.addSource(new FiniteTestSource<>(rows), typeInformation)\n+        .map(CONVERTER::toInternal, RowDataTypeInfo.of(SimpleDataUtil.ROW_TYPE));\n+  }\n+\n+  private Pair<List<Row>, List<Record>> generateData() {\n+    String[] worlds = new String[] {\"hello\", \"world\", \"foo\", \"bar\", \"apache\", \"foundation\"};\n+    List<Row> rows = Lists.newArrayList();\n+    List<Record> expected = Lists.newArrayList();\n+    for (int i = 0; i < worlds.length; i++) {\n+      rows.add(Row.of(i + 1, worlds[i]));\n+      Record record = SimpleDataUtil.createRecord(i + 1, worlds[i]);\n+      expected.add(record);\n+      expected.add(record);\n+    }\n+    return Pair.of(rows, expected);\n+  }\n+\n+  @Test\n+  public void testStreamSQL() throws Exception {\n+    Pair<List<Row>, List<Record>> data = generateData();\n+    List<Row> rows = data.first();\n+    List<Record> expected = data.second();\n+    DataStream<RowData> stream = generateInputStream(rows);\n+\n+    // Register the rows into a temporary table named 'sourceTable'.\n+    tEnv.createTemporaryView(\"sourceTable\", tEnv.fromDataStream(stream, $(\"id\"), $(\"data\")));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "82f798700f2cb4c0b11ced1f6ec6e4d7fa5af141"}, "originalPosition": 163}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTYzMzIzOA==", "bodyText": "You can just add a method like:\n  def execInsertSqlAndWaitResult(tEnv: TableEnvironment, insert: String): JobExecutionResult = {\n    tEnv.executeSql(insert).getJobClient.get\n      .getJobExecutionResult(Thread.currentThread.getContextClassLoader)\n      .get\n  }", "url": "https://github.com/apache/iceberg/pull/1348#discussion_r481633238", "createdAt": "2020-09-02T03:56:40Z", "author": {"login": "JingsongLi"}, "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java", "diffHunk": "@@ -0,0 +1,215 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.concurrent.ExecutionException;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.RowTypeInfo;\n+import org.apache.flink.streaming.api.TimeCharacteristic;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.util.FiniteTestSource;\n+import org.apache.flink.table.api.EnvironmentSettings;\n+import org.apache.flink.table.api.TableEnvironment;\n+import org.apache.flink.table.api.TableResult;\n+import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;\n+import org.apache.flink.table.api.config.TableConfigOptions;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.util.DataFormatConverters;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.test.util.AbstractTestBase;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.util.Pair;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import static org.apache.flink.table.api.Expressions.$;\n+\n+@RunWith(Parameterized.class)\n+public class TestFlinkTableSink extends AbstractTestBase {\n+  private static final Configuration CONF = new Configuration();\n+  private static final DataFormatConverters.RowConverter CONVERTER = new DataFormatConverters.RowConverter(\n+      SimpleDataUtil.FLINK_SCHEMA.getFieldDataTypes());\n+\n+  private static final String TABLE_NAME = \"flink_table\";\n+\n+  @Rule\n+  public TemporaryFolder tempFolder = new TemporaryFolder();\n+  private String tablePath;\n+  private String warehouse;\n+  private Map<String, String> properties;\n+  private Catalog catalog;\n+  private StreamExecutionEnvironment env;\n+  private StreamTableEnvironment tEnv;\n+\n+  private final FileFormat format;\n+  private final int parallelism;\n+\n+  @Parameterized.Parameters(name = \"{index}: format={0}, parallelism={2}\")\n+  public static Iterable<Object[]> data() {\n+    return Arrays.asList(\n+        new Object[] {\"avro\", 1},\n+        new Object[] {\"avro\", 2},\n+        new Object[] {\"orc\", 1},\n+        new Object[] {\"orc\", 2},\n+        new Object[] {\"parquet\", 1},\n+        new Object[] {\"parquet\", 2}\n+    );\n+  }\n+\n+  public TestFlinkTableSink(String format, int parallelism) {\n+    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));\n+    this.parallelism = parallelism;\n+  }\n+\n+  @Before\n+  public void before() throws IOException {\n+    File folder = tempFolder.newFolder();\n+    warehouse = folder.getAbsolutePath();\n+\n+    tablePath = warehouse.concat(\"/default/\").concat(TABLE_NAME);\n+    Assert.assertTrue(\"Should create the table path correctly.\", new File(tablePath).mkdirs());\n+\n+    properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format.name());\n+    catalog = new HadoopCatalog(CONF, warehouse);\n+\n+    env = StreamExecutionEnvironment.getExecutionEnvironment();\n+    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n+    env.enableCheckpointing(400);\n+    env.setParallelism(parallelism);\n+\n+    EnvironmentSettings settings = EnvironmentSettings\n+        .newInstance()\n+        .useBlinkPlanner()\n+        .inStreamingMode()\n+        .build();\n+    tEnv = StreamTableEnvironment.create(env, settings);\n+    tEnv.executeSql(String.format(\"create catalog iceberg_catalog with (\" +\n+        \"'type'='iceberg', 'catalog-type'='hadoop', 'warehouse'='%s')\", warehouse));\n+    tEnv.executeSql(\"use catalog iceberg_catalog\");\n+    tEnv.getConfig().getConfiguration().set(TableConfigOptions.TABLE_DYNAMIC_TABLE_OPTIONS_ENABLED, true);\n+\n+    catalog.createTable(TableIdentifier.parse(\"default.\" + TABLE_NAME),\n+        SimpleDataUtil.SCHEMA,\n+        PartitionSpec.unpartitioned(),\n+        properties);\n+  }\n+\n+  private DataStream<RowData> generateInputStream(List<Row> rows) {\n+    TypeInformation<Row> typeInformation = new RowTypeInfo(SimpleDataUtil.FLINK_SCHEMA.getFieldTypes());\n+    return env.addSource(new FiniteTestSource<>(rows), typeInformation)\n+        .map(CONVERTER::toInternal, RowDataTypeInfo.of(SimpleDataUtil.ROW_TYPE));\n+  }\n+\n+  private Pair<List<Row>, List<Record>> generateData() {\n+    String[] worlds = new String[] {\"hello\", \"world\", \"foo\", \"bar\", \"apache\", \"foundation\"};\n+    List<Row> rows = Lists.newArrayList();\n+    List<Record> expected = Lists.newArrayList();\n+    for (int i = 0; i < worlds.length; i++) {\n+      rows.add(Row.of(i + 1, worlds[i]));\n+      Record record = SimpleDataUtil.createRecord(i + 1, worlds[i]);\n+      expected.add(record);\n+      expected.add(record);\n+    }\n+    return Pair.of(rows, expected);\n+  }\n+\n+  @Test\n+  public void testStreamSQL() throws Exception {\n+    Pair<List<Row>, List<Record>> data = generateData();\n+    List<Row> rows = data.first();\n+    List<Record> expected = data.second();\n+    DataStream<RowData> stream = generateInputStream(rows);\n+\n+    // Register the rows into a temporary table named 'sourceTable'.\n+    tEnv.createTemporaryView(\"sourceTable\", tEnv.fromDataStream(stream, $(\"id\"), $(\"data\")));\n+\n+    // Redirect the records from source table to destination table.\n+    String insertSQL = String.format(\"INSERT INTO %s SELECT id,data from sourceTable\", TABLE_NAME);\n+    TableResult result = tEnv.executeSql(insertSQL);\n+    waitComplete(result);\n+\n+    // Assert the table records as expected.\n+    SimpleDataUtil.assertTableRecords(tablePath, expected);\n+  }\n+\n+  @Test\n+  public void testBatchSQL() throws Exception {\n+    EnvironmentSettings settings = EnvironmentSettings\n+        .newInstance()\n+        .inBatchMode()\n+        .useBlinkPlanner()\n+        .build();\n+    TableEnvironment batchEnv = TableEnvironment.create(settings);\n+    batchEnv.executeSql(String.format(\"create catalog batch_catalog with (\" +\n+        \"'type'='iceberg', 'catalog-type'='hadoop', 'warehouse'='%s')\", warehouse));\n+    batchEnv.executeSql(\"use catalog batch_catalog\");\n+    batchEnv.getConfig().getConfiguration().set(TableConfigOptions.TABLE_DYNAMIC_TABLE_OPTIONS_ENABLED, true);\n+\n+    // Create source table.\n+    catalog.createTable(TableIdentifier.parse(\"default.sourceTable\"),\n+        SimpleDataUtil.SCHEMA,\n+        PartitionSpec.unpartitioned(),\n+        properties);\n+\n+    TableResult result;\n+    String[] words = new String[] {\"hello\", \"world\", \"apache\"};\n+    List<Record> expected = Lists.newArrayList();\n+    for (int i = 0; i < words.length; i++) {\n+      expected.add(SimpleDataUtil.createRecord(i, words[i]));\n+      result = batchEnv.executeSql(String.format(\"INSERT INTO sourceTable SELECT %d, '%s'\", i, words[i]));\n+      waitComplete(result);\n+    }\n+\n+    // Assert the table records as expected.\n+    SimpleDataUtil.assertTableRecords(warehouse.concat(\"/default/sourceTable\"), expected);\n+  }\n+\n+  private static void waitComplete(TableResult result) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "82f798700f2cb4c0b11ced1f6ec6e4d7fa5af141"}, "originalPosition": 206}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTYzNjM4Ng==", "bodyText": "We can add TODO for these interfaces:\nImplement OverwritableTableSink, so in the Flink SQL, user can write these SQLs:\nINSERT OVERWRITE t ...\nImplement PartitionableTableSink, user can write:\nINSERT OVERWRITE/INTO t PARTITION(...)", "url": "https://github.com/apache/iceberg/pull/1348#discussion_r481636386", "createdAt": "2020-09-02T03:59:53Z", "author": {"login": "JingsongLi"}, "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergTableSink.java", "diffHunk": "@@ -0,0 +1,90 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.util.Arrays;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.DataStreamSink;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.api.ValidationException;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.sinks.AppendStreamTableSink;\n+import org.apache.flink.table.sinks.TableSink;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.flink.sink.FlinkSink;\n+\n+public class IcebergTableSink implements AppendStreamTableSink<RowData> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "82f798700f2cb4c0b11ced1f6ec6e4d7fa5af141"}, "originalPosition": 37}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "65518921d866782d8d364da2ac72524d28a57404", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/65518921d866782d8d364da2ac72524d28a57404", "committedDate": "2020-09-02T10:08:51Z", "message": "Add TODO to implement PartitionedTableSink"}, "afterCommit": {"oid": "ea63017d5efeb3964192c67903abb502ed53c1d2", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/ea63017d5efeb3964192c67903abb502ed53c1d2", "committedDate": "2020-09-02T11:19:52Z", "message": "Add TODO to implement PartitionedTableSink"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDgwNzI5MTk5", "url": "https://github.com/apache/iceberg/pull/1348#pullrequestreview-480729199", "createdAt": "2020-09-02T11:32:15Z", "commit": {"oid": "ea63017d5efeb3964192c67903abb502ed53c1d2"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQxMTozMjoxNVrOHLq5zg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQxMTozMjoxNVrOHLq5zg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTk5OTMxMA==", "bodyText": "Think about this unit test again, we'd better to extend the FlinkCatalogTestBase  so that we could cover both hive and hadoop catalog cases.", "url": "https://github.com/apache/iceberg/pull/1348#discussion_r481999310", "createdAt": "2020-09-02T11:32:15Z", "author": {"login": "openinx"}, "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java", "diffHunk": "@@ -0,0 +1,181 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.concurrent.ExecutionException;\n+import org.apache.flink.streaming.api.TimeCharacteristic;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.table.api.EnvironmentSettings;\n+import org.apache.flink.table.api.Expressions;\n+import org.apache.flink.table.api.Table;\n+import org.apache.flink.table.api.TableEnvironment;\n+import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.test.util.AbstractTestBase;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+@RunWith(Parameterized.class)\n+public class TestFlinkTableSink extends AbstractTestBase {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ea63017d5efeb3964192c67903abb502ed53c1d2"}, "originalPosition": 58}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDgwODM3NjAz", "url": "https://github.com/apache/iceberg/pull/1348#pullrequestreview-480837603", "createdAt": "2020-09-02T13:48:19Z", "commit": {"oid": "d6e7c259184f19d6b611ebdfc19910335aa8bf3e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQxMzo0ODoxOVrOHLv_gA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQxMzo0ODoxOVrOHLv_gA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjA4MjY4OA==", "bodyText": "We could use flink DDL to create table here if #1393 get merged.", "url": "https://github.com/apache/iceberg/pull/1348#discussion_r482082688", "createdAt": "2020-09-02T13:48:19Z", "author": {"login": "openinx"}, "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.flink.streaming.api.TimeCharacteristic;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.table.api.EnvironmentSettings;\n+import org.apache.flink.table.api.Expressions;\n+import org.apache.flink.table.api.Table;\n+import org.apache.flink.table.api.TableEnvironment;\n+import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.junit.After;\n+import org.junit.Assume;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+@RunWith(Parameterized.class)\n+public class TestFlinkTableSink extends FlinkCatalogTestBase {\n+  private static final String TABLE_NAME = \"test_table\";\n+  private TableEnvironment tEnv;\n+  private org.apache.iceberg.Table icebergTable;\n+\n+  private final FileFormat format;\n+  private final boolean isStreamingJob;\n+\n+  @Parameterized.Parameters(name = \"{index}: format={0}, isStreaming={1}, catalogName={2}, baseNamespace={3}\")\n+  public static Iterable<Object[]> parameters() {\n+    List<Object[]> parameters = Lists.newArrayList();\n+    for (FileFormat format : new FileFormat[] {FileFormat.ORC, FileFormat.AVRO, FileFormat.PARQUET}) {\n+      for (Boolean isStreaming : new Boolean[] {true, false}) {\n+        for (Object[] catalogParams : FlinkCatalogTestBase.parameters()) {\n+          String catalogName = (String) catalogParams[0];\n+          String[] baseNamespace = (String[]) catalogParams[1];\n+          parameters.add(new Object[] {format, isStreaming, catalogName, baseNamespace});\n+        }\n+      }\n+    }\n+    return parameters;\n+  }\n+\n+  public TestFlinkTableSink(FileFormat format, Boolean isStreamingJob, String catalogName, String[] baseNamespace) {\n+    super(catalogName, baseNamespace);\n+    this.format = format;\n+    this.isStreamingJob = isStreamingJob;\n+  }\n+\n+  @Override\n+  protected TableEnvironment getTableEnv() {\n+    if (tEnv == null) {\n+      synchronized (this) {\n+        EnvironmentSettings.Builder settingsBuilder = EnvironmentSettings\n+            .newInstance()\n+            .useBlinkPlanner();\n+        if (isStreamingJob) {\n+          settingsBuilder.inStreamingMode();\n+          StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+          env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n+          env.enableCheckpointing(400);\n+          tEnv = StreamTableEnvironment.create(env, settingsBuilder.build());\n+        } else {\n+          settingsBuilder.inBatchMode();\n+          tEnv = TableEnvironment.create(settingsBuilder.build());\n+        }\n+      }\n+    }\n+    return tEnv;\n+  }\n+\n+  @Before\n+  public void before() {\n+    sql(\"CREATE DATABASE %s\", flinkDatabase);\n+    sql(\"USE CATALOG %s\", catalogName);\n+    sql(\"USE %s\", DATABASE);\n+\n+    Map<String, String> properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format.name());\n+    this.icebergTable = validationCatalog", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d6e7c259184f19d6b611ebdfc19910335aa8bf3e"}, "originalPosition": 103}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDgxMzc1OTA5", "url": "https://github.com/apache/iceberg/pull/1348#pullrequestreview-481375909", "createdAt": "2020-09-02T23:15:35Z", "commit": {"oid": "2eec2057a685beab08d98a02efa46ed0eb86dfb5"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQyMzoxNTozNVrOHMO8wQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQyMzoxNTozNVrOHMO8wQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjU4OTg4OQ==", "bodyText": "Minor: it would be nice to have more context here. Maybe the table loader should define a toString that could be used in the error message here.", "url": "https://github.com/apache/iceberg/pull/1348#discussion_r482589889", "createdAt": "2020-09-02T23:15:35Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java", "diffHunk": "@@ -155,16 +158,29 @@ public Builder tableSchema(TableSchema newTableSchema) {\n       return this;\n     }\n \n+    public Builder overwrite(boolean newOverwrite) {\n+      this.overwrite = newOverwrite;\n+      return this;\n+    }\n+\n     @SuppressWarnings(\"unchecked\")\n     public DataStreamSink<RowData> build() {\n       Preconditions.checkArgument(rowDataInput != null,\n           \"Please use forRowData() to initialize the input DataStream.\");\n-      Preconditions.checkNotNull(table, \"Table shouldn't be null\");\n       Preconditions.checkNotNull(tableLoader, \"Table loader shouldn't be null\");\n       Preconditions.checkNotNull(hadoopConf, \"Hadoop configuration shouldn't be null\");\n \n+      if (table == null) {\n+        tableLoader.open(hadoopConf);\n+        try (TableLoader loader = tableLoader) {\n+          this.table = loader.loadTable();\n+        } catch (IOException e) {\n+          throw new UncheckedIOException(\"Failed to load iceberg table.\", e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2eec2057a685beab08d98a02efa46ed0eb86dfb5"}, "originalPosition": 39}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDgxMzc3MDQ5", "url": "https://github.com/apache/iceberg/pull/1348#pullrequestreview-481377049", "createdAt": "2020-09-02T23:18:48Z", "commit": {"oid": "2eec2057a685beab08d98a02efa46ed0eb86dfb5"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQyMzoxODo0OFrOHMPFCg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQyMzoxODo0OFrOHMPFCg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjU5MjAxMA==", "bodyText": "I just want to note that we don't encourage the use of ReplacePartitions because the data it deletes is implicit. It is better to specify what data should be overwritten, like in the new API for Spark:\ndf.writeTo(\"iceberg.db.table\").overwrite($\"date\" === \"2020-09-01\")\nIf Flink's semantics are to replace partitions for overwrite, then it should be okay. But I highly recommend being more explicit about data replacement.", "url": "https://github.com/apache/iceberg/pull/1348#discussion_r482592010", "createdAt": "2020-09-02T23:18:48Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java", "diffHunk": "@@ -164,16 +168,51 @@ private void commitUpToCheckpoint(long checkpointId) {\n       pendingDataFiles.addAll(dataFiles);\n     }\n \n-    AppendFiles appendFiles = table.newAppend();\n-    pendingDataFiles.forEach(appendFiles::appendFile);\n-    appendFiles.set(MAX_COMMITTED_CHECKPOINT_ID, Long.toString(checkpointId));\n-    appendFiles.set(FLINK_JOB_ID, flinkJobId);\n-    appendFiles.commit();\n+    if (replacePartitions) {\n+      replacePartitions(pendingDataFiles, checkpointId);\n+    } else {\n+      append(pendingDataFiles, checkpointId);\n+    }\n \n     // Clear the committed data files from dataFilesPerCheckpoint.\n     pendingFileMap.clear();\n   }\n \n+  private void replacePartitions(List<DataFile> dataFiles, long checkpointId) {\n+    ReplacePartitions dynamicOverwrite = table.newReplacePartitions();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2eec2057a685beab08d98a02efa46ed0eb86dfb5"}, "originalPosition": 50}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDgxMzc3NTMy", "url": "https://github.com/apache/iceberg/pull/1348#pullrequestreview-481377532", "createdAt": "2020-09-02T23:20:08Z", "commit": {"oid": "2eec2057a685beab08d98a02efa46ed0eb86dfb5"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQyMzoyMDowOVrOHMPI0A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQyMzoyMDowOVrOHMPI0A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjU5Mjk3Ng==", "bodyText": "Can this be done automatically when a write completes, or is this a completely separate copy of the table?", "url": "https://github.com/apache/iceberg/pull/1348#discussion_r482592976", "createdAt": "2020-09-02T23:20:09Z", "author": {"login": "rdblue"}, "path": "flink/src/test/java/org/apache/iceberg/flink/SimpleDataUtil.java", "diffHunk": "@@ -126,12 +126,16 @@ public static void assertTableRows(String tablePath, List<RowData> expected) thr\n     assertTableRecords(tablePath, expectedRecords);\n   }\n \n-  public static void assertTableRecords(String tablePath, List<Record> expected) throws IOException {\n-    Preconditions.checkArgument(expected != null, \"expected records shouldn't be null\");\n-    Table newTable = new HadoopTables().load(tablePath);\n-    try (CloseableIterable<Record> iterable = IcebergGenerics.read(newTable).build()) {\n+  public static void assertTableRecords(Table table, List<Record> expected) throws IOException {\n+    table.refresh();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2eec2057a685beab08d98a02efa46ed0eb86dfb5"}, "originalPosition": 9}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDgxMzc4MTMy", "url": "https://github.com/apache/iceberg/pull/1348#pullrequestreview-481378132", "createdAt": "2020-09-02T23:21:51Z", "commit": {"oid": "2eec2057a685beab08d98a02efa46ed0eb86dfb5"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQyMzoyMTo1MVrOHMPNkA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQyMzoyMTo1MVrOHMPNkA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjU5NDE5Mg==", "bodyText": "It would be good to also have a partitioned test.", "url": "https://github.com/apache/iceberg/pull/1348#discussion_r482594192", "createdAt": "2020-09-02T23:21:51Z", "author": {"login": "rdblue"}, "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.flink.streaming.api.TimeCharacteristic;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.table.api.EnvironmentSettings;\n+import org.apache.flink.table.api.Expressions;\n+import org.apache.flink.table.api.Table;\n+import org.apache.flink.table.api.TableEnvironment;\n+import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.junit.After;\n+import org.junit.Assume;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+@RunWith(Parameterized.class)\n+public class TestFlinkTableSink extends FlinkCatalogTestBase {\n+  private static final String TABLE_NAME = \"test_table\";\n+  private TableEnvironment tEnv;\n+  private org.apache.iceberg.Table icebergTable;\n+\n+  private final FileFormat format;\n+  private final boolean isStreamingJob;\n+\n+  @Parameterized.Parameters(name = \"{index}: format={0}, isStreaming={1}, catalogName={2}, baseNamespace={3}\")\n+  public static Iterable<Object[]> parameters() {\n+    List<Object[]> parameters = Lists.newArrayList();\n+    for (FileFormat format : new FileFormat[] {FileFormat.ORC, FileFormat.AVRO, FileFormat.PARQUET}) {\n+      for (Boolean isStreaming : new Boolean[] {true, false}) {\n+        for (Object[] catalogParams : FlinkCatalogTestBase.parameters()) {\n+          String catalogName = (String) catalogParams[0];\n+          String[] baseNamespace = (String[]) catalogParams[1];\n+          parameters.add(new Object[] {format, isStreaming, catalogName, baseNamespace});\n+        }\n+      }\n+    }\n+    return parameters;\n+  }\n+\n+  public TestFlinkTableSink(FileFormat format, Boolean isStreamingJob, String catalogName, String[] baseNamespace) {\n+    super(catalogName, baseNamespace);\n+    this.format = format;\n+    this.isStreamingJob = isStreamingJob;\n+  }\n+\n+  @Override\n+  protected TableEnvironment getTableEnv() {\n+    if (tEnv == null) {\n+      synchronized (this) {\n+        EnvironmentSettings.Builder settingsBuilder = EnvironmentSettings\n+            .newInstance()\n+            .useBlinkPlanner();\n+        if (isStreamingJob) {\n+          settingsBuilder.inStreamingMode();\n+          StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+          env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n+          env.enableCheckpointing(400);\n+          tEnv = StreamTableEnvironment.create(env, settingsBuilder.build());\n+        } else {\n+          settingsBuilder.inBatchMode();\n+          tEnv = TableEnvironment.create(settingsBuilder.build());\n+        }\n+      }\n+    }\n+    return tEnv;\n+  }\n+\n+  @Before\n+  public void before() {\n+    sql(\"CREATE DATABASE %s\", flinkDatabase);\n+    sql(\"USE CATALOG %s\", catalogName);\n+    sql(\"USE %s\", DATABASE);\n+\n+    Map<String, String> properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format.name());\n+    this.icebergTable = validationCatalog\n+        .createTable(TableIdentifier.of(icebergNamespace, TABLE_NAME),\n+            SimpleDataUtil.SCHEMA,\n+            PartitionSpec.unpartitioned(),\n+            properties);\n+  }\n+\n+  @After\n+  public void clean() {\n+    sql(\"DROP TABLE IF EXISTS %s.%s\", flinkDatabase, TABLE_NAME);\n+    sql(\"DROP DATABASE IF EXISTS %s\", flinkDatabase);\n+  }\n+\n+  @Test\n+  public void testStreamSQL() throws Exception {\n+    // Register the rows into a temporary table.\n+    Table sourceTable = getTableEnv().fromValues(SimpleDataUtil.FLINK_SCHEMA.toRowDataType(),\n+        Expressions.row(1, \"hello\"),\n+        Expressions.row(2, \"world\"),\n+        Expressions.row(3, (String) null),\n+        Expressions.row(null, \"bar\")\n+    );\n+    getTableEnv().createTemporaryView(\"sourceTable\", sourceTable);\n+\n+    // Redirect the records from source table to destination table.\n+    sql(\"INSERT INTO %s SELECT id,data from sourceTable\", TABLE_NAME);\n+\n+    // Assert the table records as expected.\n+    SimpleDataUtil.assertTableRecords(icebergTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"hello\"),\n+        SimpleDataUtil.createRecord(2, \"world\"),\n+        SimpleDataUtil.createRecord(3, null),\n+        SimpleDataUtil.createRecord(null, \"bar\")\n+    ));\n+  }\n+\n+  @Test\n+  public void testOverwriteTable() throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2eec2057a685beab08d98a02efa46ed0eb86dfb5"}, "originalPosition": 140}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7a328cf866a622a867201be07e01826b04e8960e", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/7a328cf866a622a867201be07e01826b04e8960e", "committedDate": "2020-09-03T01:48:32Z", "message": "Flink: Support table sink."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "23ddefb60ef49b5fbdc0917ed7fe32cd95c1224d", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/23ddefb60ef49b5fbdc0917ed7fe32cd95c1224d", "committedDate": "2020-09-03T01:49:30Z", "message": "Minior fixes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f733b3e6c578632428d222b8d7a52621230e7459", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/f733b3e6c578632428d222b8d7a52621230e7459", "committedDate": "2020-09-03T01:49:30Z", "message": "Remove the public modifier"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "17af5a15b1ae44bbd90005a8bbe86f8e23edeb4f", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/17af5a15b1ae44bbd90005a8bbe86f8e23edeb4f", "committedDate": "2020-09-03T01:49:30Z", "message": "Pass the table loader rather than CatalogLoader to IcebergTabelSink"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "aa3326689f99238dd007603931db5a4e7262d151", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/aa3326689f99238dd007603931db5a4e7262d151", "committedDate": "2020-09-03T01:49:30Z", "message": "Remove the implementation for IcebergTableSink#configure because it is deprecated."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f9760c31094f8b1e7f99c4d9220b6116748bb355", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/f9760c31094f8b1e7f99c4d9220b6116748bb355", "committedDate": "2020-09-03T01:49:30Z", "message": "Refactor the unit tests."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5393428404f5ab8724381e6f85ad458cb70c9504", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/5393428404f5ab8724381e6f85ad458cb70c9504", "committedDate": "2020-09-03T01:49:30Z", "message": "Create an executeSQLAndWaitResult to execute sql"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9df390bc5d8fb7a344d335eb36a57df7abbaadc2", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/9df390bc5d8fb7a344d335eb36a57df7abbaadc2", "committedDate": "2020-09-03T01:49:30Z", "message": "Implement the OverwritableTableSink"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9215ced81fc5f0ba34bf19cc39b64201d740b6fe", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/9215ced81fc5f0ba34bf19cc39b64201d740b6fe", "committedDate": "2020-09-03T01:49:30Z", "message": "Add TODO to implement PartitionedTableSink"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "53a16d957035e970a6416ca6712972625a258a17", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/53a16d957035e970a6416ca6712972625a258a17", "committedDate": "2020-09-03T01:51:25Z", "message": "Make the case extend the FlinkCatalogTestBase"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "01bc54d013dfec407fce9289cfebed2055ee6244", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/01bc54d013dfec407fce9289cfebed2055ee6244", "committedDate": "2020-09-03T01:51:25Z", "message": "Fix the broken unit tests."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "afb94a8e60ad3a91f973a2ba7b69e791e334d30d", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/afb94a8e60ad3a91f973a2ba7b69e791e334d30d", "committedDate": "2020-09-03T01:51:25Z", "message": "Minor changes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7587cb1643d92759751be85517cd1844ff8937d5", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/7587cb1643d92759751be85517cd1844ff8937d5", "committedDate": "2020-09-03T03:56:48Z", "message": "Addressing comments from Ryan"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "2eec2057a685beab08d98a02efa46ed0eb86dfb5", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/2eec2057a685beab08d98a02efa46ed0eb86dfb5", "committedDate": "2020-09-02T13:48:55Z", "message": "Minor changes"}, "afterCommit": {"oid": "7587cb1643d92759751be85517cd1844ff8937d5", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/7587cb1643d92759751be85517cd1844ff8937d5", "committedDate": "2020-09-03T03:56:48Z", "message": "Addressing comments from Ryan"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "57ec91c154d3dc79daab77945f4b2724e37a7ef9", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/57ec91c154d3dc79daab77945f4b2724e37a7ef9", "committedDate": "2020-09-03T04:27:33Z", "message": "Minior changes."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "3ffd1155fb76d47b98e052af3461c9dd268234eb", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/3ffd1155fb76d47b98e052af3461c9dd268234eb", "committedDate": "2020-09-03T04:12:08Z", "message": "Minior changes."}, "afterCommit": {"oid": "57ec91c154d3dc79daab77945f4b2724e37a7ef9", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/57ec91c154d3dc79daab77945f4b2724e37a7ef9", "committedDate": "2020-09-03T04:27:33Z", "message": "Minior changes."}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4062, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}