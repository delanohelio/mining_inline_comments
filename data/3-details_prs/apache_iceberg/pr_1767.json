{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTIwNzYxMTQ5", "number": 1767, "title": "AWS: Add progressive multipart upload to S3FileIO", "bodyText": "Add progressive upload to S3OutputStream using multipart upload.\nA few key changes are:\n\nThe output stream will switch from single PUT based upload to multipart when the multipart size * threshold is reached.\nStaging files will be rotated and uploaded async as output written\nStaging files will be deleted when the upload completes to reduce local disk requirements\nAn attempt will be made to abort the upload upon failure, but this is not guaranteed (Bucket lifecycle rules should be implemented to ensure data is cleaned up.)", "createdAt": "2020-11-13T18:23:49Z", "url": "https://github.com/apache/iceberg/pull/1767", "merged": true, "mergeCommit": {"oid": "5e3f9198e5675a852df4f0e1c28b4e3cf6630f86"}, "closed": true, "closedAt": "2020-11-21T18:01:27Z", "author": {"login": "danielcweeks"}, "timelineItems": {"totalCount": 26, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdcMiUwAFqTUzMDMxNzIzMw==", "endCursor": "Y3Vyc29yOnYyOpPPAAABdeiJ9kgFqTUzNTg3MTg4NA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMwMzE3MjMz", "url": "https://github.com/apache/iceberg/pull/1767#pullrequestreview-530317233", "createdAt": "2020-11-13T18:42:15Z", "commit": {"oid": "c805102c1e886633f2e5e8a33a616a1f359090ce"}, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QxODo0Mzo0OFrOHy61kg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QxOTo0MDoyOFrOHy8kWw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzE1NDgzNA==", "bodyText": "In #1754 I chose to directly store the necessary arguments instead of storing the entire property map. Not sure which way is better, what do you think?", "url": "https://github.com/apache/iceberg/pull/1767#discussion_r523154834", "createdAt": "2020-11-13T18:43:48Z", "author": {"login": "jackye1995"}, "path": "aws/src/main/java/org/apache/iceberg/aws/s3/BaseS3File.java", "diffHunk": "@@ -29,10 +31,18 @@\n   private final S3Client client;\n   private final S3URI uri;\n   private HeadObjectResponse metadata;\n+  private Map<String, String> properties;\n \n   BaseS3File(S3Client client, S3URI uri) {\n     this.client = client;\n     this.uri = uri;\n+    this.properties = Collections.emptyMap();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c805102c1e886633f2e5e8a33a616a1f359090ce"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzE1NTcyNQ==", "bodyText": "Also in #1754 and #1633 I created a file AwsCatalogProperties to centralize all the properties, we can move all the properties there. I am not sure if . is allowed character in Spark and Flink catalog config property, let me verify that and come back later.", "url": "https://github.com/apache/iceberg/pull/1767#discussion_r523155725", "createdAt": "2020-11-13T18:45:34Z", "author": {"login": "jackye1995"}, "path": "aws/src/main/java/org/apache/iceberg/aws/s3/S3OutputStream.java", "diffHunk": "@@ -19,42 +19,83 @@\n \n package org.apache.iceberg.aws.s3;\n \n+import java.io.BufferedInputStream;\n import java.io.BufferedOutputStream;\n+import java.io.ByteArrayInputStream;\n import java.io.File;\n+import java.io.FileInputStream;\n import java.io.FileOutputStream;\n import java.io.IOException;\n-import java.io.OutputStream;\n+import java.io.InputStream;\n+import java.io.SequenceInputStream;\n+import java.io.UncheckedIOException;\n import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n import org.apache.iceberg.io.PositionOutputStream;\n import org.apache.iceberg.relocated.com.google.common.base.Joiner;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.base.Predicates;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.io.CountingOutputStream;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n import software.amazon.awssdk.core.sync.RequestBody;\n import software.amazon.awssdk.services.s3.S3Client;\n+import software.amazon.awssdk.services.s3.model.AbortMultipartUploadRequest;\n+import software.amazon.awssdk.services.s3.model.AbortMultipartUploadResponse;\n+import software.amazon.awssdk.services.s3.model.CompleteMultipartUploadRequest;\n+import software.amazon.awssdk.services.s3.model.CompletedMultipartUpload;\n+import software.amazon.awssdk.services.s3.model.CompletedPart;\n+import software.amazon.awssdk.services.s3.model.CreateMultipartUploadRequest;\n import software.amazon.awssdk.services.s3.model.PutObjectRequest;\n+import software.amazon.awssdk.services.s3.model.UploadPartRequest;\n+import software.amazon.awssdk.services.s3.model.UploadPartResponse;\n \n class S3OutputStream extends PositionOutputStream {\n   private static final Logger LOG = LoggerFactory.getLogger(S3OutputStream.class);\n \n+  static final String MULTIPART_SIZE = \"s3fileio.multipart.size\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c805102c1e886633f2e5e8a33a616a1f359090ce"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzE2MDMzNQ==", "bodyText": "Looks like multiPartThresholdFactor is only used once in write with multiPartSize * multiPartThresholdFactor, so why not just use a configuration multiPartThresholdSize that defines the actual threshold to start multipart upload?", "url": "https://github.com/apache/iceberg/pull/1767#discussion_r523160335", "createdAt": "2020-11-13T18:54:28Z", "author": {"login": "jackye1995"}, "path": "aws/src/main/java/org/apache/iceberg/aws/s3/S3OutputStream.java", "diffHunk": "@@ -19,42 +19,83 @@\n \n package org.apache.iceberg.aws.s3;\n \n+import java.io.BufferedInputStream;\n import java.io.BufferedOutputStream;\n+import java.io.ByteArrayInputStream;\n import java.io.File;\n+import java.io.FileInputStream;\n import java.io.FileOutputStream;\n import java.io.IOException;\n-import java.io.OutputStream;\n+import java.io.InputStream;\n+import java.io.SequenceInputStream;\n+import java.io.UncheckedIOException;\n import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n import org.apache.iceberg.io.PositionOutputStream;\n import org.apache.iceberg.relocated.com.google.common.base.Joiner;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.base.Predicates;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.io.CountingOutputStream;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n import software.amazon.awssdk.core.sync.RequestBody;\n import software.amazon.awssdk.services.s3.S3Client;\n+import software.amazon.awssdk.services.s3.model.AbortMultipartUploadRequest;\n+import software.amazon.awssdk.services.s3.model.AbortMultipartUploadResponse;\n+import software.amazon.awssdk.services.s3.model.CompleteMultipartUploadRequest;\n+import software.amazon.awssdk.services.s3.model.CompletedMultipartUpload;\n+import software.amazon.awssdk.services.s3.model.CompletedPart;\n+import software.amazon.awssdk.services.s3.model.CreateMultipartUploadRequest;\n import software.amazon.awssdk.services.s3.model.PutObjectRequest;\n+import software.amazon.awssdk.services.s3.model.UploadPartRequest;\n+import software.amazon.awssdk.services.s3.model.UploadPartResponse;\n \n class S3OutputStream extends PositionOutputStream {\n   private static final Logger LOG = LoggerFactory.getLogger(S3OutputStream.class);\n \n+  static final String MULTIPART_SIZE = \"s3fileio.multipart.size\";\n+  static final String MULTIPART_THRESHOLD_FACTOR = \"s3fileio.multipart.threshold\";\n+\n+  static final int MIN_MULTIPART_UPLOAD_SIZE = 5 * 1024 * 1024;\n+  static final int DEFAULT_MULTIPART_SIZE = 32 * 1024 * 1024;\n+  static final double DEFAULT_MULTIPART_THRESHOLD = 1.5;\n+\n   private final StackTraceElement[] createStack;\n   private final S3Client s3;\n   private final S3URI location;\n \n-  private final OutputStream stream;\n-  private final File stagingFile;\n-  private long pos = 0;\n+  private CountingOutputStream stream;\n+  private final List<File> stagingFiles = Lists.newArrayList();\n+  private File currentStagingFile;\n+  private String multipartUploadId;\n+  private final Map<File, CompletableFuture<CompletedPart>> multiPartMap = Maps.newHashMap();\n+  private final int multiPartSize;\n+  private final double multiPartThresholdFactor;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c805102c1e886633f2e5e8a33a616a1f359090ce"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzE4MjQ0Mg==", "bodyText": "Can we also delete the temp file progressively instead of deleting everything after close() is called?", "url": "https://github.com/apache/iceberg/pull/1767#discussion_r523182442", "createdAt": "2020-11-13T19:39:03Z", "author": {"login": "jackye1995"}, "path": "aws/src/main/java/org/apache/iceberg/aws/s3/S3OutputStream.java", "diffHunk": "@@ -87,18 +168,105 @@ public void close() throws IOException {\n \n     super.close();\n     closed = true;\n+    currentStagingFile = null;\n \n     try {\n       stream.close();\n \n-      s3.putObject(\n-          PutObjectRequest.builder().bucket(location.bucket()).key(location.key()).build(),\n-          RequestBody.fromFile(stagingFile));\n+      completeUploads();\n     } finally {\n-      if (!stagingFile.delete()) {\n-        LOG.warn(\"Could not delete temporary file: {}\", stagingFile);\n+      stagingFiles.forEach(f -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c805102c1e886633f2e5e8a33a616a1f359090ce"}, "originalPosition": 164}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzE4MzE5NQ==", "bodyText": "If we want to make it async, can we directly leverage S3AsyncClient?", "url": "https://github.com/apache/iceberg/pull/1767#discussion_r523183195", "createdAt": "2020-11-13T19:40:28Z", "author": {"login": "jackye1995"}, "path": "aws/src/main/java/org/apache/iceberg/aws/s3/S3OutputStream.java", "diffHunk": "@@ -87,18 +168,105 @@ public void close() throws IOException {\n \n     super.close();\n     closed = true;\n+    currentStagingFile = null;\n \n     try {\n       stream.close();\n \n-      s3.putObject(\n-          PutObjectRequest.builder().bucket(location.bucket()).key(location.key()).build(),\n-          RequestBody.fromFile(stagingFile));\n+      completeUploads();\n     } finally {\n-      if (!stagingFile.delete()) {\n-        LOG.warn(\"Could not delete temporary file: {}\", stagingFile);\n+      stagingFiles.forEach(f -> {\n+        if (f.exists() && !f.delete()) {\n+          LOG.warn(\"Could not delete temporary file: {}\", f);\n+        }\n+      });\n+    }\n+  }\n+\n+  private void initializeMultiPartUpload() {\n+    multipartUploadId = s3.createMultipartUpload(CreateMultipartUploadRequest.builder()\n+        .bucket(location.bucket()).key(location.key()).build()).uploadId();\n+  }\n+\n+  private void uploadParts() {\n+    // exit if multipart has not been initiated\n+    if (multipartUploadId == null) {\n+      return;\n+    }\n+\n+    stagingFiles.stream()\n+        // do not upload the file currently being written\n+        .filter(f -> currentStagingFile == null || !currentStagingFile.equals(f))\n+        // do not upload any files that have already been processed\n+        .filter(Predicates.not(multiPartMap::containsKey))\n+        .forEach(f -> {\n+          UploadPartRequest uploadRequest = UploadPartRequest.builder()\n+              .bucket(location.bucket())\n+              .key(location.key())\n+              .uploadId(multipartUploadId)\n+              .partNumber(stagingFiles.indexOf(f) + 1)\n+              .contentLength(f.length())\n+              .build();\n+\n+          CompletableFuture<CompletedPart> future = CompletableFuture.supplyAsync(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c805102c1e886633f2e5e8a33a616a1f359090ce"}, "originalPosition": 197}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMwNDc1NDI0", "url": "https://github.com/apache/iceberg/pull/1767#pullrequestreview-530475424", "createdAt": "2020-11-13T23:19:54Z", "commit": {"oid": "ba3f8e5e5cdd37f6d78c7240c3b59da3b4fc7ac8"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QyMzoxOTo1NFrOHzCkGA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QyMzoxOTo1NFrOHzCkGA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzI4MTQzMg==", "bodyText": "If this check were done outside of synchronized as well, then we wouldn't need to get the lock each time a file is opened. There isn't a huge chance of lock contention, but it is always nice to avoid locking when possible.", "url": "https://github.com/apache/iceberg/pull/1767#discussion_r523281432", "createdAt": "2020-11-13T23:19:54Z", "author": {"login": "rdblue"}, "path": "aws/src/main/java/org/apache/iceberg/aws/s3/S3OutputStream.java", "diffHunk": "@@ -19,42 +19,105 @@\n \n package org.apache.iceberg.aws.s3;\n \n+import java.io.BufferedInputStream;\n import java.io.BufferedOutputStream;\n+import java.io.ByteArrayInputStream;\n import java.io.File;\n+import java.io.FileInputStream;\n import java.io.FileOutputStream;\n import java.io.IOException;\n-import java.io.OutputStream;\n+import java.io.InputStream;\n+import java.io.SequenceInputStream;\n+import java.io.UncheckedIOException;\n import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ThreadPoolExecutor;\n+import java.util.stream.Collectors;\n import org.apache.iceberg.io.PositionOutputStream;\n import org.apache.iceberg.relocated.com.google.common.base.Joiner;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.base.Predicates;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.io.CountingOutputStream;\n+import org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors;\n+import org.apache.iceberg.relocated.com.google.common.util.concurrent.ThreadFactoryBuilder;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n import software.amazon.awssdk.core.sync.RequestBody;\n import software.amazon.awssdk.services.s3.S3Client;\n+import software.amazon.awssdk.services.s3.model.AbortMultipartUploadRequest;\n+import software.amazon.awssdk.services.s3.model.CompleteMultipartUploadRequest;\n+import software.amazon.awssdk.services.s3.model.CompletedMultipartUpload;\n+import software.amazon.awssdk.services.s3.model.CompletedPart;\n+import software.amazon.awssdk.services.s3.model.CreateMultipartUploadRequest;\n import software.amazon.awssdk.services.s3.model.PutObjectRequest;\n+import software.amazon.awssdk.services.s3.model.UploadPartRequest;\n+import software.amazon.awssdk.services.s3.model.UploadPartResponse;\n \n class S3OutputStream extends PositionOutputStream {\n   private static final Logger LOG = LoggerFactory.getLogger(S3OutputStream.class);\n \n+  static final String UPLOAD_POOL_SIZE  = \"s3fileio.multipart.num-threads\";\n+  static final String MULTIPART_SIZE = \"s3fileio.multipart.part.size\";\n+  static final String MULTIPART_THRESHOLD_FACTOR = \"s3fileio.multipart.threshold\";\n+\n+  static final int DEFAULT_UPLOAD_WORKER_POOL_SIZE = Runtime.getRuntime().availableProcessors();\n+  static final int MIN_MULTIPART_UPLOAD_SIZE = 5 * 1024 * 1024;\n+  static final int DEFAULT_MULTIPART_SIZE = 32 * 1024 * 1024;\n+  static final double DEFAULT_MULTIPART_THRESHOLD = 1.5;\n+\n+  private static ExecutorService executorService;\n+\n   private final StackTraceElement[] createStack;\n   private final S3Client s3;\n   private final S3URI location;\n \n-  private final OutputStream stream;\n-  private final File stagingFile;\n-  private long pos = 0;\n+  private CountingOutputStream stream;\n+  private final List<File> stagingFiles = Lists.newArrayList();\n+  private File currentStagingFile;\n+  private String multipartUploadId;\n+  private final Map<File, CompletableFuture<CompletedPart>> multiPartMap = Maps.newHashMap();\n+  private final int multiPartSize;\n+  private final double multiPartThresholdFactor;\n \n+  private long pos = 0;\n   private boolean closed = false;\n \n-  S3OutputStream(S3Client s3, S3URI location) throws IOException {\n+  S3OutputStream(Map<String, String> properties, S3Client s3, S3URI location) throws IOException {\n+    synchronized (this) {\n+      if (executorService == null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ba3f8e5e5cdd37f6d78c7240c3b59da3b4fc7ac8"}, "originalPosition": 82}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMwNDc1NzAy", "url": "https://github.com/apache/iceberg/pull/1767#pullrequestreview-530475702", "createdAt": "2020-11-13T23:20:51Z", "commit": {"oid": "ba3f8e5e5cdd37f6d78c7240c3b59da3b4fc7ac8"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QyMzoyMDo1MVrOHzClJg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QyMzoyMDo1MVrOHzClJg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzI4MTcwMg==", "bodyText": "Iceberg has PropertyUtil.propertyAsInt for this situation.", "url": "https://github.com/apache/iceberg/pull/1767#discussion_r523281702", "createdAt": "2020-11-13T23:20:51Z", "author": {"login": "rdblue"}, "path": "aws/src/main/java/org/apache/iceberg/aws/s3/S3OutputStream.java", "diffHunk": "@@ -19,42 +19,105 @@\n \n package org.apache.iceberg.aws.s3;\n \n+import java.io.BufferedInputStream;\n import java.io.BufferedOutputStream;\n+import java.io.ByteArrayInputStream;\n import java.io.File;\n+import java.io.FileInputStream;\n import java.io.FileOutputStream;\n import java.io.IOException;\n-import java.io.OutputStream;\n+import java.io.InputStream;\n+import java.io.SequenceInputStream;\n+import java.io.UncheckedIOException;\n import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ThreadPoolExecutor;\n+import java.util.stream.Collectors;\n import org.apache.iceberg.io.PositionOutputStream;\n import org.apache.iceberg.relocated.com.google.common.base.Joiner;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.base.Predicates;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.io.CountingOutputStream;\n+import org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors;\n+import org.apache.iceberg.relocated.com.google.common.util.concurrent.ThreadFactoryBuilder;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n import software.amazon.awssdk.core.sync.RequestBody;\n import software.amazon.awssdk.services.s3.S3Client;\n+import software.amazon.awssdk.services.s3.model.AbortMultipartUploadRequest;\n+import software.amazon.awssdk.services.s3.model.CompleteMultipartUploadRequest;\n+import software.amazon.awssdk.services.s3.model.CompletedMultipartUpload;\n+import software.amazon.awssdk.services.s3.model.CompletedPart;\n+import software.amazon.awssdk.services.s3.model.CreateMultipartUploadRequest;\n import software.amazon.awssdk.services.s3.model.PutObjectRequest;\n+import software.amazon.awssdk.services.s3.model.UploadPartRequest;\n+import software.amazon.awssdk.services.s3.model.UploadPartResponse;\n \n class S3OutputStream extends PositionOutputStream {\n   private static final Logger LOG = LoggerFactory.getLogger(S3OutputStream.class);\n \n+  static final String UPLOAD_POOL_SIZE  = \"s3fileio.multipart.num-threads\";\n+  static final String MULTIPART_SIZE = \"s3fileio.multipart.part.size\";\n+  static final String MULTIPART_THRESHOLD_FACTOR = \"s3fileio.multipart.threshold\";\n+\n+  static final int DEFAULT_UPLOAD_WORKER_POOL_SIZE = Runtime.getRuntime().availableProcessors();\n+  static final int MIN_MULTIPART_UPLOAD_SIZE = 5 * 1024 * 1024;\n+  static final int DEFAULT_MULTIPART_SIZE = 32 * 1024 * 1024;\n+  static final double DEFAULT_MULTIPART_THRESHOLD = 1.5;\n+\n+  private static ExecutorService executorService;\n+\n   private final StackTraceElement[] createStack;\n   private final S3Client s3;\n   private final S3URI location;\n \n-  private final OutputStream stream;\n-  private final File stagingFile;\n-  private long pos = 0;\n+  private CountingOutputStream stream;\n+  private final List<File> stagingFiles = Lists.newArrayList();\n+  private File currentStagingFile;\n+  private String multipartUploadId;\n+  private final Map<File, CompletableFuture<CompletedPart>> multiPartMap = Maps.newHashMap();\n+  private final int multiPartSize;\n+  private final double multiPartThresholdFactor;\n \n+  private long pos = 0;\n   private boolean closed = false;\n \n-  S3OutputStream(S3Client s3, S3URI location) throws IOException {\n+  S3OutputStream(Map<String, String> properties, S3Client s3, S3URI location) throws IOException {\n+    synchronized (this) {\n+      if (executorService == null) {\n+        executorService = MoreExecutors.getExitingExecutorService(\n+            (ThreadPoolExecutor) Executors.newFixedThreadPool(\n+                Integer.parseInt(properties.getOrDefault(UPLOAD_POOL_SIZE,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ba3f8e5e5cdd37f6d78c7240c3b59da3b4fc7ac8"}, "originalPosition": 85}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMwNDc5Mzk5", "url": "https://github.com/apache/iceberg/pull/1767#pullrequestreview-530479399", "createdAt": "2020-11-13T23:34:16Z", "commit": {"oid": "ba3f8e5e5cdd37f6d78c7240c3b59da3b4fc7ac8"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QyMzozNDoxNlrOHzCycg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QyMzozNDoxNlrOHzCycg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzI4NTEwNg==", "bodyText": "Nit: Could you move orElseGet to a new line?", "url": "https://github.com/apache/iceberg/pull/1767#discussion_r523285106", "createdAt": "2020-11-13T23:34:16Z", "author": {"login": "rdblue"}, "path": "aws/src/main/java/org/apache/iceberg/aws/s3/S3OutputStream.java", "diffHunk": "@@ -87,17 +190,105 @@ public void close() throws IOException {\n \n     super.close();\n     closed = true;\n+    currentStagingFile = null;\n \n     try {\n       stream.close();\n \n+      completeUploads();\n+    } finally {\n+      stagingFiles.forEach(f -> {\n+        if (f.exists() && !f.delete()) {\n+          LOG.warn(\"Could not delete temporary file: {}\", f);\n+        }\n+      });\n+    }\n+  }\n+\n+  private void initializeMultiPartUpload() {\n+    multipartUploadId = s3.createMultipartUpload(CreateMultipartUploadRequest.builder()\n+        .bucket(location.bucket()).key(location.key()).build()).uploadId();\n+  }\n+\n+  private void uploadParts() {\n+    // exit if multipart has not been initiated\n+    if (multipartUploadId == null) {\n+      return;\n+    }\n+\n+    stagingFiles.stream()\n+        // do not upload the file currently being written\n+        .filter(f -> currentStagingFile == null || !currentStagingFile.equals(f))\n+        // do not upload any files that have already been processed\n+        .filter(Predicates.not(multiPartMap::containsKey))\n+        .forEach(f -> {\n+          UploadPartRequest uploadRequest = UploadPartRequest.builder()\n+              .bucket(location.bucket())\n+              .key(location.key())\n+              .uploadId(multipartUploadId)\n+              .partNumber(stagingFiles.indexOf(f) + 1)\n+              .contentLength(f.length())\n+              .build();\n+\n+          CompletableFuture<CompletedPart> future = CompletableFuture.supplyAsync(\n+              () -> {\n+                UploadPartResponse response = s3.uploadPart(uploadRequest, RequestBody.fromFile(f));\n+                return CompletedPart.builder().eTag(response.eTag()).partNumber(uploadRequest.partNumber()).build();\n+              },\n+              executorService\n+          );\n+\n+          multiPartMap.put(f, future);\n+        });\n+  }\n+\n+  private void completeMultiPartUpload() throws IOException {\n+    try {\n+      List<CompletedPart> completedParts =\n+          multiPartMap.values()\n+              .stream()\n+              .map(CompletableFuture::join)\n+              .sorted(Comparator.comparing(CompletedPart::partNumber))\n+              .collect(Collectors.toList());\n+\n+      s3.completeMultipartUpload(CompleteMultipartUploadRequest.builder()\n+          .bucket(location.bucket()).key(location.key())\n+          .uploadId(multipartUploadId)\n+          .multipartUpload(CompletedMultipartUpload.builder().parts(completedParts).build()).build());\n+    } catch (CompletionException e) {\n+      abortUpload();\n+      throw new IOException(\"Multipart upload failed for upload id: \" + multipartUploadId, e);\n+    }\n+  }\n+\n+  private void abortUpload() {\n+    if (multipartUploadId != null) {\n+      s3.abortMultipartUpload(AbortMultipartUploadRequest.builder()\n+          .bucket(location.bucket()).key(location.key()).uploadId(multipartUploadId).build());\n+    }\n+  }\n+\n+  private void completeUploads() throws IOException {\n+    if (multipartUploadId == null) {\n+      long contentLength = stagingFiles.stream().mapToLong(File::length).sum();\n+      InputStream contentStream = new BufferedInputStream(stagingFiles.stream()\n+          .map(S3OutputStream::uncheckedInputStream)\n+          .reduce(SequenceInputStream::new).orElseGet(() -> new ByteArrayInputStream(new byte[0])));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ba3f8e5e5cdd37f6d78c7240c3b59da3b4fc7ac8"}, "originalPosition": 257}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMwNDgxNjM1", "url": "https://github.com/apache/iceberg/pull/1767#pullrequestreview-530481635", "createdAt": "2020-11-13T23:42:05Z", "commit": {"oid": "ba3f8e5e5cdd37f6d78c7240c3b59da3b4fc7ac8"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QyMzo0MjowNVrOHzC6Lg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QyMzo0MjowNVrOHzC6Lg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzI4NzA4Ng==", "bodyText": "It looks like uploading the last part depends on the line above where currentStagingFile is set to null. Otherwise, the current part is ignored in uploadParts. This leads to slightly confusing behavior for completeUploads because it won't include the last part unless that assumption is met.\nWhat about making completeUploads ensure the current stream is closed and null using a closeCurrent method?", "url": "https://github.com/apache/iceberg/pull/1767#discussion_r523287086", "createdAt": "2020-11-13T23:42:05Z", "author": {"login": "rdblue"}, "path": "aws/src/main/java/org/apache/iceberg/aws/s3/S3OutputStream.java", "diffHunk": "@@ -87,17 +190,105 @@ public void close() throws IOException {\n \n     super.close();\n     closed = true;\n+    currentStagingFile = null;\n \n     try {\n       stream.close();\n \n+      completeUploads();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ba3f8e5e5cdd37f6d78c7240c3b59da3b4fc7ac8"}, "originalPosition": 179}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMwNDgyODM0", "url": "https://github.com/apache/iceberg/pull/1767#pullrequestreview-530482834", "createdAt": "2020-11-13T23:46:55Z", "commit": {"oid": "ba3f8e5e5cdd37f6d78c7240c3b59da3b4fc7ac8"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QyMzo0Njo1NlrOHzC-Zw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QyMzo0Njo1NlrOHzC-Zw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzI4ODE2Nw==", "bodyText": "Here's another place where we can add reliability using Tasks. That has a callback for failure that can fail with its own exception, which will be suppressed instead of thrown in place of the CompletionException.", "url": "https://github.com/apache/iceberg/pull/1767#discussion_r523288167", "createdAt": "2020-11-13T23:46:56Z", "author": {"login": "rdblue"}, "path": "aws/src/main/java/org/apache/iceberg/aws/s3/S3OutputStream.java", "diffHunk": "@@ -87,17 +190,105 @@ public void close() throws IOException {\n \n     super.close();\n     closed = true;\n+    currentStagingFile = null;\n \n     try {\n       stream.close();\n \n+      completeUploads();\n+    } finally {\n+      stagingFiles.forEach(f -> {\n+        if (f.exists() && !f.delete()) {\n+          LOG.warn(\"Could not delete temporary file: {}\", f);\n+        }\n+      });\n+    }\n+  }\n+\n+  private void initializeMultiPartUpload() {\n+    multipartUploadId = s3.createMultipartUpload(CreateMultipartUploadRequest.builder()\n+        .bucket(location.bucket()).key(location.key()).build()).uploadId();\n+  }\n+\n+  private void uploadParts() {\n+    // exit if multipart has not been initiated\n+    if (multipartUploadId == null) {\n+      return;\n+    }\n+\n+    stagingFiles.stream()\n+        // do not upload the file currently being written\n+        .filter(f -> currentStagingFile == null || !currentStagingFile.equals(f))\n+        // do not upload any files that have already been processed\n+        .filter(Predicates.not(multiPartMap::containsKey))\n+        .forEach(f -> {\n+          UploadPartRequest uploadRequest = UploadPartRequest.builder()\n+              .bucket(location.bucket())\n+              .key(location.key())\n+              .uploadId(multipartUploadId)\n+              .partNumber(stagingFiles.indexOf(f) + 1)\n+              .contentLength(f.length())\n+              .build();\n+\n+          CompletableFuture<CompletedPart> future = CompletableFuture.supplyAsync(\n+              () -> {\n+                UploadPartResponse response = s3.uploadPart(uploadRequest, RequestBody.fromFile(f));\n+                return CompletedPart.builder().eTag(response.eTag()).partNumber(uploadRequest.partNumber()).build();\n+              },\n+              executorService\n+          );\n+\n+          multiPartMap.put(f, future);\n+        });\n+  }\n+\n+  private void completeMultiPartUpload() throws IOException {\n+    try {\n+      List<CompletedPart> completedParts =\n+          multiPartMap.values()\n+              .stream()\n+              .map(CompletableFuture::join)\n+              .sorted(Comparator.comparing(CompletedPart::partNumber))\n+              .collect(Collectors.toList());\n+\n+      s3.completeMultipartUpload(CompleteMultipartUploadRequest.builder()\n+          .bucket(location.bucket()).key(location.key())\n+          .uploadId(multipartUploadId)\n+          .multipartUpload(CompletedMultipartUpload.builder().parts(completedParts).build()).build());\n+    } catch (CompletionException e) {\n+      abortUpload();\n+      throw new IOException(\"Multipart upload failed for upload id: \" + multipartUploadId, e);\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ba3f8e5e5cdd37f6d78c7240c3b59da3b4fc7ac8"}, "originalPosition": 242}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMwNDgzMzI5", "url": "https://github.com/apache/iceberg/pull/1767#pullrequestreview-530483329", "createdAt": "2020-11-13T23:48:59Z", "commit": {"oid": "ba3f8e5e5cdd37f6d78c7240c3b59da3b4fc7ac8"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QyMzo0ODo1OVrOHzDAJw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QyMzo0ODo1OVrOHzDAJw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzI4ODYxNQ==", "bodyText": "When this is used, the class will never switch to progressive upload. Is that intentional?", "url": "https://github.com/apache/iceberg/pull/1767#discussion_r523288615", "createdAt": "2020-11-13T23:48:59Z", "author": {"login": "rdblue"}, "path": "aws/src/main/java/org/apache/iceberg/aws/s3/S3OutputStream.java", "diffHunk": "@@ -69,14 +132,54 @@ public void flush() throws IOException {\n \n   @Override\n   public void write(int b) throws IOException {\n+    if (stream.getCount() >= multiPartSize) {\n+      newStream();\n+      uploadParts();\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ba3f8e5e5cdd37f6d78c7240c3b59da3b4fc7ac8"}, "originalPosition": 121}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMwNTY3MzY3", "url": "https://github.com/apache/iceberg/pull/1767#pullrequestreview-530567367", "createdAt": "2020-11-14T09:52:09Z", "commit": {"oid": "ba3f8e5e5cdd37f6d78c7240c3b59da3b4fc7ac8"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNFQwOTo1MjoxMFrOHzJ2xg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNFQwOTo1MjoxMFrOHzJ2xg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzQwMDkwMg==", "bodyText": "maybe also try to initialize the multipart upload up here so that the individual parts can get kicked off immediately during the loop? or at the beginning of the method?\n(This loop's uploadParts currently returns immediately if this is before initialization right?)", "url": "https://github.com/apache/iceberg/pull/1767#discussion_r523400902", "createdAt": "2020-11-14T09:52:10Z", "author": {"login": "johnclara"}, "path": "aws/src/main/java/org/apache/iceberg/aws/s3/S3OutputStream.java", "diffHunk": "@@ -69,14 +132,54 @@ public void flush() throws IOException {\n \n   @Override\n   public void write(int b) throws IOException {\n+    if (stream.getCount() >= multiPartSize) {\n+      newStream();\n+      uploadParts();\n+    }\n+\n     stream.write(b);\n     pos += 1;\n   }\n \n   @Override\n   public void write(byte[] b, int off, int len) throws IOException {\n-    stream.write(b, off, len);\n+    int remaining = len;\n+    int relativeOffset = off;\n+\n+    // Write the remainder of the part size to the staging file\n+    // and continue to write new staging files if the write is\n+    // larger than the part size.\n+    while (stream.getCount() + remaining > multiPartSize) {\n+      int writeSize = multiPartSize - (int) stream.getCount();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ba3f8e5e5cdd37f6d78c7240c3b59da3b4fc7ac8"}, "originalPosition": 137}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMwNzY3MTg1", "url": "https://github.com/apache/iceberg/pull/1767#pullrequestreview-530767185", "createdAt": "2020-11-15T04:55:51Z", "commit": {"oid": "ba3f8e5e5cdd37f6d78c7240c3b59da3b4fc7ac8"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNVQwNDo1NTo1MVrOHzcmJA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNVQwNDo1NTo1MVrOHzcmJA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzcwNzk0MA==", "bodyText": "s3a supports canned acls for s3 requests, not sure if this should also support them?\nMy team had to use them once for writing from within account A to a bucket owned by bucket B\nWe also had to add it to rdblue/s3committer:\nhttps://github.com/rdblue/s3committer/blob/master/src/main/java/com/netflix/bdp/s3/S3Util.java#L73\n    InitiateMultipartUploadRequest initiateMultipartUploadRequest = new InitiateMultipartUploadRequest(bucket, key)\n          .withCannedACL(CannedAccessControlList.BucketOwnerFullControl);\n    InitiateMultipartUploadResult initiate = client.initiateMultipartUpload(initiateMultipartUploadRequest);", "url": "https://github.com/apache/iceberg/pull/1767#discussion_r523707940", "createdAt": "2020-11-15T04:55:51Z", "author": {"login": "johnclara"}, "path": "aws/src/main/java/org/apache/iceberg/aws/s3/S3OutputStream.java", "diffHunk": "@@ -87,17 +190,105 @@ public void close() throws IOException {\n \n     super.close();\n     closed = true;\n+    currentStagingFile = null;\n \n     try {\n       stream.close();\n \n+      completeUploads();\n+    } finally {\n+      stagingFiles.forEach(f -> {\n+        if (f.exists() && !f.delete()) {\n+          LOG.warn(\"Could not delete temporary file: {}\", f);\n+        }\n+      });\n+    }\n+  }\n+\n+  private void initializeMultiPartUpload() {\n+    multipartUploadId = s3.createMultipartUpload(CreateMultipartUploadRequest.builder()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ba3f8e5e5cdd37f6d78c7240c3b59da3b4fc7ac8"}, "originalPosition": 190}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMwODMzNzUy", "url": "https://github.com/apache/iceberg/pull/1767#pullrequestreview-530833752", "createdAt": "2020-11-15T20:48:07Z", "commit": {"oid": "ba3f8e5e5cdd37f6d78c7240c3b59da3b4fc7ac8"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNVQyMDo0ODowN1rOHzi64w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNVQyMDo0ODowN1rOHzi64w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzgxMTU1NQ==", "bodyText": "Not sure if the staging directory should be configurable at some point (not this PR)?\nFor our clients in docker containers, we've been thinking about moving s3a's staging area to be under a mount optimized for IO.\nWe haven't done any tests to see if this gives us any noticeable performance gains.", "url": "https://github.com/apache/iceberg/pull/1767#discussion_r523811555", "createdAt": "2020-11-15T20:48:07Z", "author": {"login": "johnclara"}, "path": "aws/src/main/java/org/apache/iceberg/aws/s3/S3OutputStream.java", "diffHunk": "@@ -69,14 +132,54 @@ public void flush() throws IOException {\n \n   @Override\n   public void write(int b) throws IOException {\n+    if (stream.getCount() >= multiPartSize) {\n+      newStream();\n+      uploadParts();\n+    }\n+\n     stream.write(b);\n     pos += 1;\n   }\n \n   @Override\n   public void write(byte[] b, int off, int len) throws IOException {\n-    stream.write(b, off, len);\n+    int remaining = len;\n+    int relativeOffset = off;\n+\n+    // Write the remainder of the part size to the staging file\n+    // and continue to write new staging files if the write is\n+    // larger than the part size.\n+    while (stream.getCount() + remaining > multiPartSize) {\n+      int writeSize = multiPartSize - (int) stream.getCount();\n+\n+      stream.write(b, relativeOffset, writeSize);\n+      remaining -= writeSize;\n+      relativeOffset += writeSize;\n+\n+      newStream();\n+      uploadParts();\n+    }\n+\n+    stream.write(b, relativeOffset, remaining);\n     pos += len;\n+\n+    // switch to multipart upload\n+    if (multipartUploadId == null && pos >= multiPartSize * multiPartThresholdFactor) {\n+      initializeMultiPartUpload();\n+      uploadParts();\n+    }\n+  }\n+\n+  private void newStream() throws IOException {\n+    if (stream != null) {\n+      stream.close();\n+    }\n+\n+    currentStagingFile = File.createTempFile(\"s3fileio-\", \".tmp\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ba3f8e5e5cdd37f6d78c7240c3b59da3b4fc7ac8"}, "originalPosition": 162}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "ba3f8e5e5cdd37f6d78c7240c3b59da3b4fc7ac8", "author": {"user": {"login": "danielcweeks", "name": "Daniel Weeks"}}, "url": "https://github.com/apache/iceberg/commit/ba3f8e5e5cdd37f6d78c7240c3b59da3b4fc7ac8", "committedDate": "2020-11-13T19:45:27Z", "message": "Add executor service for async tasks per errorprone"}, "afterCommit": {"oid": "c6ecb61b33bed32da331f69b762295dc4a2a0db0", "author": {"user": {"login": "danielcweeks", "name": "Daniel Weeks"}}, "url": "https://github.com/apache/iceberg/commit/c6ecb61b33bed32da331f69b762295dc4a2a0db0", "committedDate": "2020-11-17T22:01:31Z", "message": "Refactor setting encryption for requests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c2d164aea35c26c51099eec6d2bae47af72579ef", "author": {"user": {"login": "danielcweeks", "name": "Daniel Weeks"}}, "url": "https://github.com/apache/iceberg/commit/c2d164aea35c26c51099eec6d2bae47af72579ef", "committedDate": "2020-11-20T20:41:05Z", "message": "AWS: Add progressive multipart upload to S3FileIO"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1d6aa4ca26b9b3c4dbaa712cac64455b0f5813a6", "author": {"user": {"login": "danielcweeks", "name": "Daniel Weeks"}}, "url": "https://github.com/apache/iceberg/commit/1d6aa4ca26b9b3c4dbaa712cac64455b0f5813a6", "committedDate": "2020-11-20T20:41:05Z", "message": "Fix test after rebase"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a0f917a2d572f5af90c89b2ab100200514c9c9d1", "author": {"user": {"login": "danielcweeks", "name": "Daniel Weeks"}}, "url": "https://github.com/apache/iceberg/commit/a0f917a2d572f5af90c89b2ab100200514c9c9d1", "committedDate": "2020-11-20T20:41:05Z", "message": "Simply the complete and ensure parts are in order"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ef308aedabad38737c23e10cedf06557e5667c9b", "author": {"user": {"login": "danielcweeks", "name": "Daniel Weeks"}}, "url": "https://github.com/apache/iceberg/commit/ef308aedabad38737c23e10cedf06557e5667c9b", "committedDate": "2020-11-20T20:41:05Z", "message": "Add sort to stream"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1ba0d8b404d8b0940202bfd96cbd011aea8bdf05", "author": {"user": {"login": "danielcweeks", "name": "Daniel Weeks"}}, "url": "https://github.com/apache/iceberg/commit/1ba0d8b404d8b0940202bfd96cbd011aea8bdf05", "committedDate": "2020-11-20T20:41:05Z", "message": "Checkstyle"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f361ee77ea2d3c609ebc704733b2cc8ed18b3df1", "author": {"user": {"login": "danielcweeks", "name": "Daniel Weeks"}}, "url": "https://github.com/apache/iceberg/commit/f361ee77ea2d3c609ebc704733b2cc8ed18b3df1", "committedDate": "2020-11-20T20:41:05Z", "message": "Add abort attempt back to complete"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a5bc7a45a05148e8798f10ce38806bff4a4f5e89", "author": {"user": {"login": "danielcweeks", "name": "Daniel Weeks"}}, "url": "https://github.com/apache/iceberg/commit/a5bc7a45a05148e8798f10ce38806bff4a4f5e89", "committedDate": "2020-11-20T20:41:05Z", "message": "Initialize only once"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0a4f9b2af55f23554dd5b8cce75f4c6058536335", "author": {"user": {"login": "danielcweeks", "name": "Daniel Weeks"}}, "url": "https://github.com/apache/iceberg/commit/0a4f9b2af55f23554dd5b8cce75f4c6058536335", "committedDate": "2020-11-20T20:41:05Z", "message": "Add executor service for async tasks per errorprone"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "89cbfb20db81470446af7f47a4fb10fc5b04ebfa", "author": {"user": {"login": "danielcweeks", "name": "Daniel Weeks"}}, "url": "https://github.com/apache/iceberg/commit/89cbfb20db81470446af7f47a4fb10fc5b04ebfa", "committedDate": "2020-11-20T20:41:05Z", "message": "Address comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f97ee71c7058dc993789724ee2f40dbb3d76924b", "author": {"user": {"login": "danielcweeks", "name": "Daniel Weeks"}}, "url": "https://github.com/apache/iceberg/commit/f97ee71c7058dc993789724ee2f40dbb3d76924b", "committedDate": "2020-11-20T20:43:41Z", "message": "Refactor setting encryption for requests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c9b69cc5429f3824dedcf6db3cc1da60ef7e14d7", "author": {"user": {"login": "danielcweeks", "name": "Daniel Weeks"}}, "url": "https://github.com/apache/iceberg/commit/c9b69cc5429f3824dedcf6db3cc1da60ef7e14d7", "committedDate": "2020-11-20T20:44:34Z", "message": "Fix defaults to no-arg AwsProperties"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cf117183a41028f82ebc8da9f4f6391e6b001d9b", "author": {"user": {"login": "danielcweeks", "name": "Daniel Weeks"}}, "url": "https://github.com/apache/iceberg/commit/cf117183a41028f82ebc8da9f4f6391e6b001d9b", "committedDate": "2020-11-20T20:46:14Z", "message": "Address some failure cases and add more testing"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ac1aac22eadbf923c1ebbd486431ab779fe69c03", "author": {"user": {"login": "danielcweeks", "name": "Daniel Weeks"}}, "url": "https://github.com/apache/iceberg/commit/ac1aac22eadbf923c1ebbd486431ab779fe69c03", "committedDate": "2020-11-20T20:46:16Z", "message": "Checkstyle"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "3679ea7081b88e2c8e1f93f152a55ea4ed238429", "author": {"user": {"login": "danielcweeks", "name": "Daniel Weeks"}}, "url": "https://github.com/apache/iceberg/commit/3679ea7081b88e2c8e1f93f152a55ea4ed238429", "committedDate": "2020-11-20T20:38:15Z", "message": "Checkstyle"}, "afterCommit": {"oid": "ac1aac22eadbf923c1ebbd486431ab779fe69c03", "author": {"user": {"login": "danielcweeks", "name": "Daniel Weeks"}}, "url": "https://github.com/apache/iceberg/commit/ac1aac22eadbf923c1ebbd486431ab779fe69c03", "committedDate": "2020-11-20T20:46:16Z", "message": "Checkstyle"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM1ODcxODg0", "url": "https://github.com/apache/iceberg/pull/1767#pullrequestreview-535871884", "createdAt": "2020-11-21T02:02:37Z", "commit": {"oid": "ac1aac22eadbf923c1ebbd486431ab779fe69c03"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3749, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}