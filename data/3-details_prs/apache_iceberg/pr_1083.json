{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDI1Nzg1MDAw", "number": 1083, "title": "Add DataFile rewrite Action support for Iceberg", "bodyText": "This PR propose to add DataFile rewrite Action to Iceberg. The purpose of adding this functionality is to merge the small files into large ones to reduce the FileSystem overhead.\nThis PR uses read.split.target-size to control the read size of each task, and uses write.target-file-size-bytes to control the write size of each task. It also takes care of table partition to compact the files under each partition (without shuffling).  Besides user could specify the Filter expressions to only compact the files in some specific partitions.", "createdAt": "2020-06-01T06:36:28Z", "url": "https://github.com/apache/iceberg/pull/1083", "merged": true, "mergeCommit": {"oid": "cad124967ceb740178a3f65b75f19017494b430d"}, "closed": true, "closedAt": "2020-06-11T20:29:24Z", "author": {"login": "jerryshao"}, "timelineItems": {"totalCount": 39, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcnFliHgFqTQyMjEyNjY3NA==", "endCursor": "Y3Vyc29yOnYyOpPPAAABcqQTMkgFqTQyOTA1Mjc3Nw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIyMTI2Njc0", "url": "https://github.com/apache/iceberg/pull/1083#pullrequestreview-422126674", "createdAt": "2020-06-01T19:39:38Z", "commit": {"oid": "2dc587785ba1d1a2dd1b72aa85aea3bd30966f68"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMVQxOTozOTozOFrOGdXjDg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMVQxOTozOTozOFrOGdXjDg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzQ0NzY5NA==", "bodyText": "Nit: should be deletedDataFiles.", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r433447694", "createdAt": "2020-06-01T19:39:38Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesActionResult.java", "diffHunk": "@@ -0,0 +1,50 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.List;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+\n+public class RewriteDataFilesActionResult {\n+\n+  private static final RewriteDataFilesActionResult EMPTY =\n+      new RewriteDataFilesActionResult(ImmutableList.of(), ImmutableList.of());\n+\n+  private List<DataFile> deletedDataFiles;\n+  private List<DataFile> addedDataFiles;\n+\n+  public RewriteDataFilesActionResult(List<DataFile> deletedDataFiles, List<DataFile> addedDataFiles) {\n+    this.deletedDataFiles = deletedDataFiles;\n+    this.addedDataFiles = addedDataFiles;\n+  }\n+\n+  static RewriteDataFilesActionResult empty() {\n+    return EMPTY;\n+  }\n+\n+  public List<DataFile> deleteDataFiles() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2dc587785ba1d1a2dd1b72aa85aea3bd30966f68"}, "originalPosition": 43}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIyMjQzMzgy", "url": "https://github.com/apache/iceberg/pull/1083#pullrequestreview-422243382", "createdAt": "2020-06-01T23:19:39Z", "commit": {"oid": "2dc587785ba1d1a2dd1b72aa85aea3bd30966f68"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMVQyMzoxOTozOVrOGddGYQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMVQyMzoxOTozOVrOGddGYQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUzODY1Nw==", "bodyText": "It looks like if the aborts succeed, then the task returns instead of throwing t. Is that correct behavior? I see that this is calling context.markTaskFailed(originalThrowable). Is that done instead of throwing the exception?", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r433538657", "createdAt": "2020-06-01T23:19:39Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/RowDataRewriter.java", "diffHunk": "@@ -0,0 +1,138 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import java.io.Serializable;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.streaming.MicroBatchExecution;\n+import org.apache.spark.sql.sources.v2.writer.DataWriter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RowDataRewriter implements Serializable {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RowDataRewriter.class);\n+\n+  private final transient JavaSparkContext sparkContext;\n+  private final Broadcast<FileIO> fileIO;\n+  private final Broadcast<EncryptionManager> encryptionManager;\n+  private final String tableSchema;\n+  private final Writer.WriterFactory writerFactory;\n+  private final boolean caseSensitive;\n+\n+  public RowDataRewriter(Table table, JavaSparkContext sparkContext, PartitionSpec spec, boolean caseSensitive,\n+                         Broadcast<FileIO> fileIO, Broadcast<EncryptionManager> encryptionManager,\n+                         long targetDataFileSizeInBytes) {\n+    this.sparkContext = sparkContext;\n+    this.fileIO = fileIO;\n+    this.encryptionManager = encryptionManager;\n+\n+    this.caseSensitive = caseSensitive;\n+    this.tableSchema = SchemaParser.toJson(table.schema());\n+\n+    String formatString = table.properties().getOrDefault(\n+        TableProperties.DEFAULT_FILE_FORMAT, TableProperties.DEFAULT_FILE_FORMAT_DEFAULT);\n+    FileFormat fileFormat = FileFormat.valueOf(formatString.toUpperCase(Locale.ENGLISH));\n+    this.writerFactory = new Writer.WriterFactory(spec, fileFormat, table.locationProvider(), table.properties(),\n+        fileIO, encryptionManager, targetDataFileSizeInBytes, table.schema(), SparkSchemaUtil.convert(table.schema()));\n+  }\n+\n+  public List<DataFile> rewriteDataForTasks(CloseableIterable<CombinedScanTask> tasks) {\n+    List<CombinedScanTask> taskList = Lists.newArrayList(tasks);\n+    int parallelism = taskList.size();\n+\n+    JavaRDD<CombinedScanTask> taskRDD = sparkContext.parallelize(taskList, parallelism);\n+    JavaRDD<Writer.TaskCommit> taskCommitRDD = taskRDD.map(task -> rewriteDataForTask(task));\n+\n+    return taskCommitRDD.collect().stream()\n+        .flatMap(taskCommit -> Arrays.stream(taskCommit.files()))\n+        .collect(Collectors.toList());\n+  }\n+\n+  private Writer.TaskCommit rewriteDataForTask(CombinedScanTask task) throws Exception {\n+    TaskContext context = TaskContext.get();\n+\n+    RowDataReader dataReader = new RowDataReader(task, SchemaParser.fromJson(tableSchema),\n+        SchemaParser.fromJson(tableSchema), fileIO.value(), encryptionManager.value(), caseSensitive);\n+\n+    int partitionId = context.partitionId();\n+    long taskId = context.taskAttemptId();\n+    long epochId = Long.parseLong(\n+        Optional.ofNullable(context.getLocalProperty(MicroBatchExecution.BATCH_ID_KEY())).orElse(\"0\"));\n+    DataWriter<InternalRow> dataWriter = writerFactory.createDataWriter(partitionId, taskId, epochId);\n+\n+    Throwable originalThrowable = null;\n+    try {\n+      while (dataReader.next()) {\n+        InternalRow row = dataReader.get();\n+        dataWriter.write(row);\n+      }\n+\n+      dataReader.close();\n+      return (Writer.TaskCommit) dataWriter.commit();\n+\n+    } catch (Throwable t) {\n+      originalThrowable = t;\n+      try {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2dc587785ba1d1a2dd1b72aa85aea3bd30966f68"}, "originalPosition": 114}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIyMjQzNDcz", "url": "https://github.com/apache/iceberg/pull/1083#pullrequestreview-422243473", "createdAt": "2020-06-01T23:19:54Z", "commit": {"oid": "2dc587785ba1d1a2dd1b72aa85aea3bd30966f68"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMVQyMzoxOTo1NFrOGddGrQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMVQyMzoxOTo1NFrOGddGrQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUzODczMw==", "bodyText": "Why not throw the original throwable?", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r433538733", "createdAt": "2020-06-01T23:19:54Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/RowDataRewriter.java", "diffHunk": "@@ -0,0 +1,138 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import java.io.Serializable;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.streaming.MicroBatchExecution;\n+import org.apache.spark.sql.sources.v2.writer.DataWriter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RowDataRewriter implements Serializable {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RowDataRewriter.class);\n+\n+  private final transient JavaSparkContext sparkContext;\n+  private final Broadcast<FileIO> fileIO;\n+  private final Broadcast<EncryptionManager> encryptionManager;\n+  private final String tableSchema;\n+  private final Writer.WriterFactory writerFactory;\n+  private final boolean caseSensitive;\n+\n+  public RowDataRewriter(Table table, JavaSparkContext sparkContext, PartitionSpec spec, boolean caseSensitive,\n+                         Broadcast<FileIO> fileIO, Broadcast<EncryptionManager> encryptionManager,\n+                         long targetDataFileSizeInBytes) {\n+    this.sparkContext = sparkContext;\n+    this.fileIO = fileIO;\n+    this.encryptionManager = encryptionManager;\n+\n+    this.caseSensitive = caseSensitive;\n+    this.tableSchema = SchemaParser.toJson(table.schema());\n+\n+    String formatString = table.properties().getOrDefault(\n+        TableProperties.DEFAULT_FILE_FORMAT, TableProperties.DEFAULT_FILE_FORMAT_DEFAULT);\n+    FileFormat fileFormat = FileFormat.valueOf(formatString.toUpperCase(Locale.ENGLISH));\n+    this.writerFactory = new Writer.WriterFactory(spec, fileFormat, table.locationProvider(), table.properties(),\n+        fileIO, encryptionManager, targetDataFileSizeInBytes, table.schema(), SparkSchemaUtil.convert(table.schema()));\n+  }\n+\n+  public List<DataFile> rewriteDataForTasks(CloseableIterable<CombinedScanTask> tasks) {\n+    List<CombinedScanTask> taskList = Lists.newArrayList(tasks);\n+    int parallelism = taskList.size();\n+\n+    JavaRDD<CombinedScanTask> taskRDD = sparkContext.parallelize(taskList, parallelism);\n+    JavaRDD<Writer.TaskCommit> taskCommitRDD = taskRDD.map(task -> rewriteDataForTask(task));\n+\n+    return taskCommitRDD.collect().stream()\n+        .flatMap(taskCommit -> Arrays.stream(taskCommit.files()))\n+        .collect(Collectors.toList());\n+  }\n+\n+  private Writer.TaskCommit rewriteDataForTask(CombinedScanTask task) throws Exception {\n+    TaskContext context = TaskContext.get();\n+\n+    RowDataReader dataReader = new RowDataReader(task, SchemaParser.fromJson(tableSchema),\n+        SchemaParser.fromJson(tableSchema), fileIO.value(), encryptionManager.value(), caseSensitive);\n+\n+    int partitionId = context.partitionId();\n+    long taskId = context.taskAttemptId();\n+    long epochId = Long.parseLong(\n+        Optional.ofNullable(context.getLocalProperty(MicroBatchExecution.BATCH_ID_KEY())).orElse(\"0\"));\n+    DataWriter<InternalRow> dataWriter = writerFactory.createDataWriter(partitionId, taskId, epochId);\n+\n+    Throwable originalThrowable = null;\n+    try {\n+      while (dataReader.next()) {\n+        InternalRow row = dataReader.get();\n+        dataWriter.write(row);\n+      }\n+\n+      dataReader.close();\n+      return (Writer.TaskCommit) dataWriter.commit();\n+\n+    } catch (Throwable t) {\n+      originalThrowable = t;\n+      try {\n+        LOG.error(\"Aborting task\", originalThrowable);\n+        context.markTaskFailed(originalThrowable);\n+\n+        LOG.error(\"Aborting commit for partition {} (task {}, attempt {}, stage {}.{})\",\n+            partitionId, taskId, context.attemptNumber(), context.stageId(), context.stageAttemptNumber());\n+        dataReader.close();\n+        dataWriter.abort();\n+        LOG.error(\"Aborted commit for partition {} (task {}, attempt {}, stage {}.{})\",\n+            partitionId, taskId, context.taskAttemptId(), context.stageId(), context.stageAttemptNumber());\n+\n+      } catch (Throwable inner) {\n+        if (originalThrowable != inner) {\n+          originalThrowable.addSuppressed(inner);\n+          LOG.warn(\"Suppressing exception in catch: {}\", inner.getMessage(), inner);\n+        }\n+\n+        // Wrap throwable in an Exception\n+        throw new Exception(originalThrowable);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2dc587785ba1d1a2dd1b72aa85aea3bd30966f68"}, "originalPosition": 132}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIyMjQzNzkx", "url": "https://github.com/apache/iceberg/pull/1083#pullrequestreview-422243791", "createdAt": "2020-06-01T23:20:49Z", "commit": {"oid": "2dc587785ba1d1a2dd1b72aa85aea3bd30966f68"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMVQyMzoyMDo0OVrOGddHlw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMVQyMzoyMDo0OVrOGddHlw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUzODk2Nw==", "bodyText": "Close should be called within a finally block, I think.", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r433538967", "createdAt": "2020-06-01T23:20:49Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/RowDataRewriter.java", "diffHunk": "@@ -0,0 +1,138 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import java.io.Serializable;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.streaming.MicroBatchExecution;\n+import org.apache.spark.sql.sources.v2.writer.DataWriter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RowDataRewriter implements Serializable {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RowDataRewriter.class);\n+\n+  private final transient JavaSparkContext sparkContext;\n+  private final Broadcast<FileIO> fileIO;\n+  private final Broadcast<EncryptionManager> encryptionManager;\n+  private final String tableSchema;\n+  private final Writer.WriterFactory writerFactory;\n+  private final boolean caseSensitive;\n+\n+  public RowDataRewriter(Table table, JavaSparkContext sparkContext, PartitionSpec spec, boolean caseSensitive,\n+                         Broadcast<FileIO> fileIO, Broadcast<EncryptionManager> encryptionManager,\n+                         long targetDataFileSizeInBytes) {\n+    this.sparkContext = sparkContext;\n+    this.fileIO = fileIO;\n+    this.encryptionManager = encryptionManager;\n+\n+    this.caseSensitive = caseSensitive;\n+    this.tableSchema = SchemaParser.toJson(table.schema());\n+\n+    String formatString = table.properties().getOrDefault(\n+        TableProperties.DEFAULT_FILE_FORMAT, TableProperties.DEFAULT_FILE_FORMAT_DEFAULT);\n+    FileFormat fileFormat = FileFormat.valueOf(formatString.toUpperCase(Locale.ENGLISH));\n+    this.writerFactory = new Writer.WriterFactory(spec, fileFormat, table.locationProvider(), table.properties(),\n+        fileIO, encryptionManager, targetDataFileSizeInBytes, table.schema(), SparkSchemaUtil.convert(table.schema()));\n+  }\n+\n+  public List<DataFile> rewriteDataForTasks(CloseableIterable<CombinedScanTask> tasks) {\n+    List<CombinedScanTask> taskList = Lists.newArrayList(tasks);\n+    int parallelism = taskList.size();\n+\n+    JavaRDD<CombinedScanTask> taskRDD = sparkContext.parallelize(taskList, parallelism);\n+    JavaRDD<Writer.TaskCommit> taskCommitRDD = taskRDD.map(task -> rewriteDataForTask(task));\n+\n+    return taskCommitRDD.collect().stream()\n+        .flatMap(taskCommit -> Arrays.stream(taskCommit.files()))\n+        .collect(Collectors.toList());\n+  }\n+\n+  private Writer.TaskCommit rewriteDataForTask(CombinedScanTask task) throws Exception {\n+    TaskContext context = TaskContext.get();\n+\n+    RowDataReader dataReader = new RowDataReader(task, SchemaParser.fromJson(tableSchema),\n+        SchemaParser.fromJson(tableSchema), fileIO.value(), encryptionManager.value(), caseSensitive);\n+\n+    int partitionId = context.partitionId();\n+    long taskId = context.taskAttemptId();\n+    long epochId = Long.parseLong(\n+        Optional.ofNullable(context.getLocalProperty(MicroBatchExecution.BATCH_ID_KEY())).orElse(\"0\"));\n+    DataWriter<InternalRow> dataWriter = writerFactory.createDataWriter(partitionId, taskId, epochId);\n+\n+    Throwable originalThrowable = null;\n+    try {\n+      while (dataReader.next()) {\n+        InternalRow row = dataReader.get();\n+        dataWriter.write(row);\n+      }\n+\n+      dataReader.close();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2dc587785ba1d1a2dd1b72aa85aea3bd30966f68"}, "originalPosition": 109}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIyMjQ1OTU1", "url": "https://github.com/apache/iceberg/pull/1083#pullrequestreview-422245955", "createdAt": "2020-06-01T23:27:15Z", "commit": {"oid": "2dc587785ba1d1a2dd1b72aa85aea3bd30966f68"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMVQyMzoyNzoxNVrOGddOmg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMVQyMzoyNzoxNVrOGddOmg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzU0MDc2Mg==", "bodyText": "Why does this set the epoch ID? I think this is unnecessary and could just default to 0.", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r433540762", "createdAt": "2020-06-01T23:27:15Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/RowDataRewriter.java", "diffHunk": "@@ -0,0 +1,138 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import java.io.Serializable;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.streaming.MicroBatchExecution;\n+import org.apache.spark.sql.sources.v2.writer.DataWriter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RowDataRewriter implements Serializable {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RowDataRewriter.class);\n+\n+  private final transient JavaSparkContext sparkContext;\n+  private final Broadcast<FileIO> fileIO;\n+  private final Broadcast<EncryptionManager> encryptionManager;\n+  private final String tableSchema;\n+  private final Writer.WriterFactory writerFactory;\n+  private final boolean caseSensitive;\n+\n+  public RowDataRewriter(Table table, JavaSparkContext sparkContext, PartitionSpec spec, boolean caseSensitive,\n+                         Broadcast<FileIO> fileIO, Broadcast<EncryptionManager> encryptionManager,\n+                         long targetDataFileSizeInBytes) {\n+    this.sparkContext = sparkContext;\n+    this.fileIO = fileIO;\n+    this.encryptionManager = encryptionManager;\n+\n+    this.caseSensitive = caseSensitive;\n+    this.tableSchema = SchemaParser.toJson(table.schema());\n+\n+    String formatString = table.properties().getOrDefault(\n+        TableProperties.DEFAULT_FILE_FORMAT, TableProperties.DEFAULT_FILE_FORMAT_DEFAULT);\n+    FileFormat fileFormat = FileFormat.valueOf(formatString.toUpperCase(Locale.ENGLISH));\n+    this.writerFactory = new Writer.WriterFactory(spec, fileFormat, table.locationProvider(), table.properties(),\n+        fileIO, encryptionManager, targetDataFileSizeInBytes, table.schema(), SparkSchemaUtil.convert(table.schema()));\n+  }\n+\n+  public List<DataFile> rewriteDataForTasks(CloseableIterable<CombinedScanTask> tasks) {\n+    List<CombinedScanTask> taskList = Lists.newArrayList(tasks);\n+    int parallelism = taskList.size();\n+\n+    JavaRDD<CombinedScanTask> taskRDD = sparkContext.parallelize(taskList, parallelism);\n+    JavaRDD<Writer.TaskCommit> taskCommitRDD = taskRDD.map(task -> rewriteDataForTask(task));\n+\n+    return taskCommitRDD.collect().stream()\n+        .flatMap(taskCommit -> Arrays.stream(taskCommit.files()))\n+        .collect(Collectors.toList());\n+  }\n+\n+  private Writer.TaskCommit rewriteDataForTask(CombinedScanTask task) throws Exception {\n+    TaskContext context = TaskContext.get();\n+\n+    RowDataReader dataReader = new RowDataReader(task, SchemaParser.fromJson(tableSchema),\n+        SchemaParser.fromJson(tableSchema), fileIO.value(), encryptionManager.value(), caseSensitive);\n+\n+    int partitionId = context.partitionId();\n+    long taskId = context.taskAttemptId();\n+    long epochId = Long.parseLong(\n+        Optional.ofNullable(context.getLocalProperty(MicroBatchExecution.BATCH_ID_KEY())).orElse(\"0\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2dc587785ba1d1a2dd1b72aa85aea3bd30966f68"}, "originalPosition": 99}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIyMjQ3MDM5", "url": "https://github.com/apache/iceberg/pull/1083#pullrequestreview-422247039", "createdAt": "2020-06-01T23:30:18Z", "commit": {"oid": "2dc587785ba1d1a2dd1b72aa85aea3bd30966f68"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMVQyMzozMDoxOFrOGddSIw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMVQyMzozMDoxOFrOGddSIw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzU0MTY2Nw==", "bodyText": "This sets the partition spec that will be used when writing new data files, but that isn't obvious from the configuration method name. How about outputSpecId or something more descriptive? I think this should also be documented for callers.", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r433541667", "createdAt": "2020-06-01T23:30:18Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,186 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.spark.source.RowDataRewriter;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RewriteDataFilesAction\n+    extends BaseSnapshotUpdateAction<RewriteDataFilesAction, RewriteDataFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteDataFilesAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final EncryptionManager encryptionManager;\n+  private final long targetDataFileSizeBytes;\n+\n+  private PartitionSpec spec = null;\n+  private Expression filter;\n+\n+  RewriteDataFilesAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.table = table;\n+    this.spec = table.spec();\n+    this.filter = Expressions.alwaysTrue();\n+    this.targetDataFileSizeBytes = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES,\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n+\n+    if (table.io() instanceof HadoopFileIO) {\n+      // we need to use Spark's SerializableConfiguration to avoid issues with Kryo serialization\n+      SerializableConfiguration conf = new SerializableConfiguration(((HadoopFileIO) table.io()).conf());\n+      this.fileIO = new HadoopFileIO(conf::value);\n+    } else {\n+      this.fileIO = table.io();\n+    }\n+    this.encryptionManager = table.encryption();\n+  }\n+\n+  @Override\n+  protected RewriteDataFilesAction self() {\n+    return this;\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  public RewriteDataFilesAction specId(int specId) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2dc587785ba1d1a2dd1b72aa85aea3bd30966f68"}, "originalPosition": 101}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIyMjUzMzE3", "url": "https://github.com/apache/iceberg/pull/1083#pullrequestreview-422253317", "createdAt": "2020-06-01T23:49:40Z", "commit": {"oid": "2dc587785ba1d1a2dd1b72aa85aea3bd30966f68"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMVQyMzo0OTo0MFrOGddmrg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMVQyMzo0OTo0MFrOGddmrg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzU0NjkyNg==", "bodyText": "This method iterates through tasks and builds the grouped result in memory. The result doesn't need to be a CloseableIterable because it is in memory and there shouldn't be any resources to close.\nThis also doesn't close tasks or the iterator used (which is what closing tasks would do). Instead, it combines each value list into a CloseableIterable with the original tasks. I think the intent was to close tasks when the result of this method is consumed. But tasks is never really closed because the collect doesn't handle a CloseableIterable any differently than an Iterable. In addition, by combining each partition list with tasks, closing each one would close tasks again, which isn't a good idea.\nLet's refactor this by passing in tasks.iterator(). That will be a CloseableIterator that should be closed after it is consumed. Like this:\ntry {\n  tasksIter.forEachRemaining(task -> {\n    ...\n  });\n} finally {\n  tasksIter.close()\n}", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r433546926", "createdAt": "2020-06-01T23:49:40Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,186 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.spark.source.RowDataRewriter;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RewriteDataFilesAction\n+    extends BaseSnapshotUpdateAction<RewriteDataFilesAction, RewriteDataFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteDataFilesAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final EncryptionManager encryptionManager;\n+  private final long targetDataFileSizeBytes;\n+\n+  private PartitionSpec spec = null;\n+  private Expression filter;\n+\n+  RewriteDataFilesAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.table = table;\n+    this.spec = table.spec();\n+    this.filter = Expressions.alwaysTrue();\n+    this.targetDataFileSizeBytes = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES,\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n+\n+    if (table.io() instanceof HadoopFileIO) {\n+      // we need to use Spark's SerializableConfiguration to avoid issues with Kryo serialization\n+      SerializableConfiguration conf = new SerializableConfiguration(((HadoopFileIO) table.io()).conf());\n+      this.fileIO = new HadoopFileIO(conf::value);\n+    } else {\n+      this.fileIO = table.io();\n+    }\n+    this.encryptionManager = table.encryption();\n+  }\n+\n+  @Override\n+  protected RewriteDataFilesAction self() {\n+    return this;\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  public RewriteDataFilesAction specId(int specId) {\n+    Preconditions.checkArgument(table.specs().containsKey(specId), \"Invalid spec id %d\", specId);\n+    this.spec = table.specs().get(specId);\n+    return this;\n+  }\n+\n+  public RewriteDataFilesAction filter(Expression expr) {\n+    this.filter = Expressions.and(filter, expr);\n+    return this;\n+  }\n+\n+  @Override\n+  public RewriteDataFilesActionResult execute() {\n+    CloseableIterable<FileScanTask> fileScanTasks = table.newScan()\n+        .filter(filter)\n+        .planFiles();\n+    List<DataFile> currentDataFiles =\n+        Lists.newArrayList(CloseableIterable.transform(fileScanTasks, FileScanTask::file));\n+    if (currentDataFiles.isEmpty()) {\n+      return RewriteDataFilesActionResult.empty();\n+    }\n+\n+    List<CloseableIterable<FileScanTask>> groupedTasks = groupTasksByPartition(fileScanTasks);\n+\n+    long splitSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    int lookbak = PropertyUtil.propertyAsInt(\n+        table.properties(),\n+        TableProperties.SPLIT_LOOKBACK,\n+        TableProperties.SPLIT_LOOKBACK_DEFAULT);\n+    long openFileCost = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_OPEN_FILE_COST,\n+        TableProperties.SPLIT_LOOKBACK_DEFAULT);\n+\n+    List<CloseableIterable<CombinedScanTask>> groupedCombinedTasks = groupedTasks.stream()\n+        .map(tasks -> {\n+          CloseableIterable<FileScanTask> splitTasks = TableScanUtil.splitFiles(tasks, splitSize);\n+          return TableScanUtil.planTasks(splitTasks, splitSize, lookbak, openFileCost);\n+        })\n+        .collect(Collectors.toList());\n+    CloseableIterable<CombinedScanTask> combinedScanTasks = CloseableIterable.concat(groupedCombinedTasks);\n+\n+    Broadcast<FileIO> io = sparkContext.broadcast(fileIO);\n+    Broadcast<EncryptionManager> encryption = sparkContext.broadcast(encryptionManager);\n+    boolean caseSensitive = Boolean.parseBoolean(spark.conf().get(\"spark.sql.caseSensitive\", \"false\"));\n+\n+    RowDataRewriter rowDataRewriter = new RowDataRewriter(table, sparkContext, spec,\n+        caseSensitive, io, encryption, targetDataFileSizeBytes);\n+    List<DataFile> addedDataFiles = rowDataRewriter.rewriteDataForTasks(combinedScanTasks);\n+\n+    replaceDataFiles(currentDataFiles, addedDataFiles);\n+\n+    return new RewriteDataFilesActionResult(currentDataFiles, addedDataFiles);\n+  }\n+\n+  private List<CloseableIterable<FileScanTask>> groupTasksByPartition(CloseableIterable<FileScanTask> tasks) {\n+    Map<StructLike, List<FileScanTask>> tasksGroupedByPartition = Maps.newHashMap();\n+\n+    tasks.forEach(task -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2dc587785ba1d1a2dd1b72aa85aea3bd30966f68"}, "originalPosition": 162}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIyMjU0ODA4", "url": "https://github.com/apache/iceberg/pull/1083#pullrequestreview-422254808", "createdAt": "2020-06-01T23:54:19Z", "commit": {"oid": "2dc587785ba1d1a2dd1b72aa85aea3bd30966f68"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMVQyMzo1NDoxOVrOGddr4w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMVQyMzo1NDoxOVrOGddr4w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzU0ODI1OQ==", "bodyText": "While I agree that using the table values as defaults is a good idea, I think this should also expose configuration methods for combined size and lookback.\nThis is important because lookback(1) is used to prevent files from being reordered. Because only one bin is open at a time, either a file fits or it doesn't so the order of files is not changed. That's important when compacting a partition written with a global ORDER BY, which will distribute the sort column data across files. Iceberg uses that distribution to prune files when users filter by those fields.", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r433548259", "createdAt": "2020-06-01T23:54:19Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,186 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.spark.source.RowDataRewriter;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RewriteDataFilesAction\n+    extends BaseSnapshotUpdateAction<RewriteDataFilesAction, RewriteDataFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteDataFilesAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final EncryptionManager encryptionManager;\n+  private final long targetDataFileSizeBytes;\n+\n+  private PartitionSpec spec = null;\n+  private Expression filter;\n+\n+  RewriteDataFilesAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.table = table;\n+    this.spec = table.spec();\n+    this.filter = Expressions.alwaysTrue();\n+    this.targetDataFileSizeBytes = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES,\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n+\n+    if (table.io() instanceof HadoopFileIO) {\n+      // we need to use Spark's SerializableConfiguration to avoid issues with Kryo serialization\n+      SerializableConfiguration conf = new SerializableConfiguration(((HadoopFileIO) table.io()).conf());\n+      this.fileIO = new HadoopFileIO(conf::value);\n+    } else {\n+      this.fileIO = table.io();\n+    }\n+    this.encryptionManager = table.encryption();\n+  }\n+\n+  @Override\n+  protected RewriteDataFilesAction self() {\n+    return this;\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  public RewriteDataFilesAction specId(int specId) {\n+    Preconditions.checkArgument(table.specs().containsKey(specId), \"Invalid spec id %d\", specId);\n+    this.spec = table.specs().get(specId);\n+    return this;\n+  }\n+\n+  public RewriteDataFilesAction filter(Expression expr) {\n+    this.filter = Expressions.and(filter, expr);\n+    return this;\n+  }\n+\n+  @Override\n+  public RewriteDataFilesActionResult execute() {\n+    CloseableIterable<FileScanTask> fileScanTasks = table.newScan()\n+        .filter(filter)\n+        .planFiles();\n+    List<DataFile> currentDataFiles =\n+        Lists.newArrayList(CloseableIterable.transform(fileScanTasks, FileScanTask::file));\n+    if (currentDataFiles.isEmpty()) {\n+      return RewriteDataFilesActionResult.empty();\n+    }\n+\n+    List<CloseableIterable<FileScanTask>> groupedTasks = groupTasksByPartition(fileScanTasks);\n+\n+    long splitSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    int lookbak = PropertyUtil.propertyAsInt(\n+        table.properties(),\n+        TableProperties.SPLIT_LOOKBACK,\n+        TableProperties.SPLIT_LOOKBACK_DEFAULT);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2dc587785ba1d1a2dd1b72aa85aea3bd30966f68"}, "originalPosition": 132}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIyMjU4MTQ0", "url": "https://github.com/apache/iceberg/pull/1083#pullrequestreview-422258144", "createdAt": "2020-06-02T00:04:48Z", "commit": {"oid": "2dc587785ba1d1a2dd1b72aa85aea3bd30966f68"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQwMDowNDo0OFrOGdd3KQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQwMDowNDo0OFrOGdd3KQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzU1MTE0NQ==", "bodyText": "We typically use the boolean threw pattern instead of using a broad catch like this. The purpose is to still try to clean up even if the problem wasn't an Exception (like AssertionError):\nboolean threw = true;\ntry {\n  RewriteFiles = table.newRewrite().rewriteFiles(...).commit()\n  threw = false;\n} finally {\n  if (threw) {\n    Tasks.foreach(...)\n        .run(fileIO::deleteFile);\n  }\n}", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r433551145", "createdAt": "2020-06-02T00:04:48Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,186 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.spark.source.RowDataRewriter;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RewriteDataFilesAction\n+    extends BaseSnapshotUpdateAction<RewriteDataFilesAction, RewriteDataFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteDataFilesAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final EncryptionManager encryptionManager;\n+  private final long targetDataFileSizeBytes;\n+\n+  private PartitionSpec spec = null;\n+  private Expression filter;\n+\n+  RewriteDataFilesAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.table = table;\n+    this.spec = table.spec();\n+    this.filter = Expressions.alwaysTrue();\n+    this.targetDataFileSizeBytes = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES,\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n+\n+    if (table.io() instanceof HadoopFileIO) {\n+      // we need to use Spark's SerializableConfiguration to avoid issues with Kryo serialization\n+      SerializableConfiguration conf = new SerializableConfiguration(((HadoopFileIO) table.io()).conf());\n+      this.fileIO = new HadoopFileIO(conf::value);\n+    } else {\n+      this.fileIO = table.io();\n+    }\n+    this.encryptionManager = table.encryption();\n+  }\n+\n+  @Override\n+  protected RewriteDataFilesAction self() {\n+    return this;\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  public RewriteDataFilesAction specId(int specId) {\n+    Preconditions.checkArgument(table.specs().containsKey(specId), \"Invalid spec id %d\", specId);\n+    this.spec = table.specs().get(specId);\n+    return this;\n+  }\n+\n+  public RewriteDataFilesAction filter(Expression expr) {\n+    this.filter = Expressions.and(filter, expr);\n+    return this;\n+  }\n+\n+  @Override\n+  public RewriteDataFilesActionResult execute() {\n+    CloseableIterable<FileScanTask> fileScanTasks = table.newScan()\n+        .filter(filter)\n+        .planFiles();\n+    List<DataFile> currentDataFiles =\n+        Lists.newArrayList(CloseableIterable.transform(fileScanTasks, FileScanTask::file));\n+    if (currentDataFiles.isEmpty()) {\n+      return RewriteDataFilesActionResult.empty();\n+    }\n+\n+    List<CloseableIterable<FileScanTask>> groupedTasks = groupTasksByPartition(fileScanTasks);\n+\n+    long splitSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    int lookbak = PropertyUtil.propertyAsInt(\n+        table.properties(),\n+        TableProperties.SPLIT_LOOKBACK,\n+        TableProperties.SPLIT_LOOKBACK_DEFAULT);\n+    long openFileCost = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_OPEN_FILE_COST,\n+        TableProperties.SPLIT_LOOKBACK_DEFAULT);\n+\n+    List<CloseableIterable<CombinedScanTask>> groupedCombinedTasks = groupedTasks.stream()\n+        .map(tasks -> {\n+          CloseableIterable<FileScanTask> splitTasks = TableScanUtil.splitFiles(tasks, splitSize);\n+          return TableScanUtil.planTasks(splitTasks, splitSize, lookbak, openFileCost);\n+        })\n+        .collect(Collectors.toList());\n+    CloseableIterable<CombinedScanTask> combinedScanTasks = CloseableIterable.concat(groupedCombinedTasks);\n+\n+    Broadcast<FileIO> io = sparkContext.broadcast(fileIO);\n+    Broadcast<EncryptionManager> encryption = sparkContext.broadcast(encryptionManager);\n+    boolean caseSensitive = Boolean.parseBoolean(spark.conf().get(\"spark.sql.caseSensitive\", \"false\"));\n+\n+    RowDataRewriter rowDataRewriter = new RowDataRewriter(table, sparkContext, spec,\n+        caseSensitive, io, encryption, targetDataFileSizeBytes);\n+    List<DataFile> addedDataFiles = rowDataRewriter.rewriteDataForTasks(combinedScanTasks);\n+\n+    replaceDataFiles(currentDataFiles, addedDataFiles);\n+\n+    return new RewriteDataFilesActionResult(currentDataFiles, addedDataFiles);\n+  }\n+\n+  private List<CloseableIterable<FileScanTask>> groupTasksByPartition(CloseableIterable<FileScanTask> tasks) {\n+    Map<StructLike, List<FileScanTask>> tasksGroupedByPartition = Maps.newHashMap();\n+\n+    tasks.forEach(task -> {\n+      List<FileScanTask> taskList = tasksGroupedByPartition.getOrDefault(task.file().partition(), Lists.newArrayList());\n+      taskList.add(task);\n+      tasksGroupedByPartition.put(task.file().partition(), taskList);\n+    });\n+\n+    return tasksGroupedByPartition.values().stream()\n+        .map(list -> CloseableIterable.combine(list, tasks))\n+        .collect(Collectors.toList());\n+  }\n+\n+  private void replaceDataFiles(Iterable<DataFile> deletedDataFiles, Iterable<DataFile> addedDataFiles) {\n+    try {\n+      RewriteFiles rewriteFiles = table.newRewrite();\n+      rewriteFiles.rewriteFiles(Sets.newHashSet(deletedDataFiles), Sets.newHashSet(addedDataFiles));\n+      commit(rewriteFiles);\n+    } catch (Exception e) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2dc587785ba1d1a2dd1b72aa85aea3bd30966f68"}, "originalPosition": 178}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIyMjU5NjE3", "url": "https://github.com/apache/iceberg/pull/1083#pullrequestreview-422259617", "createdAt": "2020-06-02T00:09:08Z", "commit": {"oid": "2dc587785ba1d1a2dd1b72aa85aea3bd30966f68"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQwMDowOTowOFrOGdd8Dg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQwMDowOTowOFrOGdd8Dg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzU1MjM5OA==", "bodyText": "The purpose of this class doesn't look well-defined. The Spark context is passed in, but so are already broadcasted variables. The main function here looks like it handles calling the DSv2 components and is serializable. If that's the case, then I think it would be cleaner to refactor the Spark context out and just pass in an RDD of CombinedScanTask.", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r433552398", "createdAt": "2020-06-02T00:09:08Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/RowDataRewriter.java", "diffHunk": "@@ -0,0 +1,138 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import java.io.Serializable;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.streaming.MicroBatchExecution;\n+import org.apache.spark.sql.sources.v2.writer.DataWriter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RowDataRewriter implements Serializable {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RowDataRewriter.class);\n+\n+  private final transient JavaSparkContext sparkContext;\n+  private final Broadcast<FileIO> fileIO;\n+  private final Broadcast<EncryptionManager> encryptionManager;\n+  private final String tableSchema;\n+  private final Writer.WriterFactory writerFactory;\n+  private final boolean caseSensitive;\n+\n+  public RowDataRewriter(Table table, JavaSparkContext sparkContext, PartitionSpec spec, boolean caseSensitive,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2dc587785ba1d1a2dd1b72aa85aea3bd30966f68"}, "originalPosition": 61}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIyMjU5OTE0", "url": "https://github.com/apache/iceberg/pull/1083#pullrequestreview-422259914", "createdAt": "2020-06-02T00:09:58Z", "commit": {"oid": "2dc587785ba1d1a2dd1b72aa85aea3bd30966f68"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQwMDowOTo1OFrOGdd86w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQwMDowOTo1OFrOGdd86w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzU1MjYxOQ==", "bodyText": "You can omit empty constructors.", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r433552619", "createdAt": "2020-06-02T00:09:58Z", "author": {"login": "rdblue"}, "path": "spark/src/test/java/org/apache/iceberg/actions/TestRewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,300 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.File;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.source.ThreeColumnRecord;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+\n+public class TestRewriteDataFilesAction {\n+\n+  private static final HadoopTables TABLES = new HadoopTables(new Configuration());\n+  private static final Schema SCHEMA = new Schema(\n+      optional(1, \"c1\", Types.IntegerType.get()),\n+      optional(2, \"c2\", Types.StringType.get()),\n+      optional(3, \"c3\", Types.StringType.get())\n+  );\n+\n+  private static SparkSession spark;\n+\n+  @BeforeClass\n+  public static void startSpark() {\n+    TestRewriteDataFilesAction.spark = SparkSession.builder()\n+        .master(\"local[2]\")\n+        .getOrCreate();\n+  }\n+\n+  @AfterClass\n+  public static void stopSpark() {\n+    SparkSession currentSpark = TestRewriteDataFilesAction.spark;\n+    TestRewriteDataFilesAction.spark = null;\n+    currentSpark.stop();\n+  }\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  private String tableLocation = null;\n+\n+  public TestRewriteDataFilesAction() {\n+  }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2dc587785ba1d1a2dd1b72aa85aea3bd30966f68"}, "originalPosition": 83}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIyMjYzMTky", "url": "https://github.com/apache/iceberg/pull/1083#pullrequestreview-422263192", "createdAt": "2020-06-02T00:19:46Z", "commit": {"oid": "2dc587785ba1d1a2dd1b72aa85aea3bd30966f68"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQwMDoxOTo0NlrOGdeHWQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQwMDoxOTo0NlrOGdeHWQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzU1NTI4OQ==", "bodyText": "This is going to iterate through fileScanTasks before groupTasksByPartition also iterates through fileScanTasks, so it will cause job planning to happen twice. Instead, I think that this check should be moved after groupTasksByPartition, which would return a Map. If the map has no entries, then there is no work to do.\nThis should also be more aggressive. If a partition has just one file to compact, then that partition should be removed because there is no work to do.", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r433555289", "createdAt": "2020-06-02T00:19:46Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,186 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.spark.source.RowDataRewriter;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RewriteDataFilesAction\n+    extends BaseSnapshotUpdateAction<RewriteDataFilesAction, RewriteDataFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteDataFilesAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final EncryptionManager encryptionManager;\n+  private final long targetDataFileSizeBytes;\n+\n+  private PartitionSpec spec = null;\n+  private Expression filter;\n+\n+  RewriteDataFilesAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.table = table;\n+    this.spec = table.spec();\n+    this.filter = Expressions.alwaysTrue();\n+    this.targetDataFileSizeBytes = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES,\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n+\n+    if (table.io() instanceof HadoopFileIO) {\n+      // we need to use Spark's SerializableConfiguration to avoid issues with Kryo serialization\n+      SerializableConfiguration conf = new SerializableConfiguration(((HadoopFileIO) table.io()).conf());\n+      this.fileIO = new HadoopFileIO(conf::value);\n+    } else {\n+      this.fileIO = table.io();\n+    }\n+    this.encryptionManager = table.encryption();\n+  }\n+\n+  @Override\n+  protected RewriteDataFilesAction self() {\n+    return this;\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  public RewriteDataFilesAction specId(int specId) {\n+    Preconditions.checkArgument(table.specs().containsKey(specId), \"Invalid spec id %d\", specId);\n+    this.spec = table.specs().get(specId);\n+    return this;\n+  }\n+\n+  public RewriteDataFilesAction filter(Expression expr) {\n+    this.filter = Expressions.and(filter, expr);\n+    return this;\n+  }\n+\n+  @Override\n+  public RewriteDataFilesActionResult execute() {\n+    CloseableIterable<FileScanTask> fileScanTasks = table.newScan()\n+        .filter(filter)\n+        .planFiles();\n+    List<DataFile> currentDataFiles =\n+        Lists.newArrayList(CloseableIterable.transform(fileScanTasks, FileScanTask::file));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2dc587785ba1d1a2dd1b72aa85aea3bd30966f68"}, "originalPosition": 118}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIyMjYzNjAx", "url": "https://github.com/apache/iceberg/pull/1083#pullrequestreview-422263601", "createdAt": "2020-06-02T00:21:09Z", "commit": {"oid": "2dc587785ba1d1a2dd1b72aa85aea3bd30966f68"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQwMDoyMTowOVrOGdeIoA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQwMDoyMTowOVrOGdeIoA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzU1NTYxNg==", "bodyText": "This should use StructLikeWrapper for the key instead of StructLike to handle CharSequence values correctly.", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r433555616", "createdAt": "2020-06-02T00:21:09Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,186 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.spark.source.RowDataRewriter;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RewriteDataFilesAction\n+    extends BaseSnapshotUpdateAction<RewriteDataFilesAction, RewriteDataFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteDataFilesAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final EncryptionManager encryptionManager;\n+  private final long targetDataFileSizeBytes;\n+\n+  private PartitionSpec spec = null;\n+  private Expression filter;\n+\n+  RewriteDataFilesAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.table = table;\n+    this.spec = table.spec();\n+    this.filter = Expressions.alwaysTrue();\n+    this.targetDataFileSizeBytes = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES,\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n+\n+    if (table.io() instanceof HadoopFileIO) {\n+      // we need to use Spark's SerializableConfiguration to avoid issues with Kryo serialization\n+      SerializableConfiguration conf = new SerializableConfiguration(((HadoopFileIO) table.io()).conf());\n+      this.fileIO = new HadoopFileIO(conf::value);\n+    } else {\n+      this.fileIO = table.io();\n+    }\n+    this.encryptionManager = table.encryption();\n+  }\n+\n+  @Override\n+  protected RewriteDataFilesAction self() {\n+    return this;\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  public RewriteDataFilesAction specId(int specId) {\n+    Preconditions.checkArgument(table.specs().containsKey(specId), \"Invalid spec id %d\", specId);\n+    this.spec = table.specs().get(specId);\n+    return this;\n+  }\n+\n+  public RewriteDataFilesAction filter(Expression expr) {\n+    this.filter = Expressions.and(filter, expr);\n+    return this;\n+  }\n+\n+  @Override\n+  public RewriteDataFilesActionResult execute() {\n+    CloseableIterable<FileScanTask> fileScanTasks = table.newScan()\n+        .filter(filter)\n+        .planFiles();\n+    List<DataFile> currentDataFiles =\n+        Lists.newArrayList(CloseableIterable.transform(fileScanTasks, FileScanTask::file));\n+    if (currentDataFiles.isEmpty()) {\n+      return RewriteDataFilesActionResult.empty();\n+    }\n+\n+    List<CloseableIterable<FileScanTask>> groupedTasks = groupTasksByPartition(fileScanTasks);\n+\n+    long splitSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    int lookbak = PropertyUtil.propertyAsInt(\n+        table.properties(),\n+        TableProperties.SPLIT_LOOKBACK,\n+        TableProperties.SPLIT_LOOKBACK_DEFAULT);\n+    long openFileCost = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_OPEN_FILE_COST,\n+        TableProperties.SPLIT_LOOKBACK_DEFAULT);\n+\n+    List<CloseableIterable<CombinedScanTask>> groupedCombinedTasks = groupedTasks.stream()\n+        .map(tasks -> {\n+          CloseableIterable<FileScanTask> splitTasks = TableScanUtil.splitFiles(tasks, splitSize);\n+          return TableScanUtil.planTasks(splitTasks, splitSize, lookbak, openFileCost);\n+        })\n+        .collect(Collectors.toList());\n+    CloseableIterable<CombinedScanTask> combinedScanTasks = CloseableIterable.concat(groupedCombinedTasks);\n+\n+    Broadcast<FileIO> io = sparkContext.broadcast(fileIO);\n+    Broadcast<EncryptionManager> encryption = sparkContext.broadcast(encryptionManager);\n+    boolean caseSensitive = Boolean.parseBoolean(spark.conf().get(\"spark.sql.caseSensitive\", \"false\"));\n+\n+    RowDataRewriter rowDataRewriter = new RowDataRewriter(table, sparkContext, spec,\n+        caseSensitive, io, encryption, targetDataFileSizeBytes);\n+    List<DataFile> addedDataFiles = rowDataRewriter.rewriteDataForTasks(combinedScanTasks);\n+\n+    replaceDataFiles(currentDataFiles, addedDataFiles);\n+\n+    return new RewriteDataFilesActionResult(currentDataFiles, addedDataFiles);\n+  }\n+\n+  private List<CloseableIterable<FileScanTask>> groupTasksByPartition(CloseableIterable<FileScanTask> tasks) {\n+    Map<StructLike, List<FileScanTask>> tasksGroupedByPartition = Maps.newHashMap();\n+\n+    tasks.forEach(task -> {\n+      List<FileScanTask> taskList = tasksGroupedByPartition.getOrDefault(task.file().partition(), Lists.newArrayList());\n+      taskList.add(task);\n+      tasksGroupedByPartition.put(task.file().partition(), taskList);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2dc587785ba1d1a2dd1b72aa85aea3bd30966f68"}, "originalPosition": 165}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIyNjAzNDY1", "url": "https://github.com/apache/iceberg/pull/1083#pullrequestreview-422603465", "createdAt": "2020-06-02T11:58:09Z", "commit": {"oid": "2dc587785ba1d1a2dd1b72aa85aea3bd30966f68"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQxMTo1ODowOVrOGduUXw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQxMjoyMjoyMlrOGdvF7Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzgyMDc2Nw==", "bodyText": "typo", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r433820767", "createdAt": "2020-06-02T11:58:09Z", "author": {"login": "Jiayi-Liao"}, "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,186 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.spark.source.RowDataRewriter;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RewriteDataFilesAction\n+    extends BaseSnapshotUpdateAction<RewriteDataFilesAction, RewriteDataFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteDataFilesAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final EncryptionManager encryptionManager;\n+  private final long targetDataFileSizeBytes;\n+\n+  private PartitionSpec spec = null;\n+  private Expression filter;\n+\n+  RewriteDataFilesAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.table = table;\n+    this.spec = table.spec();\n+    this.filter = Expressions.alwaysTrue();\n+    this.targetDataFileSizeBytes = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES,\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n+\n+    if (table.io() instanceof HadoopFileIO) {\n+      // we need to use Spark's SerializableConfiguration to avoid issues with Kryo serialization\n+      SerializableConfiguration conf = new SerializableConfiguration(((HadoopFileIO) table.io()).conf());\n+      this.fileIO = new HadoopFileIO(conf::value);\n+    } else {\n+      this.fileIO = table.io();\n+    }\n+    this.encryptionManager = table.encryption();\n+  }\n+\n+  @Override\n+  protected RewriteDataFilesAction self() {\n+    return this;\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  public RewriteDataFilesAction specId(int specId) {\n+    Preconditions.checkArgument(table.specs().containsKey(specId), \"Invalid spec id %d\", specId);\n+    this.spec = table.specs().get(specId);\n+    return this;\n+  }\n+\n+  public RewriteDataFilesAction filter(Expression expr) {\n+    this.filter = Expressions.and(filter, expr);\n+    return this;\n+  }\n+\n+  @Override\n+  public RewriteDataFilesActionResult execute() {\n+    CloseableIterable<FileScanTask> fileScanTasks = table.newScan()\n+        .filter(filter)\n+        .planFiles();\n+    List<DataFile> currentDataFiles =\n+        Lists.newArrayList(CloseableIterable.transform(fileScanTasks, FileScanTask::file));\n+    if (currentDataFiles.isEmpty()) {\n+      return RewriteDataFilesActionResult.empty();\n+    }\n+\n+    List<CloseableIterable<FileScanTask>> groupedTasks = groupTasksByPartition(fileScanTasks);\n+\n+    long splitSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    int lookbak = PropertyUtil.propertyAsInt(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2dc587785ba1d1a2dd1b72aa85aea3bd30966f68"}, "originalPosition": 129}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzgyMTE1NA==", "bodyText": "shoudn't be SPLIT_OPEN_FILE_COST_DEFAULT?", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r433821154", "createdAt": "2020-06-02T11:58:58Z", "author": {"login": "Jiayi-Liao"}, "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,186 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.spark.source.RowDataRewriter;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RewriteDataFilesAction\n+    extends BaseSnapshotUpdateAction<RewriteDataFilesAction, RewriteDataFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteDataFilesAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final EncryptionManager encryptionManager;\n+  private final long targetDataFileSizeBytes;\n+\n+  private PartitionSpec spec = null;\n+  private Expression filter;\n+\n+  RewriteDataFilesAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.table = table;\n+    this.spec = table.spec();\n+    this.filter = Expressions.alwaysTrue();\n+    this.targetDataFileSizeBytes = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES,\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n+\n+    if (table.io() instanceof HadoopFileIO) {\n+      // we need to use Spark's SerializableConfiguration to avoid issues with Kryo serialization\n+      SerializableConfiguration conf = new SerializableConfiguration(((HadoopFileIO) table.io()).conf());\n+      this.fileIO = new HadoopFileIO(conf::value);\n+    } else {\n+      this.fileIO = table.io();\n+    }\n+    this.encryptionManager = table.encryption();\n+  }\n+\n+  @Override\n+  protected RewriteDataFilesAction self() {\n+    return this;\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  public RewriteDataFilesAction specId(int specId) {\n+    Preconditions.checkArgument(table.specs().containsKey(specId), \"Invalid spec id %d\", specId);\n+    this.spec = table.specs().get(specId);\n+    return this;\n+  }\n+\n+  public RewriteDataFilesAction filter(Expression expr) {\n+    this.filter = Expressions.and(filter, expr);\n+    return this;\n+  }\n+\n+  @Override\n+  public RewriteDataFilesActionResult execute() {\n+    CloseableIterable<FileScanTask> fileScanTasks = table.newScan()\n+        .filter(filter)\n+        .planFiles();\n+    List<DataFile> currentDataFiles =\n+        Lists.newArrayList(CloseableIterable.transform(fileScanTasks, FileScanTask::file));\n+    if (currentDataFiles.isEmpty()) {\n+      return RewriteDataFilesActionResult.empty();\n+    }\n+\n+    List<CloseableIterable<FileScanTask>> groupedTasks = groupTasksByPartition(fileScanTasks);\n+\n+    long splitSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    int lookbak = PropertyUtil.propertyAsInt(\n+        table.properties(),\n+        TableProperties.SPLIT_LOOKBACK,\n+        TableProperties.SPLIT_LOOKBACK_DEFAULT);\n+    long openFileCost = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_OPEN_FILE_COST,\n+        TableProperties.SPLIT_LOOKBACK_DEFAULT);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2dc587785ba1d1a2dd1b72aa85aea3bd30966f68"}, "originalPosition": 136}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzgzMzQ1Mw==", "bodyText": "Same as Line 118?", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r433833453", "createdAt": "2020-06-02T12:22:22Z", "author": {"login": "Jiayi-Liao"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/RowDataRewriter.java", "diffHunk": "@@ -0,0 +1,138 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import java.io.Serializable;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.streaming.MicroBatchExecution;\n+import org.apache.spark.sql.sources.v2.writer.DataWriter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RowDataRewriter implements Serializable {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RowDataRewriter.class);\n+\n+  private final transient JavaSparkContext sparkContext;\n+  private final Broadcast<FileIO> fileIO;\n+  private final Broadcast<EncryptionManager> encryptionManager;\n+  private final String tableSchema;\n+  private final Writer.WriterFactory writerFactory;\n+  private final boolean caseSensitive;\n+\n+  public RowDataRewriter(Table table, JavaSparkContext sparkContext, PartitionSpec spec, boolean caseSensitive,\n+                         Broadcast<FileIO> fileIO, Broadcast<EncryptionManager> encryptionManager,\n+                         long targetDataFileSizeInBytes) {\n+    this.sparkContext = sparkContext;\n+    this.fileIO = fileIO;\n+    this.encryptionManager = encryptionManager;\n+\n+    this.caseSensitive = caseSensitive;\n+    this.tableSchema = SchemaParser.toJson(table.schema());\n+\n+    String formatString = table.properties().getOrDefault(\n+        TableProperties.DEFAULT_FILE_FORMAT, TableProperties.DEFAULT_FILE_FORMAT_DEFAULT);\n+    FileFormat fileFormat = FileFormat.valueOf(formatString.toUpperCase(Locale.ENGLISH));\n+    this.writerFactory = new Writer.WriterFactory(spec, fileFormat, table.locationProvider(), table.properties(),\n+        fileIO, encryptionManager, targetDataFileSizeInBytes, table.schema(), SparkSchemaUtil.convert(table.schema()));\n+  }\n+\n+  public List<DataFile> rewriteDataForTasks(CloseableIterable<CombinedScanTask> tasks) {\n+    List<CombinedScanTask> taskList = Lists.newArrayList(tasks);\n+    int parallelism = taskList.size();\n+\n+    JavaRDD<CombinedScanTask> taskRDD = sparkContext.parallelize(taskList, parallelism);\n+    JavaRDD<Writer.TaskCommit> taskCommitRDD = taskRDD.map(task -> rewriteDataForTask(task));\n+\n+    return taskCommitRDD.collect().stream()\n+        .flatMap(taskCommit -> Arrays.stream(taskCommit.files()))\n+        .collect(Collectors.toList());\n+  }\n+\n+  private Writer.TaskCommit rewriteDataForTask(CombinedScanTask task) throws Exception {\n+    TaskContext context = TaskContext.get();\n+\n+    RowDataReader dataReader = new RowDataReader(task, SchemaParser.fromJson(tableSchema),\n+        SchemaParser.fromJson(tableSchema), fileIO.value(), encryptionManager.value(), caseSensitive);\n+\n+    int partitionId = context.partitionId();\n+    long taskId = context.taskAttemptId();\n+    long epochId = Long.parseLong(\n+        Optional.ofNullable(context.getLocalProperty(MicroBatchExecution.BATCH_ID_KEY())).orElse(\"0\"));\n+    DataWriter<InternalRow> dataWriter = writerFactory.createDataWriter(partitionId, taskId, epochId);\n+\n+    Throwable originalThrowable = null;\n+    try {\n+      while (dataReader.next()) {\n+        InternalRow row = dataReader.get();\n+        dataWriter.write(row);\n+      }\n+\n+      dataReader.close();\n+      return (Writer.TaskCommit) dataWriter.commit();\n+\n+    } catch (Throwable t) {\n+      originalThrowable = t;\n+      try {\n+        LOG.error(\"Aborting task\", originalThrowable);\n+        context.markTaskFailed(originalThrowable);\n+\n+        LOG.error(\"Aborting commit for partition {} (task {}, attempt {}, stage {}.{})\",\n+            partitionId, taskId, context.attemptNumber(), context.stageId(), context.stageAttemptNumber());\n+        dataReader.close();\n+        dataWriter.abort();\n+        LOG.error(\"Aborted commit for partition {} (task {}, attempt {}, stage {}.{})\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2dc587785ba1d1a2dd1b72aa85aea3bd30966f68"}, "originalPosition": 122}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "2dc587785ba1d1a2dd1b72aa85aea3bd30966f68", "author": {"user": {"login": "jerryshao", "name": "Saisai Shao"}}, "url": "https://github.com/apache/iceberg/commit/2dc587785ba1d1a2dd1b72aa85aea3bd30966f68", "committedDate": "2020-06-01T06:35:31Z", "message": "Fix guava relocate issue"}, "afterCommit": {"oid": "dca22e0fae297feb6bddcf4d36ec76f2efdaeccc", "author": {"user": {"login": "jerryshao", "name": "Saisai Shao"}}, "url": "https://github.com/apache/iceberg/commit/dca22e0fae297feb6bddcf4d36ec76f2efdaeccc", "committedDate": "2020-06-02T13:21:46Z", "message": "Address the comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIyOTE2ODE4", "url": "https://github.com/apache/iceberg/pull/1083#pullrequestreview-422916818", "createdAt": "2020-06-02T17:50:15Z", "commit": {"oid": "dca22e0fae297feb6bddcf4d36ec76f2efdaeccc"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQxNzo1MDoxNVrOGd9DWg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQxNzo1MDoxNVrOGd9DWg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDA2MjE3MA==", "bodyText": "I think that rewriteDataFile is redundant because this is a rewrite data files action. How about targetSizeInBytes instead?", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r434062170", "createdAt": "2020-06-02T17:50:15Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,268 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.relocated.com.google.common.collect.Streams;\n+import org.apache.iceberg.spark.source.RowDataRewriter;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.StructLikeWrapper;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RewriteDataFilesAction\n+    extends BaseSnapshotUpdateAction<RewriteDataFilesAction, RewriteDataFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteDataFilesAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final EncryptionManager encryptionManager;\n+  private long rewriteDataFileSizeInBytes;\n+  private int splitLookback;\n+\n+  private PartitionSpec spec = null;\n+  private Expression filter;\n+\n+  RewriteDataFilesAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.table = table;\n+    this.spec = table.spec();\n+    this.filter = Expressions.alwaysTrue();\n+\n+    long splitSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    long targetFileSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES,\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n+    this.rewriteDataFileSizeInBytes = Math.min(splitSize, targetFileSize);\n+\n+    this.rewriteDataFileSizeInBytes = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    this.splitLookback = PropertyUtil.propertyAsInt(\n+        table.properties(),\n+        TableProperties.SPLIT_LOOKBACK,\n+        TableProperties.SPLIT_LOOKBACK_DEFAULT);\n+\n+    if (table.io() instanceof HadoopFileIO) {\n+      // we need to use Spark's SerializableConfiguration to avoid issues with Kryo serialization\n+      SerializableConfiguration conf = new SerializableConfiguration(((HadoopFileIO) table.io()).conf());\n+      this.fileIO = new HadoopFileIO(conf::value);\n+    } else {\n+      this.fileIO = table.io();\n+    }\n+    this.encryptionManager = table.encryption();\n+  }\n+\n+  @Override\n+  protected RewriteDataFilesAction self() {\n+    return this;\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  /**\n+   * Pass a PartitionSepc id to specify which PartitionSpec should be used in DataFile rewrite\n+   *\n+   * @param specId PartitionSpec id to rewrite\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction outputSpecId(int specId) {\n+    Preconditions.checkArgument(table.specs().containsKey(specId), \"Invalid spec id %d\", specId);\n+    this.spec = table.specs().get(specId);\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the rewrite data file size in bytes\n+   *\n+   * @param rewriteDataFileSize size of rewrite data file\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction rewriteDataFileSizeInBytes(long rewriteDataFileSize) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dca22e0fae297feb6bddcf4d36ec76f2efdaeccc"}, "originalPosition": 139}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIyOTIzMTA4", "url": "https://github.com/apache/iceberg/pull/1083#pullrequestreview-422923108", "createdAt": "2020-06-02T17:55:05Z", "commit": {"oid": "dca22e0fae297feb6bddcf4d36ec76f2efdaeccc"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQxNzo1NTowNVrOGd9ObQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQxNzo1NTowNVrOGd9ObQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDA2NTAwNQ==", "bodyText": "I think it would help to give a bit more context here. This is the number of \"bins\" considered when trying to pack the next file split into a task. Increasing this usually makes tasks a bit more even by considering more ways to pack file regions into a single task, but with extra planning cost.\nThis should also mention that the bin packing can reorder the incoming file regions, so to preserve order for lower/upper bounds in file metadata, the caller can use a lookback of 1.", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r434065005", "createdAt": "2020-06-02T17:55:05Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,268 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.relocated.com.google.common.collect.Streams;\n+import org.apache.iceberg.spark.source.RowDataRewriter;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.StructLikeWrapper;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RewriteDataFilesAction\n+    extends BaseSnapshotUpdateAction<RewriteDataFilesAction, RewriteDataFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteDataFilesAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final EncryptionManager encryptionManager;\n+  private long rewriteDataFileSizeInBytes;\n+  private int splitLookback;\n+\n+  private PartitionSpec spec = null;\n+  private Expression filter;\n+\n+  RewriteDataFilesAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.table = table;\n+    this.spec = table.spec();\n+    this.filter = Expressions.alwaysTrue();\n+\n+    long splitSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    long targetFileSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES,\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n+    this.rewriteDataFileSizeInBytes = Math.min(splitSize, targetFileSize);\n+\n+    this.rewriteDataFileSizeInBytes = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    this.splitLookback = PropertyUtil.propertyAsInt(\n+        table.properties(),\n+        TableProperties.SPLIT_LOOKBACK,\n+        TableProperties.SPLIT_LOOKBACK_DEFAULT);\n+\n+    if (table.io() instanceof HadoopFileIO) {\n+      // we need to use Spark's SerializableConfiguration to avoid issues with Kryo serialization\n+      SerializableConfiguration conf = new SerializableConfiguration(((HadoopFileIO) table.io()).conf());\n+      this.fileIO = new HadoopFileIO(conf::value);\n+    } else {\n+      this.fileIO = table.io();\n+    }\n+    this.encryptionManager = table.encryption();\n+  }\n+\n+  @Override\n+  protected RewriteDataFilesAction self() {\n+    return this;\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  /**\n+   * Pass a PartitionSepc id to specify which PartitionSpec should be used in DataFile rewrite\n+   *\n+   * @param specId PartitionSpec id to rewrite\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction outputSpecId(int specId) {\n+    Preconditions.checkArgument(table.specs().containsKey(specId), \"Invalid spec id %d\", specId);\n+    this.spec = table.specs().get(specId);\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the rewrite data file size in bytes\n+   *\n+   * @param rewriteDataFileSize size of rewrite data file\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction rewriteDataFileSizeInBytes(long rewriteDataFileSize) {\n+    Preconditions.checkArgument(rewriteDataFileSize > 0L, \"Invalid rewrite data file size in bytes %d\",\n+        rewriteDataFileSize);\n+    this.rewriteDataFileSizeInBytes = rewriteDataFileSize;\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the split lookback", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dca22e0fae297feb6bddcf4d36ec76f2efdaeccc"}, "originalPosition": 147}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIyOTI5MDEy", "url": "https://github.com/apache/iceberg/pull/1083#pullrequestreview-422929012", "createdAt": "2020-06-02T17:58:42Z", "commit": {"oid": "dca22e0fae297feb6bddcf4d36ec76f2efdaeccc"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQxNzo1ODo0MlrOGd9XHA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQxNzo1ODo0MlrOGd9XHA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDA2NzIyOA==", "bodyText": "I think it is important to mention that all files that may contain data matching the filter may be rewritten.\nWhen planning, the filter is used to match files inclusively: if a file may contain a matching row, the file matches. When matching file stats, the same inclusive approach is used. So if a user has a bucked table, bucket(id, 16) and asks to rewrite where id = X, then this will match any data file in id_bucket = bucket(X, 16).", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r434067228", "createdAt": "2020-06-02T17:58:42Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,268 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.relocated.com.google.common.collect.Streams;\n+import org.apache.iceberg.spark.source.RowDataRewriter;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.StructLikeWrapper;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RewriteDataFilesAction\n+    extends BaseSnapshotUpdateAction<RewriteDataFilesAction, RewriteDataFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteDataFilesAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final EncryptionManager encryptionManager;\n+  private long rewriteDataFileSizeInBytes;\n+  private int splitLookback;\n+\n+  private PartitionSpec spec = null;\n+  private Expression filter;\n+\n+  RewriteDataFilesAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.table = table;\n+    this.spec = table.spec();\n+    this.filter = Expressions.alwaysTrue();\n+\n+    long splitSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    long targetFileSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES,\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n+    this.rewriteDataFileSizeInBytes = Math.min(splitSize, targetFileSize);\n+\n+    this.rewriteDataFileSizeInBytes = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    this.splitLookback = PropertyUtil.propertyAsInt(\n+        table.properties(),\n+        TableProperties.SPLIT_LOOKBACK,\n+        TableProperties.SPLIT_LOOKBACK_DEFAULT);\n+\n+    if (table.io() instanceof HadoopFileIO) {\n+      // we need to use Spark's SerializableConfiguration to avoid issues with Kryo serialization\n+      SerializableConfiguration conf = new SerializableConfiguration(((HadoopFileIO) table.io()).conf());\n+      this.fileIO = new HadoopFileIO(conf::value);\n+    } else {\n+      this.fileIO = table.io();\n+    }\n+    this.encryptionManager = table.encryption();\n+  }\n+\n+  @Override\n+  protected RewriteDataFilesAction self() {\n+    return this;\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  /**\n+   * Pass a PartitionSepc id to specify which PartitionSpec should be used in DataFile rewrite\n+   *\n+   * @param specId PartitionSpec id to rewrite\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction outputSpecId(int specId) {\n+    Preconditions.checkArgument(table.specs().containsKey(specId), \"Invalid spec id %d\", specId);\n+    this.spec = table.specs().get(specId);\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the rewrite data file size in bytes\n+   *\n+   * @param rewriteDataFileSize size of rewrite data file\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction rewriteDataFileSizeInBytes(long rewriteDataFileSize) {\n+    Preconditions.checkArgument(rewriteDataFileSize > 0L, \"Invalid rewrite data file size in bytes %d\",\n+        rewriteDataFileSize);\n+    this.rewriteDataFileSizeInBytes = rewriteDataFileSize;\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the split lookback\n+   *\n+   * @param lookback lookback number to split\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction splitLookback(int lookback) {\n+    Preconditions.checkArgument(lookback > 0L, \"Invalid split lookback %d\", lookback);\n+    this.splitLookback = lookback;\n+    return this;\n+  }\n+\n+  /**\n+   * Pass a row Expression to filter DataFiles to be rewritten. This is typically used when only rewriting DatFiles\n+   * under some partitions.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dca22e0fae297feb6bddcf4d36ec76f2efdaeccc"}, "originalPosition": 160}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIzMDAxNTAw", "url": "https://github.com/apache/iceberg/pull/1083#pullrequestreview-423001500", "createdAt": "2020-06-02T19:41:01Z", "commit": {"oid": "dca22e0fae297feb6bddcf4d36ec76f2efdaeccc"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQxOTo0MTowMVrOGeBWAw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQxOTo0MTowMVrOGeBWAw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDEzMjQ4Mw==", "bodyText": "This needs to re-throw the exception, not just swallow it.\nAnd since this should throw the exception, I don't think that it is necessary to log it. Whatever catches it above should log it. So maybe just remove this catch.", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r434132483", "createdAt": "2020-06-02T19:41:01Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,268 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.relocated.com.google.common.collect.Streams;\n+import org.apache.iceberg.spark.source.RowDataRewriter;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.StructLikeWrapper;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RewriteDataFilesAction\n+    extends BaseSnapshotUpdateAction<RewriteDataFilesAction, RewriteDataFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteDataFilesAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final EncryptionManager encryptionManager;\n+  private long rewriteDataFileSizeInBytes;\n+  private int splitLookback;\n+\n+  private PartitionSpec spec = null;\n+  private Expression filter;\n+\n+  RewriteDataFilesAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.table = table;\n+    this.spec = table.spec();\n+    this.filter = Expressions.alwaysTrue();\n+\n+    long splitSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    long targetFileSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES,\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n+    this.rewriteDataFileSizeInBytes = Math.min(splitSize, targetFileSize);\n+\n+    this.rewriteDataFileSizeInBytes = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    this.splitLookback = PropertyUtil.propertyAsInt(\n+        table.properties(),\n+        TableProperties.SPLIT_LOOKBACK,\n+        TableProperties.SPLIT_LOOKBACK_DEFAULT);\n+\n+    if (table.io() instanceof HadoopFileIO) {\n+      // we need to use Spark's SerializableConfiguration to avoid issues with Kryo serialization\n+      SerializableConfiguration conf = new SerializableConfiguration(((HadoopFileIO) table.io()).conf());\n+      this.fileIO = new HadoopFileIO(conf::value);\n+    } else {\n+      this.fileIO = table.io();\n+    }\n+    this.encryptionManager = table.encryption();\n+  }\n+\n+  @Override\n+  protected RewriteDataFilesAction self() {\n+    return this;\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  /**\n+   * Pass a PartitionSepc id to specify which PartitionSpec should be used in DataFile rewrite\n+   *\n+   * @param specId PartitionSpec id to rewrite\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction outputSpecId(int specId) {\n+    Preconditions.checkArgument(table.specs().containsKey(specId), \"Invalid spec id %d\", specId);\n+    this.spec = table.specs().get(specId);\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the rewrite data file size in bytes\n+   *\n+   * @param rewriteDataFileSize size of rewrite data file\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction rewriteDataFileSizeInBytes(long rewriteDataFileSize) {\n+    Preconditions.checkArgument(rewriteDataFileSize > 0L, \"Invalid rewrite data file size in bytes %d\",\n+        rewriteDataFileSize);\n+    this.rewriteDataFileSizeInBytes = rewriteDataFileSize;\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the split lookback\n+   *\n+   * @param lookback lookback number to split\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction splitLookback(int lookback) {\n+    Preconditions.checkArgument(lookback > 0L, \"Invalid split lookback %d\", lookback);\n+    this.splitLookback = lookback;\n+    return this;\n+  }\n+\n+  /**\n+   * Pass a row Expression to filter DataFiles to be rewritten. This is typically used when only rewriting DatFiles\n+   * under some partitions.\n+   *\n+   * @param expr Expression to filter out DataFiles\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction filter(Expression expr) {\n+    this.filter = Expressions.and(filter, expr);\n+    return this;\n+  }\n+\n+  @Override\n+  public RewriteDataFilesActionResult execute() {\n+    CloseableIterable<FileScanTask> fileScanTasks = table.newScan()\n+        .filter(filter)\n+        .planFiles();\n+\n+    Map<StructLikeWrapper, List<FileScanTask>> groupedTasks = groupTasksByPartition(fileScanTasks.iterator());\n+    // Nothing to rewrite if the table is empty.\n+    if (groupedTasks.isEmpty()) {\n+      return RewriteDataFilesActionResult.empty();\n+    }\n+\n+    Map<StructLikeWrapper, List<FileScanTask>> filteredGroupedTasks = groupedTasks.entrySet().stream()\n+        .filter(kv -> kv.getValue().size() > 1)\n+        .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+    // Nothing to rewrite if there's only one DataFile in each partition.\n+    if (filteredGroupedTasks.isEmpty()) {\n+      return RewriteDataFilesActionResult.empty();\n+    }\n+\n+    long openFileCost = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_OPEN_FILE_COST,\n+        TableProperties.SPLIT_OPEN_FILE_COST_DEFAULT);\n+\n+    // Split and combine tasks under each partition\n+    List<CloseableIterable<CombinedScanTask>> groupedCombinedTasks = filteredGroupedTasks.values().stream()\n+        .map(scanTasks -> {\n+          CloseableIterable<FileScanTask> splitTasks = TableScanUtil.splitFiles(\n+              CloseableIterable.withNoopClose(scanTasks), rewriteDataFileSizeInBytes);\n+          return TableScanUtil.planTasks(splitTasks, rewriteDataFileSizeInBytes, splitLookback, openFileCost);\n+        })\n+        .collect(Collectors.toList());\n+    List<CombinedScanTask> combinedScanTasks = groupedCombinedTasks.stream()\n+        .flatMap(Streams::stream)\n+        .collect(Collectors.toList());\n+\n+    JavaRDD<CombinedScanTask> taskRDD = sparkContext.parallelize(combinedScanTasks, combinedScanTasks.size());\n+\n+    Broadcast<FileIO> io = sparkContext.broadcast(fileIO);\n+    Broadcast<EncryptionManager> encryption = sparkContext.broadcast(encryptionManager);\n+    boolean caseSensitive = Boolean.parseBoolean(spark.conf().get(\"spark.sql.caseSensitive\", \"false\"));\n+\n+    RowDataRewriter rowDataRewriter = new RowDataRewriter(table, spec, caseSensitive, io, encryption,\n+        rewriteDataFileSizeInBytes);\n+\n+    List<DataFile> addedDataFiles = rowDataRewriter.rewriteDataForTasks(taskRDD);\n+    List<DataFile> currentDataFiles = filteredGroupedTasks.values().stream()\n+        .flatMap(tasks -> tasks.stream().map(FileScanTask::file))\n+        .collect(Collectors.toList());\n+    replaceDataFiles(currentDataFiles, addedDataFiles);\n+\n+    return new RewriteDataFilesActionResult(currentDataFiles, addedDataFiles);\n+  }\n+\n+  private Map<StructLikeWrapper, List<FileScanTask>> groupTasksByPartition(CloseableIterator<FileScanTask> tasksIter) {\n+    Map<StructLikeWrapper, List<FileScanTask>> tasksGroupedByPartition = Maps.newHashMap();\n+\n+    try {\n+      tasksIter.forEachRemaining(task -> {\n+        StructLikeWrapper structLike = StructLikeWrapper.wrap(task.file().partition());\n+        List<FileScanTask> taskList = tasksGroupedByPartition.getOrDefault(structLike, Lists.newArrayList());\n+        taskList.add(task);\n+        tasksGroupedByPartition.put(structLike, taskList);\n+      });\n+    } finally {\n+      try {\n+        tasksIter.close();\n+      } catch (IOException ioe) {\n+        LOG.warn(\"Faile to close task iterator\", ioe);\n+      }\n+    }\n+\n+    return tasksGroupedByPartition;\n+  }\n+\n+  private void replaceDataFiles(Iterable<DataFile> deletedDataFiles, Iterable<DataFile> addedDataFiles) {\n+    boolean threw = true;\n+\n+    try {\n+      RewriteFiles rewriteFiles = table.newRewrite();\n+      rewriteFiles.rewriteFiles(Sets.newHashSet(deletedDataFiles), Sets.newHashSet(addedDataFiles));\n+      commit(rewriteFiles);\n+      threw = false;\n+\n+    } catch (Throwable t) {\n+      LOG.error(\"Failed to rewrite DataFiles\", t);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dca22e0fae297feb6bddcf4d36ec76f2efdaeccc"}, "originalPosition": 256}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIzMDAzMjk1", "url": "https://github.com/apache/iceberg/pull/1083#pullrequestreview-423003295", "createdAt": "2020-06-02T19:43:44Z", "commit": {"oid": "dca22e0fae297feb6bddcf4d36ec76f2efdaeccc"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQxOTo0Mzo0NVrOGeBbLQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQxOTo0Mzo0NVrOGeBbLQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDEzMzgwNQ==", "bodyText": "Could this use ManifestGroup instead of a scan? The scan will emit a scan event, but this isn't really a table scan.\nAlso, this should be wrapped in a try-with-resources block so that the CloseableIterable is closed.", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r434133805", "createdAt": "2020-06-02T19:43:45Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,268 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.relocated.com.google.common.collect.Streams;\n+import org.apache.iceberg.spark.source.RowDataRewriter;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.StructLikeWrapper;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RewriteDataFilesAction\n+    extends BaseSnapshotUpdateAction<RewriteDataFilesAction, RewriteDataFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteDataFilesAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final EncryptionManager encryptionManager;\n+  private long rewriteDataFileSizeInBytes;\n+  private int splitLookback;\n+\n+  private PartitionSpec spec = null;\n+  private Expression filter;\n+\n+  RewriteDataFilesAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.table = table;\n+    this.spec = table.spec();\n+    this.filter = Expressions.alwaysTrue();\n+\n+    long splitSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    long targetFileSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES,\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n+    this.rewriteDataFileSizeInBytes = Math.min(splitSize, targetFileSize);\n+\n+    this.rewriteDataFileSizeInBytes = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    this.splitLookback = PropertyUtil.propertyAsInt(\n+        table.properties(),\n+        TableProperties.SPLIT_LOOKBACK,\n+        TableProperties.SPLIT_LOOKBACK_DEFAULT);\n+\n+    if (table.io() instanceof HadoopFileIO) {\n+      // we need to use Spark's SerializableConfiguration to avoid issues with Kryo serialization\n+      SerializableConfiguration conf = new SerializableConfiguration(((HadoopFileIO) table.io()).conf());\n+      this.fileIO = new HadoopFileIO(conf::value);\n+    } else {\n+      this.fileIO = table.io();\n+    }\n+    this.encryptionManager = table.encryption();\n+  }\n+\n+  @Override\n+  protected RewriteDataFilesAction self() {\n+    return this;\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  /**\n+   * Pass a PartitionSepc id to specify which PartitionSpec should be used in DataFile rewrite\n+   *\n+   * @param specId PartitionSpec id to rewrite\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction outputSpecId(int specId) {\n+    Preconditions.checkArgument(table.specs().containsKey(specId), \"Invalid spec id %d\", specId);\n+    this.spec = table.specs().get(specId);\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the rewrite data file size in bytes\n+   *\n+   * @param rewriteDataFileSize size of rewrite data file\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction rewriteDataFileSizeInBytes(long rewriteDataFileSize) {\n+    Preconditions.checkArgument(rewriteDataFileSize > 0L, \"Invalid rewrite data file size in bytes %d\",\n+        rewriteDataFileSize);\n+    this.rewriteDataFileSizeInBytes = rewriteDataFileSize;\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the split lookback\n+   *\n+   * @param lookback lookback number to split\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction splitLookback(int lookback) {\n+    Preconditions.checkArgument(lookback > 0L, \"Invalid split lookback %d\", lookback);\n+    this.splitLookback = lookback;\n+    return this;\n+  }\n+\n+  /**\n+   * Pass a row Expression to filter DataFiles to be rewritten. This is typically used when only rewriting DatFiles\n+   * under some partitions.\n+   *\n+   * @param expr Expression to filter out DataFiles\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction filter(Expression expr) {\n+    this.filter = Expressions.and(filter, expr);\n+    return this;\n+  }\n+\n+  @Override\n+  public RewriteDataFilesActionResult execute() {\n+    CloseableIterable<FileScanTask> fileScanTasks = table.newScan()\n+        .filter(filter)\n+        .planFiles();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dca22e0fae297feb6bddcf4d36ec76f2efdaeccc"}, "originalPosition": 174}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIzMDA0MzY3", "url": "https://github.com/apache/iceberg/pull/1083#pullrequestreview-423004367", "createdAt": "2020-06-02T19:45:20Z", "commit": {"oid": "dca22e0fae297feb6bddcf4d36ec76f2efdaeccc"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQxOTo0NToyMFrOGeBeVw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQxOTo0NToyMFrOGeBeVw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDEzNDYxNQ==", "bodyText": "Since this is checked below, it's not necessary to have 2. Not a big deal to keep it though.", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r434134615", "createdAt": "2020-06-02T19:45:20Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,268 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.relocated.com.google.common.collect.Streams;\n+import org.apache.iceberg.spark.source.RowDataRewriter;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.StructLikeWrapper;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RewriteDataFilesAction\n+    extends BaseSnapshotUpdateAction<RewriteDataFilesAction, RewriteDataFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteDataFilesAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final EncryptionManager encryptionManager;\n+  private long rewriteDataFileSizeInBytes;\n+  private int splitLookback;\n+\n+  private PartitionSpec spec = null;\n+  private Expression filter;\n+\n+  RewriteDataFilesAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.table = table;\n+    this.spec = table.spec();\n+    this.filter = Expressions.alwaysTrue();\n+\n+    long splitSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    long targetFileSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES,\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n+    this.rewriteDataFileSizeInBytes = Math.min(splitSize, targetFileSize);\n+\n+    this.rewriteDataFileSizeInBytes = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    this.splitLookback = PropertyUtil.propertyAsInt(\n+        table.properties(),\n+        TableProperties.SPLIT_LOOKBACK,\n+        TableProperties.SPLIT_LOOKBACK_DEFAULT);\n+\n+    if (table.io() instanceof HadoopFileIO) {\n+      // we need to use Spark's SerializableConfiguration to avoid issues with Kryo serialization\n+      SerializableConfiguration conf = new SerializableConfiguration(((HadoopFileIO) table.io()).conf());\n+      this.fileIO = new HadoopFileIO(conf::value);\n+    } else {\n+      this.fileIO = table.io();\n+    }\n+    this.encryptionManager = table.encryption();\n+  }\n+\n+  @Override\n+  protected RewriteDataFilesAction self() {\n+    return this;\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  /**\n+   * Pass a PartitionSepc id to specify which PartitionSpec should be used in DataFile rewrite\n+   *\n+   * @param specId PartitionSpec id to rewrite\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction outputSpecId(int specId) {\n+    Preconditions.checkArgument(table.specs().containsKey(specId), \"Invalid spec id %d\", specId);\n+    this.spec = table.specs().get(specId);\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the rewrite data file size in bytes\n+   *\n+   * @param rewriteDataFileSize size of rewrite data file\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction rewriteDataFileSizeInBytes(long rewriteDataFileSize) {\n+    Preconditions.checkArgument(rewriteDataFileSize > 0L, \"Invalid rewrite data file size in bytes %d\",\n+        rewriteDataFileSize);\n+    this.rewriteDataFileSizeInBytes = rewriteDataFileSize;\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the split lookback\n+   *\n+   * @param lookback lookback number to split\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction splitLookback(int lookback) {\n+    Preconditions.checkArgument(lookback > 0L, \"Invalid split lookback %d\", lookback);\n+    this.splitLookback = lookback;\n+    return this;\n+  }\n+\n+  /**\n+   * Pass a row Expression to filter DataFiles to be rewritten. This is typically used when only rewriting DatFiles\n+   * under some partitions.\n+   *\n+   * @param expr Expression to filter out DataFiles\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction filter(Expression expr) {\n+    this.filter = Expressions.and(filter, expr);\n+    return this;\n+  }\n+\n+  @Override\n+  public RewriteDataFilesActionResult execute() {\n+    CloseableIterable<FileScanTask> fileScanTasks = table.newScan()\n+        .filter(filter)\n+        .planFiles();\n+\n+    Map<StructLikeWrapper, List<FileScanTask>> groupedTasks = groupTasksByPartition(fileScanTasks.iterator());\n+    // Nothing to rewrite if the table is empty.\n+    if (groupedTasks.isEmpty()) {\n+      return RewriteDataFilesActionResult.empty();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dca22e0fae297feb6bddcf4d36ec76f2efdaeccc"}, "originalPosition": 179}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIzMDA2MTU2", "url": "https://github.com/apache/iceberg/pull/1083#pullrequestreview-423006156", "createdAt": "2020-06-02T19:47:53Z", "commit": {"oid": "dca22e0fae297feb6bddcf4d36ec76f2efdaeccc"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQxOTo0Nzo1NFrOGeBjuQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQxOTo0Nzo1NFrOGeBjuQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDEzNTk5Mw==", "bodyText": "Why not combine this with the previous statement?", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r434135993", "createdAt": "2020-06-02T19:47:54Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,268 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.relocated.com.google.common.collect.Streams;\n+import org.apache.iceberg.spark.source.RowDataRewriter;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.StructLikeWrapper;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RewriteDataFilesAction\n+    extends BaseSnapshotUpdateAction<RewriteDataFilesAction, RewriteDataFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteDataFilesAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final EncryptionManager encryptionManager;\n+  private long rewriteDataFileSizeInBytes;\n+  private int splitLookback;\n+\n+  private PartitionSpec spec = null;\n+  private Expression filter;\n+\n+  RewriteDataFilesAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.table = table;\n+    this.spec = table.spec();\n+    this.filter = Expressions.alwaysTrue();\n+\n+    long splitSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    long targetFileSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES,\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n+    this.rewriteDataFileSizeInBytes = Math.min(splitSize, targetFileSize);\n+\n+    this.rewriteDataFileSizeInBytes = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    this.splitLookback = PropertyUtil.propertyAsInt(\n+        table.properties(),\n+        TableProperties.SPLIT_LOOKBACK,\n+        TableProperties.SPLIT_LOOKBACK_DEFAULT);\n+\n+    if (table.io() instanceof HadoopFileIO) {\n+      // we need to use Spark's SerializableConfiguration to avoid issues with Kryo serialization\n+      SerializableConfiguration conf = new SerializableConfiguration(((HadoopFileIO) table.io()).conf());\n+      this.fileIO = new HadoopFileIO(conf::value);\n+    } else {\n+      this.fileIO = table.io();\n+    }\n+    this.encryptionManager = table.encryption();\n+  }\n+\n+  @Override\n+  protected RewriteDataFilesAction self() {\n+    return this;\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  /**\n+   * Pass a PartitionSepc id to specify which PartitionSpec should be used in DataFile rewrite\n+   *\n+   * @param specId PartitionSpec id to rewrite\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction outputSpecId(int specId) {\n+    Preconditions.checkArgument(table.specs().containsKey(specId), \"Invalid spec id %d\", specId);\n+    this.spec = table.specs().get(specId);\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the rewrite data file size in bytes\n+   *\n+   * @param rewriteDataFileSize size of rewrite data file\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction rewriteDataFileSizeInBytes(long rewriteDataFileSize) {\n+    Preconditions.checkArgument(rewriteDataFileSize > 0L, \"Invalid rewrite data file size in bytes %d\",\n+        rewriteDataFileSize);\n+    this.rewriteDataFileSizeInBytes = rewriteDataFileSize;\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the split lookback\n+   *\n+   * @param lookback lookback number to split\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction splitLookback(int lookback) {\n+    Preconditions.checkArgument(lookback > 0L, \"Invalid split lookback %d\", lookback);\n+    this.splitLookback = lookback;\n+    return this;\n+  }\n+\n+  /**\n+   * Pass a row Expression to filter DataFiles to be rewritten. This is typically used when only rewriting DatFiles\n+   * under some partitions.\n+   *\n+   * @param expr Expression to filter out DataFiles\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction filter(Expression expr) {\n+    this.filter = Expressions.and(filter, expr);\n+    return this;\n+  }\n+\n+  @Override\n+  public RewriteDataFilesActionResult execute() {\n+    CloseableIterable<FileScanTask> fileScanTasks = table.newScan()\n+        .filter(filter)\n+        .planFiles();\n+\n+    Map<StructLikeWrapper, List<FileScanTask>> groupedTasks = groupTasksByPartition(fileScanTasks.iterator());\n+    // Nothing to rewrite if the table is empty.\n+    if (groupedTasks.isEmpty()) {\n+      return RewriteDataFilesActionResult.empty();\n+    }\n+\n+    Map<StructLikeWrapper, List<FileScanTask>> filteredGroupedTasks = groupedTasks.entrySet().stream()\n+        .filter(kv -> kv.getValue().size() > 1)\n+        .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+    // Nothing to rewrite if there's only one DataFile in each partition.\n+    if (filteredGroupedTasks.isEmpty()) {\n+      return RewriteDataFilesActionResult.empty();\n+    }\n+\n+    long openFileCost = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_OPEN_FILE_COST,\n+        TableProperties.SPLIT_OPEN_FILE_COST_DEFAULT);\n+\n+    // Split and combine tasks under each partition\n+    List<CloseableIterable<CombinedScanTask>> groupedCombinedTasks = filteredGroupedTasks.values().stream()\n+        .map(scanTasks -> {\n+          CloseableIterable<FileScanTask> splitTasks = TableScanUtil.splitFiles(\n+              CloseableIterable.withNoopClose(scanTasks), rewriteDataFileSizeInBytes);\n+          return TableScanUtil.planTasks(splitTasks, rewriteDataFileSizeInBytes, splitLookback, openFileCost);\n+        })\n+        .collect(Collectors.toList());\n+    List<CombinedScanTask> combinedScanTasks = groupedCombinedTasks.stream()\n+        .flatMap(Streams::stream)\n+        .collect(Collectors.toList());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dca22e0fae297feb6bddcf4d36ec76f2efdaeccc"}, "originalPosition": 205}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIzMDU3NTQy", "url": "https://github.com/apache/iceberg/pull/1083#pullrequestreview-423057542", "createdAt": "2020-06-02T21:06:16Z", "commit": {"oid": "dca22e0fae297feb6bddcf4d36ec76f2efdaeccc"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQyMTowNjoxNlrOGeD92A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQyMTowNjoxNlrOGeD92A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDE3NTQ0OA==", "bodyText": "Why not catch (Throwable originalThrowable) instead? I don't see the value of having this outside of the catch block.", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r434175448", "createdAt": "2020-06-02T21:06:16Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/RowDataRewriter.java", "diffHunk": "@@ -0,0 +1,125 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import java.io.Serializable;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.sources.v2.writer.DataWriter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RowDataRewriter implements Serializable {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RowDataRewriter.class);\n+\n+  private final Broadcast<FileIO> fileIO;\n+  private final Broadcast<EncryptionManager> encryptionManager;\n+  private final String tableSchema;\n+  private final Writer.WriterFactory writerFactory;\n+  private final boolean caseSensitive;\n+\n+  public RowDataRewriter(Table table, PartitionSpec spec, boolean caseSensitive,\n+                         Broadcast<FileIO> fileIO, Broadcast<EncryptionManager> encryptionManager,\n+                         long targetDataFileSizeInBytes) {\n+    this.fileIO = fileIO;\n+    this.encryptionManager = encryptionManager;\n+\n+    this.caseSensitive = caseSensitive;\n+    this.tableSchema = SchemaParser.toJson(table.schema());\n+\n+    String formatString = table.properties().getOrDefault(\n+        TableProperties.DEFAULT_FILE_FORMAT, TableProperties.DEFAULT_FILE_FORMAT_DEFAULT);\n+    FileFormat fileFormat = FileFormat.valueOf(formatString.toUpperCase(Locale.ENGLISH));\n+    this.writerFactory = new Writer.WriterFactory(spec, fileFormat, table.locationProvider(), table.properties(),\n+        fileIO, encryptionManager, targetDataFileSizeInBytes, table.schema(), SparkSchemaUtil.convert(table.schema()));\n+  }\n+\n+  public List<DataFile> rewriteDataForTasks(JavaRDD<CombinedScanTask> taskRDD) {\n+    JavaRDD<Writer.TaskCommit> taskCommitRDD = taskRDD.map(this::rewriteDataForTask);\n+\n+    return taskCommitRDD.collect().stream()\n+        .flatMap(taskCommit -> Arrays.stream(taskCommit.files()))\n+        .collect(Collectors.toList());\n+  }\n+\n+  private Writer.TaskCommit rewriteDataForTask(CombinedScanTask task) {\n+    TaskContext context = TaskContext.get();\n+\n+    RowDataReader dataReader = new RowDataReader(task, SchemaParser.fromJson(tableSchema),\n+        SchemaParser.fromJson(tableSchema), fileIO.value(), encryptionManager.value(), caseSensitive);\n+\n+    int partitionId = context.partitionId();\n+    long taskId = context.taskAttemptId();\n+    DataWriter<InternalRow> dataWriter = writerFactory.createDataWriter(partitionId, taskId, 0);\n+\n+    Throwable originalThrowable = null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dca22e0fae297feb6bddcf4d36ec76f2efdaeccc"}, "originalPosition": 89}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIzMDYwMjcw", "url": "https://github.com/apache/iceberg/pull/1083#pullrequestreview-423060270", "createdAt": "2020-06-02T21:10:47Z", "commit": {"oid": "dca22e0fae297feb6bddcf4d36ec76f2efdaeccc"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQyMToxMDo0N1rOGeEF6A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQyMToxMDo0N1rOGeEF6A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDE3NzUxMg==", "bodyText": "I think this should check whether originalThrowable is an Exception and throw it if so. That way we don't needlessly wrap it. Only throwables that aren't exceptions should be wrapped.", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r434177512", "createdAt": "2020-06-02T21:10:47Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/RowDataRewriter.java", "diffHunk": "@@ -0,0 +1,125 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import java.io.Serializable;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.sources.v2.writer.DataWriter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RowDataRewriter implements Serializable {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RowDataRewriter.class);\n+\n+  private final Broadcast<FileIO> fileIO;\n+  private final Broadcast<EncryptionManager> encryptionManager;\n+  private final String tableSchema;\n+  private final Writer.WriterFactory writerFactory;\n+  private final boolean caseSensitive;\n+\n+  public RowDataRewriter(Table table, PartitionSpec spec, boolean caseSensitive,\n+                         Broadcast<FileIO> fileIO, Broadcast<EncryptionManager> encryptionManager,\n+                         long targetDataFileSizeInBytes) {\n+    this.fileIO = fileIO;\n+    this.encryptionManager = encryptionManager;\n+\n+    this.caseSensitive = caseSensitive;\n+    this.tableSchema = SchemaParser.toJson(table.schema());\n+\n+    String formatString = table.properties().getOrDefault(\n+        TableProperties.DEFAULT_FILE_FORMAT, TableProperties.DEFAULT_FILE_FORMAT_DEFAULT);\n+    FileFormat fileFormat = FileFormat.valueOf(formatString.toUpperCase(Locale.ENGLISH));\n+    this.writerFactory = new Writer.WriterFactory(spec, fileFormat, table.locationProvider(), table.properties(),\n+        fileIO, encryptionManager, targetDataFileSizeInBytes, table.schema(), SparkSchemaUtil.convert(table.schema()));\n+  }\n+\n+  public List<DataFile> rewriteDataForTasks(JavaRDD<CombinedScanTask> taskRDD) {\n+    JavaRDD<Writer.TaskCommit> taskCommitRDD = taskRDD.map(this::rewriteDataForTask);\n+\n+    return taskCommitRDD.collect().stream()\n+        .flatMap(taskCommit -> Arrays.stream(taskCommit.files()))\n+        .collect(Collectors.toList());\n+  }\n+\n+  private Writer.TaskCommit rewriteDataForTask(CombinedScanTask task) {\n+    TaskContext context = TaskContext.get();\n+\n+    RowDataReader dataReader = new RowDataReader(task, SchemaParser.fromJson(tableSchema),\n+        SchemaParser.fromJson(tableSchema), fileIO.value(), encryptionManager.value(), caseSensitive);\n+\n+    int partitionId = context.partitionId();\n+    long taskId = context.taskAttemptId();\n+    DataWriter<InternalRow> dataWriter = writerFactory.createDataWriter(partitionId, taskId, 0);\n+\n+    Throwable originalThrowable = null;\n+    try {\n+      while (dataReader.next()) {\n+        InternalRow row = dataReader.get();\n+        dataWriter.write(row);\n+      }\n+\n+      dataReader.close();\n+      dataReader = null;\n+      return (Writer.TaskCommit) dataWriter.commit();\n+\n+    } catch (Throwable t) {\n+      originalThrowable = t;\n+      try {\n+        LOG.error(\"Aborting task\", originalThrowable);\n+        context.markTaskFailed(originalThrowable);\n+\n+        LOG.error(\"Aborting commit for partition {} (task {}, attempt {}, stage {}.{})\",\n+            partitionId, taskId, context.attemptNumber(), context.stageId(), context.stageAttemptNumber());\n+        if (dataReader != null) {\n+          dataReader.close();\n+        }\n+        dataWriter.abort();\n+        LOG.error(\"Aborted commit for partition {} (task {}, attempt {}, stage {}.{})\",\n+            partitionId, taskId, context.taskAttemptId(), context.stageId(), context.stageAttemptNumber());\n+\n+      } catch (Throwable inner) {\n+        if (originalThrowable != inner) {\n+          originalThrowable.addSuppressed(inner);\n+          LOG.warn(\"Suppressing exception in catch: {}\", inner.getMessage(), inner);\n+        }\n+      }\n+\n+      throw new RuntimeException(originalThrowable);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dca22e0fae297feb6bddcf4d36ec76f2efdaeccc"}, "originalPosition": 122}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIzNzgyOTYx", "url": "https://github.com/apache/iceberg/pull/1083#pullrequestreview-423782961", "createdAt": "2020-06-03T17:37:03Z", "commit": {"oid": "1008765e306a97813637bb8f603d4ebe117b6e4c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxNzozNzowNFrOGemeTQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxNzozNzowNFrOGemeTQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc0MDgxMw==", "bodyText": "Nit: typo Faile -> Failed.", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r434740813", "createdAt": "2020-06-03T17:37:04Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.relocated.com.google.common.collect.Streams;\n+import org.apache.iceberg.spark.source.RowDataRewriter;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.StructLikeWrapper;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RewriteDataFilesAction\n+    extends BaseSnapshotUpdateAction<RewriteDataFilesAction, RewriteDataFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteDataFilesAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final EncryptionManager encryptionManager;\n+  private long targetSizeInBytes;\n+  private int splitLookback;\n+\n+  private PartitionSpec spec = null;\n+  private Expression filter;\n+\n+  RewriteDataFilesAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.table = table;\n+    this.spec = table.spec();\n+    this.filter = Expressions.alwaysTrue();\n+\n+    long splitSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    long targetFileSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES,\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n+    this.targetSizeInBytes = Math.min(splitSize, targetFileSize);\n+\n+    this.splitLookback = PropertyUtil.propertyAsInt(\n+        table.properties(),\n+        TableProperties.SPLIT_LOOKBACK,\n+        TableProperties.SPLIT_LOOKBACK_DEFAULT);\n+\n+    if (table.io() instanceof HadoopFileIO) {\n+      // we need to use Spark's SerializableConfiguration to avoid issues with Kryo serialization\n+      SerializableConfiguration conf = new SerializableConfiguration(((HadoopFileIO) table.io()).conf());\n+      this.fileIO = new HadoopFileIO(conf::value);\n+    } else {\n+      this.fileIO = table.io();\n+    }\n+    this.encryptionManager = table.encryption();\n+  }\n+\n+  @Override\n+  protected RewriteDataFilesAction self() {\n+    return this;\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  /**\n+   * Pass a PartitionSepc id to specify which PartitionSpec should be used in DataFile rewrite\n+   *\n+   * @param specId PartitionSpec id to rewrite\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction outputSpecId(int specId) {\n+    Preconditions.checkArgument(table.specs().containsKey(specId), \"Invalid spec id %d\", specId);\n+    this.spec = table.specs().get(specId);\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the target rewrite data file size in bytes\n+   *\n+   * @param targetSize size in bytes of rewrite data file\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction targetSizeInBytes(long targetSize) {\n+    Preconditions.checkArgument(targetSize > 0L, \"Invalid target rewrite data file size in bytes %d\",\n+        targetSize);\n+    this.targetSizeInBytes = targetSize;\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the number of \"bins\" considered when trying to pack the next file split into a task.\n+   * Increasing this usually makes tasks a bit more even by considering more ways to pack file regions into a single\n+   * task with extra planning cost.\n+   *\n+   * This configuration can reorder the incoming file regions, to preserve order for lower/upper bounds in file\n+   * metadata, user can use a lookback of 1.\n+   *\n+   * @param lookback number of \"bins\" considered when trying to pack the next file split into a task.\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction splitLookback(int lookback) {\n+    Preconditions.checkArgument(lookback > 0L, \"Invalid split lookback %d\", lookback);\n+    this.splitLookback = lookback;\n+    return this;\n+  }\n+\n+  /**\n+   * Pass a row Expression to filter DataFiles to be rewritten. Note that all files that may contain data matching the\n+   * filter may be rewritten.\n+   *\n+   * @param expr Expression to filter out DataFiles\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction filter(Expression expr) {\n+    this.filter = Expressions.and(filter, expr);\n+    return this;\n+  }\n+\n+  @Override\n+  public RewriteDataFilesActionResult execute() {\n+    CloseableIterable<FileScanTask> fileScanTasks = table.newScan()\n+        .filter(filter)\n+        .planFiles();\n+\n+    Map<StructLikeWrapper, List<FileScanTask>> groupedTasks = groupTasksByPartition(fileScanTasks.iterator());\n+    Map<StructLikeWrapper, List<FileScanTask>> filteredGroupedTasks = groupedTasks.entrySet().stream()\n+        .filter(kv -> kv.getValue().size() > 1)\n+        .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+    // Nothing to rewrite if there's only one DataFile in each partition.\n+    if (filteredGroupedTasks.isEmpty()) {\n+      return RewriteDataFilesActionResult.empty();\n+    }\n+\n+    long openFileCost = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_OPEN_FILE_COST,\n+        TableProperties.SPLIT_OPEN_FILE_COST_DEFAULT);\n+\n+    // Split and combine tasks under each partition\n+    List<CombinedScanTask> combinedScanTasks = filteredGroupedTasks.values().stream()\n+        .map(scanTasks -> {\n+          CloseableIterable<FileScanTask> splitTasks = TableScanUtil.splitFiles(\n+              CloseableIterable.withNoopClose(scanTasks), targetSizeInBytes);\n+          return TableScanUtil.planTasks(splitTasks, targetSizeInBytes, splitLookback, openFileCost);\n+        })\n+        .flatMap(Streams::stream)\n+        .collect(Collectors.toList());\n+\n+    JavaRDD<CombinedScanTask> taskRDD = sparkContext.parallelize(combinedScanTasks, combinedScanTasks.size());\n+\n+    Broadcast<FileIO> io = sparkContext.broadcast(fileIO);\n+    Broadcast<EncryptionManager> encryption = sparkContext.broadcast(encryptionManager);\n+    boolean caseSensitive = Boolean.parseBoolean(spark.conf().get(\"spark.sql.caseSensitive\", \"false\"));\n+\n+    RowDataRewriter rowDataRewriter =\n+        new RowDataRewriter(table, spec, caseSensitive, io, encryption, targetSizeInBytes);\n+\n+    List<DataFile> addedDataFiles = rowDataRewriter.rewriteDataForTasks(taskRDD);\n+    List<DataFile> currentDataFiles = filteredGroupedTasks.values().stream()\n+        .flatMap(tasks -> tasks.stream().map(FileScanTask::file))\n+        .collect(Collectors.toList());\n+    replaceDataFiles(currentDataFiles, addedDataFiles);\n+\n+    return new RewriteDataFilesActionResult(currentDataFiles, addedDataFiles);\n+  }\n+\n+  private Map<StructLikeWrapper, List<FileScanTask>> groupTasksByPartition(CloseableIterator<FileScanTask> tasksIter) {\n+    Map<StructLikeWrapper, List<FileScanTask>> tasksGroupedByPartition = Maps.newHashMap();\n+\n+    try {\n+      tasksIter.forEachRemaining(task -> {\n+        StructLikeWrapper structLike = StructLikeWrapper.wrap(task.file().partition());\n+        List<FileScanTask> taskList = tasksGroupedByPartition.getOrDefault(structLike, Lists.newArrayList());\n+        taskList.add(task);\n+        tasksGroupedByPartition.put(structLike, taskList);\n+      });\n+    } finally {\n+      try {\n+        tasksIter.close();\n+      } catch (IOException ioe) {\n+        LOG.warn(\"Faile to close task iterator\", ioe);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1008765e306a97813637bb8f603d4ebe117b6e4c"}, "originalPosition": 234}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIzNzg1MjM3", "url": "https://github.com/apache/iceberg/pull/1083#pullrequestreview-423785237", "createdAt": "2020-06-03T17:40:05Z", "commit": {"oid": "1008765e306a97813637bb8f603d4ebe117b6e4c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxNzo0MDowNVrOGemk-g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxNzo0MDowNVrOGemk-g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc0MjUyMg==", "bodyText": "Javadoc requires <p> on empty lines if you want a paragraph break.", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r434742522", "createdAt": "2020-06-03T17:40:05Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.relocated.com.google.common.collect.Streams;\n+import org.apache.iceberg.spark.source.RowDataRewriter;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.StructLikeWrapper;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RewriteDataFilesAction\n+    extends BaseSnapshotUpdateAction<RewriteDataFilesAction, RewriteDataFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteDataFilesAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final EncryptionManager encryptionManager;\n+  private long targetSizeInBytes;\n+  private int splitLookback;\n+\n+  private PartitionSpec spec = null;\n+  private Expression filter;\n+\n+  RewriteDataFilesAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.table = table;\n+    this.spec = table.spec();\n+    this.filter = Expressions.alwaysTrue();\n+\n+    long splitSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    long targetFileSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES,\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n+    this.targetSizeInBytes = Math.min(splitSize, targetFileSize);\n+\n+    this.splitLookback = PropertyUtil.propertyAsInt(\n+        table.properties(),\n+        TableProperties.SPLIT_LOOKBACK,\n+        TableProperties.SPLIT_LOOKBACK_DEFAULT);\n+\n+    if (table.io() instanceof HadoopFileIO) {\n+      // we need to use Spark's SerializableConfiguration to avoid issues with Kryo serialization\n+      SerializableConfiguration conf = new SerializableConfiguration(((HadoopFileIO) table.io()).conf());\n+      this.fileIO = new HadoopFileIO(conf::value);\n+    } else {\n+      this.fileIO = table.io();\n+    }\n+    this.encryptionManager = table.encryption();\n+  }\n+\n+  @Override\n+  protected RewriteDataFilesAction self() {\n+    return this;\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  /**\n+   * Pass a PartitionSepc id to specify which PartitionSpec should be used in DataFile rewrite\n+   *\n+   * @param specId PartitionSpec id to rewrite\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction outputSpecId(int specId) {\n+    Preconditions.checkArgument(table.specs().containsKey(specId), \"Invalid spec id %d\", specId);\n+    this.spec = table.specs().get(specId);\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the target rewrite data file size in bytes\n+   *\n+   * @param targetSize size in bytes of rewrite data file\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction targetSizeInBytes(long targetSize) {\n+    Preconditions.checkArgument(targetSize > 0L, \"Invalid target rewrite data file size in bytes %d\",\n+        targetSize);\n+    this.targetSizeInBytes = targetSize;\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the number of \"bins\" considered when trying to pack the next file split into a task.\n+   * Increasing this usually makes tasks a bit more even by considering more ways to pack file regions into a single\n+   * task with extra planning cost.\n+   *", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1008765e306a97813637bb8f603d4ebe117b6e4c"}, "originalPosition": 146}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIzNzkwMDk1", "url": "https://github.com/apache/iceberg/pull/1083#pullrequestreview-423790095", "createdAt": "2020-06-03T17:46:28Z", "commit": {"oid": "1008765e306a97813637bb8f603d4ebe117b6e4c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxNzo0NjoyOFrOGemy8A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxNzo0NjoyOFrOGemy8A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc0NjA5Ng==", "bodyText": "This creates a new ArrayList for every element in the iterator to pass in as a default value. It would be better to use a ListMultimap instead so that you pass a supplier to create the lists:\n  ListMultimap<...> groups = Multimaps.newListMultimap(Maps.newHashMap(), Lists::newArrayList);\n  ...\n  groups.put(wrapper, task);", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r434746096", "createdAt": "2020-06-03T17:46:28Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.relocated.com.google.common.collect.Streams;\n+import org.apache.iceberg.spark.source.RowDataRewriter;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.StructLikeWrapper;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RewriteDataFilesAction\n+    extends BaseSnapshotUpdateAction<RewriteDataFilesAction, RewriteDataFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteDataFilesAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final EncryptionManager encryptionManager;\n+  private long targetSizeInBytes;\n+  private int splitLookback;\n+\n+  private PartitionSpec spec = null;\n+  private Expression filter;\n+\n+  RewriteDataFilesAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.table = table;\n+    this.spec = table.spec();\n+    this.filter = Expressions.alwaysTrue();\n+\n+    long splitSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    long targetFileSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES,\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n+    this.targetSizeInBytes = Math.min(splitSize, targetFileSize);\n+\n+    this.splitLookback = PropertyUtil.propertyAsInt(\n+        table.properties(),\n+        TableProperties.SPLIT_LOOKBACK,\n+        TableProperties.SPLIT_LOOKBACK_DEFAULT);\n+\n+    if (table.io() instanceof HadoopFileIO) {\n+      // we need to use Spark's SerializableConfiguration to avoid issues with Kryo serialization\n+      SerializableConfiguration conf = new SerializableConfiguration(((HadoopFileIO) table.io()).conf());\n+      this.fileIO = new HadoopFileIO(conf::value);\n+    } else {\n+      this.fileIO = table.io();\n+    }\n+    this.encryptionManager = table.encryption();\n+  }\n+\n+  @Override\n+  protected RewriteDataFilesAction self() {\n+    return this;\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  /**\n+   * Pass a PartitionSepc id to specify which PartitionSpec should be used in DataFile rewrite\n+   *\n+   * @param specId PartitionSpec id to rewrite\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction outputSpecId(int specId) {\n+    Preconditions.checkArgument(table.specs().containsKey(specId), \"Invalid spec id %d\", specId);\n+    this.spec = table.specs().get(specId);\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the target rewrite data file size in bytes\n+   *\n+   * @param targetSize size in bytes of rewrite data file\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction targetSizeInBytes(long targetSize) {\n+    Preconditions.checkArgument(targetSize > 0L, \"Invalid target rewrite data file size in bytes %d\",\n+        targetSize);\n+    this.targetSizeInBytes = targetSize;\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the number of \"bins\" considered when trying to pack the next file split into a task.\n+   * Increasing this usually makes tasks a bit more even by considering more ways to pack file regions into a single\n+   * task with extra planning cost.\n+   *\n+   * This configuration can reorder the incoming file regions, to preserve order for lower/upper bounds in file\n+   * metadata, user can use a lookback of 1.\n+   *\n+   * @param lookback number of \"bins\" considered when trying to pack the next file split into a task.\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction splitLookback(int lookback) {\n+    Preconditions.checkArgument(lookback > 0L, \"Invalid split lookback %d\", lookback);\n+    this.splitLookback = lookback;\n+    return this;\n+  }\n+\n+  /**\n+   * Pass a row Expression to filter DataFiles to be rewritten. Note that all files that may contain data matching the\n+   * filter may be rewritten.\n+   *\n+   * @param expr Expression to filter out DataFiles\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction filter(Expression expr) {\n+    this.filter = Expressions.and(filter, expr);\n+    return this;\n+  }\n+\n+  @Override\n+  public RewriteDataFilesActionResult execute() {\n+    CloseableIterable<FileScanTask> fileScanTasks = table.newScan()\n+        .filter(filter)\n+        .planFiles();\n+\n+    Map<StructLikeWrapper, List<FileScanTask>> groupedTasks = groupTasksByPartition(fileScanTasks.iterator());\n+    Map<StructLikeWrapper, List<FileScanTask>> filteredGroupedTasks = groupedTasks.entrySet().stream()\n+        .filter(kv -> kv.getValue().size() > 1)\n+        .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+    // Nothing to rewrite if there's only one DataFile in each partition.\n+    if (filteredGroupedTasks.isEmpty()) {\n+      return RewriteDataFilesActionResult.empty();\n+    }\n+\n+    long openFileCost = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_OPEN_FILE_COST,\n+        TableProperties.SPLIT_OPEN_FILE_COST_DEFAULT);\n+\n+    // Split and combine tasks under each partition\n+    List<CombinedScanTask> combinedScanTasks = filteredGroupedTasks.values().stream()\n+        .map(scanTasks -> {\n+          CloseableIterable<FileScanTask> splitTasks = TableScanUtil.splitFiles(\n+              CloseableIterable.withNoopClose(scanTasks), targetSizeInBytes);\n+          return TableScanUtil.planTasks(splitTasks, targetSizeInBytes, splitLookback, openFileCost);\n+        })\n+        .flatMap(Streams::stream)\n+        .collect(Collectors.toList());\n+\n+    JavaRDD<CombinedScanTask> taskRDD = sparkContext.parallelize(combinedScanTasks, combinedScanTasks.size());\n+\n+    Broadcast<FileIO> io = sparkContext.broadcast(fileIO);\n+    Broadcast<EncryptionManager> encryption = sparkContext.broadcast(encryptionManager);\n+    boolean caseSensitive = Boolean.parseBoolean(spark.conf().get(\"spark.sql.caseSensitive\", \"false\"));\n+\n+    RowDataRewriter rowDataRewriter =\n+        new RowDataRewriter(table, spec, caseSensitive, io, encryption, targetSizeInBytes);\n+\n+    List<DataFile> addedDataFiles = rowDataRewriter.rewriteDataForTasks(taskRDD);\n+    List<DataFile> currentDataFiles = filteredGroupedTasks.values().stream()\n+        .flatMap(tasks -> tasks.stream().map(FileScanTask::file))\n+        .collect(Collectors.toList());\n+    replaceDataFiles(currentDataFiles, addedDataFiles);\n+\n+    return new RewriteDataFilesActionResult(currentDataFiles, addedDataFiles);\n+  }\n+\n+  private Map<StructLikeWrapper, List<FileScanTask>> groupTasksByPartition(CloseableIterator<FileScanTask> tasksIter) {\n+    Map<StructLikeWrapper, List<FileScanTask>> tasksGroupedByPartition = Maps.newHashMap();\n+\n+    try {\n+      tasksIter.forEachRemaining(task -> {\n+        StructLikeWrapper structLike = StructLikeWrapper.wrap(task.file().partition());\n+        List<FileScanTask> taskList = tasksGroupedByPartition.getOrDefault(structLike, Lists.newArrayList());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1008765e306a97813637bb8f603d4ebe117b6e4c"}, "originalPosition": 226}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI1NDIyMDE0", "url": "https://github.com/apache/iceberg/pull/1083#pullrequestreview-425422014", "createdAt": "2020-06-05T15:50:46Z", "commit": {"oid": "c70da1b6275f5ab8c0c260405eda8b038aa37673"}, "state": "COMMENTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNVQxNTo1MDo0NlrOGfz_EA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNVQxNjoxNjoyMFrOGf028A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjAxMDc2OA==", "bodyText": "Will it make sense to move this logic into BaseAction as it is used also used while rewriting manifests?", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r436010768", "createdAt": "2020-06-05T15:50:46Z", "author": {"login": "aokolnychyi"}, "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,275 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ListMultimap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimaps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.relocated.com.google.common.collect.Streams;\n+import org.apache.iceberg.spark.source.RowDataRewriter;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.StructLikeWrapper;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RewriteDataFilesAction\n+    extends BaseSnapshotUpdateAction<RewriteDataFilesAction, RewriteDataFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteDataFilesAction.class);\n+\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final EncryptionManager encryptionManager;\n+  private final boolean caseSensitive;\n+  private long targetSizeInBytes;\n+  private int splitLookback;\n+\n+  private PartitionSpec spec = null;\n+  private Expression filter;\n+\n+  RewriteDataFilesAction(SparkSession spark, Table table) {\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.table = table;\n+    this.spec = table.spec();\n+    this.filter = Expressions.alwaysTrue();\n+    this.caseSensitive = Boolean.parseBoolean(spark.conf().get(\"spark.sql.caseSensitive\", \"false\"));\n+\n+    long splitSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    long targetFileSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES,\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n+    this.targetSizeInBytes = Math.min(splitSize, targetFileSize);\n+\n+    this.splitLookback = PropertyUtil.propertyAsInt(\n+        table.properties(),\n+        TableProperties.SPLIT_LOOKBACK,\n+        TableProperties.SPLIT_LOOKBACK_DEFAULT);\n+\n+    if (table.io() instanceof HadoopFileIO) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c70da1b6275f5ab8c0c260405eda8b038aa37673"}, "originalPosition": 100}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjAxMjU3Mw==", "bodyText": "nit: typo in PartitionSepc", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r436012573", "createdAt": "2020-06-05T15:53:53Z", "author": {"login": "aokolnychyi"}, "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,275 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ListMultimap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimaps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.relocated.com.google.common.collect.Streams;\n+import org.apache.iceberg.spark.source.RowDataRewriter;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.StructLikeWrapper;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RewriteDataFilesAction\n+    extends BaseSnapshotUpdateAction<RewriteDataFilesAction, RewriteDataFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteDataFilesAction.class);\n+\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final EncryptionManager encryptionManager;\n+  private final boolean caseSensitive;\n+  private long targetSizeInBytes;\n+  private int splitLookback;\n+\n+  private PartitionSpec spec = null;\n+  private Expression filter;\n+\n+  RewriteDataFilesAction(SparkSession spark, Table table) {\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.table = table;\n+    this.spec = table.spec();\n+    this.filter = Expressions.alwaysTrue();\n+    this.caseSensitive = Boolean.parseBoolean(spark.conf().get(\"spark.sql.caseSensitive\", \"false\"));\n+\n+    long splitSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    long targetFileSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES,\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n+    this.targetSizeInBytes = Math.min(splitSize, targetFileSize);\n+\n+    this.splitLookback = PropertyUtil.propertyAsInt(\n+        table.properties(),\n+        TableProperties.SPLIT_LOOKBACK,\n+        TableProperties.SPLIT_LOOKBACK_DEFAULT);\n+\n+    if (table.io() instanceof HadoopFileIO) {\n+      // we need to use Spark's SerializableConfiguration to avoid issues with Kryo serialization\n+      SerializableConfiguration conf = new SerializableConfiguration(((HadoopFileIO) table.io()).conf());\n+      this.fileIO = new HadoopFileIO(conf::value);\n+    } else {\n+      this.fileIO = table.io();\n+    }\n+    this.encryptionManager = table.encryption();\n+  }\n+\n+  @Override\n+  protected RewriteDataFilesAction self() {\n+    return this;\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  /**\n+   * Pass a PartitionSepc id to specify which PartitionSpec should be used in DataFile rewrite", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c70da1b6275f5ab8c0c260405eda8b038aa37673"}, "originalPosition": 121}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjAxNDM5MQ==", "bodyText": "Shouldn't we use openFileCost as zero while bin-packing?", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r436014391", "createdAt": "2020-06-05T15:57:00Z", "author": {"login": "aokolnychyi"}, "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,275 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ListMultimap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimaps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.relocated.com.google.common.collect.Streams;\n+import org.apache.iceberg.spark.source.RowDataRewriter;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.StructLikeWrapper;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RewriteDataFilesAction\n+    extends BaseSnapshotUpdateAction<RewriteDataFilesAction, RewriteDataFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteDataFilesAction.class);\n+\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final EncryptionManager encryptionManager;\n+  private final boolean caseSensitive;\n+  private long targetSizeInBytes;\n+  private int splitLookback;\n+\n+  private PartitionSpec spec = null;\n+  private Expression filter;\n+\n+  RewriteDataFilesAction(SparkSession spark, Table table) {\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.table = table;\n+    this.spec = table.spec();\n+    this.filter = Expressions.alwaysTrue();\n+    this.caseSensitive = Boolean.parseBoolean(spark.conf().get(\"spark.sql.caseSensitive\", \"false\"));\n+\n+    long splitSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    long targetFileSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES,\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n+    this.targetSizeInBytes = Math.min(splitSize, targetFileSize);\n+\n+    this.splitLookback = PropertyUtil.propertyAsInt(\n+        table.properties(),\n+        TableProperties.SPLIT_LOOKBACK,\n+        TableProperties.SPLIT_LOOKBACK_DEFAULT);\n+\n+    if (table.io() instanceof HadoopFileIO) {\n+      // we need to use Spark's SerializableConfiguration to avoid issues with Kryo serialization\n+      SerializableConfiguration conf = new SerializableConfiguration(((HadoopFileIO) table.io()).conf());\n+      this.fileIO = new HadoopFileIO(conf::value);\n+    } else {\n+      this.fileIO = table.io();\n+    }\n+    this.encryptionManager = table.encryption();\n+  }\n+\n+  @Override\n+  protected RewriteDataFilesAction self() {\n+    return this;\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  /**\n+   * Pass a PartitionSepc id to specify which PartitionSpec should be used in DataFile rewrite\n+   *\n+   * @param specId PartitionSpec id to rewrite\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction outputSpecId(int specId) {\n+    Preconditions.checkArgument(table.specs().containsKey(specId), \"Invalid spec id %d\", specId);\n+    this.spec = table.specs().get(specId);\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the target rewrite data file size in bytes\n+   *\n+   * @param targetSize size in bytes of rewrite data file\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction targetSizeInBytes(long targetSize) {\n+    Preconditions.checkArgument(targetSize > 0L, \"Invalid target rewrite data file size in bytes %d\",\n+        targetSize);\n+    this.targetSizeInBytes = targetSize;\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the number of \"bins\" considered when trying to pack the next file split into a task.\n+   * Increasing this usually makes tasks a bit more even by considering more ways to pack file regions into a single\n+   * task with extra planning cost.\n+   * <p>\n+   * This configuration can reorder the incoming file regions, to preserve order for lower/upper bounds in file\n+   * metadata, user can use a lookback of 1.\n+   *\n+   * @param lookback number of \"bins\" considered when trying to pack the next file split into a task.\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction splitLookback(int lookback) {\n+    Preconditions.checkArgument(lookback > 0L, \"Invalid split lookback %d\", lookback);\n+    this.splitLookback = lookback;\n+    return this;\n+  }\n+\n+  /**\n+   * Pass a row Expression to filter DataFiles to be rewritten. Note that all files that may contain data matching the\n+   * filter may be rewritten.\n+   *\n+   * @param expr Expression to filter out DataFiles\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction filter(Expression expr) {\n+    this.filter = Expressions.and(filter, expr);\n+    return this;\n+  }\n+\n+  @Override\n+  public RewriteDataFilesActionResult execute() {\n+    CloseableIterable<FileScanTask> fileScanTasks = null;\n+    try {\n+      fileScanTasks = table.newScan()\n+          .caseSensitive(caseSensitive)\n+          .filter(filter)\n+          .planFiles();\n+    } finally {\n+      try {\n+        if (fileScanTasks != null) {\n+          fileScanTasks.close();\n+        }\n+      } catch (IOException ioe) {\n+        LOG.warn(\"Failed to close task iterable\", ioe);\n+      }\n+    }\n+\n+    Map<StructLikeWrapper, Collection<FileScanTask>> groupedTasks = groupTasksByPartition(fileScanTasks.iterator());\n+    Map<StructLikeWrapper, Collection<FileScanTask>> filteredGroupedTasks = groupedTasks.entrySet().stream()\n+        .filter(kv -> kv.getValue().size() > 1)\n+        .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+    // Nothing to rewrite if there's only one DataFile in each partition.\n+    if (filteredGroupedTasks.isEmpty()) {\n+      return RewriteDataFilesActionResult.empty();\n+    }\n+\n+    long openFileCost = PropertyUtil.propertyAsLong(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c70da1b6275f5ab8c0c260405eda8b038aa37673"}, "originalPosition": 202}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjAxODE5MA==", "bodyText": "Can we simplify this block by using catch (Exception e) and rethrowing the original exception? Otherwise, we won't know what happened.", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r436018190", "createdAt": "2020-06-05T16:03:44Z", "author": {"login": "aokolnychyi"}, "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,275 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ListMultimap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimaps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.relocated.com.google.common.collect.Streams;\n+import org.apache.iceberg.spark.source.RowDataRewriter;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.StructLikeWrapper;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RewriteDataFilesAction\n+    extends BaseSnapshotUpdateAction<RewriteDataFilesAction, RewriteDataFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteDataFilesAction.class);\n+\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final EncryptionManager encryptionManager;\n+  private final boolean caseSensitive;\n+  private long targetSizeInBytes;\n+  private int splitLookback;\n+\n+  private PartitionSpec spec = null;\n+  private Expression filter;\n+\n+  RewriteDataFilesAction(SparkSession spark, Table table) {\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.table = table;\n+    this.spec = table.spec();\n+    this.filter = Expressions.alwaysTrue();\n+    this.caseSensitive = Boolean.parseBoolean(spark.conf().get(\"spark.sql.caseSensitive\", \"false\"));\n+\n+    long splitSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    long targetFileSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES,\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n+    this.targetSizeInBytes = Math.min(splitSize, targetFileSize);\n+\n+    this.splitLookback = PropertyUtil.propertyAsInt(\n+        table.properties(),\n+        TableProperties.SPLIT_LOOKBACK,\n+        TableProperties.SPLIT_LOOKBACK_DEFAULT);\n+\n+    if (table.io() instanceof HadoopFileIO) {\n+      // we need to use Spark's SerializableConfiguration to avoid issues with Kryo serialization\n+      SerializableConfiguration conf = new SerializableConfiguration(((HadoopFileIO) table.io()).conf());\n+      this.fileIO = new HadoopFileIO(conf::value);\n+    } else {\n+      this.fileIO = table.io();\n+    }\n+    this.encryptionManager = table.encryption();\n+  }\n+\n+  @Override\n+  protected RewriteDataFilesAction self() {\n+    return this;\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  /**\n+   * Pass a PartitionSepc id to specify which PartitionSpec should be used in DataFile rewrite\n+   *\n+   * @param specId PartitionSpec id to rewrite\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction outputSpecId(int specId) {\n+    Preconditions.checkArgument(table.specs().containsKey(specId), \"Invalid spec id %d\", specId);\n+    this.spec = table.specs().get(specId);\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the target rewrite data file size in bytes\n+   *\n+   * @param targetSize size in bytes of rewrite data file\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction targetSizeInBytes(long targetSize) {\n+    Preconditions.checkArgument(targetSize > 0L, \"Invalid target rewrite data file size in bytes %d\",\n+        targetSize);\n+    this.targetSizeInBytes = targetSize;\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the number of \"bins\" considered when trying to pack the next file split into a task.\n+   * Increasing this usually makes tasks a bit more even by considering more ways to pack file regions into a single\n+   * task with extra planning cost.\n+   * <p>\n+   * This configuration can reorder the incoming file regions, to preserve order for lower/upper bounds in file\n+   * metadata, user can use a lookback of 1.\n+   *\n+   * @param lookback number of \"bins\" considered when trying to pack the next file split into a task.\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction splitLookback(int lookback) {\n+    Preconditions.checkArgument(lookback > 0L, \"Invalid split lookback %d\", lookback);\n+    this.splitLookback = lookback;\n+    return this;\n+  }\n+\n+  /**\n+   * Pass a row Expression to filter DataFiles to be rewritten. Note that all files that may contain data matching the\n+   * filter may be rewritten.\n+   *\n+   * @param expr Expression to filter out DataFiles\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction filter(Expression expr) {\n+    this.filter = Expressions.and(filter, expr);\n+    return this;\n+  }\n+\n+  @Override\n+  public RewriteDataFilesActionResult execute() {\n+    CloseableIterable<FileScanTask> fileScanTasks = null;\n+    try {\n+      fileScanTasks = table.newScan()\n+          .caseSensitive(caseSensitive)\n+          .filter(filter)\n+          .planFiles();\n+    } finally {\n+      try {\n+        if (fileScanTasks != null) {\n+          fileScanTasks.close();\n+        }\n+      } catch (IOException ioe) {\n+        LOG.warn(\"Failed to close task iterable\", ioe);\n+      }\n+    }\n+\n+    Map<StructLikeWrapper, Collection<FileScanTask>> groupedTasks = groupTasksByPartition(fileScanTasks.iterator());\n+    Map<StructLikeWrapper, Collection<FileScanTask>> filteredGroupedTasks = groupedTasks.entrySet().stream()\n+        .filter(kv -> kv.getValue().size() > 1)\n+        .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+    // Nothing to rewrite if there's only one DataFile in each partition.\n+    if (filteredGroupedTasks.isEmpty()) {\n+      return RewriteDataFilesActionResult.empty();\n+    }\n+\n+    long openFileCost = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_OPEN_FILE_COST,\n+        TableProperties.SPLIT_OPEN_FILE_COST_DEFAULT);\n+\n+    // Split and combine tasks under each partition\n+    List<CombinedScanTask> combinedScanTasks = filteredGroupedTasks.values().stream()\n+        .map(scanTasks -> {\n+          CloseableIterable<FileScanTask> splitTasks = TableScanUtil.splitFiles(\n+              CloseableIterable.withNoopClose(scanTasks), targetSizeInBytes);\n+          return TableScanUtil.planTasks(splitTasks, targetSizeInBytes, splitLookback, openFileCost);\n+        })\n+        .flatMap(Streams::stream)\n+        .collect(Collectors.toList());\n+\n+    JavaRDD<CombinedScanTask> taskRDD = sparkContext.parallelize(combinedScanTasks, combinedScanTasks.size());\n+\n+    Broadcast<FileIO> io = sparkContext.broadcast(fileIO);\n+    Broadcast<EncryptionManager> encryption = sparkContext.broadcast(encryptionManager);\n+\n+    RowDataRewriter rowDataRewriter =\n+        new RowDataRewriter(table, spec, caseSensitive, io, encryption, targetSizeInBytes);\n+\n+    List<DataFile> addedDataFiles = rowDataRewriter.rewriteDataForTasks(taskRDD);\n+    List<DataFile> currentDataFiles = filteredGroupedTasks.values().stream()\n+        .flatMap(tasks -> tasks.stream().map(FileScanTask::file))\n+        .collect(Collectors.toList());\n+    replaceDataFiles(currentDataFiles, addedDataFiles);\n+\n+    return new RewriteDataFilesActionResult(currentDataFiles, addedDataFiles);\n+  }\n+\n+  private Map<StructLikeWrapper, Collection<FileScanTask>> groupTasksByPartition(\n+      CloseableIterator<FileScanTask> tasksIter) {\n+    ListMultimap<StructLikeWrapper, FileScanTask> tasksGroupedByPartition = Multimaps.newListMultimap(\n+        Maps.newHashMap(), Lists::newArrayList);\n+\n+    try {\n+      tasksIter.forEachRemaining(task -> {\n+        StructLikeWrapper structLike = StructLikeWrapper.wrap(task.file().partition());\n+        tasksGroupedByPartition.put(structLike, task);\n+      });\n+\n+    } finally {\n+      try {\n+        tasksIter.close();\n+      } catch (IOException ioe) {\n+        LOG.warn(\"Failed to close task iterator\", ioe);\n+      }\n+    }\n+\n+    return tasksGroupedByPartition.asMap();\n+  }\n+\n+  private void replaceDataFiles(Iterable<DataFile> deletedDataFiles, Iterable<DataFile> addedDataFiles) {\n+    boolean threw = true;\n+\n+    try {\n+      RewriteFiles rewriteFiles = table.newRewrite();\n+      rewriteFiles.rewriteFiles(Sets.newHashSet(deletedDataFiles), Sets.newHashSet(addedDataFiles));\n+      commit(rewriteFiles);\n+      threw = false;\n+\n+    } finally {\n+      if (threw) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c70da1b6275f5ab8c0c260405eda8b038aa37673"}, "originalPosition": 266}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjAxOTY3Nw==", "bodyText": "This is the place where we need to call ignoreResiduals() to make sure we read complete files.", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r436019677", "createdAt": "2020-06-05T16:06:20Z", "author": {"login": "aokolnychyi"}, "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,275 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ListMultimap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimaps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.relocated.com.google.common.collect.Streams;\n+import org.apache.iceberg.spark.source.RowDataRewriter;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.StructLikeWrapper;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RewriteDataFilesAction\n+    extends BaseSnapshotUpdateAction<RewriteDataFilesAction, RewriteDataFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteDataFilesAction.class);\n+\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final EncryptionManager encryptionManager;\n+  private final boolean caseSensitive;\n+  private long targetSizeInBytes;\n+  private int splitLookback;\n+\n+  private PartitionSpec spec = null;\n+  private Expression filter;\n+\n+  RewriteDataFilesAction(SparkSession spark, Table table) {\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.table = table;\n+    this.spec = table.spec();\n+    this.filter = Expressions.alwaysTrue();\n+    this.caseSensitive = Boolean.parseBoolean(spark.conf().get(\"spark.sql.caseSensitive\", \"false\"));\n+\n+    long splitSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    long targetFileSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES,\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n+    this.targetSizeInBytes = Math.min(splitSize, targetFileSize);\n+\n+    this.splitLookback = PropertyUtil.propertyAsInt(\n+        table.properties(),\n+        TableProperties.SPLIT_LOOKBACK,\n+        TableProperties.SPLIT_LOOKBACK_DEFAULT);\n+\n+    if (table.io() instanceof HadoopFileIO) {\n+      // we need to use Spark's SerializableConfiguration to avoid issues with Kryo serialization\n+      SerializableConfiguration conf = new SerializableConfiguration(((HadoopFileIO) table.io()).conf());\n+      this.fileIO = new HadoopFileIO(conf::value);\n+    } else {\n+      this.fileIO = table.io();\n+    }\n+    this.encryptionManager = table.encryption();\n+  }\n+\n+  @Override\n+  protected RewriteDataFilesAction self() {\n+    return this;\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  /**\n+   * Pass a PartitionSepc id to specify which PartitionSpec should be used in DataFile rewrite\n+   *\n+   * @param specId PartitionSpec id to rewrite\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction outputSpecId(int specId) {\n+    Preconditions.checkArgument(table.specs().containsKey(specId), \"Invalid spec id %d\", specId);\n+    this.spec = table.specs().get(specId);\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the target rewrite data file size in bytes\n+   *\n+   * @param targetSize size in bytes of rewrite data file\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction targetSizeInBytes(long targetSize) {\n+    Preconditions.checkArgument(targetSize > 0L, \"Invalid target rewrite data file size in bytes %d\",\n+        targetSize);\n+    this.targetSizeInBytes = targetSize;\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the number of \"bins\" considered when trying to pack the next file split into a task.\n+   * Increasing this usually makes tasks a bit more even by considering more ways to pack file regions into a single\n+   * task with extra planning cost.\n+   * <p>\n+   * This configuration can reorder the incoming file regions, to preserve order for lower/upper bounds in file\n+   * metadata, user can use a lookback of 1.\n+   *\n+   * @param lookback number of \"bins\" considered when trying to pack the next file split into a task.\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction splitLookback(int lookback) {\n+    Preconditions.checkArgument(lookback > 0L, \"Invalid split lookback %d\", lookback);\n+    this.splitLookback = lookback;\n+    return this;\n+  }\n+\n+  /**\n+   * Pass a row Expression to filter DataFiles to be rewritten. Note that all files that may contain data matching the\n+   * filter may be rewritten.\n+   *\n+   * @param expr Expression to filter out DataFiles\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction filter(Expression expr) {\n+    this.filter = Expressions.and(filter, expr);\n+    return this;\n+  }\n+\n+  @Override\n+  public RewriteDataFilesActionResult execute() {\n+    CloseableIterable<FileScanTask> fileScanTasks = null;\n+    try {\n+      fileScanTasks = table.newScan()\n+          .caseSensitive(caseSensitive)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c70da1b6275f5ab8c0c260405eda8b038aa37673"}, "originalPosition": 179}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjAyNTA3Mg==", "bodyText": "I would advise adding a test where a file we rewrite has multiple row groups and our filter expression matches only one of them. It is not an easy test but should be worth it.\nThis can be done by setting TableProperties.PARQUET_ROW_GROUP_SIZE_BYTES to a small value like 100, inserting a relatively large number of rows sorted by a column, and using a filter on that column that matches only last 1-2 records.", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r436025072", "createdAt": "2020-06-05T16:16:20Z", "author": {"login": "aokolnychyi"}, "path": "spark/src/test/java/org/apache/iceberg/actions/TestRewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,297 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.File;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.source.ThreeColumnRecord;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+\n+public class TestRewriteDataFilesAction {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c70da1b6275f5ab8c0c260405eda8b038aa37673"}, "originalPosition": 52}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5d00e101627b83af956e45adb05667835d6fb0fe", "author": {"user": {"login": "jerryshao", "name": "Saisai Shao"}}, "url": "https://github.com/apache/iceberg/commit/5d00e101627b83af956e45adb05667835d6fb0fe", "committedDate": "2020-06-08T09:31:48Z", "message": "Add rewrite DataFile support for Iceberg Action"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4a7b98b5a1b1eae0855c4489c9aa0f71a9bd09dd", "author": {"user": {"login": "jerryshao", "name": "Saisai Shao"}}, "url": "https://github.com/apache/iceberg/commit/4a7b98b5a1b1eae0855c4489c9aa0f71a9bd09dd", "committedDate": "2020-06-08T09:31:48Z", "message": "Fix guava relocate issue"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1c26f2415a1fdfd6d5479454ca879b541f94fb25", "author": {"user": {"login": "jerryshao", "name": "Saisai Shao"}}, "url": "https://github.com/apache/iceberg/commit/1c26f2415a1fdfd6d5479454ca879b541f94fb25", "committedDate": "2020-06-08T09:31:48Z", "message": "Address the comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "daa1097af2802b21c87339f348218e48af7a808c", "author": {"user": {"login": "jerryshao", "name": "Saisai Shao"}}, "url": "https://github.com/apache/iceberg/commit/daa1097af2802b21c87339f348218e48af7a808c", "committedDate": "2020-06-08T09:31:48Z", "message": "Address the comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4b64ba99ec6ee6110a6886f6583a5168de5bb194", "author": {"user": {"login": "jerryshao", "name": "Saisai Shao"}}, "url": "https://github.com/apache/iceberg/commit/4b64ba99ec6ee6110a6886f6583a5168de5bb194", "committedDate": "2020-06-08T09:31:48Z", "message": "Address the comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "986518ac2c6bc8147356691d55df00ac350e5a37", "author": {"user": {"login": "jerryshao", "name": "Saisai Shao"}}, "url": "https://github.com/apache/iceberg/commit/986518ac2c6bc8147356691d55df00ac350e5a37", "committedDate": "2020-06-09T03:31:40Z", "message": "Address the comments"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "c70da1b6275f5ab8c0c260405eda8b038aa37673", "author": {"user": {"login": "jerryshao", "name": "Saisai Shao"}}, "url": "https://github.com/apache/iceberg/commit/c70da1b6275f5ab8c0c260405eda8b038aa37673", "committedDate": "2020-06-04T06:14:22Z", "message": "Address the comments"}, "afterCommit": {"oid": "986518ac2c6bc8147356691d55df00ac350e5a37", "author": {"user": {"login": "jerryshao", "name": "Saisai Shao"}}, "url": "https://github.com/apache/iceberg/commit/986518ac2c6bc8147356691d55df00ac350e5a37", "committedDate": "2020-06-09T03:31:40Z", "message": "Address the comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2bd9f4aee4714ec519370efe4929965b75b6e5c0", "author": {"user": {"login": "jerryshao", "name": "Saisai Shao"}}, "url": "https://github.com/apache/iceberg/commit/2bd9f4aee4714ec519370efe4929965b75b6e5c0", "committedDate": "2020-06-10T02:58:19Z", "message": "Address the comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI4NTE5NDk0", "url": "https://github.com/apache/iceberg/pull/1083#pullrequestreview-428519494", "createdAt": "2020-06-11T00:44:09Z", "commit": {"oid": "2bd9f4aee4714ec519370efe4929965b75b6e5c0"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQwMDo0NDowOVrOGiK04w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQwMDo0NDowOVrOGiK04w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQ4MjE0Nw==", "bodyText": "How do we know there will be 2 files for c3=0?\nLooks like the table is unpartitioned, so each task will create a single output file. Are there only 2 tasks?", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r438482147", "createdAt": "2020-06-11T00:44:09Z", "author": {"login": "rdblue"}, "path": "spark/src/test/java/org/apache/iceberg/actions/TestRewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.File;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.source.ThreeColumnRecord;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+\n+public class TestRewriteDataFilesAction {\n+\n+  private static final HadoopTables TABLES = new HadoopTables(new Configuration());\n+  private static final Schema SCHEMA = new Schema(\n+      optional(1, \"c1\", Types.IntegerType.get()),\n+      optional(2, \"c2\", Types.StringType.get()),\n+      optional(3, \"c3\", Types.StringType.get())\n+  );\n+\n+  private static SparkSession spark;\n+\n+  @BeforeClass\n+  public static void startSpark() {\n+    TestRewriteDataFilesAction.spark = SparkSession.builder()\n+        .master(\"local[2]\")\n+        .getOrCreate();\n+  }\n+\n+  @AfterClass\n+  public static void stopSpark() {\n+    SparkSession currentSpark = TestRewriteDataFilesAction.spark;\n+    TestRewriteDataFilesAction.spark = null;\n+    currentSpark.stop();\n+  }\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  private String tableLocation = null;\n+\n+  @Before\n+  public void setupTableLocation() throws Exception {\n+    File tableDir = temp.newFolder();\n+    this.tableLocation = tableDir.toURI().toString();\n+  }\n+\n+  @Test\n+  public void testRewriteDataFilesEmptyTable() {\n+    PartitionSpec spec = PartitionSpec.unpartitioned();\n+    Map<String, String> options = Maps.newHashMap();\n+    Table table = TABLES.create(SCHEMA, spec, options, tableLocation);\n+\n+    Assert.assertNull(\"Table must be empty\", table.currentSnapshot());\n+\n+    Actions actions = Actions.forTable(table);\n+\n+    actions.rewriteDataFiles().execute();\n+\n+    Assert.assertNull(\"Table must stay empty\", table.currentSnapshot());\n+  }\n+\n+  @Test\n+  public void testRewriteDataFilesUnpartitionedTable() {\n+    PartitionSpec spec = PartitionSpec.unpartitioned();\n+    Map<String, String> options = Maps.newHashMap();\n+    Table table = TABLES.create(SCHEMA, spec, options, tableLocation);\n+\n+    List<ThreeColumnRecord> records1 = Lists.newArrayList(\n+        new ThreeColumnRecord(1, null, \"AAAA\"),\n+        new ThreeColumnRecord(1, \"BBBBBBBBBB\", \"BBBB\")\n+    );\n+    writeRecords(records1);\n+\n+    List<ThreeColumnRecord> records2 = Lists.newArrayList(\n+        new ThreeColumnRecord(2, \"CCCCCCCCCC\", \"CCCC\"),\n+        new ThreeColumnRecord(2, \"DDDDDDDDDD\", \"DDDD\")\n+    );\n+    writeRecords(records2);\n+\n+    table.refresh();\n+\n+    CloseableIterable<FileScanTask> tasks = table.newScan().planFiles();\n+    List<DataFile> dataFiles = Lists.newArrayList(CloseableIterable.transform(tasks, FileScanTask::file));\n+    Assert.assertEquals(\"Should have 4 data files before rewrite\", 4, dataFiles.size());\n+\n+    Actions actions = Actions.forTable(table);\n+\n+    RewriteDataFilesActionResult result = actions.rewriteDataFiles().execute();\n+    Assert.assertEquals(\"Action should rewrite 4 data files\", 4, result.deletedDataFiles().size());\n+    Assert.assertEquals(\"Action should add 1 data file\", 1, result.addedDataFiles().size());\n+\n+    table.refresh();\n+\n+    CloseableIterable<FileScanTask> tasks1 = table.newScan().planFiles();\n+    List<DataFile> dataFiles1 = Lists.newArrayList(CloseableIterable.transform(tasks1, FileScanTask::file));\n+    Assert.assertEquals(\"Should have 1 data files before rewrite\", 1, dataFiles1.size());\n+\n+    List<ThreeColumnRecord> expectedRecords = Lists.newArrayList();\n+    expectedRecords.addAll(records1);\n+    expectedRecords.addAll(records2);\n+\n+    Dataset<Row> resultDF = spark.read().format(\"iceberg\").load(tableLocation);\n+    List<ThreeColumnRecord> actualRecords = resultDF.sort(\"c1\", \"c2\")\n+        .as(Encoders.bean(ThreeColumnRecord.class))\n+        .collectAsList();\n+\n+    Assert.assertEquals(\"Rows must match\", expectedRecords, actualRecords);\n+  }\n+\n+  @Test\n+  public void testRewriteDataFilesPartitionedTable() {\n+    PartitionSpec spec = PartitionSpec.builderFor(SCHEMA)\n+        .identity(\"c1\")\n+        .truncate(\"c2\", 2)\n+        .build();\n+    Map<String, String> options = Maps.newHashMap();\n+    Table table = TABLES.create(SCHEMA, spec, options, tableLocation);\n+\n+    List<ThreeColumnRecord> records1 = Lists.newArrayList(\n+        new ThreeColumnRecord(1, \"AAAAAAAAAA\", \"AAAA\"),\n+        new ThreeColumnRecord(1, \"AAAAAAAAAA\", \"CCCC\")\n+    );\n+    writeRecords(records1);\n+\n+    List<ThreeColumnRecord> records2 = Lists.newArrayList(\n+        new ThreeColumnRecord(1, \"BBBBBBBBBB\", \"BBBB\"),\n+        new ThreeColumnRecord(1, \"BBBBBBBBBB\", \"DDDD\")\n+    );\n+    writeRecords(records2);\n+\n+    List<ThreeColumnRecord> records3 = Lists.newArrayList(\n+        new ThreeColumnRecord(2, \"AAAAAAAAAA\", \"EEEE\"),\n+        new ThreeColumnRecord(2, \"AAAAAAAAAA\", \"GGGG\")\n+    );\n+    writeRecords(records3);\n+\n+    List<ThreeColumnRecord> records4 = Lists.newArrayList(\n+        new ThreeColumnRecord(2, \"BBBBBBBBBB\", \"FFFF\"),\n+        new ThreeColumnRecord(2, \"BBBBBBBBBB\", \"HHHH\")\n+    );\n+    writeRecords(records4);\n+\n+    table.refresh();\n+\n+    CloseableIterable<FileScanTask> tasks = table.newScan().planFiles();\n+    List<DataFile> dataFiles = Lists.newArrayList(CloseableIterable.transform(tasks, FileScanTask::file));\n+    Assert.assertEquals(\"Should have 8 data files before rewrite\", 8, dataFiles.size());\n+\n+    Actions actions = Actions.forTable(table);\n+\n+    RewriteDataFilesActionResult result = actions.rewriteDataFiles().execute();\n+    Assert.assertEquals(\"Action should rewrite 8 data files\", 8, result.deletedDataFiles().size());\n+    Assert.assertEquals(\"Action should add 4 data file\", 4, result.addedDataFiles().size());\n+\n+    table.refresh();\n+\n+    CloseableIterable<FileScanTask> tasks1 = table.newScan().planFiles();\n+    List<DataFile> dataFiles1 = Lists.newArrayList(CloseableIterable.transform(tasks1, FileScanTask::file));\n+    Assert.assertEquals(\"Should have 4 data files before rewrite\", 4, dataFiles1.size());\n+\n+    List<ThreeColumnRecord> expectedRecords = Lists.newArrayList();\n+    expectedRecords.addAll(records1);\n+    expectedRecords.addAll(records2);\n+    expectedRecords.addAll(records3);\n+    expectedRecords.addAll(records4);\n+\n+    Dataset<Row> resultDF = spark.read().format(\"iceberg\").load(tableLocation);\n+    List<ThreeColumnRecord> actualRecords = resultDF.sort(\"c1\", \"c2\", \"c3\")\n+        .as(Encoders.bean(ThreeColumnRecord.class))\n+        .collectAsList();\n+\n+    Assert.assertEquals(\"Rows must match\", expectedRecords, actualRecords);\n+  }\n+\n+  @Test\n+  public void testRewriteDataFilesWithFilter() {\n+    PartitionSpec spec = PartitionSpec.builderFor(SCHEMA)\n+        .identity(\"c1\")\n+        .truncate(\"c2\", 2)\n+        .build();\n+    Map<String, String> options = Maps.newHashMap();\n+    Table table = TABLES.create(SCHEMA, spec, options, tableLocation);\n+\n+    List<ThreeColumnRecord> records1 = Lists.newArrayList(\n+        new ThreeColumnRecord(1, \"AAAAAAAAAA\", \"AAAA\"),\n+        new ThreeColumnRecord(1, \"AAAAAAAAAA\", \"CCCC\")\n+    );\n+    writeRecords(records1);\n+\n+    List<ThreeColumnRecord> records2 = Lists.newArrayList(\n+        new ThreeColumnRecord(1, \"BBBBBBBBBB\", \"BBBB\"),\n+        new ThreeColumnRecord(1, \"BBBBBBBBBB\", \"DDDD\")\n+    );\n+    writeRecords(records2);\n+\n+    List<ThreeColumnRecord> records3 = Lists.newArrayList(\n+        new ThreeColumnRecord(2, \"AAAAAAAAAA\", \"EEEE\"),\n+        new ThreeColumnRecord(2, \"AAAAAAAAAA\", \"GGGG\")\n+    );\n+    writeRecords(records3);\n+\n+    List<ThreeColumnRecord> records4 = Lists.newArrayList(\n+        new ThreeColumnRecord(2, \"BBBBBBBBBB\", \"FFFF\"),\n+        new ThreeColumnRecord(2, \"BBBBBBBBBB\", \"HHHH\")\n+    );\n+    writeRecords(records4);\n+\n+    table.refresh();\n+\n+    CloseableIterable<FileScanTask> tasks = table.newScan().planFiles();\n+    List<DataFile> dataFiles = Lists.newArrayList(CloseableIterable.transform(tasks, FileScanTask::file));\n+    Assert.assertEquals(\"Should have 8 data files before rewrite\", 8, dataFiles.size());\n+\n+    Actions actions = Actions.forTable(table);\n+\n+    RewriteDataFilesActionResult result = actions\n+        .rewriteDataFiles()\n+        .filter(Expressions.equal(\"c1\", 1))\n+        .filter(Expressions.startsWith(\"c2\", \"AA\"))\n+        .execute();\n+    Assert.assertEquals(\"Action should rewrite 2 data files\", 2, result.deletedDataFiles().size());\n+    Assert.assertEquals(\"Action should add 1 data file\", 1, result.addedDataFiles().size());\n+\n+    table.refresh();\n+\n+    CloseableIterable<FileScanTask> tasks1 = table.newScan().planFiles();\n+    List<DataFile> dataFiles1 = Lists.newArrayList(CloseableIterable.transform(tasks1, FileScanTask::file));\n+    Assert.assertEquals(\"Should have 7 data files before rewrite\", 7, dataFiles1.size());\n+\n+    List<ThreeColumnRecord> expectedRecords = Lists.newArrayList();\n+    expectedRecords.addAll(records1);\n+    expectedRecords.addAll(records2);\n+    expectedRecords.addAll(records3);\n+    expectedRecords.addAll(records4);\n+\n+    Dataset<Row> resultDF = spark.read().format(\"iceberg\").load(tableLocation);\n+    List<ThreeColumnRecord> actualRecords = resultDF.sort(\"c1\", \"c2\", \"c3\")\n+        .as(Encoders.bean(ThreeColumnRecord.class))\n+        .collectAsList();\n+\n+    Assert.assertEquals(\"Rows must match\", expectedRecords, actualRecords);\n+  }\n+\n+  @Test\n+  public void testRewriteLargeTableHasResiduals() {\n+    PartitionSpec spec = PartitionSpec.builderFor(SCHEMA).build();\n+    Map<String, String> options = Maps.newHashMap();\n+    options.put(TableProperties.PARQUET_ROW_GROUP_SIZE_BYTES, \"100\");\n+    Table table = TABLES.create(SCHEMA, spec, options, tableLocation);\n+\n+    // all records belong to the same partition\n+    List<ThreeColumnRecord> records = Lists.newArrayList();\n+    for (int i = 0; i < 100; i++) {\n+      records.add(new ThreeColumnRecord(i, String.valueOf(i), String.valueOf(i % 4)));\n+    }\n+    Dataset<Row> df = spark.createDataFrame(records, ThreeColumnRecord.class);\n+    writeDF(df);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2bd9f4aee4714ec519370efe4929965b75b6e5c0"}, "originalPosition": 299}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI4NTIwMjA3", "url": "https://github.com/apache/iceberg/pull/1083#pullrequestreview-428520207", "createdAt": "2020-06-11T00:46:24Z", "commit": {"oid": "2bd9f4aee4714ec519370efe4929965b75b6e5c0"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQwMDo0NjoyNVrOGiK3bA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQwMDo0NjoyNVrOGiK3bA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQ4Mjc5Ng==", "bodyText": "I don't think it is necessary to have this assertion or to run the scan planning above. This tests that ignoreResiduals works properly, but we can assume that for this test.", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r438482796", "createdAt": "2020-06-11T00:46:25Z", "author": {"login": "rdblue"}, "path": "spark/src/test/java/org/apache/iceberg/actions/TestRewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.File;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.source.ThreeColumnRecord;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+\n+public class TestRewriteDataFilesAction {\n+\n+  private static final HadoopTables TABLES = new HadoopTables(new Configuration());\n+  private static final Schema SCHEMA = new Schema(\n+      optional(1, \"c1\", Types.IntegerType.get()),\n+      optional(2, \"c2\", Types.StringType.get()),\n+      optional(3, \"c3\", Types.StringType.get())\n+  );\n+\n+  private static SparkSession spark;\n+\n+  @BeforeClass\n+  public static void startSpark() {\n+    TestRewriteDataFilesAction.spark = SparkSession.builder()\n+        .master(\"local[2]\")\n+        .getOrCreate();\n+  }\n+\n+  @AfterClass\n+  public static void stopSpark() {\n+    SparkSession currentSpark = TestRewriteDataFilesAction.spark;\n+    TestRewriteDataFilesAction.spark = null;\n+    currentSpark.stop();\n+  }\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  private String tableLocation = null;\n+\n+  @Before\n+  public void setupTableLocation() throws Exception {\n+    File tableDir = temp.newFolder();\n+    this.tableLocation = tableDir.toURI().toString();\n+  }\n+\n+  @Test\n+  public void testRewriteDataFilesEmptyTable() {\n+    PartitionSpec spec = PartitionSpec.unpartitioned();\n+    Map<String, String> options = Maps.newHashMap();\n+    Table table = TABLES.create(SCHEMA, spec, options, tableLocation);\n+\n+    Assert.assertNull(\"Table must be empty\", table.currentSnapshot());\n+\n+    Actions actions = Actions.forTable(table);\n+\n+    actions.rewriteDataFiles().execute();\n+\n+    Assert.assertNull(\"Table must stay empty\", table.currentSnapshot());\n+  }\n+\n+  @Test\n+  public void testRewriteDataFilesUnpartitionedTable() {\n+    PartitionSpec spec = PartitionSpec.unpartitioned();\n+    Map<String, String> options = Maps.newHashMap();\n+    Table table = TABLES.create(SCHEMA, spec, options, tableLocation);\n+\n+    List<ThreeColumnRecord> records1 = Lists.newArrayList(\n+        new ThreeColumnRecord(1, null, \"AAAA\"),\n+        new ThreeColumnRecord(1, \"BBBBBBBBBB\", \"BBBB\")\n+    );\n+    writeRecords(records1);\n+\n+    List<ThreeColumnRecord> records2 = Lists.newArrayList(\n+        new ThreeColumnRecord(2, \"CCCCCCCCCC\", \"CCCC\"),\n+        new ThreeColumnRecord(2, \"DDDDDDDDDD\", \"DDDD\")\n+    );\n+    writeRecords(records2);\n+\n+    table.refresh();\n+\n+    CloseableIterable<FileScanTask> tasks = table.newScan().planFiles();\n+    List<DataFile> dataFiles = Lists.newArrayList(CloseableIterable.transform(tasks, FileScanTask::file));\n+    Assert.assertEquals(\"Should have 4 data files before rewrite\", 4, dataFiles.size());\n+\n+    Actions actions = Actions.forTable(table);\n+\n+    RewriteDataFilesActionResult result = actions.rewriteDataFiles().execute();\n+    Assert.assertEquals(\"Action should rewrite 4 data files\", 4, result.deletedDataFiles().size());\n+    Assert.assertEquals(\"Action should add 1 data file\", 1, result.addedDataFiles().size());\n+\n+    table.refresh();\n+\n+    CloseableIterable<FileScanTask> tasks1 = table.newScan().planFiles();\n+    List<DataFile> dataFiles1 = Lists.newArrayList(CloseableIterable.transform(tasks1, FileScanTask::file));\n+    Assert.assertEquals(\"Should have 1 data files before rewrite\", 1, dataFiles1.size());\n+\n+    List<ThreeColumnRecord> expectedRecords = Lists.newArrayList();\n+    expectedRecords.addAll(records1);\n+    expectedRecords.addAll(records2);\n+\n+    Dataset<Row> resultDF = spark.read().format(\"iceberg\").load(tableLocation);\n+    List<ThreeColumnRecord> actualRecords = resultDF.sort(\"c1\", \"c2\")\n+        .as(Encoders.bean(ThreeColumnRecord.class))\n+        .collectAsList();\n+\n+    Assert.assertEquals(\"Rows must match\", expectedRecords, actualRecords);\n+  }\n+\n+  @Test\n+  public void testRewriteDataFilesPartitionedTable() {\n+    PartitionSpec spec = PartitionSpec.builderFor(SCHEMA)\n+        .identity(\"c1\")\n+        .truncate(\"c2\", 2)\n+        .build();\n+    Map<String, String> options = Maps.newHashMap();\n+    Table table = TABLES.create(SCHEMA, spec, options, tableLocation);\n+\n+    List<ThreeColumnRecord> records1 = Lists.newArrayList(\n+        new ThreeColumnRecord(1, \"AAAAAAAAAA\", \"AAAA\"),\n+        new ThreeColumnRecord(1, \"AAAAAAAAAA\", \"CCCC\")\n+    );\n+    writeRecords(records1);\n+\n+    List<ThreeColumnRecord> records2 = Lists.newArrayList(\n+        new ThreeColumnRecord(1, \"BBBBBBBBBB\", \"BBBB\"),\n+        new ThreeColumnRecord(1, \"BBBBBBBBBB\", \"DDDD\")\n+    );\n+    writeRecords(records2);\n+\n+    List<ThreeColumnRecord> records3 = Lists.newArrayList(\n+        new ThreeColumnRecord(2, \"AAAAAAAAAA\", \"EEEE\"),\n+        new ThreeColumnRecord(2, \"AAAAAAAAAA\", \"GGGG\")\n+    );\n+    writeRecords(records3);\n+\n+    List<ThreeColumnRecord> records4 = Lists.newArrayList(\n+        new ThreeColumnRecord(2, \"BBBBBBBBBB\", \"FFFF\"),\n+        new ThreeColumnRecord(2, \"BBBBBBBBBB\", \"HHHH\")\n+    );\n+    writeRecords(records4);\n+\n+    table.refresh();\n+\n+    CloseableIterable<FileScanTask> tasks = table.newScan().planFiles();\n+    List<DataFile> dataFiles = Lists.newArrayList(CloseableIterable.transform(tasks, FileScanTask::file));\n+    Assert.assertEquals(\"Should have 8 data files before rewrite\", 8, dataFiles.size());\n+\n+    Actions actions = Actions.forTable(table);\n+\n+    RewriteDataFilesActionResult result = actions.rewriteDataFiles().execute();\n+    Assert.assertEquals(\"Action should rewrite 8 data files\", 8, result.deletedDataFiles().size());\n+    Assert.assertEquals(\"Action should add 4 data file\", 4, result.addedDataFiles().size());\n+\n+    table.refresh();\n+\n+    CloseableIterable<FileScanTask> tasks1 = table.newScan().planFiles();\n+    List<DataFile> dataFiles1 = Lists.newArrayList(CloseableIterable.transform(tasks1, FileScanTask::file));\n+    Assert.assertEquals(\"Should have 4 data files before rewrite\", 4, dataFiles1.size());\n+\n+    List<ThreeColumnRecord> expectedRecords = Lists.newArrayList();\n+    expectedRecords.addAll(records1);\n+    expectedRecords.addAll(records2);\n+    expectedRecords.addAll(records3);\n+    expectedRecords.addAll(records4);\n+\n+    Dataset<Row> resultDF = spark.read().format(\"iceberg\").load(tableLocation);\n+    List<ThreeColumnRecord> actualRecords = resultDF.sort(\"c1\", \"c2\", \"c3\")\n+        .as(Encoders.bean(ThreeColumnRecord.class))\n+        .collectAsList();\n+\n+    Assert.assertEquals(\"Rows must match\", expectedRecords, actualRecords);\n+  }\n+\n+  @Test\n+  public void testRewriteDataFilesWithFilter() {\n+    PartitionSpec spec = PartitionSpec.builderFor(SCHEMA)\n+        .identity(\"c1\")\n+        .truncate(\"c2\", 2)\n+        .build();\n+    Map<String, String> options = Maps.newHashMap();\n+    Table table = TABLES.create(SCHEMA, spec, options, tableLocation);\n+\n+    List<ThreeColumnRecord> records1 = Lists.newArrayList(\n+        new ThreeColumnRecord(1, \"AAAAAAAAAA\", \"AAAA\"),\n+        new ThreeColumnRecord(1, \"AAAAAAAAAA\", \"CCCC\")\n+    );\n+    writeRecords(records1);\n+\n+    List<ThreeColumnRecord> records2 = Lists.newArrayList(\n+        new ThreeColumnRecord(1, \"BBBBBBBBBB\", \"BBBB\"),\n+        new ThreeColumnRecord(1, \"BBBBBBBBBB\", \"DDDD\")\n+    );\n+    writeRecords(records2);\n+\n+    List<ThreeColumnRecord> records3 = Lists.newArrayList(\n+        new ThreeColumnRecord(2, \"AAAAAAAAAA\", \"EEEE\"),\n+        new ThreeColumnRecord(2, \"AAAAAAAAAA\", \"GGGG\")\n+    );\n+    writeRecords(records3);\n+\n+    List<ThreeColumnRecord> records4 = Lists.newArrayList(\n+        new ThreeColumnRecord(2, \"BBBBBBBBBB\", \"FFFF\"),\n+        new ThreeColumnRecord(2, \"BBBBBBBBBB\", \"HHHH\")\n+    );\n+    writeRecords(records4);\n+\n+    table.refresh();\n+\n+    CloseableIterable<FileScanTask> tasks = table.newScan().planFiles();\n+    List<DataFile> dataFiles = Lists.newArrayList(CloseableIterable.transform(tasks, FileScanTask::file));\n+    Assert.assertEquals(\"Should have 8 data files before rewrite\", 8, dataFiles.size());\n+\n+    Actions actions = Actions.forTable(table);\n+\n+    RewriteDataFilesActionResult result = actions\n+        .rewriteDataFiles()\n+        .filter(Expressions.equal(\"c1\", 1))\n+        .filter(Expressions.startsWith(\"c2\", \"AA\"))\n+        .execute();\n+    Assert.assertEquals(\"Action should rewrite 2 data files\", 2, result.deletedDataFiles().size());\n+    Assert.assertEquals(\"Action should add 1 data file\", 1, result.addedDataFiles().size());\n+\n+    table.refresh();\n+\n+    CloseableIterable<FileScanTask> tasks1 = table.newScan().planFiles();\n+    List<DataFile> dataFiles1 = Lists.newArrayList(CloseableIterable.transform(tasks1, FileScanTask::file));\n+    Assert.assertEquals(\"Should have 7 data files before rewrite\", 7, dataFiles1.size());\n+\n+    List<ThreeColumnRecord> expectedRecords = Lists.newArrayList();\n+    expectedRecords.addAll(records1);\n+    expectedRecords.addAll(records2);\n+    expectedRecords.addAll(records3);\n+    expectedRecords.addAll(records4);\n+\n+    Dataset<Row> resultDF = spark.read().format(\"iceberg\").load(tableLocation);\n+    List<ThreeColumnRecord> actualRecords = resultDF.sort(\"c1\", \"c2\", \"c3\")\n+        .as(Encoders.bean(ThreeColumnRecord.class))\n+        .collectAsList();\n+\n+    Assert.assertEquals(\"Rows must match\", expectedRecords, actualRecords);\n+  }\n+\n+  @Test\n+  public void testRewriteLargeTableHasResiduals() {\n+    PartitionSpec spec = PartitionSpec.builderFor(SCHEMA).build();\n+    Map<String, String> options = Maps.newHashMap();\n+    options.put(TableProperties.PARQUET_ROW_GROUP_SIZE_BYTES, \"100\");\n+    Table table = TABLES.create(SCHEMA, spec, options, tableLocation);\n+\n+    // all records belong to the same partition\n+    List<ThreeColumnRecord> records = Lists.newArrayList();\n+    for (int i = 0; i < 100; i++) {\n+      records.add(new ThreeColumnRecord(i, String.valueOf(i), String.valueOf(i % 4)));\n+    }\n+    Dataset<Row> df = spark.createDataFrame(records, ThreeColumnRecord.class);\n+    writeDF(df);\n+\n+    table.refresh();\n+\n+    CloseableIterable<FileScanTask> tasks = table.newScan()\n+        .ignoreResiduals()\n+        .filter(Expressions.equal(\"c3\", \"0\"))\n+        .planFiles();\n+    for (FileScanTask task : tasks) {\n+      Assert.assertEquals(\"Residuals must be ignored\", Expressions.alwaysTrue(), task.residual());\n+    }\n+    List<DataFile> dataFiles = Lists.newArrayList(CloseableIterable.transform(tasks, FileScanTask::file));\n+    Assert.assertEquals(\"Should have 2 data files before rewrite\", 2, dataFiles.size());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2bd9f4aee4714ec519370efe4929965b75b6e5c0"}, "originalPosition": 311}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI5MDUyNzc3", "url": "https://github.com/apache/iceberg/pull/1083#pullrequestreview-429052777", "createdAt": "2020-06-11T15:50:21Z", "commit": {"oid": "2bd9f4aee4714ec519370efe4929965b75b6e5c0"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQxNTo1MDoyMVrOGijxrQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQxNTo1MDoyMVrOGijxrQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODg5MDkyNQ==", "bodyText": "Hm, if my split size is 128 MB and target file size is 512 MB, what is the size of files that we will create?", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r438890925", "createdAt": "2020-06-11T15:50:21Z", "author": {"login": "aokolnychyi"}, "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,281 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ListMultimap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimaps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.relocated.com.google.common.collect.Streams;\n+import org.apache.iceberg.spark.source.RowDataRewriter;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.StructLikeWrapper;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.SparkSession;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RewriteDataFilesAction\n+    extends BaseSnapshotUpdateAction<RewriteDataFilesAction, RewriteDataFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteDataFilesAction.class);\n+\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final EncryptionManager encryptionManager;\n+  private final boolean caseSensitive;\n+  private long targetSizeInBytes;\n+  private int splitLookback;\n+  private long splitOpenFileCost;\n+\n+  private PartitionSpec spec = null;\n+  private Expression filter;\n+\n+  RewriteDataFilesAction(SparkSession spark, Table table) {\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.table = table;\n+    this.spec = table.spec();\n+    this.filter = Expressions.alwaysTrue();\n+    this.caseSensitive = Boolean.parseBoolean(spark.conf().get(\"spark.sql.caseSensitive\", \"false\"));\n+\n+    long splitSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    long targetFileSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES,\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n+    this.targetSizeInBytes = Math.min(splitSize, targetFileSize);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2bd9f4aee4714ec519370efe4929965b75b6e5c0"}, "originalPosition": 92}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4475, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}