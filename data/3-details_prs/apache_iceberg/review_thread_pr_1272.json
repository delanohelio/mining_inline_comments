{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDU5MDc3MzI1", "number": 1272, "reviewThreads": {"totalCount": 20, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxNDozNTozOVrOETtSWw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNVQwMDo0MjozNlrOEbzv3Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5MTAwMzc5OnYy", "diffSide": "RIGHT", "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxNDozNTozOVrOG5l6LQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMjo1MDo1N1rOG56YgQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA0MzExNw==", "bodyText": "This class seems don't have to be public, only the FlinkParquetReader will access those readers.  It also don't need to be accessed by other classes I think.", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r463043117", "createdAt": "2020-07-30T14:35:39Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java", "diffHunk": "@@ -19,64 +19,723 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.math.BigDecimal;\n+import java.math.BigInteger;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n import java.util.List;\n-import org.apache.flink.types.Row;\n+import java.util.Map;\n+import org.apache.commons.lang3.ArrayUtils;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RawValueData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n import org.apache.iceberg.Schema;\n-import org.apache.iceberg.data.parquet.BaseParquetReaders;\n+import org.apache.iceberg.parquet.ParquetSchemaUtil;\n import org.apache.iceberg.parquet.ParquetValueReader;\n import org.apache.iceberg.parquet.ParquetValueReaders;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetReaders extends BaseParquetReaders<Row> {\n+public class FlinkParquetReaders {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e78c2ecdee84c3cf0d54afa84e111faaa0e498df"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM3ODU2MQ==", "bodyText": "Make sense to me.", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r463378561", "createdAt": "2020-07-31T02:50:57Z", "author": {"login": "chenjunjiedada"}, "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java", "diffHunk": "@@ -19,64 +19,723 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.math.BigDecimal;\n+import java.math.BigInteger;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n import java.util.List;\n-import org.apache.flink.types.Row;\n+import java.util.Map;\n+import org.apache.commons.lang3.ArrayUtils;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RawValueData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n import org.apache.iceberg.Schema;\n-import org.apache.iceberg.data.parquet.BaseParquetReaders;\n+import org.apache.iceberg.parquet.ParquetSchemaUtil;\n import org.apache.iceberg.parquet.ParquetValueReader;\n import org.apache.iceberg.parquet.ParquetValueReaders;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetReaders extends BaseParquetReaders<Row> {\n+public class FlinkParquetReaders {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA0MzExNw=="}, "originalCommit": {"oid": "e78c2ecdee84c3cf0d54afa84e111faaa0e498df"}, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5MTAxNTkxOnYy", "diffSide": "RIGHT", "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxNDozODoxOFrOG5mB3g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMzoxNjo0NlrOG56uJA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA0NTA4Ng==", "bodyText": "nit: the comment is not complete ?", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r463045086", "createdAt": "2020-07-30T14:38:18Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java", "diffHunk": "@@ -19,64 +19,723 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.math.BigDecimal;\n+import java.math.BigInteger;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n import java.util.List;\n-import org.apache.flink.types.Row;\n+import java.util.Map;\n+import org.apache.commons.lang3.ArrayUtils;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RawValueData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n import org.apache.iceberg.Schema;\n-import org.apache.iceberg.data.parquet.BaseParquetReaders;\n+import org.apache.iceberg.parquet.ParquetSchemaUtil;\n import org.apache.iceberg.parquet.ParquetValueReader;\n import org.apache.iceberg.parquet.ParquetValueReaders;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetReaders extends BaseParquetReaders<Row> {\n+public class FlinkParquetReaders {\n+  private FlinkParquetReaders() {\n+  }\n \n-  private static final FlinkParquetReaders INSTANCE = new FlinkParquetReaders();\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema, MessageType fileSchema) {\n+    return buildReader(expectedSchema, fileSchema, ImmutableMap.of());\n+  }\n \n-  private FlinkParquetReaders() {\n+  @SuppressWarnings(\"unchecked\")\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema,\n+                                                        MessageType fileSchema,\n+                                                        Map<Integer, ?> idToConstant) {\n+    ReadBuilder builder = new ReadBuilder(fileSchema, idToConstant);\n+    if (ParquetSchemaUtil.hasIds(fileSchema)) {\n+      return (ParquetValueReader<RowData>)\n+          TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema, builder);\n+    } else {\n+      return (ParquetValueReader<RowData>)\n+          TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n+              new FallbackReadBuilder(builder));\n+    }\n+  }\n+\n+  private static class FallbackReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private MessageType type;\n+    private final TypeWithSchemaVisitor<ParquetValueReader<?>> builder;\n+\n+    FallbackReadBuilder(TypeWithSchemaVisitor<ParquetValueReader<?>> builder) {\n+      this.builder = builder;\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> message(Types.StructType expected, MessageType message,\n+                                         List<ParquetValueReader<?>> fieldReaders) {\n+      // the top level matches by ID, but the remaining IDs are missing\n+      this.type = message;\n+      return builder.struct(expected, message, fieldReaders);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> struct(Types.StructType ignored, GroupType struct,\n+                                        List<ParquetValueReader<?>> fieldReaders) {\n+      // the expected struct is ignored because nested fields are never found when the", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e78c2ecdee84c3cf0d54afa84e111faaa0e498df"}, "originalPosition": 84}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM4NDEwMA==", "bodyText": "fixed.", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r463384100", "createdAt": "2020-07-31T03:16:46Z", "author": {"login": "chenjunjiedada"}, "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java", "diffHunk": "@@ -19,64 +19,723 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.math.BigDecimal;\n+import java.math.BigInteger;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n import java.util.List;\n-import org.apache.flink.types.Row;\n+import java.util.Map;\n+import org.apache.commons.lang3.ArrayUtils;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RawValueData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n import org.apache.iceberg.Schema;\n-import org.apache.iceberg.data.parquet.BaseParquetReaders;\n+import org.apache.iceberg.parquet.ParquetSchemaUtil;\n import org.apache.iceberg.parquet.ParquetValueReader;\n import org.apache.iceberg.parquet.ParquetValueReaders;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetReaders extends BaseParquetReaders<Row> {\n+public class FlinkParquetReaders {\n+  private FlinkParquetReaders() {\n+  }\n \n-  private static final FlinkParquetReaders INSTANCE = new FlinkParquetReaders();\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema, MessageType fileSchema) {\n+    return buildReader(expectedSchema, fileSchema, ImmutableMap.of());\n+  }\n \n-  private FlinkParquetReaders() {\n+  @SuppressWarnings(\"unchecked\")\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema,\n+                                                        MessageType fileSchema,\n+                                                        Map<Integer, ?> idToConstant) {\n+    ReadBuilder builder = new ReadBuilder(fileSchema, idToConstant);\n+    if (ParquetSchemaUtil.hasIds(fileSchema)) {\n+      return (ParquetValueReader<RowData>)\n+          TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema, builder);\n+    } else {\n+      return (ParquetValueReader<RowData>)\n+          TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n+              new FallbackReadBuilder(builder));\n+    }\n+  }\n+\n+  private static class FallbackReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private MessageType type;\n+    private final TypeWithSchemaVisitor<ParquetValueReader<?>> builder;\n+\n+    FallbackReadBuilder(TypeWithSchemaVisitor<ParquetValueReader<?>> builder) {\n+      this.builder = builder;\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> message(Types.StructType expected, MessageType message,\n+                                         List<ParquetValueReader<?>> fieldReaders) {\n+      // the top level matches by ID, but the remaining IDs are missing\n+      this.type = message;\n+      return builder.struct(expected, message, fieldReaders);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> struct(Types.StructType ignored, GroupType struct,\n+                                        List<ParquetValueReader<?>> fieldReaders) {\n+      // the expected struct is ignored because nested fields are never found when the", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA0NTA4Ng=="}, "originalCommit": {"oid": "e78c2ecdee84c3cf0d54afa84e111faaa0e498df"}, "originalPosition": 84}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5MTAzODc3OnYy", "diffSide": "RIGHT", "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxNDo0MzoxMlrOG5mP7Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQyMDowMjoxM1rOG6UjvA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA0ODY4NQ==", "bodyText": "Q: is there any problem here ?  the original type is TIME_MICROS, while the reader name is TimeMillisReader ?", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r463048685", "createdAt": "2020-07-30T14:43:12Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java", "diffHunk": "@@ -19,64 +19,723 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.math.BigDecimal;\n+import java.math.BigInteger;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n import java.util.List;\n-import org.apache.flink.types.Row;\n+import java.util.Map;\n+import org.apache.commons.lang3.ArrayUtils;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RawValueData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n import org.apache.iceberg.Schema;\n-import org.apache.iceberg.data.parquet.BaseParquetReaders;\n+import org.apache.iceberg.parquet.ParquetSchemaUtil;\n import org.apache.iceberg.parquet.ParquetValueReader;\n import org.apache.iceberg.parquet.ParquetValueReaders;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetReaders extends BaseParquetReaders<Row> {\n+public class FlinkParquetReaders {\n+  private FlinkParquetReaders() {\n+  }\n \n-  private static final FlinkParquetReaders INSTANCE = new FlinkParquetReaders();\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema, MessageType fileSchema) {\n+    return buildReader(expectedSchema, fileSchema, ImmutableMap.of());\n+  }\n \n-  private FlinkParquetReaders() {\n+  @SuppressWarnings(\"unchecked\")\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema,\n+                                                        MessageType fileSchema,\n+                                                        Map<Integer, ?> idToConstant) {\n+    ReadBuilder builder = new ReadBuilder(fileSchema, idToConstant);\n+    if (ParquetSchemaUtil.hasIds(fileSchema)) {\n+      return (ParquetValueReader<RowData>)\n+          TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema, builder);\n+    } else {\n+      return (ParquetValueReader<RowData>)\n+          TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n+              new FallbackReadBuilder(builder));\n+    }\n+  }\n+\n+  private static class FallbackReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private MessageType type;\n+    private final TypeWithSchemaVisitor<ParquetValueReader<?>> builder;\n+\n+    FallbackReadBuilder(TypeWithSchemaVisitor<ParquetValueReader<?>> builder) {\n+      this.builder = builder;\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> message(Types.StructType expected, MessageType message,\n+                                         List<ParquetValueReader<?>> fieldReaders) {\n+      // the top level matches by ID, but the remaining IDs are missing\n+      this.type = message;\n+      return builder.struct(expected, message, fieldReaders);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> struct(Types.StructType ignored, GroupType struct,\n+                                        List<ParquetValueReader<?>> fieldReaders) {\n+      // the expected struct is ignored because nested fields are never found when the\n+      List<ParquetValueReader<?>> newFields = Lists.newArrayListWithExpectedSize(\n+          fieldReaders.size());\n+      List<Type> types = Lists.newArrayListWithExpectedSize(fieldReaders.size());\n+      List<Type> fields = struct.getFields();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        Type fieldType = fields.get(i);\n+        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n+        newFields.add(ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n+        types.add(fieldType);\n+      }\n+\n+      return new RowDataReader(types, newFields);\n+    }\n+  }\n+\n+  private static class ReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private final MessageType type;\n+    private final Map<Integer, ?> idToConstant;\n+\n+    ReadBuilder(MessageType type, Map<Integer, ?> idToConstant) {\n+      this.type = type;\n+      this.idToConstant = idToConstant;\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> message(Types.StructType expected, MessageType message,\n+                                         List<ParquetValueReader<?>> fieldReaders) {\n+      return struct(expected, message.asGroupType(), fieldReaders);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> struct(Types.StructType expected, GroupType struct,\n+                                        List<ParquetValueReader<?>> fieldReaders) {\n+      // match the expected struct's order\n+      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n+      Map<Integer, Type> typesById = Maps.newHashMap();\n+      List<Type> fields = struct.getFields();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        Type fieldType = fields.get(i);\n+        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n+        if (fieldType.getId() != null) {\n+          int id = fieldType.getId().intValue();\n+          readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n+          typesById.put(id, fieldType);\n+        }\n+      }\n+\n+      List<Types.NestedField> expectedFields = expected != null ?\n+          expected.fields() : ImmutableList.of();\n+      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n+          expectedFields.size());\n+      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n+      for (Types.NestedField field : expectedFields) {\n+        int id = field.fieldId();\n+        if (idToConstant.containsKey(id)) {\n+          // containsKey is used because the constant may be null\n+          reorderedFields.add(ParquetValueReaders.constant(idToConstant.get(id)));\n+          types.add(null);\n+        } else {\n+          ParquetValueReader<?> reader = readersById.get(id);\n+          if (reader != null) {\n+            reorderedFields.add(reader);\n+            types.add(typesById.get(id));\n+          } else {\n+            reorderedFields.add(ParquetValueReaders.nulls());\n+            types.add(null);\n+          }\n+        }\n+      }\n+\n+      return new RowDataReader(types, reorderedFields);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> list(Types.ListType expectedList, GroupType array,\n+                                      ParquetValueReader<?> elementReader) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type elementType = repeated.getType(0);\n+      int elementD = type.getMaxDefinitionLevel(path(elementType.getName())) - 1;\n+\n+      return new ArrayReader<>(repeatedD, repeatedR, ParquetValueReaders.option(elementType, elementD, elementReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> map(Types.MapType expectedMap, GroupType map,\n+                                     ParquetValueReader<?> keyReader,\n+                                     ParquetValueReader<?> valueReader) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type keyType = repeatedKeyValue.getType(0);\n+      int keyD = type.getMaxDefinitionLevel(path(keyType.getName())) - 1;\n+      Type valueType = repeatedKeyValue.getType(1);\n+      int valueD = type.getMaxDefinitionLevel(path(valueType.getName())) - 1;\n+\n+      return new MapReader<>(repeatedD, repeatedR,\n+          ParquetValueReaders.option(keyType, keyD, keyReader),\n+          ParquetValueReaders.option(valueType, valueD, valueReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> primitive(org.apache.iceberg.types.Type.PrimitiveType expected,\n+                                           PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return new StringReader(desc);\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+          case DATE:\n+            if (expected != null && expected.typeId() == Types.LongType.get().typeId()) {\n+              return new ParquetValueReaders.IntAsLongReader(desc);\n+            } else {\n+              return new ParquetValueReaders.UnboxedReader<>(desc);\n+            }\n+          case TIME_MICROS:\n+            return new TimeMillisReader(desc);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e78c2ecdee84c3cf0d54afa84e111faaa0e498df"}, "originalPosition": 214}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM2NDI5NA==", "bodyText": "This is because Flink only supports milliseconds and the parquet store microseconds, so the naming express that it reads out milliseconds.", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r463364294", "createdAt": "2020-07-31T01:52:14Z", "author": {"login": "chenjunjiedada"}, "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java", "diffHunk": "@@ -19,64 +19,723 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.math.BigDecimal;\n+import java.math.BigInteger;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n import java.util.List;\n-import org.apache.flink.types.Row;\n+import java.util.Map;\n+import org.apache.commons.lang3.ArrayUtils;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RawValueData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n import org.apache.iceberg.Schema;\n-import org.apache.iceberg.data.parquet.BaseParquetReaders;\n+import org.apache.iceberg.parquet.ParquetSchemaUtil;\n import org.apache.iceberg.parquet.ParquetValueReader;\n import org.apache.iceberg.parquet.ParquetValueReaders;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetReaders extends BaseParquetReaders<Row> {\n+public class FlinkParquetReaders {\n+  private FlinkParquetReaders() {\n+  }\n \n-  private static final FlinkParquetReaders INSTANCE = new FlinkParquetReaders();\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema, MessageType fileSchema) {\n+    return buildReader(expectedSchema, fileSchema, ImmutableMap.of());\n+  }\n \n-  private FlinkParquetReaders() {\n+  @SuppressWarnings(\"unchecked\")\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema,\n+                                                        MessageType fileSchema,\n+                                                        Map<Integer, ?> idToConstant) {\n+    ReadBuilder builder = new ReadBuilder(fileSchema, idToConstant);\n+    if (ParquetSchemaUtil.hasIds(fileSchema)) {\n+      return (ParquetValueReader<RowData>)\n+          TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema, builder);\n+    } else {\n+      return (ParquetValueReader<RowData>)\n+          TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n+              new FallbackReadBuilder(builder));\n+    }\n+  }\n+\n+  private static class FallbackReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private MessageType type;\n+    private final TypeWithSchemaVisitor<ParquetValueReader<?>> builder;\n+\n+    FallbackReadBuilder(TypeWithSchemaVisitor<ParquetValueReader<?>> builder) {\n+      this.builder = builder;\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> message(Types.StructType expected, MessageType message,\n+                                         List<ParquetValueReader<?>> fieldReaders) {\n+      // the top level matches by ID, but the remaining IDs are missing\n+      this.type = message;\n+      return builder.struct(expected, message, fieldReaders);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> struct(Types.StructType ignored, GroupType struct,\n+                                        List<ParquetValueReader<?>> fieldReaders) {\n+      // the expected struct is ignored because nested fields are never found when the\n+      List<ParquetValueReader<?>> newFields = Lists.newArrayListWithExpectedSize(\n+          fieldReaders.size());\n+      List<Type> types = Lists.newArrayListWithExpectedSize(fieldReaders.size());\n+      List<Type> fields = struct.getFields();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        Type fieldType = fields.get(i);\n+        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n+        newFields.add(ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n+        types.add(fieldType);\n+      }\n+\n+      return new RowDataReader(types, newFields);\n+    }\n+  }\n+\n+  private static class ReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private final MessageType type;\n+    private final Map<Integer, ?> idToConstant;\n+\n+    ReadBuilder(MessageType type, Map<Integer, ?> idToConstant) {\n+      this.type = type;\n+      this.idToConstant = idToConstant;\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> message(Types.StructType expected, MessageType message,\n+                                         List<ParquetValueReader<?>> fieldReaders) {\n+      return struct(expected, message.asGroupType(), fieldReaders);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> struct(Types.StructType expected, GroupType struct,\n+                                        List<ParquetValueReader<?>> fieldReaders) {\n+      // match the expected struct's order\n+      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n+      Map<Integer, Type> typesById = Maps.newHashMap();\n+      List<Type> fields = struct.getFields();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        Type fieldType = fields.get(i);\n+        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n+        if (fieldType.getId() != null) {\n+          int id = fieldType.getId().intValue();\n+          readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n+          typesById.put(id, fieldType);\n+        }\n+      }\n+\n+      List<Types.NestedField> expectedFields = expected != null ?\n+          expected.fields() : ImmutableList.of();\n+      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n+          expectedFields.size());\n+      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n+      for (Types.NestedField field : expectedFields) {\n+        int id = field.fieldId();\n+        if (idToConstant.containsKey(id)) {\n+          // containsKey is used because the constant may be null\n+          reorderedFields.add(ParquetValueReaders.constant(idToConstant.get(id)));\n+          types.add(null);\n+        } else {\n+          ParquetValueReader<?> reader = readersById.get(id);\n+          if (reader != null) {\n+            reorderedFields.add(reader);\n+            types.add(typesById.get(id));\n+          } else {\n+            reorderedFields.add(ParquetValueReaders.nulls());\n+            types.add(null);\n+          }\n+        }\n+      }\n+\n+      return new RowDataReader(types, reorderedFields);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> list(Types.ListType expectedList, GroupType array,\n+                                      ParquetValueReader<?> elementReader) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type elementType = repeated.getType(0);\n+      int elementD = type.getMaxDefinitionLevel(path(elementType.getName())) - 1;\n+\n+      return new ArrayReader<>(repeatedD, repeatedR, ParquetValueReaders.option(elementType, elementD, elementReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> map(Types.MapType expectedMap, GroupType map,\n+                                     ParquetValueReader<?> keyReader,\n+                                     ParquetValueReader<?> valueReader) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type keyType = repeatedKeyValue.getType(0);\n+      int keyD = type.getMaxDefinitionLevel(path(keyType.getName())) - 1;\n+      Type valueType = repeatedKeyValue.getType(1);\n+      int valueD = type.getMaxDefinitionLevel(path(valueType.getName())) - 1;\n+\n+      return new MapReader<>(repeatedD, repeatedR,\n+          ParquetValueReaders.option(keyType, keyD, keyReader),\n+          ParquetValueReaders.option(valueType, valueD, valueReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> primitive(org.apache.iceberg.types.Type.PrimitiveType expected,\n+                                           PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return new StringReader(desc);\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+          case DATE:\n+            if (expected != null && expected.typeId() == Types.LongType.get().typeId()) {\n+              return new ParquetValueReaders.IntAsLongReader(desc);\n+            } else {\n+              return new ParquetValueReaders.UnboxedReader<>(desc);\n+            }\n+          case TIME_MICROS:\n+            return new TimeMillisReader(desc);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA0ODY4NQ=="}, "originalCommit": {"oid": "e78c2ecdee84c3cf0d54afa84e111faaa0e498df"}, "originalPosition": 214}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzgwNzQyMA==", "bodyText": "I agree this is confusing. There are other places where we use a unit in the class name to indicate the unit being read. Instead, let's be more specific and use something like LossyMicrosToMillisTimeReader.", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r463807420", "createdAt": "2020-07-31T20:02:13Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java", "diffHunk": "@@ -19,64 +19,723 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.math.BigDecimal;\n+import java.math.BigInteger;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n import java.util.List;\n-import org.apache.flink.types.Row;\n+import java.util.Map;\n+import org.apache.commons.lang3.ArrayUtils;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RawValueData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n import org.apache.iceberg.Schema;\n-import org.apache.iceberg.data.parquet.BaseParquetReaders;\n+import org.apache.iceberg.parquet.ParquetSchemaUtil;\n import org.apache.iceberg.parquet.ParquetValueReader;\n import org.apache.iceberg.parquet.ParquetValueReaders;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetReaders extends BaseParquetReaders<Row> {\n+public class FlinkParquetReaders {\n+  private FlinkParquetReaders() {\n+  }\n \n-  private static final FlinkParquetReaders INSTANCE = new FlinkParquetReaders();\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema, MessageType fileSchema) {\n+    return buildReader(expectedSchema, fileSchema, ImmutableMap.of());\n+  }\n \n-  private FlinkParquetReaders() {\n+  @SuppressWarnings(\"unchecked\")\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema,\n+                                                        MessageType fileSchema,\n+                                                        Map<Integer, ?> idToConstant) {\n+    ReadBuilder builder = new ReadBuilder(fileSchema, idToConstant);\n+    if (ParquetSchemaUtil.hasIds(fileSchema)) {\n+      return (ParquetValueReader<RowData>)\n+          TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema, builder);\n+    } else {\n+      return (ParquetValueReader<RowData>)\n+          TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n+              new FallbackReadBuilder(builder));\n+    }\n+  }\n+\n+  private static class FallbackReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private MessageType type;\n+    private final TypeWithSchemaVisitor<ParquetValueReader<?>> builder;\n+\n+    FallbackReadBuilder(TypeWithSchemaVisitor<ParquetValueReader<?>> builder) {\n+      this.builder = builder;\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> message(Types.StructType expected, MessageType message,\n+                                         List<ParquetValueReader<?>> fieldReaders) {\n+      // the top level matches by ID, but the remaining IDs are missing\n+      this.type = message;\n+      return builder.struct(expected, message, fieldReaders);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> struct(Types.StructType ignored, GroupType struct,\n+                                        List<ParquetValueReader<?>> fieldReaders) {\n+      // the expected struct is ignored because nested fields are never found when the\n+      List<ParquetValueReader<?>> newFields = Lists.newArrayListWithExpectedSize(\n+          fieldReaders.size());\n+      List<Type> types = Lists.newArrayListWithExpectedSize(fieldReaders.size());\n+      List<Type> fields = struct.getFields();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        Type fieldType = fields.get(i);\n+        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n+        newFields.add(ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n+        types.add(fieldType);\n+      }\n+\n+      return new RowDataReader(types, newFields);\n+    }\n+  }\n+\n+  private static class ReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private final MessageType type;\n+    private final Map<Integer, ?> idToConstant;\n+\n+    ReadBuilder(MessageType type, Map<Integer, ?> idToConstant) {\n+      this.type = type;\n+      this.idToConstant = idToConstant;\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> message(Types.StructType expected, MessageType message,\n+                                         List<ParquetValueReader<?>> fieldReaders) {\n+      return struct(expected, message.asGroupType(), fieldReaders);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> struct(Types.StructType expected, GroupType struct,\n+                                        List<ParquetValueReader<?>> fieldReaders) {\n+      // match the expected struct's order\n+      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n+      Map<Integer, Type> typesById = Maps.newHashMap();\n+      List<Type> fields = struct.getFields();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        Type fieldType = fields.get(i);\n+        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n+        if (fieldType.getId() != null) {\n+          int id = fieldType.getId().intValue();\n+          readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n+          typesById.put(id, fieldType);\n+        }\n+      }\n+\n+      List<Types.NestedField> expectedFields = expected != null ?\n+          expected.fields() : ImmutableList.of();\n+      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n+          expectedFields.size());\n+      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n+      for (Types.NestedField field : expectedFields) {\n+        int id = field.fieldId();\n+        if (idToConstant.containsKey(id)) {\n+          // containsKey is used because the constant may be null\n+          reorderedFields.add(ParquetValueReaders.constant(idToConstant.get(id)));\n+          types.add(null);\n+        } else {\n+          ParquetValueReader<?> reader = readersById.get(id);\n+          if (reader != null) {\n+            reorderedFields.add(reader);\n+            types.add(typesById.get(id));\n+          } else {\n+            reorderedFields.add(ParquetValueReaders.nulls());\n+            types.add(null);\n+          }\n+        }\n+      }\n+\n+      return new RowDataReader(types, reorderedFields);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> list(Types.ListType expectedList, GroupType array,\n+                                      ParquetValueReader<?> elementReader) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type elementType = repeated.getType(0);\n+      int elementD = type.getMaxDefinitionLevel(path(elementType.getName())) - 1;\n+\n+      return new ArrayReader<>(repeatedD, repeatedR, ParquetValueReaders.option(elementType, elementD, elementReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> map(Types.MapType expectedMap, GroupType map,\n+                                     ParquetValueReader<?> keyReader,\n+                                     ParquetValueReader<?> valueReader) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type keyType = repeatedKeyValue.getType(0);\n+      int keyD = type.getMaxDefinitionLevel(path(keyType.getName())) - 1;\n+      Type valueType = repeatedKeyValue.getType(1);\n+      int valueD = type.getMaxDefinitionLevel(path(valueType.getName())) - 1;\n+\n+      return new MapReader<>(repeatedD, repeatedR,\n+          ParquetValueReaders.option(keyType, keyD, keyReader),\n+          ParquetValueReaders.option(valueType, valueD, valueReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> primitive(org.apache.iceberg.types.Type.PrimitiveType expected,\n+                                           PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return new StringReader(desc);\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+          case DATE:\n+            if (expected != null && expected.typeId() == Types.LongType.get().typeId()) {\n+              return new ParquetValueReaders.IntAsLongReader(desc);\n+            } else {\n+              return new ParquetValueReaders.UnboxedReader<>(desc);\n+            }\n+          case TIME_MICROS:\n+            return new TimeMillisReader(desc);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA0ODY4NQ=="}, "originalCommit": {"oid": "e78c2ecdee84c3cf0d54afa84e111faaa0e498df"}, "originalPosition": 214}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5MTA2NTk5OnYy", "diffSide": "RIGHT", "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxNDo0ODo0M1rOG5mgNg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMTo1NDo1OVrOG55jSw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA1Mjg1NA==", "bodyText": "Will any subclass of ReadBuilder access the message type ?", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r463052854", "createdAt": "2020-07-30T14:48:43Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java", "diffHunk": "@@ -19,64 +19,723 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.math.BigDecimal;\n+import java.math.BigInteger;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n import java.util.List;\n-import org.apache.flink.types.Row;\n+import java.util.Map;\n+import org.apache.commons.lang3.ArrayUtils;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RawValueData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n import org.apache.iceberg.Schema;\n-import org.apache.iceberg.data.parquet.BaseParquetReaders;\n+import org.apache.iceberg.parquet.ParquetSchemaUtil;\n import org.apache.iceberg.parquet.ParquetValueReader;\n import org.apache.iceberg.parquet.ParquetValueReaders;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetReaders extends BaseParquetReaders<Row> {\n+public class FlinkParquetReaders {\n+  private FlinkParquetReaders() {\n+  }\n \n-  private static final FlinkParquetReaders INSTANCE = new FlinkParquetReaders();\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema, MessageType fileSchema) {\n+    return buildReader(expectedSchema, fileSchema, ImmutableMap.of());\n+  }\n \n-  private FlinkParquetReaders() {\n+  @SuppressWarnings(\"unchecked\")\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema,\n+                                                        MessageType fileSchema,\n+                                                        Map<Integer, ?> idToConstant) {\n+    ReadBuilder builder = new ReadBuilder(fileSchema, idToConstant);\n+    if (ParquetSchemaUtil.hasIds(fileSchema)) {\n+      return (ParquetValueReader<RowData>)\n+          TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema, builder);\n+    } else {\n+      return (ParquetValueReader<RowData>)\n+          TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n+              new FallbackReadBuilder(builder));\n+    }\n+  }\n+\n+  private static class FallbackReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private MessageType type;\n+    private final TypeWithSchemaVisitor<ParquetValueReader<?>> builder;\n+\n+    FallbackReadBuilder(TypeWithSchemaVisitor<ParquetValueReader<?>> builder) {\n+      this.builder = builder;\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> message(Types.StructType expected, MessageType message,\n+                                         List<ParquetValueReader<?>> fieldReaders) {\n+      // the top level matches by ID, but the remaining IDs are missing\n+      this.type = message;\n+      return builder.struct(expected, message, fieldReaders);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> struct(Types.StructType ignored, GroupType struct,\n+                                        List<ParquetValueReader<?>> fieldReaders) {\n+      // the expected struct is ignored because nested fields are never found when the\n+      List<ParquetValueReader<?>> newFields = Lists.newArrayListWithExpectedSize(\n+          fieldReaders.size());\n+      List<Type> types = Lists.newArrayListWithExpectedSize(fieldReaders.size());\n+      List<Type> fields = struct.getFields();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        Type fieldType = fields.get(i);\n+        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n+        newFields.add(ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n+        types.add(fieldType);\n+      }\n+\n+      return new RowDataReader(types, newFields);\n+    }\n+  }\n+\n+  private static class ReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private final MessageType type;\n+    private final Map<Integer, ?> idToConstant;\n+\n+    ReadBuilder(MessageType type, Map<Integer, ?> idToConstant) {\n+      this.type = type;\n+      this.idToConstant = idToConstant;\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> message(Types.StructType expected, MessageType message,\n+                                         List<ParquetValueReader<?>> fieldReaders) {\n+      return struct(expected, message.asGroupType(), fieldReaders);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> struct(Types.StructType expected, GroupType struct,\n+                                        List<ParquetValueReader<?>> fieldReaders) {\n+      // match the expected struct's order\n+      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n+      Map<Integer, Type> typesById = Maps.newHashMap();\n+      List<Type> fields = struct.getFields();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        Type fieldType = fields.get(i);\n+        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n+        if (fieldType.getId() != null) {\n+          int id = fieldType.getId().intValue();\n+          readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n+          typesById.put(id, fieldType);\n+        }\n+      }\n+\n+      List<Types.NestedField> expectedFields = expected != null ?\n+          expected.fields() : ImmutableList.of();\n+      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n+          expectedFields.size());\n+      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n+      for (Types.NestedField field : expectedFields) {\n+        int id = field.fieldId();\n+        if (idToConstant.containsKey(id)) {\n+          // containsKey is used because the constant may be null\n+          reorderedFields.add(ParquetValueReaders.constant(idToConstant.get(id)));\n+          types.add(null);\n+        } else {\n+          ParquetValueReader<?> reader = readersById.get(id);\n+          if (reader != null) {\n+            reorderedFields.add(reader);\n+            types.add(typesById.get(id));\n+          } else {\n+            reorderedFields.add(ParquetValueReaders.nulls());\n+            types.add(null);\n+          }\n+        }\n+      }\n+\n+      return new RowDataReader(types, reorderedFields);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> list(Types.ListType expectedList, GroupType array,\n+                                      ParquetValueReader<?> elementReader) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type elementType = repeated.getType(0);\n+      int elementD = type.getMaxDefinitionLevel(path(elementType.getName())) - 1;\n+\n+      return new ArrayReader<>(repeatedD, repeatedR, ParquetValueReaders.option(elementType, elementD, elementReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> map(Types.MapType expectedMap, GroupType map,\n+                                     ParquetValueReader<?> keyReader,\n+                                     ParquetValueReader<?> valueReader) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type keyType = repeatedKeyValue.getType(0);\n+      int keyD = type.getMaxDefinitionLevel(path(keyType.getName())) - 1;\n+      Type valueType = repeatedKeyValue.getType(1);\n+      int valueD = type.getMaxDefinitionLevel(path(valueType.getName())) - 1;\n+\n+      return new MapReader<>(repeatedD, repeatedR,\n+          ParquetValueReaders.option(keyType, keyD, keyReader),\n+          ParquetValueReaders.option(valueType, valueD, valueReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> primitive(org.apache.iceberg.types.Type.PrimitiveType expected,\n+                                           PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return new StringReader(desc);\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+          case DATE:\n+            if (expected != null && expected.typeId() == Types.LongType.get().typeId()) {\n+              return new ParquetValueReaders.IntAsLongReader(desc);\n+            } else {\n+              return new ParquetValueReaders.UnboxedReader<>(desc);\n+            }\n+          case TIME_MICROS:\n+            return new TimeMillisReader(desc);\n+          case INT_64:\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          case TIMESTAMP_MICROS:\n+            return new TimestampMicroReader(desc);\n+          case DECIMAL:\n+            DecimalLogicalTypeAnnotation decimal = (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return new BinaryDecimalReader(desc, decimal.getScale());\n+              case INT64:\n+                return new LongDecimalReader(desc, decimal.getPrecision(), decimal.getScale());\n+              case INT32:\n+                return new IntegerDecimalReader(desc, decimal.getPrecision(), decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          case BSON:\n+            return new ParquetValueReaders.ByteArrayReader(desc);\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      }\n+\n+      switch (primitive.getPrimitiveTypeName()) {\n+        case FIXED_LEN_BYTE_ARRAY:\n+        case BINARY:\n+          return new ParquetValueReaders.ByteArrayReader(desc);\n+        case INT32:\n+          if (expected != null && expected.typeId() == org.apache.iceberg.types.Type.TypeID.LONG) {\n+            return new ParquetValueReaders.IntAsLongReader(desc);\n+          } else {\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          }\n+        case FLOAT:\n+          if (expected != null && expected.typeId() == org.apache.iceberg.types.Type.TypeID.DOUBLE) {\n+            return new ParquetValueReaders.FloatAsDoubleReader(desc);\n+          } else {\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          }\n+        case BOOLEAN:\n+        case INT64:\n+        case DOUBLE:\n+          return new ParquetValueReaders.UnboxedReader<>(desc);\n+        default:\n+          throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+      }\n+    }\n+\n+    protected MessageType type() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e78c2ecdee84c3cf0d54afa84e111faaa0e498df"}, "originalPosition": 266}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM2NDkzOQ==", "bodyText": "Previously, the FallbackReader uses it. Now I think this could be removed since the fallback reader defines its own type . That is because we can't get the type from passing builder.", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r463364939", "createdAt": "2020-07-31T01:54:59Z", "author": {"login": "chenjunjiedada"}, "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java", "diffHunk": "@@ -19,64 +19,723 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.math.BigDecimal;\n+import java.math.BigInteger;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n import java.util.List;\n-import org.apache.flink.types.Row;\n+import java.util.Map;\n+import org.apache.commons.lang3.ArrayUtils;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RawValueData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n import org.apache.iceberg.Schema;\n-import org.apache.iceberg.data.parquet.BaseParquetReaders;\n+import org.apache.iceberg.parquet.ParquetSchemaUtil;\n import org.apache.iceberg.parquet.ParquetValueReader;\n import org.apache.iceberg.parquet.ParquetValueReaders;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetReaders extends BaseParquetReaders<Row> {\n+public class FlinkParquetReaders {\n+  private FlinkParquetReaders() {\n+  }\n \n-  private static final FlinkParquetReaders INSTANCE = new FlinkParquetReaders();\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema, MessageType fileSchema) {\n+    return buildReader(expectedSchema, fileSchema, ImmutableMap.of());\n+  }\n \n-  private FlinkParquetReaders() {\n+  @SuppressWarnings(\"unchecked\")\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema,\n+                                                        MessageType fileSchema,\n+                                                        Map<Integer, ?> idToConstant) {\n+    ReadBuilder builder = new ReadBuilder(fileSchema, idToConstant);\n+    if (ParquetSchemaUtil.hasIds(fileSchema)) {\n+      return (ParquetValueReader<RowData>)\n+          TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema, builder);\n+    } else {\n+      return (ParquetValueReader<RowData>)\n+          TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n+              new FallbackReadBuilder(builder));\n+    }\n+  }\n+\n+  private static class FallbackReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private MessageType type;\n+    private final TypeWithSchemaVisitor<ParquetValueReader<?>> builder;\n+\n+    FallbackReadBuilder(TypeWithSchemaVisitor<ParquetValueReader<?>> builder) {\n+      this.builder = builder;\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> message(Types.StructType expected, MessageType message,\n+                                         List<ParquetValueReader<?>> fieldReaders) {\n+      // the top level matches by ID, but the remaining IDs are missing\n+      this.type = message;\n+      return builder.struct(expected, message, fieldReaders);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> struct(Types.StructType ignored, GroupType struct,\n+                                        List<ParquetValueReader<?>> fieldReaders) {\n+      // the expected struct is ignored because nested fields are never found when the\n+      List<ParquetValueReader<?>> newFields = Lists.newArrayListWithExpectedSize(\n+          fieldReaders.size());\n+      List<Type> types = Lists.newArrayListWithExpectedSize(fieldReaders.size());\n+      List<Type> fields = struct.getFields();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        Type fieldType = fields.get(i);\n+        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n+        newFields.add(ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n+        types.add(fieldType);\n+      }\n+\n+      return new RowDataReader(types, newFields);\n+    }\n+  }\n+\n+  private static class ReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private final MessageType type;\n+    private final Map<Integer, ?> idToConstant;\n+\n+    ReadBuilder(MessageType type, Map<Integer, ?> idToConstant) {\n+      this.type = type;\n+      this.idToConstant = idToConstant;\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> message(Types.StructType expected, MessageType message,\n+                                         List<ParquetValueReader<?>> fieldReaders) {\n+      return struct(expected, message.asGroupType(), fieldReaders);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> struct(Types.StructType expected, GroupType struct,\n+                                        List<ParquetValueReader<?>> fieldReaders) {\n+      // match the expected struct's order\n+      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n+      Map<Integer, Type> typesById = Maps.newHashMap();\n+      List<Type> fields = struct.getFields();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        Type fieldType = fields.get(i);\n+        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n+        if (fieldType.getId() != null) {\n+          int id = fieldType.getId().intValue();\n+          readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n+          typesById.put(id, fieldType);\n+        }\n+      }\n+\n+      List<Types.NestedField> expectedFields = expected != null ?\n+          expected.fields() : ImmutableList.of();\n+      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n+          expectedFields.size());\n+      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n+      for (Types.NestedField field : expectedFields) {\n+        int id = field.fieldId();\n+        if (idToConstant.containsKey(id)) {\n+          // containsKey is used because the constant may be null\n+          reorderedFields.add(ParquetValueReaders.constant(idToConstant.get(id)));\n+          types.add(null);\n+        } else {\n+          ParquetValueReader<?> reader = readersById.get(id);\n+          if (reader != null) {\n+            reorderedFields.add(reader);\n+            types.add(typesById.get(id));\n+          } else {\n+            reorderedFields.add(ParquetValueReaders.nulls());\n+            types.add(null);\n+          }\n+        }\n+      }\n+\n+      return new RowDataReader(types, reorderedFields);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> list(Types.ListType expectedList, GroupType array,\n+                                      ParquetValueReader<?> elementReader) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type elementType = repeated.getType(0);\n+      int elementD = type.getMaxDefinitionLevel(path(elementType.getName())) - 1;\n+\n+      return new ArrayReader<>(repeatedD, repeatedR, ParquetValueReaders.option(elementType, elementD, elementReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> map(Types.MapType expectedMap, GroupType map,\n+                                     ParquetValueReader<?> keyReader,\n+                                     ParquetValueReader<?> valueReader) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type keyType = repeatedKeyValue.getType(0);\n+      int keyD = type.getMaxDefinitionLevel(path(keyType.getName())) - 1;\n+      Type valueType = repeatedKeyValue.getType(1);\n+      int valueD = type.getMaxDefinitionLevel(path(valueType.getName())) - 1;\n+\n+      return new MapReader<>(repeatedD, repeatedR,\n+          ParquetValueReaders.option(keyType, keyD, keyReader),\n+          ParquetValueReaders.option(valueType, valueD, valueReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> primitive(org.apache.iceberg.types.Type.PrimitiveType expected,\n+                                           PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return new StringReader(desc);\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+          case DATE:\n+            if (expected != null && expected.typeId() == Types.LongType.get().typeId()) {\n+              return new ParquetValueReaders.IntAsLongReader(desc);\n+            } else {\n+              return new ParquetValueReaders.UnboxedReader<>(desc);\n+            }\n+          case TIME_MICROS:\n+            return new TimeMillisReader(desc);\n+          case INT_64:\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          case TIMESTAMP_MICROS:\n+            return new TimestampMicroReader(desc);\n+          case DECIMAL:\n+            DecimalLogicalTypeAnnotation decimal = (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return new BinaryDecimalReader(desc, decimal.getScale());\n+              case INT64:\n+                return new LongDecimalReader(desc, decimal.getPrecision(), decimal.getScale());\n+              case INT32:\n+                return new IntegerDecimalReader(desc, decimal.getPrecision(), decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          case BSON:\n+            return new ParquetValueReaders.ByteArrayReader(desc);\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      }\n+\n+      switch (primitive.getPrimitiveTypeName()) {\n+        case FIXED_LEN_BYTE_ARRAY:\n+        case BINARY:\n+          return new ParquetValueReaders.ByteArrayReader(desc);\n+        case INT32:\n+          if (expected != null && expected.typeId() == org.apache.iceberg.types.Type.TypeID.LONG) {\n+            return new ParquetValueReaders.IntAsLongReader(desc);\n+          } else {\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          }\n+        case FLOAT:\n+          if (expected != null && expected.typeId() == org.apache.iceberg.types.Type.TypeID.DOUBLE) {\n+            return new ParquetValueReaders.FloatAsDoubleReader(desc);\n+          } else {\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          }\n+        case BOOLEAN:\n+        case INT64:\n+        case DOUBLE:\n+          return new ParquetValueReaders.UnboxedReader<>(desc);\n+        default:\n+          throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+      }\n+    }\n+\n+    protected MessageType type() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA1Mjg1NA=="}, "originalCommit": {"oid": "e78c2ecdee84c3cf0d54afa84e111faaa0e498df"}, "originalPosition": 266}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5MTE2NjMxOnYy", "diffSide": "RIGHT", "path": "flink/src/test/java/org/apache/iceberg/flink/data/RandomData.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxNToxMDo1NFrOG5ne3Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMzoyMjo0OVrOG56zCg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA2ODg5Mw==", "bodyText": "Seems it could share the common code with RandomGenericData#generate ?  Make the RandomGenericData#generate to return a Iterable ?", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r463068893", "createdAt": "2020-07-30T15:10:54Z", "author": {"login": "openinx"}, "path": "flink/src/test/java/org/apache/iceberg/flink/data/RandomData.java", "diffHunk": "@@ -88,20 +105,187 @@ public Row next() {\n     };\n   }\n \n+  private static Iterable<Record> generateIcebergGenerics(Schema schema, int numRecords,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e78c2ecdee84c3cf0d54afa84e111faaa0e498df"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM3MzQyMg==", "bodyText": "You are right, let me refactor this.", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r463373422", "createdAt": "2020-07-31T02:30:07Z", "author": {"login": "chenjunjiedada"}, "path": "flink/src/test/java/org/apache/iceberg/flink/data/RandomData.java", "diffHunk": "@@ -88,20 +105,187 @@ public Row next() {\n     };\n   }\n \n+  private static Iterable<Record> generateIcebergGenerics(Schema schema, int numRecords,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA2ODg5Mw=="}, "originalCommit": {"oid": "e78c2ecdee84c3cf0d54afa84e111faaa0e498df"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM4NTM1NA==", "bodyText": "This method accepts a Record supplier and then generate records. We should keep it for generating fallback records and dictionary encoded records.  But for generateRecords method we can update it to call RandomGenericData#generate directly.", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r463385354", "createdAt": "2020-07-31T03:22:49Z", "author": {"login": "chenjunjiedada"}, "path": "flink/src/test/java/org/apache/iceberg/flink/data/RandomData.java", "diffHunk": "@@ -88,20 +105,187 @@ public Row next() {\n     };\n   }\n \n+  private static Iterable<Record> generateIcebergGenerics(Schema schema, int numRecords,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA2ODg5Mw=="}, "originalCommit": {"oid": "e78c2ecdee84c3cf0d54afa84e111faaa0e498df"}, "originalPosition": 46}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5MTIzNzA2OnYy", "diffSide": "RIGHT", "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxNToyNjozNFrOG5oKvA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMjowMDo0NFrOG55ojg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA4MDEyNA==", "bodyText": "The reader is named TimeMillisReader, and the writer is TimeMicrosWriter, could them be symmetrical ?", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r463080124", "createdAt": "2020-07-30T15:26:34Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java", "diffHunk": "@@ -19,38 +19,457 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.math.BigDecimal;\n+import java.util.Iterator;\n import java.util.List;\n-import org.apache.flink.types.Row;\n-import org.apache.iceberg.data.parquet.BaseParquetWriter;\n+import java.util.Map;\n+import java.util.NoSuchElementException;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.MapType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.RowType.RowField;\n+import org.apache.flink.table.types.logical.SmallIntType;\n+import org.apache.flink.table.types.logical.TinyIntType;\n+import org.apache.iceberg.parquet.ParquetValueReaders;\n import org.apache.iceberg.parquet.ParquetValueWriter;\n import org.apache.iceberg.parquet.ParquetValueWriters;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetWriters extends BaseParquetWriter<Row> {\n+public class FlinkParquetWriters {\n+  private FlinkParquetWriters() {\n+  }\n \n-  private static final FlinkParquetWriters INSTANCE = new FlinkParquetWriters();\n+  @SuppressWarnings(\"unchecked\")\n+  public static <T> ParquetValueWriter<T> buildWriter(LogicalType schema, MessageType type) {\n+    return (ParquetValueWriter<T>) ParquetWithFlinkSchemaVisitor.visit(schema, type, new WriteBuilder(type));\n+  }\n \n-  private FlinkParquetWriters() {\n+  private static class WriteBuilder extends ParquetWithFlinkSchemaVisitor<ParquetValueWriter<?>> {\n+    private final MessageType type;\n+\n+    WriteBuilder(MessageType type) {\n+      this.type = type;\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> message(RowType sStruct, MessageType message, List<ParquetValueWriter<?>> fields) {\n+      return struct(sStruct, message.asGroupType(), fields);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> struct(RowType sStruct, GroupType struct,\n+                                        List<ParquetValueWriter<?>> fieldWriters) {\n+      List<Type> fields = struct.getFields();\n+      List<RowField> flinkFields = sStruct.getFields();\n+      List<ParquetValueWriter<?>> writers = Lists.newArrayListWithExpectedSize(fieldWriters.size());\n+      List<LogicalType> flinkTypes = Lists.newArrayList();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        writers.add(newOption(struct.getType(i), fieldWriters.get(i)));\n+        flinkTypes.add(flinkFields.get(i).getType());\n+      }\n+\n+      return new RowDataWriter(writers, flinkTypes);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> list(ArrayType sArray, GroupType array, ParquetValueWriter<?> elementWriter) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath);\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath);\n+\n+      return new ArrayDataWriter<>(repeatedD, repeatedR,\n+          newOption(repeated.getType(0), elementWriter),\n+          sArray.getElementType());\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> map(MapType sMap, GroupType map,\n+                                     ParquetValueWriter<?> keyWriter, ParquetValueWriter<?> valueWriter) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath);\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath);\n+\n+      return new MapDataWriter<>(repeatedD, repeatedR,\n+          newOption(repeatedKeyValue.getType(0), keyWriter),\n+          newOption(repeatedKeyValue.getType(1), valueWriter),\n+          sMap.getKeyType(), sMap.getValueType());\n+    }\n+\n+\n+    private ParquetValueWriter<?> newOption(org.apache.parquet.schema.Type fieldType, ParquetValueWriter<?> writer) {\n+      int maxD = type.getMaxDefinitionLevel(path(fieldType.getName()));\n+      return ParquetValueWriters.option(fieldType, maxD, writer);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> primitive(LogicalType sType, PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return strings(desc);\n+          case DATE:\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+            return ints(sType, desc);\n+          case INT_64:\n+            return ParquetValueWriters.longs(desc);\n+          case TIME_MICROS:\n+            return timeMicros(desc);\n+          case TIMESTAMP_MICROS:\n+            return timestamps(desc);\n+          case DECIMAL:\n+            DecimalLogicalTypeAnnotation decimal = (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case INT32:\n+                return decimalAsInteger(desc, decimal.getPrecision(), decimal.getScale());\n+              case INT64:\n+                return decimalAsLong(desc, decimal.getPrecision(), decimal.getScale());\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return decimalAsFixed(desc, decimal.getPrecision(), decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          case BSON:\n+            return byteArrays(desc);\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      }\n+\n+      switch (primitive.getPrimitiveTypeName()) {\n+        case FIXED_LEN_BYTE_ARRAY:\n+        case BINARY:\n+          return byteArrays(desc);\n+        case BOOLEAN:\n+          return ParquetValueWriters.booleans(desc);\n+        case INT32:\n+          return ints(sType, desc);\n+        case INT64:\n+          return ParquetValueWriters.longs(desc);\n+        case FLOAT:\n+          return ParquetValueWriters.floats(desc);\n+        case DOUBLE:\n+          return ParquetValueWriters.doubles(desc);\n+        default:\n+          throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+      }\n+    }\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<?> ints(LogicalType type, ColumnDescriptor desc) {\n+    if (type instanceof TinyIntType) {\n+      return ParquetValueWriters.tinyints(desc);\n+    } else if (type instanceof SmallIntType) {\n+      return ParquetValueWriters.shorts(desc);\n+    }\n+    return ParquetValueWriters.ints(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<StringData> strings(ColumnDescriptor desc) {\n+    return new StringDataWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<Integer> timeMicros(ColumnDescriptor desc) {\n+    return new TimeMicrosWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsInteger(ColumnDescriptor desc,\n+                                                                                   int precision, int scale) {\n+    return new IntegerDecimalWriter(desc, precision, scale);\n+  }\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsLong(ColumnDescriptor desc,\n+                                                                            int precision, int scale) {\n+    return new LongDecimalWriter(desc, precision, scale);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsFixed(ColumnDescriptor desc,\n+                                                                             int precision, int scale) {\n+    return new FixedDecimalWriter(desc, precision, scale);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<TimestampData> timestamps(ColumnDescriptor desc) {\n+    return new TimestampDataWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<byte[]> byteArrays(ColumnDescriptor desc) {\n+    return new ByteArrayWriter(desc);\n+  }\n+\n+  private static class StringDataWriter extends ParquetValueWriters.PrimitiveWriter<StringData> {\n+    private StringDataWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, StringData value) {\n+      column.writeBinary(repetitionLevel, Binary.fromReusedByteArray(value.toBytes()));\n+    }\n+  }\n+\n+  private static class TimeMicrosWriter extends ParquetValueWriters.PrimitiveWriter<Integer> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e78c2ecdee84c3cf0d54afa84e111faaa0e498df"}, "originalPosition": 224}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM2NjI4Ng==", "bodyText": "The naming logic is what we actually perform. In the reader side, we read in the milliseconds for Flink. In the writer side, we write out microseconds for Parquet.", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r463366286", "createdAt": "2020-07-31T02:00:44Z", "author": {"login": "chenjunjiedada"}, "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java", "diffHunk": "@@ -19,38 +19,457 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.math.BigDecimal;\n+import java.util.Iterator;\n import java.util.List;\n-import org.apache.flink.types.Row;\n-import org.apache.iceberg.data.parquet.BaseParquetWriter;\n+import java.util.Map;\n+import java.util.NoSuchElementException;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.MapType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.RowType.RowField;\n+import org.apache.flink.table.types.logical.SmallIntType;\n+import org.apache.flink.table.types.logical.TinyIntType;\n+import org.apache.iceberg.parquet.ParquetValueReaders;\n import org.apache.iceberg.parquet.ParquetValueWriter;\n import org.apache.iceberg.parquet.ParquetValueWriters;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetWriters extends BaseParquetWriter<Row> {\n+public class FlinkParquetWriters {\n+  private FlinkParquetWriters() {\n+  }\n \n-  private static final FlinkParquetWriters INSTANCE = new FlinkParquetWriters();\n+  @SuppressWarnings(\"unchecked\")\n+  public static <T> ParquetValueWriter<T> buildWriter(LogicalType schema, MessageType type) {\n+    return (ParquetValueWriter<T>) ParquetWithFlinkSchemaVisitor.visit(schema, type, new WriteBuilder(type));\n+  }\n \n-  private FlinkParquetWriters() {\n+  private static class WriteBuilder extends ParquetWithFlinkSchemaVisitor<ParquetValueWriter<?>> {\n+    private final MessageType type;\n+\n+    WriteBuilder(MessageType type) {\n+      this.type = type;\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> message(RowType sStruct, MessageType message, List<ParquetValueWriter<?>> fields) {\n+      return struct(sStruct, message.asGroupType(), fields);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> struct(RowType sStruct, GroupType struct,\n+                                        List<ParquetValueWriter<?>> fieldWriters) {\n+      List<Type> fields = struct.getFields();\n+      List<RowField> flinkFields = sStruct.getFields();\n+      List<ParquetValueWriter<?>> writers = Lists.newArrayListWithExpectedSize(fieldWriters.size());\n+      List<LogicalType> flinkTypes = Lists.newArrayList();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        writers.add(newOption(struct.getType(i), fieldWriters.get(i)));\n+        flinkTypes.add(flinkFields.get(i).getType());\n+      }\n+\n+      return new RowDataWriter(writers, flinkTypes);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> list(ArrayType sArray, GroupType array, ParquetValueWriter<?> elementWriter) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath);\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath);\n+\n+      return new ArrayDataWriter<>(repeatedD, repeatedR,\n+          newOption(repeated.getType(0), elementWriter),\n+          sArray.getElementType());\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> map(MapType sMap, GroupType map,\n+                                     ParquetValueWriter<?> keyWriter, ParquetValueWriter<?> valueWriter) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath);\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath);\n+\n+      return new MapDataWriter<>(repeatedD, repeatedR,\n+          newOption(repeatedKeyValue.getType(0), keyWriter),\n+          newOption(repeatedKeyValue.getType(1), valueWriter),\n+          sMap.getKeyType(), sMap.getValueType());\n+    }\n+\n+\n+    private ParquetValueWriter<?> newOption(org.apache.parquet.schema.Type fieldType, ParquetValueWriter<?> writer) {\n+      int maxD = type.getMaxDefinitionLevel(path(fieldType.getName()));\n+      return ParquetValueWriters.option(fieldType, maxD, writer);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> primitive(LogicalType sType, PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return strings(desc);\n+          case DATE:\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+            return ints(sType, desc);\n+          case INT_64:\n+            return ParquetValueWriters.longs(desc);\n+          case TIME_MICROS:\n+            return timeMicros(desc);\n+          case TIMESTAMP_MICROS:\n+            return timestamps(desc);\n+          case DECIMAL:\n+            DecimalLogicalTypeAnnotation decimal = (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case INT32:\n+                return decimalAsInteger(desc, decimal.getPrecision(), decimal.getScale());\n+              case INT64:\n+                return decimalAsLong(desc, decimal.getPrecision(), decimal.getScale());\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return decimalAsFixed(desc, decimal.getPrecision(), decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          case BSON:\n+            return byteArrays(desc);\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      }\n+\n+      switch (primitive.getPrimitiveTypeName()) {\n+        case FIXED_LEN_BYTE_ARRAY:\n+        case BINARY:\n+          return byteArrays(desc);\n+        case BOOLEAN:\n+          return ParquetValueWriters.booleans(desc);\n+        case INT32:\n+          return ints(sType, desc);\n+        case INT64:\n+          return ParquetValueWriters.longs(desc);\n+        case FLOAT:\n+          return ParquetValueWriters.floats(desc);\n+        case DOUBLE:\n+          return ParquetValueWriters.doubles(desc);\n+        default:\n+          throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+      }\n+    }\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<?> ints(LogicalType type, ColumnDescriptor desc) {\n+    if (type instanceof TinyIntType) {\n+      return ParquetValueWriters.tinyints(desc);\n+    } else if (type instanceof SmallIntType) {\n+      return ParquetValueWriters.shorts(desc);\n+    }\n+    return ParquetValueWriters.ints(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<StringData> strings(ColumnDescriptor desc) {\n+    return new StringDataWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<Integer> timeMicros(ColumnDescriptor desc) {\n+    return new TimeMicrosWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsInteger(ColumnDescriptor desc,\n+                                                                                   int precision, int scale) {\n+    return new IntegerDecimalWriter(desc, precision, scale);\n+  }\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsLong(ColumnDescriptor desc,\n+                                                                            int precision, int scale) {\n+    return new LongDecimalWriter(desc, precision, scale);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsFixed(ColumnDescriptor desc,\n+                                                                             int precision, int scale) {\n+    return new FixedDecimalWriter(desc, precision, scale);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<TimestampData> timestamps(ColumnDescriptor desc) {\n+    return new TimestampDataWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<byte[]> byteArrays(ColumnDescriptor desc) {\n+    return new ByteArrayWriter(desc);\n+  }\n+\n+  private static class StringDataWriter extends ParquetValueWriters.PrimitiveWriter<StringData> {\n+    private StringDataWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, StringData value) {\n+      column.writeBinary(repetitionLevel, Binary.fromReusedByteArray(value.toBytes()));\n+    }\n+  }\n+\n+  private static class TimeMicrosWriter extends ParquetValueWriters.PrimitiveWriter<Integer> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA4MDEyNA=="}, "originalCommit": {"oid": "e78c2ecdee84c3cf0d54afa84e111faaa0e498df"}, "originalPosition": 224}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5MTI1MDY5OnYy", "diffSide": "RIGHT", "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxNToyOTozOFrOG5oTbA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMVQxMzozOTowOVrOG-4OMw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA4MjM0OA==", "bodyText": "Seem the upper bound of precision of IntegerDecimalWriter is 9 ?  Could we add the precision <= 9  assertion ?", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r463082348", "createdAt": "2020-07-30T15:29:38Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java", "diffHunk": "@@ -19,38 +19,457 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.math.BigDecimal;\n+import java.util.Iterator;\n import java.util.List;\n-import org.apache.flink.types.Row;\n-import org.apache.iceberg.data.parquet.BaseParquetWriter;\n+import java.util.Map;\n+import java.util.NoSuchElementException;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.MapType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.RowType.RowField;\n+import org.apache.flink.table.types.logical.SmallIntType;\n+import org.apache.flink.table.types.logical.TinyIntType;\n+import org.apache.iceberg.parquet.ParquetValueReaders;\n import org.apache.iceberg.parquet.ParquetValueWriter;\n import org.apache.iceberg.parquet.ParquetValueWriters;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetWriters extends BaseParquetWriter<Row> {\n+public class FlinkParquetWriters {\n+  private FlinkParquetWriters() {\n+  }\n \n-  private static final FlinkParquetWriters INSTANCE = new FlinkParquetWriters();\n+  @SuppressWarnings(\"unchecked\")\n+  public static <T> ParquetValueWriter<T> buildWriter(LogicalType schema, MessageType type) {\n+    return (ParquetValueWriter<T>) ParquetWithFlinkSchemaVisitor.visit(schema, type, new WriteBuilder(type));\n+  }\n \n-  private FlinkParquetWriters() {\n+  private static class WriteBuilder extends ParquetWithFlinkSchemaVisitor<ParquetValueWriter<?>> {\n+    private final MessageType type;\n+\n+    WriteBuilder(MessageType type) {\n+      this.type = type;\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> message(RowType sStruct, MessageType message, List<ParquetValueWriter<?>> fields) {\n+      return struct(sStruct, message.asGroupType(), fields);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> struct(RowType sStruct, GroupType struct,\n+                                        List<ParquetValueWriter<?>> fieldWriters) {\n+      List<Type> fields = struct.getFields();\n+      List<RowField> flinkFields = sStruct.getFields();\n+      List<ParquetValueWriter<?>> writers = Lists.newArrayListWithExpectedSize(fieldWriters.size());\n+      List<LogicalType> flinkTypes = Lists.newArrayList();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        writers.add(newOption(struct.getType(i), fieldWriters.get(i)));\n+        flinkTypes.add(flinkFields.get(i).getType());\n+      }\n+\n+      return new RowDataWriter(writers, flinkTypes);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> list(ArrayType sArray, GroupType array, ParquetValueWriter<?> elementWriter) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath);\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath);\n+\n+      return new ArrayDataWriter<>(repeatedD, repeatedR,\n+          newOption(repeated.getType(0), elementWriter),\n+          sArray.getElementType());\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> map(MapType sMap, GroupType map,\n+                                     ParquetValueWriter<?> keyWriter, ParquetValueWriter<?> valueWriter) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath);\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath);\n+\n+      return new MapDataWriter<>(repeatedD, repeatedR,\n+          newOption(repeatedKeyValue.getType(0), keyWriter),\n+          newOption(repeatedKeyValue.getType(1), valueWriter),\n+          sMap.getKeyType(), sMap.getValueType());\n+    }\n+\n+\n+    private ParquetValueWriter<?> newOption(org.apache.parquet.schema.Type fieldType, ParquetValueWriter<?> writer) {\n+      int maxD = type.getMaxDefinitionLevel(path(fieldType.getName()));\n+      return ParquetValueWriters.option(fieldType, maxD, writer);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> primitive(LogicalType sType, PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return strings(desc);\n+          case DATE:\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+            return ints(sType, desc);\n+          case INT_64:\n+            return ParquetValueWriters.longs(desc);\n+          case TIME_MICROS:\n+            return timeMicros(desc);\n+          case TIMESTAMP_MICROS:\n+            return timestamps(desc);\n+          case DECIMAL:\n+            DecimalLogicalTypeAnnotation decimal = (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case INT32:\n+                return decimalAsInteger(desc, decimal.getPrecision(), decimal.getScale());\n+              case INT64:\n+                return decimalAsLong(desc, decimal.getPrecision(), decimal.getScale());\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return decimalAsFixed(desc, decimal.getPrecision(), decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          case BSON:\n+            return byteArrays(desc);\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      }\n+\n+      switch (primitive.getPrimitiveTypeName()) {\n+        case FIXED_LEN_BYTE_ARRAY:\n+        case BINARY:\n+          return byteArrays(desc);\n+        case BOOLEAN:\n+          return ParquetValueWriters.booleans(desc);\n+        case INT32:\n+          return ints(sType, desc);\n+        case INT64:\n+          return ParquetValueWriters.longs(desc);\n+        case FLOAT:\n+          return ParquetValueWriters.floats(desc);\n+        case DOUBLE:\n+          return ParquetValueWriters.doubles(desc);\n+        default:\n+          throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+      }\n+    }\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<?> ints(LogicalType type, ColumnDescriptor desc) {\n+    if (type instanceof TinyIntType) {\n+      return ParquetValueWriters.tinyints(desc);\n+    } else if (type instanceof SmallIntType) {\n+      return ParquetValueWriters.shorts(desc);\n+    }\n+    return ParquetValueWriters.ints(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<StringData> strings(ColumnDescriptor desc) {\n+    return new StringDataWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<Integer> timeMicros(ColumnDescriptor desc) {\n+    return new TimeMicrosWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsInteger(ColumnDescriptor desc,\n+                                                                                   int precision, int scale) {\n+    return new IntegerDecimalWriter(desc, precision, scale);\n+  }\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsLong(ColumnDescriptor desc,\n+                                                                            int precision, int scale) {\n+    return new LongDecimalWriter(desc, precision, scale);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsFixed(ColumnDescriptor desc,\n+                                                                             int precision, int scale) {\n+    return new FixedDecimalWriter(desc, precision, scale);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<TimestampData> timestamps(ColumnDescriptor desc) {\n+    return new TimestampDataWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<byte[]> byteArrays(ColumnDescriptor desc) {\n+    return new ByteArrayWriter(desc);\n+  }\n+\n+  private static class StringDataWriter extends ParquetValueWriters.PrimitiveWriter<StringData> {\n+    private StringDataWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, StringData value) {\n+      column.writeBinary(repetitionLevel, Binary.fromReusedByteArray(value.toBytes()));\n+    }\n+  }\n+\n+  private static class TimeMicrosWriter extends ParquetValueWriters.PrimitiveWriter<Integer> {\n+    private TimeMicrosWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, Integer value) {\n+      long micros = Long.valueOf(value) * 1000;\n+      column.writeLong(repetitionLevel, micros);\n+    }\n+  }\n+\n+  private static class IntegerDecimalWriter extends ParquetValueWriters.PrimitiveWriter<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    private IntegerDecimalWriter(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, DecimalData decimal) {\n+      Preconditions.checkArgument(decimal.scale() == scale,\n+          \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, decimal);\n+      Preconditions.checkArgument(decimal.precision() <= precision,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e78c2ecdee84c3cf0d54afa84e111faaa0e498df"}, "originalPosition": 250}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM3MDMwNg==", "bodyText": "Will use the latest DecimalUtil.", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r463370306", "createdAt": "2020-07-31T02:17:37Z", "author": {"login": "chenjunjiedada"}, "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java", "diffHunk": "@@ -19,38 +19,457 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.math.BigDecimal;\n+import java.util.Iterator;\n import java.util.List;\n-import org.apache.flink.types.Row;\n-import org.apache.iceberg.data.parquet.BaseParquetWriter;\n+import java.util.Map;\n+import java.util.NoSuchElementException;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.MapType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.RowType.RowField;\n+import org.apache.flink.table.types.logical.SmallIntType;\n+import org.apache.flink.table.types.logical.TinyIntType;\n+import org.apache.iceberg.parquet.ParquetValueReaders;\n import org.apache.iceberg.parquet.ParquetValueWriter;\n import org.apache.iceberg.parquet.ParquetValueWriters;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetWriters extends BaseParquetWriter<Row> {\n+public class FlinkParquetWriters {\n+  private FlinkParquetWriters() {\n+  }\n \n-  private static final FlinkParquetWriters INSTANCE = new FlinkParquetWriters();\n+  @SuppressWarnings(\"unchecked\")\n+  public static <T> ParquetValueWriter<T> buildWriter(LogicalType schema, MessageType type) {\n+    return (ParquetValueWriter<T>) ParquetWithFlinkSchemaVisitor.visit(schema, type, new WriteBuilder(type));\n+  }\n \n-  private FlinkParquetWriters() {\n+  private static class WriteBuilder extends ParquetWithFlinkSchemaVisitor<ParquetValueWriter<?>> {\n+    private final MessageType type;\n+\n+    WriteBuilder(MessageType type) {\n+      this.type = type;\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> message(RowType sStruct, MessageType message, List<ParquetValueWriter<?>> fields) {\n+      return struct(sStruct, message.asGroupType(), fields);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> struct(RowType sStruct, GroupType struct,\n+                                        List<ParquetValueWriter<?>> fieldWriters) {\n+      List<Type> fields = struct.getFields();\n+      List<RowField> flinkFields = sStruct.getFields();\n+      List<ParquetValueWriter<?>> writers = Lists.newArrayListWithExpectedSize(fieldWriters.size());\n+      List<LogicalType> flinkTypes = Lists.newArrayList();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        writers.add(newOption(struct.getType(i), fieldWriters.get(i)));\n+        flinkTypes.add(flinkFields.get(i).getType());\n+      }\n+\n+      return new RowDataWriter(writers, flinkTypes);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> list(ArrayType sArray, GroupType array, ParquetValueWriter<?> elementWriter) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath);\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath);\n+\n+      return new ArrayDataWriter<>(repeatedD, repeatedR,\n+          newOption(repeated.getType(0), elementWriter),\n+          sArray.getElementType());\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> map(MapType sMap, GroupType map,\n+                                     ParquetValueWriter<?> keyWriter, ParquetValueWriter<?> valueWriter) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath);\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath);\n+\n+      return new MapDataWriter<>(repeatedD, repeatedR,\n+          newOption(repeatedKeyValue.getType(0), keyWriter),\n+          newOption(repeatedKeyValue.getType(1), valueWriter),\n+          sMap.getKeyType(), sMap.getValueType());\n+    }\n+\n+\n+    private ParquetValueWriter<?> newOption(org.apache.parquet.schema.Type fieldType, ParquetValueWriter<?> writer) {\n+      int maxD = type.getMaxDefinitionLevel(path(fieldType.getName()));\n+      return ParquetValueWriters.option(fieldType, maxD, writer);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> primitive(LogicalType sType, PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return strings(desc);\n+          case DATE:\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+            return ints(sType, desc);\n+          case INT_64:\n+            return ParquetValueWriters.longs(desc);\n+          case TIME_MICROS:\n+            return timeMicros(desc);\n+          case TIMESTAMP_MICROS:\n+            return timestamps(desc);\n+          case DECIMAL:\n+            DecimalLogicalTypeAnnotation decimal = (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case INT32:\n+                return decimalAsInteger(desc, decimal.getPrecision(), decimal.getScale());\n+              case INT64:\n+                return decimalAsLong(desc, decimal.getPrecision(), decimal.getScale());\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return decimalAsFixed(desc, decimal.getPrecision(), decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          case BSON:\n+            return byteArrays(desc);\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      }\n+\n+      switch (primitive.getPrimitiveTypeName()) {\n+        case FIXED_LEN_BYTE_ARRAY:\n+        case BINARY:\n+          return byteArrays(desc);\n+        case BOOLEAN:\n+          return ParquetValueWriters.booleans(desc);\n+        case INT32:\n+          return ints(sType, desc);\n+        case INT64:\n+          return ParquetValueWriters.longs(desc);\n+        case FLOAT:\n+          return ParquetValueWriters.floats(desc);\n+        case DOUBLE:\n+          return ParquetValueWriters.doubles(desc);\n+        default:\n+          throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+      }\n+    }\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<?> ints(LogicalType type, ColumnDescriptor desc) {\n+    if (type instanceof TinyIntType) {\n+      return ParquetValueWriters.tinyints(desc);\n+    } else if (type instanceof SmallIntType) {\n+      return ParquetValueWriters.shorts(desc);\n+    }\n+    return ParquetValueWriters.ints(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<StringData> strings(ColumnDescriptor desc) {\n+    return new StringDataWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<Integer> timeMicros(ColumnDescriptor desc) {\n+    return new TimeMicrosWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsInteger(ColumnDescriptor desc,\n+                                                                                   int precision, int scale) {\n+    return new IntegerDecimalWriter(desc, precision, scale);\n+  }\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsLong(ColumnDescriptor desc,\n+                                                                            int precision, int scale) {\n+    return new LongDecimalWriter(desc, precision, scale);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsFixed(ColumnDescriptor desc,\n+                                                                             int precision, int scale) {\n+    return new FixedDecimalWriter(desc, precision, scale);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<TimestampData> timestamps(ColumnDescriptor desc) {\n+    return new TimestampDataWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<byte[]> byteArrays(ColumnDescriptor desc) {\n+    return new ByteArrayWriter(desc);\n+  }\n+\n+  private static class StringDataWriter extends ParquetValueWriters.PrimitiveWriter<StringData> {\n+    private StringDataWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, StringData value) {\n+      column.writeBinary(repetitionLevel, Binary.fromReusedByteArray(value.toBytes()));\n+    }\n+  }\n+\n+  private static class TimeMicrosWriter extends ParquetValueWriters.PrimitiveWriter<Integer> {\n+    private TimeMicrosWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, Integer value) {\n+      long micros = Long.valueOf(value) * 1000;\n+      column.writeLong(repetitionLevel, micros);\n+    }\n+  }\n+\n+  private static class IntegerDecimalWriter extends ParquetValueWriters.PrimitiveWriter<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    private IntegerDecimalWriter(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, DecimalData decimal) {\n+      Preconditions.checkArgument(decimal.scale() == scale,\n+          \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, decimal);\n+      Preconditions.checkArgument(decimal.precision() <= precision,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA4MjM0OA=="}, "originalCommit": {"oid": "e78c2ecdee84c3cf0d54afa84e111faaa0e498df"}, "originalPosition": 250}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODU4NjAzNQ==", "bodyText": "Seems DecimalUtil doesn't handle this. I fixed in the new commit.", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r468586035", "createdAt": "2020-08-11T13:39:09Z", "author": {"login": "chenjunjiedada"}, "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java", "diffHunk": "@@ -19,38 +19,457 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.math.BigDecimal;\n+import java.util.Iterator;\n import java.util.List;\n-import org.apache.flink.types.Row;\n-import org.apache.iceberg.data.parquet.BaseParquetWriter;\n+import java.util.Map;\n+import java.util.NoSuchElementException;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.MapType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.RowType.RowField;\n+import org.apache.flink.table.types.logical.SmallIntType;\n+import org.apache.flink.table.types.logical.TinyIntType;\n+import org.apache.iceberg.parquet.ParquetValueReaders;\n import org.apache.iceberg.parquet.ParquetValueWriter;\n import org.apache.iceberg.parquet.ParquetValueWriters;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetWriters extends BaseParquetWriter<Row> {\n+public class FlinkParquetWriters {\n+  private FlinkParquetWriters() {\n+  }\n \n-  private static final FlinkParquetWriters INSTANCE = new FlinkParquetWriters();\n+  @SuppressWarnings(\"unchecked\")\n+  public static <T> ParquetValueWriter<T> buildWriter(LogicalType schema, MessageType type) {\n+    return (ParquetValueWriter<T>) ParquetWithFlinkSchemaVisitor.visit(schema, type, new WriteBuilder(type));\n+  }\n \n-  private FlinkParquetWriters() {\n+  private static class WriteBuilder extends ParquetWithFlinkSchemaVisitor<ParquetValueWriter<?>> {\n+    private final MessageType type;\n+\n+    WriteBuilder(MessageType type) {\n+      this.type = type;\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> message(RowType sStruct, MessageType message, List<ParquetValueWriter<?>> fields) {\n+      return struct(sStruct, message.asGroupType(), fields);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> struct(RowType sStruct, GroupType struct,\n+                                        List<ParquetValueWriter<?>> fieldWriters) {\n+      List<Type> fields = struct.getFields();\n+      List<RowField> flinkFields = sStruct.getFields();\n+      List<ParquetValueWriter<?>> writers = Lists.newArrayListWithExpectedSize(fieldWriters.size());\n+      List<LogicalType> flinkTypes = Lists.newArrayList();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        writers.add(newOption(struct.getType(i), fieldWriters.get(i)));\n+        flinkTypes.add(flinkFields.get(i).getType());\n+      }\n+\n+      return new RowDataWriter(writers, flinkTypes);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> list(ArrayType sArray, GroupType array, ParquetValueWriter<?> elementWriter) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath);\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath);\n+\n+      return new ArrayDataWriter<>(repeatedD, repeatedR,\n+          newOption(repeated.getType(0), elementWriter),\n+          sArray.getElementType());\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> map(MapType sMap, GroupType map,\n+                                     ParquetValueWriter<?> keyWriter, ParquetValueWriter<?> valueWriter) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath);\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath);\n+\n+      return new MapDataWriter<>(repeatedD, repeatedR,\n+          newOption(repeatedKeyValue.getType(0), keyWriter),\n+          newOption(repeatedKeyValue.getType(1), valueWriter),\n+          sMap.getKeyType(), sMap.getValueType());\n+    }\n+\n+\n+    private ParquetValueWriter<?> newOption(org.apache.parquet.schema.Type fieldType, ParquetValueWriter<?> writer) {\n+      int maxD = type.getMaxDefinitionLevel(path(fieldType.getName()));\n+      return ParquetValueWriters.option(fieldType, maxD, writer);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> primitive(LogicalType sType, PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return strings(desc);\n+          case DATE:\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+            return ints(sType, desc);\n+          case INT_64:\n+            return ParquetValueWriters.longs(desc);\n+          case TIME_MICROS:\n+            return timeMicros(desc);\n+          case TIMESTAMP_MICROS:\n+            return timestamps(desc);\n+          case DECIMAL:\n+            DecimalLogicalTypeAnnotation decimal = (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case INT32:\n+                return decimalAsInteger(desc, decimal.getPrecision(), decimal.getScale());\n+              case INT64:\n+                return decimalAsLong(desc, decimal.getPrecision(), decimal.getScale());\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return decimalAsFixed(desc, decimal.getPrecision(), decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          case BSON:\n+            return byteArrays(desc);\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      }\n+\n+      switch (primitive.getPrimitiveTypeName()) {\n+        case FIXED_LEN_BYTE_ARRAY:\n+        case BINARY:\n+          return byteArrays(desc);\n+        case BOOLEAN:\n+          return ParquetValueWriters.booleans(desc);\n+        case INT32:\n+          return ints(sType, desc);\n+        case INT64:\n+          return ParquetValueWriters.longs(desc);\n+        case FLOAT:\n+          return ParquetValueWriters.floats(desc);\n+        case DOUBLE:\n+          return ParquetValueWriters.doubles(desc);\n+        default:\n+          throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+      }\n+    }\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<?> ints(LogicalType type, ColumnDescriptor desc) {\n+    if (type instanceof TinyIntType) {\n+      return ParquetValueWriters.tinyints(desc);\n+    } else if (type instanceof SmallIntType) {\n+      return ParquetValueWriters.shorts(desc);\n+    }\n+    return ParquetValueWriters.ints(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<StringData> strings(ColumnDescriptor desc) {\n+    return new StringDataWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<Integer> timeMicros(ColumnDescriptor desc) {\n+    return new TimeMicrosWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsInteger(ColumnDescriptor desc,\n+                                                                                   int precision, int scale) {\n+    return new IntegerDecimalWriter(desc, precision, scale);\n+  }\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsLong(ColumnDescriptor desc,\n+                                                                            int precision, int scale) {\n+    return new LongDecimalWriter(desc, precision, scale);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsFixed(ColumnDescriptor desc,\n+                                                                             int precision, int scale) {\n+    return new FixedDecimalWriter(desc, precision, scale);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<TimestampData> timestamps(ColumnDescriptor desc) {\n+    return new TimestampDataWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<byte[]> byteArrays(ColumnDescriptor desc) {\n+    return new ByteArrayWriter(desc);\n+  }\n+\n+  private static class StringDataWriter extends ParquetValueWriters.PrimitiveWriter<StringData> {\n+    private StringDataWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, StringData value) {\n+      column.writeBinary(repetitionLevel, Binary.fromReusedByteArray(value.toBytes()));\n+    }\n+  }\n+\n+  private static class TimeMicrosWriter extends ParquetValueWriters.PrimitiveWriter<Integer> {\n+    private TimeMicrosWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, Integer value) {\n+      long micros = Long.valueOf(value) * 1000;\n+      column.writeLong(repetitionLevel, micros);\n+    }\n+  }\n+\n+  private static class IntegerDecimalWriter extends ParquetValueWriters.PrimitiveWriter<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    private IntegerDecimalWriter(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, DecimalData decimal) {\n+      Preconditions.checkArgument(decimal.scale() == scale,\n+          \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, decimal);\n+      Preconditions.checkArgument(decimal.precision() <= precision,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA4MjM0OA=="}, "originalCommit": {"oid": "e78c2ecdee84c3cf0d54afa84e111faaa0e498df"}, "originalPosition": 250}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5MTI1NDEyOnYy", "diffSide": "RIGHT", "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxNTozMDoyMVrOG5oViA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNVQwMDozMzo0MlrOHF843Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA4Mjg4OA==", "bodyText": "Also could we add the precision <= 18 assertion ?", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r463082888", "createdAt": "2020-07-30T15:30:21Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java", "diffHunk": "@@ -19,38 +19,457 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.math.BigDecimal;\n+import java.util.Iterator;\n import java.util.List;\n-import org.apache.flink.types.Row;\n-import org.apache.iceberg.data.parquet.BaseParquetWriter;\n+import java.util.Map;\n+import java.util.NoSuchElementException;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.MapType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.RowType.RowField;\n+import org.apache.flink.table.types.logical.SmallIntType;\n+import org.apache.flink.table.types.logical.TinyIntType;\n+import org.apache.iceberg.parquet.ParquetValueReaders;\n import org.apache.iceberg.parquet.ParquetValueWriter;\n import org.apache.iceberg.parquet.ParquetValueWriters;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetWriters extends BaseParquetWriter<Row> {\n+public class FlinkParquetWriters {\n+  private FlinkParquetWriters() {\n+  }\n \n-  private static final FlinkParquetWriters INSTANCE = new FlinkParquetWriters();\n+  @SuppressWarnings(\"unchecked\")\n+  public static <T> ParquetValueWriter<T> buildWriter(LogicalType schema, MessageType type) {\n+    return (ParquetValueWriter<T>) ParquetWithFlinkSchemaVisitor.visit(schema, type, new WriteBuilder(type));\n+  }\n \n-  private FlinkParquetWriters() {\n+  private static class WriteBuilder extends ParquetWithFlinkSchemaVisitor<ParquetValueWriter<?>> {\n+    private final MessageType type;\n+\n+    WriteBuilder(MessageType type) {\n+      this.type = type;\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> message(RowType sStruct, MessageType message, List<ParquetValueWriter<?>> fields) {\n+      return struct(sStruct, message.asGroupType(), fields);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> struct(RowType sStruct, GroupType struct,\n+                                        List<ParquetValueWriter<?>> fieldWriters) {\n+      List<Type> fields = struct.getFields();\n+      List<RowField> flinkFields = sStruct.getFields();\n+      List<ParquetValueWriter<?>> writers = Lists.newArrayListWithExpectedSize(fieldWriters.size());\n+      List<LogicalType> flinkTypes = Lists.newArrayList();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        writers.add(newOption(struct.getType(i), fieldWriters.get(i)));\n+        flinkTypes.add(flinkFields.get(i).getType());\n+      }\n+\n+      return new RowDataWriter(writers, flinkTypes);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> list(ArrayType sArray, GroupType array, ParquetValueWriter<?> elementWriter) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath);\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath);\n+\n+      return new ArrayDataWriter<>(repeatedD, repeatedR,\n+          newOption(repeated.getType(0), elementWriter),\n+          sArray.getElementType());\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> map(MapType sMap, GroupType map,\n+                                     ParquetValueWriter<?> keyWriter, ParquetValueWriter<?> valueWriter) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath);\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath);\n+\n+      return new MapDataWriter<>(repeatedD, repeatedR,\n+          newOption(repeatedKeyValue.getType(0), keyWriter),\n+          newOption(repeatedKeyValue.getType(1), valueWriter),\n+          sMap.getKeyType(), sMap.getValueType());\n+    }\n+\n+\n+    private ParquetValueWriter<?> newOption(org.apache.parquet.schema.Type fieldType, ParquetValueWriter<?> writer) {\n+      int maxD = type.getMaxDefinitionLevel(path(fieldType.getName()));\n+      return ParquetValueWriters.option(fieldType, maxD, writer);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> primitive(LogicalType sType, PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return strings(desc);\n+          case DATE:\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+            return ints(sType, desc);\n+          case INT_64:\n+            return ParquetValueWriters.longs(desc);\n+          case TIME_MICROS:\n+            return timeMicros(desc);\n+          case TIMESTAMP_MICROS:\n+            return timestamps(desc);\n+          case DECIMAL:\n+            DecimalLogicalTypeAnnotation decimal = (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case INT32:\n+                return decimalAsInteger(desc, decimal.getPrecision(), decimal.getScale());\n+              case INT64:\n+                return decimalAsLong(desc, decimal.getPrecision(), decimal.getScale());\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return decimalAsFixed(desc, decimal.getPrecision(), decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          case BSON:\n+            return byteArrays(desc);\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      }\n+\n+      switch (primitive.getPrimitiveTypeName()) {\n+        case FIXED_LEN_BYTE_ARRAY:\n+        case BINARY:\n+          return byteArrays(desc);\n+        case BOOLEAN:\n+          return ParquetValueWriters.booleans(desc);\n+        case INT32:\n+          return ints(sType, desc);\n+        case INT64:\n+          return ParquetValueWriters.longs(desc);\n+        case FLOAT:\n+          return ParquetValueWriters.floats(desc);\n+        case DOUBLE:\n+          return ParquetValueWriters.doubles(desc);\n+        default:\n+          throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+      }\n+    }\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<?> ints(LogicalType type, ColumnDescriptor desc) {\n+    if (type instanceof TinyIntType) {\n+      return ParquetValueWriters.tinyints(desc);\n+    } else if (type instanceof SmallIntType) {\n+      return ParquetValueWriters.shorts(desc);\n+    }\n+    return ParquetValueWriters.ints(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<StringData> strings(ColumnDescriptor desc) {\n+    return new StringDataWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<Integer> timeMicros(ColumnDescriptor desc) {\n+    return new TimeMicrosWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsInteger(ColumnDescriptor desc,\n+                                                                                   int precision, int scale) {\n+    return new IntegerDecimalWriter(desc, precision, scale);\n+  }\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsLong(ColumnDescriptor desc,\n+                                                                            int precision, int scale) {\n+    return new LongDecimalWriter(desc, precision, scale);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsFixed(ColumnDescriptor desc,\n+                                                                             int precision, int scale) {\n+    return new FixedDecimalWriter(desc, precision, scale);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<TimestampData> timestamps(ColumnDescriptor desc) {\n+    return new TimestampDataWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<byte[]> byteArrays(ColumnDescriptor desc) {\n+    return new ByteArrayWriter(desc);\n+  }\n+\n+  private static class StringDataWriter extends ParquetValueWriters.PrimitiveWriter<StringData> {\n+    private StringDataWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, StringData value) {\n+      column.writeBinary(repetitionLevel, Binary.fromReusedByteArray(value.toBytes()));\n+    }\n+  }\n+\n+  private static class TimeMicrosWriter extends ParquetValueWriters.PrimitiveWriter<Integer> {\n+    private TimeMicrosWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, Integer value) {\n+      long micros = Long.valueOf(value) * 1000;\n+      column.writeLong(repetitionLevel, micros);\n+    }\n+  }\n+\n+  private static class IntegerDecimalWriter extends ParquetValueWriters.PrimitiveWriter<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    private IntegerDecimalWriter(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, DecimalData decimal) {\n+      Preconditions.checkArgument(decimal.scale() == scale,\n+          \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, decimal);\n+      Preconditions.checkArgument(decimal.precision() <= precision,\n+          \"Cannot write value as decimal(%s,%s), too large: %s\", precision, scale, decimal);\n+\n+      column.writeInteger(repetitionLevel, (int) decimal.toUnscaledLong());\n+    }\n+  }\n+\n+  private static class LongDecimalWriter extends ParquetValueWriters.PrimitiveWriter<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    private LongDecimalWriter(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, DecimalData decimal) {\n+      Preconditions.checkArgument(decimal.scale() == scale,\n+          \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, decimal);\n+      Preconditions.checkArgument(decimal.precision() <= precision,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e78c2ecdee84c3cf0d54afa84e111faaa0e498df"}, "originalPosition": 271}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTE5NzIxOQ==", "bodyText": "How about adding this when allocating the writer? Seems like that would be a suitable place since here we are checking Flink type.", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r469197219", "createdAt": "2020-08-12T11:43:40Z", "author": {"login": "chenjunjiedada"}, "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java", "diffHunk": "@@ -19,38 +19,457 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.math.BigDecimal;\n+import java.util.Iterator;\n import java.util.List;\n-import org.apache.flink.types.Row;\n-import org.apache.iceberg.data.parquet.BaseParquetWriter;\n+import java.util.Map;\n+import java.util.NoSuchElementException;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.MapType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.RowType.RowField;\n+import org.apache.flink.table.types.logical.SmallIntType;\n+import org.apache.flink.table.types.logical.TinyIntType;\n+import org.apache.iceberg.parquet.ParquetValueReaders;\n import org.apache.iceberg.parquet.ParquetValueWriter;\n import org.apache.iceberg.parquet.ParquetValueWriters;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetWriters extends BaseParquetWriter<Row> {\n+public class FlinkParquetWriters {\n+  private FlinkParquetWriters() {\n+  }\n \n-  private static final FlinkParquetWriters INSTANCE = new FlinkParquetWriters();\n+  @SuppressWarnings(\"unchecked\")\n+  public static <T> ParquetValueWriter<T> buildWriter(LogicalType schema, MessageType type) {\n+    return (ParquetValueWriter<T>) ParquetWithFlinkSchemaVisitor.visit(schema, type, new WriteBuilder(type));\n+  }\n \n-  private FlinkParquetWriters() {\n+  private static class WriteBuilder extends ParquetWithFlinkSchemaVisitor<ParquetValueWriter<?>> {\n+    private final MessageType type;\n+\n+    WriteBuilder(MessageType type) {\n+      this.type = type;\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> message(RowType sStruct, MessageType message, List<ParquetValueWriter<?>> fields) {\n+      return struct(sStruct, message.asGroupType(), fields);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> struct(RowType sStruct, GroupType struct,\n+                                        List<ParquetValueWriter<?>> fieldWriters) {\n+      List<Type> fields = struct.getFields();\n+      List<RowField> flinkFields = sStruct.getFields();\n+      List<ParquetValueWriter<?>> writers = Lists.newArrayListWithExpectedSize(fieldWriters.size());\n+      List<LogicalType> flinkTypes = Lists.newArrayList();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        writers.add(newOption(struct.getType(i), fieldWriters.get(i)));\n+        flinkTypes.add(flinkFields.get(i).getType());\n+      }\n+\n+      return new RowDataWriter(writers, flinkTypes);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> list(ArrayType sArray, GroupType array, ParquetValueWriter<?> elementWriter) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath);\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath);\n+\n+      return new ArrayDataWriter<>(repeatedD, repeatedR,\n+          newOption(repeated.getType(0), elementWriter),\n+          sArray.getElementType());\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> map(MapType sMap, GroupType map,\n+                                     ParquetValueWriter<?> keyWriter, ParquetValueWriter<?> valueWriter) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath);\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath);\n+\n+      return new MapDataWriter<>(repeatedD, repeatedR,\n+          newOption(repeatedKeyValue.getType(0), keyWriter),\n+          newOption(repeatedKeyValue.getType(1), valueWriter),\n+          sMap.getKeyType(), sMap.getValueType());\n+    }\n+\n+\n+    private ParquetValueWriter<?> newOption(org.apache.parquet.schema.Type fieldType, ParquetValueWriter<?> writer) {\n+      int maxD = type.getMaxDefinitionLevel(path(fieldType.getName()));\n+      return ParquetValueWriters.option(fieldType, maxD, writer);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> primitive(LogicalType sType, PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return strings(desc);\n+          case DATE:\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+            return ints(sType, desc);\n+          case INT_64:\n+            return ParquetValueWriters.longs(desc);\n+          case TIME_MICROS:\n+            return timeMicros(desc);\n+          case TIMESTAMP_MICROS:\n+            return timestamps(desc);\n+          case DECIMAL:\n+            DecimalLogicalTypeAnnotation decimal = (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case INT32:\n+                return decimalAsInteger(desc, decimal.getPrecision(), decimal.getScale());\n+              case INT64:\n+                return decimalAsLong(desc, decimal.getPrecision(), decimal.getScale());\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return decimalAsFixed(desc, decimal.getPrecision(), decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          case BSON:\n+            return byteArrays(desc);\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      }\n+\n+      switch (primitive.getPrimitiveTypeName()) {\n+        case FIXED_LEN_BYTE_ARRAY:\n+        case BINARY:\n+          return byteArrays(desc);\n+        case BOOLEAN:\n+          return ParquetValueWriters.booleans(desc);\n+        case INT32:\n+          return ints(sType, desc);\n+        case INT64:\n+          return ParquetValueWriters.longs(desc);\n+        case FLOAT:\n+          return ParquetValueWriters.floats(desc);\n+        case DOUBLE:\n+          return ParquetValueWriters.doubles(desc);\n+        default:\n+          throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+      }\n+    }\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<?> ints(LogicalType type, ColumnDescriptor desc) {\n+    if (type instanceof TinyIntType) {\n+      return ParquetValueWriters.tinyints(desc);\n+    } else if (type instanceof SmallIntType) {\n+      return ParquetValueWriters.shorts(desc);\n+    }\n+    return ParquetValueWriters.ints(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<StringData> strings(ColumnDescriptor desc) {\n+    return new StringDataWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<Integer> timeMicros(ColumnDescriptor desc) {\n+    return new TimeMicrosWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsInteger(ColumnDescriptor desc,\n+                                                                                   int precision, int scale) {\n+    return new IntegerDecimalWriter(desc, precision, scale);\n+  }\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsLong(ColumnDescriptor desc,\n+                                                                            int precision, int scale) {\n+    return new LongDecimalWriter(desc, precision, scale);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsFixed(ColumnDescriptor desc,\n+                                                                             int precision, int scale) {\n+    return new FixedDecimalWriter(desc, precision, scale);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<TimestampData> timestamps(ColumnDescriptor desc) {\n+    return new TimestampDataWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<byte[]> byteArrays(ColumnDescriptor desc) {\n+    return new ByteArrayWriter(desc);\n+  }\n+\n+  private static class StringDataWriter extends ParquetValueWriters.PrimitiveWriter<StringData> {\n+    private StringDataWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, StringData value) {\n+      column.writeBinary(repetitionLevel, Binary.fromReusedByteArray(value.toBytes()));\n+    }\n+  }\n+\n+  private static class TimeMicrosWriter extends ParquetValueWriters.PrimitiveWriter<Integer> {\n+    private TimeMicrosWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, Integer value) {\n+      long micros = Long.valueOf(value) * 1000;\n+      column.writeLong(repetitionLevel, micros);\n+    }\n+  }\n+\n+  private static class IntegerDecimalWriter extends ParquetValueWriters.PrimitiveWriter<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    private IntegerDecimalWriter(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, DecimalData decimal) {\n+      Preconditions.checkArgument(decimal.scale() == scale,\n+          \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, decimal);\n+      Preconditions.checkArgument(decimal.precision() <= precision,\n+          \"Cannot write value as decimal(%s,%s), too large: %s\", precision, scale, decimal);\n+\n+      column.writeInteger(repetitionLevel, (int) decimal.toUnscaledLong());\n+    }\n+  }\n+\n+  private static class LongDecimalWriter extends ParquetValueWriters.PrimitiveWriter<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    private LongDecimalWriter(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, DecimalData decimal) {\n+      Preconditions.checkArgument(decimal.scale() == scale,\n+          \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, decimal);\n+      Preconditions.checkArgument(decimal.precision() <= precision,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA4Mjg4OA=="}, "originalCommit": {"oid": "e78c2ecdee84c3cf0d54afa84e111faaa0e498df"}, "originalPosition": 271}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzcwMzg5Nw==", "bodyText": "@openinx, any comment?", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r473703897", "createdAt": "2020-08-20T07:38:28Z", "author": {"login": "chenjunjiedada"}, "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java", "diffHunk": "@@ -19,38 +19,457 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.math.BigDecimal;\n+import java.util.Iterator;\n import java.util.List;\n-import org.apache.flink.types.Row;\n-import org.apache.iceberg.data.parquet.BaseParquetWriter;\n+import java.util.Map;\n+import java.util.NoSuchElementException;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.MapType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.RowType.RowField;\n+import org.apache.flink.table.types.logical.SmallIntType;\n+import org.apache.flink.table.types.logical.TinyIntType;\n+import org.apache.iceberg.parquet.ParquetValueReaders;\n import org.apache.iceberg.parquet.ParquetValueWriter;\n import org.apache.iceberg.parquet.ParquetValueWriters;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetWriters extends BaseParquetWriter<Row> {\n+public class FlinkParquetWriters {\n+  private FlinkParquetWriters() {\n+  }\n \n-  private static final FlinkParquetWriters INSTANCE = new FlinkParquetWriters();\n+  @SuppressWarnings(\"unchecked\")\n+  public static <T> ParquetValueWriter<T> buildWriter(LogicalType schema, MessageType type) {\n+    return (ParquetValueWriter<T>) ParquetWithFlinkSchemaVisitor.visit(schema, type, new WriteBuilder(type));\n+  }\n \n-  private FlinkParquetWriters() {\n+  private static class WriteBuilder extends ParquetWithFlinkSchemaVisitor<ParquetValueWriter<?>> {\n+    private final MessageType type;\n+\n+    WriteBuilder(MessageType type) {\n+      this.type = type;\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> message(RowType sStruct, MessageType message, List<ParquetValueWriter<?>> fields) {\n+      return struct(sStruct, message.asGroupType(), fields);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> struct(RowType sStruct, GroupType struct,\n+                                        List<ParquetValueWriter<?>> fieldWriters) {\n+      List<Type> fields = struct.getFields();\n+      List<RowField> flinkFields = sStruct.getFields();\n+      List<ParquetValueWriter<?>> writers = Lists.newArrayListWithExpectedSize(fieldWriters.size());\n+      List<LogicalType> flinkTypes = Lists.newArrayList();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        writers.add(newOption(struct.getType(i), fieldWriters.get(i)));\n+        flinkTypes.add(flinkFields.get(i).getType());\n+      }\n+\n+      return new RowDataWriter(writers, flinkTypes);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> list(ArrayType sArray, GroupType array, ParquetValueWriter<?> elementWriter) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath);\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath);\n+\n+      return new ArrayDataWriter<>(repeatedD, repeatedR,\n+          newOption(repeated.getType(0), elementWriter),\n+          sArray.getElementType());\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> map(MapType sMap, GroupType map,\n+                                     ParquetValueWriter<?> keyWriter, ParquetValueWriter<?> valueWriter) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath);\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath);\n+\n+      return new MapDataWriter<>(repeatedD, repeatedR,\n+          newOption(repeatedKeyValue.getType(0), keyWriter),\n+          newOption(repeatedKeyValue.getType(1), valueWriter),\n+          sMap.getKeyType(), sMap.getValueType());\n+    }\n+\n+\n+    private ParquetValueWriter<?> newOption(org.apache.parquet.schema.Type fieldType, ParquetValueWriter<?> writer) {\n+      int maxD = type.getMaxDefinitionLevel(path(fieldType.getName()));\n+      return ParquetValueWriters.option(fieldType, maxD, writer);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> primitive(LogicalType sType, PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return strings(desc);\n+          case DATE:\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+            return ints(sType, desc);\n+          case INT_64:\n+            return ParquetValueWriters.longs(desc);\n+          case TIME_MICROS:\n+            return timeMicros(desc);\n+          case TIMESTAMP_MICROS:\n+            return timestamps(desc);\n+          case DECIMAL:\n+            DecimalLogicalTypeAnnotation decimal = (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case INT32:\n+                return decimalAsInteger(desc, decimal.getPrecision(), decimal.getScale());\n+              case INT64:\n+                return decimalAsLong(desc, decimal.getPrecision(), decimal.getScale());\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return decimalAsFixed(desc, decimal.getPrecision(), decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          case BSON:\n+            return byteArrays(desc);\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      }\n+\n+      switch (primitive.getPrimitiveTypeName()) {\n+        case FIXED_LEN_BYTE_ARRAY:\n+        case BINARY:\n+          return byteArrays(desc);\n+        case BOOLEAN:\n+          return ParquetValueWriters.booleans(desc);\n+        case INT32:\n+          return ints(sType, desc);\n+        case INT64:\n+          return ParquetValueWriters.longs(desc);\n+        case FLOAT:\n+          return ParquetValueWriters.floats(desc);\n+        case DOUBLE:\n+          return ParquetValueWriters.doubles(desc);\n+        default:\n+          throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+      }\n+    }\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<?> ints(LogicalType type, ColumnDescriptor desc) {\n+    if (type instanceof TinyIntType) {\n+      return ParquetValueWriters.tinyints(desc);\n+    } else if (type instanceof SmallIntType) {\n+      return ParquetValueWriters.shorts(desc);\n+    }\n+    return ParquetValueWriters.ints(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<StringData> strings(ColumnDescriptor desc) {\n+    return new StringDataWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<Integer> timeMicros(ColumnDescriptor desc) {\n+    return new TimeMicrosWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsInteger(ColumnDescriptor desc,\n+                                                                                   int precision, int scale) {\n+    return new IntegerDecimalWriter(desc, precision, scale);\n+  }\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsLong(ColumnDescriptor desc,\n+                                                                            int precision, int scale) {\n+    return new LongDecimalWriter(desc, precision, scale);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsFixed(ColumnDescriptor desc,\n+                                                                             int precision, int scale) {\n+    return new FixedDecimalWriter(desc, precision, scale);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<TimestampData> timestamps(ColumnDescriptor desc) {\n+    return new TimestampDataWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<byte[]> byteArrays(ColumnDescriptor desc) {\n+    return new ByteArrayWriter(desc);\n+  }\n+\n+  private static class StringDataWriter extends ParquetValueWriters.PrimitiveWriter<StringData> {\n+    private StringDataWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, StringData value) {\n+      column.writeBinary(repetitionLevel, Binary.fromReusedByteArray(value.toBytes()));\n+    }\n+  }\n+\n+  private static class TimeMicrosWriter extends ParquetValueWriters.PrimitiveWriter<Integer> {\n+    private TimeMicrosWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, Integer value) {\n+      long micros = Long.valueOf(value) * 1000;\n+      column.writeLong(repetitionLevel, micros);\n+    }\n+  }\n+\n+  private static class IntegerDecimalWriter extends ParquetValueWriters.PrimitiveWriter<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    private IntegerDecimalWriter(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, DecimalData decimal) {\n+      Preconditions.checkArgument(decimal.scale() == scale,\n+          \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, decimal);\n+      Preconditions.checkArgument(decimal.precision() <= precision,\n+          \"Cannot write value as decimal(%s,%s), too large: %s\", precision, scale, decimal);\n+\n+      column.writeInteger(repetitionLevel, (int) decimal.toUnscaledLong());\n+    }\n+  }\n+\n+  private static class LongDecimalWriter extends ParquetValueWriters.PrimitiveWriter<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    private LongDecimalWriter(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, DecimalData decimal) {\n+      Preconditions.checkArgument(decimal.scale() == scale,\n+          \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, decimal);\n+      Preconditions.checkArgument(decimal.precision() <= precision,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA4Mjg4OA=="}, "originalCommit": {"oid": "e78c2ecdee84c3cf0d54afa84e111faaa0e498df"}, "originalPosition": 271}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjAwMjUyNQ==", "bodyText": "I think it would be better to do this in the constructor, like @chenjunjiedada suggests. That way we have a check that precision is not larger than the maximum allowed by the type, and that the correct writer is used for the type.", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r476002525", "createdAt": "2020-08-25T00:33:42Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java", "diffHunk": "@@ -19,38 +19,457 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.math.BigDecimal;\n+import java.util.Iterator;\n import java.util.List;\n-import org.apache.flink.types.Row;\n-import org.apache.iceberg.data.parquet.BaseParquetWriter;\n+import java.util.Map;\n+import java.util.NoSuchElementException;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.MapType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.RowType.RowField;\n+import org.apache.flink.table.types.logical.SmallIntType;\n+import org.apache.flink.table.types.logical.TinyIntType;\n+import org.apache.iceberg.parquet.ParquetValueReaders;\n import org.apache.iceberg.parquet.ParquetValueWriter;\n import org.apache.iceberg.parquet.ParquetValueWriters;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetWriters extends BaseParquetWriter<Row> {\n+public class FlinkParquetWriters {\n+  private FlinkParquetWriters() {\n+  }\n \n-  private static final FlinkParquetWriters INSTANCE = new FlinkParquetWriters();\n+  @SuppressWarnings(\"unchecked\")\n+  public static <T> ParquetValueWriter<T> buildWriter(LogicalType schema, MessageType type) {\n+    return (ParquetValueWriter<T>) ParquetWithFlinkSchemaVisitor.visit(schema, type, new WriteBuilder(type));\n+  }\n \n-  private FlinkParquetWriters() {\n+  private static class WriteBuilder extends ParquetWithFlinkSchemaVisitor<ParquetValueWriter<?>> {\n+    private final MessageType type;\n+\n+    WriteBuilder(MessageType type) {\n+      this.type = type;\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> message(RowType sStruct, MessageType message, List<ParquetValueWriter<?>> fields) {\n+      return struct(sStruct, message.asGroupType(), fields);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> struct(RowType sStruct, GroupType struct,\n+                                        List<ParquetValueWriter<?>> fieldWriters) {\n+      List<Type> fields = struct.getFields();\n+      List<RowField> flinkFields = sStruct.getFields();\n+      List<ParquetValueWriter<?>> writers = Lists.newArrayListWithExpectedSize(fieldWriters.size());\n+      List<LogicalType> flinkTypes = Lists.newArrayList();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        writers.add(newOption(struct.getType(i), fieldWriters.get(i)));\n+        flinkTypes.add(flinkFields.get(i).getType());\n+      }\n+\n+      return new RowDataWriter(writers, flinkTypes);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> list(ArrayType sArray, GroupType array, ParquetValueWriter<?> elementWriter) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath);\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath);\n+\n+      return new ArrayDataWriter<>(repeatedD, repeatedR,\n+          newOption(repeated.getType(0), elementWriter),\n+          sArray.getElementType());\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> map(MapType sMap, GroupType map,\n+                                     ParquetValueWriter<?> keyWriter, ParquetValueWriter<?> valueWriter) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath);\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath);\n+\n+      return new MapDataWriter<>(repeatedD, repeatedR,\n+          newOption(repeatedKeyValue.getType(0), keyWriter),\n+          newOption(repeatedKeyValue.getType(1), valueWriter),\n+          sMap.getKeyType(), sMap.getValueType());\n+    }\n+\n+\n+    private ParquetValueWriter<?> newOption(org.apache.parquet.schema.Type fieldType, ParquetValueWriter<?> writer) {\n+      int maxD = type.getMaxDefinitionLevel(path(fieldType.getName()));\n+      return ParquetValueWriters.option(fieldType, maxD, writer);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> primitive(LogicalType sType, PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return strings(desc);\n+          case DATE:\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+            return ints(sType, desc);\n+          case INT_64:\n+            return ParquetValueWriters.longs(desc);\n+          case TIME_MICROS:\n+            return timeMicros(desc);\n+          case TIMESTAMP_MICROS:\n+            return timestamps(desc);\n+          case DECIMAL:\n+            DecimalLogicalTypeAnnotation decimal = (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case INT32:\n+                return decimalAsInteger(desc, decimal.getPrecision(), decimal.getScale());\n+              case INT64:\n+                return decimalAsLong(desc, decimal.getPrecision(), decimal.getScale());\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return decimalAsFixed(desc, decimal.getPrecision(), decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          case BSON:\n+            return byteArrays(desc);\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      }\n+\n+      switch (primitive.getPrimitiveTypeName()) {\n+        case FIXED_LEN_BYTE_ARRAY:\n+        case BINARY:\n+          return byteArrays(desc);\n+        case BOOLEAN:\n+          return ParquetValueWriters.booleans(desc);\n+        case INT32:\n+          return ints(sType, desc);\n+        case INT64:\n+          return ParquetValueWriters.longs(desc);\n+        case FLOAT:\n+          return ParquetValueWriters.floats(desc);\n+        case DOUBLE:\n+          return ParquetValueWriters.doubles(desc);\n+        default:\n+          throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+      }\n+    }\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<?> ints(LogicalType type, ColumnDescriptor desc) {\n+    if (type instanceof TinyIntType) {\n+      return ParquetValueWriters.tinyints(desc);\n+    } else if (type instanceof SmallIntType) {\n+      return ParquetValueWriters.shorts(desc);\n+    }\n+    return ParquetValueWriters.ints(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<StringData> strings(ColumnDescriptor desc) {\n+    return new StringDataWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<Integer> timeMicros(ColumnDescriptor desc) {\n+    return new TimeMicrosWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsInteger(ColumnDescriptor desc,\n+                                                                                   int precision, int scale) {\n+    return new IntegerDecimalWriter(desc, precision, scale);\n+  }\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsLong(ColumnDescriptor desc,\n+                                                                            int precision, int scale) {\n+    return new LongDecimalWriter(desc, precision, scale);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsFixed(ColumnDescriptor desc,\n+                                                                             int precision, int scale) {\n+    return new FixedDecimalWriter(desc, precision, scale);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<TimestampData> timestamps(ColumnDescriptor desc) {\n+    return new TimestampDataWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<byte[]> byteArrays(ColumnDescriptor desc) {\n+    return new ByteArrayWriter(desc);\n+  }\n+\n+  private static class StringDataWriter extends ParquetValueWriters.PrimitiveWriter<StringData> {\n+    private StringDataWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, StringData value) {\n+      column.writeBinary(repetitionLevel, Binary.fromReusedByteArray(value.toBytes()));\n+    }\n+  }\n+\n+  private static class TimeMicrosWriter extends ParquetValueWriters.PrimitiveWriter<Integer> {\n+    private TimeMicrosWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, Integer value) {\n+      long micros = Long.valueOf(value) * 1000;\n+      column.writeLong(repetitionLevel, micros);\n+    }\n+  }\n+\n+  private static class IntegerDecimalWriter extends ParquetValueWriters.PrimitiveWriter<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    private IntegerDecimalWriter(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, DecimalData decimal) {\n+      Preconditions.checkArgument(decimal.scale() == scale,\n+          \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, decimal);\n+      Preconditions.checkArgument(decimal.precision() <= precision,\n+          \"Cannot write value as decimal(%s,%s), too large: %s\", precision, scale, decimal);\n+\n+      column.writeInteger(repetitionLevel, (int) decimal.toUnscaledLong());\n+    }\n+  }\n+\n+  private static class LongDecimalWriter extends ParquetValueWriters.PrimitiveWriter<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    private LongDecimalWriter(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, DecimalData decimal) {\n+      Preconditions.checkArgument(decimal.scale() == scale,\n+          \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, decimal);\n+      Preconditions.checkArgument(decimal.precision() <= precision,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA4Mjg4OA=="}, "originalCommit": {"oid": "e78c2ecdee84c3cf0d54afa84e111faaa0e498df"}, "originalPosition": 271}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzMDk0MTQ4OnYy", "diffSide": "RIGHT", "path": "flink/src/main/java/org/apache/iceberg/flink/RowTaskWriterFactory.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQwODowMjowOVrOG_WQBg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQxMToxMDo0NVrOG_cmXA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTA3ODAyMg==", "bodyText": "This should have simiar issue to the comment, which will break the unit test. If we rebase the master once  #1320 get merged, then it should have no problem.", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r469078022", "createdAt": "2020-08-12T08:02:09Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/RowTaskWriterFactory.java", "diffHunk": "@@ -120,11 +121,12 @@ private FlinkFileAppenderFactory(Schema schema, Map<String, String> props) {\n     @Override\n     public FileAppender<Row> newAppender(OutputFile outputFile, FileFormat format) {\n       MetricsConfig metricsConfig = MetricsConfig.fromProperties(props);\n+      LogicalType logicalType = FlinkSchemaUtil.convert(schema);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "940e4356566be9ca245b95c50a76f95727824b88"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTA4NzI4MQ==", "bodyText": "BTW,  we may also need to add the parquet into the parameterized unit tests, such as TestIcebergStreamWriter & TestTaskWriters.", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r469087281", "createdAt": "2020-08-12T08:19:26Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/RowTaskWriterFactory.java", "diffHunk": "@@ -120,11 +121,12 @@ private FlinkFileAppenderFactory(Schema schema, Map<String, String> props) {\n     @Override\n     public FileAppender<Row> newAppender(OutputFile outputFile, FileFormat format) {\n       MetricsConfig metricsConfig = MetricsConfig.fromProperties(props);\n+      LogicalType logicalType = FlinkSchemaUtil.convert(schema);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTA3ODAyMg=="}, "originalCommit": {"oid": "940e4356566be9ca245b95c50a76f95727824b88"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTE4MjA0NA==", "bodyText": "Agreed,  will take a look when these PRs get in.", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r469182044", "createdAt": "2020-08-12T11:10:45Z", "author": {"login": "chenjunjiedada"}, "path": "flink/src/main/java/org/apache/iceberg/flink/RowTaskWriterFactory.java", "diffHunk": "@@ -120,11 +121,12 @@ private FlinkFileAppenderFactory(Schema schema, Map<String, String> props) {\n     @Override\n     public FileAppender<Row> newAppender(OutputFile outputFile, FileFormat format) {\n       MetricsConfig metricsConfig = MetricsConfig.fromProperties(props);\n+      LogicalType logicalType = FlinkSchemaUtil.convert(schema);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTA3ODAyMg=="}, "originalCommit": {"oid": "940e4356566be9ca245b95c50a76f95727824b88"}, "originalPosition": 12}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzMDk4NTY0OnYy", "diffSide": "RIGHT", "path": "flink/src/test/java/org/apache/iceberg/flink/data/RandomData.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQwODoxNToyOVrOG_WrlQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQxMToxMDozMVrOG_cmBg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTA4NTA3Nw==", "bodyText": "We could use RandomRowData#generate  when rebasing the patch https://github.com/apache/iceberg/pull/1320/files#diff-4b2a9fd76495497db9212d74bf03f671R33.", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r469085077", "createdAt": "2020-08-12T08:15:29Z", "author": {"login": "openinx"}, "path": "flink/src/test/java/org/apache/iceberg/flink/data/RandomData.java", "diffHunk": "@@ -88,20 +104,153 @@ public Row next() {\n     };\n   }\n \n+  private static Iterable<RowData> generateRowData(Schema schema, int numRecords,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "940e4356566be9ca245b95c50a76f95727824b88"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTE4MTk1OA==", "bodyText": "Will do.", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r469181958", "createdAt": "2020-08-12T11:10:31Z", "author": {"login": "chenjunjiedada"}, "path": "flink/src/test/java/org/apache/iceberg/flink/data/RandomData.java", "diffHunk": "@@ -88,20 +104,153 @@ public Row next() {\n     };\n   }\n \n+  private static Iterable<RowData> generateRowData(Schema schema, int numRecords,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTA4NTA3Nw=="}, "originalCommit": {"oid": "940e4356566be9ca245b95c50a76f95727824b88"}, "originalPosition": 45}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzMTA1MTM3OnYy", "diffSide": "RIGHT", "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQwODozMjowOVrOG_XUjg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQxMTozMTo0N1rOG_dMKA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTA5NTU2Ng==", "bodyText": "Seems it should be decimal.precision <= precision ?", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r469095566", "createdAt": "2020-08-12T08:32:09Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java", "diffHunk": "@@ -19,38 +19,436 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.util.Iterator;\n import java.util.List;\n-import org.apache.flink.types.Row;\n-import org.apache.iceberg.data.parquet.BaseParquetWriter;\n+import java.util.Map;\n+import java.util.NoSuchElementException;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.MapType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.RowType.RowField;\n+import org.apache.flink.table.types.logical.SmallIntType;\n+import org.apache.flink.table.types.logical.TinyIntType;\n+import org.apache.iceberg.parquet.ParquetValueReaders;\n import org.apache.iceberg.parquet.ParquetValueWriter;\n import org.apache.iceberg.parquet.ParquetValueWriters;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.DecimalUtil;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetWriters extends BaseParquetWriter<Row> {\n+public class FlinkParquetWriters {\n+  private FlinkParquetWriters() {\n+  }\n \n-  private static final FlinkParquetWriters INSTANCE = new FlinkParquetWriters();\n+  @SuppressWarnings(\"unchecked\")\n+  public static <T> ParquetValueWriter<T> buildWriter(LogicalType schema, MessageType type) {\n+    return (ParquetValueWriter<T>) ParquetWithFlinkSchemaVisitor.visit(schema, type, new WriteBuilder(type));\n+  }\n \n-  private FlinkParquetWriters() {\n+  private static class WriteBuilder extends ParquetWithFlinkSchemaVisitor<ParquetValueWriter<?>> {\n+    private final MessageType type;\n+\n+    WriteBuilder(MessageType type) {\n+      this.type = type;\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> message(RowType sStruct, MessageType message, List<ParquetValueWriter<?>> fields) {\n+      return struct(sStruct, message.asGroupType(), fields);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> struct(RowType sStruct, GroupType struct,\n+                                        List<ParquetValueWriter<?>> fieldWriters) {\n+      List<Type> fields = struct.getFields();\n+      List<RowField> flinkFields = sStruct.getFields();\n+      List<ParquetValueWriter<?>> writers = Lists.newArrayListWithExpectedSize(fieldWriters.size());\n+      List<LogicalType> flinkTypes = Lists.newArrayList();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        writers.add(newOption(struct.getType(i), fieldWriters.get(i)));\n+        flinkTypes.add(flinkFields.get(i).getType());\n+      }\n+\n+      return new RowDataWriter(writers, flinkTypes);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> list(ArrayType sArray, GroupType array, ParquetValueWriter<?> elementWriter) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath);\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath);\n+\n+      return new ArrayDataWriter<>(repeatedD, repeatedR,\n+          newOption(repeated.getType(0), elementWriter),\n+          sArray.getElementType());\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> map(MapType sMap, GroupType map,\n+                                     ParquetValueWriter<?> keyWriter, ParquetValueWriter<?> valueWriter) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath);\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath);\n+\n+      return new MapDataWriter<>(repeatedD, repeatedR,\n+          newOption(repeatedKeyValue.getType(0), keyWriter),\n+          newOption(repeatedKeyValue.getType(1), valueWriter),\n+          sMap.getKeyType(), sMap.getValueType());\n+    }\n+\n+\n+    private ParquetValueWriter<?> newOption(org.apache.parquet.schema.Type fieldType, ParquetValueWriter<?> writer) {\n+      int maxD = type.getMaxDefinitionLevel(path(fieldType.getName()));\n+      return ParquetValueWriters.option(fieldType, maxD, writer);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> primitive(LogicalType sType, PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return strings(desc);\n+          case DATE:\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+            return ints(sType, desc);\n+          case INT_64:\n+            return ParquetValueWriters.longs(desc);\n+          case TIME_MICROS:\n+            return timeMicros(desc);\n+          case TIMESTAMP_MICROS:\n+            return timestamps(desc);\n+          case DECIMAL:\n+            DecimalLogicalTypeAnnotation decimal = (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case INT32:\n+                return decimalAsInteger(desc, decimal.getPrecision(), decimal.getScale());\n+              case INT64:\n+                return decimalAsLong(desc, decimal.getPrecision(), decimal.getScale());\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return decimalAsFixed(desc, decimal.getPrecision(), decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          case BSON:\n+            return byteArrays(desc);\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      }\n+\n+      switch (primitive.getPrimitiveTypeName()) {\n+        case FIXED_LEN_BYTE_ARRAY:\n+        case BINARY:\n+          return byteArrays(desc);\n+        case BOOLEAN:\n+          return ParquetValueWriters.booleans(desc);\n+        case INT32:\n+          return ints(sType, desc);\n+        case INT64:\n+          return ParquetValueWriters.longs(desc);\n+        case FLOAT:\n+          return ParquetValueWriters.floats(desc);\n+        case DOUBLE:\n+          return ParquetValueWriters.doubles(desc);\n+        default:\n+          throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+      }\n+    }\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<?> ints(LogicalType type, ColumnDescriptor desc) {\n+    if (type instanceof TinyIntType) {\n+      return ParquetValueWriters.tinyints(desc);\n+    } else if (type instanceof SmallIntType) {\n+      return ParquetValueWriters.shorts(desc);\n+    }\n+    return ParquetValueWriters.ints(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<StringData> strings(ColumnDescriptor desc) {\n+    return new StringDataWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<Integer> timeMicros(ColumnDescriptor desc) {\n+    return new TimeMicrosWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsInteger(ColumnDescriptor desc,\n+                                                                                   int precision, int scale) {\n+    return new IntegerDecimalWriter(desc, precision, scale);\n+  }\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsLong(ColumnDescriptor desc,\n+                                                                            int precision, int scale) {\n+    return new LongDecimalWriter(desc, precision, scale);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsFixed(ColumnDescriptor desc,\n+                                                                             int precision, int scale) {\n+    return new FixedDecimalWriter(desc, precision, scale);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<TimestampData> timestamps(ColumnDescriptor desc) {\n+    return new TimestampDataWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<byte[]> byteArrays(ColumnDescriptor desc) {\n+    return new ByteArrayWriter(desc);\n+  }\n+\n+  private static class StringDataWriter extends ParquetValueWriters.PrimitiveWriter<StringData> {\n+    private StringDataWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, StringData value) {\n+      column.writeBinary(repetitionLevel, Binary.fromReusedByteArray(value.toBytes()));\n+    }\n+  }\n+\n+  private static class TimeMicrosWriter extends ParquetValueWriters.PrimitiveWriter<Integer> {\n+    private TimeMicrosWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, Integer value) {\n+      long micros = Long.valueOf(value) * 1000;\n+      column.writeLong(repetitionLevel, micros);\n+    }\n+  }\n+\n+  private static class IntegerDecimalWriter extends ParquetValueWriters.PrimitiveWriter<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    private IntegerDecimalWriter(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, DecimalData decimal) {\n+      Preconditions.checkArgument(decimal.scale() == scale,\n+          \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, decimal);\n+      Preconditions.checkArgument(decimal.precision() <= 9,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "940e4356566be9ca245b95c50a76f95727824b88"}, "originalPosition": 250}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTE5MTcyMA==", "bodyText": "Seems like I misunderstood your comments, let me update this.", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r469191720", "createdAt": "2020-08-12T11:31:47Z", "author": {"login": "chenjunjiedada"}, "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java", "diffHunk": "@@ -19,38 +19,436 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.util.Iterator;\n import java.util.List;\n-import org.apache.flink.types.Row;\n-import org.apache.iceberg.data.parquet.BaseParquetWriter;\n+import java.util.Map;\n+import java.util.NoSuchElementException;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.MapType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.RowType.RowField;\n+import org.apache.flink.table.types.logical.SmallIntType;\n+import org.apache.flink.table.types.logical.TinyIntType;\n+import org.apache.iceberg.parquet.ParquetValueReaders;\n import org.apache.iceberg.parquet.ParquetValueWriter;\n import org.apache.iceberg.parquet.ParquetValueWriters;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.DecimalUtil;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetWriters extends BaseParquetWriter<Row> {\n+public class FlinkParquetWriters {\n+  private FlinkParquetWriters() {\n+  }\n \n-  private static final FlinkParquetWriters INSTANCE = new FlinkParquetWriters();\n+  @SuppressWarnings(\"unchecked\")\n+  public static <T> ParquetValueWriter<T> buildWriter(LogicalType schema, MessageType type) {\n+    return (ParquetValueWriter<T>) ParquetWithFlinkSchemaVisitor.visit(schema, type, new WriteBuilder(type));\n+  }\n \n-  private FlinkParquetWriters() {\n+  private static class WriteBuilder extends ParquetWithFlinkSchemaVisitor<ParquetValueWriter<?>> {\n+    private final MessageType type;\n+\n+    WriteBuilder(MessageType type) {\n+      this.type = type;\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> message(RowType sStruct, MessageType message, List<ParquetValueWriter<?>> fields) {\n+      return struct(sStruct, message.asGroupType(), fields);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> struct(RowType sStruct, GroupType struct,\n+                                        List<ParquetValueWriter<?>> fieldWriters) {\n+      List<Type> fields = struct.getFields();\n+      List<RowField> flinkFields = sStruct.getFields();\n+      List<ParquetValueWriter<?>> writers = Lists.newArrayListWithExpectedSize(fieldWriters.size());\n+      List<LogicalType> flinkTypes = Lists.newArrayList();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        writers.add(newOption(struct.getType(i), fieldWriters.get(i)));\n+        flinkTypes.add(flinkFields.get(i).getType());\n+      }\n+\n+      return new RowDataWriter(writers, flinkTypes);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> list(ArrayType sArray, GroupType array, ParquetValueWriter<?> elementWriter) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath);\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath);\n+\n+      return new ArrayDataWriter<>(repeatedD, repeatedR,\n+          newOption(repeated.getType(0), elementWriter),\n+          sArray.getElementType());\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> map(MapType sMap, GroupType map,\n+                                     ParquetValueWriter<?> keyWriter, ParquetValueWriter<?> valueWriter) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath);\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath);\n+\n+      return new MapDataWriter<>(repeatedD, repeatedR,\n+          newOption(repeatedKeyValue.getType(0), keyWriter),\n+          newOption(repeatedKeyValue.getType(1), valueWriter),\n+          sMap.getKeyType(), sMap.getValueType());\n+    }\n+\n+\n+    private ParquetValueWriter<?> newOption(org.apache.parquet.schema.Type fieldType, ParquetValueWriter<?> writer) {\n+      int maxD = type.getMaxDefinitionLevel(path(fieldType.getName()));\n+      return ParquetValueWriters.option(fieldType, maxD, writer);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> primitive(LogicalType sType, PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return strings(desc);\n+          case DATE:\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+            return ints(sType, desc);\n+          case INT_64:\n+            return ParquetValueWriters.longs(desc);\n+          case TIME_MICROS:\n+            return timeMicros(desc);\n+          case TIMESTAMP_MICROS:\n+            return timestamps(desc);\n+          case DECIMAL:\n+            DecimalLogicalTypeAnnotation decimal = (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case INT32:\n+                return decimalAsInteger(desc, decimal.getPrecision(), decimal.getScale());\n+              case INT64:\n+                return decimalAsLong(desc, decimal.getPrecision(), decimal.getScale());\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return decimalAsFixed(desc, decimal.getPrecision(), decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          case BSON:\n+            return byteArrays(desc);\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      }\n+\n+      switch (primitive.getPrimitiveTypeName()) {\n+        case FIXED_LEN_BYTE_ARRAY:\n+        case BINARY:\n+          return byteArrays(desc);\n+        case BOOLEAN:\n+          return ParquetValueWriters.booleans(desc);\n+        case INT32:\n+          return ints(sType, desc);\n+        case INT64:\n+          return ParquetValueWriters.longs(desc);\n+        case FLOAT:\n+          return ParquetValueWriters.floats(desc);\n+        case DOUBLE:\n+          return ParquetValueWriters.doubles(desc);\n+        default:\n+          throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+      }\n+    }\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<?> ints(LogicalType type, ColumnDescriptor desc) {\n+    if (type instanceof TinyIntType) {\n+      return ParquetValueWriters.tinyints(desc);\n+    } else if (type instanceof SmallIntType) {\n+      return ParquetValueWriters.shorts(desc);\n+    }\n+    return ParquetValueWriters.ints(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<StringData> strings(ColumnDescriptor desc) {\n+    return new StringDataWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<Integer> timeMicros(ColumnDescriptor desc) {\n+    return new TimeMicrosWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsInteger(ColumnDescriptor desc,\n+                                                                                   int precision, int scale) {\n+    return new IntegerDecimalWriter(desc, precision, scale);\n+  }\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsLong(ColumnDescriptor desc,\n+                                                                            int precision, int scale) {\n+    return new LongDecimalWriter(desc, precision, scale);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsFixed(ColumnDescriptor desc,\n+                                                                             int precision, int scale) {\n+    return new FixedDecimalWriter(desc, precision, scale);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<TimestampData> timestamps(ColumnDescriptor desc) {\n+    return new TimestampDataWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<byte[]> byteArrays(ColumnDescriptor desc) {\n+    return new ByteArrayWriter(desc);\n+  }\n+\n+  private static class StringDataWriter extends ParquetValueWriters.PrimitiveWriter<StringData> {\n+    private StringDataWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, StringData value) {\n+      column.writeBinary(repetitionLevel, Binary.fromReusedByteArray(value.toBytes()));\n+    }\n+  }\n+\n+  private static class TimeMicrosWriter extends ParquetValueWriters.PrimitiveWriter<Integer> {\n+    private TimeMicrosWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, Integer value) {\n+      long micros = Long.valueOf(value) * 1000;\n+      column.writeLong(repetitionLevel, micros);\n+    }\n+  }\n+\n+  private static class IntegerDecimalWriter extends ParquetValueWriters.PrimitiveWriter<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    private IntegerDecimalWriter(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, DecimalData decimal) {\n+      Preconditions.checkArgument(decimal.scale() == scale,\n+          \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, decimal);\n+      Preconditions.checkArgument(decimal.precision() <= 9,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTA5NTU2Ng=="}, "originalCommit": {"oid": "940e4356566be9ca245b95c50a76f95727824b88"}, "originalPosition": 250}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzMTA1MzY5OnYy", "diffSide": "RIGHT", "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQwODozMjo0N1rOG_XV-g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQwODozMjo0N1rOG_XV-g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTA5NTkzMA==", "bodyText": "ditto", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r469095930", "createdAt": "2020-08-12T08:32:47Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java", "diffHunk": "@@ -19,38 +19,436 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.util.Iterator;\n import java.util.List;\n-import org.apache.flink.types.Row;\n-import org.apache.iceberg.data.parquet.BaseParquetWriter;\n+import java.util.Map;\n+import java.util.NoSuchElementException;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.MapType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.RowType.RowField;\n+import org.apache.flink.table.types.logical.SmallIntType;\n+import org.apache.flink.table.types.logical.TinyIntType;\n+import org.apache.iceberg.parquet.ParquetValueReaders;\n import org.apache.iceberg.parquet.ParquetValueWriter;\n import org.apache.iceberg.parquet.ParquetValueWriters;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.DecimalUtil;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetWriters extends BaseParquetWriter<Row> {\n+public class FlinkParquetWriters {\n+  private FlinkParquetWriters() {\n+  }\n \n-  private static final FlinkParquetWriters INSTANCE = new FlinkParquetWriters();\n+  @SuppressWarnings(\"unchecked\")\n+  public static <T> ParquetValueWriter<T> buildWriter(LogicalType schema, MessageType type) {\n+    return (ParquetValueWriter<T>) ParquetWithFlinkSchemaVisitor.visit(schema, type, new WriteBuilder(type));\n+  }\n \n-  private FlinkParquetWriters() {\n+  private static class WriteBuilder extends ParquetWithFlinkSchemaVisitor<ParquetValueWriter<?>> {\n+    private final MessageType type;\n+\n+    WriteBuilder(MessageType type) {\n+      this.type = type;\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> message(RowType sStruct, MessageType message, List<ParquetValueWriter<?>> fields) {\n+      return struct(sStruct, message.asGroupType(), fields);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> struct(RowType sStruct, GroupType struct,\n+                                        List<ParquetValueWriter<?>> fieldWriters) {\n+      List<Type> fields = struct.getFields();\n+      List<RowField> flinkFields = sStruct.getFields();\n+      List<ParquetValueWriter<?>> writers = Lists.newArrayListWithExpectedSize(fieldWriters.size());\n+      List<LogicalType> flinkTypes = Lists.newArrayList();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        writers.add(newOption(struct.getType(i), fieldWriters.get(i)));\n+        flinkTypes.add(flinkFields.get(i).getType());\n+      }\n+\n+      return new RowDataWriter(writers, flinkTypes);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> list(ArrayType sArray, GroupType array, ParquetValueWriter<?> elementWriter) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath);\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath);\n+\n+      return new ArrayDataWriter<>(repeatedD, repeatedR,\n+          newOption(repeated.getType(0), elementWriter),\n+          sArray.getElementType());\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> map(MapType sMap, GroupType map,\n+                                     ParquetValueWriter<?> keyWriter, ParquetValueWriter<?> valueWriter) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath);\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath);\n+\n+      return new MapDataWriter<>(repeatedD, repeatedR,\n+          newOption(repeatedKeyValue.getType(0), keyWriter),\n+          newOption(repeatedKeyValue.getType(1), valueWriter),\n+          sMap.getKeyType(), sMap.getValueType());\n+    }\n+\n+\n+    private ParquetValueWriter<?> newOption(org.apache.parquet.schema.Type fieldType, ParquetValueWriter<?> writer) {\n+      int maxD = type.getMaxDefinitionLevel(path(fieldType.getName()));\n+      return ParquetValueWriters.option(fieldType, maxD, writer);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> primitive(LogicalType sType, PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return strings(desc);\n+          case DATE:\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+            return ints(sType, desc);\n+          case INT_64:\n+            return ParquetValueWriters.longs(desc);\n+          case TIME_MICROS:\n+            return timeMicros(desc);\n+          case TIMESTAMP_MICROS:\n+            return timestamps(desc);\n+          case DECIMAL:\n+            DecimalLogicalTypeAnnotation decimal = (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case INT32:\n+                return decimalAsInteger(desc, decimal.getPrecision(), decimal.getScale());\n+              case INT64:\n+                return decimalAsLong(desc, decimal.getPrecision(), decimal.getScale());\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return decimalAsFixed(desc, decimal.getPrecision(), decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          case BSON:\n+            return byteArrays(desc);\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      }\n+\n+      switch (primitive.getPrimitiveTypeName()) {\n+        case FIXED_LEN_BYTE_ARRAY:\n+        case BINARY:\n+          return byteArrays(desc);\n+        case BOOLEAN:\n+          return ParquetValueWriters.booleans(desc);\n+        case INT32:\n+          return ints(sType, desc);\n+        case INT64:\n+          return ParquetValueWriters.longs(desc);\n+        case FLOAT:\n+          return ParquetValueWriters.floats(desc);\n+        case DOUBLE:\n+          return ParquetValueWriters.doubles(desc);\n+        default:\n+          throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+      }\n+    }\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<?> ints(LogicalType type, ColumnDescriptor desc) {\n+    if (type instanceof TinyIntType) {\n+      return ParquetValueWriters.tinyints(desc);\n+    } else if (type instanceof SmallIntType) {\n+      return ParquetValueWriters.shorts(desc);\n+    }\n+    return ParquetValueWriters.ints(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<StringData> strings(ColumnDescriptor desc) {\n+    return new StringDataWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<Integer> timeMicros(ColumnDescriptor desc) {\n+    return new TimeMicrosWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsInteger(ColumnDescriptor desc,\n+                                                                                   int precision, int scale) {\n+    return new IntegerDecimalWriter(desc, precision, scale);\n+  }\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsLong(ColumnDescriptor desc,\n+                                                                            int precision, int scale) {\n+    return new LongDecimalWriter(desc, precision, scale);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsFixed(ColumnDescriptor desc,\n+                                                                             int precision, int scale) {\n+    return new FixedDecimalWriter(desc, precision, scale);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<TimestampData> timestamps(ColumnDescriptor desc) {\n+    return new TimestampDataWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<byte[]> byteArrays(ColumnDescriptor desc) {\n+    return new ByteArrayWriter(desc);\n+  }\n+\n+  private static class StringDataWriter extends ParquetValueWriters.PrimitiveWriter<StringData> {\n+    private StringDataWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, StringData value) {\n+      column.writeBinary(repetitionLevel, Binary.fromReusedByteArray(value.toBytes()));\n+    }\n+  }\n+\n+  private static class TimeMicrosWriter extends ParquetValueWriters.PrimitiveWriter<Integer> {\n+    private TimeMicrosWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, Integer value) {\n+      long micros = Long.valueOf(value) * 1000;\n+      column.writeLong(repetitionLevel, micros);\n+    }\n+  }\n+\n+  private static class IntegerDecimalWriter extends ParquetValueWriters.PrimitiveWriter<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    private IntegerDecimalWriter(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, DecimalData decimal) {\n+      Preconditions.checkArgument(decimal.scale() == scale,\n+          \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, decimal);\n+      Preconditions.checkArgument(decimal.precision() <= 9,\n+          \"Cannot write value as decimal(%s,%s), too large: %s\", precision, scale, decimal);\n+\n+      column.writeInteger(repetitionLevel, (int) decimal.toUnscaledLong());\n+    }\n+  }\n+\n+  private static class LongDecimalWriter extends ParquetValueWriters.PrimitiveWriter<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    private LongDecimalWriter(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, DecimalData decimal) {\n+      Preconditions.checkArgument(decimal.scale() == scale,\n+          \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, decimal);\n+      Preconditions.checkArgument(decimal.precision() <= 18,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "940e4356566be9ca245b95c50a76f95727824b88"}, "originalPosition": 271}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzMTA2NzA0OnYy", "diffSide": "RIGHT", "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQwODozNjoyMVrOG_XeCg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQxMToyMToxMlrOG_c4Zw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTA5Nzk5NA==", "bodyText": "How about moving this ElementIterator to be a static class, then the map's EntryIterator could share it ?  Seems we could do it,  you could decide wether there is necessary.", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r469097994", "createdAt": "2020-08-12T08:36:21Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java", "diffHunk": "@@ -19,38 +19,436 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.util.Iterator;\n import java.util.List;\n-import org.apache.flink.types.Row;\n-import org.apache.iceberg.data.parquet.BaseParquetWriter;\n+import java.util.Map;\n+import java.util.NoSuchElementException;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.MapType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.RowType.RowField;\n+import org.apache.flink.table.types.logical.SmallIntType;\n+import org.apache.flink.table.types.logical.TinyIntType;\n+import org.apache.iceberg.parquet.ParquetValueReaders;\n import org.apache.iceberg.parquet.ParquetValueWriter;\n import org.apache.iceberg.parquet.ParquetValueWriters;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.DecimalUtil;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetWriters extends BaseParquetWriter<Row> {\n+public class FlinkParquetWriters {\n+  private FlinkParquetWriters() {\n+  }\n \n-  private static final FlinkParquetWriters INSTANCE = new FlinkParquetWriters();\n+  @SuppressWarnings(\"unchecked\")\n+  public static <T> ParquetValueWriter<T> buildWriter(LogicalType schema, MessageType type) {\n+    return (ParquetValueWriter<T>) ParquetWithFlinkSchemaVisitor.visit(schema, type, new WriteBuilder(type));\n+  }\n \n-  private FlinkParquetWriters() {\n+  private static class WriteBuilder extends ParquetWithFlinkSchemaVisitor<ParquetValueWriter<?>> {\n+    private final MessageType type;\n+\n+    WriteBuilder(MessageType type) {\n+      this.type = type;\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> message(RowType sStruct, MessageType message, List<ParquetValueWriter<?>> fields) {\n+      return struct(sStruct, message.asGroupType(), fields);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> struct(RowType sStruct, GroupType struct,\n+                                        List<ParquetValueWriter<?>> fieldWriters) {\n+      List<Type> fields = struct.getFields();\n+      List<RowField> flinkFields = sStruct.getFields();\n+      List<ParquetValueWriter<?>> writers = Lists.newArrayListWithExpectedSize(fieldWriters.size());\n+      List<LogicalType> flinkTypes = Lists.newArrayList();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        writers.add(newOption(struct.getType(i), fieldWriters.get(i)));\n+        flinkTypes.add(flinkFields.get(i).getType());\n+      }\n+\n+      return new RowDataWriter(writers, flinkTypes);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> list(ArrayType sArray, GroupType array, ParquetValueWriter<?> elementWriter) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath);\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath);\n+\n+      return new ArrayDataWriter<>(repeatedD, repeatedR,\n+          newOption(repeated.getType(0), elementWriter),\n+          sArray.getElementType());\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> map(MapType sMap, GroupType map,\n+                                     ParquetValueWriter<?> keyWriter, ParquetValueWriter<?> valueWriter) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath);\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath);\n+\n+      return new MapDataWriter<>(repeatedD, repeatedR,\n+          newOption(repeatedKeyValue.getType(0), keyWriter),\n+          newOption(repeatedKeyValue.getType(1), valueWriter),\n+          sMap.getKeyType(), sMap.getValueType());\n+    }\n+\n+\n+    private ParquetValueWriter<?> newOption(org.apache.parquet.schema.Type fieldType, ParquetValueWriter<?> writer) {\n+      int maxD = type.getMaxDefinitionLevel(path(fieldType.getName()));\n+      return ParquetValueWriters.option(fieldType, maxD, writer);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> primitive(LogicalType sType, PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return strings(desc);\n+          case DATE:\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+            return ints(sType, desc);\n+          case INT_64:\n+            return ParquetValueWriters.longs(desc);\n+          case TIME_MICROS:\n+            return timeMicros(desc);\n+          case TIMESTAMP_MICROS:\n+            return timestamps(desc);\n+          case DECIMAL:\n+            DecimalLogicalTypeAnnotation decimal = (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case INT32:\n+                return decimalAsInteger(desc, decimal.getPrecision(), decimal.getScale());\n+              case INT64:\n+                return decimalAsLong(desc, decimal.getPrecision(), decimal.getScale());\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return decimalAsFixed(desc, decimal.getPrecision(), decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          case BSON:\n+            return byteArrays(desc);\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      }\n+\n+      switch (primitive.getPrimitiveTypeName()) {\n+        case FIXED_LEN_BYTE_ARRAY:\n+        case BINARY:\n+          return byteArrays(desc);\n+        case BOOLEAN:\n+          return ParquetValueWriters.booleans(desc);\n+        case INT32:\n+          return ints(sType, desc);\n+        case INT64:\n+          return ParquetValueWriters.longs(desc);\n+        case FLOAT:\n+          return ParquetValueWriters.floats(desc);\n+        case DOUBLE:\n+          return ParquetValueWriters.doubles(desc);\n+        default:\n+          throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+      }\n+    }\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<?> ints(LogicalType type, ColumnDescriptor desc) {\n+    if (type instanceof TinyIntType) {\n+      return ParquetValueWriters.tinyints(desc);\n+    } else if (type instanceof SmallIntType) {\n+      return ParquetValueWriters.shorts(desc);\n+    }\n+    return ParquetValueWriters.ints(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<StringData> strings(ColumnDescriptor desc) {\n+    return new StringDataWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<Integer> timeMicros(ColumnDescriptor desc) {\n+    return new TimeMicrosWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsInteger(ColumnDescriptor desc,\n+                                                                                   int precision, int scale) {\n+    return new IntegerDecimalWriter(desc, precision, scale);\n+  }\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsLong(ColumnDescriptor desc,\n+                                                                            int precision, int scale) {\n+    return new LongDecimalWriter(desc, precision, scale);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsFixed(ColumnDescriptor desc,\n+                                                                             int precision, int scale) {\n+    return new FixedDecimalWriter(desc, precision, scale);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<TimestampData> timestamps(ColumnDescriptor desc) {\n+    return new TimestampDataWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<byte[]> byteArrays(ColumnDescriptor desc) {\n+    return new ByteArrayWriter(desc);\n+  }\n+\n+  private static class StringDataWriter extends ParquetValueWriters.PrimitiveWriter<StringData> {\n+    private StringDataWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, StringData value) {\n+      column.writeBinary(repetitionLevel, Binary.fromReusedByteArray(value.toBytes()));\n+    }\n+  }\n+\n+  private static class TimeMicrosWriter extends ParquetValueWriters.PrimitiveWriter<Integer> {\n+    private TimeMicrosWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, Integer value) {\n+      long micros = Long.valueOf(value) * 1000;\n+      column.writeLong(repetitionLevel, micros);\n+    }\n+  }\n+\n+  private static class IntegerDecimalWriter extends ParquetValueWriters.PrimitiveWriter<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    private IntegerDecimalWriter(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, DecimalData decimal) {\n+      Preconditions.checkArgument(decimal.scale() == scale,\n+          \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, decimal);\n+      Preconditions.checkArgument(decimal.precision() <= 9,\n+          \"Cannot write value as decimal(%s,%s), too large: %s\", precision, scale, decimal);\n+\n+      column.writeInteger(repetitionLevel, (int) decimal.toUnscaledLong());\n+    }\n+  }\n+\n+  private static class LongDecimalWriter extends ParquetValueWriters.PrimitiveWriter<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    private LongDecimalWriter(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, DecimalData decimal) {\n+      Preconditions.checkArgument(decimal.scale() == scale,\n+          \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, decimal);\n+      Preconditions.checkArgument(decimal.precision() <= 18,\n+          \"Cannot write value as decimal(%s,%s), too large: %s\", precision, scale, decimal);\n+\n+      column.writeLong(repetitionLevel, decimal.toUnscaledLong());\n+    }\n+  }\n+\n+  private static class FixedDecimalWriter extends ParquetValueWriters.PrimitiveWriter<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+    private final ThreadLocal<byte[]> bytes;\n+\n+    private FixedDecimalWriter(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+      this.bytes = ThreadLocal.withInitial(() -> new byte[TypeUtil.decimalRequiredBytes(precision)]);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, DecimalData decimal) {\n+      byte[] binary = DecimalUtil.toReusedFixLengthBytes(precision, scale, decimal.toBigDecimal(), bytes.get());\n+      column.writeBinary(repetitionLevel, Binary.fromReusedByteArray(binary));\n+    }\n+  }\n+\n+  private static class TimestampDataWriter extends ParquetValueWriters.PrimitiveWriter<TimestampData> {\n+    private TimestampDataWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, TimestampData value) {\n+      column.writeLong(repetitionLevel, value.getMillisecond() * 1000 + value.getNanoOfMillisecond() / 1000);\n+    }\n+  }\n+\n+  private static class ByteArrayWriter extends ParquetValueWriters.PrimitiveWriter<byte[]> {\n+    private ByteArrayWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, byte[] bytes) {\n+      column.writeBinary(repetitionLevel, Binary.fromReusedByteArray(bytes));\n+    }\n   }\n \n-  public static ParquetValueWriter<Row> buildWriter(MessageType type) {\n-    return INSTANCE.createWriter(type);\n+  private static class ArrayDataWriter<E> extends ParquetValueWriters.RepeatedWriter<ArrayData, E> {\n+    private final LogicalType elementType;\n+\n+    private ArrayDataWriter(int definitionLevel, int repetitionLevel,\n+                            ParquetValueWriter<E> writer, LogicalType elementType) {\n+      super(definitionLevel, repetitionLevel, writer);\n+      this.elementType = elementType;\n+    }\n+\n+    @Override\n+    protected Iterator<E> elements(ArrayData list) {\n+      return new ElementIterator<>(list);\n+    }\n+\n+    private class ElementIterator<E> implements Iterator<E> {\n+      private final int size;\n+      private final ArrayData list;\n+      private int index;\n+\n+      private ElementIterator(ArrayData list) {\n+        this.list = list;\n+        size = list.size();\n+        index = 0;\n+      }\n+\n+      @Override\n+      public boolean hasNext() {\n+        return index != size;\n+      }\n+\n+      @Override\n+      @SuppressWarnings(\"unchecked\")\n+      public E next() {\n+        if (index >= size) {\n+          throw new NoSuchElementException();\n+        }\n+\n+        E element;\n+        if (list.isNullAt(index)) {\n+          element = null;\n+        } else {\n+          element = (E) ArrayData.createElementGetter(elementType).getElementOrNull(list, index);\n+        }\n+\n+        index += 1;\n+\n+        return element;\n+      }\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "940e4356566be9ca245b95c50a76f95727824b88"}, "originalPosition": 369}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTE4NjY2Mw==", "bodyText": "I 'm not sure how can it be shared with EntryIterator.", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r469186663", "createdAt": "2020-08-12T11:21:12Z", "author": {"login": "chenjunjiedada"}, "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java", "diffHunk": "@@ -19,38 +19,436 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.util.Iterator;\n import java.util.List;\n-import org.apache.flink.types.Row;\n-import org.apache.iceberg.data.parquet.BaseParquetWriter;\n+import java.util.Map;\n+import java.util.NoSuchElementException;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.MapType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.RowType.RowField;\n+import org.apache.flink.table.types.logical.SmallIntType;\n+import org.apache.flink.table.types.logical.TinyIntType;\n+import org.apache.iceberg.parquet.ParquetValueReaders;\n import org.apache.iceberg.parquet.ParquetValueWriter;\n import org.apache.iceberg.parquet.ParquetValueWriters;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.DecimalUtil;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetWriters extends BaseParquetWriter<Row> {\n+public class FlinkParquetWriters {\n+  private FlinkParquetWriters() {\n+  }\n \n-  private static final FlinkParquetWriters INSTANCE = new FlinkParquetWriters();\n+  @SuppressWarnings(\"unchecked\")\n+  public static <T> ParquetValueWriter<T> buildWriter(LogicalType schema, MessageType type) {\n+    return (ParquetValueWriter<T>) ParquetWithFlinkSchemaVisitor.visit(schema, type, new WriteBuilder(type));\n+  }\n \n-  private FlinkParquetWriters() {\n+  private static class WriteBuilder extends ParquetWithFlinkSchemaVisitor<ParquetValueWriter<?>> {\n+    private final MessageType type;\n+\n+    WriteBuilder(MessageType type) {\n+      this.type = type;\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> message(RowType sStruct, MessageType message, List<ParquetValueWriter<?>> fields) {\n+      return struct(sStruct, message.asGroupType(), fields);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> struct(RowType sStruct, GroupType struct,\n+                                        List<ParquetValueWriter<?>> fieldWriters) {\n+      List<Type> fields = struct.getFields();\n+      List<RowField> flinkFields = sStruct.getFields();\n+      List<ParquetValueWriter<?>> writers = Lists.newArrayListWithExpectedSize(fieldWriters.size());\n+      List<LogicalType> flinkTypes = Lists.newArrayList();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        writers.add(newOption(struct.getType(i), fieldWriters.get(i)));\n+        flinkTypes.add(flinkFields.get(i).getType());\n+      }\n+\n+      return new RowDataWriter(writers, flinkTypes);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> list(ArrayType sArray, GroupType array, ParquetValueWriter<?> elementWriter) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath);\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath);\n+\n+      return new ArrayDataWriter<>(repeatedD, repeatedR,\n+          newOption(repeated.getType(0), elementWriter),\n+          sArray.getElementType());\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> map(MapType sMap, GroupType map,\n+                                     ParquetValueWriter<?> keyWriter, ParquetValueWriter<?> valueWriter) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath);\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath);\n+\n+      return new MapDataWriter<>(repeatedD, repeatedR,\n+          newOption(repeatedKeyValue.getType(0), keyWriter),\n+          newOption(repeatedKeyValue.getType(1), valueWriter),\n+          sMap.getKeyType(), sMap.getValueType());\n+    }\n+\n+\n+    private ParquetValueWriter<?> newOption(org.apache.parquet.schema.Type fieldType, ParquetValueWriter<?> writer) {\n+      int maxD = type.getMaxDefinitionLevel(path(fieldType.getName()));\n+      return ParquetValueWriters.option(fieldType, maxD, writer);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> primitive(LogicalType sType, PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return strings(desc);\n+          case DATE:\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+            return ints(sType, desc);\n+          case INT_64:\n+            return ParquetValueWriters.longs(desc);\n+          case TIME_MICROS:\n+            return timeMicros(desc);\n+          case TIMESTAMP_MICROS:\n+            return timestamps(desc);\n+          case DECIMAL:\n+            DecimalLogicalTypeAnnotation decimal = (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case INT32:\n+                return decimalAsInteger(desc, decimal.getPrecision(), decimal.getScale());\n+              case INT64:\n+                return decimalAsLong(desc, decimal.getPrecision(), decimal.getScale());\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return decimalAsFixed(desc, decimal.getPrecision(), decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          case BSON:\n+            return byteArrays(desc);\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      }\n+\n+      switch (primitive.getPrimitiveTypeName()) {\n+        case FIXED_LEN_BYTE_ARRAY:\n+        case BINARY:\n+          return byteArrays(desc);\n+        case BOOLEAN:\n+          return ParquetValueWriters.booleans(desc);\n+        case INT32:\n+          return ints(sType, desc);\n+        case INT64:\n+          return ParquetValueWriters.longs(desc);\n+        case FLOAT:\n+          return ParquetValueWriters.floats(desc);\n+        case DOUBLE:\n+          return ParquetValueWriters.doubles(desc);\n+        default:\n+          throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+      }\n+    }\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<?> ints(LogicalType type, ColumnDescriptor desc) {\n+    if (type instanceof TinyIntType) {\n+      return ParquetValueWriters.tinyints(desc);\n+    } else if (type instanceof SmallIntType) {\n+      return ParquetValueWriters.shorts(desc);\n+    }\n+    return ParquetValueWriters.ints(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<StringData> strings(ColumnDescriptor desc) {\n+    return new StringDataWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<Integer> timeMicros(ColumnDescriptor desc) {\n+    return new TimeMicrosWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsInteger(ColumnDescriptor desc,\n+                                                                                   int precision, int scale) {\n+    return new IntegerDecimalWriter(desc, precision, scale);\n+  }\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsLong(ColumnDescriptor desc,\n+                                                                            int precision, int scale) {\n+    return new LongDecimalWriter(desc, precision, scale);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsFixed(ColumnDescriptor desc,\n+                                                                             int precision, int scale) {\n+    return new FixedDecimalWriter(desc, precision, scale);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<TimestampData> timestamps(ColumnDescriptor desc) {\n+    return new TimestampDataWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<byte[]> byteArrays(ColumnDescriptor desc) {\n+    return new ByteArrayWriter(desc);\n+  }\n+\n+  private static class StringDataWriter extends ParquetValueWriters.PrimitiveWriter<StringData> {\n+    private StringDataWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, StringData value) {\n+      column.writeBinary(repetitionLevel, Binary.fromReusedByteArray(value.toBytes()));\n+    }\n+  }\n+\n+  private static class TimeMicrosWriter extends ParquetValueWriters.PrimitiveWriter<Integer> {\n+    private TimeMicrosWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, Integer value) {\n+      long micros = Long.valueOf(value) * 1000;\n+      column.writeLong(repetitionLevel, micros);\n+    }\n+  }\n+\n+  private static class IntegerDecimalWriter extends ParquetValueWriters.PrimitiveWriter<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    private IntegerDecimalWriter(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, DecimalData decimal) {\n+      Preconditions.checkArgument(decimal.scale() == scale,\n+          \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, decimal);\n+      Preconditions.checkArgument(decimal.precision() <= 9,\n+          \"Cannot write value as decimal(%s,%s), too large: %s\", precision, scale, decimal);\n+\n+      column.writeInteger(repetitionLevel, (int) decimal.toUnscaledLong());\n+    }\n+  }\n+\n+  private static class LongDecimalWriter extends ParquetValueWriters.PrimitiveWriter<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    private LongDecimalWriter(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, DecimalData decimal) {\n+      Preconditions.checkArgument(decimal.scale() == scale,\n+          \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, decimal);\n+      Preconditions.checkArgument(decimal.precision() <= 18,\n+          \"Cannot write value as decimal(%s,%s), too large: %s\", precision, scale, decimal);\n+\n+      column.writeLong(repetitionLevel, decimal.toUnscaledLong());\n+    }\n+  }\n+\n+  private static class FixedDecimalWriter extends ParquetValueWriters.PrimitiveWriter<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+    private final ThreadLocal<byte[]> bytes;\n+\n+    private FixedDecimalWriter(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+      this.bytes = ThreadLocal.withInitial(() -> new byte[TypeUtil.decimalRequiredBytes(precision)]);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, DecimalData decimal) {\n+      byte[] binary = DecimalUtil.toReusedFixLengthBytes(precision, scale, decimal.toBigDecimal(), bytes.get());\n+      column.writeBinary(repetitionLevel, Binary.fromReusedByteArray(binary));\n+    }\n+  }\n+\n+  private static class TimestampDataWriter extends ParquetValueWriters.PrimitiveWriter<TimestampData> {\n+    private TimestampDataWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, TimestampData value) {\n+      column.writeLong(repetitionLevel, value.getMillisecond() * 1000 + value.getNanoOfMillisecond() / 1000);\n+    }\n+  }\n+\n+  private static class ByteArrayWriter extends ParquetValueWriters.PrimitiveWriter<byte[]> {\n+    private ByteArrayWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, byte[] bytes) {\n+      column.writeBinary(repetitionLevel, Binary.fromReusedByteArray(bytes));\n+    }\n   }\n \n-  public static ParquetValueWriter<Row> buildWriter(MessageType type) {\n-    return INSTANCE.createWriter(type);\n+  private static class ArrayDataWriter<E> extends ParquetValueWriters.RepeatedWriter<ArrayData, E> {\n+    private final LogicalType elementType;\n+\n+    private ArrayDataWriter(int definitionLevel, int repetitionLevel,\n+                            ParquetValueWriter<E> writer, LogicalType elementType) {\n+      super(definitionLevel, repetitionLevel, writer);\n+      this.elementType = elementType;\n+    }\n+\n+    @Override\n+    protected Iterator<E> elements(ArrayData list) {\n+      return new ElementIterator<>(list);\n+    }\n+\n+    private class ElementIterator<E> implements Iterator<E> {\n+      private final int size;\n+      private final ArrayData list;\n+      private int index;\n+\n+      private ElementIterator(ArrayData list) {\n+        this.list = list;\n+        size = list.size();\n+        index = 0;\n+      }\n+\n+      @Override\n+      public boolean hasNext() {\n+        return index != size;\n+      }\n+\n+      @Override\n+      @SuppressWarnings(\"unchecked\")\n+      public E next() {\n+        if (index >= size) {\n+          throw new NoSuchElementException();\n+        }\n+\n+        E element;\n+        if (list.isNullAt(index)) {\n+          element = null;\n+        } else {\n+          element = (E) ArrayData.createElementGetter(elementType).getElementOrNull(list, index);\n+        }\n+\n+        index += 1;\n+\n+        return element;\n+      }\n+    }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTA5Nzk5NA=="}, "originalCommit": {"oid": "940e4356566be9ca245b95c50a76f95727824b88"}, "originalPosition": 369}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzMTA4ODM2OnYy", "diffSide": "RIGHT", "path": "flink/src/main/java/org/apache/iceberg/flink/data/ParquetWithFlinkSchemaVisitor.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQwODo0MjoxMVrOG_XrFA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNVQwMDo0NDoxM1rOHF9U3w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTEwMTMzMg==", "bodyText": "TODO: we could share both flink and spark ParquetSchemaVisitor in a common class , can be a separate issue.", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r469101332", "createdAt": "2020-08-12T08:42:11Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/data/ParquetWithFlinkSchemaVisitor.java", "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.data;\n+\n+import java.util.Deque;\n+import java.util.List;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.MapType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.RowType.RowField;\n+import org.apache.iceberg.avro.AvroSchemaUtil;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.OriginalType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n+\n+public class ParquetWithFlinkSchemaVisitor<T> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "940e4356566be9ca245b95c50a76f95727824b88"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTE4NDM2Mw==", "bodyText": "Agreed, I would prefer to do the refactor in a separated PR.", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r469184363", "createdAt": "2020-08-12T11:16:00Z", "author": {"login": "chenjunjiedada"}, "path": "flink/src/main/java/org/apache/iceberg/flink/data/ParquetWithFlinkSchemaVisitor.java", "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.data;\n+\n+import java.util.Deque;\n+import java.util.List;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.MapType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.RowType.RowField;\n+import org.apache.iceberg.avro.AvroSchemaUtil;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.OriginalType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n+\n+public class ParquetWithFlinkSchemaVisitor<T> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTEwMTMzMg=="}, "originalCommit": {"oid": "940e4356566be9ca245b95c50a76f95727824b88"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjAwOTY5NQ==", "bodyText": "Yes, a WithPartner visitor like @JingsongLi added would be great.", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r476009695", "createdAt": "2020-08-25T00:44:13Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/data/ParquetWithFlinkSchemaVisitor.java", "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.data;\n+\n+import java.util.Deque;\n+import java.util.List;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.MapType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.RowType.RowField;\n+import org.apache.iceberg.avro.AvroSchemaUtil;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.OriginalType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n+\n+public class ParquetWithFlinkSchemaVisitor<T> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTEwMTMzMg=="}, "originalCommit": {"oid": "940e4356566be9ca245b95c50a76f95727824b88"}, "originalPosition": 38}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk3NTg4NTk0OnYy", "diffSide": "RIGHT", "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNVQwMDoyNjozNlrOHF8ndg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNVQwMDoyNjozNlrOHF8ndg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTk5ODA3MA==", "bodyText": "Nit: s in sType indicates Spark. The equivalent here would be fType or a better name.", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r475998070", "createdAt": "2020-08-25T00:26:36Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java", "diffHunk": "@@ -19,38 +19,436 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.util.Iterator;\n import java.util.List;\n-import org.apache.flink.types.Row;\n-import org.apache.iceberg.data.parquet.BaseParquetWriter;\n+import java.util.Map;\n+import java.util.NoSuchElementException;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.MapType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.RowType.RowField;\n+import org.apache.flink.table.types.logical.SmallIntType;\n+import org.apache.flink.table.types.logical.TinyIntType;\n+import org.apache.iceberg.parquet.ParquetValueReaders;\n import org.apache.iceberg.parquet.ParquetValueWriter;\n import org.apache.iceberg.parquet.ParquetValueWriters;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.DecimalUtil;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetWriters extends BaseParquetWriter<Row> {\n+public class FlinkParquetWriters {\n+  private FlinkParquetWriters() {\n+  }\n \n-  private static final FlinkParquetWriters INSTANCE = new FlinkParquetWriters();\n+  @SuppressWarnings(\"unchecked\")\n+  public static <T> ParquetValueWriter<T> buildWriter(LogicalType schema, MessageType type) {\n+    return (ParquetValueWriter<T>) ParquetWithFlinkSchemaVisitor.visit(schema, type, new WriteBuilder(type));\n+  }\n \n-  private FlinkParquetWriters() {\n+  private static class WriteBuilder extends ParquetWithFlinkSchemaVisitor<ParquetValueWriter<?>> {\n+    private final MessageType type;\n+\n+    WriteBuilder(MessageType type) {\n+      this.type = type;\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> message(RowType sStruct, MessageType message, List<ParquetValueWriter<?>> fields) {\n+      return struct(sStruct, message.asGroupType(), fields);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> struct(RowType sStruct, GroupType struct,\n+                                        List<ParquetValueWriter<?>> fieldWriters) {\n+      List<Type> fields = struct.getFields();\n+      List<RowField> flinkFields = sStruct.getFields();\n+      List<ParquetValueWriter<?>> writers = Lists.newArrayListWithExpectedSize(fieldWriters.size());\n+      List<LogicalType> flinkTypes = Lists.newArrayList();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        writers.add(newOption(struct.getType(i), fieldWriters.get(i)));\n+        flinkTypes.add(flinkFields.get(i).getType());\n+      }\n+\n+      return new RowDataWriter(writers, flinkTypes);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> list(ArrayType sArray, GroupType array, ParquetValueWriter<?> elementWriter) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath);\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath);\n+\n+      return new ArrayDataWriter<>(repeatedD, repeatedR,\n+          newOption(repeated.getType(0), elementWriter),\n+          sArray.getElementType());\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> map(MapType sMap, GroupType map,\n+                                     ParquetValueWriter<?> keyWriter, ParquetValueWriter<?> valueWriter) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath);\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath);\n+\n+      return new MapDataWriter<>(repeatedD, repeatedR,\n+          newOption(repeatedKeyValue.getType(0), keyWriter),\n+          newOption(repeatedKeyValue.getType(1), valueWriter),\n+          sMap.getKeyType(), sMap.getValueType());\n+    }\n+\n+\n+    private ParquetValueWriter<?> newOption(org.apache.parquet.schema.Type fieldType, ParquetValueWriter<?> writer) {\n+      int maxD = type.getMaxDefinitionLevel(path(fieldType.getName()));\n+      return ParquetValueWriters.option(fieldType, maxD, writer);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> primitive(LogicalType sType, PrimitiveType primitive) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "caa4ca34dc48e334bc213c2117221d75fd597e8b"}, "originalPosition": 112}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk3NTkwNzY4OnYy", "diffSide": "RIGHT", "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNVQwMDozMjoxOVrOHF81tQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNVQwMDozMjoxOVrOHF81tQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjAwMTcxNw==", "bodyText": "This conversion from Integer doesn't make much sense. Java exposes 2 valueOf with string arguments and one with a primitive long argument. The last is what is called here. In that case, this is implicitly casting Integer to long, boxing the result, and then multiplying to produce a primitive.\nIt would be better to use value.longValue() * 1000 instead.", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r476001717", "createdAt": "2020-08-25T00:32:19Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java", "diffHunk": "@@ -19,38 +19,436 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.util.Iterator;\n import java.util.List;\n-import org.apache.flink.types.Row;\n-import org.apache.iceberg.data.parquet.BaseParquetWriter;\n+import java.util.Map;\n+import java.util.NoSuchElementException;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.MapType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.RowType.RowField;\n+import org.apache.flink.table.types.logical.SmallIntType;\n+import org.apache.flink.table.types.logical.TinyIntType;\n+import org.apache.iceberg.parquet.ParquetValueReaders;\n import org.apache.iceberg.parquet.ParquetValueWriter;\n import org.apache.iceberg.parquet.ParquetValueWriters;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.DecimalUtil;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetWriters extends BaseParquetWriter<Row> {\n+public class FlinkParquetWriters {\n+  private FlinkParquetWriters() {\n+  }\n \n-  private static final FlinkParquetWriters INSTANCE = new FlinkParquetWriters();\n+  @SuppressWarnings(\"unchecked\")\n+  public static <T> ParquetValueWriter<T> buildWriter(LogicalType schema, MessageType type) {\n+    return (ParquetValueWriter<T>) ParquetWithFlinkSchemaVisitor.visit(schema, type, new WriteBuilder(type));\n+  }\n \n-  private FlinkParquetWriters() {\n+  private static class WriteBuilder extends ParquetWithFlinkSchemaVisitor<ParquetValueWriter<?>> {\n+    private final MessageType type;\n+\n+    WriteBuilder(MessageType type) {\n+      this.type = type;\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> message(RowType sStruct, MessageType message, List<ParquetValueWriter<?>> fields) {\n+      return struct(sStruct, message.asGroupType(), fields);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> struct(RowType sStruct, GroupType struct,\n+                                        List<ParquetValueWriter<?>> fieldWriters) {\n+      List<Type> fields = struct.getFields();\n+      List<RowField> flinkFields = sStruct.getFields();\n+      List<ParquetValueWriter<?>> writers = Lists.newArrayListWithExpectedSize(fieldWriters.size());\n+      List<LogicalType> flinkTypes = Lists.newArrayList();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        writers.add(newOption(struct.getType(i), fieldWriters.get(i)));\n+        flinkTypes.add(flinkFields.get(i).getType());\n+      }\n+\n+      return new RowDataWriter(writers, flinkTypes);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> list(ArrayType sArray, GroupType array, ParquetValueWriter<?> elementWriter) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath);\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath);\n+\n+      return new ArrayDataWriter<>(repeatedD, repeatedR,\n+          newOption(repeated.getType(0), elementWriter),\n+          sArray.getElementType());\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> map(MapType sMap, GroupType map,\n+                                     ParquetValueWriter<?> keyWriter, ParquetValueWriter<?> valueWriter) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath);\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath);\n+\n+      return new MapDataWriter<>(repeatedD, repeatedR,\n+          newOption(repeatedKeyValue.getType(0), keyWriter),\n+          newOption(repeatedKeyValue.getType(1), valueWriter),\n+          sMap.getKeyType(), sMap.getValueType());\n+    }\n+\n+\n+    private ParquetValueWriter<?> newOption(org.apache.parquet.schema.Type fieldType, ParquetValueWriter<?> writer) {\n+      int maxD = type.getMaxDefinitionLevel(path(fieldType.getName()));\n+      return ParquetValueWriters.option(fieldType, maxD, writer);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> primitive(LogicalType sType, PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return strings(desc);\n+          case DATE:\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+            return ints(sType, desc);\n+          case INT_64:\n+            return ParquetValueWriters.longs(desc);\n+          case TIME_MICROS:\n+            return timeMicros(desc);\n+          case TIMESTAMP_MICROS:\n+            return timestamps(desc);\n+          case DECIMAL:\n+            DecimalLogicalTypeAnnotation decimal = (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case INT32:\n+                return decimalAsInteger(desc, decimal.getPrecision(), decimal.getScale());\n+              case INT64:\n+                return decimalAsLong(desc, decimal.getPrecision(), decimal.getScale());\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return decimalAsFixed(desc, decimal.getPrecision(), decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          case BSON:\n+            return byteArrays(desc);\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      }\n+\n+      switch (primitive.getPrimitiveTypeName()) {\n+        case FIXED_LEN_BYTE_ARRAY:\n+        case BINARY:\n+          return byteArrays(desc);\n+        case BOOLEAN:\n+          return ParquetValueWriters.booleans(desc);\n+        case INT32:\n+          return ints(sType, desc);\n+        case INT64:\n+          return ParquetValueWriters.longs(desc);\n+        case FLOAT:\n+          return ParquetValueWriters.floats(desc);\n+        case DOUBLE:\n+          return ParquetValueWriters.doubles(desc);\n+        default:\n+          throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+      }\n+    }\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<?> ints(LogicalType type, ColumnDescriptor desc) {\n+    if (type instanceof TinyIntType) {\n+      return ParquetValueWriters.tinyints(desc);\n+    } else if (type instanceof SmallIntType) {\n+      return ParquetValueWriters.shorts(desc);\n+    }\n+    return ParquetValueWriters.ints(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<StringData> strings(ColumnDescriptor desc) {\n+    return new StringDataWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<Integer> timeMicros(ColumnDescriptor desc) {\n+    return new TimeMicrosWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsInteger(ColumnDescriptor desc,\n+                                                                                   int precision, int scale) {\n+    return new IntegerDecimalWriter(desc, precision, scale);\n+  }\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsLong(ColumnDescriptor desc,\n+                                                                                int precision, int scale) {\n+    return new LongDecimalWriter(desc, precision, scale);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsFixed(ColumnDescriptor desc,\n+                                                                                 int precision, int scale) {\n+    return new FixedDecimalWriter(desc, precision, scale);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<TimestampData> timestamps(ColumnDescriptor desc) {\n+    return new TimestampDataWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<byte[]> byteArrays(ColumnDescriptor desc) {\n+    return new ByteArrayWriter(desc);\n+  }\n+\n+  private static class StringDataWriter extends ParquetValueWriters.PrimitiveWriter<StringData> {\n+    private StringDataWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, StringData value) {\n+      column.writeBinary(repetitionLevel, Binary.fromReusedByteArray(value.toBytes()));\n+    }\n+  }\n+\n+  private static class TimeMicrosWriter extends ParquetValueWriters.PrimitiveWriter<Integer> {\n+    private TimeMicrosWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, Integer value) {\n+      long micros = Long.valueOf(value) * 1000;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "caa4ca34dc48e334bc213c2117221d75fd597e8b"}, "originalPosition": 231}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk3NTkyNzE1OnYy", "diffSide": "RIGHT", "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNVQwMDozNzoyM1rOHF9ClQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNVQwMTo0NjoxN1rOHF_0pw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjAwNTAxMw==", "bodyText": "This method is called in a tight loop, so for performance any preparation that can be done in advance should be.\nThat means this getter should be created in the constructor and stored as an instance field. Then it can be called here.\nAlso, if there is already a null check above, does this need to call getElementOrNull or should it just call a get variant that assumes the value is non-null?\nAlternatively, you could replace the if here:\nE element = (E) getter.getElementOrNull(list, index);", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r476005013", "createdAt": "2020-08-25T00:37:23Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java", "diffHunk": "@@ -19,38 +19,436 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.util.Iterator;\n import java.util.List;\n-import org.apache.flink.types.Row;\n-import org.apache.iceberg.data.parquet.BaseParquetWriter;\n+import java.util.Map;\n+import java.util.NoSuchElementException;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.MapType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.RowType.RowField;\n+import org.apache.flink.table.types.logical.SmallIntType;\n+import org.apache.flink.table.types.logical.TinyIntType;\n+import org.apache.iceberg.parquet.ParquetValueReaders;\n import org.apache.iceberg.parquet.ParquetValueWriter;\n import org.apache.iceberg.parquet.ParquetValueWriters;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.DecimalUtil;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetWriters extends BaseParquetWriter<Row> {\n+public class FlinkParquetWriters {\n+  private FlinkParquetWriters() {\n+  }\n \n-  private static final FlinkParquetWriters INSTANCE = new FlinkParquetWriters();\n+  @SuppressWarnings(\"unchecked\")\n+  public static <T> ParquetValueWriter<T> buildWriter(LogicalType schema, MessageType type) {\n+    return (ParquetValueWriter<T>) ParquetWithFlinkSchemaVisitor.visit(schema, type, new WriteBuilder(type));\n+  }\n \n-  private FlinkParquetWriters() {\n+  private static class WriteBuilder extends ParquetWithFlinkSchemaVisitor<ParquetValueWriter<?>> {\n+    private final MessageType type;\n+\n+    WriteBuilder(MessageType type) {\n+      this.type = type;\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> message(RowType sStruct, MessageType message, List<ParquetValueWriter<?>> fields) {\n+      return struct(sStruct, message.asGroupType(), fields);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> struct(RowType sStruct, GroupType struct,\n+                                        List<ParquetValueWriter<?>> fieldWriters) {\n+      List<Type> fields = struct.getFields();\n+      List<RowField> flinkFields = sStruct.getFields();\n+      List<ParquetValueWriter<?>> writers = Lists.newArrayListWithExpectedSize(fieldWriters.size());\n+      List<LogicalType> flinkTypes = Lists.newArrayList();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        writers.add(newOption(struct.getType(i), fieldWriters.get(i)));\n+        flinkTypes.add(flinkFields.get(i).getType());\n+      }\n+\n+      return new RowDataWriter(writers, flinkTypes);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> list(ArrayType sArray, GroupType array, ParquetValueWriter<?> elementWriter) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath);\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath);\n+\n+      return new ArrayDataWriter<>(repeatedD, repeatedR,\n+          newOption(repeated.getType(0), elementWriter),\n+          sArray.getElementType());\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> map(MapType sMap, GroupType map,\n+                                     ParquetValueWriter<?> keyWriter, ParquetValueWriter<?> valueWriter) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath);\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath);\n+\n+      return new MapDataWriter<>(repeatedD, repeatedR,\n+          newOption(repeatedKeyValue.getType(0), keyWriter),\n+          newOption(repeatedKeyValue.getType(1), valueWriter),\n+          sMap.getKeyType(), sMap.getValueType());\n+    }\n+\n+\n+    private ParquetValueWriter<?> newOption(org.apache.parquet.schema.Type fieldType, ParquetValueWriter<?> writer) {\n+      int maxD = type.getMaxDefinitionLevel(path(fieldType.getName()));\n+      return ParquetValueWriters.option(fieldType, maxD, writer);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> primitive(LogicalType sType, PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return strings(desc);\n+          case DATE:\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+            return ints(sType, desc);\n+          case INT_64:\n+            return ParquetValueWriters.longs(desc);\n+          case TIME_MICROS:\n+            return timeMicros(desc);\n+          case TIMESTAMP_MICROS:\n+            return timestamps(desc);\n+          case DECIMAL:\n+            DecimalLogicalTypeAnnotation decimal = (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case INT32:\n+                return decimalAsInteger(desc, decimal.getPrecision(), decimal.getScale());\n+              case INT64:\n+                return decimalAsLong(desc, decimal.getPrecision(), decimal.getScale());\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return decimalAsFixed(desc, decimal.getPrecision(), decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          case BSON:\n+            return byteArrays(desc);\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      }\n+\n+      switch (primitive.getPrimitiveTypeName()) {\n+        case FIXED_LEN_BYTE_ARRAY:\n+        case BINARY:\n+          return byteArrays(desc);\n+        case BOOLEAN:\n+          return ParquetValueWriters.booleans(desc);\n+        case INT32:\n+          return ints(sType, desc);\n+        case INT64:\n+          return ParquetValueWriters.longs(desc);\n+        case FLOAT:\n+          return ParquetValueWriters.floats(desc);\n+        case DOUBLE:\n+          return ParquetValueWriters.doubles(desc);\n+        default:\n+          throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+      }\n+    }\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<?> ints(LogicalType type, ColumnDescriptor desc) {\n+    if (type instanceof TinyIntType) {\n+      return ParquetValueWriters.tinyints(desc);\n+    } else if (type instanceof SmallIntType) {\n+      return ParquetValueWriters.shorts(desc);\n+    }\n+    return ParquetValueWriters.ints(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<StringData> strings(ColumnDescriptor desc) {\n+    return new StringDataWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<Integer> timeMicros(ColumnDescriptor desc) {\n+    return new TimeMicrosWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsInteger(ColumnDescriptor desc,\n+                                                                                   int precision, int scale) {\n+    return new IntegerDecimalWriter(desc, precision, scale);\n+  }\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsLong(ColumnDescriptor desc,\n+                                                                                int precision, int scale) {\n+    return new LongDecimalWriter(desc, precision, scale);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsFixed(ColumnDescriptor desc,\n+                                                                                 int precision, int scale) {\n+    return new FixedDecimalWriter(desc, precision, scale);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<TimestampData> timestamps(ColumnDescriptor desc) {\n+    return new TimestampDataWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<byte[]> byteArrays(ColumnDescriptor desc) {\n+    return new ByteArrayWriter(desc);\n+  }\n+\n+  private static class StringDataWriter extends ParquetValueWriters.PrimitiveWriter<StringData> {\n+    private StringDataWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, StringData value) {\n+      column.writeBinary(repetitionLevel, Binary.fromReusedByteArray(value.toBytes()));\n+    }\n+  }\n+\n+  private static class TimeMicrosWriter extends ParquetValueWriters.PrimitiveWriter<Integer> {\n+    private TimeMicrosWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, Integer value) {\n+      long micros = Long.valueOf(value) * 1000;\n+      column.writeLong(repetitionLevel, micros);\n+    }\n+  }\n+\n+  private static class IntegerDecimalWriter extends ParquetValueWriters.PrimitiveWriter<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    private IntegerDecimalWriter(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, DecimalData decimal) {\n+      Preconditions.checkArgument(decimal.scale() == scale,\n+          \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, decimal);\n+      Preconditions.checkArgument(decimal.precision() <= precision,\n+          \"Cannot write value as decimal(%s,%s), too large: %s\", precision, scale, decimal);\n+\n+      column.writeInteger(repetitionLevel, (int) decimal.toUnscaledLong());\n+    }\n+  }\n+\n+  private static class LongDecimalWriter extends ParquetValueWriters.PrimitiveWriter<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    private LongDecimalWriter(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, DecimalData decimal) {\n+      Preconditions.checkArgument(decimal.scale() == scale,\n+          \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, decimal);\n+      Preconditions.checkArgument(decimal.precision() <= precision,\n+          \"Cannot write value as decimal(%s,%s), too large: %s\", precision, scale, decimal);\n+\n+      column.writeLong(repetitionLevel, decimal.toUnscaledLong());\n+    }\n+  }\n+\n+  private static class FixedDecimalWriter extends ParquetValueWriters.PrimitiveWriter<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+    private final ThreadLocal<byte[]> bytes;\n+\n+    private FixedDecimalWriter(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+      this.bytes = ThreadLocal.withInitial(() -> new byte[TypeUtil.decimalRequiredBytes(precision)]);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, DecimalData decimal) {\n+      byte[] binary = DecimalUtil.toReusedFixLengthBytes(precision, scale, decimal.toBigDecimal(), bytes.get());\n+      column.writeBinary(repetitionLevel, Binary.fromReusedByteArray(binary));\n+    }\n+  }\n+\n+  private static class TimestampDataWriter extends ParquetValueWriters.PrimitiveWriter<TimestampData> {\n+    private TimestampDataWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, TimestampData value) {\n+      column.writeLong(repetitionLevel, value.getMillisecond() * 1000 + value.getNanoOfMillisecond() / 1000);\n+    }\n+  }\n+\n+  private static class ByteArrayWriter extends ParquetValueWriters.PrimitiveWriter<byte[]> {\n+    private ByteArrayWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, byte[] bytes) {\n+      column.writeBinary(repetitionLevel, Binary.fromReusedByteArray(bytes));\n+    }\n   }\n \n-  public static ParquetValueWriter<Row> buildWriter(MessageType type) {\n-    return INSTANCE.createWriter(type);\n+  private static class ArrayDataWriter<E> extends ParquetValueWriters.RepeatedWriter<ArrayData, E> {\n+    private final LogicalType elementType;\n+\n+    private ArrayDataWriter(int definitionLevel, int repetitionLevel,\n+                            ParquetValueWriter<E> writer, LogicalType elementType) {\n+      super(definitionLevel, repetitionLevel, writer);\n+      this.elementType = elementType;\n+    }\n+\n+    @Override\n+    protected Iterator<E> elements(ArrayData list) {\n+      return new ElementIterator<>(list);\n+    }\n+\n+    private class ElementIterator<E> implements Iterator<E> {\n+      private final int size;\n+      private final ArrayData list;\n+      private int index;\n+\n+      private ElementIterator(ArrayData list) {\n+        this.list = list;\n+        size = list.size();\n+        index = 0;\n+      }\n+\n+      @Override\n+      public boolean hasNext() {\n+        return index != size;\n+      }\n+\n+      @Override\n+      @SuppressWarnings(\"unchecked\")\n+      public E next() {\n+        if (index >= size) {\n+          throw new NoSuchElementException();\n+        }\n+\n+        E element;\n+        if (list.isNullAt(index)) {\n+          element = null;\n+        } else {\n+          element = (E) ArrayData.createElementGetter(elementType).getElementOrNull(list, index);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "caa4ca34dc48e334bc213c2117221d75fd597e8b"}, "originalPosition": 362}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjA1MDU5OQ==", "bodyText": "That means this getter should be created in the constructor and stored as an instance field. Then it can be called here.\n\nYeah,  that sounds good to me, great point.\n\ndoes this need to call getElementOrNull or should it just call a get variant that assumes the value is non-null?\nThe getter in ArrayData don't have a get  interface,  it have only the interface:\n\n\t/**\n\t * Accessor for getting the elements of an array during runtime.\n\t *\n\t * @see #createElementGetter(LogicalType)\n\t */\n\tinterface ElementGetter extends Serializable {\n\t\t@Nullable Object getElementOrNull(ArrayData array, int pos);\n\t}\nReplacing the if-else to be E element = (E) getter.getElementOrNull(list, index);  sounds reasonable to me.", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r476050599", "createdAt": "2020-08-25T01:46:17Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java", "diffHunk": "@@ -19,38 +19,436 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.util.Iterator;\n import java.util.List;\n-import org.apache.flink.types.Row;\n-import org.apache.iceberg.data.parquet.BaseParquetWriter;\n+import java.util.Map;\n+import java.util.NoSuchElementException;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.MapType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.RowType.RowField;\n+import org.apache.flink.table.types.logical.SmallIntType;\n+import org.apache.flink.table.types.logical.TinyIntType;\n+import org.apache.iceberg.parquet.ParquetValueReaders;\n import org.apache.iceberg.parquet.ParquetValueWriter;\n import org.apache.iceberg.parquet.ParquetValueWriters;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.DecimalUtil;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetWriters extends BaseParquetWriter<Row> {\n+public class FlinkParquetWriters {\n+  private FlinkParquetWriters() {\n+  }\n \n-  private static final FlinkParquetWriters INSTANCE = new FlinkParquetWriters();\n+  @SuppressWarnings(\"unchecked\")\n+  public static <T> ParquetValueWriter<T> buildWriter(LogicalType schema, MessageType type) {\n+    return (ParquetValueWriter<T>) ParquetWithFlinkSchemaVisitor.visit(schema, type, new WriteBuilder(type));\n+  }\n \n-  private FlinkParquetWriters() {\n+  private static class WriteBuilder extends ParquetWithFlinkSchemaVisitor<ParquetValueWriter<?>> {\n+    private final MessageType type;\n+\n+    WriteBuilder(MessageType type) {\n+      this.type = type;\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> message(RowType sStruct, MessageType message, List<ParquetValueWriter<?>> fields) {\n+      return struct(sStruct, message.asGroupType(), fields);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> struct(RowType sStruct, GroupType struct,\n+                                        List<ParquetValueWriter<?>> fieldWriters) {\n+      List<Type> fields = struct.getFields();\n+      List<RowField> flinkFields = sStruct.getFields();\n+      List<ParquetValueWriter<?>> writers = Lists.newArrayListWithExpectedSize(fieldWriters.size());\n+      List<LogicalType> flinkTypes = Lists.newArrayList();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        writers.add(newOption(struct.getType(i), fieldWriters.get(i)));\n+        flinkTypes.add(flinkFields.get(i).getType());\n+      }\n+\n+      return new RowDataWriter(writers, flinkTypes);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> list(ArrayType sArray, GroupType array, ParquetValueWriter<?> elementWriter) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath);\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath);\n+\n+      return new ArrayDataWriter<>(repeatedD, repeatedR,\n+          newOption(repeated.getType(0), elementWriter),\n+          sArray.getElementType());\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> map(MapType sMap, GroupType map,\n+                                     ParquetValueWriter<?> keyWriter, ParquetValueWriter<?> valueWriter) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath);\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath);\n+\n+      return new MapDataWriter<>(repeatedD, repeatedR,\n+          newOption(repeatedKeyValue.getType(0), keyWriter),\n+          newOption(repeatedKeyValue.getType(1), valueWriter),\n+          sMap.getKeyType(), sMap.getValueType());\n+    }\n+\n+\n+    private ParquetValueWriter<?> newOption(org.apache.parquet.schema.Type fieldType, ParquetValueWriter<?> writer) {\n+      int maxD = type.getMaxDefinitionLevel(path(fieldType.getName()));\n+      return ParquetValueWriters.option(fieldType, maxD, writer);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> primitive(LogicalType sType, PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return strings(desc);\n+          case DATE:\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+            return ints(sType, desc);\n+          case INT_64:\n+            return ParquetValueWriters.longs(desc);\n+          case TIME_MICROS:\n+            return timeMicros(desc);\n+          case TIMESTAMP_MICROS:\n+            return timestamps(desc);\n+          case DECIMAL:\n+            DecimalLogicalTypeAnnotation decimal = (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case INT32:\n+                return decimalAsInteger(desc, decimal.getPrecision(), decimal.getScale());\n+              case INT64:\n+                return decimalAsLong(desc, decimal.getPrecision(), decimal.getScale());\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return decimalAsFixed(desc, decimal.getPrecision(), decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          case BSON:\n+            return byteArrays(desc);\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      }\n+\n+      switch (primitive.getPrimitiveTypeName()) {\n+        case FIXED_LEN_BYTE_ARRAY:\n+        case BINARY:\n+          return byteArrays(desc);\n+        case BOOLEAN:\n+          return ParquetValueWriters.booleans(desc);\n+        case INT32:\n+          return ints(sType, desc);\n+        case INT64:\n+          return ParquetValueWriters.longs(desc);\n+        case FLOAT:\n+          return ParquetValueWriters.floats(desc);\n+        case DOUBLE:\n+          return ParquetValueWriters.doubles(desc);\n+        default:\n+          throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+      }\n+    }\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<?> ints(LogicalType type, ColumnDescriptor desc) {\n+    if (type instanceof TinyIntType) {\n+      return ParquetValueWriters.tinyints(desc);\n+    } else if (type instanceof SmallIntType) {\n+      return ParquetValueWriters.shorts(desc);\n+    }\n+    return ParquetValueWriters.ints(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<StringData> strings(ColumnDescriptor desc) {\n+    return new StringDataWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<Integer> timeMicros(ColumnDescriptor desc) {\n+    return new TimeMicrosWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsInteger(ColumnDescriptor desc,\n+                                                                                   int precision, int scale) {\n+    return new IntegerDecimalWriter(desc, precision, scale);\n+  }\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsLong(ColumnDescriptor desc,\n+                                                                                int precision, int scale) {\n+    return new LongDecimalWriter(desc, precision, scale);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsFixed(ColumnDescriptor desc,\n+                                                                                 int precision, int scale) {\n+    return new FixedDecimalWriter(desc, precision, scale);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<TimestampData> timestamps(ColumnDescriptor desc) {\n+    return new TimestampDataWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<byte[]> byteArrays(ColumnDescriptor desc) {\n+    return new ByteArrayWriter(desc);\n+  }\n+\n+  private static class StringDataWriter extends ParquetValueWriters.PrimitiveWriter<StringData> {\n+    private StringDataWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, StringData value) {\n+      column.writeBinary(repetitionLevel, Binary.fromReusedByteArray(value.toBytes()));\n+    }\n+  }\n+\n+  private static class TimeMicrosWriter extends ParquetValueWriters.PrimitiveWriter<Integer> {\n+    private TimeMicrosWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, Integer value) {\n+      long micros = Long.valueOf(value) * 1000;\n+      column.writeLong(repetitionLevel, micros);\n+    }\n+  }\n+\n+  private static class IntegerDecimalWriter extends ParquetValueWriters.PrimitiveWriter<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    private IntegerDecimalWriter(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, DecimalData decimal) {\n+      Preconditions.checkArgument(decimal.scale() == scale,\n+          \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, decimal);\n+      Preconditions.checkArgument(decimal.precision() <= precision,\n+          \"Cannot write value as decimal(%s,%s), too large: %s\", precision, scale, decimal);\n+\n+      column.writeInteger(repetitionLevel, (int) decimal.toUnscaledLong());\n+    }\n+  }\n+\n+  private static class LongDecimalWriter extends ParquetValueWriters.PrimitiveWriter<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    private LongDecimalWriter(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, DecimalData decimal) {\n+      Preconditions.checkArgument(decimal.scale() == scale,\n+          \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, decimal);\n+      Preconditions.checkArgument(decimal.precision() <= precision,\n+          \"Cannot write value as decimal(%s,%s), too large: %s\", precision, scale, decimal);\n+\n+      column.writeLong(repetitionLevel, decimal.toUnscaledLong());\n+    }\n+  }\n+\n+  private static class FixedDecimalWriter extends ParquetValueWriters.PrimitiveWriter<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+    private final ThreadLocal<byte[]> bytes;\n+\n+    private FixedDecimalWriter(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+      this.bytes = ThreadLocal.withInitial(() -> new byte[TypeUtil.decimalRequiredBytes(precision)]);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, DecimalData decimal) {\n+      byte[] binary = DecimalUtil.toReusedFixLengthBytes(precision, scale, decimal.toBigDecimal(), bytes.get());\n+      column.writeBinary(repetitionLevel, Binary.fromReusedByteArray(binary));\n+    }\n+  }\n+\n+  private static class TimestampDataWriter extends ParquetValueWriters.PrimitiveWriter<TimestampData> {\n+    private TimestampDataWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, TimestampData value) {\n+      column.writeLong(repetitionLevel, value.getMillisecond() * 1000 + value.getNanoOfMillisecond() / 1000);\n+    }\n+  }\n+\n+  private static class ByteArrayWriter extends ParquetValueWriters.PrimitiveWriter<byte[]> {\n+    private ByteArrayWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, byte[] bytes) {\n+      column.writeBinary(repetitionLevel, Binary.fromReusedByteArray(bytes));\n+    }\n   }\n \n-  public static ParquetValueWriter<Row> buildWriter(MessageType type) {\n-    return INSTANCE.createWriter(type);\n+  private static class ArrayDataWriter<E> extends ParquetValueWriters.RepeatedWriter<ArrayData, E> {\n+    private final LogicalType elementType;\n+\n+    private ArrayDataWriter(int definitionLevel, int repetitionLevel,\n+                            ParquetValueWriter<E> writer, LogicalType elementType) {\n+      super(definitionLevel, repetitionLevel, writer);\n+      this.elementType = elementType;\n+    }\n+\n+    @Override\n+    protected Iterator<E> elements(ArrayData list) {\n+      return new ElementIterator<>(list);\n+    }\n+\n+    private class ElementIterator<E> implements Iterator<E> {\n+      private final int size;\n+      private final ArrayData list;\n+      private int index;\n+\n+      private ElementIterator(ArrayData list) {\n+        this.list = list;\n+        size = list.size();\n+        index = 0;\n+      }\n+\n+      @Override\n+      public boolean hasNext() {\n+        return index != size;\n+      }\n+\n+      @Override\n+      @SuppressWarnings(\"unchecked\")\n+      public E next() {\n+        if (index >= size) {\n+          throw new NoSuchElementException();\n+        }\n+\n+        E element;\n+        if (list.isNullAt(index)) {\n+          element = null;\n+        } else {\n+          element = (E) ArrayData.createElementGetter(elementType).getElementOrNull(list, index);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjAwNTAxMw=="}, "originalCommit": {"oid": "caa4ca34dc48e334bc213c2117221d75fd597e8b"}, "originalPosition": 362}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk3NTkyOTg3OnYy", "diffSide": "RIGHT", "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNVQwMDozODowM1rOHF9Eeg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNVQwMDozODozNVrOHF9F6Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjAwNTQ5OA==", "bodyText": "Same here. The getters for keys and values should be instance fields.", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r476005498", "createdAt": "2020-08-25T00:38:03Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java", "diffHunk": "@@ -19,38 +19,436 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.util.Iterator;\n import java.util.List;\n-import org.apache.flink.types.Row;\n-import org.apache.iceberg.data.parquet.BaseParquetWriter;\n+import java.util.Map;\n+import java.util.NoSuchElementException;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.MapType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.RowType.RowField;\n+import org.apache.flink.table.types.logical.SmallIntType;\n+import org.apache.flink.table.types.logical.TinyIntType;\n+import org.apache.iceberg.parquet.ParquetValueReaders;\n import org.apache.iceberg.parquet.ParquetValueWriter;\n import org.apache.iceberg.parquet.ParquetValueWriters;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.DecimalUtil;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetWriters extends BaseParquetWriter<Row> {\n+public class FlinkParquetWriters {\n+  private FlinkParquetWriters() {\n+  }\n \n-  private static final FlinkParquetWriters INSTANCE = new FlinkParquetWriters();\n+  @SuppressWarnings(\"unchecked\")\n+  public static <T> ParquetValueWriter<T> buildWriter(LogicalType schema, MessageType type) {\n+    return (ParquetValueWriter<T>) ParquetWithFlinkSchemaVisitor.visit(schema, type, new WriteBuilder(type));\n+  }\n \n-  private FlinkParquetWriters() {\n+  private static class WriteBuilder extends ParquetWithFlinkSchemaVisitor<ParquetValueWriter<?>> {\n+    private final MessageType type;\n+\n+    WriteBuilder(MessageType type) {\n+      this.type = type;\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> message(RowType sStruct, MessageType message, List<ParquetValueWriter<?>> fields) {\n+      return struct(sStruct, message.asGroupType(), fields);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> struct(RowType sStruct, GroupType struct,\n+                                        List<ParquetValueWriter<?>> fieldWriters) {\n+      List<Type> fields = struct.getFields();\n+      List<RowField> flinkFields = sStruct.getFields();\n+      List<ParquetValueWriter<?>> writers = Lists.newArrayListWithExpectedSize(fieldWriters.size());\n+      List<LogicalType> flinkTypes = Lists.newArrayList();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        writers.add(newOption(struct.getType(i), fieldWriters.get(i)));\n+        flinkTypes.add(flinkFields.get(i).getType());\n+      }\n+\n+      return new RowDataWriter(writers, flinkTypes);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> list(ArrayType sArray, GroupType array, ParquetValueWriter<?> elementWriter) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath);\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath);\n+\n+      return new ArrayDataWriter<>(repeatedD, repeatedR,\n+          newOption(repeated.getType(0), elementWriter),\n+          sArray.getElementType());\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> map(MapType sMap, GroupType map,\n+                                     ParquetValueWriter<?> keyWriter, ParquetValueWriter<?> valueWriter) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath);\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath);\n+\n+      return new MapDataWriter<>(repeatedD, repeatedR,\n+          newOption(repeatedKeyValue.getType(0), keyWriter),\n+          newOption(repeatedKeyValue.getType(1), valueWriter),\n+          sMap.getKeyType(), sMap.getValueType());\n+    }\n+\n+\n+    private ParquetValueWriter<?> newOption(org.apache.parquet.schema.Type fieldType, ParquetValueWriter<?> writer) {\n+      int maxD = type.getMaxDefinitionLevel(path(fieldType.getName()));\n+      return ParquetValueWriters.option(fieldType, maxD, writer);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> primitive(LogicalType sType, PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return strings(desc);\n+          case DATE:\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+            return ints(sType, desc);\n+          case INT_64:\n+            return ParquetValueWriters.longs(desc);\n+          case TIME_MICROS:\n+            return timeMicros(desc);\n+          case TIMESTAMP_MICROS:\n+            return timestamps(desc);\n+          case DECIMAL:\n+            DecimalLogicalTypeAnnotation decimal = (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case INT32:\n+                return decimalAsInteger(desc, decimal.getPrecision(), decimal.getScale());\n+              case INT64:\n+                return decimalAsLong(desc, decimal.getPrecision(), decimal.getScale());\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return decimalAsFixed(desc, decimal.getPrecision(), decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          case BSON:\n+            return byteArrays(desc);\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      }\n+\n+      switch (primitive.getPrimitiveTypeName()) {\n+        case FIXED_LEN_BYTE_ARRAY:\n+        case BINARY:\n+          return byteArrays(desc);\n+        case BOOLEAN:\n+          return ParquetValueWriters.booleans(desc);\n+        case INT32:\n+          return ints(sType, desc);\n+        case INT64:\n+          return ParquetValueWriters.longs(desc);\n+        case FLOAT:\n+          return ParquetValueWriters.floats(desc);\n+        case DOUBLE:\n+          return ParquetValueWriters.doubles(desc);\n+        default:\n+          throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+      }\n+    }\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<?> ints(LogicalType type, ColumnDescriptor desc) {\n+    if (type instanceof TinyIntType) {\n+      return ParquetValueWriters.tinyints(desc);\n+    } else if (type instanceof SmallIntType) {\n+      return ParquetValueWriters.shorts(desc);\n+    }\n+    return ParquetValueWriters.ints(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<StringData> strings(ColumnDescriptor desc) {\n+    return new StringDataWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<Integer> timeMicros(ColumnDescriptor desc) {\n+    return new TimeMicrosWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsInteger(ColumnDescriptor desc,\n+                                                                                   int precision, int scale) {\n+    return new IntegerDecimalWriter(desc, precision, scale);\n+  }\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsLong(ColumnDescriptor desc,\n+                                                                                int precision, int scale) {\n+    return new LongDecimalWriter(desc, precision, scale);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsFixed(ColumnDescriptor desc,\n+                                                                                 int precision, int scale) {\n+    return new FixedDecimalWriter(desc, precision, scale);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<TimestampData> timestamps(ColumnDescriptor desc) {\n+    return new TimestampDataWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<byte[]> byteArrays(ColumnDescriptor desc) {\n+    return new ByteArrayWriter(desc);\n+  }\n+\n+  private static class StringDataWriter extends ParquetValueWriters.PrimitiveWriter<StringData> {\n+    private StringDataWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, StringData value) {\n+      column.writeBinary(repetitionLevel, Binary.fromReusedByteArray(value.toBytes()));\n+    }\n+  }\n+\n+  private static class TimeMicrosWriter extends ParquetValueWriters.PrimitiveWriter<Integer> {\n+    private TimeMicrosWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, Integer value) {\n+      long micros = Long.valueOf(value) * 1000;\n+      column.writeLong(repetitionLevel, micros);\n+    }\n+  }\n+\n+  private static class IntegerDecimalWriter extends ParquetValueWriters.PrimitiveWriter<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    private IntegerDecimalWriter(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, DecimalData decimal) {\n+      Preconditions.checkArgument(decimal.scale() == scale,\n+          \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, decimal);\n+      Preconditions.checkArgument(decimal.precision() <= precision,\n+          \"Cannot write value as decimal(%s,%s), too large: %s\", precision, scale, decimal);\n+\n+      column.writeInteger(repetitionLevel, (int) decimal.toUnscaledLong());\n+    }\n+  }\n+\n+  private static class LongDecimalWriter extends ParquetValueWriters.PrimitiveWriter<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    private LongDecimalWriter(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, DecimalData decimal) {\n+      Preconditions.checkArgument(decimal.scale() == scale,\n+          \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, decimal);\n+      Preconditions.checkArgument(decimal.precision() <= precision,\n+          \"Cannot write value as decimal(%s,%s), too large: %s\", precision, scale, decimal);\n+\n+      column.writeLong(repetitionLevel, decimal.toUnscaledLong());\n+    }\n+  }\n+\n+  private static class FixedDecimalWriter extends ParquetValueWriters.PrimitiveWriter<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+    private final ThreadLocal<byte[]> bytes;\n+\n+    private FixedDecimalWriter(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+      this.bytes = ThreadLocal.withInitial(() -> new byte[TypeUtil.decimalRequiredBytes(precision)]);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, DecimalData decimal) {\n+      byte[] binary = DecimalUtil.toReusedFixLengthBytes(precision, scale, decimal.toBigDecimal(), bytes.get());\n+      column.writeBinary(repetitionLevel, Binary.fromReusedByteArray(binary));\n+    }\n+  }\n+\n+  private static class TimestampDataWriter extends ParquetValueWriters.PrimitiveWriter<TimestampData> {\n+    private TimestampDataWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, TimestampData value) {\n+      column.writeLong(repetitionLevel, value.getMillisecond() * 1000 + value.getNanoOfMillisecond() / 1000);\n+    }\n+  }\n+\n+  private static class ByteArrayWriter extends ParquetValueWriters.PrimitiveWriter<byte[]> {\n+    private ByteArrayWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, byte[] bytes) {\n+      column.writeBinary(repetitionLevel, Binary.fromReusedByteArray(bytes));\n+    }\n   }\n \n-  public static ParquetValueWriter<Row> buildWriter(MessageType type) {\n-    return INSTANCE.createWriter(type);\n+  private static class ArrayDataWriter<E> extends ParquetValueWriters.RepeatedWriter<ArrayData, E> {\n+    private final LogicalType elementType;\n+\n+    private ArrayDataWriter(int definitionLevel, int repetitionLevel,\n+                            ParquetValueWriter<E> writer, LogicalType elementType) {\n+      super(definitionLevel, repetitionLevel, writer);\n+      this.elementType = elementType;\n+    }\n+\n+    @Override\n+    protected Iterator<E> elements(ArrayData list) {\n+      return new ElementIterator<>(list);\n+    }\n+\n+    private class ElementIterator<E> implements Iterator<E> {\n+      private final int size;\n+      private final ArrayData list;\n+      private int index;\n+\n+      private ElementIterator(ArrayData list) {\n+        this.list = list;\n+        size = list.size();\n+        index = 0;\n+      }\n+\n+      @Override\n+      public boolean hasNext() {\n+        return index != size;\n+      }\n+\n+      @Override\n+      @SuppressWarnings(\"unchecked\")\n+      public E next() {\n+        if (index >= size) {\n+          throw new NoSuchElementException();\n+        }\n+\n+        E element;\n+        if (list.isNullAt(index)) {\n+          element = null;\n+        } else {\n+          element = (E) ArrayData.createElementGetter(elementType).getElementOrNull(list, index);\n+        }\n+\n+        index += 1;\n+\n+        return element;\n+      }\n+    }\n   }\n \n-  @Override\n-  protected ParquetValueWriters.StructWriter<Row> createStructWriter(List<ParquetValueWriter<?>> writers) {\n-    return new RowWriter(writers);\n+  private static class MapDataWriter<K, V> extends ParquetValueWriters.RepeatedKeyValueWriter<MapData, K, V> {\n+    private final LogicalType keyType;\n+    private final LogicalType valueType;\n+\n+    private MapDataWriter(int definitionLevel, int repetitionLevel,\n+                          ParquetValueWriter<K> keyWriter, ParquetValueWriter<V> valueWriter,\n+                          LogicalType keyType, LogicalType valueType) {\n+      super(definitionLevel, repetitionLevel, keyWriter, valueWriter);\n+      this.keyType = keyType;\n+      this.valueType = valueType;\n+    }\n+\n+    @Override\n+    protected Iterator<Map.Entry<K, V>> pairs(MapData map) {\n+      return new EntryIterator<>(map);\n+    }\n+\n+    private class EntryIterator<K, V> implements Iterator<Map.Entry<K, V>> {\n+      private final int size;\n+      private final ArrayData keys;\n+      private final ArrayData values;\n+      private final ParquetValueReaders.ReusableEntry<K, V> entry;\n+      private int index;\n+\n+      private EntryIterator(MapData map) {\n+        size = map.size();\n+        keys = map.keyArray();\n+        values = map.valueArray();\n+        entry = new ParquetValueReaders.ReusableEntry<>();\n+        index = 0;\n+      }\n+\n+      @Override\n+      public boolean hasNext() {\n+        return index != size;\n+      }\n+\n+      @Override\n+      @SuppressWarnings(\"unchecked\")\n+      public Map.Entry<K, V> next() {\n+        if (index >= size) {\n+          throw new NoSuchElementException();\n+        }\n+\n+        if (values.isNullAt(index)) {\n+          entry.set((K) ArrayData.createElementGetter(keyType).getElementOrNull(keys, index), null);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "caa4ca34dc48e334bc213c2117221d75fd597e8b"}, "originalPosition": 420}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjAwNTg2NQ==", "bodyText": "Keys are not allowed to be null, so there should be no need to call getElementOrNull for the key.", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r476005865", "createdAt": "2020-08-25T00:38:35Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java", "diffHunk": "@@ -19,38 +19,436 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.util.Iterator;\n import java.util.List;\n-import org.apache.flink.types.Row;\n-import org.apache.iceberg.data.parquet.BaseParquetWriter;\n+import java.util.Map;\n+import java.util.NoSuchElementException;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.MapType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.RowType.RowField;\n+import org.apache.flink.table.types.logical.SmallIntType;\n+import org.apache.flink.table.types.logical.TinyIntType;\n+import org.apache.iceberg.parquet.ParquetValueReaders;\n import org.apache.iceberg.parquet.ParquetValueWriter;\n import org.apache.iceberg.parquet.ParquetValueWriters;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.DecimalUtil;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetWriters extends BaseParquetWriter<Row> {\n+public class FlinkParquetWriters {\n+  private FlinkParquetWriters() {\n+  }\n \n-  private static final FlinkParquetWriters INSTANCE = new FlinkParquetWriters();\n+  @SuppressWarnings(\"unchecked\")\n+  public static <T> ParquetValueWriter<T> buildWriter(LogicalType schema, MessageType type) {\n+    return (ParquetValueWriter<T>) ParquetWithFlinkSchemaVisitor.visit(schema, type, new WriteBuilder(type));\n+  }\n \n-  private FlinkParquetWriters() {\n+  private static class WriteBuilder extends ParquetWithFlinkSchemaVisitor<ParquetValueWriter<?>> {\n+    private final MessageType type;\n+\n+    WriteBuilder(MessageType type) {\n+      this.type = type;\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> message(RowType sStruct, MessageType message, List<ParquetValueWriter<?>> fields) {\n+      return struct(sStruct, message.asGroupType(), fields);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> struct(RowType sStruct, GroupType struct,\n+                                        List<ParquetValueWriter<?>> fieldWriters) {\n+      List<Type> fields = struct.getFields();\n+      List<RowField> flinkFields = sStruct.getFields();\n+      List<ParquetValueWriter<?>> writers = Lists.newArrayListWithExpectedSize(fieldWriters.size());\n+      List<LogicalType> flinkTypes = Lists.newArrayList();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        writers.add(newOption(struct.getType(i), fieldWriters.get(i)));\n+        flinkTypes.add(flinkFields.get(i).getType());\n+      }\n+\n+      return new RowDataWriter(writers, flinkTypes);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> list(ArrayType sArray, GroupType array, ParquetValueWriter<?> elementWriter) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath);\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath);\n+\n+      return new ArrayDataWriter<>(repeatedD, repeatedR,\n+          newOption(repeated.getType(0), elementWriter),\n+          sArray.getElementType());\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> map(MapType sMap, GroupType map,\n+                                     ParquetValueWriter<?> keyWriter, ParquetValueWriter<?> valueWriter) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath);\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath);\n+\n+      return new MapDataWriter<>(repeatedD, repeatedR,\n+          newOption(repeatedKeyValue.getType(0), keyWriter),\n+          newOption(repeatedKeyValue.getType(1), valueWriter),\n+          sMap.getKeyType(), sMap.getValueType());\n+    }\n+\n+\n+    private ParquetValueWriter<?> newOption(org.apache.parquet.schema.Type fieldType, ParquetValueWriter<?> writer) {\n+      int maxD = type.getMaxDefinitionLevel(path(fieldType.getName()));\n+      return ParquetValueWriters.option(fieldType, maxD, writer);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> primitive(LogicalType sType, PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return strings(desc);\n+          case DATE:\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+            return ints(sType, desc);\n+          case INT_64:\n+            return ParquetValueWriters.longs(desc);\n+          case TIME_MICROS:\n+            return timeMicros(desc);\n+          case TIMESTAMP_MICROS:\n+            return timestamps(desc);\n+          case DECIMAL:\n+            DecimalLogicalTypeAnnotation decimal = (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case INT32:\n+                return decimalAsInteger(desc, decimal.getPrecision(), decimal.getScale());\n+              case INT64:\n+                return decimalAsLong(desc, decimal.getPrecision(), decimal.getScale());\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return decimalAsFixed(desc, decimal.getPrecision(), decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          case BSON:\n+            return byteArrays(desc);\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      }\n+\n+      switch (primitive.getPrimitiveTypeName()) {\n+        case FIXED_LEN_BYTE_ARRAY:\n+        case BINARY:\n+          return byteArrays(desc);\n+        case BOOLEAN:\n+          return ParquetValueWriters.booleans(desc);\n+        case INT32:\n+          return ints(sType, desc);\n+        case INT64:\n+          return ParquetValueWriters.longs(desc);\n+        case FLOAT:\n+          return ParquetValueWriters.floats(desc);\n+        case DOUBLE:\n+          return ParquetValueWriters.doubles(desc);\n+        default:\n+          throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+      }\n+    }\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<?> ints(LogicalType type, ColumnDescriptor desc) {\n+    if (type instanceof TinyIntType) {\n+      return ParquetValueWriters.tinyints(desc);\n+    } else if (type instanceof SmallIntType) {\n+      return ParquetValueWriters.shorts(desc);\n+    }\n+    return ParquetValueWriters.ints(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<StringData> strings(ColumnDescriptor desc) {\n+    return new StringDataWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<Integer> timeMicros(ColumnDescriptor desc) {\n+    return new TimeMicrosWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsInteger(ColumnDescriptor desc,\n+                                                                                   int precision, int scale) {\n+    return new IntegerDecimalWriter(desc, precision, scale);\n+  }\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsLong(ColumnDescriptor desc,\n+                                                                                int precision, int scale) {\n+    return new LongDecimalWriter(desc, precision, scale);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsFixed(ColumnDescriptor desc,\n+                                                                                 int precision, int scale) {\n+    return new FixedDecimalWriter(desc, precision, scale);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<TimestampData> timestamps(ColumnDescriptor desc) {\n+    return new TimestampDataWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<byte[]> byteArrays(ColumnDescriptor desc) {\n+    return new ByteArrayWriter(desc);\n+  }\n+\n+  private static class StringDataWriter extends ParquetValueWriters.PrimitiveWriter<StringData> {\n+    private StringDataWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, StringData value) {\n+      column.writeBinary(repetitionLevel, Binary.fromReusedByteArray(value.toBytes()));\n+    }\n+  }\n+\n+  private static class TimeMicrosWriter extends ParquetValueWriters.PrimitiveWriter<Integer> {\n+    private TimeMicrosWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, Integer value) {\n+      long micros = Long.valueOf(value) * 1000;\n+      column.writeLong(repetitionLevel, micros);\n+    }\n+  }\n+\n+  private static class IntegerDecimalWriter extends ParquetValueWriters.PrimitiveWriter<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    private IntegerDecimalWriter(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, DecimalData decimal) {\n+      Preconditions.checkArgument(decimal.scale() == scale,\n+          \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, decimal);\n+      Preconditions.checkArgument(decimal.precision() <= precision,\n+          \"Cannot write value as decimal(%s,%s), too large: %s\", precision, scale, decimal);\n+\n+      column.writeInteger(repetitionLevel, (int) decimal.toUnscaledLong());\n+    }\n+  }\n+\n+  private static class LongDecimalWriter extends ParquetValueWriters.PrimitiveWriter<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    private LongDecimalWriter(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, DecimalData decimal) {\n+      Preconditions.checkArgument(decimal.scale() == scale,\n+          \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, decimal);\n+      Preconditions.checkArgument(decimal.precision() <= precision,\n+          \"Cannot write value as decimal(%s,%s), too large: %s\", precision, scale, decimal);\n+\n+      column.writeLong(repetitionLevel, decimal.toUnscaledLong());\n+    }\n+  }\n+\n+  private static class FixedDecimalWriter extends ParquetValueWriters.PrimitiveWriter<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+    private final ThreadLocal<byte[]> bytes;\n+\n+    private FixedDecimalWriter(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+      this.bytes = ThreadLocal.withInitial(() -> new byte[TypeUtil.decimalRequiredBytes(precision)]);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, DecimalData decimal) {\n+      byte[] binary = DecimalUtil.toReusedFixLengthBytes(precision, scale, decimal.toBigDecimal(), bytes.get());\n+      column.writeBinary(repetitionLevel, Binary.fromReusedByteArray(binary));\n+    }\n+  }\n+\n+  private static class TimestampDataWriter extends ParquetValueWriters.PrimitiveWriter<TimestampData> {\n+    private TimestampDataWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, TimestampData value) {\n+      column.writeLong(repetitionLevel, value.getMillisecond() * 1000 + value.getNanoOfMillisecond() / 1000);\n+    }\n+  }\n+\n+  private static class ByteArrayWriter extends ParquetValueWriters.PrimitiveWriter<byte[]> {\n+    private ByteArrayWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, byte[] bytes) {\n+      column.writeBinary(repetitionLevel, Binary.fromReusedByteArray(bytes));\n+    }\n   }\n \n-  public static ParquetValueWriter<Row> buildWriter(MessageType type) {\n-    return INSTANCE.createWriter(type);\n+  private static class ArrayDataWriter<E> extends ParquetValueWriters.RepeatedWriter<ArrayData, E> {\n+    private final LogicalType elementType;\n+\n+    private ArrayDataWriter(int definitionLevel, int repetitionLevel,\n+                            ParquetValueWriter<E> writer, LogicalType elementType) {\n+      super(definitionLevel, repetitionLevel, writer);\n+      this.elementType = elementType;\n+    }\n+\n+    @Override\n+    protected Iterator<E> elements(ArrayData list) {\n+      return new ElementIterator<>(list);\n+    }\n+\n+    private class ElementIterator<E> implements Iterator<E> {\n+      private final int size;\n+      private final ArrayData list;\n+      private int index;\n+\n+      private ElementIterator(ArrayData list) {\n+        this.list = list;\n+        size = list.size();\n+        index = 0;\n+      }\n+\n+      @Override\n+      public boolean hasNext() {\n+        return index != size;\n+      }\n+\n+      @Override\n+      @SuppressWarnings(\"unchecked\")\n+      public E next() {\n+        if (index >= size) {\n+          throw new NoSuchElementException();\n+        }\n+\n+        E element;\n+        if (list.isNullAt(index)) {\n+          element = null;\n+        } else {\n+          element = (E) ArrayData.createElementGetter(elementType).getElementOrNull(list, index);\n+        }\n+\n+        index += 1;\n+\n+        return element;\n+      }\n+    }\n   }\n \n-  @Override\n-  protected ParquetValueWriters.StructWriter<Row> createStructWriter(List<ParquetValueWriter<?>> writers) {\n-    return new RowWriter(writers);\n+  private static class MapDataWriter<K, V> extends ParquetValueWriters.RepeatedKeyValueWriter<MapData, K, V> {\n+    private final LogicalType keyType;\n+    private final LogicalType valueType;\n+\n+    private MapDataWriter(int definitionLevel, int repetitionLevel,\n+                          ParquetValueWriter<K> keyWriter, ParquetValueWriter<V> valueWriter,\n+                          LogicalType keyType, LogicalType valueType) {\n+      super(definitionLevel, repetitionLevel, keyWriter, valueWriter);\n+      this.keyType = keyType;\n+      this.valueType = valueType;\n+    }\n+\n+    @Override\n+    protected Iterator<Map.Entry<K, V>> pairs(MapData map) {\n+      return new EntryIterator<>(map);\n+    }\n+\n+    private class EntryIterator<K, V> implements Iterator<Map.Entry<K, V>> {\n+      private final int size;\n+      private final ArrayData keys;\n+      private final ArrayData values;\n+      private final ParquetValueReaders.ReusableEntry<K, V> entry;\n+      private int index;\n+\n+      private EntryIterator(MapData map) {\n+        size = map.size();\n+        keys = map.keyArray();\n+        values = map.valueArray();\n+        entry = new ParquetValueReaders.ReusableEntry<>();\n+        index = 0;\n+      }\n+\n+      @Override\n+      public boolean hasNext() {\n+        return index != size;\n+      }\n+\n+      @Override\n+      @SuppressWarnings(\"unchecked\")\n+      public Map.Entry<K, V> next() {\n+        if (index >= size) {\n+          throw new NoSuchElementException();\n+        }\n+\n+        if (values.isNullAt(index)) {\n+          entry.set((K) ArrayData.createElementGetter(keyType).getElementOrNull(keys, index), null);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjAwNTQ5OA=="}, "originalCommit": {"oid": "caa4ca34dc48e334bc213c2117221d75fd597e8b"}, "originalPosition": 420}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk3NTkzMzY4OnYy", "diffSide": "RIGHT", "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNVQwMDozODo1N1rOHF9HDw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNVQwMDozODo1N1rOHF9HDw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjAwNjE1OQ==", "bodyText": "Each getter should be stored as a field in an array.", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r476006159", "createdAt": "2020-08-25T00:38:57Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java", "diffHunk": "@@ -19,38 +19,436 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.util.Iterator;\n import java.util.List;\n-import org.apache.flink.types.Row;\n-import org.apache.iceberg.data.parquet.BaseParquetWriter;\n+import java.util.Map;\n+import java.util.NoSuchElementException;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.MapType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.RowType.RowField;\n+import org.apache.flink.table.types.logical.SmallIntType;\n+import org.apache.flink.table.types.logical.TinyIntType;\n+import org.apache.iceberg.parquet.ParquetValueReaders;\n import org.apache.iceberg.parquet.ParquetValueWriter;\n import org.apache.iceberg.parquet.ParquetValueWriters;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.DecimalUtil;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetWriters extends BaseParquetWriter<Row> {\n+public class FlinkParquetWriters {\n+  private FlinkParquetWriters() {\n+  }\n \n-  private static final FlinkParquetWriters INSTANCE = new FlinkParquetWriters();\n+  @SuppressWarnings(\"unchecked\")\n+  public static <T> ParquetValueWriter<T> buildWriter(LogicalType schema, MessageType type) {\n+    return (ParquetValueWriter<T>) ParquetWithFlinkSchemaVisitor.visit(schema, type, new WriteBuilder(type));\n+  }\n \n-  private FlinkParquetWriters() {\n+  private static class WriteBuilder extends ParquetWithFlinkSchemaVisitor<ParquetValueWriter<?>> {\n+    private final MessageType type;\n+\n+    WriteBuilder(MessageType type) {\n+      this.type = type;\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> message(RowType sStruct, MessageType message, List<ParquetValueWriter<?>> fields) {\n+      return struct(sStruct, message.asGroupType(), fields);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> struct(RowType sStruct, GroupType struct,\n+                                        List<ParquetValueWriter<?>> fieldWriters) {\n+      List<Type> fields = struct.getFields();\n+      List<RowField> flinkFields = sStruct.getFields();\n+      List<ParquetValueWriter<?>> writers = Lists.newArrayListWithExpectedSize(fieldWriters.size());\n+      List<LogicalType> flinkTypes = Lists.newArrayList();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        writers.add(newOption(struct.getType(i), fieldWriters.get(i)));\n+        flinkTypes.add(flinkFields.get(i).getType());\n+      }\n+\n+      return new RowDataWriter(writers, flinkTypes);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> list(ArrayType sArray, GroupType array, ParquetValueWriter<?> elementWriter) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath);\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath);\n+\n+      return new ArrayDataWriter<>(repeatedD, repeatedR,\n+          newOption(repeated.getType(0), elementWriter),\n+          sArray.getElementType());\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> map(MapType sMap, GroupType map,\n+                                     ParquetValueWriter<?> keyWriter, ParquetValueWriter<?> valueWriter) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath);\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath);\n+\n+      return new MapDataWriter<>(repeatedD, repeatedR,\n+          newOption(repeatedKeyValue.getType(0), keyWriter),\n+          newOption(repeatedKeyValue.getType(1), valueWriter),\n+          sMap.getKeyType(), sMap.getValueType());\n+    }\n+\n+\n+    private ParquetValueWriter<?> newOption(org.apache.parquet.schema.Type fieldType, ParquetValueWriter<?> writer) {\n+      int maxD = type.getMaxDefinitionLevel(path(fieldType.getName()));\n+      return ParquetValueWriters.option(fieldType, maxD, writer);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> primitive(LogicalType sType, PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return strings(desc);\n+          case DATE:\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+            return ints(sType, desc);\n+          case INT_64:\n+            return ParquetValueWriters.longs(desc);\n+          case TIME_MICROS:\n+            return timeMicros(desc);\n+          case TIMESTAMP_MICROS:\n+            return timestamps(desc);\n+          case DECIMAL:\n+            DecimalLogicalTypeAnnotation decimal = (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case INT32:\n+                return decimalAsInteger(desc, decimal.getPrecision(), decimal.getScale());\n+              case INT64:\n+                return decimalAsLong(desc, decimal.getPrecision(), decimal.getScale());\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return decimalAsFixed(desc, decimal.getPrecision(), decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          case BSON:\n+            return byteArrays(desc);\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      }\n+\n+      switch (primitive.getPrimitiveTypeName()) {\n+        case FIXED_LEN_BYTE_ARRAY:\n+        case BINARY:\n+          return byteArrays(desc);\n+        case BOOLEAN:\n+          return ParquetValueWriters.booleans(desc);\n+        case INT32:\n+          return ints(sType, desc);\n+        case INT64:\n+          return ParquetValueWriters.longs(desc);\n+        case FLOAT:\n+          return ParquetValueWriters.floats(desc);\n+        case DOUBLE:\n+          return ParquetValueWriters.doubles(desc);\n+        default:\n+          throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+      }\n+    }\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<?> ints(LogicalType type, ColumnDescriptor desc) {\n+    if (type instanceof TinyIntType) {\n+      return ParquetValueWriters.tinyints(desc);\n+    } else if (type instanceof SmallIntType) {\n+      return ParquetValueWriters.shorts(desc);\n+    }\n+    return ParquetValueWriters.ints(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<StringData> strings(ColumnDescriptor desc) {\n+    return new StringDataWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<Integer> timeMicros(ColumnDescriptor desc) {\n+    return new TimeMicrosWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsInteger(ColumnDescriptor desc,\n+                                                                                   int precision, int scale) {\n+    return new IntegerDecimalWriter(desc, precision, scale);\n+  }\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsLong(ColumnDescriptor desc,\n+                                                                                int precision, int scale) {\n+    return new LongDecimalWriter(desc, precision, scale);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsFixed(ColumnDescriptor desc,\n+                                                                                 int precision, int scale) {\n+    return new FixedDecimalWriter(desc, precision, scale);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<TimestampData> timestamps(ColumnDescriptor desc) {\n+    return new TimestampDataWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<byte[]> byteArrays(ColumnDescriptor desc) {\n+    return new ByteArrayWriter(desc);\n+  }\n+\n+  private static class StringDataWriter extends ParquetValueWriters.PrimitiveWriter<StringData> {\n+    private StringDataWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, StringData value) {\n+      column.writeBinary(repetitionLevel, Binary.fromReusedByteArray(value.toBytes()));\n+    }\n+  }\n+\n+  private static class TimeMicrosWriter extends ParquetValueWriters.PrimitiveWriter<Integer> {\n+    private TimeMicrosWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, Integer value) {\n+      long micros = Long.valueOf(value) * 1000;\n+      column.writeLong(repetitionLevel, micros);\n+    }\n+  }\n+\n+  private static class IntegerDecimalWriter extends ParquetValueWriters.PrimitiveWriter<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    private IntegerDecimalWriter(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, DecimalData decimal) {\n+      Preconditions.checkArgument(decimal.scale() == scale,\n+          \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, decimal);\n+      Preconditions.checkArgument(decimal.precision() <= precision,\n+          \"Cannot write value as decimal(%s,%s), too large: %s\", precision, scale, decimal);\n+\n+      column.writeInteger(repetitionLevel, (int) decimal.toUnscaledLong());\n+    }\n+  }\n+\n+  private static class LongDecimalWriter extends ParquetValueWriters.PrimitiveWriter<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    private LongDecimalWriter(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, DecimalData decimal) {\n+      Preconditions.checkArgument(decimal.scale() == scale,\n+          \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, decimal);\n+      Preconditions.checkArgument(decimal.precision() <= precision,\n+          \"Cannot write value as decimal(%s,%s), too large: %s\", precision, scale, decimal);\n+\n+      column.writeLong(repetitionLevel, decimal.toUnscaledLong());\n+    }\n+  }\n+\n+  private static class FixedDecimalWriter extends ParquetValueWriters.PrimitiveWriter<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+    private final ThreadLocal<byte[]> bytes;\n+\n+    private FixedDecimalWriter(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+      this.bytes = ThreadLocal.withInitial(() -> new byte[TypeUtil.decimalRequiredBytes(precision)]);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, DecimalData decimal) {\n+      byte[] binary = DecimalUtil.toReusedFixLengthBytes(precision, scale, decimal.toBigDecimal(), bytes.get());\n+      column.writeBinary(repetitionLevel, Binary.fromReusedByteArray(binary));\n+    }\n+  }\n+\n+  private static class TimestampDataWriter extends ParquetValueWriters.PrimitiveWriter<TimestampData> {\n+    private TimestampDataWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, TimestampData value) {\n+      column.writeLong(repetitionLevel, value.getMillisecond() * 1000 + value.getNanoOfMillisecond() / 1000);\n+    }\n+  }\n+\n+  private static class ByteArrayWriter extends ParquetValueWriters.PrimitiveWriter<byte[]> {\n+    private ByteArrayWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, byte[] bytes) {\n+      column.writeBinary(repetitionLevel, Binary.fromReusedByteArray(bytes));\n+    }\n   }\n \n-  public static ParquetValueWriter<Row> buildWriter(MessageType type) {\n-    return INSTANCE.createWriter(type);\n+  private static class ArrayDataWriter<E> extends ParquetValueWriters.RepeatedWriter<ArrayData, E> {\n+    private final LogicalType elementType;\n+\n+    private ArrayDataWriter(int definitionLevel, int repetitionLevel,\n+                            ParquetValueWriter<E> writer, LogicalType elementType) {\n+      super(definitionLevel, repetitionLevel, writer);\n+      this.elementType = elementType;\n+    }\n+\n+    @Override\n+    protected Iterator<E> elements(ArrayData list) {\n+      return new ElementIterator<>(list);\n+    }\n+\n+    private class ElementIterator<E> implements Iterator<E> {\n+      private final int size;\n+      private final ArrayData list;\n+      private int index;\n+\n+      private ElementIterator(ArrayData list) {\n+        this.list = list;\n+        size = list.size();\n+        index = 0;\n+      }\n+\n+      @Override\n+      public boolean hasNext() {\n+        return index != size;\n+      }\n+\n+      @Override\n+      @SuppressWarnings(\"unchecked\")\n+      public E next() {\n+        if (index >= size) {\n+          throw new NoSuchElementException();\n+        }\n+\n+        E element;\n+        if (list.isNullAt(index)) {\n+          element = null;\n+        } else {\n+          element = (E) ArrayData.createElementGetter(elementType).getElementOrNull(list, index);\n+        }\n+\n+        index += 1;\n+\n+        return element;\n+      }\n+    }\n   }\n \n-  @Override\n-  protected ParquetValueWriters.StructWriter<Row> createStructWriter(List<ParquetValueWriter<?>> writers) {\n-    return new RowWriter(writers);\n+  private static class MapDataWriter<K, V> extends ParquetValueWriters.RepeatedKeyValueWriter<MapData, K, V> {\n+    private final LogicalType keyType;\n+    private final LogicalType valueType;\n+\n+    private MapDataWriter(int definitionLevel, int repetitionLevel,\n+                          ParquetValueWriter<K> keyWriter, ParquetValueWriter<V> valueWriter,\n+                          LogicalType keyType, LogicalType valueType) {\n+      super(definitionLevel, repetitionLevel, keyWriter, valueWriter);\n+      this.keyType = keyType;\n+      this.valueType = valueType;\n+    }\n+\n+    @Override\n+    protected Iterator<Map.Entry<K, V>> pairs(MapData map) {\n+      return new EntryIterator<>(map);\n+    }\n+\n+    private class EntryIterator<K, V> implements Iterator<Map.Entry<K, V>> {\n+      private final int size;\n+      private final ArrayData keys;\n+      private final ArrayData values;\n+      private final ParquetValueReaders.ReusableEntry<K, V> entry;\n+      private int index;\n+\n+      private EntryIterator(MapData map) {\n+        size = map.size();\n+        keys = map.keyArray();\n+        values = map.valueArray();\n+        entry = new ParquetValueReaders.ReusableEntry<>();\n+        index = 0;\n+      }\n+\n+      @Override\n+      public boolean hasNext() {\n+        return index != size;\n+      }\n+\n+      @Override\n+      @SuppressWarnings(\"unchecked\")\n+      public Map.Entry<K, V> next() {\n+        if (index >= size) {\n+          throw new NoSuchElementException();\n+        }\n+\n+        if (values.isNullAt(index)) {\n+          entry.set((K) ArrayData.createElementGetter(keyType).getElementOrNull(keys, index), null);\n+        } else {\n+          entry.set((K) ArrayData.createElementGetter(keyType).getElementOrNull(keys, index),\n+              (V) ArrayData.createElementGetter(valueType).getElementOrNull(values, index));\n+        }\n+\n+        index += 1;\n+\n+        return entry;\n+      }\n+    }\n   }\n \n-  private static class RowWriter extends ParquetValueWriters.StructWriter<Row> {\n+  private static class RowDataWriter extends ParquetValueWriters.StructWriter<RowData> {\n+    private final List<LogicalType> types;\n \n-    private RowWriter(List<ParquetValueWriter<?>> writers) {\n+    RowDataWriter(List<ParquetValueWriter<?>> writers, List<LogicalType> types) {\n       super(writers);\n+      this.types = types;\n     }\n \n     @Override\n-    protected Object get(Row row, int index) {\n-      return row.getField(index);\n+    protected Object get(RowData struct, int index) {\n+      return RowData.createFieldGetter(types.get(index), index).getFieldOrNull(struct);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "caa4ca34dc48e334bc213c2117221d75fd597e8b"}, "originalPosition": 447}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk3NTk0ODQ1OnYy", "diffSide": "RIGHT", "path": "flink/src/test/java/org/apache/iceberg/flink/data/TestHelpers.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNVQwMDo0MjozNlrOHF9Q-A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNVQwMDo0MjozNlrOHF9Q-A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjAwODY5Ng==", "bodyText": "Thanks for fixing these.", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r476008696", "createdAt": "2020-08-25T00:42:36Z", "author": {"login": "rdblue"}, "path": "flink/src/test/java/org/apache/iceberg/flink/data/TestHelpers.java", "diffHunk": "@@ -66,15 +65,12 @@ public static void assertRowData(Types.StructType structType, LogicalType rowTyp\n     for (int i = 0; i < types.size(); i += 1) {\n       Object expected = expectedRecord.get(i);\n       LogicalType logicalType = ((RowType) rowType).getTypeAt(i);\n-\n-      final int fieldPos = i;\n       assertEquals(types.get(i), logicalType, expected,\n-          () -> RowData.createFieldGetter(logicalType, fieldPos).getFieldOrNull(actualRowData));\n+          RowData.createFieldGetter(logicalType, i).getFieldOrNull(actualRowData));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "caa4ca34dc48e334bc213c2117221d75fd597e8b"}, "originalPosition": 16}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3818, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}