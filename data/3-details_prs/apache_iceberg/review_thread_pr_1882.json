{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTMzMzUxNjk2", "number": 1882, "reviewThreads": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QwNzozNjoxN1rOFBov7g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QwNzozNjoxN1rOFBov7g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM3MjYwNTI2OnYy", "diffSide": "RIGHT", "path": "flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkInputFormat.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QwNzozNjoxN1rOIAZaqg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQwMjoxNTo1MVrOIBDtnA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzI4NzMzOA==", "bodyText": "I run this unit test under my host without the fixed null checks, and got this exception:\nTest testBasicProjection[format=parquet](org.apache.iceberg.flink.source.TestFlinkInputFormat) failed with:\njava.lang.NullPointerException\n\tat org.apache.iceberg.flink.data.FlinkParquetReaders$ReadBuilder.primitive(FlinkParquetReaders.java:197)\n\tat org.apache.iceberg.flink.data.FlinkParquetReaders$ReadBuilder.primitive(FlinkParquetReaders.java:73)\n\tat org.apache.iceberg.parquet.TypeWithSchemaVisitor.visit(TypeWithSchemaVisitor.java:52)\n\tat org.apache.iceberg.parquet.TypeWithSchemaVisitor.visitField(TypeWithSchemaVisitor.java:155)\n\tat org.apache.iceberg.parquet.TypeWithSchemaVisitor.visitFields(TypeWithSchemaVisitor.java:169)\n\tat org.apache.iceberg.parquet.TypeWithSchemaVisitor.visit(TypeWithSchemaVisitor.java:47)\n\tat org.apache.iceberg.flink.data.FlinkParquetReaders.buildReader(FlinkParquetReaders.java:68)\n\tat org.apache.iceberg.flink.source.RowDataIterator.lambda$newParquetIterable$1(RowDataIterator.java:126)\n\tat org.apache.iceberg.parquet.ReadConf.<init>(ReadConf.java:118)\n\tat org.apache.iceberg.parquet.ParquetReader.init(ParquetReader.java:66)\n\tat org.apache.iceberg.parquet.ParquetReader.iterator(ParquetReader.java:77)\n\tat org.apache.iceberg.flink.source.RowDataIterator.openTaskIterator(RowDataIterator.java:76)\n\tat org.apache.iceberg.flink.source.DataIterator.updateCurrentIterator(DataIterator.java:102)\n\tat org.apache.iceberg.flink.source.DataIterator.hasNext(DataIterator.java:84)\n\tat org.apache.iceberg.flink.source.FlinkInputFormat.reachedEnd(FlinkInputFormat.java:100)\n\tat org.apache.iceberg.flink.TestHelpers.readRowData(TestHelpers.java:91)\n\tat org.apache.iceberg.flink.TestHelpers.readRows(TestHelpers.java:105)\n\tat org.apache.iceberg.flink.source.TestFlinkInputFormat.runFormat(TestFlinkInputFormat.java:132)\n\tat org.apache.iceberg.flink.source.TestFlinkInputFormat.testBasicProjection(TestFlinkInputFormat.java:120)\n\tat java.lang.Thread.run(Thread.java:748)\nChecked the code,  it currently will build a parquet value reader like the following if only project the id and data field:\nRowDataReader\n         |--> UnboxedReader\n         |--> StringReader\n         |--> MillisToTimestampReader\n\nAlthough the MillisToTimestampReader won't read any real data when projecting id and data only.  Now we will change to fill it with a null reader,  I'm not quite sure whether there's other reason that we fill it with a default non-null reader in the previous design.   Still take a look for more details.\nThanks @chenjunjiedada for providing this corner case, I'm pretty sure we need to fix this( current way or just check the expected refs like the following)\ndiff --git a/flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java b/flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java\nindex 3c9bd5cd..4cb94d2b 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java\n@@ -194,13 +194,13 @@ public class FlinkParquetReaders {\n           case INT_64:\n             return new ParquetValueReaders.UnboxedReader<>(desc);\n           case TIMESTAMP_MICROS:\n-            if (((Types.TimestampType) expected).shouldAdjustToUTC()) {\n+            if (expected != null && ((Types.TimestampType) expected).shouldAdjustToUTC()) {\n               return new MicrosToTimestampTzReader(desc);\n             } else {\n               return new MicrosToTimestampReader(desc);\n             }\n           case TIMESTAMP_MILLIS:\n-            if (((Types.TimestampType) expected).shouldAdjustToUTC()) {\n+            if (expected != null && ((Types.TimestampType) expected).shouldAdjustToUTC()) {\n               return new MillisToTimestampTzReader(desc);\n             } else {\n               return new MillisToTimestampReader(desc);", "url": "https://github.com/apache/iceberg/pull/1882#discussion_r537287338", "createdAt": "2020-12-07T07:36:17Z", "author": {"login": "openinx"}, "path": "flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkInputFormat.java", "diffHunk": "@@ -100,6 +100,33 @@ public void testNestedProjection() throws Exception {\n     assertRows(result, expected);\n   }\n \n+  @Test\n+  public void testBasicProjection() throws IOException {\n+    Schema writeSchema = new Schema(\n+        Types.NestedField.required(0, \"id\", Types.LongType.get()),\n+        Types.NestedField.optional(1, \"data\", Types.StringType.get()),\n+        Types.NestedField.optional(2, \"time\", Types.TimestampType.withZone())\n+    );\n+\n+    Table table = catalog.createTable(TableIdentifier.of(\"default\", \"t\"), writeSchema);\n+\n+    List<Record> writeRecords = RandomGenericData.generate(writeSchema, 2, 0L);\n+    new GenericAppenderHelper(table, fileFormat, TEMPORARY_FOLDER).appendToTable(writeRecords);\n+\n+    TableSchema projectedSchema = TableSchema.builder()\n+        .field(\"id\", DataTypes.BIGINT())\n+        .field(\"data\", DataTypes.STRING())\n+        .build();\n+    List<Row> result = runFormat(FlinkSource.forRowData().tableLoader(loader()).project(projectedSchema).buildFormat());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a38062667532f32f4ca9ec0b6dabf799ae7edf84"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzQ5MDg0MA==", "bodyText": "@rdblue  What do you think about this ?", "url": "https://github.com/apache/iceberg/pull/1882#discussion_r537490840", "createdAt": "2020-12-07T13:05:12Z", "author": {"login": "openinx"}, "path": "flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkInputFormat.java", "diffHunk": "@@ -100,6 +100,33 @@ public void testNestedProjection() throws Exception {\n     assertRows(result, expected);\n   }\n \n+  @Test\n+  public void testBasicProjection() throws IOException {\n+    Schema writeSchema = new Schema(\n+        Types.NestedField.required(0, \"id\", Types.LongType.get()),\n+        Types.NestedField.optional(1, \"data\", Types.StringType.get()),\n+        Types.NestedField.optional(2, \"time\", Types.TimestampType.withZone())\n+    );\n+\n+    Table table = catalog.createTable(TableIdentifier.of(\"default\", \"t\"), writeSchema);\n+\n+    List<Record> writeRecords = RandomGenericData.generate(writeSchema, 2, 0L);\n+    new GenericAppenderHelper(table, fileFormat, TEMPORARY_FOLDER).appendToTable(writeRecords);\n+\n+    TableSchema projectedSchema = TableSchema.builder()\n+        .field(\"id\", DataTypes.BIGINT())\n+        .field(\"data\", DataTypes.STRING())\n+        .build();\n+    List<Row> result = runFormat(FlinkSource.forRowData().tableLoader(loader()).project(projectedSchema).buildFormat());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzI4NzMzOA=="}, "originalCommit": {"oid": "a38062667532f32f4ca9ec0b6dabf799ae7edf84"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzY4NTc3OA==", "bodyText": "I like the fix in this PR. If an expected type is not present, then it is fine to return null and to skip adding it to the field map in the struct method. I'd rather do that than return a different reader. At least that way, we don't create extra readers that won't be used anyway.", "url": "https://github.com/apache/iceberg/pull/1882#discussion_r537685778", "createdAt": "2020-12-07T17:22:45Z", "author": {"login": "rdblue"}, "path": "flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkInputFormat.java", "diffHunk": "@@ -100,6 +100,33 @@ public void testNestedProjection() throws Exception {\n     assertRows(result, expected);\n   }\n \n+  @Test\n+  public void testBasicProjection() throws IOException {\n+    Schema writeSchema = new Schema(\n+        Types.NestedField.required(0, \"id\", Types.LongType.get()),\n+        Types.NestedField.optional(1, \"data\", Types.StringType.get()),\n+        Types.NestedField.optional(2, \"time\", Types.TimestampType.withZone())\n+    );\n+\n+    Table table = catalog.createTable(TableIdentifier.of(\"default\", \"t\"), writeSchema);\n+\n+    List<Record> writeRecords = RandomGenericData.generate(writeSchema, 2, 0L);\n+    new GenericAppenderHelper(table, fileFormat, TEMPORARY_FOLDER).appendToTable(writeRecords);\n+\n+    TableSchema projectedSchema = TableSchema.builder()\n+        .field(\"id\", DataTypes.BIGINT())\n+        .field(\"data\", DataTypes.STRING())\n+        .build();\n+    List<Row> result = runFormat(FlinkSource.forRowData().tableLoader(loader()).project(projectedSchema).buildFormat());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzI4NzMzOA=="}, "originalCommit": {"oid": "a38062667532f32f4ca9ec0b6dabf799ae7edf84"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzk4MDMxNg==", "bodyText": "I'm okay about merging this patch.  One thing I think we may need to do is:  it's better to extend the TestRowProjection  to all file format ( parquet/orc etc),  currently we only test the avro read projections.\nMaybe it similar to the spark unit tests TestSparkReadProjection", "url": "https://github.com/apache/iceberg/pull/1882#discussion_r537980316", "createdAt": "2020-12-08T02:15:51Z", "author": {"login": "openinx"}, "path": "flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkInputFormat.java", "diffHunk": "@@ -100,6 +100,33 @@ public void testNestedProjection() throws Exception {\n     assertRows(result, expected);\n   }\n \n+  @Test\n+  public void testBasicProjection() throws IOException {\n+    Schema writeSchema = new Schema(\n+        Types.NestedField.required(0, \"id\", Types.LongType.get()),\n+        Types.NestedField.optional(1, \"data\", Types.StringType.get()),\n+        Types.NestedField.optional(2, \"time\", Types.TimestampType.withZone())\n+    );\n+\n+    Table table = catalog.createTable(TableIdentifier.of(\"default\", \"t\"), writeSchema);\n+\n+    List<Record> writeRecords = RandomGenericData.generate(writeSchema, 2, 0L);\n+    new GenericAppenderHelper(table, fileFormat, TEMPORARY_FOLDER).appendToTable(writeRecords);\n+\n+    TableSchema projectedSchema = TableSchema.builder()\n+        .field(\"id\", DataTypes.BIGINT())\n+        .field(\"data\", DataTypes.STRING())\n+        .build();\n+    List<Row> result = runFormat(FlinkSource.forRowData().tableLoader(loader()).project(projectedSchema).buildFormat());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzI4NzMzOA=="}, "originalCommit": {"oid": "a38062667532f32f4ca9ec0b6dabf799ae7edf84"}, "originalPosition": 21}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3273, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}