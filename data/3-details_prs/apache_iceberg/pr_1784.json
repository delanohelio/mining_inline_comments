{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTIzNDUyNjE2", "number": 1784, "title": "Fix Resolving of SparkSession Table's Metadata Tables", "bodyText": "Do to an issue within Spark's Resolution rules we cannot acces a table in the session\ncatalog with a multipart identifier. Because we also cannot determine whether the underlying\nIceberg table is Hadoop or Hive based we also cannot know the right method for reading the\ntable. To work around this for now we attempt to first load the table as a HiveTable, if\nthe table isn't found we fall back and attempt to load it as a Hadoop table.", "createdAt": "2020-11-18T20:30:26Z", "url": "https://github.com/apache/iceberg/pull/1784", "merged": true, "mergeCommit": {"oid": "064d8028be0a77b57d66baad7bc7fe3b9f5ebf77"}, "closed": true, "closedAt": "2020-11-20T23:02:07Z", "author": {"login": "RussellSpitzer"}, "timelineItems": {"totalCount": 39, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdd0L1rAH2gAyNTIzNDUyNjE2OjcwM2I2OGNmNmE3Y2MyMGI3ZmVmYjc1ZjdlN2M0ZTdiMThiNjI1NzU=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdefRQSAFqTUzNTgyMzk5Ng==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "703b68cf6a7cc20b7fefb75f7e7c4e7b18b62575", "author": {"user": {"login": "RussellSpitzer", "name": "Russell Spitzer"}}, "url": "https://github.com/apache/iceberg/commit/703b68cf6a7cc20b7fefb75f7e7c4e7b18b62575", "committedDate": "2020-11-18T20:29:02Z", "message": "Fix Resolving of SparkSession Table's Metadata Tables\n\nDo to an issue within Spark's Resolution rules we cannot acces a table in the session\ncatalog with a multipart identifier. Because we also cannot determine whether the underlying\nIceberg table is Hadoop or Hive based we also cannot know the right method for reading the\ntable. To work around this for now we attempt to first load the table as a HiveTable, if\nthe table isn't found we fall back and attempt to load it as a Hadoop table."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMzODUwOTc5", "url": "https://github.com/apache/iceberg/pull/1784#pullrequestreview-533850979", "createdAt": "2020-11-18T20:31:29Z", "commit": {"oid": "703b68cf6a7cc20b7fefb75f7e7c4e7b18b62575"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQyMDozMToyOVrOH2A_Pg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQyMDozMToyOVrOH2A_Pg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQwMTM0Mg==", "bodyText": "One huge drawback to this is we can't actually use any Catalog Options that have been specified, so if we have options in the future than alter the way a Metadata table is read we won't have them here.", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r526401342", "createdAt": "2020-11-18T20:31:29Z", "author": {"login": "RussellSpitzer"}, "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "diffHunk": "@@ -137,7 +138,19 @@\n     }\n     // Try catalog based name based resolution\n     try {\n-      return spark.table(tableName + \".\" + type);\n+      if (tableName.startsWith(\"spark_catalog\")) {\n+        // Do to the design of Spark, we cannot pass multi element namespaces to the session catalog\n+        // We also don't know whether the Catalog is Hive or Hadoop Based, so we will try to load it\n+        // in the hive manner first, then fall back and try the location if we have completely run out of options\n+        // TODO remove this when we have Spark workaround for multipart identifiers in SparkSessionCatalog\n+        try {\n+          return noCatalogReader.load(tableName.replaceFirst(\"spark_catalog\\\\.\", \"\") + \".\" + type);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "703b68cf6a7cc20b7fefb75f7e7c4e7b18b62575"}, "originalPosition": 19}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMzOTEyNzk5", "url": "https://github.com/apache/iceberg/pull/1784#pullrequestreview-533912799", "createdAt": "2020-11-18T21:56:29Z", "commit": {"oid": "703b68cf6a7cc20b7fefb75f7e7c4e7b18b62575"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQyMTo1NjoyOVrOH2D5Zw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQyMTo1NjoyOVrOH2D5Zw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQ0ODk5OQ==", "bodyText": "Would it make sense to call it dataFrameReader instead of noCatalogReader everywhere?", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r526448999", "createdAt": "2020-11-18T21:56:29Z", "author": {"login": "aokolnychyi"}, "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "diffHunk": "@@ -137,7 +138,19 @@\n     }\n     // Try catalog based name based resolution\n     try {\n-      return spark.table(tableName + \".\" + type);\n+      if (tableName.startsWith(\"spark_catalog\")) {\n+        // Do to the design of Spark, we cannot pass multi element namespaces to the session catalog\n+        // We also don't know whether the Catalog is Hive or Hadoop Based, so we will try to load it\n+        // in the hive manner first, then fall back and try the location if we have completely run out of options\n+        // TODO remove this when we have Spark workaround for multipart identifiers in SparkSessionCatalog\n+        try {\n+          return noCatalogReader.load(tableName.replaceFirst(\"spark_catalog\\\\.\", \"\") + \".\" + type);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "703b68cf6a7cc20b7fefb75f7e7c4e7b18b62575"}, "originalPosition": 19}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMzOTEzNzQ0", "url": "https://github.com/apache/iceberg/pull/1784#pullrequestreview-533913744", "createdAt": "2020-11-18T21:57:50Z", "commit": {"oid": "703b68cf6a7cc20b7fefb75f7e7c4e7b18b62575"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQyMTo1Nzo1MFrOH2D8Tw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQyMTo1Nzo1MFrOH2D8Tw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQ0OTc0Mw==", "bodyText": "Do we want to put this logic into a separate method like loadMetadataUsingCatalog or something?", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r526449743", "createdAt": "2020-11-18T21:57:50Z", "author": {"login": "aokolnychyi"}, "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "diffHunk": "@@ -137,7 +138,19 @@\n     }\n     // Try catalog based name based resolution\n     try {\n-      return spark.table(tableName + \".\" + type);\n+      if (tableName.startsWith(\"spark_catalog\")) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "703b68cf6a7cc20b7fefb75f7e7c4e7b18b62575"}, "originalPosition": 13}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMzOTE0ODU0", "url": "https://github.com/apache/iceberg/pull/1784#pullrequestreview-533914854", "createdAt": "2020-11-18T21:59:23Z", "commit": {"oid": "703b68cf6a7cc20b7fefb75f7e7c4e7b18b62575"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQyMTo1OToyM1rOH2D_kg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQyMTo1OToyM1rOH2D_kg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQ1MDU3OA==", "bodyText": "nit: throws Exception?", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r526450578", "createdAt": "2020-11-18T21:59:23Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/test/java/org/apache/iceberg/actions/TestRemoveOrphanFilesAction3.java", "diffHunk": "@@ -110,4 +112,56 @@ public void testSparkCatalogHiveTable() throws TableAlreadyExistsException, NoSu\n         results.contains(\"file:\" + location + \"/data/trashfile\"));\n   }\n \n+  @Test\n+  public void testSparkSessionCatalogHadoopTable()\n+      throws TableAlreadyExistsException, NoSuchTableException, IOException, NoSuchNamespaceException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "703b68cf6a7cc20b7fefb75f7e7c4e7b18b62575"}, "originalPosition": 34}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMzOTE0OTM4", "url": "https://github.com/apache/iceberg/pull/1784#pullrequestreview-533914938", "createdAt": "2020-11-18T21:59:31Z", "commit": {"oid": "703b68cf6a7cc20b7fefb75f7e7c4e7b18b62575"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQyMTo1OTozMVrOH2D_yA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQyMTo1OTozMVrOH2D_yA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQ1MDYzMg==", "bodyText": "nit: same here", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r526450632", "createdAt": "2020-11-18T21:59:31Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/test/java/org/apache/iceberg/actions/TestRemoveOrphanFilesAction3.java", "diffHunk": "@@ -110,4 +112,56 @@ public void testSparkCatalogHiveTable() throws TableAlreadyExistsException, NoSu\n         results.contains(\"file:\" + location + \"/data/trashfile\"));\n   }\n \n+  @Test\n+  public void testSparkSessionCatalogHadoopTable()\n+      throws TableAlreadyExistsException, NoSuchTableException, IOException, NoSuchNamespaceException {\n+    spark.conf().set(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\");\n+    spark.conf().set(\"spark.sql.catalog.spark_catalog.type\", \"hadoop\");\n+    spark.conf().set(\"spark.sql.catalog.spark_catalog.warehouse\", tableLocation);\n+    SparkSessionCatalog cat = (SparkSessionCatalog) spark.sessionState().catalogManager().v2SessionCatalog();\n+\n+    String[] database = {\"default\"};\n+    Identifier id = Identifier.of(database, \"table\");\n+    Map<String, String> options = Maps.newHashMap();\n+    Transform[] transforms = {};\n+    cat.createTable(id, SparkSchemaUtil.convert(SCHEMA), transforms, options);\n+    SparkTable table = (SparkTable) cat.loadTable(id);\n+\n+    spark.sql(\"INSERT INTO default.table VALUES (1,1,1)\");\n+\n+    String location = table.table().location().replaceFirst(\"file:\", \"\");\n+    new File(location + \"/data/trashfile\").createNewFile();\n+\n+    List<String> results = Actions.forTable(table.table()).removeOrphanFiles()\n+        .olderThan(System.currentTimeMillis() + 1000).execute();\n+    Assert.assertTrue(\"trash file should be removed\",\n+        results.contains(\"file:\" + location + \"/data/trashfile\"));\n+  }\n+\n+  @Test\n+  public void testSparkSessionCatalogHiveTable()\n+      throws TableAlreadyExistsException, NoSuchTableException, IOException, NoSuchNamespaceException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "703b68cf6a7cc20b7fefb75f7e7c4e7b18b62575"}, "originalPosition": 60}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6e621037c6d46acb3f96370e3fd752eff8be1ecb", "author": {"user": {"login": "RussellSpitzer", "name": "Russell Spitzer"}}, "url": "https://github.com/apache/iceberg/commit/6e621037c6d46acb3f96370e3fd752eff8be1ecb", "committedDate": "2020-11-18T22:40:58Z", "message": "Reviewer Comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMzOTgwNTEw", "url": "https://github.com/apache/iceberg/pull/1784#pullrequestreview-533980510", "createdAt": "2020-11-19T00:05:54Z", "commit": {"oid": "6e621037c6d46acb3f96370e3fd752eff8be1ecb"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQwMDowNTo1NFrOH2HTEw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQwMDowNTo1NFrOH2HTEw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjUwNDcyMw==", "bodyText": "Nit: typo: \"due\" not \"do\".", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r526504723", "createdAt": "2020-11-19T00:05:54Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "diffHunk": "@@ -128,16 +129,35 @@\n     return manifestDF.union(otherMetadataFileDF).union(manifestListDF);\n   }\n \n+  private static Dataset<Row> loadMetadataTableFromCatalog(SparkSession spark, String tableName, String tableLocation,\n+                                                           MetadataTableType type) {\n+    DataFrameReader dataFrameReader = spark.read().format(\"iceberg\");\n+    if (tableName.startsWith(\"spark_catalog\")) {\n+      // Do to the design of Spark, we cannot pass multi-element namespaces to the session catalog.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6e621037c6d46acb3f96370e3fd752eff8be1ecb"}, "originalPosition": 16}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMzOTgwODU5", "url": "https://github.com/apache/iceberg/pull/1784#pullrequestreview-533980859", "createdAt": "2020-11-19T00:06:45Z", "commit": {"oid": "6e621037c6d46acb3f96370e3fd752eff8be1ecb"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQwMDowNjo0NVrOH2HUOg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQwMDowNjo0NVrOH2HUOg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjUwNTAxOA==", "bodyText": "I think this is a bug in Spark. There isn't a work-around that I know of.", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r526505018", "createdAt": "2020-11-19T00:06:45Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "diffHunk": "@@ -128,16 +129,35 @@\n     return manifestDF.union(otherMetadataFileDF).union(manifestListDF);\n   }\n \n+  private static Dataset<Row> loadMetadataTableFromCatalog(SparkSession spark, String tableName, String tableLocation,\n+                                                           MetadataTableType type) {\n+    DataFrameReader dataFrameReader = spark.read().format(\"iceberg\");\n+    if (tableName.startsWith(\"spark_catalog\")) {\n+      // Do to the design of Spark, we cannot pass multi-element namespaces to the session catalog.\n+      // We also don't know whether the Catalog is Hive or Hadoop Based so we can't just load one way or the other.\n+      // Instead we will try to load the metadata table in the hive manner first, then fall back and try the\n+      // hadoop location method if that fails\n+      // TODO remove this when we have Spark workaround for multipart identifiers in SparkSessionCatalog", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6e621037c6d46acb3f96370e3fd752eff8be1ecb"}, "originalPosition": 20}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMzOTgyMjMz", "url": "https://github.com/apache/iceberg/pull/1784#pullrequestreview-533982233", "createdAt": "2020-11-19T00:10:10Z", "commit": {"oid": "6e621037c6d46acb3f96370e3fd752eff8be1ecb"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQwMDoxMDoxMFrOH2HYww==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQwMDoxMDoxMFrOH2HYww==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjUwNjE3OQ==", "bodyText": "If we know that the catalog is spark_catalog, then we should just try to load without removing the catalog name. If we remove the catalog name, then we don't know that the right table will be loaded because the Spark catalog may not be the session's current catalog.\nAnd, if the metadata table type works then so would using the prefix spark_catalog. Names like spark_catalog.db.table work, it is just spark_catalog.db.table.meta that does not. If meta is added and the current catalog is spark_catalog, then I think it will fail no matter what.", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r526506179", "createdAt": "2020-11-19T00:10:10Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "diffHunk": "@@ -128,16 +129,35 @@\n     return manifestDF.union(otherMetadataFileDF).union(manifestListDF);\n   }\n \n+  private static Dataset<Row> loadMetadataTableFromCatalog(SparkSession spark, String tableName, String tableLocation,\n+                                                           MetadataTableType type) {\n+    DataFrameReader dataFrameReader = spark.read().format(\"iceberg\");\n+    if (tableName.startsWith(\"spark_catalog\")) {\n+      // Do to the design of Spark, we cannot pass multi-element namespaces to the session catalog.\n+      // We also don't know whether the Catalog is Hive or Hadoop Based so we can't just load one way or the other.\n+      // Instead we will try to load the metadata table in the hive manner first, then fall back and try the\n+      // hadoop location method if that fails\n+      // TODO remove this when we have Spark workaround for multipart identifiers in SparkSessionCatalog\n+      try {\n+        return dataFrameReader.load(tableName.replaceFirst(\"spark_catalog\\\\.\", \"\") + \".\" + type);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6e621037c6d46acb3f96370e3fd752eff8be1ecb"}, "originalPosition": 22}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMzOTgyNzkw", "url": "https://github.com/apache/iceberg/pull/1784#pullrequestreview-533982790", "createdAt": "2020-11-19T00:11:30Z", "commit": {"oid": "6e621037c6d46acb3f96370e3fd752eff8be1ecb"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQwMDoxMTozMFrOH2HamA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQwMDoxMTozMFrOH2HamA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjUwNjY0OA==", "bodyText": "Missing return?", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r526506648", "createdAt": "2020-11-19T00:11:30Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "diffHunk": "@@ -128,16 +129,35 @@\n     return manifestDF.union(otherMetadataFileDF).union(manifestListDF);\n   }\n \n+  private static Dataset<Row> loadMetadataTableFromCatalog(SparkSession spark, String tableName, String tableLocation,\n+                                                           MetadataTableType type) {\n+    DataFrameReader dataFrameReader = spark.read().format(\"iceberg\");\n+    if (tableName.startsWith(\"spark_catalog\")) {\n+      // Do to the design of Spark, we cannot pass multi-element namespaces to the session catalog.\n+      // We also don't know whether the Catalog is Hive or Hadoop Based so we can't just load one way or the other.\n+      // Instead we will try to load the metadata table in the hive manner first, then fall back and try the\n+      // hadoop location method if that fails\n+      // TODO remove this when we have Spark workaround for multipart identifiers in SparkSessionCatalog\n+      try {\n+        return dataFrameReader.load(tableName.replaceFirst(\"spark_catalog\\\\.\", \"\") + \".\" + type);\n+      } catch (NoSuchTableException noSuchTableException) {\n+        return dataFrameReader.load(tableLocation + \"#\" + type);\n+      }\n+    } else {\n+      return spark.table(tableName + \".\" + type);\n+    }\n+  }\n+\n   protected static Dataset<Row> loadMetadataTable(SparkSession spark, String tableName, String tableLocation,\n                                                   MetadataTableType type) {\n-    DataFrameReader noCatalogReader = spark.read().format(\"iceberg\");\n+    DataFrameReader dataFrameReader = spark.read().format(\"iceberg\");\n     if (tableName.contains(\"/\")) {\n       // Hadoop Table or Metadata location passed, load without a catalog\n-      return noCatalogReader.load(tableName + \"#\" + type);\n+      return dataFrameReader.load(tableName + \"#\" + type);\n     }\n     // Try catalog based name based resolution\n     try {\n-      return spark.table(tableName + \".\" + type);\n+      loadMetadataTableFromCatalog(spark, tableName, tableLocation, type);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6e621037c6d46acb3f96370e3fd752eff8be1ecb"}, "originalPosition": 43}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMzOTg4OTIy", "url": "https://github.com/apache/iceberg/pull/1784#pullrequestreview-533988922", "createdAt": "2020-11-19T00:27:16Z", "commit": {"oid": "6e621037c6d46acb3f96370e3fd752eff8be1ecb"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQwMDoyNzoxNlrOH2HvOQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQwMDoyNzoxNlrOH2HvOQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjUxMTkyOQ==", "bodyText": "I think this test case only works because HiveCatalogs uses the value of hive.metastore.uris from the environment's hive-site.xml. By removing spark_catalog and then using the DataFrameReader, the Hive catalog from HiveCatalogs is used, which has the same URI.\nI think this is actually the right thing to do, but I would do it more directly and obviously so that it is clear what is happening:\n\nGet the session catalog from the catalog manager\nIf the session catalog is a SparkSessionCatalog, get the underlying Iceberg catalog\nUse the Iceberg catalog to load the metadata table, because it accepts the full table identifier", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r526511929", "createdAt": "2020-11-19T00:27:16Z", "author": {"login": "rdblue"}, "path": "spark3/src/test/java/org/apache/iceberg/actions/TestRemoveOrphanFilesAction3.java", "diffHunk": "@@ -110,4 +112,54 @@ public void testSparkCatalogHiveTable() throws TableAlreadyExistsException, NoSu\n         results.contains(\"file:\" + location + \"/data/trashfile\"));\n   }\n \n+  @Test\n+  public void testSparkSessionCatalogHadoopTable() throws Exception {\n+    spark.conf().set(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\");\n+    spark.conf().set(\"spark.sql.catalog.spark_catalog.type\", \"hadoop\");\n+    spark.conf().set(\"spark.sql.catalog.spark_catalog.warehouse\", tableLocation);\n+    SparkSessionCatalog cat = (SparkSessionCatalog) spark.sessionState().catalogManager().v2SessionCatalog();\n+\n+    String[] database = {\"default\"};\n+    Identifier id = Identifier.of(database, \"table\");\n+    Map<String, String> options = Maps.newHashMap();\n+    Transform[] transforms = {};\n+    cat.createTable(id, SparkSchemaUtil.convert(SCHEMA), transforms, options);\n+    SparkTable table = (SparkTable) cat.loadTable(id);\n+\n+    spark.sql(\"INSERT INTO default.table VALUES (1,1,1)\");\n+\n+    String location = table.table().location().replaceFirst(\"file:\", \"\");\n+    new File(location + \"/data/trashfile\").createNewFile();\n+\n+    List<String> results = Actions.forTable(table.table()).removeOrphanFiles()\n+        .olderThan(System.currentTimeMillis() + 1000).execute();\n+    Assert.assertTrue(\"trash file should be removed\",\n+        results.contains(\"file:\" + location + \"/data/trashfile\"));\n+  }\n+\n+  @Test\n+  public void testSparkSessionCatalogHiveTable() throws Exception {\n+    spark.conf().set(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\");\n+    spark.conf().set(\"spark.sql.catalog.spark_catalog.type\", \"hive\");\n+    SparkSessionCatalog cat = (SparkSessionCatalog) spark.sessionState().catalogManager().v2SessionCatalog();\n+\n+    String[] database = {\"default\"};\n+    Identifier id = Identifier.of(database, \"sessioncattest\");\n+    Map<String, String> options = Maps.newHashMap();\n+    Transform[] transforms = {};\n+    cat.dropTable(id);\n+    cat.createTable(id, SparkSchemaUtil.convert(SCHEMA), transforms, options);\n+    SparkTable table = (SparkTable) cat.loadTable(id);\n+\n+    spark.sql(\"INSERT INTO default.sessioncattest VALUES (1,1,1)\");\n+\n+    String location = table.table().location().replaceFirst(\"file:\", \"\");\n+    new File(location + \"/data/trashfile\").createNewFile();\n+\n+    List<String> results = Actions.forTable(table.table()).removeOrphanFiles()\n+        .olderThan(System.currentTimeMillis() + 1000).execute();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6e621037c6d46acb3f96370e3fd752eff8be1ecb"}, "originalPosition": 86}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c3588beb33bfcca24ed2be0394da7316356631b8", "author": {"user": {"login": "RussellSpitzer", "name": "Russell Spitzer"}}, "url": "https://github.com/apache/iceberg/commit/c3588beb33bfcca24ed2be0394da7316356631b8", "committedDate": "2020-11-19T05:16:25Z", "message": "Few Reviewer Comments"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "a1c4e794ba29e3bc4827072f5df6aec1770d711b", "author": {"user": {"login": "RussellSpitzer", "name": "Russell Spitzer"}}, "url": "https://github.com/apache/iceberg/commit/a1c4e794ba29e3bc4827072f5df6aec1770d711b", "committedDate": "2020-11-19T04:49:40Z", "message": "Few Reviewer Comments"}, "afterCommit": {"oid": "c3588beb33bfcca24ed2be0394da7316356631b8", "author": {"user": {"login": "RussellSpitzer", "name": "Russell Spitzer"}}, "url": "https://github.com/apache/iceberg/commit/c3588beb33bfcca24ed2be0394da7316356631b8", "committedDate": "2020-11-19T05:16:25Z", "message": "Few Reviewer Comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e2b762efb8ed72928fc62e873779bd1abe11bfcb", "author": {"user": {"login": "RussellSpitzer", "name": "Russell Spitzer"}}, "url": "https://github.com/apache/iceberg/commit/e2b762efb8ed72928fc62e873779bd1abe11bfcb", "committedDate": "2020-11-19T20:30:19Z", "message": "Change to use Spark3 Specific pathway for resolution"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM0ODM0NDE1", "url": "https://github.com/apache/iceberg/pull/1784#pullrequestreview-534834415", "createdAt": "2020-11-19T20:42:39Z", "commit": {"oid": "e2b762efb8ed72928fc62e873779bd1abe11bfcb"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQyMDo0Mjo0MFrOH2w1jw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQyMDo0Mjo0MFrOH2w1jw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE4NTI5NQ==", "bodyText": "Everything below here was pulled from the Create PR and is basically a copy of the Catalog And Identifier Resolution methods from Spark.", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r527185295", "createdAt": "2020-11-19T20:42:40Z", "author": {"login": "RussellSpitzer"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java", "diffHunk": "@@ -548,4 +565,83 @@ private static String sqlString(org.apache.iceberg.expressions.Literal<?> lit) {\n       }\n     }\n   }\n+\n+  /*\n+   * Because Spark does not allow more than 1 piece in the namespace for a Session Catalog table, we circumvent\n+   * the entire resolution path for tables and instead look up the table directly ourselves. This lets us correctly\n+   * get metadata tables for the SessionCatalog, if we didn't have to work around this we could just use spark.table.\n+   */\n+  private static Dataset<Row> loadCatalogMetadataTable(SparkSession spark, String name, MetadataTableType type)\n+      throws CatalogNotFoundException, ParseException, NoSuchTableException {\n+\n+    CatalogAndIdentifier catalogAndIdentifier = catalogAndIdentifier(spark, name);\n+    if (catalogAndIdentifier.catalog instanceof BaseCatalog) {\n+      BaseCatalog catalog = (BaseCatalog) catalogAndIdentifier.catalog;\n+      Identifier baseIdent = catalogAndIdentifier.identifier;\n+      Identifier metaIdent = Identifier.of(ArrayUtils.add(baseIdent.namespace(), baseIdent.name()), type.name());\n+      Table metaTable = catalog.loadTable(metaIdent);\n+      return Dataset.ofRows(spark, DataSourceV2Relation.create(metaTable, Some.apply(catalog), Some.apply(metaIdent)));\n+    } else {\n+      throw new CatalogNotFoundException(String.format(\"Cannot cast %s as an Iceberg catalog\",\n+          catalogAndIdentifier.catalog.name()));\n+    }\n+  }\n+\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, String name) throws ParseException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e2b762efb8ed72928fc62e873779bd1abe11bfcb"}, "originalPosition": 66}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM0ODM0OTE2", "url": "https://github.com/apache/iceberg/pull/1784#pullrequestreview-534834916", "createdAt": "2020-11-19T20:43:29Z", "commit": {"oid": "e2b762efb8ed72928fc62e873779bd1abe11bfcb"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQyMDo0MzoyOVrOH2w3EA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQyMDo0MzoyOVrOH2w3EA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE4NTY4MA==", "bodyText": "This is how Spark would have made the relation from our metadata table if it didn't think multiple pieces in the Namespace was a dealbreaker.", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r527185680", "createdAt": "2020-11-19T20:43:29Z", "author": {"login": "RussellSpitzer"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java", "diffHunk": "@@ -548,4 +565,83 @@ private static String sqlString(org.apache.iceberg.expressions.Literal<?> lit) {\n       }\n     }\n   }\n+\n+  /*\n+   * Because Spark does not allow more than 1 piece in the namespace for a Session Catalog table, we circumvent\n+   * the entire resolution path for tables and instead look up the table directly ourselves. This lets us correctly\n+   * get metadata tables for the SessionCatalog, if we didn't have to work around this we could just use spark.table.\n+   */\n+  private static Dataset<Row> loadCatalogMetadataTable(SparkSession spark, String name, MetadataTableType type)\n+      throws CatalogNotFoundException, ParseException, NoSuchTableException {\n+\n+    CatalogAndIdentifier catalogAndIdentifier = catalogAndIdentifier(spark, name);\n+    if (catalogAndIdentifier.catalog instanceof BaseCatalog) {\n+      BaseCatalog catalog = (BaseCatalog) catalogAndIdentifier.catalog;\n+      Identifier baseIdent = catalogAndIdentifier.identifier;\n+      Identifier metaIdent = Identifier.of(ArrayUtils.add(baseIdent.namespace(), baseIdent.name()), type.name());\n+      Table metaTable = catalog.loadTable(metaIdent);\n+      return Dataset.ofRows(spark, DataSourceV2Relation.create(metaTable, Some.apply(catalog), Some.apply(metaIdent)));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e2b762efb8ed72928fc62e873779bd1abe11bfcb"}, "originalPosition": 59}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM0ODU3MjIw", "url": "https://github.com/apache/iceberg/pull/1784#pullrequestreview-534857220", "createdAt": "2020-11-19T21:15:09Z", "commit": {"oid": "e2b762efb8ed72928fc62e873779bd1abe11bfcb"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQyMToxNToxMFrOH2x7RA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQyMToxNToxMFrOH2x7RA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIwMzE0MA==", "bodyText": "How stable do we expect implicits to be?", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r527203140", "createdAt": "2020-11-19T21:15:10Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java", "diffHunk": "@@ -548,4 +565,83 @@ private static String sqlString(org.apache.iceberg.expressions.Literal<?> lit) {\n       }\n     }\n   }\n+\n+  /*\n+   * Because Spark does not allow more than 1 piece in the namespace for a Session Catalog table, we circumvent\n+   * the entire resolution path for tables and instead look up the table directly ourselves. This lets us correctly\n+   * get metadata tables for the SessionCatalog, if we didn't have to work around this we could just use spark.table.\n+   */\n+  private static Dataset<Row> loadCatalogMetadataTable(SparkSession spark, String name, MetadataTableType type)\n+      throws CatalogNotFoundException, ParseException, NoSuchTableException {\n+\n+    CatalogAndIdentifier catalogAndIdentifier = catalogAndIdentifier(spark, name);\n+    if (catalogAndIdentifier.catalog instanceof BaseCatalog) {\n+      BaseCatalog catalog = (BaseCatalog) catalogAndIdentifier.catalog;\n+      Identifier baseIdent = catalogAndIdentifier.identifier;\n+      Identifier metaIdent = Identifier.of(ArrayUtils.add(baseIdent.namespace(), baseIdent.name()), type.name());\n+      Table metaTable = catalog.loadTable(metaIdent);\n+      return Dataset.ofRows(spark, DataSourceV2Relation.create(metaTable, Some.apply(catalog), Some.apply(metaIdent)));\n+    } else {\n+      throw new CatalogNotFoundException(String.format(\"Cannot cast %s as an Iceberg catalog\",\n+          catalogAndIdentifier.catalog.name()));\n+    }\n+  }\n+\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, String name) throws ParseException {\n+    return catalogAndIdentifier(spark,\n+          JavaConverters.seqAsJavaList(spark.sessionState().sqlParser().parseMultipartIdentifier(name)));\n+  }\n+\n+  /**\n+   * A modified version of Spark's LookupCatalog.CatalogAndIdentifier.unapply\n+   * Attempts to find the catalog and identifier a multipart identifier represents\n+   * @param spark Spark session to use for resolution\n+   * @param nameParts Multipart identifier representing a table\n+   * @return The CatalogPlugin and Identifier for the table\n+   */\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, List<String> nameParts) {\n+    Seq<String> namePartsSeq = JavaConverters.asScalaIterator(nameParts.iterator()).toSeq();\n+    Preconditions.checkArgument(namePartsSeq.nonEmpty(),\n+        \"Cannot determine catalog and Identifier from empty name parts\");\n+    CatalogPlugin currentCatalog = spark.sessionState().catalogManager().currentCatalog();\n+    String[] currentNamespace = spark.sessionState().catalogManager().currentNamespace();\n+    if (namePartsSeq.length() == 1) {\n+      return new CatalogAndIdentifier(currentCatalog, Identifier.of(currentNamespace, namePartsSeq.head()));\n+    } else {\n+      try {\n+        CatalogPlugin namedCatalog = spark.sessionState().catalogManager().catalog(namePartsSeq.head());\n+        return new CatalogAndIdentifier(namedCatalog,\n+            CatalogV2Implicits.MultipartIdentifierHelper((Seq<String>) namePartsSeq.tail()).asIdentifier());\n+      } catch (Exception e) {\n+        return new CatalogAndIdentifier(currentCatalog,\n+            CatalogV2Implicits.MultipartIdentifierHelper(namePartsSeq).asIdentifier());\n+      }\n+    }\n+  }\n+\n+  public static TableIdentifier toTableIdentifier(Identifier table) {\n+    return new CatalogV2Implicits.IdentifierHelper(table).asTableIdentifier();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e2b762efb8ed72928fc62e873779bd1abe11bfcb"}, "originalPosition": 99}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM0ODU3NTU5", "url": "https://github.com/apache/iceberg/pull/1784#pullrequestreview-534857559", "createdAt": "2020-11-19T21:15:36Z", "commit": {"oid": "e2b762efb8ed72928fc62e873779bd1abe11bfcb"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQyMToxNTozN1rOH2x8dA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQyMToxNTozN1rOH2x8dA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIwMzQ0NA==", "bodyText": "Can we factor this out into a separate method like toIdentifier?", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r527203444", "createdAt": "2020-11-19T21:15:37Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java", "diffHunk": "@@ -548,4 +565,83 @@ private static String sqlString(org.apache.iceberg.expressions.Literal<?> lit) {\n       }\n     }\n   }\n+\n+  /*\n+   * Because Spark does not allow more than 1 piece in the namespace for a Session Catalog table, we circumvent\n+   * the entire resolution path for tables and instead look up the table directly ourselves. This lets us correctly\n+   * get metadata tables for the SessionCatalog, if we didn't have to work around this we could just use spark.table.\n+   */\n+  private static Dataset<Row> loadCatalogMetadataTable(SparkSession spark, String name, MetadataTableType type)\n+      throws CatalogNotFoundException, ParseException, NoSuchTableException {\n+\n+    CatalogAndIdentifier catalogAndIdentifier = catalogAndIdentifier(spark, name);\n+    if (catalogAndIdentifier.catalog instanceof BaseCatalog) {\n+      BaseCatalog catalog = (BaseCatalog) catalogAndIdentifier.catalog;\n+      Identifier baseIdent = catalogAndIdentifier.identifier;\n+      Identifier metaIdent = Identifier.of(ArrayUtils.add(baseIdent.namespace(), baseIdent.name()), type.name());\n+      Table metaTable = catalog.loadTable(metaIdent);\n+      return Dataset.ofRows(spark, DataSourceV2Relation.create(metaTable, Some.apply(catalog), Some.apply(metaIdent)));\n+    } else {\n+      throw new CatalogNotFoundException(String.format(\"Cannot cast %s as an Iceberg catalog\",\n+          catalogAndIdentifier.catalog.name()));\n+    }\n+  }\n+\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, String name) throws ParseException {\n+    return catalogAndIdentifier(spark,\n+          JavaConverters.seqAsJavaList(spark.sessionState().sqlParser().parseMultipartIdentifier(name)));\n+  }\n+\n+  /**\n+   * A modified version of Spark's LookupCatalog.CatalogAndIdentifier.unapply\n+   * Attempts to find the catalog and identifier a multipart identifier represents\n+   * @param spark Spark session to use for resolution\n+   * @param nameParts Multipart identifier representing a table\n+   * @return The CatalogPlugin and Identifier for the table\n+   */\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, List<String> nameParts) {\n+    Seq<String> namePartsSeq = JavaConverters.asScalaIterator(nameParts.iterator()).toSeq();\n+    Preconditions.checkArgument(namePartsSeq.nonEmpty(),\n+        \"Cannot determine catalog and Identifier from empty name parts\");\n+    CatalogPlugin currentCatalog = spark.sessionState().catalogManager().currentCatalog();\n+    String[] currentNamespace = spark.sessionState().catalogManager().currentNamespace();\n+    if (namePartsSeq.length() == 1) {\n+      return new CatalogAndIdentifier(currentCatalog, Identifier.of(currentNamespace, namePartsSeq.head()));\n+    } else {\n+      try {\n+        CatalogPlugin namedCatalog = spark.sessionState().catalogManager().catalog(namePartsSeq.head());\n+        return new CatalogAndIdentifier(namedCatalog,\n+            CatalogV2Implicits.MultipartIdentifierHelper((Seq<String>) namePartsSeq.tail()).asIdentifier());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e2b762efb8ed72928fc62e873779bd1abe11bfcb"}, "originalPosition": 90}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM0ODU4MDI4", "url": "https://github.com/apache/iceberg/pull/1784#pullrequestreview-534858028", "createdAt": "2020-11-19T21:16:18Z", "commit": {"oid": "e2b762efb8ed72928fc62e873779bd1abe11bfcb"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQyMToxNjoxOFrOH2x98g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQyMToxNjoxOFrOH2x98g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIwMzgyNg==", "bodyText": "Can we do the precondition first in the method?", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r527203826", "createdAt": "2020-11-19T21:16:18Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java", "diffHunk": "@@ -548,4 +565,83 @@ private static String sqlString(org.apache.iceberg.expressions.Literal<?> lit) {\n       }\n     }\n   }\n+\n+  /*\n+   * Because Spark does not allow more than 1 piece in the namespace for a Session Catalog table, we circumvent\n+   * the entire resolution path for tables and instead look up the table directly ourselves. This lets us correctly\n+   * get metadata tables for the SessionCatalog, if we didn't have to work around this we could just use spark.table.\n+   */\n+  private static Dataset<Row> loadCatalogMetadataTable(SparkSession spark, String name, MetadataTableType type)\n+      throws CatalogNotFoundException, ParseException, NoSuchTableException {\n+\n+    CatalogAndIdentifier catalogAndIdentifier = catalogAndIdentifier(spark, name);\n+    if (catalogAndIdentifier.catalog instanceof BaseCatalog) {\n+      BaseCatalog catalog = (BaseCatalog) catalogAndIdentifier.catalog;\n+      Identifier baseIdent = catalogAndIdentifier.identifier;\n+      Identifier metaIdent = Identifier.of(ArrayUtils.add(baseIdent.namespace(), baseIdent.name()), type.name());\n+      Table metaTable = catalog.loadTable(metaIdent);\n+      return Dataset.ofRows(spark, DataSourceV2Relation.create(metaTable, Some.apply(catalog), Some.apply(metaIdent)));\n+    } else {\n+      throw new CatalogNotFoundException(String.format(\"Cannot cast %s as an Iceberg catalog\",\n+          catalogAndIdentifier.catalog.name()));\n+    }\n+  }\n+\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, String name) throws ParseException {\n+    return catalogAndIdentifier(spark,\n+          JavaConverters.seqAsJavaList(spark.sessionState().sqlParser().parseMultipartIdentifier(name)));\n+  }\n+\n+  /**\n+   * A modified version of Spark's LookupCatalog.CatalogAndIdentifier.unapply\n+   * Attempts to find the catalog and identifier a multipart identifier represents\n+   * @param spark Spark session to use for resolution\n+   * @param nameParts Multipart identifier representing a table\n+   * @return The CatalogPlugin and Identifier for the table\n+   */\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, List<String> nameParts) {\n+    Seq<String> namePartsSeq = JavaConverters.asScalaIterator(nameParts.iterator()).toSeq();\n+    Preconditions.checkArgument(namePartsSeq.nonEmpty(),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e2b762efb8ed72928fc62e873779bd1abe11bfcb"}, "originalPosition": 80}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM0ODU4NDE1", "url": "https://github.com/apache/iceberg/pull/1784#pullrequestreview-534858415", "createdAt": "2020-11-19T21:16:53Z", "commit": {"oid": "e2b762efb8ed72928fc62e873779bd1abe11bfcb"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQyMToxNjo1NFrOH2x_Ng==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQyMToxNjo1NFrOH2x_Ng==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIwNDE1MA==", "bodyText": "Can we introduce CatalogManager catalogManager variable and reuse it in all lines below?", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r527204150", "createdAt": "2020-11-19T21:16:54Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java", "diffHunk": "@@ -548,4 +565,83 @@ private static String sqlString(org.apache.iceberg.expressions.Literal<?> lit) {\n       }\n     }\n   }\n+\n+  /*\n+   * Because Spark does not allow more than 1 piece in the namespace for a Session Catalog table, we circumvent\n+   * the entire resolution path for tables and instead look up the table directly ourselves. This lets us correctly\n+   * get metadata tables for the SessionCatalog, if we didn't have to work around this we could just use spark.table.\n+   */\n+  private static Dataset<Row> loadCatalogMetadataTable(SparkSession spark, String name, MetadataTableType type)\n+      throws CatalogNotFoundException, ParseException, NoSuchTableException {\n+\n+    CatalogAndIdentifier catalogAndIdentifier = catalogAndIdentifier(spark, name);\n+    if (catalogAndIdentifier.catalog instanceof BaseCatalog) {\n+      BaseCatalog catalog = (BaseCatalog) catalogAndIdentifier.catalog;\n+      Identifier baseIdent = catalogAndIdentifier.identifier;\n+      Identifier metaIdent = Identifier.of(ArrayUtils.add(baseIdent.namespace(), baseIdent.name()), type.name());\n+      Table metaTable = catalog.loadTable(metaIdent);\n+      return Dataset.ofRows(spark, DataSourceV2Relation.create(metaTable, Some.apply(catalog), Some.apply(metaIdent)));\n+    } else {\n+      throw new CatalogNotFoundException(String.format(\"Cannot cast %s as an Iceberg catalog\",\n+          catalogAndIdentifier.catalog.name()));\n+    }\n+  }\n+\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, String name) throws ParseException {\n+    return catalogAndIdentifier(spark,\n+          JavaConverters.seqAsJavaList(spark.sessionState().sqlParser().parseMultipartIdentifier(name)));\n+  }\n+\n+  /**\n+   * A modified version of Spark's LookupCatalog.CatalogAndIdentifier.unapply\n+   * Attempts to find the catalog and identifier a multipart identifier represents\n+   * @param spark Spark session to use for resolution\n+   * @param nameParts Multipart identifier representing a table\n+   * @return The CatalogPlugin and Identifier for the table\n+   */\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, List<String> nameParts) {\n+    Seq<String> namePartsSeq = JavaConverters.asScalaIterator(nameParts.iterator()).toSeq();\n+    Preconditions.checkArgument(namePartsSeq.nonEmpty(),\n+        \"Cannot determine catalog and Identifier from empty name parts\");\n+    CatalogPlugin currentCatalog = spark.sessionState().catalogManager().currentCatalog();\n+    String[] currentNamespace = spark.sessionState().catalogManager().currentNamespace();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e2b762efb8ed72928fc62e873779bd1abe11bfcb"}, "originalPosition": 83}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM0ODU5OTA0", "url": "https://github.com/apache/iceberg/pull/1784#pullrequestreview-534859904", "createdAt": "2020-11-19T21:19:04Z", "commit": {"oid": "e2b762efb8ed72928fc62e873779bd1abe11bfcb"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQyMToxOTowNFrOH2yDww==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQyMToxOTowNFrOH2yDww==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIwNTMxNQ==", "bodyText": "Can we split it into multiple lines?\n... parser = spark.sessionState().sqlParser();\n... nameParts = parser.parseMultipartIdentifier(name);\nreturn catalogAndIdentifier(spark, JavaConverters.seqAsJavaList(nameParts));", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r527205315", "createdAt": "2020-11-19T21:19:04Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java", "diffHunk": "@@ -548,4 +565,83 @@ private static String sqlString(org.apache.iceberg.expressions.Literal<?> lit) {\n       }\n     }\n   }\n+\n+  /*\n+   * Because Spark does not allow more than 1 piece in the namespace for a Session Catalog table, we circumvent\n+   * the entire resolution path for tables and instead look up the table directly ourselves. This lets us correctly\n+   * get metadata tables for the SessionCatalog, if we didn't have to work around this we could just use spark.table.\n+   */\n+  private static Dataset<Row> loadCatalogMetadataTable(SparkSession spark, String name, MetadataTableType type)\n+      throws CatalogNotFoundException, ParseException, NoSuchTableException {\n+\n+    CatalogAndIdentifier catalogAndIdentifier = catalogAndIdentifier(spark, name);\n+    if (catalogAndIdentifier.catalog instanceof BaseCatalog) {\n+      BaseCatalog catalog = (BaseCatalog) catalogAndIdentifier.catalog;\n+      Identifier baseIdent = catalogAndIdentifier.identifier;\n+      Identifier metaIdent = Identifier.of(ArrayUtils.add(baseIdent.namespace(), baseIdent.name()), type.name());\n+      Table metaTable = catalog.loadTable(metaIdent);\n+      return Dataset.ofRows(spark, DataSourceV2Relation.create(metaTable, Some.apply(catalog), Some.apply(metaIdent)));\n+    } else {\n+      throw new CatalogNotFoundException(String.format(\"Cannot cast %s as an Iceberg catalog\",\n+          catalogAndIdentifier.catalog.name()));\n+    }\n+  }\n+\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, String name) throws ParseException {\n+    return catalogAndIdentifier(spark,\n+          JavaConverters.seqAsJavaList(spark.sessionState().sqlParser().parseMultipartIdentifier(name)));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e2b762efb8ed72928fc62e873779bd1abe11bfcb"}, "originalPosition": 68}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM0ODYwNTY2", "url": "https://github.com/apache/iceberg/pull/1784#pullrequestreview-534860566", "createdAt": "2020-11-19T21:20:00Z", "commit": {"oid": "e2b762efb8ed72928fc62e873779bd1abe11bfcb"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQyMToyMDowMVrOH2yF-g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQyMToyMDowMVrOH2yF-g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIwNTg4Mg==", "bodyText": "Can we invert the condition and check if it starts with 3?", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r527205882", "createdAt": "2020-11-19T21:20:01Z", "author": {"login": "aokolnychyi"}, "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "diffHunk": "@@ -128,33 +127,56 @@\n     return manifestDF.union(otherMetadataFileDF).union(manifestListDF);\n   }\n \n+  private static boolean isExpectedCatalogLookupException(Exception exception) {\n+    return exception.getMessage().contains(\"AnalysisException\") ||\n+        exception.getMessage().contains(\"SparkException\") ||\n+        exception.getMessage().contains(\"NoSuchTableException\") ||\n+        exception.getMessage().contains(\"CatalogNotFoundException\");\n+  }\n+\n+  // Attempt to use Spark3 Catalog resolution if available on the path\n+  private static DynMethods.StaticMethod loadCatalogImpl = null;\n+\n+  private static Dataset<Row> loadCatalogMetadataTable(SparkSession spark, String tableName, MetadataTableType type)\n+      throws NoSuchMethodException {\n+    if (loadCatalogImpl == null) {\n+      loadCatalogImpl = DynMethods.builder(\"loadCatalogMetadataTable\")\n+          .hiddenImpl(\"org.apache.iceberg.spark.Spark3Util\",\n+              SparkSession.class, String.class, MetadataTableType.class)\n+          .buildStaticChecked();\n+    }\n+    return loadCatalogImpl.invoke(spark, tableName, type);\n+  }\n+\n   protected static Dataset<Row> loadMetadataTable(SparkSession spark, String tableName, String tableLocation,\n                                                   MetadataTableType type) {\n-    DataFrameReader noCatalogReader = spark.read().format(\"iceberg\");\n+    DataFrameReader dataFrameReader = spark.read().format(\"iceberg\");\n     if (tableName.contains(\"/\")) {\n       // Hadoop Table or Metadata location passed, load without a catalog\n-      return noCatalogReader.load(tableName + \"#\" + type);\n+      return dataFrameReader.load(tableName + \"#\" + type);\n     }\n-    // Try catalog based name based resolution\n     try {\n-      return spark.table(tableName + \".\" + type);\n-    } catch (Exception e) {\n-      if (!(e instanceof ParseException || e instanceof AnalysisException)) {\n-        // Rethrow unexpected exceptions\n-        throw e;\n+      // Try DSV2 catalog based name based resolution\n+      if (!spark.version().startsWith(\"2\")) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e2b762efb8ed72928fc62e873779bd1abe11bfcb"}, "originalPosition": 64}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "43c5c5ff1970d66c711e7fd8613924c4242b3fb3", "author": {"user": {"login": "RussellSpitzer", "name": "Russell Spitzer"}}, "url": "https://github.com/apache/iceberg/commit/43c5c5ff1970d66c711e7fd8613924c4242b3fb3", "committedDate": "2020-11-19T22:05:16Z", "message": "Remove Scalaisms for pure Java Code"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "10434a3d5c1f99505d5426643e1c1c3b29755855", "author": {"user": {"login": "RussellSpitzer", "name": "Russell Spitzer"}}, "url": "https://github.com/apache/iceberg/commit/10434a3d5c1f99505d5426643e1c1c3b29755855", "committedDate": "2020-11-19T22:00:52Z", "message": "Remove Scalaisms for pure Java Code"}, "afterCommit": {"oid": "43c5c5ff1970d66c711e7fd8613924c4242b3fb3", "author": {"user": {"login": "RussellSpitzer", "name": "Russell Spitzer"}}, "url": "https://github.com/apache/iceberg/commit/43c5c5ff1970d66c711e7fd8613924c4242b3fb3", "committedDate": "2020-11-19T22:05:16Z", "message": "Remove Scalaisms for pure Java Code"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM0OTQxNDI4", "url": "https://github.com/apache/iceberg/pull/1784#pullrequestreview-534941428", "createdAt": "2020-11-19T23:30:03Z", "commit": {"oid": "43c5c5ff1970d66c711e7fd8613924c4242b3fb3"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQyMzozMDowM1rOH22MCQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQyMzozMDowM1rOH22MCQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzI3Mjk2OQ==", "bodyText": "Just realized I can move all of this into the Spark3Util class and just return null if we can't find the table via that method", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r527272969", "createdAt": "2020-11-19T23:30:03Z", "author": {"login": "RussellSpitzer"}, "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "diffHunk": "@@ -128,33 +127,56 @@\n     return manifestDF.union(otherMetadataFileDF).union(manifestListDF);\n   }\n \n+  private static boolean isExpectedCatalogLookupException(Exception exception) {\n+    return exception.getMessage().contains(\"AnalysisException\") ||", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43c5c5ff1970d66c711e7fd8613924c4242b3fb3"}, "originalPosition": 27}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "44ddb7efc88cf2179a27eca3a91616e980eac40e", "author": {"user": {"login": "RussellSpitzer", "name": "Russell Spitzer"}}, "url": "https://github.com/apache/iceberg/commit/44ddb7efc88cf2179a27eca3a91616e980eac40e", "committedDate": "2020-11-20T00:20:06Z", "message": "Move Exception handling from loadCatalogMetadataTable into Spark3\n\nThis way we can handle the exceptions with the proper classes. Now instead we\nreturn null if we can't find the correct table."}, "afterCommit": {"oid": "a0bbdcd72ff4127cc79df827a6fbdaa3d41d338b", "author": {"user": {"login": "RussellSpitzer", "name": "Russell Spitzer"}}, "url": "https://github.com/apache/iceberg/commit/a0bbdcd72ff4127cc79df827a6fbdaa3d41d338b", "committedDate": "2020-11-20T00:25:26Z", "message": "Move Exception handling from loadCatalogMetadataTable into Spark3\n\nThis way we can handle the exceptions with the proper classes. Now instead we\nreturn null if we can't find the correct table."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ea63004ed1a8ebb51bf42b442562141eb0d27cfb", "author": {"user": {"login": "RussellSpitzer", "name": "Russell Spitzer"}}, "url": "https://github.com/apache/iceberg/commit/ea63004ed1a8ebb51bf42b442562141eb0d27cfb", "committedDate": "2020-11-20T01:02:20Z", "message": "Move Exception handling from loadCatalogMetadataTable into Spark3\n\nThis way we can handle the exceptions with the proper classes. Now instead we\nreturn null if we can't find the correct table."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "a0bbdcd72ff4127cc79df827a6fbdaa3d41d338b", "author": {"user": {"login": "RussellSpitzer", "name": "Russell Spitzer"}}, "url": "https://github.com/apache/iceberg/commit/a0bbdcd72ff4127cc79df827a6fbdaa3d41d338b", "committedDate": "2020-11-20T00:25:26Z", "message": "Move Exception handling from loadCatalogMetadataTable into Spark3\n\nThis way we can handle the exceptions with the proper classes. Now instead we\nreturn null if we can't find the correct table."}, "afterCommit": {"oid": "ea63004ed1a8ebb51bf42b442562141eb0d27cfb", "author": {"user": {"login": "RussellSpitzer", "name": "Russell Spitzer"}}, "url": "https://github.com/apache/iceberg/commit/ea63004ed1a8ebb51bf42b442562141eb0d27cfb", "committedDate": "2020-11-20T01:02:20Z", "message": "Move Exception handling from loadCatalogMetadataTable into Spark3\n\nThis way we can handle the exceptions with the proper classes. Now instead we\nreturn null if we can't find the correct table."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM1MDQ4NDI1", "url": "https://github.com/apache/iceberg/pull/1784#pullrequestreview-535048425", "createdAt": "2020-11-20T03:33:00Z", "commit": {"oid": "ea63004ed1a8ebb51bf42b442562141eb0d27cfb"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQwMzozMzowMVrOH28ORQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQwMzozMzowMVrOH28ORQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzM3MTg0NQ==", "bodyText": "nit: I'd put the relation into a separate var to keep this on one line.", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r527371845", "createdAt": "2020-11-20T03:33:01Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java", "diffHunk": "@@ -548,4 +564,97 @@ private static String sqlString(org.apache.iceberg.expressions.Literal<?> lit) {\n       }\n     }\n   }\n+\n+  /**\n+   * Returns a Metadata Table Dataset if it can be loaded from a Spark V2 Catalog\n+   *\n+   * Because Spark does not allow more than 1 piece in the namespace for a Session Catalog table, we circumvent\n+   * the entire resolution path for tables and instead look up the table directly ourselves. This lets us correctly\n+   * get metadata tables for the SessionCatalog, if we didn't have to work around this we could just use spark.table.\n+   *\n+   * @param spark SparkSession used for looking up catalog references and tables\n+   * @param name The multipart identifier of the base Iceberg table\n+   * @param type The type of metadata table to load\n+   * @return null if we cannot find the Metadata Table, a Dataset of rows otherwise\n+   */\n+  private static Dataset<Row> loadCatalogMetadataTable(SparkSession spark, String name, MetadataTableType type) {\n+    try {\n+      CatalogAndIdentifier catalogAndIdentifier = catalogAndIdentifier(spark, name);\n+      if (catalogAndIdentifier.catalog instanceof BaseCatalog) {\n+        BaseCatalog catalog = (BaseCatalog) catalogAndIdentifier.catalog;\n+        Identifier baseIdent = catalogAndIdentifier.identifier;\n+        Identifier metaIdent = Identifier.of(ArrayUtils.add(baseIdent.namespace(), baseIdent.name()), type.name());\n+        Table metaTable = catalog.loadTable(metaIdent);\n+        return Dataset\n+            .ofRows(spark, DataSourceV2Relation.create(metaTable, Some.apply(catalog), Some.apply(metaIdent)));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ea63004ed1a8ebb51bf42b442562141eb0d27cfb"}, "originalPosition": 65}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "59b573c7dec4a0b3fdef881fcea640fc27c6f7e2", "author": {"user": {"login": "RussellSpitzer", "name": "Russell Spitzer"}}, "url": "https://github.com/apache/iceberg/commit/59b573c7dec4a0b3fdef881fcea640fc27c6f7e2", "committedDate": "2020-11-20T05:00:03Z", "message": "Reviewer Comments"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "a2d63918ad5d5060cee9c700c8ce91027741aec2", "author": {"user": {"login": "RussellSpitzer", "name": "Russell Spitzer"}}, "url": "https://github.com/apache/iceberg/commit/a2d63918ad5d5060cee9c700c8ce91027741aec2", "committedDate": "2020-11-20T04:58:28Z", "message": "Reviewer Comments"}, "afterCommit": {"oid": "59b573c7dec4a0b3fdef881fcea640fc27c6f7e2", "author": {"user": {"login": "RussellSpitzer", "name": "Russell Spitzer"}}, "url": "https://github.com/apache/iceberg/commit/59b573c7dec4a0b3fdef881fcea640fc27c6f7e2", "committedDate": "2020-11-20T05:00:03Z", "message": "Reviewer Comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM1NzU5OTg0", "url": "https://github.com/apache/iceberg/pull/1784#pullrequestreview-535759984", "createdAt": "2020-11-20T20:37:40Z", "commit": {"oid": "59b573c7dec4a0b3fdef881fcea640fc27c6f7e2"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQyMDozNzo0MVrOH3f96A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQyMDozNzo0MVrOH3f96A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzk1NzQ4MA==", "bodyText": "This doesn't need to load the method each time it is called. Usually, I would add orNoop and call build to construct a static field. Then in this method, you'd just need to check whether you have the method or noop:\n  Preconditions.checkArgument(!LOAD_CATALOG.isNoop(), \"Cannot find Spark3Util class ...\");\n  LOAD_CATALOG.invoke(spark, tableName, type);", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r527957480", "createdAt": "2020-11-20T20:37:41Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "diffHunk": "@@ -128,33 +127,48 @@\n     return manifestDF.union(otherMetadataFileDF).union(manifestListDF);\n   }\n \n+  // Attempt to use Spark3 Catalog resolution if available on the path\n+  private static DynMethods.StaticMethod loadCatalogImpl = null;\n+\n+  private static Dataset<Row> loadCatalogMetadataTable(SparkSession spark, String tableName, MetadataTableType type) {\n+    if (loadCatalogImpl == null) {\n+      try {\n+        loadCatalogImpl = DynMethods.builder(\"loadCatalogMetadataTable\")\n+            .hiddenImpl(\"org.apache.iceberg.spark.Spark3Util\",\n+                SparkSession.class, String.class, MetadataTableType.class)\n+            .buildStaticChecked();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "59b573c7dec4a0b3fdef881fcea640fc27c6f7e2"}, "originalPosition": 35}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM1NzYwNjIx", "url": "https://github.com/apache/iceberg/pull/1784#pullrequestreview-535760621", "createdAt": "2020-11-20T20:38:50Z", "commit": {"oid": "59b573c7dec4a0b3fdef881fcea640fc27c6f7e2"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQyMDozODo1MVrOH3f_mg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQyMDozODo1MVrOH3f_mg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzk1NzkxNA==", "bodyText": "Nit: should have an empty line after the last block.", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r527957914", "createdAt": "2020-11-20T20:38:51Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "diffHunk": "@@ -128,33 +127,48 @@\n     return manifestDF.union(otherMetadataFileDF).union(manifestListDF);\n   }\n \n+  // Attempt to use Spark3 Catalog resolution if available on the path\n+  private static DynMethods.StaticMethod loadCatalogImpl = null;\n+\n+  private static Dataset<Row> loadCatalogMetadataTable(SparkSession spark, String tableName, MetadataTableType type) {\n+    if (loadCatalogImpl == null) {\n+      try {\n+        loadCatalogImpl = DynMethods.builder(\"loadCatalogMetadataTable\")\n+            .hiddenImpl(\"org.apache.iceberg.spark.Spark3Util\",\n+                SparkSession.class, String.class, MetadataTableType.class)\n+            .buildStaticChecked();\n+      } catch (NoSuchMethodException e) {\n+        throw new IllegalArgumentException(\"Cannot find Spark3Util class but Spark 3 is being used.\", e);\n+      }\n+    }\n+    return loadCatalogImpl.invoke(spark, tableName, type);\n+  }\n+\n   protected static Dataset<Row> loadMetadataTable(SparkSession spark, String tableName, String tableLocation,\n                                                   MetadataTableType type) {\n-    DataFrameReader noCatalogReader = spark.read().format(\"iceberg\");\n+    DataFrameReader dataFrameReader = spark.read().format(\"iceberg\");\n     if (tableName.contains(\"/\")) {\n       // Hadoop Table or Metadata location passed, load without a catalog\n-      return noCatalogReader.load(tableName + \"#\" + type);\n+      return dataFrameReader.load(tableName + \"#\" + type);\n     }\n-    // Try catalog based name based resolution\n-    try {\n-      return spark.table(tableName + \".\" + type);\n-    } catch (Exception e) {\n-      if (!(e instanceof ParseException || e instanceof AnalysisException)) {\n-        // Rethrow unexpected exceptions\n-        throw e;\n-      }\n-      // Catalog based resolution failed, our catalog may be a non-DatasourceV2 Catalog\n-      if (tableName.startsWith(\"hadoop.\")) {\n-        // Try loading by location as Hadoop table without Catalog\n-        return noCatalogReader.load(tableLocation + \"#\" + type);\n-      } else if (tableName.startsWith(\"hive\")) {\n-        // Try loading by name as a Hive table without Catalog\n-        return noCatalogReader.load(tableName.replaceFirst(\"hive\\\\.\", \"\") + \".\" + type);\n-      } else {\n-        throw new IllegalArgumentException(String.format(\n-            \"Cannot find the metadata table for %s of type %s\", tableName, type));\n+    // Try DSV2 catalog based name based resolution\n+    if (spark.version().startsWith(\"3\")) {\n+      Dataset<Row> catalogMetadataTable = loadCatalogMetadataTable(spark, tableName, type);\n+      if (catalogMetadataTable != null) {\n+        return catalogMetadataTable;\n       }\n     }\n+    // Catalog based resolution failed, our catalog may be a non-DatasourceV2 Catalog", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "59b573c7dec4a0b3fdef881fcea640fc27c6f7e2"}, "originalPosition": 77}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM1NzYyNTg4", "url": "https://github.com/apache/iceberg/pull/1784#pullrequestreview-535762588", "createdAt": "2020-11-20T20:42:26Z", "commit": {"oid": "59b573c7dec4a0b3fdef881fcea640fc27c6f7e2"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "52e4d4ef1eefafb23acdc0f7523c055a1366be87", "author": {"user": {"login": "RussellSpitzer", "name": "Russell Spitzer"}}, "url": "https://github.com/apache/iceberg/commit/52e4d4ef1eefafb23acdc0f7523c055a1366be87", "committedDate": "2020-11-20T21:09:09Z", "message": "More Reviewer Comments\n\nSwitch to static field for reflected Spark3Util method call"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "9707d233885825383159bdc89b723c2f6c12e8d8", "author": {"user": {"login": "RussellSpitzer", "name": "Russell Spitzer"}}, "url": "https://github.com/apache/iceberg/commit/9707d233885825383159bdc89b723c2f6c12e8d8", "committedDate": "2020-11-20T21:00:50Z", "message": "More Reviewer Comments\n\nSwitch to static field for reflected Spark3Util method call"}, "afterCommit": {"oid": "52e4d4ef1eefafb23acdc0f7523c055a1366be87", "author": {"user": {"login": "RussellSpitzer", "name": "Russell Spitzer"}}, "url": "https://github.com/apache/iceberg/commit/52e4d4ef1eefafb23acdc0f7523c055a1366be87", "committedDate": "2020-11-20T21:09:09Z", "message": "More Reviewer Comments\n\nSwitch to static field for reflected Spark3Util method call"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "927e44d8623d3879893a2bd07d475c7baf585cb5", "author": {"user": {"login": "RussellSpitzer", "name": "Russell Spitzer"}}, "url": "https://github.com/apache/iceberg/commit/927e44d8623d3879893a2bd07d475c7baf585cb5", "committedDate": "2020-11-20T21:58:59Z", "message": "Fix Reflective Invocation, Needs Static Context"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM1ODIzOTk2", "url": "https://github.com/apache/iceberg/pull/1784#pullrequestreview-535823996", "createdAt": "2020-11-20T22:40:52Z", "commit": {"oid": "927e44d8623d3879893a2bd07d475c7baf585cb5"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3782, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}