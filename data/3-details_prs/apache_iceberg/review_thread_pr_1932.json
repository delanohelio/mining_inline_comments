{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTM5ODc3ODM2", "number": 1932, "reviewThreads": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQwMDowNzowOFrOFFUvhg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQwMDowOTo1MVrOFFU2JA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQxMTI3MDQ2OnYy", "diffSide": "RIGHT", "path": "spark2/src/test/java/org/apache/iceberg/examples/SchemaEvolutionTest.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQwMDowNzowOFrOIFykhA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQwMDowNzowOFrOIFykhA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mjk0MjM0MA==", "bodyText": "This shouldn't use Spark test internals. There are assertions in Iceberg to check rows.", "url": "https://github.com/apache/iceberg/pull/1932#discussion_r542942340", "createdAt": "2020-12-15T00:07:08Z", "author": {"login": "rdblue"}, "path": "spark2/src/test/java/org/apache/iceberg/examples/SchemaEvolutionTest.java", "diffHunk": "@@ -84,29 +98,56 @@ public void before() throws IOException {\n \n   @Test\n   public void addColumnToSchema() {\n-    table.updateSchema().addColumn(\"publisher\", Types.StringType.get()).commit();\n \n-    Dataset<Row> df2 = spark.read().json(dataLocation + \"new-books.json\");\n+    String fieldName = \"publisher\";\n+    Schema schema = table.schema();\n+    Assert.assertNull(schema.findField(fieldName));\n+\n+    table.updateSchema().addColumn(fieldName, Types.StringType.get()).commit();\n+\n+    Dataset<Row> df1 = spark.read()\n+        .json(dataLocation + \"books.json\");\n+    df1 = df1.withColumn(fieldName, new Column(Literal$.MODULE$.apply(null)))\n+        .selectExpr(\"title\", \"price\", \"author\", \"cast(published as timestamp)\", \"genre\", \"publisher\");\n \n+    Dataset<Row> df2 = spark.read().json(dataLocation + \"new-books.json\")\n+        .selectExpr(\"title\", \"price\", \"author\", \"cast(published as timestamp)\", \"genre\", \"publisher\");\n+    List<Row> expected = df1.union(df2)\n+        .collectAsList();\n+\n+    // Append data\n     df2.select(df2.col(\"title\"), df2.col(\"price\").cast(DataTypes.IntegerType),\n         df2.col(\"author\"), df2.col(\"published\").cast(DataTypes.TimestampType),\n         df2.col(\"genre\"), df2.col(\"publisher\")).write()\n         .format(\"iceberg\")\n         .mode(\"append\")\n         .save(tableLocation.toString());\n+\n+    // Read iceberg table\n+    Dataset<Row> iceberg = spark.read()\n+        .format(\"iceberg\")\n+        .load(tableLocation.toString());\n+\n+    String error = QueryTest$.MODULE$.checkAnswer(iceberg, expected);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "992d665b06aefec1462aa0387c55d0c7bfa59d5d"}, "originalPosition": 89}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQxMTI3NzY2OnYy", "diffSide": "RIGHT", "path": "spark2/src/test/java/org/apache/iceberg/examples/SchemaEvolutionTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQwMDowODoxMlrOIFyo3g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxOTo1NjoyN1rOIGdcqw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mjk0MzQ1NA==", "bodyText": "I don't think that this additional check is needed. This is an example, not really a correctness or unit test. So we want the example to be as simple as possible to be readable as an example of using the API. This only lives in tests so that it is kept up to date and we know it doesn't fail.", "url": "https://github.com/apache/iceberg/pull/1932#discussion_r542943454", "createdAt": "2020-12-15T00:08:12Z", "author": {"login": "rdblue"}, "path": "spark2/src/test/java/org/apache/iceberg/examples/SchemaEvolutionTest.java", "diffHunk": "@@ -84,29 +98,56 @@ public void before() throws IOException {\n \n   @Test\n   public void addColumnToSchema() {\n-    table.updateSchema().addColumn(\"publisher\", Types.StringType.get()).commit();\n \n-    Dataset<Row> df2 = spark.read().json(dataLocation + \"new-books.json\");\n+    String fieldName = \"publisher\";\n+    Schema schema = table.schema();\n+    Assert.assertNull(schema.findField(fieldName));\n+\n+    table.updateSchema().addColumn(fieldName, Types.StringType.get()).commit();\n+\n+    Dataset<Row> df1 = spark.read()\n+        .json(dataLocation + \"books.json\");\n+    df1 = df1.withColumn(fieldName, new Column(Literal$.MODULE$.apply(null)))\n+        .selectExpr(\"title\", \"price\", \"author\", \"cast(published as timestamp)\", \"genre\", \"publisher\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "992d665b06aefec1462aa0387c55d0c7bfa59d5d"}, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzY0NDg0Mw==", "bodyText": "@rdblue Thanks for inputs, I have removed the correctness related changes.", "url": "https://github.com/apache/iceberg/pull/1932#discussion_r543644843", "createdAt": "2020-12-15T19:56:27Z", "author": {"login": "karuppayya"}, "path": "spark2/src/test/java/org/apache/iceberg/examples/SchemaEvolutionTest.java", "diffHunk": "@@ -84,29 +98,56 @@ public void before() throws IOException {\n \n   @Test\n   public void addColumnToSchema() {\n-    table.updateSchema().addColumn(\"publisher\", Types.StringType.get()).commit();\n \n-    Dataset<Row> df2 = spark.read().json(dataLocation + \"new-books.json\");\n+    String fieldName = \"publisher\";\n+    Schema schema = table.schema();\n+    Assert.assertNull(schema.findField(fieldName));\n+\n+    table.updateSchema().addColumn(fieldName, Types.StringType.get()).commit();\n+\n+    Dataset<Row> df1 = spark.read()\n+        .json(dataLocation + \"books.json\");\n+    df1 = df1.withColumn(fieldName, new Column(Literal$.MODULE$.apply(null)))\n+        .selectExpr(\"title\", \"price\", \"author\", \"cast(published as timestamp)\", \"genre\", \"publisher\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mjk0MzQ1NA=="}, "originalCommit": {"oid": "992d665b06aefec1462aa0387c55d0c7bfa59d5d"}, "originalPosition": 69}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQxMTI4MTQ5OnYy", "diffSide": "RIGHT", "path": "spark2/src/test/java/org/apache/iceberg/examples/SchemaEvolutionTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQwMDowODo1MFrOIFyrew==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxOTo1ODoxN1rOIGdg5w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mjk0NDEyMw==", "bodyText": "Style: no blocks of imports, just one list ordered alphabetically.", "url": "https://github.com/apache/iceberg/pull/1932#discussion_r542944123", "createdAt": "2020-12-15T00:08:50Z", "author": {"login": "rdblue"}, "path": "spark2/src/test/java/org/apache/iceberg/examples/SchemaEvolutionTest.java", "diffHunk": "@@ -22,18 +22,27 @@\n import java.io.File;\n import java.io.IOException;\n import java.nio.file.Files;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.stream.Stream;\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "992d665b06aefec1462aa0387c55d0c7bfa59d5d"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzY0NTkyNw==", "bodyText": "done.", "url": "https://github.com/apache/iceberg/pull/1932#discussion_r543645927", "createdAt": "2020-12-15T19:58:17Z", "author": {"login": "karuppayya"}, "path": "spark2/src/test/java/org/apache/iceberg/examples/SchemaEvolutionTest.java", "diffHunk": "@@ -22,18 +22,27 @@\n import java.io.File;\n import java.io.IOException;\n import java.nio.file.Files;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.stream.Stream;\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mjk0NDEyMw=="}, "originalCommit": {"oid": "992d665b06aefec1462aa0387c55d0c7bfa59d5d"}, "originalPosition": 8}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQxMTI4NDA5OnYy", "diffSide": "RIGHT", "path": "spark2/src/test/java/org/apache/iceberg/examples/SchemaEvolutionTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQwMDowOToxNlrOIFytTA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxOTo1ODoyNlrOIGdhSQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mjk0NDU4OA==", "bodyText": "Style: no wildcard imports because it isn't obvious where symbols are coming from.", "url": "https://github.com/apache/iceberg/pull/1932#discussion_r542944588", "createdAt": "2020-12-15T00:09:16Z", "author": {"login": "rdblue"}, "path": "spark2/src/test/java/org/apache/iceberg/examples/SchemaEvolutionTest.java", "diffHunk": "@@ -22,18 +22,27 @@\n import java.io.File;\n import java.io.IOException;\n import java.nio.file.Files;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.stream.Stream;\n+\n import org.apache.commons.io.FileUtils;\n import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.hadoop.HadoopTables;\n import org.apache.iceberg.types.Types;\n+import org.apache.spark.sql.Column;\n import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.QueryTest$;\n import org.apache.spark.sql.Row;\n import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.expressions.Literal$;\n import org.apache.spark.sql.types.DataTypes;\n-import org.junit.Before;\n-import org.junit.Test;\n+import org.apache.spark.sql.types.LongType$;\n+import org.apache.spark.sql.types.StructField;\n+import org.junit.*;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "992d665b06aefec1462aa0387c55d0c7bfa59d5d"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzY0NjAyNQ==", "bodyText": "done", "url": "https://github.com/apache/iceberg/pull/1932#discussion_r543646025", "createdAt": "2020-12-15T19:58:26Z", "author": {"login": "karuppayya"}, "path": "spark2/src/test/java/org/apache/iceberg/examples/SchemaEvolutionTest.java", "diffHunk": "@@ -22,18 +22,27 @@\n import java.io.File;\n import java.io.IOException;\n import java.nio.file.Files;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.stream.Stream;\n+\n import org.apache.commons.io.FileUtils;\n import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.hadoop.HadoopTables;\n import org.apache.iceberg.types.Types;\n+import org.apache.spark.sql.Column;\n import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.QueryTest$;\n import org.apache.spark.sql.Row;\n import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.expressions.Literal$;\n import org.apache.spark.sql.types.DataTypes;\n-import org.junit.Before;\n-import org.junit.Test;\n+import org.apache.spark.sql.types.LongType$;\n+import org.apache.spark.sql.types.StructField;\n+import org.junit.*;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mjk0NDU4OA=="}, "originalCommit": {"oid": "992d665b06aefec1462aa0387c55d0c7bfa59d5d"}, "originalPosition": 26}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQxMTI4NjYxOnYy", "diffSide": "RIGHT", "path": "spark2/src/test/java/org/apache/iceberg/examples/SchemaEvolutionTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQwMDowOTo0MlrOIFyu8w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxOTo1ODozNVrOIGdhvQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mjk0NTAxMQ==", "bodyText": "This are good assertions to have, but they should be JUnit assertions and not assert.", "url": "https://github.com/apache/iceberg/pull/1932#discussion_r542945011", "createdAt": "2020-12-15T00:09:42Z", "author": {"login": "rdblue"}, "path": "spark2/src/test/java/org/apache/iceberg/examples/SchemaEvolutionTest.java", "diffHunk": "@@ -120,13 +161,25 @@ public void renameColumn() {\n \n     results.createOrReplaceTempView(\"table\");\n     spark.sql(\"select * from table\").show();\n+    List<String> fields = Arrays.asList(spark.sql(\"select * from table\").schema().names());\n+    assert (fields.contains(\"writer\"));\n+    assert (!fields.contains(\"author\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "992d665b06aefec1462aa0387c55d0c7bfa59d5d"}, "originalPosition": 122}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzY0NjE0MQ==", "bodyText": "done", "url": "https://github.com/apache/iceberg/pull/1932#discussion_r543646141", "createdAt": "2020-12-15T19:58:35Z", "author": {"login": "karuppayya"}, "path": "spark2/src/test/java/org/apache/iceberg/examples/SchemaEvolutionTest.java", "diffHunk": "@@ -120,13 +161,25 @@ public void renameColumn() {\n \n     results.createOrReplaceTempView(\"table\");\n     spark.sql(\"select * from table\").show();\n+    List<String> fields = Arrays.asList(spark.sql(\"select * from table\").schema().names());\n+    assert (fields.contains(\"writer\"));\n+    assert (!fields.contains(\"author\"));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mjk0NTAxMQ=="}, "originalCommit": {"oid": "992d665b06aefec1462aa0387c55d0c7bfa59d5d"}, "originalPosition": 122}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQxMTI4NzQwOnYy", "diffSide": "RIGHT", "path": "spark2/src/test/java/org/apache/iceberg/examples/SchemaEvolutionTest.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQwMDowOTo1MVrOIFyvXA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQwMDowOTo1MVrOIFyvXA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mjk0NTExNg==", "bodyText": "This is a good assertion to add.", "url": "https://github.com/apache/iceberg/pull/1932#discussion_r542945116", "createdAt": "2020-12-15T00:09:51Z", "author": {"login": "rdblue"}, "path": "spark2/src/test/java/org/apache/iceberg/examples/SchemaEvolutionTest.java", "diffHunk": "@@ -84,29 +98,56 @@ public void before() throws IOException {\n \n   @Test\n   public void addColumnToSchema() {\n-    table.updateSchema().addColumn(\"publisher\", Types.StringType.get()).commit();\n \n-    Dataset<Row> df2 = spark.read().json(dataLocation + \"new-books.json\");\n+    String fieldName = \"publisher\";\n+    Schema schema = table.schema();\n+    Assert.assertNull(schema.findField(fieldName));\n+\n+    table.updateSchema().addColumn(fieldName, Types.StringType.get()).commit();\n+\n+    Dataset<Row> df1 = spark.read()\n+        .json(dataLocation + \"books.json\");\n+    df1 = df1.withColumn(fieldName, new Column(Literal$.MODULE$.apply(null)))\n+        .selectExpr(\"title\", \"price\", \"author\", \"cast(published as timestamp)\", \"genre\", \"publisher\");\n \n+    Dataset<Row> df2 = spark.read().json(dataLocation + \"new-books.json\")\n+        .selectExpr(\"title\", \"price\", \"author\", \"cast(published as timestamp)\", \"genre\", \"publisher\");\n+    List<Row> expected = df1.union(df2)\n+        .collectAsList();\n+\n+    // Append data\n     df2.select(df2.col(\"title\"), df2.col(\"price\").cast(DataTypes.IntegerType),\n         df2.col(\"author\"), df2.col(\"published\").cast(DataTypes.TimestampType),\n         df2.col(\"genre\"), df2.col(\"publisher\")).write()\n         .format(\"iceberg\")\n         .mode(\"append\")\n         .save(tableLocation.toString());\n+\n+    // Read iceberg table\n+    Dataset<Row> iceberg = spark.read()\n+        .format(\"iceberg\")\n+        .load(tableLocation.toString());\n+\n+    String error = QueryTest$.MODULE$.checkAnswer(iceberg, expected);\n+    Assert.assertNull(error);\n+\n   }\n \n   @Test\n   public void deleteColumnFromSchema() {\n-    table.updateSchema().deleteColumn(\"genre\").commit();\n+    List<Row> rows = spark.read()\n+        .format(\"iceberg\")\n+        .load(tableLocation.toString())\n+        .drop(\"genre\").collectAsList();\n \n+    table.updateSchema().deleteColumn(\"genre\").commit();\n     table.refresh();\n+\n     Dataset<Row> results = spark.read()\n         .format(\"iceberg\")\n         .load(tableLocation.toString());\n-\n-    results.createOrReplaceTempView(\"table\");\n-    spark.sql(\"select * from table\").show();\n+    Assert.assertFalse(Arrays.asList(results.schema().names()).contains(\"genre\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "992d665b06aefec1462aa0387c55d0c7bfa59d5d"}, "originalPosition": 111}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3333, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}