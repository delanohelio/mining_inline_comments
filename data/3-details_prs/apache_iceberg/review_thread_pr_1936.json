{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTM5OTc1NDIy", "number": 1936, "reviewThreads": {"totalCount": 25, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xMVQxMDozMToxN1rOFNLJsA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yMlQwNDowOTozMVrOFRuxmQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ5MzU4NTEyOnYy", "diffSide": "RIGHT", "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkTableOptions.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xMVQxMDozMToxN1rOIRPgJQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xMVQxMDozMToxN1rOIRPgJQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDk1MDY5Mw==", "bodyText": "The defaultValue(true) says deprecated now.  Let's change it to:\n  public static final ConfigOption<Boolean> TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM =\n      ConfigOptions.key(\"table.exec.iceberg.infer-source-parallelism\").booleanType().defaultValue(true)\n          .withDescription(\"If is false, parallelism of source are set by config.\\n\" +\n              \"If is true, source parallelism is inferred according to splits number.\\n\");\nThe similar thing in TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX.", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r554950693", "createdAt": "2021-01-11T10:31:17Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkTableOptions.java", "diffHunk": "@@ -0,0 +1,39 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+\n+import org.apache.flink.configuration.ConfigOption;\n+import org.apache.flink.configuration.ConfigOptions;\n+\n+public class FlinkTableOptions {\n+\n+  private FlinkTableOptions() {\n+  }\n+\n+  public static final ConfigOption<Boolean> TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM =\n+      ConfigOptions.key(\"table.exec.iceberg.infer-source-parallelism\").defaultValue(true)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6820d4e965c6b97a83bc5a97ae2c22bb3f2e5413"}, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ5MzYwNzUzOnYy", "diffSide": "RIGHT", "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xMVQxMDozNzowNlrOIRPtYg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xMVQxMDozNzowNlrOIRPtYg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDk1NDA4Mg==", "bodyText": "How about moving those lines into a separate method ?", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r554954082", "createdAt": "2021-01-11T10:37:06Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "diffHunk": "@@ -195,7 +205,29 @@ public FlinkInputFormat buildFormat() {\n       Preconditions.checkNotNull(env, \"StreamExecutionEnvironment should not be null\");\n       FlinkInputFormat format = buildFormat();\n       if (isBounded(context)) {\n-        return env.createInput(format, rowTypeInfo);\n+        int parallelism = flinkConf.get(ExecutionConfigOptions.TABLE_EXEC_RESOURCE_DEFAULT_PARALLELISM);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6820d4e965c6b97a83bc5a97ae2c22bb3f2e5413"}, "originalPosition": 46}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ5Mzc3MDM5OnYy", "diffSide": "RIGHT", "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xMVQxMToyMTo0NFrOIRROrg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xMVQxMToyMTo0NFrOIRROrg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDk3ODk5MA==", "bodyText": "Nit: use UncheckedIOException here.\n            throw new UncheckedIOException(\"Failed to create iceberg input splits for table: \" + table, e);", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r554978990", "createdAt": "2021-01-11T11:21:44Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "diffHunk": "@@ -195,7 +205,29 @@ public FlinkInputFormat buildFormat() {\n       Preconditions.checkNotNull(env, \"StreamExecutionEnvironment should not be null\");\n       FlinkInputFormat format = buildFormat();\n       if (isBounded(context)) {\n-        return env.createInput(format, rowTypeInfo);\n+        int parallelism = flinkConf.get(ExecutionConfigOptions.TABLE_EXEC_RESOURCE_DEFAULT_PARALLELISM);\n+        if (flinkConf.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM)) {\n+          int max = flinkConf.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX);\n+          if (max < 1) {\n+            throw new IllegalConfigurationException(\n+                FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX.key() + \" cannot be less than 1\");\n+          }\n+\n+          int splitNum = 0;\n+          try {\n+            FlinkInputSplit[] splits = format.createInputSplits(0);\n+            splitNum = splits.length;\n+          } catch (IOException e) {\n+            throw new RuntimeException(\"get input split  error.\", e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6820d4e965c6b97a83bc5a97ae2c22bb3f2e5413"}, "originalPosition": 59}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ5Mzc3MjU5OnYy", "diffSide": "RIGHT", "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xMVQxMToyMjoyOVrOIRRQEA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xMVQxMToyMjoyOVrOIRRQEA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDk3OTM0NA==", "bodyText": "nit: Preconditions.checkState ?", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r554979344", "createdAt": "2021-01-11T11:22:29Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "diffHunk": "@@ -195,7 +205,29 @@ public FlinkInputFormat buildFormat() {\n       Preconditions.checkNotNull(env, \"StreamExecutionEnvironment should not be null\");\n       FlinkInputFormat format = buildFormat();\n       if (isBounded(context)) {\n-        return env.createInput(format, rowTypeInfo);\n+        int parallelism = flinkConf.get(ExecutionConfigOptions.TABLE_EXEC_RESOURCE_DEFAULT_PARALLELISM);\n+        if (flinkConf.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM)) {\n+          int max = flinkConf.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX);\n+          if (max < 1) {\n+            throw new IllegalConfigurationException(\n+                FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX.key() + \" cannot be less than 1\");\n+          }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6820d4e965c6b97a83bc5a97ae2c22bb3f2e5413"}, "originalPosition": 52}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ5Mzc4MDQ0OnYy", "diffSide": "RIGHT", "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xMVQxMToyNToxMFrOIRRU3Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xMlQwMjoxNzo1OVrOIRvCyA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDk4MDU3Mw==", "bodyText": "It may be overflow when casting the long limit to integer  ?  I'd like to use (int) Math.min(parallelism, limit).", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r554980573", "createdAt": "2021-01-11T11:25:10Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "diffHunk": "@@ -195,7 +205,29 @@ public FlinkInputFormat buildFormat() {\n       Preconditions.checkNotNull(env, \"StreamExecutionEnvironment should not be null\");\n       FlinkInputFormat format = buildFormat();\n       if (isBounded(context)) {\n-        return env.createInput(format, rowTypeInfo);\n+        int parallelism = flinkConf.get(ExecutionConfigOptions.TABLE_EXEC_RESOURCE_DEFAULT_PARALLELISM);\n+        if (flinkConf.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM)) {\n+          int max = flinkConf.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX);\n+          if (max < 1) {\n+            throw new IllegalConfigurationException(\n+                FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX.key() + \" cannot be less than 1\");\n+          }\n+\n+          int splitNum = 0;\n+          try {\n+            FlinkInputSplit[] splits = format.createInputSplits(0);\n+            splitNum = splits.length;\n+          } catch (IOException e) {\n+            throw new RuntimeException(\"get input split  error.\", e);\n+          }\n+\n+          parallelism = Math.min(splitNum, max);\n+        }\n+\n+        parallelism = limit > 0 ? Math.min(parallelism, (int) limit) : parallelism;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6820d4e965c6b97a83bc5a97ae2c22bb3f2e5413"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTQ2NzQ2NA==", "bodyText": "the parallelism is int type and the  limit is long type,  Math.min(parallelism, limit) will throws an exception,I add a judgment to prevent overflow.\n      int limitInt = limit > Integer.MAX_VALUE ? Integer.MAX_VALUE : (int) limit;\n      parallelism = limitInt > 0 ? Math.min(parallelism, limitInt) : parallelism;", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r555467464", "createdAt": "2021-01-12T02:17:59Z", "author": {"login": "zhangjun0x01"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "diffHunk": "@@ -195,7 +205,29 @@ public FlinkInputFormat buildFormat() {\n       Preconditions.checkNotNull(env, \"StreamExecutionEnvironment should not be null\");\n       FlinkInputFormat format = buildFormat();\n       if (isBounded(context)) {\n-        return env.createInput(format, rowTypeInfo);\n+        int parallelism = flinkConf.get(ExecutionConfigOptions.TABLE_EXEC_RESOURCE_DEFAULT_PARALLELISM);\n+        if (flinkConf.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM)) {\n+          int max = flinkConf.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX);\n+          if (max < 1) {\n+            throw new IllegalConfigurationException(\n+                FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX.key() + \" cannot be less than 1\");\n+          }\n+\n+          int splitNum = 0;\n+          try {\n+            FlinkInputSplit[] splits = format.createInputSplits(0);\n+            splitNum = splits.length;\n+          } catch (IOException e) {\n+            throw new RuntimeException(\"get input split  error.\", e);\n+          }\n+\n+          parallelism = Math.min(splitNum, max);\n+        }\n+\n+        parallelism = limit > 0 ? Math.min(parallelism, (int) limit) : parallelism;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDk4MDU3Mw=="}, "originalCommit": {"oid": "6820d4e965c6b97a83bc5a97ae2c22bb3f2e5413"}, "originalPosition": 65}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ5Mzc5OTIxOnYy", "diffSide": "RIGHT", "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSource.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xMVQxMTozMDozMFrOIRRgAA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQwMzo0MDowOFrOIVZBzw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDk4MzQyNA==", "bodyText": "Is there another way to assert the parallelism as expected value ?  Here we're using flink's planner to get the ExecNode ,  I'm concerning that we're using flink's Internal codes which would be a big trouble when upgrading the flink version.  Pls see this PR #1956", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r554983424", "createdAt": "2021-01-11T11:30:30Z", "author": {"login": "openinx"}, "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSource.java", "diffHunk": "@@ -103,4 +124,45 @@ public void testLimitPushDown() {\n     Assert.assertEquals(\"should have 1 record\", 1, mixedResult.size());\n     Assert.assertArrayEquals(\"Should produce the expected records\", mixedResult.get(0), new Object[] {1, \"a\"});\n   }\n+\n+  @Test\n+  public void testParallelismOptimize() {\n+    sql(\"INSERT INTO %s  VALUES (1,'hello')\", TABLE_NAME);\n+    sql(\"INSERT INTO %s  VALUES (2,'iceberg')\", TABLE_NAME);\n+\n+    TableEnvironment tenv = getTableEnv();\n+\n+    // empty table ,parallelism at least 1\n+    Table tableEmpty = tenv.sqlQuery(String.format(\"SELECT * FROM %s\", TABLE_NAME));\n+    testParallelismSettingTranslateAndAssert(1, tableEmpty, tenv);\n+\n+    // make sure to generate 2 CombinedScanTasks\n+    org.apache.iceberg.Table table = validationCatalog.loadTable(TableIdentifier.of(icebergNamespace, TABLE_NAME));\n+    Stream<FileScanTask> stream = StreamSupport.stream(table.newScan().planFiles().spliterator(), false);\n+    Optional<FileScanTask> fileScanTaskOptional =  stream.max(Comparator.comparing(FileScanTask::length));\n+    Assert.assertTrue(fileScanTaskOptional.isPresent());\n+    long maxFileLen = fileScanTaskOptional.get().length();\n+    sql(\"ALTER TABLE %s SET ('read.split.open-file-cost'='1', 'read.split.target-size'='%s')\", TABLE_NAME, maxFileLen);\n+\n+    // 2 splits ,the parallelism is  2\n+    Table tableSelect = tenv.sqlQuery(String.format(\"SELECT * FROM %s\", TABLE_NAME));\n+    testParallelismSettingTranslateAndAssert(2, tableSelect, tenv);\n+\n+    // 2 splits  and limit is 1 ,the parallelism is  1\n+    Table tableLimit = tenv.sqlQuery(String.format(\"SELECT * FROM %s LIMIT 1\", TABLE_NAME));\n+    testParallelismSettingTranslateAndAssert(1, tableLimit, tenv);\n+  }\n+\n+  private void testParallelismSettingTranslateAndAssert(int expected, Table table, TableEnvironment tEnv) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6820d4e965c6b97a83bc5a97ae2c22bb3f2e5413"}, "originalPosition": 83}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTMwMTA3MQ==", "bodyText": "Flink 1.12 does refactor ExecNode, I found an easier way to assert parallelism, I will update it later", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r559301071", "createdAt": "2021-01-18T03:40:08Z", "author": {"login": "zhangjun0x01"}, "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSource.java", "diffHunk": "@@ -103,4 +124,45 @@ public void testLimitPushDown() {\n     Assert.assertEquals(\"should have 1 record\", 1, mixedResult.size());\n     Assert.assertArrayEquals(\"Should produce the expected records\", mixedResult.get(0), new Object[] {1, \"a\"});\n   }\n+\n+  @Test\n+  public void testParallelismOptimize() {\n+    sql(\"INSERT INTO %s  VALUES (1,'hello')\", TABLE_NAME);\n+    sql(\"INSERT INTO %s  VALUES (2,'iceberg')\", TABLE_NAME);\n+\n+    TableEnvironment tenv = getTableEnv();\n+\n+    // empty table ,parallelism at least 1\n+    Table tableEmpty = tenv.sqlQuery(String.format(\"SELECT * FROM %s\", TABLE_NAME));\n+    testParallelismSettingTranslateAndAssert(1, tableEmpty, tenv);\n+\n+    // make sure to generate 2 CombinedScanTasks\n+    org.apache.iceberg.Table table = validationCatalog.loadTable(TableIdentifier.of(icebergNamespace, TABLE_NAME));\n+    Stream<FileScanTask> stream = StreamSupport.stream(table.newScan().planFiles().spliterator(), false);\n+    Optional<FileScanTask> fileScanTaskOptional =  stream.max(Comparator.comparing(FileScanTask::length));\n+    Assert.assertTrue(fileScanTaskOptional.isPresent());\n+    long maxFileLen = fileScanTaskOptional.get().length();\n+    sql(\"ALTER TABLE %s SET ('read.split.open-file-cost'='1', 'read.split.target-size'='%s')\", TABLE_NAME, maxFileLen);\n+\n+    // 2 splits ,the parallelism is  2\n+    Table tableSelect = tenv.sqlQuery(String.format(\"SELECT * FROM %s\", TABLE_NAME));\n+    testParallelismSettingTranslateAndAssert(2, tableSelect, tenv);\n+\n+    // 2 splits  and limit is 1 ,the parallelism is  1\n+    Table tableLimit = tenv.sqlQuery(String.format(\"SELECT * FROM %s LIMIT 1\", TABLE_NAME));\n+    testParallelismSettingTranslateAndAssert(1, tableLimit, tenv);\n+  }\n+\n+  private void testParallelismSettingTranslateAndAssert(int expected, Table table, TableEnvironment tEnv) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDk4MzQyNA=="}, "originalCommit": {"oid": "6820d4e965c6b97a83bc5a97ae2c22bb3f2e5413"}, "originalPosition": 83}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ5MzgwNjk4OnYy", "diffSide": "RIGHT", "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSource.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xMVQxMTozMjo1OVrOIRRkvQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xMVQxMTozMjo1OVrOIRRkvQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDk4NDYzNw==", "bodyText": "nit:  testParallelismOptimize -> testInferedParallelism", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r554984637", "createdAt": "2021-01-11T11:32:59Z", "author": {"login": "openinx"}, "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSource.java", "diffHunk": "@@ -103,4 +124,45 @@ public void testLimitPushDown() {\n     Assert.assertEquals(\"should have 1 record\", 1, mixedResult.size());\n     Assert.assertArrayEquals(\"Should produce the expected records\", mixedResult.get(0), new Object[] {1, \"a\"});\n   }\n+\n+  @Test\n+  public void testParallelismOptimize() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6820d4e965c6b97a83bc5a97ae2c22bb3f2e5413"}, "originalPosition": 56}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ5MzgzMTYyOnYy", "diffSide": "RIGHT", "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSource.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xMVQxMTozOTo1M1rOIRRy6Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xMVQxMTozOTo1M1rOIRRy6Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDk4ODI2NQ==", "bodyText": "nit: how about introducing a small method:\n private Table sqlQuery(String sql, Object... args) {\n    return getTableEnv().sqlQuery(String.format(sql, args));\n  }", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r554988265", "createdAt": "2021-01-11T11:39:53Z", "author": {"login": "openinx"}, "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSource.java", "diffHunk": "@@ -103,4 +124,45 @@ public void testLimitPushDown() {\n     Assert.assertEquals(\"should have 1 record\", 1, mixedResult.size());\n     Assert.assertArrayEquals(\"Should produce the expected records\", mixedResult.get(0), new Object[] {1, \"a\"});\n   }\n+\n+  @Test\n+  public void testParallelismOptimize() {\n+    sql(\"INSERT INTO %s  VALUES (1,'hello')\", TABLE_NAME);\n+    sql(\"INSERT INTO %s  VALUES (2,'iceberg')\", TABLE_NAME);\n+\n+    TableEnvironment tenv = getTableEnv();\n+\n+    // empty table ,parallelism at least 1\n+    Table tableEmpty = tenv.sqlQuery(String.format(\"SELECT * FROM %s\", TABLE_NAME));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6820d4e965c6b97a83bc5a97ae2c22bb3f2e5413"}, "originalPosition": 63}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUzMDg0NDMwOnYy", "diffSide": "RIGHT", "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergTableSource.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yMFQwNjo0OTowNlrOIWvUgA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yMFQwNjo0OTowNlrOIWvUgA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDcxNDg4MA==", "bodyText": "Nit:  I'd like to change this builder chain like the following ( That's more easy to read the change):\n  @Override\n  public DataStream<RowData> getDataStream(StreamExecutionEnvironment execEnv) {\n    return FlinkSource.forRowData()\n        .env(execEnv)\n        .tableLoader(loader)\n        .project(getProjectedSchema())\n        .limit(limit)\n        .filters(filters)\n        .flinkConf(readableConfig)\n        .properties(properties)\n        .build();\n  }", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r560714880", "createdAt": "2021-01-20T06:49:06Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergTableSource.java", "diffHunk": "@@ -79,13 +84,13 @@ public boolean isBounded() {\n \n   @Override\n   public TableSource<RowData> projectFields(int[] fields) {\n-    return new IcebergTableSource(loader, schema, properties, fields, isLimitPushDown, limit, filters);\n+    return new IcebergTableSource(loader, schema, properties, fields, isLimitPushDown, limit, filters, readableConfig);\n   }\n \n   @Override\n   public DataStream<RowData> getDataStream(StreamExecutionEnvironment execEnv) {\n     return FlinkSource.forRowData().env(execEnv).tableLoader(loader).project(getProjectedSchema()).limit(limit)\n-        .filters(filters).properties(properties).build();\n+        .filters(filters).flinkConf(readableConfig).properties(properties).build();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a8ef6a3d0347c084e0cb128a5ac94e6a78b8959f"}, "originalPosition": 54}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUzMDg3MjEwOnYy", "diffSide": "RIGHT", "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkTableOptions.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yMFQwNjo1OTozOVrOIWvkFA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yMFQwNjo1OTozOVrOIWvkFA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDcxODg2OA==", "bodyText": "Nit: it's more clear to make each option definition into a separate line:\n      ConfigOptions.key(\"table.exec.iceberg.infer-source-parallelism\")\n          .booleanType()\n          .defaultValue(true)\n          .withDescription(\"If is false, parallelism of source are set by config.\\n\" +\n              \"If is true, source parallelism is inferred according to splits number.\\n\");", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r560718868", "createdAt": "2021-01-20T06:59:39Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkTableOptions.java", "diffHunk": "@@ -0,0 +1,39 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+\n+import org.apache.flink.configuration.ConfigOption;\n+import org.apache.flink.configuration.ConfigOptions;\n+\n+public class FlinkTableOptions {\n+\n+  private FlinkTableOptions() {\n+  }\n+\n+  public static final ConfigOption<Boolean> TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM =\n+      ConfigOptions.key(\"table.exec.iceberg.infer-source-parallelism\").booleanType().defaultValue(true)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a8ef6a3d0347c084e0cb128a5ac94e6a78b8959f"}, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUzMDg3MjY0OnYy", "diffSide": "RIGHT", "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkTableOptions.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yMFQwNjo1OTo0N1rOIWvkXw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yMFQwNjo1OTo0N1rOIWvkXw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDcxODk0Mw==", "bodyText": "ditto", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r560718943", "createdAt": "2021-01-20T06:59:47Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkTableOptions.java", "diffHunk": "@@ -0,0 +1,39 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+\n+import org.apache.flink.configuration.ConfigOption;\n+import org.apache.flink.configuration.ConfigOptions;\n+\n+public class FlinkTableOptions {\n+\n+  private FlinkTableOptions() {\n+  }\n+\n+  public static final ConfigOption<Boolean> TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM =\n+      ConfigOptions.key(\"table.exec.iceberg.infer-source-parallelism\").booleanType().defaultValue(true)\n+          .withDescription(\"If is false, parallelism of source are set by config.\\n\" +\n+              \"If is true, source parallelism is inferred according to splits number.\\n\");\n+\n+  public static final ConfigOption<Integer> TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX =\n+      ConfigOptions.key(\"table.exec.iceberg.infer-source-parallelism.max\").intType().defaultValue(100)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a8ef6a3d0347c084e0cb128a5ac94e6a78b8959f"}, "originalPosition": 37}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUzMDkwODI0OnYy", "diffSide": "RIGHT", "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yMFQwNzoxMzozMFrOIWv4ag==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yMFQwNzoxMzozMFrOIWv4ag==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDcyNDA3NA==", "bodyText": "In this comment, I think I did not describe the things  clearly.   I mean  we could move the inferParallelism into a separate method, don't have to contains the DataStream constructing or chaining methods.\nprivate int inferParallelism(FlinkInputFormat format, ScanContext context) {\n   // ....\n}", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r560724074", "createdAt": "2021-01-20T07:13:30Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "diffHunk": "@@ -197,7 +206,7 @@ public FlinkInputFormat buildFormat() {\n       TypeInformation<RowData> typeInfo = RowDataTypeInfo.of(FlinkSchemaUtil.convert(context.project()));\n \n       if (!context.isStreaming()) {\n-        return env.createInput(format, typeInfo);\n+        return createInputDataStream(format, context, typeInfo);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a8ef6a3d0347c084e0cb128a5ac94e6a78b8959f"}, "originalPosition": 45}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUzMDkxODg1OnYy", "diffSide": "RIGHT", "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yMFQwNzoxNzoyM1rOIWv-lw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yMFQwNzoxNzoyM1rOIWv-lw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDcyNTY1NQ==", "bodyText": "Nit:  I'd like to make this code more readable:\n      if (context.limit() > 0) {\n        int limit = context.limit() >= Integer.MAX_VALUE ? Integer.MAX_VALUE : (int) context.limit();\n        parallelism = Math.min(parallelism, limit);\n      }\n\n     // parallelism must be positive.\n      parallelism = Math.max(1, parallelism);", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r560725655", "createdAt": "2021-01-20T07:17:23Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "diffHunk": "@@ -208,6 +217,30 @@ public FlinkInputFormat buildFormat() {\n             .transform(readerOperatorName, typeInfo, StreamingReaderOperator.factory(format));\n       }\n     }\n+\n+    private DataStream<RowData> createInputDataStream(FlinkInputFormat format, ScanContext context,\n+                                                      TypeInformation<RowData> typeInfo) {\n+      int parallelism = flinkConf.get(ExecutionConfigOptions.TABLE_EXEC_RESOURCE_DEFAULT_PARALLELISM);\n+      if (flinkConf.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM)) {\n+        int max = flinkConf.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX);\n+        Preconditions.checkState(max >= 1,\n+            FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX.key() + \" cannot be less than 1\");\n+        int splitNum = 0;\n+        try {\n+          FlinkInputSplit[] splits = format.createInputSplits(0);\n+          splitNum = splits.length;\n+        } catch (IOException e) {\n+          throw new UncheckedIOException(\"Failed to create iceberg input splits for table: \" + table, e);\n+        }\n+\n+        parallelism = Math.min(splitNum, max);\n+      }\n+\n+      int limitInt = context.limit() > Integer.MAX_VALUE ? Integer.MAX_VALUE : (int) context.limit();\n+      parallelism = limitInt > 0 ? Math.min(parallelism, limitInt) : parallelism;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a8ef6a3d0347c084e0cb128a5ac94e6a78b8959f"}, "originalPosition": 73}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUzMDkzNDQ1OnYy", "diffSide": "RIGHT", "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yMFQwNzoyMjoyNVrOIWwHfw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yMVQwMzowMzozNlrOIXhI0Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDcyNzkzNQ==", "bodyText": "For those users that write flink batch jobs in Java API ,  they will always pass a flink's Configuration, right ?   So how about defining this as  org.apache.flink.configuration.Configuraiton  ?", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r560727935", "createdAt": "2021-01-20T07:22:25Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "diffHunk": "@@ -70,6 +73,7 @@ public static Builder forRowData() {\n     private Table table;\n     private TableLoader tableLoader;\n     private TableSchema projectedSchema;\n+    private ReadableConfig flinkConf;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a8ef6a3d0347c084e0cb128a5ac94e6a78b8959f"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MTUwMjI0MQ==", "bodyText": "When we construct the IcebergTableSource, we use the TableSourceFactory.Context#getConfiguration method to get the configuration. This method returns a ReadableConfig, so we use ReadableConfig instead of Configuration. In addition, Configuration is the implementation class of the ReadableConfig interface, so I think ReadableConfig should not has a problem", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r561502241", "createdAt": "2021-01-21T02:35:26Z", "author": {"login": "zhangjun0x01"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "diffHunk": "@@ -70,6 +73,7 @@ public static Builder forRowData() {\n     private Table table;\n     private TableLoader tableLoader;\n     private TableSchema projectedSchema;\n+    private ReadableConfig flinkConf;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDcyNzkzNQ=="}, "originalCommit": {"oid": "a8ef6a3d0347c084e0cb128a5ac94e6a78b8959f"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MTUzMTA4OQ==", "bodyText": "Got your point,  I'd prefer to use flink's Configuration  because  it will be exposed to flink developers as an API in  FlinkSource,  using the unified Configuration will be more straightforward for them.  But as you said,  the  TableSourceFactory#Context is exposing the ReadableConfig,  I also did not find a correct way to convert ReadableConfig to Configuration.   OK, I think we could use ReadableConfig here,  thanks.", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r561531089", "createdAt": "2021-01-21T03:03:36Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "diffHunk": "@@ -70,6 +73,7 @@ public static Builder forRowData() {\n     private Table table;\n     private TableLoader tableLoader;\n     private TableSchema projectedSchema;\n+    private ReadableConfig flinkConf;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDcyNzkzNQ=="}, "originalCommit": {"oid": "a8ef6a3d0347c084e0cb128a5ac94e6a78b8959f"}, "originalPosition": 24}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUzMDk0MDkxOnYy", "diffSide": "RIGHT", "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yMFQwNzoyNDo1M1rOIWwLLQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yMFQwNzoyNDo1M1rOIWwLLQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDcyODg3Nw==", "bodyText": "Nit: use maxInterParallelism pls.", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r560728877", "createdAt": "2021-01-20T07:24:53Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "diffHunk": "@@ -208,6 +217,30 @@ public FlinkInputFormat buildFormat() {\n             .transform(readerOperatorName, typeInfo, StreamingReaderOperator.factory(format));\n       }\n     }\n+\n+    private DataStream<RowData> createInputDataStream(FlinkInputFormat format, ScanContext context,\n+                                                      TypeInformation<RowData> typeInfo) {\n+      int parallelism = flinkConf.get(ExecutionConfigOptions.TABLE_EXEC_RESOURCE_DEFAULT_PARALLELISM);\n+      if (flinkConf.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM)) {\n+        int max = flinkConf.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a8ef6a3d0347c084e0cb128a5ac94e6a78b8959f"}, "originalPosition": 58}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUzMDk0NjgyOnYy", "diffSide": "RIGHT", "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yMFQwNzoyNjo1MlrOIWwOdA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yMFQwNzoyNjo1MlrOIWwOdA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDcyOTcxNg==", "bodyText": "Nit:  this assignment is redundant ( from intellij).", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r560729716", "createdAt": "2021-01-20T07:26:52Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "diffHunk": "@@ -208,6 +217,30 @@ public FlinkInputFormat buildFormat() {\n             .transform(readerOperatorName, typeInfo, StreamingReaderOperator.factory(format));\n       }\n     }\n+\n+    private DataStream<RowData> createInputDataStream(FlinkInputFormat format, ScanContext context,\n+                                                      TypeInformation<RowData> typeInfo) {\n+      int parallelism = flinkConf.get(ExecutionConfigOptions.TABLE_EXEC_RESOURCE_DEFAULT_PARALLELISM);\n+      if (flinkConf.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM)) {\n+        int max = flinkConf.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX);\n+        Preconditions.checkState(max >= 1,\n+            FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX.key() + \" cannot be less than 1\");\n+        int splitNum = 0;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a8ef6a3d0347c084e0cb128a5ac94e6a78b8959f"}, "originalPosition": 61}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUzMDk5ODc1OnYy", "diffSide": "RIGHT", "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSource.java", "isResolved": false, "comments": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yMFQwNzo0Mzo1OVrOIWwsgg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yMVQwNjo0NjozMlrOIXnlCA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDczNzQxMA==", "bodyText": "Shouldn't the inferParallelism only affect the batch job (See FlinkSource#Builder#build)?  So there's no reason that providing unit test in streaming  mode ?\nIn my mind,  Providing unit tests to check whether the inferParallelism() is returning the expected parallelism value is enough for this changes.   Seems like The ITCase is validating the behavior of DataStreamSource#setParallelism ,  we could think it's always correct because it's a basic API in flink.", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r560737410", "createdAt": "2021-01-20T07:43:59Z", "author": {"login": "openinx"}, "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSource.java", "diffHunk": "@@ -685,4 +782,60 @@ public void testSqlParseError() {\n     AssertHelpers.assertThrows(\"The NaN is not supported by flink now. \",\n         NumberFormatException.class, () -> sql(sqlParseErrorLTE));\n   }\n+\n+  /**\n+   * The sql can be executed in both streaming and batch mode, in order to get the parallelism, we convert the flink\n+   * Table to flink DataStream, so we only use streaming mode here.\n+   *\n+   * @throws TableNotExistException table not exist exception\n+   */\n+  @Test\n+  public void testInferedParallelism() throws TableNotExistException {\n+    Assume.assumeTrue(\"The execute mode should  be streaming mode\", isStreamingJob);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a8ef6a3d0347c084e0cb128a5ac94e6a78b8959f"}, "originalPosition": 567}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDczODQ2OQ==", "bodyText": "In this way, we don't have to change so many codes in this class. Maybe we could just add unit tests in TestFlinkScan.java", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r560738469", "createdAt": "2021-01-20T07:46:06Z", "author": {"login": "openinx"}, "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSource.java", "diffHunk": "@@ -685,4 +782,60 @@ public void testSqlParseError() {\n     AssertHelpers.assertThrows(\"The NaN is not supported by flink now. \",\n         NumberFormatException.class, () -> sql(sqlParseErrorLTE));\n   }\n+\n+  /**\n+   * The sql can be executed in both streaming and batch mode, in order to get the parallelism, we convert the flink\n+   * Table to flink DataStream, so we only use streaming mode here.\n+   *\n+   * @throws TableNotExistException table not exist exception\n+   */\n+  @Test\n+  public void testInferedParallelism() throws TableNotExistException {\n+    Assume.assumeTrue(\"The execute mode should  be streaming mode\", isStreamingJob);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDczNzQxMA=="}, "originalCommit": {"oid": "a8ef6a3d0347c084e0cb128a5ac94e6a78b8959f"}, "originalPosition": 567}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDg1NzU0OQ==", "bodyText": "I found that in this test method,I use the flink streaming mode,but it still enter the batch mode (here), I check the code,found that FlinkSource.Builder#build mthod judge streaming mode or batch mode by the conf of ScanContext instead of flink conf. will this confuse users?", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r560857549", "createdAt": "2021-01-20T10:37:48Z", "author": {"login": "zhangjun0x01"}, "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSource.java", "diffHunk": "@@ -685,4 +782,60 @@ public void testSqlParseError() {\n     AssertHelpers.assertThrows(\"The NaN is not supported by flink now. \",\n         NumberFormatException.class, () -> sql(sqlParseErrorLTE));\n   }\n+\n+  /**\n+   * The sql can be executed in both streaming and batch mode, in order to get the parallelism, we convert the flink\n+   * Table to flink DataStream, so we only use streaming mode here.\n+   *\n+   * @throws TableNotExistException table not exist exception\n+   */\n+  @Test\n+  public void testInferedParallelism() throws TableNotExistException {\n+    Assume.assumeTrue(\"The execute mode should  be streaming mode\", isStreamingJob);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDczNzQxMA=="}, "originalCommit": {"oid": "a8ef6a3d0347c084e0cb128a5ac94e6a78b8959f"}, "originalPosition": 567}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDk4MzU0Ng==", "bodyText": "Okay, that's a great point.  I think it will confuse users,  the correct way is :   Set the ScanContext's properties firstly (use the following fromFlinkConf) if someone provides a flink configuration,  that is similar to the ScanContext#fromProperties:\ndiff --git a/flink/src/main/java/org/apache/iceberg/flink/source/ScanContext.java b/flink/src/main/java/org/apache/iceberg/flink/source/ScanContext.java\nindex 2896efb3..c56e3311 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/source/ScanContext.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/source/ScanContext.java\n@@ -292,10 +292,7 @@ class ScanContext implements Serializable {\n       return this;\n     }\n \n-    Builder fromProperties(Map<String, String> properties) {\n-      Configuration config = new Configuration();\n-      properties.forEach(config::setString);\n-\n+    Builder fromFlinkConf(Configuration config) {\n       return this.useSnapshotId(config.get(SNAPSHOT_ID))\n           .caseSensitive(config.get(CASE_SENSITIVE))\n           .asOfTimestamp(config.get(AS_OF_TIMESTAMP))\n@@ -305,7 +302,14 @@ class ScanContext implements Serializable {\n           .splitLookback(config.get(SPLIT_LOOKBACK))\n           .splitOpenFileCost(config.get(SPLIT_FILE_OPEN_COST))\n           .streaming(config.get(STREAMING))\n-          .monitorInterval(config.get(MONITOR_INTERVAL))\n+          .monitorInterval(config.get(MONITOR_INTERVAL));\n+    }\n+\n+    Builder fromProperties(Map<String, String> properties) {\n+      Configuration config = new Configuration();\n+      properties.forEach(config::setString);\n+\n+      return fromFlinkConf(config)\n           .nameMapping(properties.get(DEFAULT_NAME_MAPPING));\n     }", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r560983546", "createdAt": "2021-01-20T14:05:08Z", "author": {"login": "openinx"}, "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSource.java", "diffHunk": "@@ -685,4 +782,60 @@ public void testSqlParseError() {\n     AssertHelpers.assertThrows(\"The NaN is not supported by flink now. \",\n         NumberFormatException.class, () -> sql(sqlParseErrorLTE));\n   }\n+\n+  /**\n+   * The sql can be executed in both streaming and batch mode, in order to get the parallelism, we convert the flink\n+   * Table to flink DataStream, so we only use streaming mode here.\n+   *\n+   * @throws TableNotExistException table not exist exception\n+   */\n+  @Test\n+  public void testInferedParallelism() throws TableNotExistException {\n+    Assume.assumeTrue(\"The execute mode should  be streaming mode\", isStreamingJob);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDczNzQxMA=="}, "originalCommit": {"oid": "a8ef6a3d0347c084e0cb128a5ac94e6a78b8959f"}, "originalPosition": 567}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDk5MjM3OA==", "bodyText": "If someone provides both flink Configuration and iceberg's properties, then we should use the flink's Configuration values overwrite the iceberg's properties because  properties is a table-level settings  while the flink's Configuration is a job-level settings. It is reasonable for fine-grained configuration to ovewrite coarse-grained configuration.", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r560992378", "createdAt": "2021-01-20T14:16:45Z", "author": {"login": "openinx"}, "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSource.java", "diffHunk": "@@ -685,4 +782,60 @@ public void testSqlParseError() {\n     AssertHelpers.assertThrows(\"The NaN is not supported by flink now. \",\n         NumberFormatException.class, () -> sql(sqlParseErrorLTE));\n   }\n+\n+  /**\n+   * The sql can be executed in both streaming and batch mode, in order to get the parallelism, we convert the flink\n+   * Table to flink DataStream, so we only use streaming mode here.\n+   *\n+   * @throws TableNotExistException table not exist exception\n+   */\n+  @Test\n+  public void testInferedParallelism() throws TableNotExistException {\n+    Assume.assumeTrue(\"The execute mode should  be streaming mode\", isStreamingJob);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDczNzQxMA=="}, "originalCommit": {"oid": "a8ef6a3d0347c084e0cb128a5ac94e6a78b8959f"}, "originalPosition": 567}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MTQ2MTY3MA==", "bodyText": "I think these are two different concepts.\nIn flink, whether using batch mode or streaming mode, we can read batch data. Flink treats batch jobs as bounded streaming jobs, so there should be no problem whether it is using batch mode or streaming mode to read batch data.  In addition, flink will use StreamExecutionEnvironment (DataStream) to do batch tasks and stream tasks uniformly (the  doc link) . The batch mode may expire, so I think we should also use StreamExecutionEnvironment (DataStream) for batch tasks as much as possible.\nWhen we use StreamExecutionEnvironment , in FlinkSource.Builder#build method,\nthe if and else block in this method are both streaming jobs, if block is a bounded streaming jobs, maybe we can rename ScanContext#isStreaming field to ScanContext#isStreamingRead, which will be easier to understand. If code block is bounded stream job (batch), else code block to do long-running stream job.\n    if (!context.isStreaming()) {\n        int parallelism = inferParallelism(format, context);\n        return env.createInput(format, typeInfo).setParallelism(parallelism);\n      } else {\n        StreamingMonitorFunction function = new StreamingMonitorFunction(tableLoader, context);\n\n        String monitorFunctionName = String.format(\"Iceberg table (%s) monitor\", table);\n        String readerOperatorName = String.format(\"Iceberg table (%s) reader\", table);\n\n        return env.addSource(function, monitorFunctionName)\n            .transform(readerOperatorName, typeInfo, StreamingReaderOperator.factory(format));\n      }", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r561461670", "createdAt": "2021-01-21T01:49:19Z", "author": {"login": "zhangjun0x01"}, "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSource.java", "diffHunk": "@@ -685,4 +782,60 @@ public void testSqlParseError() {\n     AssertHelpers.assertThrows(\"The NaN is not supported by flink now. \",\n         NumberFormatException.class, () -> sql(sqlParseErrorLTE));\n   }\n+\n+  /**\n+   * The sql can be executed in both streaming and batch mode, in order to get the parallelism, we convert the flink\n+   * Table to flink DataStream, so we only use streaming mode here.\n+   *\n+   * @throws TableNotExistException table not exist exception\n+   */\n+  @Test\n+  public void testInferedParallelism() throws TableNotExistException {\n+    Assume.assumeTrue(\"The execute mode should  be streaming mode\", isStreamingJob);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDczNzQxMA=="}, "originalCommit": {"oid": "a8ef6a3d0347c084e0cb128a5ac94e6a78b8959f"}, "originalPosition": 567}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MTQ5MTI4OA==", "bodyText": "In my mind, Providing unit tests to check whether the inferParallelism() is returning the expected parallelism value is enough for this changes. Seems like The ITCase is validating the behavior of DataStreamSource#setParallelism , we could think it's always correct because it's a basic API in flink.\n\nI think it\u2019s better not to use the inferParallelism method to get the parallelism to do assertion, because the inferParallelism method is private and is an internal method of iceberg. Just as you commented that it is best not to use the internal code of flink, I think we should try to use public APIs to get information.\nThe current TestFlinkTableSource class uses batch mode for unit test. In order not to modify too much code, we can move the testInferedParallelism method to other test classes, such as TestFlinkScan.java.\nSo I think we can use DataStream.getTransformation().getParallelism(); to get the parallelism of the flink operator. This method is public api of flink. Even if flink is upgraded in the future, it should not be modified. What do you think?", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r561491288", "createdAt": "2021-01-21T02:24:29Z", "author": {"login": "zhangjun0x01"}, "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSource.java", "diffHunk": "@@ -685,4 +782,60 @@ public void testSqlParseError() {\n     AssertHelpers.assertThrows(\"The NaN is not supported by flink now. \",\n         NumberFormatException.class, () -> sql(sqlParseErrorLTE));\n   }\n+\n+  /**\n+   * The sql can be executed in both streaming and batch mode, in order to get the parallelism, we convert the flink\n+   * Table to flink DataStream, so we only use streaming mode here.\n+   *\n+   * @throws TableNotExistException table not exist exception\n+   */\n+  @Test\n+  public void testInferedParallelism() throws TableNotExistException {\n+    Assume.assumeTrue(\"The execute mode should  be streaming mode\", isStreamingJob);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDczNzQxMA=="}, "originalCommit": {"oid": "a8ef6a3d0347c084e0cb128a5ac94e6a78b8959f"}, "originalPosition": 567}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MTUwNjMwMA==", "bodyText": "The isStreaming  indicate whether the flink source is a streaming source (In our mind) ,  not say it's a streaming job or batch job.  The hive table source also has the similar configure key :\n    public static final ConfigOption<Boolean> STREAMING_SOURCE_ENABLE =\n            key(\"streaming-source.enable\")\n                    .booleanType()\n                    .defaultValue(false)\n                    .withDescription(\n                            \"Enable streaming source or not.\\n\"\n                                    + \" NOTES: Please make sure that each partition/file should be written\"\n                                    + \" atomically, otherwise the reader may get incomplete data.\");\nIf we think this iceberg configure key is not very clear,  I think we could propose another separate PR to align with hive configure key.  Let's focus on this parallelism issue here,  what do you think ?", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r561506300", "createdAt": "2021-01-21T02:39:25Z", "author": {"login": "openinx"}, "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSource.java", "diffHunk": "@@ -685,4 +782,60 @@ public void testSqlParseError() {\n     AssertHelpers.assertThrows(\"The NaN is not supported by flink now. \",\n         NumberFormatException.class, () -> sql(sqlParseErrorLTE));\n   }\n+\n+  /**\n+   * The sql can be executed in both streaming and batch mode, in order to get the parallelism, we convert the flink\n+   * Table to flink DataStream, so we only use streaming mode here.\n+   *\n+   * @throws TableNotExistException table not exist exception\n+   */\n+  @Test\n+  public void testInferedParallelism() throws TableNotExistException {\n+    Assume.assumeTrue(\"The execute mode should  be streaming mode\", isStreamingJob);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDczNzQxMA=="}, "originalCommit": {"oid": "a8ef6a3d0347c084e0cb128a5ac94e6a78b8959f"}, "originalPosition": 567}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MTUxMzAxNQ==", "bodyText": "yes,it should be a streaming source, like kafka. If necessary, we can open a separate PR to discuss this.", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r561513015", "createdAt": "2021-01-21T02:45:57Z", "author": {"login": "zhangjun0x01"}, "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSource.java", "diffHunk": "@@ -685,4 +782,60 @@ public void testSqlParseError() {\n     AssertHelpers.assertThrows(\"The NaN is not supported by flink now. \",\n         NumberFormatException.class, () -> sql(sqlParseErrorLTE));\n   }\n+\n+  /**\n+   * The sql can be executed in both streaming and batch mode, in order to get the parallelism, we convert the flink\n+   * Table to flink DataStream, so we only use streaming mode here.\n+   *\n+   * @throws TableNotExistException table not exist exception\n+   */\n+  @Test\n+  public void testInferedParallelism() throws TableNotExistException {\n+    Assume.assumeTrue(\"The execute mode should  be streaming mode\", isStreamingJob);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDczNzQxMA=="}, "originalCommit": {"oid": "a8ef6a3d0347c084e0cb128a5ac94e6a78b8959f"}, "originalPosition": 567}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MTUxNTE1Ng==", "bodyText": "Just as you commented that it is best not to use the internal code of flink, I think we should try to use public APIs to get information.\n\nThe comment that saying we'd better not use flink's Internal API because that would introduce extra upgrade complexity (new flink version may breaks those internal API so we iceberg have to adjust the codes,  finally maintaining different versions of flink will bring us a lot of burden).\nWriting iceberg unit tests based on our iceberg's non-public ( we usually use package-access ) method is OK because there's no extra burden .", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r561515156", "createdAt": "2021-01-21T02:47:59Z", "author": {"login": "openinx"}, "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSource.java", "diffHunk": "@@ -685,4 +782,60 @@ public void testSqlParseError() {\n     AssertHelpers.assertThrows(\"The NaN is not supported by flink now. \",\n         NumberFormatException.class, () -> sql(sqlParseErrorLTE));\n   }\n+\n+  /**\n+   * The sql can be executed in both streaming and batch mode, in order to get the parallelism, we convert the flink\n+   * Table to flink DataStream, so we only use streaming mode here.\n+   *\n+   * @throws TableNotExistException table not exist exception\n+   */\n+  @Test\n+  public void testInferedParallelism() throws TableNotExistException {\n+    Assume.assumeTrue(\"The execute mode should  be streaming mode\", isStreamingJob);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDczNzQxMA=="}, "originalCommit": {"oid": "a8ef6a3d0347c084e0cb128a5ac94e6a78b8959f"}, "originalPosition": 567}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MTYzNjYxNg==", "bodyText": "I update the pr,move the testInferedParallelism method to TestFlinkScanSql,use  FlinkSource.Builder#inferParallelism method to do the assertion", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r561636616", "createdAt": "2021-01-21T06:46:32Z", "author": {"login": "zhangjun0x01"}, "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSource.java", "diffHunk": "@@ -685,4 +782,60 @@ public void testSqlParseError() {\n     AssertHelpers.assertThrows(\"The NaN is not supported by flink now. \",\n         NumberFormatException.class, () -> sql(sqlParseErrorLTE));\n   }\n+\n+  /**\n+   * The sql can be executed in both streaming and batch mode, in order to get the parallelism, we convert the flink\n+   * Table to flink DataStream, so we only use streaming mode here.\n+   *\n+   * @throws TableNotExistException table not exist exception\n+   */\n+  @Test\n+  public void testInferedParallelism() throws TableNotExistException {\n+    Assume.assumeTrue(\"The execute mode should  be streaming mode\", isStreamingJob);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDczNzQxMA=="}, "originalCommit": {"oid": "a8ef6a3d0347c084e0cb128a5ac94e6a78b8959f"}, "originalPosition": 567}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzU0MTA4NDI0OnYy", "diffSide": "RIGHT", "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yMlQwMjoyMzoyOVrOIYSPUA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yMlQwMjoyMzoyOVrOIYSPUA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjMzNTU2OA==", "bodyText": "Nit:  maxInterParallelism -> maxInferParallelism,  seems like it's a typo ?", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r562335568", "createdAt": "2021-01-22T02:23:29Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "diffHunk": "@@ -208,6 +218,33 @@ public FlinkInputFormat buildFormat() {\n             .transform(readerOperatorName, typeInfo, StreamingReaderOperator.factory(format));\n       }\n     }\n+\n+    int inferParallelism(FlinkInputFormat format, ScanContext context) {\n+      int parallelism = readableConfig.get(ExecutionConfigOptions.TABLE_EXEC_RESOURCE_DEFAULT_PARALLELISM);\n+      if (readableConfig.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM)) {\n+        int maxInterParallelism = readableConfig.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "47ae11a8a83dee8adebc3a66301a21d3652f0092"}, "originalPosition": 58}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzU0MTEwMjkxOnYy", "diffSide": "RIGHT", "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergTableSource.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yMlQwMjoyNzoyMFrOIYSZ4w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yMlQwMjoyNzoyMFrOIYSZ4w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjMzODI3NQ==", "bodyText": "Nit:  Let's move this line to line60,  so that the assignment order of IcebergTableSource constructor could align with these definitions.", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r562338275", "createdAt": "2021-01-22T02:27:20Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergTableSource.java", "diffHunk": "@@ -51,25 +52,29 @@\n   private final TableLoader loader;\n   private final TableSchema schema;\n   private final Map<String, String> properties;\n+  private final ReadableConfig readableConfig;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "47ae11a8a83dee8adebc3a66301a21d3652f0092"}, "originalPosition": 12}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzU0MTExNjA1OnYy", "diffSide": "RIGHT", "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergTableSource.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yMlQwMjoyOTo1NVrOIYSg3A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yMlQwMjoyOTo1NVrOIYSg3A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjM0MDA2MA==", "bodyText": "Nit:  maybe we'd better also align the orders as above commented.", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r562340060", "createdAt": "2021-01-22T02:29:55Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergTableSource.java", "diffHunk": "@@ -79,13 +84,20 @@ public boolean isBounded() {\n \n   @Override\n   public TableSource<RowData> projectFields(int[] fields) {\n-    return new IcebergTableSource(loader, schema, properties, fields, isLimitPushDown, limit, filters);\n+    return new IcebergTableSource(loader, schema, properties, fields, isLimitPushDown, limit, filters, readableConfig);\n   }\n \n   @Override\n   public DataStream<RowData> getDataStream(StreamExecutionEnvironment execEnv) {\n-    return FlinkSource.forRowData().env(execEnv).tableLoader(loader).project(getProjectedSchema()).limit(limit)\n-        .filters(filters).properties(properties).build();\n+    return FlinkSource.forRowData()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "47ae11a8a83dee8adebc3a66301a21d3652f0092"}, "originalPosition": 54}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzU0MTMzODA0OnYy", "diffSide": "RIGHT", "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSource.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yMlQwMzo1NDowNFrOIYUd2A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yMlQwODowMToyOVrOIYZQKQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjM3MjA1Ng==", "bodyText": "We can disable the table.exec.iceberg.infer-source-parallelism  for all the batch tests by default, then we don't have to change all cases from this file.   Actually,  we have wrote many unit tests which depends on the parallelism, for example  this PR #2064.  Using the inter-parallelism for batch unit tests will introduce extra complexity and instability,  so I recommend to disable the infer parallelism in our batch unit tests by default:\ndiff --git a/flink/src/test/java/org/apache/iceberg/flink/FlinkTestBase.java b/flink/src/test/java/org/apache/iceberg/flink/FlinkTestBase.java\nindex 5b8e58cf..ab3d56ea 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/FlinkTestBase.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/FlinkTestBase.java\n@@ -62,10 +62,17 @@ public abstract class FlinkTestBase extends AbstractTestBase {\n     if (tEnv == null) {\n       synchronized (this) {\n         if (tEnv == null) {\n-          this.tEnv = TableEnvironment.create(EnvironmentSettings\n+          EnvironmentSettings settings = EnvironmentSettings\n               .newInstance()\n               .useBlinkPlanner()\n-              .inBatchMode().build());\n+              .inBatchMode()\n+              .build();\n+\n+          TableEnvironment env = TableEnvironment.create(settings);\n+          env.getConfig().getConfiguration()\n+              .set(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM, false);\n+\n+          tEnv = env;\n         }\n       }\n     }", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r562372056", "createdAt": "2021-01-22T03:54:04Z", "author": {"login": "openinx"}, "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSource.java", "diffHunk": "@@ -137,7 +136,10 @@ public void testFilterPushDownEqual() {\n     Assert.assertEquals(\"Should have 1 record\", 1, result.size());\n     Assert.assertArrayEquals(\"Should produce the expected record\", expectRecord, result.get(0));\n \n-    Assert.assertEquals(\"Should create only one scan\", 1, scanEventCount);\n+    // Because we add infer parallelism, all data files will be scanned first.\n+    // Flink will call FlinkInputFormat#createInputSplits method to scan the data files,\n+    // plus the operation to get the execution plan, so there are three scan event.\n+    Assert.assertEquals(\"Should create 3 scans\", 3, scanEventCount);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "47ae11a8a83dee8adebc3a66301a21d3652f0092"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjQ1MDQ3Mw==", "bodyText": "yes,I update it", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r562450473", "createdAt": "2021-01-22T08:01:29Z", "author": {"login": "zhangjun0x01"}, "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSource.java", "diffHunk": "@@ -137,7 +136,10 @@ public void testFilterPushDownEqual() {\n     Assert.assertEquals(\"Should have 1 record\", 1, result.size());\n     Assert.assertArrayEquals(\"Should produce the expected record\", expectRecord, result.get(0));\n \n-    Assert.assertEquals(\"Should create only one scan\", 1, scanEventCount);\n+    // Because we add infer parallelism, all data files will be scanned first.\n+    // Flink will call FlinkInputFormat#createInputSplits method to scan the data files,\n+    // plus the operation to get the execution plan, so there are three scan event.\n+    Assert.assertEquals(\"Should create 3 scans\", 3, scanEventCount);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjM3MjA1Ng=="}, "originalCommit": {"oid": "47ae11a8a83dee8adebc3a66301a21d3652f0092"}, "originalPosition": 25}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzU0MTM0NjE2OnYy", "diffSide": "RIGHT", "path": "flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkScanSql.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yMlQwMzo1ODoxMVrOIYUiXg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yMlQwMzo1ODoxMVrOIYUiXg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjM3MzIxNA==", "bodyText": "Nit:  inter parallelism should be at least 1.", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r562373214", "createdAt": "2021-01-22T03:58:11Z", "author": {"login": "openinx"}, "path": "flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkScanSql.java", "diffHunk": "@@ -106,6 +109,51 @@ public void testResiduals() throws Exception {\n     assertRecords(runWithFilter(filter, \"where dt='2020-03-20' and id=123\"), expectedRecords, SCHEMA);\n   }\n \n+  @Test\n+  public void testInferedParallelism() throws IOException {\n+    Table table = catalog.createTable(TableIdentifier.of(\"default\", \"t\"), SCHEMA, SPEC);\n+\n+    TableLoader tableLoader = TableLoader.fromHadoopTable(table.location());\n+    FlinkInputFormat flinkInputFormat = FlinkSource.forRowData().tableLoader(tableLoader).table(table).buildFormat();\n+    ScanContext scanContext = ScanContext.builder().build();\n+\n+    // Empty table ,parallelism at least 1", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "47ae11a8a83dee8adebc3a66301a21d3652f0092"}, "originalPosition": 30}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzU0MTM1MDQyOnYy", "diffSide": "RIGHT", "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yMlQwNDowMDoyOVrOIYUktQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yMlQwODowMToxNFrOIYZPxQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjM3MzgxMw==", "bodyText": "Should we provide a new Configuration()  for this variable ?  Otherwise,  it will just throw NPE if people forget to provide a flinkConf in FlinkSource#Builder because we don't check the nullable in interParallelism.", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r562373813", "createdAt": "2021-01-22T04:00:29Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "diffHunk": "@@ -70,6 +73,7 @@ public static Builder forRowData() {\n     private Table table;\n     private TableLoader tableLoader;\n     private TableSchema projectedSchema;\n+    private ReadableConfig readableConfig;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "47ae11a8a83dee8adebc3a66301a21d3652f0092"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjQ1MDM3Mw==", "bodyText": "yes,I add the new Configuration() for default.", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r562450373", "createdAt": "2021-01-22T08:01:14Z", "author": {"login": "zhangjun0x01"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "diffHunk": "@@ -70,6 +73,7 @@ public static Builder forRowData() {\n     private Table table;\n     private TableLoader tableLoader;\n     private TableSchema projectedSchema;\n+    private ReadableConfig readableConfig;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjM3MzgxMw=="}, "originalCommit": {"oid": "47ae11a8a83dee8adebc3a66301a21d3652f0092"}, "originalPosition": 24}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzU0MTM1OTQ2OnYy", "diffSide": "RIGHT", "path": "flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkScanSql.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yMlQwNDowNjozMlrOIYUqDw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yMlQwODowMDozNVrOIYZOcg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjM3NTE4Mw==", "bodyText": "Those random generated records will be located in partition 2020-03-21 ?   I guess it's not.", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r562375183", "createdAt": "2021-01-22T04:06:32Z", "author": {"login": "openinx"}, "path": "flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkScanSql.java", "diffHunk": "@@ -106,6 +109,51 @@ public void testResiduals() throws Exception {\n     assertRecords(runWithFilter(filter, \"where dt='2020-03-20' and id=123\"), expectedRecords, SCHEMA);\n   }\n \n+  @Test\n+  public void testInferedParallelism() throws IOException {\n+    Table table = catalog.createTable(TableIdentifier.of(\"default\", \"t\"), SCHEMA, SPEC);\n+\n+    TableLoader tableLoader = TableLoader.fromHadoopTable(table.location());\n+    FlinkInputFormat flinkInputFormat = FlinkSource.forRowData().tableLoader(tableLoader).table(table).buildFormat();\n+    ScanContext scanContext = ScanContext.builder().build();\n+\n+    // Empty table ,parallelism at least 1\n+    int parallelism = FlinkSource.forRowData()\n+        .flinkConf(new Configuration())\n+        .inferParallelism(flinkInputFormat, scanContext);\n+    Assert.assertEquals(\"Should produce the expected parallelism.\", 1, parallelism);\n+\n+    List<Record> writeRecords = RandomGenericData.generate(SCHEMA, 2, 0L);\n+    writeRecords.get(0).set(1, 123L);\n+    writeRecords.get(0).set(2, \"2020-03-20\");\n+    writeRecords.get(1).set(1, 456L);\n+    writeRecords.get(1).set(2, \"2020-03-20\");\n+\n+    GenericAppenderHelper helper = new GenericAppenderHelper(table, fileFormat, TEMPORARY_FOLDER);\n+\n+    DataFile dataFile1 = helper.writeFile(TestHelpers.Row.of(\"2020-03-20\", 0), writeRecords);\n+    DataFile dataFile2 = helper.writeFile(TestHelpers.Row.of(\"2020-03-21\", 0),\n+        RandomGenericData.generate(SCHEMA, 2, 0L));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "47ae11a8a83dee8adebc3a66301a21d3652f0092"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjQ1MDAzNA==", "bodyText": "At first I copy the code from TestFlinkScanSql#testResiduals method to gererate 2 datafiles.\nI think there should be no problem about the partition. writeRecords will write to the partition 2020-03-20, and randomly generate two records into the partition 2020-03-21.\nBut for simplicity, I modified the code to randomly generate two records for each partition.", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r562450034", "createdAt": "2021-01-22T08:00:35Z", "author": {"login": "zhangjun0x01"}, "path": "flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkScanSql.java", "diffHunk": "@@ -106,6 +109,51 @@ public void testResiduals() throws Exception {\n     assertRecords(runWithFilter(filter, \"where dt='2020-03-20' and id=123\"), expectedRecords, SCHEMA);\n   }\n \n+  @Test\n+  public void testInferedParallelism() throws IOException {\n+    Table table = catalog.createTable(TableIdentifier.of(\"default\", \"t\"), SCHEMA, SPEC);\n+\n+    TableLoader tableLoader = TableLoader.fromHadoopTable(table.location());\n+    FlinkInputFormat flinkInputFormat = FlinkSource.forRowData().tableLoader(tableLoader).table(table).buildFormat();\n+    ScanContext scanContext = ScanContext.builder().build();\n+\n+    // Empty table ,parallelism at least 1\n+    int parallelism = FlinkSource.forRowData()\n+        .flinkConf(new Configuration())\n+        .inferParallelism(flinkInputFormat, scanContext);\n+    Assert.assertEquals(\"Should produce the expected parallelism.\", 1, parallelism);\n+\n+    List<Record> writeRecords = RandomGenericData.generate(SCHEMA, 2, 0L);\n+    writeRecords.get(0).set(1, 123L);\n+    writeRecords.get(0).set(2, \"2020-03-20\");\n+    writeRecords.get(1).set(1, 456L);\n+    writeRecords.get(1).set(2, \"2020-03-20\");\n+\n+    GenericAppenderHelper helper = new GenericAppenderHelper(table, fileFormat, TEMPORARY_FOLDER);\n+\n+    DataFile dataFile1 = helper.writeFile(TestHelpers.Row.of(\"2020-03-20\", 0), writeRecords);\n+    DataFile dataFile2 = helper.writeFile(TestHelpers.Row.of(\"2020-03-21\", 0),\n+        RandomGenericData.generate(SCHEMA, 2, 0L));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjM3NTE4Mw=="}, "originalCommit": {"oid": "47ae11a8a83dee8adebc3a66301a21d3652f0092"}, "originalPosition": 46}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzU0MTM2NDczOnYy", "diffSide": "RIGHT", "path": "flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkScanSql.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yMlQwNDowOTozMVrOIYUtCA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yMlQwNzo1NDo0MVrOIYZCPA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjM3NTk0NA==", "bodyText": "I think there're other test cases that we don't cover, it's good to cover those tests.\n\ntable.exec.iceberg.infer-source-parallelism=false;\ntable.exec.iceberg.infer-source-parallelism.max <= numberOfSplits;\ntable.exec.iceberg.infer-source-parallelism.max > numberOfSplits;\ntable.exec.iceberg.infer-source-parallelism.max > limit;\ntable.exec.iceberg.infer-source-parallelism.max <= limit;\n\nDivide those cases into small method if necessary.", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r562375944", "createdAt": "2021-01-22T04:09:31Z", "author": {"login": "openinx"}, "path": "flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkScanSql.java", "diffHunk": "@@ -106,6 +109,51 @@ public void testResiduals() throws Exception {\n     assertRecords(runWithFilter(filter, \"where dt='2020-03-20' and id=123\"), expectedRecords, SCHEMA);\n   }\n \n+  @Test\n+  public void testInferedParallelism() throws IOException {\n+    Table table = catalog.createTable(TableIdentifier.of(\"default\", \"t\"), SCHEMA, SPEC);\n+\n+    TableLoader tableLoader = TableLoader.fromHadoopTable(table.location());\n+    FlinkInputFormat flinkInputFormat = FlinkSource.forRowData().tableLoader(tableLoader).table(table).buildFormat();\n+    ScanContext scanContext = ScanContext.builder().build();\n+\n+    // Empty table ,parallelism at least 1\n+    int parallelism = FlinkSource.forRowData()\n+        .flinkConf(new Configuration())\n+        .inferParallelism(flinkInputFormat, scanContext);\n+    Assert.assertEquals(\"Should produce the expected parallelism.\", 1, parallelism);\n+\n+    List<Record> writeRecords = RandomGenericData.generate(SCHEMA, 2, 0L);\n+    writeRecords.get(0).set(1, 123L);\n+    writeRecords.get(0).set(2, \"2020-03-20\");\n+    writeRecords.get(1).set(1, 456L);\n+    writeRecords.get(1).set(2, \"2020-03-20\");\n+\n+    GenericAppenderHelper helper = new GenericAppenderHelper(table, fileFormat, TEMPORARY_FOLDER);\n+\n+    DataFile dataFile1 = helper.writeFile(TestHelpers.Row.of(\"2020-03-20\", 0), writeRecords);\n+    DataFile dataFile2 = helper.writeFile(TestHelpers.Row.of(\"2020-03-21\", 0),\n+        RandomGenericData.generate(SCHEMA, 2, 0L));\n+    helper.appendToTable(dataFile1, dataFile2);\n+\n+    // Make sure to generate 2 CombinedScanTasks\n+    long maxFileLen = Math.max(dataFile1.fileSizeInBytes(), dataFile2.fileSizeInBytes());\n+    executeSQL(String\n+        .format(\"ALTER TABLE t SET ('read.split.open-file-cost'='1', 'read.split.target-size'='%s')\", maxFileLen));\n+\n+    // 2 splits ,the parallelism is  2\n+    parallelism = FlinkSource.forRowData()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "47ae11a8a83dee8adebc3a66301a21d3652f0092"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjQ0NjkwOA==", "bodyText": "I add the test case , but I did not split these test cases into different methods because they share a lot of code. If they are split, there may be a lot of duplicate code.", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r562446908", "createdAt": "2021-01-22T07:54:41Z", "author": {"login": "zhangjun0x01"}, "path": "flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkScanSql.java", "diffHunk": "@@ -106,6 +109,51 @@ public void testResiduals() throws Exception {\n     assertRecords(runWithFilter(filter, \"where dt='2020-03-20' and id=123\"), expectedRecords, SCHEMA);\n   }\n \n+  @Test\n+  public void testInferedParallelism() throws IOException {\n+    Table table = catalog.createTable(TableIdentifier.of(\"default\", \"t\"), SCHEMA, SPEC);\n+\n+    TableLoader tableLoader = TableLoader.fromHadoopTable(table.location());\n+    FlinkInputFormat flinkInputFormat = FlinkSource.forRowData().tableLoader(tableLoader).table(table).buildFormat();\n+    ScanContext scanContext = ScanContext.builder().build();\n+\n+    // Empty table ,parallelism at least 1\n+    int parallelism = FlinkSource.forRowData()\n+        .flinkConf(new Configuration())\n+        .inferParallelism(flinkInputFormat, scanContext);\n+    Assert.assertEquals(\"Should produce the expected parallelism.\", 1, parallelism);\n+\n+    List<Record> writeRecords = RandomGenericData.generate(SCHEMA, 2, 0L);\n+    writeRecords.get(0).set(1, 123L);\n+    writeRecords.get(0).set(2, \"2020-03-20\");\n+    writeRecords.get(1).set(1, 456L);\n+    writeRecords.get(1).set(2, \"2020-03-20\");\n+\n+    GenericAppenderHelper helper = new GenericAppenderHelper(table, fileFormat, TEMPORARY_FOLDER);\n+\n+    DataFile dataFile1 = helper.writeFile(TestHelpers.Row.of(\"2020-03-20\", 0), writeRecords);\n+    DataFile dataFile2 = helper.writeFile(TestHelpers.Row.of(\"2020-03-21\", 0),\n+        RandomGenericData.generate(SCHEMA, 2, 0L));\n+    helper.appendToTable(dataFile1, dataFile2);\n+\n+    // Make sure to generate 2 CombinedScanTasks\n+    long maxFileLen = Math.max(dataFile1.fileSizeInBytes(), dataFile2.fileSizeInBytes());\n+    executeSQL(String\n+        .format(\"ALTER TABLE t SET ('read.split.open-file-cost'='1', 'read.split.target-size'='%s')\", maxFileLen));\n+\n+    // 2 splits ,the parallelism is  2\n+    parallelism = FlinkSource.forRowData()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjM3NTk0NA=="}, "originalCommit": {"oid": "47ae11a8a83dee8adebc3a66301a21d3652f0092"}, "originalPosition": 55}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3041, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}