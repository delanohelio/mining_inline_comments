{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTI5NTExMjE5", "number": 1852, "title": "Spark: Add plans & rules for DELETE", "bodyText": "This PR adds the core logic for executing DELETE statements in Spark.", "createdAt": "2020-11-30T12:18:26Z", "url": "https://github.com/apache/iceberg/pull/1852", "merged": true, "mergeCommit": {"oid": "dcc539aaf671b4642a8fda95ab299ac98cfc89c9"}, "closed": true, "closedAt": "2020-12-02T14:10:04Z", "author": {"login": "aokolnychyi"}, "timelineItems": {"totalCount": 19, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdhkV9LAH2gAyNTI5NTExMjE5OmM3N2M2MTMwZTg1OWFjNjcxZWRkZTk0N2QzYzI0NDUyYmNiNDgyNmE=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdh9RxAgFqTU0MjEyNjg0NA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a", "author": {"user": {"login": "aokolnychyi", "name": "Anton Okolnychyi"}}, "url": "https://github.com/apache/iceberg/commit/c77c6130e859ac671edde947d3c24452bcb4826a", "committedDate": "2020-11-30T12:17:18Z", "message": "Spark: Add plans & rules for DELETE"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQwODYyMjg5", "url": "https://github.com/apache/iceberg/pull/1852#pullrequestreview-540862289", "createdAt": "2020-11-30T12:28:56Z", "commit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxMjoyODo1NlrOH749Gg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxMjoyODo1NlrOH749Gg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjU2MTE3OA==", "bodyText": "I feel like repartition should be optional and should be configured by the user. We can control it via table properties.\nWe currently plan to add the following properties:\nwrite.delete.isolation\nwrite.delete.mode\n\nThis config may be specific to Spark so we can call it engine.spark.write.delete.align-records or similar.", "url": "https://github.com/apache/iceberg/pull/1852#discussion_r532561178", "createdAt": "2020-11-30T12:28:56Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteDelete.scala", "diffHunk": "@@ -0,0 +1,152 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import java.util.UUID\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{sources, AnalysisException}\n+import org.apache.spark.sql.catalyst.expressions.{Alias, Ascending, Attribute, AttributeReference, EqualNullSafe, Expression, InputFileName, Literal, Not, PredicateHelper, SortOrder, SubqueryExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{Aggregate, DeleteFromTable, DynamicFileFilter, Filter, LogicalPlan, Project, RepartitionByExpression, ReplaceData, Sort}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.connector.catalog.Table\n+import org.apache.spark.sql.connector.iceberg.catalog.ExtendedSupportsDelete\n+import org.apache.spark.sql.connector.iceberg.read.SupportsFileFilter\n+import org.apache.spark.sql.connector.iceberg.write.MergeBuilder\n+import org.apache.spark.sql.connector.write.{LogicalWriteInfo, LogicalWriteInfoImpl}\n+import org.apache.spark.sql.execution.datasources.DataSourceStrategy\n+import org.apache.spark.sql.execution.datasources.v2.{DataSourceV2Relation, DataSourceV2ScanRelation, PushDownUtils}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{BooleanType, StructType}\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+// TODO: should be part of early scan push down after the delete condition is optimized\n+object RewriteDelete extends Rule[LogicalPlan] with PredicateHelper with Logging {\n+\n+  import org.apache.spark.sql.execution.datasources.v2.ExtendedDataSourceV2Implicits._\n+\n+  private val FILE_NAME_COL = \"_file\"\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    // don't rewrite deletes that can be answered by passing filters to deleteWhere in SupportsDelete\n+    case d @ DeleteFromTable(r: DataSourceV2Relation, Some(cond)) if isDeleteWhereCase(r, cond) =>\n+      d\n+\n+    // rewrite all operations that require reading the table to delete records\n+    case DeleteFromTable(r: DataSourceV2Relation, Some(cond)) =>\n+      // TODO: do a switch based on whether we get BatchWrite or DeltaBatchWrite\n+      val writeInfo = newWriteInfo(r.schema)\n+      val mergeBuilder = r.table.asMergeable.newMergeBuilder(writeInfo)\n+\n+      val scanPlan = buildScanPlan(r.table, r.output, mergeBuilder, cond)\n+\n+      val remainingRowFilter = Not(EqualNullSafe(cond, Literal(true, BooleanType)))\n+      val remainingRowsPlan = Filter(remainingRowFilter, scanPlan)\n+\n+      val batchWrite = mergeBuilder.asWriteBuilder.buildForBatch()\n+      val writePlan = buildWritePlan(remainingRowsPlan, r.output)\n+      ReplaceData(r, batchWrite, writePlan)\n+  }\n+\n+  private def buildScanPlan(\n+      table: Table,\n+      output: Seq[AttributeReference],\n+      mergeBuilder: MergeBuilder,\n+      cond: Expression): LogicalPlan = {\n+\n+    val scanBuilder = mergeBuilder.asScanBuilder\n+\n+    val predicates = splitConjunctivePredicates(cond)\n+    val normalizedPredicates = DataSourceStrategy.normalizeExprs(predicates, output)\n+    PushDownUtils.pushFilters(scanBuilder, normalizedPredicates)\n+\n+    val scan = scanBuilder.build()\n+    val scanRelation = DataSourceV2ScanRelation(table, scan, output)\n+\n+    val scanPlan = scan match {\n+      case _: SupportsFileFilter =>\n+        val matchingFilePlan = buildFileFilterPlan(cond, scanRelation)\n+        val dynamicFileFilter = DynamicFileFilter(scanRelation, matchingFilePlan)\n+        dynamicFileFilter\n+      case _ =>\n+        scanRelation\n+    }\n+\n+    // include file name so that we can group data back\n+    val fileNameExpr = Alias(InputFileName(), FILE_NAME_COL)()\n+    Project(scanPlan.output :+ fileNameExpr, scanPlan)\n+  }\n+\n+  private def buildWritePlan(\n+      remainingRowsPlan: LogicalPlan,\n+      output: Seq[AttributeReference]): LogicalPlan = {\n+\n+    // TODO: sort by _pos to keep the original ordering of rows\n+    // TODO: consider setting a file size limit\n+\n+    val fileNameCol = findOutputAttr(remainingRowsPlan, FILE_NAME_COL)\n+    val numShufflePartitions = SQLConf.get.numShufflePartitions\n+    val repartition = RepartitionByExpression(Seq(fileNameCol), remainingRowsPlan, numShufflePartitions)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "originalPosition": 105}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQwOTQ2MTQ3", "url": "https://github.com/apache/iceberg/pull/1852#pullrequestreview-540946147", "createdAt": "2020-11-30T14:15:01Z", "commit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxNDoxNTowMVrOH7892g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxNDoxNTowMVrOH7892g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjYyNjkwNg==", "bodyText": "I'm a little confused why we want to fail this for all requests, not just those which might follow the Iceberg Delete pathway. Could you elaborate a little more? It looks to me like this will fail all DeleteFromTable operation regardless of whether an Iceberg table is involved.", "url": "https://github.com/apache/iceberg/pull/1852#discussion_r532626906", "createdAt": "2020-11-30T14:15:01Z", "author": {"login": "RussellSpitzer"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/DeleteFromTablePredicateCheck.scala", "diffHunk": "@@ -0,0 +1,49 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.expressions.{Expression, InSubquery, Not}\n+import org.apache.spark.sql.catalyst.plans.logical.{DeleteFromTable, LogicalPlan}\n+\n+object DeleteFromTablePredicateCheck extends (LogicalPlan => Unit) {\n+\n+  override def apply(plan: LogicalPlan): Unit = {\n+    plan foreach {\n+      case DeleteFromTable(_, Some(condition)) if hasNullAwarePredicateWithinNot(condition) =>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "originalPosition": 30}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQwOTQ5NTUw", "url": "https://github.com/apache/iceberg/pull/1852#pullrequestreview-540949550", "createdAt": "2020-11-30T14:18:45Z", "commit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxNDoxODo0NVrOH79IDA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxNDoxODo0NVrOH79IDA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjYyOTUxNg==", "bodyText": "do we have to worry about \"In\" as well? Or only InSubquery?", "url": "https://github.com/apache/iceberg/pull/1852#discussion_r532629516", "createdAt": "2020-11-30T14:18:45Z", "author": {"login": "RussellSpitzer"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/DeleteFromTablePredicateCheck.scala", "diffHunk": "@@ -0,0 +1,49 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.expressions.{Expression, InSubquery, Not}\n+import org.apache.spark.sql.catalyst.plans.logical.{DeleteFromTable, LogicalPlan}\n+\n+object DeleteFromTablePredicateCheck extends (LogicalPlan => Unit) {\n+\n+  override def apply(plan: LogicalPlan): Unit = {\n+    plan foreach {\n+      case DeleteFromTable(_, Some(condition)) if hasNullAwarePredicateWithinNot(condition) =>\n+        // this limitation is present since SPARK-25154 fix is not yet available\n+        // we use Not(EqualsNullSafe(cond, true)) when deciding which records to keep\n+        // such conditions are rewritten by Spark as an existential join and currently Spark\n+        // does not handle correctly NOT IN subqueries nested into other expressions\n+        failAnalysis(\"Null-aware predicate sub-queries are not currently supported in DELETE\")\n+\n+      case _ => // OK\n+    }\n+  }\n+\n+  private def hasNullAwarePredicateWithinNot(cond: Expression): Boolean = {\n+    cond.find {\n+      case Not(expr) if expr.find(_.isInstanceOf[InSubquery]).isDefined => true", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "originalPosition": 43}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQwOTU2MTE0", "url": "https://github.com/apache/iceberg/pull/1852#pullrequestreview-540956114", "createdAt": "2020-11-30T14:25:43Z", "commit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxNDoyNTo0M1rOH79bww==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxNDoyNTo0M1rOH79bww==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjYzNDU2Mw==", "bodyText": "Maybe just me, but I would rename these 'cons' and 'table\" to be consistent with the naming directly above.", "url": "https://github.com/apache/iceberg/pull/1852#discussion_r532634563", "createdAt": "2020-11-30T14:25:43Z", "author": {"login": "RussellSpitzer"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/OptimizeConditionsInRowLevelOperations.scala", "diffHunk": "@@ -0,0 +1,46 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.catalyst.expressions.{Expression, Literal, SubqueryExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{DeleteFromTable, Filter, LocalRelation, LogicalPlan}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanRelation\n+\n+// we have to optimize expressions used in delete/update before we can rewrite row-level operations\n+// otherwise, we will have to deal with redundant casts and will not detect noop deletes\n+// it is a temp solution since we cannot inject rewrite of row-level ops after operator optimizations\n+object OptimizeConditionsInRowLevelOperations extends Rule[LogicalPlan] {\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan transform {\n+    case d @ DeleteFromTable(table, cond) if !SubqueryExpression.hasSubquery(cond.getOrElse(Literal.TrueLiteral)) =>\n+      val optimizedCond = optimizeCondition(cond.getOrElse(Literal.TrueLiteral), table)\n+      d.copy(condition = Some(optimizedCond))\n+  }\n+\n+  private def optimizeCondition(condition: Expression, targetTable: LogicalPlan): Expression = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "originalPosition": 38}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQwOTU5MjUz", "url": "https://github.com/apache/iceberg/pull/1852#pullrequestreview-540959253", "createdAt": "2020-11-30T14:28:58Z", "commit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxNDoyODo1OVrOH79klQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxNDoyODo1OVrOH79klQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjYzNjgyMQ==", "bodyText": "Is this exhaustive? Shouldn't we also have the possibility of resolving into other relations as well?", "url": "https://github.com/apache/iceberg/pull/1852#discussion_r532636821", "createdAt": "2020-11-30T14:28:59Z", "author": {"login": "RussellSpitzer"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/OptimizeConditionsInRowLevelOperations.scala", "diffHunk": "@@ -0,0 +1,46 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.catalyst.expressions.{Expression, Literal, SubqueryExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{DeleteFromTable, Filter, LocalRelation, LogicalPlan}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanRelation\n+\n+// we have to optimize expressions used in delete/update before we can rewrite row-level operations\n+// otherwise, we will have to deal with redundant casts and will not detect noop deletes\n+// it is a temp solution since we cannot inject rewrite of row-level ops after operator optimizations\n+object OptimizeConditionsInRowLevelOperations extends Rule[LogicalPlan] {\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan transform {\n+    case d @ DeleteFromTable(table, cond) if !SubqueryExpression.hasSubquery(cond.getOrElse(Literal.TrueLiteral)) =>\n+      val optimizedCond = optimizeCondition(cond.getOrElse(Literal.TrueLiteral), table)\n+      d.copy(condition = Some(optimizedCond))\n+  }\n+\n+  private def optimizeCondition(condition: Expression, targetTable: LogicalPlan): Expression = {\n+    val optimizer = SparkSession.active.sessionState.optimizer\n+    optimizer.execute(Filter(condition, targetTable)) match {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "originalPosition": 40}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQwOTYzODg0", "url": "https://github.com/apache/iceberg/pull/1852#pullrequestreview-540963884", "createdAt": "2020-11-30T14:33:52Z", "commit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxNDozMzo1MlrOH79yOw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxNDozMzo1MlrOH79yOw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjY0MDMxNQ==", "bodyText": "The first and last case make sense here to me, If we get a Filter out of our optimization we want the new condition. If we end up with DataSourcecv2ScanRelation this is essentially a \"delete everything\" request and we pass back true.\nBut if we end up with a LocalRelation why do we want to delete nothing? Is this basically saying that we ended up trying to delete but the condition applied in the delete just results in not actually effecting a DSV2 table?", "url": "https://github.com/apache/iceberg/pull/1852#discussion_r532640315", "createdAt": "2020-11-30T14:33:52Z", "author": {"login": "RussellSpitzer"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/OptimizeConditionsInRowLevelOperations.scala", "diffHunk": "@@ -0,0 +1,46 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.catalyst.expressions.{Expression, Literal, SubqueryExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{DeleteFromTable, Filter, LocalRelation, LogicalPlan}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanRelation\n+\n+// we have to optimize expressions used in delete/update before we can rewrite row-level operations\n+// otherwise, we will have to deal with redundant casts and will not detect noop deletes\n+// it is a temp solution since we cannot inject rewrite of row-level ops after operator optimizations\n+object OptimizeConditionsInRowLevelOperations extends Rule[LogicalPlan] {\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan transform {\n+    case d @ DeleteFromTable(table, cond) if !SubqueryExpression.hasSubquery(cond.getOrElse(Literal.TrueLiteral)) =>\n+      val optimizedCond = optimizeCondition(cond.getOrElse(Literal.TrueLiteral), table)\n+      d.copy(condition = Some(optimizedCond))\n+  }\n+\n+  private def optimizeCondition(condition: Expression, targetTable: LogicalPlan): Expression = {\n+    val optimizer = SparkSession.active.sessionState.optimizer\n+    optimizer.execute(Filter(condition, targetTable)) match {\n+      case Filter(optimizedCondition, _) => optimizedCondition", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "originalPosition": 41}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQwOTgyMTUw", "url": "https://github.com/apache/iceberg/pull/1852#pullrequestreview-540982150", "createdAt": "2020-11-30T14:52:32Z", "commit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxNDo1MjozMlrOH7-pgQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxNDo1MjozMlrOH7-pgQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjY1NDQ2NQ==", "bodyText": "This \"d\" is lonely", "url": "https://github.com/apache/iceberg/pull/1852#discussion_r532654465", "createdAt": "2020-11-30T14:52:32Z", "author": {"login": "RussellSpitzer"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteDelete.scala", "diffHunk": "@@ -0,0 +1,152 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import java.util.UUID\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{sources, AnalysisException}\n+import org.apache.spark.sql.catalyst.expressions.{Alias, Ascending, Attribute, AttributeReference, EqualNullSafe, Expression, InputFileName, Literal, Not, PredicateHelper, SortOrder, SubqueryExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{Aggregate, DeleteFromTable, DynamicFileFilter, Filter, LogicalPlan, Project, RepartitionByExpression, ReplaceData, Sort}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.connector.catalog.Table\n+import org.apache.spark.sql.connector.iceberg.catalog.ExtendedSupportsDelete\n+import org.apache.spark.sql.connector.iceberg.read.SupportsFileFilter\n+import org.apache.spark.sql.connector.iceberg.write.MergeBuilder\n+import org.apache.spark.sql.connector.write.{LogicalWriteInfo, LogicalWriteInfoImpl}\n+import org.apache.spark.sql.execution.datasources.DataSourceStrategy\n+import org.apache.spark.sql.execution.datasources.v2.{DataSourceV2Relation, DataSourceV2ScanRelation, PushDownUtils}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{BooleanType, StructType}\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+// TODO: should be part of early scan push down after the delete condition is optimized\n+object RewriteDelete extends Rule[LogicalPlan] with PredicateHelper with Logging {\n+\n+  import org.apache.spark.sql.execution.datasources.v2.ExtendedDataSourceV2Implicits._\n+\n+  private val FILE_NAME_COL = \"_file\"\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    // don't rewrite deletes that can be answered by passing filters to deleteWhere in SupportsDelete\n+    case d @ DeleteFromTable(r: DataSourceV2Relation, Some(cond)) if isDeleteWhereCase(r, cond) =>\n+      d", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "originalPosition": 49}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQwOTg3OTQ1", "url": "https://github.com/apache/iceberg/pull/1852#pullrequestreview-540987945", "createdAt": "2020-11-30T14:58:15Z", "commit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxNDo1ODoxNVrOH7-6uQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxNDo1ODoxNVrOH7-6uQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjY1ODg3Mw==", "bodyText": "maybe this should be \"mergeWrite\"?", "url": "https://github.com/apache/iceberg/pull/1852#discussion_r532658873", "createdAt": "2020-11-30T14:58:15Z", "author": {"login": "RussellSpitzer"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteDelete.scala", "diffHunk": "@@ -0,0 +1,152 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import java.util.UUID\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{sources, AnalysisException}\n+import org.apache.spark.sql.catalyst.expressions.{Alias, Ascending, Attribute, AttributeReference, EqualNullSafe, Expression, InputFileName, Literal, Not, PredicateHelper, SortOrder, SubqueryExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{Aggregate, DeleteFromTable, DynamicFileFilter, Filter, LogicalPlan, Project, RepartitionByExpression, ReplaceData, Sort}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.connector.catalog.Table\n+import org.apache.spark.sql.connector.iceberg.catalog.ExtendedSupportsDelete\n+import org.apache.spark.sql.connector.iceberg.read.SupportsFileFilter\n+import org.apache.spark.sql.connector.iceberg.write.MergeBuilder\n+import org.apache.spark.sql.connector.write.{LogicalWriteInfo, LogicalWriteInfoImpl}\n+import org.apache.spark.sql.execution.datasources.DataSourceStrategy\n+import org.apache.spark.sql.execution.datasources.v2.{DataSourceV2Relation, DataSourceV2ScanRelation, PushDownUtils}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{BooleanType, StructType}\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+// TODO: should be part of early scan push down after the delete condition is optimized\n+object RewriteDelete extends Rule[LogicalPlan] with PredicateHelper with Logging {\n+\n+  import org.apache.spark.sql.execution.datasources.v2.ExtendedDataSourceV2Implicits._\n+\n+  private val FILE_NAME_COL = \"_file\"\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    // don't rewrite deletes that can be answered by passing filters to deleteWhere in SupportsDelete\n+    case d @ DeleteFromTable(r: DataSourceV2Relation, Some(cond)) if isDeleteWhereCase(r, cond) =>\n+      d\n+\n+    // rewrite all operations that require reading the table to delete records\n+    case DeleteFromTable(r: DataSourceV2Relation, Some(cond)) =>\n+      // TODO: do a switch based on whether we get BatchWrite or DeltaBatchWrite\n+      val writeInfo = newWriteInfo(r.schema)\n+      val mergeBuilder = r.table.asMergeable.newMergeBuilder(writeInfo)\n+\n+      val scanPlan = buildScanPlan(r.table, r.output, mergeBuilder, cond)\n+\n+      val remainingRowFilter = Not(EqualNullSafe(cond, Literal(true, BooleanType)))\n+      val remainingRowsPlan = Filter(remainingRowFilter, scanPlan)\n+\n+      val batchWrite = mergeBuilder.asWriteBuilder.buildForBatch()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "originalPosition": 62}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQwOTg4MTU3", "url": "https://github.com/apache/iceberg/pull/1852#pullrequestreview-540988157", "createdAt": "2020-11-30T14:58:26Z", "commit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxNDo1ODoyN1rOH7-7Wg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxNDo1ODoyN1rOH7-7Wg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjY1OTAzNA==", "bodyText": "and this remainingWrite", "url": "https://github.com/apache/iceberg/pull/1852#discussion_r532659034", "createdAt": "2020-11-30T14:58:27Z", "author": {"login": "RussellSpitzer"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteDelete.scala", "diffHunk": "@@ -0,0 +1,152 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import java.util.UUID\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{sources, AnalysisException}\n+import org.apache.spark.sql.catalyst.expressions.{Alias, Ascending, Attribute, AttributeReference, EqualNullSafe, Expression, InputFileName, Literal, Not, PredicateHelper, SortOrder, SubqueryExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{Aggregate, DeleteFromTable, DynamicFileFilter, Filter, LogicalPlan, Project, RepartitionByExpression, ReplaceData, Sort}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.connector.catalog.Table\n+import org.apache.spark.sql.connector.iceberg.catalog.ExtendedSupportsDelete\n+import org.apache.spark.sql.connector.iceberg.read.SupportsFileFilter\n+import org.apache.spark.sql.connector.iceberg.write.MergeBuilder\n+import org.apache.spark.sql.connector.write.{LogicalWriteInfo, LogicalWriteInfoImpl}\n+import org.apache.spark.sql.execution.datasources.DataSourceStrategy\n+import org.apache.spark.sql.execution.datasources.v2.{DataSourceV2Relation, DataSourceV2ScanRelation, PushDownUtils}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{BooleanType, StructType}\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+// TODO: should be part of early scan push down after the delete condition is optimized\n+object RewriteDelete extends Rule[LogicalPlan] with PredicateHelper with Logging {\n+\n+  import org.apache.spark.sql.execution.datasources.v2.ExtendedDataSourceV2Implicits._\n+\n+  private val FILE_NAME_COL = \"_file\"\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    // don't rewrite deletes that can be answered by passing filters to deleteWhere in SupportsDelete\n+    case d @ DeleteFromTable(r: DataSourceV2Relation, Some(cond)) if isDeleteWhereCase(r, cond) =>\n+      d\n+\n+    // rewrite all operations that require reading the table to delete records\n+    case DeleteFromTable(r: DataSourceV2Relation, Some(cond)) =>\n+      // TODO: do a switch based on whether we get BatchWrite or DeltaBatchWrite\n+      val writeInfo = newWriteInfo(r.schema)\n+      val mergeBuilder = r.table.asMergeable.newMergeBuilder(writeInfo)\n+\n+      val scanPlan = buildScanPlan(r.table, r.output, mergeBuilder, cond)\n+\n+      val remainingRowFilter = Not(EqualNullSafe(cond, Literal(true, BooleanType)))\n+      val remainingRowsPlan = Filter(remainingRowFilter, scanPlan)\n+\n+      val batchWrite = mergeBuilder.asWriteBuilder.buildForBatch()\n+      val writePlan = buildWritePlan(remainingRowsPlan, r.output)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "originalPosition": 63}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQwOTk1MjI5", "url": "https://github.com/apache/iceberg/pull/1852#pullrequestreview-540995229", "createdAt": "2020-11-30T15:05:39Z", "commit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxNTowNTozOVrOH7_P_g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxNTowNTozOVrOH7_P_g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjY2NDMxOA==", "bodyText": "This could potentially be much larger than the row data we are moving around, Maybe not important now but we may want to just switch this to a hash of the file name? or something small?", "url": "https://github.com/apache/iceberg/pull/1852#discussion_r532664318", "createdAt": "2020-11-30T15:05:39Z", "author": {"login": "RussellSpitzer"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteDelete.scala", "diffHunk": "@@ -0,0 +1,152 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import java.util.UUID\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{sources, AnalysisException}\n+import org.apache.spark.sql.catalyst.expressions.{Alias, Ascending, Attribute, AttributeReference, EqualNullSafe, Expression, InputFileName, Literal, Not, PredicateHelper, SortOrder, SubqueryExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{Aggregate, DeleteFromTable, DynamicFileFilter, Filter, LogicalPlan, Project, RepartitionByExpression, ReplaceData, Sort}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.connector.catalog.Table\n+import org.apache.spark.sql.connector.iceberg.catalog.ExtendedSupportsDelete\n+import org.apache.spark.sql.connector.iceberg.read.SupportsFileFilter\n+import org.apache.spark.sql.connector.iceberg.write.MergeBuilder\n+import org.apache.spark.sql.connector.write.{LogicalWriteInfo, LogicalWriteInfoImpl}\n+import org.apache.spark.sql.execution.datasources.DataSourceStrategy\n+import org.apache.spark.sql.execution.datasources.v2.{DataSourceV2Relation, DataSourceV2ScanRelation, PushDownUtils}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{BooleanType, StructType}\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+// TODO: should be part of early scan push down after the delete condition is optimized\n+object RewriteDelete extends Rule[LogicalPlan] with PredicateHelper with Logging {\n+\n+  import org.apache.spark.sql.execution.datasources.v2.ExtendedDataSourceV2Implicits._\n+\n+  private val FILE_NAME_COL = \"_file\"\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    // don't rewrite deletes that can be answered by passing filters to deleteWhere in SupportsDelete\n+    case d @ DeleteFromTable(r: DataSourceV2Relation, Some(cond)) if isDeleteWhereCase(r, cond) =>\n+      d\n+\n+    // rewrite all operations that require reading the table to delete records\n+    case DeleteFromTable(r: DataSourceV2Relation, Some(cond)) =>\n+      // TODO: do a switch based on whether we get BatchWrite or DeltaBatchWrite\n+      val writeInfo = newWriteInfo(r.schema)\n+      val mergeBuilder = r.table.asMergeable.newMergeBuilder(writeInfo)\n+\n+      val scanPlan = buildScanPlan(r.table, r.output, mergeBuilder, cond)\n+\n+      val remainingRowFilter = Not(EqualNullSafe(cond, Literal(true, BooleanType)))\n+      val remainingRowsPlan = Filter(remainingRowFilter, scanPlan)\n+\n+      val batchWrite = mergeBuilder.asWriteBuilder.buildForBatch()\n+      val writePlan = buildWritePlan(remainingRowsPlan, r.output)\n+      ReplaceData(r, batchWrite, writePlan)\n+  }\n+\n+  private def buildScanPlan(\n+      table: Table,\n+      output: Seq[AttributeReference],\n+      mergeBuilder: MergeBuilder,\n+      cond: Expression): LogicalPlan = {\n+\n+    val scanBuilder = mergeBuilder.asScanBuilder\n+\n+    val predicates = splitConjunctivePredicates(cond)\n+    val normalizedPredicates = DataSourceStrategy.normalizeExprs(predicates, output)\n+    PushDownUtils.pushFilters(scanBuilder, normalizedPredicates)\n+\n+    val scan = scanBuilder.build()\n+    val scanRelation = DataSourceV2ScanRelation(table, scan, output)\n+\n+    val scanPlan = scan match {\n+      case _: SupportsFileFilter =>\n+        val matchingFilePlan = buildFileFilterPlan(cond, scanRelation)\n+        val dynamicFileFilter = DynamicFileFilter(scanRelation, matchingFilePlan)\n+        dynamicFileFilter\n+      case _ =>\n+        scanRelation\n+    }\n+\n+    // include file name so that we can group data back\n+    val fileNameExpr = Alias(InputFileName(), FILE_NAME_COL)()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "originalPosition": 92}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQwOTk3NDgw", "url": "https://github.com/apache/iceberg/pull/1852#pullrequestreview-540997480", "createdAt": "2020-11-30T15:07:54Z", "commit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxNTowNzo1NFrOH7_WoQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxNTowNzo1NFrOH7_WoQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjY2NjAxNw==", "bodyText": "What filters won't we be able to translate?", "url": "https://github.com/apache/iceberg/pull/1852#discussion_r532666017", "createdAt": "2020-11-30T15:07:54Z", "author": {"login": "RussellSpitzer"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteDelete.scala", "diffHunk": "@@ -0,0 +1,152 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import java.util.UUID\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{sources, AnalysisException}\n+import org.apache.spark.sql.catalyst.expressions.{Alias, Ascending, Attribute, AttributeReference, EqualNullSafe, Expression, InputFileName, Literal, Not, PredicateHelper, SortOrder, SubqueryExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{Aggregate, DeleteFromTable, DynamicFileFilter, Filter, LogicalPlan, Project, RepartitionByExpression, ReplaceData, Sort}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.connector.catalog.Table\n+import org.apache.spark.sql.connector.iceberg.catalog.ExtendedSupportsDelete\n+import org.apache.spark.sql.connector.iceberg.read.SupportsFileFilter\n+import org.apache.spark.sql.connector.iceberg.write.MergeBuilder\n+import org.apache.spark.sql.connector.write.{LogicalWriteInfo, LogicalWriteInfoImpl}\n+import org.apache.spark.sql.execution.datasources.DataSourceStrategy\n+import org.apache.spark.sql.execution.datasources.v2.{DataSourceV2Relation, DataSourceV2ScanRelation, PushDownUtils}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{BooleanType, StructType}\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+// TODO: should be part of early scan push down after the delete condition is optimized\n+object RewriteDelete extends Rule[LogicalPlan] with PredicateHelper with Logging {\n+\n+  import org.apache.spark.sql.execution.datasources.v2.ExtendedDataSourceV2Implicits._\n+\n+  private val FILE_NAME_COL = \"_file\"\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    // don't rewrite deletes that can be answered by passing filters to deleteWhere in SupportsDelete\n+    case d @ DeleteFromTable(r: DataSourceV2Relation, Some(cond)) if isDeleteWhereCase(r, cond) =>\n+      d\n+\n+    // rewrite all operations that require reading the table to delete records\n+    case DeleteFromTable(r: DataSourceV2Relation, Some(cond)) =>\n+      // TODO: do a switch based on whether we get BatchWrite or DeltaBatchWrite\n+      val writeInfo = newWriteInfo(r.schema)\n+      val mergeBuilder = r.table.asMergeable.newMergeBuilder(writeInfo)\n+\n+      val scanPlan = buildScanPlan(r.table, r.output, mergeBuilder, cond)\n+\n+      val remainingRowFilter = Not(EqualNullSafe(cond, Literal(true, BooleanType)))\n+      val remainingRowsPlan = Filter(remainingRowFilter, scanPlan)\n+\n+      val batchWrite = mergeBuilder.asWriteBuilder.buildForBatch()\n+      val writePlan = buildWritePlan(remainingRowsPlan, r.output)\n+      ReplaceData(r, batchWrite, writePlan)\n+  }\n+\n+  private def buildScanPlan(\n+      table: Table,\n+      output: Seq[AttributeReference],\n+      mergeBuilder: MergeBuilder,\n+      cond: Expression): LogicalPlan = {\n+\n+    val scanBuilder = mergeBuilder.asScanBuilder\n+\n+    val predicates = splitConjunctivePredicates(cond)\n+    val normalizedPredicates = DataSourceStrategy.normalizeExprs(predicates, output)\n+    PushDownUtils.pushFilters(scanBuilder, normalizedPredicates)\n+\n+    val scan = scanBuilder.build()\n+    val scanRelation = DataSourceV2ScanRelation(table, scan, output)\n+\n+    val scanPlan = scan match {\n+      case _: SupportsFileFilter =>\n+        val matchingFilePlan = buildFileFilterPlan(cond, scanRelation)\n+        val dynamicFileFilter = DynamicFileFilter(scanRelation, matchingFilePlan)\n+        dynamicFileFilter\n+      case _ =>\n+        scanRelation\n+    }\n+\n+    // include file name so that we can group data back\n+    val fileNameExpr = Alias(InputFileName(), FILE_NAME_COL)()\n+    Project(scanPlan.output :+ fileNameExpr, scanPlan)\n+  }\n+\n+  private def buildWritePlan(\n+      remainingRowsPlan: LogicalPlan,\n+      output: Seq[AttributeReference]): LogicalPlan = {\n+\n+    // TODO: sort by _pos to keep the original ordering of rows\n+    // TODO: consider setting a file size limit\n+\n+    val fileNameCol = findOutputAttr(remainingRowsPlan, FILE_NAME_COL)\n+    val numShufflePartitions = SQLConf.get.numShufflePartitions\n+    val repartition = RepartitionByExpression(Seq(fileNameCol), remainingRowsPlan, numShufflePartitions)\n+    val sort = Sort(Seq(SortOrder(fileNameCol, Ascending)), global = false, repartition)\n+    Project(output, sort)\n+  }\n+\n+  private def isDeleteWhereCase(relation: DataSourceV2Relation, cond: Expression): Boolean = {\n+    relation.table match {\n+      case t: ExtendedSupportsDelete if !SubqueryExpression.hasSubquery(cond) =>\n+        val predicates = splitConjunctivePredicates(cond)\n+        val normalizedPredicates = DataSourceStrategy.normalizeExprs(predicates, relation.output)\n+        val dataSourceFilters = toDataSourceFilters(normalizedPredicates)\n+        val allPredicatesTranslated = normalizedPredicates.size == dataSourceFilters.length", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "originalPosition": 116}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQxMDAwMTU3", "url": "https://github.com/apache/iceberg/pull/1852#pullrequestreview-541000157", "createdAt": "2020-11-30T15:10:37Z", "commit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxNToxMDozOFrOH7_ejQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxNToxMDozOFrOH7_ejQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjY2ODA0NQ==", "bodyText": "Maybe NonCachingBatchScanExec?", "url": "https://github.com/apache/iceberg/pull/1852#discussion_r532668045", "createdAt": "2020-11-30T15:10:38Z", "author": {"login": "RussellSpitzer"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/ExtendedBatchScanExec.scala", "diffHunk": "@@ -0,0 +1,58 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.AttributeReference\n+import org.apache.spark.sql.catalyst.plans.QueryPlan\n+import org.apache.spark.sql.connector.read.{InputPartition, PartitionReaderFactory, Scan}\n+\n+// The only reason we need this class and cannot reuse BatchScanExec is because\n+// BatchScanExec caches input partitions and we cannot apply file filtering before execution\n+// Spark calls supportsColumnar during physical planning which, in turn, triggers split planning\n+// We must ensure the result is not cached so that we can push down file filters later\n+// The only difference compared to BatchScanExec is that we are using def instead of lazy val for splits\n+case class ExtendedBatchScanExec(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "originalPosition": 33}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQxMDAxOTc3", "url": "https://github.com/apache/iceberg/pull/1852#pullrequestreview-541001977", "createdAt": "2020-11-30T15:12:27Z", "commit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxNToxMjoyN1rOH7_j5A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxNToxMjoyN1rOH7_j5A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjY2OTQxMg==", "bodyText": "Currently this is just \"supports row level merge and delete\" so maybe we should detail that instead of \"row level operations\" which I feel like is a bit more vague.", "url": "https://github.com/apache/iceberg/pull/1852#discussion_r532669412", "createdAt": "2020-11-30T15:12:27Z", "author": {"login": "RussellSpitzer"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/ExtendedDataSourceV2Implicits.scala", "diffHunk": "@@ -0,0 +1,38 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.connector.catalog.Table\n+import org.apache.spark.sql.connector.iceberg.catalog.SupportsMerge\n+\n+// must be merged with DataSourceV2Implicits in Spark\n+object ExtendedDataSourceV2Implicits {\n+  implicit class TableHelper(table: Table) {\n+    def asMergeable: SupportsMerge = {\n+      table match {\n+        case support: SupportsMerge =>\n+          support\n+        case _ =>\n+          throw new AnalysisException(s\"Table does not support row level operations: ${table.name}\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "originalPosition": 34}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQxMDA0MDI0", "url": "https://github.com/apache/iceberg/pull/1852#pullrequestreview-541004024", "createdAt": "2020-11-30T15:14:31Z", "commit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQxMjk4Nzk2", "url": "https://github.com/apache/iceberg/pull/1852#pullrequestreview-541298796", "createdAt": "2020-11-30T21:13:12Z", "commit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQyMToxMzoxM1rOH8ODZw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQyMToxMzoxM1rOH8ODZw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjkwNjg1NQ==", "bodyText": "What about isMetadataDelete? I think that's more clear why we would not rewrite the plan. All of these plans are technically DELETE FROM ... WHERE.", "url": "https://github.com/apache/iceberg/pull/1852#discussion_r532906855", "createdAt": "2020-11-30T21:13:13Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteDelete.scala", "diffHunk": "@@ -0,0 +1,152 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import java.util.UUID\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{sources, AnalysisException}\n+import org.apache.spark.sql.catalyst.expressions.{Alias, Ascending, Attribute, AttributeReference, EqualNullSafe, Expression, InputFileName, Literal, Not, PredicateHelper, SortOrder, SubqueryExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{Aggregate, DeleteFromTable, DynamicFileFilter, Filter, LogicalPlan, Project, RepartitionByExpression, ReplaceData, Sort}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.connector.catalog.Table\n+import org.apache.spark.sql.connector.iceberg.catalog.ExtendedSupportsDelete\n+import org.apache.spark.sql.connector.iceberg.read.SupportsFileFilter\n+import org.apache.spark.sql.connector.iceberg.write.MergeBuilder\n+import org.apache.spark.sql.connector.write.{LogicalWriteInfo, LogicalWriteInfoImpl}\n+import org.apache.spark.sql.execution.datasources.DataSourceStrategy\n+import org.apache.spark.sql.execution.datasources.v2.{DataSourceV2Relation, DataSourceV2ScanRelation, PushDownUtils}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{BooleanType, StructType}\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+// TODO: should be part of early scan push down after the delete condition is optimized\n+object RewriteDelete extends Rule[LogicalPlan] with PredicateHelper with Logging {\n+\n+  import org.apache.spark.sql.execution.datasources.v2.ExtendedDataSourceV2Implicits._\n+\n+  private val FILE_NAME_COL = \"_file\"\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    // don't rewrite deletes that can be answered by passing filters to deleteWhere in SupportsDelete\n+    case d @ DeleteFromTable(r: DataSourceV2Relation, Some(cond)) if isDeleteWhereCase(r, cond) =>\n+      d\n+\n+    // rewrite all operations that require reading the table to delete records\n+    case DeleteFromTable(r: DataSourceV2Relation, Some(cond)) =>\n+      // TODO: do a switch based on whether we get BatchWrite or DeltaBatchWrite\n+      val writeInfo = newWriteInfo(r.schema)\n+      val mergeBuilder = r.table.asMergeable.newMergeBuilder(writeInfo)\n+\n+      val scanPlan = buildScanPlan(r.table, r.output, mergeBuilder, cond)\n+\n+      val remainingRowFilter = Not(EqualNullSafe(cond, Literal(true, BooleanType)))\n+      val remainingRowsPlan = Filter(remainingRowFilter, scanPlan)\n+\n+      val batchWrite = mergeBuilder.asWriteBuilder.buildForBatch()\n+      val writePlan = buildWritePlan(remainingRowsPlan, r.output)\n+      ReplaceData(r, batchWrite, writePlan)\n+  }\n+\n+  private def buildScanPlan(\n+      table: Table,\n+      output: Seq[AttributeReference],\n+      mergeBuilder: MergeBuilder,\n+      cond: Expression): LogicalPlan = {\n+\n+    val scanBuilder = mergeBuilder.asScanBuilder\n+\n+    val predicates = splitConjunctivePredicates(cond)\n+    val normalizedPredicates = DataSourceStrategy.normalizeExprs(predicates, output)\n+    PushDownUtils.pushFilters(scanBuilder, normalizedPredicates)\n+\n+    val scan = scanBuilder.build()\n+    val scanRelation = DataSourceV2ScanRelation(table, scan, output)\n+\n+    val scanPlan = scan match {\n+      case _: SupportsFileFilter =>\n+        val matchingFilePlan = buildFileFilterPlan(cond, scanRelation)\n+        val dynamicFileFilter = DynamicFileFilter(scanRelation, matchingFilePlan)\n+        dynamicFileFilter\n+      case _ =>\n+        scanRelation\n+    }\n+\n+    // include file name so that we can group data back\n+    val fileNameExpr = Alias(InputFileName(), FILE_NAME_COL)()\n+    Project(scanPlan.output :+ fileNameExpr, scanPlan)\n+  }\n+\n+  private def buildWritePlan(\n+      remainingRowsPlan: LogicalPlan,\n+      output: Seq[AttributeReference]): LogicalPlan = {\n+\n+    // TODO: sort by _pos to keep the original ordering of rows\n+    // TODO: consider setting a file size limit\n+\n+    val fileNameCol = findOutputAttr(remainingRowsPlan, FILE_NAME_COL)\n+    val numShufflePartitions = SQLConf.get.numShufflePartitions\n+    val repartition = RepartitionByExpression(Seq(fileNameCol), remainingRowsPlan, numShufflePartitions)\n+    val sort = Sort(Seq(SortOrder(fileNameCol, Ascending)), global = false, repartition)\n+    Project(output, sort)\n+  }\n+\n+  private def isDeleteWhereCase(relation: DataSourceV2Relation, cond: Expression): Boolean = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "originalPosition": 110}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQxOTIwOTYy", "url": "https://github.com/apache/iceberg/pull/1852#pullrequestreview-541920962", "createdAt": "2020-12-01T14:01:21Z", "commit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxNDowMToyMVrOH8t4sg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxNDowMToyMVrOH8t4sg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzQyODQwMg==", "bodyText": "I want to bring attention to these TODOs. It would require a bit of effort for the first one but the second one is something we can do now. The main question is whether we want to allow any extra size overhead before closing files. For example, there may be a file with 1.1 GB of data and our target size can be 1GB. Having a hard limit would mean we will cut 1.1 GB file into two.", "url": "https://github.com/apache/iceberg/pull/1852#discussion_r533428402", "createdAt": "2020-12-01T14:01:21Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteDelete.scala", "diffHunk": "@@ -0,0 +1,152 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import java.util.UUID\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{sources, AnalysisException}\n+import org.apache.spark.sql.catalyst.expressions.{Alias, Ascending, Attribute, AttributeReference, EqualNullSafe, Expression, InputFileName, Literal, Not, PredicateHelper, SortOrder, SubqueryExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{Aggregate, DeleteFromTable, DynamicFileFilter, Filter, LogicalPlan, Project, RepartitionByExpression, ReplaceData, Sort}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.connector.catalog.Table\n+import org.apache.spark.sql.connector.iceberg.catalog.ExtendedSupportsDelete\n+import org.apache.spark.sql.connector.iceberg.read.SupportsFileFilter\n+import org.apache.spark.sql.connector.iceberg.write.MergeBuilder\n+import org.apache.spark.sql.connector.write.{LogicalWriteInfo, LogicalWriteInfoImpl}\n+import org.apache.spark.sql.execution.datasources.DataSourceStrategy\n+import org.apache.spark.sql.execution.datasources.v2.{DataSourceV2Relation, DataSourceV2ScanRelation, PushDownUtils}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{BooleanType, StructType}\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+// TODO: should be part of early scan push down after the delete condition is optimized\n+object RewriteDelete extends Rule[LogicalPlan] with PredicateHelper with Logging {\n+\n+  import org.apache.spark.sql.execution.datasources.v2.ExtendedDataSourceV2Implicits._\n+\n+  private val FILE_NAME_COL = \"_file\"\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    // don't rewrite deletes that can be answered by passing filters to deleteWhere in SupportsDelete\n+    case d @ DeleteFromTable(r: DataSourceV2Relation, Some(cond)) if isDeleteWhereCase(r, cond) =>\n+      d\n+\n+    // rewrite all operations that require reading the table to delete records\n+    case DeleteFromTable(r: DataSourceV2Relation, Some(cond)) =>\n+      // TODO: do a switch based on whether we get BatchWrite or DeltaBatchWrite\n+      val writeInfo = newWriteInfo(r.schema)\n+      val mergeBuilder = r.table.asMergeable.newMergeBuilder(writeInfo)\n+\n+      val scanPlan = buildScanPlan(r.table, r.output, mergeBuilder, cond)\n+\n+      val remainingRowFilter = Not(EqualNullSafe(cond, Literal(true, BooleanType)))\n+      val remainingRowsPlan = Filter(remainingRowFilter, scanPlan)\n+\n+      val batchWrite = mergeBuilder.asWriteBuilder.buildForBatch()\n+      val writePlan = buildWritePlan(remainingRowsPlan, r.output)\n+      ReplaceData(r, batchWrite, writePlan)\n+  }\n+\n+  private def buildScanPlan(\n+      table: Table,\n+      output: Seq[AttributeReference],\n+      mergeBuilder: MergeBuilder,\n+      cond: Expression): LogicalPlan = {\n+\n+    val scanBuilder = mergeBuilder.asScanBuilder\n+\n+    val predicates = splitConjunctivePredicates(cond)\n+    val normalizedPredicates = DataSourceStrategy.normalizeExprs(predicates, output)\n+    PushDownUtils.pushFilters(scanBuilder, normalizedPredicates)\n+\n+    val scan = scanBuilder.build()\n+    val scanRelation = DataSourceV2ScanRelation(table, scan, output)\n+\n+    val scanPlan = scan match {\n+      case _: SupportsFileFilter =>\n+        val matchingFilePlan = buildFileFilterPlan(cond, scanRelation)\n+        val dynamicFileFilter = DynamicFileFilter(scanRelation, matchingFilePlan)\n+        dynamicFileFilter\n+      case _ =>\n+        scanRelation\n+    }\n+\n+    // include file name so that we can group data back\n+    val fileNameExpr = Alias(InputFileName(), FILE_NAME_COL)()\n+    Project(scanPlan.output :+ fileNameExpr, scanPlan)\n+  }\n+\n+  private def buildWritePlan(\n+      remainingRowsPlan: LogicalPlan,\n+      output: Seq[AttributeReference]): LogicalPlan = {\n+\n+    // TODO: sort by _pos to keep the original ordering of rows\n+    // TODO: consider setting a file size limit", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "originalPosition": 101}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cff020bfa8c9d995eaee4ed6a171405f1854653f", "author": {"user": {"login": "aokolnychyi", "name": "Anton Okolnychyi"}}, "url": "https://github.com/apache/iceberg/commit/cff020bfa8c9d995eaee4ed6a171405f1854653f", "committedDate": "2020-12-01T14:05:44Z", "message": "Review round 1"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQyMTI2ODQ0", "url": "https://github.com/apache/iceberg/pull/1852#pullrequestreview-542126844", "createdAt": "2020-12-01T17:20:21Z", "commit": {"oid": "cff020bfa8c9d995eaee4ed6a171405f1854653f"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3466, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}