{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzcyNTI3NDg0", "number": 2040, "title": "fix #1992 Reimplement boundedElasticScheduler with reentrancy but less efficient work stealing", "bodyText": "This should fix #1973 and fix #1992\nThe general idea is to abandon the facade Worker and instead always submit tasks to an executor-backed worker. In order of preference, when an operator requests a Worker:\n\nif thread cap not reached, create and pick a new worker\nelse if idle workers, pick an idle worker\nelse pick a busy worker\n\nThis implies a behavior under contention that is closer to parallel(), with a pool that is expected to be quite larger than the typical parallel pool. Livelocks can still happen but they seem to be far too frequent with the current implementation anyway.\nThe drawback is that once we get to pick a busy worker, there's no telling when its tasks (typically blocking tasks for a BoundedElasticScheduler) will finish. So even though another executor might become idle in the meantime, the operator's tasks will be pinned to the (potentially still busy) executor initially picked.\nTo try to counter that effect a bit, we use a priority queue for the busy executors, favoring executors that are tied to less Workers (and thus operators). We don't yet go as far as factoring in the task queue of each executor.\nFinally, one noticeable change is that the second int parameter in the API, maxPendingTask, is now influencing EACH executor's queue, and is not a shared counter. It should be safe in the sense that the number set with previous version in mind is bound to be over-dimensionned for the new version, but it would be recommended for users to scale that number back.", "createdAt": "2020-02-07T17:46:20Z", "url": "https://github.com/reactor/reactor-core/pull/2040", "merged": true, "mergeCommit": {"oid": "c0a7bb8f4fce039dea2e40e7b9f6c06e9762aa28"}, "closed": true, "closedAt": "2020-02-18T10:40:08Z", "author": {"login": "simonbasle"}, "timelineItems": {"totalCount": 18, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcCDDqngBqjMwMTg1MzI5Njg=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcFP2EMABqjMwNDQzNTMxNjY=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "4af677dde6594d8b99215fc017fc327d6cdd59d3", "author": {"user": {"login": "simonbasle", "name": "Simon Basl\u00e9"}}, "url": "https://github.com/reactor/reactor-core/commit/4af677dde6594d8b99215fc017fc327d6cdd59d3", "committedDate": "2020-02-06T16:33:42Z", "message": "WIP eviction test"}, "afterCommit": {"oid": "6e0d168d8f6c8a49f7a47e1f2729ff9f3aba6303", "author": {"user": {"login": "simonbasle", "name": "Simon Basl\u00e9"}}, "url": "https://github.com/reactor/reactor-core/commit/6e0d168d8f6c8a49f7a47e1f2729ff9f3aba6303", "committedDate": "2020-02-07T17:47:21Z", "message": "WIP eviction test"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU2MDkxNjc2", "url": "https://github.com/reactor/reactor-core/pull/2040#pullrequestreview-356091676", "createdAt": "2020-02-10T16:44:14Z", "commit": {"oid": "71598282865e36d03c459deabe2e95c8faf9edca"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMFQxNjo0NDoxNFrOFntVjg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMFQxNjo0NDoxNFrOFntVjg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzE4MTU4Mg==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             * Copyright (c) 2011-2017 Pivotal Software Inc, All Rights Reserved.\n          \n          \n            \n             * Copyright (c) 2011-Present Pivotal Software Inc, All Rights Reserved.", "url": "https://github.com/reactor/reactor-core/pull/2040#discussion_r377181582", "createdAt": "2020-02-10T16:44:14Z", "author": {"login": "bsideup"}, "path": "reactor-core/src/main/java/reactor/core/scheduler/BoundedElasticScheduler.java", "diffHunk": "@@ -1,315 +1,239 @@\n /*\n- * Copyright (c) 2011-Present Pivotal Software Inc, All Rights Reserved.\n+ * Copyright (c) 2011-2017 Pivotal Software Inc, All Rights Reserved.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "71598282865e36d03c459deabe2e95c8faf9edca"}, "originalPosition": 3}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU2MDk0NTEz", "url": "https://github.com/reactor/reactor-core/pull/2040#pullrequestreview-356094513", "createdAt": "2020-02-10T16:47:45Z", "commit": {"oid": "71598282865e36d03c459deabe2e95c8faf9edca"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMFQxNjo0Nzo0NVrOFnteWQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMFQxNjo0Nzo0NVrOFnteWQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzE4MzgzMw==", "bodyText": "Educational question: can't we just refer the (volatile) field here? Why do we need to get it via the updater?", "url": "https://github.com/reactor/reactor-core/pull/2040#discussion_r377183833", "createdAt": "2020-02-10T16:47:45Z", "author": {"login": "bsideup"}, "path": "reactor-core/src/main/java/reactor/core/scheduler/BoundedElasticScheduler.java", "diffHunk": "@@ -1,315 +1,239 @@\n /*\n- * Copyright (c) 2011-Present Pivotal Software Inc, All Rights Reserved.\n+ * Copyright (c) 2011-2017 Pivotal Software Inc, All Rights Reserved.\n  *\n  * Licensed under the Apache License, Version 2.0 (the \"License\");\n  * you may not use this file except in compliance with the License.\n  * You may obtain a copy of the License at\n  *\n- *        https://www.apache.org/licenses/LICENSE-2.0\n+ *       https://www.apache.org/licenses/LICENSE-2.0\n  *\n  * Unless required by applicable law or agreed to in writing, software\n  * distributed under the License is distributed on an \"AS IS\" BASIS,\n  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n-\n package reactor.core.scheduler;\n \n+import java.time.Clock;\n+import java.time.Instant;\n+import java.time.ZoneId;\n import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Comparator;\n import java.util.Deque;\n import java.util.List;\n+import java.util.Objects;\n+import java.util.PriorityQueue;\n import java.util.Queue;\n+import java.util.concurrent.Callable;\n import java.util.concurrent.ConcurrentLinkedDeque;\n-import java.util.concurrent.ConcurrentLinkedQueue;\n+import java.util.concurrent.ExecutionException;\n import java.util.concurrent.Executors;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.PriorityBlockingQueue;\n+import java.util.concurrent.RejectedExecutionException;\n+import java.util.concurrent.RejectedExecutionHandler;\n import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledFuture;\n import java.util.concurrent.ScheduledThreadPoolExecutor;\n import java.util.concurrent.ThreadFactory;\n+import java.util.concurrent.ThreadPoolExecutor;\n import java.util.concurrent.TimeUnit;\n-import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicInteger;\n import java.util.concurrent.atomic.AtomicIntegerFieldUpdater;\n import java.util.concurrent.atomic.AtomicLong;\n-import java.util.concurrent.atomic.AtomicReference;\n import java.util.concurrent.atomic.AtomicReferenceFieldUpdater;\n-import java.util.function.LongSupplier;\n-import java.util.function.Supplier;\n import java.util.stream.Stream;\n \n import reactor.core.Disposable;\n import reactor.core.Disposables;\n import reactor.core.Exceptions;\n import reactor.core.Scannable;\n-import reactor.util.annotation.Nullable;\n \n /**\n- * Dynamically creates ScheduledExecutorService-based Workers and caches the thread pools, reusing\n- * them once the Workers have been shut down. This scheduler is time-capable (can schedule\n- * with delay / periodically).\n- * <p>\n- * The maximum number of created thread pools is capped. Tasks submitted after the cap has been\n- * reached can be enqueued up to a second limit, possibly lifted by using {@link Integer#MAX_VALUE}.\n- * <p>\n- * The default time-to-live for unused thread pools is 60 seconds, use the\n- * appropriate constructor to set a different value.\n- * <p>\n- * This scheduler is not restartable.\n+ * Scheduler that hosts a pool of 0-N single-threaded {@link BoundedScheduledExecutorService} and exposes workers\n+ * backed by these executors, making it suited for moderate amount of blocking work. Note that requests for workers\n+ * will pick an executor in a round-robin fashion, so tasks from a given worker might arbitrarily be impeded by\n+ * long-running tasks of a sibling worker (and tasks are pinned to a given executor, so they won't be stolen\n+ * by an idle executor).\n+ *\n+ * This scheduler is time-capable (can schedule with delay / periodically).\n  *\n  * @author Simon Basl\u00e9\n  */\n-final class BoundedElasticScheduler\n-\t\timplements Scheduler, Supplier<ScheduledExecutorService>, Scannable {\n+final class BoundedElasticScheduler implements Scheduler, Scannable {\n \n-\tstatic final AtomicLong COUNTER = new AtomicLong();\n+\tstatic final int DEFAULT_TTL_SECONDS = 60;\n+\n+\tstatic final AtomicLong EVICTOR_COUNTER = new AtomicLong();\n \n \tstatic final ThreadFactory EVICTOR_FACTORY = r -> {\n-\t\tThread t = new Thread(r, \"elasticBounded-evictor-\" + COUNTER.incrementAndGet());\n+\t\tThread t = new Thread(r, Schedulers.BOUNDED_ELASTIC + \"-evictor-\" + EVICTOR_COUNTER.incrementAndGet());\n \t\tt.setDaemon(true);\n \t\treturn t;\n \t};\n \n-\tstatic final CachedService SHUTDOWN            = new CachedService(null);\n-\tstatic final int           DEFAULT_TTL_SECONDS = 60;\n-\n-\tfinal ThreadFactory              factory;\n-\tfinal int                        ttlSeconds;\n-\tfinal int                        threadCap;\n-\tfinal int                        deferredTaskCap;\n-\tfinal Deque<CachedServiceExpiry> idleServicesWithExpiry;\n-\tfinal Queue<DeferredFacade>      deferredFacades;\n-\tfinal Queue<CachedService>       allServices;\n-\tfinal ScheduledExecutorService   evictor;\n+\tstatic final BoundedServices SHUTDOWN;\n+\tstatic final BoundedState    CREATING;\n+\n+\tstatic {\n+\t\tSHUTDOWN = new BoundedServices();\n+\t\tSHUTDOWN.dispose();\n+\t\tScheduledExecutorService s = Executors.newSingleThreadScheduledExecutor();\n+\t\ts.shutdownNow();\n+\t\tCREATING = new BoundedState(SHUTDOWN, s) {\n+\t\t\t@Override\n+\t\t\tpublic String toString() {\n+\t\t\t\treturn \"CREATING BoundedState\";\n+\t\t\t}\n+\t\t};\n+\t\tCREATING.markCount = -1; //always -1, ensures tryPick never returns true\n+\t\tCREATING.idleSinceTimestamp = -1; //consider evicted\n+\t}\n \n-\tvolatile boolean shutdown;\n+\tfinal int maxThreads;\n+\tfinal int maxTaskQueuedPerThread;\n \n-\tvolatile int                                                    remainingThreads;\n-\tstatic final AtomicIntegerFieldUpdater<BoundedElasticScheduler> REMAINING_THREADS =\n-\t\t\tAtomicIntegerFieldUpdater.newUpdater(BoundedElasticScheduler.class, \"remainingThreads\");\n+\tfinal Clock         clock;\n+\tfinal ThreadFactory factory;\n+\tfinal long          ttlMillis;\n \n-\tvolatile int                                                   remainingDeferredTasks;\n-\tstatic final AtomicIntegerFieldUpdater<BoundedElasticScheduler>REMAINING_DEFERRED_TASKS =\n-\t\t\tAtomicIntegerFieldUpdater.newUpdater(BoundedElasticScheduler.class, \"remainingDeferredTasks\");\n+\tvolatile BoundedServices boundedServices;\n+\tstatic final AtomicReferenceFieldUpdater<BoundedElasticScheduler, BoundedServices> BOUNDED_SERVICES =\n+\t\t\tAtomicReferenceFieldUpdater.newUpdater(BoundedElasticScheduler.class, BoundedServices.class, \"boundedServices\");\n \n+\tvolatile ScheduledExecutorService evictor;\n+\tstatic final AtomicReferenceFieldUpdater<BoundedElasticScheduler, ScheduledExecutorService> EVICTOR =\n+\t\t\tAtomicReferenceFieldUpdater.newUpdater(BoundedElasticScheduler.class, ScheduledExecutorService.class, \"evictor\");\n \n-\tBoundedElasticScheduler(int threadCap, int deferredTaskCap, ThreadFactory factory, int ttlSeconds) {\n-\t\tif (ttlSeconds < 0) {\n-\t\t\tthrow new IllegalArgumentException(\"ttlSeconds must be positive, was: \" + ttlSeconds);\n+\t/**\n+\t * This constructor lets define millisecond-grained TTLs and a custome {@link Clock},\n+\t * which can be useful for tests.\n+\t */\n+\tBoundedElasticScheduler(int maxThreads, int maxTaskQueuedPerThread,\n+\t\t\tThreadFactory threadFactory, long ttlMillis, Clock clock) {\n+\t\tif (ttlMillis <= 0) {\n+\t\t\tthrow new IllegalArgumentException(\"TTL must be strictly positive, was \" + ttlMillis + \"ms\");\n \t\t}\n-\t\tthis.ttlSeconds = ttlSeconds;\n-\t\tif (threadCap < 1) {\n-\t\t\tthrow new IllegalArgumentException(\"threadCap must be strictly positive, was: \" + threadCap);\n+\t\tif (maxThreads <= 0) {\n+\t\t\tthrow new IllegalArgumentException(\"maxThreads must be strictly positive, was \" + maxThreads);\n \t\t}\n-\t\tif (deferredTaskCap < 1) {\n-\t\t\tthrow new IllegalArgumentException(\"deferredTaskCap must be strictly positive, was: \" + deferredTaskCap);\n+\t\tif (maxTaskQueuedPerThread <= 0) {\n+\t\t\tthrow new IllegalArgumentException(\"maxTaskQueuedPerThread must be strictly positive, was \" + maxTaskQueuedPerThread);\n \t\t}\n-\t\tthis.threadCap = threadCap;\n-\t\tthis.remainingThreads = threadCap;\n-\t\tthis.deferredTaskCap = deferredTaskCap;\n-\t\tthis.remainingDeferredTasks = deferredTaskCap;\n-\t\tthis.factory = factory;\n-\t\tthis.idleServicesWithExpiry = new ConcurrentLinkedDeque<>();\n-\t\tthis.deferredFacades = new ConcurrentLinkedQueue<>();\n-\t\tthis.allServices = new ConcurrentLinkedQueue<>();\n+\t\tthis.maxThreads = maxThreads;\n+\t\tthis.maxTaskQueuedPerThread = maxTaskQueuedPerThread;\n+\t\tthis.factory = threadFactory;\n+\t\tthis.clock = Objects.requireNonNull(clock, \"A Clock must be provided\");\n+\t\tthis.ttlMillis = ttlMillis;\n+\n+\t\tthis.boundedServices = new BoundedServices(this);\n \t\tthis.evictor = Executors.newScheduledThreadPool(1, EVICTOR_FACTORY);\n-\t\tthis.evictor.scheduleAtFixedRate(() -> this.eviction(System::currentTimeMillis),\n-\t\t\t\tttlSeconds,\n-\t\t\t\tttlSeconds,\n-\t\t\t\tTimeUnit.SECONDS);\n+\t\tevictor.scheduleAtFixedRate(boundedServices::eviction,\n+\t\t\t\tttlMillis,\n+\t\t\t\tttlMillis,\n+\t\t\t\tTimeUnit.MILLISECONDS);\n \t}\n \n \t/**\n-\t * Instantiates the default {@link ScheduledExecutorService} for the BoundedElasticScheduler\n-\t * ({@code Executors.newScheduledThreadPoolExecutor} with core and max pool size of 1).\n+\t * Create a {@link BoundedElasticScheduler} with the given configuration. Note that backing threads\n+\t * (or executors) can be shared by each {@link reactor.core.scheduler.Scheduler.Worker}, so each worker\n+\t * can contribute to the task queue size.\n+\t *\n+\t * @param maxThreads the maximum number of backing threads to spawn, must be strictly positive\n+\t * @param maxTaskQueuedPerThread the maximum amount of tasks an executor can queue up\n+\t * @param factory the {@link ThreadFactory} to name the backing threads\n+\t * @param ttlSeconds the time-to-live (TTL) of idle threads, in seconds\n \t */\n-\t@Override\n-\tpublic ScheduledExecutorService get() {\n-\t\tScheduledThreadPoolExecutor poolExecutor = new ScheduledThreadPoolExecutor(1, factory);\n-\t\tpoolExecutor.setMaximumPoolSize(1);\n-\t\tpoolExecutor.setRemoveOnCancelPolicy(true);\n-\t\treturn poolExecutor;\n+\tBoundedElasticScheduler(int maxThreads, int maxTaskQueuedPerThread, ThreadFactory factory, int ttlSeconds) {\n+\t\tthis(maxThreads,\n+\t\t\t\tmaxTaskQueuedPerThread, factory, ttlSeconds * 1000, Clock.tickSeconds(ZoneId.systemDefault()));\n \t}\n \n-\t@Override\n-\tpublic void start() {\n-\t\tthrow new UnsupportedOperationException(\"Restarting not supported yet\");\n+\t/**\n+\t * Instantiates the default {@link ScheduledExecutorService} for the scheduler\n+\t * ({@code Executors.newScheduledThreadPoolExecutor} with core and max pool size of 1).\n+\t */\n+\tBoundedScheduledExecutorService createBoundedExecutorService() {\n+\t\treturn new BoundedScheduledExecutorService(this.maxTaskQueuedPerThread, this.factory);\n \t}\n \n \t@Override\n \tpublic boolean isDisposed() {\n-\t\treturn shutdown;\n+\t\treturn BOUNDED_SERVICES.get(this) == SHUTDOWN;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "71598282865e36d03c459deabe2e95c8faf9edca"}, "originalPosition": 239}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU2MDk2OTQ2", "url": "https://github.com/reactor/reactor-core/pull/2040#pullrequestreview-356096946", "createdAt": "2020-02-10T16:50:32Z", "commit": {"oid": "71598282865e36d03c459deabe2e95c8faf9edca"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMFQxNjo1MDozMlrOFntl7Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMFQxNjo1MDozMlrOFntl7Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzE4NTc3Mw==", "bodyText": "nit: please extract the constant (to be used in dispose())", "url": "https://github.com/reactor/reactor-core/pull/2040#discussion_r377185773", "createdAt": "2020-02-10T16:50:32Z", "author": {"login": "bsideup"}, "path": "reactor-core/src/main/java/reactor/core/scheduler/BoundedElasticScheduler.java", "diffHunk": "@@ -319,491 +243,554 @@ public String toString() {\n \t\tif (factory instanceof ReactorThreadFactory) {\n \t\t\tts.append('\\\"').append(((ReactorThreadFactory) factory).get()).append(\"\\\",\");\n \t\t}\n-\t\tts.append(\"maxThreads=\").append(threadCap)\n-\t\t  .append(\",maxTaskQueued=\").append(deferredTaskCap == Integer.MAX_VALUE ? \"unbounded\" : deferredTaskCap)\n-\t\t  .append(\",ttl=\").append(ttlSeconds).append(\"s)\");\n+\t\tts.append(\"maxThreads=\").append(maxThreads)\n+\t\t  .append(\",maxTaskQueuedPerThread=\").append(maxTaskQueuedPerThread == Integer.MAX_VALUE ? \"unbounded\" : maxTaskQueuedPerThread)\n+\t\t  .append(\",ttl=\");\n+\t\tif (ttlMillis < 1000) {\n+\t\t\tts.append(ttlMillis).append(\"ms)\");\n+\t\t}\n+\t\telse {\n+\t\t\tts.append(ttlMillis / 1000).append(\"s)\");\n+\t\t}\n \t\treturn ts.toString();\n \t}\n \n+\t/**\n+\t * @return a best effort total count of the spinned up executors\n+\t */\n+\tint estimateSize() {\n+\t\treturn BOUNDED_SERVICES.get(this).get();\n+\t}\n+\n+\t/**\n+\t * @return a best effort total count of the busy executors\n+\t */\n+\tint estimateBusy() {\n+\t\treturn BOUNDED_SERVICES.get(this).busyQueue.size();\n+\t}\n+\n+\t/**\n+\t * @return a best effort total count of the idle executors\n+\t */\n+\tint estimateIdle() {\n+\t\treturn BOUNDED_SERVICES.get(this).idleQueue.size();\n+\t}\n+\n+\t/**\n+\t * Best effort snapshot of the remaining queue capacity for pending tasks across all the backing executors.\n+\t *\n+\t * @return the total task capacity, or {@literal -1} if any backing executor's task queue size cannot be instrumented\n+\t */\n+\tint estimateRemainingTaskCapacity() {\n+\t\tQueue<BoundedState> busyQueue = BOUNDED_SERVICES.get(this).busyQueue;\n+\t\tint totalTaskCapacity = maxTaskQueuedPerThread * maxThreads;\n+\t\tfor (BoundedState state : busyQueue) {\n+\t\t\tint stateQueueSize = state.estimateQueueSize();\n+\t\t\tif (stateQueueSize >= 0) {\n+\t\t\t\ttotalTaskCapacity -= stateQueueSize;\n+\t\t\t}\n+\t\t\telse {\n+\t\t\t\treturn -1;\n+\t\t\t}\n+\t\t}\n+\t\treturn totalTaskCapacity;\n+\t}\n+\n \t@Override\n \tpublic Object scanUnsafe(Attr key) {\n \t\tif (key == Attr.TERMINATED || key == Attr.CANCELLED) return isDisposed();\n-\t\tif (key == Attr.CAPACITY) return threadCap;\n-\t\tif (key == Attr.BUFFERED) return idleServicesWithExpiry.size(); //BUFFERED: number of workers alive and backed by thread\n+\t\tif (key == Attr.BUFFERED) return estimateSize();\n+\t\tif (key == Attr.CAPACITY) return maxThreads;\n \t\tif (key == Attr.NAME) return this.toString();\n \n \t\treturn null;\n \t}\n \n \t@Override\n-\t\t//TODO re-evaluate the inners? should these include deferredWorkers? allServices?\n \tpublic Stream<? extends Scannable> inners() {\n-\t\treturn idleServicesWithExpiry.stream()\n-\t\t                             .map(cached -> cached.cached);\n+\t\tBoundedServices services = BOUNDED_SERVICES.get(this);\n+\t\treturn Stream.concat(services.busyQueue.stream(), services.idleQueue.stream())\n+\t\t             .filter(obj -> obj != null && obj != CREATING);\n \t}\n \n-\tvoid eviction(LongSupplier nowSupplier) {\n-\t\tlong now = nowSupplier.getAsLong();\n-\t\tList<CachedServiceExpiry> list = new ArrayList<>(idleServicesWithExpiry);\n-\t\tfor (CachedServiceExpiry e : list) {\n-\t\t\tif (e.expireMillis < now) {\n-\t\t\t\tif (idleServicesWithExpiry.remove(e)) {\n-\t\t\t\t\te.cached.exec.shutdownNow();\n-\t\t\t\t\tallServices.remove(e.cached);\n-\t\t\t\t\tREMAINING_THREADS.incrementAndGet(this);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n+\t@Override\n+\tpublic Worker createWorker() {\n+\t\tBoundedState picked = BOUNDED_SERVICES.get(this)\n+\t\t                                      .pick();\n+\t\tExecutorServiceWorker worker = new ExecutorServiceWorker(picked.executor);\n+\t\tworker.tasks.add(picked); //this ensures the BoundedState will be released when worker is disposed\n+\t\treturn worker;\n \t}\n \n-\tstatic final class CachedService implements Disposable, Scannable {\n \n-\t\tfinal BoundedElasticScheduler  parent;\n-\t\tfinal ScheduledExecutorService exec;\n+\tstatic final class BoundedServices extends AtomicInteger implements Disposable {\n \n-\t\tCachedService(@Nullable BoundedElasticScheduler parent) {\n+\t\tfinal BoundedElasticScheduler             parent;\n+\t\t//duplicated Clock field from parent so that SHUTDOWN can be instantiated and partially used\n+\t\tfinal Clock                               clock;\n+\t\tfinal Deque<BoundedState>                 idleQueue;\n+\t\tfinal PriorityBlockingQueue<BoundedState> busyQueue;\n+\n+\t\t//constructor for SHUTDOWN\n+\t\tprivate BoundedServices() {\n+\t\t\tthis.parent = null;\n+\t\t\tthis.clock = Clock.fixed(Instant.EPOCH, ZoneId.systemDefault());\n+\t\t\tthis.busyQueue = new PriorityBlockingQueue<>();\n+\t\t\tthis.idleQueue = new ConcurrentLinkedDeque<>();\n+\t\t}\n+\n+\t\tBoundedServices(BoundedElasticScheduler parent) {\n \t\t\tthis.parent = parent;\n-\t\t\tif (parent != null) {\n-\t\t\t\tthis.exec = Schedulers.decorateExecutorService(parent, parent.get());\n-\t\t\t}\n-\t\t\telse {\n-\t\t\t\tthis.exec = Executors.newSingleThreadScheduledExecutor();\n-\t\t\t\tthis.exec.shutdownNow();\n+\t\t\tthis.clock = parent.clock;\n+\t\t\tthis.busyQueue = new PriorityBlockingQueue<>(parent.maxThreads,\n+\t\t\t\t\tComparator.comparingInt(bs -> bs.markCount));\n+\t\t\tthis.idleQueue = new ConcurrentLinkedDeque<>();\n+\t\t}\n+\n+\t\t/**\n+\t\t * Trigger the eviction by computing the oldest acceptable timestamp and letting each {@link BoundedState}\n+\t\t * check (and potentially shutdown) itself.\n+\t\t */\n+\t\tvoid eviction() {\n+\t\t\tfinal long evictionTimestamp = parent.clock.millis();\n+\t\t\tList<BoundedState> idleCandidates = new ArrayList<>(idleQueue);\n+\t\t\tfor (BoundedState candidate : idleCandidates) {\n+\t\t\t\tif (candidate.tryEvict(evictionTimestamp, parent.ttlMillis)) {\n+\t\t\t\t\tidleQueue.remove(candidate);\n+\t\t\t\t\tdecrementAndGet();\n+\t\t\t\t}\n \t\t\t}\n \t\t}\n \n-\t\t@Override\n-\t\tpublic void dispose() {\n-\t\t\tif (exec != null) {\n-\t\t\t\tif (this != SHUTDOWN && !parent.shutdown) {\n-\t\t\t\t\t//in case of work, re-create an ActiveWorker\n-\t\t\t\t\tDeferredFacade deferredFacade = parent.deferredFacades.poll();\n-\t\t\t\t\tif (deferredFacade != null) {\n-\t\t\t\t\t\tdeferredFacade.setService(this);\n+\t\t/**\n+\t\t * Pick a {@link BoundedState}, prioritizing idle ones then spinning up a new one if enough capacity.\n+\t\t * Otherwise, picks an active one by taking from a {@link PriorityQueue}. The picking is\n+\t\t * optimistically re-attempted if the picked slot cannot be marked as picked.\n+\t\t *\n+\t\t * @return the picked {@link BoundedState}\n+\t\t */\n+\t\tBoundedState pick() {\n+\t\t\tfor (;;) {\n+\t\t\t\tint a = get();\n+\t\t\t\tif (a == -1) {\n+\t\t\t\t\treturn CREATING; //synonym for shutdown, since the underlying executor is shut down\n+\t\t\t\t}\n+\n+\t\t\t\tif (!idleQueue.isEmpty()) {\n+\t\t\t\t\t//try to find an idle resource\n+\t\t\t\t\tBoundedState bs = idleQueue.pollLast();\n+\t\t\t\t\tif (bs != null && bs.markPicked()) {\n+\t\t\t\t\t\tbusyQueue.add(bs);\n+\t\t\t\t\t\treturn bs;\n \t\t\t\t\t}\n-\t\t\t\t\telse {\n-\t\t\t\t\t\t//if no more work, the service is put back at end of the cached queue and new expiry is started\n-\t\t\t\t\t\tCachedServiceExpiry e = new CachedServiceExpiry(this,\n-\t\t\t\t\t\t\t\tSystem.currentTimeMillis() + parent.ttlSeconds * 1000L);\n-\t\t\t\t\t\tparent.idleServicesWithExpiry.offerLast(e);\n-\t\t\t\t\t\tif (parent.shutdown) {\n-\t\t\t\t\t\t\tif (parent.idleServicesWithExpiry.remove(e)) {\n-\t\t\t\t\t\t\t\texec.shutdownNow();\n-\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t}\n+\t\t\t\t\t//else optimistically retry\n+\t\t\t\t}\n+\t\t\t\telse if (a < parent.maxThreads) {\n+\t\t\t\t\t//try to build a new resource\n+\t\t\t\t\tif (compareAndSet(a, a + 1)) {\n+\t\t\t\t\t\tScheduledExecutorService s = Schedulers.decorateExecutorService(parent, parent.createBoundedExecutorService());\n+\t\t\t\t\t\tBoundedState newState = new BoundedState(this, s);\n+\t\t\t\t\t\tnewState.markPicked(); //no need to check return value as this one is brand new\n+\t\t\t\t\t\tbusyQueue.add(newState);\n+\t\t\t\t\t\treturn newState;\n+\t\t\t\t\t}\n+\t\t\t\t\t//else optimistically retry\n+\t\t\t\t}\n+\t\t\t\telse {\n+\t\t\t\t\t//pick the least busy one\n+\t\t\t\t\tBoundedState s;\n+\t\t\t\t\ts = busyQueue.poll();\n+\t\t\t\t\tif (s != null && s.markPicked()) {\n+\t\t\t\t\t\tbusyQueue.add(s); //put it back in the queue with updated priority\n+\t\t\t\t\t\treturn s;\n \t\t\t\t\t}\n+\t\t\t\t\t//else optimistically retry\n \t\t\t\t}\n \t\t\t}\n \t\t}\n \n-\t\t@Override\n-\t\tpublic Object scanUnsafe(Attr key) {\n-\t\t\tif (key == Attr.NAME) return parent.scanUnsafe(key);\n-\t\t\tif (key == Attr.PARENT) return parent;\n-\t\t\tif (key == Attr.TERMINATED || key == Attr.CANCELLED) return isDisposed();\n-\t\t\tif (key == Attr.CAPACITY) {\n-\t\t\t\t//assume 1 if unknown, otherwise use the one from underlying executor\n-\t\t\t\tInteger capacity = (Integer) Schedulers.scanExecutor(exec, key);\n-\t\t\t\tif (capacity == null || capacity == -1) return 1;\n-\t\t\t}\n-\t\t\treturn Schedulers.scanExecutor(exec, key);\n+\t\tvoid setIdle(BoundedState boundedState) {\n+\t\t\tthis.idleQueue.add(boundedState);\n+\t\t\tthis.busyQueue.remove(boundedState);\n \t\t}\n-\t}\n \n-\tstatic final class CachedServiceExpiry {\n-\n-\t\tfinal CachedService cached;\n-\t\tfinal long          expireMillis;\n+\t\t@Override\n+\t\tpublic boolean isDisposed() {\n+\t\t\treturn get() == -1;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "71598282865e36d03c459deabe2e95c8faf9edca"}, "originalPosition": 706}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "6aea16c4a9c3e3b4d9e664976ad0c1abe1cb9507", "author": {"user": {"login": "simonbasle", "name": "Simon Basl\u00e9"}}, "url": "https://github.com/reactor/reactor-core/commit/6aea16c4a9c3e3b4d9e664976ad0c1abe1cb9507", "committedDate": "2020-02-11T09:00:54Z", "message": "add task start leeway of 10ms to another test"}, "afterCommit": {"oid": "0d02a07b529ecdb2ff0bf0114fc7156a45673f76", "author": {"user": {"login": "simonbasle", "name": "Simon Basl\u00e9"}}, "url": "https://github.com/reactor/reactor-core/commit/0d02a07b529ecdb2ff0bf0114fc7156a45673f76", "committedDate": "2020-02-14T09:46:51Z", "message": "added smoke test for gh1973"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU4ODg0NzM3", "url": "https://github.com/reactor/reactor-core/pull/2040#pullrequestreview-358884737", "createdAt": "2020-02-14T11:41:03Z", "commit": {"oid": "baf4aa6f21d00dc634fd7a74114e08909e0e3408"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQxMTo0MTowM1rOFpz-Rw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQxMTo0MTowM1rOFpz-Rw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTM4NzQ2Mw==", "bodyText": "nit: this.boundedServices = , this.evictor =  and scheduleAtFixedRate can be replaced with a single start() since they share the implementation.", "url": "https://github.com/reactor/reactor-core/pull/2040#discussion_r379387463", "createdAt": "2020-02-14T11:41:03Z", "author": {"login": "bsideup"}, "path": "reactor-core/src/main/java/reactor/core/scheduler/BoundedElasticScheduler.java", "diffHunk": "@@ -5,311 +5,236 @@\n  * you may not use this file except in compliance with the License.\n  * You may obtain a copy of the License at\n  *\n- *        https://www.apache.org/licenses/LICENSE-2.0\n+ *       https://www.apache.org/licenses/LICENSE-2.0\n  *\n  * Unless required by applicable law or agreed to in writing, software\n  * distributed under the License is distributed on an \"AS IS\" BASIS,\n  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n-\n package reactor.core.scheduler;\n \n+import java.time.Clock;\n+import java.time.Instant;\n+import java.time.ZoneId;\n import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Comparator;\n import java.util.Deque;\n import java.util.List;\n+import java.util.Objects;\n+import java.util.PriorityQueue;\n import java.util.Queue;\n+import java.util.concurrent.Callable;\n import java.util.concurrent.ConcurrentLinkedDeque;\n-import java.util.concurrent.ConcurrentLinkedQueue;\n+import java.util.concurrent.ExecutionException;\n import java.util.concurrent.Executors;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.PriorityBlockingQueue;\n+import java.util.concurrent.RejectedExecutionException;\n+import java.util.concurrent.RejectedExecutionHandler;\n import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledFuture;\n import java.util.concurrent.ScheduledThreadPoolExecutor;\n import java.util.concurrent.ThreadFactory;\n+import java.util.concurrent.ThreadPoolExecutor;\n import java.util.concurrent.TimeUnit;\n-import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicInteger;\n import java.util.concurrent.atomic.AtomicIntegerFieldUpdater;\n import java.util.concurrent.atomic.AtomicLong;\n-import java.util.concurrent.atomic.AtomicReference;\n import java.util.concurrent.atomic.AtomicReferenceFieldUpdater;\n-import java.util.function.LongSupplier;\n-import java.util.function.Supplier;\n import java.util.stream.Stream;\n \n import reactor.core.Disposable;\n import reactor.core.Disposables;\n import reactor.core.Exceptions;\n import reactor.core.Scannable;\n-import reactor.util.annotation.Nullable;\n \n /**\n- * Dynamically creates ScheduledExecutorService-based Workers and caches the thread pools, reusing\n- * them once the Workers have been shut down. This scheduler is time-capable (can schedule\n- * with delay / periodically).\n- * <p>\n- * The maximum number of created thread pools is capped. Tasks submitted after the cap has been\n- * reached can be enqueued up to a second limit, possibly lifted by using {@link Integer#MAX_VALUE}.\n- * <p>\n- * The default time-to-live for unused thread pools is 60 seconds, use the\n- * appropriate constructor to set a different value.\n- * <p>\n- * This scheduler is not restartable.\n+ * Scheduler that hosts a pool of 0-N single-threaded {@link BoundedScheduledExecutorService} and exposes workers\n+ * backed by these executors, making it suited for moderate amount of blocking work. Note that requests for workers\n+ * will pick an executor in a round-robin fashion, so tasks from a given worker might arbitrarily be impeded by\n+ * long-running tasks of a sibling worker (and tasks are pinned to a given executor, so they won't be stolen\n+ * by an idle executor).\n+ *\n+ * This scheduler is time-capable (can schedule with delay / periodically).\n  *\n  * @author Simon Basl\u00e9\n  */\n-final class BoundedElasticScheduler\n-\t\timplements Scheduler, Supplier<ScheduledExecutorService>, Scannable {\n+final class BoundedElasticScheduler implements Scheduler, Scannable {\n \n-\tstatic final AtomicLong COUNTER = new AtomicLong();\n+\tstatic final int DEFAULT_TTL_SECONDS = 60;\n+\n+\tstatic final AtomicLong EVICTOR_COUNTER = new AtomicLong();\n \n \tstatic final ThreadFactory EVICTOR_FACTORY = r -> {\n-\t\tThread t = new Thread(r, \"elasticBounded-evictor-\" + COUNTER.incrementAndGet());\n+\t\tThread t = new Thread(r, Schedulers.BOUNDED_ELASTIC + \"-evictor-\" + EVICTOR_COUNTER.incrementAndGet());\n \t\tt.setDaemon(true);\n \t\treturn t;\n \t};\n \n-\tstatic final CachedService SHUTDOWN            = new CachedService(null);\n-\tstatic final int           DEFAULT_TTL_SECONDS = 60;\n-\n-\tfinal ThreadFactory              factory;\n-\tfinal int                        ttlSeconds;\n-\tfinal int                        threadCap;\n-\tfinal int                        deferredTaskCap;\n-\tfinal Deque<CachedServiceExpiry> idleServicesWithExpiry;\n-\tfinal Queue<DeferredFacade>      deferredFacades;\n-\tfinal Queue<CachedService>       allServices;\n-\tfinal ScheduledExecutorService   evictor;\n+\tstatic final int             DISPOSED = -1;\n+\tstatic final BoundedServices SHUTDOWN;\n+\tstatic final BoundedState    CREATING;\n+\n+\tstatic {\n+\t\tSHUTDOWN = new BoundedServices();\n+\t\tSHUTDOWN.dispose();\n+\t\tScheduledExecutorService s = Executors.newSingleThreadScheduledExecutor();\n+\t\ts.shutdownNow();\n+\t\tCREATING = new BoundedState(SHUTDOWN, s) {\n+\t\t\t@Override\n+\t\t\tpublic String toString() {\n+\t\t\t\treturn \"CREATING BoundedState\";\n+\t\t\t}\n+\t\t};\n+\t\tCREATING.markCount = -1; //always -1, ensures tryPick never returns true\n+\t\tCREATING.idleSinceTimestamp = -1; //consider evicted\n+\t}\n \n-\tvolatile boolean shutdown;\n+\tfinal int maxThreads;\n+\tfinal int maxTaskQueuedPerThread;\n \n-\tvolatile int                                                    remainingThreads;\n-\tstatic final AtomicIntegerFieldUpdater<BoundedElasticScheduler> REMAINING_THREADS =\n-\t\t\tAtomicIntegerFieldUpdater.newUpdater(BoundedElasticScheduler.class, \"remainingThreads\");\n+\tfinal Clock         clock;\n+\tfinal ThreadFactory factory;\n+\tfinal long          ttlMillis;\n \n-\tvolatile int                                                   remainingDeferredTasks;\n-\tstatic final AtomicIntegerFieldUpdater<BoundedElasticScheduler>REMAINING_DEFERRED_TASKS =\n-\t\t\tAtomicIntegerFieldUpdater.newUpdater(BoundedElasticScheduler.class, \"remainingDeferredTasks\");\n+\tvolatile BoundedServices boundedServices;\n+\tstatic final AtomicReferenceFieldUpdater<BoundedElasticScheduler, BoundedServices> BOUNDED_SERVICES =\n+\t\t\tAtomicReferenceFieldUpdater.newUpdater(BoundedElasticScheduler.class, BoundedServices.class, \"boundedServices\");\n \n+\tvolatile ScheduledExecutorService evictor;\n+\tstatic final AtomicReferenceFieldUpdater<BoundedElasticScheduler, ScheduledExecutorService> EVICTOR =\n+\t\t\tAtomicReferenceFieldUpdater.newUpdater(BoundedElasticScheduler.class, ScheduledExecutorService.class, \"evictor\");\n \n-\tBoundedElasticScheduler(int threadCap, int deferredTaskCap, ThreadFactory factory, int ttlSeconds) {\n-\t\tif (ttlSeconds < 0) {\n-\t\t\tthrow new IllegalArgumentException(\"ttlSeconds must be positive, was: \" + ttlSeconds);\n+\t/**\n+\t * This constructor lets define millisecond-grained TTLs and a custome {@link Clock},\n+\t * which can be useful for tests.\n+\t */\n+\tBoundedElasticScheduler(int maxThreads, int maxTaskQueuedPerThread,\n+\t\t\tThreadFactory threadFactory, long ttlMillis, Clock clock) {\n+\t\tif (ttlMillis <= 0) {\n+\t\t\tthrow new IllegalArgumentException(\"TTL must be strictly positive, was \" + ttlMillis + \"ms\");\n \t\t}\n-\t\tthis.ttlSeconds = ttlSeconds;\n-\t\tif (threadCap < 1) {\n-\t\t\tthrow new IllegalArgumentException(\"threadCap must be strictly positive, was: \" + threadCap);\n+\t\tif (maxThreads <= 0) {\n+\t\t\tthrow new IllegalArgumentException(\"maxThreads must be strictly positive, was \" + maxThreads);\n \t\t}\n-\t\tif (deferredTaskCap < 1) {\n-\t\t\tthrow new IllegalArgumentException(\"deferredTaskCap must be strictly positive, was: \" + deferredTaskCap);\n+\t\tif (maxTaskQueuedPerThread <= 0) {\n+\t\t\tthrow new IllegalArgumentException(\"maxTaskQueuedPerThread must be strictly positive, was \" + maxTaskQueuedPerThread);\n \t\t}\n-\t\tthis.threadCap = threadCap;\n-\t\tthis.remainingThreads = threadCap;\n-\t\tthis.deferredTaskCap = deferredTaskCap;\n-\t\tthis.remainingDeferredTasks = deferredTaskCap;\n-\t\tthis.factory = factory;\n-\t\tthis.idleServicesWithExpiry = new ConcurrentLinkedDeque<>();\n-\t\tthis.deferredFacades = new ConcurrentLinkedQueue<>();\n-\t\tthis.allServices = new ConcurrentLinkedQueue<>();\n+\t\tthis.maxThreads = maxThreads;\n+\t\tthis.maxTaskQueuedPerThread = maxTaskQueuedPerThread;\n+\t\tthis.factory = threadFactory;\n+\t\tthis.clock = Objects.requireNonNull(clock, \"A Clock must be provided\");\n+\t\tthis.ttlMillis = ttlMillis;\n+\n+\t\tthis.boundedServices = new BoundedServices(this);\n \t\tthis.evictor = Executors.newScheduledThreadPool(1, EVICTOR_FACTORY);\n-\t\tthis.evictor.scheduleAtFixedRate(() -> this.eviction(System::currentTimeMillis),\n-\t\t\t\tttlSeconds,\n-\t\t\t\tttlSeconds,\n-\t\t\t\tTimeUnit.SECONDS);\n+\t\tevictor.scheduleAtFixedRate(boundedServices::eviction,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "baf4aa6f21d00dc634fd7a74114e08909e0e3408"}, "originalPosition": 192}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU4ODg4NTgy", "url": "https://github.com/reactor/reactor-core/pull/2040#pullrequestreview-358888582", "createdAt": "2020-02-14T11:48:55Z", "commit": {"oid": "baf4aa6f21d00dc634fd7a74114e08909e0e3408"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQxMTo0ODo1NVrOFp0J6Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQxMTo0ODo1NVrOFp0J6Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTM5MDQ0MQ==", "bodyText": "unrelated nit: ExecutorServiceWorker#tasks is very misleading (especially when populated from places like here), ExecutorServiceWorker#disposables or something would make it much easier to understand what it actually does (since it does not execute tasks, only disposes them)", "url": "https://github.com/reactor/reactor-core/pull/2040#discussion_r379390441", "createdAt": "2020-02-14T11:48:55Z", "author": {"login": "bsideup"}, "path": "reactor-core/src/main/java/reactor/core/scheduler/BoundedElasticScheduler.java", "diffHunk": "@@ -319,492 +244,554 @@ public String toString() {\n \t\tif (factory instanceof ReactorThreadFactory) {\n \t\t\tts.append('\\\"').append(((ReactorThreadFactory) factory).get()).append(\"\\\",\");\n \t\t}\n-\t\tts.append(\"maxThreads=\").append(threadCap)\n-\t\t  .append(\",maxTaskQueued=\").append(deferredTaskCap == Integer.MAX_VALUE ? \"unbounded\" : deferredTaskCap)\n-\t\t  .append(\",ttl=\").append(ttlSeconds).append(\"s)\");\n+\t\tts.append(\"maxThreads=\").append(maxThreads)\n+\t\t  .append(\",maxTaskQueuedPerThread=\").append(maxTaskQueuedPerThread == Integer.MAX_VALUE ? \"unbounded\" : maxTaskQueuedPerThread)\n+\t\t  .append(\",ttl=\");\n+\t\tif (ttlMillis < 1000) {\n+\t\t\tts.append(ttlMillis).append(\"ms)\");\n+\t\t}\n+\t\telse {\n+\t\t\tts.append(ttlMillis / 1000).append(\"s)\");\n+\t\t}\n \t\treturn ts.toString();\n \t}\n \n+\t/**\n+\t * @return a best effort total count of the spinned up executors\n+\t */\n+\tint estimateSize() {\n+\t\treturn BOUNDED_SERVICES.get(this).get();\n+\t}\n+\n+\t/**\n+\t * @return a best effort total count of the busy executors\n+\t */\n+\tint estimateBusy() {\n+\t\treturn BOUNDED_SERVICES.get(this).busyQueue.size();\n+\t}\n+\n+\t/**\n+\t * @return a best effort total count of the idle executors\n+\t */\n+\tint estimateIdle() {\n+\t\treturn BOUNDED_SERVICES.get(this).idleQueue.size();\n+\t}\n+\n+\t/**\n+\t * Best effort snapshot of the remaining queue capacity for pending tasks across all the backing executors.\n+\t *\n+\t * @return the total task capacity, or {@literal -1} if any backing executor's task queue size cannot be instrumented\n+\t */\n+\tint estimateRemainingTaskCapacity() {\n+\t\tQueue<BoundedState> busyQueue = BOUNDED_SERVICES.get(this).busyQueue;\n+\t\tint totalTaskCapacity = maxTaskQueuedPerThread * maxThreads;\n+\t\tfor (BoundedState state : busyQueue) {\n+\t\t\tint stateQueueSize = state.estimateQueueSize();\n+\t\t\tif (stateQueueSize >= 0) {\n+\t\t\t\ttotalTaskCapacity -= stateQueueSize;\n+\t\t\t}\n+\t\t\telse {\n+\t\t\t\treturn -1;\n+\t\t\t}\n+\t\t}\n+\t\treturn totalTaskCapacity;\n+\t}\n+\n \t@Override\n \tpublic Object scanUnsafe(Attr key) {\n \t\tif (key == Attr.TERMINATED || key == Attr.CANCELLED) return isDisposed();\n-\t\tif (key == Attr.CAPACITY) return threadCap;\n-\t\tif (key == Attr.BUFFERED) return idleServicesWithExpiry.size(); //BUFFERED: number of workers alive and backed by thread\n+\t\tif (key == Attr.BUFFERED) return estimateSize();\n+\t\tif (key == Attr.CAPACITY) return maxThreads;\n \t\tif (key == Attr.NAME) return this.toString();\n \n \t\treturn null;\n \t}\n \n \t@Override\n-\t\t//TODO re-evaluate the inners? should these include deferredWorkers? allServices?\n \tpublic Stream<? extends Scannable> inners() {\n-\t\treturn idleServicesWithExpiry.stream()\n-\t\t                             .map(cached -> cached.cached);\n+\t\tBoundedServices services = BOUNDED_SERVICES.get(this);\n+\t\treturn Stream.concat(services.busyQueue.stream(), services.idleQueue.stream())\n+\t\t             .filter(obj -> obj != null && obj != CREATING);\n \t}\n \n-\tvoid eviction(LongSupplier nowSupplier) {\n-\t\tlong now = nowSupplier.getAsLong();\n-\t\tList<CachedServiceExpiry> list = new ArrayList<>(idleServicesWithExpiry);\n-\t\tfor (CachedServiceExpiry e : list) {\n-\t\t\tif (e.expireMillis < now) {\n-\t\t\t\tif (idleServicesWithExpiry.remove(e)) {\n-\t\t\t\t\te.cached.exec.shutdownNow();\n-\t\t\t\t\tallServices.remove(e.cached);\n-\t\t\t\t\tREMAINING_THREADS.incrementAndGet(this);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n+\t@Override\n+\tpublic Worker createWorker() {\n+\t\tBoundedState picked = BOUNDED_SERVICES.get(this)\n+\t\t                                      .pick();\n+\t\tExecutorServiceWorker worker = new ExecutorServiceWorker(picked.executor);\n+\t\tworker.tasks.add(picked); //this ensures the BoundedState will be released when worker is disposed", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "baf4aa6f21d00dc634fd7a74114e08909e0e3408"}, "originalPosition": 560}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU4OTAzMTE5", "url": "https://github.com/reactor/reactor-core/pull/2040#pullrequestreview-358903119", "createdAt": "2020-02-14T12:18:37Z", "commit": {"oid": "baf4aa6f21d00dc634fd7a74114e08909e0e3408"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQxMjoxODozN1rOFp013A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQxMjoxODozN1rOFp013A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTQwMTY5Mg==", "bodyText": "This constant seems to be reused between BoundedServices and BoundedState.\nI would suggest having two, in each class, especially given that BoundedServices uses it for checking a variable that is a counter (although I haven't checked BoundedState), so that the value is more meaningless in the given domain", "url": "https://github.com/reactor/reactor-core/pull/2040#discussion_r379401692", "createdAt": "2020-02-14T12:18:37Z", "author": {"login": "bsideup"}, "path": "reactor-core/src/main/java/reactor/core/scheduler/BoundedElasticScheduler.java", "diffHunk": "@@ -5,311 +5,236 @@\n  * you may not use this file except in compliance with the License.\n  * You may obtain a copy of the License at\n  *\n- *        https://www.apache.org/licenses/LICENSE-2.0\n+ *       https://www.apache.org/licenses/LICENSE-2.0\n  *\n  * Unless required by applicable law or agreed to in writing, software\n  * distributed under the License is distributed on an \"AS IS\" BASIS,\n  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n-\n package reactor.core.scheduler;\n \n+import java.time.Clock;\n+import java.time.Instant;\n+import java.time.ZoneId;\n import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Comparator;\n import java.util.Deque;\n import java.util.List;\n+import java.util.Objects;\n+import java.util.PriorityQueue;\n import java.util.Queue;\n+import java.util.concurrent.Callable;\n import java.util.concurrent.ConcurrentLinkedDeque;\n-import java.util.concurrent.ConcurrentLinkedQueue;\n+import java.util.concurrent.ExecutionException;\n import java.util.concurrent.Executors;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.PriorityBlockingQueue;\n+import java.util.concurrent.RejectedExecutionException;\n+import java.util.concurrent.RejectedExecutionHandler;\n import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledFuture;\n import java.util.concurrent.ScheduledThreadPoolExecutor;\n import java.util.concurrent.ThreadFactory;\n+import java.util.concurrent.ThreadPoolExecutor;\n import java.util.concurrent.TimeUnit;\n-import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicInteger;\n import java.util.concurrent.atomic.AtomicIntegerFieldUpdater;\n import java.util.concurrent.atomic.AtomicLong;\n-import java.util.concurrent.atomic.AtomicReference;\n import java.util.concurrent.atomic.AtomicReferenceFieldUpdater;\n-import java.util.function.LongSupplier;\n-import java.util.function.Supplier;\n import java.util.stream.Stream;\n \n import reactor.core.Disposable;\n import reactor.core.Disposables;\n import reactor.core.Exceptions;\n import reactor.core.Scannable;\n-import reactor.util.annotation.Nullable;\n \n /**\n- * Dynamically creates ScheduledExecutorService-based Workers and caches the thread pools, reusing\n- * them once the Workers have been shut down. This scheduler is time-capable (can schedule\n- * with delay / periodically).\n- * <p>\n- * The maximum number of created thread pools is capped. Tasks submitted after the cap has been\n- * reached can be enqueued up to a second limit, possibly lifted by using {@link Integer#MAX_VALUE}.\n- * <p>\n- * The default time-to-live for unused thread pools is 60 seconds, use the\n- * appropriate constructor to set a different value.\n- * <p>\n- * This scheduler is not restartable.\n+ * Scheduler that hosts a pool of 0-N single-threaded {@link BoundedScheduledExecutorService} and exposes workers\n+ * backed by these executors, making it suited for moderate amount of blocking work. Note that requests for workers\n+ * will pick an executor in a round-robin fashion, so tasks from a given worker might arbitrarily be impeded by\n+ * long-running tasks of a sibling worker (and tasks are pinned to a given executor, so they won't be stolen\n+ * by an idle executor).\n+ *\n+ * This scheduler is time-capable (can schedule with delay / periodically).\n  *\n  * @author Simon Basl\u00e9\n  */\n-final class BoundedElasticScheduler\n-\t\timplements Scheduler, Supplier<ScheduledExecutorService>, Scannable {\n+final class BoundedElasticScheduler implements Scheduler, Scannable {\n \n-\tstatic final AtomicLong COUNTER = new AtomicLong();\n+\tstatic final int DEFAULT_TTL_SECONDS = 60;\n+\n+\tstatic final AtomicLong EVICTOR_COUNTER = new AtomicLong();\n \n \tstatic final ThreadFactory EVICTOR_FACTORY = r -> {\n-\t\tThread t = new Thread(r, \"elasticBounded-evictor-\" + COUNTER.incrementAndGet());\n+\t\tThread t = new Thread(r, Schedulers.BOUNDED_ELASTIC + \"-evictor-\" + EVICTOR_COUNTER.incrementAndGet());\n \t\tt.setDaemon(true);\n \t\treturn t;\n \t};\n \n-\tstatic final CachedService SHUTDOWN            = new CachedService(null);\n-\tstatic final int           DEFAULT_TTL_SECONDS = 60;\n-\n-\tfinal ThreadFactory              factory;\n-\tfinal int                        ttlSeconds;\n-\tfinal int                        threadCap;\n-\tfinal int                        deferredTaskCap;\n-\tfinal Deque<CachedServiceExpiry> idleServicesWithExpiry;\n-\tfinal Queue<DeferredFacade>      deferredFacades;\n-\tfinal Queue<CachedService>       allServices;\n-\tfinal ScheduledExecutorService   evictor;\n+\tstatic final int             DISPOSED = -1;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "baf4aa6f21d00dc634fd7a74114e08909e0e3408"}, "originalPosition": 108}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU4OTAzNTkz", "url": "https://github.com/reactor/reactor-core/pull/2040#pullrequestreview-358903593", "createdAt": "2020-02-14T12:19:39Z", "commit": {"oid": "baf4aa6f21d00dc634fd7a74114e08909e0e3408"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQxMjoxOTozOVrOFp03JQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQxMjoxOTozOVrOFp03JQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTQwMjAyMQ==", "bodyText": "WDYT about using continue here (and also in the next two cases)? Otherwise, if we accidentally add some logic to the end of this loop, it will get executed when inner if resolves to false", "url": "https://github.com/reactor/reactor-core/pull/2040#discussion_r379402021", "createdAt": "2020-02-14T12:19:39Z", "author": {"login": "bsideup"}, "path": "reactor-core/src/main/java/reactor/core/scheduler/BoundedElasticScheduler.java", "diffHunk": "@@ -319,492 +244,554 @@ public String toString() {\n \t\tif (factory instanceof ReactorThreadFactory) {\n \t\t\tts.append('\\\"').append(((ReactorThreadFactory) factory).get()).append(\"\\\",\");\n \t\t}\n-\t\tts.append(\"maxThreads=\").append(threadCap)\n-\t\t  .append(\",maxTaskQueued=\").append(deferredTaskCap == Integer.MAX_VALUE ? \"unbounded\" : deferredTaskCap)\n-\t\t  .append(\",ttl=\").append(ttlSeconds).append(\"s)\");\n+\t\tts.append(\"maxThreads=\").append(maxThreads)\n+\t\t  .append(\",maxTaskQueuedPerThread=\").append(maxTaskQueuedPerThread == Integer.MAX_VALUE ? \"unbounded\" : maxTaskQueuedPerThread)\n+\t\t  .append(\",ttl=\");\n+\t\tif (ttlMillis < 1000) {\n+\t\t\tts.append(ttlMillis).append(\"ms)\");\n+\t\t}\n+\t\telse {\n+\t\t\tts.append(ttlMillis / 1000).append(\"s)\");\n+\t\t}\n \t\treturn ts.toString();\n \t}\n \n+\t/**\n+\t * @return a best effort total count of the spinned up executors\n+\t */\n+\tint estimateSize() {\n+\t\treturn BOUNDED_SERVICES.get(this).get();\n+\t}\n+\n+\t/**\n+\t * @return a best effort total count of the busy executors\n+\t */\n+\tint estimateBusy() {\n+\t\treturn BOUNDED_SERVICES.get(this).busyQueue.size();\n+\t}\n+\n+\t/**\n+\t * @return a best effort total count of the idle executors\n+\t */\n+\tint estimateIdle() {\n+\t\treturn BOUNDED_SERVICES.get(this).idleQueue.size();\n+\t}\n+\n+\t/**\n+\t * Best effort snapshot of the remaining queue capacity for pending tasks across all the backing executors.\n+\t *\n+\t * @return the total task capacity, or {@literal -1} if any backing executor's task queue size cannot be instrumented\n+\t */\n+\tint estimateRemainingTaskCapacity() {\n+\t\tQueue<BoundedState> busyQueue = BOUNDED_SERVICES.get(this).busyQueue;\n+\t\tint totalTaskCapacity = maxTaskQueuedPerThread * maxThreads;\n+\t\tfor (BoundedState state : busyQueue) {\n+\t\t\tint stateQueueSize = state.estimateQueueSize();\n+\t\t\tif (stateQueueSize >= 0) {\n+\t\t\t\ttotalTaskCapacity -= stateQueueSize;\n+\t\t\t}\n+\t\t\telse {\n+\t\t\t\treturn -1;\n+\t\t\t}\n+\t\t}\n+\t\treturn totalTaskCapacity;\n+\t}\n+\n \t@Override\n \tpublic Object scanUnsafe(Attr key) {\n \t\tif (key == Attr.TERMINATED || key == Attr.CANCELLED) return isDisposed();\n-\t\tif (key == Attr.CAPACITY) return threadCap;\n-\t\tif (key == Attr.BUFFERED) return idleServicesWithExpiry.size(); //BUFFERED: number of workers alive and backed by thread\n+\t\tif (key == Attr.BUFFERED) return estimateSize();\n+\t\tif (key == Attr.CAPACITY) return maxThreads;\n \t\tif (key == Attr.NAME) return this.toString();\n \n \t\treturn null;\n \t}\n \n \t@Override\n-\t\t//TODO re-evaluate the inners? should these include deferredWorkers? allServices?\n \tpublic Stream<? extends Scannable> inners() {\n-\t\treturn idleServicesWithExpiry.stream()\n-\t\t                             .map(cached -> cached.cached);\n+\t\tBoundedServices services = BOUNDED_SERVICES.get(this);\n+\t\treturn Stream.concat(services.busyQueue.stream(), services.idleQueue.stream())\n+\t\t             .filter(obj -> obj != null && obj != CREATING);\n \t}\n \n-\tvoid eviction(LongSupplier nowSupplier) {\n-\t\tlong now = nowSupplier.getAsLong();\n-\t\tList<CachedServiceExpiry> list = new ArrayList<>(idleServicesWithExpiry);\n-\t\tfor (CachedServiceExpiry e : list) {\n-\t\t\tif (e.expireMillis < now) {\n-\t\t\t\tif (idleServicesWithExpiry.remove(e)) {\n-\t\t\t\t\te.cached.exec.shutdownNow();\n-\t\t\t\t\tallServices.remove(e.cached);\n-\t\t\t\t\tREMAINING_THREADS.incrementAndGet(this);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n+\t@Override\n+\tpublic Worker createWorker() {\n+\t\tBoundedState picked = BOUNDED_SERVICES.get(this)\n+\t\t                                      .pick();\n+\t\tExecutorServiceWorker worker = new ExecutorServiceWorker(picked.executor);\n+\t\tworker.tasks.add(picked); //this ensures the BoundedState will be released when worker is disposed\n+\t\treturn worker;\n \t}\n \n-\tstatic final class CachedService implements Disposable, Scannable {\n \n-\t\tfinal BoundedElasticScheduler  parent;\n-\t\tfinal ScheduledExecutorService exec;\n+\tstatic final class BoundedServices extends AtomicInteger implements Disposable {\n \n-\t\tCachedService(@Nullable BoundedElasticScheduler parent) {\n+\t\tfinal BoundedElasticScheduler             parent;\n+\t\t//duplicated Clock field from parent so that SHUTDOWN can be instantiated and partially used\n+\t\tfinal Clock                               clock;\n+\t\tfinal Deque<BoundedState>                 idleQueue;\n+\t\tfinal PriorityBlockingQueue<BoundedState> busyQueue;\n+\n+\t\t//constructor for SHUTDOWN\n+\t\tprivate BoundedServices() {\n+\t\t\tthis.parent = null;\n+\t\t\tthis.clock = Clock.fixed(Instant.EPOCH, ZoneId.systemDefault());\n+\t\t\tthis.busyQueue = new PriorityBlockingQueue<>();\n+\t\t\tthis.idleQueue = new ConcurrentLinkedDeque<>();\n+\t\t}\n+\n+\t\tBoundedServices(BoundedElasticScheduler parent) {\n \t\t\tthis.parent = parent;\n-\t\t\tif (parent != null) {\n-\t\t\t\tthis.exec = Schedulers.decorateExecutorService(parent, parent.get());\n-\t\t\t}\n-\t\t\telse {\n-\t\t\t\tthis.exec = Executors.newSingleThreadScheduledExecutor();\n-\t\t\t\tthis.exec.shutdownNow();\n+\t\t\tthis.clock = parent.clock;\n+\t\t\tthis.busyQueue = new PriorityBlockingQueue<>(parent.maxThreads,\n+\t\t\t\t\tComparator.comparingInt(bs -> bs.markCount));\n+\t\t\tthis.idleQueue = new ConcurrentLinkedDeque<>();\n+\t\t}\n+\n+\t\t/**\n+\t\t * Trigger the eviction by computing the oldest acceptable timestamp and letting each {@link BoundedState}\n+\t\t * check (and potentially shutdown) itself.\n+\t\t */\n+\t\tvoid eviction() {\n+\t\t\tfinal long evictionTimestamp = parent.clock.millis();\n+\t\t\tList<BoundedState> idleCandidates = new ArrayList<>(idleQueue);\n+\t\t\tfor (BoundedState candidate : idleCandidates) {\n+\t\t\t\tif (candidate.tryEvict(evictionTimestamp, parent.ttlMillis)) {\n+\t\t\t\t\tidleQueue.remove(candidate);\n+\t\t\t\t\tdecrementAndGet();\n+\t\t\t\t}\n \t\t\t}\n \t\t}\n \n-\t\t@Override\n-\t\tpublic void dispose() {\n-\t\t\tif (exec != null) {\n-\t\t\t\tif (this != SHUTDOWN && !parent.shutdown) {\n-\t\t\t\t\t//in case of work, re-create an ActiveWorker\n-\t\t\t\t\tDeferredFacade deferredFacade = parent.deferredFacades.poll();\n-\t\t\t\t\tif (deferredFacade != null) {\n-\t\t\t\t\t\tdeferredFacade.setService(this);\n+\t\t/**\n+\t\t * Pick a {@link BoundedState}, prioritizing idle ones then spinning up a new one if enough capacity.\n+\t\t * Otherwise, picks an active one by taking from a {@link PriorityQueue}. The picking is\n+\t\t * optimistically re-attempted if the picked slot cannot be marked as picked.\n+\t\t *\n+\t\t * @return the picked {@link BoundedState}\n+\t\t */\n+\t\tBoundedState pick() {\n+\t\t\tfor (;;) {\n+\t\t\t\tint a = get();\n+\t\t\t\tif (a == DISPOSED) {\n+\t\t\t\t\treturn CREATING; //synonym for shutdown, since the underlying executor is shut down\n+\t\t\t\t}\n+\n+\t\t\t\tif (!idleQueue.isEmpty()) {\n+\t\t\t\t\t//try to find an idle resource\n+\t\t\t\t\tBoundedState bs = idleQueue.pollLast();\n+\t\t\t\t\tif (bs != null && bs.markPicked()) {\n+\t\t\t\t\t\tbusyQueue.add(bs);\n+\t\t\t\t\t\treturn bs;\n \t\t\t\t\t}\n-\t\t\t\t\telse {\n-\t\t\t\t\t\t//if no more work, the service is put back at end of the cached queue and new expiry is started\n-\t\t\t\t\t\tCachedServiceExpiry e = new CachedServiceExpiry(this,\n-\t\t\t\t\t\t\t\tSystem.currentTimeMillis() + parent.ttlSeconds * 1000L);\n-\t\t\t\t\t\tparent.idleServicesWithExpiry.offerLast(e);\n-\t\t\t\t\t\tif (parent.shutdown) {\n-\t\t\t\t\t\t\tif (parent.idleServicesWithExpiry.remove(e)) {\n-\t\t\t\t\t\t\t\texec.shutdownNow();\n-\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t}\n+\t\t\t\t\t//else optimistically retry", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "baf4aa6f21d00dc634fd7a74114e08909e0e3408"}, "originalPosition": 653}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU4OTA0NTAx", "url": "https://github.com/reactor/reactor-core/pull/2040#pullrequestreview-358904501", "createdAt": "2020-02-14T12:21:38Z", "commit": {"oid": "baf4aa6f21d00dc634fd7a74114e08909e0e3408"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQxMjoyMTozOFrOFp05sw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQxMjoyMTozOFrOFp05sw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTQwMjY3NQ==", "bodyText": "nit: BoundedState s; and s =  can be merged", "url": "https://github.com/reactor/reactor-core/pull/2040#discussion_r379402675", "createdAt": "2020-02-14T12:21:38Z", "author": {"login": "bsideup"}, "path": "reactor-core/src/main/java/reactor/core/scheduler/BoundedElasticScheduler.java", "diffHunk": "@@ -319,492 +244,554 @@ public String toString() {\n \t\tif (factory instanceof ReactorThreadFactory) {\n \t\t\tts.append('\\\"').append(((ReactorThreadFactory) factory).get()).append(\"\\\",\");\n \t\t}\n-\t\tts.append(\"maxThreads=\").append(threadCap)\n-\t\t  .append(\",maxTaskQueued=\").append(deferredTaskCap == Integer.MAX_VALUE ? \"unbounded\" : deferredTaskCap)\n-\t\t  .append(\",ttl=\").append(ttlSeconds).append(\"s)\");\n+\t\tts.append(\"maxThreads=\").append(maxThreads)\n+\t\t  .append(\",maxTaskQueuedPerThread=\").append(maxTaskQueuedPerThread == Integer.MAX_VALUE ? \"unbounded\" : maxTaskQueuedPerThread)\n+\t\t  .append(\",ttl=\");\n+\t\tif (ttlMillis < 1000) {\n+\t\t\tts.append(ttlMillis).append(\"ms)\");\n+\t\t}\n+\t\telse {\n+\t\t\tts.append(ttlMillis / 1000).append(\"s)\");\n+\t\t}\n \t\treturn ts.toString();\n \t}\n \n+\t/**\n+\t * @return a best effort total count of the spinned up executors\n+\t */\n+\tint estimateSize() {\n+\t\treturn BOUNDED_SERVICES.get(this).get();\n+\t}\n+\n+\t/**\n+\t * @return a best effort total count of the busy executors\n+\t */\n+\tint estimateBusy() {\n+\t\treturn BOUNDED_SERVICES.get(this).busyQueue.size();\n+\t}\n+\n+\t/**\n+\t * @return a best effort total count of the idle executors\n+\t */\n+\tint estimateIdle() {\n+\t\treturn BOUNDED_SERVICES.get(this).idleQueue.size();\n+\t}\n+\n+\t/**\n+\t * Best effort snapshot of the remaining queue capacity for pending tasks across all the backing executors.\n+\t *\n+\t * @return the total task capacity, or {@literal -1} if any backing executor's task queue size cannot be instrumented\n+\t */\n+\tint estimateRemainingTaskCapacity() {\n+\t\tQueue<BoundedState> busyQueue = BOUNDED_SERVICES.get(this).busyQueue;\n+\t\tint totalTaskCapacity = maxTaskQueuedPerThread * maxThreads;\n+\t\tfor (BoundedState state : busyQueue) {\n+\t\t\tint stateQueueSize = state.estimateQueueSize();\n+\t\t\tif (stateQueueSize >= 0) {\n+\t\t\t\ttotalTaskCapacity -= stateQueueSize;\n+\t\t\t}\n+\t\t\telse {\n+\t\t\t\treturn -1;\n+\t\t\t}\n+\t\t}\n+\t\treturn totalTaskCapacity;\n+\t}\n+\n \t@Override\n \tpublic Object scanUnsafe(Attr key) {\n \t\tif (key == Attr.TERMINATED || key == Attr.CANCELLED) return isDisposed();\n-\t\tif (key == Attr.CAPACITY) return threadCap;\n-\t\tif (key == Attr.BUFFERED) return idleServicesWithExpiry.size(); //BUFFERED: number of workers alive and backed by thread\n+\t\tif (key == Attr.BUFFERED) return estimateSize();\n+\t\tif (key == Attr.CAPACITY) return maxThreads;\n \t\tif (key == Attr.NAME) return this.toString();\n \n \t\treturn null;\n \t}\n \n \t@Override\n-\t\t//TODO re-evaluate the inners? should these include deferredWorkers? allServices?\n \tpublic Stream<? extends Scannable> inners() {\n-\t\treturn idleServicesWithExpiry.stream()\n-\t\t                             .map(cached -> cached.cached);\n+\t\tBoundedServices services = BOUNDED_SERVICES.get(this);\n+\t\treturn Stream.concat(services.busyQueue.stream(), services.idleQueue.stream())\n+\t\t             .filter(obj -> obj != null && obj != CREATING);\n \t}\n \n-\tvoid eviction(LongSupplier nowSupplier) {\n-\t\tlong now = nowSupplier.getAsLong();\n-\t\tList<CachedServiceExpiry> list = new ArrayList<>(idleServicesWithExpiry);\n-\t\tfor (CachedServiceExpiry e : list) {\n-\t\t\tif (e.expireMillis < now) {\n-\t\t\t\tif (idleServicesWithExpiry.remove(e)) {\n-\t\t\t\t\te.cached.exec.shutdownNow();\n-\t\t\t\t\tallServices.remove(e.cached);\n-\t\t\t\t\tREMAINING_THREADS.incrementAndGet(this);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n+\t@Override\n+\tpublic Worker createWorker() {\n+\t\tBoundedState picked = BOUNDED_SERVICES.get(this)\n+\t\t                                      .pick();\n+\t\tExecutorServiceWorker worker = new ExecutorServiceWorker(picked.executor);\n+\t\tworker.tasks.add(picked); //this ensures the BoundedState will be released when worker is disposed\n+\t\treturn worker;\n \t}\n \n-\tstatic final class CachedService implements Disposable, Scannable {\n \n-\t\tfinal BoundedElasticScheduler  parent;\n-\t\tfinal ScheduledExecutorService exec;\n+\tstatic final class BoundedServices extends AtomicInteger implements Disposable {\n \n-\t\tCachedService(@Nullable BoundedElasticScheduler parent) {\n+\t\tfinal BoundedElasticScheduler             parent;\n+\t\t//duplicated Clock field from parent so that SHUTDOWN can be instantiated and partially used\n+\t\tfinal Clock                               clock;\n+\t\tfinal Deque<BoundedState>                 idleQueue;\n+\t\tfinal PriorityBlockingQueue<BoundedState> busyQueue;\n+\n+\t\t//constructor for SHUTDOWN\n+\t\tprivate BoundedServices() {\n+\t\t\tthis.parent = null;\n+\t\t\tthis.clock = Clock.fixed(Instant.EPOCH, ZoneId.systemDefault());\n+\t\t\tthis.busyQueue = new PriorityBlockingQueue<>();\n+\t\t\tthis.idleQueue = new ConcurrentLinkedDeque<>();\n+\t\t}\n+\n+\t\tBoundedServices(BoundedElasticScheduler parent) {\n \t\t\tthis.parent = parent;\n-\t\t\tif (parent != null) {\n-\t\t\t\tthis.exec = Schedulers.decorateExecutorService(parent, parent.get());\n-\t\t\t}\n-\t\t\telse {\n-\t\t\t\tthis.exec = Executors.newSingleThreadScheduledExecutor();\n-\t\t\t\tthis.exec.shutdownNow();\n+\t\t\tthis.clock = parent.clock;\n+\t\t\tthis.busyQueue = new PriorityBlockingQueue<>(parent.maxThreads,\n+\t\t\t\t\tComparator.comparingInt(bs -> bs.markCount));\n+\t\t\tthis.idleQueue = new ConcurrentLinkedDeque<>();\n+\t\t}\n+\n+\t\t/**\n+\t\t * Trigger the eviction by computing the oldest acceptable timestamp and letting each {@link BoundedState}\n+\t\t * check (and potentially shutdown) itself.\n+\t\t */\n+\t\tvoid eviction() {\n+\t\t\tfinal long evictionTimestamp = parent.clock.millis();\n+\t\t\tList<BoundedState> idleCandidates = new ArrayList<>(idleQueue);\n+\t\t\tfor (BoundedState candidate : idleCandidates) {\n+\t\t\t\tif (candidate.tryEvict(evictionTimestamp, parent.ttlMillis)) {\n+\t\t\t\t\tidleQueue.remove(candidate);\n+\t\t\t\t\tdecrementAndGet();\n+\t\t\t\t}\n \t\t\t}\n \t\t}\n \n-\t\t@Override\n-\t\tpublic void dispose() {\n-\t\t\tif (exec != null) {\n-\t\t\t\tif (this != SHUTDOWN && !parent.shutdown) {\n-\t\t\t\t\t//in case of work, re-create an ActiveWorker\n-\t\t\t\t\tDeferredFacade deferredFacade = parent.deferredFacades.poll();\n-\t\t\t\t\tif (deferredFacade != null) {\n-\t\t\t\t\t\tdeferredFacade.setService(this);\n+\t\t/**\n+\t\t * Pick a {@link BoundedState}, prioritizing idle ones then spinning up a new one if enough capacity.\n+\t\t * Otherwise, picks an active one by taking from a {@link PriorityQueue}. The picking is\n+\t\t * optimistically re-attempted if the picked slot cannot be marked as picked.\n+\t\t *\n+\t\t * @return the picked {@link BoundedState}\n+\t\t */\n+\t\tBoundedState pick() {\n+\t\t\tfor (;;) {\n+\t\t\t\tint a = get();\n+\t\t\t\tif (a == DISPOSED) {\n+\t\t\t\t\treturn CREATING; //synonym for shutdown, since the underlying executor is shut down\n+\t\t\t\t}\n+\n+\t\t\t\tif (!idleQueue.isEmpty()) {\n+\t\t\t\t\t//try to find an idle resource\n+\t\t\t\t\tBoundedState bs = idleQueue.pollLast();\n+\t\t\t\t\tif (bs != null && bs.markPicked()) {\n+\t\t\t\t\t\tbusyQueue.add(bs);\n+\t\t\t\t\t\treturn bs;\n \t\t\t\t\t}\n-\t\t\t\t\telse {\n-\t\t\t\t\t\t//if no more work, the service is put back at end of the cached queue and new expiry is started\n-\t\t\t\t\t\tCachedServiceExpiry e = new CachedServiceExpiry(this,\n-\t\t\t\t\t\t\t\tSystem.currentTimeMillis() + parent.ttlSeconds * 1000L);\n-\t\t\t\t\t\tparent.idleServicesWithExpiry.offerLast(e);\n-\t\t\t\t\t\tif (parent.shutdown) {\n-\t\t\t\t\t\t\tif (parent.idleServicesWithExpiry.remove(e)) {\n-\t\t\t\t\t\t\t\texec.shutdownNow();\n-\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t}\n+\t\t\t\t\t//else optimistically retry\n+\t\t\t\t}\n+\t\t\t\telse if (a < parent.maxThreads) {\n+\t\t\t\t\t//try to build a new resource\n+\t\t\t\t\tif (compareAndSet(a, a + 1)) {\n+\t\t\t\t\t\tScheduledExecutorService s = Schedulers.decorateExecutorService(parent, parent.createBoundedExecutorService());\n+\t\t\t\t\t\tBoundedState newState = new BoundedState(this, s);\n+\t\t\t\t\t\tnewState.markPicked(); //no need to check return value as this one is brand new\n+\t\t\t\t\t\tbusyQueue.add(newState);\n+\t\t\t\t\t\treturn newState;\n+\t\t\t\t\t}\n+\t\t\t\t\t//else optimistically retry\n+\t\t\t\t}\n+\t\t\t\telse {\n+\t\t\t\t\t//pick the least busy one\n+\t\t\t\t\tBoundedState s;\n+\t\t\t\t\ts = busyQueue.poll();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "baf4aa6f21d00dc634fd7a74114e08909e0e3408"}, "originalPosition": 669}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU4OTA2MTA0", "url": "https://github.com/reactor/reactor-core/pull/2040#pullrequestreview-358906104", "createdAt": "2020-02-14T12:24:56Z", "commit": {"oid": "baf4aa6f21d00dc634fd7a74114e08909e0e3408"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQxMjoyNDo1NlrOFp0-dQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQxMjoyNDo1NlrOFp0-dQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTQwMzg5Mw==", "bodyText": "We need to be careful with the order here, otherwise:\n\nwe add it to idleQueue\nin another thread, pick() polls it and adds to the busy queue and returns\nnext statement removes it from the busy queue", "url": "https://github.com/reactor/reactor-core/pull/2040#discussion_r379403893", "createdAt": "2020-02-14T12:24:56Z", "author": {"login": "bsideup"}, "path": "reactor-core/src/main/java/reactor/core/scheduler/BoundedElasticScheduler.java", "diffHunk": "@@ -319,492 +244,554 @@ public String toString() {\n \t\tif (factory instanceof ReactorThreadFactory) {\n \t\t\tts.append('\\\"').append(((ReactorThreadFactory) factory).get()).append(\"\\\",\");\n \t\t}\n-\t\tts.append(\"maxThreads=\").append(threadCap)\n-\t\t  .append(\",maxTaskQueued=\").append(deferredTaskCap == Integer.MAX_VALUE ? \"unbounded\" : deferredTaskCap)\n-\t\t  .append(\",ttl=\").append(ttlSeconds).append(\"s)\");\n+\t\tts.append(\"maxThreads=\").append(maxThreads)\n+\t\t  .append(\",maxTaskQueuedPerThread=\").append(maxTaskQueuedPerThread == Integer.MAX_VALUE ? \"unbounded\" : maxTaskQueuedPerThread)\n+\t\t  .append(\",ttl=\");\n+\t\tif (ttlMillis < 1000) {\n+\t\t\tts.append(ttlMillis).append(\"ms)\");\n+\t\t}\n+\t\telse {\n+\t\t\tts.append(ttlMillis / 1000).append(\"s)\");\n+\t\t}\n \t\treturn ts.toString();\n \t}\n \n+\t/**\n+\t * @return a best effort total count of the spinned up executors\n+\t */\n+\tint estimateSize() {\n+\t\treturn BOUNDED_SERVICES.get(this).get();\n+\t}\n+\n+\t/**\n+\t * @return a best effort total count of the busy executors\n+\t */\n+\tint estimateBusy() {\n+\t\treturn BOUNDED_SERVICES.get(this).busyQueue.size();\n+\t}\n+\n+\t/**\n+\t * @return a best effort total count of the idle executors\n+\t */\n+\tint estimateIdle() {\n+\t\treturn BOUNDED_SERVICES.get(this).idleQueue.size();\n+\t}\n+\n+\t/**\n+\t * Best effort snapshot of the remaining queue capacity for pending tasks across all the backing executors.\n+\t *\n+\t * @return the total task capacity, or {@literal -1} if any backing executor's task queue size cannot be instrumented\n+\t */\n+\tint estimateRemainingTaskCapacity() {\n+\t\tQueue<BoundedState> busyQueue = BOUNDED_SERVICES.get(this).busyQueue;\n+\t\tint totalTaskCapacity = maxTaskQueuedPerThread * maxThreads;\n+\t\tfor (BoundedState state : busyQueue) {\n+\t\t\tint stateQueueSize = state.estimateQueueSize();\n+\t\t\tif (stateQueueSize >= 0) {\n+\t\t\t\ttotalTaskCapacity -= stateQueueSize;\n+\t\t\t}\n+\t\t\telse {\n+\t\t\t\treturn -1;\n+\t\t\t}\n+\t\t}\n+\t\treturn totalTaskCapacity;\n+\t}\n+\n \t@Override\n \tpublic Object scanUnsafe(Attr key) {\n \t\tif (key == Attr.TERMINATED || key == Attr.CANCELLED) return isDisposed();\n-\t\tif (key == Attr.CAPACITY) return threadCap;\n-\t\tif (key == Attr.BUFFERED) return idleServicesWithExpiry.size(); //BUFFERED: number of workers alive and backed by thread\n+\t\tif (key == Attr.BUFFERED) return estimateSize();\n+\t\tif (key == Attr.CAPACITY) return maxThreads;\n \t\tif (key == Attr.NAME) return this.toString();\n \n \t\treturn null;\n \t}\n \n \t@Override\n-\t\t//TODO re-evaluate the inners? should these include deferredWorkers? allServices?\n \tpublic Stream<? extends Scannable> inners() {\n-\t\treturn idleServicesWithExpiry.stream()\n-\t\t                             .map(cached -> cached.cached);\n+\t\tBoundedServices services = BOUNDED_SERVICES.get(this);\n+\t\treturn Stream.concat(services.busyQueue.stream(), services.idleQueue.stream())\n+\t\t             .filter(obj -> obj != null && obj != CREATING);\n \t}\n \n-\tvoid eviction(LongSupplier nowSupplier) {\n-\t\tlong now = nowSupplier.getAsLong();\n-\t\tList<CachedServiceExpiry> list = new ArrayList<>(idleServicesWithExpiry);\n-\t\tfor (CachedServiceExpiry e : list) {\n-\t\t\tif (e.expireMillis < now) {\n-\t\t\t\tif (idleServicesWithExpiry.remove(e)) {\n-\t\t\t\t\te.cached.exec.shutdownNow();\n-\t\t\t\t\tallServices.remove(e.cached);\n-\t\t\t\t\tREMAINING_THREADS.incrementAndGet(this);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n+\t@Override\n+\tpublic Worker createWorker() {\n+\t\tBoundedState picked = BOUNDED_SERVICES.get(this)\n+\t\t                                      .pick();\n+\t\tExecutorServiceWorker worker = new ExecutorServiceWorker(picked.executor);\n+\t\tworker.tasks.add(picked); //this ensures the BoundedState will be released when worker is disposed\n+\t\treturn worker;\n \t}\n \n-\tstatic final class CachedService implements Disposable, Scannable {\n \n-\t\tfinal BoundedElasticScheduler  parent;\n-\t\tfinal ScheduledExecutorService exec;\n+\tstatic final class BoundedServices extends AtomicInteger implements Disposable {\n \n-\t\tCachedService(@Nullable BoundedElasticScheduler parent) {\n+\t\tfinal BoundedElasticScheduler             parent;\n+\t\t//duplicated Clock field from parent so that SHUTDOWN can be instantiated and partially used\n+\t\tfinal Clock                               clock;\n+\t\tfinal Deque<BoundedState>                 idleQueue;\n+\t\tfinal PriorityBlockingQueue<BoundedState> busyQueue;\n+\n+\t\t//constructor for SHUTDOWN\n+\t\tprivate BoundedServices() {\n+\t\t\tthis.parent = null;\n+\t\t\tthis.clock = Clock.fixed(Instant.EPOCH, ZoneId.systemDefault());\n+\t\t\tthis.busyQueue = new PriorityBlockingQueue<>();\n+\t\t\tthis.idleQueue = new ConcurrentLinkedDeque<>();\n+\t\t}\n+\n+\t\tBoundedServices(BoundedElasticScheduler parent) {\n \t\t\tthis.parent = parent;\n-\t\t\tif (parent != null) {\n-\t\t\t\tthis.exec = Schedulers.decorateExecutorService(parent, parent.get());\n-\t\t\t}\n-\t\t\telse {\n-\t\t\t\tthis.exec = Executors.newSingleThreadScheduledExecutor();\n-\t\t\t\tthis.exec.shutdownNow();\n+\t\t\tthis.clock = parent.clock;\n+\t\t\tthis.busyQueue = new PriorityBlockingQueue<>(parent.maxThreads,\n+\t\t\t\t\tComparator.comparingInt(bs -> bs.markCount));\n+\t\t\tthis.idleQueue = new ConcurrentLinkedDeque<>();\n+\t\t}\n+\n+\t\t/**\n+\t\t * Trigger the eviction by computing the oldest acceptable timestamp and letting each {@link BoundedState}\n+\t\t * check (and potentially shutdown) itself.\n+\t\t */\n+\t\tvoid eviction() {\n+\t\t\tfinal long evictionTimestamp = parent.clock.millis();\n+\t\t\tList<BoundedState> idleCandidates = new ArrayList<>(idleQueue);\n+\t\t\tfor (BoundedState candidate : idleCandidates) {\n+\t\t\t\tif (candidate.tryEvict(evictionTimestamp, parent.ttlMillis)) {\n+\t\t\t\t\tidleQueue.remove(candidate);\n+\t\t\t\t\tdecrementAndGet();\n+\t\t\t\t}\n \t\t\t}\n \t\t}\n \n-\t\t@Override\n-\t\tpublic void dispose() {\n-\t\t\tif (exec != null) {\n-\t\t\t\tif (this != SHUTDOWN && !parent.shutdown) {\n-\t\t\t\t\t//in case of work, re-create an ActiveWorker\n-\t\t\t\t\tDeferredFacade deferredFacade = parent.deferredFacades.poll();\n-\t\t\t\t\tif (deferredFacade != null) {\n-\t\t\t\t\t\tdeferredFacade.setService(this);\n+\t\t/**\n+\t\t * Pick a {@link BoundedState}, prioritizing idle ones then spinning up a new one if enough capacity.\n+\t\t * Otherwise, picks an active one by taking from a {@link PriorityQueue}. The picking is\n+\t\t * optimistically re-attempted if the picked slot cannot be marked as picked.\n+\t\t *\n+\t\t * @return the picked {@link BoundedState}\n+\t\t */\n+\t\tBoundedState pick() {\n+\t\t\tfor (;;) {\n+\t\t\t\tint a = get();\n+\t\t\t\tif (a == DISPOSED) {\n+\t\t\t\t\treturn CREATING; //synonym for shutdown, since the underlying executor is shut down\n+\t\t\t\t}\n+\n+\t\t\t\tif (!idleQueue.isEmpty()) {\n+\t\t\t\t\t//try to find an idle resource\n+\t\t\t\t\tBoundedState bs = idleQueue.pollLast();\n+\t\t\t\t\tif (bs != null && bs.markPicked()) {\n+\t\t\t\t\t\tbusyQueue.add(bs);\n+\t\t\t\t\t\treturn bs;\n \t\t\t\t\t}\n-\t\t\t\t\telse {\n-\t\t\t\t\t\t//if no more work, the service is put back at end of the cached queue and new expiry is started\n-\t\t\t\t\t\tCachedServiceExpiry e = new CachedServiceExpiry(this,\n-\t\t\t\t\t\t\t\tSystem.currentTimeMillis() + parent.ttlSeconds * 1000L);\n-\t\t\t\t\t\tparent.idleServicesWithExpiry.offerLast(e);\n-\t\t\t\t\t\tif (parent.shutdown) {\n-\t\t\t\t\t\t\tif (parent.idleServicesWithExpiry.remove(e)) {\n-\t\t\t\t\t\t\t\texec.shutdownNow();\n-\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t}\n+\t\t\t\t\t//else optimistically retry\n+\t\t\t\t}\n+\t\t\t\telse if (a < parent.maxThreads) {\n+\t\t\t\t\t//try to build a new resource\n+\t\t\t\t\tif (compareAndSet(a, a + 1)) {\n+\t\t\t\t\t\tScheduledExecutorService s = Schedulers.decorateExecutorService(parent, parent.createBoundedExecutorService());\n+\t\t\t\t\t\tBoundedState newState = new BoundedState(this, s);\n+\t\t\t\t\t\tnewState.markPicked(); //no need to check return value as this one is brand new\n+\t\t\t\t\t\tbusyQueue.add(newState);\n+\t\t\t\t\t\treturn newState;\n+\t\t\t\t\t}\n+\t\t\t\t\t//else optimistically retry\n+\t\t\t\t}\n+\t\t\t\telse {\n+\t\t\t\t\t//pick the least busy one\n+\t\t\t\t\tBoundedState s;\n+\t\t\t\t\ts = busyQueue.poll();\n+\t\t\t\t\tif (s != null && s.markPicked()) {\n+\t\t\t\t\t\tbusyQueue.add(s); //put it back in the queue with updated priority\n+\t\t\t\t\t\treturn s;\n \t\t\t\t\t}\n+\t\t\t\t\t//else optimistically retry\n \t\t\t\t}\n \t\t\t}\n \t\t}\n \n-\t\t@Override\n-\t\tpublic Object scanUnsafe(Attr key) {\n-\t\t\tif (key == Attr.NAME) return parent.scanUnsafe(key);\n-\t\t\tif (key == Attr.PARENT) return parent;\n-\t\t\tif (key == Attr.TERMINATED || key == Attr.CANCELLED) return isDisposed();\n-\t\t\tif (key == Attr.CAPACITY) {\n-\t\t\t\t//assume 1 if unknown, otherwise use the one from underlying executor\n-\t\t\t\tInteger capacity = (Integer) Schedulers.scanExecutor(exec, key);\n-\t\t\t\tif (capacity == null || capacity == -1) return 1;\n-\t\t\t}\n-\t\t\treturn Schedulers.scanExecutor(exec, key);\n+\t\tvoid setIdle(BoundedState boundedState) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "baf4aa6f21d00dc634fd7a74114e08909e0e3408"}, "originalPosition": 690}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU4OTE0Njkw", "url": "https://github.com/reactor/reactor-core/pull/2040#pullrequestreview-358914690", "createdAt": "2020-02-14T12:42:05Z", "commit": {"oid": "baf4aa6f21d00dc634fd7a74114e08909e0e3408"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQxMjo0MjowNVrOFp1YoQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQxMjo0MjowNVrOFp1YoQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTQxMDU5Mw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \t\t * give ttlMillis. When eligible for eviction, the executor is shut down and the\n          \n          \n            \n            \t\t * given ttlMillis. When eligible for eviction, the executor is shut down and the", "url": "https://github.com/reactor/reactor-core/pull/2040#discussion_r379410593", "createdAt": "2020-02-14T12:42:05Z", "author": {"login": "bsideup"}, "path": "reactor-core/src/main/java/reactor/core/scheduler/BoundedElasticScheduler.java", "diffHunk": "@@ -319,492 +244,554 @@ public String toString() {\n \t\tif (factory instanceof ReactorThreadFactory) {\n \t\t\tts.append('\\\"').append(((ReactorThreadFactory) factory).get()).append(\"\\\",\");\n \t\t}\n-\t\tts.append(\"maxThreads=\").append(threadCap)\n-\t\t  .append(\",maxTaskQueued=\").append(deferredTaskCap == Integer.MAX_VALUE ? \"unbounded\" : deferredTaskCap)\n-\t\t  .append(\",ttl=\").append(ttlSeconds).append(\"s)\");\n+\t\tts.append(\"maxThreads=\").append(maxThreads)\n+\t\t  .append(\",maxTaskQueuedPerThread=\").append(maxTaskQueuedPerThread == Integer.MAX_VALUE ? \"unbounded\" : maxTaskQueuedPerThread)\n+\t\t  .append(\",ttl=\");\n+\t\tif (ttlMillis < 1000) {\n+\t\t\tts.append(ttlMillis).append(\"ms)\");\n+\t\t}\n+\t\telse {\n+\t\t\tts.append(ttlMillis / 1000).append(\"s)\");\n+\t\t}\n \t\treturn ts.toString();\n \t}\n \n+\t/**\n+\t * @return a best effort total count of the spinned up executors\n+\t */\n+\tint estimateSize() {\n+\t\treturn BOUNDED_SERVICES.get(this).get();\n+\t}\n+\n+\t/**\n+\t * @return a best effort total count of the busy executors\n+\t */\n+\tint estimateBusy() {\n+\t\treturn BOUNDED_SERVICES.get(this).busyQueue.size();\n+\t}\n+\n+\t/**\n+\t * @return a best effort total count of the idle executors\n+\t */\n+\tint estimateIdle() {\n+\t\treturn BOUNDED_SERVICES.get(this).idleQueue.size();\n+\t}\n+\n+\t/**\n+\t * Best effort snapshot of the remaining queue capacity for pending tasks across all the backing executors.\n+\t *\n+\t * @return the total task capacity, or {@literal -1} if any backing executor's task queue size cannot be instrumented\n+\t */\n+\tint estimateRemainingTaskCapacity() {\n+\t\tQueue<BoundedState> busyQueue = BOUNDED_SERVICES.get(this).busyQueue;\n+\t\tint totalTaskCapacity = maxTaskQueuedPerThread * maxThreads;\n+\t\tfor (BoundedState state : busyQueue) {\n+\t\t\tint stateQueueSize = state.estimateQueueSize();\n+\t\t\tif (stateQueueSize >= 0) {\n+\t\t\t\ttotalTaskCapacity -= stateQueueSize;\n+\t\t\t}\n+\t\t\telse {\n+\t\t\t\treturn -1;\n+\t\t\t}\n+\t\t}\n+\t\treturn totalTaskCapacity;\n+\t}\n+\n \t@Override\n \tpublic Object scanUnsafe(Attr key) {\n \t\tif (key == Attr.TERMINATED || key == Attr.CANCELLED) return isDisposed();\n-\t\tif (key == Attr.CAPACITY) return threadCap;\n-\t\tif (key == Attr.BUFFERED) return idleServicesWithExpiry.size(); //BUFFERED: number of workers alive and backed by thread\n+\t\tif (key == Attr.BUFFERED) return estimateSize();\n+\t\tif (key == Attr.CAPACITY) return maxThreads;\n \t\tif (key == Attr.NAME) return this.toString();\n \n \t\treturn null;\n \t}\n \n \t@Override\n-\t\t//TODO re-evaluate the inners? should these include deferredWorkers? allServices?\n \tpublic Stream<? extends Scannable> inners() {\n-\t\treturn idleServicesWithExpiry.stream()\n-\t\t                             .map(cached -> cached.cached);\n+\t\tBoundedServices services = BOUNDED_SERVICES.get(this);\n+\t\treturn Stream.concat(services.busyQueue.stream(), services.idleQueue.stream())\n+\t\t             .filter(obj -> obj != null && obj != CREATING);\n \t}\n \n-\tvoid eviction(LongSupplier nowSupplier) {\n-\t\tlong now = nowSupplier.getAsLong();\n-\t\tList<CachedServiceExpiry> list = new ArrayList<>(idleServicesWithExpiry);\n-\t\tfor (CachedServiceExpiry e : list) {\n-\t\t\tif (e.expireMillis < now) {\n-\t\t\t\tif (idleServicesWithExpiry.remove(e)) {\n-\t\t\t\t\te.cached.exec.shutdownNow();\n-\t\t\t\t\tallServices.remove(e.cached);\n-\t\t\t\t\tREMAINING_THREADS.incrementAndGet(this);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n+\t@Override\n+\tpublic Worker createWorker() {\n+\t\tBoundedState picked = BOUNDED_SERVICES.get(this)\n+\t\t                                      .pick();\n+\t\tExecutorServiceWorker worker = new ExecutorServiceWorker(picked.executor);\n+\t\tworker.tasks.add(picked); //this ensures the BoundedState will be released when worker is disposed\n+\t\treturn worker;\n \t}\n \n-\tstatic final class CachedService implements Disposable, Scannable {\n \n-\t\tfinal BoundedElasticScheduler  parent;\n-\t\tfinal ScheduledExecutorService exec;\n+\tstatic final class BoundedServices extends AtomicInteger implements Disposable {\n \n-\t\tCachedService(@Nullable BoundedElasticScheduler parent) {\n+\t\tfinal BoundedElasticScheduler             parent;\n+\t\t//duplicated Clock field from parent so that SHUTDOWN can be instantiated and partially used\n+\t\tfinal Clock                               clock;\n+\t\tfinal Deque<BoundedState>                 idleQueue;\n+\t\tfinal PriorityBlockingQueue<BoundedState> busyQueue;\n+\n+\t\t//constructor for SHUTDOWN\n+\t\tprivate BoundedServices() {\n+\t\t\tthis.parent = null;\n+\t\t\tthis.clock = Clock.fixed(Instant.EPOCH, ZoneId.systemDefault());\n+\t\t\tthis.busyQueue = new PriorityBlockingQueue<>();\n+\t\t\tthis.idleQueue = new ConcurrentLinkedDeque<>();\n+\t\t}\n+\n+\t\tBoundedServices(BoundedElasticScheduler parent) {\n \t\t\tthis.parent = parent;\n-\t\t\tif (parent != null) {\n-\t\t\t\tthis.exec = Schedulers.decorateExecutorService(parent, parent.get());\n-\t\t\t}\n-\t\t\telse {\n-\t\t\t\tthis.exec = Executors.newSingleThreadScheduledExecutor();\n-\t\t\t\tthis.exec.shutdownNow();\n+\t\t\tthis.clock = parent.clock;\n+\t\t\tthis.busyQueue = new PriorityBlockingQueue<>(parent.maxThreads,\n+\t\t\t\t\tComparator.comparingInt(bs -> bs.markCount));\n+\t\t\tthis.idleQueue = new ConcurrentLinkedDeque<>();\n+\t\t}\n+\n+\t\t/**\n+\t\t * Trigger the eviction by computing the oldest acceptable timestamp and letting each {@link BoundedState}\n+\t\t * check (and potentially shutdown) itself.\n+\t\t */\n+\t\tvoid eviction() {\n+\t\t\tfinal long evictionTimestamp = parent.clock.millis();\n+\t\t\tList<BoundedState> idleCandidates = new ArrayList<>(idleQueue);\n+\t\t\tfor (BoundedState candidate : idleCandidates) {\n+\t\t\t\tif (candidate.tryEvict(evictionTimestamp, parent.ttlMillis)) {\n+\t\t\t\t\tidleQueue.remove(candidate);\n+\t\t\t\t\tdecrementAndGet();\n+\t\t\t\t}\n \t\t\t}\n \t\t}\n \n-\t\t@Override\n-\t\tpublic void dispose() {\n-\t\t\tif (exec != null) {\n-\t\t\t\tif (this != SHUTDOWN && !parent.shutdown) {\n-\t\t\t\t\t//in case of work, re-create an ActiveWorker\n-\t\t\t\t\tDeferredFacade deferredFacade = parent.deferredFacades.poll();\n-\t\t\t\t\tif (deferredFacade != null) {\n-\t\t\t\t\t\tdeferredFacade.setService(this);\n+\t\t/**\n+\t\t * Pick a {@link BoundedState}, prioritizing idle ones then spinning up a new one if enough capacity.\n+\t\t * Otherwise, picks an active one by taking from a {@link PriorityQueue}. The picking is\n+\t\t * optimistically re-attempted if the picked slot cannot be marked as picked.\n+\t\t *\n+\t\t * @return the picked {@link BoundedState}\n+\t\t */\n+\t\tBoundedState pick() {\n+\t\t\tfor (;;) {\n+\t\t\t\tint a = get();\n+\t\t\t\tif (a == DISPOSED) {\n+\t\t\t\t\treturn CREATING; //synonym for shutdown, since the underlying executor is shut down\n+\t\t\t\t}\n+\n+\t\t\t\tif (!idleQueue.isEmpty()) {\n+\t\t\t\t\t//try to find an idle resource\n+\t\t\t\t\tBoundedState bs = idleQueue.pollLast();\n+\t\t\t\t\tif (bs != null && bs.markPicked()) {\n+\t\t\t\t\t\tbusyQueue.add(bs);\n+\t\t\t\t\t\treturn bs;\n \t\t\t\t\t}\n-\t\t\t\t\telse {\n-\t\t\t\t\t\t//if no more work, the service is put back at end of the cached queue and new expiry is started\n-\t\t\t\t\t\tCachedServiceExpiry e = new CachedServiceExpiry(this,\n-\t\t\t\t\t\t\t\tSystem.currentTimeMillis() + parent.ttlSeconds * 1000L);\n-\t\t\t\t\t\tparent.idleServicesWithExpiry.offerLast(e);\n-\t\t\t\t\t\tif (parent.shutdown) {\n-\t\t\t\t\t\t\tif (parent.idleServicesWithExpiry.remove(e)) {\n-\t\t\t\t\t\t\t\texec.shutdownNow();\n-\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t}\n+\t\t\t\t\t//else optimistically retry\n+\t\t\t\t}\n+\t\t\t\telse if (a < parent.maxThreads) {\n+\t\t\t\t\t//try to build a new resource\n+\t\t\t\t\tif (compareAndSet(a, a + 1)) {\n+\t\t\t\t\t\tScheduledExecutorService s = Schedulers.decorateExecutorService(parent, parent.createBoundedExecutorService());\n+\t\t\t\t\t\tBoundedState newState = new BoundedState(this, s);\n+\t\t\t\t\t\tnewState.markPicked(); //no need to check return value as this one is brand new\n+\t\t\t\t\t\tbusyQueue.add(newState);\n+\t\t\t\t\t\treturn newState;\n+\t\t\t\t\t}\n+\t\t\t\t\t//else optimistically retry\n+\t\t\t\t}\n+\t\t\t\telse {\n+\t\t\t\t\t//pick the least busy one\n+\t\t\t\t\tBoundedState s;\n+\t\t\t\t\ts = busyQueue.poll();\n+\t\t\t\t\tif (s != null && s.markPicked()) {\n+\t\t\t\t\t\tbusyQueue.add(s); //put it back in the queue with updated priority\n+\t\t\t\t\t\treturn s;\n \t\t\t\t\t}\n+\t\t\t\t\t//else optimistically retry\n \t\t\t\t}\n \t\t\t}\n \t\t}\n \n-\t\t@Override\n-\t\tpublic Object scanUnsafe(Attr key) {\n-\t\t\tif (key == Attr.NAME) return parent.scanUnsafe(key);\n-\t\t\tif (key == Attr.PARENT) return parent;\n-\t\t\tif (key == Attr.TERMINATED || key == Attr.CANCELLED) return isDisposed();\n-\t\t\tif (key == Attr.CAPACITY) {\n-\t\t\t\t//assume 1 if unknown, otherwise use the one from underlying executor\n-\t\t\t\tInteger capacity = (Integer) Schedulers.scanExecutor(exec, key);\n-\t\t\t\tif (capacity == null || capacity == -1) return 1;\n-\t\t\t}\n-\t\t\treturn Schedulers.scanExecutor(exec, key);\n+\t\tvoid setIdle(BoundedState boundedState) {\n+\t\t\tthis.idleQueue.add(boundedState);\n+\t\t\tthis.busyQueue.remove(boundedState);\n \t\t}\n-\t}\n \n-\tstatic final class CachedServiceExpiry {\n-\n-\t\tfinal CachedService cached;\n-\t\tfinal long          expireMillis;\n+\t\t@Override\n+\t\tpublic boolean isDisposed() {\n+\t\t\treturn get() == DISPOSED;\n+\t\t}\n \n-\t\tCachedServiceExpiry(CachedService cached, long expireMillis) {\n-\t\t\tthis.cached = cached;\n-\t\t\tthis.expireMillis = expireMillis;\n+\t\t@Override\n+\t\tpublic void dispose() {\n+\t\t\tset(DISPOSED);\n+\t\t\tidleQueue.forEach(BoundedState::shutdown);\n+\t\t\tbusyQueue.forEach(BoundedState::shutdown);\n \t\t}\n \t}\n \n-\tstatic final class ActiveWorker extends AtomicBoolean implements Worker, Scannable {\n-\n-\t\tfinal CachedService cached;\n-\t\tfinal Composite tasks;\n+\t/**\n+\t * A class that encapsulate state around the {@link BoundedScheduledExecutorService} and\n+\t * atomically marking them picked/idle.\n+\t */\n+\tstatic class BoundedState implements Disposable, Scannable {\n \n-\t\tActiveWorker(CachedService cached) {\n-\t\t\tthis.cached = cached;\n-\t\t\tthis.tasks = Disposables.composite();\n-\t\t}\n+\t\tfinal BoundedServices          parent;\n+\t\tfinal ScheduledExecutorService executor;\n \n-\t\t@Override\n-\t\tpublic Disposable schedule(Runnable task) {\n-\t\t\treturn Schedulers.workerSchedule(cached.exec,\n-\t\t\t\t\ttasks,\n-\t\t\t\t\ttask,\n-\t\t\t\t\t0L,\n-\t\t\t\t\tTimeUnit.MILLISECONDS);\n-\t\t}\n+\t\tlong idleSinceTimestamp = -1L;\n \n-\t\t@Override\n-\t\tpublic Disposable schedule(Runnable task, long delay, TimeUnit unit) {\n-\t\t\treturn Schedulers.workerSchedule(cached.exec, tasks, task, delay, unit);\n-\t\t}\n+\t\tvolatile int markCount;\n+\t\tstatic final AtomicIntegerFieldUpdater<BoundedState> MARK_COUNT = AtomicIntegerFieldUpdater.newUpdater(BoundedState.class, \"markCount\");\n \n-\t\t@Override\n-\t\tpublic Disposable schedulePeriodically(Runnable task,\n-\t\t\t\tlong initialDelay,\n-\t\t\t\tlong period,\n-\t\t\t\tTimeUnit unit) {\n-\t\t\treturn Schedulers.workerSchedulePeriodically(cached.exec,\n-\t\t\t\t\ttasks,\n-\t\t\t\t\ttask,\n-\t\t\t\t\tinitialDelay,\n-\t\t\t\t\tperiod,\n-\t\t\t\t\tunit);\n+\t\tBoundedState(BoundedServices parent, ScheduledExecutorService executor) {\n+\t\t\tthis.parent = parent;\n+\t\t\tthis.executor = executor;\n+\t\t}\n+\n+\t\t/**\n+\t\t * @return the queue size if the executor is a {@link ScheduledThreadPoolExecutor}, -1 otherwise\n+\t\t */\n+\t\tint estimateQueueSize() {\n+\t\t\tif (executor instanceof ScheduledThreadPoolExecutor) {\n+\t\t\t\treturn ((ScheduledThreadPoolExecutor) executor).getQueue().size();\n+\t\t\t}\n+\t\t\treturn -1;\n+\t\t}\n+\n+\t\t/**\n+\t\t * Try to mark this {@link BoundedState} as picked.\n+\t\t *\n+\t\t * @return true if this state could atomically be marked as picked, false if\n+\t\t * eviction started on it in the meantime\n+\t\t */\n+\t\tboolean markPicked() {\n+\t\t\tfor(;;) {\n+\t\t\t\tint i = MARK_COUNT.get(this);\n+\t\t\t\tif (i == DISPOSED) {\n+\t\t\t\t\treturn false; //being evicted\n+\t\t\t\t}\n+\t\t\t\tif (MARK_COUNT.compareAndSet(this, i, i + 1)) {\n+\t\t\t\t\treturn true;\n+\t\t\t\t}\n+\t\t\t}\n \t\t}\n \n+\t\t/**\n+\t\t * Check if this {@link BoundedState} should be evicted by comparing its idleSince\n+\t\t * timestamp to the evictionTimestamp and comparing the difference with the\n+\t\t * give ttlMillis. When eligible for eviction, the executor is shut down and the", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "baf4aa6f21d00dc634fd7a74114e08909e0e3408"}, "originalPosition": 797}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU4OTE1NDY1", "url": "https://github.com/reactor/reactor-core/pull/2040#pullrequestreview-358915465", "createdAt": "2020-02-14T12:43:29Z", "commit": {"oid": "baf4aa6f21d00dc634fd7a74114e08909e0e3408"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQxMjo0MzoyOVrOFp1bQg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQxMjo0MzoyOVrOFp1bQg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTQxMTI2Ng==", "bodyText": "nit: since ttlMillis is always positive elapsed > 0 here is redundant", "url": "https://github.com/reactor/reactor-core/pull/2040#discussion_r379411266", "createdAt": "2020-02-14T12:43:29Z", "author": {"login": "bsideup"}, "path": "reactor-core/src/main/java/reactor/core/scheduler/BoundedElasticScheduler.java", "diffHunk": "@@ -319,492 +244,554 @@ public String toString() {\n \t\tif (factory instanceof ReactorThreadFactory) {\n \t\t\tts.append('\\\"').append(((ReactorThreadFactory) factory).get()).append(\"\\\",\");\n \t\t}\n-\t\tts.append(\"maxThreads=\").append(threadCap)\n-\t\t  .append(\",maxTaskQueued=\").append(deferredTaskCap == Integer.MAX_VALUE ? \"unbounded\" : deferredTaskCap)\n-\t\t  .append(\",ttl=\").append(ttlSeconds).append(\"s)\");\n+\t\tts.append(\"maxThreads=\").append(maxThreads)\n+\t\t  .append(\",maxTaskQueuedPerThread=\").append(maxTaskQueuedPerThread == Integer.MAX_VALUE ? \"unbounded\" : maxTaskQueuedPerThread)\n+\t\t  .append(\",ttl=\");\n+\t\tif (ttlMillis < 1000) {\n+\t\t\tts.append(ttlMillis).append(\"ms)\");\n+\t\t}\n+\t\telse {\n+\t\t\tts.append(ttlMillis / 1000).append(\"s)\");\n+\t\t}\n \t\treturn ts.toString();\n \t}\n \n+\t/**\n+\t * @return a best effort total count of the spinned up executors\n+\t */\n+\tint estimateSize() {\n+\t\treturn BOUNDED_SERVICES.get(this).get();\n+\t}\n+\n+\t/**\n+\t * @return a best effort total count of the busy executors\n+\t */\n+\tint estimateBusy() {\n+\t\treturn BOUNDED_SERVICES.get(this).busyQueue.size();\n+\t}\n+\n+\t/**\n+\t * @return a best effort total count of the idle executors\n+\t */\n+\tint estimateIdle() {\n+\t\treturn BOUNDED_SERVICES.get(this).idleQueue.size();\n+\t}\n+\n+\t/**\n+\t * Best effort snapshot of the remaining queue capacity for pending tasks across all the backing executors.\n+\t *\n+\t * @return the total task capacity, or {@literal -1} if any backing executor's task queue size cannot be instrumented\n+\t */\n+\tint estimateRemainingTaskCapacity() {\n+\t\tQueue<BoundedState> busyQueue = BOUNDED_SERVICES.get(this).busyQueue;\n+\t\tint totalTaskCapacity = maxTaskQueuedPerThread * maxThreads;\n+\t\tfor (BoundedState state : busyQueue) {\n+\t\t\tint stateQueueSize = state.estimateQueueSize();\n+\t\t\tif (stateQueueSize >= 0) {\n+\t\t\t\ttotalTaskCapacity -= stateQueueSize;\n+\t\t\t}\n+\t\t\telse {\n+\t\t\t\treturn -1;\n+\t\t\t}\n+\t\t}\n+\t\treturn totalTaskCapacity;\n+\t}\n+\n \t@Override\n \tpublic Object scanUnsafe(Attr key) {\n \t\tif (key == Attr.TERMINATED || key == Attr.CANCELLED) return isDisposed();\n-\t\tif (key == Attr.CAPACITY) return threadCap;\n-\t\tif (key == Attr.BUFFERED) return idleServicesWithExpiry.size(); //BUFFERED: number of workers alive and backed by thread\n+\t\tif (key == Attr.BUFFERED) return estimateSize();\n+\t\tif (key == Attr.CAPACITY) return maxThreads;\n \t\tif (key == Attr.NAME) return this.toString();\n \n \t\treturn null;\n \t}\n \n \t@Override\n-\t\t//TODO re-evaluate the inners? should these include deferredWorkers? allServices?\n \tpublic Stream<? extends Scannable> inners() {\n-\t\treturn idleServicesWithExpiry.stream()\n-\t\t                             .map(cached -> cached.cached);\n+\t\tBoundedServices services = BOUNDED_SERVICES.get(this);\n+\t\treturn Stream.concat(services.busyQueue.stream(), services.idleQueue.stream())\n+\t\t             .filter(obj -> obj != null && obj != CREATING);\n \t}\n \n-\tvoid eviction(LongSupplier nowSupplier) {\n-\t\tlong now = nowSupplier.getAsLong();\n-\t\tList<CachedServiceExpiry> list = new ArrayList<>(idleServicesWithExpiry);\n-\t\tfor (CachedServiceExpiry e : list) {\n-\t\t\tif (e.expireMillis < now) {\n-\t\t\t\tif (idleServicesWithExpiry.remove(e)) {\n-\t\t\t\t\te.cached.exec.shutdownNow();\n-\t\t\t\t\tallServices.remove(e.cached);\n-\t\t\t\t\tREMAINING_THREADS.incrementAndGet(this);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n+\t@Override\n+\tpublic Worker createWorker() {\n+\t\tBoundedState picked = BOUNDED_SERVICES.get(this)\n+\t\t                                      .pick();\n+\t\tExecutorServiceWorker worker = new ExecutorServiceWorker(picked.executor);\n+\t\tworker.tasks.add(picked); //this ensures the BoundedState will be released when worker is disposed\n+\t\treturn worker;\n \t}\n \n-\tstatic final class CachedService implements Disposable, Scannable {\n \n-\t\tfinal BoundedElasticScheduler  parent;\n-\t\tfinal ScheduledExecutorService exec;\n+\tstatic final class BoundedServices extends AtomicInteger implements Disposable {\n \n-\t\tCachedService(@Nullable BoundedElasticScheduler parent) {\n+\t\tfinal BoundedElasticScheduler             parent;\n+\t\t//duplicated Clock field from parent so that SHUTDOWN can be instantiated and partially used\n+\t\tfinal Clock                               clock;\n+\t\tfinal Deque<BoundedState>                 idleQueue;\n+\t\tfinal PriorityBlockingQueue<BoundedState> busyQueue;\n+\n+\t\t//constructor for SHUTDOWN\n+\t\tprivate BoundedServices() {\n+\t\t\tthis.parent = null;\n+\t\t\tthis.clock = Clock.fixed(Instant.EPOCH, ZoneId.systemDefault());\n+\t\t\tthis.busyQueue = new PriorityBlockingQueue<>();\n+\t\t\tthis.idleQueue = new ConcurrentLinkedDeque<>();\n+\t\t}\n+\n+\t\tBoundedServices(BoundedElasticScheduler parent) {\n \t\t\tthis.parent = parent;\n-\t\t\tif (parent != null) {\n-\t\t\t\tthis.exec = Schedulers.decorateExecutorService(parent, parent.get());\n-\t\t\t}\n-\t\t\telse {\n-\t\t\t\tthis.exec = Executors.newSingleThreadScheduledExecutor();\n-\t\t\t\tthis.exec.shutdownNow();\n+\t\t\tthis.clock = parent.clock;\n+\t\t\tthis.busyQueue = new PriorityBlockingQueue<>(parent.maxThreads,\n+\t\t\t\t\tComparator.comparingInt(bs -> bs.markCount));\n+\t\t\tthis.idleQueue = new ConcurrentLinkedDeque<>();\n+\t\t}\n+\n+\t\t/**\n+\t\t * Trigger the eviction by computing the oldest acceptable timestamp and letting each {@link BoundedState}\n+\t\t * check (and potentially shutdown) itself.\n+\t\t */\n+\t\tvoid eviction() {\n+\t\t\tfinal long evictionTimestamp = parent.clock.millis();\n+\t\t\tList<BoundedState> idleCandidates = new ArrayList<>(idleQueue);\n+\t\t\tfor (BoundedState candidate : idleCandidates) {\n+\t\t\t\tif (candidate.tryEvict(evictionTimestamp, parent.ttlMillis)) {\n+\t\t\t\t\tidleQueue.remove(candidate);\n+\t\t\t\t\tdecrementAndGet();\n+\t\t\t\t}\n \t\t\t}\n \t\t}\n \n-\t\t@Override\n-\t\tpublic void dispose() {\n-\t\t\tif (exec != null) {\n-\t\t\t\tif (this != SHUTDOWN && !parent.shutdown) {\n-\t\t\t\t\t//in case of work, re-create an ActiveWorker\n-\t\t\t\t\tDeferredFacade deferredFacade = parent.deferredFacades.poll();\n-\t\t\t\t\tif (deferredFacade != null) {\n-\t\t\t\t\t\tdeferredFacade.setService(this);\n+\t\t/**\n+\t\t * Pick a {@link BoundedState}, prioritizing idle ones then spinning up a new one if enough capacity.\n+\t\t * Otherwise, picks an active one by taking from a {@link PriorityQueue}. The picking is\n+\t\t * optimistically re-attempted if the picked slot cannot be marked as picked.\n+\t\t *\n+\t\t * @return the picked {@link BoundedState}\n+\t\t */\n+\t\tBoundedState pick() {\n+\t\t\tfor (;;) {\n+\t\t\t\tint a = get();\n+\t\t\t\tif (a == DISPOSED) {\n+\t\t\t\t\treturn CREATING; //synonym for shutdown, since the underlying executor is shut down\n+\t\t\t\t}\n+\n+\t\t\t\tif (!idleQueue.isEmpty()) {\n+\t\t\t\t\t//try to find an idle resource\n+\t\t\t\t\tBoundedState bs = idleQueue.pollLast();\n+\t\t\t\t\tif (bs != null && bs.markPicked()) {\n+\t\t\t\t\t\tbusyQueue.add(bs);\n+\t\t\t\t\t\treturn bs;\n \t\t\t\t\t}\n-\t\t\t\t\telse {\n-\t\t\t\t\t\t//if no more work, the service is put back at end of the cached queue and new expiry is started\n-\t\t\t\t\t\tCachedServiceExpiry e = new CachedServiceExpiry(this,\n-\t\t\t\t\t\t\t\tSystem.currentTimeMillis() + parent.ttlSeconds * 1000L);\n-\t\t\t\t\t\tparent.idleServicesWithExpiry.offerLast(e);\n-\t\t\t\t\t\tif (parent.shutdown) {\n-\t\t\t\t\t\t\tif (parent.idleServicesWithExpiry.remove(e)) {\n-\t\t\t\t\t\t\t\texec.shutdownNow();\n-\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t}\n+\t\t\t\t\t//else optimistically retry\n+\t\t\t\t}\n+\t\t\t\telse if (a < parent.maxThreads) {\n+\t\t\t\t\t//try to build a new resource\n+\t\t\t\t\tif (compareAndSet(a, a + 1)) {\n+\t\t\t\t\t\tScheduledExecutorService s = Schedulers.decorateExecutorService(parent, parent.createBoundedExecutorService());\n+\t\t\t\t\t\tBoundedState newState = new BoundedState(this, s);\n+\t\t\t\t\t\tnewState.markPicked(); //no need to check return value as this one is brand new\n+\t\t\t\t\t\tbusyQueue.add(newState);\n+\t\t\t\t\t\treturn newState;\n+\t\t\t\t\t}\n+\t\t\t\t\t//else optimistically retry\n+\t\t\t\t}\n+\t\t\t\telse {\n+\t\t\t\t\t//pick the least busy one\n+\t\t\t\t\tBoundedState s;\n+\t\t\t\t\ts = busyQueue.poll();\n+\t\t\t\t\tif (s != null && s.markPicked()) {\n+\t\t\t\t\t\tbusyQueue.add(s); //put it back in the queue with updated priority\n+\t\t\t\t\t\treturn s;\n \t\t\t\t\t}\n+\t\t\t\t\t//else optimistically retry\n \t\t\t\t}\n \t\t\t}\n \t\t}\n \n-\t\t@Override\n-\t\tpublic Object scanUnsafe(Attr key) {\n-\t\t\tif (key == Attr.NAME) return parent.scanUnsafe(key);\n-\t\t\tif (key == Attr.PARENT) return parent;\n-\t\t\tif (key == Attr.TERMINATED || key == Attr.CANCELLED) return isDisposed();\n-\t\t\tif (key == Attr.CAPACITY) {\n-\t\t\t\t//assume 1 if unknown, otherwise use the one from underlying executor\n-\t\t\t\tInteger capacity = (Integer) Schedulers.scanExecutor(exec, key);\n-\t\t\t\tif (capacity == null || capacity == -1) return 1;\n-\t\t\t}\n-\t\t\treturn Schedulers.scanExecutor(exec, key);\n+\t\tvoid setIdle(BoundedState boundedState) {\n+\t\t\tthis.idleQueue.add(boundedState);\n+\t\t\tthis.busyQueue.remove(boundedState);\n \t\t}\n-\t}\n \n-\tstatic final class CachedServiceExpiry {\n-\n-\t\tfinal CachedService cached;\n-\t\tfinal long          expireMillis;\n+\t\t@Override\n+\t\tpublic boolean isDisposed() {\n+\t\t\treturn get() == DISPOSED;\n+\t\t}\n \n-\t\tCachedServiceExpiry(CachedService cached, long expireMillis) {\n-\t\t\tthis.cached = cached;\n-\t\t\tthis.expireMillis = expireMillis;\n+\t\t@Override\n+\t\tpublic void dispose() {\n+\t\t\tset(DISPOSED);\n+\t\t\tidleQueue.forEach(BoundedState::shutdown);\n+\t\t\tbusyQueue.forEach(BoundedState::shutdown);\n \t\t}\n \t}\n \n-\tstatic final class ActiveWorker extends AtomicBoolean implements Worker, Scannable {\n-\n-\t\tfinal CachedService cached;\n-\t\tfinal Composite tasks;\n+\t/**\n+\t * A class that encapsulate state around the {@link BoundedScheduledExecutorService} and\n+\t * atomically marking them picked/idle.\n+\t */\n+\tstatic class BoundedState implements Disposable, Scannable {\n \n-\t\tActiveWorker(CachedService cached) {\n-\t\t\tthis.cached = cached;\n-\t\t\tthis.tasks = Disposables.composite();\n-\t\t}\n+\t\tfinal BoundedServices          parent;\n+\t\tfinal ScheduledExecutorService executor;\n \n-\t\t@Override\n-\t\tpublic Disposable schedule(Runnable task) {\n-\t\t\treturn Schedulers.workerSchedule(cached.exec,\n-\t\t\t\t\ttasks,\n-\t\t\t\t\ttask,\n-\t\t\t\t\t0L,\n-\t\t\t\t\tTimeUnit.MILLISECONDS);\n-\t\t}\n+\t\tlong idleSinceTimestamp = -1L;\n \n-\t\t@Override\n-\t\tpublic Disposable schedule(Runnable task, long delay, TimeUnit unit) {\n-\t\t\treturn Schedulers.workerSchedule(cached.exec, tasks, task, delay, unit);\n-\t\t}\n+\t\tvolatile int markCount;\n+\t\tstatic final AtomicIntegerFieldUpdater<BoundedState> MARK_COUNT = AtomicIntegerFieldUpdater.newUpdater(BoundedState.class, \"markCount\");\n \n-\t\t@Override\n-\t\tpublic Disposable schedulePeriodically(Runnable task,\n-\t\t\t\tlong initialDelay,\n-\t\t\t\tlong period,\n-\t\t\t\tTimeUnit unit) {\n-\t\t\treturn Schedulers.workerSchedulePeriodically(cached.exec,\n-\t\t\t\t\ttasks,\n-\t\t\t\t\ttask,\n-\t\t\t\t\tinitialDelay,\n-\t\t\t\t\tperiod,\n-\t\t\t\t\tunit);\n+\t\tBoundedState(BoundedServices parent, ScheduledExecutorService executor) {\n+\t\t\tthis.parent = parent;\n+\t\t\tthis.executor = executor;\n+\t\t}\n+\n+\t\t/**\n+\t\t * @return the queue size if the executor is a {@link ScheduledThreadPoolExecutor}, -1 otherwise\n+\t\t */\n+\t\tint estimateQueueSize() {\n+\t\t\tif (executor instanceof ScheduledThreadPoolExecutor) {\n+\t\t\t\treturn ((ScheduledThreadPoolExecutor) executor).getQueue().size();\n+\t\t\t}\n+\t\t\treturn -1;\n+\t\t}\n+\n+\t\t/**\n+\t\t * Try to mark this {@link BoundedState} as picked.\n+\t\t *\n+\t\t * @return true if this state could atomically be marked as picked, false if\n+\t\t * eviction started on it in the meantime\n+\t\t */\n+\t\tboolean markPicked() {\n+\t\t\tfor(;;) {\n+\t\t\t\tint i = MARK_COUNT.get(this);\n+\t\t\t\tif (i == DISPOSED) {\n+\t\t\t\t\treturn false; //being evicted\n+\t\t\t\t}\n+\t\t\t\tif (MARK_COUNT.compareAndSet(this, i, i + 1)) {\n+\t\t\t\t\treturn true;\n+\t\t\t\t}\n+\t\t\t}\n \t\t}\n \n+\t\t/**\n+\t\t * Check if this {@link BoundedState} should be evicted by comparing its idleSince\n+\t\t * timestamp to the evictionTimestamp and comparing the difference with the\n+\t\t * give ttlMillis. When eligible for eviction, the executor is shut down and the\n+\t\t * method returns true (to remove the state from the array).\n+\t\t *\n+\t\t * @param evictionTimestamp the timestamp at which the eviction process is running\n+\t\t * @param ttlMillis the maximum idle duration\n+\t\t * @return true if this {@link BoundedState} has shut down itself as part of eviction, false otherwise\n+\t\t */\n+\t\tboolean tryEvict(long evictionTimestamp, long ttlMillis) {\n+\t\t\tlong idleSince = this.idleSinceTimestamp;\n+\t\t\tif (idleSince < 0) return false;\n+\t\t\tlong elapsed = evictionTimestamp - idleSince;\n+\t\t\tif (elapsed > 0 && elapsed >= ttlMillis) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "baf4aa6f21d00dc634fd7a74114e08909e0e3408"}, "originalPosition": 808}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU4OTE5MjMz", "url": "https://github.com/reactor/reactor-core/pull/2040#pullrequestreview-358919233", "createdAt": "2020-02-14T12:51:01Z", "commit": {"oid": "baf4aa6f21d00dc634fd7a74114e08909e0e3408"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQxMjo1MTowMVrOFp1m4A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQxMjo1MTowMVrOFp1m4A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTQxNDI0MA==", "bodyText": "nit: could use an early return instead of a comment ;)", "url": "https://github.com/reactor/reactor-core/pull/2040#discussion_r379414240", "createdAt": "2020-02-14T12:51:01Z", "author": {"login": "bsideup"}, "path": "reactor-core/src/main/java/reactor/core/scheduler/BoundedElasticScheduler.java", "diffHunk": "@@ -319,492 +244,554 @@ public String toString() {\n \t\tif (factory instanceof ReactorThreadFactory) {\n \t\t\tts.append('\\\"').append(((ReactorThreadFactory) factory).get()).append(\"\\\",\");\n \t\t}\n-\t\tts.append(\"maxThreads=\").append(threadCap)\n-\t\t  .append(\",maxTaskQueued=\").append(deferredTaskCap == Integer.MAX_VALUE ? \"unbounded\" : deferredTaskCap)\n-\t\t  .append(\",ttl=\").append(ttlSeconds).append(\"s)\");\n+\t\tts.append(\"maxThreads=\").append(maxThreads)\n+\t\t  .append(\",maxTaskQueuedPerThread=\").append(maxTaskQueuedPerThread == Integer.MAX_VALUE ? \"unbounded\" : maxTaskQueuedPerThread)\n+\t\t  .append(\",ttl=\");\n+\t\tif (ttlMillis < 1000) {\n+\t\t\tts.append(ttlMillis).append(\"ms)\");\n+\t\t}\n+\t\telse {\n+\t\t\tts.append(ttlMillis / 1000).append(\"s)\");\n+\t\t}\n \t\treturn ts.toString();\n \t}\n \n+\t/**\n+\t * @return a best effort total count of the spinned up executors\n+\t */\n+\tint estimateSize() {\n+\t\treturn BOUNDED_SERVICES.get(this).get();\n+\t}\n+\n+\t/**\n+\t * @return a best effort total count of the busy executors\n+\t */\n+\tint estimateBusy() {\n+\t\treturn BOUNDED_SERVICES.get(this).busyQueue.size();\n+\t}\n+\n+\t/**\n+\t * @return a best effort total count of the idle executors\n+\t */\n+\tint estimateIdle() {\n+\t\treturn BOUNDED_SERVICES.get(this).idleQueue.size();\n+\t}\n+\n+\t/**\n+\t * Best effort snapshot of the remaining queue capacity for pending tasks across all the backing executors.\n+\t *\n+\t * @return the total task capacity, or {@literal -1} if any backing executor's task queue size cannot be instrumented\n+\t */\n+\tint estimateRemainingTaskCapacity() {\n+\t\tQueue<BoundedState> busyQueue = BOUNDED_SERVICES.get(this).busyQueue;\n+\t\tint totalTaskCapacity = maxTaskQueuedPerThread * maxThreads;\n+\t\tfor (BoundedState state : busyQueue) {\n+\t\t\tint stateQueueSize = state.estimateQueueSize();\n+\t\t\tif (stateQueueSize >= 0) {\n+\t\t\t\ttotalTaskCapacity -= stateQueueSize;\n+\t\t\t}\n+\t\t\telse {\n+\t\t\t\treturn -1;\n+\t\t\t}\n+\t\t}\n+\t\treturn totalTaskCapacity;\n+\t}\n+\n \t@Override\n \tpublic Object scanUnsafe(Attr key) {\n \t\tif (key == Attr.TERMINATED || key == Attr.CANCELLED) return isDisposed();\n-\t\tif (key == Attr.CAPACITY) return threadCap;\n-\t\tif (key == Attr.BUFFERED) return idleServicesWithExpiry.size(); //BUFFERED: number of workers alive and backed by thread\n+\t\tif (key == Attr.BUFFERED) return estimateSize();\n+\t\tif (key == Attr.CAPACITY) return maxThreads;\n \t\tif (key == Attr.NAME) return this.toString();\n \n \t\treturn null;\n \t}\n \n \t@Override\n-\t\t//TODO re-evaluate the inners? should these include deferredWorkers? allServices?\n \tpublic Stream<? extends Scannable> inners() {\n-\t\treturn idleServicesWithExpiry.stream()\n-\t\t                             .map(cached -> cached.cached);\n+\t\tBoundedServices services = BOUNDED_SERVICES.get(this);\n+\t\treturn Stream.concat(services.busyQueue.stream(), services.idleQueue.stream())\n+\t\t             .filter(obj -> obj != null && obj != CREATING);\n \t}\n \n-\tvoid eviction(LongSupplier nowSupplier) {\n-\t\tlong now = nowSupplier.getAsLong();\n-\t\tList<CachedServiceExpiry> list = new ArrayList<>(idleServicesWithExpiry);\n-\t\tfor (CachedServiceExpiry e : list) {\n-\t\t\tif (e.expireMillis < now) {\n-\t\t\t\tif (idleServicesWithExpiry.remove(e)) {\n-\t\t\t\t\te.cached.exec.shutdownNow();\n-\t\t\t\t\tallServices.remove(e.cached);\n-\t\t\t\t\tREMAINING_THREADS.incrementAndGet(this);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n+\t@Override\n+\tpublic Worker createWorker() {\n+\t\tBoundedState picked = BOUNDED_SERVICES.get(this)\n+\t\t                                      .pick();\n+\t\tExecutorServiceWorker worker = new ExecutorServiceWorker(picked.executor);\n+\t\tworker.tasks.add(picked); //this ensures the BoundedState will be released when worker is disposed\n+\t\treturn worker;\n \t}\n \n-\tstatic final class CachedService implements Disposable, Scannable {\n \n-\t\tfinal BoundedElasticScheduler  parent;\n-\t\tfinal ScheduledExecutorService exec;\n+\tstatic final class BoundedServices extends AtomicInteger implements Disposable {\n \n-\t\tCachedService(@Nullable BoundedElasticScheduler parent) {\n+\t\tfinal BoundedElasticScheduler             parent;\n+\t\t//duplicated Clock field from parent so that SHUTDOWN can be instantiated and partially used\n+\t\tfinal Clock                               clock;\n+\t\tfinal Deque<BoundedState>                 idleQueue;\n+\t\tfinal PriorityBlockingQueue<BoundedState> busyQueue;\n+\n+\t\t//constructor for SHUTDOWN\n+\t\tprivate BoundedServices() {\n+\t\t\tthis.parent = null;\n+\t\t\tthis.clock = Clock.fixed(Instant.EPOCH, ZoneId.systemDefault());\n+\t\t\tthis.busyQueue = new PriorityBlockingQueue<>();\n+\t\t\tthis.idleQueue = new ConcurrentLinkedDeque<>();\n+\t\t}\n+\n+\t\tBoundedServices(BoundedElasticScheduler parent) {\n \t\t\tthis.parent = parent;\n-\t\t\tif (parent != null) {\n-\t\t\t\tthis.exec = Schedulers.decorateExecutorService(parent, parent.get());\n-\t\t\t}\n-\t\t\telse {\n-\t\t\t\tthis.exec = Executors.newSingleThreadScheduledExecutor();\n-\t\t\t\tthis.exec.shutdownNow();\n+\t\t\tthis.clock = parent.clock;\n+\t\t\tthis.busyQueue = new PriorityBlockingQueue<>(parent.maxThreads,\n+\t\t\t\t\tComparator.comparingInt(bs -> bs.markCount));\n+\t\t\tthis.idleQueue = new ConcurrentLinkedDeque<>();\n+\t\t}\n+\n+\t\t/**\n+\t\t * Trigger the eviction by computing the oldest acceptable timestamp and letting each {@link BoundedState}\n+\t\t * check (and potentially shutdown) itself.\n+\t\t */\n+\t\tvoid eviction() {\n+\t\t\tfinal long evictionTimestamp = parent.clock.millis();\n+\t\t\tList<BoundedState> idleCandidates = new ArrayList<>(idleQueue);\n+\t\t\tfor (BoundedState candidate : idleCandidates) {\n+\t\t\t\tif (candidate.tryEvict(evictionTimestamp, parent.ttlMillis)) {\n+\t\t\t\t\tidleQueue.remove(candidate);\n+\t\t\t\t\tdecrementAndGet();\n+\t\t\t\t}\n \t\t\t}\n \t\t}\n \n-\t\t@Override\n-\t\tpublic void dispose() {\n-\t\t\tif (exec != null) {\n-\t\t\t\tif (this != SHUTDOWN && !parent.shutdown) {\n-\t\t\t\t\t//in case of work, re-create an ActiveWorker\n-\t\t\t\t\tDeferredFacade deferredFacade = parent.deferredFacades.poll();\n-\t\t\t\t\tif (deferredFacade != null) {\n-\t\t\t\t\t\tdeferredFacade.setService(this);\n+\t\t/**\n+\t\t * Pick a {@link BoundedState}, prioritizing idle ones then spinning up a new one if enough capacity.\n+\t\t * Otherwise, picks an active one by taking from a {@link PriorityQueue}. The picking is\n+\t\t * optimistically re-attempted if the picked slot cannot be marked as picked.\n+\t\t *\n+\t\t * @return the picked {@link BoundedState}\n+\t\t */\n+\t\tBoundedState pick() {\n+\t\t\tfor (;;) {\n+\t\t\t\tint a = get();\n+\t\t\t\tif (a == DISPOSED) {\n+\t\t\t\t\treturn CREATING; //synonym for shutdown, since the underlying executor is shut down\n+\t\t\t\t}\n+\n+\t\t\t\tif (!idleQueue.isEmpty()) {\n+\t\t\t\t\t//try to find an idle resource\n+\t\t\t\t\tBoundedState bs = idleQueue.pollLast();\n+\t\t\t\t\tif (bs != null && bs.markPicked()) {\n+\t\t\t\t\t\tbusyQueue.add(bs);\n+\t\t\t\t\t\treturn bs;\n \t\t\t\t\t}\n-\t\t\t\t\telse {\n-\t\t\t\t\t\t//if no more work, the service is put back at end of the cached queue and new expiry is started\n-\t\t\t\t\t\tCachedServiceExpiry e = new CachedServiceExpiry(this,\n-\t\t\t\t\t\t\t\tSystem.currentTimeMillis() + parent.ttlSeconds * 1000L);\n-\t\t\t\t\t\tparent.idleServicesWithExpiry.offerLast(e);\n-\t\t\t\t\t\tif (parent.shutdown) {\n-\t\t\t\t\t\t\tif (parent.idleServicesWithExpiry.remove(e)) {\n-\t\t\t\t\t\t\t\texec.shutdownNow();\n-\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t}\n+\t\t\t\t\t//else optimistically retry\n+\t\t\t\t}\n+\t\t\t\telse if (a < parent.maxThreads) {\n+\t\t\t\t\t//try to build a new resource\n+\t\t\t\t\tif (compareAndSet(a, a + 1)) {\n+\t\t\t\t\t\tScheduledExecutorService s = Schedulers.decorateExecutorService(parent, parent.createBoundedExecutorService());\n+\t\t\t\t\t\tBoundedState newState = new BoundedState(this, s);\n+\t\t\t\t\t\tnewState.markPicked(); //no need to check return value as this one is brand new\n+\t\t\t\t\t\tbusyQueue.add(newState);\n+\t\t\t\t\t\treturn newState;\n+\t\t\t\t\t}\n+\t\t\t\t\t//else optimistically retry\n+\t\t\t\t}\n+\t\t\t\telse {\n+\t\t\t\t\t//pick the least busy one\n+\t\t\t\t\tBoundedState s;\n+\t\t\t\t\ts = busyQueue.poll();\n+\t\t\t\t\tif (s != null && s.markPicked()) {\n+\t\t\t\t\t\tbusyQueue.add(s); //put it back in the queue with updated priority\n+\t\t\t\t\t\treturn s;\n \t\t\t\t\t}\n+\t\t\t\t\t//else optimistically retry\n \t\t\t\t}\n \t\t\t}\n \t\t}\n \n-\t\t@Override\n-\t\tpublic Object scanUnsafe(Attr key) {\n-\t\t\tif (key == Attr.NAME) return parent.scanUnsafe(key);\n-\t\t\tif (key == Attr.PARENT) return parent;\n-\t\t\tif (key == Attr.TERMINATED || key == Attr.CANCELLED) return isDisposed();\n-\t\t\tif (key == Attr.CAPACITY) {\n-\t\t\t\t//assume 1 if unknown, otherwise use the one from underlying executor\n-\t\t\t\tInteger capacity = (Integer) Schedulers.scanExecutor(exec, key);\n-\t\t\t\tif (capacity == null || capacity == -1) return 1;\n-\t\t\t}\n-\t\t\treturn Schedulers.scanExecutor(exec, key);\n+\t\tvoid setIdle(BoundedState boundedState) {\n+\t\t\tthis.idleQueue.add(boundedState);\n+\t\t\tthis.busyQueue.remove(boundedState);\n \t\t}\n-\t}\n \n-\tstatic final class CachedServiceExpiry {\n-\n-\t\tfinal CachedService cached;\n-\t\tfinal long          expireMillis;\n+\t\t@Override\n+\t\tpublic boolean isDisposed() {\n+\t\t\treturn get() == DISPOSED;\n+\t\t}\n \n-\t\tCachedServiceExpiry(CachedService cached, long expireMillis) {\n-\t\t\tthis.cached = cached;\n-\t\t\tthis.expireMillis = expireMillis;\n+\t\t@Override\n+\t\tpublic void dispose() {\n+\t\t\tset(DISPOSED);\n+\t\t\tidleQueue.forEach(BoundedState::shutdown);\n+\t\t\tbusyQueue.forEach(BoundedState::shutdown);\n \t\t}\n \t}\n \n-\tstatic final class ActiveWorker extends AtomicBoolean implements Worker, Scannable {\n-\n-\t\tfinal CachedService cached;\n-\t\tfinal Composite tasks;\n+\t/**\n+\t * A class that encapsulate state around the {@link BoundedScheduledExecutorService} and\n+\t * atomically marking them picked/idle.\n+\t */\n+\tstatic class BoundedState implements Disposable, Scannable {\n \n-\t\tActiveWorker(CachedService cached) {\n-\t\t\tthis.cached = cached;\n-\t\t\tthis.tasks = Disposables.composite();\n-\t\t}\n+\t\tfinal BoundedServices          parent;\n+\t\tfinal ScheduledExecutorService executor;\n \n-\t\t@Override\n-\t\tpublic Disposable schedule(Runnable task) {\n-\t\t\treturn Schedulers.workerSchedule(cached.exec,\n-\t\t\t\t\ttasks,\n-\t\t\t\t\ttask,\n-\t\t\t\t\t0L,\n-\t\t\t\t\tTimeUnit.MILLISECONDS);\n-\t\t}\n+\t\tlong idleSinceTimestamp = -1L;\n \n-\t\t@Override\n-\t\tpublic Disposable schedule(Runnable task, long delay, TimeUnit unit) {\n-\t\t\treturn Schedulers.workerSchedule(cached.exec, tasks, task, delay, unit);\n-\t\t}\n+\t\tvolatile int markCount;\n+\t\tstatic final AtomicIntegerFieldUpdater<BoundedState> MARK_COUNT = AtomicIntegerFieldUpdater.newUpdater(BoundedState.class, \"markCount\");\n \n-\t\t@Override\n-\t\tpublic Disposable schedulePeriodically(Runnable task,\n-\t\t\t\tlong initialDelay,\n-\t\t\t\tlong period,\n-\t\t\t\tTimeUnit unit) {\n-\t\t\treturn Schedulers.workerSchedulePeriodically(cached.exec,\n-\t\t\t\t\ttasks,\n-\t\t\t\t\ttask,\n-\t\t\t\t\tinitialDelay,\n-\t\t\t\t\tperiod,\n-\t\t\t\t\tunit);\n+\t\tBoundedState(BoundedServices parent, ScheduledExecutorService executor) {\n+\t\t\tthis.parent = parent;\n+\t\t\tthis.executor = executor;\n+\t\t}\n+\n+\t\t/**\n+\t\t * @return the queue size if the executor is a {@link ScheduledThreadPoolExecutor}, -1 otherwise\n+\t\t */\n+\t\tint estimateQueueSize() {\n+\t\t\tif (executor instanceof ScheduledThreadPoolExecutor) {\n+\t\t\t\treturn ((ScheduledThreadPoolExecutor) executor).getQueue().size();\n+\t\t\t}\n+\t\t\treturn -1;\n+\t\t}\n+\n+\t\t/**\n+\t\t * Try to mark this {@link BoundedState} as picked.\n+\t\t *\n+\t\t * @return true if this state could atomically be marked as picked, false if\n+\t\t * eviction started on it in the meantime\n+\t\t */\n+\t\tboolean markPicked() {\n+\t\t\tfor(;;) {\n+\t\t\t\tint i = MARK_COUNT.get(this);\n+\t\t\t\tif (i == DISPOSED) {\n+\t\t\t\t\treturn false; //being evicted\n+\t\t\t\t}\n+\t\t\t\tif (MARK_COUNT.compareAndSet(this, i, i + 1)) {\n+\t\t\t\t\treturn true;\n+\t\t\t\t}\n+\t\t\t}\n \t\t}\n \n+\t\t/**\n+\t\t * Check if this {@link BoundedState} should be evicted by comparing its idleSince\n+\t\t * timestamp to the evictionTimestamp and comparing the difference with the\n+\t\t * give ttlMillis. When eligible for eviction, the executor is shut down and the\n+\t\t * method returns true (to remove the state from the array).\n+\t\t *\n+\t\t * @param evictionTimestamp the timestamp at which the eviction process is running\n+\t\t * @param ttlMillis the maximum idle duration\n+\t\t * @return true if this {@link BoundedState} has shut down itself as part of eviction, false otherwise\n+\t\t */\n+\t\tboolean tryEvict(long evictionTimestamp, long ttlMillis) {\n+\t\t\tlong idleSince = this.idleSinceTimestamp;\n+\t\t\tif (idleSince < 0) return false;\n+\t\t\tlong elapsed = evictionTimestamp - idleSince;\n+\t\t\tif (elapsed > 0 && elapsed >= ttlMillis) {\n+\t\t\t\tif (MARK_COUNT.compareAndSet(this, 0, DISPOSED)) {\n+\t\t\t\t\texecutor.shutdownNow();\n+\t\t\t\t\treturn true;\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\treturn false;\n+\t\t}\n+\n+\t\t/**\n+\t\t * Release the {@link BoundedState}, ie atomically decrease the counter of times it has been picked\n+\t\t * and mark as idle if that counter reaches 0.\n+\t\t * This is called when a worker is done using the executor. {@link #dispose()} is an alias\n+\t\t * to this method (for APIs that take a {@link Disposable}).\n+\t\t *\n+\t\t * @see #shutdown()\n+\t\t * @see #dispose()\n+\t\t */\n+\t\tvoid release() {\n+\t\t\tint picked = MARK_COUNT.decrementAndGet(this);\n+\t\t\tif (picked == 0) {\n+\t\t\t\tthis.idleSinceTimestamp = parent.clock.millis();\n+\t\t\t\tparent.setIdle(this);\n+\t\t\t}\n+\t\t\telse if (picked > 0) {\n+\t\t\t\tthis.idleSinceTimestamp = -1L;\n+\t\t\t}\n+\t\t\t// -1 means being evicted, do nothing", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "baf4aa6f21d00dc634fd7a74114e08909e0e3408"}, "originalPosition": 835}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU4OTIwMzA3", "url": "https://github.com/reactor/reactor-core/pull/2040#pullrequestreview-358920307", "createdAt": "2020-02-14T12:53:19Z", "commit": {"oid": "baf4aa6f21d00dc634fd7a74114e08909e0e3408"}, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU5ODM4MjYx", "url": "https://github.com/reactor/reactor-core/pull/2040#pullrequestreview-359838261", "createdAt": "2020-02-17T16:00:43Z", "commit": {"oid": "997554b7c2ad419679b633183fb688dab3dde486"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cd2beff8eb25e39cffbdeaba644ebf0fb04574b5", "author": {"user": {"login": "simonbasle", "name": "Simon Basl\u00e9"}}, "url": "https://github.com/reactor/reactor-core/commit/cd2beff8eb25e39cffbdeaba644ebf0fb04574b5", "committedDate": "2020-02-17T16:19:27Z", "message": "fix #1992 Reimplement BoundedElasticScheduler to allow reentrancy\n\nThe general idea is to abandon the facade Worker and instead always\nsubmit tasks to an executor-backed worker. In order of preference, when\nan operator requests a Worker:\n\n - if thread cap not reached, create and pick a new worker\n - else if idle workers, pick an idle worker\n - else pick a busy worker\n\nThis implies a behavior under contention that is closer to parallel(),\nbut with a pool that is expected to be quite larger than the typical\nparallel pool.\n\nThe drawback is that once we get to pick a busy worker, there's no\ntelling when its tasks (typically blocking tasks for a\nBoundedElasticScheduler) will finish. So even though another executor\nmight become idle in the meantime, the operator's tasks will be pinned\nto the (potentially still busy) executor initially picked.\n\nTo try to counter that effect a bit, we use a priority queue for the\nbusy executors, favoring executors that are tied to less Workers (and\nthus less operators). We don't yet go as far as factoring in the task\nqueue of each executor.\n\nFinally, one noticeable change is that the second int parameter in\nthe API, maxPendingTask, is now influencing EACH executor's queue\ninstead of being a shared counter. It should be safe in the sense that\nthe number set with previous version in mind is bound to be\nover-dimensionned for the new version, but it would be recommended for\nusers to reconsider that number."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "997554b7c2ad419679b633183fb688dab3dde486", "author": {"user": {"login": "simonbasle", "name": "Simon Basl\u00e9"}}, "url": "https://github.com/reactor/reactor-core/commit/997554b7c2ad419679b633183fb688dab3dde486", "committedDate": "2020-02-17T15:54:49Z", "message": "add race test and fix 2 race conditions"}, "afterCommit": {"oid": "cd2beff8eb25e39cffbdeaba644ebf0fb04574b5", "author": {"user": {"login": "simonbasle", "name": "Simon Basl\u00e9"}}, "url": "https://github.com/reactor/reactor-core/commit/cd2beff8eb25e39cffbdeaba644ebf0fb04574b5", "committedDate": "2020-02-17T16:19:27Z", "message": "fix #1992 Reimplement BoundedElasticScheduler to allow reentrancy\n\nThe general idea is to abandon the facade Worker and instead always\nsubmit tasks to an executor-backed worker. In order of preference, when\nan operator requests a Worker:\n\n - if thread cap not reached, create and pick a new worker\n - else if idle workers, pick an idle worker\n - else pick a busy worker\n\nThis implies a behavior under contention that is closer to parallel(),\nbut with a pool that is expected to be quite larger than the typical\nparallel pool.\n\nThe drawback is that once we get to pick a busy worker, there's no\ntelling when its tasks (typically blocking tasks for a\nBoundedElasticScheduler) will finish. So even though another executor\nmight become idle in the meantime, the operator's tasks will be pinned\nto the (potentially still busy) executor initially picked.\n\nTo try to counter that effect a bit, we use a priority queue for the\nbusy executors, favoring executors that are tied to less Workers (and\nthus less operators). We don't yet go as far as factoring in the task\nqueue of each executor.\n\nFinally, one noticeable change is that the second int parameter in\nthe API, maxPendingTask, is now influencing EACH executor's queue\ninstead of being a shared counter. It should be safe in the sense that\nthe number set with previous version in mind is bound to be\nover-dimensionned for the new version, but it would be recommended for\nusers to reconsider that number."}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2908, "cost": 1, "resetAt": "2021-11-01T16:37:27Z"}}}