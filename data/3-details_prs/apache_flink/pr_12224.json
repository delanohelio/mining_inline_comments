{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDE5NDE5MTgy", "number": 12224, "title": "[FLINK-17594][filesystem] Support Hadoop path-based part-file writer.", "bodyText": "What is the purpose of the change\nThis PR is a re-pick of #12134 and rebased it to the latest master for merging again.\nThis PR introduced the Hadoop-path-based part file writer that supports writing data to specific Hadoop path. With this writer the data is first written to a temporary in-progress path and then commit to the final path with renaming. This PR supports streaming Hive sink to reuse the StreamingFileSink to write files.\nBrief change log\n\na97fc8c introduces the haoop-path-based part file writer.\n\nVerifying this change\nThis change added tests and can be verified as follows:\n\nAdded test that validates the data is successfully written.\n\nDoes this pull request potentially affect one of the following parts:\n\nDependencies (does it add or upgrade a dependency): no\nThe public API, i.e., is any changed class annotated with @Public(Evolving): no\nThe serializers: no\nThe runtime per-record code paths (performance sensitive): no\nAnything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: no\nThe S3 file system connector: no\n\nDocumentation\n\nDoes this pull request introduce a new feature? no\nIf yes, how is the feature documented? not applicable", "createdAt": "2020-05-18T10:48:55Z", "url": "https://github.com/apache/flink/pull/12224", "merged": true, "mergeCommit": {"oid": "3b3ad3d25c79a4aa6e7daeb947654256efd4457b"}, "closed": true, "closedAt": "2020-05-19T04:10:06Z", "author": {"login": "gaoyunhaii"}, "timelineItems": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcido-zgBqjMzNDY3MzIzMzU=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcisW_3gBqjMzNDk5NjQ3MTQ=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "b69c2c817f23783decacf80a1d3c510e1b03a061", "author": {"user": {"login": "gaoyunhaii", "name": "Yun Gao"}}, "url": "https://github.com/apache/flink/commit/b69c2c817f23783decacf80a1d3c510e1b03a061", "committedDate": "2020-05-18T10:32:53Z", "message": "[FLINK-17594][filesystem] Support Hadoop path-based part-file writer."}, "afterCommit": {"oid": "354620f64883486ddf2c02dddf0633b51d0444d9", "author": {"user": {"login": "gaoyunhaii", "name": "Yun Gao"}}, "url": "https://github.com/apache/flink/commit/354620f64883486ddf2c02dddf0633b51d0444d9", "committedDate": "2020-05-18T10:51:20Z", "message": "[FLINK-17594][filesystem] Support Hadoop path-based part-file writer."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "354620f64883486ddf2c02dddf0633b51d0444d9", "author": {"user": {"login": "gaoyunhaii", "name": "Yun Gao"}}, "url": "https://github.com/apache/flink/commit/354620f64883486ddf2c02dddf0633b51d0444d9", "committedDate": "2020-05-18T10:51:20Z", "message": "[FLINK-17594][filesystem] Support Hadoop path-based part-file writer."}, "afterCommit": {"oid": "e1a615f3322b72015953856f1316691fd0c77896", "author": {"user": {"login": "gaoyunhaii", "name": "Yun Gao"}}, "url": "https://github.com/apache/flink/commit/e1a615f3322b72015953856f1316691fd0c77896", "committedDate": "2020-05-18T11:30:08Z", "message": "[FLINK-17594][filesystem] Support Hadoop path-based part-file writer."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "e1a615f3322b72015953856f1316691fd0c77896", "author": {"user": {"login": "gaoyunhaii", "name": "Yun Gao"}}, "url": "https://github.com/apache/flink/commit/e1a615f3322b72015953856f1316691fd0c77896", "committedDate": "2020-05-18T11:30:08Z", "message": "[FLINK-17594][filesystem] Support Hadoop path-based part-file writer."}, "afterCommit": {"oid": "aa5d10422ec019419efbb3a0e9c1f0900b960529", "author": {"user": {"login": "gaoyunhaii", "name": "Yun Gao"}}, "url": "https://github.com/apache/flink/commit/aa5d10422ec019419efbb3a0e9c1f0900b960529", "committedDate": "2020-05-18T12:11:07Z", "message": "Rename bucket name & fix the comments"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "aa5d10422ec019419efbb3a0e9c1f0900b960529", "author": {"user": {"login": "gaoyunhaii", "name": "Yun Gao"}}, "url": "https://github.com/apache/flink/commit/aa5d10422ec019419efbb3a0e9c1f0900b960529", "committedDate": "2020-05-18T12:11:07Z", "message": "Rename bucket name & fix the comments"}, "afterCommit": {"oid": "c4f7289627bf6830b87866e75fb0cb270e064956", "author": {"user": {"login": "gaoyunhaii", "name": "Yun Gao"}}, "url": "https://github.com/apache/flink/commit/c4f7289627bf6830b87866e75fb0cb270e064956", "committedDate": "2020-05-18T14:41:18Z", "message": "[FLINK-17594][filesystem] Support Hadoop path-based part-file writer."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDEzNzM5Mjg4", "url": "https://github.com/apache/flink/pull/12224#pullrequestreview-413739288", "createdAt": "2020-05-18T16:08:39Z", "commit": {"oid": "c4f7289627bf6830b87866e75fb0cb270e064956"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQxNjowODozOVrOGW-D_g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQxNjowODozOVrOGW-D_g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjczODY4Ng==", "bodyText": "commit should throw exception.\ncommitAfterRecovery should keep silently.\n\nDo I understand correctly? Can you add tests?\nMaybe I provided a bad example before. checkFileExists should be assertFileExists.", "url": "https://github.com/apache/flink/pull/12224#discussion_r426738686", "createdAt": "2020-05-18T16:08:39Z", "author": {"login": "JingsongLi"}, "path": "flink-formats/flink-hadoop-bulk/src/main/java/org/apache/flink/formats/hadoop/bulk/committer/HadoopRenameFileCommitter.java", "diffHunk": "@@ -0,0 +1,100 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.hadoop.bulk.committer;\n+\n+import org.apache.flink.formats.hadoop.bulk.HadoopFileCommitter;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+\n+import java.io.IOException;\n+\n+import static org.apache.flink.util.Preconditions.checkArgument;\n+\n+/**\n+ * The Hadoop file committer that directly rename the in-progress file\n+ * to the target file. For FileSystem like S3, renaming may lead to\n+ * additional copies.\n+ */\n+public class HadoopRenameFileCommitter implements HadoopFileCommitter {\n+\n+\tprivate final Configuration configuration;\n+\n+\tprivate final Path targetFilePath;\n+\n+\tprivate final Path inProgressFilePath;\n+\n+\tpublic HadoopRenameFileCommitter(Configuration configuration, Path targetFilePath) {\n+\t\tthis.configuration = configuration;\n+\t\tthis.targetFilePath = targetFilePath;\n+\t\tthis.inProgressFilePath = generateInProgressFilePath();\n+\t}\n+\n+\t@Override\n+\tpublic Path getTargetFilePath() {\n+\t\treturn targetFilePath;\n+\t}\n+\n+\t@Override\n+\tpublic Path getInProgressFilePath() {\n+\t\treturn inProgressFilePath;\n+\t}\n+\n+\t@Override\n+\tpublic void preCommit() {\n+\t\t// Do nothing.\n+\t}\n+\n+\t@Override\n+\tpublic void commit() throws IOException {\n+\t\trename(false);\n+\t}\n+\n+\t@Override\n+\tpublic void commitAfterRecovery() throws IOException {\n+\t\trename(true);\n+\t}\n+\n+\tprivate void rename(boolean checkFileExists) throws IOException {\n+\t\tFileSystem fileSystem = FileSystem.get(targetFilePath.toUri(), configuration);\n+\n+\t\tif (checkFileExists && !fileSystem.exists(inProgressFilePath)) {\n+\t\t\tthrow new IOException(String.format(\"In progress file(%s) not exists.\", inProgressFilePath));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c4f7289627bf6830b87866e75fb0cb270e064956"}, "originalPosition": 79}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE0MDUyMTU1", "url": "https://github.com/apache/flink/pull/12224#pullrequestreview-414052155", "createdAt": "2020-05-19T02:07:10Z", "commit": {"oid": "64b01f03d9a850df4e21c014abfad92cfb845de8"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "dff69138c41f2e3be2c825414258ef040ff62c9b", "author": {"user": {"login": "gaoyunhaii", "name": "Yun Gao"}}, "url": "https://github.com/apache/flink/commit/dff69138c41f2e3be2c825414258ef040ff62c9b", "committedDate": "2020-05-19T03:59:46Z", "message": "[FLINK-17594][filesystem] Support Hadoop path-based part-file writer."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "64b01f03d9a850df4e21c014abfad92cfb845de8", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/flink/commit/64b01f03d9a850df4e21c014abfad92cfb845de8", "committedDate": "2020-05-19T02:06:27Z", "message": "checkstyle"}, "afterCommit": {"oid": "dff69138c41f2e3be2c825414258ef040ff62c9b", "author": {"user": {"login": "gaoyunhaii", "name": "Yun Gao"}}, "url": "https://github.com/apache/flink/commit/dff69138c41f2e3be2c825414258ef040ff62c9b", "committedDate": "2020-05-19T03:59:46Z", "message": "[FLINK-17594][filesystem] Support Hadoop path-based part-file writer."}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4160, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}