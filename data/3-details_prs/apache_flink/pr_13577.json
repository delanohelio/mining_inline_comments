{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTAwODkwMzUz", "number": 13577, "title": "[FLINK-16579][table] Upgrade Calcite version to 1.26 for Flink SQL", "bodyText": "What is the purpose of the change\nUpgrade the Calcite version to 1.26.0. As a dependency, the Guava version to 29.0-jre and janino version to 3.0.11\nBrief change log\n\nUpgrade the sql parser calcite version to 1.26.0\nUpgrade the planners calcite version to 1.26.0\n\nVerifying this change\nExisting tests.\nDoes this pull request potentially affect one of the following parts:\n\nDependencies (does it add or upgrade a dependency): yes\nThe public API, i.e., is any changed class annotated with @Public(Evolving): no\nThe serializers: no\nThe runtime per-record code paths (performance sensitive): no\nAnything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: no\nThe S3 file system connector: no\n\nDocumentation\n\nDoes this pull request introduce a new feature? no", "createdAt": "2020-10-10T02:05:38Z", "url": "https://github.com/apache/flink/pull/13577", "merged": true, "mergeCommit": {"oid": "dc1da6a52496f67a52c99d7d39a5dd7266a42f5a"}, "closed": true, "closedAt": "2020-10-20T08:40:36Z", "author": {"login": "danny0405"}, "timelineItems": {"totalCount": 26, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdRFD8jgBqjM4NjI0NDkzMTE=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdUUenqAFqTUxMjQ1NTU0MA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "82b178f86a0afb4422d71ae0f0c40f4b3641e901", "author": {"user": {"login": "danny0405", "name": "Danny Chan"}}, "url": "https://github.com/apache/flink/commit/82b178f86a0afb4422d71ae0f0c40f4b3641e901", "committedDate": "2020-10-09T15:09:59Z", "message": "Fix test failure"}, "afterCommit": {"oid": "cde6b367bcfc5c76ee9004a6399c734ec890c6d1", "author": {"user": {"login": "danny0405", "name": "Danny Chan"}}, "url": "https://github.com/apache/flink/commit/cde6b367bcfc5c76ee9004a6399c734ec890c6d1", "committedDate": "2020-10-10T06:47:22Z", "message": "Fix test failure"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "cde6b367bcfc5c76ee9004a6399c734ec890c6d1", "author": {"user": {"login": "danny0405", "name": "Danny Chan"}}, "url": "https://github.com/apache/flink/commit/cde6b367bcfc5c76ee9004a6399c734ec890c6d1", "committedDate": "2020-10-10T06:47:22Z", "message": "Fix test failure"}, "afterCommit": {"oid": "66f4d1a8a98fd2a0736b0f8e14dbe765cedf33f6", "author": {"user": {"login": "danny0405", "name": "Danny Chan"}}, "url": "https://github.com/apache/flink/commit/66f4d1a8a98fd2a0736b0f8e14dbe765cedf33f6", "committedDate": "2020-10-10T07:44:02Z", "message": "Fix test failure"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "66f4d1a8a98fd2a0736b0f8e14dbe765cedf33f6", "author": {"user": {"login": "danny0405", "name": "Danny Chan"}}, "url": "https://github.com/apache/flink/commit/66f4d1a8a98fd2a0736b0f8e14dbe765cedf33f6", "committedDate": "2020-10-10T07:44:02Z", "message": "Fix test failure"}, "afterCommit": {"oid": "039b08f4bdd8a60a89fa324cc0213f4fb99a9e8f", "author": {"user": {"login": "danny0405", "name": "Danny Chan"}}, "url": "https://github.com/apache/flink/commit/039b08f4bdd8a60a89fa324cc0213f4fb99a9e8f", "committedDate": "2020-10-10T08:41:26Z", "message": "[FLINK-16579][table] All kinds of left plan changes\n\n* The predicate normalization now only happens during planning, that means it does not change in the digest anymore\n* The HOP and SESSION window names changes to $HOP and $SESSION, both are deprecated\n* IS NOT DISTINCT FROM is expanded in the plan now\n* Many sort aggregate changes to hash aggregate which are more efficient"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA2MDc3NDUw", "url": "https://github.com/apache/flink/pull/13577#pullrequestreview-506077450", "createdAt": "2020-10-10T08:02:24Z", "commit": {"oid": "66f4d1a8a98fd2a0736b0f8e14dbe765cedf33f6"}, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMFQwODowMjoyNVrOHfeJYA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMFQwODo0MjoyN1rOHfeXMw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc2MTgyNA==", "bodyText": "Why we have to depend on this dependency now?", "url": "https://github.com/apache/flink/pull/13577#discussion_r502761824", "createdAt": "2020-10-10T08:02:25Z", "author": {"login": "wuchong"}, "path": "flink-table/flink-table-planner-blink/pom.xml", "diffHunk": "@@ -189,10 +194,6 @@ under the License.\n \t\t\t\t\t<groupId>org.apache.commons</groupId>\n \t\t\t\t\t<artifactId>commons-dbcp2</artifactId>\n \t\t\t\t</exclusion>\n-\t\t\t\t<exclusion>\n-\t\t\t\t\t<groupId>com.esri.geometry</groupId>\n-\t\t\t\t\t<artifactId>esri-geometry-api</artifactId>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "66f4d1a8a98fd2a0736b0f8e14dbe765cedf33f6"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc2MzU5Nw==", "bodyText": "What about changing to use @Ignore +\t@Test annotations and also update the comment above?\nThis can avoid the IDEA warns this method is never used.", "url": "https://github.com/apache/flink/pull/13577#discussion_r502763597", "createdAt": "2020-10-10T08:21:48Z", "author": {"login": "wuchong"}, "path": "flink-table/flink-sql-parser-hive/src/test/java/org/apache/flink/sql/parser/hive/FlinkHiveSqlParserImplTest.java", "diffHunk": "@@ -35,15 +35,12 @@ protected SqlParserImplFactory parserImplFactory() {\n \t}\n \n \t// overrides test methods that we don't support\n-\t@Override", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "66f4d1a8a98fd2a0736b0f8e14dbe765cedf33f6"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc2MzY0Nw==", "bodyText": "Use @Test instead?", "url": "https://github.com/apache/flink/pull/13577#discussion_r502763647", "createdAt": "2020-10-10T08:22:23Z", "author": {"login": "wuchong"}, "path": "flink-table/flink-sql-parser/src/test/java/org/apache/flink/sql/parser/FlinkSqlParserImplTest.java", "diffHunk": "@@ -994,7 +992,6 @@ public void testShowViews() {\n \n \t// Override the test because our ROW field type default is nullable,\n \t// which is different with Calcite.\n-\t@Override", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "66f4d1a8a98fd2a0736b0f8e14dbe765cedf33f6"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc2NDc3Mg==", "bodyText": "It seems that dynamicOptions is never used in this class, should we remove it?", "url": "https://github.com/apache/flink/pull/13577#discussion_r502764772", "createdAt": "2020-10-10T08:35:13Z", "author": {"login": "wuchong"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/schema/TableSourceTable.scala", "diffHunk": "@@ -65,15 +65,8 @@ class TableSourceTable(\n     statistic) {\n \n   override def getQualifiedName: util.List[String] = {\n-    val names = super.getQualifiedName\n     val builder = ImmutableList.builder[String]()\n-    builder.addAll(names)\n-    if (dynamicOptions.size() != 0) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "66f4d1a8a98fd2a0736b0f8e14dbe765cedf33f6"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc2NTM2Mw==", "bodyText": "Is it a temporary solution? Why we have to override the default value in interface?\nBut this change is only needed in legacy planner?", "url": "https://github.com/apache/flink/pull/13577#discussion_r502765363", "createdAt": "2020-10-10T08:42:27Z", "author": {"login": "wuchong"}, "path": "flink-table/flink-table-planner/src/main/java/org/apache/calcite/schema/Statistic.java", "diffHunk": "@@ -0,0 +1,73 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to you under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.calcite.schema;\n+\n+import org.apache.calcite.rel.RelCollation;\n+import org.apache.calcite.rel.RelDistribution;\n+import org.apache.calcite.rel.RelReferentialConstraint;\n+import org.apache.calcite.util.ImmutableBitSet;\n+\n+import java.util.Collections;\n+import java.util.List;\n+\n+/**\n+ * Statistics about a {@link Table}.\n+ *\n+ * <p>Each of the methods may return {@code null} meaning \"not known\".</p>\n+ *\n+ * <p>Changes:\n+ *\n+ * <ul>\n+ *     <li>Line 61: default collations change from null to empty list.</li>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "039b08f4bdd8a60a89fa324cc0213f4fb99a9e8f"}, "originalPosition": 35}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "1fe9b6df03a206823ad28df58cf87ad7078eeb26", "author": {"user": {"login": "danny0405", "name": "Danny Chan"}}, "url": "https://github.com/apache/flink/commit/1fe9b6df03a206823ad28df58cf87ad7078eeb26", "committedDate": "2020-10-10T09:47:42Z", "message": "Fix the review comments"}, "afterCommit": {"oid": "25608fcfaf896c97019360e45ae4a2c9fe96ae63", "author": {"user": {"login": "danny0405", "name": "Danny Chan"}}, "url": "https://github.com/apache/flink/commit/25608fcfaf896c97019360e45ae4a2c9fe96ae63", "committedDate": "2020-10-12T11:30:42Z", "message": "Fix the test error"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "7610d0b3e65d7d857b0c8c47d99ff482bc46e2ee", "author": {"user": {"login": "danny0405", "name": "Danny Chan"}}, "url": "https://github.com/apache/flink/commit/7610d0b3e65d7d857b0c8c47d99ff482bc46e2ee", "committedDate": "2020-10-12T12:00:52Z", "message": "Fix the review comments"}, "afterCommit": {"oid": "7a38dfe2a8668cc543609d5e9f4a8887b8230b0e", "author": {"user": {"login": "danny0405", "name": "Danny Chan"}}, "url": "https://github.com/apache/flink/commit/7a38dfe2a8668cc543609d5e9f4a8887b8230b0e", "committedDate": "2020-10-13T04:12:43Z", "message": "Fix test failure"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "4c7d37a3b02ae9221b3f9a798ba9fd0523ec4bb9", "author": {"user": {"login": "danny0405", "name": "Danny Chan"}}, "url": "https://github.com/apache/flink/commit/4c7d37a3b02ae9221b3f9a798ba9fd0523ec4bb9", "committedDate": "2020-10-14T02:34:53Z", "message": "Fix test failure"}, "afterCommit": {"oid": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164", "author": {"user": {"login": "danny0405", "name": "Danny Chan"}}, "url": "https://github.com/apache/flink/commit/3db5dd700d90b99ec0a5d70d9b27a0df41a44164", "committedDate": "2020-10-14T07:14:51Z", "message": "Fix test failure\n\nRe-trigger the Azure tests"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA3OTU1OTky", "url": "https://github.com/apache/flink/pull/13577#pullrequestreview-507955992", "createdAt": "2020-10-14T02:45:35Z", "commit": {"oid": "4c7d37a3b02ae9221b3f9a798ba9fd0523ec4bb9"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 44, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQwMjo0NTozNlrOHhAJ0A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQxMTozNjozOVrOHhOueg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDM2NzU2OA==", "bodyText": "missed to update?", "url": "https://github.com/apache/flink/pull/13577#discussion_r504367568", "createdAt": "2020-10-14T02:45:36Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-sql-parser/src/test/java/org/apache/flink/sql/parser/FlinkSqlParserImplTest.java", "diffHunk": "@@ -188,7 +190,6 @@ public void testDescribeTable() {\n \t/**\n \t * Here we override the super method to avoid test error from `describe statement` supported in original calcite.\n \t */\n-\t@Override", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4c7d37a3b02ae9221b3f9a798ba9fd0523ec4bb9"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDM2OTA3Ng==", "bodyText": "legacy planner also has customer statistic class: FlinkStatistic", "url": "https://github.com/apache/flink/pull/13577#discussion_r504369076", "createdAt": "2020-10-14T02:51:23Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner/src/main/java/org/apache/calcite/schema/Statistic.java", "diffHunk": "@@ -0,0 +1,73 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to you under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.calcite.schema;\n+\n+import org.apache.calcite.rel.RelCollation;\n+import org.apache.calcite.rel.RelDistribution;\n+import org.apache.calcite.rel.RelReferentialConstraint;\n+import org.apache.calcite.util.ImmutableBitSet;\n+\n+import java.util.Collections;\n+import java.util.List;\n+\n+/**\n+ * Statistics about a {@link Table}.\n+ *\n+ * <p>Each of the methods may return {@code null} meaning \"not known\".</p>\n+ *\n+ * <p>Changes:\n+ *\n+ * <ul>\n+ *     <li>Line 61: default collations change from null to empty list.</li>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc2NTM2Mw=="}, "originalCommit": {"oid": "039b08f4bdd8a60a89fa324cc0213f4fb99a9e8f"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDM4MDY4Nw==", "bodyText": "nit: util.Collections", "url": "https://github.com/apache/flink/pull/13577#discussion_r504380687", "createdAt": "2020-10-14T03:35:31Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/functions/utils/AggSqlFunction.scala", "diffHunk": "@@ -165,18 +165,28 @@ object AggSqlFunction {\n     }\n   }\n \n-  private[flink] def createOperandTypeChecker(\n+  private[flink] def createOperandMetadata(\n       name: String,\n       aggregateFunction: ImperativeAggregateFunction[_, _],\n       externalAccType: DataType)\n-    : SqlOperandTypeChecker = {\n+    : SqlOperandMetadata = {\n \n     val methods = checkAndExtractMethods(aggregateFunction, \"accumulate\")\n \n     /**\n       * Operand type checker based on [[AggregateFunction]] given information.\n       */\n-    new SqlOperandTypeChecker {\n+    new SqlOperandMetadata {\n+      override def paramNames(): util.List[String] = {\n+        // Does not support named parameters.\n+        Collections.emptyList()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4c7d37a3b02ae9221b3f9a798ba9fd0523ec4bb9"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDM4MTgwOA==", "bodyText": "is this necessary?", "url": "https://github.com/apache/flink/pull/13577#discussion_r504381808", "createdAt": "2020-10-14T03:39:48Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/functions/utils/TableSqlFunction.scala", "diffHunk": "@@ -40,12 +44,19 @@ class TableSqlFunction(\n     functionImpl: FlinkTableFunctionImpl[_])\n   extends SqlUserDefinedTableFunction(\n     new SqlIdentifier(name, SqlParserPos.ZERO),\n+    SqlKind.OTHER_FUNCTION,\n     ReturnTypes.CURSOR,\n     createEvalOperandTypeInference(name, tableFunction, typeFactory),\n-    createEvalOperandTypeChecker(name, tableFunction),\n-    null,\n+    createEvalOperandMetadata(name, tableFunction),\n     functionImpl) {\n \n+  override def getRowTypeInference: SqlReturnTypeInference = new SqlReturnTypeInference {\n+    override def inferReturnType(opBinding: SqlOperatorBinding): RelDataType = {\n+      // The arguments should never be used.\n+      functionImpl.getRowType(opBinding.getTypeFactory, Collections.emptyList())\n+    }\n+  }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4c7d37a3b02ae9221b3f9a798ba9fd0523ec4bb9"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDM4ODQ5MQ==", "bodyText": "does null has special meaning ? if not, return empty list to avoid NPE", "url": "https://github.com/apache/flink/pull/13577#discussion_r504388491", "createdAt": "2020-10-14T04:06:20Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/functions/utils/UserDefinedFunctionUtils.scala", "diffHunk": "@@ -372,6 +373,9 @@ object UserDefinedFunctionUtils {\n \n       override def getConsistency: Consistency = Consistency.NONE\n \n+      override def paramTypes(typeFactory: RelDataTypeFactory): util.List[RelDataType] = null", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4c7d37a3b02ae9221b3f9a798ba9fd0523ec4bb9"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQzNjgzOA==", "bodyText": "can we simplify this result express ?  I think (1L:BIGINT..2L:BIGINT) ... (29L:BIGINT..30L:BIGINT) is meaningless, because b is bigint type, not double type.\nI find the result of another CalcTest does not contain type (BIGINT), can we also simply it ?", "url": "https://github.com/apache/flink/pull/13577#discussion_r504436838", "createdAt": "2020-10-14T06:42:02Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/api/batch/sql/CalcTest.scala", "diffHunk": "@@ -73,12 +78,23 @@ class CalcTest extends TableTestBase {\n     val util = batchTestUtil()\n     val table = util.addTable[(Int, Long, String)](\"MyTable\", 'a, 'b, 'c)\n \n-    val resultStr = (1 to 30).map(i => s\"$i:BIGINT\").mkString(\", \")\n+    val resultStr = \"Sarg[(-\u221e..1L:BIGINT), (1L:BIGINT..2L:BIGINT), \" +\n+        \"(2L:BIGINT..3L:BIGINT), (3L:BIGINT..4L:BIGINT), \" +\n+        \"(4L:BIGINT..5L:BIGINT), (5L:BIGINT..6L:BIGINT), \" +\n+        \"(6L:BIGINT..7L:BIGINT), (7L:BIGINT..8L:BIGINT), \" +\n+        \"(8L:BIGINT..9L:BIGINT), (9L:BIGINT..10L:BIGINT), \" +\n+        \"(10L:BIGINT..11L:BIGINT), (11L:BIGINT..12L:BIGINT), (12L:BIGINT..13L:BIGINT), \" +\n+        \"(13L:BIGINT..14L:BIGINT), (14L:BIGINT..15L:BIGINT), (15L:BIGINT..16L:BIGINT), \" +\n+        \"(16L:BIGINT..17L:BIGINT), (17L:BIGINT..18L:BIGINT), (18L:BIGINT..19L:BIGINT), \" +\n+        \"(19L:BIGINT..20L:BIGINT), (20L:BIGINT..21L:BIGINT), (21L:BIGINT..22L:BIGINT), \" +\n+        \"(22L:BIGINT..23L:BIGINT), (23L:BIGINT..24L:BIGINT), (24L:BIGINT..25L:BIGINT), \" +\n+        \"(25L:BIGINT..26L:BIGINT), (26L:BIGINT..27L:BIGINT), (27L:BIGINT..28L:BIGINT), \" +\n+        \"(28L:BIGINT..29L:BIGINT), (29L:BIGINT..30L:BIGINT), (30L:BIGINT..+\u221e)]:BIGINT\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4c7d37a3b02ae9221b3f9a798ba9fd0523ec4bb9"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQzNzgxNQ==", "bodyText": "it can be simplified as (1 to 30).map(i => s\"${i}L:BIGINT\").mkString(\"Sarg[\", \", \", \"]:BIGINT\")", "url": "https://github.com/apache/flink/pull/13577#discussion_r504437815", "createdAt": "2020-10-14T06:44:20Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/api/batch/sql/CalcTest.scala", "diffHunk": "@@ -54,12 +54,17 @@ class CalcTest extends TableTestBase {\n     val util = batchTestUtil()\n     val table = util.addTable[(Int, Long, String)](\"MyTable\", 'a, 'b, 'c)\n \n-    val resultStr = (1 to 30).map(i => s\"$i:BIGINT\").mkString(\", \")\n+    val resultStr = \"Sarg[1L:BIGINT, 2L:BIGINT, 3L:BIGINT, 4L:BIGINT, 5L:BIGINT, \" +\n+        \"6L:BIGINT, 7L:BIGINT, 8L:BIGINT, 9L:BIGINT, 10L:BIGINT, 11L:BIGINT, \" +\n+        \"12L:BIGINT, 13L:BIGINT, 14L:BIGINT, 15L:BIGINT, 16L:BIGINT, \" +\n+        \"17L:BIGINT, 18L:BIGINT, 19L:BIGINT, 20L:BIGINT, 21L:BIGINT, \" +\n+        \"22L:BIGINT, 23L:BIGINT, 24L:BIGINT, 25L:BIGINT, 26L:BIGINT, \" +\n+        \"27L:BIGINT, 28L:BIGINT, 29L:BIGINT, 30L:BIGINT]:BIGINT\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4c7d37a3b02ae9221b3f9a798ba9fd0523ec4bb9"}, "originalPosition": 10}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQ0MDMxNQ==", "bodyText": "can SEARCH(c, Sarg['xx']:CHAR(2) be simplified as =(c, 'xx')", "url": "https://github.com/apache/flink/pull/13577#discussion_r504440315", "createdAt": "2020-10-14T06:50:10Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/api/stream/table/CalcTest.scala", "diffHunk": "@@ -114,11 +114,13 @@ class CalcTest extends TableTestBase {\n     val resultTable = sourceTable.select('a, 'b, 'c)\n       .where((1 to 30).map($\"b\" === _).reduce((ex1, ex2) => ex1 || ex2) && ($\"c\" === \"xx\"))\n \n+    val operands = \"Sarg[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, \" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4c7d37a3b02ae9221b3f9a798ba9fd0523ec4bb9"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQ0Mjk1OQ==", "bodyText": "Is it within our expectations? what changes (or bug fixs) cause this ?", "url": "https://github.com/apache/flink/pull/13577#discussion_r504442959", "createdAt": "2020-10-14T06:55:58Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/expressions/ScalarFunctionsTest.scala", "diffHunk": "@@ -2679,15 +2679,15 @@ class ScalarFunctionsTest extends ScalarTypesTestBase {\n       \"1017-11-29 22:58:58.998\")\n \n     val QUARTER = Seq(\n-      \"2018-03-01 22:58:58.998\",\n-      \"2018-08-31 22:58:58.998\",\n+      \"2018-02-28 22:58:58.998\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4c7d37a3b02ae9221b3f9a798ba9fd0523ec4bb9"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQ0NDg0Nw==", "bodyText": "the plan is worse than before, because more fields need to shuffle.", "url": "https://github.com/apache/flink/pull/13577#discussion_r504444847", "createdAt": "2020-10-14T06:59:56Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/plan/TimeIndicatorConversionTest.scala", "diffHunk": "@@ -204,20 +204,16 @@ class TimeIndicatorConversionTest extends TableTestBase {\n \n     val result = t.unionAll(t).select('rowtime)\n \n-    val expected = binaryNode(\n-      \"DataStreamUnion\",\n-      unaryNode(\n-        \"DataStreamCalc\",\n+    val expected = unaryNode(\n+      \"DataStreamCalc\",\n+      binaryNode(\n+        \"DataStreamUnion\",\n         streamTableNode(t),\n-        term(\"select\", \"rowtime\")\n-      ),\n-      unaryNode(\n-        \"DataStreamCalc\",\n         streamTableNode(t),\n-        term(\"select\", \"rowtime\")\n+        term(\"all\", \"true\"),\n+        term(\"union all\", \"rowtime, long, int\")\n       ),\n-      term(\"all\", \"true\"),\n-      term(\"union all\", \"rowtime\")\n+      term(\"select\", \"rowtime\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4c7d37a3b02ae9221b3f9a798ba9fd0523ec4bb9"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQ1MTAyMA==", "bodyText": "nit: Line 671 ~ 681", "url": "https://github.com/apache/flink/pull/13577#discussion_r504451020", "createdAt": "2020-10-14T07:12:50Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/calcite/sql2rel/RelDecorrelator.java", "diffHunk": "@@ -0,0 +1,2955 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to you under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.calcite.sql2rel;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import com.google.common.collect.ImmutableSortedMap;\n+import com.google.common.collect.ImmutableSortedSet;\n+import com.google.common.collect.Iterables;\n+import com.google.common.collect.Multimap;\n+import com.google.common.collect.MultimapBuilder;\n+import com.google.common.collect.Sets;\n+import com.google.common.collect.SortedSetMultimap;\n+import org.apache.calcite.linq4j.Ord;\n+import org.apache.calcite.linq4j.function.Function2;\n+import org.apache.calcite.plan.Context;\n+import org.apache.calcite.plan.RelOptCluster;\n+import org.apache.calcite.plan.RelOptCostImpl;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.plan.RelOptUtil;\n+import org.apache.calcite.plan.RelRule;\n+import org.apache.calcite.plan.hep.HepPlanner;\n+import org.apache.calcite.plan.hep.HepProgram;\n+import org.apache.calcite.plan.hep.HepRelVertex;\n+import org.apache.calcite.rel.BiRel;\n+import org.apache.calcite.rel.RelCollation;\n+import org.apache.calcite.rel.RelHomogeneousShuttle;\n+import org.apache.calcite.rel.RelNode;\n+import org.apache.calcite.rel.core.Aggregate;\n+import org.apache.calcite.rel.core.AggregateCall;\n+import org.apache.calcite.rel.core.Correlate;\n+import org.apache.calcite.rel.core.CorrelationId;\n+import org.apache.calcite.rel.core.Filter;\n+import org.apache.calcite.rel.core.Join;\n+import org.apache.calcite.rel.core.JoinRelType;\n+import org.apache.calcite.rel.core.Project;\n+import org.apache.calcite.rel.core.RelFactories;\n+import org.apache.calcite.rel.core.Sort;\n+import org.apache.calcite.rel.core.Values;\n+import org.apache.calcite.rel.logical.LogicalAggregate;\n+import org.apache.calcite.rel.logical.LogicalCorrelate;\n+import org.apache.calcite.rel.logical.LogicalFilter;\n+import org.apache.calcite.rel.logical.LogicalJoin;\n+import org.apache.calcite.rel.logical.LogicalProject;\n+import org.apache.calcite.rel.logical.LogicalSnapshot;\n+import org.apache.calcite.rel.logical.LogicalTableFunctionScan;\n+import org.apache.calcite.rel.metadata.RelMdUtil;\n+import org.apache.calcite.rel.metadata.RelMetadataQuery;\n+import org.apache.calcite.rel.rules.CoreRules;\n+import org.apache.calcite.rel.rules.FilterCorrelateRule;\n+import org.apache.calcite.rel.rules.FilterJoinRule;\n+import org.apache.calcite.rel.rules.FilterProjectTransposeRule;\n+import org.apache.calcite.rel.type.RelDataType;\n+import org.apache.calcite.rel.type.RelDataTypeFactory;\n+import org.apache.calcite.rel.type.RelDataTypeField;\n+import org.apache.calcite.rex.RexBuilder;\n+import org.apache.calcite.rex.RexCall;\n+import org.apache.calcite.rex.RexCorrelVariable;\n+import org.apache.calcite.rex.RexFieldAccess;\n+import org.apache.calcite.rex.RexInputRef;\n+import org.apache.calcite.rex.RexLiteral;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.calcite.rex.RexShuttle;\n+import org.apache.calcite.rex.RexSubQuery;\n+import org.apache.calcite.rex.RexUtil;\n+import org.apache.calcite.rex.RexVisitorImpl;\n+import org.apache.calcite.sql.SqlExplainFormat;\n+import org.apache.calcite.sql.SqlExplainLevel;\n+import org.apache.calcite.sql.SqlFunction;\n+import org.apache.calcite.sql.SqlKind;\n+import org.apache.calcite.sql.SqlOperator;\n+import org.apache.calcite.sql.fun.SqlCountAggFunction;\n+import org.apache.calcite.sql.fun.SqlSingleValueAggFunction;\n+import org.apache.calcite.sql.fun.SqlStdOperatorTable;\n+import org.apache.calcite.tools.RelBuilder;\n+import org.apache.calcite.tools.RelBuilderFactory;\n+import org.apache.calcite.util.Holder;\n+import org.apache.calcite.util.ImmutableBeans;\n+import org.apache.calcite.util.ImmutableBitSet;\n+import org.apache.calcite.util.Litmus;\n+import org.apache.calcite.util.Pair;\n+import org.apache.calcite.util.ReflectUtil;\n+import org.apache.calcite.util.ReflectiveVisitor;\n+import org.apache.calcite.util.Util;\n+import org.apache.calcite.util.mapping.Mappings;\n+import org.apache.calcite.util.trace.CalciteTrace;\n+import org.slf4j.Logger;\n+\n+import javax.annotation.Nonnull;\n+\n+import java.math.BigDecimal;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NavigableMap;\n+import java.util.Objects;\n+import java.util.Set;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Copied to fix CALCITE-4333, should be removed for the next Calcite upgrade.\n+ *\n+ * <p>Changes: Line 672 ~ Line 682, Line 430 ~ Line 441.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4c7d37a3b02ae9221b3f9a798ba9fd0523ec4bb9"}, "originalPosition": 124}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQ1NTMxMQ==", "bodyText": "please also remove the TODO in SqlDateTimeUtils class around line 1488 since CALCITE-3199 is fixed", "url": "https://github.com/apache/flink/pull/13577#discussion_r504455311", "createdAt": "2020-10-14T07:21:00Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/codegen/calls/BuiltInMethods.scala", "diffHunk": "@@ -458,19 +458,15 @@ object BuiltInMethods {\n   val TRUNCATE_DEC = Types.lookupMethod(classOf[SqlFunctionUtils], \"struncate\",\n     classOf[DecimalData], classOf[Int])\n \n-  // TODO: remove if CALCITE-3199 fixed\n-  //  https://issues.apache.org/jira/browse/CALCITE-3199\n-  val UNIX_DATE_CEIL = Types.lookupMethod(classOf[SqlDateTimeUtils], \"unixDateCeil\",\n+  val UNIX_DATE_CEIL = Types.lookupMethod(classOf[DateTimeUtils], \"unixDateCeil\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQ1NTc0OA==", "bodyText": "return empty list to avoid NPE or throw an exception here directly", "url": "https://github.com/apache/flink/pull/13577#discussion_r504455748", "createdAt": "2020-10-14T07:21:53Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/functions/utils/AggSqlFunction.scala", "diffHunk": "@@ -165,18 +165,28 @@ object AggSqlFunction {\n     }\n   }\n \n-  private[flink] def createOperandTypeChecker(\n+  private[flink] def createOperandMetadata(\n       name: String,\n       aggregateFunction: ImperativeAggregateFunction[_, _],\n       externalAccType: DataType)\n-    : SqlOperandTypeChecker = {\n+    : SqlOperandMetadata = {\n \n     val methods = checkAndExtractMethods(aggregateFunction, \"accumulate\")\n \n     /**\n       * Operand type checker based on [[AggregateFunction]] given information.\n       */\n-    new SqlOperandTypeChecker {\n+    new SqlOperandMetadata {\n+      override def paramNames(): util.List[String] = {\n+        // Does not support named parameters.\n+        Collections.emptyList()\n+      }\n+\n+      override def paramTypes(typeFactory: RelDataTypeFactory): util.List[RelDataType] = {\n+        // This should be never invoked.\n+        null", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164"}, "originalPosition": 76}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQ1Njg2NQ==", "bodyText": "never use", "url": "https://github.com/apache/flink/pull/13577#discussion_r504456865", "createdAt": "2020-10-14T07:24:07Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/functions/utils/TableSqlFunction.scala", "diffHunk": "@@ -129,20 +138,46 @@ object TableSqlFunction {\n         }\n   }\n \n-  private[flink] def createOperandTypeChecker(\n+  private[flink] def createOperandMetadata(\n       name: String,\n-      udtf: TableFunction[_]): SqlOperandTypeChecker = {\n-    new OperandTypeChecker(name, udtf, checkAndExtractMethods(udtf, \"eval\"))\n+      udtf: TableFunction[_]): SqlOperandMetadata = {\n+    new OperandMetadata(name, udtf, checkAndExtractMethods(udtf, \"eval\"))\n+  }\n+\n+  /**\n+   * Converts arguments from [[org.apache.calcite.sql.SqlNode]] to\n+   * java object format.\n+   *\n+   * @param callBinding Operator bound to arguments\n+   * @param function target function to get parameter types from\n+   * @param opName name of the operator to use in error message\n+   * @return converted list of arguments\n+   */\n+  private[flink] def convertArguments(\n+      callBinding: SqlOperatorBinding,\n+      function: org.apache.calcite.schema.Function,\n+      opName: SqlIdentifier): util.List[Object] = {\n+    val arguments = new util.ArrayList[Object](callBinding.getOperandCount)\n+    0 until callBinding.getOperandCount foreach { i =>\n+      val operandType = callBinding.getOperandType(i)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQ1NzMxNw==", "bodyText": "ditto", "url": "https://github.com/apache/flink/pull/13577#discussion_r504457317", "createdAt": "2020-10-14T07:24:49Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/functions/utils/TableSqlFunction.scala", "diffHunk": "@@ -191,4 +226,8 @@ class OperandTypeChecker(\n   override def isOptional(i: Int): Boolean = false\n \n   override def getConsistency: Consistency = Consistency.NONE\n+\n+  override def paramTypes(typeFactory: RelDataTypeFactory): util.List[RelDataType] = null", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164"}, "originalPosition": 115}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQ1OTY5OA==", "bodyText": "is this necessary in this pr or support it in another pr ?\nbtw, please add related test in FlinkRelMdUniqueKeysTest", "url": "https://github.com/apache/flink/pull/13577#discussion_r504459698", "createdAt": "2020-10-14T07:29:14Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdUniqueKeys.scala", "diffHunk": "@@ -70,9 +70,29 @@ class FlinkRelMdUniqueKeys private extends MetadataHandler[BuiltInMetadata.Uniqu\n   private def getTableUniqueKeys(\n       tableSource: TableSource[_],\n       relOptTable: RelOptTable): JSet[ImmutableBitSet] = {\n-    // TODO get uniqueKeys from TableSchema of TableSource\n \n     relOptTable match {\n+      case sourceTable: TableSourceTable =>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQ2MDY1Mw==", "bodyText": "ditto.\nnit, please move this close to TableScan part, this could make the code easier to read.", "url": "https://github.com/apache/flink/pull/13577#discussion_r504460653", "createdAt": "2020-10-14T07:30:56Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdUniqueKeys.scala", "diffHunk": "@@ -539,6 +559,25 @@ class FlinkRelMdUniqueKeys private extends MetadataHandler[BuiltInMetadata.Uniqu\n     }\n   }\n \n+  def getUniqueKeys(\n+      rel: TableFunctionScan,\n+      mq: RelMetadataQuery,\n+      ignoreNulls: Boolean): JSet[ImmutableBitSet] = {\n+    if (rel.getInputs.size() == 1\n+        && rel.getCall.asInstanceOf[RexCall].getOperator.isInstanceOf[SqlWindowTableFunction]) {\n+      mq.getUniqueKeys(rel.getInput(0), ignoreNulls)\n+    } else {\n+      null\n+    }\n+  }\n+\n+  def getUniqueKeys(\n+      rel: WatermarkAssigner,\n+      mq: RelMetadataQuery,\n+      ignoreNulls: Boolean): JSet[ImmutableBitSet] = {\n+    mq.getUniqueKeys(rel.getInput, ignoreNulls)\n+  }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164"}, "originalPosition": 91}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQ2MjUxMg==", "bodyText": "can we make sure all INs have been converted to SEARCH ?", "url": "https://github.com/apache/flink/pull/13577#discussion_r504462512", "createdAt": "2020-10-14T07:34:19Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/metadata/SelectivityEstimator.scala", "diffHunk": "@@ -187,8 +188,8 @@ class SelectivityEstimator(rel: RelNode, mq: FlinkRelMetadataQuery)\n       case IS_NOT_NULL =>\n         estimateIsNotNull(operands.head)\n \n-      case IN =>\n-        estimateIn(operands.head, operands.slice(1, operands.size))\n+      case SEARCH =>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQ2NTc5Mg==", "bodyText": "do we meet some corner cases ?", "url": "https://github.com/apache/flink/pull/13577#discussion_r504465792", "createdAt": "2020-10-14T07:40:04Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecNestedLoopJoin.scala", "diffHunk": "@@ -94,7 +94,13 @@ class BatchExecNestedLoopJoin(\n       (buildRowSize + BinaryRowDataSerializer.LENGTH_SIZE_IN_BYTES) * shuffleBuildCount(mq)\n     val cpuCost = leftRowCnt * rightRowCnt\n     val costFactory = planner.getCostFactory.asInstanceOf[FlinkCostFactory]\n-    costFactory.makeCost(mq.getRowCount(this), cpuCost, 0, 0, memoryCost)\n+    val cost = costFactory.makeCost(mq.getRowCount(this), cpuCost, 0, 0, memoryCost)\n+    if (singleRowJoin) {\n+      // Make single row join more preferable than non-single row join.\n+      cost.multiplyBy(0.99)\n+    } else {\n+      cost\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQ2NzY2NA==", "bodyText": "remove this directly?", "url": "https://github.com/apache/flink/pull/13577#discussion_r504467664", "createdAt": "2020-10-14T07:43:31Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/rules/FlinkStreamRuleSets.scala", "diffHunk": "@@ -108,15 +108,16 @@ object FlinkStreamRuleSets {\n       REDUCE_EXPRESSION_RULES.asScala ++\n       List(\n         //removes constant keys from an Agg\n-        AggregateProjectPullUpConstantsRule.INSTANCE,\n+        CoreRules.AGGREGATE_PROJECT_PULL_UP_CONSTANTS,\n         // fix: FLINK-17553 unsupported call error when constant exists in group window key\n         // this rule will merge the project generated by AggregateProjectPullUpConstantsRule and\n         // make sure window aggregate can be correctly rewritten by StreamLogicalWindowAggregateRule\n-        ProjectMergeRule.INSTANCE,\n+        CoreRules.PROJECT_MERGE,\n         StreamLogicalWindowAggregateRule.INSTANCE,\n+//        WindowJoinRewriteRule.INSTANCE,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164"}, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQ2OTY1MA==", "bodyText": "please also update the param name in java doc", "url": "https://github.com/apache/flink/pull/13577#discussion_r504469650", "createdAt": "2020-10-14T07:46:58Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/ColumnIntervalUtil.scala", "diffHunk": "@@ -198,9 +198,14 @@ object ColumnIntervalUtil {\n     */\n   def getColumnIntervalWithFilter(\n       originInterval: Option[ValueInterval],\n-      predicate: RexNode,\n+      oriPred: RexNode,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQ3MTI0MA==", "bodyText": "provide an utility method in FlinkRexUtil,  there is a lot of similar code", "url": "https://github.com/apache/flink/pull/13577#discussion_r504471240", "createdAt": "2020-10-14T07:49:51Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/ColumnIntervalUtil.scala", "diffHunk": "@@ -198,9 +198,14 @@ object ColumnIntervalUtil {\n     */\n   def getColumnIntervalWithFilter(\n       originInterval: Option[ValueInterval],\n-      predicate: RexNode,\n+      oriPred: RexNode,\n       inputRef: Int,\n       rexBuilder: RexBuilder): ValueInterval = {\n+    val predicate = if (oriPred.getKind == SqlKind.SEARCH) {\n+      RexUtil.expandSearch(rexBuilder, null, oriPred)\n+    } else {\n+      oriPred\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQ3Mjc2Nw==", "bodyText": "It seems this method is not been used in any where.", "url": "https://github.com/apache/flink/pull/13577#discussion_r504472767", "createdAt": "2020-10-14T07:52:19Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/FlinkRexUtil.scala", "diffHunk": "@@ -343,6 +344,54 @@ object FlinkRexUtil {\n       }\n     })\n \n+  /**\n+   * Returns whether a given tree contains any {@link RexInputRef} nodes\n+   * with given indices.\n+   *\n+   * @param node a RexNode tree\n+   */\n+  def containsInputRef(node: RexNode, refs: JSet[Integer]): Boolean = try {\n+    val visitor = new RexVisitorImpl[Void](true) {\n+      override def visitInputRef(inputRef: RexInputRef): Void = {\n+        if (refs.contains(inputRef.getIndex)) {\n+          throw new Util.FoundOne(inputRef)\n+        }\n+        null\n+      }\n+    }\n+    node.accept(visitor)\n+    false\n+  } catch {\n+    case e: Util.FoundOne =>\n+      Util.swallow(e, null)\n+      true\n+  }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQ3MzAyNA==", "bodyText": "revert this ?", "url": "https://github.com/apache/flink/pull/13577#discussion_r504473024", "createdAt": "2020-10-14T07:52:45Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/FlinkRexUtil.scala", "diffHunk": "@@ -353,7 +402,7 @@ object FlinkRexUtil {\n     */\n   private[flink] def adjustInputRef(\n       expr: RexNode,\n-      fieldsOldToNewIndexMapping: Map[Int, Int]): RexNode = expr.accept(\n+      fieldsOldToNewIndexMapping: Map[Int, Int]) = expr.accept(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164"}, "originalPosition": 77}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQ3NDEzMQ==", "bodyText": "Very useful abstraction ~", "url": "https://github.com/apache/flink/pull/13577#discussion_r504474131", "createdAt": "2020-10-14T07:54:42Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/RelShuttles.scala", "diffHunk": "@@ -20,15 +20,15 @@ package org.apache.flink.table.planner.plan.utils\n import org.apache.flink.table.planner.catalog.QueryOperationCatalogViewTable\n \n import com.google.common.collect.Sets\n-import org.apache.calcite.plan.{RelOptUtil, ViewExpanders}\n-import org.apache.calcite.rel.core.{TableFunctionScan, TableScan}\n+import org.apache.calcite.plan.ViewExpanders\n+import org.apache.calcite.rel.core.TableScan\n import org.apache.calcite.rel.logical._\n-import org.apache.calcite.rel.{RelNode, RelShuttle, RelShuttleImpl}\n+import org.apache.calcite.rel.{RelHomogeneousShuttle, RelNode, RelShuttleImpl}\n import org.apache.calcite.rex.{RexNode, RexShuttle, RexSubQuery}\n \n import scala.collection.JavaConversions._\n \n-class DefaultRelShuttle extends RelShuttle {\n+class DefaultRelShuttle extends RelHomogeneousShuttle {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQ3NjA4MA==", "bodyText": "RowTableFunction(s) does not work now ?", "url": "https://github.com/apache/flink/pull/13577#discussion_r504476080", "createdAt": "2020-10-14T07:57:32Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/runtime/stream/sql/FunctionITCase.java", "diffHunk": "@@ -768,7 +768,8 @@ public void testRowTableFunction() throws Exception {\n \n \t\ttEnv().createTemporarySystemFunction(\"RowTableFunction\", RowTableFunction.class);\n \t\ttEnv().executeSql(\n-\t\t\t\t\"INSERT INTO SinkTable SELECT t.s, t.sa FROM SourceTable, LATERAL TABLE(RowTableFunction(s)) t\")\n+\t\t\t\t\"INSERT INTO SinkTable SELECT t.s, t.sa FROM SourceTable source, \"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQ4MTQyNA==", "bodyText": "this is a bad case, more data will be shuffled.", "url": "https://github.com/apache/flink/pull/13577#discussion_r504481424", "createdAt": "2020-10-14T08:06:18Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/batch/sql/agg/HashAggregateTest.xml", "diffHunk": "@@ -564,16 +562,14 @@ HashAggregate(isMerge=[true], select=[Final_COUNT(count1$0) AS EXPR$0]), rowType\n     <Resource name=\"planBefore\">\n       <![CDATA[\n LogicalAggregate(group=[{}], EXPR$0=[COUNT()]), rowType=[RecordType(BIGINT EXPR$0)]\n-+- LogicalProject($f0=[0]), rowType=[RecordType(INTEGER $f0)]\n-   +- LogicalTableScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(byte, short, int, long, float, double, boolean, string, date, time, timestamp, decimal3020, decimal105)]]]), rowType=[RecordType(TINYINT byte, SMALLINT short, INTEGER int, BIGINT long, FLOAT float, DOUBLE double, BOOLEAN boolean, VARCHAR(2147483647) string, DATE date, TIME(0) time, TIMESTAMP(3) timestamp, DECIMAL(30, 20) decimal3020, DECIMAL(10, 5) decimal105)]\n++- LogicalTableScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(byte, short, int, long, float, double, boolean, string, date, time, timestamp, decimal3020, decimal105)]]]), rowType=[RecordType(TINYINT byte, SMALLINT short, INTEGER int, BIGINT long, FLOAT float, DOUBLE double, BOOLEAN boolean, VARCHAR(2147483647) string, DATE date, TIME(0) time, TIMESTAMP(3) timestamp, DECIMAL(30, 20) decimal3020, DECIMAL(10, 5) decimal105)]\n ]]>\n     </Resource>\n     <Resource name=\"planAfter\">\n       <![CDATA[\n HashAggregate(isMerge=[false], select=[COUNT(*) AS EXPR$0]), rowType=[RecordType(BIGINT EXPR$0)]\n-+- Exchange(distribution=[single]), rowType=[RecordType(INTEGER $f0)]\n-   +- Calc(select=[0 AS $f0]), rowType=[RecordType(INTEGER $f0)]\n-      +- LegacyTableSourceScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(byte, short, int, long, float, double, boolean, string, date, time, timestamp, decimal3020, decimal105)]]], fields=[byte, short, int, long, float, double, boolean, string, date, time, timestamp, decimal3020, decimal105]), rowType=[RecordType(TINYINT byte, SMALLINT short, INTEGER int, BIGINT long, FLOAT float, DOUBLE double, BOOLEAN boolean, VARCHAR(2147483647) string, DATE date, TIME(0) time, TIMESTAMP(3) timestamp, DECIMAL(30, 20) decimal3020, DECIMAL(10, 5) decimal105)]\n++- Exchange(distribution=[single]), rowType=[RecordType(TINYINT byte, SMALLINT short, INTEGER int, BIGINT long, FLOAT float, DOUBLE double, BOOLEAN boolean, VARCHAR(2147483647) string, DATE date, TIME(0) time, TIMESTAMP(3) timestamp, DECIMAL(30, 20) decimal3020, DECIMAL(10, 5) decimal105)]", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQ4MjI4MA==", "bodyText": "ditto", "url": "https://github.com/apache/flink/pull/13577#discussion_r504482280", "createdAt": "2020-10-14T08:07:42Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/batch/sql/agg/SortAggregateTest.xml", "diffHunk": "@@ -574,16 +572,14 @@ SortAggregate(isMerge=[true], select=[Final_COUNT(count1$0) AS EXPR$0]), rowType\n     <Resource name=\"planBefore\">\n       <![CDATA[\n LogicalAggregate(group=[{}], EXPR$0=[COUNT()]), rowType=[RecordType(BIGINT EXPR$0)]\n-+- LogicalProject($f0=[0]), rowType=[RecordType(INTEGER $f0)]\n-   +- LogicalTableScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(byte, short, int, long, float, double, boolean, string, date, time, timestamp, decimal3020, decimal105)]]]), rowType=[RecordType(TINYINT byte, SMALLINT short, INTEGER int, BIGINT long, FLOAT float, DOUBLE double, BOOLEAN boolean, VARCHAR(2147483647) string, DATE date, TIME(0) time, TIMESTAMP(3) timestamp, DECIMAL(30, 20) decimal3020, DECIMAL(10, 5) decimal105)]\n++- LogicalTableScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(byte, short, int, long, float, double, boolean, string, date, time, timestamp, decimal3020, decimal105)]]]), rowType=[RecordType(TINYINT byte, SMALLINT short, INTEGER int, BIGINT long, FLOAT float, DOUBLE double, BOOLEAN boolean, VARCHAR(2147483647) string, DATE date, TIME(0) time, TIMESTAMP(3) timestamp, DECIMAL(30, 20) decimal3020, DECIMAL(10, 5) decimal105)]\n ]]>\n     </Resource>\n     <Resource name=\"planAfter\">\n       <![CDATA[\n SortAggregate(isMerge=[false], select=[COUNT(*) AS EXPR$0]), rowType=[RecordType(BIGINT EXPR$0)]\n-+- Exchange(distribution=[single]), rowType=[RecordType(INTEGER $f0)]\n-   +- Calc(select=[0 AS $f0]), rowType=[RecordType(INTEGER $f0)]\n-      +- LegacyTableSourceScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(byte, short, int, long, float, double, boolean, string, date, time, timestamp, decimal3020, decimal105)]]], fields=[byte, short, int, long, float, double, boolean, string, date, time, timestamp, decimal3020, decimal105]), rowType=[RecordType(TINYINT byte, SMALLINT short, INTEGER int, BIGINT long, FLOAT float, DOUBLE double, BOOLEAN boolean, VARCHAR(2147483647) string, DATE date, TIME(0) time, TIMESTAMP(3) timestamp, DECIMAL(30, 20) decimal3020, DECIMAL(10, 5) decimal105)]\n++- Exchange(distribution=[single]), rowType=[RecordType(TINYINT byte, SMALLINT short, INTEGER int, BIGINT long, FLOAT float, DOUBLE double, BOOLEAN boolean, VARCHAR(2147483647) string, DATE date, TIME(0) time, TIMESTAMP(3) timestamp, DECIMAL(30, 20) decimal3020, DECIMAL(10, 5) decimal105)]", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQ4NTI1OA==", "bodyText": "AVG does not been rewritten as SUM / COUNT now ?", "url": "https://github.com/apache/flink/pull/13577#discussion_r504485258", "createdAt": "2020-10-14T08:12:20Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/batch/sql/agg/WindowAggregateTest.xml", "diffHunk": "@@ -699,10 +699,10 @@ LogicalProject(s=[$1], a=[$2], wStart=[TUMBLE_START($0)])\n     </Resource>\n     <Resource name=\"planAfter\">\n       <![CDATA[\n-Calc(select=[CAST(CASE(=($f1, 0), null:INTEGER, s)) AS s, CAST(CAST(/(CASE(=($f1, 0), null:INTEGER, s), $f1))) AS a, w$start AS wStart])\n-+- HashWindowAggregate(window=[TumblingGroupWindow('w$, b, 900000)], properties=[w$start, w$end, w$rowtime], select=[Final_$SUM0(sum$0) AS s, Final_COUNT(count1$1) AS $f1])\n+Calc(select=[CAST(s) AS s, CAST(a) AS a, w$start AS wStart])\n++- HashWindowAggregate(window=[TumblingGroupWindow('w$, b, 900000)], properties=[w$start, w$end, w$rowtime], select=[Final_SUM(sum$0) AS s, Final_AVG(sum$1, count$2) AS a])", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164"}, "originalPosition": 124}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQ5NDYzNA==", "bodyText": "bad case!", "url": "https://github.com/apache/flink/pull/13577#discussion_r504494634", "createdAt": "2020-10-14T08:27:16Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/batch/sql/RankTest.xml", "diffHunk": "@@ -142,19 +171,17 @@ WHERE rk <= 2 AND rk > -2\n       <![CDATA[\n LogicalProject(a=[$0], b=[$1], rk=[$2])\n +- LogicalFilter(condition=[AND(<=($2, 2), >($2, -2))])\n-   +- LogicalProject(a=[$0], b=[$1], rk=[RANK() OVER (PARTITION BY $1, $2 ORDER BY $0 NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)])\n+   +- LogicalProject(a=[$0], b=[$1], rk=[RANK() OVER (PARTITION BY $1, $2 ORDER BY $0 NULLS FIRST)])\n       +- LogicalTableScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]])\n ]]>\n     </Resource>\n     <Resource name=\"planAfter\">\n       <![CDATA[\n-Calc(select=[a, b, $2])\n-+- Rank(rankType=[RANK], rankRange=[rankStart=-1, rankEnd=2], partitionBy=[b, c], orderBy=[a ASC], global=[true], select=[a, b, c, $2])\n+Calc(select=[a, b, w0$o0 AS $2], where=[SEARCH(w0$o0, Sarg[(-2..2]])])\n++- OverAggregate(partitionBy=[b, c], orderBy=[a ASC], window#0=[RANK(*) AS w0$o0 RANG BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW], select=[a, b, c, w0$o0])", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164"}, "originalPosition": 94}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDUwNjQ5NQ==", "bodyText": "bad case", "url": "https://github.com/apache/flink/pull/13577#discussion_r504506495", "createdAt": "2020-10-14T08:45:20Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/rules/logical/FlinkLogicalRankRuleForConstantRangeTest.xml", "diffHunk": "@@ -178,14 +178,14 @@ WHERE rk <= 2 AND rk > -2\n       <![CDATA[\n LogicalProject(a=[$0], b=[$1], rk=[$2])\n +- LogicalFilter(condition=[AND(<=($2, 2), >($2, -2))])\n-   +- LogicalProject(a=[$0], b=[$1], rk=[RANK() OVER (PARTITION BY $1, $2 ORDER BY $0 NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)])\n+   +- LogicalProject(a=[$0], b=[$1], rk=[RANK() OVER (PARTITION BY $1, $2 ORDER BY $0 NULLS FIRST)])\n       +- LogicalTableScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]])\n ]]>\n     </Resource>\n     <Resource name=\"planAfter\">\n       <![CDATA[\n-FlinkLogicalCalc(select=[a, b, $2])\n-+- FlinkLogicalRank(rankType=[RANK], rankRange=[rankStart=-1, rankEnd=2], partitionBy=[b,c], orderBy=[a ASC], select=[a, b, c, $2])\n+FlinkLogicalCalc(select=[a, b, w0$o0 AS $2], where=[SEARCH(w0$o0, Sarg[(-2..2]])])\n++- FlinkLogicalOverAggregate(window#0=[window(partition {1, 2} order by [0 ASC-nulls-first] aggs [RANK()])])", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164"}, "originalPosition": 100}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDUwNzQyOQ==", "bodyText": "bad case", "url": "https://github.com/apache/flink/pull/13577#discussion_r504507429", "createdAt": "2020-10-14T08:46:42Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/rules/logical/FlinkLogicalRankRuleForRangeEndTest.xml", "diffHunk": "@@ -131,14 +131,14 @@ WHERE rk <= 2 AND rk > -2\n       <![CDATA[\n LogicalProject(a=[$0], b=[$1], rk=[$2])\n +- LogicalFilter(condition=[AND(<=($2, 2), >($2, -2))])\n-   +- LogicalProject(a=[$0], b=[$1], rk=[RANK() OVER (PARTITION BY $1, $2 ORDER BY $0 NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)])\n+   +- LogicalProject(a=[$0], b=[$1], rk=[RANK() OVER (PARTITION BY $1, $2 ORDER BY $0 NULLS FIRST)])\n       +- LogicalTableScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]])\n ]]>\n     </Resource>\n     <Resource name=\"planAfter\">\n       <![CDATA[\n-FlinkLogicalCalc(select=[a, b, w0$o0])\n-+- FlinkLogicalRank(rankType=[RANK], rankRange=[rankStart=-1, rankEnd=2], partitionBy=[b,c], orderBy=[a ASC], select=[a, b, c, w0$o0])\n+FlinkLogicalCalc(select=[a, b, w0$o0 AS $2], where=[SEARCH(w0$o0, Sarg[(-2..2]])])\n++- FlinkLogicalOverAggregate(window#0=[window(partition {1, 2} order by [0 ASC-nulls-first] aggs [RANK()])])", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164"}, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDUwODcyMg==", "bodyText": "bad case", "url": "https://github.com/apache/flink/pull/13577#discussion_r504508722", "createdAt": "2020-10-14T08:48:42Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/rules/logical/RankNumberColumnRemoveRuleTest.xml", "diffHunk": "@@ -30,16 +30,15 @@ WHERE rank_num >= 1 AND rank_num < 2\n       <![CDATA[\n LogicalProject(a=[$0], rank_num=[$4])\n +- LogicalFilter(condition=[AND(>=($4, 1), <($4, 2))])\n-   +- LogicalProject(a=[$0], b=[$1], c=[$2], rowtime=[$3], rank_num=[RANK() OVER (PARTITION BY $0 ORDER BY $3 DESC NULLS LAST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)])\n+   +- LogicalProject(a=[$0], b=[$1], c=[$2], rowtime=[$3], rank_num=[RANK() OVER (PARTITION BY $0 ORDER BY $3 DESC NULLS LAST)])\n       +- LogicalTableScan(table=[[default_catalog, default_database, MyTable]])\n ]]>\n     </Resource>\n     <Resource name=\"planAfter\">\n       <![CDATA[\n-FlinkLogicalCalc(select=[a, w0$o0])\n-+- FlinkLogicalRank(rankType=[RANK], rankRange=[rankStart=1, rankEnd=1], partitionBy=[a], orderBy=[rowtime DESC], select=[a, rowtime, w0$o0])\n-   +- FlinkLogicalCalc(select=[a, rowtime])\n-      +- FlinkLogicalDataStreamTableScan(table=[[default_catalog, default_database, MyTable]])\n+FlinkLogicalCalc(select=[a, w0$o0 AS rank_num], where=[SEARCH(w0$o0, Sarg[[1..2)])])\n++- FlinkLogicalOverAggregate(window#0=[window(partition {0} order by [3 DESC-nulls-last] aggs [RANK()])])", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDUwOTIzMA==", "bodyText": "ditto", "url": "https://github.com/apache/flink/pull/13577#discussion_r504509230", "createdAt": "2020-10-14T08:49:24Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/rules/logical/RankNumberColumnRemoveRuleTest.xml", "diffHunk": "@@ -57,16 +56,15 @@ WHERE rank_num >= 1 AND rank_num < 3\n       <![CDATA[\n LogicalProject(a=[$0], rank_num=[$4])\n +- LogicalFilter(condition=[AND(>=($4, 1), <($4, 3))])\n-   +- LogicalProject(a=[$0], b=[$1], c=[$2], rowtime=[$3], rank_num=[ROW_NUMBER() OVER (PARTITION BY $0 ORDER BY $3 DESC NULLS LAST ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)])\n+   +- LogicalProject(a=[$0], b=[$1], c=[$2], rowtime=[$3], rank_num=[ROW_NUMBER() OVER (PARTITION BY $0 ORDER BY $3 DESC NULLS LAST)])\n       +- LogicalTableScan(table=[[default_catalog, default_database, MyTable]])\n ]]>\n     </Resource>\n     <Resource name=\"planAfter\">\n       <![CDATA[\n-FlinkLogicalCalc(select=[a, w0$o0])\n-+- FlinkLogicalRank(rankType=[ROW_NUMBER], rankRange=[rankStart=1, rankEnd=2], partitionBy=[a], orderBy=[rowtime DESC], select=[a, rowtime, w0$o0])\n-   +- FlinkLogicalCalc(select=[a, rowtime])\n-      +- FlinkLogicalDataStreamTableScan(table=[[default_catalog, default_database, MyTable]])\n+FlinkLogicalCalc(select=[a, w0$o0 AS rank_num], where=[SEARCH(w0$o0, Sarg[[1..3)])])\n++- FlinkLogicalOverAggregate(window#0=[window(partition {0} order by [3 DESC-nulls-last] rows between UNBOUNDED PRECEDING and CURRENT ROW aggs [ROW_NUMBER()])])", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDUwOTMzMQ==", "bodyText": "ditto", "url": "https://github.com/apache/flink/pull/13577#discussion_r504509331", "createdAt": "2020-10-14T08:49:32Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/rules/logical/RankNumberColumnRemoveRuleTest.xml", "diffHunk": "@@ -84,16 +82,15 @@ WHERE rank_num >= 1 AND rank_num < 2\n       <![CDATA[\n LogicalProject(a=[$0])\n +- LogicalFilter(condition=[AND(>=($4, 1), <($4, 2))])\n-   +- LogicalProject(a=[$0], b=[$1], c=[$2], rowtime=[$3], rank_num=[ROW_NUMBER() OVER (PARTITION BY $0 ORDER BY $3 DESC NULLS LAST ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)])\n+   +- LogicalProject(a=[$0], b=[$1], c=[$2], rowtime=[$3], rank_num=[ROW_NUMBER() OVER (PARTITION BY $0 ORDER BY $3 DESC NULLS LAST)])\n       +- LogicalTableScan(table=[[default_catalog, default_database, MyTable]])\n ]]>\n     </Resource>\n     <Resource name=\"planAfter\">\n       <![CDATA[\n-FlinkLogicalCalc(select=[a])\n-+- FlinkLogicalRank(rankType=[ROW_NUMBER], rankRange=[rankStart=1, rankEnd=1], partitionBy=[a], orderBy=[rowtime DESC], select=[a, rowtime])\n-   +- FlinkLogicalCalc(select=[a, rowtime])\n-      +- FlinkLogicalDataStreamTableScan(table=[[default_catalog, default_database, MyTable]])\n+FlinkLogicalCalc(select=[a], where=[SEARCH(w0$o0, Sarg[[1..2)])])\n++- FlinkLogicalOverAggregate(window#0=[window(partition {0} order by [3 DESC-nulls-last] rows between UNBOUNDED PRECEDING and CURRENT ROW aggs [ROW_NUMBER()])])", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDUwOTM5Nw==", "bodyText": "ditto", "url": "https://github.com/apache/flink/pull/13577#discussion_r504509397", "createdAt": "2020-10-14T08:49:38Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/rules/logical/RankNumberColumnRemoveRuleTest.xml", "diffHunk": "@@ -111,16 +108,15 @@ WHERE rank_num >= 1 AND rank_num < 2\n       <![CDATA[\n LogicalProject(a=[$0], rank_num=[$4])\n +- LogicalFilter(condition=[AND(>=($4, 1), <($4, 2))])\n-   +- LogicalProject(a=[$0], b=[$1], c=[$2], rowtime=[$3], rank_num=[ROW_NUMBER() OVER (PARTITION BY $0 ORDER BY $3 DESC NULLS LAST ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)])\n+   +- LogicalProject(a=[$0], b=[$1], c=[$2], rowtime=[$3], rank_num=[ROW_NUMBER() OVER (PARTITION BY $0 ORDER BY $3 DESC NULLS LAST)])\n       +- LogicalTableScan(table=[[default_catalog, default_database, MyTable]])\n ]]>\n     </Resource>\n     <Resource name=\"planAfter\">\n       <![CDATA[\n-FlinkLogicalCalc(select=[a, 1:BIGINT AS w0$o0])\n-+- FlinkLogicalRank(rankType=[ROW_NUMBER], rankRange=[rankStart=1, rankEnd=1], partitionBy=[a], orderBy=[rowtime DESC], select=[a, rowtime])\n-   +- FlinkLogicalCalc(select=[a, rowtime])\n-      +- FlinkLogicalDataStreamTableScan(table=[[default_catalog, default_database, MyTable]])\n+FlinkLogicalCalc(select=[a, w0$o0 AS rank_num], where=[SEARCH(w0$o0, Sarg[[1..2)])])\n++- FlinkLogicalOverAggregate(window#0=[window(partition {0} order by [3 DESC-nulls-last] rows between UNBOUNDED PRECEDING and CURRENT ROW aggs [ROW_NUMBER()])])", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164"}, "originalPosition": 79}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDUxNDY2Mg==", "bodyText": "do you know which jira causes this join-order change without join-order rule ?", "url": "https://github.com/apache/flink/pull/13577#discussion_r504514662", "createdAt": "2020-10-14T08:57:15Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/stream/sql/join/SemiAntiJoinTest.xml", "diffHunk": "@@ -516,28 +516,28 @@ LogicalFilter(condition=[<>($cor0.b, $1)])\n       <![CDATA[\n Join(joinType=[LeftAntiJoin], where=[<>(b, e)], select=[a, b, c], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey])\n :- Exchange(distribution=[single])\n-:  +- Join(joinType=[LeftSemiJoin], where=[$f0], select=[a, b, c], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey])\n-:     :- Exchange(distribution=[single])\n-:     :  +- Join(joinType=[LeftAntiJoin], where=[AND(OR(=(b, i), IS NULL(b), IS NULL(i)), =(c, k))], select=[a, b, c], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey])\n-:     :     :- Exchange(distribution=[hash[c]])\n+:  +- Join(joinType=[LeftAntiJoin], where=[AND(OR(IS NULL(b), IS NULL(i), =(b, i)), =(c, k))], select=[a, b, c], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey])", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDUxNzM0Nw==", "bodyText": "bad case, shuffle more data", "url": "https://github.com/apache/flink/pull/13577#discussion_r504517347", "createdAt": "2020-10-14T09:01:12Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/stream/sql/TableScanTest.xml", "diffHunk": "@@ -23,17 +23,16 @@ limitations under the License.\n     <Resource name=\"planBefore\">\n       <![CDATA[\n LogicalAggregate(group=[{}], EXPR$0=[COUNT()])\n-+- LogicalProject($f0=[0])\n-   +- LogicalFilter(condition=[>($1, 1)])\n-      +- LogicalTableScan(table=[[default_catalog, default_database, src]])\n++- LogicalFilter(condition=[>($1, 1)])\n+   +- LogicalTableScan(table=[[default_catalog, default_database, src]])\n ]]>\n     </Resource>\n     <Resource name=\"planAfter\">\n       <![CDATA[\n GroupAggregate(select=[COUNT_RETRACT(*) AS EXPR$0], changelogMode=[I,UA,D])\n +- Exchange(distribution=[single], changelogMode=[I,UB,UA])\n-   +- Calc(select=[0 AS $f0], where=[>(a, 1)], changelogMode=[I,UB,UA])\n-      +- TableSourceScan(table=[[default_catalog, default_database, src, filter=[], project=[a]]], fields=[a], changelogMode=[I,UB,UA])\n+   +- Calc(select=[ts, a, b], where=[>(a, 1)], changelogMode=[I,UB,UA])", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDUxNzk4Mg==", "bodyText": "bad case", "url": "https://github.com/apache/flink/pull/13577#discussion_r504517982", "createdAt": "2020-10-14T09:02:05Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/stream/sql/TableSourceTest.xml", "diffHunk": "@@ -68,15 +68,14 @@ Calc(select=[rtime])\n     <Resource name=\"planBefore\">\n       <![CDATA[\n LogicalAggregate(group=[{}], EXPR$0=[COUNT()])\n-+- LogicalProject($f0=[1])\n-   +- LogicalTableScan(table=[[default_catalog, default_database, T]])\n++- LogicalTableScan(table=[[default_catalog, default_database, T]])\n ]]>\n     </Resource>\n     <Resource name=\"planAfter\">\n       <![CDATA[\n GroupAggregate(select=[COUNT(*) AS EXPR$0])\n +- Exchange(distribution=[single])\n-   +- TableSourceScan(table=[[default_catalog, default_database, T, project=[]]], fields=[])\n+   +- TableSourceScan(table=[[default_catalog, default_database, T]], fields=[id, name])", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDUxODg3Nw==", "bodyText": "ditto", "url": "https://github.com/apache/flink/pull/13577#discussion_r504518877", "createdAt": "2020-10-14T09:03:30Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/stream/table/SetOperatorsTest.xml", "diffHunk": "@@ -100,10 +100,9 @@ LogicalProject(b=[$1], c=[$2])\n     </Resource>\n     <Resource name=\"planAfter\">\n       <![CDATA[\n-Union(all=[true], union=[b, c])\n-:- Calc(select=[b, c])\n-:  +- LegacyTableSourceScan(table=[[default_catalog, default_database, left, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])\n-+- Calc(select=[b, c])\n+Calc(select=[b, c])", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDUxOTYxMw==", "bodyText": "remove unused imports", "url": "https://github.com/apache/flink/pull/13577#discussion_r504519613", "createdAt": "2020-10-14T09:04:33Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/batch/table/CorrelateTest.scala", "diffHunk": "@@ -23,7 +23,7 @@ import org.apache.flink.table.api._\n import org.apache.flink.table.planner.plan.optimize.program.FlinkBatchProgram\n import org.apache.flink.table.planner.utils.{MockPythonTableFunction, TableFunc0, TableFunc1, TableTestBase}\n \n-import org.apache.calcite.rel.rules.{CalcMergeRule, FilterCalcMergeRule, ProjectCalcMergeRule}\n+import org.apache.calcite.rel.rules.{CalcMergeRule, CoreRules, FilterCalcMergeRule, ProjectCalcMergeRule}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDUzNTI3MA==", "bodyText": "use assertEquals with delta parameter, we only need to compare two-bit precision.", "url": "https://github.com/apache/flink/pull/13577#discussion_r504535270", "createdAt": "2020-10-14T09:29:27Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdDistinctRowCountTest.scala", "diffHunk": "@@ -91,35 +91,43 @@ class FlinkRelMdDistinctRowCountTest extends FlinkRelMdHandlerTestBase {\n     assertEquals(1.0, mq.getDistinctRowCount(logicalProject, ImmutableBitSet.of(), null))\n     assertEquals(50.0, mq.getDistinctRowCount(logicalProject, ImmutableBitSet.of(0), null))\n     assertEquals(48.0, mq.getDistinctRowCount(logicalProject, ImmutableBitSet.of(1), null))\n-    assertEquals(16.96, mq.getDistinctRowCount(logicalProject, ImmutableBitSet.of(2), null), 1e-2)\n+    assertEquals(17.13976902522821,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDYwMzk3Nw==", "bodyText": "is this within our expectations?", "url": "https://github.com/apache/flink/pull/13577#discussion_r504603977", "createdAt": "2020-10-14T11:31:55Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdPopulationSizeTest.scala", "diffHunk": "@@ -294,28 +306,28 @@ class FlinkRelMdPopulationSizeTest extends FlinkRelMdHandlerTestBase {\n \n     assertEquals(1.0, mq.getPopulationSize(logicalLeftJoinNotOnUniqueKeys, ImmutableBitSet.of()))\n     assertEquals(2.0E7, mq.getPopulationSize(logicalLeftJoinNotOnUniqueKeys, ImmutableBitSet.of(0)))\n-    assertEquals(505696447.06,\n+    assertEquals(5.056964454581646E8,\n       mq.getPopulationSize(logicalLeftJoinNotOnUniqueKeys, ImmutableBitSet.of(1)), 1e-2)\n-    assertEquals(799999979.15,\n+    assertEquals(8.0E8,\n       mq.getPopulationSize(logicalLeftJoinNotOnUniqueKeys, ImmutableBitSet.of(1, 5)), 1e-2)\n-    assertEquals(793772745.78,\n+    assertEquals(7.937719925300186E8,\n       mq.getPopulationSize(logicalLeftJoinNotOnUniqueKeys, ImmutableBitSet.of(0, 6)), 1e-2)\n \n     assertEquals(1.0,\n       mq.getPopulationSize(logicalRightJoinOnLHSUniqueKeys, ImmutableBitSet.of()))\n-    assertEquals(12642411.178,\n+    assertEquals(1.2642411364806734E7,\n       mq.getPopulationSize(logicalRightJoinOnLHSUniqueKeys, ImmutableBitSet.of(0)), 1e-2)\n-    assertEquals(19752070.37,\n+    assertEquals(1.9752070270976853E7,\n       mq.getPopulationSize(logicalRightJoinOnLHSUniqueKeys, ImmutableBitSet.of(1)), 1e-2)\n-    assertEquals(19999999.87,\n+    assertEquals(2.0E7,\n       mq.getPopulationSize(logicalRightJoinOnLHSUniqueKeys, ImmutableBitSet.of(1, 5)), 1e-2)\n-    assertEquals(19996088.14,\n+    assertEquals(1.9996069026214965E7,\n       mq.getPopulationSize(logicalRightJoinOnLHSUniqueKeys, ImmutableBitSet.of(0, 6)), 1e-2)\n \n     assertEquals(1.0, mq.getPopulationSize(logicalFullJoinWithoutEquiCond, ImmutableBitSet.of()))\n     assertEquals(2.0E7, mq.getPopulationSize(logicalFullJoinWithoutEquiCond, ImmutableBitSet.of(0)))\n     assertEquals(8.0E8, mq.getPopulationSize(logicalFullJoinWithoutEquiCond, ImmutableBitSet.of(1)))\n-    assertEquals(6.295509444597865E15,\n+    assertEquals(8.0E15,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164"}, "originalPosition": 106}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDYwNjMzMA==", "bodyText": "we can implement a rule to re-add a projection node for one phase aggregation", "url": "https://github.com/apache/flink/pull/13577#discussion_r504606330", "createdAt": "2020-10-14T11:36:39Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoLegacyTableSourceScanRuleTest.scala", "diffHunk": "@@ -111,6 +111,9 @@ class PushProjectIntoLegacyTableSourceScanRuleTest extends TableTestBase {\n \n   @Test\n   def testProjectWithoutInputRef(): Unit = {\n+    // Regression by: CALCITE-4220,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164"}, "originalPosition": 4}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "73c9c695ef458188e79e04e4549109020617564d", "author": {"user": {"login": "danny0405", "name": "Danny Chan"}}, "url": "https://github.com/apache/flink/commit/73c9c695ef458188e79e04e4549109020617564d", "committedDate": "2020-10-16T03:24:00Z", "message": "Fix review comments"}, "afterCommit": {"oid": "95841cbd815bd7dc9e773e00d164f961fd9bef1e", "author": {"user": {"login": "danny0405", "name": "Danny Chan"}}, "url": "https://github.com/apache/flink/commit/95841cbd815bd7dc9e773e00d164f961fd9bef1e", "committedDate": "2020-10-16T05:48:51Z", "message": "Fix review comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b14e01c94da3395f712b1a7f5f062fef1e367fb2", "author": {"user": {"login": "danny0405", "name": "Danny Chan"}}, "url": "https://github.com/apache/flink/commit/b14e01c94da3395f712b1a7f5f062fef1e367fb2", "committedDate": "2020-10-19T03:15:39Z", "message": "[FLINK-16579][table] Update Calcite version of pom and NOTICE file\n\nAs a dependency, the Guava version upgrade to 29.0-jre, janino version upgrade to 3.0.11"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1b3686abb009159aa2062b3bf178bae7a9cee08e", "author": {"user": {"login": "danny0405", "name": "Danny Chan"}}, "url": "https://github.com/apache/flink/commit/1b3686abb009159aa2062b3bf178bae7a9cee08e", "committedDate": "2020-10-19T03:15:39Z", "message": "[FLINK-16579][table] Upgrade SQL parser Calcite version to 1.26.0"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1e6804f7a0f4b3165115feca6dd6bdb66a46baa9", "author": {"user": {"login": "danny0405", "name": "Danny Chan"}}, "url": "https://github.com/apache/flink/commit/1e6804f7a0f4b3165115feca6dd6bdb66a46baa9", "committedDate": "2020-10-19T03:21:12Z", "message": "[FLINK-16579][table] Fix the API change for planner\n\n* SqlParser, SqlValidator and SqlToRelConverter now have a Config bean for configuration\n* Since CALCITE-2082, all the UDF needs a SqlOperandMetadata for operand type inference\n  (before the change, it is SqlOperandTypeChecker)\n* Since CALCITE-4215, Statistic default collations change from empty list to null"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3d780cebbed6e5301a78543489dc6a7c4ec6d273", "author": {"user": {"login": "danny0405", "name": "Danny Chan"}}, "url": "https://github.com/apache/flink/commit/3d780cebbed6e5301a78543489dc6a7c4ec6d273", "committedDate": "2020-10-19T03:21:12Z", "message": "[FLINK-16579][table] Replace the rule instance with new one since CALCITE-3923\n\nThe core rules are changed to support new way of parameterization, and moved to CoreRules."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "40f569428531fd0f9a5075e32f0f819b0d0328ea", "author": {"user": {"login": "danny0405", "name": "Danny Chan"}}, "url": "https://github.com/apache/flink/commit/40f569428531fd0f9a5075e32f0f819b0d0328ea", "committedDate": "2020-10-19T03:21:13Z", "message": "[FLINK-16579][table] Since CALCITE-3877, the default over window bounds does not print out in the plan digest"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4c0252a6db5699cf5505e356aae94ae49bb4fe01", "author": {"user": {"login": "danny0405", "name": "Danny Chan"}}, "url": "https://github.com/apache/flink/commit/4c0252a6db5699cf5505e356aae94ae49bb4fe01", "committedDate": "2020-10-19T03:21:13Z", "message": "[FLINK-16579][table] Since CALCITE-3877, the default over window bounds does not print out in the plan digest"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d978e4fdfcf24f694bd1eadee0dc3ddcf029fd43", "author": {"user": {"login": "danny0405", "name": "Danny Chan"}}, "url": "https://github.com/apache/flink/commit/d978e4fdfcf24f694bd1eadee0dc3ddcf029fd43", "committedDate": "2020-10-19T03:21:13Z", "message": "[FLINK-16579][table] Since CALCITE-4220, the aggregate is promoted automatically after sql to rel conversion"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fbeaabe5b9f2f64500f68b38192dbac4cb0c3bfa", "author": {"user": {"login": "danny0405", "name": "Danny Chan"}}, "url": "https://github.com/apache/flink/commit/fbeaabe5b9f2f64500f68b38192dbac4cb0c3bfa", "committedDate": "2020-10-19T03:21:13Z", "message": "[FLINK-16579][table] The NDV algorithm has been tweaked since CALCITE-4132"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "95841cbd815bd7dc9e773e00d164f961fd9bef1e", "author": {"user": {"login": "danny0405", "name": "Danny Chan"}}, "url": "https://github.com/apache/flink/commit/95841cbd815bd7dc9e773e00d164f961fd9bef1e", "committedDate": "2020-10-16T05:48:51Z", "message": "Fix review comments"}, "afterCommit": {"oid": "b56c3e8c4176504173eb1bb4dcc312082dabc4bc", "author": {"user": {"login": "danny0405", "name": "Danny Chan"}}, "url": "https://github.com/apache/flink/commit/b56c3e8c4176504173eb1bb4dcc312082dabc4bc", "committedDate": "2020-10-19T03:21:15Z", "message": "Fix review comments"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "b56c3e8c4176504173eb1bb4dcc312082dabc4bc", "author": {"user": {"login": "danny0405", "name": "Danny Chan"}}, "url": "https://github.com/apache/flink/commit/b56c3e8c4176504173eb1bb4dcc312082dabc4bc", "committedDate": "2020-10-19T03:21:15Z", "message": "Fix review comments"}, "afterCommit": {"oid": "cee7bbd18b87444a31483cd90607d9757b043ab6", "author": {"user": {"login": "danny0405", "name": "Danny Chan"}}, "url": "https://github.com/apache/flink/commit/cee7bbd18b87444a31483cd90607d9757b043ab6", "committedDate": "2020-10-19T03:31:39Z", "message": "Fix review comments"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "cee7bbd18b87444a31483cd90607d9757b043ab6", "author": {"user": {"login": "danny0405", "name": "Danny Chan"}}, "url": "https://github.com/apache/flink/commit/cee7bbd18b87444a31483cd90607d9757b043ab6", "committedDate": "2020-10-19T03:31:39Z", "message": "Fix review comments"}, "afterCommit": {"oid": "98165e15f06942ca55663fd9c1de667f8008139e", "author": {"user": {"login": "danny0405", "name": "Danny Chan"}}, "url": "https://github.com/apache/flink/commit/98165e15f06942ca55663fd9c1de667f8008139e", "committedDate": "2020-10-19T03:51:50Z", "message": "[FLINK-16579][table] All kinds of left plan changes\n\n* The predicate normalization now only happens during planning, that means it does not change in the digest anymore\n* The HOP and SESSION window names changes to $HOP and $SESSION, both are deprecated\n* IS NOT DISTINCT FROM is expanded in the plan now\n* Many sort aggregate changes to hash aggregate which are more efficient\n\nFix review comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "08fada0fcc5270803b39a711b20f519c3e6c7fe4", "author": {"user": {"login": "danny0405", "name": "Danny Chan"}}, "url": "https://github.com/apache/flink/commit/08fada0fcc5270803b39a711b20f519c3e6c7fe4", "committedDate": "2020-10-19T06:41:49Z", "message": "[FLINK-16579][table] All kinds of left plan changes\n\n* The predicate normalization now only happens during planning, that means it does not change in the digest anymore\n* The HOP and SESSION window names changes to $HOP and $SESSION, both are deprecated\n* IS NOT DISTINCT FROM is expanded in the plan now\n* Many sort aggregate changes to hash aggregate which are more efficient\n\nFix review comments"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "98165e15f06942ca55663fd9c1de667f8008139e", "author": {"user": {"login": "danny0405", "name": "Danny Chan"}}, "url": "https://github.com/apache/flink/commit/98165e15f06942ca55663fd9c1de667f8008139e", "committedDate": "2020-10-19T03:51:50Z", "message": "[FLINK-16579][table] All kinds of left plan changes\n\n* The predicate normalization now only happens during planning, that means it does not change in the digest anymore\n* The HOP and SESSION window names changes to $HOP and $SESSION, both are deprecated\n* IS NOT DISTINCT FROM is expanded in the plan now\n* Many sort aggregate changes to hash aggregate which are more efficient\n\nFix review comments"}, "afterCommit": {"oid": "08fada0fcc5270803b39a711b20f519c3e6c7fe4", "author": {"user": {"login": "danny0405", "name": "Danny Chan"}}, "url": "https://github.com/apache/flink/commit/08fada0fcc5270803b39a711b20f519c3e6c7fe4", "committedDate": "2020-10-19T06:41:49Z", "message": "[FLINK-16579][table] All kinds of left plan changes\n\n* The predicate normalization now only happens during planning, that means it does not change in the digest anymore\n* The HOP and SESSION window names changes to $HOP and $SESSION, both are deprecated\n* IS NOT DISTINCT FROM is expanded in the plan now\n* Many sort aggregate changes to hash aggregate which are more efficient\n\nFix review comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTExNzI4MjIy", "url": "https://github.com/apache/flink/pull/13577#pullrequestreview-511728222", "createdAt": "2020-10-19T12:55:09Z", "commit": {"oid": "08fada0fcc5270803b39a711b20f519c3e6c7fe4"}, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xOVQxMjo1NTowOVrOHkM7dQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xOVQxMzowOTozNVrOHkNe0A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzcyMjYxMw==", "bodyText": "remove this ?", "url": "https://github.com/apache/flink/pull/13577#discussion_r507722613", "createdAt": "2020-10-19T12:55:09Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/functions/utils/TableSqlFunction.scala", "diffHunk": "@@ -82,9 +83,16 @@ class TableSqlFunction(\n \n   override def toString: String = displayName\n \n-  override def getRowType(\n+  override def getRowTypeInference: SqlReturnTypeInference = new SqlReturnTypeInference {\n+    override def inferReturnType(opBinding: SqlOperatorBinding): RelDataType = {\n+      val arguments = convertArguments(opBinding, functionImpl, getNameAsId)\n+      getRowType(opBinding.getTypeFactory, arguments)\n+    }\n+  }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "08fada0fcc5270803b39a711b20f519c3e6c7fe4"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzcyMzE3MA==", "bodyText": "unused import", "url": "https://github.com/apache/flink/pull/13577#discussion_r507723170", "createdAt": "2020-10-19T12:56:00Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/functions/utils/AggSqlFunction.scala", "diffHunk": "@@ -19,24 +19,25 @@\n package org.apache.flink.table.planner.functions.utils\n \n import org.apache.flink.table.api.ValidationException\n-import org.apache.flink.table.functions.{AggregateFunction, FunctionIdentifier, TableAggregateFunction, ImperativeAggregateFunction}\n+import org.apache.flink.table.functions.{AggregateFunction, FunctionIdentifier, ImperativeAggregateFunction, TableAggregateFunction}\n+import org.apache.flink.table.planner.JList\n import org.apache.flink.table.planner.calcite.FlinkTypeFactory\n import org.apache.flink.table.planner.functions.bridging.BridgingSqlAggFunction\n-import org.apache.flink.table.planner.functions.utils.AggSqlFunction.{createOperandTypeChecker, createOperandTypeInference, createReturnTypeInference}\n+import org.apache.flink.table.planner.functions.utils.AggSqlFunction.{createOperandMetadata, createOperandTypeInference, createReturnTypeInference}\n import org.apache.flink.table.planner.functions.utils.UserDefinedFunctionUtils._\n import org.apache.flink.table.runtime.types.LogicalTypeDataTypeConverter.fromDataTypeToLogicalType\n import org.apache.flink.table.types.DataType\n import org.apache.flink.table.types.logical.LogicalType\n \n-import org.apache.calcite.rel.`type`.RelDataType\n+import org.apache.calcite.rel.`type`.{RelDataType, RelDataTypeFactory}\n import org.apache.calcite.sql._\n import org.apache.calcite.sql.`type`.SqlOperandTypeChecker.Consistency\n import org.apache.calcite.sql.`type`._\n import org.apache.calcite.sql.parser.SqlParserPos\n import org.apache.calcite.sql.validate.SqlUserDefinedAggFunction\n import org.apache.calcite.util.Optionality\n \n-import java.util\n+import java.util.Collections", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "08fada0fcc5270803b39a711b20f519c3e6c7fe4"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzcyNTgzNA==", "bodyText": "is this change necessary ?", "url": "https://github.com/apache/flink/pull/13577#discussion_r507725834", "createdAt": "2020-10-19T13:00:02Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdUniqueKeys.scala", "diffHunk": "@@ -58,6 +61,18 @@ class FlinkRelMdUniqueKeys private extends MetadataHandler[BuiltInMetadata.Uniqu\n     getTableUniqueKeys(null, rel.getTable)\n   }\n \n+  def getUniqueKeys(\n+      rel: TableFunctionScan,\n+      mq: RelMetadataQuery,\n+      ignoreNulls: Boolean): JSet[ImmutableBitSet] = {\n+    if (rel.getInputs.size() == 1\n+        && rel.getCall.asInstanceOf[RexCall].getOperator.isInstanceOf[SqlWindowTableFunction]) {\n+      mq.getUniqueKeys(rel.getInput(0), ignoreNulls)\n+    } else {\n+      null\n+    }\n+  }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "08fada0fcc5270803b39a711b20f519c3e6c7fe4"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzczMDI3OQ==", "bodyText": "this also need to update", "url": "https://github.com/apache/flink/pull/13577#discussion_r507730279", "createdAt": "2020-10-19T13:07:17Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdPopulationSizeTest.scala", "diffHunk": "@@ -66,19 +66,25 @@ class FlinkRelMdPopulationSizeTest extends FlinkRelMdHandlerTestBase {\n     assertEquals(1.0, mq.getPopulationSize(logicalProject, ImmutableBitSet.of()))\n     assertEquals(50.0, mq.getPopulationSize(logicalProject, ImmutableBitSet.of(0)))\n     assertEquals(48.0, mq.getPopulationSize(logicalProject, ImmutableBitSet.of(1)))\n-    assertEquals(16.22, mq.getPopulationSize(logicalProject, ImmutableBitSet.of(2)), 1e-2)\n-    assertEquals(6.98, mq.getPopulationSize(logicalProject, ImmutableBitSet.of(3)), 1e-2)\n-    assertEquals(20.09, mq.getPopulationSize(logicalProject, ImmutableBitSet.of(4)), 1e-2)\n-    assertEquals(20.09, mq.getPopulationSize(logicalProject, ImmutableBitSet.of(5)), 1e-2)\n+    assertEquals(16.43531528030365,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "08fada0fcc5270803b39a711b20f519c3e6c7fe4"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzczMTY2NA==", "bodyText": "unnecessary change", "url": "https://github.com/apache/flink/pull/13577#discussion_r507731664", "createdAt": "2020-10-19T13:09:35Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/util/collections/ByteHashSet.java", "diffHunk": "@@ -26,7 +26,7 @@\n \n \tprotected boolean[] used;\n \n-\tpublic ByteHashSet() {\n+\tpublic ByteHashSet(final int expected) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "08fada0fcc5270803b39a711b20f519c3e6c7fe4"}, "originalPosition": 5}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ba0ed645a060dc91363fb4334dadff0edcfb0e40", "author": {"user": {"login": "danny0405", "name": "Danny Chan"}}, "url": "https://github.com/apache/flink/commit/ba0ed645a060dc91363fb4334dadff0edcfb0e40", "committedDate": "2020-10-20T02:10:08Z", "message": "Fix the review comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1123f2ba056792dc7dd24feb4d49c6c235f47389", "author": {"user": {"login": "danny0405", "name": "Danny Chan"}}, "url": "https://github.com/apache/flink/commit/1123f2ba056792dc7dd24feb4d49c6c235f47389", "committedDate": "2020-10-20T03:29:54Z", "message": "Fix review comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTEyNDU1NTQw", "url": "https://github.com/apache/flink/pull/13577#pullrequestreview-512455540", "createdAt": "2020-10-20T08:27:16Z", "commit": {"oid": "1123f2ba056792dc7dd24feb4d49c6c235f47389"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3128, "cost": 1, "resetAt": "2021-10-28T16:48:13Z"}}}