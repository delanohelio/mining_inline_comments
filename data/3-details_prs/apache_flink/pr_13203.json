{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDcwNjM3NzUz", "number": 13203, "title": "[FLINK-18984][python][docs] Add tutorial documentation for Python DataStream API", "bodyText": "What is the purpose of the change\nThis pull request adds tutorial documentation for Python DataStream API.\nBrief change log\n\nAdds tutorial documentation for Python DataStream API.\nAdd pointer of Python DataStream API in Try Flink.\n\nVerifying this change\nThis change is a trivial rework / code cleanup without any test coverage.\nDoes this pull request potentially affect one of the following parts:\n\nDependencies (does it add or upgrade a dependency): (no)\nThe public API, i.e., is any changed class annotated with @Public(Evolving): (no)\nThe serializers: (no)\nThe runtime per-record code paths (performance sensitive): (no)\nAnything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (no)\nThe S3 file system connector: (no)\n\nDocumentation\n\nDoes this pull request introduce a new feature? (no)", "createdAt": "2020-08-20T03:41:59Z", "url": "https://github.com/apache/flink/pull/13203", "merged": true, "mergeCommit": {"oid": "5b18a7496d210eb2174bd0c7e02c67c4acf67ca9"}, "closed": true, "closedAt": "2020-09-04T11:26:50Z", "author": {"login": "hequn8128"}, "timelineItems": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdCcJtjAFqTQ3NDQ5ODA3Ng==", "endCursor": "Y3Vyc29yOnYyOpPPAAABdFjcF0gBqjM3MjkyODg5MDU=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc0NDk4MDc2", "url": "https://github.com/apache/flink/pull/13203#pullrequestreview-474498076", "createdAt": "2020-08-25T13:47:26Z", "commit": {"oid": "7170e6d063f5c8e7ddf4104d4beca360be3d8136"}, "state": "COMMENTED", "comments": {"totalCount": 13, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNVQxMzo0NzoyN1rOHGY6QA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNVQxOTowODoxMFrOHGl9xA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjQ2MTYzMg==", "bodyText": "Some context on what is the DataStream API + the same for the Table API tutorial would be helpful. There is a nice snippet from the \"Fraud Detection with the DataStream API\" tutorial that could be used here, like:\n\"Apache Flink offers a DataStream API for building robust, stateful streaming applications. It provides fine-grained control over state and time, which allows for the implementation of advanced event-driven systems. In this step-by-step guide, you\u2019ll learn how to build a stateful streaming application with PyFlink and the DataStream API.\"\n(In the same way, the Table API tutorial can use the introduction from \"Real Time Reporting with the Table API\".)", "url": "https://github.com/apache/flink/pull/13203#discussion_r476461632", "createdAt": "2020-08-25T13:47:27Z", "author": {"login": "morsapaes"}, "path": "docs/dev/python/getting-started/tutorial/datastream_tutorial.md", "diffHunk": "@@ -0,0 +1,126 @@\n+---\n+title: \"Python DataStream API Tutorial\"\n+nav-parent_id: python_tutorial\n+nav-pos: 30\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+This walkthrough will quickly get you started building a pure Python Flink DataStream project.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7170e6d063f5c8e7ddf4104d4beca360be3d8136"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjQ3NjczNw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            DataStream API applications begin by declaring a `StreamExecutionEnvironment`.\n          \n          \n            \n            This is the context in which a streaming program is executed.\n          \n          \n            \n            It can be used for setting execution parameters such as restart strategy, default parallelism, etc.\n          \n          \n            \n            DataStream API applications begin by declaring an execution environment (`StreamExecutionEnvironment`), the context in which a streaming program is executed. This is what you will use to set the properties of your job (e.g. default parallelism, restart strategy), create your sources and finally trigger the execution of the job.", "url": "https://github.com/apache/flink/pull/13203#discussion_r476476737", "createdAt": "2020-08-25T14:07:22Z", "author": {"login": "morsapaes"}, "path": "docs/dev/python/getting-started/tutorial/datastream_tutorial.md", "diffHunk": "@@ -0,0 +1,126 @@\n+---\n+title: \"Python DataStream API Tutorial\"\n+nav-parent_id: python_tutorial\n+nav-pos: 30\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+This walkthrough will quickly get you started building a pure Python Flink DataStream project.\n+\n+Please refer to the PyFlink [installation guide]({{ site.baseurl }}/dev/python/getting-started/installation.html) on how to set up the Python execution environments.\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+## Setting up a Python Project\n+\n+You can begin by creating a Python project and installing the PyFlink package following the [installation guide]({{ site.baseurl }}/dev/python/getting-started/installation.html#installation-of-pyflink).\n+\n+## Writing a Flink Python DataStream API Program\n+\n+DataStream API applications begin by declaring a `StreamExecutionEnvironment`.\n+This is the context in which a streaming program is executed.\n+It can be used for setting execution parameters such as restart strategy, default parallelism, etc.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7170e6d063f5c8e7ddf4104d4beca360be3d8136"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjQ4NDM4MA==", "bodyText": "Installation with pip is pretty straightforward, so why not just add this to the tutorial instead of making the user go to a different page?\nIf we are restructuring these anyways, I'd suggest to follow the same structure as the existing tutorials: https://ci.apache.org/projects/flink/flink-docs-release-1.11/try-flink/datastream_api.html", "url": "https://github.com/apache/flink/pull/13203#discussion_r476484380", "createdAt": "2020-08-25T14:17:06Z", "author": {"login": "morsapaes"}, "path": "docs/dev/python/getting-started/tutorial/datastream_tutorial.md", "diffHunk": "@@ -0,0 +1,126 @@\n+---\n+title: \"Python DataStream API Tutorial\"\n+nav-parent_id: python_tutorial\n+nav-pos: 30\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+This walkthrough will quickly get you started building a pure Python Flink DataStream project.\n+\n+Please refer to the PyFlink [installation guide]({{ site.baseurl }}/dev/python/getting-started/installation.html) on how to set up the Python execution environments.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7170e6d063f5c8e7ddf4104d4beca360be3d8136"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjUxMDQxNg==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Once a `StreamExecutionEnvironment` created, you can declare your source with it.\n          \n          \n            \n            Once a `StreamExecutionEnvironment` is created, you can use it to declare your _source_. Sources ingest data from external systems, such as Apache Kafka, Rabbit MQ, or Apache Pulsar, into Flink Jobs. \n          \n          \n            \n            \n          \n          \n            \n            To keep things simple, this walkthrough uses a source that is backed by a collection of elements.", "url": "https://github.com/apache/flink/pull/13203#discussion_r476510416", "createdAt": "2020-08-25T14:50:50Z", "author": {"login": "morsapaes"}, "path": "docs/dev/python/getting-started/tutorial/datastream_tutorial.md", "diffHunk": "@@ -0,0 +1,126 @@\n+---\n+title: \"Python DataStream API Tutorial\"\n+nav-parent_id: python_tutorial\n+nav-pos: 30\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+This walkthrough will quickly get you started building a pure Python Flink DataStream project.\n+\n+Please refer to the PyFlink [installation guide]({{ site.baseurl }}/dev/python/getting-started/installation.html) on how to set up the Python execution environments.\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+## Setting up a Python Project\n+\n+You can begin by creating a Python project and installing the PyFlink package following the [installation guide]({{ site.baseurl }}/dev/python/getting-started/installation.html#installation-of-pyflink).\n+\n+## Writing a Flink Python DataStream API Program\n+\n+DataStream API applications begin by declaring a `StreamExecutionEnvironment`.\n+This is the context in which a streaming program is executed.\n+It can be used for setting execution parameters such as restart strategy, default parallelism, etc.\n+\n+{% highlight python %}\n+env = StreamExecutionEnvironment.get_execution_environment()\n+env.set_parallelism(1)\n+{% endhighlight %}\n+\n+Once a `StreamExecutionEnvironment` created, you can declare your source with it.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7170e6d063f5c8e7ddf4104d4beca360be3d8136"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjU5MDA1MQ==", "bodyText": "I get that this is reusing existing sample code, but it'd be nice to evolve the example to use a more relevant use case in the future.\n(This is actually a reminder to myself, as I get my hands in PyFlink. \ud83d\ude43 )", "url": "https://github.com/apache/flink/pull/13203#discussion_r476590051", "createdAt": "2020-08-25T16:45:07Z", "author": {"login": "morsapaes"}, "path": "docs/dev/python/getting-started/tutorial/datastream_tutorial.md", "diffHunk": "@@ -0,0 +1,126 @@\n+---\n+title: \"Python DataStream API Tutorial\"\n+nav-parent_id: python_tutorial\n+nav-pos: 30\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+This walkthrough will quickly get you started building a pure Python Flink DataStream project.\n+\n+Please refer to the PyFlink [installation guide]({{ site.baseurl }}/dev/python/getting-started/installation.html) on how to set up the Python execution environments.\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+## Setting up a Python Project\n+\n+You can begin by creating a Python project and installing the PyFlink package following the [installation guide]({{ site.baseurl }}/dev/python/getting-started/installation.html#installation-of-pyflink).\n+\n+## Writing a Flink Python DataStream API Program\n+\n+DataStream API applications begin by declaring a `StreamExecutionEnvironment`.\n+This is the context in which a streaming program is executed.\n+It can be used for setting execution parameters such as restart strategy, default parallelism, etc.\n+\n+{% highlight python %}\n+env = StreamExecutionEnvironment.get_execution_environment()\n+env.set_parallelism(1)\n+{% endhighlight %}\n+\n+Once a `StreamExecutionEnvironment` created, you can declare your source with it.\n+\n+{% highlight python %}\n+ds = env.from_collection(\n+    collection=[(1, 'aaa'), (2, 'bbb')],", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7170e6d063f5c8e7ddf4104d4beca360be3d8136"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY0ODczNQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            This creates a data stream from the given collection. The type is that of the elements in the collection. In this example, the type is a Row type with two fields. The type of the first field is integer type while the second is string type.\n          \n          \n            \n            This creates a data stream from the given collection, with the same type as that of the elements in it (here, a `ROW` type with a INT field and a STRING field).", "url": "https://github.com/apache/flink/pull/13203#discussion_r476648735", "createdAt": "2020-08-25T18:20:25Z", "author": {"login": "morsapaes"}, "path": "docs/dev/python/getting-started/tutorial/datastream_tutorial.md", "diffHunk": "@@ -0,0 +1,126 @@\n+---\n+title: \"Python DataStream API Tutorial\"\n+nav-parent_id: python_tutorial\n+nav-pos: 30\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+This walkthrough will quickly get you started building a pure Python Flink DataStream project.\n+\n+Please refer to the PyFlink [installation guide]({{ site.baseurl }}/dev/python/getting-started/installation.html) on how to set up the Python execution environments.\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+## Setting up a Python Project\n+\n+You can begin by creating a Python project and installing the PyFlink package following the [installation guide]({{ site.baseurl }}/dev/python/getting-started/installation.html#installation-of-pyflink).\n+\n+## Writing a Flink Python DataStream API Program\n+\n+DataStream API applications begin by declaring a `StreamExecutionEnvironment`.\n+This is the context in which a streaming program is executed.\n+It can be used for setting execution parameters such as restart strategy, default parallelism, etc.\n+\n+{% highlight python %}\n+env = StreamExecutionEnvironment.get_execution_environment()\n+env.set_parallelism(1)\n+{% endhighlight %}\n+\n+Once a `StreamExecutionEnvironment` created, you can declare your source with it.\n+\n+{% highlight python %}\n+ds = env.from_collection(\n+    collection=[(1, 'aaa'), (2, 'bbb')],\n+    type_info=Types.ROW([Types.INT(), Types.STRING()]))\n+{% endhighlight %}\n+\n+This creates a data stream from the given collection. The type is that of the elements in the collection. In this example, the type is a Row type with two fields. The type of the first field is integer type while the second is string type.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7170e6d063f5c8e7ddf4104d4beca360be3d8136"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY1MDM2MQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            You can now perform transformations on the datastream or writes the data into external system with sink.\n          \n          \n            \n            You can now perform transformations on this data stream, or just write the data to an external system using a _sink_. This walkthrough uses the `StreamingFileSink` sink connector to write the data into a file in the `/tmp/output` directory.", "url": "https://github.com/apache/flink/pull/13203#discussion_r476650361", "createdAt": "2020-08-25T18:23:18Z", "author": {"login": "morsapaes"}, "path": "docs/dev/python/getting-started/tutorial/datastream_tutorial.md", "diffHunk": "@@ -0,0 +1,126 @@\n+---\n+title: \"Python DataStream API Tutorial\"\n+nav-parent_id: python_tutorial\n+nav-pos: 30\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+This walkthrough will quickly get you started building a pure Python Flink DataStream project.\n+\n+Please refer to the PyFlink [installation guide]({{ site.baseurl }}/dev/python/getting-started/installation.html) on how to set up the Python execution environments.\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+## Setting up a Python Project\n+\n+You can begin by creating a Python project and installing the PyFlink package following the [installation guide]({{ site.baseurl }}/dev/python/getting-started/installation.html#installation-of-pyflink).\n+\n+## Writing a Flink Python DataStream API Program\n+\n+DataStream API applications begin by declaring a `StreamExecutionEnvironment`.\n+This is the context in which a streaming program is executed.\n+It can be used for setting execution parameters such as restart strategy, default parallelism, etc.\n+\n+{% highlight python %}\n+env = StreamExecutionEnvironment.get_execution_environment()\n+env.set_parallelism(1)\n+{% endhighlight %}\n+\n+Once a `StreamExecutionEnvironment` created, you can declare your source with it.\n+\n+{% highlight python %}\n+ds = env.from_collection(\n+    collection=[(1, 'aaa'), (2, 'bbb')],\n+    type_info=Types.ROW([Types.INT(), Types.STRING()]))\n+{% endhighlight %}\n+\n+This creates a data stream from the given collection. The type is that of the elements in the collection. In this example, the type is a Row type with two fields. The type of the first field is integer type while the second is string type.\n+\n+You can now perform transformations on the datastream or writes the data into external system with sink.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7170e6d063f5c8e7ddf4104d4beca360be3d8136"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY2OTM2MQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Finally you must execute the actual Flink Python DataStream API job.\n          \n          \n            \n            The last step is to execute the actual PyFlink DataStream API job. PyFlink applications are built lazily and shipped to the cluster for execution only once fully formed. To execute an application, you simply call `env.execute(job_name)`.", "url": "https://github.com/apache/flink/pull/13203#discussion_r476669361", "createdAt": "2020-08-25T18:57:03Z", "author": {"login": "morsapaes"}, "path": "docs/dev/python/getting-started/tutorial/datastream_tutorial.md", "diffHunk": "@@ -0,0 +1,126 @@\n+---\n+title: \"Python DataStream API Tutorial\"\n+nav-parent_id: python_tutorial\n+nav-pos: 30\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+This walkthrough will quickly get you started building a pure Python Flink DataStream project.\n+\n+Please refer to the PyFlink [installation guide]({{ site.baseurl }}/dev/python/getting-started/installation.html) on how to set up the Python execution environments.\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+## Setting up a Python Project\n+\n+You can begin by creating a Python project and installing the PyFlink package following the [installation guide]({{ site.baseurl }}/dev/python/getting-started/installation.html#installation-of-pyflink).\n+\n+## Writing a Flink Python DataStream API Program\n+\n+DataStream API applications begin by declaring a `StreamExecutionEnvironment`.\n+This is the context in which a streaming program is executed.\n+It can be used for setting execution parameters such as restart strategy, default parallelism, etc.\n+\n+{% highlight python %}\n+env = StreamExecutionEnvironment.get_execution_environment()\n+env.set_parallelism(1)\n+{% endhighlight %}\n+\n+Once a `StreamExecutionEnvironment` created, you can declare your source with it.\n+\n+{% highlight python %}\n+ds = env.from_collection(\n+    collection=[(1, 'aaa'), (2, 'bbb')],\n+    type_info=Types.ROW([Types.INT(), Types.STRING()]))\n+{% endhighlight %}\n+\n+This creates a data stream from the given collection. The type is that of the elements in the collection. In this example, the type is a Row type with two fields. The type of the first field is integer type while the second is string type.\n+\n+You can now perform transformations on the datastream or writes the data into external system with sink.\n+\n+{% highlight python %}\n+ds.add_sink(StreamingFileSink\n+    .for_row_format('/tmp/output', SimpleStringEncoder())\n+    .build())\n+{% endhighlight %}\n+\n+Finally you must execute the actual Flink Python DataStream API job.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7170e6d063f5c8e7ddf4104d4beca360be3d8136"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY3MDUwMw==", "bodyText": "Is there a reason to use \"Flink Python\" instead of PyFlink (the question applies to the whole walkthrough)?", "url": "https://github.com/apache/flink/pull/13203#discussion_r476670503", "createdAt": "2020-08-25T18:59:00Z", "author": {"login": "morsapaes"}, "path": "docs/dev/python/getting-started/tutorial/datastream_tutorial.md", "diffHunk": "@@ -0,0 +1,126 @@\n+---\n+title: \"Python DataStream API Tutorial\"\n+nav-parent_id: python_tutorial\n+nav-pos: 30\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+This walkthrough will quickly get you started building a pure Python Flink DataStream project.\n+\n+Please refer to the PyFlink [installation guide]({{ site.baseurl }}/dev/python/getting-started/installation.html) on how to set up the Python execution environments.\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+## Setting up a Python Project\n+\n+You can begin by creating a Python project and installing the PyFlink package following the [installation guide]({{ site.baseurl }}/dev/python/getting-started/installation.html#installation-of-pyflink).\n+\n+## Writing a Flink Python DataStream API Program\n+\n+DataStream API applications begin by declaring a `StreamExecutionEnvironment`.\n+This is the context in which a streaming program is executed.\n+It can be used for setting execution parameters such as restart strategy, default parallelism, etc.\n+\n+{% highlight python %}\n+env = StreamExecutionEnvironment.get_execution_environment()\n+env.set_parallelism(1)\n+{% endhighlight %}\n+\n+Once a `StreamExecutionEnvironment` created, you can declare your source with it.\n+\n+{% highlight python %}\n+ds = env.from_collection(\n+    collection=[(1, 'aaa'), (2, 'bbb')],\n+    type_info=Types.ROW([Types.INT(), Types.STRING()]))\n+{% endhighlight %}\n+\n+This creates a data stream from the given collection. The type is that of the elements in the collection. In this example, the type is a Row type with two fields. The type of the first field is integer type while the second is string type.\n+\n+You can now perform transformations on the datastream or writes the data into external system with sink.\n+\n+{% highlight python %}\n+ds.add_sink(StreamingFileSink\n+    .for_row_format('/tmp/output', SimpleStringEncoder())\n+    .build())\n+{% endhighlight %}\n+\n+Finally you must execute the actual Flink Python DataStream API job.\n+All operations, such as creating sources, transformations and sinks are lazy.\n+Only when `env.execute(job_name)` is called will runs the job.\n+\n+{% highlight python %}\n+env.execute(\"tutorial_job\")\n+{% endhighlight %}\n+\n+The complete code so far:\n+\n+{% highlight python %}\n+from pyflink.common.serialization import SimpleStringEncoder\n+from pyflink.common.typeinfo import Types\n+from pyflink.datastream import StreamExecutionEnvironment\n+from pyflink.datastream.connectors import StreamingFileSink\n+\n+\n+def tutorial():\n+    env = StreamExecutionEnvironment.get_execution_environment()\n+    env.set_parallelism(1)\n+    ds = env.from_collection(\n+        collection=[(1, 'aaa'), (2, 'bbb')],\n+        type_info=Types.ROW([Types.INT(), Types.STRING()]))\n+    ds.add_sink(StreamingFileSink\n+                .for_row_format('/tmp/output', SimpleStringEncoder())\n+                .build())\n+    env.execute(\"tutorial_job\")\n+\n+\n+if __name__ == '__main__':\n+    tutorial()\n+{% endhighlight %}\n+\n+## Executing a Flink Python DataStream API Program", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7170e6d063f5c8e7ddf4104d4beca360be3d8136"}, "originalPosition": 98}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY3MTcwMw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Firstly, make sure the output directory is not existed:\n          \n          \n            \n            Now that you defined your PyFlink program, you can run it! First, make sure that the output directory doesn't exist:", "url": "https://github.com/apache/flink/pull/13203#discussion_r476671703", "createdAt": "2020-08-25T19:01:06Z", "author": {"login": "morsapaes"}, "path": "docs/dev/python/getting-started/tutorial/datastream_tutorial.md", "diffHunk": "@@ -0,0 +1,126 @@\n+---\n+title: \"Python DataStream API Tutorial\"\n+nav-parent_id: python_tutorial\n+nav-pos: 30\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+This walkthrough will quickly get you started building a pure Python Flink DataStream project.\n+\n+Please refer to the PyFlink [installation guide]({{ site.baseurl }}/dev/python/getting-started/installation.html) on how to set up the Python execution environments.\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+## Setting up a Python Project\n+\n+You can begin by creating a Python project and installing the PyFlink package following the [installation guide]({{ site.baseurl }}/dev/python/getting-started/installation.html#installation-of-pyflink).\n+\n+## Writing a Flink Python DataStream API Program\n+\n+DataStream API applications begin by declaring a `StreamExecutionEnvironment`.\n+This is the context in which a streaming program is executed.\n+It can be used for setting execution parameters such as restart strategy, default parallelism, etc.\n+\n+{% highlight python %}\n+env = StreamExecutionEnvironment.get_execution_environment()\n+env.set_parallelism(1)\n+{% endhighlight %}\n+\n+Once a `StreamExecutionEnvironment` created, you can declare your source with it.\n+\n+{% highlight python %}\n+ds = env.from_collection(\n+    collection=[(1, 'aaa'), (2, 'bbb')],\n+    type_info=Types.ROW([Types.INT(), Types.STRING()]))\n+{% endhighlight %}\n+\n+This creates a data stream from the given collection. The type is that of the elements in the collection. In this example, the type is a Row type with two fields. The type of the first field is integer type while the second is string type.\n+\n+You can now perform transformations on the datastream or writes the data into external system with sink.\n+\n+{% highlight python %}\n+ds.add_sink(StreamingFileSink\n+    .for_row_format('/tmp/output', SimpleStringEncoder())\n+    .build())\n+{% endhighlight %}\n+\n+Finally you must execute the actual Flink Python DataStream API job.\n+All operations, such as creating sources, transformations and sinks are lazy.\n+Only when `env.execute(job_name)` is called will runs the job.\n+\n+{% highlight python %}\n+env.execute(\"tutorial_job\")\n+{% endhighlight %}\n+\n+The complete code so far:\n+\n+{% highlight python %}\n+from pyflink.common.serialization import SimpleStringEncoder\n+from pyflink.common.typeinfo import Types\n+from pyflink.datastream import StreamExecutionEnvironment\n+from pyflink.datastream.connectors import StreamingFileSink\n+\n+\n+def tutorial():\n+    env = StreamExecutionEnvironment.get_execution_environment()\n+    env.set_parallelism(1)\n+    ds = env.from_collection(\n+        collection=[(1, 'aaa'), (2, 'bbb')],\n+        type_info=Types.ROW([Types.INT(), Types.STRING()]))\n+    ds.add_sink(StreamingFileSink\n+                .for_row_format('/tmp/output', SimpleStringEncoder())\n+                .build())\n+    env.execute(\"tutorial_job\")\n+\n+\n+if __name__ == '__main__':\n+    tutorial()\n+{% endhighlight %}\n+\n+## Executing a Flink Python DataStream API Program\n+Firstly, make sure the output directory is not existed:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7170e6d063f5c8e7ddf4104d4beca360be3d8136"}, "originalPosition": 99}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY3MTk5NQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Next, you can run this example on the command line:\n          \n          \n            \n            Next, you can run the example you just created on the command line:", "url": "https://github.com/apache/flink/pull/13203#discussion_r476671995", "createdAt": "2020-08-25T19:01:38Z", "author": {"login": "morsapaes"}, "path": "docs/dev/python/getting-started/tutorial/datastream_tutorial.md", "diffHunk": "@@ -0,0 +1,126 @@\n+---\n+title: \"Python DataStream API Tutorial\"\n+nav-parent_id: python_tutorial\n+nav-pos: 30\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+This walkthrough will quickly get you started building a pure Python Flink DataStream project.\n+\n+Please refer to the PyFlink [installation guide]({{ site.baseurl }}/dev/python/getting-started/installation.html) on how to set up the Python execution environments.\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+## Setting up a Python Project\n+\n+You can begin by creating a Python project and installing the PyFlink package following the [installation guide]({{ site.baseurl }}/dev/python/getting-started/installation.html#installation-of-pyflink).\n+\n+## Writing a Flink Python DataStream API Program\n+\n+DataStream API applications begin by declaring a `StreamExecutionEnvironment`.\n+This is the context in which a streaming program is executed.\n+It can be used for setting execution parameters such as restart strategy, default parallelism, etc.\n+\n+{% highlight python %}\n+env = StreamExecutionEnvironment.get_execution_environment()\n+env.set_parallelism(1)\n+{% endhighlight %}\n+\n+Once a `StreamExecutionEnvironment` created, you can declare your source with it.\n+\n+{% highlight python %}\n+ds = env.from_collection(\n+    collection=[(1, 'aaa'), (2, 'bbb')],\n+    type_info=Types.ROW([Types.INT(), Types.STRING()]))\n+{% endhighlight %}\n+\n+This creates a data stream from the given collection. The type is that of the elements in the collection. In this example, the type is a Row type with two fields. The type of the first field is integer type while the second is string type.\n+\n+You can now perform transformations on the datastream or writes the data into external system with sink.\n+\n+{% highlight python %}\n+ds.add_sink(StreamingFileSink\n+    .for_row_format('/tmp/output', SimpleStringEncoder())\n+    .build())\n+{% endhighlight %}\n+\n+Finally you must execute the actual Flink Python DataStream API job.\n+All operations, such as creating sources, transformations and sinks are lazy.\n+Only when `env.execute(job_name)` is called will runs the job.\n+\n+{% highlight python %}\n+env.execute(\"tutorial_job\")\n+{% endhighlight %}\n+\n+The complete code so far:\n+\n+{% highlight python %}\n+from pyflink.common.serialization import SimpleStringEncoder\n+from pyflink.common.typeinfo import Types\n+from pyflink.datastream import StreamExecutionEnvironment\n+from pyflink.datastream.connectors import StreamingFileSink\n+\n+\n+def tutorial():\n+    env = StreamExecutionEnvironment.get_execution_environment()\n+    env.set_parallelism(1)\n+    ds = env.from_collection(\n+        collection=[(1, 'aaa'), (2, 'bbb')],\n+        type_info=Types.ROW([Types.INT(), Types.STRING()]))\n+    ds.add_sink(StreamingFileSink\n+                .for_row_format('/tmp/output', SimpleStringEncoder())\n+                .build())\n+    env.execute(\"tutorial_job\")\n+\n+\n+if __name__ == '__main__':\n+    tutorial()\n+{% endhighlight %}\n+\n+## Executing a Flink Python DataStream API Program\n+Firstly, make sure the output directory is not existed:\n+\n+{% highlight bash %}\n+rm -rf /tmp/output\n+{% endhighlight %}\n+\n+Next, you can run this example on the command line:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7170e6d063f5c8e7ddf4104d4beca360be3d8136"}, "originalPosition": 105}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY3NDM0OQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The command builds and runs the Python DataStream API program in a local mini cluster.\n          \n          \n            \n            You can also submit the Python DataStream API program to a remote cluster, you can refer\n          \n          \n            \n            [Job Submission Examples]({{ site.baseurl }}/ops/cli.html#job-submission-examples)\n          \n          \n            \n            for more details.\n          \n          \n            \n            The command builds and runs your PyFlink program in a local mini cluster. You can alternatively submit it to a remote cluster using the instructions detailed in [Job Submission Examples]({{ site.baseurl }}/ops/cli.html#job-submission-examples).", "url": "https://github.com/apache/flink/pull/13203#discussion_r476674349", "createdAt": "2020-08-25T19:05:52Z", "author": {"login": "morsapaes"}, "path": "docs/dev/python/getting-started/tutorial/datastream_tutorial.md", "diffHunk": "@@ -0,0 +1,126 @@\n+---\n+title: \"Python DataStream API Tutorial\"\n+nav-parent_id: python_tutorial\n+nav-pos: 30\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+This walkthrough will quickly get you started building a pure Python Flink DataStream project.\n+\n+Please refer to the PyFlink [installation guide]({{ site.baseurl }}/dev/python/getting-started/installation.html) on how to set up the Python execution environments.\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+## Setting up a Python Project\n+\n+You can begin by creating a Python project and installing the PyFlink package following the [installation guide]({{ site.baseurl }}/dev/python/getting-started/installation.html#installation-of-pyflink).\n+\n+## Writing a Flink Python DataStream API Program\n+\n+DataStream API applications begin by declaring a `StreamExecutionEnvironment`.\n+This is the context in which a streaming program is executed.\n+It can be used for setting execution parameters such as restart strategy, default parallelism, etc.\n+\n+{% highlight python %}\n+env = StreamExecutionEnvironment.get_execution_environment()\n+env.set_parallelism(1)\n+{% endhighlight %}\n+\n+Once a `StreamExecutionEnvironment` created, you can declare your source with it.\n+\n+{% highlight python %}\n+ds = env.from_collection(\n+    collection=[(1, 'aaa'), (2, 'bbb')],\n+    type_info=Types.ROW([Types.INT(), Types.STRING()]))\n+{% endhighlight %}\n+\n+This creates a data stream from the given collection. The type is that of the elements in the collection. In this example, the type is a Row type with two fields. The type of the first field is integer type while the second is string type.\n+\n+You can now perform transformations on the datastream or writes the data into external system with sink.\n+\n+{% highlight python %}\n+ds.add_sink(StreamingFileSink\n+    .for_row_format('/tmp/output', SimpleStringEncoder())\n+    .build())\n+{% endhighlight %}\n+\n+Finally you must execute the actual Flink Python DataStream API job.\n+All operations, such as creating sources, transformations and sinks are lazy.\n+Only when `env.execute(job_name)` is called will runs the job.\n+\n+{% highlight python %}\n+env.execute(\"tutorial_job\")\n+{% endhighlight %}\n+\n+The complete code so far:\n+\n+{% highlight python %}\n+from pyflink.common.serialization import SimpleStringEncoder\n+from pyflink.common.typeinfo import Types\n+from pyflink.datastream import StreamExecutionEnvironment\n+from pyflink.datastream.connectors import StreamingFileSink\n+\n+\n+def tutorial():\n+    env = StreamExecutionEnvironment.get_execution_environment()\n+    env.set_parallelism(1)\n+    ds = env.from_collection(\n+        collection=[(1, 'aaa'), (2, 'bbb')],\n+        type_info=Types.ROW([Types.INT(), Types.STRING()]))\n+    ds.add_sink(StreamingFileSink\n+                .for_row_format('/tmp/output', SimpleStringEncoder())\n+                .build())\n+    env.execute(\"tutorial_job\")\n+\n+\n+if __name__ == '__main__':\n+    tutorial()\n+{% endhighlight %}\n+\n+## Executing a Flink Python DataStream API Program\n+Firstly, make sure the output directory is not existed:\n+\n+{% highlight bash %}\n+rm -rf /tmp/output\n+{% endhighlight %}\n+\n+Next, you can run this example on the command line:\n+\n+{% highlight bash %}\n+$ python datastream_tutorial.py\n+{% endhighlight %}\n+\n+The command builds and runs the Python DataStream API program in a local mini cluster.\n+You can also submit the Python DataStream API program to a remote cluster, you can refer\n+[Job Submission Examples]({{ site.baseurl }}/ops/cli.html#job-submission-examples)\n+for more details.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7170e6d063f5c8e7ddf4104d4beca360be3d8136"}, "originalPosition": 114}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY3NTUyNA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            This should get you started with writing your own Flink Python DataStream API programs.\n          \n          \n            \n            To learn more about the Python DataStream API, you can refer\n          \n          \n            \n            [Flink Python API Docs]({{ site.pythondocs_baseurl }}/api/python) for more details.\n          \n          \n            \n            This walkthrough gives you the foundations to get started writing your own PyFlink DataStream API programs. To learn more about the Python DataStream API, please refer to [Flink Python API Docs]({{ site.pythondocs_baseurl }}/api/python) for more details.", "url": "https://github.com/apache/flink/pull/13203#discussion_r476675524", "createdAt": "2020-08-25T19:08:10Z", "author": {"login": "morsapaes"}, "path": "docs/dev/python/getting-started/tutorial/datastream_tutorial.md", "diffHunk": "@@ -0,0 +1,126 @@\n+---\n+title: \"Python DataStream API Tutorial\"\n+nav-parent_id: python_tutorial\n+nav-pos: 30\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+This walkthrough will quickly get you started building a pure Python Flink DataStream project.\n+\n+Please refer to the PyFlink [installation guide]({{ site.baseurl }}/dev/python/getting-started/installation.html) on how to set up the Python execution environments.\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+## Setting up a Python Project\n+\n+You can begin by creating a Python project and installing the PyFlink package following the [installation guide]({{ site.baseurl }}/dev/python/getting-started/installation.html#installation-of-pyflink).\n+\n+## Writing a Flink Python DataStream API Program\n+\n+DataStream API applications begin by declaring a `StreamExecutionEnvironment`.\n+This is the context in which a streaming program is executed.\n+It can be used for setting execution parameters such as restart strategy, default parallelism, etc.\n+\n+{% highlight python %}\n+env = StreamExecutionEnvironment.get_execution_environment()\n+env.set_parallelism(1)\n+{% endhighlight %}\n+\n+Once a `StreamExecutionEnvironment` created, you can declare your source with it.\n+\n+{% highlight python %}\n+ds = env.from_collection(\n+    collection=[(1, 'aaa'), (2, 'bbb')],\n+    type_info=Types.ROW([Types.INT(), Types.STRING()]))\n+{% endhighlight %}\n+\n+This creates a data stream from the given collection. The type is that of the elements in the collection. In this example, the type is a Row type with two fields. The type of the first field is integer type while the second is string type.\n+\n+You can now perform transformations on the datastream or writes the data into external system with sink.\n+\n+{% highlight python %}\n+ds.add_sink(StreamingFileSink\n+    .for_row_format('/tmp/output', SimpleStringEncoder())\n+    .build())\n+{% endhighlight %}\n+\n+Finally you must execute the actual Flink Python DataStream API job.\n+All operations, such as creating sources, transformations and sinks are lazy.\n+Only when `env.execute(job_name)` is called will runs the job.\n+\n+{% highlight python %}\n+env.execute(\"tutorial_job\")\n+{% endhighlight %}\n+\n+The complete code so far:\n+\n+{% highlight python %}\n+from pyflink.common.serialization import SimpleStringEncoder\n+from pyflink.common.typeinfo import Types\n+from pyflink.datastream import StreamExecutionEnvironment\n+from pyflink.datastream.connectors import StreamingFileSink\n+\n+\n+def tutorial():\n+    env = StreamExecutionEnvironment.get_execution_environment()\n+    env.set_parallelism(1)\n+    ds = env.from_collection(\n+        collection=[(1, 'aaa'), (2, 'bbb')],\n+        type_info=Types.ROW([Types.INT(), Types.STRING()]))\n+    ds.add_sink(StreamingFileSink\n+                .for_row_format('/tmp/output', SimpleStringEncoder())\n+                .build())\n+    env.execute(\"tutorial_job\")\n+\n+\n+if __name__ == '__main__':\n+    tutorial()\n+{% endhighlight %}\n+\n+## Executing a Flink Python DataStream API Program\n+Firstly, make sure the output directory is not existed:\n+\n+{% highlight bash %}\n+rm -rf /tmp/output\n+{% endhighlight %}\n+\n+Next, you can run this example on the command line:\n+\n+{% highlight bash %}\n+$ python datastream_tutorial.py\n+{% endhighlight %}\n+\n+The command builds and runs the Python DataStream API program in a local mini cluster.\n+You can also submit the Python DataStream API program to a remote cluster, you can refer\n+[Job Submission Examples]({{ site.baseurl }}/ops/cli.html#job-submission-examples)\n+for more details.\n+\n+Finally, you can see the execution result on the command line:\n+\n+{% highlight bash %}\n+$ find /tmp/output -type f -exec cat {} \\;\n+1,aaa\n+2,bbb\n+{% endhighlight %}\n+\n+This should get you started with writing your own Flink Python DataStream API programs.\n+To learn more about the Python DataStream API, you can refer\n+[Flink Python API Docs]({{ site.pythondocs_baseurl }}/api/python) for more details.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7170e6d063f5c8e7ddf4104d4beca360be3d8136"}, "originalPosition": 126}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0f4bf89990af64e8f36eb93998cab9e7aad514f7", "author": {"user": null}, "url": "https://github.com/apache/flink/commit/0f4bf89990af64e8f36eb93998cab9e7aad514f7", "committedDate": "2020-09-04T10:59:06Z", "message": "[FLINK-18984][python][docs] Add tutorial documentation for Python DataStream API"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b10a7049e17ffcb63d7d7bc8e89f57ac20043c4f", "author": {"user": null}, "url": "https://github.com/apache/flink/commit/b10a7049e17ffcb63d7d7bc8e89f57ac20043c4f", "committedDate": "2020-09-04T11:01:56Z", "message": "address grammar problems"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "09ac422e9f2e3cbf6f94fe756a1b4d1d3dfaa536", "author": {"user": null}, "url": "https://github.com/apache/flink/commit/09ac422e9f2e3cbf6f94fe756a1b4d1d3dfaa536", "committedDate": "2020-09-04T11:01:57Z", "message": "refactor page structure for Python DataStream API tutorial"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4e797f1d9290680c13ce2939cb77326948e955a5", "author": {"user": null}, "url": "https://github.com/apache/flink/commit/4e797f1d9290680c13ce2939cb77326948e955a5", "committedDate": "2020-09-04T11:04:32Z", "message": "refactor page structure for Python Table API tutorial"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9ddca3393464e6d839f91679cad151704d188ad7", "author": {"user": null}, "url": "https://github.com/apache/flink/commit/9ddca3393464e6d839f91679cad151704d188ad7", "committedDate": "2020-09-04T11:23:38Z", "message": "rebase"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "688923779cd4c4a10c3f05d7a5aa030ad4434785", "author": {"user": null}, "url": "https://github.com/apache/flink/commit/688923779cd4c4a10c3f05d7a5aa030ad4434785", "committedDate": "2020-08-27T14:55:42Z", "message": "refactor page structure for Python Table API tutorial"}, "afterCommit": {"oid": "9ddca3393464e6d839f91679cad151704d188ad7", "author": {"user": null}, "url": "https://github.com/apache/flink/commit/9ddca3393464e6d839f91679cad151704d188ad7", "committedDate": "2020-09-04T11:23:38Z", "message": "rebase"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4652, "cost": 1, "resetAt": "2021-10-28T16:48:13Z"}}}