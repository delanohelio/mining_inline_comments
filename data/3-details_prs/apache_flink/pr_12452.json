{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDI2OTI4MDI0", "number": 12452, "title": "[FLINK-18056][fs-connector] Hadoop path-based file writer adds UUID to in-progress file to avoid conflicts", "bodyText": "What is the purpose of the change\nCurrently the Hadoop path-based file writer uses the same in-progress file for writing the same file. If users does not override the remaining in-progress file explicitly after failover, the job will fail due to trying to recreating an existing files. This PR fixes this issue by adding UUID to the in-progress files.\nBrief change log\n-3f7e6da089bea0dd3a2f2fc2bfafb88a13aa482c adds the UUID to the in-progress file name and enhances the tests.\nVerifying this change\n(Please pick either of the following options)\n\nThis change is tested by extending the current rename committer test to also cover the case that the writer does not override the file.\n\nDoes this pull request potentially affect one of the following parts:\n\nDependencies (does it add or upgrade a dependency): no\nThe public API, i.e., is any changed class annotated with @Public(Evolving): no\nThe serializers: no\nThe runtime per-record code paths (performance sensitive): no\nAnything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: no\nThe S3 file system connector: no\n\nDocumentation\n\nDoes this pull request introduce a new feature? no\nIf yes, how is the feature documented? not applicable", "createdAt": "2020-06-03T02:41:00Z", "url": "https://github.com/apache/flink/pull/12452", "merged": true, "mergeCommit": {"oid": "45f42f8e6bf2c2d360a6d4fff4f23d53b4dd4f8a"}, "closed": true, "closedAt": "2020-06-09T05:47:54Z", "author": {"login": "gaoyunhaii"}, "timelineItems": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcoAnTdgBqjM0MDc3MjEyNjc=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcpcIWDABqjM0MjI4MTM2MzE=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "7cb10bd1269179bbe66f5b840c0f0d2494c66a55", "author": {"user": {"login": "gaoyunhaii", "name": "Yun Gao"}}, "url": "https://github.com/apache/flink/commit/7cb10bd1269179bbe66f5b840c0f0d2494c66a55", "committedDate": "2020-06-03T02:32:25Z", "message": "[FLINK-18056][fs-connector] Removing the remaining in-progress file on initialization for hadoop path-based writer"}, "afterCommit": {"oid": "fef42fb8497d9822a12ab18114ecb0ea4f36509f", "author": {"user": {"login": "gaoyunhaii", "name": "Yun Gao"}}, "url": "https://github.com/apache/flink/commit/fef42fb8497d9822a12ab18114ecb0ea4f36509f", "committedDate": "2020-06-04T16:25:33Z", "message": "Fix checkstyle"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "fef42fb8497d9822a12ab18114ecb0ea4f36509f", "author": {"user": {"login": "gaoyunhaii", "name": "Yun Gao"}}, "url": "https://github.com/apache/flink/commit/fef42fb8497d9822a12ab18114ecb0ea4f36509f", "committedDate": "2020-06-04T16:25:33Z", "message": "Fix checkstyle"}, "afterCommit": {"oid": "3f7e6da089bea0dd3a2f2fc2bfafb88a13aa482c", "author": {"user": {"login": "gaoyunhaii", "name": "Yun Gao"}}, "url": "https://github.com/apache/flink/commit/3f7e6da089bea0dd3a2f2fc2bfafb88a13aa482c", "committedDate": "2020-06-05T08:02:22Z", "message": "[FLINK-18056][fs-connector] Hadoop path-based file writer adds UUID to in-progress file to avoid conflicts"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI1MTExMTU3", "url": "https://github.com/apache/flink/pull/12452#pullrequestreview-425111157", "createdAt": "2020-06-05T08:45:56Z", "commit": {"oid": "3f7e6da089bea0dd3a2f2fc2bfafb88a13aa482c"}, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNVQwODo0NTo1N1rOGflzLQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNVQwODo1OToxM1rOGfmQIw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTc3ODM0OQ==", "bodyText": "OFF?", "url": "https://github.com/apache/flink/pull/12452#discussion_r435778349", "createdAt": "2020-06-05T08:45:57Z", "author": {"login": "JingsongLi"}, "path": "flink-formats/flink-hadoop-bulk/src/test/resources/log4j2-test.properties", "diffHunk": "@@ -18,7 +18,7 @@\n \n # Set root logger level to OFF to not flood build logs\n # set manually to INFO for debugging purposes\n-rootLogger.level = OFF", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3f7e6da089bea0dd3a2f2fc2bfafb88a13aa482c"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTc4MjM5OA==", "bodyText": "just 12 + pathBytes.length + inProgressBytes.length", "url": "https://github.com/apache/flink/pull/12452#discussion_r435782398", "createdAt": "2020-06-05T08:53:14Z", "author": {"login": "JingsongLi"}, "path": "flink-formats/flink-hadoop-bulk/src/main/java/org/apache/flink/formats/hadoop/bulk/HadoopPathBasedPartFileWriter.java", "diffHunk": "@@ -142,14 +150,21 @@ public int getVersion() {\n \t\t\t\tthrow new UnsupportedOperationException(\"Only HadoopPathBasedPendingFileRecoverable is supported.\");\n \t\t\t}\n \n-\t\t\tPath path = ((HadoopPathBasedPendingFileRecoverable) pendingFileRecoverable).getPath();\n+\t\t\tHadoopPathBasedPendingFileRecoverable hadoopRecoverable =\n+\t\t\t\t(HadoopPathBasedPendingFileRecoverable) pendingFileRecoverable;\n+\t\t\tPath path = hadoopRecoverable.getPath();\n+\t\t\tPath inProgressPath = hadoopRecoverable.getInProgressPath();\n+\n \t\t\tbyte[] pathBytes = path.toUri().toString().getBytes(CHARSET);\n+\t\t\tbyte[] inProgressBytes = inProgressPath.toUri().toString().getBytes(CHARSET);\n \n-\t\t\tbyte[] targetBytes = new byte[8 + pathBytes.length];\n+\t\t\tbyte[] targetBytes = new byte[8 + pathBytes.length + 4 + inProgressBytes.length];", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3f7e6da089bea0dd3a2f2fc2bfafb88a13aa482c"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTc4MzE2Mg==", "bodyText": "I think you can add some comments to explain in HadoopPathBasedPartFileWriter, what we store in state.", "url": "https://github.com/apache/flink/pull/12452#discussion_r435783162", "createdAt": "2020-06-05T08:54:39Z", "author": {"login": "JingsongLi"}, "path": "flink-formats/flink-hadoop-bulk/src/main/java/org/apache/flink/formats/hadoop/bulk/HadoopPathBasedPartFileWriter.java", "diffHunk": "@@ -142,14 +150,21 @@ public int getVersion() {\n \t\t\t\tthrow new UnsupportedOperationException(\"Only HadoopPathBasedPendingFileRecoverable is supported.\");\n \t\t\t}\n ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3f7e6da089bea0dd3a2f2fc2bfafb88a13aa482c"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTc4MzkxMA==", "bodyText": "Can be consistent with HadoopFsRecoverable, Path targetFile, Path tempFile", "url": "https://github.com/apache/flink/pull/12452#discussion_r435783910", "createdAt": "2020-06-05T08:56:00Z", "author": {"login": "JingsongLi"}, "path": "flink-formats/flink-hadoop-bulk/src/main/java/org/apache/flink/formats/hadoop/bulk/HadoopPathBasedPartFileWriter.java", "diffHunk": "@@ -103,21 +103,29 @@ public void commitAfterRecovery() throws IOException {\n \n \t\tpublic PendingFileRecoverable getRecoverable() {\n \t\t\treturn new HadoopPathBasedPendingFileRecoverable(\n-\t\t\t\tfileCommitter.getTargetFilePath());\n+\t\t\t\tfileCommitter.getTargetFilePath(),\n+\t\t\t\tfileCommitter.getInProgressFilePath());\n \t\t}\n \t}\n \n \t@VisibleForTesting\n \tstatic class HadoopPathBasedPendingFileRecoverable implements PendingFileRecoverable {\n \t\tprivate final Path path;\n \n-\t\tpublic HadoopPathBasedPendingFileRecoverable(Path path) {\n+\t\tprivate final Path inProgressPath;\n+\n+\t\tpublic HadoopPathBasedPendingFileRecoverable(Path path, Path inProgressPath) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3f7e6da089bea0dd3a2f2fc2bfafb88a13aa482c"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTc4NTc2Mw==", "bodyText": "\".inprogress\" -> \".inprogress.\"", "url": "https://github.com/apache/flink/pull/12452#discussion_r435785763", "createdAt": "2020-06-05T08:59:13Z", "author": {"login": "JingsongLi"}, "path": "flink-formats/flink-hadoop-bulk/src/main/java/org/apache/flink/formats/hadoop/bulk/committer/HadoopRenameFileCommitter.java", "diffHunk": "@@ -96,12 +107,19 @@ private void rename(boolean assertFileExists) throws IOException {\n \t\t}\n \t}\n \n-\tprivate Path generateInProgressFilePath() {\n+\tprivate Path generateInProgressFilePath() throws IOException {\n \t\tcheckArgument(targetFilePath.isAbsolute(), \"Target file must be absolute\");\n \n+\t\tFileSystem fileSystem = FileSystem.get(targetFilePath.toUri(), configuration);\n+\n \t\tPath parent = targetFilePath.getParent();\n \t\tString name = targetFilePath.getName();\n \n-\t\treturn new Path(parent, \".\" + name + \".inprogress\");\n+\t\twhile (true) {\n+\t\t\tPath candidate = new Path(parent, \".\" + name + \".inprogress\" + UUID.randomUUID().toString());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3f7e6da089bea0dd3a2f2fc2bfafb88a13aa482c"}, "originalPosition": 47}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI1ODgxMDA0", "url": "https://github.com/apache/flink/pull/12452#pullrequestreview-425881004", "createdAt": "2020-06-08T02:08:51Z", "commit": {"oid": "2623aa665436055f45f765be51c5553bb1b82c43"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "6205078fa55038115732d0b4e224225eb2f96658", "author": {"user": {"login": "gaoyunhaii", "name": "Yun Gao"}}, "url": "https://github.com/apache/flink/commit/6205078fa55038115732d0b4e224225eb2f96658", "committedDate": "2020-06-08T11:33:37Z", "message": "Remove the S3 tests"}, "afterCommit": {"oid": "248476005c601939c42dab33d7f689809090ff35", "author": {"user": {"login": "gaoyunhaii", "name": "Yun Gao"}}, "url": "https://github.com/apache/flink/commit/248476005c601939c42dab33d7f689809090ff35", "committedDate": "2020-06-08T11:39:21Z", "message": "Remove the S3 tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "de8e31736af54c3a8e00a38994b949051aa1b98c", "author": {"user": {"login": "gaoyunhaii", "name": "Yun Gao"}}, "url": "https://github.com/apache/flink/commit/de8e31736af54c3a8e00a38994b949051aa1b98c", "committedDate": "2020-06-09T03:03:12Z", "message": "[FLINK-18056][fs-connector] Hadoop path-based file writer adds UUID to in-progress file to avoid conflicts"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "248476005c601939c42dab33d7f689809090ff35", "author": {"user": {"login": "gaoyunhaii", "name": "Yun Gao"}}, "url": "https://github.com/apache/flink/commit/248476005c601939c42dab33d7f689809090ff35", "committedDate": "2020-06-08T11:39:21Z", "message": "Remove the S3 tests"}, "afterCommit": {"oid": "de8e31736af54c3a8e00a38994b949051aa1b98c", "author": {"user": {"login": "gaoyunhaii", "name": "Yun Gao"}}, "url": "https://github.com/apache/flink/commit/de8e31736af54c3a8e00a38994b949051aa1b98c", "committedDate": "2020-06-09T03:03:12Z", "message": "[FLINK-18056][fs-connector] Hadoop path-based file writer adds UUID to in-progress file to avoid conflicts"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4222, "cost": 1, "resetAt": "2021-10-28T16:48:13Z"}}}