{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDUwODU4MDg1", "number": 12919, "title": "[FLINK-16048][avro] Support read/write confluent schema registry avro\u2026", "bodyText": "\u2026 data from Kafka\nWhat is the purpose of the change\nSupports read/write with SQL using schema registry avro format.\nThe format details\nThe factory identifier (or format id)\nThere are 2 candidates now ~\navro-sr: the pattern borrowed from KSQL JSON_SR format [1]\navro-confluent: the pattern borrowed from Clickhouse AvroConfluent [2]\nPersonally i would prefer avro-sr because it is more concise and the confluent is a company name which i think is not that suitable for a format name.\nThe format attributes\n\n\n\nOptions\nrequired\nRemark\n\n\n\n\nschema-registry.url\ntrue\nURL to connect to schema registry service\n\n\nschema-registry.subject\nfalse\nSubject name to write to the Schema Registry service, required for sink\n\n\n\nNote: the avro schema string is always inferred from the DDL schema, so user should keep the nullability correct (DDL type default is nullable but avro default is non-nullable).\nBrief change log\n\nAdd avro-sr read/write row data format\nAdd tests\n\nVerifying this change\nAdded tests.\nDoes this pull request potentially affect one of the following parts:\n\nDependencies (does it add or upgrade a dependency): no\nThe public API, i.e., is any changed class annotated with @Public(Evolving): no\nThe serializers: no\nThe runtime per-record code paths (performance sensitive): no\nAnything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: no\nThe S3 file system connector: (yes / no / don't know) no\n\nDocumentation\n\nDoes this pull request introduce a new feature? yes\nIf yes, how is the feature documented? not documented", "createdAt": "2020-07-17T07:18:46Z", "url": "https://github.com/apache/flink/pull/12919", "merged": true, "mergeCommit": {"oid": "1c09c23810cf844001dd70d3b78a7a60b49611c7"}, "closed": true, "closedAt": "2020-07-30T13:09:27Z", "author": {"login": "danny0405"}, "timelineItems": {"totalCount": 22, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABc11ovOAFqTQ1MDc1NzI1MQ==", "endCursor": "Y3Vyc29yOnYyOpPPAAABdSEun2gFqTUwNzIxNjMzOA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDUwNzU3MjUx", "url": "https://github.com/apache/flink/pull/12919#pullrequestreview-450757251", "createdAt": "2020-07-17T15:33:31Z", "commit": {"oid": "bac18583fd0ba4855eebd76409198e1fb3fc3314"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxNTozMzozMVrOGzXoXQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxNTozMzozMVrOGzXoXQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjUxNzcyNQ==", "bodyText": "Update the version in the License notice file", "url": "https://github.com/apache/flink/pull/12919#discussion_r456517725", "createdAt": "2020-07-17T15:33:31Z", "author": {"login": "sjwiesman"}, "path": "flink-formats/flink-avro-confluent-registry/pom.xml", "diffHunk": "@@ -30,7 +30,7 @@ under the License.\n \t<artifactId>flink-avro-confluent-registry</artifactId>\n \n \t<properties>\n-\t\t<confluent.schema.registry.version>4.1.0</confluent.schema.registry.version>\n+\t\t<confluent.schema.registry.version>5.5.1</confluent.schema.registry.version>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bac18583fd0ba4855eebd76409198e1fb3fc3314"}, "originalPosition": 5}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "87d74c99c293fba3090208d89f687be7dbe3f3ab", "author": {"user": {"login": "danny0405", "name": "Danny Chan"}}, "url": "https://github.com/apache/flink/commit/87d74c99c293fba3090208d89f687be7dbe3f3ab", "committedDate": "2020-07-20T03:00:20Z", "message": "Fix the review comments"}, "afterCommit": {"oid": "48d837f15c74d134f2ba8b8e8f7ea27e6b62299f", "author": {"user": {"login": "danny0405", "name": "Danny Chan"}}, "url": "https://github.com/apache/flink/commit/48d837f15c74d134f2ba8b8e8f7ea27e6b62299f", "committedDate": "2020-07-20T05:47:08Z", "message": "Fix the review comments"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "0829a1d9fbbdffccf0399ff0a0c4dc9a959c1b24", "author": {"user": {"login": "danny0405", "name": "Danny Chan"}}, "url": "https://github.com/apache/flink/commit/0829a1d9fbbdffccf0399ff0a0c4dc9a959c1b24", "committedDate": "2020-07-21T02:39:39Z", "message": "Fix the AvroSchemaConverter#convertToDataType for nullability"}, "afterCommit": {"oid": "edb952c0f8ae4394b7f5238f4fea39878106a775", "author": {"user": {"login": "danny0405", "name": "Danny Chan"}}, "url": "https://github.com/apache/flink/commit/edb952c0f8ae4394b7f5238f4fea39878106a775", "committedDate": "2020-07-21T06:16:14Z", "message": "[FLINK-16048][avro] Support read/write confluent schema registry avro data from Kafka"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDUxNDM5ODYz", "url": "https://github.com/apache/flink/pull/12919#pullrequestreview-451439863", "createdAt": "2020-07-20T09:24:34Z", "commit": {"oid": "48d837f15c74d134f2ba8b8e8f7ea27e6b62299f"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQwOToyNDozNFrOG0CLaQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQwOToyNDozNFrOG0CLaQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzIxNDgyNQ==", "bodyText": "default scope? + @Internal", "url": "https://github.com/apache/flink/pull/12919#discussion_r457214825", "createdAt": "2020-07-20T09:24:34Z", "author": {"login": "dawidwys"}, "path": "flink-formats/flink-avro-confluent-registry/src/main/java/org/apache/flink/formats/avro/registry/confluent/CachedSchemaCoderProvider.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.avro.registry.confluent;\n+\n+import org.apache.flink.formats.avro.SchemaCoder;\n+\n+import io.confluent.kafka.schemaregistry.client.CachedSchemaRegistryClient;\n+\n+import javax.annotation.Nullable;\n+\n+import java.util.Objects;\n+\n+/** A {@link SchemaCoder.SchemaCoderProvider} that uses a cached schema registry\n+ * client underlying. **/\n+public class CachedSchemaCoderProvider implements SchemaCoder.SchemaCoderProvider {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "48d837f15c74d134f2ba8b8e8f7ea27e6b62299f"}, "originalPosition": 31}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "8984f0fbb7914ce69763e0a2b7afb869621bdd0d", "author": {"user": {"login": "danny0405", "name": "Danny Chan"}}, "url": "https://github.com/apache/flink/commit/8984f0fbb7914ce69763e0a2b7afb869621bdd0d", "committedDate": "2020-07-21T12:37:11Z", "message": "Fix the review comments"}, "afterCommit": {"oid": "d1e4ba7690be134e21d193fbc1cb01aa51aaeb9b", "author": {"user": {"login": "danny0405", "name": "Danny Chan"}}, "url": "https://github.com/apache/flink/commit/d1e4ba7690be134e21d193fbc1cb01aa51aaeb9b", "committedDate": "2020-07-22T03:51:41Z", "message": "Fix the review comments"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "d1e4ba7690be134e21d193fbc1cb01aa51aaeb9b", "author": {"user": {"login": "danny0405", "name": "Danny Chan"}}, "url": "https://github.com/apache/flink/commit/d1e4ba7690be134e21d193fbc1cb01aa51aaeb9b", "committedDate": "2020-07-22T03:51:41Z", "message": "Fix the review comments"}, "afterCommit": {"oid": "f0da3cee91d22ec20cbba1b6c5be45da1440cf05", "author": {"user": {"login": "danny0405", "name": "Danny Chan"}}, "url": "https://github.com/apache/flink/commit/f0da3cee91d22ec20cbba1b6c5be45da1440cf05", "committedDate": "2020-07-22T06:28:45Z", "message": "[FLINK-16048][avro] Support read/write confluent schema registry avro data from Kafka"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDUzMDU0OTc3", "url": "https://github.com/apache/flink/pull/12919#pullrequestreview-453054977", "createdAt": "2020-07-22T07:19:27Z", "commit": {"oid": "f0da3cee91d22ec20cbba1b6c5be45da1440cf05"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 14, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQwNzoxOToyOFrOG1V06Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQwOToyMToyNVrOG1aKyQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODU4NTMyMQ==", "bodyText": "Do we still need to extract this class after the latest changes?", "url": "https://github.com/apache/flink/pull/12919#discussion_r458585321", "createdAt": "2020-07-22T07:19:28Z", "author": {"login": "dawidwys"}, "path": "flink-formats/flink-avro-confluent-registry/src/main/java/org/apache/flink/formats/avro/registry/confluent/CachedSchemaCoderProvider.java", "diffHunk": "@@ -0,0 +1,76 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.avro.registry.confluent;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.formats.avro.SchemaCoder;\n+\n+import io.confluent.kafka.schemaregistry.client.CachedSchemaRegistryClient;\n+\n+import javax.annotation.Nullable;\n+\n+import java.util.Objects;\n+\n+/** A {@link SchemaCoder.SchemaCoderProvider} that uses a cached schema registry\n+ * client underlying. **/\n+@Internal\n+class CachedSchemaCoderProvider implements SchemaCoder.SchemaCoderProvider {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f0da3cee91d22ec20cbba1b6c5be45da1440cf05"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODU4NTQwMQ==", "bodyText": "ditto", "url": "https://github.com/apache/flink/pull/12919#discussion_r458585401", "createdAt": "2020-07-22T07:19:38Z", "author": {"login": "dawidwys"}, "path": "flink-formats/flink-avro-confluent-registry/src/main/java/org/apache/flink/formats/avro/registry/confluent/ConfluentRegistryAvroDeserializationSchema.java", "diffHunk": "@@ -114,23 +113,4 @@ private ConfluentRegistryAvroDeserializationSchema(Class<T> recordClazz, @Nullab\n \t\t\tnew CachedSchemaCoderProvider(url, identityMapCapacity)\n \t\t);\n \t}\n-\n-\tprivate static class CachedSchemaCoderProvider implements SchemaCoder.SchemaCoderProvider {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f0da3cee91d22ec20cbba1b6c5be45da1440cf05"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODU4NTQ2MQ==", "bodyText": "ditto", "url": "https://github.com/apache/flink/pull/12919#discussion_r458585461", "createdAt": "2020-07-22T07:19:44Z", "author": {"login": "dawidwys"}, "path": "flink-formats/flink-avro-confluent-registry/src/main/java/org/apache/flink/formats/avro/registry/confluent/ConfluentRegistryAvroSerializationSchema.java", "diffHunk": "@@ -92,25 +91,4 @@ private ConfluentRegistryAvroSerializationSchema(Class<T> recordClazz, Schema sc\n \t\t\tnew CachedSchemaCoderProvider(subject, schemaRegistryUrl, DEFAULT_IDENTITY_MAP_CAPACITY)\n \t\t);\n \t}\n-\n-\tprivate static class CachedSchemaCoderProvider implements SchemaCoder.SchemaCoderProvider {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f0da3cee91d22ec20cbba1b6c5be45da1440cf05"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODYwNDg0MQ==", "bodyText": "Why do we need that in the @Before block? Can't we just initialize it statically?", "url": "https://github.com/apache/flink/pull/12919#discussion_r458604841", "createdAt": "2020-07-22T07:55:03Z", "author": {"login": "dawidwys"}, "path": "flink-formats/flink-avro-confluent-registry/src/test/java/org/apache/flink/formats/avro/registry/confluent/RegistryAvroFormatFactoryTest.java", "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.avro.registry.confluent;\n+\n+import org.apache.flink.api.common.serialization.DeserializationSchema;\n+import org.apache.flink.api.common.serialization.SerializationSchema;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.formats.avro.AvroRowDataDeserializationSchema;\n+import org.apache.flink.formats.avro.AvroRowDataSerializationSchema;\n+import org.apache.flink.formats.avro.AvroToRowDataConverters;\n+import org.apache.flink.formats.avro.RowDataToAvroConverters;\n+import org.apache.flink.formats.avro.typeutils.AvroSchemaConverter;\n+import org.apache.flink.table.api.DataTypes;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.api.ValidationException;\n+import org.apache.flink.table.catalog.CatalogTableImpl;\n+import org.apache.flink.table.catalog.ObjectIdentifier;\n+import org.apache.flink.table.connector.sink.DynamicTableSink;\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.factories.FactoryUtil;\n+import org.apache.flink.table.factories.TestDynamicTableFactory;\n+import org.apache.flink.table.runtime.connector.source.ScanRuntimeProviderContext;\n+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n+import org.apache.flink.table.types.logical.RowType;\n+\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.ExpectedException;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.function.Consumer;\n+\n+import static org.apache.flink.core.testutils.FlinkMatchers.containsCause;\n+import static org.junit.Assert.assertEquals;\n+\n+/**\n+ * Tests for the {@link RegistryAvroFormatFactory}.\n+ */\n+public class RegistryAvroFormatFactoryTest {\n+\tprivate TableSchema schema;\n+\tprivate RowType rowType;\n+\tprivate String subject;\n+\tprivate String registryURL;\n+\n+\t@Rule\n+\tpublic ExpectedException thrown = ExpectedException.none();\n+\n+\t@Before\n+\tpublic void before() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f0da3cee91d22ec20cbba1b6c5be45da1440cf05"}, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODYxMTA3Nw==", "bodyText": "Please don't use assert. I can't think of a reason to use an assert in a test. assert is an assertion you can disable via a compiler flag. Why would you want to disable assertions in tests? If you want to check the type of actualSource use e.g.\nassertThat(actualSink, instanceOf(TestDynamicTableFactory.DynamicTableSinkMock.class));", "url": "https://github.com/apache/flink/pull/12919#discussion_r458611077", "createdAt": "2020-07-22T08:05:51Z", "author": {"login": "dawidwys"}, "path": "flink-formats/flink-avro-confluent-registry/src/test/java/org/apache/flink/formats/avro/registry/confluent/RegistryAvroFormatFactoryTest.java", "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.avro.registry.confluent;\n+\n+import org.apache.flink.api.common.serialization.DeserializationSchema;\n+import org.apache.flink.api.common.serialization.SerializationSchema;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.formats.avro.AvroRowDataDeserializationSchema;\n+import org.apache.flink.formats.avro.AvroRowDataSerializationSchema;\n+import org.apache.flink.formats.avro.AvroToRowDataConverters;\n+import org.apache.flink.formats.avro.RowDataToAvroConverters;\n+import org.apache.flink.formats.avro.typeutils.AvroSchemaConverter;\n+import org.apache.flink.table.api.DataTypes;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.api.ValidationException;\n+import org.apache.flink.table.catalog.CatalogTableImpl;\n+import org.apache.flink.table.catalog.ObjectIdentifier;\n+import org.apache.flink.table.connector.sink.DynamicTableSink;\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.factories.FactoryUtil;\n+import org.apache.flink.table.factories.TestDynamicTableFactory;\n+import org.apache.flink.table.runtime.connector.source.ScanRuntimeProviderContext;\n+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n+import org.apache.flink.table.types.logical.RowType;\n+\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.ExpectedException;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.function.Consumer;\n+\n+import static org.apache.flink.core.testutils.FlinkMatchers.containsCause;\n+import static org.junit.Assert.assertEquals;\n+\n+/**\n+ * Tests for the {@link RegistryAvroFormatFactory}.\n+ */\n+public class RegistryAvroFormatFactoryTest {\n+\tprivate TableSchema schema;\n+\tprivate RowType rowType;\n+\tprivate String subject;\n+\tprivate String registryURL;\n+\n+\t@Rule\n+\tpublic ExpectedException thrown = ExpectedException.none();\n+\n+\t@Before\n+\tpublic void before() {\n+\t\tthis.schema = TableSchema.builder()\n+\t\t\t\t.field(\"a\", DataTypes.STRING())\n+\t\t\t\t.field(\"b\", DataTypes.INT())\n+\t\t\t\t.field(\"c\", DataTypes.BOOLEAN())\n+\t\t\t\t.build();\n+\t\tthis.rowType = (RowType) schema.toRowDataType().getLogicalType();\n+\t\tthis.subject = \"test-subject\";\n+\t\tthis.registryURL = \"http://localhost:8081\";\n+\t}\n+\n+\t@Test\n+\tpublic void testSeDeSchema() {\n+\t\tfinal AvroRowDataDeserializationSchema expectedDeser =\n+\t\t\t\tnew AvroRowDataDeserializationSchema(\n+\t\t\t\t\t\tConfluentRegistryAvroDeserializationSchema.forGeneric(\n+\t\t\t\t\t\t\t\tAvroSchemaConverter.convertToSchema(rowType),\n+\t\t\t\t\t\t\t\tregistryURL),\n+\t\t\t\t\t\tAvroToRowDataConverters.createRowConverter(rowType),\n+\t\t\t\t\t\tInternalTypeInfo.of(rowType));\n+\n+\t\tfinal Map<String, String> options = getAllOptions();\n+\n+\t\tfinal DynamicTableSource actualSource = createTableSource(options);\n+\t\tassert actualSource instanceof TestDynamicTableFactory.DynamicTableSourceMock;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f0da3cee91d22ec20cbba1b6c5be45da1440cf05"}, "originalPosition": 92}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODYyNjg3NA==", "bodyText": "You have two completely independent tests in the single method. Please split it into two separate tests. We should always aim to test a single thing at a time. The benefits are:\n\nBoth tests are always executed. Independent of the result of the other.\nIt's easier to debug. You don't need to run the first case if the second fails.", "url": "https://github.com/apache/flink/pull/12919#discussion_r458626874", "createdAt": "2020-07-22T08:32:21Z", "author": {"login": "dawidwys"}, "path": "flink-formats/flink-avro-confluent-registry/src/test/java/org/apache/flink/formats/avro/registry/confluent/RegistryAvroFormatFactoryTest.java", "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.avro.registry.confluent;\n+\n+import org.apache.flink.api.common.serialization.DeserializationSchema;\n+import org.apache.flink.api.common.serialization.SerializationSchema;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.formats.avro.AvroRowDataDeserializationSchema;\n+import org.apache.flink.formats.avro.AvroRowDataSerializationSchema;\n+import org.apache.flink.formats.avro.AvroToRowDataConverters;\n+import org.apache.flink.formats.avro.RowDataToAvroConverters;\n+import org.apache.flink.formats.avro.typeutils.AvroSchemaConverter;\n+import org.apache.flink.table.api.DataTypes;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.api.ValidationException;\n+import org.apache.flink.table.catalog.CatalogTableImpl;\n+import org.apache.flink.table.catalog.ObjectIdentifier;\n+import org.apache.flink.table.connector.sink.DynamicTableSink;\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.factories.FactoryUtil;\n+import org.apache.flink.table.factories.TestDynamicTableFactory;\n+import org.apache.flink.table.runtime.connector.source.ScanRuntimeProviderContext;\n+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n+import org.apache.flink.table.types.logical.RowType;\n+\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.ExpectedException;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.function.Consumer;\n+\n+import static org.apache.flink.core.testutils.FlinkMatchers.containsCause;\n+import static org.junit.Assert.assertEquals;\n+\n+/**\n+ * Tests for the {@link RegistryAvroFormatFactory}.\n+ */\n+public class RegistryAvroFormatFactoryTest {\n+\tprivate TableSchema schema;\n+\tprivate RowType rowType;\n+\tprivate String subject;\n+\tprivate String registryURL;\n+\n+\t@Rule\n+\tpublic ExpectedException thrown = ExpectedException.none();\n+\n+\t@Before\n+\tpublic void before() {\n+\t\tthis.schema = TableSchema.builder()\n+\t\t\t\t.field(\"a\", DataTypes.STRING())\n+\t\t\t\t.field(\"b\", DataTypes.INT())\n+\t\t\t\t.field(\"c\", DataTypes.BOOLEAN())\n+\t\t\t\t.build();\n+\t\tthis.rowType = (RowType) schema.toRowDataType().getLogicalType();\n+\t\tthis.subject = \"test-subject\";\n+\t\tthis.registryURL = \"http://localhost:8081\";\n+\t}\n+\n+\t@Test\n+\tpublic void testSeDeSchema() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f0da3cee91d22ec20cbba1b6c5be45da1440cf05"}, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODYyODMzNQ==", "bodyText": "getAllOptions -> getDefaultOptions", "url": "https://github.com/apache/flink/pull/12919#discussion_r458628335", "createdAt": "2020-07-22T08:34:52Z", "author": {"login": "dawidwys"}, "path": "flink-formats/flink-avro-confluent-registry/src/test/java/org/apache/flink/formats/avro/registry/confluent/RegistryAvroFormatFactoryTest.java", "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.avro.registry.confluent;\n+\n+import org.apache.flink.api.common.serialization.DeserializationSchema;\n+import org.apache.flink.api.common.serialization.SerializationSchema;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.formats.avro.AvroRowDataDeserializationSchema;\n+import org.apache.flink.formats.avro.AvroRowDataSerializationSchema;\n+import org.apache.flink.formats.avro.AvroToRowDataConverters;\n+import org.apache.flink.formats.avro.RowDataToAvroConverters;\n+import org.apache.flink.formats.avro.typeutils.AvroSchemaConverter;\n+import org.apache.flink.table.api.DataTypes;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.api.ValidationException;\n+import org.apache.flink.table.catalog.CatalogTableImpl;\n+import org.apache.flink.table.catalog.ObjectIdentifier;\n+import org.apache.flink.table.connector.sink.DynamicTableSink;\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.factories.FactoryUtil;\n+import org.apache.flink.table.factories.TestDynamicTableFactory;\n+import org.apache.flink.table.runtime.connector.source.ScanRuntimeProviderContext;\n+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n+import org.apache.flink.table.types.logical.RowType;\n+\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.ExpectedException;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.function.Consumer;\n+\n+import static org.apache.flink.core.testutils.FlinkMatchers.containsCause;\n+import static org.junit.Assert.assertEquals;\n+\n+/**\n+ * Tests for the {@link RegistryAvroFormatFactory}.\n+ */\n+public class RegistryAvroFormatFactoryTest {\n+\tprivate TableSchema schema;\n+\tprivate RowType rowType;\n+\tprivate String subject;\n+\tprivate String registryURL;\n+\n+\t@Rule\n+\tpublic ExpectedException thrown = ExpectedException.none();\n+\n+\t@Before\n+\tpublic void before() {\n+\t\tthis.schema = TableSchema.builder()\n+\t\t\t\t.field(\"a\", DataTypes.STRING())\n+\t\t\t\t.field(\"b\", DataTypes.INT())\n+\t\t\t\t.field(\"c\", DataTypes.BOOLEAN())\n+\t\t\t\t.build();\n+\t\tthis.rowType = (RowType) schema.toRowDataType().getLogicalType();\n+\t\tthis.subject = \"test-subject\";\n+\t\tthis.registryURL = \"http://localhost:8081\";\n+\t}\n+\n+\t@Test\n+\tpublic void testSeDeSchema() {\n+\t\tfinal AvroRowDataDeserializationSchema expectedDeser =\n+\t\t\t\tnew AvroRowDataDeserializationSchema(\n+\t\t\t\t\t\tConfluentRegistryAvroDeserializationSchema.forGeneric(\n+\t\t\t\t\t\t\t\tAvroSchemaConverter.convertToSchema(rowType),\n+\t\t\t\t\t\t\t\tregistryURL),\n+\t\t\t\t\t\tAvroToRowDataConverters.createRowConverter(rowType),\n+\t\t\t\t\t\tInternalTypeInfo.of(rowType));\n+\n+\t\tfinal Map<String, String> options = getAllOptions();\n+\n+\t\tfinal DynamicTableSource actualSource = createTableSource(options);\n+\t\tassert actualSource instanceof TestDynamicTableFactory.DynamicTableSourceMock;\n+\t\tTestDynamicTableFactory.DynamicTableSourceMock scanSourceMock =\n+\t\t\t\t(TestDynamicTableFactory.DynamicTableSourceMock) actualSource;\n+\n+\t\tDeserializationSchema<RowData> actualDeser = scanSourceMock.valueFormat\n+\t\t\t\t.createRuntimeDecoder(\n+\t\t\t\t\t\tScanRuntimeProviderContext.INSTANCE,\n+\t\t\t\t\t\tschema.toRowDataType());\n+\n+\t\tassertEquals(expectedDeser, actualDeser);\n+\n+\t\tfinal AvroRowDataSerializationSchema expectedSer =\n+\t\t\t\tnew AvroRowDataSerializationSchema(\n+\t\t\t\t\t\trowType,\n+\t\t\t\t\t\tConfluentRegistryAvroSerializationSchema.forGeneric(\n+\t\t\t\t\t\t\t\tsubject,\n+\t\t\t\t\t\t\t\tAvroSchemaConverter.convertToSchema(rowType),\n+\t\t\t\t\t\t\t\tregistryURL),\n+\t\t\t\t\t\tRowDataToAvroConverters.createRowConverter(rowType));\n+\n+\t\tfinal DynamicTableSink actualSink = createTableSink(options);\n+\t\tassert actualSink instanceof TestDynamicTableFactory.DynamicTableSinkMock;\n+\t\tTestDynamicTableFactory.DynamicTableSinkMock sinkMock =\n+\t\t\t\t(TestDynamicTableFactory.DynamicTableSinkMock) actualSink;\n+\n+\t\tSerializationSchema<RowData> actualSer = sinkMock.valueFormat\n+\t\t\t\t.createRuntimeEncoder(\n+\t\t\t\t\t\tnull,\n+\t\t\t\t\t\tschema.toRowDataType());\n+\n+\t\tassertEquals(expectedSer, actualSer);\n+\t}\n+\n+\t@Test\n+\tpublic void testMissingSubjectForSink() {\n+\t\tthrown.expect(ValidationException.class);\n+\t\tthrown.expect(\n+\t\t\t\tcontainsCause(\n+\t\t\t\t\t\tnew ValidationException(\"Option avro-sr.schema-registry.subject \"\n+\t\t\t\t\t\t\t\t+ \"is required for serialization\")));\n+\n+\t\tfinal Map<String, String> options =\n+\t\t\t\tgetModifiedOptions(opts -> opts.remove(\"avro-sr.schema-registry.subject\"));\n+\n+\t\tcreateTableSink(options);\n+\t}\n+\n+\t// ------------------------------------------------------------------------\n+\t//  Utilities\n+\t// ------------------------------------------------------------------------\n+\n+\t/**\n+\t * Returns the full options modified by the given consumer {@code optionModifier}.\n+\t *\n+\t * @param optionModifier Consumer to modify the options\n+\t */\n+\tprivate Map<String, String> getModifiedOptions(Consumer<Map<String, String>> optionModifier) {\n+\t\tMap<String, String> options = getAllOptions();\n+\t\toptionModifier.accept(options);\n+\t\treturn options;\n+\t}\n+\n+\tprivate Map<String, String> getAllOptions() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f0da3cee91d22ec20cbba1b6c5be45da1440cf05"}, "originalPosition": 154}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODYzMDQxMA==", "bodyText": "Unnecessary change.", "url": "https://github.com/apache/flink/pull/12919#discussion_r458630410", "createdAt": "2020-07-22T08:38:20Z", "author": {"login": "dawidwys"}, "path": "flink-formats/flink-avro/pom.xml", "diffHunk": "@@ -140,6 +132,14 @@ under the License.\n \t\t\t<scope>test</scope>\n \t\t</dependency>\n \n+\t\t<!-- Avro RowData schema test dependency -->", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f0da3cee91d22ec20cbba1b6c5be45da1440cf05"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODYzMTUxNA==", "bodyText": "There is no point in using a large number here. Use  2L here.", "url": "https://github.com/apache/flink/pull/12919#discussion_r458631514", "createdAt": "2020-07-22T08:40:05Z", "author": {"login": "dawidwys"}, "path": "flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/AvroRowDataDeserializationSchema.java", "diffHunk": "@@ -75,17 +44,10 @@\n @PublicEvolving\n public class AvroRowDataDeserializationSchema implements DeserializationSchema<RowData> {\n \n-\tprivate static final long serialVersionUID = 1L;\n-\n-\t/**\n-\t * Used for converting Date type.\n-\t */\n-\tprivate static final int MILLIS_PER_DAY = 86400_000;\n+\tprivate static final long serialVersionUID = 9055890466043022732L;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f0da3cee91d22ec20cbba1b6c5be45da1440cf05"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODY0NTU0OQ==", "bodyText": "Those tests have nothing to do with schema registry.\nThey test the same logic as in AvroRowDataDeSerializationSchemaTest", "url": "https://github.com/apache/flink/pull/12919#discussion_r458645549", "createdAt": "2020-07-22T09:02:59Z", "author": {"login": "dawidwys"}, "path": "flink-formats/flink-avro/src/test/java/org/apache/flink/formats/avro/RegistryAvroRowDataSeDeSchemaTest.java", "diffHunk": "@@ -0,0 +1,129 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.avro;\n+\n+import org.apache.flink.formats.avro.generated.Address;\n+import org.apache.flink.formats.avro.typeutils.AvroSchemaConverter;\n+import org.apache.flink.formats.avro.utils.TestDataGenerator;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.util.Random;\n+\n+import static org.apache.flink.formats.avro.utils.AvroTestUtils.writeRecord;\n+import static org.hamcrest.core.Is.is;\n+import static org.junit.Assert.assertArrayEquals;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertThat;\n+\n+/**\n+ * Tests for {@link AvroRowDataDeserializationSchema} and\n+ * {@link AvroRowDataSerializationSchema} for schema registry avro.\n+ */\n+public class RegistryAvroRowDataSeDeSchemaTest {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f0da3cee91d22ec20cbba1b6c5be45da1440cf05"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODY1MDE1MQ==", "bodyText": "@Internal", "url": "https://github.com/apache/flink/pull/12919#discussion_r458650151", "createdAt": "2020-07-22T09:10:45Z", "author": {"login": "dawidwys"}, "path": "flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/AvroToRowDataConverters.java", "diffHunk": "@@ -0,0 +1,246 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.avro;\n+\n+import org.apache.flink.table.api.DataTypes;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.GenericArrayData;\n+import org.apache.flink.table.data.GenericMapData;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.DecimalType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.utils.LogicalTypeUtils;\n+\n+import org.apache.avro.generic.GenericFixed;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.joda.time.DateTime;\n+import org.joda.time.DateTimeFieldType;\n+import org.joda.time.LocalDate;\n+\n+import java.io.Serializable;\n+import java.lang.reflect.Array;\n+import java.nio.ByteBuffer;\n+import java.sql.Timestamp;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.apache.flink.formats.avro.typeutils.AvroSchemaConverter.extractValueTypeToAvroMap;\n+import static org.joda.time.DateTimeConstants.MILLIS_PER_DAY;\n+\n+/** Tool class used to convert from Avro {@link GenericRecord} to {@link RowData}. **/\n+public class AvroToRowDataConverters {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f0da3cee91d22ec20cbba1b6c5be45da1440cf05"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODY1MzMwMQ==", "bodyText": "Could we add tests for this method?", "url": "https://github.com/apache/flink/pull/12919#discussion_r458653301", "createdAt": "2020-07-22T09:16:10Z", "author": {"login": "dawidwys"}, "path": "flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/typeutils/AvroSchemaConverter.java", "diffHunk": "@@ -169,6 +171,121 @@ private AvroSchemaConverter() {\n \t\tthrow new IllegalArgumentException(\"Unsupported Avro type '\" + schema.getType() + \"'.\");\n \t}\n \n+\t/**\n+\t * Converts an Avro schema string into a nested row structure with deterministic field order and data\n+\t * types that are compatible with Flink's Table & SQL API.\n+\t *\n+\t * @param avroSchemaString Avro schema definition string\n+\t *\n+\t * @return data type matching the schema\n+\t */\n+\tpublic static DataType convertToDataType(String avroSchemaString) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f0da3cee91d22ec20cbba1b6c5be45da1440cf05"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODY1NTE3Nw==", "bodyText": "How about we remove this ctor? IMO the logic from this ctor should be only in the factory. I know that this class in theory is PublicEvolving but practically it is only usable from Table API through the factory. Therefore in my opinion it is safe to drop this ctor.\nThe same applies to SerializationSchema.", "url": "https://github.com/apache/flink/pull/12919#discussion_r458655177", "createdAt": "2020-07-22T09:19:23Z", "author": {"login": "dawidwys"}, "path": "flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/AvroRowDataSerializationSchema.java", "diffHunk": "@@ -64,61 +41,59 @@\n \n \tprivate static final long serialVersionUID = 1L;\n \n+\t/** Nested schema to serialize the {@link GenericRecord} into bytes. **/\n+\tprivate final SerializationSchema<GenericRecord> nestedSchema;\n+\n \t/**\n \t * Logical type describing the input type.\n \t */\n \tprivate final RowType rowType;\n \n-\t/**\n-\t * Runtime instance that performs the actual work.\n-\t */\n-\tprivate final SerializationRuntimeConverter runtimeConverter;\n-\n \t/**\n \t * Avro serialization schema.\n \t */\n \tprivate transient Schema schema;\n \n \t/**\n-\t * Writer to serialize Avro record into a Avro bytes.\n-\t */\n-\tprivate transient DatumWriter<IndexedRecord> datumWriter;\n-\n-\t/**\n-\t * Output stream to serialize records into byte array.\n+\t * Runtime instance that performs the actual work.\n \t */\n-\tprivate transient ByteArrayOutputStream arrayOutputStream;\n+\tprivate final RowDataToAvroConverters.RowDataToAvroConverter runtimeConverter;\n \n \t/**\n-\t * Low-level class for serialization of Avro values.\n+\t * Creates an Avro serialization schema with the given record row type.\n \t */\n-\tprivate transient Encoder encoder;\n+\tpublic AvroRowDataSerializationSchema(RowType rowType) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f0da3cee91d22ec20cbba1b6c5be45da1440cf05"}, "originalPosition": 77}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODY1NjQ1Nw==", "bodyText": "IMO we should add a simple test for serializing and deserializing using schema registry. It does not need to be very in depth, but  so that it checks that everything is well connected.", "url": "https://github.com/apache/flink/pull/12919#discussion_r458656457", "createdAt": "2020-07-22T09:21:25Z", "author": {"login": "dawidwys"}, "path": "flink-formats/flink-avro/src/test/java/org/apache/flink/formats/avro/RegistryAvroRowDataSeDeSchemaTest.java", "diffHunk": "@@ -0,0 +1,129 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.avro;\n+\n+import org.apache.flink.formats.avro.generated.Address;\n+import org.apache.flink.formats.avro.typeutils.AvroSchemaConverter;\n+import org.apache.flink.formats.avro.utils.TestDataGenerator;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.util.Random;\n+\n+import static org.apache.flink.formats.avro.utils.AvroTestUtils.writeRecord;\n+import static org.hamcrest.core.Is.is;\n+import static org.junit.Assert.assertArrayEquals;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertThat;\n+\n+/**\n+ * Tests for {@link AvroRowDataDeserializationSchema} and\n+ * {@link AvroRowDataSerializationSchema} for schema registry avro.\n+ */\n+public class RegistryAvroRowDataSeDeSchemaTest {\n+\tprivate static final String ADDRESS_SCHEMA = \"\" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f0da3cee91d22ec20cbba1b6c5be45da1440cf05"}, "originalPosition": 45}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "f006afeec4c8ee25dfe12b944e2cf4260239ca1e", "author": {"user": {"login": "danny0405", "name": "Danny Chan"}}, "url": "https://github.com/apache/flink/commit/f006afeec4c8ee25dfe12b944e2cf4260239ca1e", "committedDate": "2020-07-22T11:26:34Z", "message": "Fix the review comment address"}, "afterCommit": {"oid": "9104e12b0394cd6d578d2380ca4554b75e6e00f9", "author": {"user": {"login": "danny0405", "name": "Danny Chan"}}, "url": "https://github.com/apache/flink/commit/9104e12b0394cd6d578d2380ca4554b75e6e00f9", "committedDate": "2020-07-22T11:31:27Z", "message": "Fix the review comment address"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "22ed53e6e047b379e0ee568298600afd9283b2b8", "author": {"user": {"login": "danny0405", "name": "Danny Chan"}}, "url": "https://github.com/apache/flink/commit/22ed53e6e047b379e0ee568298600afd9283b2b8", "committedDate": "2020-07-23T08:04:04Z", "message": "Add schema registry serve in test and modify the tests"}, "afterCommit": {"oid": "870bafa5f797f69a43f713b554355838b2447cf8", "author": {"user": {"login": "danny0405", "name": "Danny Chan"}}, "url": "https://github.com/apache/flink/commit/870bafa5f797f69a43f713b554355838b2447cf8", "committedDate": "2020-07-23T10:51:20Z", "message": "Add schema registry serve in test and modify the tests"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "870bafa5f797f69a43f713b554355838b2447cf8", "author": {"user": {"login": "danny0405", "name": "Danny Chan"}}, "url": "https://github.com/apache/flink/commit/870bafa5f797f69a43f713b554355838b2447cf8", "committedDate": "2020-07-23T10:51:20Z", "message": "Add schema registry serve in test and modify the tests"}, "afterCommit": {"oid": "9b557c718fe731e8d5c58e7c5d9c3452a245ee5a", "author": {"user": {"login": "danny0405", "name": "Danny Chan"}}, "url": "https://github.com/apache/flink/commit/9b557c718fe731e8d5c58e7c5d9c3452a245ee5a", "committedDate": "2020-07-23T10:57:08Z", "message": "Add schema registry serve in test and modify the tests"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "9b557c718fe731e8d5c58e7c5d9c3452a245ee5a", "author": {"user": {"login": "danny0405", "name": "Danny Chan"}}, "url": "https://github.com/apache/flink/commit/9b557c718fe731e8d5c58e7c5d9c3452a245ee5a", "committedDate": "2020-07-23T10:57:08Z", "message": "Add schema registry serve in test and modify the tests"}, "afterCommit": {"oid": "71218ee49095663a641e56889831536a2a2e69ea", "author": {"user": {"login": "danny0405", "name": "Danny Chan"}}, "url": "https://github.com/apache/flink/commit/71218ee49095663a641e56889831536a2a2e69ea", "committedDate": "2020-07-23T12:03:52Z", "message": "Add schema registry serve in test and modify the tests"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "71218ee49095663a641e56889831536a2a2e69ea", "author": {"user": {"login": "danny0405", "name": "Danny Chan"}}, "url": "https://github.com/apache/flink/commit/71218ee49095663a641e56889831536a2a2e69ea", "committedDate": "2020-07-23T12:03:52Z", "message": "Add schema registry serve in test and modify the tests"}, "afterCommit": {"oid": "6839c54eedcdca926b8304782fabcb0dc529c5a6", "author": {"user": {"login": "danny0405", "name": "Danny Chan"}}, "url": "https://github.com/apache/flink/commit/6839c54eedcdca926b8304782fabcb0dc529c5a6", "committedDate": "2020-07-24T02:26:47Z", "message": "Add schema registry serve in test and modify the tests"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "6839c54eedcdca926b8304782fabcb0dc529c5a6", "author": {"user": {"login": "danny0405", "name": "Danny Chan"}}, "url": "https://github.com/apache/flink/commit/6839c54eedcdca926b8304782fabcb0dc529c5a6", "committedDate": "2020-07-24T02:26:47Z", "message": "Add schema registry serve in test and modify the tests"}, "afterCommit": {"oid": "9d8870894b4d9d434c45b58339985aed3b76a8be", "author": {"user": {"login": "danny0405", "name": "Danny Chan"}}, "url": "https://github.com/apache/flink/commit/9d8870894b4d9d434c45b58339985aed3b76a8be", "committedDate": "2020-07-24T04:20:21Z", "message": "Add schema registry serve in test and modify the tests"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "9d8870894b4d9d434c45b58339985aed3b76a8be", "author": {"user": {"login": "danny0405", "name": "Danny Chan"}}, "url": "https://github.com/apache/flink/commit/9d8870894b4d9d434c45b58339985aed3b76a8be", "committedDate": "2020-07-24T04:20:21Z", "message": "Add schema registry serve in test and modify the tests"}, "afterCommit": {"oid": "bc53f692ab7edf110e6c7c39202e50ed1ec0c05d", "author": {"user": {"login": "danny0405", "name": "Danny Chan"}}, "url": "https://github.com/apache/flink/commit/bc53f692ab7edf110e6c7c39202e50ed1ec0c05d", "committedDate": "2020-07-24T06:30:03Z", "message": "[FLINK-16048][avro] Support read/write confluent schema registry avro data from Kafka"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "bc53f692ab7edf110e6c7c39202e50ed1ec0c05d", "author": {"user": {"login": "danny0405", "name": "Danny Chan"}}, "url": "https://github.com/apache/flink/commit/bc53f692ab7edf110e6c7c39202e50ed1ec0c05d", "committedDate": "2020-07-24T06:30:03Z", "message": "[FLINK-16048][avro] Support read/write confluent schema registry avro data from Kafka"}, "afterCommit": {"oid": "0d49193a68ad28f6d2965bc8bfd57c59b2b21935", "author": {"user": {"login": "danny0405", "name": "Danny Chan"}}, "url": "https://github.com/apache/flink/commit/0d49193a68ad28f6d2965bc8bfd57c59b2b21935", "committedDate": "2020-07-24T07:17:11Z", "message": "[FLINK-16048][avro] Support read/write confluent schema registry avro data from Kafka"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "810321d988a8284eb54c2963f22a049dc06ac8aa", "author": {"user": {"login": "danny0405", "name": "Danny Chan"}}, "url": "https://github.com/apache/flink/commit/810321d988a8284eb54c2963f22a049dc06ac8aa", "committedDate": "2020-07-24T07:55:09Z", "message": "[FLINK-16048][avro] Support read/write confluent schema registry avro data from Kafka"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "0d49193a68ad28f6d2965bc8bfd57c59b2b21935", "author": {"user": {"login": "danny0405", "name": "Danny Chan"}}, "url": "https://github.com/apache/flink/commit/0d49193a68ad28f6d2965bc8bfd57c59b2b21935", "committedDate": "2020-07-24T07:17:11Z", "message": "[FLINK-16048][avro] Support read/write confluent schema registry avro data from Kafka"}, "afterCommit": {"oid": "810321d988a8284eb54c2963f22a049dc06ac8aa", "author": {"user": {"login": "danny0405", "name": "Danny Chan"}}, "url": "https://github.com/apache/flink/commit/810321d988a8284eb54c2963f22a049dc06ac8aa", "committedDate": "2020-07-24T07:55:09Z", "message": "[FLINK-16048][avro] Support read/write confluent schema registry avro data from Kafka"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6bd8c02de778a8ff2f34a19d5beee414beac3f69", "author": {"user": {"login": "danny0405", "name": "Danny Chan"}}, "url": "https://github.com/apache/flink/commit/6bd8c02de778a8ff2f34a19d5beee414beac3f69", "committedDate": "2020-07-24T10:34:03Z", "message": "Fix the review comments"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "38511b78cd5152fe5805f83752de31045b620ecb", "author": {"user": {"login": "danny0405", "name": "Danny Chan"}}, "url": "https://github.com/apache/flink/commit/38511b78cd5152fe5805f83752de31045b620ecb", "committedDate": "2020-07-24T10:30:51Z", "message": "Fix the review comments"}, "afterCommit": {"oid": "6bd8c02de778a8ff2f34a19d5beee414beac3f69", "author": {"user": {"login": "danny0405", "name": "Danny Chan"}}, "url": "https://github.com/apache/flink/commit/6bd8c02de778a8ff2f34a19d5beee414beac3f69", "committedDate": "2020-07-24T10:34:03Z", "message": "Fix the review comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU0ODQ2MzEw", "url": "https://github.com/apache/flink/pull/12919#pullrequestreview-454846310", "createdAt": "2020-07-24T12:28:59Z", "commit": {"oid": "6bd8c02de778a8ff2f34a19d5beee414beac3f69"}, "state": "APPROVED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNFQxMjoyODo1OVrOG2tgIg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNFQxMjozMToxN1rOG2tkLw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDAyMTc5NA==", "bodyText": "nit: use SUBJECT", "url": "https://github.com/apache/flink/pull/12919#discussion_r460021794", "createdAt": "2020-07-24T12:28:59Z", "author": {"login": "dawidwys"}, "path": "flink-formats/flink-avro-confluent-registry/src/test/java/org/apache/flink/formats/avro/registry/confluent/RegistryAvroRowDataSeDeSchemaTest.java", "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.avro.registry.confluent;\n+\n+import org.apache.flink.formats.avro.AvroRowDataDeserializationSchema;\n+import org.apache.flink.formats.avro.AvroRowDataSerializationSchema;\n+import org.apache.flink.formats.avro.AvroToRowDataConverters;\n+import org.apache.flink.formats.avro.RegistryAvroDeserializationSchema;\n+import org.apache.flink.formats.avro.RegistryAvroSerializationSchema;\n+import org.apache.flink.formats.avro.RowDataToAvroConverters;\n+import org.apache.flink.formats.avro.generated.Address;\n+import org.apache.flink.formats.avro.typeutils.AvroSchemaConverter;\n+import org.apache.flink.formats.avro.utils.TestDataGenerator;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.binary.BinaryStringData;\n+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+\n+import io.confluent.kafka.schemaregistry.client.MockSchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.GenericRecord;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.ExpectedException;\n+\n+import java.io.IOException;\n+import java.util.Random;\n+\n+import static org.apache.flink.core.testutils.FlinkMatchers.containsCause;\n+import static org.apache.flink.formats.avro.utils.AvroTestUtils.writeRecord;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.core.Is.is;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertThat;\n+\n+/**\n+ * Tests for {@link AvroRowDataDeserializationSchema} and\n+ * {@link AvroRowDataSerializationSchema} for schema registry avro.\n+ */\n+public class RegistryAvroRowDataSeDeSchemaTest {\n+\tprivate static final Schema ADDRESS_SCHEMA = Address.getClassSchema();\n+\n+\tprivate static final Schema ADDRESS_SCHEMA_COMPATIBLE = new Schema.Parser().parse(\n+\t\t\t\"\" +\n+\t\t\t\t\t\"{\\\"namespace\\\": \\\"org.apache.flink.formats.avro.generated\\\",\\n\" +\n+\t\t\t\t\t\" \\\"type\\\": \\\"record\\\",\\n\" +\n+\t\t\t\t\t\" \\\"name\\\": \\\"Address\\\",\\n\" +\n+\t\t\t\t\t\" \\\"fields\\\": [\\n\" +\n+\t\t\t\t\t\"     {\\\"name\\\": \\\"num\\\", \\\"type\\\": \\\"int\\\"},\\n\" +\n+\t\t\t\t\t\"     {\\\"name\\\": \\\"street\\\", \\\"type\\\": \\\"string\\\"}\\n\" +\n+\t\t\t\t\t\"  ]\\n\" +\n+\t\t\t\t\t\"}\");\n+\n+\tprivate static final String SUBJECT = \"address-value\";\n+\n+\tprivate static SchemaRegistryClient client;\n+\n+\tprivate Address address;\n+\n+\t@Rule\n+\tpublic ExpectedException expectedEx = ExpectedException.none();\n+\n+\t@BeforeClass\n+\tpublic static void beforeClass() {\n+\t\tclient = new MockSchemaRegistryClient();\n+\t}\n+\n+\t@Before\n+\tpublic void before() {\n+\t\tthis.address = TestDataGenerator.generateRandomAddress(new Random());\n+\t}\n+\n+\t@After\n+\tpublic void after() throws IOException, RestClientException {\n+\t\tclient.deleteSubject(SUBJECT);\n+\t}\n+\n+\t@Test\n+\tpublic void testRowDataWriteReadWithFullSchema() throws Exception {\n+\t\ttestRowDataWriteReadWithSchema(ADDRESS_SCHEMA);\n+\t}\n+\n+\t@Test\n+\tpublic void testRowDataWriteReadWithCompatibleSchema() throws Exception {\n+\t\ttestRowDataWriteReadWithSchema(ADDRESS_SCHEMA_COMPATIBLE);\n+\t\t// Validates new schema has been registered.\n+\t\tassertThat(client.getAllVersions(\"address-value\").size(), is(1));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6bd8c02de778a8ff2f34a19d5beee414beac3f69"}, "originalPosition": 110}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDAyMjgzMQ==", "bodyText": "Remove this method? If you prefer to keep it add missing javadoc.", "url": "https://github.com/apache/flink/pull/12919#discussion_r460022831", "createdAt": "2020-07-24T12:31:17Z", "author": {"login": "dawidwys"}, "path": "flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/RegistryAvroDeserializationSchema.java", "diffHunk": "@@ -52,13 +53,22 @@\n \t * @param schemaCoderProvider schema provider that allows instantiation of {@link SchemaCoder} that will be used for\n \t *                            schema reading\n \t */\n-\tprotected RegistryAvroDeserializationSchema(Class<T> recordClazz, @Nullable Schema reader,\n+\tpublic RegistryAvroDeserializationSchema(Class<T> recordClazz, @Nullable Schema reader,\n \t\t\tSchemaCoder.SchemaCoderProvider schemaCoderProvider) {\n \t\tsuper(recordClazz, reader);\n \t\tthis.schemaCoderProvider = schemaCoderProvider;\n \t\tthis.schemaCoder = schemaCoderProvider.get();\n \t}\n \n+\tpublic static RegistryAvroDeserializationSchema<GenericRecord> forGeneric(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6bd8c02de778a8ff2f34a19d5beee414beac3f69"}, "originalPosition": 20}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "52518eecfce65f5adceda689fa720f15c85413b6", "author": {"user": {"login": "danny0405", "name": "Danny Chan"}}, "url": "https://github.com/apache/flink/commit/52518eecfce65f5adceda689fa720f15c85413b6", "committedDate": "2020-07-25T00:04:55Z", "message": "Fix the review comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA3MjE2MzM4", "url": "https://github.com/apache/flink/pull/12919#pullrequestreview-507216338", "createdAt": "2020-10-13T08:58:25Z", "commit": {"oid": "52518eecfce65f5adceda689fa720f15c85413b6"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QwODo1ODoyNVrOHgchlQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QwODo1ODoyNVrOHgchlQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzc4MzgyOQ==", "bodyText": "This file does not exist in the release jar (https://mvnrepository.com/artifact/org.apache.flink/flink-avro-confluent-registry/1.11.2)\uff0c but some other things.\nWithout this flie, we could not use it in sql-client.sh...\nMaybe there is any mistake of the maven-shade-plugin config in pom.xml?", "url": "https://github.com/apache/flink/pull/12919#discussion_r503783829", "createdAt": "2020-10-13T08:58:25Z", "author": {"login": "homepy"}, "path": "flink-formats/flink-avro-confluent-registry/src/main/resources/META-INF/services/org.apache.flink.table.factories.Factory", "diffHunk": "@@ -0,0 +1,16 @@\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+org.apache.flink.formats.avro.registry.confluent.RegistryAvroFormatFactory", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52518eecfce65f5adceda689fa720f15c85413b6"}, "originalPosition": 16}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2885, "cost": 1, "resetAt": "2021-10-28T16:48:13Z"}}}