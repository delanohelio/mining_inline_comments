{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDExNDU3NTky", "number": 11962, "title": "[FLINK-17462][format][csv] Support CSV serialization and deseriazation schema for RowData type", "bodyText": "What is the purpose of the change\nAdd support CsvRowDataDeserializationSchema and CsvRowDataSerializationSchema for the new data structure RowData. This will be used by new TableSource and TableSink connectors.\nThe implemented CSV schema feature aligns with the exsiting ones.\nBrief change log\n\nCsvRowDataDeserializationSchema for deserialize csv byte[] into RowData.\nCsvRowDataSerializationSchema for serialize RowData into csv byte[].\n\nVerifying this change\n\nPorted tests from CsvRowDeSerializationSchemaTest to CsvRowDataSerDeSchemaTest.\n\nDoes this pull request potentially affect one of the following parts:\n\nDependencies (does it add or upgrade a dependency): yes\nThe public API, i.e., is any changed class annotated with @Public(Evolving): no\nThe serializers: no\nThe runtime per-record code paths (performance sensitive): yes\nAnything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: no\nThe S3 file system connector: no\n\nDocumentation\n\nDoes this pull request introduce a new feature? yes\nIf yes, how is the feature documented? not applicable", "createdAt": "2020-04-30T12:56:25Z", "url": "https://github.com/apache/flink/pull/11962", "merged": true, "mergeCommit": {"oid": "b1e436a109c6473b794f02ec0a853d1ae6df6c83"}, "closed": true, "closedAt": "2020-05-06T03:40:19Z", "author": {"login": "wuchong"}, "timelineItems": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABccsnKxAH2gAyNDExNDU3NTkyOjk4NDgxZTgyMTk4NDE1MDAyODljYWJkOGQzZTdiMWIwNzg3MjA3ZWE=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcefZFcAFqTQwNjI3MzE5Ng==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "98481e8219841500289cabd8d3e7b1b0787207ea", "author": {"user": {"login": "wuchong", "name": "Jark Wu"}}, "url": "https://github.com/apache/flink/commit/98481e8219841500289cabd8d3e7b1b0787207ea", "committedDate": "2020-04-30T12:54:34Z", "message": "[FLINK-17462][format][csv] Support CSV serialization and deseriazation schema for RowData type"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDAzNTA2NjQ2", "url": "https://github.com/apache/flink/pull/11962#pullrequestreview-403506646", "createdAt": "2020-04-30T13:14:49Z", "commit": {"oid": "98481e8219841500289cabd8d3e7b1b0787207ea"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0zMFQxMzoxNDo0OVrOGOoraw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0zMFQxMzozMToxM1rOGOpXCQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzk5OTcyMw==", "bodyText": "Just use jsonNode.isBoolean() ? jsonNode.asBoolean() : Boolean.parseBoolean(jsonNode.asText().trim())?\nSame below.", "url": "https://github.com/apache/flink/pull/11962#discussion_r417999723", "createdAt": "2020-04-30T13:14:49Z", "author": {"login": "JingsongLi"}, "path": "flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvRowDataDeserializationSchema.java", "diffHunk": "@@ -0,0 +1,458 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.csv;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.common.serialization.DeserializationSchema;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.GenericArrayData;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.DecimalType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.utils.LogicalTypeUtils;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.JsonNode;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ObjectReader;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.node.ArrayNode;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvMapper;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvSchema;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.lang.reflect.Array;\n+import java.math.BigDecimal;\n+import java.sql.Date;\n+import java.sql.Time;\n+import java.sql.Timestamp;\n+import java.time.LocalTime;\n+import java.util.Arrays;\n+import java.util.Objects;\n+\n+/**\n+ * Deserialization schema from CSV to Flink Table & SQL internal data structures.\n+ *\n+ * <p>Deserializes a <code>byte[]</code> message as a {@link JsonNode} and\n+ * converts it to {@link RowData}.\n+ *\n+ * <p>Failure during deserialization are forwarded as wrapped {@link IOException}s.\n+ */\n+@Internal\n+public final class CsvRowDataDeserializationSchema implements DeserializationSchema<RowData> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\t/** Type information describing the result type. */\n+\tprivate final TypeInformation<RowData> resultTypeInfo;\n+\n+\t/** Runtime instance that performs the actual work. */\n+\tprivate final DeserializationRuntimeConverter runtimeConverter;\n+\n+\t/** Schema describing the input CSV data. */\n+\tprivate final CsvSchema csvSchema;\n+\n+\t/** Object reader used to read rows. It is configured by {@link CsvSchema}. */\n+\tprivate final ObjectReader objectReader;\n+\n+\t/** Flag indicating whether to ignore invalid fields/rows (default: throw an exception). */\n+\tprivate final boolean ignoreParseErrors;\n+\n+\tprivate CsvRowDataDeserializationSchema(\n+\t\t\tRowType rowType,\n+\t\t\tTypeInformation<RowData> resultTypeInfo,\n+\t\t\tCsvSchema csvSchema,\n+\t\t\tboolean ignoreParseErrors) {\n+\t\tthis.resultTypeInfo = resultTypeInfo;\n+\t\tthis.runtimeConverter = createRowConverter(rowType, true);\n+\t\tthis.csvSchema = CsvRowSchemaConverter.convert(rowType);\n+\t\tthis.objectReader = new CsvMapper().readerFor(JsonNode.class).with(csvSchema);\n+\t\tthis.ignoreParseErrors = ignoreParseErrors;\n+\t}\n+\n+\t/**\n+\t * A builder for creating a {@link CsvRowDataDeserializationSchema}.\n+\t */\n+\t@Internal\n+\tpublic static class Builder {\n+\n+\t\tprivate final RowType rowType;\n+\t\tprivate final TypeInformation<RowData> resultTypeInfo;\n+\t\tprivate CsvSchema csvSchema;\n+\t\tprivate boolean ignoreParseErrors;\n+\n+\t\t/**\n+\t\t * Creates a CSV deserialization schema for the given {@link TypeInformation} with\n+\t\t * optional parameters.\n+\t\t */\n+\t\tpublic Builder(RowType rowType, TypeInformation<RowData> resultTypeInfo) {\n+\t\t\tPreconditions.checkNotNull(rowType, \"RowType must not be null.\");\n+\t\t\tPreconditions.checkNotNull(resultTypeInfo, \"Result type information must not be null.\");\n+\t\t\tthis.rowType = rowType;\n+\t\t\tthis.resultTypeInfo = resultTypeInfo;\n+\t\t\tthis.csvSchema = CsvRowSchemaConverter.convert(rowType);\n+\t\t}\n+\n+\t\tpublic Builder setFieldDelimiter(char delimiter) {\n+\t\t\tthis.csvSchema = this.csvSchema.rebuild().setColumnSeparator(delimiter).build();\n+\t\t\treturn this;\n+\t\t}\n+\n+\t\tpublic Builder setAllowComments(boolean allowComments) {\n+\t\t\tthis.csvSchema = this.csvSchema.rebuild().setAllowComments(allowComments).build();\n+\t\t\treturn this;\n+\t\t}\n+\n+\t\tpublic Builder setArrayElementDelimiter(String delimiter) {\n+\t\t\tPreconditions.checkNotNull(delimiter, \"Array element delimiter must not be null.\");\n+\t\t\tthis.csvSchema = this.csvSchema.rebuild().setArrayElementSeparator(delimiter).build();\n+\t\t\treturn this;\n+\t\t}\n+\n+\t\tpublic Builder setQuoteCharacter(char c) {\n+\t\t\tthis.csvSchema = this.csvSchema.rebuild().setQuoteChar(c).build();\n+\t\t\treturn this;\n+\t\t}\n+\n+\t\tpublic Builder setEscapeCharacter(char c) {\n+\t\t\tthis.csvSchema = this.csvSchema.rebuild().setEscapeChar(c).build();\n+\t\t\treturn this;\n+\t\t}\n+\n+\t\tpublic Builder setNullLiteral(String nullLiteral) {\n+\t\t\tPreconditions.checkNotNull(nullLiteral, \"Null literal must not be null.\");\n+\t\t\tthis.csvSchema = this.csvSchema.rebuild().setNullValue(nullLiteral).build();\n+\t\t\treturn this;\n+\t\t}\n+\n+\t\tpublic Builder setIgnoreParseErrors(boolean ignoreParseErrors) {\n+\t\t\tthis.ignoreParseErrors = ignoreParseErrors;\n+\t\t\treturn this;\n+\t\t}\n+\n+\t\tpublic CsvRowDataDeserializationSchema build() {\n+\t\t\treturn new CsvRowDataDeserializationSchema(\n+\t\t\t\trowType,\n+\t\t\t\tresultTypeInfo,\n+\t\t\t\tcsvSchema,\n+\t\t\t\tignoreParseErrors);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic RowData deserialize(byte[] message) throws IOException {\n+\t\ttry {\n+\t\t\tfinal JsonNode root = objectReader.readValue(message);\n+\t\t\treturn (RowData) runtimeConverter.convert(root);\n+\t\t} catch (Throwable t) {\n+\t\t\tif (ignoreParseErrors) {\n+\t\t\t\treturn null;\n+\t\t\t}\n+\t\t\tthrow new IOException(\"Failed to deserialize CSV row '\" + new String(message) + \"'.\", t);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic boolean isEndOfStream(RowData nextElement) {\n+\t\treturn false;\n+\t}\n+\n+\t@Override\n+\tpublic TypeInformation<RowData> getProducedType() {\n+\t\treturn resultTypeInfo;\n+\t}\n+\n+\t@Override\n+\tpublic boolean equals(Object o) {\n+\t\tif (this == o) {\n+\t\t\treturn true;\n+\t\t}\n+\t\tif (o == null || o.getClass() != this.getClass()) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tfinal CsvRowDataDeserializationSchema that = (CsvRowDataDeserializationSchema) o;\n+\t\tfinal CsvSchema otherSchema = that.csvSchema;\n+\n+\t\treturn resultTypeInfo.equals(that.resultTypeInfo) &&\n+\t\t\tignoreParseErrors == that.ignoreParseErrors &&\n+\t\t\tcsvSchema.getColumnSeparator() == otherSchema.getColumnSeparator() &&\n+\t\t\tcsvSchema.allowsComments() == otherSchema.allowsComments() &&\n+\t\t\tcsvSchema.getArrayElementSeparator().equals(otherSchema.getArrayElementSeparator()) &&\n+\t\t\tcsvSchema.getQuoteChar() == otherSchema.getQuoteChar() &&\n+\t\t\tcsvSchema.getEscapeChar() == otherSchema.getEscapeChar() &&\n+\t\t\tArrays.equals(csvSchema.getNullValue(), otherSchema.getNullValue());\n+\t}\n+\n+\t@Override\n+\tpublic int hashCode() {\n+\t\treturn Objects.hash(\n+\t\t\tresultTypeInfo,\n+\t\t\tignoreParseErrors,\n+\t\t\tcsvSchema.getColumnSeparator(),\n+\t\t\tcsvSchema.allowsComments(),\n+\t\t\tcsvSchema.getArrayElementSeparator(),\n+\t\t\tcsvSchema.getQuoteChar(),\n+\t\t\tcsvSchema.getEscapeChar(),\n+\t\t\tcsvSchema.getNullValue());\n+\t}\n+\n+\t// -------------------------------------------------------------------------------------\n+\t// Runtime Converters\n+\t// -------------------------------------------------------------------------------------\n+\n+\t// -------------------------------------------------------------------------------------\n+\t// Runtime Converters\n+\t// -------------------------------------------------------------------------------------\n+\n+\t/**\n+\t * Runtime converter that converts {@link JsonNode}s into objects of Flink Table & SQL\n+\t * internal data structures.\n+\t */\n+\t@FunctionalInterface\n+\tprivate interface DeserializationRuntimeConverter extends Serializable {\n+\t\tObject convert(JsonNode jsonNode);\n+\t}\n+\n+\tprivate DeserializationRuntimeConverter createRowConverter(RowType rowType, boolean isTopLevel) {\n+\t\tfinal DeserializationRuntimeConverter[] fieldConverters = rowType.getFields().stream()\n+\t\t\t.map(RowType.RowField::getType)\n+\t\t\t.map(this::createNullableConverter)\n+\t\t\t.toArray(DeserializationRuntimeConverter[]::new);\n+\t\tfinal String[] fieldNames = rowType.getFieldNames().toArray(new String[0]);\n+\t\tfinal int arity = fieldNames.length;\n+\n+\t\treturn jsonNode -> {\n+\t\t\tint nodeSize = jsonNode.size();\n+\n+\t\t\tvalidateArity(arity, nodeSize, ignoreParseErrors);\n+\n+\t\t\tGenericRowData row = new GenericRowData(arity);\n+\t\t\tfor (int i = 0; i < arity; i++) {\n+\t\t\t\tJsonNode field;\n+\t\t\t\t// Jackson only supports mapping by name in the first level\n+\t\t\t\tif (isTopLevel) {\n+\t\t\t\t\tfield = jsonNode.get(fieldNames[i]);\n+\t\t\t\t} else {\n+\t\t\t\t\tfield = jsonNode.get(i);\n+\t\t\t\t}\n+\t\t\t\tif (field == null) {\n+\t\t\t\t\trow.setField(i, null);\n+\t\t\t\t} else {\n+\t\t\t\t\trow.setField(i, fieldConverters[i].convert(field));\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\treturn row;\n+\t\t};\n+\t}\n+\n+\t/**\n+\t * Creates a runtime converter which is null safe.\n+\t */\n+\tprivate DeserializationRuntimeConverter createNullableConverter(LogicalType type) {\n+\t\tfinal DeserializationRuntimeConverter converter = createConverter(type);\n+\t\treturn jsonNode -> {\n+\t\t\tif (jsonNode == null || jsonNode.isNull()) {\n+\t\t\t\treturn null;\n+\t\t\t}\n+\t\t\ttry {\n+\t\t\t\treturn converter.convert(jsonNode);\n+\t\t\t} catch (Throwable t) {\n+\t\t\t\tif (!ignoreParseErrors) {\n+\t\t\t\t\tthrow t;\n+\t\t\t\t}\n+\t\t\t\treturn null;\n+\t\t\t}\n+\t\t};\n+\t}\n+\n+\t/**\n+\t * Creates a runtime converter which assuming input object is not null.\n+\t */\n+\tprivate DeserializationRuntimeConverter createConverter(LogicalType type) {\n+\t\tswitch (type.getTypeRoot()) {\n+\t\t\tcase NULL:\n+\t\t\t\treturn jsonNode -> null;\n+\t\t\tcase BOOLEAN:\n+\t\t\t\treturn this::convertToBoolean;\n+\t\t\tcase TINYINT:\n+\t\t\t\treturn jsonNode -> Byte.parseByte(jsonNode.asText().trim());\n+\t\t\tcase SMALLINT:\n+\t\t\t\treturn jsonNode -> Short.parseShort(jsonNode.asText().trim());\n+\t\t\tcase INTEGER:\n+\t\t\tcase INTERVAL_YEAR_MONTH:\n+\t\t\t\treturn this::convertToInt;\n+\t\t\tcase BIGINT:\n+\t\t\tcase INTERVAL_DAY_TIME:\n+\t\t\t\treturn this::convertToLong;\n+\t\t\tcase DATE:\n+\t\t\t\treturn this::convertToDate;\n+\t\t\tcase TIME_WITHOUT_TIME_ZONE:\n+\t\t\t\treturn this::convertToTime;\n+\t\t\tcase TIMESTAMP_WITH_TIME_ZONE:\n+\t\t\tcase TIMESTAMP_WITHOUT_TIME_ZONE:\n+\t\t\t\treturn this::convertToTimestamp;\n+\t\t\tcase FLOAT:\n+\t\t\t\treturn this::convertToFloat;\n+\t\t\tcase DOUBLE:\n+\t\t\t\treturn this::convertToDouble;\n+\t\t\tcase CHAR:\n+\t\t\tcase VARCHAR:\n+\t\t\t\treturn this::convertToString;\n+\t\t\tcase BINARY:\n+\t\t\tcase VARBINARY:\n+\t\t\t\treturn this::convertToBytes;\n+\t\t\tcase DECIMAL:\n+\t\t\t\treturn createDecimalConverter((DecimalType) type);\n+\t\t\tcase ARRAY:\n+\t\t\t\treturn createArrayConverter((ArrayType) type);\n+\t\t\tcase ROW:\n+\t\t\t\treturn createRowConverter((RowType) type, false);\n+\t\t\tcase MAP:\n+\t\t\tcase MULTISET:\n+\t\t\tcase RAW:\n+\t\t\tdefault:\n+\t\t\t\tthrow new UnsupportedOperationException(\"Unsupported type: \" + type);\n+\t\t}\n+\t}\n+\n+\tprivate boolean convertToBoolean(JsonNode jsonNode) {\n+\t\tif (jsonNode.isBoolean()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "98481e8219841500289cabd8d3e7b1b0787207ea"}, "originalPosition": 340}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzk5OTg4OQ==", "bodyText": "Why we can't use node.asBoolean() directly?", "url": "https://github.com/apache/flink/pull/11962#discussion_r417999889", "createdAt": "2020-04-30T13:15:06Z", "author": {"login": "JingsongLi"}, "path": "flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvRowDataDeserializationSchema.java", "diffHunk": "@@ -0,0 +1,458 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.csv;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.common.serialization.DeserializationSchema;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.GenericArrayData;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.DecimalType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.utils.LogicalTypeUtils;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.JsonNode;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ObjectReader;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.node.ArrayNode;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvMapper;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvSchema;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.lang.reflect.Array;\n+import java.math.BigDecimal;\n+import java.sql.Date;\n+import java.sql.Time;\n+import java.sql.Timestamp;\n+import java.time.LocalTime;\n+import java.util.Arrays;\n+import java.util.Objects;\n+\n+/**\n+ * Deserialization schema from CSV to Flink Table & SQL internal data structures.\n+ *\n+ * <p>Deserializes a <code>byte[]</code> message as a {@link JsonNode} and\n+ * converts it to {@link RowData}.\n+ *\n+ * <p>Failure during deserialization are forwarded as wrapped {@link IOException}s.\n+ */\n+@Internal\n+public final class CsvRowDataDeserializationSchema implements DeserializationSchema<RowData> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\t/** Type information describing the result type. */\n+\tprivate final TypeInformation<RowData> resultTypeInfo;\n+\n+\t/** Runtime instance that performs the actual work. */\n+\tprivate final DeserializationRuntimeConverter runtimeConverter;\n+\n+\t/** Schema describing the input CSV data. */\n+\tprivate final CsvSchema csvSchema;\n+\n+\t/** Object reader used to read rows. It is configured by {@link CsvSchema}. */\n+\tprivate final ObjectReader objectReader;\n+\n+\t/** Flag indicating whether to ignore invalid fields/rows (default: throw an exception). */\n+\tprivate final boolean ignoreParseErrors;\n+\n+\tprivate CsvRowDataDeserializationSchema(\n+\t\t\tRowType rowType,\n+\t\t\tTypeInformation<RowData> resultTypeInfo,\n+\t\t\tCsvSchema csvSchema,\n+\t\t\tboolean ignoreParseErrors) {\n+\t\tthis.resultTypeInfo = resultTypeInfo;\n+\t\tthis.runtimeConverter = createRowConverter(rowType, true);\n+\t\tthis.csvSchema = CsvRowSchemaConverter.convert(rowType);\n+\t\tthis.objectReader = new CsvMapper().readerFor(JsonNode.class).with(csvSchema);\n+\t\tthis.ignoreParseErrors = ignoreParseErrors;\n+\t}\n+\n+\t/**\n+\t * A builder for creating a {@link CsvRowDataDeserializationSchema}.\n+\t */\n+\t@Internal\n+\tpublic static class Builder {\n+\n+\t\tprivate final RowType rowType;\n+\t\tprivate final TypeInformation<RowData> resultTypeInfo;\n+\t\tprivate CsvSchema csvSchema;\n+\t\tprivate boolean ignoreParseErrors;\n+\n+\t\t/**\n+\t\t * Creates a CSV deserialization schema for the given {@link TypeInformation} with\n+\t\t * optional parameters.\n+\t\t */\n+\t\tpublic Builder(RowType rowType, TypeInformation<RowData> resultTypeInfo) {\n+\t\t\tPreconditions.checkNotNull(rowType, \"RowType must not be null.\");\n+\t\t\tPreconditions.checkNotNull(resultTypeInfo, \"Result type information must not be null.\");\n+\t\t\tthis.rowType = rowType;\n+\t\t\tthis.resultTypeInfo = resultTypeInfo;\n+\t\t\tthis.csvSchema = CsvRowSchemaConverter.convert(rowType);\n+\t\t}\n+\n+\t\tpublic Builder setFieldDelimiter(char delimiter) {\n+\t\t\tthis.csvSchema = this.csvSchema.rebuild().setColumnSeparator(delimiter).build();\n+\t\t\treturn this;\n+\t\t}\n+\n+\t\tpublic Builder setAllowComments(boolean allowComments) {\n+\t\t\tthis.csvSchema = this.csvSchema.rebuild().setAllowComments(allowComments).build();\n+\t\t\treturn this;\n+\t\t}\n+\n+\t\tpublic Builder setArrayElementDelimiter(String delimiter) {\n+\t\t\tPreconditions.checkNotNull(delimiter, \"Array element delimiter must not be null.\");\n+\t\t\tthis.csvSchema = this.csvSchema.rebuild().setArrayElementSeparator(delimiter).build();\n+\t\t\treturn this;\n+\t\t}\n+\n+\t\tpublic Builder setQuoteCharacter(char c) {\n+\t\t\tthis.csvSchema = this.csvSchema.rebuild().setQuoteChar(c).build();\n+\t\t\treturn this;\n+\t\t}\n+\n+\t\tpublic Builder setEscapeCharacter(char c) {\n+\t\t\tthis.csvSchema = this.csvSchema.rebuild().setEscapeChar(c).build();\n+\t\t\treturn this;\n+\t\t}\n+\n+\t\tpublic Builder setNullLiteral(String nullLiteral) {\n+\t\t\tPreconditions.checkNotNull(nullLiteral, \"Null literal must not be null.\");\n+\t\t\tthis.csvSchema = this.csvSchema.rebuild().setNullValue(nullLiteral).build();\n+\t\t\treturn this;\n+\t\t}\n+\n+\t\tpublic Builder setIgnoreParseErrors(boolean ignoreParseErrors) {\n+\t\t\tthis.ignoreParseErrors = ignoreParseErrors;\n+\t\t\treturn this;\n+\t\t}\n+\n+\t\tpublic CsvRowDataDeserializationSchema build() {\n+\t\t\treturn new CsvRowDataDeserializationSchema(\n+\t\t\t\trowType,\n+\t\t\t\tresultTypeInfo,\n+\t\t\t\tcsvSchema,\n+\t\t\t\tignoreParseErrors);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic RowData deserialize(byte[] message) throws IOException {\n+\t\ttry {\n+\t\t\tfinal JsonNode root = objectReader.readValue(message);\n+\t\t\treturn (RowData) runtimeConverter.convert(root);\n+\t\t} catch (Throwable t) {\n+\t\t\tif (ignoreParseErrors) {\n+\t\t\t\treturn null;\n+\t\t\t}\n+\t\t\tthrow new IOException(\"Failed to deserialize CSV row '\" + new String(message) + \"'.\", t);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic boolean isEndOfStream(RowData nextElement) {\n+\t\treturn false;\n+\t}\n+\n+\t@Override\n+\tpublic TypeInformation<RowData> getProducedType() {\n+\t\treturn resultTypeInfo;\n+\t}\n+\n+\t@Override\n+\tpublic boolean equals(Object o) {\n+\t\tif (this == o) {\n+\t\t\treturn true;\n+\t\t}\n+\t\tif (o == null || o.getClass() != this.getClass()) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tfinal CsvRowDataDeserializationSchema that = (CsvRowDataDeserializationSchema) o;\n+\t\tfinal CsvSchema otherSchema = that.csvSchema;\n+\n+\t\treturn resultTypeInfo.equals(that.resultTypeInfo) &&\n+\t\t\tignoreParseErrors == that.ignoreParseErrors &&\n+\t\t\tcsvSchema.getColumnSeparator() == otherSchema.getColumnSeparator() &&\n+\t\t\tcsvSchema.allowsComments() == otherSchema.allowsComments() &&\n+\t\t\tcsvSchema.getArrayElementSeparator().equals(otherSchema.getArrayElementSeparator()) &&\n+\t\t\tcsvSchema.getQuoteChar() == otherSchema.getQuoteChar() &&\n+\t\t\tcsvSchema.getEscapeChar() == otherSchema.getEscapeChar() &&\n+\t\t\tArrays.equals(csvSchema.getNullValue(), otherSchema.getNullValue());\n+\t}\n+\n+\t@Override\n+\tpublic int hashCode() {\n+\t\treturn Objects.hash(\n+\t\t\tresultTypeInfo,\n+\t\t\tignoreParseErrors,\n+\t\t\tcsvSchema.getColumnSeparator(),\n+\t\t\tcsvSchema.allowsComments(),\n+\t\t\tcsvSchema.getArrayElementSeparator(),\n+\t\t\tcsvSchema.getQuoteChar(),\n+\t\t\tcsvSchema.getEscapeChar(),\n+\t\t\tcsvSchema.getNullValue());\n+\t}\n+\n+\t// -------------------------------------------------------------------------------------\n+\t// Runtime Converters\n+\t// -------------------------------------------------------------------------------------\n+\n+\t// -------------------------------------------------------------------------------------\n+\t// Runtime Converters\n+\t// -------------------------------------------------------------------------------------\n+\n+\t/**\n+\t * Runtime converter that converts {@link JsonNode}s into objects of Flink Table & SQL\n+\t * internal data structures.\n+\t */\n+\t@FunctionalInterface\n+\tprivate interface DeserializationRuntimeConverter extends Serializable {\n+\t\tObject convert(JsonNode jsonNode);\n+\t}\n+\n+\tprivate DeserializationRuntimeConverter createRowConverter(RowType rowType, boolean isTopLevel) {\n+\t\tfinal DeserializationRuntimeConverter[] fieldConverters = rowType.getFields().stream()\n+\t\t\t.map(RowType.RowField::getType)\n+\t\t\t.map(this::createNullableConverter)\n+\t\t\t.toArray(DeserializationRuntimeConverter[]::new);\n+\t\tfinal String[] fieldNames = rowType.getFieldNames().toArray(new String[0]);\n+\t\tfinal int arity = fieldNames.length;\n+\n+\t\treturn jsonNode -> {\n+\t\t\tint nodeSize = jsonNode.size();\n+\n+\t\t\tvalidateArity(arity, nodeSize, ignoreParseErrors);\n+\n+\t\t\tGenericRowData row = new GenericRowData(arity);\n+\t\t\tfor (int i = 0; i < arity; i++) {\n+\t\t\t\tJsonNode field;\n+\t\t\t\t// Jackson only supports mapping by name in the first level\n+\t\t\t\tif (isTopLevel) {\n+\t\t\t\t\tfield = jsonNode.get(fieldNames[i]);\n+\t\t\t\t} else {\n+\t\t\t\t\tfield = jsonNode.get(i);\n+\t\t\t\t}\n+\t\t\t\tif (field == null) {\n+\t\t\t\t\trow.setField(i, null);\n+\t\t\t\t} else {\n+\t\t\t\t\trow.setField(i, fieldConverters[i].convert(field));\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\treturn row;\n+\t\t};\n+\t}\n+\n+\t/**\n+\t * Creates a runtime converter which is null safe.\n+\t */\n+\tprivate DeserializationRuntimeConverter createNullableConverter(LogicalType type) {\n+\t\tfinal DeserializationRuntimeConverter converter = createConverter(type);\n+\t\treturn jsonNode -> {\n+\t\t\tif (jsonNode == null || jsonNode.isNull()) {\n+\t\t\t\treturn null;\n+\t\t\t}\n+\t\t\ttry {\n+\t\t\t\treturn converter.convert(jsonNode);\n+\t\t\t} catch (Throwable t) {\n+\t\t\t\tif (!ignoreParseErrors) {\n+\t\t\t\t\tthrow t;\n+\t\t\t\t}\n+\t\t\t\treturn null;\n+\t\t\t}\n+\t\t};\n+\t}\n+\n+\t/**\n+\t * Creates a runtime converter which assuming input object is not null.\n+\t */\n+\tprivate DeserializationRuntimeConverter createConverter(LogicalType type) {\n+\t\tswitch (type.getTypeRoot()) {\n+\t\t\tcase NULL:\n+\t\t\t\treturn jsonNode -> null;\n+\t\t\tcase BOOLEAN:\n+\t\t\t\treturn this::convertToBoolean;\n+\t\t\tcase TINYINT:\n+\t\t\t\treturn jsonNode -> Byte.parseByte(jsonNode.asText().trim());\n+\t\t\tcase SMALLINT:\n+\t\t\t\treturn jsonNode -> Short.parseShort(jsonNode.asText().trim());\n+\t\t\tcase INTEGER:\n+\t\t\tcase INTERVAL_YEAR_MONTH:\n+\t\t\t\treturn this::convertToInt;\n+\t\t\tcase BIGINT:\n+\t\t\tcase INTERVAL_DAY_TIME:\n+\t\t\t\treturn this::convertToLong;\n+\t\t\tcase DATE:\n+\t\t\t\treturn this::convertToDate;\n+\t\t\tcase TIME_WITHOUT_TIME_ZONE:\n+\t\t\t\treturn this::convertToTime;\n+\t\t\tcase TIMESTAMP_WITH_TIME_ZONE:\n+\t\t\tcase TIMESTAMP_WITHOUT_TIME_ZONE:\n+\t\t\t\treturn this::convertToTimestamp;\n+\t\t\tcase FLOAT:\n+\t\t\t\treturn this::convertToFloat;\n+\t\t\tcase DOUBLE:\n+\t\t\t\treturn this::convertToDouble;\n+\t\t\tcase CHAR:\n+\t\t\tcase VARCHAR:\n+\t\t\t\treturn this::convertToString;\n+\t\t\tcase BINARY:\n+\t\t\tcase VARBINARY:\n+\t\t\t\treturn this::convertToBytes;\n+\t\t\tcase DECIMAL:\n+\t\t\t\treturn createDecimalConverter((DecimalType) type);\n+\t\t\tcase ARRAY:\n+\t\t\t\treturn createArrayConverter((ArrayType) type);\n+\t\t\tcase ROW:\n+\t\t\t\treturn createRowConverter((RowType) type, false);\n+\t\t\tcase MAP:\n+\t\t\tcase MULTISET:\n+\t\t\tcase RAW:\n+\t\t\tdefault:\n+\t\t\t\tthrow new UnsupportedOperationException(\"Unsupported type: \" + type);\n+\t\t}\n+\t}\n+\n+\tprivate boolean convertToBoolean(JsonNode jsonNode) {\n+\t\tif (jsonNode.isBoolean()) {\n+\t\t\t// avoid redundant toString and parseBoolean, for better performance", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "98481e8219841500289cabd8d3e7b1b0787207ea"}, "originalPosition": 341}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODAwMTk2OQ==", "bodyText": "Loose mills precision here, use toNanoOfDay?\nBTW, add tests for this.", "url": "https://github.com/apache/flink/pull/11962#discussion_r418001969", "createdAt": "2020-04-30T13:18:12Z", "author": {"login": "JingsongLi"}, "path": "flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvRowDataDeserializationSchema.java", "diffHunk": "@@ -0,0 +1,458 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.csv;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.common.serialization.DeserializationSchema;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.GenericArrayData;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.DecimalType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.utils.LogicalTypeUtils;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.JsonNode;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ObjectReader;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.node.ArrayNode;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvMapper;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvSchema;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.lang.reflect.Array;\n+import java.math.BigDecimal;\n+import java.sql.Date;\n+import java.sql.Time;\n+import java.sql.Timestamp;\n+import java.time.LocalTime;\n+import java.util.Arrays;\n+import java.util.Objects;\n+\n+/**\n+ * Deserialization schema from CSV to Flink Table & SQL internal data structures.\n+ *\n+ * <p>Deserializes a <code>byte[]</code> message as a {@link JsonNode} and\n+ * converts it to {@link RowData}.\n+ *\n+ * <p>Failure during deserialization are forwarded as wrapped {@link IOException}s.\n+ */\n+@Internal\n+public final class CsvRowDataDeserializationSchema implements DeserializationSchema<RowData> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\t/** Type information describing the result type. */\n+\tprivate final TypeInformation<RowData> resultTypeInfo;\n+\n+\t/** Runtime instance that performs the actual work. */\n+\tprivate final DeserializationRuntimeConverter runtimeConverter;\n+\n+\t/** Schema describing the input CSV data. */\n+\tprivate final CsvSchema csvSchema;\n+\n+\t/** Object reader used to read rows. It is configured by {@link CsvSchema}. */\n+\tprivate final ObjectReader objectReader;\n+\n+\t/** Flag indicating whether to ignore invalid fields/rows (default: throw an exception). */\n+\tprivate final boolean ignoreParseErrors;\n+\n+\tprivate CsvRowDataDeserializationSchema(\n+\t\t\tRowType rowType,\n+\t\t\tTypeInformation<RowData> resultTypeInfo,\n+\t\t\tCsvSchema csvSchema,\n+\t\t\tboolean ignoreParseErrors) {\n+\t\tthis.resultTypeInfo = resultTypeInfo;\n+\t\tthis.runtimeConverter = createRowConverter(rowType, true);\n+\t\tthis.csvSchema = CsvRowSchemaConverter.convert(rowType);\n+\t\tthis.objectReader = new CsvMapper().readerFor(JsonNode.class).with(csvSchema);\n+\t\tthis.ignoreParseErrors = ignoreParseErrors;\n+\t}\n+\n+\t/**\n+\t * A builder for creating a {@link CsvRowDataDeserializationSchema}.\n+\t */\n+\t@Internal\n+\tpublic static class Builder {\n+\n+\t\tprivate final RowType rowType;\n+\t\tprivate final TypeInformation<RowData> resultTypeInfo;\n+\t\tprivate CsvSchema csvSchema;\n+\t\tprivate boolean ignoreParseErrors;\n+\n+\t\t/**\n+\t\t * Creates a CSV deserialization schema for the given {@link TypeInformation} with\n+\t\t * optional parameters.\n+\t\t */\n+\t\tpublic Builder(RowType rowType, TypeInformation<RowData> resultTypeInfo) {\n+\t\t\tPreconditions.checkNotNull(rowType, \"RowType must not be null.\");\n+\t\t\tPreconditions.checkNotNull(resultTypeInfo, \"Result type information must not be null.\");\n+\t\t\tthis.rowType = rowType;\n+\t\t\tthis.resultTypeInfo = resultTypeInfo;\n+\t\t\tthis.csvSchema = CsvRowSchemaConverter.convert(rowType);\n+\t\t}\n+\n+\t\tpublic Builder setFieldDelimiter(char delimiter) {\n+\t\t\tthis.csvSchema = this.csvSchema.rebuild().setColumnSeparator(delimiter).build();\n+\t\t\treturn this;\n+\t\t}\n+\n+\t\tpublic Builder setAllowComments(boolean allowComments) {\n+\t\t\tthis.csvSchema = this.csvSchema.rebuild().setAllowComments(allowComments).build();\n+\t\t\treturn this;\n+\t\t}\n+\n+\t\tpublic Builder setArrayElementDelimiter(String delimiter) {\n+\t\t\tPreconditions.checkNotNull(delimiter, \"Array element delimiter must not be null.\");\n+\t\t\tthis.csvSchema = this.csvSchema.rebuild().setArrayElementSeparator(delimiter).build();\n+\t\t\treturn this;\n+\t\t}\n+\n+\t\tpublic Builder setQuoteCharacter(char c) {\n+\t\t\tthis.csvSchema = this.csvSchema.rebuild().setQuoteChar(c).build();\n+\t\t\treturn this;\n+\t\t}\n+\n+\t\tpublic Builder setEscapeCharacter(char c) {\n+\t\t\tthis.csvSchema = this.csvSchema.rebuild().setEscapeChar(c).build();\n+\t\t\treturn this;\n+\t\t}\n+\n+\t\tpublic Builder setNullLiteral(String nullLiteral) {\n+\t\t\tPreconditions.checkNotNull(nullLiteral, \"Null literal must not be null.\");\n+\t\t\tthis.csvSchema = this.csvSchema.rebuild().setNullValue(nullLiteral).build();\n+\t\t\treturn this;\n+\t\t}\n+\n+\t\tpublic Builder setIgnoreParseErrors(boolean ignoreParseErrors) {\n+\t\t\tthis.ignoreParseErrors = ignoreParseErrors;\n+\t\t\treturn this;\n+\t\t}\n+\n+\t\tpublic CsvRowDataDeserializationSchema build() {\n+\t\t\treturn new CsvRowDataDeserializationSchema(\n+\t\t\t\trowType,\n+\t\t\t\tresultTypeInfo,\n+\t\t\t\tcsvSchema,\n+\t\t\t\tignoreParseErrors);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic RowData deserialize(byte[] message) throws IOException {\n+\t\ttry {\n+\t\t\tfinal JsonNode root = objectReader.readValue(message);\n+\t\t\treturn (RowData) runtimeConverter.convert(root);\n+\t\t} catch (Throwable t) {\n+\t\t\tif (ignoreParseErrors) {\n+\t\t\t\treturn null;\n+\t\t\t}\n+\t\t\tthrow new IOException(\"Failed to deserialize CSV row '\" + new String(message) + \"'.\", t);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic boolean isEndOfStream(RowData nextElement) {\n+\t\treturn false;\n+\t}\n+\n+\t@Override\n+\tpublic TypeInformation<RowData> getProducedType() {\n+\t\treturn resultTypeInfo;\n+\t}\n+\n+\t@Override\n+\tpublic boolean equals(Object o) {\n+\t\tif (this == o) {\n+\t\t\treturn true;\n+\t\t}\n+\t\tif (o == null || o.getClass() != this.getClass()) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tfinal CsvRowDataDeserializationSchema that = (CsvRowDataDeserializationSchema) o;\n+\t\tfinal CsvSchema otherSchema = that.csvSchema;\n+\n+\t\treturn resultTypeInfo.equals(that.resultTypeInfo) &&\n+\t\t\tignoreParseErrors == that.ignoreParseErrors &&\n+\t\t\tcsvSchema.getColumnSeparator() == otherSchema.getColumnSeparator() &&\n+\t\t\tcsvSchema.allowsComments() == otherSchema.allowsComments() &&\n+\t\t\tcsvSchema.getArrayElementSeparator().equals(otherSchema.getArrayElementSeparator()) &&\n+\t\t\tcsvSchema.getQuoteChar() == otherSchema.getQuoteChar() &&\n+\t\t\tcsvSchema.getEscapeChar() == otherSchema.getEscapeChar() &&\n+\t\t\tArrays.equals(csvSchema.getNullValue(), otherSchema.getNullValue());\n+\t}\n+\n+\t@Override\n+\tpublic int hashCode() {\n+\t\treturn Objects.hash(\n+\t\t\tresultTypeInfo,\n+\t\t\tignoreParseErrors,\n+\t\t\tcsvSchema.getColumnSeparator(),\n+\t\t\tcsvSchema.allowsComments(),\n+\t\t\tcsvSchema.getArrayElementSeparator(),\n+\t\t\tcsvSchema.getQuoteChar(),\n+\t\t\tcsvSchema.getEscapeChar(),\n+\t\t\tcsvSchema.getNullValue());\n+\t}\n+\n+\t// -------------------------------------------------------------------------------------\n+\t// Runtime Converters\n+\t// -------------------------------------------------------------------------------------\n+\n+\t// -------------------------------------------------------------------------------------\n+\t// Runtime Converters\n+\t// -------------------------------------------------------------------------------------\n+\n+\t/**\n+\t * Runtime converter that converts {@link JsonNode}s into objects of Flink Table & SQL\n+\t * internal data structures.\n+\t */\n+\t@FunctionalInterface\n+\tprivate interface DeserializationRuntimeConverter extends Serializable {\n+\t\tObject convert(JsonNode jsonNode);\n+\t}\n+\n+\tprivate DeserializationRuntimeConverter createRowConverter(RowType rowType, boolean isTopLevel) {\n+\t\tfinal DeserializationRuntimeConverter[] fieldConverters = rowType.getFields().stream()\n+\t\t\t.map(RowType.RowField::getType)\n+\t\t\t.map(this::createNullableConverter)\n+\t\t\t.toArray(DeserializationRuntimeConverter[]::new);\n+\t\tfinal String[] fieldNames = rowType.getFieldNames().toArray(new String[0]);\n+\t\tfinal int arity = fieldNames.length;\n+\n+\t\treturn jsonNode -> {\n+\t\t\tint nodeSize = jsonNode.size();\n+\n+\t\t\tvalidateArity(arity, nodeSize, ignoreParseErrors);\n+\n+\t\t\tGenericRowData row = new GenericRowData(arity);\n+\t\t\tfor (int i = 0; i < arity; i++) {\n+\t\t\t\tJsonNode field;\n+\t\t\t\t// Jackson only supports mapping by name in the first level\n+\t\t\t\tif (isTopLevel) {\n+\t\t\t\t\tfield = jsonNode.get(fieldNames[i]);\n+\t\t\t\t} else {\n+\t\t\t\t\tfield = jsonNode.get(i);\n+\t\t\t\t}\n+\t\t\t\tif (field == null) {\n+\t\t\t\t\trow.setField(i, null);\n+\t\t\t\t} else {\n+\t\t\t\t\trow.setField(i, fieldConverters[i].convert(field));\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\treturn row;\n+\t\t};\n+\t}\n+\n+\t/**\n+\t * Creates a runtime converter which is null safe.\n+\t */\n+\tprivate DeserializationRuntimeConverter createNullableConverter(LogicalType type) {\n+\t\tfinal DeserializationRuntimeConverter converter = createConverter(type);\n+\t\treturn jsonNode -> {\n+\t\t\tif (jsonNode == null || jsonNode.isNull()) {\n+\t\t\t\treturn null;\n+\t\t\t}\n+\t\t\ttry {\n+\t\t\t\treturn converter.convert(jsonNode);\n+\t\t\t} catch (Throwable t) {\n+\t\t\t\tif (!ignoreParseErrors) {\n+\t\t\t\t\tthrow t;\n+\t\t\t\t}\n+\t\t\t\treturn null;\n+\t\t\t}\n+\t\t};\n+\t}\n+\n+\t/**\n+\t * Creates a runtime converter which assuming input object is not null.\n+\t */\n+\tprivate DeserializationRuntimeConverter createConverter(LogicalType type) {\n+\t\tswitch (type.getTypeRoot()) {\n+\t\t\tcase NULL:\n+\t\t\t\treturn jsonNode -> null;\n+\t\t\tcase BOOLEAN:\n+\t\t\t\treturn this::convertToBoolean;\n+\t\t\tcase TINYINT:\n+\t\t\t\treturn jsonNode -> Byte.parseByte(jsonNode.asText().trim());\n+\t\t\tcase SMALLINT:\n+\t\t\t\treturn jsonNode -> Short.parseShort(jsonNode.asText().trim());\n+\t\t\tcase INTEGER:\n+\t\t\tcase INTERVAL_YEAR_MONTH:\n+\t\t\t\treturn this::convertToInt;\n+\t\t\tcase BIGINT:\n+\t\t\tcase INTERVAL_DAY_TIME:\n+\t\t\t\treturn this::convertToLong;\n+\t\t\tcase DATE:\n+\t\t\t\treturn this::convertToDate;\n+\t\t\tcase TIME_WITHOUT_TIME_ZONE:\n+\t\t\t\treturn this::convertToTime;\n+\t\t\tcase TIMESTAMP_WITH_TIME_ZONE:\n+\t\t\tcase TIMESTAMP_WITHOUT_TIME_ZONE:\n+\t\t\t\treturn this::convertToTimestamp;\n+\t\t\tcase FLOAT:\n+\t\t\t\treturn this::convertToFloat;\n+\t\t\tcase DOUBLE:\n+\t\t\t\treturn this::convertToDouble;\n+\t\t\tcase CHAR:\n+\t\t\tcase VARCHAR:\n+\t\t\t\treturn this::convertToString;\n+\t\t\tcase BINARY:\n+\t\t\tcase VARBINARY:\n+\t\t\t\treturn this::convertToBytes;\n+\t\t\tcase DECIMAL:\n+\t\t\t\treturn createDecimalConverter((DecimalType) type);\n+\t\t\tcase ARRAY:\n+\t\t\t\treturn createArrayConverter((ArrayType) type);\n+\t\t\tcase ROW:\n+\t\t\t\treturn createRowConverter((RowType) type, false);\n+\t\t\tcase MAP:\n+\t\t\tcase MULTISET:\n+\t\t\tcase RAW:\n+\t\t\tdefault:\n+\t\t\t\tthrow new UnsupportedOperationException(\"Unsupported type: \" + type);\n+\t\t}\n+\t}\n+\n+\tprivate boolean convertToBoolean(JsonNode jsonNode) {\n+\t\tif (jsonNode.isBoolean()) {\n+\t\t\t// avoid redundant toString and parseBoolean, for better performance\n+\t\t\treturn jsonNode.asBoolean();\n+\t\t} else {\n+\t\t\treturn Boolean.parseBoolean(jsonNode.asText().trim());\n+\t\t}\n+\t}\n+\n+\tprivate int convertToInt(JsonNode jsonNode) {\n+\t\tif (jsonNode.canConvertToInt()) {\n+\t\t\t// avoid redundant toString and parseInt, for better performance\n+\t\t\treturn jsonNode.asInt();\n+\t\t} else {\n+\t\t\treturn Integer.parseInt(jsonNode.asText().trim());\n+\t\t}\n+\t}\n+\n+\tprivate long convertToLong(JsonNode jsonNode) {\n+\t\tif (jsonNode.canConvertToLong()) {\n+\t\t\t// avoid redundant toString and parseLong, for better performance\n+\t\t\treturn jsonNode.asLong();\n+\t\t} else {\n+\t\t\treturn Long.parseLong(jsonNode.asText().trim());\n+\t\t}\n+\t}\n+\n+\tprivate double convertToDouble(JsonNode jsonNode) {\n+\t\tif (jsonNode.isDouble()) {\n+\t\t\t// avoid redundant toString and parseDouble, for better performance\n+\t\t\treturn jsonNode.asDouble();\n+\t\t} else {\n+\t\t\treturn Double.parseDouble(jsonNode.asText().trim());\n+\t\t}\n+\t}\n+\n+\tprivate float convertToFloat(JsonNode jsonNode) {\n+\t\tif (jsonNode.isDouble()) {\n+\t\t\t// avoid redundant toString and parseDouble, for better performance\n+\t\t\treturn (float) jsonNode.asDouble();\n+\t\t} else {\n+\t\t\treturn Float.parseFloat(jsonNode.asText().trim());\n+\t\t}\n+\t}\n+\n+\tprivate int convertToDate(JsonNode jsonNode) {\n+\t\t// csv currently is using Date.valueOf() to parse date string\n+\t\treturn (int) Date.valueOf(jsonNode.asText()).toLocalDate().toEpochDay();\n+\t}\n+\n+\tprivate int convertToTime(JsonNode jsonNode) {\n+\t\t// csv currently is using Time.valueOf() to parse time string\n+\t\tLocalTime localTime = Time.valueOf(jsonNode.asText()).toLocalTime();\n+\t\t// get number of milliseconds of the day\n+\t\treturn localTime.toSecondOfDay() * 1000;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "98481e8219841500289cabd8d3e7b1b0787207ea"}, "originalPosition": 393}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODAxMDg4OQ==", "bodyText": "I think you should use something like ParquetRowDataWriter.FieldWriter.", "url": "https://github.com/apache/flink/pull/11962#discussion_r418010889", "createdAt": "2020-04-30T13:31:13Z", "author": {"login": "JingsongLi"}, "path": "flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvRowDataSerializationSchema.java", "diffHunk": "@@ -0,0 +1,377 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.csv;\n+\n+import org.apache.flink.annotation.PublicEvolving;\n+import org.apache.flink.api.common.serialization.SerializationSchema;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.JsonNode;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ObjectWriter;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.node.ArrayNode;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.node.ContainerNode;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.node.ObjectNode;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvMapper;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvSchema;\n+\n+import java.io.Serializable;\n+import java.math.BigDecimal;\n+import java.time.LocalDate;\n+import java.time.LocalTime;\n+import java.time.format.DateTimeFormatter;\n+import java.time.format.DateTimeFormatterBuilder;\n+import java.util.Arrays;\n+import java.util.Objects;\n+\n+import static java.time.format.DateTimeFormatter.ISO_LOCAL_DATE;\n+import static java.time.format.DateTimeFormatter.ISO_LOCAL_TIME;\n+\n+/**\n+ * Serialization schema that serializes an object of Flink Table & SQL internal data structure\n+ * into a CSV bytes.\n+ *\n+ * <p>Serializes the input row into a {@link JsonNode} and\n+ * converts it into <code>byte[]</code>.\n+ *\n+ * <p>Result <code>byte[]</code> messages can be deserialized using {@link CsvRowDataDeserializationSchema}.\n+ */\n+@PublicEvolving\n+public final class CsvRowDataSerializationSchema implements SerializationSchema<RowData> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\t/** Logical row type describing the input CSV data. */\n+\tprivate final RowType rowType;\n+\n+\t/** Runtime instance that performs the actual work. */\n+\tprivate final SerializationRuntimeConverter runtimeConverter;\n+\n+\t/** CsvMapper used to write {@link JsonNode} into bytes. */\n+\tprivate final CsvMapper csvMapper;\n+\n+\t/** Schema describing the input CSV data. */\n+\tprivate final CsvSchema csvSchema;\n+\n+\t/** Object writer used to write rows. It is configured by {@link CsvSchema}. */\n+\tprivate final ObjectWriter objectWriter;\n+\n+\t/** Reusable object node. */\n+\tprivate transient ObjectNode root;\n+\n+\tprivate CsvRowDataSerializationSchema(\n+\t\t\tRowType rowType,\n+\t\t\tCsvSchema csvSchema) {\n+\t\tthis.rowType = rowType;\n+\t\tthis.runtimeConverter = createRowConverter(rowType, true);\n+\t\tthis.csvMapper = new CsvMapper();\n+\t\tthis.csvSchema = csvSchema;\n+\t\tthis.objectWriter = csvMapper.writer(csvSchema);\n+\t}\n+\n+\t/**\n+\t * A builder for creating a {@link CsvRowDataSerializationSchema}.\n+\t */\n+\t@PublicEvolving\n+\tpublic static class Builder {\n+\n+\t\tprivate final RowType rowType;\n+\t\tprivate CsvSchema csvSchema;\n+\n+\t\t/**\n+\t\t * Creates a {@link CsvRowDataSerializationSchema} expecting the given {@link RowType}.\n+\t\t *\n+\t\t * @param rowType logical row type used to create schema.\n+\t\t */\n+\t\tpublic Builder(RowType rowType) {\n+\t\t\tPreconditions.checkNotNull(rowType, \"Row type must not be null.\");\n+\n+\t\t\tthis.rowType = rowType;\n+\t\t\tthis.csvSchema = CsvRowSchemaConverter.convert(rowType);\n+\t\t}\n+\n+\t\tpublic Builder setFieldDelimiter(char c) {\n+\t\t\tthis.csvSchema = this.csvSchema.rebuild().setColumnSeparator(c).build();\n+\t\t\treturn this;\n+\t\t}\n+\n+\t\tpublic Builder setLineDelimiter(String delimiter) {\n+\t\t\tPreconditions.checkNotNull(delimiter, \"Delimiter must not be null.\");\n+\t\t\tif (!delimiter.equals(\"\\n\") && !delimiter.equals(\"\\r\") && !delimiter.equals(\"\\r\\n\") && !delimiter.equals(\"\")) {\n+\t\t\t\tthrow new IllegalArgumentException(\n+\t\t\t\t\t\"Unsupported new line delimiter. Only \\\\n, \\\\r, \\\\r\\\\n, or empty string are supported.\");\n+\t\t\t}\n+\t\t\tthis.csvSchema = this.csvSchema.rebuild().setLineSeparator(delimiter).build();\n+\t\t\treturn this;\n+\t\t}\n+\n+\t\tpublic Builder setArrayElementDelimiter(String delimiter) {\n+\t\t\tPreconditions.checkNotNull(delimiter, \"Delimiter must not be null.\");\n+\t\t\tthis.csvSchema = this.csvSchema.rebuild().setArrayElementSeparator(delimiter).build();\n+\t\t\treturn this;\n+\t\t}\n+\n+\t\tpublic Builder disableQuoteCharacter() {\n+\t\t\tthis.csvSchema = this.csvSchema.rebuild().disableQuoteChar().build();\n+\t\t\treturn this;\n+\t\t}\n+\n+\t\tpublic Builder setQuoteCharacter(char c) {\n+\t\t\tthis.csvSchema = this.csvSchema.rebuild().setQuoteChar(c).build();\n+\t\t\treturn this;\n+\t\t}\n+\n+\t\tpublic Builder setEscapeCharacter(char c) {\n+\t\t\tthis.csvSchema = this.csvSchema.rebuild().setEscapeChar(c).build();\n+\t\t\treturn this;\n+\t\t}\n+\n+\t\tpublic Builder setNullLiteral(String s) {\n+\t\t\tthis.csvSchema = this.csvSchema.rebuild().setNullValue(s).build();\n+\t\t\treturn this;\n+\t\t}\n+\n+\t\tpublic CsvRowDataSerializationSchema build() {\n+\t\t\treturn new CsvRowDataSerializationSchema(\n+\t\t\t\trowType,\n+\t\t\t\tcsvSchema);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic byte[] serialize(RowData row) {\n+\t\tif (root == null) {\n+\t\t\troot = csvMapper.createObjectNode();\n+\t\t}\n+\t\ttry {\n+\t\t\truntimeConverter.convert(csvMapper, root, row);\n+\t\t\treturn objectWriter.writeValueAsBytes(root);\n+\t\t} catch (Throwable t) {\n+\t\t\tthrow new RuntimeException(\"Could not serialize row '\" + row + \"'.\", t);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic boolean equals(Object o) {\n+\t\tif (o == null || o.getClass() != this.getClass()) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tif (this == o) {\n+\t\t\treturn true;\n+\t\t}\n+\t\tfinal CsvRowDataSerializationSchema that = (CsvRowDataSerializationSchema) o;\n+\t\tfinal CsvSchema otherSchema = that.csvSchema;\n+\n+\t\treturn rowType.equals(that.rowType) &&\n+\t\t\tcsvSchema.getColumnSeparator() == otherSchema.getColumnSeparator() &&\n+\t\t\tArrays.equals(csvSchema.getLineSeparator(), otherSchema.getLineSeparator()) &&\n+\t\t\tcsvSchema.getArrayElementSeparator().equals(otherSchema.getArrayElementSeparator()) &&\n+\t\t\tcsvSchema.getQuoteChar() == otherSchema.getQuoteChar() &&\n+\t\t\tcsvSchema.getEscapeChar() == otherSchema.getEscapeChar() &&\n+\t\t\tArrays.equals(csvSchema.getNullValue(), otherSchema.getNullValue());\n+\t}\n+\n+\t@Override\n+\tpublic int hashCode() {\n+\t\treturn Objects.hash(\n+\t\t\trowType,\n+\t\t\tcsvSchema.getColumnSeparator(),\n+\t\t\tcsvSchema.getLineSeparator(),\n+\t\t\tcsvSchema.getArrayElementSeparator(),\n+\t\t\tcsvSchema.getQuoteChar(),\n+\t\t\tcsvSchema.getEscapeChar(),\n+\t\t\tcsvSchema.getNullValue());\n+\t}\n+\n+\t// --------------------------------------------------------------------------------\n+\t// Runtime Converters\n+\t// --------------------------------------------------------------------------------\n+\n+\t/**\n+\t * Runtime converter that converts objects of Flink Table & SQL internal data structures\n+\t * to corresponding {@link JsonNode}s.\n+\t */\n+\tprivate interface SerializationRuntimeConverter extends Serializable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "98481e8219841500289cabd8d3e7b1b0787207ea"}, "originalPosition": 216}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "676245d476a27b37ecb0701333ff5e410a5f44c0", "author": {"user": {"login": "wuchong", "name": "Jark Wu"}}, "url": "https://github.com/apache/flink/commit/676245d476a27b37ecb0701333ff5e410a5f44c0", "committedDate": "2020-05-05T14:35:18Z", "message": "Use getters instead of RowData.get() utility in serialize schema"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDA2MjczMTk2", "url": "https://github.com/apache/flink/pull/11962#pullrequestreview-406273196", "createdAt": "2020-05-06T02:38:16Z", "commit": {"oid": "676245d476a27b37ecb0701333ff5e410a5f44c0"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4467, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}