{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDAzNzM4NDU4", "number": 11755, "reviewThreads": {"totalCount": 14, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwNDo0Mzo1NVrOD0jmsA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQwMjoxOTozNlrOD7d5Ag==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU2NDM1ODg4OnYy", "diffSide": "RIGHT", "path": "flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/BaseRowCsvInputformatTest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwNDo0Mzo1NVrOGJjG5A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwNDo0Mzo1NVrOGJjG5A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY2NTU3Mg==", "bodyText": "Can we abstract RowCsvInputFormatTest to reuse code?", "url": "https://github.com/apache/flink/pull/11755#discussion_r412665572", "createdAt": "2020-04-22T04:43:55Z", "author": {"login": "JingsongLi"}, "path": "flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/BaseRowCsvInputformatTest.java", "diffHunk": "@@ -0,0 +1,798 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.csv;\n+\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.api.DataTypes;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.dataformat.BaseRow;\n+import org.apache.flink.table.dataformat.GenericRow;\n+import org.apache.flink.table.dataformat.SqlTimestamp;\n+import org.apache.flink.table.dataformat.TypeGetterSetters;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+\n+import org.junit.Test;\n+\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStreamWriter;\n+import java.nio.charset.StandardCharsets;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+import static junit.framework.TestCase.assertEquals;\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.Assert.assertNull;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n+\n+/**\n+ * Test suites for {@link BaseRowCsvInputformat}.\n+ */\n+public class BaseRowCsvInputformatTest {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "00fde88b6f1a151b95301a739699cbb41964d4bd"}, "originalPosition": 55}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU2NDM2MDIzOnYy", "diffSide": "RIGHT", "path": "flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/BaseRowCsvFilesystemITCase.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwNDo0NDoyOFrOGJjHkA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwNDo0NDoyOFrOGJjHkA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY2NTc0NA==", "bodyText": "After #11796 , please add streaming test too.", "url": "https://github.com/apache/flink/pull/11755#discussion_r412665744", "createdAt": "2020-04-22T04:44:28Z", "author": {"login": "JingsongLi"}, "path": "flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/BaseRowCsvFilesystemITCase.java", "diffHunk": "@@ -0,0 +1,41 @@\n+package org.apache.flink.formats.csv;\n+\n+import org.apache.flink.table.planner.runtime.batch.sql.BatchFileSystemITCaseBase;\n+\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.List;\n+\n+/**\n+ * Test fot {@link BaseRowCsvFileSystemFormatFactory}.\n+ */\n+@RunWith(Parameterized.class)\n+public class BaseRowCsvFilesystemITCase extends BatchFileSystemITCaseBase {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "00fde88b6f1a151b95301a739699cbb41964d4bd"}, "originalPosition": 17}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU2NDM2NjczOnYy", "diffSide": "RIGHT", "path": "flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/BaseRowCsvDeSerializationTest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwNDo0NzoxOFrOGJjLBg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwNDo0NzoxOFrOGJjLBg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY2NjYzMA==", "bodyText": "This class is for single row test? I think we don't need test single row, should be include in CsvRowDeSerializationSchemaTest.\nWe should add multi-rows test, or just add tests in RowCsvFilesystemITCase.java", "url": "https://github.com/apache/flink/pull/11755#discussion_r412666630", "createdAt": "2020-04-22T04:47:18Z", "author": {"login": "JingsongLi"}, "path": "flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/BaseRowCsvDeSerializationTest.java", "diffHunk": "@@ -0,0 +1,306 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.csv;\n+\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.api.DataTypes;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.dataformat.BaseRow;\n+import org.apache.flink.table.dataformat.DataFormatConverters;\n+import org.apache.flink.table.dataformat.GenericRow;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.util.FileUtils;\n+import org.apache.flink.util.TestLogger;\n+\n+import org.junit.Assert;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStreamWriter;\n+import java.math.BigDecimal;\n+import java.nio.charset.StandardCharsets;\n+import java.sql.Date;\n+import java.sql.Time;\n+import java.sql.Timestamp;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.util.ArrayList;\n+import java.util.function.Consumer;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNull;\n+import static org.junit.Assert.fail;\n+\n+/**\n+ * Test for {@link BaseRowCsvInputformat} and {@link BaseRowCsvEncoder}.\n+ */\n+public class BaseRowCsvDeSerializationTest extends TestLogger {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "00fde88b6f1a151b95301a739699cbb41964d4bd"}, "originalPosition": 62}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU2NDM2OTk3OnYy", "diffSide": "RIGHT", "path": "flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/BaseRowCsvInputformat.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwNDo0ODo1MFrOGJjM3Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwNDo0ODo1MFrOGJjM3Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY2NzEwMQ==", "bodyText": "use RowPartitionComputer.restorePartValueFromType instead", "url": "https://github.com/apache/flink/pull/11755#discussion_r412667101", "createdAt": "2020-04-22T04:48:50Z", "author": {"login": "JingsongLi"}, "path": "flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/BaseRowCsvInputformat.java", "diffHunk": "@@ -0,0 +1,310 @@\n+package org.apache.flink.formats.csv;\n+\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeinfo.Types;\n+import org.apache.flink.api.java.typeutils.RowTypeInfo;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.api.ValidationException;\n+import org.apache.flink.table.dataformat.BaseRow;\n+import org.apache.flink.table.dataformat.BinaryString;\n+import org.apache.flink.table.dataformat.DataFormatConverters;\n+import org.apache.flink.table.dataformat.GenericRow;\n+import org.apache.flink.table.filesystem.PartitionPathUtils;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.utils.TypeConversions;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.JsonNode;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.MappingIterator;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvMapper;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvSchema;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.NoSuchElementException;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.formats.csv.CsvRowDeserializationSchema.createFieldRuntimeConverters;\n+import static org.apache.flink.formats.csv.CsvRowDeserializationSchema.validateArity;\n+import static org.apache.flink.util.Preconditions.checkNotNull;\n+\n+/**\n+ * Input format that reads csv file into {@link BaseRow}.\n+ */\n+public class BaseRowCsvInputformat extends AbstractCsvInputFormat<BaseRow> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate final List<TypeInformation> fieldTypes;\n+\tprivate final List<String> fieldNames;\n+\tprivate final int[] selectFields;\n+\tprivate final List<String> partitionKeys;\n+\tprivate final String defaultPartValue;\n+\tprivate final boolean ignoreParseErrors;\n+\tprivate final long limit;\n+\tprivate final List<String> csvFieldNames;\n+\tprivate final List<TypeInformation> csvFieldTypes;\n+\tprivate final List<String> csvSelectFieldNames;\n+\tprivate final List<TypeInformation> csvSelectTypes;\n+\tprivate final int[] csvFieldMapping;\n+\n+\tprivate transient CsvRowDeserializationSchema.RuntimeConverter runtimeConverter;\n+\tprivate transient List<DataFormatConverters.DataFormatConverter> csvSelectConverters;\n+\tprivate transient MappingIterator<JsonNode> iterator;\n+\tprivate transient boolean end;\n+\tprivate transient long emitted;\n+\t// reuse object for per record\n+\tprivate transient GenericRow rowData;\n+\n+\tprivate BaseRowCsvInputformat(\n+\t\tPath[] filePaths,\n+\t\tCsvSchema csvSchema,\n+\t\tRowTypeInfo rowType,\n+\t\tint[] selectFields,\n+\t\tList<String> partitionKeys,\n+\t\tString defaultPartValue,\n+\t\tboolean ignoreParseErrors,\n+\t\tlong limit) {\n+\t\t\tsuper(filePaths, csvSchema);\n+\t\t\tthis.partitionKeys = partitionKeys;\n+\t\t\tthis.defaultPartValue = defaultPartValue;\n+\t\t\tthis.selectFields = selectFields;\n+\t\t\tthis.ignoreParseErrors = ignoreParseErrors;\n+\t\t\tthis.limit = limit;\n+\t\t\tthis.fieldTypes = Arrays.asList(rowType.getFieldTypes());\n+\t\t\tthis.fieldNames = Arrays.asList(rowType.getFieldNames());\n+\t\t\tthis.emitted = 0;\n+\t\t\t// partition field\n+\t\t\tthis.csvFieldNames = fieldNames.stream()\n+\t\t\t\t.filter(name -> !partitionKeys.contains(name)).collect(Collectors.toList());\n+\t\t\tthis.csvFieldTypes = csvFieldNames.stream()\n+\t\t\t.map(name -> fieldTypes.get(fieldNames.indexOf(name))).collect(Collectors.toList());\n+\t\t\t// project field\n+\t\t\tList<String> selectFieldNames = Arrays.stream(selectFields)\n+\t\t\t\t.mapToObj(fieldNames::get)\n+\t\t\t\t.collect(Collectors.toList());\n+\t\t\tthis.csvSelectFieldNames = selectFieldNames.stream()\n+\t\t\t\t.filter(name -> !partitionKeys.contains(name)).collect(Collectors.toList());\n+\t\t\tthis.csvSelectTypes = csvSelectFieldNames.stream()\n+\t\t\t\t.map(name -> fieldTypes.get(fieldNames.indexOf(name))).collect(Collectors.toList());\n+\t\t\tthis.csvFieldMapping = csvSelectFieldNames.stream().mapToInt(selectFieldNames::indexOf).toArray();\n+\t}\n+\n+\t@Override\n+\tpublic void open(FileInputSplit split) throws IOException {\n+\t\tsuper.open(split);\n+\t\tthis.end = false;\n+\t\tthis.iterator = new CsvMapper()\n+\t\t\t.readerFor(JsonNode.class)\n+\t\t\t.with(csvSchema)\n+\t\t\t.readValues(csvInputStream);\n+\t\tprepareRuntimeConverter();\n+\t\tfillPartitionValueForRecord();\n+\t}\n+\n+\tprivate void fillPartitionValueForRecord() {\n+\t\trowData = new GenericRow(selectFields.length);\n+\t\tPath path = currentSplit.getPath();\n+\t\tLinkedHashMap<String, String> partSpec = PartitionPathUtils.extractPartitionSpecFromPath(path);\n+\t\tfor (int i = 0; i < selectFields.length; i++) {\n+\t\t\tint selectField = selectFields[i];\n+\t\t\tString name = fieldNames.get(selectField);\n+\t\t\tif (partitionKeys.contains(name)) {\n+\t\t\t\tString value = partSpec.get(name);\n+\t\t\t\tvalue = defaultPartValue.equals(value) ? null : value;\n+\t\t\t\trowData.setField(i, convertStringToInternal(value, fieldTypes.get(selectField)));\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate Object convertStringToInternal(String value, TypeInformation type) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "00fde88b6f1a151b95301a739699cbb41964d4bd"}, "originalPosition": 125}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMDA3NzgzOnYy", "diffSide": "RIGHT", "path": "flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvRowDataFileSystemFormatFactory.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQwMjowODo0M1rOGS3w8Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQwMjowODo0M1rOGS3w8Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ0MTIwMQ==", "bodyText": "Add empty line.", "url": "https://github.com/apache/flink/pull/11755#discussion_r422441201", "createdAt": "2020-05-09T02:08:43Z", "author": {"login": "JingsongLi"}, "path": "flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvRowDataFileSystemFormatFactory.java", "diffHunk": "@@ -0,0 +1,354 @@\n+package org.apache.flink.formats.csv;\n+\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.GenericTypeInfo;\n+import org.apache.flink.api.java.typeutils.RowTypeInfo;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.api.ValidationException;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.FileSystemFormatFactory;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.utils.PartitionPathUtils;\n+\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvSchema;\n+\n+import java.io.BufferedReader;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.io.OutputStream;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ALLOW_COMMENTS;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ARRAY_ELEMENT_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_DISABLE_QUOTE_CHARACTER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ESCAPE_CHARACTER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_FIELD_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_IGNORE_PARSE_ERRORS;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_LINE_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_NULL_LITERAL;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_QUOTE_CHARACTER;\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+import static org.apache.flink.util.Preconditions.checkNotNull;\n+\n+/**\n+ * CSV format factory for file system.\n+ */\n+public class CsvRowDataFileSystemFormatFactory implements FileSystemFormatFactory {\n+\t@Override", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "64d407d3530d308b4504282ae8fadb1503955f99"}, "originalPosition": 52}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMDA3Nzg3OnYy", "diffSide": "RIGHT", "path": "flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvRowDataFileSystemFormatFactory.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQwMjowODo1MlrOGS3w-g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQwMjowODo1MlrOGS3w-g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ0MTIxMA==", "bodyText": "Remove empty line", "url": "https://github.com/apache/flink/pull/11755#discussion_r422441210", "createdAt": "2020-05-09T02:08:52Z", "author": {"login": "JingsongLi"}, "path": "flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvRowDataFileSystemFormatFactory.java", "diffHunk": "@@ -0,0 +1,354 @@\n+package org.apache.flink.formats.csv;\n+\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.GenericTypeInfo;\n+import org.apache.flink.api.java.typeutils.RowTypeInfo;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.api.ValidationException;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.FileSystemFormatFactory;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.utils.PartitionPathUtils;\n+\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvSchema;\n+\n+import java.io.BufferedReader;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.io.OutputStream;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ALLOW_COMMENTS;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ARRAY_ELEMENT_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_DISABLE_QUOTE_CHARACTER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ESCAPE_CHARACTER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_FIELD_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_IGNORE_PARSE_ERRORS;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_LINE_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_NULL_LITERAL;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_QUOTE_CHARACTER;\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+import static org.apache.flink.util.Preconditions.checkNotNull;\n+\n+/**\n+ * CSV format factory for file system.\n+ */\n+public class CsvRowDataFileSystemFormatFactory implements FileSystemFormatFactory {\n+\t@Override\n+\tpublic InputFormat<RowData, ?> createReader(ReaderContext context) {\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "64d407d3530d308b4504282ae8fadb1503955f99"}, "originalPosition": 54}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMjY2MzUwOnYy", "diffSide": "RIGHT", "path": "flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvRowDataFileSystemFormatFactory.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMVQwNDowMzoxNVrOGTLriw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMVQxMTo1MTozMVrOGTY7SA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjc2NzQ5OQ==", "bodyText": "return null after end = true. And remove !reachedEnd() in while.\nThis can make codes clear and correct emitted number.", "url": "https://github.com/apache/flink/pull/11755#discussion_r422767499", "createdAt": "2020-05-11T04:03:15Z", "author": {"login": "JingsongLi"}, "path": "flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvRowDataFileSystemFormatFactory.java", "diffHunk": "@@ -0,0 +1,352 @@\n+package org.apache.flink.formats.csv;\n+\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.api.java.typeutils.GenericTypeInfo;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.formats.csv.CsvRowDataDeserializationSchema.DeserializationRuntimeConverter;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.FileSystemFormatFactory;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.utils.PartitionPathUtils;\n+\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.JsonNode;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.MappingIterator;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvMapper;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvSchema;\n+\n+import java.io.BufferedReader;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.io.OutputStream;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NoSuchElementException;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ALLOW_COMMENTS;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ARRAY_ELEMENT_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_DISABLE_QUOTE_CHARACTER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ESCAPE_CHARACTER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_FIELD_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_IGNORE_PARSE_ERRORS;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_LINE_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_NULL_LITERAL;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_QUOTE_CHARACTER;\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+\n+/**\n+ * CSV format factory for file system.\n+ */\n+public class CsvRowDataFileSystemFormatFactory implements FileSystemFormatFactory {\n+\t@Override\n+\tpublic InputFormat<RowData, ?> createReader(ReaderContext context) {\n+\n+\t\tDescriptorProperties properties = getValidatedProperties(context.getFormatProperties());\n+\n+\t\tRowType formatRowType = context.getFormatRowType();\n+\n+\t\tString[] fieldNames = context.getSchema().getFieldNames();\n+\t\tList<String> projectFields = Arrays.stream(context.getProjectFields())\n+\t\t\t.mapToObj(idx -> fieldNames[idx])\n+\t\t\t.collect(Collectors.toList());\n+\t\tList<String> csvFields = Arrays.stream(fieldNames)\n+\t\t\t.filter(field -> !context.getPartitionKeys().contains(field))\n+\t\t\t.collect(Collectors.toList());\n+\n+\t\tint[] csvSelectFieldToProjectFieldMapping = context.getFormatProjectFields().stream()\n+\t\t\t.mapToInt(projectFields::indexOf)\n+\t\t\t.toArray();\n+\t\tint[] csvSelectFieldToCsvFieldMapping = context.getFormatProjectFields().stream()\n+\t\t\t.mapToInt(csvFields::indexOf)\n+\t\t\t.toArray();\n+\n+\t\tCsvSchema csvSchema = buildCsvSchema(formatRowType, properties);\n+\n+\t\tboolean ignoreParseErrors = properties.getOptionalBoolean(FORMAT_IGNORE_PARSE_ERRORS).orElse(false);\n+\n+\t\treturn new CsvInputFormat(\n+\t\t\tcontext.getPaths(),\n+\t\t\tcontext.getSchema().getFieldDataTypes(),\n+\t\t\tcontext.getSchema().getFieldNames(),\n+\t\t\tcsvSchema,\n+\t\t\tformatRowType,\n+\t\t\tcontext.getProjectFields(),\n+\t\t\tcontext.getPartitionKeys(),\n+\t\t\tcontext.getDefaultPartName(),\n+\t\t\tcontext.getPushedDownLimit(),\n+\t\t\tcsvSelectFieldToProjectFieldMapping,\n+\t\t\tcsvSelectFieldToCsvFieldMapping,\n+\t\t\tignoreParseErrors);\n+\t}\n+\n+\tprivate CsvSchema buildCsvSchema(RowType rowType, DescriptorProperties properties) {\n+\t\tCsvSchema csvSchema = CsvRowSchemaConverter.convert(rowType);\n+\t\t//format properties\n+\t\tif (properties.containsKey(FORMAT_FIELD_DELIMITER)) {\n+\t\t\tcsvSchema = csvSchema.rebuild()\n+\t\t\t\t.setColumnSeparator(properties.getCharacter(FORMAT_FIELD_DELIMITER))\n+\t\t\t\t.build();\n+\t\t}\n+\t\tif (properties.containsKey(FORMAT_QUOTE_CHARACTER)) {\n+\t\t\tcsvSchema = csvSchema.rebuild()\n+\t\t\t\t.setQuoteChar(properties.getCharacter(FORMAT_QUOTE_CHARACTER))\n+\t\t\t\t.build();\n+\t\t}\n+\t\tif (properties.containsKey(FORMAT_ALLOW_COMMENTS)) {\n+\t\t\tcsvSchema = csvSchema.rebuild()\n+\t\t\t\t.setAllowComments(properties.getBoolean(FORMAT_ALLOW_COMMENTS))\n+\t\t\t\t.build();\n+\t\t}\n+\t\tif (properties.containsKey(FORMAT_ARRAY_ELEMENT_DELIMITER)) {\n+\t\t\tcsvSchema = csvSchema.rebuild()\n+\t\t\t\t.setArrayElementSeparator(properties.getString(FORMAT_ARRAY_ELEMENT_DELIMITER))\n+\t\t\t\t.build();\n+\t\t}\n+\t\tif (properties.containsKey(FORMAT_ESCAPE_CHARACTER)) {\n+\t\t\tcsvSchema = csvSchema.rebuild()\n+\t\t\t\t.setEscapeChar(properties.getCharacter(FORMAT_ESCAPE_CHARACTER))\n+\t\t\t\t.build();\n+\t\t}\n+\t\tif (properties.containsKey(FORMAT_NULL_LITERAL)) {\n+\t\t\tcsvSchema = csvSchema.rebuild()\n+\t\t\t\t.setNullValue(properties.getString(FORMAT_NULL_LITERAL))\n+\t\t\t\t.build();\n+\t\t}\n+\t\tif (properties.containsKey(FORMAT_LINE_DELIMITER)) {\n+\t\t\tcsvSchema = csvSchema.rebuild()\n+\t\t\t\t.setLineSeparator(properties.getString(FORMAT_LINE_DELIMITER))\n+\t\t\t\t.build();\n+\t\t}\n+\t\treturn csvSchema;\n+\t}\n+\n+\t@Override\n+\tpublic Optional<Encoder<RowData>> createEncoder(WriterContext context) {\n+\n+\t\tCsvRowDataSerializationSchema.Builder builder = new CsvRowDataSerializationSchema.Builder(\n+\t\t\tcontext.getFormatRowType());\n+\n+\t\tDescriptorProperties properties = getValidatedProperties(context.getFormatProperties());\n+\n+\t\tproperties.getOptionalCharacter(FORMAT_FIELD_DELIMITER)\n+\t\t\t.ifPresent(builder::setFieldDelimiter);\n+\n+\t\tproperties.getOptionalString(FORMAT_LINE_DELIMITER)\n+\t\t\t.ifPresent(builder::setLineDelimiter);\n+\n+\t\tif (properties.getOptionalBoolean(FORMAT_DISABLE_QUOTE_CHARACTER).orElse(false)) {\n+\t\t\tbuilder.disableQuoteCharacter();\n+\t\t} else {\n+\t\t\tproperties.getOptionalCharacter(FORMAT_QUOTE_CHARACTER).ifPresent(builder::setQuoteCharacter);\n+\t\t}\n+\n+\t\tproperties.getOptionalString(FORMAT_ARRAY_ELEMENT_DELIMITER)\n+\t\t\t.ifPresent(builder::setArrayElementDelimiter);\n+\n+\t\tproperties.getOptionalCharacter(FORMAT_ESCAPE_CHARACTER)\n+\t\t\t.ifPresent(builder::setEscapeCharacter);\n+\n+\t\tproperties.getOptionalString(FORMAT_NULL_LITERAL)\n+\t\t\t.ifPresent(builder::setNullLiteral);\n+\n+\t\treturn Optional.of(new CsvRowDataEncoder(builder.build()));\n+\t}\n+\n+\t@Override\n+\tpublic Optional<BulkWriter.Factory<RowData>> createBulkWriterFactory(WriterContext context) {\n+\t\treturn Optional.empty();\n+\t}\n+\n+\t@Override\n+\tpublic boolean supportsSchemaDerivation() {\n+\t\treturn true;\n+\t}\n+\n+\t@Override\n+\tpublic List<String> supportedProperties() {\n+\t\tfinal List<String> properties = new ArrayList<>();\n+\t\tproperties.add(FORMAT_FIELD_DELIMITER);\n+\t\tproperties.add(FORMAT_LINE_DELIMITER);\n+\t\tproperties.add(FORMAT_DISABLE_QUOTE_CHARACTER);\n+\t\tproperties.add(FORMAT_QUOTE_CHARACTER);\n+\t\tproperties.add(FORMAT_ALLOW_COMMENTS);\n+\t\tproperties.add(FORMAT_IGNORE_PARSE_ERRORS);\n+\t\tproperties.add(FORMAT_ARRAY_ELEMENT_DELIMITER);\n+\t\tproperties.add(FORMAT_ESCAPE_CHARACTER);\n+\t\tproperties.add(FORMAT_NULL_LITERAL);\n+\t\treturn properties;\n+\t}\n+\n+\t@Override\n+\tpublic Map<String, String> requiredContext() {\n+\t\tMap<String, String> context = new HashMap<>();\n+\t\tcontext.put(FORMAT, \"csv\");\n+\t\treturn context;\n+\t}\n+\n+\tprivate static DescriptorProperties getValidatedProperties(Map<String, String> propertiesMap) {\n+\t\tfinal DescriptorProperties properties = new DescriptorProperties(true);\n+\t\tproperties.putProperties(propertiesMap);\n+\n+\t\tproperties.validateString(FORMAT_FIELD_DELIMITER, true, 1, 1);\n+\t\tproperties.validateEnumValues(FORMAT_LINE_DELIMITER, true, Arrays.asList(\"\\r\", \"\\n\", \"\\r\\n\", \"\"));\n+\t\tproperties.validateBoolean(FORMAT_DISABLE_QUOTE_CHARACTER, true);\n+\t\tproperties.validateString(FORMAT_QUOTE_CHARACTER, true, 1, 1);\n+\t\tproperties.validateBoolean(FORMAT_ALLOW_COMMENTS, true);\n+\t\tproperties.validateBoolean(FORMAT_IGNORE_PARSE_ERRORS, true);\n+\t\tproperties.validateString(FORMAT_ARRAY_ELEMENT_DELIMITER, true, 1);\n+\t\tproperties.validateString(FORMAT_ESCAPE_CHARACTER, true, 1, 1);\n+\n+\t\treturn properties;\n+\t}\n+\n+\t/**\n+\t * InputFormat that reads csv record into {@link RowData}.\n+\t */\n+\tpublic static class CsvInputFormat extends AbstractCsvInputFormat<RowData> {\n+\t\tprivate static final long serialVersionUID = 1L;\n+\n+\t\tprivate final RowType formatRowType;\n+\t\tprivate final DataType[] fieldTypes;\n+\t\tprivate final String[] fieldNames;\n+\t\tprivate final int[] selectFields;\n+\t\tprivate final List<String> partitionKeys;\n+\t\tprivate final String defaultPartValue;\n+\t\tprivate final long limit;\n+\t\tprivate final int[] csvSelectFieldToProjectFieldMapping;\n+\t\tprivate final int[] csvSelectFieldToCsvFieldMapping;\n+\t\tprivate final boolean ignoreParseErrors;\n+\n+\t\tprivate transient InputStreamReader inputStreamReader;\n+\t\tprivate transient BufferedReader reader;\n+\t\tprivate transient boolean end;\n+\t\tprivate transient long emitted;\n+\t\t// reuse object for per record\n+\t\tprivate transient GenericRowData rowData;\n+\t\tprivate transient DeserializationRuntimeConverter runtimeConverter;\n+\t\tprivate transient MappingIterator<JsonNode> iterator;\n+\n+\t\tpublic CsvInputFormat(\n+\t\t\t\tPath[] filePaths,\n+\t\t\t\tDataType[] fieldTypes,\n+\t\t\t\tString[] fieldNames,\n+\t\t\t\tCsvSchema csvSchema,\n+\t\t\t\tRowType formatRowType,\n+\t\t\t\tint[] selectFields,\n+\t\t\t\tList<String> partitionKeys,\n+\t\t\t\tString defaultPartValue,\n+\t\t\t\tlong limit,\n+\t\t\t\tint[] csvSelectFieldToProjectFieldMapping,\n+\t\t\t\tint[] csvSelectFieldToCsvFieldMapping,\n+\t\t\t\tboolean ignoreParseErrors) {\n+\t\t\tsuper(filePaths, csvSchema);\n+\t\t\tthis.fieldTypes = fieldTypes;\n+\t\t\tthis.fieldNames = fieldNames;\n+\t\t\tthis.formatRowType = formatRowType;\n+\t\t\tthis.partitionKeys = partitionKeys;\n+\t\t\tthis.defaultPartValue = defaultPartValue;\n+\t\t\tthis.selectFields = selectFields;\n+\t\t\tthis.limit = limit;\n+\t\t\tthis.emitted = 0;\n+\t\t\tthis.csvSelectFieldToProjectFieldMapping = csvSelectFieldToProjectFieldMapping;\n+\t\t\tthis.csvSelectFieldToCsvFieldMapping = csvSelectFieldToCsvFieldMapping;\n+\t\t\tthis.ignoreParseErrors = ignoreParseErrors;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void open(FileInputSplit split) throws IOException {\n+\t\t\tsuper.open(split);\n+\t\t\tthis.end = false;\n+\t\t\tthis.inputStreamReader = new InputStreamReader(csvInputStream);\n+\t\t\tthis.reader = new BufferedReader(inputStreamReader);\n+\t\t\tthis.rowData = PartitionPathUtils.fillPartitionValueForRecord(fieldNames, fieldTypes, selectFields,\n+\t\t\t\tpartitionKeys, currentSplit.getPath(), defaultPartValue);\n+\t\t\tthis.iterator = new CsvMapper()\n+\t\t\t\t.readerFor(JsonNode.class)\n+\t\t\t\t.with(csvSchema)\n+\t\t\t\t.readValues(csvInputStream);\n+\t\t\tprepareRuntimeConverter();\n+\t\t}\n+\n+\t\tprivate void prepareRuntimeConverter(){\n+\t\t\tCsvRowDataDeserializationSchema.Builder builder = new CsvRowDataDeserializationSchema.Builder(\n+\t\t\t\tformatRowType, new GenericTypeInfo<>(RowData.class))\n+\t\t\t\t.setIgnoreParseErrors(ignoreParseErrors);\n+\t\t\tthis.runtimeConverter = builder.build().createRowConverter(formatRowType, true);\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic boolean reachedEnd() throws IOException {\n+\t\t\treturn emitted >= limit || end;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic RowData nextRecord(RowData reuse) throws IOException {\n+\t\t\tGenericRowData csvRow = null;\n+\t\t\tdo {\n+\t\t\t\ttry {\n+\t\t\t\t\tJsonNode root = iterator.nextValue();\n+\t\t\t\t\tcsvRow = (GenericRowData) runtimeConverter.convert(root);\n+\t\t\t\t} catch (NoSuchElementException e) {\n+\t\t\t\t\tend = true;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0fd33d570a8591ef7e5a523c29366a34aa6b986e"}, "originalPosition": 301}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjk4NDUyMA==", "bodyText": "I update the logic, please review again.", "url": "https://github.com/apache/flink/pull/11755#discussion_r422984520", "createdAt": "2020-05-11T11:51:31Z", "author": {"login": "leonardBang"}, "path": "flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvRowDataFileSystemFormatFactory.java", "diffHunk": "@@ -0,0 +1,352 @@\n+package org.apache.flink.formats.csv;\n+\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.api.java.typeutils.GenericTypeInfo;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.formats.csv.CsvRowDataDeserializationSchema.DeserializationRuntimeConverter;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.FileSystemFormatFactory;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.utils.PartitionPathUtils;\n+\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.JsonNode;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.MappingIterator;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvMapper;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvSchema;\n+\n+import java.io.BufferedReader;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.io.OutputStream;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NoSuchElementException;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ALLOW_COMMENTS;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ARRAY_ELEMENT_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_DISABLE_QUOTE_CHARACTER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ESCAPE_CHARACTER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_FIELD_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_IGNORE_PARSE_ERRORS;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_LINE_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_NULL_LITERAL;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_QUOTE_CHARACTER;\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+\n+/**\n+ * CSV format factory for file system.\n+ */\n+public class CsvRowDataFileSystemFormatFactory implements FileSystemFormatFactory {\n+\t@Override\n+\tpublic InputFormat<RowData, ?> createReader(ReaderContext context) {\n+\n+\t\tDescriptorProperties properties = getValidatedProperties(context.getFormatProperties());\n+\n+\t\tRowType formatRowType = context.getFormatRowType();\n+\n+\t\tString[] fieldNames = context.getSchema().getFieldNames();\n+\t\tList<String> projectFields = Arrays.stream(context.getProjectFields())\n+\t\t\t.mapToObj(idx -> fieldNames[idx])\n+\t\t\t.collect(Collectors.toList());\n+\t\tList<String> csvFields = Arrays.stream(fieldNames)\n+\t\t\t.filter(field -> !context.getPartitionKeys().contains(field))\n+\t\t\t.collect(Collectors.toList());\n+\n+\t\tint[] csvSelectFieldToProjectFieldMapping = context.getFormatProjectFields().stream()\n+\t\t\t.mapToInt(projectFields::indexOf)\n+\t\t\t.toArray();\n+\t\tint[] csvSelectFieldToCsvFieldMapping = context.getFormatProjectFields().stream()\n+\t\t\t.mapToInt(csvFields::indexOf)\n+\t\t\t.toArray();\n+\n+\t\tCsvSchema csvSchema = buildCsvSchema(formatRowType, properties);\n+\n+\t\tboolean ignoreParseErrors = properties.getOptionalBoolean(FORMAT_IGNORE_PARSE_ERRORS).orElse(false);\n+\n+\t\treturn new CsvInputFormat(\n+\t\t\tcontext.getPaths(),\n+\t\t\tcontext.getSchema().getFieldDataTypes(),\n+\t\t\tcontext.getSchema().getFieldNames(),\n+\t\t\tcsvSchema,\n+\t\t\tformatRowType,\n+\t\t\tcontext.getProjectFields(),\n+\t\t\tcontext.getPartitionKeys(),\n+\t\t\tcontext.getDefaultPartName(),\n+\t\t\tcontext.getPushedDownLimit(),\n+\t\t\tcsvSelectFieldToProjectFieldMapping,\n+\t\t\tcsvSelectFieldToCsvFieldMapping,\n+\t\t\tignoreParseErrors);\n+\t}\n+\n+\tprivate CsvSchema buildCsvSchema(RowType rowType, DescriptorProperties properties) {\n+\t\tCsvSchema csvSchema = CsvRowSchemaConverter.convert(rowType);\n+\t\t//format properties\n+\t\tif (properties.containsKey(FORMAT_FIELD_DELIMITER)) {\n+\t\t\tcsvSchema = csvSchema.rebuild()\n+\t\t\t\t.setColumnSeparator(properties.getCharacter(FORMAT_FIELD_DELIMITER))\n+\t\t\t\t.build();\n+\t\t}\n+\t\tif (properties.containsKey(FORMAT_QUOTE_CHARACTER)) {\n+\t\t\tcsvSchema = csvSchema.rebuild()\n+\t\t\t\t.setQuoteChar(properties.getCharacter(FORMAT_QUOTE_CHARACTER))\n+\t\t\t\t.build();\n+\t\t}\n+\t\tif (properties.containsKey(FORMAT_ALLOW_COMMENTS)) {\n+\t\t\tcsvSchema = csvSchema.rebuild()\n+\t\t\t\t.setAllowComments(properties.getBoolean(FORMAT_ALLOW_COMMENTS))\n+\t\t\t\t.build();\n+\t\t}\n+\t\tif (properties.containsKey(FORMAT_ARRAY_ELEMENT_DELIMITER)) {\n+\t\t\tcsvSchema = csvSchema.rebuild()\n+\t\t\t\t.setArrayElementSeparator(properties.getString(FORMAT_ARRAY_ELEMENT_DELIMITER))\n+\t\t\t\t.build();\n+\t\t}\n+\t\tif (properties.containsKey(FORMAT_ESCAPE_CHARACTER)) {\n+\t\t\tcsvSchema = csvSchema.rebuild()\n+\t\t\t\t.setEscapeChar(properties.getCharacter(FORMAT_ESCAPE_CHARACTER))\n+\t\t\t\t.build();\n+\t\t}\n+\t\tif (properties.containsKey(FORMAT_NULL_LITERAL)) {\n+\t\t\tcsvSchema = csvSchema.rebuild()\n+\t\t\t\t.setNullValue(properties.getString(FORMAT_NULL_LITERAL))\n+\t\t\t\t.build();\n+\t\t}\n+\t\tif (properties.containsKey(FORMAT_LINE_DELIMITER)) {\n+\t\t\tcsvSchema = csvSchema.rebuild()\n+\t\t\t\t.setLineSeparator(properties.getString(FORMAT_LINE_DELIMITER))\n+\t\t\t\t.build();\n+\t\t}\n+\t\treturn csvSchema;\n+\t}\n+\n+\t@Override\n+\tpublic Optional<Encoder<RowData>> createEncoder(WriterContext context) {\n+\n+\t\tCsvRowDataSerializationSchema.Builder builder = new CsvRowDataSerializationSchema.Builder(\n+\t\t\tcontext.getFormatRowType());\n+\n+\t\tDescriptorProperties properties = getValidatedProperties(context.getFormatProperties());\n+\n+\t\tproperties.getOptionalCharacter(FORMAT_FIELD_DELIMITER)\n+\t\t\t.ifPresent(builder::setFieldDelimiter);\n+\n+\t\tproperties.getOptionalString(FORMAT_LINE_DELIMITER)\n+\t\t\t.ifPresent(builder::setLineDelimiter);\n+\n+\t\tif (properties.getOptionalBoolean(FORMAT_DISABLE_QUOTE_CHARACTER).orElse(false)) {\n+\t\t\tbuilder.disableQuoteCharacter();\n+\t\t} else {\n+\t\t\tproperties.getOptionalCharacter(FORMAT_QUOTE_CHARACTER).ifPresent(builder::setQuoteCharacter);\n+\t\t}\n+\n+\t\tproperties.getOptionalString(FORMAT_ARRAY_ELEMENT_DELIMITER)\n+\t\t\t.ifPresent(builder::setArrayElementDelimiter);\n+\n+\t\tproperties.getOptionalCharacter(FORMAT_ESCAPE_CHARACTER)\n+\t\t\t.ifPresent(builder::setEscapeCharacter);\n+\n+\t\tproperties.getOptionalString(FORMAT_NULL_LITERAL)\n+\t\t\t.ifPresent(builder::setNullLiteral);\n+\n+\t\treturn Optional.of(new CsvRowDataEncoder(builder.build()));\n+\t}\n+\n+\t@Override\n+\tpublic Optional<BulkWriter.Factory<RowData>> createBulkWriterFactory(WriterContext context) {\n+\t\treturn Optional.empty();\n+\t}\n+\n+\t@Override\n+\tpublic boolean supportsSchemaDerivation() {\n+\t\treturn true;\n+\t}\n+\n+\t@Override\n+\tpublic List<String> supportedProperties() {\n+\t\tfinal List<String> properties = new ArrayList<>();\n+\t\tproperties.add(FORMAT_FIELD_DELIMITER);\n+\t\tproperties.add(FORMAT_LINE_DELIMITER);\n+\t\tproperties.add(FORMAT_DISABLE_QUOTE_CHARACTER);\n+\t\tproperties.add(FORMAT_QUOTE_CHARACTER);\n+\t\tproperties.add(FORMAT_ALLOW_COMMENTS);\n+\t\tproperties.add(FORMAT_IGNORE_PARSE_ERRORS);\n+\t\tproperties.add(FORMAT_ARRAY_ELEMENT_DELIMITER);\n+\t\tproperties.add(FORMAT_ESCAPE_CHARACTER);\n+\t\tproperties.add(FORMAT_NULL_LITERAL);\n+\t\treturn properties;\n+\t}\n+\n+\t@Override\n+\tpublic Map<String, String> requiredContext() {\n+\t\tMap<String, String> context = new HashMap<>();\n+\t\tcontext.put(FORMAT, \"csv\");\n+\t\treturn context;\n+\t}\n+\n+\tprivate static DescriptorProperties getValidatedProperties(Map<String, String> propertiesMap) {\n+\t\tfinal DescriptorProperties properties = new DescriptorProperties(true);\n+\t\tproperties.putProperties(propertiesMap);\n+\n+\t\tproperties.validateString(FORMAT_FIELD_DELIMITER, true, 1, 1);\n+\t\tproperties.validateEnumValues(FORMAT_LINE_DELIMITER, true, Arrays.asList(\"\\r\", \"\\n\", \"\\r\\n\", \"\"));\n+\t\tproperties.validateBoolean(FORMAT_DISABLE_QUOTE_CHARACTER, true);\n+\t\tproperties.validateString(FORMAT_QUOTE_CHARACTER, true, 1, 1);\n+\t\tproperties.validateBoolean(FORMAT_ALLOW_COMMENTS, true);\n+\t\tproperties.validateBoolean(FORMAT_IGNORE_PARSE_ERRORS, true);\n+\t\tproperties.validateString(FORMAT_ARRAY_ELEMENT_DELIMITER, true, 1);\n+\t\tproperties.validateString(FORMAT_ESCAPE_CHARACTER, true, 1, 1);\n+\n+\t\treturn properties;\n+\t}\n+\n+\t/**\n+\t * InputFormat that reads csv record into {@link RowData}.\n+\t */\n+\tpublic static class CsvInputFormat extends AbstractCsvInputFormat<RowData> {\n+\t\tprivate static final long serialVersionUID = 1L;\n+\n+\t\tprivate final RowType formatRowType;\n+\t\tprivate final DataType[] fieldTypes;\n+\t\tprivate final String[] fieldNames;\n+\t\tprivate final int[] selectFields;\n+\t\tprivate final List<String> partitionKeys;\n+\t\tprivate final String defaultPartValue;\n+\t\tprivate final long limit;\n+\t\tprivate final int[] csvSelectFieldToProjectFieldMapping;\n+\t\tprivate final int[] csvSelectFieldToCsvFieldMapping;\n+\t\tprivate final boolean ignoreParseErrors;\n+\n+\t\tprivate transient InputStreamReader inputStreamReader;\n+\t\tprivate transient BufferedReader reader;\n+\t\tprivate transient boolean end;\n+\t\tprivate transient long emitted;\n+\t\t// reuse object for per record\n+\t\tprivate transient GenericRowData rowData;\n+\t\tprivate transient DeserializationRuntimeConverter runtimeConverter;\n+\t\tprivate transient MappingIterator<JsonNode> iterator;\n+\n+\t\tpublic CsvInputFormat(\n+\t\t\t\tPath[] filePaths,\n+\t\t\t\tDataType[] fieldTypes,\n+\t\t\t\tString[] fieldNames,\n+\t\t\t\tCsvSchema csvSchema,\n+\t\t\t\tRowType formatRowType,\n+\t\t\t\tint[] selectFields,\n+\t\t\t\tList<String> partitionKeys,\n+\t\t\t\tString defaultPartValue,\n+\t\t\t\tlong limit,\n+\t\t\t\tint[] csvSelectFieldToProjectFieldMapping,\n+\t\t\t\tint[] csvSelectFieldToCsvFieldMapping,\n+\t\t\t\tboolean ignoreParseErrors) {\n+\t\t\tsuper(filePaths, csvSchema);\n+\t\t\tthis.fieldTypes = fieldTypes;\n+\t\t\tthis.fieldNames = fieldNames;\n+\t\t\tthis.formatRowType = formatRowType;\n+\t\t\tthis.partitionKeys = partitionKeys;\n+\t\t\tthis.defaultPartValue = defaultPartValue;\n+\t\t\tthis.selectFields = selectFields;\n+\t\t\tthis.limit = limit;\n+\t\t\tthis.emitted = 0;\n+\t\t\tthis.csvSelectFieldToProjectFieldMapping = csvSelectFieldToProjectFieldMapping;\n+\t\t\tthis.csvSelectFieldToCsvFieldMapping = csvSelectFieldToCsvFieldMapping;\n+\t\t\tthis.ignoreParseErrors = ignoreParseErrors;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void open(FileInputSplit split) throws IOException {\n+\t\t\tsuper.open(split);\n+\t\t\tthis.end = false;\n+\t\t\tthis.inputStreamReader = new InputStreamReader(csvInputStream);\n+\t\t\tthis.reader = new BufferedReader(inputStreamReader);\n+\t\t\tthis.rowData = PartitionPathUtils.fillPartitionValueForRecord(fieldNames, fieldTypes, selectFields,\n+\t\t\t\tpartitionKeys, currentSplit.getPath(), defaultPartValue);\n+\t\t\tthis.iterator = new CsvMapper()\n+\t\t\t\t.readerFor(JsonNode.class)\n+\t\t\t\t.with(csvSchema)\n+\t\t\t\t.readValues(csvInputStream);\n+\t\t\tprepareRuntimeConverter();\n+\t\t}\n+\n+\t\tprivate void prepareRuntimeConverter(){\n+\t\t\tCsvRowDataDeserializationSchema.Builder builder = new CsvRowDataDeserializationSchema.Builder(\n+\t\t\t\tformatRowType, new GenericTypeInfo<>(RowData.class))\n+\t\t\t\t.setIgnoreParseErrors(ignoreParseErrors);\n+\t\t\tthis.runtimeConverter = builder.build().createRowConverter(formatRowType, true);\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic boolean reachedEnd() throws IOException {\n+\t\t\treturn emitted >= limit || end;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic RowData nextRecord(RowData reuse) throws IOException {\n+\t\t\tGenericRowData csvRow = null;\n+\t\t\tdo {\n+\t\t\t\ttry {\n+\t\t\t\t\tJsonNode root = iterator.nextValue();\n+\t\t\t\t\tcsvRow = (GenericRowData) runtimeConverter.convert(root);\n+\t\t\t\t} catch (NoSuchElementException e) {\n+\t\t\t\t\tend = true;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjc2NzQ5OQ=="}, "originalCommit": {"oid": "0fd33d570a8591ef7e5a523c29366a34aa6b986e"}, "originalPosition": 301}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMjY2NDgzOnYy", "diffSide": "RIGHT", "path": "flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvRowDataFileSystemFormatFactory.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMVQwNDowMzo1NVrOGTLsRg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMVQwNDowMzo1NVrOGTLsRg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjc2NzY4Ng==", "bodyText": "Minor: this class can be a lambda.", "url": "https://github.com/apache/flink/pull/11755#discussion_r422767686", "createdAt": "2020-05-11T04:03:55Z", "author": {"login": "JingsongLi"}, "path": "flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvRowDataFileSystemFormatFactory.java", "diffHunk": "@@ -0,0 +1,352 @@\n+package org.apache.flink.formats.csv;\n+\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.api.java.typeutils.GenericTypeInfo;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.formats.csv.CsvRowDataDeserializationSchema.DeserializationRuntimeConverter;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.FileSystemFormatFactory;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.utils.PartitionPathUtils;\n+\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.JsonNode;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.MappingIterator;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvMapper;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvSchema;\n+\n+import java.io.BufferedReader;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.io.OutputStream;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NoSuchElementException;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ALLOW_COMMENTS;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ARRAY_ELEMENT_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_DISABLE_QUOTE_CHARACTER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ESCAPE_CHARACTER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_FIELD_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_IGNORE_PARSE_ERRORS;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_LINE_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_NULL_LITERAL;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_QUOTE_CHARACTER;\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+\n+/**\n+ * CSV format factory for file system.\n+ */\n+public class CsvRowDataFileSystemFormatFactory implements FileSystemFormatFactory {\n+\t@Override\n+\tpublic InputFormat<RowData, ?> createReader(ReaderContext context) {\n+\n+\t\tDescriptorProperties properties = getValidatedProperties(context.getFormatProperties());\n+\n+\t\tRowType formatRowType = context.getFormatRowType();\n+\n+\t\tString[] fieldNames = context.getSchema().getFieldNames();\n+\t\tList<String> projectFields = Arrays.stream(context.getProjectFields())\n+\t\t\t.mapToObj(idx -> fieldNames[idx])\n+\t\t\t.collect(Collectors.toList());\n+\t\tList<String> csvFields = Arrays.stream(fieldNames)\n+\t\t\t.filter(field -> !context.getPartitionKeys().contains(field))\n+\t\t\t.collect(Collectors.toList());\n+\n+\t\tint[] csvSelectFieldToProjectFieldMapping = context.getFormatProjectFields().stream()\n+\t\t\t.mapToInt(projectFields::indexOf)\n+\t\t\t.toArray();\n+\t\tint[] csvSelectFieldToCsvFieldMapping = context.getFormatProjectFields().stream()\n+\t\t\t.mapToInt(csvFields::indexOf)\n+\t\t\t.toArray();\n+\n+\t\tCsvSchema csvSchema = buildCsvSchema(formatRowType, properties);\n+\n+\t\tboolean ignoreParseErrors = properties.getOptionalBoolean(FORMAT_IGNORE_PARSE_ERRORS).orElse(false);\n+\n+\t\treturn new CsvInputFormat(\n+\t\t\tcontext.getPaths(),\n+\t\t\tcontext.getSchema().getFieldDataTypes(),\n+\t\t\tcontext.getSchema().getFieldNames(),\n+\t\t\tcsvSchema,\n+\t\t\tformatRowType,\n+\t\t\tcontext.getProjectFields(),\n+\t\t\tcontext.getPartitionKeys(),\n+\t\t\tcontext.getDefaultPartName(),\n+\t\t\tcontext.getPushedDownLimit(),\n+\t\t\tcsvSelectFieldToProjectFieldMapping,\n+\t\t\tcsvSelectFieldToCsvFieldMapping,\n+\t\t\tignoreParseErrors);\n+\t}\n+\n+\tprivate CsvSchema buildCsvSchema(RowType rowType, DescriptorProperties properties) {\n+\t\tCsvSchema csvSchema = CsvRowSchemaConverter.convert(rowType);\n+\t\t//format properties\n+\t\tif (properties.containsKey(FORMAT_FIELD_DELIMITER)) {\n+\t\t\tcsvSchema = csvSchema.rebuild()\n+\t\t\t\t.setColumnSeparator(properties.getCharacter(FORMAT_FIELD_DELIMITER))\n+\t\t\t\t.build();\n+\t\t}\n+\t\tif (properties.containsKey(FORMAT_QUOTE_CHARACTER)) {\n+\t\t\tcsvSchema = csvSchema.rebuild()\n+\t\t\t\t.setQuoteChar(properties.getCharacter(FORMAT_QUOTE_CHARACTER))\n+\t\t\t\t.build();\n+\t\t}\n+\t\tif (properties.containsKey(FORMAT_ALLOW_COMMENTS)) {\n+\t\t\tcsvSchema = csvSchema.rebuild()\n+\t\t\t\t.setAllowComments(properties.getBoolean(FORMAT_ALLOW_COMMENTS))\n+\t\t\t\t.build();\n+\t\t}\n+\t\tif (properties.containsKey(FORMAT_ARRAY_ELEMENT_DELIMITER)) {\n+\t\t\tcsvSchema = csvSchema.rebuild()\n+\t\t\t\t.setArrayElementSeparator(properties.getString(FORMAT_ARRAY_ELEMENT_DELIMITER))\n+\t\t\t\t.build();\n+\t\t}\n+\t\tif (properties.containsKey(FORMAT_ESCAPE_CHARACTER)) {\n+\t\t\tcsvSchema = csvSchema.rebuild()\n+\t\t\t\t.setEscapeChar(properties.getCharacter(FORMAT_ESCAPE_CHARACTER))\n+\t\t\t\t.build();\n+\t\t}\n+\t\tif (properties.containsKey(FORMAT_NULL_LITERAL)) {\n+\t\t\tcsvSchema = csvSchema.rebuild()\n+\t\t\t\t.setNullValue(properties.getString(FORMAT_NULL_LITERAL))\n+\t\t\t\t.build();\n+\t\t}\n+\t\tif (properties.containsKey(FORMAT_LINE_DELIMITER)) {\n+\t\t\tcsvSchema = csvSchema.rebuild()\n+\t\t\t\t.setLineSeparator(properties.getString(FORMAT_LINE_DELIMITER))\n+\t\t\t\t.build();\n+\t\t}\n+\t\treturn csvSchema;\n+\t}\n+\n+\t@Override\n+\tpublic Optional<Encoder<RowData>> createEncoder(WriterContext context) {\n+\n+\t\tCsvRowDataSerializationSchema.Builder builder = new CsvRowDataSerializationSchema.Builder(\n+\t\t\tcontext.getFormatRowType());\n+\n+\t\tDescriptorProperties properties = getValidatedProperties(context.getFormatProperties());\n+\n+\t\tproperties.getOptionalCharacter(FORMAT_FIELD_DELIMITER)\n+\t\t\t.ifPresent(builder::setFieldDelimiter);\n+\n+\t\tproperties.getOptionalString(FORMAT_LINE_DELIMITER)\n+\t\t\t.ifPresent(builder::setLineDelimiter);\n+\n+\t\tif (properties.getOptionalBoolean(FORMAT_DISABLE_QUOTE_CHARACTER).orElse(false)) {\n+\t\t\tbuilder.disableQuoteCharacter();\n+\t\t} else {\n+\t\t\tproperties.getOptionalCharacter(FORMAT_QUOTE_CHARACTER).ifPresent(builder::setQuoteCharacter);\n+\t\t}\n+\n+\t\tproperties.getOptionalString(FORMAT_ARRAY_ELEMENT_DELIMITER)\n+\t\t\t.ifPresent(builder::setArrayElementDelimiter);\n+\n+\t\tproperties.getOptionalCharacter(FORMAT_ESCAPE_CHARACTER)\n+\t\t\t.ifPresent(builder::setEscapeCharacter);\n+\n+\t\tproperties.getOptionalString(FORMAT_NULL_LITERAL)\n+\t\t\t.ifPresent(builder::setNullLiteral);\n+\n+\t\treturn Optional.of(new CsvRowDataEncoder(builder.build()));\n+\t}\n+\n+\t@Override\n+\tpublic Optional<BulkWriter.Factory<RowData>> createBulkWriterFactory(WriterContext context) {\n+\t\treturn Optional.empty();\n+\t}\n+\n+\t@Override\n+\tpublic boolean supportsSchemaDerivation() {\n+\t\treturn true;\n+\t}\n+\n+\t@Override\n+\tpublic List<String> supportedProperties() {\n+\t\tfinal List<String> properties = new ArrayList<>();\n+\t\tproperties.add(FORMAT_FIELD_DELIMITER);\n+\t\tproperties.add(FORMAT_LINE_DELIMITER);\n+\t\tproperties.add(FORMAT_DISABLE_QUOTE_CHARACTER);\n+\t\tproperties.add(FORMAT_QUOTE_CHARACTER);\n+\t\tproperties.add(FORMAT_ALLOW_COMMENTS);\n+\t\tproperties.add(FORMAT_IGNORE_PARSE_ERRORS);\n+\t\tproperties.add(FORMAT_ARRAY_ELEMENT_DELIMITER);\n+\t\tproperties.add(FORMAT_ESCAPE_CHARACTER);\n+\t\tproperties.add(FORMAT_NULL_LITERAL);\n+\t\treturn properties;\n+\t}\n+\n+\t@Override\n+\tpublic Map<String, String> requiredContext() {\n+\t\tMap<String, String> context = new HashMap<>();\n+\t\tcontext.put(FORMAT, \"csv\");\n+\t\treturn context;\n+\t}\n+\n+\tprivate static DescriptorProperties getValidatedProperties(Map<String, String> propertiesMap) {\n+\t\tfinal DescriptorProperties properties = new DescriptorProperties(true);\n+\t\tproperties.putProperties(propertiesMap);\n+\n+\t\tproperties.validateString(FORMAT_FIELD_DELIMITER, true, 1, 1);\n+\t\tproperties.validateEnumValues(FORMAT_LINE_DELIMITER, true, Arrays.asList(\"\\r\", \"\\n\", \"\\r\\n\", \"\"));\n+\t\tproperties.validateBoolean(FORMAT_DISABLE_QUOTE_CHARACTER, true);\n+\t\tproperties.validateString(FORMAT_QUOTE_CHARACTER, true, 1, 1);\n+\t\tproperties.validateBoolean(FORMAT_ALLOW_COMMENTS, true);\n+\t\tproperties.validateBoolean(FORMAT_IGNORE_PARSE_ERRORS, true);\n+\t\tproperties.validateString(FORMAT_ARRAY_ELEMENT_DELIMITER, true, 1);\n+\t\tproperties.validateString(FORMAT_ESCAPE_CHARACTER, true, 1, 1);\n+\n+\t\treturn properties;\n+\t}\n+\n+\t/**\n+\t * InputFormat that reads csv record into {@link RowData}.\n+\t */\n+\tpublic static class CsvInputFormat extends AbstractCsvInputFormat<RowData> {\n+\t\tprivate static final long serialVersionUID = 1L;\n+\n+\t\tprivate final RowType formatRowType;\n+\t\tprivate final DataType[] fieldTypes;\n+\t\tprivate final String[] fieldNames;\n+\t\tprivate final int[] selectFields;\n+\t\tprivate final List<String> partitionKeys;\n+\t\tprivate final String defaultPartValue;\n+\t\tprivate final long limit;\n+\t\tprivate final int[] csvSelectFieldToProjectFieldMapping;\n+\t\tprivate final int[] csvSelectFieldToCsvFieldMapping;\n+\t\tprivate final boolean ignoreParseErrors;\n+\n+\t\tprivate transient InputStreamReader inputStreamReader;\n+\t\tprivate transient BufferedReader reader;\n+\t\tprivate transient boolean end;\n+\t\tprivate transient long emitted;\n+\t\t// reuse object for per record\n+\t\tprivate transient GenericRowData rowData;\n+\t\tprivate transient DeserializationRuntimeConverter runtimeConverter;\n+\t\tprivate transient MappingIterator<JsonNode> iterator;\n+\n+\t\tpublic CsvInputFormat(\n+\t\t\t\tPath[] filePaths,\n+\t\t\t\tDataType[] fieldTypes,\n+\t\t\t\tString[] fieldNames,\n+\t\t\t\tCsvSchema csvSchema,\n+\t\t\t\tRowType formatRowType,\n+\t\t\t\tint[] selectFields,\n+\t\t\t\tList<String> partitionKeys,\n+\t\t\t\tString defaultPartValue,\n+\t\t\t\tlong limit,\n+\t\t\t\tint[] csvSelectFieldToProjectFieldMapping,\n+\t\t\t\tint[] csvSelectFieldToCsvFieldMapping,\n+\t\t\t\tboolean ignoreParseErrors) {\n+\t\t\tsuper(filePaths, csvSchema);\n+\t\t\tthis.fieldTypes = fieldTypes;\n+\t\t\tthis.fieldNames = fieldNames;\n+\t\t\tthis.formatRowType = formatRowType;\n+\t\t\tthis.partitionKeys = partitionKeys;\n+\t\t\tthis.defaultPartValue = defaultPartValue;\n+\t\t\tthis.selectFields = selectFields;\n+\t\t\tthis.limit = limit;\n+\t\t\tthis.emitted = 0;\n+\t\t\tthis.csvSelectFieldToProjectFieldMapping = csvSelectFieldToProjectFieldMapping;\n+\t\t\tthis.csvSelectFieldToCsvFieldMapping = csvSelectFieldToCsvFieldMapping;\n+\t\t\tthis.ignoreParseErrors = ignoreParseErrors;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void open(FileInputSplit split) throws IOException {\n+\t\t\tsuper.open(split);\n+\t\t\tthis.end = false;\n+\t\t\tthis.inputStreamReader = new InputStreamReader(csvInputStream);\n+\t\t\tthis.reader = new BufferedReader(inputStreamReader);\n+\t\t\tthis.rowData = PartitionPathUtils.fillPartitionValueForRecord(fieldNames, fieldTypes, selectFields,\n+\t\t\t\tpartitionKeys, currentSplit.getPath(), defaultPartValue);\n+\t\t\tthis.iterator = new CsvMapper()\n+\t\t\t\t.readerFor(JsonNode.class)\n+\t\t\t\t.with(csvSchema)\n+\t\t\t\t.readValues(csvInputStream);\n+\t\t\tprepareRuntimeConverter();\n+\t\t}\n+\n+\t\tprivate void prepareRuntimeConverter(){\n+\t\t\tCsvRowDataDeserializationSchema.Builder builder = new CsvRowDataDeserializationSchema.Builder(\n+\t\t\t\tformatRowType, new GenericTypeInfo<>(RowData.class))\n+\t\t\t\t.setIgnoreParseErrors(ignoreParseErrors);\n+\t\t\tthis.runtimeConverter = builder.build().createRowConverter(formatRowType, true);\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic boolean reachedEnd() throws IOException {\n+\t\t\treturn emitted >= limit || end;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic RowData nextRecord(RowData reuse) throws IOException {\n+\t\t\tGenericRowData csvRow = null;\n+\t\t\tdo {\n+\t\t\t\ttry {\n+\t\t\t\t\tJsonNode root = iterator.nextValue();\n+\t\t\t\t\tcsvRow = (GenericRowData) runtimeConverter.convert(root);\n+\t\t\t\t} catch (NoSuchElementException e) {\n+\t\t\t\t\tend = true;\n+\t\t\t\t} catch (Throwable t) {\n+\t\t\t\t\tif (!ignoreParseErrors) {\n+\t\t\t\t\t\tthrow new IOException(\"Failed to deserialize CSV row.\", t);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t} while (csvRow == null && !reachedEnd());\n+\n+\t\t\tGenericRowData returnRecord = null;\n+\t\t\tif (csvRow != null) {\n+\t\t\t\treturnRecord = rowData;\n+\t\t\t\tfor (int i = 0; i < csvSelectFieldToCsvFieldMapping.length; i++) {\n+\t\t\t\t\treturnRecord.setField(csvSelectFieldToProjectFieldMapping[i],\n+\t\t\t\t\t\tcsvRow.getField(csvSelectFieldToCsvFieldMapping[i]));\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\temitted++;\n+\t\t\treturn returnRecord;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void close() throws IOException {\n+\t\t\tsuper.close();\n+\t\t\tif (reader != null) {\n+\t\t\t\treader.close();\n+\t\t\t\treader = null;\n+\t\t\t}\n+\t\t\tif (inputStreamReader != null) {\n+\t\t\t\tinputStreamReader.close();\n+\t\t\t\tinputStreamReader = null;\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * A {@link Encoder} writes {@link RowData} record into {@link java.io.OutputStream} with csv format.\n+\t */\n+\tpublic static class CsvRowDataEncoder implements Encoder<RowData> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0fd33d570a8591ef7e5a523c29366a34aa6b986e"}, "originalPosition": 338}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzNjgyMDcxOnYy", "diffSide": "RIGHT", "path": "flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvRowDataFileSystemFormatFactory.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQwMjoxODowNFrOGTzusA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQwMjoxODowNFrOGTzusA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQyMzY2NA==", "bodyText": "CsvFileSystemFormatFactory", "url": "https://github.com/apache/flink/pull/11755#discussion_r423423664", "createdAt": "2020-05-12T02:18:04Z", "author": {"login": "JingsongLi"}, "path": "flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvRowDataFileSystemFormatFactory.java", "diffHunk": "@@ -0,0 +1,333 @@\n+package org.apache.flink.formats.csv;\n+\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.api.java.typeutils.GenericTypeInfo;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.formats.csv.CsvRowDataDeserializationSchema.DeserializationRuntimeConverter;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.FileSystemFormatFactory;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.utils.PartitionPathUtils;\n+\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.JsonNode;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.MappingIterator;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvMapper;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvSchema;\n+\n+import java.io.BufferedReader;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NoSuchElementException;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ALLOW_COMMENTS;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ARRAY_ELEMENT_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_DISABLE_QUOTE_CHARACTER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ESCAPE_CHARACTER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_FIELD_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_IGNORE_PARSE_ERRORS;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_LINE_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_NULL_LITERAL;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_QUOTE_CHARACTER;\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+\n+/**\n+ * CSV format factory for file system.\n+ */\n+public class CsvRowDataFileSystemFormatFactory implements FileSystemFormatFactory {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "73ca85b532ef3efcef5e7795db77c79d7bf82131"}, "originalPosition": 49}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzNjgyMTY5OnYy", "diffSide": "RIGHT", "path": "flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvRowDataFileSystemFormatFactory.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQwMjoxODozOVrOGTzvUQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQwMjoxODozOVrOGTzvUQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQyMzgyNQ==", "bodyText": "Use getOptionalChar.ifPresent getOptionalString.ifPresent", "url": "https://github.com/apache/flink/pull/11755#discussion_r423423825", "createdAt": "2020-05-12T02:18:39Z", "author": {"login": "JingsongLi"}, "path": "flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvRowDataFileSystemFormatFactory.java", "diffHunk": "@@ -0,0 +1,333 @@\n+package org.apache.flink.formats.csv;\n+\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.api.java.typeutils.GenericTypeInfo;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.formats.csv.CsvRowDataDeserializationSchema.DeserializationRuntimeConverter;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.FileSystemFormatFactory;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.utils.PartitionPathUtils;\n+\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.JsonNode;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.MappingIterator;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvMapper;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvSchema;\n+\n+import java.io.BufferedReader;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NoSuchElementException;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ALLOW_COMMENTS;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ARRAY_ELEMENT_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_DISABLE_QUOTE_CHARACTER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ESCAPE_CHARACTER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_FIELD_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_IGNORE_PARSE_ERRORS;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_LINE_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_NULL_LITERAL;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_QUOTE_CHARACTER;\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+\n+/**\n+ * CSV format factory for file system.\n+ */\n+public class CsvRowDataFileSystemFormatFactory implements FileSystemFormatFactory {\n+\n+\t@Override\n+\tpublic InputFormat<RowData, ?> createReader(ReaderContext context) {\n+\t\tDescriptorProperties properties = getValidatedProperties(context.getFormatProperties());\n+\n+\t\tRowType formatRowType = context.getFormatRowType();\n+\n+\t\tString[] fieldNames = context.getSchema().getFieldNames();\n+\t\tList<String> projectFields = Arrays.stream(context.getProjectFields())\n+\t\t\t.mapToObj(idx -> fieldNames[idx])\n+\t\t\t.collect(Collectors.toList());\n+\t\tList<String> csvFields = Arrays.stream(fieldNames)\n+\t\t\t.filter(field -> !context.getPartitionKeys().contains(field))\n+\t\t\t.collect(Collectors.toList());\n+\n+\t\tint[] csvSelectFieldToProjectFieldMapping = context.getFormatProjectFields().stream()\n+\t\t\t.mapToInt(projectFields::indexOf)\n+\t\t\t.toArray();\n+\t\tint[] csvSelectFieldToCsvFieldMapping = context.getFormatProjectFields().stream()\n+\t\t\t.mapToInt(csvFields::indexOf)\n+\t\t\t.toArray();\n+\n+\t\tCsvSchema csvSchema = buildCsvSchema(formatRowType, properties);\n+\n+\t\tboolean ignoreParseErrors = properties.getOptionalBoolean(FORMAT_IGNORE_PARSE_ERRORS).orElse(false);\n+\n+\t\treturn new CsvInputFormat(\n+\t\t\tcontext.getPaths(),\n+\t\t\tcontext.getSchema().getFieldDataTypes(),\n+\t\t\tcontext.getSchema().getFieldNames(),\n+\t\t\tcsvSchema,\n+\t\t\tformatRowType,\n+\t\t\tcontext.getProjectFields(),\n+\t\t\tcontext.getPartitionKeys(),\n+\t\t\tcontext.getDefaultPartName(),\n+\t\t\tcontext.getPushedDownLimit(),\n+\t\t\tcsvSelectFieldToProjectFieldMapping,\n+\t\t\tcsvSelectFieldToCsvFieldMapping,\n+\t\t\tignoreParseErrors);\n+\t}\n+\n+\tprivate CsvSchema buildCsvSchema(RowType rowType, DescriptorProperties properties) {\n+\t\tCsvSchema csvSchema = CsvRowSchemaConverter.convert(rowType);\n+\t\t//format properties\n+\t\tif (properties.containsKey(FORMAT_FIELD_DELIMITER)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "73ca85b532ef3efcef5e7795db77c79d7bf82131"}, "originalPosition": 94}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzNjgyMjM4OnYy", "diffSide": "RIGHT", "path": "flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/CsvRowDataFilesystemBatchITCase.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQwMjoxOTowN1rOGTzvxQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQwMjoxOTowN1rOGTzvxQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQyMzk0MQ==", "bodyText": "CsvFilesystemBatchITCase\nOthers too.", "url": "https://github.com/apache/flink/pull/11755#discussion_r423423941", "createdAt": "2020-05-12T02:19:07Z", "author": {"login": "JingsongLi"}, "path": "flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/CsvRowDataFilesystemBatchITCase.java", "diffHunk": "@@ -0,0 +1,85 @@\n+package org.apache.flink.formats.csv;\n+\n+import org.apache.flink.table.planner.runtime.batch.sql.BatchFileSystemITCaseBase;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.util.FileUtils;\n+\n+import org.junit.Test;\n+import org.junit.experimental.runners.Enclosed;\n+import org.junit.runner.RunWith;\n+\n+import java.io.File;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+/**\n+ * ITCase to test csv format for {@link CsvRowDataFileSystemFormatFactory} in batch mode.\n+ */\n+@RunWith(Enclosed.class)\n+public class CsvRowDataFilesystemBatchITCase {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "73ca85b532ef3efcef5e7795db77c79d7bf82131"}, "originalPosition": 21}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzNjgyMjg2OnYy", "diffSide": "RIGHT", "path": "flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/CsvRowDataFilesystemBatchITCase.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQwMjoxOToyN1rOGTzwEg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQwMjoxOToyN1rOGTzwEg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQyNDAxOA==", "bodyText": "Ditto.", "url": "https://github.com/apache/flink/pull/11755#discussion_r423424018", "createdAt": "2020-05-12T02:19:27Z", "author": {"login": "JingsongLi"}, "path": "flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/CsvRowDataFilesystemBatchITCase.java", "diffHunk": "@@ -0,0 +1,85 @@\n+package org.apache.flink.formats.csv;\n+\n+import org.apache.flink.table.planner.runtime.batch.sql.BatchFileSystemITCaseBase;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.util.FileUtils;\n+\n+import org.junit.Test;\n+import org.junit.experimental.runners.Enclosed;\n+import org.junit.runner.RunWith;\n+\n+import java.io.File;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+/**\n+ * ITCase to test csv format for {@link CsvRowDataFileSystemFormatFactory} in batch mode.\n+ */\n+@RunWith(Enclosed.class)\n+public class CsvRowDataFilesystemBatchITCase {\n+\n+\t/**\n+\t * General IT cases for CsvRowDataFilesystem in batch mode.\n+\t */\n+\tpublic static class GeneralCsvRowDataFilesystemBatchITCase extends BatchFileSystemITCaseBase {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "73ca85b532ef3efcef5e7795db77c79d7bf82131"}, "originalPosition": 26}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzNjgyMjk1OnYy", "diffSide": "RIGHT", "path": "flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/CsvRowDataFilesystemBatchITCase.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQwMjoxOTozMVrOGTzwHw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQwMjoxOTozMVrOGTzwHw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQyNDAzMQ==", "bodyText": "Ditto.", "url": "https://github.com/apache/flink/pull/11755#discussion_r423424031", "createdAt": "2020-05-12T02:19:31Z", "author": {"login": "JingsongLi"}, "path": "flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/CsvRowDataFilesystemBatchITCase.java", "diffHunk": "@@ -0,0 +1,85 @@\n+package org.apache.flink.formats.csv;\n+\n+import org.apache.flink.table.planner.runtime.batch.sql.BatchFileSystemITCaseBase;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.util.FileUtils;\n+\n+import org.junit.Test;\n+import org.junit.experimental.runners.Enclosed;\n+import org.junit.runner.RunWith;\n+\n+import java.io.File;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+/**\n+ * ITCase to test csv format for {@link CsvRowDataFileSystemFormatFactory} in batch mode.\n+ */\n+@RunWith(Enclosed.class)\n+public class CsvRowDataFilesystemBatchITCase {\n+\n+\t/**\n+\t * General IT cases for CsvRowDataFilesystem in batch mode.\n+\t */\n+\tpublic static class GeneralCsvRowDataFilesystemBatchITCase extends BatchFileSystemITCaseBase {\n+\n+\t\t@Override\n+\t\tpublic String[] formatProperties() {\n+\t\t\tList<String> ret = new ArrayList<>();\n+\t\t\tret.add(\"'format'='csv'\");\n+\t\t\tret.add(\"'format.field-delimiter'=';'\");\n+\t\t\tret.add(\"'format.quote-character'='#'\");\n+\t\t\treturn ret.toArray(new String[0]);\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * Enriched IT cases that including testParseError and testEscapeChar for CsvRowDataFilesystem in batch mode.\n+\t */\n+\tpublic static class EnrichedCsvRowDataFilesystemBatchITCase extends BatchFileSystemITCaseBase {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "73ca85b532ef3efcef5e7795db77c79d7bf82131"}, "originalPosition": 41}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzNjgyMzA2OnYy", "diffSide": "RIGHT", "path": "flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/CsvRowDataFilesystemStreamITCase.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQwMjoxOTozNlrOGTzwMw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQwMjoxOTozNlrOGTzwMw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQyNDA1MQ==", "bodyText": "Ditto.", "url": "https://github.com/apache/flink/pull/11755#discussion_r423424051", "createdAt": "2020-05-12T02:19:36Z", "author": {"login": "JingsongLi"}, "path": "flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/CsvRowDataFilesystemStreamITCase.java", "diffHunk": "@@ -0,0 +1,38 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.csv;\n+\n+import org.apache.flink.table.planner.runtime.stream.FsStreamingSinkITCaseBase;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * ITCase to test csv format for {@link CsvRowDataFileSystemFormatFactory} in stream mode.\n+ */\n+public class CsvRowDataFilesystemStreamITCase extends FsStreamingSinkITCaseBase {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "73ca85b532ef3efcef5e7795db77c79d7bf82131"}, "originalPosition": 29}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1627, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}