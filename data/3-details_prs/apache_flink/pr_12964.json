{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDU1NDQ1NzMx", "number": 12964, "title": "[FLINK-17426][blink planner] Dynamic Source supportsLimit pushdown", "bodyText": "What is the purpose of the change\n\nmake the DynamicSource supports LimitPushDown Rule\n\nVerifying this change\nThis change added tests and can be verified as follows:\n\nAdded LimitTest to verify the plan\nExtended LimitITCase (only batch)to verify the result limit projection\nmake the TestValueSource supports LimitPushDown\n\nDoes this pull request potentially affect one of the following parts:\n\nDependencies (does it add or upgrade a dependency): ( no)\nThe public API, i.e., is any changed class annotated with @public(Evolving): (no)\nThe serializers: ( no)\nThe runtime per-record code paths (performance sensitive): (no)\nAnything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (no)\nThe S3 file system connector: (no)\n\nDocumentation\n\nDoes this pull request introduce a new feature? (yes)\nIf yes, how is the feature documented? (JavaDocs)", "createdAt": "2020-07-23T02:20:05Z", "url": "https://github.com/apache/flink/pull/12964", "merged": true, "mergeCommit": {"oid": "3ab1a1c66772320a0901ea085cfeaa6bf161bf9a"}, "closed": true, "closedAt": "2020-07-31T10:57:27Z", "author": {"login": "liuyongvs"}, "timelineItems": {"totalCount": 12, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABc3SksKgH2gAyNDU1NDQ1NzMxOjEyZDYwMjYzM2Q4NzBlZDYzMDIwYWMwYTc5ZDhmM2FkZDk5ZWMwMjA=", "endCursor": "Y3Vyc29yOnYyOpPPAAABc6SENrgFqTQ1OTA4OTQ5OQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "12d602633d870ed63020ac0a79d8f3add99ec020", "author": {"user": {"login": "liuyongvs", "name": "Jacky Lau"}}, "url": "https://github.com/apache/flink/commit/12d602633d870ed63020ac0a79d8f3add99ec020", "committedDate": "2020-07-22T03:50:17Z", "message": "[FLINK-17426][blink-planner] supportsLimitPushDown rule."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "65f49baddd16ead7030e56ed19322c067e91cbe0", "author": {"user": {"login": "liuyongvs", "name": "Jacky Lau"}}, "url": "https://github.com/apache/flink/commit/65f49baddd16ead7030e56ed19322c067e91cbe0", "committedDate": "2020-07-22T04:22:38Z", "message": "[FLINK-17426][blink-planner] DynamicSource supports SupportsLimitPushDown."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4eadef4cfc64e49d0b71c914a375818fb29557db", "author": {"user": {"login": "liuyongvs", "name": "Jacky Lau"}}, "url": "https://github.com/apache/flink/commit/4eadef4cfc64e49d0b71c914a375818fb29557db", "committedDate": "2020-07-22T04:22:41Z", "message": "[FLINK-17426][blink-planner] fix checkstyle of line too long."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "984b8a182e5a5cc64a9e1957710e9b9fd4768a56", "author": {"user": {"login": "liuyongvs", "name": "Jacky Lau"}}, "url": "https://github.com/apache/flink/commit/984b8a182e5a5cc64a9e1957710e9b9fd4768a56", "committedDate": "2020-07-23T02:11:20Z", "message": "[FLINK-17425][blink-planner] rewrite the limit pushdown."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU0NjU3ODk1", "url": "https://github.com/apache/flink/pull/12964#pullrequestreview-454657895", "createdAt": "2020-07-24T06:28:44Z", "commit": {"oid": "984b8a182e5a5cc64a9e1957710e9b9fd4768a56"}, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNFQwNjoyODo0NFrOG2ka8g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNFQwNjo0ODoyNVrOG2kyPA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTg3MzAxMA==", "bodyText": "nit: simpler way: use noneMatch instead of ! + anyMatch.\nbtw, it's better we can also update the matches method of PushFilterIntoTableSourceScanRule", "url": "https://github.com/apache/flink/pull/12964#discussion_r459873010", "createdAt": "2020-07-24T06:28:44Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushLimitIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsLimitPushDown;\n+import org.apache.flink.table.plan.stats.TableStats;\n+import org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalSort;\n+import org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalTableSourceScan;\n+import org.apache.flink.table.planner.plan.schema.FlinkPreparingTableBase;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Sort;\n+import org.apache.calcite.rex.RexLiteral;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+\n+/**\n+ * Planner rule that tries to push limit into a {@link FlinkLogicalTableSourceScan},\n+ * which table is a {@link TableSourceTable}. And the table source in the table is a {@link SupportsLimitPushDown}.\n+ * The original limit will still be retained.\n+ * The reasons why the limit still be retained:\n+ * 1.If the source is required to return the exact number of limit number, the implementation\n+ * of the source is highly required. The source is required to accurately control the record\n+ * number of split, and the parallelism setting also need to be adjusted accordingly.\n+ * 2.When remove the limit, maybe filter will be pushed down to the source after limit pushed\n+ * down. The source need know it should do limit first and do the filter later, it is hard to\n+ * implement.\n+ * 3.We can support limit with offset, we can push down offset + fetch to table source.\n+ */\n+public class PushLimitIntoTableSourceScanRule extends RelOptRule {\n+\tpublic static final PushLimitIntoTableSourceScanRule INSTANCE = new PushLimitIntoTableSourceScanRule();\n+\n+\tpublic PushLimitIntoTableSourceScanRule() {\n+\t\tsuper(operand(FlinkLogicalSort.class,\n+\t\t\toperand(FlinkLogicalTableSourceScan.class, none())),\n+\t\t\t\"PushLimitIntoTableSourceScanRule\");\n+\t}\n+\n+\t@Override\n+\tpublic boolean matches(RelOptRuleCall call) {\n+\t\tSort sort = call.rel(0);\n+\t\tTableSourceTable tableSourceTable = call.rel(1).getTable().unwrap(TableSourceTable.class);\n+\n+\t\t// a limit can be pushed down only if it satisfies the two conditions: 1) do not have order by keys, 2) have limit.\n+\t\tboolean onlyLimit = sort.getCollation().getFieldCollations().isEmpty() && sort.fetch != null;\n+\t\treturn onlyLimit\n+\t\t\t&& tableSourceTable != null\n+\t\t\t&& tableSourceTable.tableSource() instanceof SupportsLimitPushDown\n+\t\t\t&& !Arrays.stream(tableSourceTable.extraDigests()).anyMatch(str -> str.startsWith(\"limit=[\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "984b8a182e5a5cc64a9e1957710e9b9fd4768a56"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTg3NDY3OQ==", "bodyText": "we should update the digest anyway, otherwise the rule will be applied endless loop if limit is 0.", "url": "https://github.com/apache/flink/pull/12964#discussion_r459874679", "createdAt": "2020-07-24T06:34:37Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushLimitIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsLimitPushDown;\n+import org.apache.flink.table.plan.stats.TableStats;\n+import org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalSort;\n+import org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalTableSourceScan;\n+import org.apache.flink.table.planner.plan.schema.FlinkPreparingTableBase;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Sort;\n+import org.apache.calcite.rex.RexLiteral;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+\n+/**\n+ * Planner rule that tries to push limit into a {@link FlinkLogicalTableSourceScan},\n+ * which table is a {@link TableSourceTable}. And the table source in the table is a {@link SupportsLimitPushDown}.\n+ * The original limit will still be retained.\n+ * The reasons why the limit still be retained:\n+ * 1.If the source is required to return the exact number of limit number, the implementation\n+ * of the source is highly required. The source is required to accurately control the record\n+ * number of split, and the parallelism setting also need to be adjusted accordingly.\n+ * 2.When remove the limit, maybe filter will be pushed down to the source after limit pushed\n+ * down. The source need know it should do limit first and do the filter later, it is hard to\n+ * implement.\n+ * 3.We can support limit with offset, we can push down offset + fetch to table source.\n+ */\n+public class PushLimitIntoTableSourceScanRule extends RelOptRule {\n+\tpublic static final PushLimitIntoTableSourceScanRule INSTANCE = new PushLimitIntoTableSourceScanRule();\n+\n+\tpublic PushLimitIntoTableSourceScanRule() {\n+\t\tsuper(operand(FlinkLogicalSort.class,\n+\t\t\toperand(FlinkLogicalTableSourceScan.class, none())),\n+\t\t\t\"PushLimitIntoTableSourceScanRule\");\n+\t}\n+\n+\t@Override\n+\tpublic boolean matches(RelOptRuleCall call) {\n+\t\tSort sort = call.rel(0);\n+\t\tTableSourceTable tableSourceTable = call.rel(1).getTable().unwrap(TableSourceTable.class);\n+\n+\t\t// a limit can be pushed down only if it satisfies the two conditions: 1) do not have order by keys, 2) have limit.\n+\t\tboolean onlyLimit = sort.getCollation().getFieldCollations().isEmpty() && sort.fetch != null;\n+\t\treturn onlyLimit\n+\t\t\t&& tableSourceTable != null\n+\t\t\t&& tableSourceTable.tableSource() instanceof SupportsLimitPushDown\n+\t\t\t&& !Arrays.stream(tableSourceTable.extraDigests()).anyMatch(str -> str.startsWith(\"limit=[\"));\n+\t}\n+\n+\t@Override\n+\tpublic void onMatch(RelOptRuleCall call) {\n+\t\tSort sort = call.rel(0);\n+\t\tFlinkLogicalTableSourceScan scan = call.rel(1);\n+\t\tTableSourceTable tableSourceTable = scan.getTable().unwrap(TableSourceTable.class);\n+\t\tint offset = sort.offset == null ? 0 : RexLiteral.intValue(sort.offset);\n+\t\tint limit = offset + RexLiteral.intValue(sort.fetch);\n+\n+\t\tTableSourceTable newTableSourceTable = applyLimit(limit, tableSourceTable);\n+\n+\t\tFlinkLogicalTableSourceScan newScan = FlinkLogicalTableSourceScan.create(scan.getCluster(), newTableSourceTable);\n+\t\tcall.transformTo(sort.copy(sort.getTraitSet(), Collections.singletonList(newScan)));\n+\t}\n+\n+\tprivate TableSourceTable applyLimit(\n+\t\tlong limit,\n+\t\tFlinkPreparingTableBase relOptTable) {\n+\t\tTableSourceTable oldTableSourceTable = relOptTable.unwrap(TableSourceTable.class);\n+\t\tDynamicTableSource newTableSource = oldTableSourceTable.tableSource().copy();\n+\t\t((SupportsLimitPushDown) newTableSource).applyLimit(limit);\n+\n+\t\tFlinkStatistic statistic = relOptTable.getStatistic();\n+\t\tlong newRowCount = 0;\n+\t\tif (statistic.getRowCount() != null) {\n+\t\t\tnewRowCount = Math.min(limit, statistic.getRowCount().longValue());\n+\t\t} else {\n+\t\t\tnewRowCount = limit;\n+\t\t}\n+\t\t// update TableStats after limit push down\n+\t\tTableStats newTableStats = new TableStats(newRowCount);\n+\t\tFlinkStatistic newStatistic = FlinkStatistic.builder()\n+\t\t\t.statistic(statistic)\n+\t\t\t.tableStats(newTableStats)\n+\t\t\t.build();\n+\n+\t\t// update extraDigests\n+\t\tString[] newExtraDigests = new String[0];\n+\t\tif (limit > 0) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "984b8a182e5a5cc64a9e1957710e9b9fd4768a56"}, "originalPosition": 110}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTg3NTI2OQ==", "bodyText": "It's better we can put sort.copy(sort.getTraitSet(), Collections.singletonList(newScan)) in a single line, which could make debugging more easy.", "url": "https://github.com/apache/flink/pull/12964#discussion_r459875269", "createdAt": "2020-07-24T06:36:49Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushLimitIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsLimitPushDown;\n+import org.apache.flink.table.plan.stats.TableStats;\n+import org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalSort;\n+import org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalTableSourceScan;\n+import org.apache.flink.table.planner.plan.schema.FlinkPreparingTableBase;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Sort;\n+import org.apache.calcite.rex.RexLiteral;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+\n+/**\n+ * Planner rule that tries to push limit into a {@link FlinkLogicalTableSourceScan},\n+ * which table is a {@link TableSourceTable}. And the table source in the table is a {@link SupportsLimitPushDown}.\n+ * The original limit will still be retained.\n+ * The reasons why the limit still be retained:\n+ * 1.If the source is required to return the exact number of limit number, the implementation\n+ * of the source is highly required. The source is required to accurately control the record\n+ * number of split, and the parallelism setting also need to be adjusted accordingly.\n+ * 2.When remove the limit, maybe filter will be pushed down to the source after limit pushed\n+ * down. The source need know it should do limit first and do the filter later, it is hard to\n+ * implement.\n+ * 3.We can support limit with offset, we can push down offset + fetch to table source.\n+ */\n+public class PushLimitIntoTableSourceScanRule extends RelOptRule {\n+\tpublic static final PushLimitIntoTableSourceScanRule INSTANCE = new PushLimitIntoTableSourceScanRule();\n+\n+\tpublic PushLimitIntoTableSourceScanRule() {\n+\t\tsuper(operand(FlinkLogicalSort.class,\n+\t\t\toperand(FlinkLogicalTableSourceScan.class, none())),\n+\t\t\t\"PushLimitIntoTableSourceScanRule\");\n+\t}\n+\n+\t@Override\n+\tpublic boolean matches(RelOptRuleCall call) {\n+\t\tSort sort = call.rel(0);\n+\t\tTableSourceTable tableSourceTable = call.rel(1).getTable().unwrap(TableSourceTable.class);\n+\n+\t\t// a limit can be pushed down only if it satisfies the two conditions: 1) do not have order by keys, 2) have limit.\n+\t\tboolean onlyLimit = sort.getCollation().getFieldCollations().isEmpty() && sort.fetch != null;\n+\t\treturn onlyLimit\n+\t\t\t&& tableSourceTable != null\n+\t\t\t&& tableSourceTable.tableSource() instanceof SupportsLimitPushDown\n+\t\t\t&& !Arrays.stream(tableSourceTable.extraDigests()).anyMatch(str -> str.startsWith(\"limit=[\"));\n+\t}\n+\n+\t@Override\n+\tpublic void onMatch(RelOptRuleCall call) {\n+\t\tSort sort = call.rel(0);\n+\t\tFlinkLogicalTableSourceScan scan = call.rel(1);\n+\t\tTableSourceTable tableSourceTable = scan.getTable().unwrap(TableSourceTable.class);\n+\t\tint offset = sort.offset == null ? 0 : RexLiteral.intValue(sort.offset);\n+\t\tint limit = offset + RexLiteral.intValue(sort.fetch);\n+\n+\t\tTableSourceTable newTableSourceTable = applyLimit(limit, tableSourceTable);\n+\n+\t\tFlinkLogicalTableSourceScan newScan = FlinkLogicalTableSourceScan.create(scan.getCluster(), newTableSourceTable);\n+\t\tcall.transformTo(sort.copy(sort.getTraitSet(), Collections.singletonList(newScan)));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "984b8a182e5a5cc64a9e1957710e9b9fd4768a56"}, "originalPosition": 84}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTg3Njc2Nw==", "bodyText": "this line is too long...", "url": "https://github.com/apache/flink/pull/12964#discussion_r459876767", "createdAt": "2020-07-24T06:40:42Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/factories/TestValuesTableFactory.java", "diffHunk": "@@ -359,7 +361,7 @@ private ChangelogMode parseChangelogMode(String string) {\n \t/**\n \t * Values {@link DynamicTableSource} for testing.\n \t */\n-\tprivate static class TestValuesTableSource implements ScanTableSource, LookupTableSource, SupportsProjectionPushDown, SupportsFilterPushDown {\n+\tprivate static class TestValuesTableSource implements ScanTableSource, LookupTableSource, SupportsProjectionPushDown, SupportsFilterPushDown, SupportsLimitPushDown {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "984b8a182e5a5cc64a9e1957710e9b9fd4768a56"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTg3ODk3Mg==", "bodyText": "please add a rule test to verify this rule\uff0c just like PushFilterIntoTableSourceScanRuleTest", "url": "https://github.com/apache/flink/pull/12964#discussion_r459878972", "createdAt": "2020-07-24T06:48:25Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushLimitIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsLimitPushDown;\n+import org.apache.flink.table.plan.stats.TableStats;\n+import org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalSort;\n+import org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalTableSourceScan;\n+import org.apache.flink.table.planner.plan.schema.FlinkPreparingTableBase;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Sort;\n+import org.apache.calcite.rex.RexLiteral;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+\n+/**\n+ * Planner rule that tries to push limit into a {@link FlinkLogicalTableSourceScan},\n+ * which table is a {@link TableSourceTable}. And the table source in the table is a {@link SupportsLimitPushDown}.\n+ * The original limit will still be retained.\n+ * The reasons why the limit still be retained:\n+ * 1.If the source is required to return the exact number of limit number, the implementation\n+ * of the source is highly required. The source is required to accurately control the record\n+ * number of split, and the parallelism setting also need to be adjusted accordingly.\n+ * 2.When remove the limit, maybe filter will be pushed down to the source after limit pushed\n+ * down. The source need know it should do limit first and do the filter later, it is hard to\n+ * implement.\n+ * 3.We can support limit with offset, we can push down offset + fetch to table source.\n+ */\n+public class PushLimitIntoTableSourceScanRule extends RelOptRule {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "984b8a182e5a5cc64a9e1957710e9b9fd4768a56"}, "originalPosition": 51}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0324c22a9b2c85f04481966b2a78f72e92b9c88c", "author": {"user": {"login": "liuyongvs", "name": "Jacky Lau"}}, "url": "https://github.com/apache/flink/commit/0324c22a9b2c85f04481966b2a78f72e92b9c88c", "committedDate": "2020-07-27T08:24:13Z", "message": "[FLINK-17426][blink-planner] add unit tests."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU2NDUyMDIy", "url": "https://github.com/apache/flink/pull/12964#pullrequestreview-456452022", "createdAt": "2020-07-28T09:16:37Z", "commit": {"oid": "0324c22a9b2c85f04481966b2a78f72e92b9c88c"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQwOToxNjozN1rOG4EB9w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQwOToyMzo1NFrOG4ESbw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTQzOTQ3OQ==", "bodyText": "how about we remove the if ?", "url": "https://github.com/apache/flink/pull/12964#discussion_r461439479", "createdAt": "2020-07-28T09:16:37Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushLimitIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,122 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsLimitPushDown;\n+import org.apache.flink.table.plan.stats.TableStats;\n+import org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalSort;\n+import org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalTableSourceScan;\n+import org.apache.flink.table.planner.plan.schema.FlinkPreparingTableBase;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Sort;\n+import org.apache.calcite.rex.RexLiteral;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+\n+/**\n+ * Planner rule that tries to push limit into a {@link FlinkLogicalTableSourceScan},\n+ * which table is a {@link TableSourceTable}. And the table source in the table is a {@link SupportsLimitPushDown}.\n+ * The original limit will still be retained.\n+ * The reasons why the limit still be retained:\n+ * 1.If the source is required to return the exact number of limit number, the implementation\n+ * of the source is highly required. The source is required to accurately control the record\n+ * number of split, and the parallelism setting also need to be adjusted accordingly.\n+ * 2.When remove the limit, maybe filter will be pushed down to the source after limit pushed\n+ * down. The source need know it should do limit first and do the filter later, it is hard to\n+ * implement.\n+ * 3.We can support limit with offset, we can push down offset + fetch to table source.\n+ */\n+public class PushLimitIntoTableSourceScanRule extends RelOptRule {\n+\tpublic static final PushLimitIntoTableSourceScanRule INSTANCE = new PushLimitIntoTableSourceScanRule();\n+\n+\tpublic PushLimitIntoTableSourceScanRule() {\n+\t\tsuper(operand(FlinkLogicalSort.class,\n+\t\t\toperand(FlinkLogicalTableSourceScan.class, none())),\n+\t\t\t\"PushLimitIntoTableSourceScanRule\");\n+\t}\n+\n+\t@Override\n+\tpublic boolean matches(RelOptRuleCall call) {\n+\t\tSort sort = call.rel(0);\n+\t\tTableSourceTable tableSourceTable = call.rel(1).getTable().unwrap(TableSourceTable.class);\n+\n+\t\t// a limit can be pushed down only if it satisfies the two conditions: 1) do not have order by keys, 2) have limit.\n+\t\tboolean onlyLimit = sort.getCollation().getFieldCollations().isEmpty() && sort.fetch != null;\n+\t\treturn onlyLimit\n+\t\t\t&& tableSourceTable != null\n+\t\t\t&& tableSourceTable.tableSource() instanceof SupportsLimitPushDown\n+\t\t\t&& Arrays.stream(tableSourceTable.extraDigests()).noneMatch(str -> str.startsWith(\"limit=[\"));\n+\t}\n+\n+\t@Override\n+\tpublic void onMatch(RelOptRuleCall call) {\n+\t\tSort sort = call.rel(0);\n+\t\tFlinkLogicalTableSourceScan scan = call.rel(1);\n+\t\tTableSourceTable tableSourceTable = scan.getTable().unwrap(TableSourceTable.class);\n+\t\tint offset = sort.offset == null ? 0 : RexLiteral.intValue(sort.offset);\n+\t\tint limit = offset + RexLiteral.intValue(sort.fetch);\n+\n+\t\tTableSourceTable newTableSourceTable = applyLimit(limit, tableSourceTable);\n+\n+\t\tFlinkLogicalTableSourceScan newScan = FlinkLogicalTableSourceScan.create(scan.getCluster(), newTableSourceTable);\n+\t\tSort newSort = sort.copy(sort.getTraitSet(), Collections.singletonList(newScan));\n+\t\tcall.transformTo(newSort);\n+\t}\n+\n+\tprivate TableSourceTable applyLimit(\n+\t\tlong limit,\n+\t\tFlinkPreparingTableBase relOptTable) {\n+\t\tTableSourceTable oldTableSourceTable = relOptTable.unwrap(TableSourceTable.class);\n+\t\tDynamicTableSource newTableSource = oldTableSourceTable.tableSource().copy();\n+\t\t((SupportsLimitPushDown) newTableSource).applyLimit(limit);\n+\n+\t\tFlinkStatistic statistic = relOptTable.getStatistic();\n+\t\tlong newRowCount = 0;\n+\t\tif (statistic.getRowCount() != null) {\n+\t\t\tnewRowCount = Math.min(limit, statistic.getRowCount().longValue());\n+\t\t} else {\n+\t\t\tnewRowCount = limit;\n+\t\t}\n+\t\t// update TableStats after limit push down\n+\t\tTableStats newTableStats = new TableStats(newRowCount);\n+\t\tFlinkStatistic newStatistic = FlinkStatistic.builder()\n+\t\t\t.statistic(statistic)\n+\t\t\t.tableStats(newTableStats)\n+\t\t\t.build();\n+\n+\t\t// update extraDigests\n+\t\tString[] newExtraDigests = new String[0];\n+\t\tif (limit >= 0) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0324c22a9b2c85f04481966b2a78f72e92b9c88c"}, "originalPosition": 111}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTQ0MTE3OQ==", "bodyText": "why the digest pattern is not limit=5 ?", "url": "https://github.com/apache/flink/pull/12964#discussion_r461441179", "createdAt": "2020-07-28T09:19:38Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/rules/logical/PushLimitIntoLegacyTableSourceScanRuleTest.xml", "diffHunk": "@@ -0,0 +1,171 @@\n+<?xml version=\"1.0\" ?>\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one or more\n+contributor license agreements.  See the NOTICE file distributed with\n+this work for additional information regarding copyright ownership.\n+The ASF licenses this file to you under the Apache License, Version 2.0\n+(the \"License\"); you may not use this file except in compliance with\n+the License.  You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+<Root>\n+\t<TestCase name=\"testCanPushdownLimitWithoutOffset\">\n+\t\t<Resource name=\"sql\">\n+\t\t\t<![CDATA[SELECT * FROM LimitTable LIMIT 5]]>\n+\t\t</Resource>\n+\t\t<Resource name=\"planBefore\">\n+\t\t\t<![CDATA[\n+LogicalSort(fetch=[5])\n++- LogicalProject(a=[$0], b=[$1], c=[$2])\n+   +- LogicalTableScan(table=[[default_catalog, default_database, LimitTable]])\n+]]>\n+\t\t</Resource>\n+\t\t<Resource name=\"planAfter\">\n+\t\t\t<![CDATA[\n+LogicalProject(a=[$0], b=[$1], c=[$2])\n++- FlinkLogicalSort(fetch=[5])\n+   +- FlinkLogicalLegacyTableSourceScan(table=[[default_catalog, default_database, LimitTable, source: [limit: 5]]], fields=[a, b, c])", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0324c22a9b2c85f04481966b2a78f72e92b9c88c"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTQ0MzY5NQ==", "bodyText": "nit: unify the case of Pushdown \uff1f", "url": "https://github.com/apache/flink/pull/12964#discussion_r461443695", "createdAt": "2020-07-28T09:23:54Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/rules/logical/PushLimitIntoLegacyTableSourceScanRuleTest.scala", "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.flink.table.planner.plan.rules.logical\n+\n+import org.apache.calcite.plan.hep.HepMatchOrder\n+import org.apache.calcite.rel.rules.SortProjectTransposeRule\n+import org.apache.calcite.tools.RuleSets\n+import org.apache.flink.table.api.SqlParserException\n+import org.apache.flink.table.planner.plan.nodes.logical.{FlinkLogicalLegacyTableSourceScan, FlinkLogicalSort}\n+import org.apache.flink.table.planner.plan.optimize.program.{FlinkBatchProgram, FlinkHepRuleSetProgramBuilder, HEP_RULES_EXECUTION_TYPE}\n+import org.apache.flink.table.planner.utils.{TableConfigUtils, TableTestBase}\n+import org.junit.{Before, Test}\n+\n+/**\n+ * Test for [[PushLimitIntoLegacyTableSourceScanRule]].\n+ */\n+class PushLimitIntoLegacyTableSourceScanRuleTest extends TableTestBase {\n+  protected val util = batchTestUtil()\n+\n+  @Before\n+  def setup(): Unit = {\n+    util.buildBatchProgram(FlinkBatchProgram.DEFAULT_REWRITE)\n+    val calciteConfig = TableConfigUtils.getCalciteConfig(util.tableEnv.getConfig)\n+    calciteConfig.getBatchProgram.get.addLast(\n+      \"rules\",\n+      FlinkHepRuleSetProgramBuilder.newBuilder\n+        .setHepRulesExecutionType(HEP_RULES_EXECUTION_TYPE.RULE_COLLECTION)\n+        .setHepMatchOrder(HepMatchOrder.BOTTOM_UP)\n+        .add(RuleSets.ofList(PushLimitIntoLegacyTableSourceScanRule.INSTANCE,\n+          SortProjectTransposeRule.INSTANCE,\n+          // converts calcite rel(RelNode) to flink rel(FlinkRelNode)\n+          FlinkLogicalSort.BATCH_CONVERTER,\n+          FlinkLogicalLegacyTableSourceScan.CONVERTER))\n+        .build()\n+    )\n+\n+    val ddl =\n+      s\"\"\"\n+         |CREATE TABLE LimitTable (\n+         |  a int,\n+         |  b bigint,\n+         |  c string\n+         |) WITH (\n+         |  'connector.type' = 'TestLimitableTableSource',\n+         |  'is-bounded' = 'true'\n+         |)\n+       \"\"\".stripMargin\n+    util.tableEnv.executeSql(ddl)\n+  }\n+\n+  @Test(expected = classOf[SqlParserException])\n+  def testLimitWithNegativeOffset(): Unit = {\n+    util.verifyPlan(\"SELECT a, c FROM LimitTable LIMIT 10 OFFSET -1\")\n+  }\n+\n+  @Test(expected = classOf[SqlParserException])\n+  def testNegativeLimitWithoutOffset(): Unit = {\n+    util.verifyPlan(\"SELECT * FROM LimitTable LIMIT -1\")\n+  }\n+\n+  @Test(expected = classOf[SqlParserException])\n+  def testMysqlLimit(): Unit = {\n+    util.verifyPlan(\"SELECT a, c FROM LimitTable LIMIT 1, 10\")\n+  }\n+\n+  @Test\n+  def testCanPushdownLimitWithoutOffset(): Unit = {\n+    util.verifyPlan(\"SELECT * FROM LimitTable LIMIT 5\")\n+  }\n+\n+  @Test\n+  def testCanPushdownLimitWithOffset(): Unit = {\n+    util.verifyPlan(\"SELECT a, c FROM LimitTable LIMIT 10 OFFSET 1\")\n+  }\n+\n+  @Test\n+  def testCanPushdownFetchWithOffset(): Unit = {\n+    util.verifyPlan(\"SELECT a, c FROM LimitTable OFFSET 10 ROWS FETCH NEXT 10 ROWS ONLY\")\n+  }\n+\n+  @Test\n+  def testCanPushdownFetchWithoutOffset(): Unit = {\n+    util.verifyPlan(\"SELECT a, c FROM LimitTable FETCH FIRST 10 ROWS ONLY\")\n+  }\n+\n+  @Test\n+  def testCannotPushDownWithoutLimit(): Unit = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0324c22a9b2c85f04481966b2a78f72e92b9c88c"}, "originalPosition": 102}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "141bd8e09efe79e55398adbdd0916592231fdfbf", "author": {"user": {"login": "liuyongvs", "name": "Jacky Lau"}}, "url": "https://github.com/apache/flink/commit/141bd8e09efe79e55398adbdd0916592231fdfbf", "committedDate": "2020-07-28T10:49:54Z", "message": "[FLINK-17426][blink-planner] fix reviews to unify the unit test."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU3MjgyMzc4", "url": "https://github.com/apache/flink/pull/12964#pullrequestreview-457282378", "createdAt": "2020-07-29T07:59:10Z", "commit": {"oid": "141bd8e09efe79e55398adbdd0916592231fdfbf"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQwNzo1OToxMVrOG4tClw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQwNzo1OToxMVrOG4tClw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjExMTM4Mw==", "bodyText": "why the digest pattern is not limit=5 ?", "url": "https://github.com/apache/flink/pull/12964#discussion_r462111383", "createdAt": "2020-07-29T07:59:11Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/rules/logical/PushLimitIntoTableSourceScanRuleTest.xml", "diffHunk": "@@ -0,0 +1,171 @@\n+<?xml version=\"1.0\" ?>\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one or more\n+contributor license agreements.  See the NOTICE file distributed with\n+this work for additional information regarding copyright ownership.\n+The ASF licenses this file to you under the Apache License, Version 2.0\n+(the \"License\"); you may not use this file except in compliance with\n+the License.  You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+<Root>\n+\t<TestCase name=\"testCanPushdownLimitWithoutOffset\">\n+\t\t<Resource name=\"sql\">\n+\t\t\t<![CDATA[SELECT a, c FROM LimitTable LIMIT 5]]>\n+\t\t</Resource>\n+\t\t<Resource name=\"planBefore\">\n+\t\t\t<![CDATA[\n+LogicalSort(fetch=[5])\n++- LogicalProject(a=[$0], c=[$2])\n+   +- LogicalTableScan(table=[[default_catalog, default_database, LimitTable]])\n+]]>\n+\t\t</Resource>\n+\t\t<Resource name=\"planAfter\">\n+\t\t\t<![CDATA[\n+LogicalProject(a=[$0], c=[$2])\n++- FlinkLogicalSort(fetch=[5])\n+   +- FlinkLogicalTableSourceScan(table=[[default_catalog, default_database, LimitTable, limit=[5]]], fields=[a, b, c])\n+]]>\n+\t\t</Resource>\n+\t</TestCase>\n+\t<TestCase name=\"testCanPushdownFetchWithOffset\">\n+\t\t<Resource name=\"sql\">\n+\t\t\t<![CDATA[SELECT a, c FROM LimitTable OFFSET 10 ROWS FETCH NEXT 10 ROWS ONLY]]>\n+\t\t</Resource>\n+\t\t<Resource name=\"planBefore\">\n+\t\t\t<![CDATA[\n+LogicalSort(offset=[10], fetch=[10])\n++- LogicalProject(a=[$0], c=[$2])\n+   +- LogicalTableScan(table=[[default_catalog, default_database, LimitTable]])\n+]]>\n+\t\t</Resource>\n+\t\t<Resource name=\"planAfter\">\n+\t\t\t<![CDATA[\n+LogicalProject(a=[$0], c=[$2])\n++- FlinkLogicalSort(offset=[10], fetch=[10])\n+   +- FlinkLogicalTableSourceScan(table=[[default_catalog, default_database, LimitTable, limit=[20]]], fields=[a, b, c])\n+]]>\n+\t\t</Resource>\n+\t</TestCase>\n+\t<TestCase name=\"testCanPushdownFetchWithoutOffset\">\n+\t\t<Resource name=\"sql\">\n+\t\t\t<![CDATA[SELECT a, c FROM LimitTable FETCH FIRST 10 ROWS ONLY]]>\n+\t\t</Resource>\n+\t\t<Resource name=\"planBefore\">\n+\t\t\t<![CDATA[\n+LogicalSort(fetch=[10])\n++- LogicalProject(a=[$0], c=[$2])\n+   +- LogicalTableScan(table=[[default_catalog, default_database, LimitTable]])\n+]]>\n+\t\t</Resource>\n+\t\t<Resource name=\"planAfter\">\n+\t\t\t<![CDATA[\n+LogicalProject(a=[$0], c=[$2])\n++- FlinkLogicalSort(fetch=[10])\n+   +- FlinkLogicalTableSourceScan(table=[[default_catalog, default_database, LimitTable, limit=[10]]], fields=[a, b, c])\n+]]>\n+\t\t</Resource>\n+\t</TestCase>\n+\t<TestCase name=\"testCannotPushDownWithOrderBy\">\n+\t\t<Resource name=\"sql\">\n+\t\t\t<![CDATA[SELECT a, c FROM LimitTable ORDER BY c LIMIT 10]]>\n+\t\t</Resource>\n+\t\t<Resource name=\"planBefore\">\n+\t\t\t<![CDATA[\n+LogicalSort(sort0=[$1], dir0=[ASC-nulls-first], fetch=[10])\n++- LogicalProject(a=[$0], c=[$2])\n+   +- LogicalTableScan(table=[[default_catalog, default_database, LimitTable]])\n+]]>\n+\t\t</Resource>\n+\t\t<Resource name=\"planAfter\">\n+\t\t\t<![CDATA[\n+LogicalProject(a=[$0], c=[$2])\n++- FlinkLogicalSort(sort0=[$2], dir0=[ASC-nulls-first], fetch=[10])\n+   +- FlinkLogicalTableSourceScan(table=[[default_catalog, default_database, LimitTable]], fields=[a, b, c])\n+]]>\n+\t\t</Resource>\n+\t</TestCase>\n+\t<TestCase name=\"testCanPushdownLimitWithOffset\">\n+\t\t<Resource name=\"sql\">\n+\t\t\t<![CDATA[SELECT a, c FROM LimitTable LIMIT 10 OFFSET 1]]>\n+\t\t</Resource>\n+\t\t<Resource name=\"planBefore\">\n+\t\t\t<![CDATA[\n+LogicalSort(offset=[1], fetch=[10])\n++- LogicalProject(a=[$0], c=[$2])\n+   +- LogicalTableScan(table=[[default_catalog, default_database, LimitTable]])\n+]]>\n+\t\t</Resource>\n+\t\t<Resource name=\"planAfter\">\n+\t\t\t<![CDATA[\n+LogicalProject(a=[$0], c=[$2])\n++- FlinkLogicalSort(offset=[1], fetch=[10])\n+   +- FlinkLogicalTableSourceScan(table=[[default_catalog, default_database, LimitTable, limit=[11]]], fields=[a, b, c])\n+]]>\n+\t\t</Resource>\n+\t</TestCase>\n+\t<TestCase name=\"testLimitWithoutOffset\">\n+\t\t<Resource name=\"sql\">\n+\t\t\t<![CDATA[SELECT * FROM LimitTable LIMIT 5]]>\n+\t\t</Resource>\n+\t\t<Resource name=\"planBefore\">\n+\t\t\t<![CDATA[\n+LogicalSort(fetch=[5])\n++- LogicalProject(a=[$0], b=[$1], c=[$2])\n+   +- LogicalTableScan(table=[[default_catalog, default_database, LimitTable]])\n+]]>\n+\t\t</Resource>\n+\t\t<Resource name=\"planAfter\">\n+\t\t\t<![CDATA[\n+LogicalProject(a=[$0], b=[$1], c=[$2])\n++- FlinkLogicalSort(fetch=[5])\n+   +- FlinkLogicalTableSourceScan(table=[[default_catalog, default_database, LimitTable, source: [limit: 5]]], fields=[a, b, c])", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "141bd8e09efe79e55398adbdd0916592231fdfbf"}, "originalPosition": 129}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "34dd37d992e9d3c3a0bf92e660575571d83b407c", "author": {"user": {"login": "liuyongvs", "name": "Jacky Lau"}}, "url": "https://github.com/apache/flink/commit/34dd37d992e9d3c3a0bf92e660575571d83b407c", "committedDate": "2020-07-30T06:49:51Z", "message": "[FLINK-17426][blink-planner] remove the redundant content."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU4MTYyODc5", "url": "https://github.com/apache/flink/pull/12964#pullrequestreview-458162879", "createdAt": "2020-07-30T07:49:36Z", "commit": {"oid": "34dd37d992e9d3c3a0bf92e660575571d83b407c"}, "state": "APPROVED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQwNzo0OTozN1rOG5X5dA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQwNzo1MDo0OVrOG5X8JA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjgxMzU1Ng==", "bodyText": "nit:  -> .tableStats(new TableStats(newRowCount))", "url": "https://github.com/apache/flink/pull/12964#discussion_r462813556", "createdAt": "2020-07-30T07:49:37Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushLimitIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsLimitPushDown;\n+import org.apache.flink.table.plan.stats.TableStats;\n+import org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalSort;\n+import org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalTableSourceScan;\n+import org.apache.flink.table.planner.plan.schema.FlinkPreparingTableBase;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Sort;\n+import org.apache.calcite.rex.RexLiteral;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+\n+/**\n+ * Planner rule that tries to push limit into a {@link FlinkLogicalTableSourceScan},\n+ * which table is a {@link TableSourceTable}. And the table source in the table is a {@link SupportsLimitPushDown}.\n+ * The original limit will still be retained.\n+ * The reasons why the limit still be retained:\n+ * 1.If the source is required to return the exact number of limit number, the implementation\n+ * of the source is highly required. The source is required to accurately control the record\n+ * number of split, and the parallelism setting also need to be adjusted accordingly.\n+ * 2.When remove the limit, maybe filter will be pushed down to the source after limit pushed\n+ * down. The source need know it should do limit first and do the filter later, it is hard to\n+ * implement.\n+ * 3.We can support limit with offset, we can push down offset + fetch to table source.\n+ */\n+public class PushLimitIntoTableSourceScanRule extends RelOptRule {\n+\tpublic static final PushLimitIntoTableSourceScanRule INSTANCE = new PushLimitIntoTableSourceScanRule();\n+\n+\tpublic PushLimitIntoTableSourceScanRule() {\n+\t\tsuper(operand(FlinkLogicalSort.class,\n+\t\t\toperand(FlinkLogicalTableSourceScan.class, none())),\n+\t\t\t\"PushLimitIntoTableSourceScanRule\");\n+\t}\n+\n+\t@Override\n+\tpublic boolean matches(RelOptRuleCall call) {\n+\t\tSort sort = call.rel(0);\n+\t\tTableSourceTable tableSourceTable = call.rel(1).getTable().unwrap(TableSourceTable.class);\n+\n+\t\t// a limit can be pushed down only if it satisfies the two conditions: 1) do not have order by keys, 2) have limit.\n+\t\tboolean onlyLimit = sort.getCollation().getFieldCollations().isEmpty() && sort.fetch != null;\n+\t\treturn onlyLimit\n+\t\t\t&& tableSourceTable != null\n+\t\t\t&& tableSourceTable.tableSource() instanceof SupportsLimitPushDown\n+\t\t\t&& Arrays.stream(tableSourceTable.extraDigests()).noneMatch(str -> str.startsWith(\"limit=[\"));\n+\t}\n+\n+\t@Override\n+\tpublic void onMatch(RelOptRuleCall call) {\n+\t\tSort sort = call.rel(0);\n+\t\tFlinkLogicalTableSourceScan scan = call.rel(1);\n+\t\tTableSourceTable tableSourceTable = scan.getTable().unwrap(TableSourceTable.class);\n+\t\tint offset = sort.offset == null ? 0 : RexLiteral.intValue(sort.offset);\n+\t\tint limit = offset + RexLiteral.intValue(sort.fetch);\n+\n+\t\tTableSourceTable newTableSourceTable = applyLimit(limit, tableSourceTable);\n+\n+\t\tFlinkLogicalTableSourceScan newScan = FlinkLogicalTableSourceScan.create(scan.getCluster(), newTableSourceTable);\n+\t\tSort newSort = sort.copy(sort.getTraitSet(), Collections.singletonList(newScan));\n+\t\tcall.transformTo(newSort);\n+\t}\n+\n+\tprivate TableSourceTable applyLimit(\n+\t\tlong limit,\n+\t\tFlinkPreparingTableBase relOptTable) {\n+\t\tTableSourceTable oldTableSourceTable = relOptTable.unwrap(TableSourceTable.class);\n+\t\tDynamicTableSource newTableSource = oldTableSourceTable.tableSource().copy();\n+\t\t((SupportsLimitPushDown) newTableSource).applyLimit(limit);\n+\n+\t\tFlinkStatistic statistic = relOptTable.getStatistic();\n+\t\tlong newRowCount = 0;\n+\t\tif (statistic.getRowCount() != null) {\n+\t\t\tnewRowCount = Math.min(limit, statistic.getRowCount().longValue());\n+\t\t} else {\n+\t\t\tnewRowCount = limit;\n+\t\t}\n+\t\t// update TableStats after limit push down\n+\t\tTableStats newTableStats = new TableStats(newRowCount);\n+\t\tFlinkStatistic newStatistic = FlinkStatistic.builder()\n+\t\t\t.statistic(statistic)\n+\t\t\t.tableStats(newTableStats)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "34dd37d992e9d3c3a0bf92e660575571d83b407c"}, "originalPosition": 106}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjgxNDI0NA==", "bodyText": "nit: newExtraDigests is only used once, just move new String[] {\"limit=[\" + limit + \"]\"} into here.", "url": "https://github.com/apache/flink/pull/12964#discussion_r462814244", "createdAt": "2020-07-30T07:50:49Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushLimitIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsLimitPushDown;\n+import org.apache.flink.table.plan.stats.TableStats;\n+import org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalSort;\n+import org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalTableSourceScan;\n+import org.apache.flink.table.planner.plan.schema.FlinkPreparingTableBase;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Sort;\n+import org.apache.calcite.rex.RexLiteral;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+\n+/**\n+ * Planner rule that tries to push limit into a {@link FlinkLogicalTableSourceScan},\n+ * which table is a {@link TableSourceTable}. And the table source in the table is a {@link SupportsLimitPushDown}.\n+ * The original limit will still be retained.\n+ * The reasons why the limit still be retained:\n+ * 1.If the source is required to return the exact number of limit number, the implementation\n+ * of the source is highly required. The source is required to accurately control the record\n+ * number of split, and the parallelism setting also need to be adjusted accordingly.\n+ * 2.When remove the limit, maybe filter will be pushed down to the source after limit pushed\n+ * down. The source need know it should do limit first and do the filter later, it is hard to\n+ * implement.\n+ * 3.We can support limit with offset, we can push down offset + fetch to table source.\n+ */\n+public class PushLimitIntoTableSourceScanRule extends RelOptRule {\n+\tpublic static final PushLimitIntoTableSourceScanRule INSTANCE = new PushLimitIntoTableSourceScanRule();\n+\n+\tpublic PushLimitIntoTableSourceScanRule() {\n+\t\tsuper(operand(FlinkLogicalSort.class,\n+\t\t\toperand(FlinkLogicalTableSourceScan.class, none())),\n+\t\t\t\"PushLimitIntoTableSourceScanRule\");\n+\t}\n+\n+\t@Override\n+\tpublic boolean matches(RelOptRuleCall call) {\n+\t\tSort sort = call.rel(0);\n+\t\tTableSourceTable tableSourceTable = call.rel(1).getTable().unwrap(TableSourceTable.class);\n+\n+\t\t// a limit can be pushed down only if it satisfies the two conditions: 1) do not have order by keys, 2) have limit.\n+\t\tboolean onlyLimit = sort.getCollation().getFieldCollations().isEmpty() && sort.fetch != null;\n+\t\treturn onlyLimit\n+\t\t\t&& tableSourceTable != null\n+\t\t\t&& tableSourceTable.tableSource() instanceof SupportsLimitPushDown\n+\t\t\t&& Arrays.stream(tableSourceTable.extraDigests()).noneMatch(str -> str.startsWith(\"limit=[\"));\n+\t}\n+\n+\t@Override\n+\tpublic void onMatch(RelOptRuleCall call) {\n+\t\tSort sort = call.rel(0);\n+\t\tFlinkLogicalTableSourceScan scan = call.rel(1);\n+\t\tTableSourceTable tableSourceTable = scan.getTable().unwrap(TableSourceTable.class);\n+\t\tint offset = sort.offset == null ? 0 : RexLiteral.intValue(sort.offset);\n+\t\tint limit = offset + RexLiteral.intValue(sort.fetch);\n+\n+\t\tTableSourceTable newTableSourceTable = applyLimit(limit, tableSourceTable);\n+\n+\t\tFlinkLogicalTableSourceScan newScan = FlinkLogicalTableSourceScan.create(scan.getCluster(), newTableSourceTable);\n+\t\tSort newSort = sort.copy(sort.getTraitSet(), Collections.singletonList(newScan));\n+\t\tcall.transformTo(newSort);\n+\t}\n+\n+\tprivate TableSourceTable applyLimit(\n+\t\tlong limit,\n+\t\tFlinkPreparingTableBase relOptTable) {\n+\t\tTableSourceTable oldTableSourceTable = relOptTable.unwrap(TableSourceTable.class);\n+\t\tDynamicTableSource newTableSource = oldTableSourceTable.tableSource().copy();\n+\t\t((SupportsLimitPushDown) newTableSource).applyLimit(limit);\n+\n+\t\tFlinkStatistic statistic = relOptTable.getStatistic();\n+\t\tlong newRowCount = 0;\n+\t\tif (statistic.getRowCount() != null) {\n+\t\t\tnewRowCount = Math.min(limit, statistic.getRowCount().longValue());\n+\t\t} else {\n+\t\t\tnewRowCount = limit;\n+\t\t}\n+\t\t// update TableStats after limit push down\n+\t\tTableStats newTableStats = new TableStats(newRowCount);\n+\t\tFlinkStatistic newStatistic = FlinkStatistic.builder()\n+\t\t\t.statistic(statistic)\n+\t\t\t.tableStats(newTableStats)\n+\t\t\t.build();\n+\n+\t\t// update extraDigests\n+\t\tString[] newExtraDigests = new String[]{\"limit=[\" + limit + \"]\"};\n+\n+\t\treturn oldTableSourceTable.copy(\n+\t\t\tnewTableSource,\n+\t\t\tnewStatistic,\n+\t\t\tnewExtraDigests", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "34dd37d992e9d3c3a0bf92e660575571d83b407c"}, "originalPosition": 115}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU5MDg5NDk5", "url": "https://github.com/apache/flink/pull/12964#pullrequestreview-459089499", "createdAt": "2020-07-31T10:56:35Z", "commit": {"oid": "34dd37d992e9d3c3a0bf92e660575571d83b407c"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2937, "cost": 1, "resetAt": "2021-10-28T16:48:13Z"}}}