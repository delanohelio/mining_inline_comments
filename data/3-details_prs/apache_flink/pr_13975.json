{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTE3MTA4NTU0", "number": 13975, "title": "[FLINK-20041][connector kafka] Support watermark push down for kafka \u2026", "bodyText": "\u2026in table API\n\nWhat is the purpose of the change\nSupport watemrark push down for kafka connector in table api.\nBrief change log\n\nKafkaDynamicTablesSource implements interface SupportWatermarkPushDown\n\nVerifying this change\nThis change added tests and can be verified as follows:\n\nAdded an IT case that read the data from the different partition where time is not aligned among each partition.\n\nDoes this pull request potentially affect one of the following parts:\n\nDependencies (does it add or upgrade a dependency): (yes / no)\nThe public API, i.e., is any changed class annotated with @Public(Evolving): (yes / no)\nThe serializers: (yes / no / don't know)\nThe runtime per-record code paths (performance sensitive): (yes / no / don't know)\nAnything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (yes / no / don't know)\nThe S3 file system connector: (yes / no / don't know)\n\nDocumentation\n\nDoes this pull request introduce a new feature? (yes / no)\nIf yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)", "createdAt": "2020-11-07T08:47:18Z", "url": "https://github.com/apache/flink/pull/13975", "merged": true, "mergeCommit": {"oid": "24d612bd4337f1443f379fa3bac2b6050b3d32c8"}, "closed": true, "closedAt": "2020-11-08T03:37:27Z", "author": {"login": "fsk119"}, "timelineItems": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdaHnF3AH2gAyNTE3MTA4NTU0OmUxYjcxNzQwNDI5MmFjYjI3MjIyOThiZjU4ZTgxNjIyMDQyZDliZjQ=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdaXtZUAFqTUyNTcwMTczMg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "e1b717404292acb2722298bf58e81622042d9bf4", "author": {"user": {"login": "fsk119", "name": "Shengkai "}}, "url": "https://github.com/apache/flink/commit/e1b717404292acb2722298bf58e81622042d9bf4", "committedDate": "2020-11-07T08:51:18Z", "message": "[FLINK-20041][connector kafka] Support watermark push down for kafka in table API"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "756f10b8dec53f40a49fda61e802b5ad1e0d512b", "author": {"user": {"login": "fsk119", "name": "Shengkai "}}, "url": "https://github.com/apache/flink/commit/756f10b8dec53f40a49fda61e802b5ad1e0d512b", "committedDate": "2020-11-07T08:39:09Z", "message": "[FLINK-20041][connector kafka] Support watermark push down for kafka in table API"}, "afterCommit": {"oid": "e1b717404292acb2722298bf58e81622042d9bf4", "author": {"user": {"login": "fsk119", "name": "Shengkai "}}, "url": "https://github.com/apache/flink/commit/e1b717404292acb2722298bf58e81622042d9bf4", "committedDate": "2020-11-07T08:51:18Z", "message": "[FLINK-20041][connector kafka] Support watermark push down for kafka in table API"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5947caaa8eca977df7bade88b4c8b576ebc06d2b", "author": {"user": {"login": "fsk119", "name": "Shengkai "}}, "url": "https://github.com/apache/flink/commit/5947caaa8eca977df7bade88b4c8b576ebc06d2b", "committedDate": "2020-11-07T09:34:49Z", "message": "refactor the test class"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI1NjU3MDg3", "url": "https://github.com/apache/flink/pull/13975#pullrequestreview-525657087", "createdAt": "2020-11-07T14:37:05Z", "commit": {"oid": "5947caaa8eca977df7bade88b4c8b576ebc06d2b"}, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wN1QxNDozNzowNVrOHvIdHQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wN1QxNDo0MzoxNlrOHvIfQg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTE4MzY0NQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \tprotected WatermarkStrategy<RowData> watermarkStrategy;\n          \n          \n            \n            \tprotected @Nullable WatermarkStrategy<RowData> watermarkStrategy;\n          \n      \n    \n    \n  \n\nMark it nullable.", "url": "https://github.com/apache/flink/pull/13975#discussion_r519183645", "createdAt": "2020-11-07T14:37:05Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/table/KafkaDynamicSource.java", "diffHunk": "@@ -73,6 +75,9 @@\n \t/** Metadata that is appended at the end of a physical source row. */\n \tprotected List<String> metadataKeys;\n \n+\t/** Watermark strategy that is used to generate per-partition watermark. */\n+\tprotected WatermarkStrategy<RowData> watermarkStrategy;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5947caaa8eca977df7bade88b4c8b576ebc06d2b"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTE4Mzc4OQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \t\t\treturn record.getInt(PARTITION_ID_FIELD_IN_SCHEMA) % partitions.length;\n          \n          \n            \n            \t\t\treturn partitions[record.getInt(PARTITION_ID_FIELD_IN_SCHEMA) % partitions.length];", "url": "https://github.com/apache/flink/pull/13975#discussion_r519183789", "createdAt": "2020-11-07T14:38:56Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/table/KafkaTableITCase.java", "diffHunk": "@@ -516,10 +518,103 @@ public void testKafkaSourceSinkWithKeyAndFullValue() throws Exception {\n \t\tdeleteTestTopic(topic);\n \t}\n \n+\t@Test\n+\tpublic void testPerPartitionWatermarkKafka() throws Exception {\n+\t\tif (isLegacyConnector) {\n+\t\t\treturn;\n+\t\t}\n+\t\t// we always use a different topic name for each parameterized topic,\n+\t\t// in order to make sure the topic can be created.\n+\t\tfinal String topic = \"per_partition_watermark_topic_\" + format;\n+\t\tcreateTestTopic(topic, 4, 1);\n+\n+\t\t// ---------- Produce an event time stream into Kafka -------------------\n+\t\tString groupId = standardProps.getProperty(\"group.id\");\n+\t\tString bootstraps = standardProps.getProperty(\"bootstrap.servers\");\n+\n+\t\tfinal String createTable = String.format(\n+\t\t\t\t\"CREATE TABLE kafka (\\n\"\n+\t\t\t\t\t\t+ \"  `partition_id` INT,\\n\"\n+\t\t\t\t\t\t+ \"  `name` STRING,\\n\"\n+\t\t\t\t\t\t+ \"  `timestamp` TIMESTAMP(3),\\n\"\n+\t\t\t\t\t\t+ \"  WATERMARK FOR `timestamp` AS `timestamp`\\n\"\n+\t\t\t\t\t\t+ \") WITH (\\n\"\n+\t\t\t\t\t\t+ \"  'connector' = 'kafka',\\n\"\n+\t\t\t\t\t\t+ \"  'topic' = '%s',\\n\"\n+\t\t\t\t\t\t+ \"  'properties.bootstrap.servers' = '%s',\\n\"\n+\t\t\t\t\t\t+ \"  'properties.group.id' = '%s',\\n\"\n+\t\t\t\t\t\t+ \"  'scan.startup.mode' = 'earliest-offset',\\n\"\n+\t\t\t\t\t\t+ \"  'sink.partitioner' = 'org.apache.flink.streaming.connectors.kafka.table.KafkaTableITCase$TestPartitioner',\\n\"\n+\t\t\t\t\t\t+ \"  'format' = '%s'\\n\"\n+\t\t\t\t\t\t+ \")\",\n+\t\t\t\ttopic,\n+\t\t\t\tbootstraps,\n+\t\t\t\tgroupId,\n+\t\t\t\tformat);\n+\n+\t\ttEnv.executeSql(createTable);\n+\n+\t\tString initialValues = \"INSERT INTO kafka\\n\"\n+\t\t\t\t+ \"VALUES\\n\"\n+\t\t\t\t+ \" (0, 'partition-0-name-0', TIMESTAMP '2020-03-08 13:12:11.123'),\\n\"\n+\t\t\t\t+ \" (0, 'partition-0-name-1', TIMESTAMP '2020-03-08 14:12:12.223'),\\n\"\n+\t\t\t\t+ \" (0, 'partition-0-name-2', TIMESTAMP '2020-03-08 15:12:13.323'),\\n\"\n+\t\t\t\t+ \" (1, 'partition-1-name-0', TIMESTAMP '2020-03-09 13:13:11.123'),\\n\"\n+\t\t\t\t+ \" (1, 'partition-1-name-1', TIMESTAMP '2020-03-09 15:13:11.133'),\\n\"\n+\t\t\t\t+ \" (1, 'partition-1-name-2', TIMESTAMP '2020-03-09 16:13:11.143'),\\n\"\n+\t\t\t\t+ \" (2, 'partition-2-name-0', TIMESTAMP '2020-03-10 13:12:14.123'),\\n\"\n+\t\t\t\t+ \" (3, 'partition-3-name-0', TIMESTAMP '2020-03-11 17:12:11.123')\\n\";\n+\t\ttEnv.executeSql(initialValues).await();\n+\n+\t\t// ---------- Consume stream from Kafka -------------------\n+\t\tString createSink =\n+\t\t\t\t\"CREATE TABLE MySink(\"\n+\t\t\t\t\t\t+ \"  id INT,\"\n+\t\t\t\t\t\t+ \"  name STRING,\"\n+\t\t\t\t\t\t+ \"  ts TIMESTAMP(3)\"\n+\t\t\t\t\t\t+ \") WITH (\"\n+\t\t\t\t\t\t+ \"  'connector' = 'values',\"\n+\t\t\t\t\t\t+ \"  'sink-index-of-rowtime' = '2'\"\n+\t\t\t\t\t\t+ \")\";\n+\t\ttEnv.executeSql(createSink);\n+\t\ttEnv.executeSql(\"INSERT INTO MySink SELECT * FROM kafka\");\n+\t\tfinal List<String> expected = Arrays.asList(\n+\t\t\t\"0,partition-0-name-0,2020-03-08T13:12:11.123\",\n+\t\t\t\"0,partition-0-name-1,2020-03-08T14:12:12.223\",\n+\t\t\t\"0,partition-0-name-2,2020-03-08T15:12:13.323\",\n+\t\t\t\"1,partition-1-name-0,2020-03-09T13:13:11.123\",\n+\t\t\t\"1,partition-1-name-1,2020-03-09T15:13:11.133\",\n+\t\t\t\"1,partition-1-name-2,2020-03-09T16:13:11.143\",\n+\t\t\t\"2,partition-2-name-0,2020-03-10T13:12:14.123\",\n+\t\t\t\"3,partition-3-name-0,2020-03-11T17:12:11.123\"\n+\t\t);\n+\t\tKafkaTableTestUtils.waitingExpectedResults(\"MySink\", expected, Duration.ofSeconds(5));\n+\n+\t\t// ------------- cleanup -------------------\n+\n+\t\tdeleteTestTopic(topic);\n+\t}\n+\n+\n+\n \t// --------------------------------------------------------------------------------------------\n \t// Utilities\n \t// --------------------------------------------------------------------------------------------\n \n+\t/**\n+\t * Extract the partition id from the row and set it on the record.\n+\t */\n+\tpublic static class TestPartitioner extends FlinkKafkaPartitioner<RowData> {\n+\n+\t\tprivate static final long serialVersionUID = 1L;\n+\t\tprivate static final int PARTITION_ID_FIELD_IN_SCHEMA = 0;\n+\n+\t\t@Override\n+\t\tpublic int partition(RowData record, byte[] key, byte[] value, String targetTopic, int[] partitions) {\n+\t\t\treturn record.getInt(PARTITION_ID_FIELD_IN_SCHEMA) % partitions.length;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5947caaa8eca977df7bade88b4c8b576ebc06d2b"}, "originalPosition": 113}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTE4NDAwMg==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \t\t\t\t\t\t+ \"  'sink.partitioner' = 'org.apache.flink.streaming.connectors.kafka.table.KafkaTableITCase$TestPartitioner',\\n\"\n          \n          \n            \n            \t\t\t\t\t\t+ \"  'format' = '%s'\\n\"\n          \n          \n            \n            \t\t\t\t\t\t+ \")\",\n          \n          \n            \n            \t\t\t\ttopic,\n          \n          \n            \n            \t\t\t\tbootstraps,\n          \n          \n            \n            \t\t\t\tgroupId,\n          \n          \n            \n            \t\t\t\tformat);\n          \n          \n            \n            \t\t\t\t\t\t+ \"  'sink.partitioner' = '%s',\\n\"\n          \n          \n            \n            \t\t\t\t\t\t+ \"  'format' = '%s'\\n\"\n          \n          \n            \n            \t\t\t\t\t\t+ \")\",\n          \n          \n            \n            \t\t\t\ttopic,\n          \n          \n            \n            \t\t\t\tbootstraps,\n          \n          \n            \n            \t\t\t\tgroupId,\n          \n          \n            \n            \t\t\t\tformat,\n          \n          \n            \n            \t\t\t\tTestPartitioner.class.getCanonicalName());", "url": "https://github.com/apache/flink/pull/13975#discussion_r519184002", "createdAt": "2020-11-07T14:40:46Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/table/KafkaTableITCase.java", "diffHunk": "@@ -516,10 +518,103 @@ public void testKafkaSourceSinkWithKeyAndFullValue() throws Exception {\n \t\tdeleteTestTopic(topic);\n \t}\n \n+\t@Test\n+\tpublic void testPerPartitionWatermarkKafka() throws Exception {\n+\t\tif (isLegacyConnector) {\n+\t\t\treturn;\n+\t\t}\n+\t\t// we always use a different topic name for each parameterized topic,\n+\t\t// in order to make sure the topic can be created.\n+\t\tfinal String topic = \"per_partition_watermark_topic_\" + format;\n+\t\tcreateTestTopic(topic, 4, 1);\n+\n+\t\t// ---------- Produce an event time stream into Kafka -------------------\n+\t\tString groupId = standardProps.getProperty(\"group.id\");\n+\t\tString bootstraps = standardProps.getProperty(\"bootstrap.servers\");\n+\n+\t\tfinal String createTable = String.format(\n+\t\t\t\t\"CREATE TABLE kafka (\\n\"\n+\t\t\t\t\t\t+ \"  `partition_id` INT,\\n\"\n+\t\t\t\t\t\t+ \"  `name` STRING,\\n\"\n+\t\t\t\t\t\t+ \"  `timestamp` TIMESTAMP(3),\\n\"\n+\t\t\t\t\t\t+ \"  WATERMARK FOR `timestamp` AS `timestamp`\\n\"\n+\t\t\t\t\t\t+ \") WITH (\\n\"\n+\t\t\t\t\t\t+ \"  'connector' = 'kafka',\\n\"\n+\t\t\t\t\t\t+ \"  'topic' = '%s',\\n\"\n+\t\t\t\t\t\t+ \"  'properties.bootstrap.servers' = '%s',\\n\"\n+\t\t\t\t\t\t+ \"  'properties.group.id' = '%s',\\n\"\n+\t\t\t\t\t\t+ \"  'scan.startup.mode' = 'earliest-offset',\\n\"\n+\t\t\t\t\t\t+ \"  'sink.partitioner' = 'org.apache.flink.streaming.connectors.kafka.table.KafkaTableITCase$TestPartitioner',\\n\"\n+\t\t\t\t\t\t+ \"  'format' = '%s'\\n\"\n+\t\t\t\t\t\t+ \")\",\n+\t\t\t\ttopic,\n+\t\t\t\tbootstraps,\n+\t\t\t\tgroupId,\n+\t\t\t\tformat);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5947caaa8eca977df7bade88b4c8b576ebc06d2b"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTE4NDAzOQ==", "bodyText": "Do not hard code the class path.", "url": "https://github.com/apache/flink/pull/13975#discussion_r519184039", "createdAt": "2020-11-07T14:41:09Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/table/KafkaTableITCase.java", "diffHunk": "@@ -516,10 +518,103 @@ public void testKafkaSourceSinkWithKeyAndFullValue() throws Exception {\n \t\tdeleteTestTopic(topic);\n \t}\n \n+\t@Test\n+\tpublic void testPerPartitionWatermarkKafka() throws Exception {\n+\t\tif (isLegacyConnector) {\n+\t\t\treturn;\n+\t\t}\n+\t\t// we always use a different topic name for each parameterized topic,\n+\t\t// in order to make sure the topic can be created.\n+\t\tfinal String topic = \"per_partition_watermark_topic_\" + format;\n+\t\tcreateTestTopic(topic, 4, 1);\n+\n+\t\t// ---------- Produce an event time stream into Kafka -------------------\n+\t\tString groupId = standardProps.getProperty(\"group.id\");\n+\t\tString bootstraps = standardProps.getProperty(\"bootstrap.servers\");\n+\n+\t\tfinal String createTable = String.format(\n+\t\t\t\t\"CREATE TABLE kafka (\\n\"\n+\t\t\t\t\t\t+ \"  `partition_id` INT,\\n\"\n+\t\t\t\t\t\t+ \"  `name` STRING,\\n\"\n+\t\t\t\t\t\t+ \"  `timestamp` TIMESTAMP(3),\\n\"\n+\t\t\t\t\t\t+ \"  WATERMARK FOR `timestamp` AS `timestamp`\\n\"\n+\t\t\t\t\t\t+ \") WITH (\\n\"\n+\t\t\t\t\t\t+ \"  'connector' = 'kafka',\\n\"\n+\t\t\t\t\t\t+ \"  'topic' = '%s',\\n\"\n+\t\t\t\t\t\t+ \"  'properties.bootstrap.servers' = '%s',\\n\"\n+\t\t\t\t\t\t+ \"  'properties.group.id' = '%s',\\n\"\n+\t\t\t\t\t\t+ \"  'scan.startup.mode' = 'earliest-offset',\\n\"\n+\t\t\t\t\t\t+ \"  'sink.partitioner' = 'org.apache.flink.streaming.connectors.kafka.table.KafkaTableITCase$TestPartitioner',\\n\"\n+\t\t\t\t\t\t+ \"  'format' = '%s'\\n\"\n+\t\t\t\t\t\t+ \")\",\n+\t\t\t\ttopic,\n+\t\t\t\tbootstraps,\n+\t\t\t\tgroupId,\n+\t\t\t\tformat);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTE4NDAwMg=="}, "originalCommit": {"oid": "5947caaa8eca977df7bade88b4c8b576ebc06d2b"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTE4NDE5NA==", "bodyText": "If this option controls whether to drop late data, then the option name should be something like sink.drop-late-event=true.\nThe rowtime field can be derived from WATERMARK statement (can also be declared on sink), instead of a redundant option.\nBesides, I find you only support this option on append only sink, then you should throw exceptions if the sink is not append only during createDynamicTableSink.", "url": "https://github.com/apache/flink/pull/13975#discussion_r519184194", "createdAt": "2020-11-07T14:43:16Z", "author": {"login": "wuchong"}, "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/factories/TestValuesTableFactory.java", "diffHunk": "@@ -291,6 +291,14 @@ private static RowKind parseRowKind(String rowKindShortString) {\n \t\t\t\"Optional map of 'metadata_key:data_type'. The order will be alphabetically. \" +\n \t\t\t\"The metadata is part of the data when enabled.\");\n \n+\tprivate static final ConfigOption<Integer> SINK_INDEX_OF_ROWTIME = ConfigOptions\n+\t\t.key(\"sink-index-of-rowtime\")\n+\t\t.intType()\n+\t\t.defaultValue(-1)\n+\t\t.withDeprecatedKeys(\n+\t\t\t\"Option index of the rowtime field. The default value -1 indicate that don't drop \" +\n+\t\t\t\"the late data.\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5947caaa8eca977df7bade88b4c8b576ebc06d2b"}, "originalPosition": 10}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e9d52fa97e3da9a9b4c24b2ab17037820f26b75e", "author": {"user": {"login": "fsk119", "name": "Shengkai "}}, "url": "https://github.com/apache/flink/commit/e9d52fa97e3da9a9b4c24b2ab17037820f26b75e", "committedDate": "2020-11-07T15:13:51Z", "message": "Apply suggestions from code review\r\n\r\napply suggestions\n\nCo-authored-by: Jark Wu <imjark@gmail.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "57e3cb930c4a6b2424dc11a7bb8c01134c6912e9", "author": {"user": {"login": "fsk119", "name": "Shengkai "}}, "url": "https://github.com/apache/flink/commit/57e3cb930c4a6b2424dc11a7bb8c01134c6912e9", "committedDate": "2020-11-07T16:20:08Z", "message": "address the feedback"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI1NzAxNzMy", "url": "https://github.com/apache/flink/pull/13975#pullrequestreview-525701732", "createdAt": "2020-11-08T03:36:40Z", "commit": {"oid": "57e3cb930c4a6b2424dc11a7bb8c01134c6912e9"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4479, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}