{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDQ5NDY2ODM1", "number": 12908, "title": "[FLINK-18449][table sql/api]Kafka topic discovery & partition discove\u2026", "bodyText": "\u2026ry dynamically in table api\n\nWhat is the purpose of the change\nEnable Kafka Connector topic discovery & partition discovery in table api.\nBrief change log\n\nExpose option 'topic-pattern' and 'scan.topic-partition-discovery.interval'\nAdd validation for source when setting 'topic-pattern' and 'topic' together and setting 'topic-pattern' for sink.\nRead value from Table option and use the value to build kafka consumer.\n\nVerifying this change\nThis change added tests and can be verified as follows:\n\nAdded integration tests for new features\nAdded test that validates that setting topic and topic pattern together will fail and setting 'topic-pattern' for sink will fail.\n\nDoes this pull request potentially affect one of the following parts:\n\nDependencies (does it add or upgrade a dependency): (yes / no)\nThe public API, i.e., is any changed class annotated with @Public(Evolving): (yes / no)\nThe serializers: (yes / no / don't know)\nThe runtime per-record code paths (performance sensitive): (yes / no / don't know)\nAnything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (yes / no / don't know)\nThe S3 file system connector: (yes / no / don't know)\n\nDocumentation\n\nDoes this pull request introduce a new feature? (yes / no)\nIf yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)", "createdAt": "2020-07-15T13:03:00Z", "url": "https://github.com/apache/flink/pull/12908", "merged": true, "mergeCommit": {"oid": "b8ee51b832d00d4615e249f384e13dcaa1f14ddf"}, "closed": true, "closedAt": "2020-08-20T02:55:21Z", "author": {"login": "fsk119"}, "timelineItems": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABc4-cukAFqTQ1NTU3MTYwOQ==", "endCursor": "Y3Vyc29yOnYyOpPPAAABdAUCRUgFqTQ3MDExNzY3Ng==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU1NTcxNjA5", "url": "https://github.com/apache/flink/pull/12908#pullrequestreview-455571609", "createdAt": "2020-07-27T08:21:27Z", "commit": {"oid": "1a4e5ee2a2f11ba650a98c1a211cea75736fe79d"}, "state": "COMMENTED", "comments": {"totalCount": 22, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QwODoyMToyN1rOG3YVxw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QwOTozMDoxNFrOG3a0kw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDcyMzY1NQ==", "bodyText": "Indent.", "url": "https://github.com/apache/flink/pull/12908#discussion_r460723655", "createdAt": "2020-07-27T08:21:27Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/table/Kafka010DynamicSource.java", "diffHunk": "@@ -73,17 +80,26 @@ public Kafka010DynamicSource(\n \n \t@Override\n \tprotected FlinkKafkaConsumerBase<RowData> createKafkaConsumer(\n-\t\t\tString topic,\n+\t\t\tList<String> topics,\n \t\t\tProperties properties,\n \t\t\tDeserializationSchema<RowData> deserializationSchema) {\n-\t\treturn new FlinkKafkaConsumer010<>(topic, deserializationSchema, properties);\n+\t\treturn new FlinkKafkaConsumer010<>(topics, deserializationSchema, properties);\n+\t}\n+\n+\t@Override\n+\tprotected FlinkKafkaConsumerBase<RowData> createKafkaConsumer(\n+\t\tPattern pattern,\n+\t\tProperties properties,\n+\t\tDeserializationSchema<RowData> deserializationSchema) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1a4e5ee2a2f11ba650a98c1a211cea75736fe79d"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDcyNTgwOQ==", "bodyText": "topicPattern", "url": "https://github.com/apache/flink/pull/12908#discussion_r460725809", "createdAt": "2020-07-27T08:25:07Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/table/Kafka010DynamicSource.java", "diffHunk": "@@ -73,17 +80,26 @@ public Kafka010DynamicSource(\n \n \t@Override\n \tprotected FlinkKafkaConsumerBase<RowData> createKafkaConsumer(\n-\t\t\tString topic,\n+\t\t\tList<String> topics,\n \t\t\tProperties properties,\n \t\t\tDeserializationSchema<RowData> deserializationSchema) {\n-\t\treturn new FlinkKafkaConsumer010<>(topic, deserializationSchema, properties);\n+\t\treturn new FlinkKafkaConsumer010<>(topics, deserializationSchema, properties);\n+\t}\n+\n+\t@Override\n+\tprotected FlinkKafkaConsumerBase<RowData> createKafkaConsumer(\n+\t\tPattern pattern,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1a4e5ee2a2f11ba650a98c1a211cea75736fe79d"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDcyNTkyOA==", "bodyText": "Indent.", "url": "https://github.com/apache/flink/pull/12908#discussion_r460725928", "createdAt": "2020-07-27T08:25:19Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-connector-kafka-0.11/src/main/java/org/apache/flink/streaming/connectors/kafka/table/Kafka011DynamicSource.java", "diffHunk": "@@ -73,17 +80,26 @@ public Kafka011DynamicSource(\n \n \t@Override\n \tprotected FlinkKafkaConsumerBase<RowData> createKafkaConsumer(\n-\t\t\tString topic,\n+\t\t\tList<String> topics,\n \t\t\tProperties properties,\n \t\t\tDeserializationSchema<RowData> deserializationSchema) {\n-\t\treturn new FlinkKafkaConsumer011<>(topic, deserializationSchema, properties);\n+\t\treturn new FlinkKafkaConsumer011<>(topics, deserializationSchema, properties);\n+\t}\n+\n+\t@Override\n+\tprotected FlinkKafkaConsumerBase<RowData> createKafkaConsumer(\n+\t\tPattern topicPattern,\n+\t\tProperties properties,\n+\t\tDeserializationSchema<RowData> deserializationSchema) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1a4e5ee2a2f11ba650a98c1a211cea75736fe79d"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDcyNTk5Mg==", "bodyText": "Indent.", "url": "https://github.com/apache/flink/pull/12908#discussion_r460725992", "createdAt": "2020-07-27T08:25:28Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/connectors/kafka/table/KafkaDynamicSourceBase.java", "diffHunk": "@@ -93,16 +101,21 @@\n \t *                               mode is {@link StartupMode#TIMESTAMP}.\n \t */\n \tprotected KafkaDynamicSourceBase(\n-\t\t\tDataType outputDataType,\n-\t\t\tString topic,\n-\t\t\tProperties properties,\n-\t\t\tDecodingFormat<DeserializationSchema<RowData>> decodingFormat,\n-\t\t\tStartupMode startupMode,\n-\t\t\tMap<KafkaTopicPartition, Long> specificStartupOffsets,\n-\t\t\tlong startupTimestampMillis) {\n+\t\tDataType outputDataType,\n+\t\t@Nullable List<String> topics,\n+\t\t@Nullable Pattern topicPattern,\n+\t\tProperties properties,\n+\t\tDecodingFormat<DeserializationSchema<RowData>> decodingFormat,\n+\t\tStartupMode startupMode,\n+\t\tMap<KafkaTopicPartition, Long> specificStartupOffsets,\n+\t\tlong startupTimestampMillis) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1a4e5ee2a2f11ba650a98c1a211cea75736fe79d"}, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDcyNjA2Nw==", "bodyText": "Indent.", "url": "https://github.com/apache/flink/pull/12908#discussion_r460726067", "createdAt": "2020-07-27T08:25:36Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/connectors/kafka/table/KafkaDynamicSourceBase.java", "diffHunk": "@@ -164,34 +178,44 @@ public int hashCode() {\n \t/**\n \t * Creates a version-specific Kafka consumer.\n \t *\n-\t * @param topic                 Kafka topic to consume.\n+\t * @param topics                Kafka topic to consume.\n \t * @param properties            Properties for the Kafka consumer.\n \t * @param deserializationSchema Deserialization schema to use for Kafka records.\n \t * @return The version-specific Kafka consumer\n \t */\n \tprotected abstract FlinkKafkaConsumerBase<RowData> createKafkaConsumer(\n-\t\t\tString topic,\n+\t\t\tList<String> topics,\n \t\t\tProperties properties,\n \t\t\tDeserializationSchema<RowData> deserializationSchema);\n \n+\t/**\n+\t * Creates a version-specific Kafka consumer.\n+\t *\n+\t * @param topicPattern          afka topic to consume.\n+\t * @param properties            Properties for the Kafka consumer.\n+\t * @param deserializationSchema Deserialization schema to use for Kafka records.\n+\t * @return The version-specific Kafka consumer\n+\t */\n+\tprotected abstract FlinkKafkaConsumerBase<RowData> createKafkaConsumer(\n+\t\tPattern topicPattern,\n+\t\tProperties properties,\n+\t\tDeserializationSchema<RowData> deserializationSchema);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1a4e5ee2a2f11ba650a98c1a211cea75736fe79d"}, "originalPosition": 135}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDcyNjE3Ng==", "bodyText": "Indent.", "url": "https://github.com/apache/flink/pull/12908#discussion_r460726176", "createdAt": "2020-07-27T08:25:48Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/table/KafkaDynamicSource.java", "diffHunk": "@@ -70,17 +77,26 @@ public KafkaDynamicSource(\n \n \t@Override\n \tprotected FlinkKafkaConsumerBase<RowData> createKafkaConsumer(\n-\t\t\tString topic,\n+\t\t\tList<String> topics,\n \t\t\tProperties properties,\n \t\t\tDeserializationSchema<RowData> deserializationSchema) {\n-\t\treturn new FlinkKafkaConsumer<>(topic, deserializationSchema, properties);\n+\t\treturn new FlinkKafkaConsumer<>(topics, deserializationSchema, properties);\n+\t}\n+\n+\t@Override\n+\tprotected FlinkKafkaConsumerBase<RowData> createKafkaConsumer(\n+\t\tPattern topicPattern,\n+\t\tProperties properties,\n+\t\tDeserializationSchema<RowData> deserializationSchema) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1a4e5ee2a2f11ba650a98c1a211cea75736fe79d"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDcyNzkyMQ==", "bodyText": "Split this into two exceptions.", "url": "https://github.com/apache/flink/pull/12908#discussion_r460727921", "createdAt": "2020-07-27T08:28:43Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/connectors/kafka/table/KafkaOptions.java", "diffHunk": "@@ -161,11 +176,22 @@ private KafkaOptions() {}\n \t// --------------------------------------------------------------------------------------------\n \n \tpublic static void validateTableOptions(ReadableConfig tableOptions) {\n+\t\tvalidateTopic(tableOptions);\n \t\tvalidateScanStartupMode(tableOptions);\n \t\tvalidateSinkPartitioner(tableOptions);\n \t\tvalidateSinkSemantic(tableOptions);\n \t}\n \n+\tpublic static void validateTopic(ReadableConfig tableOptions) {\n+\t\tOptional<String> topic = tableOptions.getOptional(TOPIC);\n+\t\tOptional<String> pattern = tableOptions.getOptional(TOPIC_PATTERN);\n+\n+\t\tif ((topic.isPresent() && pattern.isPresent()) || !(topic.isPresent() || pattern.isPresent())) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1a4e5ee2a2f11ba650a98c1a211cea75736fe79d"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDczMDg2MA==", "bodyText": "I would suggest to add static util methods List<String> getTopics(ReadableConfig) and Pattern getTopicPattern(ReadableConfig) in KafkaOptions.", "url": "https://github.com/apache/flink/pull/12908#discussion_r460730860", "createdAt": "2020-07-27T08:33:33Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/connectors/kafka/table/KafkaDynamicTableFactoryBase.java", "diffHunk": "@@ -72,26 +82,41 @@ public DynamicTableSource createDynamicTableSource(Context context) {\n \t\tFactoryUtil.TableFactoryHelper helper = FactoryUtil.createTableFactoryHelper(this, context);\n \n \t\tReadableConfig tableOptions = helper.getOptions();\n-\n-\t\tString topic = tableOptions.get(TOPIC);\n \t\tDecodingFormat<DeserializationSchema<RowData>> decodingFormat = helper.discoverDecodingFormat(\n \t\t\t\tDeserializationFormatFactory.class,\n \t\t\t\tFactoryUtil.FORMAT);\n+\t\tOptional<String> topic = tableOptions.getOptional(TOPIC);\n+\t\tOptional<String> pattern = tableOptions.getOptional(TOPIC_PATTERN);\n \t\t// Validate the option data type.\n \t\thelper.validateExcept(PROPERTIES_PREFIX);\n \t\t// Validate the option values.\n \t\tvalidateTableOptions(tableOptions);\n \n \t\tDataType producedDataType = context.getCatalogTable().getSchema().toPhysicalRowDataType();\n-\t\tfinal StartupOptions startupOptions = getStartupOptions(tableOptions, topic);\n+\n+\t\tfinal StartupOptions startupOptions = getStartupOptions(tableOptions);\n+\t\tfinal Properties properties = getKafkaProperties(context.getCatalogTable().getOptions());\n+\t\t// add topic-partition discovery\n+\t\tproperties.setProperty(FlinkKafkaConsumerBase.KEY_PARTITION_DISCOVERY_INTERVAL_MILLIS,\n+\t\t\tString.valueOf(tableOptions\n+\t\t\t\t.getOptional(SCAN_TOPIC_PARTITION_DISCOVERY)\n+\t\t\t\t.map(val -> val.toMillis())\n+\t\t\t\t.orElse(FlinkKafkaConsumerBase.PARTITION_DISCOVERY_DISABLED)));\n+\n \t\treturn createKafkaTableSource(\n-\t\t\t\tproducedDataType,\n-\t\t\t\ttopic,\n-\t\t\t\tgetKafkaProperties(context.getCatalogTable().getOptions()),\n-\t\t\t\tdecodingFormat,\n-\t\t\t\tstartupOptions.startupMode,\n-\t\t\t\tstartupOptions.specificOffsets,\n-\t\t\t\tstartupOptions.startupTimestampMillis);\n+\t\t\tproducedDataType,\n+\t\t\ttopic.map(value ->\n+\t\t\t\tArrays\n+\t\t\t\t\t.stream(value.split(\",\"))\n+\t\t\t\t\t.map(String::trim)\n+\t\t\t\t\t.collect(Collectors.toList()))\n+\t\t\t\t.orElse(null),\n+\t\t\tpattern.map(value -> Pattern.compile(value)).orElse(null),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1a4e5ee2a2f11ba650a98c1a211cea75736fe79d"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDczMTYzNQ==", "bodyText": "What about to have a validateTableSinkOptions and validateTableSourceOptions ? We can then move this validation to validateSinkTopic().", "url": "https://github.com/apache/flink/pull/12908#discussion_r460731635", "createdAt": "2020-07-27T08:34:55Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/connectors/kafka/table/KafkaDynamicTableFactoryBase.java", "diffHunk": "@@ -109,10 +133,18 @@ public DynamicTableSink createDynamicTableSink(Context context) {\n \t\t// Validate the option values.\n \t\tvalidateTableOptions(tableOptions);\n \n+\t\tif (tableOptions.getOptional(TOPIC_PATTERN).isPresent()){\n+\t\t\tthrow new ValidationException(\"Flink Kafka sink currently doesn't support 'topic-pattern'.\");\n+\t\t}\n+\t\tString[] topics = tableOptions.get(TOPIC).split(\",\");\n+\t\tif (topics.length > 1) {\n+\t\t\tthrow new ValidationException(\"Flink Kafka sink currently doesn't support topic list.\");\n+\t\t}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1a4e5ee2a2f11ba650a98c1a211cea75736fe79d"}, "originalPosition": 113}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDczMzA1Mg==", "bodyText": "Add a util method boolean isSingleTopic(ReadableConfig). It can also be used in sink side.", "url": "https://github.com/apache/flink/pull/12908#discussion_r460733052", "createdAt": "2020-07-27T08:37:23Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/connectors/kafka/table/KafkaOptions.java", "diffHunk": "@@ -193,6 +219,9 @@ private static void validateScanStartupMode(ReadableConfig tableOptions) {\n \t\t\t\t\t\t\t\t\tSCAN_STARTUP_SPECIFIC_OFFSETS.key(),\n \t\t\t\t\t\t\t\t\tSCAN_STARTUP_MODE_VALUE_SPECIFIC_OFFSETS));\n \t\t\t\t\t\t}\n+\t\t\t\t\t\tif (!tableOptions.getOptional(TOPIC).isPresent() || tableOptions.get(TOPIC).split(\",\").length > 1){", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1a4e5ee2a2f11ba650a98c1a211cea75736fe79d"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDczNDY4MQ==", "bodyText": "Better to use new HashMap<>().", "url": "https://github.com/apache/flink/pull/12908#discussion_r460734681", "createdAt": "2020-07-27T08:40:08Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/table/KafkaDynamicTableFactoryTestBase.java", "diffHunk": "@@ -187,6 +197,62 @@ public void testTableSourceCommitOnCheckpointsDisabled() {\n \t\tassertFalse(((FlinkKafkaConsumerBase) function).getEnableCommitOnCheckpoints());\n \t}\n \n+\t@Test\n+\tpublic void testTableSourceWithPattern() {\n+\t\t// prepare parameters for Kafka table source\n+\t\tfinal DataType producedDataType = SOURCE_SCHEMA.toPhysicalRowDataType();\n+\n+\t\tfinal Map<KafkaTopicPartition, Long> specificOffsets = new HashMap<>();\n+\n+\t\tDecodingFormat<DeserializationSchema<RowData>> decodingFormat =\n+\t\t\tnew TestFormatFactory.DecodingFormatMock(\",\", true);\n+\n+\t\t// Construct table source using options and table source factory\n+\t\tObjectIdentifier objectIdentifier = ObjectIdentifier.of(\n+\t\t\t\"default\",\n+\t\t\t\"default\",\n+\t\t\t\"scanTable\");\n+\n+\t\tfinal Map<String, String> modifiedOptions = getModifiedOptions(\n+\t\t\tgetFullSourceOptions(),\n+\t\t\toptions -> {\n+\t\t\t\toptions.remove(\"topic\");\n+\t\t\t\toptions.put(\"topic-pattern\", TOPIC_REGEX);\n+\t\t\t\toptions.put(\"scan.startup.mode\", KafkaOptions.SCAN_STARTUP_MODE_VALUE_EARLIEST);\n+\t\t\t\toptions.remove(\"scan.startup.specific-offsets\");\n+\t\t\t});\n+\t\tCatalogTable catalogTable = createKafkaSourceCatalogTable(modifiedOptions);\n+\n+\t\tfinal DynamicTableSource actualSource = FactoryUtil.createTableSource(null,\n+\t\t\tobjectIdentifier,\n+\t\t\tcatalogTable,\n+\t\t\tnew Configuration(),\n+\t\t\tThread.currentThread().getContextClassLoader());\n+\n+\t\t// Test scan source equals\n+\t\tfinal KafkaDynamicSourceBase expectedKafkaSource = getExpectedScanSource(\n+\t\t\tproducedDataType,\n+\t\t\tnull,\n+\t\t\tPattern.compile(TOPIC_REGEX),\n+\t\t\tKAFKA_PROPERTIES,\n+\t\t\tdecodingFormat,\n+\t\t\tStartupMode.EARLIEST,\n+\t\t\tspecificOffsets,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1a4e5ee2a2f11ba650a98c1a211cea75736fe79d"}, "originalPosition": 91}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDczNTUwMg==", "bodyText": "This has been verified in the other test. We don't need to test it again here.", "url": "https://github.com/apache/flink/pull/12908#discussion_r460735502", "createdAt": "2020-07-27T08:41:28Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/table/KafkaDynamicTableFactoryTestBase.java", "diffHunk": "@@ -187,6 +197,62 @@ public void testTableSourceCommitOnCheckpointsDisabled() {\n \t\tassertFalse(((FlinkKafkaConsumerBase) function).getEnableCommitOnCheckpoints());\n \t}\n \n+\t@Test\n+\tpublic void testTableSourceWithPattern() {\n+\t\t// prepare parameters for Kafka table source\n+\t\tfinal DataType producedDataType = SOURCE_SCHEMA.toPhysicalRowDataType();\n+\n+\t\tfinal Map<KafkaTopicPartition, Long> specificOffsets = new HashMap<>();\n+\n+\t\tDecodingFormat<DeserializationSchema<RowData>> decodingFormat =\n+\t\t\tnew TestFormatFactory.DecodingFormatMock(\",\", true);\n+\n+\t\t// Construct table source using options and table source factory\n+\t\tObjectIdentifier objectIdentifier = ObjectIdentifier.of(\n+\t\t\t\"default\",\n+\t\t\t\"default\",\n+\t\t\t\"scanTable\");\n+\n+\t\tfinal Map<String, String> modifiedOptions = getModifiedOptions(\n+\t\t\tgetFullSourceOptions(),\n+\t\t\toptions -> {\n+\t\t\t\toptions.remove(\"topic\");\n+\t\t\t\toptions.put(\"topic-pattern\", TOPIC_REGEX);\n+\t\t\t\toptions.put(\"scan.startup.mode\", KafkaOptions.SCAN_STARTUP_MODE_VALUE_EARLIEST);\n+\t\t\t\toptions.remove(\"scan.startup.specific-offsets\");\n+\t\t\t});\n+\t\tCatalogTable catalogTable = createKafkaSourceCatalogTable(modifiedOptions);\n+\n+\t\tfinal DynamicTableSource actualSource = FactoryUtil.createTableSource(null,\n+\t\t\tobjectIdentifier,\n+\t\t\tcatalogTable,\n+\t\t\tnew Configuration(),\n+\t\t\tThread.currentThread().getContextClassLoader());\n+\n+\t\t// Test scan source equals\n+\t\tfinal KafkaDynamicSourceBase expectedKafkaSource = getExpectedScanSource(\n+\t\t\tproducedDataType,\n+\t\t\tnull,\n+\t\t\tPattern.compile(TOPIC_REGEX),\n+\t\t\tKAFKA_PROPERTIES,\n+\t\t\tdecodingFormat,\n+\t\t\tStartupMode.EARLIEST,\n+\t\t\tspecificOffsets,\n+\t\t\t0);\n+\t\tfinal KafkaDynamicSourceBase actualKafkaSource = (KafkaDynamicSourceBase) actualSource;\n+\t\tassertEquals(actualKafkaSource, expectedKafkaSource);\n+\n+\t\t// Test Kafka consumer\n+\t\tScanTableSource.ScanRuntimeProvider provider =\n+\t\t\tactualKafkaSource.getScanRuntimeProvider(ScanRuntimeProviderContext.INSTANCE);\n+\t\tassertThat(provider, instanceOf(SourceFunctionProvider.class));\n+\t\tfinal SourceFunctionProvider sourceFunctionProvider = (SourceFunctionProvider) provider;\n+\t\tfinal SourceFunction<RowData> sourceFunction = sourceFunctionProvider.createSourceFunction();\n+\t\tassertThat(sourceFunction, instanceOf(getExpectedConsumerClass()));\n+\t\t//  Test commitOnCheckpoints flag should be true when set consumer group\n+\t\tassertTrue(((FlinkKafkaConsumerBase) sourceFunction).getEnableCommitOnCheckpoints());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1a4e5ee2a2f11ba650a98c1a211cea75736fe79d"}, "originalPosition": 104}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDczNjQxNQ==", "bodyText": "We shouldn't use thrown in one test multiple times, because only the first one will be triggered.", "url": "https://github.com/apache/flink/pull/12908#discussion_r460736415", "createdAt": "2020-07-27T08:43:01Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/table/KafkaDynamicTableFactoryTestBase.java", "diffHunk": "@@ -352,6 +442,49 @@ public void testInvalidSinkSemantic(){\n \t\t\tnew Configuration(),\n \t\t\tThread.currentThread().getContextClassLoader());\n \t}\n+\n+\t@Test\n+\tpublic void testSinkWithTopicListOrTopicPattern(){\n+\t\tObjectIdentifier objectIdentifier = ObjectIdentifier.of(\n+\t\t\t\"default\",\n+\t\t\t\"default\",\n+\t\t\t\"sinkTable\");\n+\n+\t\tMap<String, String> modifiedOptions = getModifiedOptions(\n+\t\t\tgetFullSourceOptions(),\n+\t\t\toptions -> {\n+\t\t\t\toptions.put(\"topic\", TOPICS);\n+\t\t\t\toptions.put(\"scan.startup.mode\", \"earliest-offset\");\n+\t\t\t\toptions.remove(\"specific-offsets\");\n+\t\t\t});\n+\t\tCatalogTable sinkTable = createKafkaSinkCatalogTable(modifiedOptions);\n+\n+\t\tthrown.expect(ValidationException.class);\n+\t\tthrown.expect(containsCause(new ValidationException(\"Flink Kafka sink currently doesn't support topic list.\")));\n+\t\tFactoryUtil.createTableSink(\n+\t\t\tnull,\n+\t\t\tobjectIdentifier,\n+\t\t\tsinkTable,\n+\t\t\tnew Configuration(),\n+\t\t\tThread.currentThread().getContextClassLoader());\n+\n+\t\tmodifiedOptions = getModifiedOptions(\n+\t\t\tgetFullSourceOptions(),\n+\t\t\toptions -> {\n+\t\t\t\toptions.put(\"topic-pattern\", TOPIC_REGEX);\n+\t\t\t});\n+\t\tsinkTable = createKafkaSinkCatalogTable(modifiedOptions);\n+\n+\t\tthrown.expect(ValidationException.class);\n+\t\tthrown.expect(containsCause(new ValidationException(\"Flink Kafka sink currently doesn't support 'topic-pattern'.\")));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1a4e5ee2a2f11ba650a98c1a211cea75736fe79d"}, "originalPosition": 179}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDczODg1MA==", "bodyText": "We can simplify this required for sink.", "url": "https://github.com/apache/flink/pull/12908#discussion_r460738850", "createdAt": "2020-07-27T08:46:59Z", "author": {"login": "wuchong"}, "path": "docs/dev/table/connectors/kafka.md", "diffHunk": "@@ -100,11 +100,18 @@ Connector Options\n     </tr>\n     <tr>\n       <td><h5>topic</h5></td>\n-      <td>required</td>\n+      <td>required for sink, optional for source(use 'topic-pattern' instead if not set)</td>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1a4e5ee2a2f11ba650a98c1a211cea75736fe79d"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDczODk2MA==", "bodyText": "We can simplify this optional.", "url": "https://github.com/apache/flink/pull/12908#discussion_r460738960", "createdAt": "2020-07-27T08:47:10Z", "author": {"login": "wuchong"}, "path": "docs/dev/table/connectors/kafka.md", "diffHunk": "@@ -100,11 +100,18 @@ Connector Options\n     </tr>\n     <tr>\n       <td><h5>topic</h5></td>\n-      <td>required</td>\n+      <td>required for sink, optional for source(use 'topic-pattern' instead if not set)</td>\n       <td style=\"word-wrap: break-word;\">(none)</td>\n       <td>String</td>\n-      <td>Topic name from which the table is read.</td>\n+      <td>Topic name(s) from which the table is read. It also supports topic list for source by separating topic by comma like <code>'topic-1, topic-2'</code>.</td>\n     </tr>\n+    <tr>\n+      <td><h5>topic-pattern</h5></td>\n+      <td>optional for source(use 'topic' instead if not set)</td>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1a4e5ee2a2f11ba650a98c1a211cea75736fe79d"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDc1MDkwMw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                  <td style=\"word-wrap: break-word;\">(disabled)</td>\n          \n          \n            \n                  <td style=\"word-wrap: break-word;\">(none)</td>\n          \n      \n    \n    \n  \n\nCurrentlly, we don't have disabled for Default value. We can add more explanation for the default value in Description.", "url": "https://github.com/apache/flink/pull/12908#discussion_r460750903", "createdAt": "2020-07-27T09:07:02Z", "author": {"login": "wuchong"}, "path": "docs/dev/table/connectors/kafka.md", "diffHunk": "@@ -152,6 +159,13 @@ Connector Options\n       <td>Long</td>\n       <td>Start from the specified epoch timestamp (milliseconds) used in case of <code>'timestamp'</code> startup mode.</td>\n     </tr>\n+    <tr>\n+      <td><h5>scan.topic-partition-discovery.interval</h5></td>\n+      <td>optional</td>\n+      <td style=\"word-wrap: break-word;\">(disabled)</td>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1a4e5ee2a2f11ba650a98c1a211cea75736fe79d"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDc1MTgyNA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                  <td>Optional interval for consumer to discover dynamically created Kafka partitions periodically.</td>\n          \n          \n            \n                  <td>Interval for consumer to discover dynamically created Kafka topics and partitions periodically.</td>", "url": "https://github.com/apache/flink/pull/12908#discussion_r460751824", "createdAt": "2020-07-27T09:08:45Z", "author": {"login": "wuchong"}, "path": "docs/dev/table/connectors/kafka.md", "diffHunk": "@@ -152,6 +159,13 @@ Connector Options\n       <td>Long</td>\n       <td>Start from the specified epoch timestamp (milliseconds) used in case of <code>'timestamp'</code> startup mode.</td>\n     </tr>\n+    <tr>\n+      <td><h5>scan.topic-partition-discovery.interval</h5></td>\n+      <td>optional</td>\n+      <td style=\"word-wrap: break-word;\">(disabled)</td>\n+      <td>Duration</td>\n+      <td>Optional interval for consumer to discover dynamically created Kafka partitions periodically.</td>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1a4e5ee2a2f11ba650a98c1a211cea75736fe79d"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDc1NTYwNg==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                  <td>Topic name(s) from which the table is read. It also supports topic list for source by separating topic by comma like <code>'topic-1, topic-2'</code>.</td>\n          \n          \n            \n                  <td>Topic name(s) to read data from when the table is used as source. It also supports topic list for source by separating topic by comma like <code>'topic-1, topic-2'</code>. Note, only one of \"topic-pattern\" and \"topic\" can be specified for sources. When the table is used as sink, the topic name is the topic to write data to. Note topic list is not supported for sinks.</td>", "url": "https://github.com/apache/flink/pull/12908#discussion_r460755606", "createdAt": "2020-07-27T09:15:20Z", "author": {"login": "wuchong"}, "path": "docs/dev/table/connectors/kafka.md", "diffHunk": "@@ -100,11 +100,18 @@ Connector Options\n     </tr>\n     <tr>\n       <td><h5>topic</h5></td>\n-      <td>required</td>\n+      <td>required for sink, optional for source(use 'topic-pattern' instead if not set)</td>\n       <td style=\"word-wrap: break-word;\">(none)</td>\n       <td>String</td>\n-      <td>Topic name from which the table is read.</td>\n+      <td>Topic name(s) from which the table is read. It also supports topic list for source by separating topic by comma like <code>'topic-1, topic-2'</code>.</td>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1a4e5ee2a2f11ba650a98c1a211cea75736fe79d"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDc1ODgxNA==", "bodyText": "The regular expression for a pattern of topic names to read from. All topics with names that match the specified regular expression will be subscribed by the consumer when the job starts running. Note, only one of \"topic-pattern\" and \"topic\" can be specified for sources.", "url": "https://github.com/apache/flink/pull/12908#discussion_r460758814", "createdAt": "2020-07-27T09:20:51Z", "author": {"login": "wuchong"}, "path": "docs/dev/table/connectors/kafka.md", "diffHunk": "@@ -100,11 +100,18 @@ Connector Options\n     </tr>\n     <tr>\n       <td><h5>topic</h5></td>\n-      <td>required</td>\n+      <td>required for sink, optional for source(use 'topic-pattern' instead if not set)</td>\n       <td style=\"word-wrap: break-word;\">(none)</td>\n       <td>String</td>\n-      <td>Topic name from which the table is read.</td>\n+      <td>Topic name(s) from which the table is read. It also supports topic list for source by separating topic by comma like <code>'topic-1, topic-2'</code>.</td>\n     </tr>\n+    <tr>\n+      <td><h5>topic-pattern</h5></td>\n+      <td>optional for source(use 'topic' instead if not set)</td>\n+      <td style=\"word-wrap: break-word;\">(none)</td>\n+      <td>String</td>\n+      <td>Topic pattern from which the table is read. It will use input value to build regex expression to discover matched topics.</td>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1a4e5ee2a2f11ba650a98c1a211cea75736fe79d"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDc1OTE4Nw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The config option `topic` and `topic-pattern` specifies the topics or topic pattern to consume for source. The config option `topic` can accept topic list by inputting value like 'topic-1, topic-2'. \n          \n          \n            \n            The config option `topic` and `topic-pattern` specifies the topics or topic pattern to consume for source. The config option `topic` can accept topic list using comma separator like 'topic-1, topic-2'.", "url": "https://github.com/apache/flink/pull/12908#discussion_r460759187", "createdAt": "2020-07-27T09:21:33Z", "author": {"login": "wuchong"}, "path": "docs/dev/table/connectors/kafka.md", "diffHunk": "@@ -177,6 +191,14 @@ Connector Options\n \n Features\n ----------------\n+### Topic and Partition Discovery\n+\n+The config option `topic` and `topic-pattern` specifies the topics or topic pattern to consume for source. The config option `topic` can accept topic list by inputting value like 'topic-1, topic-2'. ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1a4e5ee2a2f11ba650a98c1a211cea75736fe79d"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDc2NDA4Mw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The config option `topic-pattern` will use regex regression to discover the matched topic. The config option `scan.topic-partition-discovery.interval` enables Kafka connector to discover dynamically created Kafka partitions.\n          \n          \n            \n            The config option `topic-pattern`  will use regular expression to discover the matched topic. For example, if the `topic-pattern` is `test-topic-[0-9]`, then all topics with names that match the specified regular expression (starting with `test-topic-` and ending with a single digit)) will be subscribed by the consumer when the job starts running.\n          \n          \n            \n            \n          \n          \n            \n            To allow the consumer to discover dynamically created topics after the job started running, set a non-negative value for `scan.topic-partition-discovery.interval`. This allows the consumer to discover partitions of new topics with names that also match the specified pattern.", "url": "https://github.com/apache/flink/pull/12908#discussion_r460764083", "createdAt": "2020-07-27T09:29:51Z", "author": {"login": "wuchong"}, "path": "docs/dev/table/connectors/kafka.md", "diffHunk": "@@ -177,6 +191,14 @@ Connector Options\n \n Features\n ----------------\n+### Topic and Partition Discovery\n+\n+The config option `topic` and `topic-pattern` specifies the topics or topic pattern to consume for source. The config option `topic` can accept topic list by inputting value like 'topic-1, topic-2'. \n+The config option `topic-pattern` will use regex regression to discover the matched topic. The config option `scan.topic-partition-discovery.interval` enables Kafka connector to discover dynamically created Kafka partitions.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1a4e5ee2a2f11ba650a98c1a211cea75736fe79d"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDc2NDMwNw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Please refer to [Kafka documentation]({% link dev/connectors/kafka.md %}#kafka-consumers-topic-and-partition-discovery) for more caveats about delivery guarantees.\n          \n          \n            \n            Please refer to [Kafka DataStream Connector documentation]({% link dev/connectors/kafka.md %}#kafka-consumers-topic-and-partition-discovery) for more about topic and partition discovery.", "url": "https://github.com/apache/flink/pull/12908#discussion_r460764307", "createdAt": "2020-07-27T09:30:14Z", "author": {"login": "wuchong"}, "path": "docs/dev/table/connectors/kafka.md", "diffHunk": "@@ -177,6 +191,14 @@ Connector Options\n \n Features\n ----------------\n+### Topic and Partition Discovery\n+\n+The config option `topic` and `topic-pattern` specifies the topics or topic pattern to consume for source. The config option `topic` can accept topic list by inputting value like 'topic-1, topic-2'. \n+The config option `topic-pattern` will use regex regression to discover the matched topic. The config option `scan.topic-partition-discovery.interval` enables Kafka connector to discover dynamically created Kafka partitions.\n+\n+Please refer to [Kafka documentation]({% link dev/connectors/kafka.md %}#kafka-consumers-topic-and-partition-discovery) for more caveats about delivery guarantees.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1a4e5ee2a2f11ba650a98c1a211cea75736fe79d"}, "originalPosition": 44}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYwNTA3MTM4", "url": "https://github.com/apache/flink/pull/12908#pullrequestreview-460507138", "createdAt": "2020-08-04T05:34:00Z", "commit": {"oid": "8d2e2d4bdf6cc90a3fa7b6be4d5ad4bc469feec6"}, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQwNTozNDowMFrOG7Rt2g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQwNjo0MToxOVrOG7TG_A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgwOTQzNA==", "bodyText": "duplicate", "url": "https://github.com/apache/flink/pull/12908#discussion_r464809434", "createdAt": "2020-08-04T05:34:00Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/connectors/kafka/table/KafkaDynamicTableFactoryBase.java", "diffHunk": "@@ -168,10 +185,14 @@ protected abstract KafkaDynamicSinkBase createKafkaTableSink(\n \t@Override\n \tpublic Set<ConfigOption<?>> optionalOptions() {\n \t\tfinal Set<ConfigOption<?>> options = new HashSet<>();\n+\t\toptions.add(TOPIC);\n+\t\toptions.add(TOPIC_PATTERN);\n \t\toptions.add(PROPS_GROUP_ID);\n \t\toptions.add(SCAN_STARTUP_MODE);\n \t\toptions.add(SCAN_STARTUP_SPECIFIC_OFFSETS);\n+\t\toptions.add(SCAN_TOPIC_PARTITION_DISCOVERY);\n \t\toptions.add(SCAN_STARTUP_TIMESTAMP_MILLIS);\n+\t\toptions.add(SCAN_TOPIC_PARTITION_DISCOVERY);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8d2e2d4bdf6cc90a3fa7b6be4d5ad4bc469feec6"}, "originalPosition": 153}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgwOTk4MA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \t\t\t\t.map(val -> val.toMillis())\n          \n          \n            \n            \t\t\t\t.map(Duration::toMillis)", "url": "https://github.com/apache/flink/pull/12908#discussion_r464809980", "createdAt": "2020-08-04T05:35:53Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/connectors/kafka/table/KafkaDynamicTableFactoryBase.java", "diffHunk": "@@ -72,26 +80,34 @@ public DynamicTableSource createDynamicTableSource(Context context) {\n \t\tFactoryUtil.TableFactoryHelper helper = FactoryUtil.createTableFactoryHelper(this, context);\n \n \t\tReadableConfig tableOptions = helper.getOptions();\n-\n-\t\tString topic = tableOptions.get(TOPIC);\n \t\tDecodingFormat<DeserializationSchema<RowData>> decodingFormat = helper.discoverDecodingFormat(\n \t\t\t\tDeserializationFormatFactory.class,\n \t\t\t\tFactoryUtil.FORMAT);\n \t\t// Validate the option data type.\n \t\thelper.validateExcept(PROPERTIES_PREFIX);\n \t\t// Validate the option values.\n-\t\tvalidateTableOptions(tableOptions);\n+\t\tvalidateTableSourceOptions(tableOptions);\n \n \t\tDataType producedDataType = context.getCatalogTable().getSchema().toPhysicalRowDataType();\n-\t\tfinal StartupOptions startupOptions = getStartupOptions(tableOptions, topic);\n+\n+\t\tfinal StartupOptions startupOptions = getStartupOptions(tableOptions);\n+\t\tfinal Properties properties = getKafkaProperties(context.getCatalogTable().getOptions());\n+\t\t// add topic-partition discovery\n+\t\tproperties.setProperty(FlinkKafkaConsumerBase.KEY_PARTITION_DISCOVERY_INTERVAL_MILLIS,\n+\t\t\tString.valueOf(tableOptions\n+\t\t\t\t.getOptional(SCAN_TOPIC_PARTITION_DISCOVERY)\n+\t\t\t\t.map(val -> val.toMillis())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8d2e2d4bdf6cc90a3fa7b6be4d5ad4bc469feec6"}, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgxMDQ0Ng==", "bodyText": "\"topic-list\" -> \"topic\"? We don't have \"topic-list\" option.", "url": "https://github.com/apache/flink/pull/12908#discussion_r464810446", "createdAt": "2020-08-04T05:37:27Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/connectors/kafka/table/KafkaOptions.java", "diffHunk": "@@ -160,12 +178,45 @@ private KafkaOptions() {}\n \t// Validation\n \t// --------------------------------------------------------------------------------------------\n \n-\tpublic static void validateTableOptions(ReadableConfig tableOptions) {\n+\tpublic static void validateTableSourceOptions(ReadableConfig tableOptions) {\n+\t\tvalidateSourceTopic(tableOptions);\n \t\tvalidateScanStartupMode(tableOptions);\n+\t}\n+\n+\tpublic static void validateTableSinkOptions(ReadableConfig tableOptions) {\n+\t\tvalidateSinkTopic(tableOptions);\n \t\tvalidateSinkPartitioner(tableOptions);\n \t\tvalidateSinkSemantic(tableOptions);\n \t}\n \n+\tpublic static void validateSourceTopic(ReadableConfig tableOptions) {\n+\t\tOptional<String> topic = tableOptions.getOptional(TOPIC);\n+\t\tOptional<String> pattern = tableOptions.getOptional(TOPIC_PATTERN);\n+\n+\t\tif (topic.isPresent() && pattern.isPresent()) {\n+\t\t\tthrow new ValidationException(\"Option 'topic' and 'topic-pattern' shouldn't be set together.\");\n+\t\t}\n+\n+\t\tif (!topic.isPresent() && !pattern.isPresent()) {\n+\t\t\tthrow new ValidationException(\"Either 'topic' or 'topic-pattern' must be set.\");\n+\t\t}\n+\t}\n+\n+\tpublic static void validateSinkTopic(ReadableConfig tableOptions) {\n+\t\tString errorMessageTemp = \"Flink Kafka sink currently only supports single topic, but got %s: %s.\";\n+\t\tif (!isSingleTopic(tableOptions)) {\n+\t\t\tif (tableOptions.getOptional(TOPIC_PATTERN).isPresent()) {\n+\t\t\t\tthrow new ValidationException(String.format(\n+\t\t\t\t\terrorMessageTemp, \"'topic-pattern'\", tableOptions.get(TOPIC_PATTERN)\n+\t\t\t\t));\n+\t\t\t} else {\n+\t\t\t\tthrow new ValidationException(String.format(\n+\t\t\t\t\terrorMessageTemp, \"topic-list\", tableOptions.get(TOPIC)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8d2e2d4bdf6cc90a3fa7b6be4d5ad4bc469feec6"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgyODI4NQ==", "bodyText": "Is it still needed? Because we have set it in static block.", "url": "https://github.com/apache/flink/pull/12908#discussion_r464828285", "createdAt": "2020-08-04T06:31:07Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/table/KafkaDynamicTableFactoryTestBase.java", "diffHunk": "@@ -139,10 +152,12 @@ public void testTableSource() {\n \t\t\t\tThread.currentThread().getContextClassLoader());\n \n \t\t// Test scan source equals\n+\t\tKAFKA_SOURCE_PROPERTIES.setProperty(\"flink.partition-discovery.interval-millis\", \"1000\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8d2e2d4bdf6cc90a3fa7b6be4d5ad4bc469feec6"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgzMjI1Mg==", "bodyText": "The community recommend to use List ConfigOption for list values, framework will handle the parsing. This will also change to use ; as the separator, but this is more align with other list options. You can declare a List ConfigOption by :\n\tpublic static final ConfigOption<List<String>> TOPIC = ConfigOptions\n\t\t\t.key(\"topic\")\n\t\t\t.stringType()\n\t\t\t.asList()\n\t\t\t.noDefaultValue()\n\t\t\t.withDescription(\"...\");\nThen you can call return tableOptions.getOptional(TOPIC).map(t -> t.size() == 1).orElse(false); here.\nSorry for the late reminder.", "url": "https://github.com/apache/flink/pull/12908#discussion_r464832252", "createdAt": "2020-08-04T06:41:19Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/connectors/kafka/table/KafkaOptions.java", "diffHunk": "@@ -239,9 +293,25 @@ public static KafkaSinkSemantic getSinkSemantic(ReadableConfig tableOptions){\n \t\t}\n \t}\n \n-\tpublic static StartupOptions getStartupOptions(\n-\t\t\tReadableConfig tableOptions,\n-\t\t\tString topic) {\n+\tpublic static List<String> getSourceTopics(ReadableConfig tableOptions) {\n+\t\treturn tableOptions.getOptional(TOPIC).map(value ->\n+\t\t\tArrays\n+\t\t\t\t.stream(value.split(\",\"))\n+\t\t\t\t.map(String::trim)\n+\t\t\t\t.collect(Collectors.toList()))\n+\t\t\t.orElse(null);\n+\t}\n+\n+\tpublic static Pattern getSourceTopicPattern(ReadableConfig tableOptions) {\n+\t\treturn tableOptions.getOptional(TOPIC_PATTERN).map(value -> Pattern.compile(value)).orElse(null);\n+\t}\n+\n+\tprivate static boolean isSingleTopic(ReadableConfig tableOptions) {\n+\t\t// Option 'topic-pattern' is regarded as multi-topics.\n+\t\treturn tableOptions.getOptional(TOPIC).isPresent() && tableOptions.get(TOPIC).split(\",\").length == 1;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8d2e2d4bdf6cc90a3fa7b6be4d5ad4bc469feec6"}, "originalPosition": 127}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2769a8edbd4a4e28ca6b75f39e69d95007b5c4b2", "author": {"user": {"login": "fsk119", "name": "Shengkai "}}, "url": "https://github.com/apache/flink/commit/2769a8edbd4a4e28ca6b75f39e69d95007b5c4b2", "committedDate": "2020-08-19T04:36:39Z", "message": "[FLINK-18449][kafka][table] Support topic list and topic pattern and partition discovery for Kafka source in Table API\n\nThis closes #12908"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "4b06551a39ce8d9d6c5f19948e36d493e3d592e3", "author": {"user": {"login": "fsk119", "name": "Shengkai "}}, "url": "https://github.com/apache/flink/commit/4b06551a39ce8d9d6c5f19948e36d493e3d592e3", "committedDate": "2020-08-05T11:42:02Z", "message": "replace ConfigOption type of TOPIC with List<String> and add integration test."}, "afterCommit": {"oid": "2769a8edbd4a4e28ca6b75f39e69d95007b5c4b2", "author": {"user": {"login": "fsk119", "name": "Shengkai "}}, "url": "https://github.com/apache/flink/commit/2769a8edbd4a4e28ca6b75f39e69d95007b5c4b2", "committedDate": "2020-08-19T04:36:39Z", "message": "[FLINK-18449][kafka][table] Support topic list and topic pattern and partition discovery for Kafka source in Table API\n\nThis closes #12908"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDcwMTE3Njc2", "url": "https://github.com/apache/flink/pull/12908#pullrequestreview-470117676", "createdAt": "2020-08-19T04:33:43Z", "commit": {"oid": "4b06551a39ce8d9d6c5f19948e36d493e3d592e3"}, "state": "APPROVED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQwNDozMzo0M1rOHCyH2g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQwNDozMzo0M1rOHCyH2g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjY4MDQxMA==", "bodyText": "Currently, it is very verbose to pass through these to parameters together here and there. An improvement is that we can use KafkaTopicsDescriptor, but this can be another issue in the future.", "url": "https://github.com/apache/flink/pull/12908#discussion_r472680410", "createdAt": "2020-08-19T04:33:43Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/table/Kafka010DynamicSource.java", "diffHunk": "@@ -54,7 +59,8 @@\n \t */\n \tpublic Kafka010DynamicSource(\n \t\t\tDataType outputDataType,\n-\t\t\tString topic,\n+\t\t\t@Nullable List<String> topics,\n+\t\t\t@Nullable Pattern topicPattern,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4b06551a39ce8d9d6c5f19948e36d493e3d592e3"}, "originalPosition": 29}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3198, "cost": 1, "resetAt": "2021-10-28T16:48:13Z"}}}