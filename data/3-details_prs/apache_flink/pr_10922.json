{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzY1Njk4OTUw", "number": 10922, "title": "[FLINK-11899][parquet] Introduce parquet ColumnarRow split reader", "bodyText": "What is the purpose of the change\nParquetColumnarRowSplitReader is introduced to read parquet data in batches.\nWhen returning each row of data, instead of actually retrieving each field, we use BaseRow's abstraction to return a Columnar Row-like view.\nThis will greatly improve the downstream filtered scenarios, so that there is no need to access redundant fields on the filtered data.\nBrief change log\n\nIntroduce ColumnReaders\nIntroduce ParquetColumnarRowSplitReader\n\nVerifying this change\nParquetColumnarRowSplitReaderTest\nDoes this pull request potentially affect one of the following parts:\n\nDependencies (does it add or upgrade a dependency): no\nThe public API, i.e., is any changed class annotated with @Public(Evolving): no\nThe serializers: no\nThe runtime per-record code paths (performance sensitive): no\nAnything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no\nThe S3 file system connector: no\n\nDocumentation\n\nDoes this pull request introduce a new feature? no", "createdAt": "2020-01-22T07:08:45Z", "url": "https://github.com/apache/flink/pull/10922", "merged": true, "mergeCommit": {"oid": "1aabab86f9461ff557ef9ffe506bd67556988fc7"}, "closed": true, "closedAt": "2020-03-06T01:29:26Z", "author": {"login": "JingsongLi"}, "timelineItems": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABb9ck6PABqjI5NzY3OTY2NDY=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcKrK3vgH2gAyMzY1Njk4OTUwOjkwOGJmZDUzOWFkODRiZTQyOGY4ZGQzYzkwODY4MGQ4N2RmYjRlYzg=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "355cae10942f3c4efcb25e83ee12c8ccbcda0033", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/flink/commit/355cae10942f3c4efcb25e83ee12c8ccbcda0033", "committedDate": "2020-01-22T15:58:45Z", "message": "Update"}, "afterCommit": {"oid": "ce1b4b24d97cc6963785f1a359a67729f9c091e0", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/flink/commit/ce1b4b24d97cc6963785f1a359a67729f9c091e0", "committedDate": "2020-01-24T10:41:23Z", "message": "[FLINK-11899][parquet][table-runtime] Introduce ParquetColumnarRowSplitReader"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "ce1b4b24d97cc6963785f1a359a67729f9c091e0", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/flink/commit/ce1b4b24d97cc6963785f1a359a67729f9c091e0", "committedDate": "2020-01-24T10:41:23Z", "message": "[FLINK-11899][parquet][table-runtime] Introduce ParquetColumnarRowSplitReader"}, "afterCommit": {"oid": "f975ebd36b4b7358b265af749a71ec606413a80d", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/flink/commit/f975ebd36b4b7358b265af749a71ec606413a80d", "committedDate": "2020-01-24T12:04:50Z", "message": "[FLINK-11899][parquet][table-runtime] Introduce ParquetColumnarRowSplitReader"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "9a8d27a798323014ff9ab16b660b2e44f3099e8e", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/flink/commit/9a8d27a798323014ff9ab16b660b2e44f3099e8e", "committedDate": "2020-01-24T12:45:15Z", "message": "Fix orc vector compile"}, "afterCommit": {"oid": "6d44e21dd1e60f5093263408a9030231eab26fa5", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/flink/commit/6d44e21dd1e60f5093263408a9030231eab26fa5", "committedDate": "2020-02-14T09:45:36Z", "message": "Integrate with hive"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "6d44e21dd1e60f5093263408a9030231eab26fa5", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/flink/commit/6d44e21dd1e60f5093263408a9030231eab26fa5", "committedDate": "2020-02-14T09:45:36Z", "message": "Integrate with hive"}, "afterCommit": {"oid": "b302c52c6aa9f9408c3083466f2d62a6a6f33dd6", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/flink/commit/b302c52c6aa9f9408c3083466f2d62a6a6f33dd6", "committedDate": "2020-02-20T08:39:04Z", "message": "Update"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "d448deb8683b7b5200e8deba018fe75cd8fd92c1", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/flink/commit/d448deb8683b7b5200e8deba018fe75cd8fd92c1", "committedDate": "2020-02-21T13:08:59Z", "message": "simplify ColumnBatchGenerator"}, "afterCommit": {"oid": "6123376d9cdab1092c992dd297ee0972cde7fbf4", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/flink/commit/6123376d9cdab1092c992dd297ee0972cde7fbf4", "committedDate": "2020-02-23T07:44:19Z", "message": "[FLINK-11899][parquet][table-runtime] Introduce ParquetColumnarRowSplitReader"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzY2MjAwNTEz", "url": "https://github.com/apache/flink/pull/10922#pullrequestreview-366200513", "createdAt": "2020-02-28T07:24:31Z", "commit": {"oid": "6123376d9cdab1092c992dd297ee0972cde7fbf4"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yOFQwNzoyNDozMVrOFvrpBw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yOFQwODoyODoyNFrOFvs8LA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTU0MjQwNw==", "bodyText": "Why are these considered complex types?", "url": "https://github.com/apache/flink/pull/10922#discussion_r385542407", "createdAt": "2020-02-28T07:24:31Z", "author": {"login": "lirui-apache"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveTableInputFormat.java", "diffHunk": "@@ -113,16 +114,74 @@ public void configure(org.apache.flink.configuration.Configuration parameters) {\n \n \t@Override\n \tpublic void open(HiveTableInputSplit split) throws IOException {\n-\t\tif (!useMapRedReader && useOrcVectorizedRead(split.getHiveTablePartition())) {\n+\t\tHiveTablePartition partition = split.getHiveTablePartition();\n+\t\tif (!useMapRedReader && useOrcVectorizedRead(partition)) {\n \t\t\tthis.reader = new HiveVectorizedOrcSplitReader(\n \t\t\t\t\thiveVersion, jobConf, fieldNames, fieldTypes, selectedFields, split);\n+\t\t} else if (!useMapRedReader && useParquetVectorizedRead(partition)) {\n+\t\t\tthis.reader = new HiveVectorizedParquetSplitReader(\n+\t\t\t\t\thiveVersion, jobConf, fieldNames, fieldTypes, selectedFields, split);\n \t\t} else {\n \t\t\tthis.reader = new HiveMapredSplitReader(jobConf, partitionKeys, fieldTypes, selectedFields, split,\n \t\t\t\t\tHiveShimLoader.loadHiveShim(hiveVersion));\n \t\t}\n \t\tcurrentReadCount = 0L;\n \t}\n \n+\tprivate boolean isComplexType(LogicalType t) {\n+\t\tswitch (t.getTypeRoot()) {\n+\t\t\tcase CHAR:\n+\t\t\tcase VARCHAR:\n+\t\t\tcase BOOLEAN:\n+\t\t\tcase BINARY:\n+\t\t\tcase VARBINARY:\n+\t\t\tcase DECIMAL:\n+\t\t\tcase TINYINT:\n+\t\t\tcase SMALLINT:\n+\t\t\tcase INTEGER:\n+\t\t\tcase BIGINT:\n+\t\t\tcase FLOAT:\n+\t\t\tcase DOUBLE:\n+\t\t\tcase DATE:\n+\t\t\tcase TIME_WITHOUT_TIME_ZONE:\n+\t\t\tcase TIMESTAMP_WITHOUT_TIME_ZONE:\n+\t\t\tcase TIMESTAMP_WITH_LOCAL_TIME_ZONE:\n+\t\t\t\treturn false;\n+\t\t\tcase TIMESTAMP_WITH_TIME_ZONE:\n+\t\t\tcase INTERVAL_YEAR_MONTH:\n+\t\t\tcase INTERVAL_DAY_TIME:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6123376d9cdab1092c992dd297ee0972cde7fbf4"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTU2MzU4MQ==", "bodyText": "Just use Unsafe::ARRAY_BYTE_BASE_OFFSET?", "url": "https://github.com/apache/flink/pull/10922#discussion_r385563581", "createdAt": "2020-02-28T08:28:08Z", "author": {"login": "lirui-apache"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/dataformat/vector/heap/AbstractHeapVector.java", "diffHunk": "@@ -18,14 +18,25 @@\n \n package org.apache.flink.table.dataformat.vector.heap;\n \n-import org.apache.flink.table.dataformat.vector.AbstractColumnVector;\n+import org.apache.flink.core.memory.MemoryUtils;\n+import org.apache.flink.table.dataformat.vector.writable.AbstractWritableVector;\n \n+import java.nio.ByteOrder;\n import java.util.Arrays;\n \n /**\n  * Heap vector that nullable shared structure.\n  */\n-public abstract class AbstractHeapVector extends AbstractColumnVector {\n+public abstract class AbstractHeapVector extends AbstractWritableVector {\n+\n+\tpublic static final boolean LITTLE_ENDIAN = ByteOrder.nativeOrder() == ByteOrder.LITTLE_ENDIAN;\n+\n+\tpublic static final sun.misc.Unsafe UNSAFE = MemoryUtils.UNSAFE;\n+\tpublic static final int BYTE_ARRAY_OFFSET = UNSAFE.arrayBaseOffset(byte[].class);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6123376d9cdab1092c992dd297ee0972cde7fbf4"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTU2MzY5Mg==", "bodyText": "call setNullAt ?", "url": "https://github.com/apache/flink/pull/10922#discussion_r385563692", "createdAt": "2020-02-28T08:28:24Z", "author": {"login": "lirui-apache"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/dataformat/vector/heap/AbstractHeapVector.java", "diffHunk": "@@ -56,11 +67,31 @@ public void reset() {\n \t\tnoNulls = true;\n \t}\n \n+\t@Override\n \tpublic void setNullAt(int i) {\n+\t\tif (i >= isNull.length) {\n+\t\t\tthrow new RuntimeException();\n+\t\t}\n \t\tisNull[i] = true;\n \t\tnoNulls = false;\n \t}\n \n+\t@Override\n+\tpublic void setNulls(int i, int count) {\n+\t\tfor (int j = 0; j < count; j++) {\n+\t\t\tisNull[i + j] = true;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6123376d9cdab1092c992dd297ee0972cde7fbf4"}, "originalPosition": 44}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1177cdde9727901d1be7938a1c2d609c02181b3b", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/flink/commit/1177cdde9727901d1be7938a1c2d609c02181b3b", "committedDate": "2020-03-03T12:10:09Z", "message": "[FLINK-11899][parquet][table-runtime] Introduce ParquetColumnarRowSplitReader"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "6123376d9cdab1092c992dd297ee0972cde7fbf4", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/flink/commit/6123376d9cdab1092c992dd297ee0972cde7fbf4", "committedDate": "2020-02-23T07:44:19Z", "message": "[FLINK-11899][parquet][table-runtime] Introduce ParquetColumnarRowSplitReader"}, "afterCommit": {"oid": "1177cdde9727901d1be7938a1c2d609c02181b3b", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/flink/commit/1177cdde9727901d1be7938a1c2d609c02181b3b", "committedDate": "2020-03-03T12:10:09Z", "message": "[FLINK-11899][parquet][table-runtime] Introduce ParquetColumnarRowSplitReader"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "37cb28b6b3238516df476318cb28d30114689442", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/flink/commit/37cb28b6b3238516df476318cb28d30114689442", "committedDate": "2020-03-04T05:34:05Z", "message": "Simplify ParquetColumnarRowSplitReader"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzY5NTA2NTk2", "url": "https://github.com/apache/flink/pull/10922#pullrequestreview-369506596", "createdAt": "2020-03-05T11:52:22Z", "commit": {"oid": "37cb28b6b3238516df476318cb28d30114689442"}, "state": "APPROVED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNVQxMTo1MjoyM1rOFyQqbg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNVQxMTo1OToyMFrOFyQ2pw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODI0NjEyNg==", "bodyText": "maybe rename to readNextRowGroup?", "url": "https://github.com/apache/flink/pull/10922#discussion_r388246126", "createdAt": "2020-03-05T11:52:23Z", "author": {"login": "lirui-apache"}, "path": "flink-formats/flink-parquet/src/main/java/org/apache/flink/formats/parquet/vector/ParquetColumnarRowSplitReader.java", "diffHunk": "@@ -0,0 +1,308 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.parquet.vector;\n+\n+import org.apache.flink.formats.parquet.vector.reader.AbstractColumnReader;\n+import org.apache.flink.formats.parquet.vector.reader.ColumnReader;\n+import org.apache.flink.table.dataformat.ColumnarRow;\n+import org.apache.flink.table.dataformat.vector.ColumnVector;\n+import org.apache.flink.table.dataformat.vector.VectorizedColumnBatch;\n+import org.apache.flink.table.dataformat.vector.writable.WritableColumnVector;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.LogicalTypeRoot;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.filter2.compat.FilterCompat;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.parquet.hadoop.metadata.BlockMetaData;\n+import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.Type;\n+import org.apache.parquet.schema.Types;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+import static org.apache.flink.formats.parquet.vector.ParquetSplitReaderUtil.createColumnReader;\n+import static org.apache.flink.formats.parquet.vector.ParquetSplitReaderUtil.createWritableColumnVector;\n+import static org.apache.parquet.filter2.compat.RowGroupFilter.filterRowGroups;\n+import static org.apache.parquet.format.converter.ParquetMetadataConverter.range;\n+import static org.apache.parquet.hadoop.ParquetFileReader.readFooter;\n+import static org.apache.parquet.hadoop.ParquetInputFormat.getFilter;\n+\n+/**\n+ * This reader is used to read a {@link VectorizedColumnBatch} from input split.\n+ */\n+public class ParquetColumnarRowSplitReader implements Closeable {\n+\n+\tprivate final boolean utcTimestamp;\n+\n+\tprivate final MessageType fileSchema;\n+\n+\tprivate final MessageType requestedSchema;\n+\n+\t/**\n+\t * The total number of rows this RecordReader will eventually read. The sum of the rows of all\n+\t * the row groups.\n+\t */\n+\tprivate final long totalRowCount;\n+\n+\tprivate final WritableColumnVector[] writableVectors;\n+\n+\tprivate final VectorizedColumnBatch columnarBatch;\n+\n+\tprivate final ColumnarRow row;\n+\n+\tprivate final LogicalType[] selectedTypes;\n+\n+\tprivate final int batchSize;\n+\n+\tprivate ParquetFileReader reader;\n+\n+\t/**\n+\t * For each request column, the reader to read this column. This is NULL if this column is\n+\t * missing from the file, in which case we populate the attribute with NULL.\n+\t */\n+\tprivate ColumnReader[] columnReaders;\n+\n+\t/**\n+\t * The number of rows that have been returned.\n+\t */\n+\tprivate long rowsReturned;\n+\n+\t/**\n+\t * The number of rows that have been reading, including the current in flight row group.\n+\t */\n+\tprivate long totalCountLoadedSoFar;\n+\n+\t// the index of the next row to return\n+\tprivate int nextRow;\n+\n+\t// the number of rows in the current batch\n+\tprivate int rowsInBatch;\n+\n+\tpublic ParquetColumnarRowSplitReader(\n+\t\t\tboolean utcTimestamp,\n+\t\t\tConfiguration conf,\n+\t\t\tLogicalType[] selectedTypes,\n+\t\t\tString[] selectedFieldNames,\n+\t\t\tColumnBatchGenerator generator,\n+\t\t\tint batchSize,\n+\t\t\tPath path,\n+\t\t\tlong splitStart,\n+\t\t\tlong splitLength) throws IOException {\n+\t\tthis.utcTimestamp = utcTimestamp;\n+\t\tthis.selectedTypes = selectedTypes;\n+\t\tthis.batchSize = batchSize;\n+\t\t// then we need to apply the predicate push down filter\n+\t\tParquetMetadata footer = readFooter(conf, path, range(splitStart, splitLength));\n+\t\tMessageType fileSchema = footer.getFileMetaData().getSchema();\n+\t\tFilterCompat.Filter filter = getFilter(conf);\n+\t\tList<BlockMetaData> blocks = filterRowGroups(filter, footer.getBlocks(), fileSchema);\n+\n+\t\tthis.fileSchema = footer.getFileMetaData().getSchema();\n+\t\tthis.requestedSchema = clipParquetSchema(fileSchema, selectedFieldNames);\n+\t\tthis.reader = new ParquetFileReader(\n+\t\t\t\tconf, footer.getFileMetaData(), path, blocks, requestedSchema.getColumns());\n+\n+\t\tlong totalRowCount = 0;\n+\t\tfor (BlockMetaData block : blocks) {\n+\t\t\ttotalRowCount += block.getRowCount();\n+\t\t}\n+\t\tthis.totalRowCount = totalRowCount;\n+\t\tthis.nextRow = 0;\n+\t\tthis.rowsInBatch = 0;\n+\t\tthis.rowsReturned = 0;\n+\n+\t\tcheckSchema();\n+\n+\t\tthis.writableVectors = createWritableVectors();\n+\t\tthis.columnarBatch = generator.generate(createReadableVectors());\n+\t\tthis.row = new ColumnarRow(columnarBatch);\n+\t}\n+\n+\t/**\n+\t * Clips `parquetSchema` according to `fieldNames`.\n+\t */\n+\tprivate static MessageType clipParquetSchema(GroupType parquetSchema, String[] fieldNames) {\n+\t\tType[] types = new Type[fieldNames.length];\n+\t\tfor (int i = 0; i < fieldNames.length; ++i) {\n+\t\t\tString fieldName = fieldNames[i];\n+\t\t\tif (parquetSchema.getFieldIndex(fieldName) < 0) {\n+\t\t\t\tthrow new IllegalArgumentException(fieldName + \" does not exist\");\n+\t\t\t}\n+\t\t\ttypes[i] = parquetSchema.getType(fieldName);\n+\t\t}\n+\t\treturn Types.buildMessage().addFields(types).named(\"flink-parquet\");\n+\t}\n+\n+\tprivate WritableColumnVector[] createWritableVectors() {\n+\t\tWritableColumnVector[] columns = new WritableColumnVector[selectedTypes.length];\n+\t\tfor (int i = 0; i < selectedTypes.length; i++) {\n+\t\t\tcolumns[i] = createWritableColumnVector(\n+\t\t\t\t\tbatchSize,\n+\t\t\t\t\tselectedTypes[i],\n+\t\t\t\t\trequestedSchema.getColumns().get(i).getPrimitiveType());\n+\t\t}\n+\t\treturn columns;\n+\t}\n+\n+\t/**\n+\t * Create readable vectors from writable vectors.\n+\t * Especially for decimal, see {@link ParquetDecimalVector}.\n+\t */\n+\tprivate ColumnVector[] createReadableVectors() {\n+\t\tColumnVector[] vectors = new ColumnVector[writableVectors.length];\n+\t\tfor (int i = 0; i < writableVectors.length; i++) {\n+\t\t\tvectors[i] = selectedTypes[i].getTypeRoot() == LogicalTypeRoot.DECIMAL ?\n+\t\t\t\t\tnew ParquetDecimalVector(writableVectors[i]) :\n+\t\t\t\t\twritableVectors[i];\n+\t\t}\n+\t\treturn vectors;\n+\t}\n+\n+\tprivate void checkSchema() throws IOException, UnsupportedOperationException {\n+\t\tif (selectedTypes.length != requestedSchema.getFieldCount()) {\n+\t\t\tthrow new RuntimeException(\"The quality of field type is incompatible with the request schema!\");\n+\t\t}\n+\n+\t\t/*\n+\t\t * Check that the requested schema is supported.\n+\t\t */\n+\t\tfor (int i = 0; i < requestedSchema.getFieldCount(); ++i) {\n+\t\t\tType t = requestedSchema.getFields().get(i);\n+\t\t\tif (!t.isPrimitive() || t.isRepetition(Type.Repetition.REPEATED)) {\n+\t\t\t\tthrow new UnsupportedOperationException(\"Complex types not supported.\");\n+\t\t\t}\n+\n+\t\t\tString[] colPath = requestedSchema.getPaths().get(i);\n+\t\t\tif (fileSchema.containsPath(colPath)) {\n+\t\t\t\tColumnDescriptor fd = fileSchema.getColumnDescription(colPath);\n+\t\t\t\tif (!fd.equals(requestedSchema.getColumns().get(i))) {\n+\t\t\t\t\tthrow new UnsupportedOperationException(\"Schema evolution not supported.\");\n+\t\t\t\t}\n+\t\t\t} else {\n+\t\t\t\tif (requestedSchema.getColumns().get(i).getMaxDefinitionLevel() == 0) {\n+\t\t\t\t\t// Column is missing in data but the required data is non-nullable. This file is invalid.\n+\t\t\t\t\tthrow new IOException(\"Required column is missing in data file. Col: \" + Arrays.toString(colPath));\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * Method used to check if the end of the input is reached.\n+\t *\n+\t * @return True if the end is reached, otherwise false.\n+\t * @throws IOException Thrown, if an I/O error occurred.\n+\t */\n+\tpublic boolean reachedEnd() throws IOException {\n+\t\treturn !ensureBatch();\n+\t}\n+\n+\tpublic ColumnarRow nextRecord() {\n+\t\t// return the next row\n+\t\trow.setRowId(this.nextRow++);\n+\t\treturn row;\n+\t}\n+\n+\t/**\n+\t * Checks if there is at least one row left in the batch to return. If no more row are\n+\t * available, it reads another batch of rows.\n+\t *\n+\t * @return Returns true if there is one more row to return, false otherwise.\n+\t * @throws IOException throw if an exception happens while reading a batch.\n+\t */\n+\tprivate boolean ensureBatch() throws IOException {\n+\t\tif (nextRow >= rowsInBatch) {\n+\t\t\t// No more rows available in the Rows array.\n+\t\t\tnextRow = 0;\n+\t\t\t// Try to read the next batch if rows from the file.\n+\t\t\treturn nextBatch();\n+\t\t}\n+\t\t// there is at least one Row left in the Rows array.\n+\t\treturn true;\n+\t}\n+\n+\t/**\n+\t * Advances to the next batch of rows. Returns false if there are no more.\n+\t */\n+\tprivate boolean nextBatch() throws IOException {\n+\t\tfor (WritableColumnVector v : writableVectors) {\n+\t\t\tv.reset();\n+\t\t}\n+\t\tcolumnarBatch.setNumRows(0);\n+\t\tif (rowsReturned >= totalRowCount) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tcheckEndOfRowGroup();\n+\n+\t\tint num = (int) Math.min(batchSize, totalCountLoadedSoFar - rowsReturned);\n+\t\tfor (int i = 0; i < columnReaders.length; ++i) {\n+\t\t\t//noinspection unchecked\n+\t\t\tcolumnReaders[i].readToVector(num, writableVectors[i]);\n+\t\t}\n+\t\trowsReturned += num;\n+\t\tcolumnarBatch.setNumRows(num);\n+\t\trowsInBatch = num;\n+\t\treturn true;\n+\t}\n+\n+\tprivate void checkEndOfRowGroup() throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "37cb28b6b3238516df476318cb28d30114689442"}, "originalPosition": 273}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODI0OTI1NQ==", "bodyText": "selParquetFields?", "url": "https://github.com/apache/flink/pull/10922#discussion_r388249255", "createdAt": "2020-03-05T11:59:20Z", "author": {"login": "lirui-apache"}, "path": "flink-formats/flink-parquet/src/main/java/org/apache/flink/formats/parquet/vector/ParquetSplitReaderUtil.java", "diffHunk": "@@ -0,0 +1,384 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.parquet.vector;\n+\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.formats.parquet.vector.reader.BooleanColumnReader;\n+import org.apache.flink.formats.parquet.vector.reader.ByteColumnReader;\n+import org.apache.flink.formats.parquet.vector.reader.BytesColumnReader;\n+import org.apache.flink.formats.parquet.vector.reader.ColumnReader;\n+import org.apache.flink.formats.parquet.vector.reader.DoubleColumnReader;\n+import org.apache.flink.formats.parquet.vector.reader.FixedLenBytesColumnReader;\n+import org.apache.flink.formats.parquet.vector.reader.FloatColumnReader;\n+import org.apache.flink.formats.parquet.vector.reader.IntColumnReader;\n+import org.apache.flink.formats.parquet.vector.reader.LongColumnReader;\n+import org.apache.flink.formats.parquet.vector.reader.ShortColumnReader;\n+import org.apache.flink.formats.parquet.vector.reader.TimestampColumnReader;\n+import org.apache.flink.table.dataformat.Decimal;\n+import org.apache.flink.table.dataformat.SqlTimestamp;\n+import org.apache.flink.table.dataformat.vector.ColumnVector;\n+import org.apache.flink.table.dataformat.vector.VectorizedColumnBatch;\n+import org.apache.flink.table.dataformat.vector.heap.HeapBooleanVector;\n+import org.apache.flink.table.dataformat.vector.heap.HeapByteVector;\n+import org.apache.flink.table.dataformat.vector.heap.HeapBytesVector;\n+import org.apache.flink.table.dataformat.vector.heap.HeapDoubleVector;\n+import org.apache.flink.table.dataformat.vector.heap.HeapFloatVector;\n+import org.apache.flink.table.dataformat.vector.heap.HeapIntVector;\n+import org.apache.flink.table.dataformat.vector.heap.HeapLongVector;\n+import org.apache.flink.table.dataformat.vector.heap.HeapShortVector;\n+import org.apache.flink.table.dataformat.vector.heap.HeapTimestampVector;\n+import org.apache.flink.table.dataformat.vector.writable.WritableColumnVector;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.BigIntType;\n+import org.apache.flink.table.types.logical.DecimalType;\n+import org.apache.flink.table.types.logical.IntType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.VarBinaryType;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.page.PageReader;\n+import org.apache.parquet.schema.OriginalType;\n+import org.apache.parquet.schema.PrimitiveType;\n+\n+import java.io.IOException;\n+import java.math.BigDecimal;\n+import java.nio.charset.StandardCharsets;\n+import java.sql.Date;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.runtime.functions.SqlDateTimeUtils.dateToInternal;\n+import static org.apache.parquet.Preconditions.checkArgument;\n+\n+/**\n+ * Util for generating {@link ParquetColumnarRowSplitReader}.\n+ */\n+public class ParquetSplitReaderUtil {\n+\n+\t/**\n+\t * Util for generating partitioned {@link ParquetColumnarRowSplitReader}.\n+\t */\n+\tpublic static ParquetColumnarRowSplitReader genPartColumnarRowReader(\n+\t\t\tboolean utcTimestamp,\n+\t\t\tConfiguration conf,\n+\t\t\tString[] fullFieldNames,\n+\t\t\tDataType[] fullFieldTypes,\n+\t\t\tMap<String, Object> partitionSpec,\n+\t\t\tint[] selectedFields,\n+\t\t\tint batchSize,\n+\t\t\tPath path,\n+\t\t\tlong splitStart,\n+\t\t\tlong splitLength) throws IOException {\n+\t\tList<String> nonPartNames = Arrays.stream(fullFieldNames)\n+\t\t\t\t.filter(n -> !partitionSpec.containsKey(n))\n+\t\t\t\t.collect(Collectors.toList());\n+\n+\t\tList<String> selNonPartNames = Arrays.stream(selectedFields)\n+\t\t\t\t.mapToObj(i -> fullFieldNames[i])\n+\t\t\t\t.filter(nonPartNames::contains).collect(Collectors.toList());\n+\n+\t\tint[] selOrcFields = selNonPartNames.stream()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "37cb28b6b3238516df476318cb28d30114689442"}, "originalPosition": 102}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "908bfd539ad84be428f8dd3c908680d87dfb4ec8", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/flink/commit/908bfd539ad84be428f8dd3c908680d87dfb4ec8", "committedDate": "2020-03-05T13:03:07Z", "message": "Fix comments"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4428, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}