{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTE1MTE1MzQ2", "number": 13915, "title": "[FLINK-19365][hive] Migrate Hive source to FLIP-27 source interface f\u2026", "bodyText": "\u2026or batch\n\nWhat is the purpose of the change\nMigrate hive connector to FLIP-27 source for batch reading\nBrief change log\n\nImplement HiveSourceSplit as sub-class of FileSourceSplit. It remembers the partition which a split belongs to.\nImplement HiveSourceSplitSerializer as SerDe for HiveSourceSplit.\nImplement HiveSource as sub-class of AbstractFileSource, and works with RowData and HiveSourceSplit.\nImplement HiveSourceFileEnumerator as an implementation of FileEnumerator, but it generates splits based on partitions rather than paths.\nImplement HiveBulkFormatAdapter as an implementation of BulkFormat<RowData, HiveSourceSplit>. It delegates the reading to other BulkFormat instances based on the information we have from each partition to read.\nUpdate PartitionValueConverter to take partition col name when converting the partition value. This makes it easier for hive to reuse ParquetColumnarRowInputFormat because the partition values have been pre-computed.\n\nVerifying this change\nExisting tests\nDoes this pull request potentially affect one of the following parts:\n\nDependencies (does it add or upgrade a dependency): yes\nThe public API, i.e., is any changed class annotated with @Public(Evolving): no\nThe serializers: yes\nThe runtime per-record code paths (performance sensitive): yes\nAnything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: no\nThe S3 file system connector: no\n\nDocumentation\n\nDoes this pull request introduce a new feature? no\nIf yes, how is the feature documented? NA", "createdAt": "2020-11-04T03:47:35Z", "url": "https://github.com/apache/flink/pull/13915", "merged": true, "mergeCommit": {"oid": "56207e8a578394c01022bbe848d03cf8b97c2380"}, "closed": true, "closedAt": "2020-11-05T08:00:26Z", "author": {"login": "lirui-apache"}, "timelineItems": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdZGEPKAFqTUyMzAyNjcyNw==", "endCursor": "Y3Vyc29yOnYyOpPPAAABdZdrIbAFqTUyNDAwMTMzOQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIzMDI2NzI3", "url": "https://github.com/apache/flink/pull/13915#pullrequestreview-523026727", "createdAt": "2020-11-04T03:50:45Z", "commit": {"oid": "197131cd8bb420b5914d07af898afd04089af554"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQwMzo1MDo0NlrOHtIPpw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQwNDoyMTo0NVrOHtIpfg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzA4MzA0Nw==", "bodyText": "@nullable Long limit", "url": "https://github.com/apache/flink/pull/13915#discussion_r517083047", "createdAt": "2020-11-04T03:50:46Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveSource.java", "diffHunk": "@@ -0,0 +1,168 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.ResultTypeQueryable;\n+import org.apache.flink.connector.file.src.AbstractFileSource;\n+import org.apache.flink.connector.file.src.reader.BulkFormat;\n+import org.apache.flink.connectors.hive.read.HiveBulkFormatAdapter;\n+import org.apache.flink.connectors.hive.read.HiveSourceSplit;\n+import org.apache.flink.core.io.SimpleVersionedSerializer;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.runtime.types.TypeInfoDataTypeConverter;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.mapred.FileSplit;\n+import org.apache.hadoop.mapred.InputFormat;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.util.ReflectionUtils;\n+\n+import javax.annotation.Nullable;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.stream.IntStream;\n+\n+import static org.apache.flink.connector.file.src.FileSource.DEFAULT_SPLIT_ASSIGNER;\n+import static org.apache.flink.util.Preconditions.checkNotNull;\n+import static org.apache.hadoop.mapreduce.lib.input.FileInputFormat.INPUT_DIR;\n+\n+/**\n+ * A unified data source that reads a hive table.\n+ */\n+public class HiveSource extends AbstractFileSource<RowData, HiveSourceSplit> implements ResultTypeQueryable<RowData> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\t// DataType of the records to be returned, with projection applied (if any)\n+\tprivate final DataType producedDataType;\n+\n+\tHiveSource(\n+\t\t\tJobConf jobConf,\n+\t\t\tCatalogTable catalogTable,\n+\t\t\tList<HiveTablePartition> partitions,\n+\t\t\tint[] projectedFields,\n+\t\t\tlong limit,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "197131cd8bb420b5914d07af898afd04089af554"}, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzA4MzYxMQ==", "bodyText": "A RowType is enough", "url": "https://github.com/apache/flink/pull/13915#discussion_r517083611", "createdAt": "2020-11-04T03:53:36Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveSource.java", "diffHunk": "@@ -0,0 +1,168 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.ResultTypeQueryable;\n+import org.apache.flink.connector.file.src.AbstractFileSource;\n+import org.apache.flink.connector.file.src.reader.BulkFormat;\n+import org.apache.flink.connectors.hive.read.HiveBulkFormatAdapter;\n+import org.apache.flink.connectors.hive.read.HiveSourceSplit;\n+import org.apache.flink.core.io.SimpleVersionedSerializer;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.runtime.types.TypeInfoDataTypeConverter;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.mapred.FileSplit;\n+import org.apache.hadoop.mapred.InputFormat;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.util.ReflectionUtils;\n+\n+import javax.annotation.Nullable;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.stream.IntStream;\n+\n+import static org.apache.flink.connector.file.src.FileSource.DEFAULT_SPLIT_ASSIGNER;\n+import static org.apache.flink.util.Preconditions.checkNotNull;\n+import static org.apache.hadoop.mapreduce.lib.input.FileInputFormat.INPUT_DIR;\n+\n+/**\n+ * A unified data source that reads a hive table.\n+ */\n+public class HiveSource extends AbstractFileSource<RowData, HiveSourceSplit> implements ResultTypeQueryable<RowData> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\t// DataType of the records to be returned, with projection applied (if any)\n+\tprivate final DataType producedDataType;\n+\n+\tHiveSource(\n+\t\t\tJobConf jobConf,\n+\t\t\tCatalogTable catalogTable,\n+\t\t\tList<HiveTablePartition> partitions,\n+\t\t\tint[] projectedFields,\n+\t\t\tlong limit,\n+\t\t\tString hiveVersion,\n+\t\t\tboolean useMapRedReader,\n+\t\t\tboolean isStreamingSource,\n+\t\t\tDataType producedDataType) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "197131cd8bb420b5914d07af898afd04089af554"}, "originalPosition": 72}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzA4MzY4MQ==", "bodyText": "InternalTypeInfo.of(rowType)\nBTW, why override this?", "url": "https://github.com/apache/flink/pull/13915#discussion_r517083681", "createdAt": "2020-11-04T03:53:59Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveSource.java", "diffHunk": "@@ -0,0 +1,168 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.ResultTypeQueryable;\n+import org.apache.flink.connector.file.src.AbstractFileSource;\n+import org.apache.flink.connector.file.src.reader.BulkFormat;\n+import org.apache.flink.connectors.hive.read.HiveBulkFormatAdapter;\n+import org.apache.flink.connectors.hive.read.HiveSourceSplit;\n+import org.apache.flink.core.io.SimpleVersionedSerializer;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.runtime.types.TypeInfoDataTypeConverter;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.mapred.FileSplit;\n+import org.apache.hadoop.mapred.InputFormat;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.util.ReflectionUtils;\n+\n+import javax.annotation.Nullable;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.stream.IntStream;\n+\n+import static org.apache.flink.connector.file.src.FileSource.DEFAULT_SPLIT_ASSIGNER;\n+import static org.apache.flink.util.Preconditions.checkNotNull;\n+import static org.apache.hadoop.mapreduce.lib.input.FileInputFormat.INPUT_DIR;\n+\n+/**\n+ * A unified data source that reads a hive table.\n+ */\n+public class HiveSource extends AbstractFileSource<RowData, HiveSourceSplit> implements ResultTypeQueryable<RowData> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\t// DataType of the records to be returned, with projection applied (if any)\n+\tprivate final DataType producedDataType;\n+\n+\tHiveSource(\n+\t\t\tJobConf jobConf,\n+\t\t\tCatalogTable catalogTable,\n+\t\t\tList<HiveTablePartition> partitions,\n+\t\t\tint[] projectedFields,\n+\t\t\tlong limit,\n+\t\t\tString hiveVersion,\n+\t\t\tboolean useMapRedReader,\n+\t\t\tboolean isStreamingSource,\n+\t\t\tDataType producedDataType) {\n+\t\tsuper(\n+\t\t\t\tnew org.apache.flink.core.fs.Path[1],\n+\t\t\t\tnew HiveSourceFileEnumerator.Provider(partitions, new JobConfWrapper(jobConf)),\n+\t\t\t\tDEFAULT_SPLIT_ASSIGNER,\n+\t\t\t\tcreateBulkFormat(new JobConf(jobConf), catalogTable, projectedFields, hiveVersion, producedDataType, useMapRedReader, limit),\n+\t\t\t\tnull);\n+\t\tPreconditions.checkArgument(!isStreamingSource, \"HiveSource currently only supports bounded mode\");\n+\t\tthis.producedDataType = producedDataType;\n+\t}\n+\n+\t@Override\n+\tpublic SimpleVersionedSerializer<HiveSourceSplit> getSplitSerializer() {\n+\t\treturn HiveSourceSplitSerializer.INSTANCE;\n+\t}\n+\n+\t@Override\n+\tpublic TypeInformation<RowData> getProducedType() {\n+\t\treturn (TypeInformation<RowData>) TypeInfoDataTypeConverter.fromDataTypeToTypeInfo(producedDataType);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "197131cd8bb420b5914d07af898afd04089af554"}, "originalPosition": 90}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzA4NDA3Mw==", "bodyText": "Why in here? Should in HiveSourceFileEnumerator", "url": "https://github.com/apache/flink/pull/13915#discussion_r517084073", "createdAt": "2020-11-04T03:55:33Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveSource.java", "diffHunk": "@@ -0,0 +1,168 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.ResultTypeQueryable;\n+import org.apache.flink.connector.file.src.AbstractFileSource;\n+import org.apache.flink.connector.file.src.reader.BulkFormat;\n+import org.apache.flink.connectors.hive.read.HiveBulkFormatAdapter;\n+import org.apache.flink.connectors.hive.read.HiveSourceSplit;\n+import org.apache.flink.core.io.SimpleVersionedSerializer;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.runtime.types.TypeInfoDataTypeConverter;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.mapred.FileSplit;\n+import org.apache.hadoop.mapred.InputFormat;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.util.ReflectionUtils;\n+\n+import javax.annotation.Nullable;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.stream.IntStream;\n+\n+import static org.apache.flink.connector.file.src.FileSource.DEFAULT_SPLIT_ASSIGNER;\n+import static org.apache.flink.util.Preconditions.checkNotNull;\n+import static org.apache.hadoop.mapreduce.lib.input.FileInputFormat.INPUT_DIR;\n+\n+/**\n+ * A unified data source that reads a hive table.\n+ */\n+public class HiveSource extends AbstractFileSource<RowData, HiveSourceSplit> implements ResultTypeQueryable<RowData> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\t// DataType of the records to be returned, with projection applied (if any)\n+\tprivate final DataType producedDataType;\n+\n+\tHiveSource(\n+\t\t\tJobConf jobConf,\n+\t\t\tCatalogTable catalogTable,\n+\t\t\tList<HiveTablePartition> partitions,\n+\t\t\tint[] projectedFields,\n+\t\t\tlong limit,\n+\t\t\tString hiveVersion,\n+\t\t\tboolean useMapRedReader,\n+\t\t\tboolean isStreamingSource,\n+\t\t\tDataType producedDataType) {\n+\t\tsuper(\n+\t\t\t\tnew org.apache.flink.core.fs.Path[1],\n+\t\t\t\tnew HiveSourceFileEnumerator.Provider(partitions, new JobConfWrapper(jobConf)),\n+\t\t\t\tDEFAULT_SPLIT_ASSIGNER,\n+\t\t\t\tcreateBulkFormat(new JobConf(jobConf), catalogTable, projectedFields, hiveVersion, producedDataType, useMapRedReader, limit),\n+\t\t\t\tnull);\n+\t\tPreconditions.checkArgument(!isStreamingSource, \"HiveSource currently only supports bounded mode\");\n+\t\tthis.producedDataType = producedDataType;\n+\t}\n+\n+\t@Override\n+\tpublic SimpleVersionedSerializer<HiveSourceSplit> getSplitSerializer() {\n+\t\treturn HiveSourceSplitSerializer.INSTANCE;\n+\t}\n+\n+\t@Override\n+\tpublic TypeInformation<RowData> getProducedType() {\n+\t\treturn (TypeInformation<RowData>) TypeInfoDataTypeConverter.fromDataTypeToTypeInfo(producedDataType);\n+\t}\n+\n+\tprivate static BulkFormat<RowData, HiveSourceSplit> createBulkFormat(\n+\t\t\tJobConf jobConf,\n+\t\t\tCatalogTable catalogTable,\n+\t\t\t@Nullable int[] projectedFields,\n+\t\t\tString hiveVersion,\n+\t\t\tDataType producedDataType,\n+\t\t\tboolean useMapRedReader,\n+\t\t\tlong limit) {\n+\t\tcheckNotNull(catalogTable, \"catalogTable can not be null.\");\n+\t\treturn new HiveBulkFormatAdapter(\n+\t\t\t\tnew JobConfWrapper(jobConf),\n+\t\t\t\tcatalogTable.getPartitionKeys(),\n+\t\t\t\tcatalogTable.getSchema().getFieldNames(),\n+\t\t\t\tcatalogTable.getSchema().getFieldDataTypes(),\n+\t\t\t\tprojectedFields != null ? projectedFields : IntStream.range(0, catalogTable.getSchema().getFieldCount()).toArray(),\n+\t\t\t\thiveVersion,\n+\t\t\t\tproducedDataType,\n+\t\t\t\tuseMapRedReader,\n+\t\t\t\tlimit);\n+\t}\n+\n+\tpublic static List<HiveSourceSplit> createInputSplits(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "197131cd8bb420b5914d07af898afd04089af554"}, "originalPosition": 114}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzA4NDI4Mg==", "bodyText": "Already has producedType, I think projectedFields can be removed", "url": "https://github.com/apache/flink/pull/13915#discussion_r517084282", "createdAt": "2020-11-04T03:56:34Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveSource.java", "diffHunk": "@@ -0,0 +1,168 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.ResultTypeQueryable;\n+import org.apache.flink.connector.file.src.AbstractFileSource;\n+import org.apache.flink.connector.file.src.reader.BulkFormat;\n+import org.apache.flink.connectors.hive.read.HiveBulkFormatAdapter;\n+import org.apache.flink.connectors.hive.read.HiveSourceSplit;\n+import org.apache.flink.core.io.SimpleVersionedSerializer;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.runtime.types.TypeInfoDataTypeConverter;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.mapred.FileSplit;\n+import org.apache.hadoop.mapred.InputFormat;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.util.ReflectionUtils;\n+\n+import javax.annotation.Nullable;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.stream.IntStream;\n+\n+import static org.apache.flink.connector.file.src.FileSource.DEFAULT_SPLIT_ASSIGNER;\n+import static org.apache.flink.util.Preconditions.checkNotNull;\n+import static org.apache.hadoop.mapreduce.lib.input.FileInputFormat.INPUT_DIR;\n+\n+/**\n+ * A unified data source that reads a hive table.\n+ */\n+public class HiveSource extends AbstractFileSource<RowData, HiveSourceSplit> implements ResultTypeQueryable<RowData> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\t// DataType of the records to be returned, with projection applied (if any)\n+\tprivate final DataType producedDataType;\n+\n+\tHiveSource(\n+\t\t\tJobConf jobConf,\n+\t\t\tCatalogTable catalogTable,\n+\t\t\tList<HiveTablePartition> partitions,\n+\t\t\tint[] projectedFields,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "197131cd8bb420b5914d07af898afd04089af554"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzA4NDQzMw==", "bodyText": "Better in HiveSourceFileEnumerator", "url": "https://github.com/apache/flink/pull/13915#discussion_r517084433", "createdAt": "2020-11-04T03:57:17Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveSource.java", "diffHunk": "@@ -0,0 +1,168 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.ResultTypeQueryable;\n+import org.apache.flink.connector.file.src.AbstractFileSource;\n+import org.apache.flink.connector.file.src.reader.BulkFormat;\n+import org.apache.flink.connectors.hive.read.HiveBulkFormatAdapter;\n+import org.apache.flink.connectors.hive.read.HiveSourceSplit;\n+import org.apache.flink.core.io.SimpleVersionedSerializer;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.runtime.types.TypeInfoDataTypeConverter;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.mapred.FileSplit;\n+import org.apache.hadoop.mapred.InputFormat;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.util.ReflectionUtils;\n+\n+import javax.annotation.Nullable;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.stream.IntStream;\n+\n+import static org.apache.flink.connector.file.src.FileSource.DEFAULT_SPLIT_ASSIGNER;\n+import static org.apache.flink.util.Preconditions.checkNotNull;\n+import static org.apache.hadoop.mapreduce.lib.input.FileInputFormat.INPUT_DIR;\n+\n+/**\n+ * A unified data source that reads a hive table.\n+ */\n+public class HiveSource extends AbstractFileSource<RowData, HiveSourceSplit> implements ResultTypeQueryable<RowData> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\t// DataType of the records to be returned, with projection applied (if any)\n+\tprivate final DataType producedDataType;\n+\n+\tHiveSource(\n+\t\t\tJobConf jobConf,\n+\t\t\tCatalogTable catalogTable,\n+\t\t\tList<HiveTablePartition> partitions,\n+\t\t\tint[] projectedFields,\n+\t\t\tlong limit,\n+\t\t\tString hiveVersion,\n+\t\t\tboolean useMapRedReader,\n+\t\t\tboolean isStreamingSource,\n+\t\t\tDataType producedDataType) {\n+\t\tsuper(\n+\t\t\t\tnew org.apache.flink.core.fs.Path[1],\n+\t\t\t\tnew HiveSourceFileEnumerator.Provider(partitions, new JobConfWrapper(jobConf)),\n+\t\t\t\tDEFAULT_SPLIT_ASSIGNER,\n+\t\t\t\tcreateBulkFormat(new JobConf(jobConf), catalogTable, projectedFields, hiveVersion, producedDataType, useMapRedReader, limit),\n+\t\t\t\tnull);\n+\t\tPreconditions.checkArgument(!isStreamingSource, \"HiveSource currently only supports bounded mode\");\n+\t\tthis.producedDataType = producedDataType;\n+\t}\n+\n+\t@Override\n+\tpublic SimpleVersionedSerializer<HiveSourceSplit> getSplitSerializer() {\n+\t\treturn HiveSourceSplitSerializer.INSTANCE;\n+\t}\n+\n+\t@Override\n+\tpublic TypeInformation<RowData> getProducedType() {\n+\t\treturn (TypeInformation<RowData>) TypeInfoDataTypeConverter.fromDataTypeToTypeInfo(producedDataType);\n+\t}\n+\n+\tprivate static BulkFormat<RowData, HiveSourceSplit> createBulkFormat(\n+\t\t\tJobConf jobConf,\n+\t\t\tCatalogTable catalogTable,\n+\t\t\t@Nullable int[] projectedFields,\n+\t\t\tString hiveVersion,\n+\t\t\tDataType producedDataType,\n+\t\t\tboolean useMapRedReader,\n+\t\t\tlong limit) {\n+\t\tcheckNotNull(catalogTable, \"catalogTable can not be null.\");\n+\t\treturn new HiveBulkFormatAdapter(\n+\t\t\t\tnew JobConfWrapper(jobConf),\n+\t\t\t\tcatalogTable.getPartitionKeys(),\n+\t\t\t\tcatalogTable.getSchema().getFieldNames(),\n+\t\t\t\tcatalogTable.getSchema().getFieldDataTypes(),\n+\t\t\t\tprojectedFields != null ? projectedFields : IntStream.range(0, catalogTable.getSchema().getFieldCount()).toArray(),\n+\t\t\t\thiveVersion,\n+\t\t\t\tproducedDataType,\n+\t\t\t\tuseMapRedReader,\n+\t\t\t\tlimit);\n+\t}\n+\n+\tpublic static List<HiveSourceSplit> createInputSplits(\n+\t\t\tint minNumSplits,\n+\t\t\tList<HiveTablePartition> partitions,\n+\t\t\tJobConf jobConf) throws IOException {\n+\t\tList<HiveSourceSplit> hiveSplits = new ArrayList<>();\n+\t\tFileSystem fs = null;\n+\t\tfor (HiveTablePartition partition : partitions) {\n+\t\t\tStorageDescriptor sd = partition.getStorageDescriptor();\n+\t\t\tPath inputPath = new Path(sd.getLocation());\n+\t\t\tif (fs == null) {\n+\t\t\t\tfs = inputPath.getFileSystem(jobConf);\n+\t\t\t}\n+\t\t\t// it's possible a partition exists in metastore but the data has been removed\n+\t\t\tif (!fs.exists(inputPath)) {\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\tInputFormat format;\n+\t\t\ttry {\n+\t\t\t\tformat = (InputFormat)\n+\t\t\t\t\t\tClass.forName(sd.getInputFormat(), true, Thread.currentThread().getContextClassLoader()).newInstance();\n+\t\t\t} catch (Exception e) {\n+\t\t\t\tthrow new FlinkHiveException(\"Unable to instantiate the hadoop input format\", e);\n+\t\t\t}\n+\t\t\tReflectionUtils.setConf(format, jobConf);\n+\t\t\tjobConf.set(INPUT_DIR, sd.getLocation());\n+\t\t\t//TODO: we should consider how to calculate the splits according to minNumSplits in the future.\n+\t\t\torg.apache.hadoop.mapred.InputSplit[] splitArray = format.getSplits(jobConf, minNumSplits);\n+\t\t\tfor (org.apache.hadoop.mapred.InputSplit inputSplit : splitArray) {\n+\t\t\t\tPreconditions.checkState(inputSplit instanceof FileSplit,\n+\t\t\t\t\t\t\"Unsupported InputSplit type: \" + inputSplit.getClass().getName());\n+\t\t\t\thiveSplits.add(new HiveSourceSplit((FileSplit) inputSplit, partition, null));\n+\t\t\t}\n+\t\t}\n+\n+\t\treturn hiveSplits;\n+\t}\n+\n+\tpublic static int getNumFiles(List<HiveTablePartition> partitions, JobConf jobConf) throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "197131cd8bb420b5914d07af898afd04089af554"}, "originalPosition": 151}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzA4NDg2Ng==", "bodyText": "Why do you want to create a new FileSourceSplit? Why not just use HiveSourceSplit", "url": "https://github.com/apache/flink/pull/13915#discussion_r517084866", "createdAt": "2020-11-04T03:59:15Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveSourceSplitSerializer.java", "diffHunk": "@@ -0,0 +1,100 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+import org.apache.flink.connector.file.src.FileSourceSplit;\n+import org.apache.flink.connector.file.src.FileSourceSplitSerializer;\n+import org.apache.flink.connectors.hive.read.HiveSourceSplit;\n+import org.apache.flink.core.io.SimpleVersionedSerializer;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.ObjectInputStream;\n+import java.io.ObjectOutputStream;\n+\n+import static org.apache.flink.util.Preconditions.checkArgument;\n+\n+/**\n+ * SerDe for {@link HiveSourceSplit}.\n+ */\n+public class HiveSourceSplitSerializer implements SimpleVersionedSerializer<HiveSourceSplit> {\n+\n+\tprivate static final int VERSION = 1;\n+\n+\tpublic static final HiveSourceSplitSerializer INSTANCE = new HiveSourceSplitSerializer();\n+\n+\tprivate HiveSourceSplitSerializer() {\n+\t}\n+\n+\t@Override\n+\tpublic int getVersion() {\n+\t\treturn VERSION;\n+\t}\n+\n+\t@Override\n+\tpublic byte[] serialize(HiveSourceSplit split) throws IOException {\n+\t\tcheckArgument(split.getClass() == HiveSourceSplit.class, \"Cannot serialize subclasses of HiveSourceSplit\");\n+\n+\t\tByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream();\n+\t\ttry (ObjectOutputStream outputStream = new ObjectOutputStream(byteArrayOutputStream)) {\n+\t\t\tserialize(outputStream, split);\n+\t\t}\n+\t\treturn byteArrayOutputStream.toByteArray();\n+\t}\n+\n+\t@Override\n+\tpublic HiveSourceSplit deserialize(int version, byte[] serialized) throws IOException {\n+\t\tif (version == 1) {\n+\t\t\ttry (ObjectInputStream inputStream = new ObjectInputStream(new ByteArrayInputStream(serialized))) {\n+\t\t\t\treturn deserializeV1(inputStream);\n+\t\t\t}\n+\t\t} else {\n+\t\t\tthrow new IOException(\"Unknown version: \" + version);\n+\t\t}\n+\t}\n+\n+\tprivate void serialize(ObjectOutputStream outputStream, HiveSourceSplit split) throws IOException {\n+\t\tbyte[] superBytes = FileSourceSplitSerializer.INSTANCE.serialize(new FileSourceSplit(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "197131cd8bb420b5914d07af898afd04089af554"}, "originalPosition": 74}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzA4NTUxMw==", "bodyText": "Maybe use InstantiationUtil.serializeObject directly?", "url": "https://github.com/apache/flink/pull/13915#discussion_r517085513", "createdAt": "2020-11-04T04:01:56Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveSourceSplitSerializer.java", "diffHunk": "@@ -0,0 +1,100 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+import org.apache.flink.connector.file.src.FileSourceSplit;\n+import org.apache.flink.connector.file.src.FileSourceSplitSerializer;\n+import org.apache.flink.connectors.hive.read.HiveSourceSplit;\n+import org.apache.flink.core.io.SimpleVersionedSerializer;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.ObjectInputStream;\n+import java.io.ObjectOutputStream;\n+\n+import static org.apache.flink.util.Preconditions.checkArgument;\n+\n+/**\n+ * SerDe for {@link HiveSourceSplit}.\n+ */\n+public class HiveSourceSplitSerializer implements SimpleVersionedSerializer<HiveSourceSplit> {\n+\n+\tprivate static final int VERSION = 1;\n+\n+\tpublic static final HiveSourceSplitSerializer INSTANCE = new HiveSourceSplitSerializer();\n+\n+\tprivate HiveSourceSplitSerializer() {\n+\t}\n+\n+\t@Override\n+\tpublic int getVersion() {\n+\t\treturn VERSION;\n+\t}\n+\n+\t@Override\n+\tpublic byte[] serialize(HiveSourceSplit split) throws IOException {\n+\t\tcheckArgument(split.getClass() == HiveSourceSplit.class, \"Cannot serialize subclasses of HiveSourceSplit\");\n+\n+\t\tByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream();\n+\t\ttry (ObjectOutputStream outputStream = new ObjectOutputStream(byteArrayOutputStream)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "197131cd8bb420b5914d07af898afd04089af554"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzA4ODAwMA==", "bodyText": "Ditto", "url": "https://github.com/apache/flink/pull/13915#discussion_r517088000", "createdAt": "2020-11-04T04:13:50Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveBulkFormatAdapter.java", "diffHunk": "@@ -0,0 +1,291 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive.read;\n+\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.connector.file.src.reader.BulkFormat;\n+import org.apache.flink.connector.file.src.util.ArrayResultIterator;\n+import org.apache.flink.connectors.hive.HiveTablePartition;\n+import org.apache.flink.connectors.hive.JobConfWrapper;\n+import org.apache.flink.formats.parquet.ParquetColumnarRowInputFormat;\n+import org.apache.flink.table.catalog.hive.client.HiveShim;\n+import org.apache.flink.table.catalog.hive.client.HiveShimLoader;\n+import org.apache.flink.table.catalog.hive.util.HiveTypeUtil;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.filesystem.PartitionValueConverter;\n+import org.apache.flink.table.runtime.types.TypeInfoDataTypeConverter;\n+import org.apache.flink.table.runtime.typeutils.RowDataSerializer;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.io.IOConstants;\n+import org.apache.hadoop.hive.serde2.ColumnProjectionUtils;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.connector.file.src.util.CheckpointedPosition.NO_OFFSET;\n+import static org.apache.flink.table.data.vector.VectorizedColumnBatch.DEFAULT_SIZE;\n+\n+/**\n+ * A BulkFormat implementation for HiveSource. This implementation delegates reading to other BulkFormat instances,\n+ * because different hive partitions may need different BulkFormat to do the reading.\n+ */\n+public class HiveBulkFormatAdapter implements BulkFormat<RowData, HiveSourceSplit> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(HiveBulkFormatAdapter.class);\n+\n+\t// schema evolution configs are not available in older versions of IOConstants, let's define them ourselves\n+\tprivate static final String SCHEMA_EVOLUTION_COLUMNS = \"schema.evolution.columns\";\n+\tprivate static final String SCHEMA_EVOLUTION_COLUMNS_TYPES = \"schema.evolution.columns.types\";\n+\n+\tprivate final JobConfWrapper jobConfWrapper;\n+\tprivate final List<String> partitionKeys;\n+\tprivate final String[] fieldNames;\n+\tprivate final DataType[] fieldTypes;\n+\t// indices of fields to be returned, with projection applied (if any)\n+\tprivate final int[] selectedFields;\n+\tprivate final String hiveVersion;\n+\tprivate final HiveShim hiveShim;\n+\tprivate final DataType producedDataType;\n+\tprivate final boolean useMapRedReader;\n+\t// We should limit the input read count of the splits, -1 represents no limit.\n+\tprivate final long limit;\n+\n+\tpublic HiveBulkFormatAdapter(JobConfWrapper jobConfWrapper, List<String> partitionKeys, String[] fieldNames, DataType[] fieldTypes,\n+\t\t\tint[] selectedFields, String hiveVersion, DataType producedDataType, boolean useMapRedReader, long limit) {\n+\t\tthis.jobConfWrapper = jobConfWrapper;\n+\t\tthis.partitionKeys = partitionKeys;\n+\t\tthis.fieldNames = fieldNames;\n+\t\tthis.fieldTypes = fieldTypes;\n+\t\tthis.selectedFields = selectedFields;\n+\t\tthis.hiveVersion = hiveVersion;\n+\t\tthis.hiveShim = HiveShimLoader.loadHiveShim(hiveVersion);\n+\t\tthis.producedDataType = producedDataType;\n+\t\tthis.useMapRedReader = useMapRedReader;\n+\t\tthis.limit = limit;\n+\t}\n+\n+\t@Override\n+\tpublic Reader<RowData> createReader(Configuration config, HiveSourceSplit split)\n+\t\t\tthrows IOException {\n+\t\treturn createBulkFormatForSplit(split).createReader(config, split);\n+\t}\n+\n+\t@Override\n+\tpublic Reader<RowData> restoreReader(Configuration config, HiveSourceSplit split) throws IOException {\n+\t\treturn createBulkFormatForSplit(split).restoreReader(config, split);\n+\t}\n+\n+\t@Override\n+\tpublic boolean isSplittable() {\n+\t\treturn true;\n+\t}\n+\n+\t@Override\n+\tpublic TypeInformation<RowData> getProducedType() {\n+\t\treturn (TypeInformation<RowData>) TypeInfoDataTypeConverter.fromDataTypeToTypeInfo(producedDataType);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "197131cd8bb420b5914d07af898afd04089af554"}, "originalPosition": 113}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzA4OTY2Mg==", "bodyText": "Why to toMapRedSplit, call another constructor?", "url": "https://github.com/apache/flink/pull/13915#discussion_r517089662", "createdAt": "2020-11-04T04:21:45Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveSourceSplit.java", "diffHunk": "@@ -0,0 +1,95 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive.read;\n+\n+import org.apache.flink.connector.file.src.FileSourceSplit;\n+import org.apache.flink.connector.file.src.util.CheckpointedPosition;\n+import org.apache.flink.connectors.hive.FlinkHiveException;\n+import org.apache.flink.connectors.hive.HiveTablePartition;\n+import org.apache.flink.core.fs.Path;\n+\n+import org.apache.hadoop.mapred.FileSplit;\n+\n+import javax.annotation.Nullable;\n+\n+import java.io.IOException;\n+\n+import static org.apache.flink.util.Preconditions.checkNotNull;\n+\n+/**\n+ * A wrapper class that wraps info needed for a file input split.\n+ * Right now, it contains info about the partition of the split.\n+ */\n+public class HiveSourceSplit extends FileSourceSplit {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprotected final HiveTablePartition hiveTablePartition;\n+\n+\tpublic HiveSourceSplit(\n+\t\t\tFileSplit fileSplit,\n+\t\t\tHiveTablePartition hiveTablePartition,\n+\t\t\t@Nullable CheckpointedPosition readerPosition) throws IOException {\n+\t\tthis(\n+\t\t\t\tfileSplit.toString(),\n+\t\t\t\tnew Path(fileSplit.getPath().toString()),\n+\t\t\t\tfileSplit.getStart(),\n+\t\t\t\tfileSplit.getLength(),\n+\t\t\t\tfileSplit.getLocations(),\n+\t\t\t\treaderPosition,\n+\t\t\t\thiveTablePartition\n+\t\t);\n+\t}\n+\n+\tpublic HiveSourceSplit(\n+\t\t\tString id,\n+\t\t\tPath filePath,\n+\t\t\tlong offset,\n+\t\t\tlong length,\n+\t\t\tString[] hostnames,\n+\t\t\t@Nullable CheckpointedPosition readerPosition,\n+\t\t\tHiveTablePartition hiveTablePartition) {\n+\t\tsuper(id, filePath, offset, length, hostnames, readerPosition);\n+\t\tthis.hiveTablePartition = checkNotNull(hiveTablePartition, \"hiveTablePartition can not be null\");\n+\t}\n+\n+\tpublic HiveTablePartition getHiveTablePartition() {\n+\t\treturn hiveTablePartition;\n+\t}\n+\n+\tpublic FileSplit toMapRedSplit() {\n+\t\treturn new FileSplit(new org.apache.hadoop.fs.Path(path().toString()), offset(), length(), hostnames());\n+\t}\n+\n+\t@Override\n+\tpublic FileSourceSplit updateWithCheckpointedPosition(@Nullable CheckpointedPosition position) {\n+\t\ttry {\n+\t\t\treturn new HiveSourceSplit(toMapRedSplit(), hiveTablePartition, position);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "197131cd8bb420b5914d07af898afd04089af554"}, "originalPosition": 83}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIzMDU4ODky", "url": "https://github.com/apache/flink/pull/13915#pullrequestreview-523058892", "createdAt": "2020-11-04T05:50:03Z", "commit": {"oid": "197131cd8bb420b5914d07af898afd04089af554"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQwNTo1MDowM1rOHtJ6Tg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQwNTo1MDowM1rOHtJ6Tg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzExMDM1MA==", "bodyText": "This will fail when partition values can not be found in path.\nI create #13919 to refactor this.", "url": "https://github.com/apache/flink/pull/13915#discussion_r517110350", "createdAt": "2020-11-04T05:50:03Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveBulkFormatAdapter.java", "diffHunk": "@@ -0,0 +1,291 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive.read;\n+\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.connector.file.src.reader.BulkFormat;\n+import org.apache.flink.connector.file.src.util.ArrayResultIterator;\n+import org.apache.flink.connectors.hive.HiveTablePartition;\n+import org.apache.flink.connectors.hive.JobConfWrapper;\n+import org.apache.flink.formats.parquet.ParquetColumnarRowInputFormat;\n+import org.apache.flink.table.catalog.hive.client.HiveShim;\n+import org.apache.flink.table.catalog.hive.client.HiveShimLoader;\n+import org.apache.flink.table.catalog.hive.util.HiveTypeUtil;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.filesystem.PartitionValueConverter;\n+import org.apache.flink.table.runtime.types.TypeInfoDataTypeConverter;\n+import org.apache.flink.table.runtime.typeutils.RowDataSerializer;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.io.IOConstants;\n+import org.apache.hadoop.hive.serde2.ColumnProjectionUtils;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.connector.file.src.util.CheckpointedPosition.NO_OFFSET;\n+import static org.apache.flink.table.data.vector.VectorizedColumnBatch.DEFAULT_SIZE;\n+\n+/**\n+ * A BulkFormat implementation for HiveSource. This implementation delegates reading to other BulkFormat instances,\n+ * because different hive partitions may need different BulkFormat to do the reading.\n+ */\n+public class HiveBulkFormatAdapter implements BulkFormat<RowData, HiveSourceSplit> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(HiveBulkFormatAdapter.class);\n+\n+\t// schema evolution configs are not available in older versions of IOConstants, let's define them ourselves\n+\tprivate static final String SCHEMA_EVOLUTION_COLUMNS = \"schema.evolution.columns\";\n+\tprivate static final String SCHEMA_EVOLUTION_COLUMNS_TYPES = \"schema.evolution.columns.types\";\n+\n+\tprivate final JobConfWrapper jobConfWrapper;\n+\tprivate final List<String> partitionKeys;\n+\tprivate final String[] fieldNames;\n+\tprivate final DataType[] fieldTypes;\n+\t// indices of fields to be returned, with projection applied (if any)\n+\tprivate final int[] selectedFields;\n+\tprivate final String hiveVersion;\n+\tprivate final HiveShim hiveShim;\n+\tprivate final DataType producedDataType;\n+\tprivate final boolean useMapRedReader;\n+\t// We should limit the input read count of the splits, -1 represents no limit.\n+\tprivate final long limit;\n+\n+\tpublic HiveBulkFormatAdapter(JobConfWrapper jobConfWrapper, List<String> partitionKeys, String[] fieldNames, DataType[] fieldTypes,\n+\t\t\tint[] selectedFields, String hiveVersion, DataType producedDataType, boolean useMapRedReader, long limit) {\n+\t\tthis.jobConfWrapper = jobConfWrapper;\n+\t\tthis.partitionKeys = partitionKeys;\n+\t\tthis.fieldNames = fieldNames;\n+\t\tthis.fieldTypes = fieldTypes;\n+\t\tthis.selectedFields = selectedFields;\n+\t\tthis.hiveVersion = hiveVersion;\n+\t\tthis.hiveShim = HiveShimLoader.loadHiveShim(hiveVersion);\n+\t\tthis.producedDataType = producedDataType;\n+\t\tthis.useMapRedReader = useMapRedReader;\n+\t\tthis.limit = limit;\n+\t}\n+\n+\t@Override\n+\tpublic Reader<RowData> createReader(Configuration config, HiveSourceSplit split)\n+\t\t\tthrows IOException {\n+\t\treturn createBulkFormatForSplit(split).createReader(config, split);\n+\t}\n+\n+\t@Override\n+\tpublic Reader<RowData> restoreReader(Configuration config, HiveSourceSplit split) throws IOException {\n+\t\treturn createBulkFormatForSplit(split).restoreReader(config, split);\n+\t}\n+\n+\t@Override\n+\tpublic boolean isSplittable() {\n+\t\treturn true;\n+\t}\n+\n+\t@Override\n+\tpublic TypeInformation<RowData> getProducedType() {\n+\t\treturn (TypeInformation<RowData>) TypeInfoDataTypeConverter.fromDataTypeToTypeInfo(producedDataType);\n+\t}\n+\n+\tprivate BulkFormat<RowData, ? super HiveSourceSplit> createBulkFormatForSplit(HiveSourceSplit split) {\n+\t\tif (!useMapRedReader && useParquetVectorizedRead(split.getHiveTablePartition())) {\n+\t\t\t// TODO: need a way to support limit push down\n+\t\t\treturn ParquetColumnarRowInputFormat.createPartitionedFormat(\n+\t\t\t\t\tjobConfWrapper.conf(),\n+\t\t\t\t\t(RowType) producedDataType.getLogicalType(),\n+\t\t\t\t\tpartitionKeys,\n+\t\t\t\t\tjobConfWrapper.conf().get(HiveConf.ConfVars.DEFAULTPARTITIONNAME.varname,\n+\t\t\t\t\t\t\tHiveConf.ConfVars.DEFAULTPARTITIONNAME.defaultStrVal),\n+\t\t\t\t\t(PartitionValueConverter) (colName, valStr, type) -> split.getHiveTablePartition().getPartitionSpec().get(colName),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "197131cd8bb420b5914d07af898afd04089af554"}, "originalPosition": 125}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIzMDYxMjEw", "url": "https://github.com/apache/flink/pull/13915#pullrequestreview-523061210", "createdAt": "2020-11-04T05:57:07Z", "commit": {"oid": "197131cd8bb420b5914d07af898afd04089af554"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQwNTo1NzowN1rOHtKBiQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQwNTo1NzowN1rOHtKBiQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzExMjIwMQ==", "bodyText": "If you don't want migrate HiveMapredSplitReader now, can you create JIRA for this?", "url": "https://github.com/apache/flink/pull/13915#discussion_r517112201", "createdAt": "2020-11-04T05:57:07Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveBulkFormatAdapter.java", "diffHunk": "@@ -0,0 +1,291 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive.read;\n+\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.connector.file.src.reader.BulkFormat;\n+import org.apache.flink.connector.file.src.util.ArrayResultIterator;\n+import org.apache.flink.connectors.hive.HiveTablePartition;\n+import org.apache.flink.connectors.hive.JobConfWrapper;\n+import org.apache.flink.formats.parquet.ParquetColumnarRowInputFormat;\n+import org.apache.flink.table.catalog.hive.client.HiveShim;\n+import org.apache.flink.table.catalog.hive.client.HiveShimLoader;\n+import org.apache.flink.table.catalog.hive.util.HiveTypeUtil;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.filesystem.PartitionValueConverter;\n+import org.apache.flink.table.runtime.types.TypeInfoDataTypeConverter;\n+import org.apache.flink.table.runtime.typeutils.RowDataSerializer;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.io.IOConstants;\n+import org.apache.hadoop.hive.serde2.ColumnProjectionUtils;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.connector.file.src.util.CheckpointedPosition.NO_OFFSET;\n+import static org.apache.flink.table.data.vector.VectorizedColumnBatch.DEFAULT_SIZE;\n+\n+/**\n+ * A BulkFormat implementation for HiveSource. This implementation delegates reading to other BulkFormat instances,\n+ * because different hive partitions may need different BulkFormat to do the reading.\n+ */\n+public class HiveBulkFormatAdapter implements BulkFormat<RowData, HiveSourceSplit> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(HiveBulkFormatAdapter.class);\n+\n+\t// schema evolution configs are not available in older versions of IOConstants, let's define them ourselves\n+\tprivate static final String SCHEMA_EVOLUTION_COLUMNS = \"schema.evolution.columns\";\n+\tprivate static final String SCHEMA_EVOLUTION_COLUMNS_TYPES = \"schema.evolution.columns.types\";\n+\n+\tprivate final JobConfWrapper jobConfWrapper;\n+\tprivate final List<String> partitionKeys;\n+\tprivate final String[] fieldNames;\n+\tprivate final DataType[] fieldTypes;\n+\t// indices of fields to be returned, with projection applied (if any)\n+\tprivate final int[] selectedFields;\n+\tprivate final String hiveVersion;\n+\tprivate final HiveShim hiveShim;\n+\tprivate final DataType producedDataType;\n+\tprivate final boolean useMapRedReader;\n+\t// We should limit the input read count of the splits, -1 represents no limit.\n+\tprivate final long limit;\n+\n+\tpublic HiveBulkFormatAdapter(JobConfWrapper jobConfWrapper, List<String> partitionKeys, String[] fieldNames, DataType[] fieldTypes,\n+\t\t\tint[] selectedFields, String hiveVersion, DataType producedDataType, boolean useMapRedReader, long limit) {\n+\t\tthis.jobConfWrapper = jobConfWrapper;\n+\t\tthis.partitionKeys = partitionKeys;\n+\t\tthis.fieldNames = fieldNames;\n+\t\tthis.fieldTypes = fieldTypes;\n+\t\tthis.selectedFields = selectedFields;\n+\t\tthis.hiveVersion = hiveVersion;\n+\t\tthis.hiveShim = HiveShimLoader.loadHiveShim(hiveVersion);\n+\t\tthis.producedDataType = producedDataType;\n+\t\tthis.useMapRedReader = useMapRedReader;\n+\t\tthis.limit = limit;\n+\t}\n+\n+\t@Override\n+\tpublic Reader<RowData> createReader(Configuration config, HiveSourceSplit split)\n+\t\t\tthrows IOException {\n+\t\treturn createBulkFormatForSplit(split).createReader(config, split);\n+\t}\n+\n+\t@Override\n+\tpublic Reader<RowData> restoreReader(Configuration config, HiveSourceSplit split) throws IOException {\n+\t\treturn createBulkFormatForSplit(split).restoreReader(config, split);\n+\t}\n+\n+\t@Override\n+\tpublic boolean isSplittable() {\n+\t\treturn true;\n+\t}\n+\n+\t@Override\n+\tpublic TypeInformation<RowData> getProducedType() {\n+\t\treturn (TypeInformation<RowData>) TypeInfoDataTypeConverter.fromDataTypeToTypeInfo(producedDataType);\n+\t}\n+\n+\tprivate BulkFormat<RowData, ? super HiveSourceSplit> createBulkFormatForSplit(HiveSourceSplit split) {\n+\t\tif (!useMapRedReader && useParquetVectorizedRead(split.getHiveTablePartition())) {\n+\t\t\t// TODO: need a way to support limit push down\n+\t\t\treturn ParquetColumnarRowInputFormat.createPartitionedFormat(\n+\t\t\t\t\tjobConfWrapper.conf(),\n+\t\t\t\t\t(RowType) producedDataType.getLogicalType(),\n+\t\t\t\t\tpartitionKeys,\n+\t\t\t\t\tjobConfWrapper.conf().get(HiveConf.ConfVars.DEFAULTPARTITIONNAME.varname,\n+\t\t\t\t\t\t\tHiveConf.ConfVars.DEFAULTPARTITIONNAME.defaultStrVal),\n+\t\t\t\t\t(PartitionValueConverter) (colName, valStr, type) -> split.getHiveTablePartition().getPartitionSpec().get(colName),\n+\t\t\t\t\tDEFAULT_SIZE,\n+\t\t\t\t\thiveVersion.startsWith(\"3\"),\n+\t\t\t\t\tfalse\n+\t\t\t);\n+\t\t} else {\n+\t\t\treturn new HiveMapRedBulkFormat();\n+\t\t}\n+\t}\n+\n+\tprivate boolean useParquetVectorizedRead(HiveTablePartition partition) {\n+\t\tboolean isParquet = partition.getStorageDescriptor().getSerdeInfo().getSerializationLib()\n+\t\t\t\t.toLowerCase().contains(\"parquet\");\n+\t\tif (!isParquet) {\n+\t\t\treturn false;\n+\t\t}\n+\n+\t\tfor (int i : selectedFields) {\n+\t\t\tif (isVectorizationUnsupported(fieldTypes[i].getLogicalType())) {\n+\t\t\t\tLOG.info(\"Fallback to hadoop mapred reader, unsupported field type: \" + fieldTypes[i]);\n+\t\t\t\treturn false;\n+\t\t\t}\n+\t\t}\n+\n+\t\tLOG.info(\"Use flink parquet ColumnarRowData reader.\");\n+\t\treturn true;\n+\t}\n+\n+\tprivate static boolean isVectorizationUnsupported(LogicalType t) {\n+\t\tswitch (t.getTypeRoot()) {\n+\t\t\tcase CHAR:\n+\t\t\tcase VARCHAR:\n+\t\t\tcase BOOLEAN:\n+\t\t\tcase BINARY:\n+\t\t\tcase VARBINARY:\n+\t\t\tcase DECIMAL:\n+\t\t\tcase TINYINT:\n+\t\t\tcase SMALLINT:\n+\t\t\tcase INTEGER:\n+\t\t\tcase BIGINT:\n+\t\t\tcase FLOAT:\n+\t\t\tcase DOUBLE:\n+\t\t\tcase DATE:\n+\t\t\tcase TIME_WITHOUT_TIME_ZONE:\n+\t\t\tcase TIMESTAMP_WITHOUT_TIME_ZONE:\n+\t\t\tcase TIMESTAMP_WITH_LOCAL_TIME_ZONE:\n+\t\t\t\treturn false;\n+\t\t\tcase TIMESTAMP_WITH_TIME_ZONE:\n+\t\t\tcase INTERVAL_YEAR_MONTH:\n+\t\t\tcase INTERVAL_DAY_TIME:\n+\t\t\tcase ARRAY:\n+\t\t\tcase MULTISET:\n+\t\t\tcase MAP:\n+\t\t\tcase ROW:\n+\t\t\tcase DISTINCT_TYPE:\n+\t\t\tcase STRUCTURED_TYPE:\n+\t\t\tcase NULL:\n+\t\t\tcase RAW:\n+\t\t\tcase SYMBOL:\n+\t\t\tdefault:\n+\t\t\t\treturn true;\n+\t\t}\n+\t}\n+\n+\tprivate class HiveMapRedBulkFormat implements BulkFormat<RowData, HiveSourceSplit> {\n+\n+\t\tprivate static final long serialVersionUID = 1L;\n+\n+\t\t@Override\n+\t\tpublic Reader<RowData> createReader(Configuration config, HiveSourceSplit split)\n+\t\t\t\tthrows IOException {\n+\t\t\treturn new HiveReader(split);\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic Reader<RowData> restoreReader(Configuration config, HiveSourceSplit split) throws IOException {\n+\t\t\tassert split.getReaderPosition().isPresent();\n+\t\t\tHiveReader hiveReader = new HiveReader(split);\n+\t\t\thiveReader.seek(split.getReaderPosition().get().getRecordsAfterOffset());\n+\t\t\treturn hiveReader;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic boolean isSplittable() {\n+\t\t\treturn true;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic TypeInformation<RowData> getProducedType() {\n+\t\t\treturn (TypeInformation<RowData>) TypeInfoDataTypeConverter.fromDataTypeToTypeInfo(producedDataType);\n+\t\t}\n+\t}\n+\n+\tprivate class HiveReader implements BulkFormat.Reader<RowData> {\n+\n+\t\tprivate final HiveMapredSplitReader hiveMapredSplitReader;\n+\t\tprivate final RowDataSerializer serializer;\n+\t\tprivate long numRead = 0;\n+\n+\t\tprivate HiveReader(HiveSourceSplit split) throws IOException {\n+\t\t\tJobConf clonedConf = new JobConf(jobConfWrapper.conf());\n+\t\t\taddSchemaToConf(clonedConf);\n+\t\t\tHiveTableInputSplit oldSplit = new HiveTableInputSplit(-1, split.toMapRedSplit(), clonedConf, split.getHiveTablePartition());\n+\t\t\thiveMapredSplitReader = new HiveMapredSplitReader(clonedConf, partitionKeys, fieldTypes, selectedFields, oldSplit, hiveShim);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "197131cd8bb420b5914d07af898afd04089af554"}, "originalPosition": 228}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6ed0a96f5a7cefad7260b8549cac601cc8cdd376", "author": {"user": {"login": "lirui-apache", "name": "Rui Li"}}, "url": "https://github.com/apache/flink/commit/6ed0a96f5a7cefad7260b8549cac601cc8cdd376", "committedDate": "2020-11-04T11:18:14Z", "message": "[FLINK-19365][hive] Migrate Hive source to FLIP-27 source interface for batch"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fe892ec28edc2ec9710fa25a8acc96c195a8a524", "author": {"user": {"login": "lirui-apache", "name": "Rui Li"}}, "url": "https://github.com/apache/flink/commit/fe892ec28edc2ec9710fa25a8acc96c195a8a524", "committedDate": "2020-11-04T11:18:15Z", "message": "address comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "54abdd1fb2ee2b4c478366d013c02631a559ac58", "author": {"user": {"login": "lirui-apache", "name": "Rui Li"}}, "url": "https://github.com/apache/flink/commit/54abdd1fb2ee2b4c478366d013c02631a559ac58", "committedDate": "2020-11-04T12:03:15Z", "message": "address comments"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "c85d0073af32760866e6fe1a4e1030bfee4dddc8", "author": {"user": {"login": "lirui-apache", "name": "Rui Li"}}, "url": "https://github.com/apache/flink/commit/c85d0073af32760866e6fe1a4e1030bfee4dddc8", "committedDate": "2020-11-04T09:33:48Z", "message": "address comments"}, "afterCommit": {"oid": "54abdd1fb2ee2b4c478366d013c02631a559ac58", "author": {"user": {"login": "lirui-apache", "name": "Rui Li"}}, "url": "https://github.com/apache/flink/commit/54abdd1fb2ee2b4c478366d013c02631a559ac58", "committedDate": "2020-11-04T12:03:15Z", "message": "address comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIzODc1MTY0", "url": "https://github.com/apache/flink/pull/13915#pullrequestreview-523875164", "createdAt": "2020-11-05T02:11:21Z", "commit": {"oid": "54abdd1fb2ee2b4c478366d013c02631a559ac58"}, "state": "COMMENTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQwMjoxMToyMVrOHtwY7A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQwMjoxNTowN1rOHtwhfQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzc0MDc4MA==", "bodyText": "I think you should use LimitableBulkFormat here.", "url": "https://github.com/apache/flink/pull/13915#discussion_r517740780", "createdAt": "2020-11-05T02:11:21Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveSource.java", "diffHunk": "@@ -0,0 +1,89 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+import org.apache.flink.api.java.typeutils.ResultTypeQueryable;\n+import org.apache.flink.connector.file.src.AbstractFileSource;\n+import org.apache.flink.connector.file.src.reader.BulkFormat;\n+import org.apache.flink.connectors.hive.read.HiveBulkFormatAdapter;\n+import org.apache.flink.connectors.hive.read.HiveSourceSplit;\n+import org.apache.flink.core.io.SimpleVersionedSerializer;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.mapred.JobConf;\n+\n+import javax.annotation.Nullable;\n+\n+import java.util.List;\n+\n+import static org.apache.flink.connector.file.src.FileSource.DEFAULT_SPLIT_ASSIGNER;\n+import static org.apache.flink.util.Preconditions.checkNotNull;\n+\n+/**\n+ * A unified data source that reads a hive table.\n+ */\n+public class HiveSource extends AbstractFileSource<RowData, HiveSourceSplit> implements ResultTypeQueryable<RowData> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tHiveSource(\n+\t\t\tJobConf jobConf,\n+\t\t\tCatalogTable catalogTable,\n+\t\t\tList<HiveTablePartition> partitions,\n+\t\t\t@Nullable Long limit,\n+\t\t\tString hiveVersion,\n+\t\t\tboolean useMapRedReader,\n+\t\t\tboolean isStreamingSource,\n+\t\t\tRowType producedDataType) {\n+\t\tsuper(\n+\t\t\t\tnew org.apache.flink.core.fs.Path[1],\n+\t\t\t\tnew HiveSourceFileEnumerator.Provider(partitions, new JobConfWrapper(jobConf)),\n+\t\t\t\tDEFAULT_SPLIT_ASSIGNER,\n+\t\t\t\tcreateBulkFormat(new JobConf(jobConf), catalogTable, hiveVersion, producedDataType, useMapRedReader, limit),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "54abdd1fb2ee2b4c478366d013c02631a559ac58"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzc0MDg4MQ==", "bodyText": "Can you change limit.intValue() / 1000 to (int) (limit / 1000)", "url": "https://github.com/apache/flink/pull/13915#discussion_r517740881", "createdAt": "2020-11-05T02:11:46Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveParallelismInference.java", "diffHunk": "@@ -57,9 +57,9 @@\n \t * Apply limit to calculate the parallelism.\n \t * Here limit is the limit in query <code>SELECT * FROM xxx LIMIT [limit]</code>.\n \t */\n-\tint limit(long limit) {\n-\t\tif (limit > 0) {\n-\t\t\tparallelism = Math.min(parallelism, (int) limit / 1000);\n+\tint limit(Long limit) {\n+\t\tif (limit != null) {\n+\t\t\tparallelism = Math.min(parallelism, limit.intValue() / 1000);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "54abdd1fb2ee2b4c478366d013c02631a559ac58"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzc0MTA2NQ==", "bodyText": "Remove limit things in this class", "url": "https://github.com/apache/flink/pull/13915#discussion_r517741065", "createdAt": "2020-11-05T02:12:23Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveBulkFormatAdapter.java", "diffHunk": "@@ -0,0 +1,304 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive.read;\n+\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.connector.file.src.reader.BulkFormat;\n+import org.apache.flink.connector.file.src.util.ArrayResultIterator;\n+import org.apache.flink.connectors.hive.HiveTablePartition;\n+import org.apache.flink.connectors.hive.JobConfWrapper;\n+import org.apache.flink.formats.parquet.ParquetColumnarRowInputFormat;\n+import org.apache.flink.table.catalog.hive.client.HiveShim;\n+import org.apache.flink.table.catalog.hive.client.HiveShimLoader;\n+import org.apache.flink.table.catalog.hive.util.HiveTypeUtil;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.filesystem.LimitableBulkFormat;\n+import org.apache.flink.table.filesystem.PartitionFieldExtractor;\n+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n+import org.apache.flink.table.runtime.typeutils.RowDataSerializer;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.hive.ql.io.IOConstants;\n+import org.apache.hadoop.hive.serde2.ColumnProjectionUtils;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.connector.file.src.util.CheckpointedPosition.NO_OFFSET;\n+import static org.apache.flink.table.data.vector.VectorizedColumnBatch.DEFAULT_SIZE;\n+\n+/**\n+ * A BulkFormat implementation for HiveSource. This implementation delegates reading to other BulkFormat instances,\n+ * because different hive partitions may need different BulkFormat to do the reading.\n+ */\n+public class HiveBulkFormatAdapter implements BulkFormat<RowData, HiveSourceSplit> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(HiveBulkFormatAdapter.class);\n+\n+\t// schema evolution configs are not available in older versions of IOConstants, let's define them ourselves\n+\tprivate static final String SCHEMA_EVOLUTION_COLUMNS = \"schema.evolution.columns\";\n+\tprivate static final String SCHEMA_EVOLUTION_COLUMNS_TYPES = \"schema.evolution.columns.types\";\n+\n+\tprivate final JobConfWrapper jobConfWrapper;\n+\tprivate final List<String> partitionKeys;\n+\tprivate final String[] fieldNames;\n+\tprivate final DataType[] fieldTypes;\n+\tprivate final String hiveVersion;\n+\tprivate final HiveShim hiveShim;\n+\tprivate final RowType producedDataType;\n+\tprivate final boolean useMapRedReader;\n+\t// We should limit the input read count of the splits, null represents no limit.\n+\tprivate final Long limit;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "54abdd1fb2ee2b4c478366d013c02631a559ac58"}, "originalPosition": 78}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzc0MTIyOA==", "bodyText": "producedRowType", "url": "https://github.com/apache/flink/pull/13915#discussion_r517741228", "createdAt": "2020-11-05T02:12:46Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveBulkFormatAdapter.java", "diffHunk": "@@ -0,0 +1,304 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive.read;\n+\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.connector.file.src.reader.BulkFormat;\n+import org.apache.flink.connector.file.src.util.ArrayResultIterator;\n+import org.apache.flink.connectors.hive.HiveTablePartition;\n+import org.apache.flink.connectors.hive.JobConfWrapper;\n+import org.apache.flink.formats.parquet.ParquetColumnarRowInputFormat;\n+import org.apache.flink.table.catalog.hive.client.HiveShim;\n+import org.apache.flink.table.catalog.hive.client.HiveShimLoader;\n+import org.apache.flink.table.catalog.hive.util.HiveTypeUtil;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.filesystem.LimitableBulkFormat;\n+import org.apache.flink.table.filesystem.PartitionFieldExtractor;\n+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n+import org.apache.flink.table.runtime.typeutils.RowDataSerializer;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.hive.ql.io.IOConstants;\n+import org.apache.hadoop.hive.serde2.ColumnProjectionUtils;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.connector.file.src.util.CheckpointedPosition.NO_OFFSET;\n+import static org.apache.flink.table.data.vector.VectorizedColumnBatch.DEFAULT_SIZE;\n+\n+/**\n+ * A BulkFormat implementation for HiveSource. This implementation delegates reading to other BulkFormat instances,\n+ * because different hive partitions may need different BulkFormat to do the reading.\n+ */\n+public class HiveBulkFormatAdapter implements BulkFormat<RowData, HiveSourceSplit> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(HiveBulkFormatAdapter.class);\n+\n+\t// schema evolution configs are not available in older versions of IOConstants, let's define them ourselves\n+\tprivate static final String SCHEMA_EVOLUTION_COLUMNS = \"schema.evolution.columns\";\n+\tprivate static final String SCHEMA_EVOLUTION_COLUMNS_TYPES = \"schema.evolution.columns.types\";\n+\n+\tprivate final JobConfWrapper jobConfWrapper;\n+\tprivate final List<String> partitionKeys;\n+\tprivate final String[] fieldNames;\n+\tprivate final DataType[] fieldTypes;\n+\tprivate final String hiveVersion;\n+\tprivate final HiveShim hiveShim;\n+\tprivate final RowType producedDataType;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "54abdd1fb2ee2b4c478366d013c02631a559ac58"}, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzc0MTQzMg==", "bodyText": "producedRowType", "url": "https://github.com/apache/flink/pull/13915#discussion_r517741432", "createdAt": "2020-11-05T02:13:03Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveSource.java", "diffHunk": "@@ -0,0 +1,89 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+import org.apache.flink.api.java.typeutils.ResultTypeQueryable;\n+import org.apache.flink.connector.file.src.AbstractFileSource;\n+import org.apache.flink.connector.file.src.reader.BulkFormat;\n+import org.apache.flink.connectors.hive.read.HiveBulkFormatAdapter;\n+import org.apache.flink.connectors.hive.read.HiveSourceSplit;\n+import org.apache.flink.core.io.SimpleVersionedSerializer;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.mapred.JobConf;\n+\n+import javax.annotation.Nullable;\n+\n+import java.util.List;\n+\n+import static org.apache.flink.connector.file.src.FileSource.DEFAULT_SPLIT_ASSIGNER;\n+import static org.apache.flink.util.Preconditions.checkNotNull;\n+\n+/**\n+ * A unified data source that reads a hive table.\n+ */\n+public class HiveSource extends AbstractFileSource<RowData, HiveSourceSplit> implements ResultTypeQueryable<RowData> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tHiveSource(\n+\t\t\tJobConf jobConf,\n+\t\t\tCatalogTable catalogTable,\n+\t\t\tList<HiveTablePartition> partitions,\n+\t\t\t@Nullable Long limit,\n+\t\t\tString hiveVersion,\n+\t\t\tboolean useMapRedReader,\n+\t\t\tboolean isStreamingSource,\n+\t\t\tRowType producedDataType) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "54abdd1fb2ee2b4c478366d013c02631a559ac58"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzc0Mjk3Mw==", "bodyText": "Please remove limit things", "url": "https://github.com/apache/flink/pull/13915#discussion_r517742973", "createdAt": "2020-11-05T02:15:07Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveBulkFormatAdapter.java", "diffHunk": "@@ -0,0 +1,304 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive.read;\n+\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.connector.file.src.reader.BulkFormat;\n+import org.apache.flink.connector.file.src.util.ArrayResultIterator;\n+import org.apache.flink.connectors.hive.HiveTablePartition;\n+import org.apache.flink.connectors.hive.JobConfWrapper;\n+import org.apache.flink.formats.parquet.ParquetColumnarRowInputFormat;\n+import org.apache.flink.table.catalog.hive.client.HiveShim;\n+import org.apache.flink.table.catalog.hive.client.HiveShimLoader;\n+import org.apache.flink.table.catalog.hive.util.HiveTypeUtil;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.filesystem.LimitableBulkFormat;\n+import org.apache.flink.table.filesystem.PartitionFieldExtractor;\n+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n+import org.apache.flink.table.runtime.typeutils.RowDataSerializer;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.hive.ql.io.IOConstants;\n+import org.apache.hadoop.hive.serde2.ColumnProjectionUtils;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.connector.file.src.util.CheckpointedPosition.NO_OFFSET;\n+import static org.apache.flink.table.data.vector.VectorizedColumnBatch.DEFAULT_SIZE;\n+\n+/**\n+ * A BulkFormat implementation for HiveSource. This implementation delegates reading to other BulkFormat instances,\n+ * because different hive partitions may need different BulkFormat to do the reading.\n+ */\n+public class HiveBulkFormatAdapter implements BulkFormat<RowData, HiveSourceSplit> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(HiveBulkFormatAdapter.class);\n+\n+\t// schema evolution configs are not available in older versions of IOConstants, let's define them ourselves\n+\tprivate static final String SCHEMA_EVOLUTION_COLUMNS = \"schema.evolution.columns\";\n+\tprivate static final String SCHEMA_EVOLUTION_COLUMNS_TYPES = \"schema.evolution.columns.types\";\n+\n+\tprivate final JobConfWrapper jobConfWrapper;\n+\tprivate final List<String> partitionKeys;\n+\tprivate final String[] fieldNames;\n+\tprivate final DataType[] fieldTypes;\n+\tprivate final String hiveVersion;\n+\tprivate final HiveShim hiveShim;\n+\tprivate final RowType producedDataType;\n+\tprivate final boolean useMapRedReader;\n+\t// We should limit the input read count of the splits, null represents no limit.\n+\tprivate final Long limit;\n+\n+\tpublic HiveBulkFormatAdapter(JobConfWrapper jobConfWrapper, List<String> partitionKeys, String[] fieldNames, DataType[] fieldTypes,\n+\t\t\tString hiveVersion, RowType producedDataType, boolean useMapRedReader, Long limit) {\n+\t\tthis.jobConfWrapper = jobConfWrapper;\n+\t\tthis.partitionKeys = partitionKeys;\n+\t\tthis.fieldNames = fieldNames;\n+\t\tthis.fieldTypes = fieldTypes;\n+\t\tthis.hiveVersion = hiveVersion;\n+\t\tthis.hiveShim = HiveShimLoader.loadHiveShim(hiveVersion);\n+\t\tthis.producedDataType = producedDataType;\n+\t\tthis.useMapRedReader = useMapRedReader;\n+\t\tthis.limit = limit;\n+\t}\n+\n+\t@Override\n+\tpublic Reader<RowData> createReader(Configuration config, HiveSourceSplit split)\n+\t\t\tthrows IOException {\n+\t\treturn createBulkFormatForSplit(split).createReader(config, split);\n+\t}\n+\n+\t@Override\n+\tpublic Reader<RowData> restoreReader(Configuration config, HiveSourceSplit split) throws IOException {\n+\t\treturn createBulkFormatForSplit(split).restoreReader(config, split);\n+\t}\n+\n+\t@Override\n+\tpublic boolean isSplittable() {\n+\t\treturn true;\n+\t}\n+\n+\t@Override\n+\tpublic TypeInformation<RowData> getProducedType() {\n+\t\treturn InternalTypeInfo.of(producedDataType);\n+\t}\n+\n+\tprivate BulkFormat<RowData, ? super HiveSourceSplit> createBulkFormatForSplit(HiveSourceSplit split) {\n+\t\tif (!useMapRedReader && useParquetVectorizedRead(split.getHiveTablePartition())) {\n+\t\t\tPartitionFieldExtractor<HiveSourceSplit> extractor = (PartitionFieldExtractor<HiveSourceSplit>)\n+\t\t\t\t\t(split1, fieldName, fieldType) -> split1.getHiveTablePartition().getPartitionSpec().get(fieldName);\n+\t\t\treturn LimitableBulkFormat.create(\n+\t\t\t\t\tParquetColumnarRowInputFormat.createPartitionedFormat(\n+\t\t\t\t\t\t\tjobConfWrapper.conf(),\n+\t\t\t\t\t\t\tproducedDataType,\n+\t\t\t\t\t\t\tpartitionKeys,\n+\t\t\t\t\t\t\textractor,\n+\t\t\t\t\t\t\tDEFAULT_SIZE,\n+\t\t\t\t\t\t\thiveVersion.startsWith(\"3\"),\n+\t\t\t\t\t\t\tfalse),\n+\t\t\t\t\tlimit);\n+\t\t} else {\n+\t\t\treturn new HiveMapRedBulkFormat();\n+\t\t}\n+\t}\n+\n+\tprivate boolean useParquetVectorizedRead(HiveTablePartition partition) {\n+\t\tboolean isParquet = partition.getStorageDescriptor().getSerdeInfo().getSerializationLib()\n+\t\t\t\t.toLowerCase().contains(\"parquet\");\n+\t\tif (!isParquet) {\n+\t\t\treturn false;\n+\t\t}\n+\n+\t\tfor (RowType.RowField field : producedDataType.getFields()) {\n+\t\t\tif (isVectorizationUnsupported(field.getType())) {\n+\t\t\t\tLOG.info(\"Fallback to hadoop mapred reader, unsupported field type: \" + field.getType());\n+\t\t\t\treturn false;\n+\t\t\t}\n+\t\t}\n+\n+\t\tLOG.info(\"Use flink parquet ColumnarRowData reader.\");\n+\t\treturn true;\n+\t}\n+\n+\tprivate static boolean isVectorizationUnsupported(LogicalType t) {\n+\t\tswitch (t.getTypeRoot()) {\n+\t\t\tcase CHAR:\n+\t\t\tcase VARCHAR:\n+\t\t\tcase BOOLEAN:\n+\t\t\tcase BINARY:\n+\t\t\tcase VARBINARY:\n+\t\t\tcase DECIMAL:\n+\t\t\tcase TINYINT:\n+\t\t\tcase SMALLINT:\n+\t\t\tcase INTEGER:\n+\t\t\tcase BIGINT:\n+\t\t\tcase FLOAT:\n+\t\t\tcase DOUBLE:\n+\t\t\tcase DATE:\n+\t\t\tcase TIME_WITHOUT_TIME_ZONE:\n+\t\t\tcase TIMESTAMP_WITHOUT_TIME_ZONE:\n+\t\t\tcase TIMESTAMP_WITH_LOCAL_TIME_ZONE:\n+\t\t\t\treturn false;\n+\t\t\tcase TIMESTAMP_WITH_TIME_ZONE:\n+\t\t\tcase INTERVAL_YEAR_MONTH:\n+\t\t\tcase INTERVAL_DAY_TIME:\n+\t\t\tcase ARRAY:\n+\t\t\tcase MULTISET:\n+\t\t\tcase MAP:\n+\t\t\tcase ROW:\n+\t\t\tcase DISTINCT_TYPE:\n+\t\t\tcase STRUCTURED_TYPE:\n+\t\t\tcase NULL:\n+\t\t\tcase RAW:\n+\t\t\tcase SYMBOL:\n+\t\t\tdefault:\n+\t\t\t\treturn true;\n+\t\t}\n+\t}\n+\n+\tprivate class HiveMapRedBulkFormat implements BulkFormat<RowData, HiveSourceSplit> {\n+\n+\t\tprivate static final long serialVersionUID = 1L;\n+\n+\t\t@Override\n+\t\tpublic Reader<RowData> createReader(Configuration config, HiveSourceSplit split)\n+\t\t\t\tthrows IOException {\n+\t\t\treturn new HiveReader(split);\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic Reader<RowData> restoreReader(Configuration config, HiveSourceSplit split) throws IOException {\n+\t\t\tassert split.getReaderPosition().isPresent();\n+\t\t\tHiveReader hiveReader = new HiveReader(split);\n+\t\t\thiveReader.seek(split.getReaderPosition().get().getRecordsAfterOffset());\n+\t\t\treturn hiveReader;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic boolean isSplittable() {\n+\t\t\treturn true;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic TypeInformation<RowData> getProducedType() {\n+\t\t\treturn InternalTypeInfo.of(producedDataType);\n+\t\t}\n+\t}\n+\n+\tprivate class HiveReader implements BulkFormat.Reader<RowData> {\n+\n+\t\tprivate final HiveMapredSplitReader hiveMapredSplitReader;\n+\t\tprivate final RowDataSerializer serializer;\n+\t\tprivate final ArrayResultIterator<RowData> iterator = new ArrayResultIterator<>();\n+\t\tprivate final int[] selectedFields;\n+\t\tprivate long numRead = 0;\n+\n+\t\tprivate HiveReader(HiveSourceSplit split) throws IOException {\n+\t\t\tselectedFields = computeSelectedFields();\n+\t\t\tJobConf clonedConf = new JobConf(jobConfWrapper.conf());\n+\t\t\taddSchemaToConf(clonedConf);\n+\t\t\tHiveTableInputSplit oldSplit = new HiveTableInputSplit(-1, split.toMapRedSplit(), clonedConf, split.getHiveTablePartition());\n+\t\t\thiveMapredSplitReader = new HiveMapredSplitReader(clonedConf, partitionKeys, fieldTypes, selectedFields, oldSplit, hiveShim);\n+\t\t\tserializer = new RowDataSerializer(producedDataType);\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic RecordIterator<RowData> readBatch() throws IOException {\n+\t\t\tRowData[] records = new RowData[DEFAULT_SIZE];\n+\t\t\tfinal long skipCount = numRead;\n+\t\t\tint num = 0;\n+\t\t\twhile (!hiveMapredSplitReader.reachedEnd() && num < DEFAULT_SIZE && !reachLimit()) {\n+\t\t\t\trecords[num++] = serializer.copy(nextRecord());\n+\t\t\t}\n+\t\t\tif (num == 0) {\n+\t\t\t\treturn null;\n+\t\t\t}\n+\t\t\titerator.set(records, num, NO_OFFSET, skipCount);\n+\t\t\treturn iterator;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void close() throws IOException {\n+\t\t\thiveMapredSplitReader.close();\n+\t\t}\n+\n+\t\tprivate RowData nextRecord() throws IOException {\n+\t\t\tRowData res = hiveMapredSplitReader.nextRecord(null);\n+\t\t\tnumRead++;\n+\t\t\treturn res;\n+\t\t}\n+\n+\t\tprivate boolean reachLimit() {\n+\t\t\treturn limit != null && numRead >= limit;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "54abdd1fb2ee2b4c478366d013c02631a559ac58"}, "originalPosition": 260}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a0ef5815192defc2fc93df716e73d8a8410367fd", "author": {"user": {"login": "lirui-apache", "name": "Rui Li"}}, "url": "https://github.com/apache/flink/commit/a0ef5815192defc2fc93df716e73d8a8410367fd", "committedDate": "2020-11-05T02:45:29Z", "message": "address comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIzODk1NTMz", "url": "https://github.com/apache/flink/pull/13915#pullrequestreview-523895533", "createdAt": "2020-11-05T03:10:36Z", "commit": {"oid": "a0ef5815192defc2fc93df716e73d8a8410367fd"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQwMzoxMDozNlrOHtyAOg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQwMzoxMDozNlrOHtyAOg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzc2NzIyNg==", "bodyText": "We should use SourceProvider instead of DataStream", "url": "https://github.com/apache/flink/pull/13915#discussion_r517767226", "createdAt": "2020-11-05T03:10:36Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSource.java", "diffHunk": "@@ -180,15 +182,26 @@ private boolean isStreamingSource() {\n \t}\n \n \tprivate DataStream<RowData> createBatchSource(StreamExecutionEnvironment execEnv,\n-\t\t\tTypeInformation<RowData> typeInfo, HiveTableInputFormat inputFormat) {\n-\t\tDataStreamSource<RowData> source = execEnv.createInput(inputFormat, typeInfo);\n+\t\t\tList<HiveTablePartition> allHivePartitions) {\n+\t\tHiveSource hiveSource = new HiveSource(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a0ef5815192defc2fc93df716e73d8a8410367fd"}, "originalPosition": 41}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI0MDAxMzM5", "url": "https://github.com/apache/flink/pull/13915#pullrequestreview-524001339", "createdAt": "2020-11-05T07:59:42Z", "commit": {"oid": "a0ef5815192defc2fc93df716e73d8a8410367fd"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4756, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}