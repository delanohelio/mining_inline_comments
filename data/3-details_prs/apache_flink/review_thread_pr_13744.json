{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTA4MTM4MDk5", "number": 13744, "reviewThreads": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQwODowMTowM1rOExyR0g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QwODo1NjowNVrOEyPkWw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNjM5NDQyOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/compact/CompactMessages.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQwODowMTowM1rOHoEKwQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQwODowMTowM1rOHoEKwQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTc3MzM3Nw==", "bodyText": "Missing serialVersionUID", "url": "https://github.com/apache/flink/pull/13744#discussion_r511773377", "createdAt": "2020-10-26T08:01:03Z", "author": {"login": "lirui-apache"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/compact/CompactMessages.java", "diffHunk": "@@ -0,0 +1,155 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem.stream.compact;\n+\n+import org.apache.flink.core.fs.Path;\n+\n+import java.io.Serializable;\n+import java.net.URI;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Util class for all compaction messages.\n+ *\n+ * <p>The compaction operator graph is:\n+ * TempFileWriter|parallel ---(InputFile&EndInputFile)---> CompactCoordinator|non-parallel\n+ * ---(CompactionUnit&EndCompaction)--->CompactOperator|parallel---(PartitionCommitInfo)--->\n+ * PartitionCommitter|non-parallel\n+ *\n+ * <p>Because the end message is a kind of barrier of record messages, they can only be transmitted\n+ * in the way of full broadcast in the link from coordinator to compact operator.\n+ */\n+public class CompactMessages {\n+\tprivate CompactMessages() {}\n+\n+\t/**\n+\t * The input of compact coordinator.\n+\t */\n+\tpublic interface CoordinatorInput extends Serializable {}\n+\n+\t/**\n+\t * A partitioned input file.\n+\t */\n+\tpublic static class InputFile implements CoordinatorInput {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "074ab07749ff798563e785d2c2fafeee1302ac74"}, "originalPosition": 51}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNjM5NTc2OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/compact/CompactMessages.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQwODowMToyOVrOHoELhg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQwODowMToyOVrOHoELhg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTc3MzU3NA==", "bodyText": "Missing serialVersionUID", "url": "https://github.com/apache/flink/pull/13744#discussion_r511773574", "createdAt": "2020-10-26T08:01:29Z", "author": {"login": "lirui-apache"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/compact/CompactMessages.java", "diffHunk": "@@ -0,0 +1,155 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem.stream.compact;\n+\n+import org.apache.flink.core.fs.Path;\n+\n+import java.io.Serializable;\n+import java.net.URI;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Util class for all compaction messages.\n+ *\n+ * <p>The compaction operator graph is:\n+ * TempFileWriter|parallel ---(InputFile&EndInputFile)---> CompactCoordinator|non-parallel\n+ * ---(CompactionUnit&EndCompaction)--->CompactOperator|parallel---(PartitionCommitInfo)--->\n+ * PartitionCommitter|non-parallel\n+ *\n+ * <p>Because the end message is a kind of barrier of record messages, they can only be transmitted\n+ * in the way of full broadcast in the link from coordinator to compact operator.\n+ */\n+public class CompactMessages {\n+\tprivate CompactMessages() {}\n+\n+\t/**\n+\t * The input of compact coordinator.\n+\t */\n+\tpublic interface CoordinatorInput extends Serializable {}\n+\n+\t/**\n+\t * A partitioned input file.\n+\t */\n+\tpublic static class InputFile implements CoordinatorInput {\n+\t\tprivate final String partition;\n+\t\tprivate final Path file;\n+\n+\t\tpublic InputFile(String partition, Path file) {\n+\t\t\tthis.partition = partition;\n+\t\t\tthis.file = file;\n+\t\t}\n+\n+\t\tpublic String getPartition() {\n+\t\t\treturn partition;\n+\t\t}\n+\n+\t\tpublic Path getFile() {\n+\t\t\treturn file;\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * A flag to end file input.\n+\t */\n+\tpublic static class EndInputFile implements CoordinatorInput {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "074ab07749ff798563e785d2c2fafeee1302ac74"}, "originalPosition": 72}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNjQyMjM1OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/compact/CompactMessages.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQwODoxMDozMFrOHoEbdQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QwMzoxODowNFrOHop9hg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTc3NzY1Mw==", "bodyText": "What's the purpose of this method?", "url": "https://github.com/apache/flink/pull/13744#discussion_r511777653", "createdAt": "2020-10-26T08:10:30Z", "author": {"login": "lirui-apache"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/compact/CompactMessages.java", "diffHunk": "@@ -0,0 +1,155 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem.stream.compact;\n+\n+import org.apache.flink.core.fs.Path;\n+\n+import java.io.Serializable;\n+import java.net.URI;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Util class for all compaction messages.\n+ *\n+ * <p>The compaction operator graph is:\n+ * TempFileWriter|parallel ---(InputFile&EndInputFile)---> CompactCoordinator|non-parallel\n+ * ---(CompactionUnit&EndCompaction)--->CompactOperator|parallel---(PartitionCommitInfo)--->\n+ * PartitionCommitter|non-parallel\n+ *\n+ * <p>Because the end message is a kind of barrier of record messages, they can only be transmitted\n+ * in the way of full broadcast in the link from coordinator to compact operator.\n+ */\n+public class CompactMessages {\n+\tprivate CompactMessages() {}\n+\n+\t/**\n+\t * The input of compact coordinator.\n+\t */\n+\tpublic interface CoordinatorInput extends Serializable {}\n+\n+\t/**\n+\t * A partitioned input file.\n+\t */\n+\tpublic static class InputFile implements CoordinatorInput {\n+\t\tprivate final String partition;\n+\t\tprivate final Path file;\n+\n+\t\tpublic InputFile(String partition, Path file) {\n+\t\t\tthis.partition = partition;\n+\t\t\tthis.file = file;\n+\t\t}\n+\n+\t\tpublic String getPartition() {\n+\t\t\treturn partition;\n+\t\t}\n+\n+\t\tpublic Path getFile() {\n+\t\t\treturn file;\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * A flag to end file input.\n+\t */\n+\tpublic static class EndInputFile implements CoordinatorInput {\n+\t\tprivate final long checkpointId;\n+\t\tprivate final int taskId;\n+\t\tprivate final int numberOfTasks;\n+\n+\t\tpublic EndInputFile(long checkpointId, int taskId, int numberOfTasks) {\n+\t\t\tthis.checkpointId = checkpointId;\n+\t\t\tthis.taskId = taskId;\n+\t\t\tthis.numberOfTasks = numberOfTasks;\n+\t\t}\n+\n+\t\tpublic long getCheckpointId() {\n+\t\t\treturn checkpointId;\n+\t\t}\n+\n+\t\tpublic int getTaskId() {\n+\t\t\treturn taskId;\n+\t\t}\n+\n+\t\tpublic int getNumberOfTasks() {\n+\t\t\treturn numberOfTasks;\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * The output of compact coordinator.\n+\t */\n+\tpublic interface CoordinatorOutput extends Serializable {}\n+\n+\t/**\n+\t * The unit of a single compaction.\n+\t */\n+\tpublic static class CompactionUnit implements CoordinatorOutput {\n+\n+\t\tprivate final int unitId;\n+\t\tprivate final String partition;\n+\n+\t\t// Store strings to improve serialization performance.\n+\t\tprivate final String[] pathStrings;\n+\n+\t\tpublic CompactionUnit(int unitId, String partition, List<Path> unit) {\n+\t\t\tthis.unitId = unitId;\n+\t\t\tthis.partition = partition;\n+\t\t\tthis.pathStrings = unit.stream()\n+\t\t\t\t\t.map(Path::toUri)\n+\t\t\t\t\t.map(URI::toString)\n+\t\t\t\t\t.toArray(String[]::new);\n+\t\t}\n+\n+\t\tpublic boolean isTaskMessage(int taskId) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "074ab07749ff798563e785d2c2fafeee1302ac74"}, "originalPosition": 121}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjM4NDgzMg==", "bodyText": "* {@link CompactionUnit} and {@link EndCompaction} must be sent to the downstream in an orderly\n * manner, while {@link EndCompaction} is broadcast emitting, so unit and endCompaction use the\n * broadcast emitting mechanism together. Since unit is broadcast, we want it to be processed by\n * a single task, so we carry the ID in the unit and let the downstream task select its own unit.", "url": "https://github.com/apache/flink/pull/13744#discussion_r512384832", "createdAt": "2020-10-27T02:48:55Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/compact/CompactMessages.java", "diffHunk": "@@ -0,0 +1,155 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem.stream.compact;\n+\n+import org.apache.flink.core.fs.Path;\n+\n+import java.io.Serializable;\n+import java.net.URI;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Util class for all compaction messages.\n+ *\n+ * <p>The compaction operator graph is:\n+ * TempFileWriter|parallel ---(InputFile&EndInputFile)---> CompactCoordinator|non-parallel\n+ * ---(CompactionUnit&EndCompaction)--->CompactOperator|parallel---(PartitionCommitInfo)--->\n+ * PartitionCommitter|non-parallel\n+ *\n+ * <p>Because the end message is a kind of barrier of record messages, they can only be transmitted\n+ * in the way of full broadcast in the link from coordinator to compact operator.\n+ */\n+public class CompactMessages {\n+\tprivate CompactMessages() {}\n+\n+\t/**\n+\t * The input of compact coordinator.\n+\t */\n+\tpublic interface CoordinatorInput extends Serializable {}\n+\n+\t/**\n+\t * A partitioned input file.\n+\t */\n+\tpublic static class InputFile implements CoordinatorInput {\n+\t\tprivate final String partition;\n+\t\tprivate final Path file;\n+\n+\t\tpublic InputFile(String partition, Path file) {\n+\t\t\tthis.partition = partition;\n+\t\t\tthis.file = file;\n+\t\t}\n+\n+\t\tpublic String getPartition() {\n+\t\t\treturn partition;\n+\t\t}\n+\n+\t\tpublic Path getFile() {\n+\t\t\treturn file;\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * A flag to end file input.\n+\t */\n+\tpublic static class EndInputFile implements CoordinatorInput {\n+\t\tprivate final long checkpointId;\n+\t\tprivate final int taskId;\n+\t\tprivate final int numberOfTasks;\n+\n+\t\tpublic EndInputFile(long checkpointId, int taskId, int numberOfTasks) {\n+\t\t\tthis.checkpointId = checkpointId;\n+\t\t\tthis.taskId = taskId;\n+\t\t\tthis.numberOfTasks = numberOfTasks;\n+\t\t}\n+\n+\t\tpublic long getCheckpointId() {\n+\t\t\treturn checkpointId;\n+\t\t}\n+\n+\t\tpublic int getTaskId() {\n+\t\t\treturn taskId;\n+\t\t}\n+\n+\t\tpublic int getNumberOfTasks() {\n+\t\t\treturn numberOfTasks;\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * The output of compact coordinator.\n+\t */\n+\tpublic interface CoordinatorOutput extends Serializable {}\n+\n+\t/**\n+\t * The unit of a single compaction.\n+\t */\n+\tpublic static class CompactionUnit implements CoordinatorOutput {\n+\n+\t\tprivate final int unitId;\n+\t\tprivate final String partition;\n+\n+\t\t// Store strings to improve serialization performance.\n+\t\tprivate final String[] pathStrings;\n+\n+\t\tpublic CompactionUnit(int unitId, String partition, List<Path> unit) {\n+\t\t\tthis.unitId = unitId;\n+\t\t\tthis.partition = partition;\n+\t\t\tthis.pathStrings = unit.stream()\n+\t\t\t\t\t.map(Path::toUri)\n+\t\t\t\t\t.map(URI::toString)\n+\t\t\t\t\t.toArray(String[]::new);\n+\t\t}\n+\n+\t\tpublic boolean isTaskMessage(int taskId) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTc3NzY1Mw=="}, "originalCommit": {"oid": "074ab07749ff798563e785d2c2fafeee1302ac74"}, "originalPosition": 121}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjM5MjU4Mg==", "bodyText": "Good catch, there is a bug here, should be:\npublic boolean isTaskMessage(int taskNumber, int taskId) {\n    return unitId % taskNumber == taskId;\n}", "url": "https://github.com/apache/flink/pull/13744#discussion_r512392582", "createdAt": "2020-10-27T03:18:04Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/compact/CompactMessages.java", "diffHunk": "@@ -0,0 +1,155 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem.stream.compact;\n+\n+import org.apache.flink.core.fs.Path;\n+\n+import java.io.Serializable;\n+import java.net.URI;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Util class for all compaction messages.\n+ *\n+ * <p>The compaction operator graph is:\n+ * TempFileWriter|parallel ---(InputFile&EndInputFile)---> CompactCoordinator|non-parallel\n+ * ---(CompactionUnit&EndCompaction)--->CompactOperator|parallel---(PartitionCommitInfo)--->\n+ * PartitionCommitter|non-parallel\n+ *\n+ * <p>Because the end message is a kind of barrier of record messages, they can only be transmitted\n+ * in the way of full broadcast in the link from coordinator to compact operator.\n+ */\n+public class CompactMessages {\n+\tprivate CompactMessages() {}\n+\n+\t/**\n+\t * The input of compact coordinator.\n+\t */\n+\tpublic interface CoordinatorInput extends Serializable {}\n+\n+\t/**\n+\t * A partitioned input file.\n+\t */\n+\tpublic static class InputFile implements CoordinatorInput {\n+\t\tprivate final String partition;\n+\t\tprivate final Path file;\n+\n+\t\tpublic InputFile(String partition, Path file) {\n+\t\t\tthis.partition = partition;\n+\t\t\tthis.file = file;\n+\t\t}\n+\n+\t\tpublic String getPartition() {\n+\t\t\treturn partition;\n+\t\t}\n+\n+\t\tpublic Path getFile() {\n+\t\t\treturn file;\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * A flag to end file input.\n+\t */\n+\tpublic static class EndInputFile implements CoordinatorInput {\n+\t\tprivate final long checkpointId;\n+\t\tprivate final int taskId;\n+\t\tprivate final int numberOfTasks;\n+\n+\t\tpublic EndInputFile(long checkpointId, int taskId, int numberOfTasks) {\n+\t\t\tthis.checkpointId = checkpointId;\n+\t\t\tthis.taskId = taskId;\n+\t\t\tthis.numberOfTasks = numberOfTasks;\n+\t\t}\n+\n+\t\tpublic long getCheckpointId() {\n+\t\t\treturn checkpointId;\n+\t\t}\n+\n+\t\tpublic int getTaskId() {\n+\t\t\treturn taskId;\n+\t\t}\n+\n+\t\tpublic int getNumberOfTasks() {\n+\t\t\treturn numberOfTasks;\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * The output of compact coordinator.\n+\t */\n+\tpublic interface CoordinatorOutput extends Serializable {}\n+\n+\t/**\n+\t * The unit of a single compaction.\n+\t */\n+\tpublic static class CompactionUnit implements CoordinatorOutput {\n+\n+\t\tprivate final int unitId;\n+\t\tprivate final String partition;\n+\n+\t\t// Store strings to improve serialization performance.\n+\t\tprivate final String[] pathStrings;\n+\n+\t\tpublic CompactionUnit(int unitId, String partition, List<Path> unit) {\n+\t\t\tthis.unitId = unitId;\n+\t\t\tthis.partition = partition;\n+\t\t\tthis.pathStrings = unit.stream()\n+\t\t\t\t\t.map(Path::toUri)\n+\t\t\t\t\t.map(URI::toString)\n+\t\t\t\t\t.toArray(String[]::new);\n+\t\t}\n+\n+\t\tpublic boolean isTaskMessage(int taskId) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTc3NzY1Mw=="}, "originalCommit": {"oid": "074ab07749ff798563e785d2c2fafeee1302ac74"}, "originalPosition": 121}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNjc3NDQ3OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/compact/CompactCoordinator.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQwOTo0NDozMlrOHoHtlA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQwOTo0NDozMlrOHoHtlA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTgzMTQ0NA==", "bodyText": "Throw an exception for unknown elements?", "url": "https://github.com/apache/flink/pull/13744#discussion_r511831444", "createdAt": "2020-10-26T09:44:32Z", "author": {"login": "lirui-apache"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/compact/CompactCoordinator.java", "diffHunk": "@@ -0,0 +1,192 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem.stream.compact;\n+\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeutils.base.ListSerializer;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.api.common.typeutils.base.MapSerializer;\n+import org.apache.flink.api.common.typeutils.base.StringSerializer;\n+import org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer;\n+import org.apache.flink.core.fs.FileSystem;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.filesystem.stream.TaskTracker;\n+import org.apache.flink.table.filesystem.stream.compact.CompactMessages.CompactionUnit;\n+import org.apache.flink.table.filesystem.stream.compact.CompactMessages.CoordinatorInput;\n+import org.apache.flink.table.filesystem.stream.compact.CompactMessages.CoordinatorOutput;\n+import org.apache.flink.table.filesystem.stream.compact.CompactMessages.EndCompaction;\n+import org.apache.flink.table.filesystem.stream.compact.CompactMessages.EndInputFile;\n+import org.apache.flink.table.filesystem.stream.compact.CompactMessages.InputFile;\n+import org.apache.flink.table.runtime.util.BinPacking;\n+import org.apache.flink.util.function.SupplierWithException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.ArrayList;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.TreeMap;\n+import java.util.function.Function;\n+\n+/**\n+ * This is the single (non-parallel) monitoring task which coordinate input files to compaction units.\n+ * - Receives in-flight input files inside checkpoint.\n+ * - Receives all upstream end input messages after the checkpoint completes successfully,\n+ *   starts coordination.\n+ *\n+ * <p>NOTE: The coordination is a stable algorithm, which can ensure that the downstream can\n+ *          perform compaction at any time without worrying about fail over.\n+ *\n+ * <p>STATE: This operator stores input files in state, after the checkpoint completes successfully,\n+ *           input files are taken out from the state for coordination.\n+ */\n+public class CompactCoordinator extends AbstractStreamOperator<CoordinatorOutput> implements\n+\t\tOneInputStreamOperator<CoordinatorInput, CoordinatorOutput> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(CompactCoordinator.class);\n+\n+\tprivate final SupplierWithException<FileSystem, IOException> fsFactory;\n+\tprivate final long targetFileSize;\n+\n+\tprivate transient FileSystem fileSystem;\n+\n+\tprivate transient ListState<Map<Long, Map<String, List<Path>>>> inputFilesState;\n+\tprivate transient TreeMap<Long, Map<String, List<Path>>> inputFiles;\n+\tprivate transient Map<String, List<Path>> currentInputFiles;\n+\n+\tprivate transient TaskTracker inputTaskTracker;\n+\n+\tpublic CompactCoordinator(\n+\t\t\tSupplierWithException<FileSystem, IOException> fsFactory,\n+\t\t\tlong targetFileSize) {\n+\t\tthis.fsFactory = fsFactory;\n+\t\tthis.targetFileSize = targetFileSize;\n+\t}\n+\n+\t@Override\n+\tpublic void initializeState(StateInitializationContext context) throws Exception {\n+\t\tsuper.initializeState(context);\n+\n+\t\tfileSystem = fsFactory.get();\n+\n+\t\tListStateDescriptor<Map<Long, Map<String, List<Path>>>> filesDescriptor =\n+\t\t\t\tnew ListStateDescriptor<>(\"files-state\", new MapSerializer<>(\n+\t\t\t\t\t\tLongSerializer.INSTANCE,\n+\t\t\t\t\t\tnew MapSerializer<>(\n+\t\t\t\t\t\t\t\tStringSerializer.INSTANCE,\n+\t\t\t\t\t\t\t\tnew ListSerializer<>(\n+\t\t\t\t\t\t\t\t\t\tnew KryoSerializer<>(Path.class, getExecutionConfig())))));\n+\t\tinputFilesState = context.getOperatorStateStore().getListState(filesDescriptor);\n+\t\tinputFiles = new TreeMap<>();\n+\t\tcurrentInputFiles = new HashMap<>();\n+\t\tif (context.isRestored()) {\n+\t\t\tinputFiles.putAll(inputFilesState.get().iterator().next());\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void processElement(StreamRecord<CoordinatorInput> element) throws Exception {\n+\t\tCoordinatorInput value = element.getValue();\n+\t\tif (value instanceof InputFile) {\n+\t\t\tInputFile file = (InputFile) value;\n+\t\t\tcurrentInputFiles.computeIfAbsent(file.getPartition(), k -> new ArrayList<>()).add(file.getFile());\n+\t\t} else if (value instanceof EndInputFile) {\n+\t\t\tEndInputFile endInputFile = (EndInputFile) value;\n+\t\t\tif (inputTaskTracker == null) {\n+\t\t\t\tinputTaskTracker = new TaskTracker(endInputFile.getNumberOfTasks());\n+\t\t\t}\n+\n+\t\t\t// ensure all files are ready to be compacted.\n+\t\t\tboolean triggerCommit = inputTaskTracker.add(\n+\t\t\t\t\tendInputFile.getCheckpointId(), endInputFile.getTaskId());\n+\t\t\tif (triggerCommit) {\n+\t\t\t\tcommitUpToCheckpoint(endInputFile.getCheckpointId());\n+\t\t\t}\n+\t\t}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "074ab07749ff798563e785d2c2fafeee1302ac74"}, "originalPosition": 134}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNjk4Mzg2OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/compact/CompactMessages.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxMDozNzo1OFrOHoJrpA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxMDozNzo1OFrOHoJrpA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTg2MzcxNg==", "bodyText": "It seems more of a flag to end checkpoint rather than file input?", "url": "https://github.com/apache/flink/pull/13744#discussion_r511863716", "createdAt": "2020-10-26T10:37:58Z", "author": {"login": "lirui-apache"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/compact/CompactMessages.java", "diffHunk": "@@ -0,0 +1,155 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem.stream.compact;\n+\n+import org.apache.flink.core.fs.Path;\n+\n+import java.io.Serializable;\n+import java.net.URI;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Util class for all compaction messages.\n+ *\n+ * <p>The compaction operator graph is:\n+ * TempFileWriter|parallel ---(InputFile&EndInputFile)---> CompactCoordinator|non-parallel\n+ * ---(CompactionUnit&EndCompaction)--->CompactOperator|parallel---(PartitionCommitInfo)--->\n+ * PartitionCommitter|non-parallel\n+ *\n+ * <p>Because the end message is a kind of barrier of record messages, they can only be transmitted\n+ * in the way of full broadcast in the link from coordinator to compact operator.\n+ */\n+public class CompactMessages {\n+\tprivate CompactMessages() {}\n+\n+\t/**\n+\t * The input of compact coordinator.\n+\t */\n+\tpublic interface CoordinatorInput extends Serializable {}\n+\n+\t/**\n+\t * A partitioned input file.\n+\t */\n+\tpublic static class InputFile implements CoordinatorInput {\n+\t\tprivate final String partition;\n+\t\tprivate final Path file;\n+\n+\t\tpublic InputFile(String partition, Path file) {\n+\t\t\tthis.partition = partition;\n+\t\t\tthis.file = file;\n+\t\t}\n+\n+\t\tpublic String getPartition() {\n+\t\t\treturn partition;\n+\t\t}\n+\n+\t\tpublic Path getFile() {\n+\t\t\treturn file;\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * A flag to end file input.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "074ab07749ff798563e785d2c2fafeee1302ac74"}, "originalPosition": 70}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNzMzNDg1OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/compact/CompactOperator.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxMjoyMDozMFrOHoM_Og==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QwMjoyNToyN1rOHopFRw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTkxNzg4Mg==", "bodyText": "If something goes wrong in this method and the job fails over, will this method be called again for the same checkpointId?", "url": "https://github.com/apache/flink/pull/13744#discussion_r511917882", "createdAt": "2020-10-26T12:20:30Z", "author": {"login": "lirui-apache"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/compact/CompactOperator.java", "diffHunk": "@@ -0,0 +1,276 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem.stream.compact;\n+\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeutils.base.ListSerializer;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.api.common.typeutils.base.MapSerializer;\n+import org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.connector.file.src.reader.BulkFormat;\n+import org.apache.flink.core.fs.FSDataInputStream;\n+import org.apache.flink.core.fs.FileSystem;\n+import org.apache.flink.core.fs.FileSystemKind;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.core.fs.RecoverableFsDataOutputStream;\n+import org.apache.flink.core.fs.RecoverableWriter;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.functions.sink.filesystem.BucketWriter;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.BoundedOneInput;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.filesystem.stream.PartitionCommitInfo;\n+import org.apache.flink.table.filesystem.stream.compact.CompactMessages.CompactionUnit;\n+import org.apache.flink.table.filesystem.stream.compact.CompactMessages.CoordinatorOutput;\n+import org.apache.flink.table.filesystem.stream.compact.CompactMessages.EndCompaction;\n+import org.apache.flink.util.IOUtils;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.flink.util.function.SupplierWithException;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NavigableMap;\n+import java.util.Set;\n+import java.util.TreeMap;\n+\n+/**\n+ * Receives compaction units to do compaction. Send partition commit information after\n+ * compaction finished.\n+ *\n+ * <p>Use {@link BulkFormat} to read and use {@link BucketWriter} to write.\n+ *\n+ * <p>STATE: This operator stores expired files in state, after the checkpoint completes successfully,\n+ *           We can ensure that these files will not be used again and they can be deleted from the\n+ *           file system.\n+ */\n+public class CompactOperator<T> extends AbstractStreamOperator<PartitionCommitInfo>\n+\t\timplements OneInputStreamOperator<CoordinatorOutput, PartitionCommitInfo>, BoundedOneInput {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate static final String UNCOMPACTED_PREFIX = \".uncompacted-\";\n+\n+\tprivate static final String COMPACTED_PREFIX = \"compacted-\";\n+\n+\tprivate final SupplierWithException<FileSystem, IOException> fsFactory;\n+\tprivate final CompactReader.Factory<T> readerFactory;\n+\tprivate final CompactWriter.Factory<T> writerFactory;\n+\n+\tprivate transient FileSystem fileSystem;\n+\n+\tprivate transient ListState<Map<Long, List<Path>>> expiredFilesState;\n+\tprivate transient TreeMap<Long, List<Path>> expiredFiles;\n+\tprivate transient List<Path> currentExpiredFiles;\n+\n+\tprivate transient Set<String> partitions;\n+\n+\tpublic CompactOperator(\n+\t\t\tSupplierWithException<FileSystem, IOException> fsFactory,\n+\t\t\tCompactReader.Factory<T> readerFactory,\n+\t\t\tCompactWriter.Factory<T> writerFactory) {\n+\t\tthis.fsFactory = fsFactory;\n+\t\tthis.readerFactory = readerFactory;\n+\t\tthis.writerFactory = writerFactory;\n+\t}\n+\n+\t@Override\n+\tpublic void initializeState(StateInitializationContext context) throws Exception {\n+\t\tsuper.initializeState(context);\n+\t\tthis.partitions = new HashSet<>();\n+\t\tthis.fileSystem = fsFactory.get();\n+\n+\t\tListStateDescriptor<Map<Long, List<Path>>> metaDescriptor =\n+\t\t\t\tnew ListStateDescriptor<>(\"expired-files\", new MapSerializer<>(\n+\t\t\t\t\t\tLongSerializer.INSTANCE,\n+\t\t\t\t\t\tnew ListSerializer<>(new KryoSerializer<>(Path.class, getExecutionConfig()))\n+\t\t\t\t));\n+\t\tthis.expiredFilesState = context.getOperatorStateStore().getListState(metaDescriptor);\n+\t\tthis.expiredFiles = new TreeMap<>();\n+\t\tthis.currentExpiredFiles = new ArrayList<>();\n+\n+\t\tif (context.isRestored()) {\n+\t\t\tthis.expiredFiles.putAll(this.expiredFilesState.get().iterator().next());\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void processElement(StreamRecord<CoordinatorOutput> element) throws Exception {\n+\t\tCoordinatorOutput value = element.getValue();\n+\t\tif (value instanceof CompactionUnit) {\n+\t\t\tCompactionUnit unit = (CompactionUnit) value;\n+\t\t\tif (unit.isTaskMessage(getRuntimeContext().getNumberOfParallelSubtasks())) {\n+\t\t\t\tString partition = unit.getPartition();\n+\t\t\t\tList<Path> paths = unit.getPaths();\n+\n+\t\t\t\tdoCompact(paths);\n+\t\t\t\tthis.partitions.add(partition);\n+\n+\t\t\t\t// Only after the current checkpoint is successfully executed can delete\n+\t\t\t\t// the expired files, so as to ensure the existence of the files.\n+\t\t\t\tthis.currentExpiredFiles.addAll(paths);\n+\t\t\t}\n+\t\t} else if (value instanceof EndCompaction) {\n+\t\t\tendCompaction(((EndCompaction) value).getCheckpointId());\n+\t\t}\n+\t}\n+\n+\tprivate void endCompaction(long checkpoint) {\n+\t\tthis.output.collect(new StreamRecord<>(new PartitionCommitInfo(\n+\t\t\t\tcheckpoint,\n+\t\t\t\tgetRuntimeContext().getIndexOfThisSubtask(),\n+\t\t\t\tgetRuntimeContext().getNumberOfParallelSubtasks(),\n+\t\t\t\tnew ArrayList<>(this.partitions))));\n+\t\tthis.partitions.clear();\n+\t}\n+\n+\t@Override\n+\tpublic void snapshotState(StateSnapshotContext context) throws Exception {\n+\t\tsuper.snapshotState(context);\n+\n+\t\texpiredFilesState.clear();\n+\t\texpiredFiles.put(context.getCheckpointId(), new ArrayList<>(currentExpiredFiles));\n+\t\texpiredFilesState.add(expiredFiles);\n+\t\tcurrentExpiredFiles.clear();\n+\t}\n+\n+\t@Override\n+\tpublic void notifyCheckpointComplete(long checkpointId) throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "074ab07749ff798563e785d2c2fafeee1302ac74"}, "originalPosition": 161}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjM3ODE4Mw==", "bodyText": "No, then will wait for next checkpoint notify.", "url": "https://github.com/apache/flink/pull/13744#discussion_r512378183", "createdAt": "2020-10-27T02:25:27Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/compact/CompactOperator.java", "diffHunk": "@@ -0,0 +1,276 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem.stream.compact;\n+\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeutils.base.ListSerializer;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.api.common.typeutils.base.MapSerializer;\n+import org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.connector.file.src.reader.BulkFormat;\n+import org.apache.flink.core.fs.FSDataInputStream;\n+import org.apache.flink.core.fs.FileSystem;\n+import org.apache.flink.core.fs.FileSystemKind;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.core.fs.RecoverableFsDataOutputStream;\n+import org.apache.flink.core.fs.RecoverableWriter;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.functions.sink.filesystem.BucketWriter;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.BoundedOneInput;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.filesystem.stream.PartitionCommitInfo;\n+import org.apache.flink.table.filesystem.stream.compact.CompactMessages.CompactionUnit;\n+import org.apache.flink.table.filesystem.stream.compact.CompactMessages.CoordinatorOutput;\n+import org.apache.flink.table.filesystem.stream.compact.CompactMessages.EndCompaction;\n+import org.apache.flink.util.IOUtils;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.flink.util.function.SupplierWithException;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NavigableMap;\n+import java.util.Set;\n+import java.util.TreeMap;\n+\n+/**\n+ * Receives compaction units to do compaction. Send partition commit information after\n+ * compaction finished.\n+ *\n+ * <p>Use {@link BulkFormat} to read and use {@link BucketWriter} to write.\n+ *\n+ * <p>STATE: This operator stores expired files in state, after the checkpoint completes successfully,\n+ *           We can ensure that these files will not be used again and they can be deleted from the\n+ *           file system.\n+ */\n+public class CompactOperator<T> extends AbstractStreamOperator<PartitionCommitInfo>\n+\t\timplements OneInputStreamOperator<CoordinatorOutput, PartitionCommitInfo>, BoundedOneInput {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate static final String UNCOMPACTED_PREFIX = \".uncompacted-\";\n+\n+\tprivate static final String COMPACTED_PREFIX = \"compacted-\";\n+\n+\tprivate final SupplierWithException<FileSystem, IOException> fsFactory;\n+\tprivate final CompactReader.Factory<T> readerFactory;\n+\tprivate final CompactWriter.Factory<T> writerFactory;\n+\n+\tprivate transient FileSystem fileSystem;\n+\n+\tprivate transient ListState<Map<Long, List<Path>>> expiredFilesState;\n+\tprivate transient TreeMap<Long, List<Path>> expiredFiles;\n+\tprivate transient List<Path> currentExpiredFiles;\n+\n+\tprivate transient Set<String> partitions;\n+\n+\tpublic CompactOperator(\n+\t\t\tSupplierWithException<FileSystem, IOException> fsFactory,\n+\t\t\tCompactReader.Factory<T> readerFactory,\n+\t\t\tCompactWriter.Factory<T> writerFactory) {\n+\t\tthis.fsFactory = fsFactory;\n+\t\tthis.readerFactory = readerFactory;\n+\t\tthis.writerFactory = writerFactory;\n+\t}\n+\n+\t@Override\n+\tpublic void initializeState(StateInitializationContext context) throws Exception {\n+\t\tsuper.initializeState(context);\n+\t\tthis.partitions = new HashSet<>();\n+\t\tthis.fileSystem = fsFactory.get();\n+\n+\t\tListStateDescriptor<Map<Long, List<Path>>> metaDescriptor =\n+\t\t\t\tnew ListStateDescriptor<>(\"expired-files\", new MapSerializer<>(\n+\t\t\t\t\t\tLongSerializer.INSTANCE,\n+\t\t\t\t\t\tnew ListSerializer<>(new KryoSerializer<>(Path.class, getExecutionConfig()))\n+\t\t\t\t));\n+\t\tthis.expiredFilesState = context.getOperatorStateStore().getListState(metaDescriptor);\n+\t\tthis.expiredFiles = new TreeMap<>();\n+\t\tthis.currentExpiredFiles = new ArrayList<>();\n+\n+\t\tif (context.isRestored()) {\n+\t\t\tthis.expiredFiles.putAll(this.expiredFilesState.get().iterator().next());\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void processElement(StreamRecord<CoordinatorOutput> element) throws Exception {\n+\t\tCoordinatorOutput value = element.getValue();\n+\t\tif (value instanceof CompactionUnit) {\n+\t\t\tCompactionUnit unit = (CompactionUnit) value;\n+\t\t\tif (unit.isTaskMessage(getRuntimeContext().getNumberOfParallelSubtasks())) {\n+\t\t\t\tString partition = unit.getPartition();\n+\t\t\t\tList<Path> paths = unit.getPaths();\n+\n+\t\t\t\tdoCompact(paths);\n+\t\t\t\tthis.partitions.add(partition);\n+\n+\t\t\t\t// Only after the current checkpoint is successfully executed can delete\n+\t\t\t\t// the expired files, so as to ensure the existence of the files.\n+\t\t\t\tthis.currentExpiredFiles.addAll(paths);\n+\t\t\t}\n+\t\t} else if (value instanceof EndCompaction) {\n+\t\t\tendCompaction(((EndCompaction) value).getCheckpointId());\n+\t\t}\n+\t}\n+\n+\tprivate void endCompaction(long checkpoint) {\n+\t\tthis.output.collect(new StreamRecord<>(new PartitionCommitInfo(\n+\t\t\t\tcheckpoint,\n+\t\t\t\tgetRuntimeContext().getIndexOfThisSubtask(),\n+\t\t\t\tgetRuntimeContext().getNumberOfParallelSubtasks(),\n+\t\t\t\tnew ArrayList<>(this.partitions))));\n+\t\tthis.partitions.clear();\n+\t}\n+\n+\t@Override\n+\tpublic void snapshotState(StateSnapshotContext context) throws Exception {\n+\t\tsuper.snapshotState(context);\n+\n+\t\texpiredFilesState.clear();\n+\t\texpiredFiles.put(context.getCheckpointId(), new ArrayList<>(currentExpiredFiles));\n+\t\texpiredFilesState.add(expiredFiles);\n+\t\tcurrentExpiredFiles.clear();\n+\t}\n+\n+\t@Override\n+\tpublic void notifyCheckpointComplete(long checkpointId) throws Exception {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTkxNzg4Mg=="}, "originalCommit": {"oid": "074ab07749ff798563e785d2c2fafeee1302ac74"}, "originalPosition": 161}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNzQwOTgzOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/filesystem/stream/compact/CompactCoordinatorTest.java", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxMjo0MTowNFrOHoNrvA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QwNjowNTo1NlrOHosm_A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTkyOTI3Ng==", "bodyText": "I think CompactCoordinator doesn't guarantee to generate CompactionUnit in any specific order of partitions, right?", "url": "https://github.com/apache/flink/pull/13744#discussion_r511929276", "createdAt": "2020-10-26T12:41:04Z", "author": {"login": "lirui-apache"}, "path": "flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/filesystem/stream/compact/CompactCoordinatorTest.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem.stream.compact;\n+\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.streaming.util.OneInputStreamOperatorTestHarness;\n+import org.apache.flink.table.filesystem.stream.compact.CompactMessages.CompactionUnit;\n+import org.apache.flink.table.filesystem.stream.compact.CompactMessages.CoordinatorInput;\n+import org.apache.flink.table.filesystem.stream.compact.CompactMessages.CoordinatorOutput;\n+import org.apache.flink.table.filesystem.stream.compact.CompactMessages.EndCompaction;\n+import org.apache.flink.table.filesystem.stream.compact.CompactMessages.EndInputFile;\n+import org.apache.flink.table.filesystem.stream.compact.CompactMessages.InputFile;\n+import org.apache.flink.util.function.ThrowingConsumer;\n+\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Test for {@link CompactCoordinator}.\n+ */\n+public class CompactCoordinatorTest extends AbstractCompactTestBase {\n+\n+\t@Test\n+\tpublic void testCoordinatorCrossCheckpoints() throws Exception {\n+\t\tAtomicReference<OperatorSubtaskState> state = new AtomicReference<>();\n+\t\trunCoordinator(harness -> {\n+\t\t\tharness.setup();\n+\t\t\tharness.open();\n+\n+\t\t\tharness.processElement(new InputFile(\"p0\", newFile(\"f0\", 3)), 0);\n+\t\t\tharness.processElement(new InputFile(\"p0\", newFile(\"f1\", 2)), 0);\n+\n+\t\t\tharness.processElement(new InputFile(\"p1\", newFile(\"f2\", 2)), 0);\n+\n+\t\t\tharness.processElement(new InputFile(\"p0\", newFile(\"f3\", 5)), 0);\n+\t\t\tharness.processElement(new InputFile(\"p0\", newFile(\"f4\", 1)), 0);\n+\n+\t\t\tharness.processElement(new InputFile(\"p1\", newFile(\"f5\", 5)), 0);\n+\t\t\tharness.processElement(new InputFile(\"p1\", newFile(\"f6\", 4)), 0);\n+\n+\t\t\tstate.set(harness.snapshot(1, 0));\n+\t\t});\n+\n+\t\trunCoordinator(harness -> {\n+\t\t\tharness.setup();\n+\t\t\tharness.initializeState(state.get());\n+\t\t\tharness.open();\n+\n+\t\t\tharness.processElement(new InputFile(\"p0\", newFile(\"f7\", 3)), 0);\n+\t\t\tharness.processElement(new InputFile(\"p0\", newFile(\"f8\", 2)), 0);\n+\n+\t\t\tstate.set(harness.snapshot(2, 0));\n+\t\t});\n+\n+\t\trunCoordinator(harness -> {\n+\t\t\tharness.setup();\n+\t\t\tharness.initializeState(state.get());\n+\t\t\tharness.open();\n+\n+\t\t\tharness.processElement(new EndInputFile(2, 0, 1), 0);\n+\n+\t\t\tList<CoordinatorOutput> outputs = harness.extractOutputValues();\n+\n+\t\t\tAssert.assertEquals(7, outputs.size());\n+\n+\t\t\tassertUnit(outputs.get(0), 0, \"p0\", Arrays.asList(\"f0\", \"f1\", \"f4\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "074ab07749ff798563e785d2c2fafeee1302ac74"}, "originalPosition": 89}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjM3ODQ2MQ==", "bodyText": "You mean the order of partitions? There is no relationship between partitions, so there is no need to guarantee this.", "url": "https://github.com/apache/flink/pull/13744#discussion_r512378461", "createdAt": "2020-10-27T02:26:27Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/filesystem/stream/compact/CompactCoordinatorTest.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem.stream.compact;\n+\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.streaming.util.OneInputStreamOperatorTestHarness;\n+import org.apache.flink.table.filesystem.stream.compact.CompactMessages.CompactionUnit;\n+import org.apache.flink.table.filesystem.stream.compact.CompactMessages.CoordinatorInput;\n+import org.apache.flink.table.filesystem.stream.compact.CompactMessages.CoordinatorOutput;\n+import org.apache.flink.table.filesystem.stream.compact.CompactMessages.EndCompaction;\n+import org.apache.flink.table.filesystem.stream.compact.CompactMessages.EndInputFile;\n+import org.apache.flink.table.filesystem.stream.compact.CompactMessages.InputFile;\n+import org.apache.flink.util.function.ThrowingConsumer;\n+\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Test for {@link CompactCoordinator}.\n+ */\n+public class CompactCoordinatorTest extends AbstractCompactTestBase {\n+\n+\t@Test\n+\tpublic void testCoordinatorCrossCheckpoints() throws Exception {\n+\t\tAtomicReference<OperatorSubtaskState> state = new AtomicReference<>();\n+\t\trunCoordinator(harness -> {\n+\t\t\tharness.setup();\n+\t\t\tharness.open();\n+\n+\t\t\tharness.processElement(new InputFile(\"p0\", newFile(\"f0\", 3)), 0);\n+\t\t\tharness.processElement(new InputFile(\"p0\", newFile(\"f1\", 2)), 0);\n+\n+\t\t\tharness.processElement(new InputFile(\"p1\", newFile(\"f2\", 2)), 0);\n+\n+\t\t\tharness.processElement(new InputFile(\"p0\", newFile(\"f3\", 5)), 0);\n+\t\t\tharness.processElement(new InputFile(\"p0\", newFile(\"f4\", 1)), 0);\n+\n+\t\t\tharness.processElement(new InputFile(\"p1\", newFile(\"f5\", 5)), 0);\n+\t\t\tharness.processElement(new InputFile(\"p1\", newFile(\"f6\", 4)), 0);\n+\n+\t\t\tstate.set(harness.snapshot(1, 0));\n+\t\t});\n+\n+\t\trunCoordinator(harness -> {\n+\t\t\tharness.setup();\n+\t\t\tharness.initializeState(state.get());\n+\t\t\tharness.open();\n+\n+\t\t\tharness.processElement(new InputFile(\"p0\", newFile(\"f7\", 3)), 0);\n+\t\t\tharness.processElement(new InputFile(\"p0\", newFile(\"f8\", 2)), 0);\n+\n+\t\t\tstate.set(harness.snapshot(2, 0));\n+\t\t});\n+\n+\t\trunCoordinator(harness -> {\n+\t\t\tharness.setup();\n+\t\t\tharness.initializeState(state.get());\n+\t\t\tharness.open();\n+\n+\t\t\tharness.processElement(new EndInputFile(2, 0, 1), 0);\n+\n+\t\t\tList<CoordinatorOutput> outputs = harness.extractOutputValues();\n+\n+\t\t\tAssert.assertEquals(7, outputs.size());\n+\n+\t\t\tassertUnit(outputs.get(0), 0, \"p0\", Arrays.asList(\"f0\", \"f1\", \"f4\"));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTkyOTI3Ng=="}, "originalCommit": {"oid": "074ab07749ff798563e785d2c2fafeee1302ac74"}, "originalPosition": 89}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjM4ODMyMw==", "bodyText": "Yeah... but then how could we assert the first output is for p0?", "url": "https://github.com/apache/flink/pull/13744#discussion_r512388323", "createdAt": "2020-10-27T03:01:35Z", "author": {"login": "lirui-apache"}, "path": "flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/filesystem/stream/compact/CompactCoordinatorTest.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem.stream.compact;\n+\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.streaming.util.OneInputStreamOperatorTestHarness;\n+import org.apache.flink.table.filesystem.stream.compact.CompactMessages.CompactionUnit;\n+import org.apache.flink.table.filesystem.stream.compact.CompactMessages.CoordinatorInput;\n+import org.apache.flink.table.filesystem.stream.compact.CompactMessages.CoordinatorOutput;\n+import org.apache.flink.table.filesystem.stream.compact.CompactMessages.EndCompaction;\n+import org.apache.flink.table.filesystem.stream.compact.CompactMessages.EndInputFile;\n+import org.apache.flink.table.filesystem.stream.compact.CompactMessages.InputFile;\n+import org.apache.flink.util.function.ThrowingConsumer;\n+\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Test for {@link CompactCoordinator}.\n+ */\n+public class CompactCoordinatorTest extends AbstractCompactTestBase {\n+\n+\t@Test\n+\tpublic void testCoordinatorCrossCheckpoints() throws Exception {\n+\t\tAtomicReference<OperatorSubtaskState> state = new AtomicReference<>();\n+\t\trunCoordinator(harness -> {\n+\t\t\tharness.setup();\n+\t\t\tharness.open();\n+\n+\t\t\tharness.processElement(new InputFile(\"p0\", newFile(\"f0\", 3)), 0);\n+\t\t\tharness.processElement(new InputFile(\"p0\", newFile(\"f1\", 2)), 0);\n+\n+\t\t\tharness.processElement(new InputFile(\"p1\", newFile(\"f2\", 2)), 0);\n+\n+\t\t\tharness.processElement(new InputFile(\"p0\", newFile(\"f3\", 5)), 0);\n+\t\t\tharness.processElement(new InputFile(\"p0\", newFile(\"f4\", 1)), 0);\n+\n+\t\t\tharness.processElement(new InputFile(\"p1\", newFile(\"f5\", 5)), 0);\n+\t\t\tharness.processElement(new InputFile(\"p1\", newFile(\"f6\", 4)), 0);\n+\n+\t\t\tstate.set(harness.snapshot(1, 0));\n+\t\t});\n+\n+\t\trunCoordinator(harness -> {\n+\t\t\tharness.setup();\n+\t\t\tharness.initializeState(state.get());\n+\t\t\tharness.open();\n+\n+\t\t\tharness.processElement(new InputFile(\"p0\", newFile(\"f7\", 3)), 0);\n+\t\t\tharness.processElement(new InputFile(\"p0\", newFile(\"f8\", 2)), 0);\n+\n+\t\t\tstate.set(harness.snapshot(2, 0));\n+\t\t});\n+\n+\t\trunCoordinator(harness -> {\n+\t\t\tharness.setup();\n+\t\t\tharness.initializeState(state.get());\n+\t\t\tharness.open();\n+\n+\t\t\tharness.processElement(new EndInputFile(2, 0, 1), 0);\n+\n+\t\t\tList<CoordinatorOutput> outputs = harness.extractOutputValues();\n+\n+\t\t\tAssert.assertEquals(7, outputs.size());\n+\n+\t\t\tassertUnit(outputs.get(0), 0, \"p0\", Arrays.asList(\"f0\", \"f1\", \"f4\"));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTkyOTI3Ng=="}, "originalCommit": {"oid": "074ab07749ff798563e785d2c2fafeee1302ac74"}, "originalPosition": 89}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjQzNDczMg==", "bodyText": "You mean we should sort it before assert?", "url": "https://github.com/apache/flink/pull/13744#discussion_r512434732", "createdAt": "2020-10-27T06:01:44Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/filesystem/stream/compact/CompactCoordinatorTest.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem.stream.compact;\n+\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.streaming.util.OneInputStreamOperatorTestHarness;\n+import org.apache.flink.table.filesystem.stream.compact.CompactMessages.CompactionUnit;\n+import org.apache.flink.table.filesystem.stream.compact.CompactMessages.CoordinatorInput;\n+import org.apache.flink.table.filesystem.stream.compact.CompactMessages.CoordinatorOutput;\n+import org.apache.flink.table.filesystem.stream.compact.CompactMessages.EndCompaction;\n+import org.apache.flink.table.filesystem.stream.compact.CompactMessages.EndInputFile;\n+import org.apache.flink.table.filesystem.stream.compact.CompactMessages.InputFile;\n+import org.apache.flink.util.function.ThrowingConsumer;\n+\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Test for {@link CompactCoordinator}.\n+ */\n+public class CompactCoordinatorTest extends AbstractCompactTestBase {\n+\n+\t@Test\n+\tpublic void testCoordinatorCrossCheckpoints() throws Exception {\n+\t\tAtomicReference<OperatorSubtaskState> state = new AtomicReference<>();\n+\t\trunCoordinator(harness -> {\n+\t\t\tharness.setup();\n+\t\t\tharness.open();\n+\n+\t\t\tharness.processElement(new InputFile(\"p0\", newFile(\"f0\", 3)), 0);\n+\t\t\tharness.processElement(new InputFile(\"p0\", newFile(\"f1\", 2)), 0);\n+\n+\t\t\tharness.processElement(new InputFile(\"p1\", newFile(\"f2\", 2)), 0);\n+\n+\t\t\tharness.processElement(new InputFile(\"p0\", newFile(\"f3\", 5)), 0);\n+\t\t\tharness.processElement(new InputFile(\"p0\", newFile(\"f4\", 1)), 0);\n+\n+\t\t\tharness.processElement(new InputFile(\"p1\", newFile(\"f5\", 5)), 0);\n+\t\t\tharness.processElement(new InputFile(\"p1\", newFile(\"f6\", 4)), 0);\n+\n+\t\t\tstate.set(harness.snapshot(1, 0));\n+\t\t});\n+\n+\t\trunCoordinator(harness -> {\n+\t\t\tharness.setup();\n+\t\t\tharness.initializeState(state.get());\n+\t\t\tharness.open();\n+\n+\t\t\tharness.processElement(new InputFile(\"p0\", newFile(\"f7\", 3)), 0);\n+\t\t\tharness.processElement(new InputFile(\"p0\", newFile(\"f8\", 2)), 0);\n+\n+\t\t\tstate.set(harness.snapshot(2, 0));\n+\t\t});\n+\n+\t\trunCoordinator(harness -> {\n+\t\t\tharness.setup();\n+\t\t\tharness.initializeState(state.get());\n+\t\t\tharness.open();\n+\n+\t\t\tharness.processElement(new EndInputFile(2, 0, 1), 0);\n+\n+\t\t\tList<CoordinatorOutput> outputs = harness.extractOutputValues();\n+\n+\t\t\tAssert.assertEquals(7, outputs.size());\n+\n+\t\t\tassertUnit(outputs.get(0), 0, \"p0\", Arrays.asList(\"f0\", \"f1\", \"f4\"));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTkyOTI3Ng=="}, "originalCommit": {"oid": "074ab07749ff798563e785d2c2fafeee1302ac74"}, "originalPosition": 89}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjQzNTk2NA==", "bodyText": "OK, I'll do it", "url": "https://github.com/apache/flink/pull/13744#discussion_r512435964", "createdAt": "2020-10-27T06:05:56Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/filesystem/stream/compact/CompactCoordinatorTest.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem.stream.compact;\n+\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.streaming.util.OneInputStreamOperatorTestHarness;\n+import org.apache.flink.table.filesystem.stream.compact.CompactMessages.CompactionUnit;\n+import org.apache.flink.table.filesystem.stream.compact.CompactMessages.CoordinatorInput;\n+import org.apache.flink.table.filesystem.stream.compact.CompactMessages.CoordinatorOutput;\n+import org.apache.flink.table.filesystem.stream.compact.CompactMessages.EndCompaction;\n+import org.apache.flink.table.filesystem.stream.compact.CompactMessages.EndInputFile;\n+import org.apache.flink.table.filesystem.stream.compact.CompactMessages.InputFile;\n+import org.apache.flink.util.function.ThrowingConsumer;\n+\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Test for {@link CompactCoordinator}.\n+ */\n+public class CompactCoordinatorTest extends AbstractCompactTestBase {\n+\n+\t@Test\n+\tpublic void testCoordinatorCrossCheckpoints() throws Exception {\n+\t\tAtomicReference<OperatorSubtaskState> state = new AtomicReference<>();\n+\t\trunCoordinator(harness -> {\n+\t\t\tharness.setup();\n+\t\t\tharness.open();\n+\n+\t\t\tharness.processElement(new InputFile(\"p0\", newFile(\"f0\", 3)), 0);\n+\t\t\tharness.processElement(new InputFile(\"p0\", newFile(\"f1\", 2)), 0);\n+\n+\t\t\tharness.processElement(new InputFile(\"p1\", newFile(\"f2\", 2)), 0);\n+\n+\t\t\tharness.processElement(new InputFile(\"p0\", newFile(\"f3\", 5)), 0);\n+\t\t\tharness.processElement(new InputFile(\"p0\", newFile(\"f4\", 1)), 0);\n+\n+\t\t\tharness.processElement(new InputFile(\"p1\", newFile(\"f5\", 5)), 0);\n+\t\t\tharness.processElement(new InputFile(\"p1\", newFile(\"f6\", 4)), 0);\n+\n+\t\t\tstate.set(harness.snapshot(1, 0));\n+\t\t});\n+\n+\t\trunCoordinator(harness -> {\n+\t\t\tharness.setup();\n+\t\t\tharness.initializeState(state.get());\n+\t\t\tharness.open();\n+\n+\t\t\tharness.processElement(new InputFile(\"p0\", newFile(\"f7\", 3)), 0);\n+\t\t\tharness.processElement(new InputFile(\"p0\", newFile(\"f8\", 2)), 0);\n+\n+\t\t\tstate.set(harness.snapshot(2, 0));\n+\t\t});\n+\n+\t\trunCoordinator(harness -> {\n+\t\t\tharness.setup();\n+\t\t\tharness.initializeState(state.get());\n+\t\t\tharness.open();\n+\n+\t\t\tharness.processElement(new EndInputFile(2, 0, 1), 0);\n+\n+\t\t\tList<CoordinatorOutput> outputs = harness.extractOutputValues();\n+\n+\t\t\tAssert.assertEquals(7, outputs.size());\n+\n+\t\t\tassertUnit(outputs.get(0), 0, \"p0\", Arrays.asList(\"f0\", \"f1\", \"f4\"));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTkyOTI3Ng=="}, "originalCommit": {"oid": "074ab07749ff798563e785d2c2fafeee1302ac74"}, "originalPosition": 89}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIxMTE5MzIzOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/filesystem/stream/compact/CompactOperatorTest.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QwODo1NjowNVrOHoxXOw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QwODo1NjowNVrOHoxXOw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjUxMzg1MQ==", "bodyText": "Also verify f3 and f6 are not compacted at this point.", "url": "https://github.com/apache/flink/pull/13744#discussion_r512513851", "createdAt": "2020-10-27T08:56:05Z", "author": {"login": "lirui-apache"}, "path": "flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/filesystem/stream/compact/CompactOperatorTest.java", "diffHunk": "@@ -105,8 +105,63 @@ public void testCompactOperator() throws Exception {\n \t\t});\n \t}\n \n+\t@Test\n+\tpublic void testUnitSelection() throws Exception {\n+\t\tOneInputStreamOperatorTestHarness<CoordinatorOutput, PartitionCommitInfo> harness0 = create(2, 0);\n+\t\tharness0.setup();\n+\t\tharness0.open();\n+\n+\t\tOneInputStreamOperatorTestHarness<CoordinatorOutput, PartitionCommitInfo> harness1 = create(2, 1);\n+\t\tharness1.setup();\n+\t\tharness1.open();\n+\n+\t\tPath f0 = newFile(\".uncompacted-f0\", 3);\n+\t\tPath f1 = newFile(\".uncompacted-f1\", 2);\n+\t\tPath f2 = newFile(\".uncompacted-f2\", 2);\n+\t\tPath f3 = newFile(\".uncompacted-f3\", 5);\n+\t\tPath f4 = newFile(\".uncompacted-f4\", 1);\n+\t\tPath f5 = newFile(\".uncompacted-f5\", 5);\n+\t\tPath f6 = newFile(\".uncompacted-f6\", 4);\n+\t\tFileSystem fs = f0.getFileSystem();\n+\n+\t\t// broadcast\n+\t\tharness0.processElement(new CompactionUnit(0, \"p0\", Arrays.asList(f0, f1, f4)), 0);\n+\t\tharness0.processElement(new CompactionUnit(1, \"p0\", Collections.singletonList(f3)), 0);\n+\t\tharness0.processElement(new CompactionUnit(2, \"p0\", Arrays.asList(f2, f5)), 0);\n+\t\tharness0.processElement(new CompactionUnit(3, \"p0\", Collections.singletonList(f6)), 0);\n+\n+\t\tharness1.processElement(new CompactionUnit(0, \"p0\", Arrays.asList(f0, f1, f4)), 0);\n+\t\tharness1.processElement(new CompactionUnit(1, \"p0\", Collections.singletonList(f3)), 0);\n+\t\tharness1.processElement(new CompactionUnit(2, \"p0\", Arrays.asList(f2, f5)), 0);\n+\t\tharness1.processElement(new CompactionUnit(3, \"p0\", Collections.singletonList(f6)), 0);\n+\n+\t\tharness0.processElement(new EndCompaction(1), 0);\n+\n+\t\t// check all compacted file generated\n+\t\tAssert.assertTrue(fs.exists(new Path(folder, \"compacted-f0\")));\n+\t\tAssert.assertTrue(fs.exists(new Path(folder, \"compacted-f2\")));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b30d49e889a255acb42f286e54d56f1760ff367e"}, "originalPosition": 47}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 79, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}