{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzk2MTI5NTM1", "number": 11574, "title": "[FLINK-16859][table-runtime] Introduce file system table factory", "bodyText": "What is the purpose of the change\nIntroduce\u00a0FileSystemTableFactory to unify all file system connectors.\nMore information in\u00a0https://cwiki.apache.org/confluence/display/FLINK/FLIP-115%3A+Filesystem+connector+in+Table\nBrief change log\n\nIntroduce FileSystemTableFactory\nIntroduce FileSystemTableSource\nIntroduce FileSystemTableSink\nIntroduce FileSystemFormatFactory interface to abstract format interface.\nIntroduce RowDataPartitionComputer to BaseRow handle.\n\nVerifying this change\n\nIntroduce FileSystemITCaseBase to do all format test things.\nFileSystemTestCsvITCase\n\nDoes this pull request potentially affect one of the following parts:\n\nDependencies (does it add or upgrade a dependency): (yes / no)\nThe public API, i.e., is any changed class annotated with @Public(Evolving): (yes / no)\nThe serializers: (yes / no / don't know)\nThe runtime per-record code paths (performance sensitive): (yes / no / don't know)\nAnything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (yes / no / don't know)\nThe S3 file system connector: (yes / no / don't know)\n\nDocumentation\n\nDoes this pull request introduce a new feature? (yes / no)\nIf yes, how is the feature documented? (JavaDocs)", "createdAt": "2020-03-31T07:23:52Z", "url": "https://github.com/apache/flink/pull/11574", "merged": true, "mergeCommit": {"oid": "b0e334afc7eb5531996b21d28e212febc7d38bf8"}, "closed": true, "closedAt": "2020-04-07T06:48:46Z", "author": {"login": "JingsongLi"}, "timelineItems": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcTkpQGgFqTM4NjA4ODgzNA==", "endCursor": "Y3Vyc29yOnYyOpPPAAABcVKgWwgBqjMyMDgwMDY3OTU=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg2MDg4ODM0", "url": "https://github.com/apache/flink/pull/11574#pullrequestreview-386088834", "createdAt": "2020-04-02T03:40:03Z", "commit": {"oid": "cd8a10137fa50eb11035082c95878fb6aa6a66c6"}, "state": "COMMENTED", "comments": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMlQwMzo0MDowM1rOF_aPHA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMlQwNDoyNDo1MFrOF_a1FA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjAzNDQ2MA==", "bodyText": "using two optional interface looks strange\uff0ccould we make them to one like createWriter(WriterContext) ?", "url": "https://github.com/apache/flink/pull/11574#discussion_r402034460", "createdAt": "2020-04-02T03:40:03Z", "author": {"login": "leonardBang"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemFormatFactory.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.dataformat.BaseRow;\n+import org.apache.flink.table.expressions.Expression;\n+import org.apache.flink.table.factories.TableFormatFactory;\n+import org.apache.flink.table.types.DataType;\n+\n+import java.util.List;\n+import java.util.Optional;\n+\n+/**\n+ * File system format factory for creating configured instances of reader and writer.\n+ */\n+@Internal\n+public interface FileSystemFormatFactory extends TableFormatFactory<BaseRow> {\n+\n+\t/**\n+\t * Create {@link InputFormat} reader.\n+\t */\n+\tInputFormat<BaseRow, ?> createReader(ReaderContext context);\n+\n+\t/**\n+\t * Create {@link Encoder} writer.\n+\t */\n+\tOptional<Encoder<BaseRow>> createEncoder(WriterContext context);\n+\n+\t/**\n+\t * Create {@link BulkWriter.Factory} writer.\n+\t */\n+\tOptional<BulkWriter.Factory<BaseRow>> createBulkWriterFactory(WriterContext context);\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd8a10137fa50eb11035082c95878fb6aa6a66c6"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjAzNDc2MA==", "bodyText": "use lowercase <p>", "url": "https://github.com/apache/flink/pull/11574#discussion_r402034760", "createdAt": "2020-04-02T03:41:18Z", "author": {"login": "leonardBang"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableFactory.java", "diffHunk": "@@ -0,0 +1,129 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.configuration.ConfigOption;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.dataformat.BaseRow;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.TableFactory;\n+import org.apache.flink.table.factories.TableFactoryService;\n+import org.apache.flink.table.factories.TableSinkFactory;\n+import org.apache.flink.table.factories.TableSourceFactory;\n+import org.apache.flink.table.sinks.TableSink;\n+import org.apache.flink.table.sources.TableSource;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.apache.flink.configuration.ConfigOptions.key;\n+import static org.apache.flink.table.descriptors.ConnectorDescriptorValidator.CONNECTOR;\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+import static org.apache.flink.table.descriptors.Schema.SCHEMA;\n+\n+/**\n+ * File system {@link TableFactory}.\n+ *\n+ * <P>File system support:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd8a10137fa50eb11035082c95878fb6aa6a66c6"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjAzNTUxOA==", "bodyText": "statement * 1. The partition ... is not a support but an agreement, we can place in new paragraph.", "url": "https://github.com/apache/flink/pull/11574#discussion_r402035518", "createdAt": "2020-04-02T03:44:37Z", "author": {"login": "leonardBang"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableFactory.java", "diffHunk": "@@ -0,0 +1,129 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.configuration.ConfigOption;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.dataformat.BaseRow;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.TableFactory;\n+import org.apache.flink.table.factories.TableFactoryService;\n+import org.apache.flink.table.factories.TableSinkFactory;\n+import org.apache.flink.table.factories.TableSourceFactory;\n+import org.apache.flink.table.sinks.TableSink;\n+import org.apache.flink.table.sources.TableSource;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.apache.flink.configuration.ConfigOptions.key;\n+import static org.apache.flink.table.descriptors.ConnectorDescriptorValidator.CONNECTOR;\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+import static org.apache.flink.table.descriptors.Schema.SCHEMA;\n+\n+/**\n+ * File system {@link TableFactory}.\n+ *\n+ * <P>File system support:\n+ * 1.The partition information should be in the file system path, whether it's a temporary", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd8a10137fa50eb11035082c95878fb6aa6a66c6"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjAzODU1OA==", "bodyText": "Here we change the CONNECTOR_VALUE\u3001PATH different with FileSystemValidator,\n public static final String CONNECTOR_TYPE_VALUE = \"filesystem\"; public static final String CONNECTOR_PATH = \"connector.path\";\nIIUC, you want to avoid conflict with current CsvTableSinkFactoryBase/CsvTableSourceFactoryBase, so if user use  path can route to this factory, use connector.path will route to oldfactory(i.e.CsvTableSinkFactoryBase/CsvTableSourceFactoryBase )\nThis is a new feature and face to user, I think we'd better add docs.", "url": "https://github.com/apache/flink/pull/11574#discussion_r402038558", "createdAt": "2020-04-02T03:58:21Z", "author": {"login": "leonardBang"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableFactory.java", "diffHunk": "@@ -0,0 +1,129 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.configuration.ConfigOption;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.dataformat.BaseRow;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.TableFactory;\n+import org.apache.flink.table.factories.TableFactoryService;\n+import org.apache.flink.table.factories.TableSinkFactory;\n+import org.apache.flink.table.factories.TableSourceFactory;\n+import org.apache.flink.table.sinks.TableSink;\n+import org.apache.flink.table.sources.TableSource;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.apache.flink.configuration.ConfigOptions.key;\n+import static org.apache.flink.table.descriptors.ConnectorDescriptorValidator.CONNECTOR;\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+import static org.apache.flink.table.descriptors.Schema.SCHEMA;\n+\n+/**\n+ * File system {@link TableFactory}.\n+ *\n+ * <P>File system support:\n+ * 1.The partition information should be in the file system path, whether it's a temporary\n+ * table or a catalog table.\n+ * 2.Support insert into (append) and insert overwrite.\n+ * 3.Support static and dynamic partition inserting.\n+ */\n+public class FileSystemTableFactory implements\n+\t\tTableSourceFactory<BaseRow>,\n+\t\tTableSinkFactory<BaseRow> {\n+\n+\tpublic static final String CONNECTOR_VALUE = \"filesystem\";\n+\tpublic static final String PATH = \"path\";\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd8a10137fa50eb11035082c95878fb6aa6a66c6"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjAzODg2Nw==", "bodyText": "Do some basic validate properties here to align with other tableSource/Sink Factory?\nI think we can do validate in FileSystemValidator\uff0c and also move fields CONNECTOR_VALUE\u3001PATH\u3001PARTITION_DEFAULT_NAME to FileSystemValidator`\uff0c maybe this will make the class more clear?", "url": "https://github.com/apache/flink/pull/11574#discussion_r402038867", "createdAt": "2020-04-02T03:59:38Z", "author": {"login": "leonardBang"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableFactory.java", "diffHunk": "@@ -0,0 +1,129 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.configuration.ConfigOption;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.dataformat.BaseRow;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.TableFactory;\n+import org.apache.flink.table.factories.TableFactoryService;\n+import org.apache.flink.table.factories.TableSinkFactory;\n+import org.apache.flink.table.factories.TableSourceFactory;\n+import org.apache.flink.table.sinks.TableSink;\n+import org.apache.flink.table.sources.TableSource;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.apache.flink.configuration.ConfigOptions.key;\n+import static org.apache.flink.table.descriptors.ConnectorDescriptorValidator.CONNECTOR;\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+import static org.apache.flink.table.descriptors.Schema.SCHEMA;\n+\n+/**\n+ * File system {@link TableFactory}.\n+ *\n+ * <P>File system support:\n+ * 1.The partition information should be in the file system path, whether it's a temporary\n+ * table or a catalog table.\n+ * 2.Support insert into (append) and insert overwrite.\n+ * 3.Support static and dynamic partition inserting.\n+ */\n+public class FileSystemTableFactory implements\n+\t\tTableSourceFactory<BaseRow>,\n+\t\tTableSinkFactory<BaseRow> {\n+\n+\tpublic static final String CONNECTOR_VALUE = \"filesystem\";\n+\tpublic static final String PATH = \"path\";\n+\n+\tpublic static final ConfigOption<String> PARTITION_DEFAULT_NAME = key(\"partition.default-name\")\n+\t\t\t.stringType()\n+\t\t\t.defaultValue(\"__DEFAULT_PARTITION__\")\n+\t\t\t.withDescription(\"The default partition name in case the dynamic partition\" +\n+\t\t\t\t\t\" column value is null/empty string\");\n+\n+\t@Override\n+\tpublic Map<String, String> requiredContext() {\n+\t\tMap<String, String> context = new HashMap<>();\n+\t\tcontext.put(CONNECTOR, CONNECTOR_VALUE);\n+\t\treturn context;\n+\t}\n+\n+\t@Override\n+\tpublic List<String> supportedProperties() {\n+\t\tList<String> properties = new ArrayList<>();\n+\n+\t\t// path\n+\t\tproperties.add(PATH);\n+\n+\t\t// schema\n+\t\tproperties.add(SCHEMA + \".#.\" + DescriptorProperties.TABLE_SCHEMA_DATA_TYPE);\n+\t\tproperties.add(SCHEMA + \".#.\" + DescriptorProperties.TABLE_SCHEMA_NAME);\n+\n+\t\tproperties.add(PARTITION_DEFAULT_NAME.key());\n+\n+\t\t// format\n+\t\tproperties.add(FORMAT);\n+\t\tproperties.add(FORMAT + \".*\");\n+\n+\t\treturn properties;\n+\t}\n+\n+\t@Override\n+\tpublic TableSource<BaseRow> createTableSource(TableSourceFactory.Context context) {\n+\t\tDescriptorProperties properties = new DescriptorProperties();\n+\t\tproperties.putProperties(context.getTable().getProperties());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd8a10137fa50eb11035082c95878fb6aa6a66c6"}, "originalPosition": 94}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjA0MjU1Ng==", "bodyText": "could we rename to BaseRowPartitionComputer before FLIP-95? otherwise the note will not match the name.", "url": "https://github.com/apache/flink/pull/11574#discussion_r402042556", "createdAt": "2020-04-02T04:16:54Z", "author": {"login": "leonardBang"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/RowDataPartitionComputer.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.table.dataformat.BaseRow;\n+import org.apache.flink.table.dataformat.GenericRow;\n+import org.apache.flink.table.dataformat.TypeGetterSetters;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+\n+import java.util.Arrays;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+/**\n+ * {@link PartitionComputer} for {@link BaseRow}.\n+ */\n+@Internal\n+public class RowDataPartitionComputer implements PartitionComputer<BaseRow> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd8a10137fa50eb11035082c95878fb6aa6a66c6"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjA0MzMzMA==", "bodyText": "The value only from  partition column, use Unsupported partition type to make it clear?", "url": "https://github.com/apache/flink/pull/11574#discussion_r402043330", "createdAt": "2020-04-02T04:20:55Z", "author": {"login": "leonardBang"}, "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/utils/TestRowDataCsvInputFormat.java", "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.utils;\n+\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.io.RichInputFormat;\n+import org.apache.flink.api.common.io.statistics.BaseStatistics;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeinfo.Types;\n+import org.apache.flink.api.java.io.RowCsvInputFormat;\n+import org.apache.flink.api.java.typeutils.RowTypeInfo;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.core.io.InputSplitAssigner;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.dataformat.BaseRow;\n+import org.apache.flink.table.dataformat.BinaryString;\n+import org.apache.flink.table.dataformat.DataFormatConverters;\n+import org.apache.flink.table.dataformat.GenericRow;\n+import org.apache.flink.table.filesystem.PartitionPathUtils;\n+import org.apache.flink.table.types.utils.TypeConversions;\n+import org.apache.flink.types.Row;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * The {@link InputFormat} that output {@link BaseRow}.\n+ */\n+public class TestRowDataCsvInputFormat extends RichInputFormat<BaseRow, FileInputSplit> {\n+\n+\tprivate final List<String> partitionKeys;\n+\tprivate final String defaultPartValue;\n+\tprivate final int[] selectFields;\n+\tprivate final long limit;\n+\tprivate final RowCsvInputFormat inputFormat;\n+\tprivate final List<TypeInformation> fieldTypes;\n+\tprivate final List<String> fieldNames;\n+\tprivate final List<DataFormatConverters.DataFormatConverter> csvSelectConverters;\n+\tprivate final int[] csvFieldMapping;\n+\n+\tprivate transient Row csvRow;\n+\tprivate transient GenericRow row;\n+\tprivate transient long emitted;\n+\n+\tpublic TestRowDataCsvInputFormat(\n+\t\t\tPath[] paths,\n+\t\t\tTableSchema schema,\n+\t\t\tList<String> partitionKeys,\n+\t\t\tString defaultPartValue,\n+\t\t\tint[] selectFields,\n+\t\t\tlong limit) {\n+\t\tthis.partitionKeys = partitionKeys;\n+\t\tthis.defaultPartValue = defaultPartValue;\n+\t\tthis.selectFields = selectFields;\n+\t\tthis.limit = limit;\n+\t\tRowTypeInfo rowType = (RowTypeInfo) schema.toRowType();\n+\t\tthis.fieldTypes = Arrays.asList(rowType.getFieldTypes());\n+\t\tthis.fieldNames = Arrays.asList(rowType.getFieldNames());\n+\n+\t\tList<String> csvFieldNames = fieldNames.stream()\n+\t\t\t\t.filter(name -> !partitionKeys.contains(name)).collect(Collectors.toList());\n+\n+\t\tList<String> selectFieldNames = Arrays.stream(selectFields)\n+\t\t\t\t.mapToObj(fieldNames::get)\n+\t\t\t\t.collect(Collectors.toList());\n+\t\tList<String> csvSelectFieldNames = selectFieldNames.stream()\n+\t\t\t\t.filter(name -> !partitionKeys.contains(name)).collect(Collectors.toList());\n+\t\tList<TypeInformation> csvSelectTypes = csvSelectFieldNames.stream()\n+\t\t\t\t.map(name -> fieldTypes.get(fieldNames.indexOf(name))).collect(Collectors.toList());\n+\t\tthis.csvSelectConverters = csvSelectTypes.stream()\n+\t\t\t\t.map(TypeConversions::fromLegacyInfoToDataType)\n+\t\t\t\t.map(DataFormatConverters::getConverterForDataType)\n+\t\t\t\t.collect(Collectors.toList());\n+\t\tint[] csvSelectFields = csvSelectFieldNames.stream().mapToInt(csvFieldNames::indexOf).toArray();\n+\t\tthis.inputFormat = new RowCsvInputFormat(\n+\t\t\t\tnull, csvSelectTypes.toArray(new TypeInformation[0]), csvSelectFields);\n+\t\tthis.inputFormat.setFilePaths(paths);\n+\n+\t\tthis.csvFieldMapping = csvSelectFieldNames.stream().mapToInt(selectFieldNames::indexOf).toArray();\n+\t\tthis.emitted = 0;\n+\t}\n+\n+\t@Override\n+\tpublic void configure(Configuration parameters) {\n+\t\tinputFormat.configure(parameters);\n+\t}\n+\n+\t@Override\n+\tpublic BaseStatistics getStatistics(BaseStatistics cachedStatistics) throws IOException {\n+\t\treturn inputFormat.getStatistics(cachedStatistics);\n+\t}\n+\n+\t@Override\n+\tpublic FileInputSplit[] createInputSplits(int minNumSplits) throws IOException {\n+\t\treturn inputFormat.createInputSplits(minNumSplits);\n+\t}\n+\n+\t@Override\n+\tpublic InputSplitAssigner getInputSplitAssigner(FileInputSplit[] inputSplits) {\n+\t\treturn inputFormat.getInputSplitAssigner(inputSplits);\n+\t}\n+\n+\t@Override\n+\tpublic void open(FileInputSplit split) throws IOException {\n+\t\tinputFormat.open(split);\n+\t\tPath path = split.getPath();\n+\t\tLinkedHashMap<String, String> partSpec = PartitionPathUtils.extractPartitionSpecFromPath(path);\n+\t\tthis.row = new GenericRow(selectFields.length);\n+\t\tfor (int i = 0; i < selectFields.length; i++) {\n+\t\t\tint selectField = selectFields[i];\n+\t\t\tString name = fieldNames.get(selectField);\n+\t\t\tif (partitionKeys.contains(name)) {\n+\t\t\t\tString value = partSpec.get(name);\n+\t\t\t\tvalue = defaultPartValue.equals(value) ? null : value;\n+\t\t\t\tthis.row.setField(\n+\t\t\t\t\t\ti, convertStringToInternal(value, fieldTypes.get(selectField)));\n+\t\t\t}\n+\t\t}\n+\t\tthis.csvRow = new Row(csvSelectConverters.size());\n+\t}\n+\n+\tprivate Object convertStringToInternal(String value, TypeInformation type) {\n+\t\tif (type.equals(Types.INT)) {\n+\t\t\treturn Integer.parseInt(value);\n+\t\t} else if (type.equals(Types.LONG)) {\n+\t\t\treturn Long.parseLong(value);\n+\t\t} else if (type.equals(Types.STRING)) {\n+\t\t\treturn BinaryString.fromString(value);\n+\t\t} else {\n+\t\t\tthrow new UnsupportedOperationException(\"Unsupported type: \" + type);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd8a10137fa50eb11035082c95878fb6aa6a66c6"}, "originalPosition": 151}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjA0NDEzNA==", "bodyText": "Add a serialVersionUID?", "url": "https://github.com/apache/flink/pull/11574#discussion_r402044134", "createdAt": "2020-04-02T04:24:40Z", "author": {"login": "leonardBang"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSink.java", "diffHunk": "@@ -0,0 +1,271 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.api.common.io.OutputFormat;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.fs.FSDataOutputStream;\n+import org.apache.flink.core.fs.FileSystem;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.DataStreamSink;\n+import org.apache.flink.table.api.TableException;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.dataformat.BaseRow;\n+import org.apache.flink.table.sinks.AppendStreamTableSink;\n+import org.apache.flink.table.sinks.OverwritableTableSink;\n+import org.apache.flink.table.sinks.PartitionableTableSink;\n+import org.apache.flink.table.sinks.TableSink;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+\n+/**\n+ * File system {@link TableSink}.\n+ */\n+public class FileSystemTableSink implements\n+\t\tAppendStreamTableSink<BaseRow>,\n+\t\tPartitionableTableSink,\n+\t\tOverwritableTableSink {\n+\n+\tprivate final TableSchema schema;\n+\tprivate final List<String> partitionKeys;\n+\tprivate final Path path;\n+\tprivate final String defaultPartName;\n+\tprivate final FileSystemFormatFactory formatFactory;\n+\n+\tprivate boolean overwrite = false;\n+\tprivate boolean dynamicGrouping = false;\n+\tprivate LinkedHashMap<String, String> staticPartitions = new LinkedHashMap<>();\n+\n+\t/**\n+\t * Construct a file system table sink.\n+\t *\n+\t * @param schema schema of the table.\n+\t * @param path directory path of the file system table.\n+\t * @param partitionKeys partition keys of the table.\n+\t * @param defaultPartName The default partition name in case the dynamic partition column value\n+\t *                        is null/empty string.\n+\t * @param formatFactory format factory to create reader.\n+\t */\n+\tpublic FileSystemTableSink(\n+\t\t\tTableSchema schema,\n+\t\t\tPath path,\n+\t\t\tList<String> partitionKeys,\n+\t\t\tString defaultPartName,\n+\t\t\tFileSystemFormatFactory formatFactory) {\n+\t\tthis.schema = schema;\n+\t\tthis.path = path;\n+\t\tthis.defaultPartName = defaultPartName;\n+\t\tthis.formatFactory = formatFactory;\n+\t\tthis.partitionKeys = partitionKeys;\n+\t}\n+\n+\t@Override\n+\tpublic final DataStreamSink<BaseRow> consumeDataStream(DataStream<BaseRow> dataStream) {\n+\t\tRowDataPartitionComputer computer = new RowDataPartitionComputer(\n+\t\t\t\tdefaultPartName,\n+\t\t\t\tschema.getFieldNames(),\n+\t\t\t\tschema.getFieldDataTypes(),\n+\t\t\t\tpartitionKeys.toArray(new String[0]));\n+\n+\t\tFileSystemOutputFormat.Builder<BaseRow> builder = new FileSystemOutputFormat.Builder<>();\n+\t\tbuilder.setPartitionComputer(computer);\n+\t\tbuilder.setDynamicGrouped(dynamicGrouping);\n+\t\tbuilder.setPartitionColumns(partitionKeys.toArray(new String[0]));\n+\t\tbuilder.setFormatFactory(createOutputFormatFactory(\n+\t\t\t\tformatFactory, this::getNonPartitionTypes));\n+\t\tbuilder.setMetaStoreFactory(createTableMetaStoreFactory(path));\n+\t\tbuilder.setOverwrite(overwrite);\n+\t\tbuilder.setStaticPartitions(staticPartitions);\n+\t\tbuilder.setTempPath(toStagingPath());\n+\t\treturn dataStream.writeUsingOutputFormat(builder.build())\n+\t\t\t\t.setParallelism(dataStream.getParallelism());\n+\t}\n+\n+\tprivate DataType[] getNonPartitionTypes() {\n+\t\treturn Arrays.stream(schema.getFieldNames())\n+\t\t\t\t.filter(name -> !partitionKeys.contains(name))\n+\t\t\t\t.map(name -> schema.getFieldDataType(name).get())\n+\t\t\t\t.toArray(DataType[]::new);\n+\t}\n+\n+\tprivate Path toStagingPath() {\n+\t\tPath stagingDir = new Path(path, \".staging_\" + System.currentTimeMillis());\n+\t\ttry {\n+\t\t\tFileSystem fs = stagingDir.getFileSystem();\n+\t\t\tPreconditions.checkState(\n+\t\t\t\t\tfs.exists(stagingDir) || fs.mkdirs(stagingDir),\n+\t\t\t\t\t\"Failed to create staging dir \" + stagingDir);\n+\t\t\treturn stagingDir;\n+\t\t} catch (IOException e) {\n+\t\t\tthrow new RuntimeException(e);\n+\t\t}\n+\t}\n+\n+\tprivate static OutputFormatFactory<BaseRow> createOutputFormatFactory(\n+\t\t\tFileSystemFormatFactory formatFactory,\n+\t\t\tFileSystemFormatFactory.WriterContext context) {\n+\t\tOptional<Encoder<BaseRow>> encoder = formatFactory.createEncoder(context);\n+\t\tOptional<BulkWriter.Factory<BaseRow>> bulk = formatFactory.createBulkWriterFactory(context);\n+\n+\t\tif (!encoder.isPresent() && !bulk.isPresent()) {\n+\t\t\tthrow new TableException(\n+\t\t\t\t\tformatFactory + \" format should implement at least one Encoder or BulkWriter\");\n+\t\t}\n+\t\treturn encoder\n+\t\t\t\t.<OutputFormatFactory<BaseRow>>map(en -> path -> createEncoderOutputFormat(en, path))\n+\t\t\t\t.orElseGet(() -> path -> createBulkWriterOutputFormat(bulk.get(), path));\n+\t}\n+\n+\tprivate static OutputFormat<BaseRow> createBulkWriterOutputFormat(\n+\t\t\tBulkWriter.Factory<BaseRow> factory,\n+\t\t\tPath path) {\n+\t\treturn new OutputFormat<BaseRow>() {\n+\n+\t\t\tprivate BulkWriter<BaseRow> writer;\n+\n+\t\t\t@Override\n+\t\t\tpublic void configure(Configuration parameters) {\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic void open(int taskNumber, int numTasks) throws IOException {\n+\t\t\t\tthis.writer = factory.create(path.getFileSystem()\n+\t\t\t\t\t\t.create(path, FileSystem.WriteMode.OVERWRITE));\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic void writeRecord(BaseRow record) throws IOException {\n+\t\t\t\twriter.addElement(record);\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic void close() throws IOException {\n+\t\t\t\twriter.flush();\n+\t\t\t\twriter.finish();\n+\t\t\t}\n+\t\t};\n+\t}\n+\n+\tprivate static OutputFormat<BaseRow> createEncoderOutputFormat(\n+\t\t\tEncoder<BaseRow> encoder,\n+\t\t\tPath path) {\n+\t\treturn new OutputFormat<BaseRow>() {\n+\n+\t\t\tprivate FSDataOutputStream output;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd8a10137fa50eb11035082c95878fb6aa6a66c6"}, "originalPosition": 181}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjA0NDE4MA==", "bodyText": "Add a serialVersionUID?", "url": "https://github.com/apache/flink/pull/11574#discussion_r402044180", "createdAt": "2020-04-02T04:24:50Z", "author": {"login": "leonardBang"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSink.java", "diffHunk": "@@ -0,0 +1,271 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.api.common.io.OutputFormat;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.fs.FSDataOutputStream;\n+import org.apache.flink.core.fs.FileSystem;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.DataStreamSink;\n+import org.apache.flink.table.api.TableException;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.dataformat.BaseRow;\n+import org.apache.flink.table.sinks.AppendStreamTableSink;\n+import org.apache.flink.table.sinks.OverwritableTableSink;\n+import org.apache.flink.table.sinks.PartitionableTableSink;\n+import org.apache.flink.table.sinks.TableSink;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+\n+/**\n+ * File system {@link TableSink}.\n+ */\n+public class FileSystemTableSink implements\n+\t\tAppendStreamTableSink<BaseRow>,\n+\t\tPartitionableTableSink,\n+\t\tOverwritableTableSink {\n+\n+\tprivate final TableSchema schema;\n+\tprivate final List<String> partitionKeys;\n+\tprivate final Path path;\n+\tprivate final String defaultPartName;\n+\tprivate final FileSystemFormatFactory formatFactory;\n+\n+\tprivate boolean overwrite = false;\n+\tprivate boolean dynamicGrouping = false;\n+\tprivate LinkedHashMap<String, String> staticPartitions = new LinkedHashMap<>();\n+\n+\t/**\n+\t * Construct a file system table sink.\n+\t *\n+\t * @param schema schema of the table.\n+\t * @param path directory path of the file system table.\n+\t * @param partitionKeys partition keys of the table.\n+\t * @param defaultPartName The default partition name in case the dynamic partition column value\n+\t *                        is null/empty string.\n+\t * @param formatFactory format factory to create reader.\n+\t */\n+\tpublic FileSystemTableSink(\n+\t\t\tTableSchema schema,\n+\t\t\tPath path,\n+\t\t\tList<String> partitionKeys,\n+\t\t\tString defaultPartName,\n+\t\t\tFileSystemFormatFactory formatFactory) {\n+\t\tthis.schema = schema;\n+\t\tthis.path = path;\n+\t\tthis.defaultPartName = defaultPartName;\n+\t\tthis.formatFactory = formatFactory;\n+\t\tthis.partitionKeys = partitionKeys;\n+\t}\n+\n+\t@Override\n+\tpublic final DataStreamSink<BaseRow> consumeDataStream(DataStream<BaseRow> dataStream) {\n+\t\tRowDataPartitionComputer computer = new RowDataPartitionComputer(\n+\t\t\t\tdefaultPartName,\n+\t\t\t\tschema.getFieldNames(),\n+\t\t\t\tschema.getFieldDataTypes(),\n+\t\t\t\tpartitionKeys.toArray(new String[0]));\n+\n+\t\tFileSystemOutputFormat.Builder<BaseRow> builder = new FileSystemOutputFormat.Builder<>();\n+\t\tbuilder.setPartitionComputer(computer);\n+\t\tbuilder.setDynamicGrouped(dynamicGrouping);\n+\t\tbuilder.setPartitionColumns(partitionKeys.toArray(new String[0]));\n+\t\tbuilder.setFormatFactory(createOutputFormatFactory(\n+\t\t\t\tformatFactory, this::getNonPartitionTypes));\n+\t\tbuilder.setMetaStoreFactory(createTableMetaStoreFactory(path));\n+\t\tbuilder.setOverwrite(overwrite);\n+\t\tbuilder.setStaticPartitions(staticPartitions);\n+\t\tbuilder.setTempPath(toStagingPath());\n+\t\treturn dataStream.writeUsingOutputFormat(builder.build())\n+\t\t\t\t.setParallelism(dataStream.getParallelism());\n+\t}\n+\n+\tprivate DataType[] getNonPartitionTypes() {\n+\t\treturn Arrays.stream(schema.getFieldNames())\n+\t\t\t\t.filter(name -> !partitionKeys.contains(name))\n+\t\t\t\t.map(name -> schema.getFieldDataType(name).get())\n+\t\t\t\t.toArray(DataType[]::new);\n+\t}\n+\n+\tprivate Path toStagingPath() {\n+\t\tPath stagingDir = new Path(path, \".staging_\" + System.currentTimeMillis());\n+\t\ttry {\n+\t\t\tFileSystem fs = stagingDir.getFileSystem();\n+\t\t\tPreconditions.checkState(\n+\t\t\t\t\tfs.exists(stagingDir) || fs.mkdirs(stagingDir),\n+\t\t\t\t\t\"Failed to create staging dir \" + stagingDir);\n+\t\t\treturn stagingDir;\n+\t\t} catch (IOException e) {\n+\t\t\tthrow new RuntimeException(e);\n+\t\t}\n+\t}\n+\n+\tprivate static OutputFormatFactory<BaseRow> createOutputFormatFactory(\n+\t\t\tFileSystemFormatFactory formatFactory,\n+\t\t\tFileSystemFormatFactory.WriterContext context) {\n+\t\tOptional<Encoder<BaseRow>> encoder = formatFactory.createEncoder(context);\n+\t\tOptional<BulkWriter.Factory<BaseRow>> bulk = formatFactory.createBulkWriterFactory(context);\n+\n+\t\tif (!encoder.isPresent() && !bulk.isPresent()) {\n+\t\t\tthrow new TableException(\n+\t\t\t\t\tformatFactory + \" format should implement at least one Encoder or BulkWriter\");\n+\t\t}\n+\t\treturn encoder\n+\t\t\t\t.<OutputFormatFactory<BaseRow>>map(en -> path -> createEncoderOutputFormat(en, path))\n+\t\t\t\t.orElseGet(() -> path -> createBulkWriterOutputFormat(bulk.get(), path));\n+\t}\n+\n+\tprivate static OutputFormat<BaseRow> createBulkWriterOutputFormat(\n+\t\t\tBulkWriter.Factory<BaseRow> factory,\n+\t\t\tPath path) {\n+\t\treturn new OutputFormat<BaseRow>() {\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd8a10137fa50eb11035082c95878fb6aa6a66c6"}, "originalPosition": 150}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg2OTUwMjU4", "url": "https://github.com/apache/flink/pull/11574#pullrequestreview-386950258", "createdAt": "2020-04-03T05:45:23Z", "commit": {"oid": "ad9d0b168f308a5205430b2dcf85e680f20449b3"}, "state": "APPROVED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QwNTo0NToyM1rOGAF0xA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QwNTo0NjozOVrOGAF1_w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjc0ODYxMg==", "bodyText": "Should we filter FORMAT.* properties here? otherwise we get all table connector properties.", "url": "https://github.com/apache/flink/pull/11574#discussion_r402748612", "createdAt": "2020-04-03T05:45:23Z", "author": {"login": "leonardBang"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableFactory.java", "diffHunk": "@@ -0,0 +1,143 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.configuration.ConfigOption;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.dataformat.BaseRow;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.TableFactory;\n+import org.apache.flink.table.factories.TableFactoryService;\n+import org.apache.flink.table.factories.TableSinkFactory;\n+import org.apache.flink.table.factories.TableSourceFactory;\n+import org.apache.flink.table.sinks.TableSink;\n+import org.apache.flink.table.sources.TableSource;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.apache.flink.configuration.ConfigOptions.key;\n+import static org.apache.flink.table.descriptors.ConnectorDescriptorValidator.CONNECTOR;\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+import static org.apache.flink.table.descriptors.Schema.SCHEMA;\n+\n+/**\n+ * File system {@link TableFactory}.\n+ *\n+ * <p>1.The partition information should be in the file system path, whether it's a temporary\n+ * table or a catalog table.\n+ * 2.Support insert into (append) and insert overwrite.\n+ * 3.Support static and dynamic partition inserting.\n+ *\n+ * <p>Migrate to new source/sink interface after FLIP-95 is ready.\n+ */\n+public class FileSystemTableFactory implements\n+\t\tTableSourceFactory<BaseRow>,\n+\t\tTableSinkFactory<BaseRow> {\n+\n+\tpublic static final String CONNECTOR_VALUE = \"filesystem\";\n+\n+\t/**\n+\t * Not use \"connector.path\" because:\n+\t * 1.Using \"connector.path\" will conflict with current batch csv source and batch csv sink.\n+\t * 2.This is compatible with FLIP-122.\n+\t */\n+\tpublic static final String PATH = \"path\";\n+\n+\t/**\n+\t * Move these properties to validator after FLINK-16904.\n+\t */\n+\tpublic static final ConfigOption<String> PARTITION_DEFAULT_NAME = key(\"partition.default-name\")\n+\t\t\t.stringType()\n+\t\t\t.defaultValue(\"__DEFAULT_PARTITION__\")\n+\t\t\t.withDescription(\"The default partition name in case the dynamic partition\" +\n+\t\t\t\t\t\" column value is null/empty string\");\n+\n+\t@Override\n+\tpublic Map<String, String> requiredContext() {\n+\t\tMap<String, String> context = new HashMap<>();\n+\t\tcontext.put(CONNECTOR, CONNECTOR_VALUE);\n+\t\treturn context;\n+\t}\n+\n+\t@Override\n+\tpublic List<String> supportedProperties() {\n+\t\tList<String> properties = new ArrayList<>();\n+\n+\t\t// path\n+\t\tproperties.add(PATH);\n+\n+\t\t// schema\n+\t\tproperties.add(SCHEMA + \".#.\" + DescriptorProperties.TABLE_SCHEMA_DATA_TYPE);\n+\t\tproperties.add(SCHEMA + \".#.\" + DescriptorProperties.TABLE_SCHEMA_NAME);\n+\n+\t\tproperties.add(PARTITION_DEFAULT_NAME.key());\n+\n+\t\t// format\n+\t\tproperties.add(FORMAT);\n+\t\tproperties.add(FORMAT + \".*\");\n+\n+\t\treturn properties;\n+\t}\n+\n+\t@Override\n+\tpublic TableSource<BaseRow> createTableSource(TableSourceFactory.Context context) {\n+\t\tDescriptorProperties properties = new DescriptorProperties();\n+\t\tproperties.putProperties(context.getTable().getProperties());\n+\n+\t\treturn new FileSystemTableSource(\n+\t\t\t\tcontext.getTable().getSchema(),\n+\t\t\t\tnew Path(properties.getString(PATH)),\n+\t\t\t\tcontext.getTable().getPartitionKeys(),\n+\t\t\t\tgetPartitionDefaultName(properties),\n+\t\t\t\tgetFormatProperties(context.getTable().getProperties()));\n+\t}\n+\n+\t@Override\n+\tpublic TableSink<BaseRow> createTableSink(TableSinkFactory.Context context) {\n+\t\tDescriptorProperties properties = new DescriptorProperties();\n+\t\tproperties.putProperties(context.getTable().getProperties());\n+\n+\t\treturn new FileSystemTableSink(\n+\t\t\t\tcontext.getTable().getSchema(),\n+\t\t\t\tnew Path(properties.getString(PATH)),\n+\t\t\t\tcontext.getTable().getPartitionKeys(),\n+\t\t\t\tgetPartitionDefaultName(properties),\n+\t\t\t\tgetFormatProperties(context.getTable().getProperties()));\n+\t}\n+\n+\tprivate static Map<String, String> getFormatProperties(Map<String, String> tableProperties) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ad9d0b168f308a5205430b2dcf85e680f20449b3"}, "originalPosition": 127}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjc0ODkyNw==", "bodyText": "how about name to bulkWriterFactory?", "url": "https://github.com/apache/flink/pull/11574#discussion_r402748927", "createdAt": "2020-04-03T05:46:39Z", "author": {"login": "leonardBang"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSink.java", "diffHunk": "@@ -0,0 +1,289 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.api.common.io.OutputFormat;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.fs.FSDataOutputStream;\n+import org.apache.flink.core.fs.FileSystem;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.DataStreamSink;\n+import org.apache.flink.table.api.TableException;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.dataformat.BaseRow;\n+import org.apache.flink.table.sinks.AppendStreamTableSink;\n+import org.apache.flink.table.sinks.OverwritableTableSink;\n+import org.apache.flink.table.sinks.PartitionableTableSink;\n+import org.apache.flink.table.sinks.TableSink;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+\n+import static org.apache.flink.table.filesystem.FileSystemTableFactory.createFormatFactory;\n+\n+/**\n+ * File system {@link TableSink}.\n+ */\n+public class FileSystemTableSink implements\n+\t\tAppendStreamTableSink<BaseRow>,\n+\t\tPartitionableTableSink,\n+\t\tOverwritableTableSink {\n+\n+\tprivate final TableSchema schema;\n+\tprivate final List<String> partitionKeys;\n+\tprivate final Path path;\n+\tprivate final String defaultPartName;\n+\tprivate final Map<String, String> formatProperties;\n+\n+\tprivate boolean overwrite = false;\n+\tprivate boolean dynamicGrouping = false;\n+\tprivate LinkedHashMap<String, String> staticPartitions = new LinkedHashMap<>();\n+\n+\t/**\n+\t * Construct a file system table sink.\n+\t *\n+\t * @param schema schema of the table.\n+\t * @param path directory path of the file system table.\n+\t * @param partitionKeys partition keys of the table.\n+\t * @param defaultPartName The default partition name in case the dynamic partition column value\n+\t *                        is null/empty string.\n+\t * @param formatProperties format properties.\n+\t */\n+\tpublic FileSystemTableSink(\n+\t\t\tTableSchema schema,\n+\t\t\tPath path,\n+\t\t\tList<String> partitionKeys,\n+\t\t\tString defaultPartName,\n+\t\t\tMap<String, String> formatProperties) {\n+\t\tthis.schema = schema;\n+\t\tthis.path = path;\n+\t\tthis.defaultPartName = defaultPartName;\n+\t\tthis.formatProperties = formatProperties;\n+\t\tthis.partitionKeys = partitionKeys;\n+\t}\n+\n+\t@Override\n+\tpublic final DataStreamSink<BaseRow> consumeDataStream(DataStream<BaseRow> dataStream) {\n+\t\tRowDataPartitionComputer computer = new RowDataPartitionComputer(\n+\t\t\t\tdefaultPartName,\n+\t\t\t\tschema.getFieldNames(),\n+\t\t\t\tschema.getFieldDataTypes(),\n+\t\t\t\tpartitionKeys.toArray(new String[0]));\n+\n+\t\tFileSystemOutputFormat.Builder<BaseRow> builder = new FileSystemOutputFormat.Builder<>();\n+\t\tbuilder.setPartitionComputer(computer);\n+\t\tbuilder.setDynamicGrouped(dynamicGrouping);\n+\t\tbuilder.setPartitionColumns(partitionKeys.toArray(new String[0]));\n+\t\tbuilder.setFormatFactory(createOutputFormatFactory());\n+\t\tbuilder.setMetaStoreFactory(createTableMetaStoreFactory(path));\n+\t\tbuilder.setOverwrite(overwrite);\n+\t\tbuilder.setStaticPartitions(staticPartitions);\n+\t\tbuilder.setTempPath(toStagingPath());\n+\t\treturn dataStream.writeUsingOutputFormat(builder.build())\n+\t\t\t\t.setParallelism(dataStream.getParallelism());\n+\t}\n+\n+\tprivate Path toStagingPath() {\n+\t\tPath stagingDir = new Path(path, \".staging_\" + System.currentTimeMillis());\n+\t\ttry {\n+\t\t\tFileSystem fs = stagingDir.getFileSystem();\n+\t\t\tPreconditions.checkState(\n+\t\t\t\t\tfs.exists(stagingDir) || fs.mkdirs(stagingDir),\n+\t\t\t\t\t\"Failed to create staging dir \" + stagingDir);\n+\t\t\treturn stagingDir;\n+\t\t} catch (IOException e) {\n+\t\t\tthrow new RuntimeException(e);\n+\t\t}\n+\t}\n+\n+\tprivate OutputFormatFactory<BaseRow> createOutputFormatFactory() {\n+\t\tFileSystemFormatFactory formatFactory = createFormatFactory(formatProperties);\n+\t\tFileSystemFormatFactory.WriterContext context = new FileSystemFormatFactory.WriterContext() {\n+\n+\t\t\t@Override\n+\t\t\tpublic TableSchema getSchema() {\n+\t\t\t\treturn schema;\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic Map<String, String> getFormatProperties() {\n+\t\t\t\treturn formatProperties;\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic List<String> getPartitionKeys() {\n+\t\t\t\treturn partitionKeys;\n+\t\t\t}\n+\t\t};\n+\n+\t\tOptional<Encoder<BaseRow>> encoder = formatFactory.createEncoder(context);\n+\t\tOptional<BulkWriter.Factory<BaseRow>> bulk = formatFactory.createBulkWriterFactory(context);\n+\n+\t\tif (!encoder.isPresent() && !bulk.isPresent()) {\n+\t\t\tthrow new TableException(\n+\t\t\t\t\tformatFactory + \" format should implement at least one Encoder or BulkWriter\");\n+\t\t}\n+\t\treturn encoder\n+\t\t\t\t.<OutputFormatFactory<BaseRow>>map(en -> path -> createEncoderOutputFormat(en, path))\n+\t\t\t\t.orElseGet(() -> {\n+\t\t\t\t\t// Optional is not serializable.\n+\t\t\t\t\tBulkWriter.Factory<BaseRow> bulkWriter = bulk.get();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ad9d0b168f308a5205430b2dcf85e680f20449b3"}, "originalPosition": 155}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "74f87e4c4a2cd7536d63b00f04635ceb8fd649b0", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/flink/commit/74f87e4c4a2cd7536d63b00f04635ceb8fd649b0", "committedDate": "2020-04-07T03:12:04Z", "message": "[FLINK-16859][table-runtime] Introduce FileSystemTableFactory"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "23b9b078c98c858127cb12564567b2cd739fa357", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/flink/commit/23b9b078c98c858127cb12564567b2cd739fa357", "committedDate": "2020-04-07T03:12:04Z", "message": "Fix keys"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "00e45724fcc6d83e5019a3c6aa1d41c040e02989", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/flink/commit/00e45724fcc6d83e5019a3c6aa1d41c040e02989", "committedDate": "2020-04-07T03:12:04Z", "message": "Fix comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cdcee42ce8162ad2b98384e5acd862d9154e66fe", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/flink/commit/cdcee42ce8162ad2b98384e5acd862d9154e66fe", "committedDate": "2020-04-07T03:12:05Z", "message": "Add getFormatProperties to context"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c1caed477e925f4af664db5e251a11e293d05659", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/flink/commit/c1caed477e925f4af664db5e251a11e293d05659", "committedDate": "2020-04-07T03:12:05Z", "message": "enhance WriterContext"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "716a90009e5fd994ebb3f1fed729a20b49398e01", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/flink/commit/716a90009e5fd994ebb3f1fed729a20b49398e01", "committedDate": "2020-04-07T03:12:05Z", "message": "ITCase use array instead of scala seq"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ff086f2fa377e6950aca8a959c0cae50ab0558c6", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/flink/commit/ff086f2fa377e6950aca8a959c0cae50ab0558c6", "committedDate": "2020-04-07T03:12:05Z", "message": "Add bulk writer test and fix bug"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a2008dc6560833b9df4a4faae6d2b783803a9c2a", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/flink/commit/a2008dc6560833b9df4a4faae6d2b783803a9c2a", "committedDate": "2020-04-07T03:12:05Z", "message": "Fix comments"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "756a68346b01c34e41fcb0b8d122e1a94207491e", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/flink/commit/756a68346b01c34e41fcb0b8d122e1a94207491e", "committedDate": "2020-04-03T06:15:57Z", "message": "Fix comments"}, "afterCommit": {"oid": "a2008dc6560833b9df4a4faae6d2b783803a9c2a", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/flink/commit/a2008dc6560833b9df4a4faae6d2b783803a9c2a", "committedDate": "2020-04-07T03:12:05Z", "message": "Fix comments"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2182, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}