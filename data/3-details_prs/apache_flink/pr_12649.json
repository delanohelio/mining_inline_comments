{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDM0MzMwMjU5", "number": 12649, "title": "[FLINK-18286] Implement type inference for GET/FLATTEN ", "bodyText": "What is the purpose of the change\nThis PR implements type inference for GET/FLATTEN expressions. Additionally it aligns the behaviour between Table API & SQL. To achieve that it fixes handling nullability of nested columns in operators such as DOT, ITEM and nested fields coming directly from a Table.\nVerifying this change\nAdded tests in\n\nInputTypeStrategiesTest for COMPOSITE strategy\nTypeStrategiesTest for GET strategy\nCompositeTypeAccessExpressionITCase IT tests for accessing nested fields in both SQL & Table API\n\nDoes this pull request potentially affect one of the following parts:\n\nDependencies (does it add or upgrade a dependency): (yes / no)\nThe public API, i.e., is any changed class annotated with @Public(Evolving): (yes / no)\nThe serializers: (yes / no / don't know)\nThe runtime per-record code paths (performance sensitive): (yes / no / don't know)\nAnything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (yes / no / don't know)\nThe S3 file system connector: (yes / no / don't know)\n\nDocumentation\n\nDoes this pull request introduce a new feature? (yes / no)\nIf yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)", "createdAt": "2020-06-15T06:57:23Z", "url": "https://github.com/apache/flink/pull/12649", "merged": true, "mergeCommit": {"oid": "46579c31f945d19c2930d628ba8269912ce466ff"}, "closed": true, "closedAt": "2020-07-17T08:50:03Z", "author": {"login": "dawidwys"}, "timelineItems": {"totalCount": 12, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcrzhaogFqTQzMDc0MzY2MA==", "endCursor": "Y3Vyc29yOnYyOpPPAAABc1eKmgABqjM1NTI4OTc0NzY=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMwNzQzNjYw", "url": "https://github.com/apache/flink/pull/12649#pullrequestreview-430743660", "createdAt": "2020-06-15T15:15:38Z", "commit": {"oid": "ba5bc62416b82d8e6f1be2c848242de2d3c546e5"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNVQxNToxNTozOFrOGj2yRQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQxMToyMjozMlrOGkW3Ig==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDI1MDk0OQ==", "bodyText": "remove false? the nullability is already excluded by LITERAL", "url": "https://github.com/apache/flink/pull/12649#discussion_r440250949", "createdAt": "2020-06-15T15:15:38Z", "author": {"login": "twalthr"}, "path": "flink-table/flink-table-common/src/main/java/org/apache/flink/table/functions/BuiltInFunctionDefinitions.java", "diffHunk": "@@ -972,13 +972,28 @@\n \t\tnew BuiltInFunctionDefinition.Builder()\n \t\t\t.name(\"flatten\")\n \t\t\t.kind(OTHER)\n-\t\t\t.outputTypeStrategy(TypeStrategies.MISSING)\n+\t\t\t.inputTypeStrategy(sequence(InputTypeStrategies.COMPOSITE))\n+\t\t\t.outputTypeStrategy(callContext -> {\n+\t\t\t\tthrow new UnsupportedOperationException(\"FLATTEN should be resolved to GET expressions\");\n+\t\t\t})\n \t\t\t.build();\n \tpublic static final BuiltInFunctionDefinition GET =\n \t\tnew BuiltInFunctionDefinition.Builder()\n \t\t\t.name(\"get\")\n \t\t\t.kind(OTHER)\n-\t\t\t.outputTypeStrategy(TypeStrategies.MISSING)\n+\t\t\t.inputTypeStrategy(\n+\t\t\t\tsequence(\n+\t\t\t\t\tInputTypeStrategies.COMPOSITE,\n+\t\t\t\t\tand(\n+\t\t\t\t\t\tInputTypeStrategies.LITERAL,\n+\t\t\t\t\t\tor(\n+\t\t\t\t\t\t\tlogical(LogicalTypeRoot.INTEGER),\n+\t\t\t\t\t\t\tlogical(LogicalTypeFamily.CHARACTER_STRING, false)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ba5bc62416b82d8e6f1be2c848242de2d3c546e5"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDI1MjM2Mg==", "bodyText": "nit: move to separate class like all other argument strategies", "url": "https://github.com/apache/flink/pull/12649#discussion_r440252362", "createdAt": "2020-06-15T15:17:36Z", "author": {"login": "twalthr"}, "path": "flink-table/flink-table-common/src/main/java/org/apache/flink/table/types/inference/InputTypeStrategies.java", "diffHunk": "@@ -198,6 +201,36 @@ public static InputTypeStrategy comparable(\n \t */\n \tpublic static final LiteralArgumentTypeStrategy LITERAL_OR_NULL = new LiteralArgumentTypeStrategy(true);\n \n+\t/**\n+\t * Strategy that checks that the argument has a composite type.\n+\t */\n+\tpublic static final ArgumentTypeStrategy COMPOSITE = new ArgumentTypeStrategy() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ba5bc62416b82d8e6f1be2c848242de2d3c546e5"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDI1MzQ5MA==", "bodyText": "nit: wrong indention", "url": "https://github.com/apache/flink/pull/12649#discussion_r440253490", "createdAt": "2020-06-15T15:19:11Z", "author": {"login": "twalthr"}, "path": "flink-table/flink-table-common/src/main/java/org/apache/flink/table/types/inference/InputTypeStrategies.java", "diffHunk": "@@ -198,6 +201,36 @@ public static InputTypeStrategy comparable(\n \t */\n \tpublic static final LiteralArgumentTypeStrategy LITERAL_OR_NULL = new LiteralArgumentTypeStrategy(true);\n \n+\t/**\n+\t * Strategy that checks that the argument has a composite type.\n+\t */\n+\tpublic static final ArgumentTypeStrategy COMPOSITE = new ArgumentTypeStrategy() {\n+\t\t@Override\n+\t\tpublic Optional<DataType> inferArgumentType(\n+\t\t\t\tCallContext callContext,\n+\t\t\t\tint argumentPos,\n+\t\t\t\tboolean throwOnFailure) {\n+\t\t\t\tDataType dataType = callContext.getArgumentDataTypes().get(argumentPos);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ba5bc62416b82d8e6f1be2c848242de2d3c546e5"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDI1OTA5Nw==", "bodyText": "I don't like that we use TableSchema here. This class doesn't belong here. Furthermore, it does not support distinct type yet. Maybe we can just introduce a DataTypeUtils.getFieldType(int) and DataTypeUtils.getFieldType(name). Can we make it support distinct type (an AtomicDataType)?", "url": "https://github.com/apache/flink/pull/12649#discussion_r440259097", "createdAt": "2020-06-15T15:27:02Z", "author": {"login": "twalthr"}, "path": "flink-table/flink-table-common/src/main/java/org/apache/flink/table/types/inference/TypeStrategies.java", "diffHunk": "@@ -315,6 +317,36 @@ public static TypeStrategy nullable(TypeStrategy initialStrategy) {\n \t\treturn Optional.of(fromLogicalToDataType(inferredType));\n \t};\n \n+\t/**\n+\t * Type strategy that returns a type of a field nested inside a composite type that is described by the second argument.\n+\t * The second argument must be a literal that describes either the nested field name or index.\n+\t */\n+\tpublic static final TypeStrategy GET = callContext -> {\n+\t\tList<DataType> argumentDataTypes = callContext.getArgumentDataTypes();\n+\t\tDataType rowDataType = argumentDataTypes.get(0);\n+\t\tTableSchema nestedSchema = DataTypeUtils.expandCompositeTypeToSchema(rowDataType);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ba5bc62416b82d8e6f1be2c848242de2d3c546e5"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDI2MjI0Nw==", "bodyText": "shall we upgrade the test base to accept AbstractDataType similar to DataStructureConverterTest? it would make the code more readable.", "url": "https://github.com/apache/flink/pull/12649#discussion_r440262247", "createdAt": "2020-06-15T15:31:30Z", "author": {"login": "twalthr"}, "path": "flink-table/flink-table-common/src/test/java/org/apache/flink/table/types/inference/InputTypeStrategiesTest.java", "diffHunk": "@@ -582,7 +587,43 @@\n \t\t\t\t\t\t\t\t\"My constraint says %s must be nullable.\",\n \t\t\t\t\t\t\t\targs -> args.get(0).getLogicalType().isNullable()))))\n \t\t\t\t.calledWithArgumentTypes(DataTypes.BOOLEAN().notNull())\n-\t\t\t\t.expectErrorMessage(\"My constraint says BOOLEAN NOT NULL must be nullable.\")\n+\t\t\t\t.expectErrorMessage(\"My constraint says BOOLEAN NOT NULL must be nullable.\"),\n+\n+\t\t\tTestSpec\n+\t\t\t\t.forStrategy(\n+\t\t\t\t\t\"Composite type strategy with ROW\",\n+\t\t\t\t\tsequence(InputTypeStrategies.COMPOSITE)\n+\t\t\t\t)\n+\t\t\t\t.calledWithArgumentTypes(DataTypes.ROW(DataTypes.FIELD(\"f0\", DataTypes.BIGINT())))\n+\t\t\t\t.expectSignature(\"f(<COMPOSITE>)\")\n+\t\t\t\t.expectArgumentTypes(DataTypes.ROW(DataTypes.FIELD(\"f0\", DataTypes.BIGINT()))),\n+\n+\t\t\tTestSpec\n+\t\t\t\t.forStrategy(\n+\t\t\t\t\t\"Composite type strategy with STRUCTURED type\",\n+\t\t\t\t\tsequence(InputTypeStrategies.COMPOSITE)\n+\t\t\t\t)\n+\t\t\t\t.calledWithArgumentTypes(new FieldsDataType(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ba5bc62416b82d8e6f1be2c848242de2d3c546e5"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDc2OTg5NA==", "bodyText": "use LogicalTypeChecks instead", "url": "https://github.com/apache/flink/pull/12649#discussion_r440769894", "createdAt": "2020-06-16T11:08:58Z", "author": {"login": "twalthr"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/codegen/ExprCodeGenerator.scala", "diffHunk": "@@ -724,6 +724,9 @@ class ExprCodeGenerator(ctx: CodeGeneratorContext, nullableInput: Boolean)\n             val key = operands(1)\n             generateMapGet(ctx, operands.head, key)\n \n+          case t: LogicalType if TypeCheckUtils.isRow(t) =>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b3dd92d0c41cee2472bb521eec89741c42bc05cd"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDc3MTcwMA==", "bodyText": "use the newly introduced testResult such that a result must be defined only once", "url": "https://github.com/apache/flink/pull/12649#discussion_r440771700", "createdAt": "2020-06-16T11:12:43Z", "author": {"login": "twalthr"}, "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/expressions/CompositeTypeAccessExpressionITCase.java", "diffHunk": "@@ -0,0 +1,264 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.expressions;\n+\n+import org.apache.flink.table.annotation.DataTypeHint;\n+import org.apache.flink.table.annotation.FunctionHint;\n+import org.apache.flink.table.api.DataTypes;\n+import org.apache.flink.table.api.EnvironmentSettings;\n+import org.apache.flink.table.api.TableEnvironment;\n+import org.apache.flink.table.api.TableResult;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.api.ValidationException;\n+import org.apache.flink.table.functions.BuiltInFunctionDefinitions;\n+import org.apache.flink.table.functions.ScalarFunction;\n+import org.apache.flink.table.functions.TableFunction;\n+import org.apache.flink.table.types.logical.LogicalTypeRoot;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.util.CloseableIterator;\n+\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.ExpectedException;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+import org.junit.runners.Suite;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+\n+import static java.util.Collections.singletonMap;\n+import static org.apache.flink.table.api.DataTypes.ARRAY;\n+import static org.apache.flink.table.api.DataTypes.BIGINT;\n+import static org.apache.flink.table.api.DataTypes.FIELD;\n+import static org.apache.flink.table.api.DataTypes.MAP;\n+import static org.apache.flink.table.api.DataTypes.ROW;\n+import static org.apache.flink.table.api.DataTypes.STRING;\n+import static org.apache.flink.table.api.Expressions.$;\n+import static org.apache.flink.table.api.Expressions.call;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertThat;\n+\n+/**\n+ * Tests for functions that access nested fields/elements of composite/collection types.\n+ */\n+@RunWith(Suite.class)\n+@Suite.SuiteClasses(\n+\t{\n+\t\tCompositeTypeAccessExpressionITCase.TableFieldAccess.class,\n+\t\tCompositeTypeAccessExpressionITCase.CallFieldAccess.class\n+\t}\n+)\n+public class CompositeTypeAccessExpressionITCase {\n+\n+\t/**\n+\t * Regular tests. See also {@link CallFieldAccess} for tests that access a nested field of an expression or\n+\t * for {@link BuiltInFunctionDefinitions#FLATTEN} which produces multiple columns from a single one.\n+\t */\n+\tpublic static class TableFieldAccess extends BuiltInFunctionTestBase {\n+\t\t@Parameterized.Parameters(name = \"{index}: {0}\")\n+\t\tpublic static List<TestSpec> testData() {\n+\t\t\treturn Arrays.asList(\n+\n+\t\t\t\t// Actually in case of SQL it does not use the GET method, but\n+\t\t\t\t// a custom logic for accessing nested fields of a Table.\n+\t\t\t\tTestSpec.forFunction(BuiltInFunctionDefinitions.GET)\n+\t\t\t\t\t.onFieldsWithData(null, Row.of(1))\n+\t\t\t\t\t.andDataTypes(\n+\t\t\t\t\t\tROW(FIELD(\"nested\", BIGINT().notNull())).nullable(),\n+\t\t\t\t\t\tROW(FIELD(\"nested\", BIGINT().notNull())).notNull()\n+\t\t\t\t\t)\n+\t\t\t\t\t.testTableApiResult($(\"f0\").get(\"nested\"), null, BIGINT().nullable())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b3dd92d0c41cee2472bb521eec89741c42bc05cd"}, "originalPosition": 88}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDc3MjgyNg==", "bodyText": "should we port AT as well or in a separate PR?", "url": "https://github.com/apache/flink/pull/12649#discussion_r440772826", "createdAt": "2020-06-16T11:15:04Z", "author": {"login": "twalthr"}, "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/expressions/CompositeTypeAccessExpressionITCase.java", "diffHunk": "@@ -0,0 +1,264 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.expressions;\n+\n+import org.apache.flink.table.annotation.DataTypeHint;\n+import org.apache.flink.table.annotation.FunctionHint;\n+import org.apache.flink.table.api.DataTypes;\n+import org.apache.flink.table.api.EnvironmentSettings;\n+import org.apache.flink.table.api.TableEnvironment;\n+import org.apache.flink.table.api.TableResult;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.api.ValidationException;\n+import org.apache.flink.table.functions.BuiltInFunctionDefinitions;\n+import org.apache.flink.table.functions.ScalarFunction;\n+import org.apache.flink.table.functions.TableFunction;\n+import org.apache.flink.table.types.logical.LogicalTypeRoot;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.util.CloseableIterator;\n+\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.ExpectedException;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+import org.junit.runners.Suite;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+\n+import static java.util.Collections.singletonMap;\n+import static org.apache.flink.table.api.DataTypes.ARRAY;\n+import static org.apache.flink.table.api.DataTypes.BIGINT;\n+import static org.apache.flink.table.api.DataTypes.FIELD;\n+import static org.apache.flink.table.api.DataTypes.MAP;\n+import static org.apache.flink.table.api.DataTypes.ROW;\n+import static org.apache.flink.table.api.DataTypes.STRING;\n+import static org.apache.flink.table.api.Expressions.$;\n+import static org.apache.flink.table.api.Expressions.call;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertThat;\n+\n+/**\n+ * Tests for functions that access nested fields/elements of composite/collection types.\n+ */\n+@RunWith(Suite.class)\n+@Suite.SuiteClasses(\n+\t{\n+\t\tCompositeTypeAccessExpressionITCase.TableFieldAccess.class,\n+\t\tCompositeTypeAccessExpressionITCase.CallFieldAccess.class\n+\t}\n+)\n+public class CompositeTypeAccessExpressionITCase {\n+\n+\t/**\n+\t * Regular tests. See also {@link CallFieldAccess} for tests that access a nested field of an expression or\n+\t * for {@link BuiltInFunctionDefinitions#FLATTEN} which produces multiple columns from a single one.\n+\t */\n+\tpublic static class TableFieldAccess extends BuiltInFunctionTestBase {\n+\t\t@Parameterized.Parameters(name = \"{index}: {0}\")\n+\t\tpublic static List<TestSpec> testData() {\n+\t\t\treturn Arrays.asList(\n+\n+\t\t\t\t// Actually in case of SQL it does not use the GET method, but\n+\t\t\t\t// a custom logic for accessing nested fields of a Table.\n+\t\t\t\tTestSpec.forFunction(BuiltInFunctionDefinitions.GET)\n+\t\t\t\t\t.onFieldsWithData(null, Row.of(1))\n+\t\t\t\t\t.andDataTypes(\n+\t\t\t\t\t\tROW(FIELD(\"nested\", BIGINT().notNull())).nullable(),\n+\t\t\t\t\t\tROW(FIELD(\"nested\", BIGINT().notNull())).notNull()\n+\t\t\t\t\t)\n+\t\t\t\t\t.testTableApiResult($(\"f0\").get(\"nested\"), null, BIGINT().nullable())\n+\t\t\t\t\t.testTableApiResult($(\"f1\").get(\"nested\"), 1L, BIGINT().notNull())\n+\t\t\t\t\t.testSqlResult(\"f0.nested\", null, BIGINT().nullable())\n+\t\t\t\t\t.testSqlResult(\"f1.nested\", 1L, BIGINT().notNull()),\n+\n+\t\t\t\t// In Calcite it maps to FlinkSqlOperatorTable.ITEM\n+\t\t\t\tTestSpec.forFunction(BuiltInFunctionDefinitions.AT)\n+\t\t\t\t\t.onFieldsWithData(null, new int[] {1}, null, singletonMap(\"nested\", 1), null, Row.of(1))\n+\t\t\t\t\t.andDataTypes(\n+\t\t\t\t\t\tARRAY(BIGINT().notNull()).nullable(),\n+\t\t\t\t\t\tARRAY(BIGINT().notNull()).notNull(),\n+\t\t\t\t\t\tMAP(STRING(), BIGINT().notNull()).nullable(),\n+\t\t\t\t\t\tMAP(STRING(), BIGINT().notNull()).notNull(),\n+\t\t\t\t\t\tROW(FIELD(\"nested\", BIGINT().notNull())).nullable(),\n+\t\t\t\t\t\tROW(FIELD(\"nested\", BIGINT().notNull())).notNull()\n+\t\t\t\t\t)\n+\t\t\t\t\t// accessing elements of MAP or ARRAY is a runtime operations,\n+\t\t\t\t\t// we do not know about the size or contents during the inference\n+\t\t\t\t\t// therefore the results are always nullable\n+\t\t\t\t\t.testSqlResult(\"f0[1]\", null, BIGINT().nullable())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b3dd92d0c41cee2472bb521eec89741c42bc05cd"}, "originalPosition": 107}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDc3NjQ4Mg==", "bodyText": "nit: very good tests in general but can we rename the two suits to FieldAccessAfterCall and FieldAccessFromTable or similar to highlight the difference?", "url": "https://github.com/apache/flink/pull/12649#discussion_r440776482", "createdAt": "2020-06-16T11:22:32Z", "author": {"login": "twalthr"}, "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/expressions/CompositeTypeAccessExpressionITCase.java", "diffHunk": "@@ -0,0 +1,264 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.expressions;\n+\n+import org.apache.flink.table.annotation.DataTypeHint;\n+import org.apache.flink.table.annotation.FunctionHint;\n+import org.apache.flink.table.api.DataTypes;\n+import org.apache.flink.table.api.EnvironmentSettings;\n+import org.apache.flink.table.api.TableEnvironment;\n+import org.apache.flink.table.api.TableResult;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.api.ValidationException;\n+import org.apache.flink.table.functions.BuiltInFunctionDefinitions;\n+import org.apache.flink.table.functions.ScalarFunction;\n+import org.apache.flink.table.functions.TableFunction;\n+import org.apache.flink.table.types.logical.LogicalTypeRoot;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.util.CloseableIterator;\n+\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.ExpectedException;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+import org.junit.runners.Suite;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+\n+import static java.util.Collections.singletonMap;\n+import static org.apache.flink.table.api.DataTypes.ARRAY;\n+import static org.apache.flink.table.api.DataTypes.BIGINT;\n+import static org.apache.flink.table.api.DataTypes.FIELD;\n+import static org.apache.flink.table.api.DataTypes.MAP;\n+import static org.apache.flink.table.api.DataTypes.ROW;\n+import static org.apache.flink.table.api.DataTypes.STRING;\n+import static org.apache.flink.table.api.Expressions.$;\n+import static org.apache.flink.table.api.Expressions.call;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertThat;\n+\n+/**\n+ * Tests for functions that access nested fields/elements of composite/collection types.\n+ */\n+@RunWith(Suite.class)\n+@Suite.SuiteClasses(\n+\t{\n+\t\tCompositeTypeAccessExpressionITCase.TableFieldAccess.class,\n+\t\tCompositeTypeAccessExpressionITCase.CallFieldAccess.class\n+\t}\n+)\n+public class CompositeTypeAccessExpressionITCase {\n+\n+\t/**\n+\t * Regular tests. See also {@link CallFieldAccess} for tests that access a nested field of an expression or\n+\t * for {@link BuiltInFunctionDefinitions#FLATTEN} which produces multiple columns from a single one.\n+\t */\n+\tpublic static class TableFieldAccess extends BuiltInFunctionTestBase {\n+\t\t@Parameterized.Parameters(name = \"{index}: {0}\")\n+\t\tpublic static List<TestSpec> testData() {\n+\t\t\treturn Arrays.asList(\n+\n+\t\t\t\t// Actually in case of SQL it does not use the GET method, but\n+\t\t\t\t// a custom logic for accessing nested fields of a Table.\n+\t\t\t\tTestSpec.forFunction(BuiltInFunctionDefinitions.GET)\n+\t\t\t\t\t.onFieldsWithData(null, Row.of(1))\n+\t\t\t\t\t.andDataTypes(\n+\t\t\t\t\t\tROW(FIELD(\"nested\", BIGINT().notNull())).nullable(),\n+\t\t\t\t\t\tROW(FIELD(\"nested\", BIGINT().notNull())).notNull()\n+\t\t\t\t\t)\n+\t\t\t\t\t.testTableApiResult($(\"f0\").get(\"nested\"), null, BIGINT().nullable())\n+\t\t\t\t\t.testTableApiResult($(\"f1\").get(\"nested\"), 1L, BIGINT().notNull())\n+\t\t\t\t\t.testSqlResult(\"f0.nested\", null, BIGINT().nullable())\n+\t\t\t\t\t.testSqlResult(\"f1.nested\", 1L, BIGINT().notNull()),\n+\n+\t\t\t\t// In Calcite it maps to FlinkSqlOperatorTable.ITEM\n+\t\t\t\tTestSpec.forFunction(BuiltInFunctionDefinitions.AT)\n+\t\t\t\t\t.onFieldsWithData(null, new int[] {1}, null, singletonMap(\"nested\", 1), null, Row.of(1))\n+\t\t\t\t\t.andDataTypes(\n+\t\t\t\t\t\tARRAY(BIGINT().notNull()).nullable(),\n+\t\t\t\t\t\tARRAY(BIGINT().notNull()).notNull(),\n+\t\t\t\t\t\tMAP(STRING(), BIGINT().notNull()).nullable(),\n+\t\t\t\t\t\tMAP(STRING(), BIGINT().notNull()).notNull(),\n+\t\t\t\t\t\tROW(FIELD(\"nested\", BIGINT().notNull())).nullable(),\n+\t\t\t\t\t\tROW(FIELD(\"nested\", BIGINT().notNull())).notNull()\n+\t\t\t\t\t)\n+\t\t\t\t\t// accessing elements of MAP or ARRAY is a runtime operations,\n+\t\t\t\t\t// we do not know about the size or contents during the inference\n+\t\t\t\t\t// therefore the results are always nullable\n+\t\t\t\t\t.testSqlResult(\"f0[1]\", null, BIGINT().nullable())\n+\t\t\t\t\t.testSqlResult(\"f1[1]\", 1L, BIGINT().nullable())\n+\t\t\t\t\t.testSqlResult(\"f2['nested']\", null, BIGINT().nullable())\n+\t\t\t\t\t.testSqlResult(\"f3['nested']\", 1L, BIGINT().nullable())\n+\n+\t\t\t\t\t// we know all the fields of a type up front, therefore we can\n+\t\t\t\t\t// derive more accurate types during the inference\n+\t\t\t\t\t.testSqlResult(\"f4['nested']\", null, BIGINT().nullable())\n+\t\t\t\t\t.testSqlResult(\"f5['nested']\", 1L, BIGINT().notNull())\n+\t\t\t);\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * A class for customized tests.\n+\t */\n+\tpublic static class CallFieldAccess {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b3dd92d0c41cee2472bb521eec89741c42bc05cd"}, "originalPosition": 123}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "2254b6c88195101b313721453d07847cf8c44cb2", "author": {"user": {"login": "dawidwys", "name": "Dawid Wysakowicz"}}, "url": "https://github.com/apache/flink/commit/2254b6c88195101b313721453d07847cf8c44cb2", "committedDate": "2020-06-16T14:29:54Z", "message": "Comments addressed"}, "afterCommit": {"oid": "326222e8eb85270297876eba22ffda57ebe592c3", "author": {"user": {"login": "dawidwys", "name": "Dawid Wysakowicz"}}, "url": "https://github.com/apache/flink/commit/326222e8eb85270297876eba22ffda57ebe592c3", "committedDate": "2020-06-16T14:32:47Z", "message": "Comments addressed"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "326222e8eb85270297876eba22ffda57ebe592c3", "author": {"user": {"login": "dawidwys", "name": "Dawid Wysakowicz"}}, "url": "https://github.com/apache/flink/commit/326222e8eb85270297876eba22ffda57ebe592c3", "committedDate": "2020-06-16T14:32:47Z", "message": "Comments addressed"}, "afterCommit": {"oid": "67905cf92819be26e49afbcf38794e340cca8e9c", "author": {"user": {"login": "dawidwys", "name": "Dawid Wysakowicz"}}, "url": "https://github.com/apache/flink/commit/67905cf92819be26e49afbcf38794e340cca8e9c", "committedDate": "2020-06-17T07:16:46Z", "message": "Comments addressed"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "67905cf92819be26e49afbcf38794e340cca8e9c", "author": {"user": {"login": "dawidwys", "name": "Dawid Wysakowicz"}}, "url": "https://github.com/apache/flink/commit/67905cf92819be26e49afbcf38794e340cca8e9c", "committedDate": "2020-06-17T07:16:46Z", "message": "Comments addressed"}, "afterCommit": {"oid": "4588093f97836f4f04c7c2c9992ea5049bf71c4c", "author": {"user": {"login": "dawidwys", "name": "Dawid Wysakowicz"}}, "url": "https://github.com/apache/flink/commit/4588093f97836f4f04c7c2c9992ea5049bf71c4c", "committedDate": "2020-06-17T08:59:37Z", "message": "Comments addressed"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "4588093f97836f4f04c7c2c9992ea5049bf71c4c", "author": {"user": {"login": "dawidwys", "name": "Dawid Wysakowicz"}}, "url": "https://github.com/apache/flink/commit/4588093f97836f4f04c7c2c9992ea5049bf71c4c", "committedDate": "2020-06-17T08:59:37Z", "message": "Comments addressed"}, "afterCommit": {"oid": "43596996d023eb47c9589f95d8baf8a3cee83016", "author": {"user": {"login": "dawidwys", "name": "Dawid Wysakowicz"}}, "url": "https://github.com/apache/flink/commit/43596996d023eb47c9589f95d8baf8a3cee83016", "committedDate": "2020-06-18T09:16:36Z", "message": "[FLINK-18286] Implement type inference for GET/FLATTEN"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM0Njc2MTc0", "url": "https://github.com/apache/flink/pull/12649#pullrequestreview-434676174", "createdAt": "2020-06-22T07:44:13Z", "commit": {"oid": "43596996d023eb47c9589f95d8baf8a3cee83016"}, "state": "APPROVED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yMlQwNzo0NDoxM1rOGm1VPA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yMlQwNzo0NDoxM1rOGm1VPA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzM3Mjg2MA==", "bodyText": "LogicalTypeRoot.ROW | LogicalTypeRoot.STRUCTURED_TYPE", "url": "https://github.com/apache/flink/pull/12649#discussion_r443372860", "createdAt": "2020-06-22T07:44:13Z", "author": {"login": "twalthr"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/codegen/ExprCodeGenerator.scala", "diffHunk": "@@ -719,18 +721,18 @@ class ExprCodeGenerator(ctx: CodeGeneratorContext, nullableInput: Boolean)\n         generateMap(ctx, resultType, operands)\n \n       case ITEM =>\n-        operands.head.resultType match {\n-          case t: LogicalType if TypeCheckUtils.isArray(t) =>\n+        operands.head.resultType.getTypeRoot match {\n+          case LogicalTypeRoot.ARRAY =>\n             val array = operands.head\n             val index = operands(1)\n             requireInteger(index)\n             generateArrayElementAt(ctx, array, index)\n \n-          case t: LogicalType if TypeCheckUtils.isMap(t) =>\n+          case LogicalTypeRoot.MAP =>\n             val key = operands(1)\n             generateMapGet(ctx, operands.head, key)\n \n-          case t: LogicalType if TypeCheckUtils.isRow(t) =>\n+          case LogicalTypeRoot.ROW =>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43596996d023eb47c9589f95d8baf8a3cee83016"}, "originalPosition": 35}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM0ODgyMTgy", "url": "https://github.com/apache/flink/pull/12649#pullrequestreview-434882182", "createdAt": "2020-06-22T12:39:35Z", "commit": {"oid": "43596996d023eb47c9589f95d8baf8a3cee83016"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yMlQxMjozOTozNlrOGm-0WA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yMlQxMjozOTozNlrOGm-0WA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzUyODI4MA==", "bodyText": "In Calcite, when the record type is nullable, the attributes are always nullable, so, this fix is not that necessary from the Calcite side.", "url": "https://github.com/apache/flink/pull/12649#discussion_r443528280", "createdAt": "2020-06-22T12:39:36Z", "author": {"login": "danny0405"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/calcite/sql/fun/SqlDotOperator.java", "diffHunk": "@@ -0,0 +1,194 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to you under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.calcite.sql.fun;\n+\n+import org.apache.calcite.rel.type.RelDataType;\n+import org.apache.calcite.rel.type.RelDataTypeFactory;\n+import org.apache.calcite.rel.type.RelDataTypeField;\n+import org.apache.calcite.sql.SqlCall;\n+import org.apache.calcite.sql.SqlCallBinding;\n+import org.apache.calcite.sql.SqlKind;\n+import org.apache.calcite.sql.SqlNode;\n+import org.apache.calcite.sql.SqlOperandCountRange;\n+import org.apache.calcite.sql.SqlOperatorBinding;\n+import org.apache.calcite.sql.SqlSpecialOperator;\n+import org.apache.calcite.sql.SqlUtil;\n+import org.apache.calcite.sql.SqlWriter;\n+import org.apache.calcite.sql.parser.SqlParserPos;\n+import org.apache.calcite.sql.type.OperandTypes;\n+import org.apache.calcite.sql.type.SqlOperandCountRanges;\n+import org.apache.calcite.sql.type.SqlSingleOperandTypeChecker;\n+import org.apache.calcite.sql.type.SqlTypeFamily;\n+import org.apache.calcite.sql.type.SqlTypeName;\n+import org.apache.calcite.sql.util.SqlBasicVisitor;\n+import org.apache.calcite.sql.util.SqlVisitor;\n+import org.apache.calcite.sql.validate.SqlValidator;\n+import org.apache.calcite.sql.validate.SqlValidatorScope;\n+import org.apache.calcite.sql.validate.SqlValidatorUtil;\n+import org.apache.calcite.util.Litmus;\n+import org.apache.calcite.util.Static;\n+\n+import java.util.Arrays;\n+\n+/**\n+ * The dot operator {@code .}, used to access a field of a\n+ * record. For example, {@code a.b}.\n+ *\n+ * <p>This class was copied over from Calcite to fix the derived type.\n+ * If the ROW is nullable force the accessed field to be nullable as well.\n+ */\n+public class SqlDotOperator extends SqlSpecialOperator {\n+  SqlDotOperator() {\n+    super(\"DOT\", SqlKind.DOT, 100, true, null, null, null);\n+  }\n+\n+  @Override public ReduceResult reduceExpr(int ordinal, TokenSequence list) {\n+    SqlNode left = list.node(ordinal - 1);\n+    SqlNode right = list.node(ordinal + 1);\n+    return new ReduceResult(ordinal - 1,\n+        ordinal + 2,\n+        createCall(\n+            SqlParserPos.sum(\n+                Arrays.asList(left.getParserPosition(),\n+                    right.getParserPosition(),\n+                    list.pos(ordinal))),\n+            left,\n+            right));\n+  }\n+\n+  @Override public void unparse(SqlWriter writer, SqlCall call, int leftPrec,\n+      int rightPrec) {\n+    final SqlWriter.Frame frame =\n+        writer.startList(SqlWriter.FrameTypeEnum.IDENTIFIER);\n+    call.operand(0).unparse(writer, leftPrec, 0);\n+    writer.sep(\".\");\n+    call.operand(1).unparse(writer, 0, 0);\n+    writer.endList(frame);\n+  }\n+\n+  @Override public SqlOperandCountRange getOperandCountRange() {\n+    return SqlOperandCountRanges.of(2);\n+  }\n+\n+  @Override public <R> void acceptCall(SqlVisitor<R> visitor, SqlCall call,\n+      boolean onlyExpressions, SqlBasicVisitor.ArgHandler<R> argHandler) {\n+    if (onlyExpressions) {\n+      // Do not visit operands[1] -- it is not an expression.\n+      argHandler.visitChild(visitor, call, 0, call.operand(0));\n+    } else {\n+      super.acceptCall(visitor, call, onlyExpressions, argHandler);\n+    }\n+  }\n+\n+  @Override public RelDataType deriveType(SqlValidator validator,\n+      SqlValidatorScope scope, SqlCall call) {\n+    final SqlNode operand = call.getOperandList().get(0);\n+    final RelDataType nodeType =\n+        validator.deriveType(scope, operand);\n+    assert nodeType != null;\n+    if (!nodeType.isStruct()) {\n+      throw SqlUtil.newContextException(operand.getParserPosition(),\n+          Static.RESOURCE.incompatibleTypes());\n+    }\n+\n+    final SqlNode fieldId = call.operand(1);\n+    final String fieldName = fieldId.toString();\n+    final RelDataTypeField field =\n+        nodeType.getField(fieldName, false, false);\n+    if (field == null) {\n+      throw SqlUtil.newContextException(fieldId.getParserPosition(),\n+          Static.RESOURCE.unknownField(fieldName));\n+    }\n+    RelDataType type = field.getType();\n+    if (nodeType.isNullable() && !type.isNullable()) {\n+        type = validator.getTypeFactory().createTypeWithNullability(type, true);\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43596996d023eb47c9589f95d8baf8a3cee83016"}, "originalPosition": 119}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cdbf1943f1d1d221a527d8b1701213a25870c710", "author": {"user": {"login": "dawidwys", "name": "Dawid Wysakowicz"}}, "url": "https://github.com/apache/flink/commit/cdbf1943f1d1d221a527d8b1701213a25870c710", "committedDate": "2020-07-16T12:03:54Z", "message": "[hotfix] Suppport ITEM for ROW types."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4dd633a9c0621b97d075c386d77b69abfaf408fe", "author": {"user": {"login": "dawidwys", "name": "Dawid Wysakowicz"}}, "url": "https://github.com/apache/flink/commit/4dd633a9c0621b97d075c386d77b69abfaf408fe", "committedDate": "2020-07-16T12:08:19Z", "message": "[FLINK-18286] Fix type inference for GET & AT Calcite functions\n\nThis commit fixes how Calcite infers/derives types for accessing nested columns of a ROW type."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6ba85a29c018561157e8591f4c4713ea12ae5c5b", "author": {"user": {"login": "dawidwys", "name": "Dawid Wysakowicz"}}, "url": "https://github.com/apache/flink/commit/6ba85a29c018561157e8591f4c4713ea12ae5c5b", "committedDate": "2020-07-16T12:08:22Z", "message": "[hotfix] Reuse a Flink cluster for expressions tests."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "93f58e49ad79331331d4d85bbe7720ecfe39f968", "author": {"user": {"login": "dawidwys", "name": "Dawid Wysakowicz"}}, "url": "https://github.com/apache/flink/commit/93f58e49ad79331331d4d85bbe7720ecfe39f968", "committedDate": "2020-07-16T12:12:31Z", "message": "[FLINK-18286] Implement type inference for GET/FLATTEN"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "43596996d023eb47c9589f95d8baf8a3cee83016", "author": {"user": {"login": "dawidwys", "name": "Dawid Wysakowicz"}}, "url": "https://github.com/apache/flink/commit/43596996d023eb47c9589f95d8baf8a3cee83016", "committedDate": "2020-06-18T09:16:36Z", "message": "[FLINK-18286] Implement type inference for GET/FLATTEN"}, "afterCommit": {"oid": "93f58e49ad79331331d4d85bbe7720ecfe39f968", "author": {"user": {"login": "dawidwys", "name": "Dawid Wysakowicz"}}, "url": "https://github.com/apache/flink/commit/93f58e49ad79331331d4d85bbe7720ecfe39f968", "committedDate": "2020-07-16T12:12:31Z", "message": "[FLINK-18286] Implement type inference for GET/FLATTEN"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3454, "cost": 1, "resetAt": "2021-10-28T16:48:13Z"}}}