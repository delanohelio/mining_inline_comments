{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTE3NzM0MDQ2", "number": 13998, "reviewThreads": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQxMDozNTowMFrOE6gIhQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQxMDozNjowN1rOE6gKPQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI5Nzc5MzMzOnYy", "diffSide": "RIGHT", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/ContinuousHiveSplitEnumerator.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQxMDozNTowMFrOH1nUoQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQxMDozNTowMFrOH1nUoQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTk4MDgzMw==", "bodyText": "Just PartitionMonitor?", "url": "https://github.com/apache/flink/pull/13998#discussion_r525980833", "createdAt": "2020-11-18T10:35:00Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/ContinuousHiveSplitEnumerator.java", "diffHunk": "@@ -158,12 +142,64 @@ public void close() throws IOException {\n \t\t}\n \t}\n \n-\tprivate Void monitorAndGetSplits() throws Exception {\n-\t\tstateLock.writeLock().lock();\n-\t\ttry {\n+\tprivate void handleNewSplits(NewSplitsAndState<T> newSplitsAndState, Throwable error) {\n+\t\tif (error != null) {\n+\t\t\t// we need to failover because the worker thread is stateful\n+\t\t\tthrow new FlinkHiveException(\"Failed to enumerate files\", error);\n+\t\t}\n+\t\tthis.currentReadOffset = newSplitsAndState.offset;\n+\t\tthis.seenPartitionsSinceOffset = newSplitsAndState.seenPartitions;\n+\t\tsplitAssigner.addSplits(new ArrayList<>(newSplitsAndState.newSplits));\n+\t\tassignSplits();\n+\t}\n+\n+\tprivate void assignSplits() {\n+\t\tfinal Iterator<Map.Entry<Integer, String>> awaitingReader = readersAwaitingSplit.entrySet().iterator();\n+\t\twhile (awaitingReader.hasNext()) {\n+\t\t\tfinal Map.Entry<Integer, String> nextAwaiting = awaitingReader.next();\n+\t\t\tfinal String hostname = nextAwaiting.getValue();\n+\t\t\tfinal int awaitingSubtask = nextAwaiting.getKey();\n+\t\t\tfinal Optional<FileSourceSplit> nextSplit = splitAssigner.getNext(hostname);\n+\t\t\tif (nextSplit.isPresent()) {\n+\t\t\t\tenumeratorContext.assignSplit((HiveSourceSplit) nextSplit.get(), awaitingSubtask);\n+\t\t\t\tawaitingReader.remove();\n+\t\t\t} else {\n+\t\t\t\tbreak;\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static class NewPartitionMonitor<T extends Comparable<T>> implements Callable<NewSplitsAndState<T>> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6beffeeaa6311b7d58dca3eea2d876ce03bbd816"}, "originalPosition": 122}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI5Nzc5MzcxOnYy", "diffSide": "RIGHT", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/ContinuousHiveSplitEnumerator.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQxMDozNTowNlrOH1nU3g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQxMDozNTowNlrOH1nU3g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTk4MDg5NA==", "bodyText": "private", "url": "https://github.com/apache/flink/pull/13998#discussion_r525980894", "createdAt": "2020-11-18T10:35:06Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/ContinuousHiveSplitEnumerator.java", "diffHunk": "@@ -158,12 +142,64 @@ public void close() throws IOException {\n \t\t}\n \t}\n \n-\tprivate Void monitorAndGetSplits() throws Exception {\n-\t\tstateLock.writeLock().lock();\n-\t\ttry {\n+\tprivate void handleNewSplits(NewSplitsAndState<T> newSplitsAndState, Throwable error) {\n+\t\tif (error != null) {\n+\t\t\t// we need to failover because the worker thread is stateful\n+\t\t\tthrow new FlinkHiveException(\"Failed to enumerate files\", error);\n+\t\t}\n+\t\tthis.currentReadOffset = newSplitsAndState.offset;\n+\t\tthis.seenPartitionsSinceOffset = newSplitsAndState.seenPartitions;\n+\t\tsplitAssigner.addSplits(new ArrayList<>(newSplitsAndState.newSplits));\n+\t\tassignSplits();\n+\t}\n+\n+\tprivate void assignSplits() {\n+\t\tfinal Iterator<Map.Entry<Integer, String>> awaitingReader = readersAwaitingSplit.entrySet().iterator();\n+\t\twhile (awaitingReader.hasNext()) {\n+\t\t\tfinal Map.Entry<Integer, String> nextAwaiting = awaitingReader.next();\n+\t\t\tfinal String hostname = nextAwaiting.getValue();\n+\t\t\tfinal int awaitingSubtask = nextAwaiting.getKey();\n+\t\t\tfinal Optional<FileSourceSplit> nextSplit = splitAssigner.getNext(hostname);\n+\t\t\tif (nextSplit.isPresent()) {\n+\t\t\t\tenumeratorContext.assignSplit((HiveSourceSplit) nextSplit.get(), awaitingSubtask);\n+\t\t\t\tawaitingReader.remove();\n+\t\t\t} else {\n+\t\t\t\tbreak;\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static class NewPartitionMonitor<T extends Comparable<T>> implements Callable<NewSplitsAndState<T>> {\n+\n+\t\t// keep these locally so that we don't need to share state with main thread\n+\t\tprivate T currentReadOffset;\n+\t\tprivate final Set<List<String>> seenPartitionsSinceOffset;\n+\n+\t\tprivate final ObjectPath tablePath;\n+\t\tprivate final JobConf jobConf;\n+\t\tprivate final ContinuousPartitionFetcher<Partition, T> fetcher;\n+\t\tprivate final HiveTableSource.HiveContinuousPartitionFetcherContext<T> fetcherContext;\n+\n+\t\tNewPartitionMonitor(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6beffeeaa6311b7d58dca3eea2d876ce03bbd816"}, "originalPosition": 133}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI5Nzc5NzczOnYy", "diffSide": "RIGHT", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/ContinuousHiveSplitEnumerator.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQxMDozNjowN1rOH1nXSg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQxMjo0NTo1M1rOH1sFSg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTk4MTUxNA==", "bodyText": "There is a thread safe problem? Two thread share a same seenPartitionsSinceOffset\nWe need return a immutable seenPartitionsSinceOffset", "url": "https://github.com/apache/flink/pull/13998#discussion_r525981514", "createdAt": "2020-11-18T10:36:07Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/ContinuousHiveSplitEnumerator.java", "diffHunk": "@@ -158,12 +142,64 @@ public void close() throws IOException {\n \t\t}\n \t}\n \n-\tprivate Void monitorAndGetSplits() throws Exception {\n-\t\tstateLock.writeLock().lock();\n-\t\ttry {\n+\tprivate void handleNewSplits(NewSplitsAndState<T> newSplitsAndState, Throwable error) {\n+\t\tif (error != null) {\n+\t\t\t// we need to failover because the worker thread is stateful\n+\t\t\tthrow new FlinkHiveException(\"Failed to enumerate files\", error);\n+\t\t}\n+\t\tthis.currentReadOffset = newSplitsAndState.offset;\n+\t\tthis.seenPartitionsSinceOffset = newSplitsAndState.seenPartitions;\n+\t\tsplitAssigner.addSplits(new ArrayList<>(newSplitsAndState.newSplits));\n+\t\tassignSplits();\n+\t}\n+\n+\tprivate void assignSplits() {\n+\t\tfinal Iterator<Map.Entry<Integer, String>> awaitingReader = readersAwaitingSplit.entrySet().iterator();\n+\t\twhile (awaitingReader.hasNext()) {\n+\t\t\tfinal Map.Entry<Integer, String> nextAwaiting = awaitingReader.next();\n+\t\t\tfinal String hostname = nextAwaiting.getValue();\n+\t\t\tfinal int awaitingSubtask = nextAwaiting.getKey();\n+\t\t\tfinal Optional<FileSourceSplit> nextSplit = splitAssigner.getNext(hostname);\n+\t\t\tif (nextSplit.isPresent()) {\n+\t\t\t\tenumeratorContext.assignSplit((HiveSourceSplit) nextSplit.get(), awaitingSubtask);\n+\t\t\t\tawaitingReader.remove();\n+\t\t\t} else {\n+\t\t\t\tbreak;\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static class NewPartitionMonitor<T extends Comparable<T>> implements Callable<NewSplitsAndState<T>> {\n+\n+\t\t// keep these locally so that we don't need to share state with main thread\n+\t\tprivate T currentReadOffset;\n+\t\tprivate final Set<List<String>> seenPartitionsSinceOffset;\n+\n+\t\tprivate final ObjectPath tablePath;\n+\t\tprivate final JobConf jobConf;\n+\t\tprivate final ContinuousPartitionFetcher<Partition, T> fetcher;\n+\t\tprivate final HiveTableSource.HiveContinuousPartitionFetcherContext<T> fetcherContext;\n+\n+\t\tNewPartitionMonitor(\n+\t\t\t\tT currentReadOffset,\n+\t\t\t\tCollection<List<String>> seenPartitionsSinceOffset,\n+\t\t\t\tObjectPath tablePath,\n+\t\t\t\tJobConf jobConf,\n+\t\t\t\tContinuousPartitionFetcher<Partition, T> fetcher,\n+\t\t\t\tHiveTableSource.HiveContinuousPartitionFetcherContext<T> fetcherContext) {\n+\t\t\tthis.currentReadOffset = currentReadOffset;\n+\t\t\tthis.seenPartitionsSinceOffset = new HashSet<>(seenPartitionsSinceOffset);\n+\t\t\tthis.tablePath = tablePath;\n+\t\t\tthis.jobConf = jobConf;\n+\t\t\tthis.fetcher = fetcher;\n+\t\t\tthis.fetcherContext = fetcherContext;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic NewSplitsAndState<T> call() throws Exception {\n \t\t\tList<Tuple2<Partition, T>> partitions = fetcher.fetchPartitions(fetcherContext, currentReadOffset);\n \t\t\tif (partitions.isEmpty()) {\n-\t\t\t\treturn null;\n+\t\t\t\treturn new NewSplitsAndState<>(Collections.emptyList(), currentReadOffset, seenPartitionsSinceOffset);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6beffeeaa6311b7d58dca3eea2d876ce03bbd816"}, "originalPosition": 153}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjA1ODgyNg==", "bodyText": "The seenPartitionsSinceOffset here is local to this class and is a copy of the set parameter in constructor. So it's not shared with main thread", "url": "https://github.com/apache/flink/pull/13998#discussion_r526058826", "createdAt": "2020-11-18T12:45:53Z", "author": {"login": "lirui-apache"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/ContinuousHiveSplitEnumerator.java", "diffHunk": "@@ -158,12 +142,64 @@ public void close() throws IOException {\n \t\t}\n \t}\n \n-\tprivate Void monitorAndGetSplits() throws Exception {\n-\t\tstateLock.writeLock().lock();\n-\t\ttry {\n+\tprivate void handleNewSplits(NewSplitsAndState<T> newSplitsAndState, Throwable error) {\n+\t\tif (error != null) {\n+\t\t\t// we need to failover because the worker thread is stateful\n+\t\t\tthrow new FlinkHiveException(\"Failed to enumerate files\", error);\n+\t\t}\n+\t\tthis.currentReadOffset = newSplitsAndState.offset;\n+\t\tthis.seenPartitionsSinceOffset = newSplitsAndState.seenPartitions;\n+\t\tsplitAssigner.addSplits(new ArrayList<>(newSplitsAndState.newSplits));\n+\t\tassignSplits();\n+\t}\n+\n+\tprivate void assignSplits() {\n+\t\tfinal Iterator<Map.Entry<Integer, String>> awaitingReader = readersAwaitingSplit.entrySet().iterator();\n+\t\twhile (awaitingReader.hasNext()) {\n+\t\t\tfinal Map.Entry<Integer, String> nextAwaiting = awaitingReader.next();\n+\t\t\tfinal String hostname = nextAwaiting.getValue();\n+\t\t\tfinal int awaitingSubtask = nextAwaiting.getKey();\n+\t\t\tfinal Optional<FileSourceSplit> nextSplit = splitAssigner.getNext(hostname);\n+\t\t\tif (nextSplit.isPresent()) {\n+\t\t\t\tenumeratorContext.assignSplit((HiveSourceSplit) nextSplit.get(), awaitingSubtask);\n+\t\t\t\tawaitingReader.remove();\n+\t\t\t} else {\n+\t\t\t\tbreak;\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static class NewPartitionMonitor<T extends Comparable<T>> implements Callable<NewSplitsAndState<T>> {\n+\n+\t\t// keep these locally so that we don't need to share state with main thread\n+\t\tprivate T currentReadOffset;\n+\t\tprivate final Set<List<String>> seenPartitionsSinceOffset;\n+\n+\t\tprivate final ObjectPath tablePath;\n+\t\tprivate final JobConf jobConf;\n+\t\tprivate final ContinuousPartitionFetcher<Partition, T> fetcher;\n+\t\tprivate final HiveTableSource.HiveContinuousPartitionFetcherContext<T> fetcherContext;\n+\n+\t\tNewPartitionMonitor(\n+\t\t\t\tT currentReadOffset,\n+\t\t\t\tCollection<List<String>> seenPartitionsSinceOffset,\n+\t\t\t\tObjectPath tablePath,\n+\t\t\t\tJobConf jobConf,\n+\t\t\t\tContinuousPartitionFetcher<Partition, T> fetcher,\n+\t\t\t\tHiveTableSource.HiveContinuousPartitionFetcherContext<T> fetcherContext) {\n+\t\t\tthis.currentReadOffset = currentReadOffset;\n+\t\t\tthis.seenPartitionsSinceOffset = new HashSet<>(seenPartitionsSinceOffset);\n+\t\t\tthis.tablePath = tablePath;\n+\t\t\tthis.jobConf = jobConf;\n+\t\t\tthis.fetcher = fetcher;\n+\t\t\tthis.fetcherContext = fetcherContext;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic NewSplitsAndState<T> call() throws Exception {\n \t\t\tList<Tuple2<Partition, T>> partitions = fetcher.fetchPartitions(fetcherContext, currentReadOffset);\n \t\t\tif (partitions.isEmpty()) {\n-\t\t\t\treturn null;\n+\t\t\t\treturn new NewSplitsAndState<>(Collections.emptyList(), currentReadOffset, seenPartitionsSinceOffset);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTk4MTUxNA=="}, "originalCommit": {"oid": "6beffeeaa6311b7d58dca3eea2d876ce03bbd816"}, "originalPosition": 153}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4858, "cost": 1, "resetAt": "2021-11-12T11:57:46Z"}}}