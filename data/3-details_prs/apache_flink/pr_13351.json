{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDgxODk1MDk0", "number": 13351, "title": "[FLINK-18990][task] Read channel state sequentially", "bodyText": "What is the purpose of the change\nRead channel state sequentially to reduce the number of seeks.\nVerifying this change\n\nAdded SequentialChannelStateReaderImplTest and ChannelStateChunkReaderTest for the new code.\nAdded RecoveredInputChannelTest to check that conversion is not permitted untill fully recovered and state consumed\nsome tests are eliminated because: 1) state in StreamTask is read unconditionally; 2) recovered channel can't be converted without being fully consumed anymore\n\nDoes this pull request potentially affect one of the following parts:\n\nDependencies (does it add or upgrade a dependency): no\nThe public API, i.e., is any changed class annotated with @Public(Evolving): (yes / no)\nThe serializers: no\nThe runtime per-record code paths (performance sensitive): no\nAnything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: yes\nThe S3 file system connector: no\n\nDocumentation\n\nDoes this pull request introduce a new feature? no\nIf yes, how is the feature documented? no", "createdAt": "2020-09-08T09:01:36Z", "url": "https://github.com/apache/flink/pull/13351", "merged": true, "mergeCommit": {"oid": "0b6c086af921ec1abc0757680ddb3285a8828bca"}, "closed": true, "closedAt": "2020-10-09T17:06:15Z", "author": {"login": "rkhachatryan"}, "timelineItems": {"totalCount": 35, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdLY73ngBqjM3OTM1OTgyNzQ=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdQzHFNgBqjM4NTkzNjkwODA=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "3c797163b965710a41ea0b427a583223b2db02a0", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/3c797163b965710a41ea0b427a583223b2db02a0", "committedDate": "2020-08-27T15:21:31Z", "message": "Unconditionally read channel state"}, "afterCommit": {"oid": "fb92ec41b039a5d5509fd0376ad305ce81ee5ad2", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/fb92ec41b039a5d5509fd0376ad305ce81ee5ad2", "committedDate": "2020-09-22T14:32:08Z", "message": "Unconditionally read channel state"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "9202329db661bc3fc8fbcdd2745b6f9cb078922c", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/9202329db661bc3fc8fbcdd2745b6f9cb078922c", "committedDate": "2020-09-22T15:11:54Z", "message": "request subpartition per channel"}, "afterCommit": {"oid": "8d443ef45b7d7db48c807c9cb4338ee92bdc3239", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/8d443ef45b7d7db48c807c9cb4338ee92bdc3239", "committedDate": "2020-09-22T19:16:02Z", "message": "Request per gate"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "8d443ef45b7d7db48c807c9cb4338ee92bdc3239", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/8d443ef45b7d7db48c807c9cb4338ee92bdc3239", "committedDate": "2020-09-22T19:16:02Z", "message": "Request per gate"}, "afterCommit": {"oid": "4ea479a303d7939af8f20b106c704a799c40cee1", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/4ea479a303d7939af8f20b106c704a799c40cee1", "committedDate": "2020-09-22T19:42:26Z", "message": "Request partition per gate"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "4ea479a303d7939af8f20b106c704a799c40cee1", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/4ea479a303d7939af8f20b106c704a799c40cee1", "committedDate": "2020-09-22T19:42:26Z", "message": "Request partition per gate"}, "afterCommit": {"oid": "a219f9406ef75b88de09418025b8c21552fbbe1c", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/a219f9406ef75b88de09418025b8c21552fbbe1c", "committedDate": "2020-09-23T14:19:54Z", "message": "[FLINK-18989][task] Remove ChannelStateReader"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "a219f9406ef75b88de09418025b8c21552fbbe1c", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/a219f9406ef75b88de09418025b8c21552fbbe1c", "committedDate": "2020-09-23T14:19:54Z", "message": "[FLINK-18989][task] Remove ChannelStateReader"}, "afterCommit": {"oid": "8e0da15242baad7de020938d24ad076b11b651c4", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/8e0da15242baad7de020938d24ad076b11b651c4", "committedDate": "2020-09-23T16:19:00Z", "message": "[FLINK-18989][task] Remove ChannelStateReader"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "8e0da15242baad7de020938d24ad076b11b651c4", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/8e0da15242baad7de020938d24ad076b11b651c4", "committedDate": "2020-09-23T16:19:00Z", "message": "[FLINK-18989][task] Remove ChannelStateReader"}, "afterCommit": {"oid": "df935f35307616478058c4991d17de06376836a2", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/df935f35307616478058c4991d17de06376836a2", "committedDate": "2020-09-23T19:02:11Z", "message": "[FLINK-18989][task] Remove ChannelStateReader"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "df935f35307616478058c4991d17de06376836a2", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/df935f35307616478058c4991d17de06376836a2", "committedDate": "2020-09-23T19:02:11Z", "message": "[FLINK-18989][task] Remove ChannelStateReader"}, "afterCommit": {"oid": "4a598b96aa0250255f8758ce4ce57df23072dac7", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/4a598b96aa0250255f8758ce4ce57df23072dac7", "committedDate": "2020-09-23T22:56:11Z", "message": "[FLINK-18989][task] Remove ChannelStateReader"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk1MzM2MjE3", "url": "https://github.com/apache/flink/pull/13351#pullrequestreview-495336217", "createdAt": "2020-09-24T08:29:31Z", "commit": {"oid": "77ec70dc867605d7855ea4a8aad21a1bfc5743bd"}, "state": "COMMENTED", "comments": {"totalCount": 31, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwODoyOTozMVrOHXPg1g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxNDoxMDoxNFrOHXc0ow==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDEzMzQ2Mg==", "bodyText": "Good fix. I'm starting to wonder if we should phase out DummyEnvironment. It seems like it lacks so much.", "url": "https://github.com/apache/flink/pull/13351#discussion_r494133462", "createdAt": "2020-09-24T08:29:31Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/test/java/org/apache/flink/runtime/operators/testutils/DummyEnvironment.java", "diffHunk": "@@ -230,7 +230,7 @@ public ResultPartitionWriter getWriter(int index) {\n \n \t@Override\n \tpublic IndexedInputGate getInputGate(int index) {\n-\t\treturn null;\n+\t\tthrow new ArrayIndexOutOfBoundsException(0);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "77ec70dc867605d7855ea4a8aad21a1bfc5743bd"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDI4NDg5Mw==", "bodyText": "Could we unify both code paths by always using\nfor (InputGate inputGate : getEnvironment().getAllInputGates()) {\n\tinputGate\n\t\t.getStateConsumedFuture()\n\t\t.thenRun(() -> mainMailboxExecutor.execute(inputGate::requestPartitions, \"Input gate request partitions\"));\n}\n\nand set getStateConsumedFuture to done if no data is available?", "url": "https://github.com/apache/flink/pull/13351#discussion_r494284893", "createdAt": "2020-09-24T12:42:08Z", "author": {"login": "AHeise"}, "path": "flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/StreamTask.java", "diffHunk": "@@ -502,33 +503,24 @@ protected void beforeInvoke() throws Exception {\n \t}\n \n \tprivate void readRecoveredChannelState() throws IOException, InterruptedException {\n-\t\tChannelStateReader reader = getEnvironment().getTaskStateManager().getChannelStateReader();\n-\t\tif (!reader.hasChannelStates()) {\n-\t\t\trequestPartitions();\n-\t\t\treturn;\n-\t\t}\n-\n-\t\tResultPartitionWriter[] writers = getEnvironment().getAllWriters();\n-\t\tif (writers != null) {\n-\t\t\tfor (ResultPartitionWriter writer : writers) {\n-\t\t\t\tif (writer instanceof CheckpointedResultPartition) {\n-\t\t\t\t\t((CheckpointedResultPartition) writer).readRecoveredState(reader);\n-\t\t\t\t} else {\n-\t\t\t\t\tthrow new IllegalStateException(\n-\t\t\t\t\t\t\t\"Cannot restore state to a non-checkpointable partition type: \" + writer);\n+\t\tSequentialChannelStateReader reader = getEnvironment().getTaskStateManager().getSequentialChannelStateReader();\n+\t\tif (reader.hasChannelStates()) {\n+\t\t\treader.readOutputData(getEnvironment().getAllWriters());\n+\t\t\tchannelIOExecutor.execute(() -> {\n+\t\t\t\ttry {\n+\t\t\t\t\treader.readInputData(getEnvironment().getAllInputGates());\n+\t\t\t\t} catch (Exception e) {\n+\t\t\t\t\tasyncExceptionHandler.handleAsyncException(\"Unable to read channel state\", e);\n \t\t\t\t}\n-\t\t\t}\n-\t\t}\n+\t\t\t});\n \n-\t\t// It would get possible benefits to recovery input side after output side, which guarantees the\n-\t\t// output can request more floating buffers from global firstly.\n-\t\tInputGate[] inputGates = getEnvironment().getAllInputGates();\n-\t\tif (inputGates != null && inputGates.length > 0) {\n-\t\t\tfor (InputGate inputGate : inputGates) {\n+\t\t\tfor (InputGate inputGate : getEnvironment().getAllInputGates()) {\n \t\t\t\tinputGate\n-\t\t\t\t\t.readRecoveredState(channelIOExecutor, reader)\n+\t\t\t\t\t.getStateConsumedFuture()\n \t\t\t\t\t.thenRun(() -> mainMailboxExecutor.execute(inputGate::requestPartitions, \"Input gate request partitions\"));\n \t\t\t}\n+\t\t} else {\n+\t\t\trequestPartitions();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 77}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDI4NjIwNw==", "bodyText": "Do we have a use for the old ChannelStateReader? I had expected to see all changes inside the existing class.", "url": "https://github.com/apache/flink/pull/13351#discussion_r494286207", "createdAt": "2020-09-24T12:44:23Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReader.java", "diffHunk": "@@ -0,0 +1,62 @@\n+package org.apache.flink.runtime.checkpoint.channel;\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.runtime.io.network.api.writer.ResultPartitionWriter;\n+import org.apache.flink.runtime.io.network.partition.consumer.InputGate;\n+\n+import java.io.IOException;\n+\n+/**\n+ * Reads channel state saved during checkpoint/savepoint.\n+ */\n+@Internal\n+public interface SequentialChannelStateReader extends AutoCloseable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDI5NDQ0Ng==", "bodyText": "It's probably easier to understand (at least for me) if you apply the extractor on call side and just pass Stream<StateObjectCollection<Handle>> into read.\nread(OperatorSubtaskState::getInputChannelState, stateHandler)\n\n->\nread(streamSubtaskStates().map(OperatorSubtaskState::getInputChannelState), stateHandler)", "url": "https://github.com/apache/flink/pull/13351#discussion_r494294446", "createdAt": "2020-09-24T12:56:44Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImpl.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.fs.FSDataInputStream;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.runtime.checkpoint.StateObjectCollection;\n+import org.apache.flink.runtime.checkpoint.TaskStateSnapshot;\n+import org.apache.flink.runtime.io.network.api.writer.ResultPartitionWriter;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.BufferBuilder;\n+import org.apache.flink.runtime.io.network.buffer.BufferConsumer;\n+import org.apache.flink.runtime.io.network.partition.consumer.InputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel;\n+import org.apache.flink.runtime.state.AbstractChannelStateHandle;\n+import org.apache.flink.runtime.state.StreamStateHandle;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+import java.util.stream.Stream;\n+\n+import static java.util.Comparator.comparingLong;\n+import static java.util.stream.Collectors.groupingBy;\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.flink.runtime.checkpoint.channel.ChannelStateByteBuffer.wrap;\n+\n+/**\n+ * {@link SequentialChannelStateReader} implementation.\n+ */\n+public class SequentialChannelStateReaderImpl implements SequentialChannelStateReader {\n+\n+\tprivate final TaskStateSnapshot taskStateSnapshot;\n+\tprivate final ChannelStateSerializer serializer;\n+\tprivate final ChannelStateChunkReader chunkReader;\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot) {\n+\t\tthis(taskStateSnapshot, new ChannelStateSerializerImpl());\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = new ChannelStateChunkReader(serializer);\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer, ChannelStateChunkReader chunkReader) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = chunkReader;\n+\t}\n+\n+\t@Override\n+\tpublic boolean hasChannelStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().anyMatch(subtaskStateEntry ->\n+\t\t\tsubtaskStateEntry.getValue().getInputChannelState().stream().anyMatch(h -> !h.getOffsets().isEmpty()) ||\n+\t\t\t\tsubtaskStateEntry.getValue().getResultSubpartitionState().stream().anyMatch(h -> !h.getOffsets().isEmpty()));\n+\t}\n+\n+\t@Override\n+\tpublic void readInputData(InputGate[] inputGates) throws IOException {\n+\t\ttry (InputChannelRecoveredStateHandler stateHandler = new InputChannelRecoveredStateHandler(inputGates)) {\n+\t\t\tread(OperatorSubtaskState::getInputChannelState, stateHandler);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void readOutputData(ResultPartitionWriter[] writers) throws IOException {\n+\t\ttry (ResultSubpartitionRecoveredStateHandler stateHandler = new ResultSubpartitionRecoveredStateHandler(writers)) {\n+\t\t\tread(OperatorSubtaskState::getResultSubpartitionState, stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void read(\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 97}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDI5NTkzMQ==", "bodyText": "Okay noob question: What is the relation between delegate and handles?\nIs that necessary because of the optimization to have small state in metadata?\nI thought that we only have one state file per subtask and now I'm a bit confused.", "url": "https://github.com/apache/flink/pull/13351#discussion_r494295931", "createdAt": "2020-09-24T12:58:59Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImpl.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.fs.FSDataInputStream;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.runtime.checkpoint.StateObjectCollection;\n+import org.apache.flink.runtime.checkpoint.TaskStateSnapshot;\n+import org.apache.flink.runtime.io.network.api.writer.ResultPartitionWriter;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.BufferBuilder;\n+import org.apache.flink.runtime.io.network.buffer.BufferConsumer;\n+import org.apache.flink.runtime.io.network.partition.consumer.InputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel;\n+import org.apache.flink.runtime.state.AbstractChannelStateHandle;\n+import org.apache.flink.runtime.state.StreamStateHandle;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+import java.util.stream.Stream;\n+\n+import static java.util.Comparator.comparingLong;\n+import static java.util.stream.Collectors.groupingBy;\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.flink.runtime.checkpoint.channel.ChannelStateByteBuffer.wrap;\n+\n+/**\n+ * {@link SequentialChannelStateReader} implementation.\n+ */\n+public class SequentialChannelStateReaderImpl implements SequentialChannelStateReader {\n+\n+\tprivate final TaskStateSnapshot taskStateSnapshot;\n+\tprivate final ChannelStateSerializer serializer;\n+\tprivate final ChannelStateChunkReader chunkReader;\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot) {\n+\t\tthis(taskStateSnapshot, new ChannelStateSerializerImpl());\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = new ChannelStateChunkReader(serializer);\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer, ChannelStateChunkReader chunkReader) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = chunkReader;\n+\t}\n+\n+\t@Override\n+\tpublic boolean hasChannelStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().anyMatch(subtaskStateEntry ->\n+\t\t\tsubtaskStateEntry.getValue().getInputChannelState().stream().anyMatch(h -> !h.getOffsets().isEmpty()) ||\n+\t\t\t\tsubtaskStateEntry.getValue().getResultSubpartitionState().stream().anyMatch(h -> !h.getOffsets().isEmpty()));\n+\t}\n+\n+\t@Override\n+\tpublic void readInputData(InputGate[] inputGates) throws IOException {\n+\t\ttry (InputChannelRecoveredStateHandler stateHandler = new InputChannelRecoveredStateHandler(inputGates)) {\n+\t\t\tread(OperatorSubtaskState::getInputChannelState, stateHandler);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void readOutputData(ResultPartitionWriter[] writers) throws IOException {\n+\t\ttry (ResultSubpartitionRecoveredStateHandler stateHandler = new ResultSubpartitionRecoveredStateHandler(writers)) {\n+\t\t\tread(OperatorSubtaskState::getResultSubpartitionState, stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void read(\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\tfor (Map.Entry<StreamStateHandle, List<Handle>> delegateAndHandles : groupByDelegate(streamSubtaskStates(), stateHandleExtractor).entrySet()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 99}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDI5Nzg4MA==", "bodyText": "When would we have length == 0? Shouldn't that be already skipped while writing? I wonder if it's worth throwing.", "url": "https://github.com/apache/flink/pull/13351#discussion_r494297880", "createdAt": "2020-09-24T13:02:00Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImpl.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.fs.FSDataInputStream;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.runtime.checkpoint.StateObjectCollection;\n+import org.apache.flink.runtime.checkpoint.TaskStateSnapshot;\n+import org.apache.flink.runtime.io.network.api.writer.ResultPartitionWriter;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.BufferBuilder;\n+import org.apache.flink.runtime.io.network.buffer.BufferConsumer;\n+import org.apache.flink.runtime.io.network.partition.consumer.InputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel;\n+import org.apache.flink.runtime.state.AbstractChannelStateHandle;\n+import org.apache.flink.runtime.state.StreamStateHandle;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+import java.util.stream.Stream;\n+\n+import static java.util.Comparator.comparingLong;\n+import static java.util.stream.Collectors.groupingBy;\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.flink.runtime.checkpoint.channel.ChannelStateByteBuffer.wrap;\n+\n+/**\n+ * {@link SequentialChannelStateReader} implementation.\n+ */\n+public class SequentialChannelStateReaderImpl implements SequentialChannelStateReader {\n+\n+\tprivate final TaskStateSnapshot taskStateSnapshot;\n+\tprivate final ChannelStateSerializer serializer;\n+\tprivate final ChannelStateChunkReader chunkReader;\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot) {\n+\t\tthis(taskStateSnapshot, new ChannelStateSerializerImpl());\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = new ChannelStateChunkReader(serializer);\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer, ChannelStateChunkReader chunkReader) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = chunkReader;\n+\t}\n+\n+\t@Override\n+\tpublic boolean hasChannelStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().anyMatch(subtaskStateEntry ->\n+\t\t\tsubtaskStateEntry.getValue().getInputChannelState().stream().anyMatch(h -> !h.getOffsets().isEmpty()) ||\n+\t\t\t\tsubtaskStateEntry.getValue().getResultSubpartitionState().stream().anyMatch(h -> !h.getOffsets().isEmpty()));\n+\t}\n+\n+\t@Override\n+\tpublic void readInputData(InputGate[] inputGates) throws IOException {\n+\t\ttry (InputChannelRecoveredStateHandler stateHandler = new InputChannelRecoveredStateHandler(inputGates)) {\n+\t\t\tread(OperatorSubtaskState::getInputChannelState, stateHandler);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void readOutputData(ResultPartitionWriter[] writers) throws IOException {\n+\t\ttry (ResultSubpartitionRecoveredStateHandler stateHandler = new ResultSubpartitionRecoveredStateHandler(writers)) {\n+\t\t\tread(OperatorSubtaskState::getResultSubpartitionState, stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void read(\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\tfor (Map.Entry<StreamStateHandle, List<Handle>> delegateAndHandles : groupByDelegate(streamSubtaskStates(), stateHandleExtractor).entrySet()) {\n+\t\t\treadSequentially(delegateAndHandles.getKey(), delegateAndHandles.getValue(), stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void readSequentially(\n+\t\t\tStreamStateHandle streamStateHandle,\n+\t\t\tList<Handle> channelStateHandles,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\ttry (FSDataInputStream is = streamStateHandle.openInputStream()) {\n+\t\t\tserializer.readHeader(is);\n+\t\t\tfor (Tuple2<Long, Info> offsetAndChannelInfo : extractOffsetsSorted(channelStateHandles)) {\n+\t\t\t\tchunkReader.readChunk(is, offsetAndChannelInfo.f0, stateHandler, offsetAndChannelInfo.f1);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate Stream<OperatorSubtaskState> streamSubtaskStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().map(Map.Entry::getValue);\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> Map<StreamStateHandle, List<Handle>> groupByDelegate(\n+\t\t\tStream<OperatorSubtaskState> states,\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor) {\n+\t\treturn states\n+\t\t\t.map(stateHandleExtractor).flatMap(Collection::stream)\n+\t\t\t.peek(validate())\n+\t\t\t.collect(groupingBy(AbstractChannelStateHandle::getDelegate));\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> Consumer<Handle> validate() {\n+\t\tSet<Info> seen = new HashSet<>();\n+\t\t// expect each channel to be described only once; otherwise, buffers in channel could be re-ordered\n+\t\treturn handle -> Preconditions.checkState(seen.add(handle.getInfo()), \"duplicate channel info: %s\");\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> List<Tuple2<Long, Info>> extractOffsetsSorted(List<Handle> channelStateHandles) {\n+\t\treturn channelStateHandles\n+\t\t\t.stream()\n+\t\t\t.flatMap(SequentialChannelStateReaderImpl::extractOffsets)\n+\t\t\t.sorted(comparingLong(offsetAndInfo -> offsetAndInfo.f0))\n+\t\t\t.collect(toList());\n+\t}\n+\n+\tprivate static  <Info, Handle extends AbstractChannelStateHandle<Info>> Stream<Tuple2<Long, Info>> extractOffsets(Handle handle) {\n+\t\treturn handle.getOffsets().stream().map(offset -> Tuple2.of(offset, handle.getInfo()));\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws Exception {\n+\t}\n+\n+}\n+\n+interface RecoveredChannelStateHandler<Info, Context> extends AutoCloseable {\n+\tTuple2<ChannelStateByteBuffer, Context> getBuffer(Info info) throws IOException;\n+\n+\tvoid recover(Info info, Context context) throws IOException;\n+}\n+\n+class InputChannelRecoveredStateHandler implements RecoveredChannelStateHandler<InputChannelInfo, Buffer> {\n+\tprivate final InputGate[] inputGates;\n+\n+\tInputChannelRecoveredStateHandler(InputGate[] inputGates) {\n+\t\tthis.inputGates = inputGates;\n+\t}\n+\n+\t@Override\n+\tpublic Tuple2<ChannelStateByteBuffer, Buffer> getBuffer(InputChannelInfo channelInfo) throws IOException {\n+\t\tRecoveredInputChannel channel = getChannel(channelInfo);\n+\t\tBuffer buffer;\n+\t\ttry {\n+\t\t\tbuffer = channel.getBuffer();\n+\t\t\treturn Tuple2.of(wrap(buffer), buffer);\n+\t\t} catch (InterruptedException e) {\n+\t\t\tthrow new IOException(e);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void recover(InputChannelInfo channelInfo, Buffer buffer) {\n+\t\tif (buffer.readableBytes() > 0) {\n+\t\t\tgetChannel(channelInfo).onRecoveredStateBuffer(buffer);\n+\t\t} else {\n+\t\t\tbuffer.recycleBuffer();\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws IOException {\n+\t\t// note that we need to finish all RecoveredInputChannels, not just those with state\n+\t\tfor (final InputGate inputGate : inputGates) {\n+\t\t\tinputGate.finishReadRecoveredState();\n+\t\t}\n+\t}\n+\n+\tprivate RecoveredInputChannel getChannel(InputChannelInfo info) {\n+\t\treturn (RecoveredInputChannel) inputGates[info.getGateIdx()].getChannel(info.getInputChannelIdx());\n+\t}\n+}\n+\n+class ResultSubpartitionRecoveredStateHandler implements RecoveredChannelStateHandler<ResultSubpartitionInfo, Tuple2<BufferBuilder, BufferConsumer>> {\n+\n+\tprivate final ResultPartitionWriter[] writers;\n+\n+\tResultSubpartitionRecoveredStateHandler(ResultPartitionWriter[] writers) {\n+\t\tthis.writers = writers;\n+\t}\n+\n+\t@Override\n+\tpublic Tuple2<ChannelStateByteBuffer, Tuple2<BufferBuilder, BufferConsumer>> getBuffer(ResultSubpartitionInfo subpartitionInfo) throws IOException {\n+\t\tBufferBuilder bufferBuilder;\n+\t\ttry {\n+\t\t\tbufferBuilder = getWriter(subpartitionInfo).getBufferBuilder(subpartitionInfo.getSubPartitionIdx());\n+\t\t} catch (InterruptedException e) {\n+\t\t\tthrow new IOException(e);\n+\t\t}\n+\t\treturn Tuple2.of(wrap(bufferBuilder), Tuple2.of(bufferBuilder, bufferBuilder.createBufferConsumer()));\n+\t}\n+\n+\t@Override\n+\tpublic void recover(ResultSubpartitionInfo subpartitionInfo, Tuple2<BufferBuilder, BufferConsumer> bufferConsumer) throws IOException {\n+\t\tbufferConsumer.f0.finish();\n+\t\tgetWriter(subpartitionInfo).addBufferConsumer(bufferConsumer.f1, subpartitionInfo.getSubPartitionIdx());\n+\t}\n+\n+\tprivate ResultPartitionWriter getWriter(ResultSubpartitionInfo info) {\n+\t\treturn writers[info.getPartitionIdx()];\n+\t}\n+\n+\t@Override\n+\tpublic void close() {\n+\t}\n+}\n+\n+class ChannelStateChunkReader {\n+\tprivate final ChannelStateSerializer serializer;\n+\n+\tChannelStateChunkReader(ChannelStateSerializer serializer) {\n+\t\tthis.serializer = serializer;\n+\t}\n+\n+\t<Info, Context, Handle extends AbstractChannelStateHandle<Info>> void readChunk(\n+\t\t\tFSDataInputStream source,\n+\t\t\tlong sourceOffset,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler,\n+\t\t\tInfo channelInfo) throws IOException {\n+\t\tif (source.getPos() != sourceOffset) {\n+\t\t\tsource.seek(sourceOffset);\n+\t\t}\n+\t\tint length = serializer.readLength(source);\n+\t\twhile (length > 0) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 250}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDMwMzAxNg==", "bodyText": "What is the main motivation for keeping readChunk in a separate class from SeqReaderImpl? Is it for unit tests?", "url": "https://github.com/apache/flink/pull/13351#discussion_r494303016", "createdAt": "2020-09-24T13:09:55Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImpl.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.fs.FSDataInputStream;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.runtime.checkpoint.StateObjectCollection;\n+import org.apache.flink.runtime.checkpoint.TaskStateSnapshot;\n+import org.apache.flink.runtime.io.network.api.writer.ResultPartitionWriter;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.BufferBuilder;\n+import org.apache.flink.runtime.io.network.buffer.BufferConsumer;\n+import org.apache.flink.runtime.io.network.partition.consumer.InputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel;\n+import org.apache.flink.runtime.state.AbstractChannelStateHandle;\n+import org.apache.flink.runtime.state.StreamStateHandle;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+import java.util.stream.Stream;\n+\n+import static java.util.Comparator.comparingLong;\n+import static java.util.stream.Collectors.groupingBy;\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.flink.runtime.checkpoint.channel.ChannelStateByteBuffer.wrap;\n+\n+/**\n+ * {@link SequentialChannelStateReader} implementation.\n+ */\n+public class SequentialChannelStateReaderImpl implements SequentialChannelStateReader {\n+\n+\tprivate final TaskStateSnapshot taskStateSnapshot;\n+\tprivate final ChannelStateSerializer serializer;\n+\tprivate final ChannelStateChunkReader chunkReader;\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot) {\n+\t\tthis(taskStateSnapshot, new ChannelStateSerializerImpl());\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = new ChannelStateChunkReader(serializer);\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer, ChannelStateChunkReader chunkReader) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = chunkReader;\n+\t}\n+\n+\t@Override\n+\tpublic boolean hasChannelStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().anyMatch(subtaskStateEntry ->\n+\t\t\tsubtaskStateEntry.getValue().getInputChannelState().stream().anyMatch(h -> !h.getOffsets().isEmpty()) ||\n+\t\t\t\tsubtaskStateEntry.getValue().getResultSubpartitionState().stream().anyMatch(h -> !h.getOffsets().isEmpty()));\n+\t}\n+\n+\t@Override\n+\tpublic void readInputData(InputGate[] inputGates) throws IOException {\n+\t\ttry (InputChannelRecoveredStateHandler stateHandler = new InputChannelRecoveredStateHandler(inputGates)) {\n+\t\t\tread(OperatorSubtaskState::getInputChannelState, stateHandler);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void readOutputData(ResultPartitionWriter[] writers) throws IOException {\n+\t\ttry (ResultSubpartitionRecoveredStateHandler stateHandler = new ResultSubpartitionRecoveredStateHandler(writers)) {\n+\t\t\tread(OperatorSubtaskState::getResultSubpartitionState, stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void read(\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\tfor (Map.Entry<StreamStateHandle, List<Handle>> delegateAndHandles : groupByDelegate(streamSubtaskStates(), stateHandleExtractor).entrySet()) {\n+\t\t\treadSequentially(delegateAndHandles.getKey(), delegateAndHandles.getValue(), stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void readSequentially(\n+\t\t\tStreamStateHandle streamStateHandle,\n+\t\t\tList<Handle> channelStateHandles,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\ttry (FSDataInputStream is = streamStateHandle.openInputStream()) {\n+\t\t\tserializer.readHeader(is);\n+\t\t\tfor (Tuple2<Long, Info> offsetAndChannelInfo : extractOffsetsSorted(channelStateHandles)) {\n+\t\t\t\tchunkReader.readChunk(is, offsetAndChannelInfo.f0, stateHandler, offsetAndChannelInfo.f1);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate Stream<OperatorSubtaskState> streamSubtaskStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().map(Map.Entry::getValue);\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> Map<StreamStateHandle, List<Handle>> groupByDelegate(\n+\t\t\tStream<OperatorSubtaskState> states,\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor) {\n+\t\treturn states\n+\t\t\t.map(stateHandleExtractor).flatMap(Collection::stream)\n+\t\t\t.peek(validate())\n+\t\t\t.collect(groupingBy(AbstractChannelStateHandle::getDelegate));\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> Consumer<Handle> validate() {\n+\t\tSet<Info> seen = new HashSet<>();\n+\t\t// expect each channel to be described only once; otherwise, buffers in channel could be re-ordered\n+\t\treturn handle -> Preconditions.checkState(seen.add(handle.getInfo()), \"duplicate channel info: %s\");\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> List<Tuple2<Long, Info>> extractOffsetsSorted(List<Handle> channelStateHandles) {\n+\t\treturn channelStateHandles\n+\t\t\t.stream()\n+\t\t\t.flatMap(SequentialChannelStateReaderImpl::extractOffsets)\n+\t\t\t.sorted(comparingLong(offsetAndInfo -> offsetAndInfo.f0))\n+\t\t\t.collect(toList());\n+\t}\n+\n+\tprivate static  <Info, Handle extends AbstractChannelStateHandle<Info>> Stream<Tuple2<Long, Info>> extractOffsets(Handle handle) {\n+\t\treturn handle.getOffsets().stream().map(offset -> Tuple2.of(offset, handle.getInfo()));\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws Exception {\n+\t}\n+\n+}\n+\n+interface RecoveredChannelStateHandler<Info, Context> extends AutoCloseable {\n+\tTuple2<ChannelStateByteBuffer, Context> getBuffer(Info info) throws IOException;\n+\n+\tvoid recover(Info info, Context context) throws IOException;\n+}\n+\n+class InputChannelRecoveredStateHandler implements RecoveredChannelStateHandler<InputChannelInfo, Buffer> {\n+\tprivate final InputGate[] inputGates;\n+\n+\tInputChannelRecoveredStateHandler(InputGate[] inputGates) {\n+\t\tthis.inputGates = inputGates;\n+\t}\n+\n+\t@Override\n+\tpublic Tuple2<ChannelStateByteBuffer, Buffer> getBuffer(InputChannelInfo channelInfo) throws IOException {\n+\t\tRecoveredInputChannel channel = getChannel(channelInfo);\n+\t\tBuffer buffer;\n+\t\ttry {\n+\t\t\tbuffer = channel.getBuffer();\n+\t\t\treturn Tuple2.of(wrap(buffer), buffer);\n+\t\t} catch (InterruptedException e) {\n+\t\t\tthrow new IOException(e);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void recover(InputChannelInfo channelInfo, Buffer buffer) {\n+\t\tif (buffer.readableBytes() > 0) {\n+\t\t\tgetChannel(channelInfo).onRecoveredStateBuffer(buffer);\n+\t\t} else {\n+\t\t\tbuffer.recycleBuffer();\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws IOException {\n+\t\t// note that we need to finish all RecoveredInputChannels, not just those with state\n+\t\tfor (final InputGate inputGate : inputGates) {\n+\t\t\tinputGate.finishReadRecoveredState();\n+\t\t}\n+\t}\n+\n+\tprivate RecoveredInputChannel getChannel(InputChannelInfo info) {\n+\t\treturn (RecoveredInputChannel) inputGates[info.getGateIdx()].getChannel(info.getInputChannelIdx());\n+\t}\n+}\n+\n+class ResultSubpartitionRecoveredStateHandler implements RecoveredChannelStateHandler<ResultSubpartitionInfo, Tuple2<BufferBuilder, BufferConsumer>> {\n+\n+\tprivate final ResultPartitionWriter[] writers;\n+\n+\tResultSubpartitionRecoveredStateHandler(ResultPartitionWriter[] writers) {\n+\t\tthis.writers = writers;\n+\t}\n+\n+\t@Override\n+\tpublic Tuple2<ChannelStateByteBuffer, Tuple2<BufferBuilder, BufferConsumer>> getBuffer(ResultSubpartitionInfo subpartitionInfo) throws IOException {\n+\t\tBufferBuilder bufferBuilder;\n+\t\ttry {\n+\t\t\tbufferBuilder = getWriter(subpartitionInfo).getBufferBuilder(subpartitionInfo.getSubPartitionIdx());\n+\t\t} catch (InterruptedException e) {\n+\t\t\tthrow new IOException(e);\n+\t\t}\n+\t\treturn Tuple2.of(wrap(bufferBuilder), Tuple2.of(bufferBuilder, bufferBuilder.createBufferConsumer()));\n+\t}\n+\n+\t@Override\n+\tpublic void recover(ResultSubpartitionInfo subpartitionInfo, Tuple2<BufferBuilder, BufferConsumer> bufferConsumer) throws IOException {\n+\t\tbufferConsumer.f0.finish();\n+\t\tgetWriter(subpartitionInfo).addBufferConsumer(bufferConsumer.f1, subpartitionInfo.getSubPartitionIdx());\n+\t}\n+\n+\tprivate ResultPartitionWriter getWriter(ResultSubpartitionInfo info) {\n+\t\treturn writers[info.getPartitionIdx()];\n+\t}\n+\n+\t@Override\n+\tpublic void close() {\n+\t}\n+}\n+\n+class ChannelStateChunkReader {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 234}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDMwNDAxMA==", "bodyText": "It would easier to read if you add a small POJO instead of using Tuple2.", "url": "https://github.com/apache/flink/pull/13351#discussion_r494304010", "createdAt": "2020-09-24T13:11:23Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImpl.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.fs.FSDataInputStream;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.runtime.checkpoint.StateObjectCollection;\n+import org.apache.flink.runtime.checkpoint.TaskStateSnapshot;\n+import org.apache.flink.runtime.io.network.api.writer.ResultPartitionWriter;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.BufferBuilder;\n+import org.apache.flink.runtime.io.network.buffer.BufferConsumer;\n+import org.apache.flink.runtime.io.network.partition.consumer.InputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel;\n+import org.apache.flink.runtime.state.AbstractChannelStateHandle;\n+import org.apache.flink.runtime.state.StreamStateHandle;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+import java.util.stream.Stream;\n+\n+import static java.util.Comparator.comparingLong;\n+import static java.util.stream.Collectors.groupingBy;\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.flink.runtime.checkpoint.channel.ChannelStateByteBuffer.wrap;\n+\n+/**\n+ * {@link SequentialChannelStateReader} implementation.\n+ */\n+public class SequentialChannelStateReaderImpl implements SequentialChannelStateReader {\n+\n+\tprivate final TaskStateSnapshot taskStateSnapshot;\n+\tprivate final ChannelStateSerializer serializer;\n+\tprivate final ChannelStateChunkReader chunkReader;\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot) {\n+\t\tthis(taskStateSnapshot, new ChannelStateSerializerImpl());\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = new ChannelStateChunkReader(serializer);\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer, ChannelStateChunkReader chunkReader) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = chunkReader;\n+\t}\n+\n+\t@Override\n+\tpublic boolean hasChannelStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().anyMatch(subtaskStateEntry ->\n+\t\t\tsubtaskStateEntry.getValue().getInputChannelState().stream().anyMatch(h -> !h.getOffsets().isEmpty()) ||\n+\t\t\t\tsubtaskStateEntry.getValue().getResultSubpartitionState().stream().anyMatch(h -> !h.getOffsets().isEmpty()));\n+\t}\n+\n+\t@Override\n+\tpublic void readInputData(InputGate[] inputGates) throws IOException {\n+\t\ttry (InputChannelRecoveredStateHandler stateHandler = new InputChannelRecoveredStateHandler(inputGates)) {\n+\t\t\tread(OperatorSubtaskState::getInputChannelState, stateHandler);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void readOutputData(ResultPartitionWriter[] writers) throws IOException {\n+\t\ttry (ResultSubpartitionRecoveredStateHandler stateHandler = new ResultSubpartitionRecoveredStateHandler(writers)) {\n+\t\t\tread(OperatorSubtaskState::getResultSubpartitionState, stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void read(\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\tfor (Map.Entry<StreamStateHandle, List<Handle>> delegateAndHandles : groupByDelegate(streamSubtaskStates(), stateHandleExtractor).entrySet()) {\n+\t\t\treadSequentially(delegateAndHandles.getKey(), delegateAndHandles.getValue(), stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void readSequentially(\n+\t\t\tStreamStateHandle streamStateHandle,\n+\t\t\tList<Handle> channelStateHandles,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\ttry (FSDataInputStream is = streamStateHandle.openInputStream()) {\n+\t\t\tserializer.readHeader(is);\n+\t\t\tfor (Tuple2<Long, Info> offsetAndChannelInfo : extractOffsetsSorted(channelStateHandles)) {\n+\t\t\t\tchunkReader.readChunk(is, offsetAndChannelInfo.f0, stateHandler, offsetAndChannelInfo.f1);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate Stream<OperatorSubtaskState> streamSubtaskStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().map(Map.Entry::getValue);\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> Map<StreamStateHandle, List<Handle>> groupByDelegate(\n+\t\t\tStream<OperatorSubtaskState> states,\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor) {\n+\t\treturn states\n+\t\t\t.map(stateHandleExtractor).flatMap(Collection::stream)\n+\t\t\t.peek(validate())\n+\t\t\t.collect(groupingBy(AbstractChannelStateHandle::getDelegate));\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> Consumer<Handle> validate() {\n+\t\tSet<Info> seen = new HashSet<>();\n+\t\t// expect each channel to be described only once; otherwise, buffers in channel could be re-ordered\n+\t\treturn handle -> Preconditions.checkState(seen.add(handle.getInfo()), \"duplicate channel info: %s\");\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> List<Tuple2<Long, Info>> extractOffsetsSorted(List<Handle> channelStateHandles) {\n+\t\treturn channelStateHandles\n+\t\t\t.stream()\n+\t\t\t.flatMap(SequentialChannelStateReaderImpl::extractOffsets)\n+\t\t\t.sorted(comparingLong(offsetAndInfo -> offsetAndInfo.f0))\n+\t\t\t.collect(toList());\n+\t}\n+\n+\tprivate static  <Info, Handle extends AbstractChannelStateHandle<Info>> Stream<Tuple2<Long, Info>> extractOffsets(Handle handle) {\n+\t\treturn handle.getOffsets().stream().map(offset -> Tuple2.of(offset, handle.getInfo()));\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws Exception {\n+\t}\n+\n+}\n+\n+interface RecoveredChannelStateHandler<Info, Context> extends AutoCloseable {\n+\tTuple2<ChannelStateByteBuffer, Context> getBuffer(Info info) throws IOException;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 154}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDMwNjU4Mg==", "bodyText": "rename parameter to bufferBuilderAndConsumer?", "url": "https://github.com/apache/flink/pull/13351#discussion_r494306582", "createdAt": "2020-09-24T13:14:53Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImpl.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.fs.FSDataInputStream;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.runtime.checkpoint.StateObjectCollection;\n+import org.apache.flink.runtime.checkpoint.TaskStateSnapshot;\n+import org.apache.flink.runtime.io.network.api.writer.ResultPartitionWriter;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.BufferBuilder;\n+import org.apache.flink.runtime.io.network.buffer.BufferConsumer;\n+import org.apache.flink.runtime.io.network.partition.consumer.InputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel;\n+import org.apache.flink.runtime.state.AbstractChannelStateHandle;\n+import org.apache.flink.runtime.state.StreamStateHandle;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+import java.util.stream.Stream;\n+\n+import static java.util.Comparator.comparingLong;\n+import static java.util.stream.Collectors.groupingBy;\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.flink.runtime.checkpoint.channel.ChannelStateByteBuffer.wrap;\n+\n+/**\n+ * {@link SequentialChannelStateReader} implementation.\n+ */\n+public class SequentialChannelStateReaderImpl implements SequentialChannelStateReader {\n+\n+\tprivate final TaskStateSnapshot taskStateSnapshot;\n+\tprivate final ChannelStateSerializer serializer;\n+\tprivate final ChannelStateChunkReader chunkReader;\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot) {\n+\t\tthis(taskStateSnapshot, new ChannelStateSerializerImpl());\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = new ChannelStateChunkReader(serializer);\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer, ChannelStateChunkReader chunkReader) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = chunkReader;\n+\t}\n+\n+\t@Override\n+\tpublic boolean hasChannelStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().anyMatch(subtaskStateEntry ->\n+\t\t\tsubtaskStateEntry.getValue().getInputChannelState().stream().anyMatch(h -> !h.getOffsets().isEmpty()) ||\n+\t\t\t\tsubtaskStateEntry.getValue().getResultSubpartitionState().stream().anyMatch(h -> !h.getOffsets().isEmpty()));\n+\t}\n+\n+\t@Override\n+\tpublic void readInputData(InputGate[] inputGates) throws IOException {\n+\t\ttry (InputChannelRecoveredStateHandler stateHandler = new InputChannelRecoveredStateHandler(inputGates)) {\n+\t\t\tread(OperatorSubtaskState::getInputChannelState, stateHandler);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void readOutputData(ResultPartitionWriter[] writers) throws IOException {\n+\t\ttry (ResultSubpartitionRecoveredStateHandler stateHandler = new ResultSubpartitionRecoveredStateHandler(writers)) {\n+\t\t\tread(OperatorSubtaskState::getResultSubpartitionState, stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void read(\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\tfor (Map.Entry<StreamStateHandle, List<Handle>> delegateAndHandles : groupByDelegate(streamSubtaskStates(), stateHandleExtractor).entrySet()) {\n+\t\t\treadSequentially(delegateAndHandles.getKey(), delegateAndHandles.getValue(), stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void readSequentially(\n+\t\t\tStreamStateHandle streamStateHandle,\n+\t\t\tList<Handle> channelStateHandles,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\ttry (FSDataInputStream is = streamStateHandle.openInputStream()) {\n+\t\t\tserializer.readHeader(is);\n+\t\t\tfor (Tuple2<Long, Info> offsetAndChannelInfo : extractOffsetsSorted(channelStateHandles)) {\n+\t\t\t\tchunkReader.readChunk(is, offsetAndChannelInfo.f0, stateHandler, offsetAndChannelInfo.f1);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate Stream<OperatorSubtaskState> streamSubtaskStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().map(Map.Entry::getValue);\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> Map<StreamStateHandle, List<Handle>> groupByDelegate(\n+\t\t\tStream<OperatorSubtaskState> states,\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor) {\n+\t\treturn states\n+\t\t\t.map(stateHandleExtractor).flatMap(Collection::stream)\n+\t\t\t.peek(validate())\n+\t\t\t.collect(groupingBy(AbstractChannelStateHandle::getDelegate));\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> Consumer<Handle> validate() {\n+\t\tSet<Info> seen = new HashSet<>();\n+\t\t// expect each channel to be described only once; otherwise, buffers in channel could be re-ordered\n+\t\treturn handle -> Preconditions.checkState(seen.add(handle.getInfo()), \"duplicate channel info: %s\");\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> List<Tuple2<Long, Info>> extractOffsetsSorted(List<Handle> channelStateHandles) {\n+\t\treturn channelStateHandles\n+\t\t\t.stream()\n+\t\t\t.flatMap(SequentialChannelStateReaderImpl::extractOffsets)\n+\t\t\t.sorted(comparingLong(offsetAndInfo -> offsetAndInfo.f0))\n+\t\t\t.collect(toList());\n+\t}\n+\n+\tprivate static  <Info, Handle extends AbstractChannelStateHandle<Info>> Stream<Tuple2<Long, Info>> extractOffsets(Handle handle) {\n+\t\treturn handle.getOffsets().stream().map(offset -> Tuple2.of(offset, handle.getInfo()));\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws Exception {\n+\t}\n+\n+}\n+\n+interface RecoveredChannelStateHandler<Info, Context> extends AutoCloseable {\n+\tTuple2<ChannelStateByteBuffer, Context> getBuffer(Info info) throws IOException;\n+\n+\tvoid recover(Info info, Context context) throws IOException;\n+}\n+\n+class InputChannelRecoveredStateHandler implements RecoveredChannelStateHandler<InputChannelInfo, Buffer> {\n+\tprivate final InputGate[] inputGates;\n+\n+\tInputChannelRecoveredStateHandler(InputGate[] inputGates) {\n+\t\tthis.inputGates = inputGates;\n+\t}\n+\n+\t@Override\n+\tpublic Tuple2<ChannelStateByteBuffer, Buffer> getBuffer(InputChannelInfo channelInfo) throws IOException {\n+\t\tRecoveredInputChannel channel = getChannel(channelInfo);\n+\t\tBuffer buffer;\n+\t\ttry {\n+\t\t\tbuffer = channel.getBuffer();\n+\t\t\treturn Tuple2.of(wrap(buffer), buffer);\n+\t\t} catch (InterruptedException e) {\n+\t\t\tthrow new IOException(e);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void recover(InputChannelInfo channelInfo, Buffer buffer) {\n+\t\tif (buffer.readableBytes() > 0) {\n+\t\t\tgetChannel(channelInfo).onRecoveredStateBuffer(buffer);\n+\t\t} else {\n+\t\t\tbuffer.recycleBuffer();\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws IOException {\n+\t\t// note that we need to finish all RecoveredInputChannels, not just those with state\n+\t\tfor (final InputGate inputGate : inputGates) {\n+\t\t\tinputGate.finishReadRecoveredState();\n+\t\t}\n+\t}\n+\n+\tprivate RecoveredInputChannel getChannel(InputChannelInfo info) {\n+\t\treturn (RecoveredInputChannel) inputGates[info.getGateIdx()].getChannel(info.getInputChannelIdx());\n+\t}\n+}\n+\n+class ResultSubpartitionRecoveredStateHandler implements RecoveredChannelStateHandler<ResultSubpartitionInfo, Tuple2<BufferBuilder, BufferConsumer>> {\n+\n+\tprivate final ResultPartitionWriter[] writers;\n+\n+\tResultSubpartitionRecoveredStateHandler(ResultPartitionWriter[] writers) {\n+\t\tthis.writers = writers;\n+\t}\n+\n+\t@Override\n+\tpublic Tuple2<ChannelStateByteBuffer, Tuple2<BufferBuilder, BufferConsumer>> getBuffer(ResultSubpartitionInfo subpartitionInfo) throws IOException {\n+\t\tBufferBuilder bufferBuilder;\n+\t\ttry {\n+\t\t\tbufferBuilder = getWriter(subpartitionInfo).getBufferBuilder(subpartitionInfo.getSubPartitionIdx());\n+\t\t} catch (InterruptedException e) {\n+\t\t\tthrow new IOException(e);\n+\t\t}\n+\t\treturn Tuple2.of(wrap(bufferBuilder), Tuple2.of(bufferBuilder, bufferBuilder.createBufferConsumer()));\n+\t}\n+\n+\t@Override\n+\tpublic void recover(ResultSubpartitionInfo subpartitionInfo, Tuple2<BufferBuilder, BufferConsumer> bufferConsumer) throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 220}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDMwODEzNw==", "bodyText": "Wouldn't it be enough to just create the consumer in recover? Then you also could go with BufferBuilder as Context.", "url": "https://github.com/apache/flink/pull/13351#discussion_r494308137", "createdAt": "2020-09-24T13:16:57Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImpl.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.fs.FSDataInputStream;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.runtime.checkpoint.StateObjectCollection;\n+import org.apache.flink.runtime.checkpoint.TaskStateSnapshot;\n+import org.apache.flink.runtime.io.network.api.writer.ResultPartitionWriter;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.BufferBuilder;\n+import org.apache.flink.runtime.io.network.buffer.BufferConsumer;\n+import org.apache.flink.runtime.io.network.partition.consumer.InputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel;\n+import org.apache.flink.runtime.state.AbstractChannelStateHandle;\n+import org.apache.flink.runtime.state.StreamStateHandle;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+import java.util.stream.Stream;\n+\n+import static java.util.Comparator.comparingLong;\n+import static java.util.stream.Collectors.groupingBy;\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.flink.runtime.checkpoint.channel.ChannelStateByteBuffer.wrap;\n+\n+/**\n+ * {@link SequentialChannelStateReader} implementation.\n+ */\n+public class SequentialChannelStateReaderImpl implements SequentialChannelStateReader {\n+\n+\tprivate final TaskStateSnapshot taskStateSnapshot;\n+\tprivate final ChannelStateSerializer serializer;\n+\tprivate final ChannelStateChunkReader chunkReader;\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot) {\n+\t\tthis(taskStateSnapshot, new ChannelStateSerializerImpl());\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = new ChannelStateChunkReader(serializer);\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer, ChannelStateChunkReader chunkReader) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = chunkReader;\n+\t}\n+\n+\t@Override\n+\tpublic boolean hasChannelStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().anyMatch(subtaskStateEntry ->\n+\t\t\tsubtaskStateEntry.getValue().getInputChannelState().stream().anyMatch(h -> !h.getOffsets().isEmpty()) ||\n+\t\t\t\tsubtaskStateEntry.getValue().getResultSubpartitionState().stream().anyMatch(h -> !h.getOffsets().isEmpty()));\n+\t}\n+\n+\t@Override\n+\tpublic void readInputData(InputGate[] inputGates) throws IOException {\n+\t\ttry (InputChannelRecoveredStateHandler stateHandler = new InputChannelRecoveredStateHandler(inputGates)) {\n+\t\t\tread(OperatorSubtaskState::getInputChannelState, stateHandler);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void readOutputData(ResultPartitionWriter[] writers) throws IOException {\n+\t\ttry (ResultSubpartitionRecoveredStateHandler stateHandler = new ResultSubpartitionRecoveredStateHandler(writers)) {\n+\t\t\tread(OperatorSubtaskState::getResultSubpartitionState, stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void read(\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\tfor (Map.Entry<StreamStateHandle, List<Handle>> delegateAndHandles : groupByDelegate(streamSubtaskStates(), stateHandleExtractor).entrySet()) {\n+\t\t\treadSequentially(delegateAndHandles.getKey(), delegateAndHandles.getValue(), stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void readSequentially(\n+\t\t\tStreamStateHandle streamStateHandle,\n+\t\t\tList<Handle> channelStateHandles,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\ttry (FSDataInputStream is = streamStateHandle.openInputStream()) {\n+\t\t\tserializer.readHeader(is);\n+\t\t\tfor (Tuple2<Long, Info> offsetAndChannelInfo : extractOffsetsSorted(channelStateHandles)) {\n+\t\t\t\tchunkReader.readChunk(is, offsetAndChannelInfo.f0, stateHandler, offsetAndChannelInfo.f1);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate Stream<OperatorSubtaskState> streamSubtaskStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().map(Map.Entry::getValue);\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> Map<StreamStateHandle, List<Handle>> groupByDelegate(\n+\t\t\tStream<OperatorSubtaskState> states,\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor) {\n+\t\treturn states\n+\t\t\t.map(stateHandleExtractor).flatMap(Collection::stream)\n+\t\t\t.peek(validate())\n+\t\t\t.collect(groupingBy(AbstractChannelStateHandle::getDelegate));\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> Consumer<Handle> validate() {\n+\t\tSet<Info> seen = new HashSet<>();\n+\t\t// expect each channel to be described only once; otherwise, buffers in channel could be re-ordered\n+\t\treturn handle -> Preconditions.checkState(seen.add(handle.getInfo()), \"duplicate channel info: %s\");\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> List<Tuple2<Long, Info>> extractOffsetsSorted(List<Handle> channelStateHandles) {\n+\t\treturn channelStateHandles\n+\t\t\t.stream()\n+\t\t\t.flatMap(SequentialChannelStateReaderImpl::extractOffsets)\n+\t\t\t.sorted(comparingLong(offsetAndInfo -> offsetAndInfo.f0))\n+\t\t\t.collect(toList());\n+\t}\n+\n+\tprivate static  <Info, Handle extends AbstractChannelStateHandle<Info>> Stream<Tuple2<Long, Info>> extractOffsets(Handle handle) {\n+\t\treturn handle.getOffsets().stream().map(offset -> Tuple2.of(offset, handle.getInfo()));\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws Exception {\n+\t}\n+\n+}\n+\n+interface RecoveredChannelStateHandler<Info, Context> extends AutoCloseable {\n+\tTuple2<ChannelStateByteBuffer, Context> getBuffer(Info info) throws IOException;\n+\n+\tvoid recover(Info info, Context context) throws IOException;\n+}\n+\n+class InputChannelRecoveredStateHandler implements RecoveredChannelStateHandler<InputChannelInfo, Buffer> {\n+\tprivate final InputGate[] inputGates;\n+\n+\tInputChannelRecoveredStateHandler(InputGate[] inputGates) {\n+\t\tthis.inputGates = inputGates;\n+\t}\n+\n+\t@Override\n+\tpublic Tuple2<ChannelStateByteBuffer, Buffer> getBuffer(InputChannelInfo channelInfo) throws IOException {\n+\t\tRecoveredInputChannel channel = getChannel(channelInfo);\n+\t\tBuffer buffer;\n+\t\ttry {\n+\t\t\tbuffer = channel.getBuffer();\n+\t\t\treturn Tuple2.of(wrap(buffer), buffer);\n+\t\t} catch (InterruptedException e) {\n+\t\t\tthrow new IOException(e);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void recover(InputChannelInfo channelInfo, Buffer buffer) {\n+\t\tif (buffer.readableBytes() > 0) {\n+\t\t\tgetChannel(channelInfo).onRecoveredStateBuffer(buffer);\n+\t\t} else {\n+\t\t\tbuffer.recycleBuffer();\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws IOException {\n+\t\t// note that we need to finish all RecoveredInputChannels, not just those with state\n+\t\tfor (final InputGate inputGate : inputGates) {\n+\t\t\tinputGate.finishReadRecoveredState();\n+\t\t}\n+\t}\n+\n+\tprivate RecoveredInputChannel getChannel(InputChannelInfo info) {\n+\t\treturn (RecoveredInputChannel) inputGates[info.getGateIdx()].getChannel(info.getInputChannelIdx());\n+\t}\n+}\n+\n+class ResultSubpartitionRecoveredStateHandler implements RecoveredChannelStateHandler<ResultSubpartitionInfo, Tuple2<BufferBuilder, BufferConsumer>> {\n+\n+\tprivate final ResultPartitionWriter[] writers;\n+\n+\tResultSubpartitionRecoveredStateHandler(ResultPartitionWriter[] writers) {\n+\t\tthis.writers = writers;\n+\t}\n+\n+\t@Override\n+\tpublic Tuple2<ChannelStateByteBuffer, Tuple2<BufferBuilder, BufferConsumer>> getBuffer(ResultSubpartitionInfo subpartitionInfo) throws IOException {\n+\t\tBufferBuilder bufferBuilder;\n+\t\ttry {\n+\t\t\tbufferBuilder = getWriter(subpartitionInfo).getBufferBuilder(subpartitionInfo.getSubPartitionIdx());\n+\t\t} catch (InterruptedException e) {\n+\t\t\tthrow new IOException(e);\n+\t\t}\n+\t\treturn Tuple2.of(wrap(bufferBuilder), Tuple2.of(bufferBuilder, bufferBuilder.createBufferConsumer()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 216}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDMxNDY1MQ==", "bodyText": "Should the context also be cleaned up or doesn't it matter because it's failing anyways?", "url": "https://github.com/apache/flink/pull/13351#discussion_r494314651", "createdAt": "2020-09-24T13:26:07Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImpl.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.fs.FSDataInputStream;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.runtime.checkpoint.StateObjectCollection;\n+import org.apache.flink.runtime.checkpoint.TaskStateSnapshot;\n+import org.apache.flink.runtime.io.network.api.writer.ResultPartitionWriter;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.BufferBuilder;\n+import org.apache.flink.runtime.io.network.buffer.BufferConsumer;\n+import org.apache.flink.runtime.io.network.partition.consumer.InputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel;\n+import org.apache.flink.runtime.state.AbstractChannelStateHandle;\n+import org.apache.flink.runtime.state.StreamStateHandle;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+import java.util.stream.Stream;\n+\n+import static java.util.Comparator.comparingLong;\n+import static java.util.stream.Collectors.groupingBy;\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.flink.runtime.checkpoint.channel.ChannelStateByteBuffer.wrap;\n+\n+/**\n+ * {@link SequentialChannelStateReader} implementation.\n+ */\n+public class SequentialChannelStateReaderImpl implements SequentialChannelStateReader {\n+\n+\tprivate final TaskStateSnapshot taskStateSnapshot;\n+\tprivate final ChannelStateSerializer serializer;\n+\tprivate final ChannelStateChunkReader chunkReader;\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot) {\n+\t\tthis(taskStateSnapshot, new ChannelStateSerializerImpl());\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = new ChannelStateChunkReader(serializer);\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer, ChannelStateChunkReader chunkReader) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = chunkReader;\n+\t}\n+\n+\t@Override\n+\tpublic boolean hasChannelStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().anyMatch(subtaskStateEntry ->\n+\t\t\tsubtaskStateEntry.getValue().getInputChannelState().stream().anyMatch(h -> !h.getOffsets().isEmpty()) ||\n+\t\t\t\tsubtaskStateEntry.getValue().getResultSubpartitionState().stream().anyMatch(h -> !h.getOffsets().isEmpty()));\n+\t}\n+\n+\t@Override\n+\tpublic void readInputData(InputGate[] inputGates) throws IOException {\n+\t\ttry (InputChannelRecoveredStateHandler stateHandler = new InputChannelRecoveredStateHandler(inputGates)) {\n+\t\t\tread(OperatorSubtaskState::getInputChannelState, stateHandler);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void readOutputData(ResultPartitionWriter[] writers) throws IOException {\n+\t\ttry (ResultSubpartitionRecoveredStateHandler stateHandler = new ResultSubpartitionRecoveredStateHandler(writers)) {\n+\t\t\tread(OperatorSubtaskState::getResultSubpartitionState, stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void read(\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\tfor (Map.Entry<StreamStateHandle, List<Handle>> delegateAndHandles : groupByDelegate(streamSubtaskStates(), stateHandleExtractor).entrySet()) {\n+\t\t\treadSequentially(delegateAndHandles.getKey(), delegateAndHandles.getValue(), stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void readSequentially(\n+\t\t\tStreamStateHandle streamStateHandle,\n+\t\t\tList<Handle> channelStateHandles,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\ttry (FSDataInputStream is = streamStateHandle.openInputStream()) {\n+\t\t\tserializer.readHeader(is);\n+\t\t\tfor (Tuple2<Long, Info> offsetAndChannelInfo : extractOffsetsSorted(channelStateHandles)) {\n+\t\t\t\tchunkReader.readChunk(is, offsetAndChannelInfo.f0, stateHandler, offsetAndChannelInfo.f1);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate Stream<OperatorSubtaskState> streamSubtaskStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().map(Map.Entry::getValue);\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> Map<StreamStateHandle, List<Handle>> groupByDelegate(\n+\t\t\tStream<OperatorSubtaskState> states,\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor) {\n+\t\treturn states\n+\t\t\t.map(stateHandleExtractor).flatMap(Collection::stream)\n+\t\t\t.peek(validate())\n+\t\t\t.collect(groupingBy(AbstractChannelStateHandle::getDelegate));\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> Consumer<Handle> validate() {\n+\t\tSet<Info> seen = new HashSet<>();\n+\t\t// expect each channel to be described only once; otherwise, buffers in channel could be re-ordered\n+\t\treturn handle -> Preconditions.checkState(seen.add(handle.getInfo()), \"duplicate channel info: %s\");\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> List<Tuple2<Long, Info>> extractOffsetsSorted(List<Handle> channelStateHandles) {\n+\t\treturn channelStateHandles\n+\t\t\t.stream()\n+\t\t\t.flatMap(SequentialChannelStateReaderImpl::extractOffsets)\n+\t\t\t.sorted(comparingLong(offsetAndInfo -> offsetAndInfo.f0))\n+\t\t\t.collect(toList());\n+\t}\n+\n+\tprivate static  <Info, Handle extends AbstractChannelStateHandle<Info>> Stream<Tuple2<Long, Info>> extractOffsets(Handle handle) {\n+\t\treturn handle.getOffsets().stream().map(offset -> Tuple2.of(offset, handle.getInfo()));\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws Exception {\n+\t}\n+\n+}\n+\n+interface RecoveredChannelStateHandler<Info, Context> extends AutoCloseable {\n+\tTuple2<ChannelStateByteBuffer, Context> getBuffer(Info info) throws IOException;\n+\n+\tvoid recover(Info info, Context context) throws IOException;\n+}\n+\n+class InputChannelRecoveredStateHandler implements RecoveredChannelStateHandler<InputChannelInfo, Buffer> {\n+\tprivate final InputGate[] inputGates;\n+\n+\tInputChannelRecoveredStateHandler(InputGate[] inputGates) {\n+\t\tthis.inputGates = inputGates;\n+\t}\n+\n+\t@Override\n+\tpublic Tuple2<ChannelStateByteBuffer, Buffer> getBuffer(InputChannelInfo channelInfo) throws IOException {\n+\t\tRecoveredInputChannel channel = getChannel(channelInfo);\n+\t\tBuffer buffer;\n+\t\ttry {\n+\t\t\tbuffer = channel.getBuffer();\n+\t\t\treturn Tuple2.of(wrap(buffer), buffer);\n+\t\t} catch (InterruptedException e) {\n+\t\t\tthrow new IOException(e);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void recover(InputChannelInfo channelInfo, Buffer buffer) {\n+\t\tif (buffer.readableBytes() > 0) {\n+\t\t\tgetChannel(channelInfo).onRecoveredStateBuffer(buffer);\n+\t\t} else {\n+\t\t\tbuffer.recycleBuffer();\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws IOException {\n+\t\t// note that we need to finish all RecoveredInputChannels, not just those with state\n+\t\tfor (final InputGate inputGate : inputGates) {\n+\t\t\tinputGate.finishReadRecoveredState();\n+\t\t}\n+\t}\n+\n+\tprivate RecoveredInputChannel getChannel(InputChannelInfo info) {\n+\t\treturn (RecoveredInputChannel) inputGates[info.getGateIdx()].getChannel(info.getInputChannelIdx());\n+\t}\n+}\n+\n+class ResultSubpartitionRecoveredStateHandler implements RecoveredChannelStateHandler<ResultSubpartitionInfo, Tuple2<BufferBuilder, BufferConsumer>> {\n+\n+\tprivate final ResultPartitionWriter[] writers;\n+\n+\tResultSubpartitionRecoveredStateHandler(ResultPartitionWriter[] writers) {\n+\t\tthis.writers = writers;\n+\t}\n+\n+\t@Override\n+\tpublic Tuple2<ChannelStateByteBuffer, Tuple2<BufferBuilder, BufferConsumer>> getBuffer(ResultSubpartitionInfo subpartitionInfo) throws IOException {\n+\t\tBufferBuilder bufferBuilder;\n+\t\ttry {\n+\t\t\tbufferBuilder = getWriter(subpartitionInfo).getBufferBuilder(subpartitionInfo.getSubPartitionIdx());\n+\t\t} catch (InterruptedException e) {\n+\t\t\tthrow new IOException(e);\n+\t\t}\n+\t\treturn Tuple2.of(wrap(bufferBuilder), Tuple2.of(bufferBuilder, bufferBuilder.createBufferConsumer()));\n+\t}\n+\n+\t@Override\n+\tpublic void recover(ResultSubpartitionInfo subpartitionInfo, Tuple2<BufferBuilder, BufferConsumer> bufferConsumer) throws IOException {\n+\t\tbufferConsumer.f0.finish();\n+\t\tgetWriter(subpartitionInfo).addBufferConsumer(bufferConsumer.f1, subpartitionInfo.getSubPartitionIdx());\n+\t}\n+\n+\tprivate ResultPartitionWriter getWriter(ResultSubpartitionInfo info) {\n+\t\treturn writers[info.getPartitionIdx()];\n+\t}\n+\n+\t@Override\n+\tpublic void close() {\n+\t}\n+}\n+\n+class ChannelStateChunkReader {\n+\tprivate final ChannelStateSerializer serializer;\n+\n+\tChannelStateChunkReader(ChannelStateSerializer serializer) {\n+\t\tthis.serializer = serializer;\n+\t}\n+\n+\t<Info, Context, Handle extends AbstractChannelStateHandle<Info>> void readChunk(\n+\t\t\tFSDataInputStream source,\n+\t\t\tlong sourceOffset,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler,\n+\t\t\tInfo channelInfo) throws IOException {\n+\t\tif (source.getPos() != sourceOffset) {\n+\t\t\tsource.seek(sourceOffset);\n+\t\t}\n+\t\tint length = serializer.readLength(source);\n+\t\twhile (length > 0) {\n+\t\t\tTuple2<ChannelStateByteBuffer, Context> bufferWithContext = stateHandler.getBuffer(channelInfo);\n+\t\t\ttry {\n+\t\t\t\twhile (length > 0 && bufferWithContext.f0.isWritable()) {\n+\t\t\t\t\tlength -= serializer.readData(source, bufferWithContext.f0, length);\n+\t\t\t\t}\n+\t\t\t} catch (Exception e) {\n+\t\t\t\tbufferWithContext.f0.recycle();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 257}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDMxOTg5Nw==", "bodyText": "This exception handling looks suspicious to me. I'd probably let the exception bubble up as is.\nIf you want to translate for some reason, I'd still use Thread.currentThread().interrupt(); to restore the interruption flag of the thread.", "url": "https://github.com/apache/flink/pull/13351#discussion_r494319897", "createdAt": "2020-09-24T13:33:04Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImpl.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.fs.FSDataInputStream;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.runtime.checkpoint.StateObjectCollection;\n+import org.apache.flink.runtime.checkpoint.TaskStateSnapshot;\n+import org.apache.flink.runtime.io.network.api.writer.ResultPartitionWriter;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.BufferBuilder;\n+import org.apache.flink.runtime.io.network.buffer.BufferConsumer;\n+import org.apache.flink.runtime.io.network.partition.consumer.InputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel;\n+import org.apache.flink.runtime.state.AbstractChannelStateHandle;\n+import org.apache.flink.runtime.state.StreamStateHandle;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+import java.util.stream.Stream;\n+\n+import static java.util.Comparator.comparingLong;\n+import static java.util.stream.Collectors.groupingBy;\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.flink.runtime.checkpoint.channel.ChannelStateByteBuffer.wrap;\n+\n+/**\n+ * {@link SequentialChannelStateReader} implementation.\n+ */\n+public class SequentialChannelStateReaderImpl implements SequentialChannelStateReader {\n+\n+\tprivate final TaskStateSnapshot taskStateSnapshot;\n+\tprivate final ChannelStateSerializer serializer;\n+\tprivate final ChannelStateChunkReader chunkReader;\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot) {\n+\t\tthis(taskStateSnapshot, new ChannelStateSerializerImpl());\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = new ChannelStateChunkReader(serializer);\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer, ChannelStateChunkReader chunkReader) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = chunkReader;\n+\t}\n+\n+\t@Override\n+\tpublic boolean hasChannelStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().anyMatch(subtaskStateEntry ->\n+\t\t\tsubtaskStateEntry.getValue().getInputChannelState().stream().anyMatch(h -> !h.getOffsets().isEmpty()) ||\n+\t\t\t\tsubtaskStateEntry.getValue().getResultSubpartitionState().stream().anyMatch(h -> !h.getOffsets().isEmpty()));\n+\t}\n+\n+\t@Override\n+\tpublic void readInputData(InputGate[] inputGates) throws IOException {\n+\t\ttry (InputChannelRecoveredStateHandler stateHandler = new InputChannelRecoveredStateHandler(inputGates)) {\n+\t\t\tread(OperatorSubtaskState::getInputChannelState, stateHandler);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void readOutputData(ResultPartitionWriter[] writers) throws IOException {\n+\t\ttry (ResultSubpartitionRecoveredStateHandler stateHandler = new ResultSubpartitionRecoveredStateHandler(writers)) {\n+\t\t\tread(OperatorSubtaskState::getResultSubpartitionState, stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void read(\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\tfor (Map.Entry<StreamStateHandle, List<Handle>> delegateAndHandles : groupByDelegate(streamSubtaskStates(), stateHandleExtractor).entrySet()) {\n+\t\t\treadSequentially(delegateAndHandles.getKey(), delegateAndHandles.getValue(), stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void readSequentially(\n+\t\t\tStreamStateHandle streamStateHandle,\n+\t\t\tList<Handle> channelStateHandles,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\ttry (FSDataInputStream is = streamStateHandle.openInputStream()) {\n+\t\t\tserializer.readHeader(is);\n+\t\t\tfor (Tuple2<Long, Info> offsetAndChannelInfo : extractOffsetsSorted(channelStateHandles)) {\n+\t\t\t\tchunkReader.readChunk(is, offsetAndChannelInfo.f0, stateHandler, offsetAndChannelInfo.f1);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate Stream<OperatorSubtaskState> streamSubtaskStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().map(Map.Entry::getValue);\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> Map<StreamStateHandle, List<Handle>> groupByDelegate(\n+\t\t\tStream<OperatorSubtaskState> states,\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor) {\n+\t\treturn states\n+\t\t\t.map(stateHandleExtractor).flatMap(Collection::stream)\n+\t\t\t.peek(validate())\n+\t\t\t.collect(groupingBy(AbstractChannelStateHandle::getDelegate));\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> Consumer<Handle> validate() {\n+\t\tSet<Info> seen = new HashSet<>();\n+\t\t// expect each channel to be described only once; otherwise, buffers in channel could be re-ordered\n+\t\treturn handle -> Preconditions.checkState(seen.add(handle.getInfo()), \"duplicate channel info: %s\");\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> List<Tuple2<Long, Info>> extractOffsetsSorted(List<Handle> channelStateHandles) {\n+\t\treturn channelStateHandles\n+\t\t\t.stream()\n+\t\t\t.flatMap(SequentialChannelStateReaderImpl::extractOffsets)\n+\t\t\t.sorted(comparingLong(offsetAndInfo -> offsetAndInfo.f0))\n+\t\t\t.collect(toList());\n+\t}\n+\n+\tprivate static  <Info, Handle extends AbstractChannelStateHandle<Info>> Stream<Tuple2<Long, Info>> extractOffsets(Handle handle) {\n+\t\treturn handle.getOffsets().stream().map(offset -> Tuple2.of(offset, handle.getInfo()));\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws Exception {\n+\t}\n+\n+}\n+\n+interface RecoveredChannelStateHandler<Info, Context> extends AutoCloseable {\n+\tTuple2<ChannelStateByteBuffer, Context> getBuffer(Info info) throws IOException;\n+\n+\tvoid recover(Info info, Context context) throws IOException;\n+}\n+\n+class InputChannelRecoveredStateHandler implements RecoveredChannelStateHandler<InputChannelInfo, Buffer> {\n+\tprivate final InputGate[] inputGates;\n+\n+\tInputChannelRecoveredStateHandler(InputGate[] inputGates) {\n+\t\tthis.inputGates = inputGates;\n+\t}\n+\n+\t@Override\n+\tpublic Tuple2<ChannelStateByteBuffer, Buffer> getBuffer(InputChannelInfo channelInfo) throws IOException {\n+\t\tRecoveredInputChannel channel = getChannel(channelInfo);\n+\t\tBuffer buffer;\n+\t\ttry {\n+\t\t\tbuffer = channel.getBuffer();\n+\t\t\treturn Tuple2.of(wrap(buffer), buffer);\n+\t\t} catch (InterruptedException e) {\n+\t\t\tthrow new IOException(e);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void recover(InputChannelInfo channelInfo, Buffer buffer) {\n+\t\tif (buffer.readableBytes() > 0) {\n+\t\t\tgetChannel(channelInfo).onRecoveredStateBuffer(buffer);\n+\t\t} else {\n+\t\t\tbuffer.recycleBuffer();\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws IOException {\n+\t\t// note that we need to finish all RecoveredInputChannels, not just those with state\n+\t\tfor (final InputGate inputGate : inputGates) {\n+\t\t\tinputGate.finishReadRecoveredState();\n+\t\t}\n+\t}\n+\n+\tprivate RecoveredInputChannel getChannel(InputChannelInfo info) {\n+\t\treturn (RecoveredInputChannel) inputGates[info.getGateIdx()].getChannel(info.getInputChannelIdx());\n+\t}\n+}\n+\n+class ResultSubpartitionRecoveredStateHandler implements RecoveredChannelStateHandler<ResultSubpartitionInfo, Tuple2<BufferBuilder, BufferConsumer>> {\n+\n+\tprivate final ResultPartitionWriter[] writers;\n+\n+\tResultSubpartitionRecoveredStateHandler(ResultPartitionWriter[] writers) {\n+\t\tthis.writers = writers;\n+\t}\n+\n+\t@Override\n+\tpublic Tuple2<ChannelStateByteBuffer, Tuple2<BufferBuilder, BufferConsumer>> getBuffer(ResultSubpartitionInfo subpartitionInfo) throws IOException {\n+\t\tBufferBuilder bufferBuilder;\n+\t\ttry {\n+\t\t\tbufferBuilder = getWriter(subpartitionInfo).getBufferBuilder(subpartitionInfo.getSubPartitionIdx());\n+\t\t} catch (InterruptedException e) {\n+\t\t\tthrow new IOException(e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 214}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDMyMTU3Mg==", "bodyText": "This check is missing on output side. Add there for symmetry?\nHowever, I'd probably rather skip writing empty buffers altogether and just use a checkState to verify it here (and remove else branch).", "url": "https://github.com/apache/flink/pull/13351#discussion_r494321572", "createdAt": "2020-09-24T13:35:18Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImpl.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.fs.FSDataInputStream;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.runtime.checkpoint.StateObjectCollection;\n+import org.apache.flink.runtime.checkpoint.TaskStateSnapshot;\n+import org.apache.flink.runtime.io.network.api.writer.ResultPartitionWriter;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.BufferBuilder;\n+import org.apache.flink.runtime.io.network.buffer.BufferConsumer;\n+import org.apache.flink.runtime.io.network.partition.consumer.InputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel;\n+import org.apache.flink.runtime.state.AbstractChannelStateHandle;\n+import org.apache.flink.runtime.state.StreamStateHandle;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+import java.util.stream.Stream;\n+\n+import static java.util.Comparator.comparingLong;\n+import static java.util.stream.Collectors.groupingBy;\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.flink.runtime.checkpoint.channel.ChannelStateByteBuffer.wrap;\n+\n+/**\n+ * {@link SequentialChannelStateReader} implementation.\n+ */\n+public class SequentialChannelStateReaderImpl implements SequentialChannelStateReader {\n+\n+\tprivate final TaskStateSnapshot taskStateSnapshot;\n+\tprivate final ChannelStateSerializer serializer;\n+\tprivate final ChannelStateChunkReader chunkReader;\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot) {\n+\t\tthis(taskStateSnapshot, new ChannelStateSerializerImpl());\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = new ChannelStateChunkReader(serializer);\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer, ChannelStateChunkReader chunkReader) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = chunkReader;\n+\t}\n+\n+\t@Override\n+\tpublic boolean hasChannelStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().anyMatch(subtaskStateEntry ->\n+\t\t\tsubtaskStateEntry.getValue().getInputChannelState().stream().anyMatch(h -> !h.getOffsets().isEmpty()) ||\n+\t\t\t\tsubtaskStateEntry.getValue().getResultSubpartitionState().stream().anyMatch(h -> !h.getOffsets().isEmpty()));\n+\t}\n+\n+\t@Override\n+\tpublic void readInputData(InputGate[] inputGates) throws IOException {\n+\t\ttry (InputChannelRecoveredStateHandler stateHandler = new InputChannelRecoveredStateHandler(inputGates)) {\n+\t\t\tread(OperatorSubtaskState::getInputChannelState, stateHandler);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void readOutputData(ResultPartitionWriter[] writers) throws IOException {\n+\t\ttry (ResultSubpartitionRecoveredStateHandler stateHandler = new ResultSubpartitionRecoveredStateHandler(writers)) {\n+\t\t\tread(OperatorSubtaskState::getResultSubpartitionState, stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void read(\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\tfor (Map.Entry<StreamStateHandle, List<Handle>> delegateAndHandles : groupByDelegate(streamSubtaskStates(), stateHandleExtractor).entrySet()) {\n+\t\t\treadSequentially(delegateAndHandles.getKey(), delegateAndHandles.getValue(), stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void readSequentially(\n+\t\t\tStreamStateHandle streamStateHandle,\n+\t\t\tList<Handle> channelStateHandles,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\ttry (FSDataInputStream is = streamStateHandle.openInputStream()) {\n+\t\t\tserializer.readHeader(is);\n+\t\t\tfor (Tuple2<Long, Info> offsetAndChannelInfo : extractOffsetsSorted(channelStateHandles)) {\n+\t\t\t\tchunkReader.readChunk(is, offsetAndChannelInfo.f0, stateHandler, offsetAndChannelInfo.f1);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate Stream<OperatorSubtaskState> streamSubtaskStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().map(Map.Entry::getValue);\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> Map<StreamStateHandle, List<Handle>> groupByDelegate(\n+\t\t\tStream<OperatorSubtaskState> states,\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor) {\n+\t\treturn states\n+\t\t\t.map(stateHandleExtractor).flatMap(Collection::stream)\n+\t\t\t.peek(validate())\n+\t\t\t.collect(groupingBy(AbstractChannelStateHandle::getDelegate));\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> Consumer<Handle> validate() {\n+\t\tSet<Info> seen = new HashSet<>();\n+\t\t// expect each channel to be described only once; otherwise, buffers in channel could be re-ordered\n+\t\treturn handle -> Preconditions.checkState(seen.add(handle.getInfo()), \"duplicate channel info: %s\");\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> List<Tuple2<Long, Info>> extractOffsetsSorted(List<Handle> channelStateHandles) {\n+\t\treturn channelStateHandles\n+\t\t\t.stream()\n+\t\t\t.flatMap(SequentialChannelStateReaderImpl::extractOffsets)\n+\t\t\t.sorted(comparingLong(offsetAndInfo -> offsetAndInfo.f0))\n+\t\t\t.collect(toList());\n+\t}\n+\n+\tprivate static  <Info, Handle extends AbstractChannelStateHandle<Info>> Stream<Tuple2<Long, Info>> extractOffsets(Handle handle) {\n+\t\treturn handle.getOffsets().stream().map(offset -> Tuple2.of(offset, handle.getInfo()));\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws Exception {\n+\t}\n+\n+}\n+\n+interface RecoveredChannelStateHandler<Info, Context> extends AutoCloseable {\n+\tTuple2<ChannelStateByteBuffer, Context> getBuffer(Info info) throws IOException;\n+\n+\tvoid recover(Info info, Context context) throws IOException;\n+}\n+\n+class InputChannelRecoveredStateHandler implements RecoveredChannelStateHandler<InputChannelInfo, Buffer> {\n+\tprivate final InputGate[] inputGates;\n+\n+\tInputChannelRecoveredStateHandler(InputGate[] inputGates) {\n+\t\tthis.inputGates = inputGates;\n+\t}\n+\n+\t@Override\n+\tpublic Tuple2<ChannelStateByteBuffer, Buffer> getBuffer(InputChannelInfo channelInfo) throws IOException {\n+\t\tRecoveredInputChannel channel = getChannel(channelInfo);\n+\t\tBuffer buffer;\n+\t\ttry {\n+\t\t\tbuffer = channel.getBuffer();\n+\t\t\treturn Tuple2.of(wrap(buffer), buffer);\n+\t\t} catch (InterruptedException e) {\n+\t\t\tthrow new IOException(e);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void recover(InputChannelInfo channelInfo, Buffer buffer) {\n+\t\tif (buffer.readableBytes() > 0) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 180}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDMyMjYxMA==", "bodyText": "nit: empty line.", "url": "https://github.com/apache/flink/pull/13351#discussion_r494322610", "createdAt": "2020-09-24T13:36:43Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImpl.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.fs.FSDataInputStream;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.runtime.checkpoint.StateObjectCollection;\n+import org.apache.flink.runtime.checkpoint.TaskStateSnapshot;\n+import org.apache.flink.runtime.io.network.api.writer.ResultPartitionWriter;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.BufferBuilder;\n+import org.apache.flink.runtime.io.network.buffer.BufferConsumer;\n+import org.apache.flink.runtime.io.network.partition.consumer.InputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel;\n+import org.apache.flink.runtime.state.AbstractChannelStateHandle;\n+import org.apache.flink.runtime.state.StreamStateHandle;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+import java.util.stream.Stream;\n+\n+import static java.util.Comparator.comparingLong;\n+import static java.util.stream.Collectors.groupingBy;\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.flink.runtime.checkpoint.channel.ChannelStateByteBuffer.wrap;\n+\n+/**\n+ * {@link SequentialChannelStateReader} implementation.\n+ */\n+public class SequentialChannelStateReaderImpl implements SequentialChannelStateReader {\n+\n+\tprivate final TaskStateSnapshot taskStateSnapshot;\n+\tprivate final ChannelStateSerializer serializer;\n+\tprivate final ChannelStateChunkReader chunkReader;\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot) {\n+\t\tthis(taskStateSnapshot, new ChannelStateSerializerImpl());\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = new ChannelStateChunkReader(serializer);\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer, ChannelStateChunkReader chunkReader) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = chunkReader;\n+\t}\n+\n+\t@Override\n+\tpublic boolean hasChannelStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().anyMatch(subtaskStateEntry ->\n+\t\t\tsubtaskStateEntry.getValue().getInputChannelState().stream().anyMatch(h -> !h.getOffsets().isEmpty()) ||\n+\t\t\t\tsubtaskStateEntry.getValue().getResultSubpartitionState().stream().anyMatch(h -> !h.getOffsets().isEmpty()));\n+\t}\n+\n+\t@Override\n+\tpublic void readInputData(InputGate[] inputGates) throws IOException {\n+\t\ttry (InputChannelRecoveredStateHandler stateHandler = new InputChannelRecoveredStateHandler(inputGates)) {\n+\t\t\tread(OperatorSubtaskState::getInputChannelState, stateHandler);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void readOutputData(ResultPartitionWriter[] writers) throws IOException {\n+\t\ttry (ResultSubpartitionRecoveredStateHandler stateHandler = new ResultSubpartitionRecoveredStateHandler(writers)) {\n+\t\t\tread(OperatorSubtaskState::getResultSubpartitionState, stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void read(\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\tfor (Map.Entry<StreamStateHandle, List<Handle>> delegateAndHandles : groupByDelegate(streamSubtaskStates(), stateHandleExtractor).entrySet()) {\n+\t\t\treadSequentially(delegateAndHandles.getKey(), delegateAndHandles.getValue(), stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void readSequentially(\n+\t\t\tStreamStateHandle streamStateHandle,\n+\t\t\tList<Handle> channelStateHandles,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\ttry (FSDataInputStream is = streamStateHandle.openInputStream()) {\n+\t\t\tserializer.readHeader(is);\n+\t\t\tfor (Tuple2<Long, Info> offsetAndChannelInfo : extractOffsetsSorted(channelStateHandles)) {\n+\t\t\t\tchunkReader.readChunk(is, offsetAndChannelInfo.f0, stateHandler, offsetAndChannelInfo.f1);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate Stream<OperatorSubtaskState> streamSubtaskStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().map(Map.Entry::getValue);\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> Map<StreamStateHandle, List<Handle>> groupByDelegate(\n+\t\t\tStream<OperatorSubtaskState> states,\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor) {\n+\t\treturn states\n+\t\t\t.map(stateHandleExtractor).flatMap(Collection::stream)\n+\t\t\t.peek(validate())\n+\t\t\t.collect(groupingBy(AbstractChannelStateHandle::getDelegate));\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> Consumer<Handle> validate() {\n+\t\tSet<Info> seen = new HashSet<>();\n+\t\t// expect each channel to be described only once; otherwise, buffers in channel could be re-ordered\n+\t\treturn handle -> Preconditions.checkState(seen.add(handle.getInfo()), \"duplicate channel info: %s\");\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> List<Tuple2<Long, Info>> extractOffsetsSorted(List<Handle> channelStateHandles) {\n+\t\treturn channelStateHandles\n+\t\t\t.stream()\n+\t\t\t.flatMap(SequentialChannelStateReaderImpl::extractOffsets)\n+\t\t\t.sorted(comparingLong(offsetAndInfo -> offsetAndInfo.f0))\n+\t\t\t.collect(toList());\n+\t}\n+\n+\tprivate static  <Info, Handle extends AbstractChannelStateHandle<Info>> Stream<Tuple2<Long, Info>> extractOffsets(Handle handle) {\n+\t\treturn handle.getOffsets().stream().map(offset -> Tuple2.of(offset, handle.getInfo()));\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws Exception {\n+\t}\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 150}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDMyMzQ0Nw==", "bodyText": "I was wondering if 5 classes per file are not too many. I haven't seen that in the code base so far. One option would be to extract RecoveredChannelStateHandler and its implementation to a separate file.", "url": "https://github.com/apache/flink/pull/13351#discussion_r494323447", "createdAt": "2020-09-24T13:37:55Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImpl.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.fs.FSDataInputStream;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.runtime.checkpoint.StateObjectCollection;\n+import org.apache.flink.runtime.checkpoint.TaskStateSnapshot;\n+import org.apache.flink.runtime.io.network.api.writer.ResultPartitionWriter;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.BufferBuilder;\n+import org.apache.flink.runtime.io.network.buffer.BufferConsumer;\n+import org.apache.flink.runtime.io.network.partition.consumer.InputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel;\n+import org.apache.flink.runtime.state.AbstractChannelStateHandle;\n+import org.apache.flink.runtime.state.StreamStateHandle;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+import java.util.stream.Stream;\n+\n+import static java.util.Comparator.comparingLong;\n+import static java.util.stream.Collectors.groupingBy;\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.flink.runtime.checkpoint.channel.ChannelStateByteBuffer.wrap;\n+\n+/**\n+ * {@link SequentialChannelStateReader} implementation.\n+ */\n+public class SequentialChannelStateReaderImpl implements SequentialChannelStateReader {\n+\n+\tprivate final TaskStateSnapshot taskStateSnapshot;\n+\tprivate final ChannelStateSerializer serializer;\n+\tprivate final ChannelStateChunkReader chunkReader;\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot) {\n+\t\tthis(taskStateSnapshot, new ChannelStateSerializerImpl());\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = new ChannelStateChunkReader(serializer);\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer, ChannelStateChunkReader chunkReader) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = chunkReader;\n+\t}\n+\n+\t@Override\n+\tpublic boolean hasChannelStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().anyMatch(subtaskStateEntry ->\n+\t\t\tsubtaskStateEntry.getValue().getInputChannelState().stream().anyMatch(h -> !h.getOffsets().isEmpty()) ||\n+\t\t\t\tsubtaskStateEntry.getValue().getResultSubpartitionState().stream().anyMatch(h -> !h.getOffsets().isEmpty()));\n+\t}\n+\n+\t@Override\n+\tpublic void readInputData(InputGate[] inputGates) throws IOException {\n+\t\ttry (InputChannelRecoveredStateHandler stateHandler = new InputChannelRecoveredStateHandler(inputGates)) {\n+\t\t\tread(OperatorSubtaskState::getInputChannelState, stateHandler);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void readOutputData(ResultPartitionWriter[] writers) throws IOException {\n+\t\ttry (ResultSubpartitionRecoveredStateHandler stateHandler = new ResultSubpartitionRecoveredStateHandler(writers)) {\n+\t\t\tread(OperatorSubtaskState::getResultSubpartitionState, stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void read(\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\tfor (Map.Entry<StreamStateHandle, List<Handle>> delegateAndHandles : groupByDelegate(streamSubtaskStates(), stateHandleExtractor).entrySet()) {\n+\t\t\treadSequentially(delegateAndHandles.getKey(), delegateAndHandles.getValue(), stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void readSequentially(\n+\t\t\tStreamStateHandle streamStateHandle,\n+\t\t\tList<Handle> channelStateHandles,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\ttry (FSDataInputStream is = streamStateHandle.openInputStream()) {\n+\t\t\tserializer.readHeader(is);\n+\t\t\tfor (Tuple2<Long, Info> offsetAndChannelInfo : extractOffsetsSorted(channelStateHandles)) {\n+\t\t\t\tchunkReader.readChunk(is, offsetAndChannelInfo.f0, stateHandler, offsetAndChannelInfo.f1);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate Stream<OperatorSubtaskState> streamSubtaskStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().map(Map.Entry::getValue);\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> Map<StreamStateHandle, List<Handle>> groupByDelegate(\n+\t\t\tStream<OperatorSubtaskState> states,\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor) {\n+\t\treturn states\n+\t\t\t.map(stateHandleExtractor).flatMap(Collection::stream)\n+\t\t\t.peek(validate())\n+\t\t\t.collect(groupingBy(AbstractChannelStateHandle::getDelegate));\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> Consumer<Handle> validate() {\n+\t\tSet<Info> seen = new HashSet<>();\n+\t\t// expect each channel to be described only once; otherwise, buffers in channel could be re-ordered\n+\t\treturn handle -> Preconditions.checkState(seen.add(handle.getInfo()), \"duplicate channel info: %s\");\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> List<Tuple2<Long, Info>> extractOffsetsSorted(List<Handle> channelStateHandles) {\n+\t\treturn channelStateHandles\n+\t\t\t.stream()\n+\t\t\t.flatMap(SequentialChannelStateReaderImpl::extractOffsets)\n+\t\t\t.sorted(comparingLong(offsetAndInfo -> offsetAndInfo.f0))\n+\t\t\t.collect(toList());\n+\t}\n+\n+\tprivate static  <Info, Handle extends AbstractChannelStateHandle<Info>> Stream<Tuple2<Long, Info>> extractOffsets(Handle handle) {\n+\t\treturn handle.getOffsets().stream().map(offset -> Tuple2.of(offset, handle.getInfo()));\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws Exception {\n+\t}\n+\n+}\n+\n+interface RecoveredChannelStateHandler<Info, Context> extends AutoCloseable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 153}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDMyNDQyMw==", "bodyText": "package-default to make clear that it is only used for testing?", "url": "https://github.com/apache/flink/pull/13351#discussion_r494324423", "createdAt": "2020-09-24T13:39:16Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImpl.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.fs.FSDataInputStream;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.runtime.checkpoint.StateObjectCollection;\n+import org.apache.flink.runtime.checkpoint.TaskStateSnapshot;\n+import org.apache.flink.runtime.io.network.api.writer.ResultPartitionWriter;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.BufferBuilder;\n+import org.apache.flink.runtime.io.network.buffer.BufferConsumer;\n+import org.apache.flink.runtime.io.network.partition.consumer.InputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel;\n+import org.apache.flink.runtime.state.AbstractChannelStateHandle;\n+import org.apache.flink.runtime.state.StreamStateHandle;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+import java.util.stream.Stream;\n+\n+import static java.util.Comparator.comparingLong;\n+import static java.util.stream.Collectors.groupingBy;\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.flink.runtime.checkpoint.channel.ChannelStateByteBuffer.wrap;\n+\n+/**\n+ * {@link SequentialChannelStateReader} implementation.\n+ */\n+public class SequentialChannelStateReaderImpl implements SequentialChannelStateReader {\n+\n+\tprivate final TaskStateSnapshot taskStateSnapshot;\n+\tprivate final ChannelStateSerializer serializer;\n+\tprivate final ChannelStateChunkReader chunkReader;\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot) {\n+\t\tthis(taskStateSnapshot, new ChannelStateSerializerImpl());\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDMyNDQ5NQ==", "bodyText": "package-default to make clear that it is only used for testing?", "url": "https://github.com/apache/flink/pull/13351#discussion_r494324495", "createdAt": "2020-09-24T13:39:21Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImpl.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.fs.FSDataInputStream;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.runtime.checkpoint.StateObjectCollection;\n+import org.apache.flink.runtime.checkpoint.TaskStateSnapshot;\n+import org.apache.flink.runtime.io.network.api.writer.ResultPartitionWriter;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.BufferBuilder;\n+import org.apache.flink.runtime.io.network.buffer.BufferConsumer;\n+import org.apache.flink.runtime.io.network.partition.consumer.InputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel;\n+import org.apache.flink.runtime.state.AbstractChannelStateHandle;\n+import org.apache.flink.runtime.state.StreamStateHandle;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+import java.util.stream.Stream;\n+\n+import static java.util.Comparator.comparingLong;\n+import static java.util.stream.Collectors.groupingBy;\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.flink.runtime.checkpoint.channel.ChannelStateByteBuffer.wrap;\n+\n+/**\n+ * {@link SequentialChannelStateReader} implementation.\n+ */\n+public class SequentialChannelStateReaderImpl implements SequentialChannelStateReader {\n+\n+\tprivate final TaskStateSnapshot taskStateSnapshot;\n+\tprivate final ChannelStateSerializer serializer;\n+\tprivate final ChannelStateChunkReader chunkReader;\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot) {\n+\t\tthis(taskStateSnapshot, new ChannelStateSerializerImpl());\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = new ChannelStateChunkReader(serializer);\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer, ChannelStateChunkReader chunkReader) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDMyODM4Mg==", "bodyText": "When would the offsets be unsorted? Is this more of a precaution or is it common?\nIs it again the small file optimization?", "url": "https://github.com/apache/flink/pull/13351#discussion_r494328382", "createdAt": "2020-09-24T13:43:57Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImpl.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.fs.FSDataInputStream;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.runtime.checkpoint.StateObjectCollection;\n+import org.apache.flink.runtime.checkpoint.TaskStateSnapshot;\n+import org.apache.flink.runtime.io.network.api.writer.ResultPartitionWriter;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.BufferBuilder;\n+import org.apache.flink.runtime.io.network.buffer.BufferConsumer;\n+import org.apache.flink.runtime.io.network.partition.consumer.InputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel;\n+import org.apache.flink.runtime.state.AbstractChannelStateHandle;\n+import org.apache.flink.runtime.state.StreamStateHandle;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+import java.util.stream.Stream;\n+\n+import static java.util.Comparator.comparingLong;\n+import static java.util.stream.Collectors.groupingBy;\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.flink.runtime.checkpoint.channel.ChannelStateByteBuffer.wrap;\n+\n+/**\n+ * {@link SequentialChannelStateReader} implementation.\n+ */\n+public class SequentialChannelStateReaderImpl implements SequentialChannelStateReader {\n+\n+\tprivate final TaskStateSnapshot taskStateSnapshot;\n+\tprivate final ChannelStateSerializer serializer;\n+\tprivate final ChannelStateChunkReader chunkReader;\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot) {\n+\t\tthis(taskStateSnapshot, new ChannelStateSerializerImpl());\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = new ChannelStateChunkReader(serializer);\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer, ChannelStateChunkReader chunkReader) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = chunkReader;\n+\t}\n+\n+\t@Override\n+\tpublic boolean hasChannelStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().anyMatch(subtaskStateEntry ->\n+\t\t\tsubtaskStateEntry.getValue().getInputChannelState().stream().anyMatch(h -> !h.getOffsets().isEmpty()) ||\n+\t\t\t\tsubtaskStateEntry.getValue().getResultSubpartitionState().stream().anyMatch(h -> !h.getOffsets().isEmpty()));\n+\t}\n+\n+\t@Override\n+\tpublic void readInputData(InputGate[] inputGates) throws IOException {\n+\t\ttry (InputChannelRecoveredStateHandler stateHandler = new InputChannelRecoveredStateHandler(inputGates)) {\n+\t\t\tread(OperatorSubtaskState::getInputChannelState, stateHandler);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void readOutputData(ResultPartitionWriter[] writers) throws IOException {\n+\t\ttry (ResultSubpartitionRecoveredStateHandler stateHandler = new ResultSubpartitionRecoveredStateHandler(writers)) {\n+\t\t\tread(OperatorSubtaskState::getResultSubpartitionState, stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void read(\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\tfor (Map.Entry<StreamStateHandle, List<Handle>> delegateAndHandles : groupByDelegate(streamSubtaskStates(), stateHandleExtractor).entrySet()) {\n+\t\t\treadSequentially(delegateAndHandles.getKey(), delegateAndHandles.getValue(), stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void readSequentially(\n+\t\t\tStreamStateHandle streamStateHandle,\n+\t\t\tList<Handle> channelStateHandles,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\ttry (FSDataInputStream is = streamStateHandle.openInputStream()) {\n+\t\t\tserializer.readHeader(is);\n+\t\t\tfor (Tuple2<Long, Info> offsetAndChannelInfo : extractOffsetsSorted(channelStateHandles)) {\n+\t\t\t\tchunkReader.readChunk(is, offsetAndChannelInfo.f0, stateHandler, offsetAndChannelInfo.f1);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate Stream<OperatorSubtaskState> streamSubtaskStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().map(Map.Entry::getValue);\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> Map<StreamStateHandle, List<Handle>> groupByDelegate(\n+\t\t\tStream<OperatorSubtaskState> states,\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor) {\n+\t\treturn states\n+\t\t\t.map(stateHandleExtractor).flatMap(Collection::stream)\n+\t\t\t.peek(validate())\n+\t\t\t.collect(groupingBy(AbstractChannelStateHandle::getDelegate));\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> Consumer<Handle> validate() {\n+\t\tSet<Info> seen = new HashSet<>();\n+\t\t// expect each channel to be described only once; otherwise, buffers in channel could be re-ordered\n+\t\treturn handle -> Preconditions.checkState(seen.add(handle.getInfo()), \"duplicate channel info: %s\");\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> List<Tuple2<Long, Info>> extractOffsetsSorted(List<Handle> channelStateHandles) {\n+\t\treturn channelStateHandles", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 136}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDMzMDI2Mg==", "bodyText": "getStream?", "url": "https://github.com/apache/flink/pull/13351#discussion_r494330262", "createdAt": "2020-09-24T13:45:28Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/test/java/org/apache/flink/runtime/checkpoint/channel/ChannelStateChunkReaderTest.java", "diffHunk": "@@ -0,0 +1,162 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.fs.FSDataInputStream;\n+import org.apache.flink.core.memory.MemorySegmentFactory;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.FreeingBufferRecycler;\n+import org.apache.flink.runtime.io.network.buffer.NetworkBuffer;\n+import org.apache.flink.runtime.state.memory.ByteStreamStateHandle;\n+\n+import org.junit.Test;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+import static org.apache.flink.util.Preconditions.checkArgument;\n+import static org.apache.flink.util.Preconditions.checkState;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n+\n+/**\n+ * {@link ChannelStateChunkReader} test.\n+ */\n+public class ChannelStateChunkReaderTest {\n+\n+\t@Test(expected = TestException.class)\n+\tpublic void testBufferRecycledOnFailure() throws IOException {\n+\t\tFailingChannelStateSerializer serializer = new FailingChannelStateSerializer();\n+\t\tTestRecoveredChannelStateHandler handler = new TestRecoveredChannelStateHandler();\n+\n+\t\ttry (FSDataInputStream stream = geStream(serializer, 10)) {\n+\t\t\tnew ChannelStateChunkReader(serializer).readChunk(stream, serializer.getHeaderLength(), handler, \"channelInfo\");\n+\t\t\tcheckState(serializer.failed);\n+\t\t\tcheckState(!handler.requestedBuffers.isEmpty());\n+\t\t} finally {\n+\t\t\tassertTrue(handler.requestedBuffers.stream().allMatch(TestChannelStateByteBuffer::isRecycled));\n+\t\t}\n+\t}\n+\n+\t@Test\n+\tpublic void testBuffersNotRequestedForEmptyStream() throws IOException {\n+\t\tChannelStateSerializer serializer = new ChannelStateSerializerImpl();\n+\t\tTestRecoveredChannelStateHandler handler = new TestRecoveredChannelStateHandler();\n+\n+\t\ttry (FSDataInputStream stream = geStream(serializer, 0)) {\n+\t\t\tnew ChannelStateChunkReader(serializer).readChunk(stream, serializer.getHeaderLength(), handler, \"channelInfo\");\n+\t\t} finally {\n+\t\t\tassertTrue(handler.requestedBuffers.isEmpty());\n+\t\t}\n+\t}\n+\n+\t@Test\n+\tpublic void testNoSeekUnnecessarily() throws IOException {\n+\t\tfinal int offset = 123;\n+\t\tfinal FSDataInputStream stream = new FSDataInputStream() {\n+\t\t\t@Override\n+\t\t\tpublic long getPos() {\n+\t\t\t\treturn offset;\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic void seek(long ignored) {\n+\t\t\t\tfail();\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic int read() {\n+\t\t\t\treturn 0;\n+\t\t\t}\n+\t\t};\n+\n+\t\tnew ChannelStateChunkReader(new ChannelStateSerializerImpl())\n+\t\t\t.readChunk(stream, offset, new TestRecoveredChannelStateHandler(), \"channelInfo\");\n+\t}\n+\n+\tprivate static class TestRecoveredChannelStateHandler implements RecoveredChannelStateHandler<Object, Object> {\n+\t\tprivate final List<TestChannelStateByteBuffer> requestedBuffers = new ArrayList<>();\n+\n+\t\t@Override\n+\t\tpublic Tuple2<ChannelStateByteBuffer, Object> getBuffer(Object o) {\n+\t\t\tTestChannelStateByteBuffer buffer = new TestChannelStateByteBuffer();\n+\t\t\trequestedBuffers.add(buffer);\n+\t\t\treturn Tuple2.of(buffer, null);\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void recover(Object o, Object o2) {\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void close() throws Exception {\n+\t\t}\n+\n+\t}\n+\n+\tprivate static class FailingChannelStateSerializer extends ChannelStateSerializerImpl {\n+\t\tprivate boolean failed;\n+\n+\t\t@Override\n+\t\tpublic int readData(InputStream stream, ChannelStateByteBuffer buffer, int bytes) {\n+\t\t\tfailed = true;\n+\t\t\tthrow new TestException();\n+\t\t}\n+\t}\n+\n+\tprivate static FSDataInputStream geStream(ChannelStateSerializer serializer, int size) throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 127}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDMzNDAwNw==", "bodyText": "I haven't understood the control flow here. Is this line really executed? Did you want to put it into finally?", "url": "https://github.com/apache/flink/pull/13351#discussion_r494334007", "createdAt": "2020-09-24T13:48:38Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/test/java/org/apache/flink/runtime/checkpoint/channel/ChannelStateChunkReaderTest.java", "diffHunk": "@@ -0,0 +1,162 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.fs.FSDataInputStream;\n+import org.apache.flink.core.memory.MemorySegmentFactory;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.FreeingBufferRecycler;\n+import org.apache.flink.runtime.io.network.buffer.NetworkBuffer;\n+import org.apache.flink.runtime.state.memory.ByteStreamStateHandle;\n+\n+import org.junit.Test;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+import static org.apache.flink.util.Preconditions.checkArgument;\n+import static org.apache.flink.util.Preconditions.checkState;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n+\n+/**\n+ * {@link ChannelStateChunkReader} test.\n+ */\n+public class ChannelStateChunkReaderTest {\n+\n+\t@Test(expected = TestException.class)\n+\tpublic void testBufferRecycledOnFailure() throws IOException {\n+\t\tFailingChannelStateSerializer serializer = new FailingChannelStateSerializer();\n+\t\tTestRecoveredChannelStateHandler handler = new TestRecoveredChannelStateHandler();\n+\n+\t\ttry (FSDataInputStream stream = geStream(serializer, 10)) {\n+\t\t\tnew ChannelStateChunkReader(serializer).readChunk(stream, serializer.getHeaderLength(), handler, \"channelInfo\");\n+\t\t\tcheckState(serializer.failed);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDMzNjQzNQ==", "bodyText": "nit: empty line. (Could you check in all classes?)", "url": "https://github.com/apache/flink/pull/13351#discussion_r494336435", "createdAt": "2020-09-24T13:51:22Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/test/java/org/apache/flink/runtime/checkpoint/channel/ChannelStateChunkReaderTest.java", "diffHunk": "@@ -0,0 +1,162 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.fs.FSDataInputStream;\n+import org.apache.flink.core.memory.MemorySegmentFactory;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.FreeingBufferRecycler;\n+import org.apache.flink.runtime.io.network.buffer.NetworkBuffer;\n+import org.apache.flink.runtime.state.memory.ByteStreamStateHandle;\n+\n+import org.junit.Test;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+import static org.apache.flink.util.Preconditions.checkArgument;\n+import static org.apache.flink.util.Preconditions.checkState;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n+\n+/**\n+ * {@link ChannelStateChunkReader} test.\n+ */\n+public class ChannelStateChunkReaderTest {\n+\n+\t@Test(expected = TestException.class)\n+\tpublic void testBufferRecycledOnFailure() throws IOException {\n+\t\tFailingChannelStateSerializer serializer = new FailingChannelStateSerializer();\n+\t\tTestRecoveredChannelStateHandler handler = new TestRecoveredChannelStateHandler();\n+\n+\t\ttry (FSDataInputStream stream = geStream(serializer, 10)) {\n+\t\t\tnew ChannelStateChunkReader(serializer).readChunk(stream, serializer.getHeaderLength(), handler, \"channelInfo\");\n+\t\t\tcheckState(serializer.failed);\n+\t\t\tcheckState(!handler.requestedBuffers.isEmpty());\n+\t\t} finally {\n+\t\t\tassertTrue(handler.requestedBuffers.stream().allMatch(TestChannelStateByteBuffer::isRecycled));\n+\t\t}\n+\t}\n+\n+\t@Test\n+\tpublic void testBuffersNotRequestedForEmptyStream() throws IOException {\n+\t\tChannelStateSerializer serializer = new ChannelStateSerializerImpl();\n+\t\tTestRecoveredChannelStateHandler handler = new TestRecoveredChannelStateHandler();\n+\n+\t\ttry (FSDataInputStream stream = geStream(serializer, 0)) {\n+\t\t\tnew ChannelStateChunkReader(serializer).readChunk(stream, serializer.getHeaderLength(), handler, \"channelInfo\");\n+\t\t} finally {\n+\t\t\tassertTrue(handler.requestedBuffers.isEmpty());\n+\t\t}\n+\t}\n+\n+\t@Test\n+\tpublic void testNoSeekUnnecessarily() throws IOException {\n+\t\tfinal int offset = 123;\n+\t\tfinal FSDataInputStream stream = new FSDataInputStream() {\n+\t\t\t@Override\n+\t\t\tpublic long getPos() {\n+\t\t\t\treturn offset;\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic void seek(long ignored) {\n+\t\t\t\tfail();\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic int read() {\n+\t\t\t\treturn 0;\n+\t\t\t}\n+\t\t};\n+\n+\t\tnew ChannelStateChunkReader(new ChannelStateSerializerImpl())\n+\t\t\t.readChunk(stream, offset, new TestRecoveredChannelStateHandler(), \"channelInfo\");\n+\t}\n+\n+\tprivate static class TestRecoveredChannelStateHandler implements RecoveredChannelStateHandler<Object, Object> {\n+\t\tprivate final List<TestChannelStateByteBuffer> requestedBuffers = new ArrayList<>();\n+\n+\t\t@Override\n+\t\tpublic Tuple2<ChannelStateByteBuffer, Object> getBuffer(Object o) {\n+\t\t\tTestChannelStateByteBuffer buffer = new TestChannelStateByteBuffer();\n+\t\t\trequestedBuffers.add(buffer);\n+\t\t\treturn Tuple2.of(buffer, null);\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void recover(Object o, Object o2) {\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void close() throws Exception {\n+\t\t}\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 114}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDMzODMxNg==", "bodyText": "Please check format string.", "url": "https://github.com/apache/flink/pull/13351#discussion_r494338316", "createdAt": "2020-09-24T13:53:41Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/test/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImplTest.java", "diffHunk": "@@ -0,0 +1,321 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.memory.MemorySegmentFactory;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.runtime.checkpoint.StateObjectCollection;\n+import org.apache.flink.runtime.checkpoint.TaskStateSnapshot;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.FreeingBufferRecycler;\n+import org.apache.flink.runtime.io.network.buffer.NetworkBuffer;\n+import org.apache.flink.runtime.io.network.buffer.NetworkBufferPool;\n+import org.apache.flink.runtime.io.network.partition.ResultPartition;\n+import org.apache.flink.runtime.io.network.partition.ResultPartitionBuilder;\n+import org.apache.flink.runtime.io.network.partition.ResultSubpartition.BufferAndBacklog;\n+import org.apache.flink.runtime.io.network.partition.ResultSubpartitionView;\n+import org.apache.flink.runtime.io.network.partition.consumer.BufferOrEvent;\n+import org.apache.flink.runtime.io.network.partition.consumer.InputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateBuilder;\n+import org.apache.flink.runtime.jobgraph.OperatorID;\n+import org.apache.flink.runtime.state.InputChannelStateHandle;\n+import org.apache.flink.runtime.state.ResultSubpartitionStateHandle;\n+import org.apache.flink.runtime.state.memory.ByteStreamStateHandle;\n+import org.apache.flink.util.function.ThrowingConsumer;\n+\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Random;\n+import java.util.function.BiFunction;\n+import java.util.function.Function;\n+\n+import static java.util.function.Function.identity;\n+import static java.util.stream.Collectors.toList;\n+import static java.util.stream.Collectors.toMap;\n+import static java.util.stream.IntStream.range;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * {@link SequentialChannelStateReaderImpl} Test.\n+ */\n+@RunWith(Parameterized.class)\n+public class SequentialChannelStateReaderImplTest {\n+\n+\t@Parameterized.Parameters(name = \"{0}: stateParLevel={1}, statePartsPerChannel={1}, stateBytesPerPart={2},  parLevel={4}, bufferSize={5}\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDMzOTU2Ng==", "bodyText": "nit: Can we use ThreadLocalRandom here? It's just a test, but it would be nice to use it everywhere it's possible.", "url": "https://github.com/apache/flink/pull/13351#discussion_r494339566", "createdAt": "2020-09-24T13:55:16Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/test/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImplTest.java", "diffHunk": "@@ -0,0 +1,321 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.memory.MemorySegmentFactory;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.runtime.checkpoint.StateObjectCollection;\n+import org.apache.flink.runtime.checkpoint.TaskStateSnapshot;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.FreeingBufferRecycler;\n+import org.apache.flink.runtime.io.network.buffer.NetworkBuffer;\n+import org.apache.flink.runtime.io.network.buffer.NetworkBufferPool;\n+import org.apache.flink.runtime.io.network.partition.ResultPartition;\n+import org.apache.flink.runtime.io.network.partition.ResultPartitionBuilder;\n+import org.apache.flink.runtime.io.network.partition.ResultSubpartition.BufferAndBacklog;\n+import org.apache.flink.runtime.io.network.partition.ResultSubpartitionView;\n+import org.apache.flink.runtime.io.network.partition.consumer.BufferOrEvent;\n+import org.apache.flink.runtime.io.network.partition.consumer.InputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateBuilder;\n+import org.apache.flink.runtime.jobgraph.OperatorID;\n+import org.apache.flink.runtime.state.InputChannelStateHandle;\n+import org.apache.flink.runtime.state.ResultSubpartitionStateHandle;\n+import org.apache.flink.runtime.state.memory.ByteStreamStateHandle;\n+import org.apache.flink.util.function.ThrowingConsumer;\n+\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Random;\n+import java.util.function.BiFunction;\n+import java.util.function.Function;\n+\n+import static java.util.function.Function.identity;\n+import static java.util.stream.Collectors.toList;\n+import static java.util.stream.Collectors.toMap;\n+import static java.util.stream.IntStream.range;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * {@link SequentialChannelStateReaderImpl} Test.\n+ */\n+@RunWith(Parameterized.class)\n+public class SequentialChannelStateReaderImplTest {\n+\n+\t@Parameterized.Parameters(name = \"{0}: stateParLevel={1}, statePartsPerChannel={1}, stateBytesPerPart={2},  parLevel={4}, bufferSize={5}\")\n+\tpublic static Object[][] parameters() {\n+\t\treturn new Object[][]{\n+\t\t\t{\"NoStateAndNoChannels\", 0, 0, 0, 0, 0},\n+\t\t\t{\"NoState\", 0, 10, 10, 10, 10},\n+\t\t\t{\"ReadPermutedStateWithEqualBuffer\", 10, 10, 10, 10, 10},\n+\t\t\t{\"ReadPermutedStateWithReducedBuffer\", 10, 10, 10, 20, 10},\n+\t\t\t{\"ReadPermutedStateWithIncreasedBuffer\", 10, 10, 10, 10, 20},\n+\t\t};\n+\t}\n+\n+\tprivate final ChannelStateSerializer serializer;\n+\tprivate final Random random;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 85}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDM0MDkyMQ==", "bodyText": "Could you please extract a function with meaningful name of the lowest level?", "url": "https://github.com/apache/flink/pull/13351#discussion_r494340921", "createdAt": "2020-09-24T13:56:56Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/test/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImplTest.java", "diffHunk": "@@ -0,0 +1,321 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.memory.MemorySegmentFactory;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.runtime.checkpoint.StateObjectCollection;\n+import org.apache.flink.runtime.checkpoint.TaskStateSnapshot;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.FreeingBufferRecycler;\n+import org.apache.flink.runtime.io.network.buffer.NetworkBuffer;\n+import org.apache.flink.runtime.io.network.buffer.NetworkBufferPool;\n+import org.apache.flink.runtime.io.network.partition.ResultPartition;\n+import org.apache.flink.runtime.io.network.partition.ResultPartitionBuilder;\n+import org.apache.flink.runtime.io.network.partition.ResultSubpartition.BufferAndBacklog;\n+import org.apache.flink.runtime.io.network.partition.ResultSubpartitionView;\n+import org.apache.flink.runtime.io.network.partition.consumer.BufferOrEvent;\n+import org.apache.flink.runtime.io.network.partition.consumer.InputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateBuilder;\n+import org.apache.flink.runtime.jobgraph.OperatorID;\n+import org.apache.flink.runtime.state.InputChannelStateHandle;\n+import org.apache.flink.runtime.state.ResultSubpartitionStateHandle;\n+import org.apache.flink.runtime.state.memory.ByteStreamStateHandle;\n+import org.apache.flink.util.function.ThrowingConsumer;\n+\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Random;\n+import java.util.function.BiFunction;\n+import java.util.function.Function;\n+\n+import static java.util.function.Function.identity;\n+import static java.util.stream.Collectors.toList;\n+import static java.util.stream.Collectors.toMap;\n+import static java.util.stream.IntStream.range;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * {@link SequentialChannelStateReaderImpl} Test.\n+ */\n+@RunWith(Parameterized.class)\n+public class SequentialChannelStateReaderImplTest {\n+\n+\t@Parameterized.Parameters(name = \"{0}: stateParLevel={1}, statePartsPerChannel={1}, stateBytesPerPart={2},  parLevel={4}, bufferSize={5}\")\n+\tpublic static Object[][] parameters() {\n+\t\treturn new Object[][]{\n+\t\t\t{\"NoStateAndNoChannels\", 0, 0, 0, 0, 0},\n+\t\t\t{\"NoState\", 0, 10, 10, 10, 10},\n+\t\t\t{\"ReadPermutedStateWithEqualBuffer\", 10, 10, 10, 10, 10},\n+\t\t\t{\"ReadPermutedStateWithReducedBuffer\", 10, 10, 10, 20, 10},\n+\t\t\t{\"ReadPermutedStateWithIncreasedBuffer\", 10, 10, 10, 10, 20},\n+\t\t};\n+\t}\n+\n+\tprivate final ChannelStateSerializer serializer;\n+\tprivate final Random random;\n+\tprivate final int parLevel;\n+\tprivate final int statePartsPerChannel;\n+\tprivate final int stateBytesPerPart;\n+\tprivate final int bufferSize;\n+\tprivate final int stateParLevel;\n+\tprivate final boolean expectToHaveState;\n+\tprivate final int buffersPerChannel;\n+\n+\tpublic SequentialChannelStateReaderImplTest(String desc, int stateParLevel, int statePartsPerChannel, int stateBytesPerPart, int parLevel, int bufferSize) {\n+\t\tthis.serializer = new ChannelStateSerializerImpl();\n+\t\tthis.random = new Random();\n+\t\tthis.parLevel = parLevel;\n+\t\tthis.statePartsPerChannel = statePartsPerChannel;\n+\t\tthis.stateBytesPerPart = stateBytesPerPart;\n+\t\tthis.bufferSize = bufferSize;\n+\t\tthis.stateParLevel = stateParLevel;\n+\t\tthis.expectToHaveState = stateParLevel * statePartsPerChannel * stateBytesPerPart > 0;\n+\t\t// will read without waiting for consumption\n+\t\tthis.buffersPerChannel = Math.max(1, statePartsPerChannel * (bufferSize >= stateBytesPerPart ? 1 : stateBytesPerPart / bufferSize));\n+\t}\n+\n+\t@Test\n+\tpublic void testReadPermutedState() throws Exception {\n+\t\tMap<InputChannelInfo, List<byte[]>> inputChannelsData = generateState(InputChannelInfo::new);\n+\t\tMap<ResultSubpartitionInfo, List<byte[]>> resultPartitionsData = generateState(ResultSubpartitionInfo::new);\n+\n+\t\tSequentialChannelStateReader reader = new SequentialChannelStateReaderImpl(buildSnapshot(writePermuted(inputChannelsData, resultPartitionsData)));\n+\t\tassertEquals(expectToHaveState, reader.hasChannelStates());\n+\n+\t\twithResultPartitions(resultPartitions -> {\n+\t\t\treader.readOutputData(resultPartitions);\n+\t\t\tassertBuffersEquals(resultPartitionsData, collectBuffers(resultPartitions));\n+\t\t});\n+\n+\t\twithInputGates(gates -> {\n+\t\t\treader.readInputData(gates);\n+\t\t\tassertBuffersEquals(inputChannelsData, collectBuffers(gates));\n+\t\t\tassertConsumed(gates);\n+\t\t});\n+\t}\n+\n+\tprivate Map<ResultSubpartitionInfo, List<Buffer>> collectBuffers(ResultPartition[] resultPartitions) throws IOException {\n+\t\tMap<ResultSubpartitionInfo, List<Buffer>> actual = new HashMap<>();\n+\t\tfor (ResultPartition resultPartition : resultPartitions) {\n+\t\t\tfor (int i = 0; i < resultPartition.getNumberOfSubpartitions(); i++) {\n+\t\t\t\tResultSubpartitionInfo info = resultPartition.getSubpartitionInfo(i);\n+\t\t\t\tResultSubpartitionView view = resultPartition.createSubpartitionView(info.getSubPartitionIdx(), () -> { /**/ });\n+\t\t\t\tfor (BufferAndBacklog buffer = view.getNextBuffer(); buffer != null; buffer = view.getNextBuffer()) {\n+\t\t\t\t\tactual.computeIfAbsent(info, unused -> new ArrayList<>()).add(buffer.buffer());\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t\treturn actual;\n+\t}\n+\n+\tprivate Map<InputChannelInfo, List<Buffer>> collectBuffers(InputGate[] gates) throws Exception {\n+\t\tMap<InputChannelInfo, List<Buffer>> actual = new HashMap<>();\n+\t\tfor (InputGate gate : gates) {\n+\t\t\tfor (Optional<BufferOrEvent> next = gate.pollNext(); next.isPresent(); next = gate.pollNext()) {\n+\t\t\t\tactual.computeIfAbsent(\n+\t\t\t\t\tnext.get().getChannelInfo(),\n+\t\t\t\t\tunused -> new ArrayList<>()).add(next.get().getBuffer());\n+\t\t\t}\n+\t\t}\n+\t\treturn actual;\n+\t}\n+\n+\tprivate void assertConsumed(InputGate[] gates) throws InterruptedException, java.util.concurrent.ExecutionException {\n+\t\tfor (InputGate gate: gates) {\n+\t\t\tassertTrue(gate.getStateConsumedFuture().isDone());\n+\t\t\tgate.getStateConsumedFuture().get();\n+\t\t}\n+\t}\n+\n+\tprivate void withInputGates(ThrowingConsumer<InputGate[], Exception> action) throws Exception {\n+\t\tSingleInputGate[] gates = new SingleInputGate[parLevel];\n+\t\tfinal int segmentsToAllocate = parLevel * parLevel * buffersPerChannel;\n+\t\tNetworkBufferPool networkBufferPool = new NetworkBufferPool(segmentsToAllocate, bufferSize);\n+\t\ttry {\n+\t\t\tfor (int i = 0; i < parLevel; i++) {\n+\t\t\t\tgates[i] = new SingleInputGateBuilder()\n+\t\t\t\t\t.setNumberOfChannels(parLevel)\n+\t\t\t\t\t.setSingleInputGateIndex(i)\n+\t\t\t\t\t.setBufferPoolFactory(networkBufferPool.createBufferPool(0, buffersPerChannel))\n+\t\t\t\t\t.setSegmentProvider(networkBufferPool)\n+\t\t\t\t\t.setChannelFactory((builder, gate) -> builder\n+\t\t\t\t\t.setNetworkBuffersPerChannel(buffersPerChannel)\n+\t\t\t\t\t.buildRemoteRecoveredChannel(gate))\n+\t\t\t\t\t.build();\n+\t\t\t\tgates[i].setup();\n+\t\t\t}\n+\t\t\taction.accept(gates);\n+\t\t} finally {\n+\t\t\tfor (InputGate inputGate: gates) {\n+\t\t\t\tinputGate.close();\n+\t\t\t}\n+\t\t\ttry {\n+\t\t\t\tassertEquals(segmentsToAllocate, networkBufferPool.getNumberOfAvailableMemorySegments());\n+\t\t\t} finally {\n+\t\t\t\tnetworkBufferPool.destroyAllBufferPools();\n+\t\t\t\tnetworkBufferPool.destroy();\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate void withResultPartitions(ThrowingConsumer<ResultPartition[], Exception> action) throws Exception {\n+\t\tint segmentsToAllocate = parLevel * parLevel * buffersPerChannel;\n+\t\tNetworkBufferPool networkBufferPool = new NetworkBufferPool(segmentsToAllocate, bufferSize);\n+\t\tResultPartition[] resultPartitions = range(0, parLevel)\n+\t\t\t.mapToObj(i -> new ResultPartitionBuilder().setResultPartitionIndex(i).setNumberOfSubpartitions(parLevel).setNetworkBufferPool(networkBufferPool).build())\n+\t\t\t.toArray(ResultPartition[]::new);\n+\t\ttry {\n+\t\t\tfor (ResultPartition resultPartition: resultPartitions) {\n+\t\t\t\tresultPartition.setup();\n+\t\t\t}\n+\t\t\taction.accept(resultPartitions);\n+\t\t} finally {\n+\t\t\tfor (ResultPartition resultPartition: resultPartitions) {\n+\t\t\t\tresultPartition.close();\n+\t\t\t}\n+\t\t\ttry {\n+\t\t\t\tassertEquals(segmentsToAllocate, networkBufferPool.getNumberOfAvailableMemorySegments());\n+\t\t\t} finally {\n+\t\t\t\tnetworkBufferPool.destroyAllBufferPools();\n+\t\t\t\tnetworkBufferPool.destroy();\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate TaskStateSnapshot buildSnapshot(Tuple2<List<InputChannelStateHandle>, List<ResultSubpartitionStateHandle>> handles) {\n+\t\treturn new TaskStateSnapshot(\n+\t\t\tCollections.singletonMap(new OperatorID(), new OperatorSubtaskState(\n+\t\t\t\tStateObjectCollection.empty(),\n+\t\t\t\tStateObjectCollection.empty(),\n+\t\t\t\tStateObjectCollection.empty(),\n+\t\t\t\tStateObjectCollection.empty(),\n+\t\t\t\tnew StateObjectCollection<>(handles.f0),\n+\t\t\t\tnew StateObjectCollection<>(handles.f1)\n+\t\t\t))\n+\t\t);\n+\t}\n+\n+\tprivate <T> Map<T, List<byte[]>> generateState(BiFunction<Integer, Integer, T> descriptorCreator) {\n+\t\treturn range(0, stateParLevel).boxed().flatMap(\n+\t\t\tgateId -> range(0, stateParLevel).mapToObj(\n+\t\t\t\tchannelId ->\n+\t\t\t\t\tdescriptorCreator.apply(gateId, channelId))).collect(toMap(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 232}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDM0MzY4MQ==", "bodyText": "NoOpBufferAvailablityListener if we ever add a new mandatory method.", "url": "https://github.com/apache/flink/pull/13351#discussion_r494343681", "createdAt": "2020-09-24T13:59:48Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/test/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImplTest.java", "diffHunk": "@@ -0,0 +1,321 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.memory.MemorySegmentFactory;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.runtime.checkpoint.StateObjectCollection;\n+import org.apache.flink.runtime.checkpoint.TaskStateSnapshot;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.FreeingBufferRecycler;\n+import org.apache.flink.runtime.io.network.buffer.NetworkBuffer;\n+import org.apache.flink.runtime.io.network.buffer.NetworkBufferPool;\n+import org.apache.flink.runtime.io.network.partition.ResultPartition;\n+import org.apache.flink.runtime.io.network.partition.ResultPartitionBuilder;\n+import org.apache.flink.runtime.io.network.partition.ResultSubpartition.BufferAndBacklog;\n+import org.apache.flink.runtime.io.network.partition.ResultSubpartitionView;\n+import org.apache.flink.runtime.io.network.partition.consumer.BufferOrEvent;\n+import org.apache.flink.runtime.io.network.partition.consumer.InputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateBuilder;\n+import org.apache.flink.runtime.jobgraph.OperatorID;\n+import org.apache.flink.runtime.state.InputChannelStateHandle;\n+import org.apache.flink.runtime.state.ResultSubpartitionStateHandle;\n+import org.apache.flink.runtime.state.memory.ByteStreamStateHandle;\n+import org.apache.flink.util.function.ThrowingConsumer;\n+\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Random;\n+import java.util.function.BiFunction;\n+import java.util.function.Function;\n+\n+import static java.util.function.Function.identity;\n+import static java.util.stream.Collectors.toList;\n+import static java.util.stream.Collectors.toMap;\n+import static java.util.stream.IntStream.range;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * {@link SequentialChannelStateReaderImpl} Test.\n+ */\n+@RunWith(Parameterized.class)\n+public class SequentialChannelStateReaderImplTest {\n+\n+\t@Parameterized.Parameters(name = \"{0}: stateParLevel={1}, statePartsPerChannel={1}, stateBytesPerPart={2},  parLevel={4}, bufferSize={5}\")\n+\tpublic static Object[][] parameters() {\n+\t\treturn new Object[][]{\n+\t\t\t{\"NoStateAndNoChannels\", 0, 0, 0, 0, 0},\n+\t\t\t{\"NoState\", 0, 10, 10, 10, 10},\n+\t\t\t{\"ReadPermutedStateWithEqualBuffer\", 10, 10, 10, 10, 10},\n+\t\t\t{\"ReadPermutedStateWithReducedBuffer\", 10, 10, 10, 20, 10},\n+\t\t\t{\"ReadPermutedStateWithIncreasedBuffer\", 10, 10, 10, 10, 20},\n+\t\t};\n+\t}\n+\n+\tprivate final ChannelStateSerializer serializer;\n+\tprivate final Random random;\n+\tprivate final int parLevel;\n+\tprivate final int statePartsPerChannel;\n+\tprivate final int stateBytesPerPart;\n+\tprivate final int bufferSize;\n+\tprivate final int stateParLevel;\n+\tprivate final boolean expectToHaveState;\n+\tprivate final int buffersPerChannel;\n+\n+\tpublic SequentialChannelStateReaderImplTest(String desc, int stateParLevel, int statePartsPerChannel, int stateBytesPerPart, int parLevel, int bufferSize) {\n+\t\tthis.serializer = new ChannelStateSerializerImpl();\n+\t\tthis.random = new Random();\n+\t\tthis.parLevel = parLevel;\n+\t\tthis.statePartsPerChannel = statePartsPerChannel;\n+\t\tthis.stateBytesPerPart = stateBytesPerPart;\n+\t\tthis.bufferSize = bufferSize;\n+\t\tthis.stateParLevel = stateParLevel;\n+\t\tthis.expectToHaveState = stateParLevel * statePartsPerChannel * stateBytesPerPart > 0;\n+\t\t// will read without waiting for consumption\n+\t\tthis.buffersPerChannel = Math.max(1, statePartsPerChannel * (bufferSize >= stateBytesPerPart ? 1 : stateBytesPerPart / bufferSize));\n+\t}\n+\n+\t@Test\n+\tpublic void testReadPermutedState() throws Exception {\n+\t\tMap<InputChannelInfo, List<byte[]>> inputChannelsData = generateState(InputChannelInfo::new);\n+\t\tMap<ResultSubpartitionInfo, List<byte[]>> resultPartitionsData = generateState(ResultSubpartitionInfo::new);\n+\n+\t\tSequentialChannelStateReader reader = new SequentialChannelStateReaderImpl(buildSnapshot(writePermuted(inputChannelsData, resultPartitionsData)));\n+\t\tassertEquals(expectToHaveState, reader.hasChannelStates());\n+\n+\t\twithResultPartitions(resultPartitions -> {\n+\t\t\treader.readOutputData(resultPartitions);\n+\t\t\tassertBuffersEquals(resultPartitionsData, collectBuffers(resultPartitions));\n+\t\t});\n+\n+\t\twithInputGates(gates -> {\n+\t\t\treader.readInputData(gates);\n+\t\t\tassertBuffersEquals(inputChannelsData, collectBuffers(gates));\n+\t\t\tassertConsumed(gates);\n+\t\t});\n+\t}\n+\n+\tprivate Map<ResultSubpartitionInfo, List<Buffer>> collectBuffers(ResultPartition[] resultPartitions) throws IOException {\n+\t\tMap<ResultSubpartitionInfo, List<Buffer>> actual = new HashMap<>();\n+\t\tfor (ResultPartition resultPartition : resultPartitions) {\n+\t\t\tfor (int i = 0; i < resultPartition.getNumberOfSubpartitions(); i++) {\n+\t\t\t\tResultSubpartitionInfo info = resultPartition.getSubpartitionInfo(i);\n+\t\t\t\tResultSubpartitionView view = resultPartition.createSubpartitionView(info.getSubPartitionIdx(), () -> { /**/ });", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 132}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDM0NzA3MQ==", "bodyText": "Okay you just did what I wrote on the last commit. Probably squashing makes sense.", "url": "https://github.com/apache/flink/pull/13351#discussion_r494347071", "createdAt": "2020-09-24T14:04:10Z", "author": {"login": "AHeise"}, "path": "flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/StreamTask.java", "diffHunk": "@@ -504,23 +502,18 @@ protected void beforeInvoke() throws Exception {\n \n \tprivate void readRecoveredChannelState() throws IOException, InterruptedException {\n \t\tSequentialChannelStateReader reader = getEnvironment().getTaskStateManager().getSequentialChannelStateReader();\n-\t\tif (reader.hasChannelStates()) {\n-\t\t\treader.readOutputData(getEnvironment().getAllWriters());\n-\t\t\tchannelIOExecutor.execute(() -> {\n-\t\t\t\ttry {\n-\t\t\t\t\treader.readInputData(getEnvironment().getAllInputGates());\n-\t\t\t\t} catch (Exception e) {\n-\t\t\t\t\tasyncExceptionHandler.handleAsyncException(\"Unable to read channel state\", e);\n-\t\t\t\t}\n-\t\t\t});\n-\n-\t\t\tfor (InputGate inputGate : getEnvironment().getAllInputGates()) {\n-\t\t\t\tinputGate\n-\t\t\t\t\t.getStateConsumedFuture()\n-\t\t\t\t\t.thenRun(() -> mainMailboxExecutor.execute(inputGate::requestPartitions, \"Input gate request partitions\"));\n+\t\treader.readOutputData(getEnvironment().getAllWriters());\n+\t\tchannelIOExecutor.execute(() -> {\n+\t\t\ttry {\n+\t\t\t\treader.readInputData(getEnvironment().getAllInputGates());\n+\t\t\t} catch (Exception e) {\n+\t\t\t\tasyncExceptionHandler.handleAsyncException(\"Unable to read channel state\", e);\n \t\t\t}\n-\t\t} else {\n-\t\t\trequestPartitions();\n+\t\t});\n+\t\tfor (InputGate inputGate : getEnvironment().getAllInputGates()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4913b2f902918470bd20236df46d60c84d97293d"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDM0NzQwOQ==", "bodyText": "nit: typo in commit message.", "url": "https://github.com/apache/flink/pull/13351#discussion_r494347409", "createdAt": "2020-09-24T14:04:37Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/api/reader/AbstractRecordReader.java", "diffHunk": "@@ -45,6 +45,8 @@\n \n \tprivate RecordDeserializer<T> currentRecordDeserializer;\n \n+\tprivate boolean finishedStateReading;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6ef9ee7deb0111ea4d1e113f9258d89a1c5b336e"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDM0ODcxOQ==", "bodyText": "protected?", "url": "https://github.com/apache/flink/pull/13351#discussion_r494348719", "createdAt": "2020-09-24T14:06:17Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/consumer/RecoveredInputChannel.java", "diffHunk": "@@ -77,13 +78,18 @@\n \t\tbufferManager = new BufferManager(inputGate.getMemorySegmentProvider(), this, 0);\n \t}\n \n+\tpublic final InputChannel toInputChannel() throws IOException {\n+\t\tPreconditions.checkState(stateConsumedFuture.isDone(), \"recovered state is not fully consumed\");\n+\t\treturn toInputChannelInternal();\n+\t}\n+\n \t@Override\n \tpublic void setChannelStateWriter(ChannelStateWriter channelStateWriter) {\n \t\tcheckState(this.channelStateWriter == null, \"Already initialized\");\n \t\tthis.channelStateWriter = checkNotNull(channelStateWriter);\n \t}\n \n-\tpublic abstract InputChannel toInputChannel() throws IOException;\n+\tpublic abstract InputChannel toInputChannelInternal() throws IOException;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6ef9ee7deb0111ea4d1e113f9258d89a1c5b336e"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDM1MDk4Ng==", "bodyText": "Okay, now I see that you removed the original ChannelStateReader. I'd probably state that in the commit that adds SeqReader.", "url": "https://github.com/apache/flink/pull/13351#discussion_r494350986", "createdAt": "2020-09-24T14:09:34Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/test/java/org/apache/flink/runtime/checkpoint/channel/ChannelStateSerializerImplTest.java", "diffHunk": "@@ -36,7 +36,6 @@\n import java.util.Random;\n \n import static org.apache.flink.runtime.checkpoint.channel.ChannelStateByteBuffer.wrap;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4a598b96aa0250255f8758ce4ce57df23072dac7"}, "originalPosition": 3}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDM1MTIwMg==", "bodyText": "What's that change about?", "url": "https://github.com/apache/flink/pull/13351#discussion_r494351202", "createdAt": "2020-09-24T14:09:50Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/test/java/org/apache/flink/runtime/checkpoint/channel/ChannelStateSerializerImplTest.java", "diffHunk": "@@ -146,4 +145,10 @@ private void readAndCheck(byte[] data, ChannelStateSerializerImpl serializer, By\n \t\t}\n \t}\n \n+\tstatic byte[] generateData(int len) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4a598b96aa0250255f8758ce4ce57df23072dac7"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDM1MTUyMw==", "bodyText": "It looks like some cleanup is needed here.", "url": "https://github.com/apache/flink/pull/13351#discussion_r494351523", "createdAt": "2020-09-24T14:10:14Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/test/java/org/apache/flink/runtime/state/ChannelPersistenceITCase.java", "diffHunk": "@@ -76,17 +67,17 @@ public void testReadWritten() throws Exception {\n \t\t\tsingletonMap(resultSubpartitionInfo, resultSubpartitionInfoData)\n \t\t);\n \n-\t\tassertArrayEquals(inputChannelInfoData, read(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4a598b96aa0250255f8758ce4ce57df23072dac7"}, "originalPosition": 42}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "4a598b96aa0250255f8758ce4ce57df23072dac7", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/4a598b96aa0250255f8758ce4ce57df23072dac7", "committedDate": "2020-09-23T22:56:11Z", "message": "[FLINK-18989][task] Remove ChannelStateReader"}, "afterCommit": {"oid": "859b19e0af50e8188addfa7ce42179b25c806c80", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/859b19e0af50e8188addfa7ce42179b25c806c80", "committedDate": "2020-09-25T18:05:58Z", "message": "[FLINK-18989][task] Address review feedback"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "859b19e0af50e8188addfa7ce42179b25c806c80", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/859b19e0af50e8188addfa7ce42179b25c806c80", "committedDate": "2020-09-25T18:05:58Z", "message": "[FLINK-18989][task] Address review feedback"}, "afterCommit": {"oid": "d5de65d33ae7216bee7ae8d55bf20839584c520f", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/d5de65d33ae7216bee7ae8d55bf20839584c520f", "committedDate": "2020-09-28T14:09:48Z", "message": "[FLINK-18989][task] Remove ChannelStateReader"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "d5de65d33ae7216bee7ae8d55bf20839584c520f", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/d5de65d33ae7216bee7ae8d55bf20839584c520f", "committedDate": "2020-09-28T14:09:48Z", "message": "[FLINK-18989][task] Remove ChannelStateReader"}, "afterCommit": {"oid": "ce1ddf0120e722f172916745adac6c4829cbae29", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/ce1ddf0120e722f172916745adac6c4829cbae29", "committedDate": "2020-09-29T06:46:57Z", "message": "[FLINK-18989][task] Remove ChannelStateReader"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk4MTQ5Nzcz", "url": "https://github.com/apache/flink/pull/13351#pullrequestreview-498149773", "createdAt": "2020-09-29T07:02:03Z", "commit": {"oid": "ce1ddf0120e722f172916745adac6c4829cbae29"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwNzowMjowNFrOHZdngg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwNzowMjowNFrOHZdngg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjQ2MTY5OA==", "bodyText": "@pnowojski,\nthis test started to fail after rebasing of Read channel state unconditionally.\nSo I replaced processSingleStep with processAll.\nPlease take a look.", "url": "https://github.com/apache/flink/pull/13351#discussion_r496461698", "createdAt": "2020-09-29T07:02:04Z", "author": {"login": "rkhachatryan"}, "path": "flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/tasks/StreamTaskMultipleInputSelectiveReadingTest.java", "diffHunk": "@@ -197,22 +198,21 @@ public void testInputStarvation() throws Exception {\n \t\t\ttestHarness.processElement(new StreamRecord<>(\"3\"), 1);\n \t\t\ttestHarness.processElement(new StreamRecord<>(\"4\"), 1);\n \n-\t\t\ttestHarness.processSingleStep();\n \t\t\texpectedOutput.add(new StreamRecord<>(\"[2]: 1\"));\n-\t\t\ttestHarness.processSingleStep();\n \t\t\texpectedOutput.add(new StreamRecord<>(\"[2]: 2\"));\n-\t\t\tassertThat(testHarness.getOutput(), contains(expectedOutput.toArray()));\n+\t\t\ttestHarness.processAll();\n+\t\t\tassertEquals(expectedOutput, new ArrayList<>(testHarness.getOutput()).subList(0, expectedOutput.size()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ce1ddf0120e722f172916745adac6c4829cbae29"}, "originalPosition": 39}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "ce1ddf0120e722f172916745adac6c4829cbae29", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/ce1ddf0120e722f172916745adac6c4829cbae29", "committedDate": "2020-09-29T06:46:57Z", "message": "[FLINK-18989][task] Remove ChannelStateReader"}, "afterCommit": {"oid": "f19eaa5205bc3ee57e8563eabe992b7377e6bd25", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/f19eaa5205bc3ee57e8563eabe992b7377e6bd25", "committedDate": "2020-09-29T07:04:42Z", "message": "[FLINK-18989][task] Remove ChannelStateReader"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk4Mjc5MzA0", "url": "https://github.com/apache/flink/pull/13351#pullrequestreview-498279304", "createdAt": "2020-09-29T09:26:05Z", "commit": {"oid": "0f5f4780b86654a878548a7f69b9474e7e3f3618"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwOToyNjowNlrOHZkV9A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwOTozNToxNlrOHZkujA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjU3MTg5Mg==", "bodyText": "An alternative way to propagate the exception would be:\n\t\t\t\ttaskInvocation.exceptionally(e -> {\n\t\t\t\t\tthrow new AssertionError(\"Task has stopped\", e);\n\t\t\t\t});\n\nIt depends if you want the test to fail through assertions or through unexpected exception.", "url": "https://github.com/apache/flink/pull/13351#discussion_r496571892", "createdAt": "2020-09-29T09:26:06Z", "author": {"login": "AHeise"}, "path": "flink-streaming-java/src/test/java/org/apache/flink/streaming/util/StreamTaskUtil.java", "diffHunk": "@@ -29,9 +30,10 @@\n  */\n public class StreamTaskUtil {\n \n-\tpublic static void waitTaskIsRunning(StreamTask<?, ?> task, CompletableFuture<Void> taskInvocation) throws InterruptedException {\n+\tpublic static void waitTaskIsRunning(StreamTask<?, ?> task, CompletableFuture<Void> taskInvocation) throws InterruptedException, ExecutionException {\n \t\twhile (!task.isRunning()) {\n \t\t\tif (taskInvocation.isDone()) {\n+\t\t\t\ttaskInvocation.get();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0f5f4780b86654a878548a7f69b9474e7e3f3618"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjU3NDQ2Mw==", "bodyText": "When looking at formatting, I usually just look for consistency and there are a few classes with this extra new line and many without. But which way is better is probably not worth debating ;).", "url": "https://github.com/apache/flink/pull/13351#discussion_r496574463", "createdAt": "2020-09-29T09:29:59Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/test/java/org/apache/flink/runtime/checkpoint/channel/ChannelStateChunkReaderTest.java", "diffHunk": "@@ -0,0 +1,162 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.fs.FSDataInputStream;\n+import org.apache.flink.core.memory.MemorySegmentFactory;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.FreeingBufferRecycler;\n+import org.apache.flink.runtime.io.network.buffer.NetworkBuffer;\n+import org.apache.flink.runtime.state.memory.ByteStreamStateHandle;\n+\n+import org.junit.Test;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+import static org.apache.flink.util.Preconditions.checkArgument;\n+import static org.apache.flink.util.Preconditions.checkState;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n+\n+/**\n+ * {@link ChannelStateChunkReader} test.\n+ */\n+public class ChannelStateChunkReaderTest {\n+\n+\t@Test(expected = TestException.class)\n+\tpublic void testBufferRecycledOnFailure() throws IOException {\n+\t\tFailingChannelStateSerializer serializer = new FailingChannelStateSerializer();\n+\t\tTestRecoveredChannelStateHandler handler = new TestRecoveredChannelStateHandler();\n+\n+\t\ttry (FSDataInputStream stream = geStream(serializer, 10)) {\n+\t\t\tnew ChannelStateChunkReader(serializer).readChunk(stream, serializer.getHeaderLength(), handler, \"channelInfo\");\n+\t\t\tcheckState(serializer.failed);\n+\t\t\tcheckState(!handler.requestedBuffers.isEmpty());\n+\t\t} finally {\n+\t\t\tassertTrue(handler.requestedBuffers.stream().allMatch(TestChannelStateByteBuffer::isRecycled));\n+\t\t}\n+\t}\n+\n+\t@Test\n+\tpublic void testBuffersNotRequestedForEmptyStream() throws IOException {\n+\t\tChannelStateSerializer serializer = new ChannelStateSerializerImpl();\n+\t\tTestRecoveredChannelStateHandler handler = new TestRecoveredChannelStateHandler();\n+\n+\t\ttry (FSDataInputStream stream = geStream(serializer, 0)) {\n+\t\t\tnew ChannelStateChunkReader(serializer).readChunk(stream, serializer.getHeaderLength(), handler, \"channelInfo\");\n+\t\t} finally {\n+\t\t\tassertTrue(handler.requestedBuffers.isEmpty());\n+\t\t}\n+\t}\n+\n+\t@Test\n+\tpublic void testNoSeekUnnecessarily() throws IOException {\n+\t\tfinal int offset = 123;\n+\t\tfinal FSDataInputStream stream = new FSDataInputStream() {\n+\t\t\t@Override\n+\t\t\tpublic long getPos() {\n+\t\t\t\treturn offset;\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic void seek(long ignored) {\n+\t\t\t\tfail();\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic int read() {\n+\t\t\t\treturn 0;\n+\t\t\t}\n+\t\t};\n+\n+\t\tnew ChannelStateChunkReader(new ChannelStateSerializerImpl())\n+\t\t\t.readChunk(stream, offset, new TestRecoveredChannelStateHandler(), \"channelInfo\");\n+\t}\n+\n+\tprivate static class TestRecoveredChannelStateHandler implements RecoveredChannelStateHandler<Object, Object> {\n+\t\tprivate final List<TestChannelStateByteBuffer> requestedBuffers = new ArrayList<>();\n+\n+\t\t@Override\n+\t\tpublic Tuple2<ChannelStateByteBuffer, Object> getBuffer(Object o) {\n+\t\t\tTestChannelStateByteBuffer buffer = new TestChannelStateByteBuffer();\n+\t\t\trequestedBuffers.add(buffer);\n+\t\t\treturn Tuple2.of(buffer, null);\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void recover(Object o, Object o2) {\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void close() throws Exception {\n+\t\t}\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDMzNjQzNQ=="}, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 114}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjU3ODE4OA==", "bodyText": "The issue is rather that Random is always strictly slower than ThreadLocalRandom because of lock acquisitions. However, you are right, as long as we not using multiple threads, we do not have contention, so the difference is marginal.", "url": "https://github.com/apache/flink/pull/13351#discussion_r496578188", "createdAt": "2020-09-29T09:35:16Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/test/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImplTest.java", "diffHunk": "@@ -0,0 +1,321 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.memory.MemorySegmentFactory;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.runtime.checkpoint.StateObjectCollection;\n+import org.apache.flink.runtime.checkpoint.TaskStateSnapshot;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.FreeingBufferRecycler;\n+import org.apache.flink.runtime.io.network.buffer.NetworkBuffer;\n+import org.apache.flink.runtime.io.network.buffer.NetworkBufferPool;\n+import org.apache.flink.runtime.io.network.partition.ResultPartition;\n+import org.apache.flink.runtime.io.network.partition.ResultPartitionBuilder;\n+import org.apache.flink.runtime.io.network.partition.ResultSubpartition.BufferAndBacklog;\n+import org.apache.flink.runtime.io.network.partition.ResultSubpartitionView;\n+import org.apache.flink.runtime.io.network.partition.consumer.BufferOrEvent;\n+import org.apache.flink.runtime.io.network.partition.consumer.InputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateBuilder;\n+import org.apache.flink.runtime.jobgraph.OperatorID;\n+import org.apache.flink.runtime.state.InputChannelStateHandle;\n+import org.apache.flink.runtime.state.ResultSubpartitionStateHandle;\n+import org.apache.flink.runtime.state.memory.ByteStreamStateHandle;\n+import org.apache.flink.util.function.ThrowingConsumer;\n+\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Random;\n+import java.util.function.BiFunction;\n+import java.util.function.Function;\n+\n+import static java.util.function.Function.identity;\n+import static java.util.stream.Collectors.toList;\n+import static java.util.stream.Collectors.toMap;\n+import static java.util.stream.IntStream.range;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * {@link SequentialChannelStateReaderImpl} Test.\n+ */\n+@RunWith(Parameterized.class)\n+public class SequentialChannelStateReaderImplTest {\n+\n+\t@Parameterized.Parameters(name = \"{0}: stateParLevel={1}, statePartsPerChannel={1}, stateBytesPerPart={2},  parLevel={4}, bufferSize={5}\")\n+\tpublic static Object[][] parameters() {\n+\t\treturn new Object[][]{\n+\t\t\t{\"NoStateAndNoChannels\", 0, 0, 0, 0, 0},\n+\t\t\t{\"NoState\", 0, 10, 10, 10, 10},\n+\t\t\t{\"ReadPermutedStateWithEqualBuffer\", 10, 10, 10, 10, 10},\n+\t\t\t{\"ReadPermutedStateWithReducedBuffer\", 10, 10, 10, 20, 10},\n+\t\t\t{\"ReadPermutedStateWithIncreasedBuffer\", 10, 10, 10, 10, 20},\n+\t\t};\n+\t}\n+\n+\tprivate final ChannelStateSerializer serializer;\n+\tprivate final Random random;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDMzOTU2Ng=="}, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 85}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "f19eaa5205bc3ee57e8563eabe992b7377e6bd25", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/f19eaa5205bc3ee57e8563eabe992b7377e6bd25", "committedDate": "2020-09-29T07:04:42Z", "message": "[FLINK-18989][task] Remove ChannelStateReader"}, "afterCommit": {"oid": "78668b9ada20adf13fc33cba2e9fd12874b47115", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/78668b9ada20adf13fc33cba2e9fd12874b47115", "committedDate": "2020-09-29T11:51:29Z", "message": "[FLINK-18989][task] Remove ChannelStateReader"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAxNzQzOTcw", "url": "https://github.com/apache/flink/pull/13351#pullrequestreview-501743970", "createdAt": "2020-10-05T06:12:38Z", "commit": {"oid": "78668b9ada20adf13fc33cba2e9fd12874b47115"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAxOTUyMDA5", "url": "https://github.com/apache/flink/pull/13351#pullrequestreview-501952009", "createdAt": "2020-10-05T11:15:38Z", "commit": {"oid": "78668b9ada20adf13fc33cba2e9fd12874b47115"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxMToxNTozOFrOHcYZWQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxMToxNTozOFrOHcYZWQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTUyMTg4MQ==", "bodyText": "This test is now not doing what it was intended.\nNow you are processing all elements from the input gate 1 before testHarness.processElement(new StreamRecord<>(\"1\"), 2); (L207/206) is being enqueued to input gate 2.\nI would guess that\n// to avoid starvation, if the input selection is ALL and availableInputsMask is not ALL,\n// always try to check and set the availability of another input\nif (inputSelectionHandler.shouldSetAvailableForAnotherInput()) {\n\tfullCheckAndSetAvailable();\n}\n\ncheck from StreamMultipleInputProcessor#selectNextReadingInputIndex is currently not tested.\nThe intention behind this test is:\n\nto have a long (just as well could be infinite) backlog of records to process on one of the inputs\nintroduce the availability change on the second input, and make sure it's checked/respected (instead of hot looping on the first input)\nalso throw in a third not selected input just to spice things a little bit\n\nWhy did you have to change this test?", "url": "https://github.com/apache/flink/pull/13351#discussion_r499521881", "createdAt": "2020-10-05T11:15:38Z", "author": {"login": "pnowojski"}, "path": "flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/tasks/StreamTaskMultipleInputSelectiveReadingTest.java", "diffHunk": "@@ -197,22 +198,21 @@ public void testInputStarvation() throws Exception {\n \t\t\ttestHarness.processElement(new StreamRecord<>(\"3\"), 1);\n \t\t\ttestHarness.processElement(new StreamRecord<>(\"4\"), 1);\n \n-\t\t\ttestHarness.processSingleStep();\n \t\t\texpectedOutput.add(new StreamRecord<>(\"[2]: 1\"));\n-\t\t\ttestHarness.processSingleStep();\n \t\t\texpectedOutput.add(new StreamRecord<>(\"[2]: 2\"));\n-\t\t\tassertThat(testHarness.getOutput(), contains(expectedOutput.toArray()));\n+\t\t\ttestHarness.processAll();\n+\t\t\tassertEquals(expectedOutput, new ArrayList<>(testHarness.getOutput()).subList(0, expectedOutput.size()));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjQ2MTY5OA=="}, "originalCommit": {"oid": "ce1ddf0120e722f172916745adac6c4829cbae29"}, "originalPosition": 39}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "78668b9ada20adf13fc33cba2e9fd12874b47115", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/78668b9ada20adf13fc33cba2e9fd12874b47115", "committedDate": "2020-09-29T11:51:29Z", "message": "[FLINK-18989][task] Remove ChannelStateReader"}, "afterCommit": {"oid": "e8c03361de3a583a8da862bcf3839de214c5cb83", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/e8c03361de3a583a8da862bcf3839de214c5cb83", "committedDate": "2020-10-05T15:35:17Z", "message": "[FLINK-18989][task] Remove non-sequental ChannelStateReader and the related code\n\n- ChannelStateReader and supporting classes\n- methods for channel state recovery in inputChannels, subpartitions, etc.\n- tests for reading channel state non-sequentially"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAyMjc0ODE1", "url": "https://github.com/apache/flink/pull/13351#pullrequestreview-502274815", "createdAt": "2020-10-05T17:34:27Z", "commit": {"oid": "e8c03361de3a583a8da862bcf3839de214c5cb83"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxNzozNDoyN1rOHcnDXg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxNzozNDoyN1rOHcnDXg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTc2MjAxNA==", "bodyText": "Would it work if replaced with testHarness.processAll ?", "url": "https://github.com/apache/flink/pull/13351#discussion_r499762014", "createdAt": "2020-10-05T17:34:27Z", "author": {"login": "pnowojski"}, "path": "flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/tasks/StreamTaskMultipleInputSelectiveReadingTest.java", "diffHunk": "@@ -183,6 +183,9 @@ public void testInputStarvation() throws Exception {\n \t\t\t\t\t.setupOutputForSingletonOperatorChain(new TestInputStarvationMultipleInputOperatorFactory())\n \t\t\t\t\t.build()) {\n \n+\t\t\tfor (int i = 0; i < testHarness.inputGates.length; i++) {\n+\t\t\t\ttestHarness.processSingleStep();\n+\t\t\t}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e8c03361de3a583a8da862bcf3839de214c5cb83"}, "originalPosition": 6}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "e8c03361de3a583a8da862bcf3839de214c5cb83", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/e8c03361de3a583a8da862bcf3839de214c5cb83", "committedDate": "2020-10-05T15:35:17Z", "message": "[FLINK-18989][task] Remove non-sequental ChannelStateReader and the related code\n\n- ChannelStateReader and supporting classes\n- methods for channel state recovery in inputChannels, subpartitions, etc.\n- tests for reading channel state non-sequentially"}, "afterCommit": {"oid": "3e38dfe739fa34ed301dc2c0303086568f138d57", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/3e38dfe739fa34ed301dc2c0303086568f138d57", "committedDate": "2020-10-05T19:29:13Z", "message": "[FLINK-18989][task] Remove non-sequental ChannelStateReader and the related code\n\n- ChannelStateReader and supporting classes\n- methods for channel state recovery in inputChannels, subpartitions, etc.\n- tests for reading channel state non-sequentially"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAyNjc0MDQw", "url": "https://github.com/apache/flink/pull/13351#pullrequestreview-502674040", "createdAt": "2020-10-06T07:55:45Z", "commit": {"oid": "3e38dfe739fa34ed301dc2c0303086568f138d57"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "3e38dfe739fa34ed301dc2c0303086568f138d57", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/3e38dfe739fa34ed301dc2c0303086568f138d57", "committedDate": "2020-10-05T19:29:13Z", "message": "[FLINK-18989][task] Remove non-sequental ChannelStateReader and the related code\n\n- ChannelStateReader and supporting classes\n- methods for channel state recovery in inputChannels, subpartitions, etc.\n- tests for reading channel state non-sequentially"}, "afterCommit": {"oid": "5f4afd19fde91ffe545666daffd8953b126de7bf", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/5f4afd19fde91ffe545666daffd8953b126de7bf", "committedDate": "2020-10-06T15:34:43Z", "message": "[FLINK-18989][task] Remove non-sequental ChannelStateReader and the related code\n\n- ChannelStateReader and supporting classes\n- methods for channel state recovery in inputChannels, subpartitions, etc.\n- tests for reading channel state non-sequentially"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "5f4afd19fde91ffe545666daffd8953b126de7bf", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/5f4afd19fde91ffe545666daffd8953b126de7bf", "committedDate": "2020-10-06T15:34:43Z", "message": "[FLINK-18989][task] Remove non-sequental ChannelStateReader and the related code\n\n- ChannelStateReader and supporting classes\n- methods for channel state recovery in inputChannels, subpartitions, etc.\n- tests for reading channel state non-sequentially"}, "afterCommit": {"oid": "ec6d68e0c67f14994082adb24e4f86dd599fc9f0", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/ec6d68e0c67f14994082adb24e4f86dd599fc9f0", "committedDate": "2020-10-06T21:21:44Z", "message": "[FLINK-18989][task] Remove non-sequental ChannelStateReader and the related code\n\n- ChannelStateReader and supporting classes\n- methods for channel state recovery in inputChannels, subpartitions, etc.\n- tests for reading channel state non-sequentially"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "ec6d68e0c67f14994082adb24e4f86dd599fc9f0", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/ec6d68e0c67f14994082adb24e4f86dd599fc9f0", "committedDate": "2020-10-06T21:21:44Z", "message": "[FLINK-18989][task] Remove non-sequental ChannelStateReader and the related code\n\n- ChannelStateReader and supporting classes\n- methods for channel state recovery in inputChannels, subpartitions, etc.\n- tests for reading channel state non-sequentially"}, "afterCommit": {"oid": "3041c4816637a7e1686723ec0e04d99db5cce14c", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/3041c4816637a7e1686723ec0e04d99db5cce14c", "committedDate": "2020-10-08T11:35:15Z", "message": "[FLINK-18989][task] Remove non-sequental ChannelStateReader and the related code\n\n- ChannelStateReader and supporting classes\n- methods for channel state recovery in inputChannels, subpartitions, etc.\n- tests for reading channel state non-sequentially"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a74023442126ad0bc056566da4616d73f98096fd", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/a74023442126ad0bc056566da4616d73f98096fd", "committedDate": "2020-10-08T11:38:36Z", "message": "[hotfix][tests] Report FailureReason in StreamTask tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cc26964340d6a074ee695b93c9ba43b665fdb0f6", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/cc26964340d6a074ee695b93c9ba43b665fdb0f6", "committedDate": "2020-10-08T11:38:36Z", "message": "[hotfix][tests] Return empty array of gates instead of null"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e574b031862fab04177e5689211b0852d9ff076a", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/e574b031862fab04177e5689211b0852d9ff076a", "committedDate": "2020-10-08T11:38:36Z", "message": "[FLINK-19385] Request partitions for each inputGate independently"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e4946c0fbc1a46d866b29edff083eb4e2370e437", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/e4946c0fbc1a46d866b29edff083eb4e2370e437", "committedDate": "2020-10-08T11:38:36Z", "message": "[task][refactor] Introduce InputChannel.setup\n\nThis removes casts to RemoteInputChannel and RecoveredInputChannel"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "eee860a9e25bc66972d967fe1aef60746180dc7e", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/eee860a9e25bc66972d967fe1aef60746180dc7e", "committedDate": "2020-10-08T11:38:36Z", "message": "[FLINK-18989][task] Read channel state sequentially\n\nNon-sequential state reader and the associated code will be removed in\nsubsequent commits."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "3041c4816637a7e1686723ec0e04d99db5cce14c", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/3041c4816637a7e1686723ec0e04d99db5cce14c", "committedDate": "2020-10-08T11:35:15Z", "message": "[FLINK-18989][task] Remove non-sequental ChannelStateReader and the related code\n\n- ChannelStateReader and supporting classes\n- methods for channel state recovery in inputChannels, subpartitions, etc.\n- tests for reading channel state non-sequentially"}, "afterCommit": {"oid": "59ca6fe974dd7270f5b6de4880b9de845eda830f", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/59ca6fe974dd7270f5b6de4880b9de845eda830f", "committedDate": "2020-10-08T11:38:36Z", "message": "[FLINK-18989][task] Remove non-sequental ChannelStateReader and the related code\n\n- ChannelStateReader and supporting classes\n- methods for channel state recovery in inputChannels, subpartitions, etc.\n- tests for reading channel state non-sequentially"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA1NTE4NjE2", "url": "https://github.com/apache/flink/pull/13351#pullrequestreview-505518616", "createdAt": "2020-10-09T09:40:38Z", "commit": {"oid": "99123f77a898f80033fc4b70e6a588c9ef61425f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOVQwOTo0MDozOVrOHfCp7A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOVQwOTo0MDozOVrOHfCp7A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjMxMTQwNA==", "bodyText": "Something is wrong in this commit. I'm suspecting that you hit some weird maven/intellij interaction. Could you please delete all log4js properties?", "url": "https://github.com/apache/flink/pull/13351#discussion_r502311404", "createdAt": "2020-10-09T09:40:39Z", "author": {"login": "AHeise"}, "path": "flink-quickstart/flink-quickstart-java/src/main/resources/archetype-resources/${project.build.directory}/classes/log4j2.properties", "diffHunk": "@@ -0,0 +1,25 @@\n+################################################################################\n+#  Licensed to the Apache Software Foundation (ASF) under one\n+#  or more contributor license agreements.  See the NOTICE file\n+#  distributed with this work for additional information\n+#  regarding copyright ownership.  The ASF licenses this file\n+#  to you under the Apache License, Version 2.0 (the\n+#  \"License\"); you may not use this file except in compliance\n+#  with the License.  You may obtain a copy of the License at\n+#\n+#      http://www.apache.org/licenses/LICENSE-2.0\n+#\n+#  Unless required by applicable law or agreed to in writing, software\n+#  distributed under the License is distributed on an \"AS IS\" BASIS,\n+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+#  See the License for the specific language governing permissions and\n+# limitations under the License.\n+################################################################################\n+\n+rootLogger.level = INFO", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "99123f77a898f80033fc4b70e6a588c9ef61425f"}, "originalPosition": 19}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5dd10853af3a821bdb567509a55c50280f55368a", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/5dd10853af3a821bdb567509a55c50280f55368a", "committedDate": "2020-10-09T09:52:17Z", "message": "[FLINK-18989][task] Read channel state unconditionally\n\nMotivation: reduce the required test coverage (tests will be removed in\na later commit)."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7cc1374100e7e9d43d62ace88673b8eaba1a1568", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/7cc1374100e7e9d43d62ace88673b8eaba1a1568", "committedDate": "2020-10-09T09:52:39Z", "message": "[FLINK-18989][task] Allow conversion of RecoveredInputChannel only after state was fully consumed\n\nMotivation:\n1. Guarantee state is not lost\n2. Reduce the required test coverage (tests will be removed in the next\ncommit)"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c0b8f40dbf063678f53fc427bcf57c4c7876f87e", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/c0b8f40dbf063678f53fc427bcf57c4c7876f87e", "committedDate": "2020-10-09T09:52:39Z", "message": "[FLINK-18989][task] Remove non-sequental ChannelStateReader and the related code\n\n- ChannelStateReader and supporting classes\n- methods for channel state recovery in inputChannels, subpartitions, etc.\n- tests for reading channel state non-sequentially"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "59ca6fe974dd7270f5b6de4880b9de845eda830f", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/59ca6fe974dd7270f5b6de4880b9de845eda830f", "committedDate": "2020-10-08T11:38:36Z", "message": "[FLINK-18989][task] Remove non-sequental ChannelStateReader and the related code\n\n- ChannelStateReader and supporting classes\n- methods for channel state recovery in inputChannels, subpartitions, etc.\n- tests for reading channel state non-sequentially"}, "afterCommit": {"oid": "c0b8f40dbf063678f53fc427bcf57c4c7876f87e", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/c0b8f40dbf063678f53fc427bcf57c4c7876f87e", "committedDate": "2020-10-09T09:52:39Z", "message": "[FLINK-18989][task] Remove non-sequental ChannelStateReader and the related code\n\n- ChannelStateReader and supporting classes\n- methods for channel state recovery in inputChannels, subpartitions, etc.\n- tests for reading channel state non-sequentially"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4510, "cost": 1, "resetAt": "2021-10-28T16:48:13Z"}}}