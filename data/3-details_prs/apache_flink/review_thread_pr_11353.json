{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzg1NDk5MjEx", "number": 11353, "reviewThreads": {"totalCount": 49, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQwMjoxNzoxOFrODm6u7Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQxNTowMDo1M1rODyi2qw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQyMTM0NzY1OnYy", "diffSide": "RIGHT", "path": "flink-yarn/src/test/java/org/apache/flink/yarn/YarnResourceManagerTest.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQwMjoxNzoxOFrOF0nIdg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQwNToyMjoyOFrOF0pkqA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcxMTQxNA==", "bodyText": "Not sure we could add underscores in the method name.", "url": "https://github.com/apache/flink/pull/11353#discussion_r390711414", "createdAt": "2020-03-11T02:17:18Z", "author": {"login": "KarmaGYZ"}, "path": "flink-yarn/src/test/java/org/apache/flink/yarn/YarnResourceManagerTest.java", "diffHunk": "@@ -584,6 +590,85 @@ public void testGetCpuExceedMaxInt() throws Exception {\n \t\t}};\n \t}\n \n+\t@Test\n+\tpublic void testWorkerSpecContainerResourceAdapter_MatchVcores() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "80a6254bc7f7b433b9c6ab26e2fa63d5c7e0ba2a"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDczODY2MA==", "bodyText": "I'm sure about this either. That's why I kept it, for discussions in the PR.\nOn one hand, I do find the following in the code style guide.\n\nNon-static fields/methods must be in lower camel case.\n\nOn the other hand, the underscores do provide better readability in the long test case name, and there are already test cases using them (e.g., MiniClusterConfigurationTest, CatalogTest, etc.).", "url": "https://github.com/apache/flink/pull/11353#discussion_r390738660", "createdAt": "2020-03-11T04:20:50Z", "author": {"login": "xintongsong"}, "path": "flink-yarn/src/test/java/org/apache/flink/yarn/YarnResourceManagerTest.java", "diffHunk": "@@ -584,6 +590,85 @@ public void testGetCpuExceedMaxInt() throws Exception {\n \t\t}};\n \t}\n \n+\t@Test\n+\tpublic void testWorkerSpecContainerResourceAdapter_MatchVcores() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcxMTQxNA=="}, "originalCommit": {"oid": "80a6254bc7f7b433b9c6ab26e2fa63d5c7e0ba2a"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDc1MTQwMA==", "bodyText": "I prefer to follow the code style guide if possible but I'm ok with it atm. Someone who has better English proficiency may give us another idea about the name.", "url": "https://github.com/apache/flink/pull/11353#discussion_r390751400", "createdAt": "2020-03-11T05:22:28Z", "author": {"login": "KarmaGYZ"}, "path": "flink-yarn/src/test/java/org/apache/flink/yarn/YarnResourceManagerTest.java", "diffHunk": "@@ -584,6 +590,85 @@ public void testGetCpuExceedMaxInt() throws Exception {\n \t\t}};\n \t}\n \n+\t@Test\n+\tpublic void testWorkerSpecContainerResourceAdapter_MatchVcores() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcxMTQxNA=="}, "originalCommit": {"oid": "80a6254bc7f7b433b9c6ab26e2fa63d5c7e0ba2a"}, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQyMTM3MDY4OnYy", "diffSide": "RIGHT", "path": "flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQwMjozMjo0M1rOF0nW6g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQwNDoyOTo1OVrOF0o6Rw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcxNTExNA==", "bodyText": "These records seem never to be cleaned up. It will not cause any problem atm though.", "url": "https://github.com/apache/flink/pull/11353#discussion_r390715114", "createdAt": "2020-03-11T02:32:43Z", "author": {"login": "KarmaGYZ"}, "path": "flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java", "diffHunk": "@@ -615,4 +632,85 @@ protected double getCpuCores(final Configuration configuration) {\n \t\t//noinspection NumericCastThatLosesPrecision\n \t\treturn cpuCoresLong;\n \t}\n+\n+\t/**\n+\t * Utility class for converting between Flink {@link WorkerResourceSpec} and Yarn {@link Resource}.\n+\t */\n+\t@VisibleForTesting\n+\tstatic class WorkerSpecContainerResourceAdapter {\n+\t\tprivate final Configuration flinkConfig;\n+\t\tprivate final int minMemMB;\n+\t\tprivate final int minVcore;\n+\t\tprivate final boolean matchVcores;\n+\t\tprivate final Map<WorkerResourceSpec, Resource> workerSpecToContainerResource;\n+\t\tprivate final Map<Resource, Collection<WorkerResourceSpec>> containerResourceToWorkerSpecs;\n+\t\tprivate final Map<Integer, Collection<Resource>> containerMemoryToContainerResource;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "80a6254bc7f7b433b9c6ab26e2fa63d5c7e0ba2a"}, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDc0MDU1MQ==", "bodyText": "I think the upper bond of the amount of records really depends on how many different WorkerResourceSpec do we have.\nIf we want to clean the unused records up, the WorkerSpecContainerResourceAdapter will need YarnResourceManager to tell it which WorkerResourceSpec is no longer needed (all corresponding TMs are completed and no pending ones). ATM, I don't see the necessity for such complexity.", "url": "https://github.com/apache/flink/pull/11353#discussion_r390740551", "createdAt": "2020-03-11T04:29:59Z", "author": {"login": "xintongsong"}, "path": "flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java", "diffHunk": "@@ -615,4 +632,85 @@ protected double getCpuCores(final Configuration configuration) {\n \t\t//noinspection NumericCastThatLosesPrecision\n \t\treturn cpuCoresLong;\n \t}\n+\n+\t/**\n+\t * Utility class for converting between Flink {@link WorkerResourceSpec} and Yarn {@link Resource}.\n+\t */\n+\t@VisibleForTesting\n+\tstatic class WorkerSpecContainerResourceAdapter {\n+\t\tprivate final Configuration flinkConfig;\n+\t\tprivate final int minMemMB;\n+\t\tprivate final int minVcore;\n+\t\tprivate final boolean matchVcores;\n+\t\tprivate final Map<WorkerResourceSpec, Resource> workerSpecToContainerResource;\n+\t\tprivate final Map<Resource, Collection<WorkerResourceSpec>> containerResourceToWorkerSpecs;\n+\t\tprivate final Map<Integer, Collection<Resource>> containerMemoryToContainerResource;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcxNTExNA=="}, "originalCommit": {"oid": "80a6254bc7f7b433b9c6ab26e2fa63d5c7e0ba2a"}, "originalPosition": 75}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQyMTQxNjA0OnYy", "diffSide": "RIGHT", "path": "flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQwMzowMzo0MlrOF0nzAw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQwNDozNTo1NlrOF0o-vQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcyMjMwNw==", "bodyText": "Seems we don't need to add it to containerMemoryToContainerResource if matchVcores is false.", "url": "https://github.com/apache/flink/pull/11353#discussion_r390722307", "createdAt": "2020-03-11T03:03:42Z", "author": {"login": "KarmaGYZ"}, "path": "flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java", "diffHunk": "@@ -615,4 +632,85 @@ protected double getCpuCores(final Configuration configuration) {\n \t\t//noinspection NumericCastThatLosesPrecision\n \t\treturn cpuCoresLong;\n \t}\n+\n+\t/**\n+\t * Utility class for converting between Flink {@link WorkerResourceSpec} and Yarn {@link Resource}.\n+\t */\n+\t@VisibleForTesting\n+\tstatic class WorkerSpecContainerResourceAdapter {\n+\t\tprivate final Configuration flinkConfig;\n+\t\tprivate final int minMemMB;\n+\t\tprivate final int minVcore;\n+\t\tprivate final boolean matchVcores;\n+\t\tprivate final Map<WorkerResourceSpec, Resource> workerSpecToContainerResource;\n+\t\tprivate final Map<Resource, Collection<WorkerResourceSpec>> containerResourceToWorkerSpecs;\n+\t\tprivate final Map<Integer, Collection<Resource>> containerMemoryToContainerResource;\n+\n+\t\t@VisibleForTesting\n+\t\tWorkerSpecContainerResourceAdapter(\n+\t\t\t\tfinal Configuration flinkConfig,\n+\t\t\t\tfinal int minMemMB,\n+\t\t\t\tfinal int minVcore,\n+\t\t\t\tfinal boolean matchVcores) {\n+\t\t\tthis.flinkConfig = Preconditions.checkNotNull(flinkConfig);\n+\t\t\tthis.minMemMB = minMemMB;\n+\t\t\tthis.minVcore = minVcore;\n+\t\t\tthis.matchVcores = matchVcores;\n+\t\t\tworkerSpecToContainerResource = new HashMap<>();\n+\t\t\tcontainerResourceToWorkerSpecs = new HashMap<>();\n+\t\t\tcontainerMemoryToContainerResource = new HashMap<>();\n+\t\t}\n+\n+\t\t@VisibleForTesting\n+\t\tResource getContainerResource(final WorkerResourceSpec workerResourceSpec) {\n+\t\t\treturn workerSpecToContainerResource.computeIfAbsent(\n+\t\t\t\tPreconditions.checkNotNull(workerResourceSpec),\n+\t\t\t\tthis::createAndMapContainerResource);\n+\t\t}\n+\n+\t\t@VisibleForTesting\n+\t\tCollection<WorkerResourceSpec> getWorkerSpecs(final Resource containerResource) {\n+\t\t\treturn getEquivalentContainerResource(containerResource).stream()\n+\t\t\t\t.flatMap(resource -> containerResourceToWorkerSpecs.getOrDefault(resource, Collections.emptyList()).stream())\n+\t\t\t\t.collect(Collectors.toList());\n+\t\t}\n+\n+\t\t@VisibleForTesting\n+\t\tCollection<Resource> getEquivalentContainerResource(final Resource containerResource) {\n+\t\t\t// Yarn might ignore the requested vcores, depending on its configurations.\n+\t\t\t// In such cases, we should also not matching vcores.\n+\t\t\treturn matchVcores ?\n+\t\t\t\tCollections.singletonList(containerResource) :\n+\t\t\t\tcontainerMemoryToContainerResource.getOrDefault(containerResource.getMemory(), Collections.emptyList());\n+\t\t}\n+\n+\t\tprivate Resource createAndMapContainerResource(final WorkerResourceSpec workerResourceSpec) {\n+\t\t\t// TODO: need to unset process/flink memory size from configuration if dynamic worker resource is activated\n+\t\t\tfinal TaskExecutorProcessSpec taskExecutorProcessSpec =\n+\t\t\t\tTaskExecutorProcessUtils.processSpecFromWorkerResourceSpec(flinkConfig, workerResourceSpec);\n+\t\t\tfinal Resource containerResource = Resource.newInstance(\n+\t\t\t\tnormalize(taskExecutorProcessSpec.getTotalProcessMemorySize().getMebiBytes(), minMemMB),\n+\t\t\t\tnormalize(taskExecutorProcessSpec.getCpuCores().getValue().intValue(), minVcore));\n+\t\t\tcontainerResourceToWorkerSpecs.computeIfAbsent(containerResource, ignored -> new ArrayList<>())\n+\t\t\t\t.add(workerResourceSpec);\n+\t\t\tcontainerMemoryToContainerResource.computeIfAbsent(containerResource.getMemory(), ignored -> new HashSet<>())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "80a6254bc7f7b433b9c6ab26e2fa63d5c7e0ba2a"}, "originalPosition": 124}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDc0MTY5Mw==", "bodyText": "True, we don't need containerMemoryToContainerResource if not matching vcores. However, I would try avoid unnecessary if-else branches.", "url": "https://github.com/apache/flink/pull/11353#discussion_r390741693", "createdAt": "2020-03-11T04:35:56Z", "author": {"login": "xintongsong"}, "path": "flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java", "diffHunk": "@@ -615,4 +632,85 @@ protected double getCpuCores(final Configuration configuration) {\n \t\t//noinspection NumericCastThatLosesPrecision\n \t\treturn cpuCoresLong;\n \t}\n+\n+\t/**\n+\t * Utility class for converting between Flink {@link WorkerResourceSpec} and Yarn {@link Resource}.\n+\t */\n+\t@VisibleForTesting\n+\tstatic class WorkerSpecContainerResourceAdapter {\n+\t\tprivate final Configuration flinkConfig;\n+\t\tprivate final int minMemMB;\n+\t\tprivate final int minVcore;\n+\t\tprivate final boolean matchVcores;\n+\t\tprivate final Map<WorkerResourceSpec, Resource> workerSpecToContainerResource;\n+\t\tprivate final Map<Resource, Collection<WorkerResourceSpec>> containerResourceToWorkerSpecs;\n+\t\tprivate final Map<Integer, Collection<Resource>> containerMemoryToContainerResource;\n+\n+\t\t@VisibleForTesting\n+\t\tWorkerSpecContainerResourceAdapter(\n+\t\t\t\tfinal Configuration flinkConfig,\n+\t\t\t\tfinal int minMemMB,\n+\t\t\t\tfinal int minVcore,\n+\t\t\t\tfinal boolean matchVcores) {\n+\t\t\tthis.flinkConfig = Preconditions.checkNotNull(flinkConfig);\n+\t\t\tthis.minMemMB = minMemMB;\n+\t\t\tthis.minVcore = minVcore;\n+\t\t\tthis.matchVcores = matchVcores;\n+\t\t\tworkerSpecToContainerResource = new HashMap<>();\n+\t\t\tcontainerResourceToWorkerSpecs = new HashMap<>();\n+\t\t\tcontainerMemoryToContainerResource = new HashMap<>();\n+\t\t}\n+\n+\t\t@VisibleForTesting\n+\t\tResource getContainerResource(final WorkerResourceSpec workerResourceSpec) {\n+\t\t\treturn workerSpecToContainerResource.computeIfAbsent(\n+\t\t\t\tPreconditions.checkNotNull(workerResourceSpec),\n+\t\t\t\tthis::createAndMapContainerResource);\n+\t\t}\n+\n+\t\t@VisibleForTesting\n+\t\tCollection<WorkerResourceSpec> getWorkerSpecs(final Resource containerResource) {\n+\t\t\treturn getEquivalentContainerResource(containerResource).stream()\n+\t\t\t\t.flatMap(resource -> containerResourceToWorkerSpecs.getOrDefault(resource, Collections.emptyList()).stream())\n+\t\t\t\t.collect(Collectors.toList());\n+\t\t}\n+\n+\t\t@VisibleForTesting\n+\t\tCollection<Resource> getEquivalentContainerResource(final Resource containerResource) {\n+\t\t\t// Yarn might ignore the requested vcores, depending on its configurations.\n+\t\t\t// In such cases, we should also not matching vcores.\n+\t\t\treturn matchVcores ?\n+\t\t\t\tCollections.singletonList(containerResource) :\n+\t\t\t\tcontainerMemoryToContainerResource.getOrDefault(containerResource.getMemory(), Collections.emptyList());\n+\t\t}\n+\n+\t\tprivate Resource createAndMapContainerResource(final WorkerResourceSpec workerResourceSpec) {\n+\t\t\t// TODO: need to unset process/flink memory size from configuration if dynamic worker resource is activated\n+\t\t\tfinal TaskExecutorProcessSpec taskExecutorProcessSpec =\n+\t\t\t\tTaskExecutorProcessUtils.processSpecFromWorkerResourceSpec(flinkConfig, workerResourceSpec);\n+\t\t\tfinal Resource containerResource = Resource.newInstance(\n+\t\t\t\tnormalize(taskExecutorProcessSpec.getTotalProcessMemorySize().getMebiBytes(), minMemMB),\n+\t\t\t\tnormalize(taskExecutorProcessSpec.getCpuCores().getValue().intValue(), minVcore));\n+\t\t\tcontainerResourceToWorkerSpecs.computeIfAbsent(containerResource, ignored -> new ArrayList<>())\n+\t\t\t\t.add(workerResourceSpec);\n+\t\t\tcontainerMemoryToContainerResource.computeIfAbsent(containerResource.getMemory(), ignored -> new HashSet<>())", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcyMjMwNw=="}, "originalCommit": {"oid": "80a6254bc7f7b433b9c6ab26e2fa63d5c7e0ba2a"}, "originalPosition": 124}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQyMTQ3MDA5OnYy", "diffSide": "RIGHT", "path": "flink-yarn/src/test/java/org/apache/flink/yarn/TestingYarnAMRMClientAsync.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQwMzo0MjozMlrOF0oTlg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQwNDo0MjowNVrOF0pDsg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDczMDY0Ng==", "bodyText": "Is there any benefit we could get from extending AMRMClientAsyncImpl instead of AMRMClientAsync? This class is annotated with Unstable.", "url": "https://github.com/apache/flink/pull/11353#discussion_r390730646", "createdAt": "2020-03-11T03:42:32Z", "author": {"login": "KarmaGYZ"}, "path": "flink-yarn/src/test/java/org/apache/flink/yarn/TestingYarnAMRMClientAsync.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.yarn;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.api.java.tuple.Tuple4;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.yarn.api.records.ContainerId;\n+import org.apache.hadoop.yarn.api.records.Priority;\n+import org.apache.hadoop.yarn.api.records.Resource;\n+import org.apache.hadoop.yarn.client.api.AMRMClient;\n+import org.apache.hadoop.yarn.client.api.async.AMRMClientAsync;\n+import org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl;\n+\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+\n+/**\n+ * A Yarn {@link AMRMClientAsync} implementation for testing.\n+ */\n+public class TestingYarnAMRMClientAsync extends AMRMClientAsyncImpl<AMRMClient.ContainerRequest> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9fc7db6c74fbc2b870daa35bd9da1a9f3132360b"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDc0Mjk2Mg==", "bodyText": "The problem is that AMRMClientAsync is an abstract class and we have to implement all its abstract methods if extending it. There are some abstract methods in later Hadoop versions that we cannot easily implement for early versions, because the absence of argument/return value type.\nAlthough AMRMClientAsyncImpl is unstable, we are not really depending on its implementation. The methods we override are all declared in AMRMClientAsync, which is stable. So it should not be a problem.", "url": "https://github.com/apache/flink/pull/11353#discussion_r390742962", "createdAt": "2020-03-11T04:42:05Z", "author": {"login": "xintongsong"}, "path": "flink-yarn/src/test/java/org/apache/flink/yarn/TestingYarnAMRMClientAsync.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.yarn;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.api.java.tuple.Tuple4;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.yarn.api.records.ContainerId;\n+import org.apache.hadoop.yarn.api.records.Priority;\n+import org.apache.hadoop.yarn.api.records.Resource;\n+import org.apache.hadoop.yarn.client.api.AMRMClient;\n+import org.apache.hadoop.yarn.client.api.async.AMRMClientAsync;\n+import org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl;\n+\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+\n+/**\n+ * A Yarn {@link AMRMClientAsync} implementation for testing.\n+ */\n+public class TestingYarnAMRMClientAsync extends AMRMClientAsyncImpl<AMRMClient.ContainerRequest> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDczMDY0Ng=="}, "originalCommit": {"oid": "9fc7db6c74fbc2b870daa35bd9da1a9f3132360b"}, "originalPosition": 42}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUwMDAyODYyOnYy", "diffSide": "RIGHT", "path": "flink-runtime/src/test/java/org/apache/flink/runtime/resourcemanager/ActiveResourceManagerTest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNToxNjozNlrOGAZ-EA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNToxNjozNlrOGAZ-EA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzA3ODY3Mg==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            public class ActiveResourceManagerTest {\n          \n          \n            \n            public class ActiveResourceManagerTest extends TestLogger {", "url": "https://github.com/apache/flink/pull/11353#discussion_r403078672", "createdAt": "2020-04-03T15:16:36Z", "author": {"login": "tillrohrmann"}, "path": "flink-runtime/src/test/java/org/apache/flink/runtime/resourcemanager/ActiveResourceManagerTest.java", "diffHunk": "@@ -0,0 +1,73 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.resourcemanager;\n+\n+import org.junit.Test;\n+\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.core.Is.is;\n+\n+/**\n+ * Tests for {@link ActiveResourceManager}.\n+ */\n+public class ActiveResourceManagerTest {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d3fc9010a987cd299f355ddc1fbeb4225b5fb418"}, "originalPosition": 29}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUwMDA2MDQ5OnYy", "diffSide": "RIGHT", "path": "flink-kubernetes/src/main/java/org/apache/flink/kubernetes/KubernetesResourceManager.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNToyNDoxNVrOGAaSDQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNToyNDoxNVrOGAaSDQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzA4Mzc4OQ==", "bodyText": "Call me old fashioned, but I think the for-each loop for (KubernetesPod pod: pods) is superior to forEach.", "url": "https://github.com/apache/flink/pull/11353#discussion_r403083789", "createdAt": "2020-04-03T15:24:15Z", "author": {"login": "tillrohrmann"}, "path": "flink-kubernetes/src/main/java/org/apache/flink/kubernetes/KubernetesResourceManager.java", "diffHunk": "@@ -183,16 +178,18 @@ public boolean stopWorker(final KubernetesWorkerNode worker) {\n \t@Override\n \tpublic void onAdded(List<KubernetesPod> pods) {\n \t\trunAsync(() -> {\n-\t\t\tfor (KubernetesPod pod : pods) {\n-\t\t\t\tif (numPendingPodRequests > 0) {\n-\t\t\t\t\tnumPendingPodRequests--;\n+\t\t\tpods.forEach(pod -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d7a0626255f2445cfe20dcb7bab1aede3af4b0d7"}, "originalPosition": 69}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUwMDA3NzkwOnYy", "diffSide": "RIGHT", "path": "flink-kubernetes/src/main/java/org/apache/flink/kubernetes/KubernetesResourceManager.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNToyODoyM1rOGAadIQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNToyODoyM1rOGAadIQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzA4NjYyNQ==", "bodyText": "I'm wondering whether this logic shouldn't go into the ActiveResourceManager. I would expect that all ActiveResourceManager implementations would need to do something similar. Maybe we could introduce notifyNewWorkerStarted(WorkerResourceSpec). This could also have the benefit that we could hide pendingWorkerCounter completely.", "url": "https://github.com/apache/flink/pull/11353#discussion_r403086625", "createdAt": "2020-04-03T15:28:23Z", "author": {"login": "tillrohrmann"}, "path": "flink-kubernetes/src/main/java/org/apache/flink/kubernetes/KubernetesResourceManager.java", "diffHunk": "@@ -183,16 +178,18 @@ public boolean stopWorker(final KubernetesWorkerNode worker) {\n \t@Override\n \tpublic void onAdded(List<KubernetesPod> pods) {\n \t\trunAsync(() -> {\n-\t\t\tfor (KubernetesPod pod : pods) {\n-\t\t\t\tif (numPendingPodRequests > 0) {\n-\t\t\t\t\tnumPendingPodRequests--;\n+\t\t\tpods.forEach(pod -> {\n+\t\t\t\tWorkerResourceSpec workerResourceSpec = podWorkerResources.get(pod.getName());\n+\t\t\t\tfinal int pendingNum = pendingWorkerCounter.getNum(workerResourceSpec);\n+\t\t\t\tif (pendingNum > 0) {\n+\t\t\t\t\tpendingWorkerCounter.decreaseAndGet(workerResourceSpec);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d7a0626255f2445cfe20dcb7bab1aede3af4b0d7"}, "originalPosition": 73}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUwMDA4NzM5OnYy", "diffSide": "RIGHT", "path": "flink-kubernetes/src/main/java/org/apache/flink/kubernetes/KubernetesResourceManager.java", "isResolved": true, "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNTozMDoyOVrOGAajBQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQwMjozODoxNFrOGDHnFg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzA4ODEzMw==", "bodyText": "Should we add a check state to ensure that we fail in case that we request a different size?", "url": "https://github.com/apache/flink/pull/11353#discussion_r403088133", "createdAt": "2020-04-03T15:30:29Z", "author": {"login": "tillrohrmann"}, "path": "flink-kubernetes/src/main/java/org/apache/flink/kubernetes/KubernetesResourceManager.java", "diffHunk": "@@ -232,57 +229,75 @@ private void recoverWorkerNodesFromPreviousAttempts() throws ResourceManagerExce\n \t\t\t++currentMaxAttemptId);\n \t}\n \n-\tprivate void requestKubernetesPod() {\n-\t\tnumPendingPodRequests++;\n+\tprivate void requestKubernetesPod(WorkerResourceSpec workerResourceSpec) {\n+\t\tfinal KubernetesTaskManagerParameters parameters =\n+\t\t\tcreateKubernetesTaskManagerParameters(workerResourceSpec);\n+\n+\t\tpodWorkerResources.put(parameters.getPodName(), workerResourceSpec);\n+\t\tfinal int pendingWorkerNum = pendingWorkerCounter.increaseAndGet(workerResourceSpec);\n \n \t\tlog.info(\"Requesting new TaskManager pod with <{},{}>. Number pending requests {}.\",\n-\t\t\tdefaultMemoryMB,\n-\t\t\tdefaultCpus,\n-\t\t\tnumPendingPodRequests);\n+\t\t\tparameters.getTaskManagerMemoryMB(),\n+\t\t\tparameters.getTaskManagerCPU(),\n+\t\t\tpendingWorkerNum);\n+\t\tlog.info(\"TaskManager {} will be started with {}.\", parameters.getPodName(), workerResourceSpec);\n+\n+\t\tfinal KubernetesPod taskManagerPod =\n+\t\t\tKubernetesTaskManagerFactory.createTaskManagerComponent(parameters);\n+\t\tkubeClient.createTaskManagerPod(taskManagerPod);\n+\t}\n+\n+\tprivate KubernetesTaskManagerParameters createKubernetesTaskManagerParameters(WorkerResourceSpec workerResourceSpec) {\n+\t\t// TODO: need to unset process/flink memory size from configuration if dynamic worker resource is activated", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d7a0626255f2445cfe20dcb7bab1aede3af4b0d7"}, "originalPosition": 116}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzA4ODQ3Mw==", "bodyText": "Btw: where do we change the Configuration?", "url": "https://github.com/apache/flink/pull/11353#discussion_r403088473", "createdAt": "2020-04-03T15:30:58Z", "author": {"login": "tillrohrmann"}, "path": "flink-kubernetes/src/main/java/org/apache/flink/kubernetes/KubernetesResourceManager.java", "diffHunk": "@@ -232,57 +229,75 @@ private void recoverWorkerNodesFromPreviousAttempts() throws ResourceManagerExce\n \t\t\t++currentMaxAttemptId);\n \t}\n \n-\tprivate void requestKubernetesPod() {\n-\t\tnumPendingPodRequests++;\n+\tprivate void requestKubernetesPod(WorkerResourceSpec workerResourceSpec) {\n+\t\tfinal KubernetesTaskManagerParameters parameters =\n+\t\t\tcreateKubernetesTaskManagerParameters(workerResourceSpec);\n+\n+\t\tpodWorkerResources.put(parameters.getPodName(), workerResourceSpec);\n+\t\tfinal int pendingWorkerNum = pendingWorkerCounter.increaseAndGet(workerResourceSpec);\n \n \t\tlog.info(\"Requesting new TaskManager pod with <{},{}>. Number pending requests {}.\",\n-\t\t\tdefaultMemoryMB,\n-\t\t\tdefaultCpus,\n-\t\t\tnumPendingPodRequests);\n+\t\t\tparameters.getTaskManagerMemoryMB(),\n+\t\t\tparameters.getTaskManagerCPU(),\n+\t\t\tpendingWorkerNum);\n+\t\tlog.info(\"TaskManager {} will be started with {}.\", parameters.getPodName(), workerResourceSpec);\n+\n+\t\tfinal KubernetesPod taskManagerPod =\n+\t\t\tKubernetesTaskManagerFactory.createTaskManagerComponent(parameters);\n+\t\tkubeClient.createTaskManagerPod(taskManagerPod);\n+\t}\n+\n+\tprivate KubernetesTaskManagerParameters createKubernetesTaskManagerParameters(WorkerResourceSpec workerResourceSpec) {\n+\t\t// TODO: need to unset process/flink memory size from configuration if dynamic worker resource is activated", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzA4ODEzMw=="}, "originalCommit": {"oid": "d7a0626255f2445cfe20dcb7bab1aede3af4b0d7"}, "originalPosition": 116}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzgzMTM1NA==", "bodyText": "The purpose of this TODO is to avoid conflict between workerResourceSpec and process.size/flink.size in flinkConfig. It is not about forbidden workers with different sizes. Actually, one of the main purpose of this PR is to make the RMs not assuming workers have the same size.\nI was thinking about change the Configuration at creating the SlotManager, where we known which implementation of SlotManager is used and unset flink/process size if the plugin supports dynamic worker sizes.", "url": "https://github.com/apache/flink/pull/11353#discussion_r403831354", "createdAt": "2020-04-06T05:04:18Z", "author": {"login": "xintongsong"}, "path": "flink-kubernetes/src/main/java/org/apache/flink/kubernetes/KubernetesResourceManager.java", "diffHunk": "@@ -232,57 +229,75 @@ private void recoverWorkerNodesFromPreviousAttempts() throws ResourceManagerExce\n \t\t\t++currentMaxAttemptId);\n \t}\n \n-\tprivate void requestKubernetesPod() {\n-\t\tnumPendingPodRequests++;\n+\tprivate void requestKubernetesPod(WorkerResourceSpec workerResourceSpec) {\n+\t\tfinal KubernetesTaskManagerParameters parameters =\n+\t\t\tcreateKubernetesTaskManagerParameters(workerResourceSpec);\n+\n+\t\tpodWorkerResources.put(parameters.getPodName(), workerResourceSpec);\n+\t\tfinal int pendingWorkerNum = pendingWorkerCounter.increaseAndGet(workerResourceSpec);\n \n \t\tlog.info(\"Requesting new TaskManager pod with <{},{}>. Number pending requests {}.\",\n-\t\t\tdefaultMemoryMB,\n-\t\t\tdefaultCpus,\n-\t\t\tnumPendingPodRequests);\n+\t\t\tparameters.getTaskManagerMemoryMB(),\n+\t\t\tparameters.getTaskManagerCPU(),\n+\t\t\tpendingWorkerNum);\n+\t\tlog.info(\"TaskManager {} will be started with {}.\", parameters.getPodName(), workerResourceSpec);\n+\n+\t\tfinal KubernetesPod taskManagerPod =\n+\t\t\tKubernetesTaskManagerFactory.createTaskManagerComponent(parameters);\n+\t\tkubeClient.createTaskManagerPod(taskManagerPod);\n+\t}\n+\n+\tprivate KubernetesTaskManagerParameters createKubernetesTaskManagerParameters(WorkerResourceSpec workerResourceSpec) {\n+\t\t// TODO: need to unset process/flink memory size from configuration if dynamic worker resource is activated", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzA4ODEzMw=="}, "originalCommit": {"oid": "d7a0626255f2445cfe20dcb7bab1aede3af4b0d7"}, "originalPosition": 116}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDEyMTIzNA==", "bodyText": "So do we have the requirement that what's written in the config and what's specified in workerResourceSpec have to be the same? If yes, then I think we should add a check state here. This will ensure that we remember what needs to be adjusted in order to support dynamic worker resources.", "url": "https://github.com/apache/flink/pull/11353#discussion_r404121234", "createdAt": "2020-04-06T14:10:32Z", "author": {"login": "tillrohrmann"}, "path": "flink-kubernetes/src/main/java/org/apache/flink/kubernetes/KubernetesResourceManager.java", "diffHunk": "@@ -232,57 +229,75 @@ private void recoverWorkerNodesFromPreviousAttempts() throws ResourceManagerExce\n \t\t\t++currentMaxAttemptId);\n \t}\n \n-\tprivate void requestKubernetesPod() {\n-\t\tnumPendingPodRequests++;\n+\tprivate void requestKubernetesPod(WorkerResourceSpec workerResourceSpec) {\n+\t\tfinal KubernetesTaskManagerParameters parameters =\n+\t\t\tcreateKubernetesTaskManagerParameters(workerResourceSpec);\n+\n+\t\tpodWorkerResources.put(parameters.getPodName(), workerResourceSpec);\n+\t\tfinal int pendingWorkerNum = pendingWorkerCounter.increaseAndGet(workerResourceSpec);\n \n \t\tlog.info(\"Requesting new TaskManager pod with <{},{}>. Number pending requests {}.\",\n-\t\t\tdefaultMemoryMB,\n-\t\t\tdefaultCpus,\n-\t\t\tnumPendingPodRequests);\n+\t\t\tparameters.getTaskManagerMemoryMB(),\n+\t\t\tparameters.getTaskManagerCPU(),\n+\t\t\tpendingWorkerNum);\n+\t\tlog.info(\"TaskManager {} will be started with {}.\", parameters.getPodName(), workerResourceSpec);\n+\n+\t\tfinal KubernetesPod taskManagerPod =\n+\t\t\tKubernetesTaskManagerFactory.createTaskManagerComponent(parameters);\n+\t\tkubeClient.createTaskManagerPod(taskManagerPod);\n+\t}\n+\n+\tprivate KubernetesTaskManagerParameters createKubernetesTaskManagerParameters(WorkerResourceSpec workerResourceSpec) {\n+\t\t// TODO: need to unset process/flink memory size from configuration if dynamic worker resource is activated", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzA4ODEzMw=="}, "originalCommit": {"oid": "d7a0626255f2445cfe20dcb7bab1aede3af4b0d7"}, "originalPosition": 116}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQ3OTc3OA==", "bodyText": "I don't think we have that requirement.\nThe purpose of unsetting flink/process size from configuration, is to make sure the values in workerResourceSpec (task heap, task off-heap, network, managed) and those not in workerResorceSpec(framework heap, framework off-heap, jvm metaspace, jvm overhead, total flink, total process) can put together w/o conflict.\nIf they cannot be put together, we don't really need a check state to remind us because the generating of taskExecutorProcessSpec will fail.", "url": "https://github.com/apache/flink/pull/11353#discussion_r404479778", "createdAt": "2020-04-07T01:13:33Z", "author": {"login": "xintongsong"}, "path": "flink-kubernetes/src/main/java/org/apache/flink/kubernetes/KubernetesResourceManager.java", "diffHunk": "@@ -232,57 +229,75 @@ private void recoverWorkerNodesFromPreviousAttempts() throws ResourceManagerExce\n \t\t\t++currentMaxAttemptId);\n \t}\n \n-\tprivate void requestKubernetesPod() {\n-\t\tnumPendingPodRequests++;\n+\tprivate void requestKubernetesPod(WorkerResourceSpec workerResourceSpec) {\n+\t\tfinal KubernetesTaskManagerParameters parameters =\n+\t\t\tcreateKubernetesTaskManagerParameters(workerResourceSpec);\n+\n+\t\tpodWorkerResources.put(parameters.getPodName(), workerResourceSpec);\n+\t\tfinal int pendingWorkerNum = pendingWorkerCounter.increaseAndGet(workerResourceSpec);\n \n \t\tlog.info(\"Requesting new TaskManager pod with <{},{}>. Number pending requests {}.\",\n-\t\t\tdefaultMemoryMB,\n-\t\t\tdefaultCpus,\n-\t\t\tnumPendingPodRequests);\n+\t\t\tparameters.getTaskManagerMemoryMB(),\n+\t\t\tparameters.getTaskManagerCPU(),\n+\t\t\tpendingWorkerNum);\n+\t\tlog.info(\"TaskManager {} will be started with {}.\", parameters.getPodName(), workerResourceSpec);\n+\n+\t\tfinal KubernetesPod taskManagerPod =\n+\t\t\tKubernetesTaskManagerFactory.createTaskManagerComponent(parameters);\n+\t\tkubeClient.createTaskManagerPod(taskManagerPod);\n+\t}\n+\n+\tprivate KubernetesTaskManagerParameters createKubernetesTaskManagerParameters(WorkerResourceSpec workerResourceSpec) {\n+\t\t// TODO: need to unset process/flink memory size from configuration if dynamic worker resource is activated", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzA4ODEzMw=="}, "originalCommit": {"oid": "d7a0626255f2445cfe20dcb7bab1aede3af4b0d7"}, "originalPosition": 116}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDYzOTY5MQ==", "bodyText": "Ok, instead of creating a TODO, I would suggest to create a JIRA ticket which is linked as a follow up. TODO's tend to be forgotten.", "url": "https://github.com/apache/flink/pull/11353#discussion_r404639691", "createdAt": "2020-04-07T08:44:07Z", "author": {"login": "tillrohrmann"}, "path": "flink-kubernetes/src/main/java/org/apache/flink/kubernetes/KubernetesResourceManager.java", "diffHunk": "@@ -232,57 +229,75 @@ private void recoverWorkerNodesFromPreviousAttempts() throws ResourceManagerExce\n \t\t\t++currentMaxAttemptId);\n \t}\n \n-\tprivate void requestKubernetesPod() {\n-\t\tnumPendingPodRequests++;\n+\tprivate void requestKubernetesPod(WorkerResourceSpec workerResourceSpec) {\n+\t\tfinal KubernetesTaskManagerParameters parameters =\n+\t\t\tcreateKubernetesTaskManagerParameters(workerResourceSpec);\n+\n+\t\tpodWorkerResources.put(parameters.getPodName(), workerResourceSpec);\n+\t\tfinal int pendingWorkerNum = pendingWorkerCounter.increaseAndGet(workerResourceSpec);\n \n \t\tlog.info(\"Requesting new TaskManager pod with <{},{}>. Number pending requests {}.\",\n-\t\t\tdefaultMemoryMB,\n-\t\t\tdefaultCpus,\n-\t\t\tnumPendingPodRequests);\n+\t\t\tparameters.getTaskManagerMemoryMB(),\n+\t\t\tparameters.getTaskManagerCPU(),\n+\t\t\tpendingWorkerNum);\n+\t\tlog.info(\"TaskManager {} will be started with {}.\", parameters.getPodName(), workerResourceSpec);\n+\n+\t\tfinal KubernetesPod taskManagerPod =\n+\t\t\tKubernetesTaskManagerFactory.createTaskManagerComponent(parameters);\n+\t\tkubeClient.createTaskManagerPod(taskManagerPod);\n+\t}\n+\n+\tprivate KubernetesTaskManagerParameters createKubernetesTaskManagerParameters(WorkerResourceSpec workerResourceSpec) {\n+\t\t// TODO: need to unset process/flink memory size from configuration if dynamic worker resource is activated", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzA4ODEzMw=="}, "originalCommit": {"oid": "d7a0626255f2445cfe20dcb7bab1aede3af4b0d7"}, "originalPosition": 116}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTkyMzYwNg==", "bodyText": "I've created FLINK-17061 to track this issue.", "url": "https://github.com/apache/flink/pull/11353#discussion_r405923606", "createdAt": "2020-04-09T02:38:14Z", "author": {"login": "xintongsong"}, "path": "flink-kubernetes/src/main/java/org/apache/flink/kubernetes/KubernetesResourceManager.java", "diffHunk": "@@ -232,57 +229,75 @@ private void recoverWorkerNodesFromPreviousAttempts() throws ResourceManagerExce\n \t\t\t++currentMaxAttemptId);\n \t}\n \n-\tprivate void requestKubernetesPod() {\n-\t\tnumPendingPodRequests++;\n+\tprivate void requestKubernetesPod(WorkerResourceSpec workerResourceSpec) {\n+\t\tfinal KubernetesTaskManagerParameters parameters =\n+\t\t\tcreateKubernetesTaskManagerParameters(workerResourceSpec);\n+\n+\t\tpodWorkerResources.put(parameters.getPodName(), workerResourceSpec);\n+\t\tfinal int pendingWorkerNum = pendingWorkerCounter.increaseAndGet(workerResourceSpec);\n \n \t\tlog.info(\"Requesting new TaskManager pod with <{},{}>. Number pending requests {}.\",\n-\t\t\tdefaultMemoryMB,\n-\t\t\tdefaultCpus,\n-\t\t\tnumPendingPodRequests);\n+\t\t\tparameters.getTaskManagerMemoryMB(),\n+\t\t\tparameters.getTaskManagerCPU(),\n+\t\t\tpendingWorkerNum);\n+\t\tlog.info(\"TaskManager {} will be started with {}.\", parameters.getPodName(), workerResourceSpec);\n+\n+\t\tfinal KubernetesPod taskManagerPod =\n+\t\t\tKubernetesTaskManagerFactory.createTaskManagerComponent(parameters);\n+\t\tkubeClient.createTaskManagerPod(taskManagerPod);\n+\t}\n+\n+\tprivate KubernetesTaskManagerParameters createKubernetesTaskManagerParameters(WorkerResourceSpec workerResourceSpec) {\n+\t\t// TODO: need to unset process/flink memory size from configuration if dynamic worker resource is activated", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzA4ODEzMw=="}, "originalCommit": {"oid": "d7a0626255f2445cfe20dcb7bab1aede3af4b0d7"}, "originalPosition": 116}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUwMDA5NjkzOnYy", "diffSide": "RIGHT", "path": "flink-kubernetes/src/main/java/org/apache/flink/kubernetes/KubernetesResourceManager.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNTozMjo0N1rOGAapBQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNTozMjo0N1rOGAapBQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzA4OTY2OQ==", "bodyText": "I think I would hide pendingWorkerCounter behind some methods which the base class provides.", "url": "https://github.com/apache/flink/pull/11353#discussion_r403089669", "createdAt": "2020-04-03T15:32:47Z", "author": {"login": "tillrohrmann"}, "path": "flink-kubernetes/src/main/java/org/apache/flink/kubernetes/KubernetesResourceManager.java", "diffHunk": "@@ -232,57 +229,75 @@ private void recoverWorkerNodesFromPreviousAttempts() throws ResourceManagerExce\n \t\t\t++currentMaxAttemptId);\n \t}\n \n-\tprivate void requestKubernetesPod() {\n-\t\tnumPendingPodRequests++;\n+\tprivate void requestKubernetesPod(WorkerResourceSpec workerResourceSpec) {\n+\t\tfinal KubernetesTaskManagerParameters parameters =\n+\t\t\tcreateKubernetesTaskManagerParameters(workerResourceSpec);\n+\n+\t\tpodWorkerResources.put(parameters.getPodName(), workerResourceSpec);\n+\t\tfinal int pendingWorkerNum = pendingWorkerCounter.increaseAndGet(workerResourceSpec);\n \n \t\tlog.info(\"Requesting new TaskManager pod with <{},{}>. Number pending requests {}.\",\n-\t\t\tdefaultMemoryMB,\n-\t\t\tdefaultCpus,\n-\t\t\tnumPendingPodRequests);\n+\t\t\tparameters.getTaskManagerMemoryMB(),\n+\t\t\tparameters.getTaskManagerCPU(),\n+\t\t\tpendingWorkerNum);\n+\t\tlog.info(\"TaskManager {} will be started with {}.\", parameters.getPodName(), workerResourceSpec);\n+\n+\t\tfinal KubernetesPod taskManagerPod =\n+\t\t\tKubernetesTaskManagerFactory.createTaskManagerComponent(parameters);\n+\t\tkubeClient.createTaskManagerPod(taskManagerPod);\n+\t}\n+\n+\tprivate KubernetesTaskManagerParameters createKubernetesTaskManagerParameters(WorkerResourceSpec workerResourceSpec) {\n+\t\t// TODO: need to unset process/flink memory size from configuration if dynamic worker resource is activated\n+\t\tfinal TaskExecutorProcessSpec taskExecutorProcessSpec =\n+\t\t\tTaskExecutorProcessUtils.processSpecFromWorkerResourceSpec(flinkConfig, workerResourceSpec);\n \n \t\tfinal String podName = String.format(\n \t\t\tTASK_MANAGER_POD_FORMAT,\n \t\t\tclusterId,\n \t\t\tcurrentMaxAttemptId,\n \t\t\t++currentMaxPodId);\n \n+\t\tfinal ContaineredTaskManagerParameters taskManagerParameters =\n+\t\t\tContaineredTaskManagerParameters.create(flinkConfig, taskExecutorProcessSpec);\n+\n \t\tfinal String dynamicProperties =\n \t\t\tBootstrapTools.getDynamicPropertiesAsString(flinkClientConfig, flinkConfig);\n \n-\t\tfinal KubernetesTaskManagerParameters kubernetesTaskManagerParameters = new KubernetesTaskManagerParameters(\n+\t\treturn new KubernetesTaskManagerParameters(\n \t\t\tflinkConfig,\n \t\t\tpodName,\n \t\t\tdynamicProperties,\n \t\t\ttaskManagerParameters);\n-\n-\t\tfinal KubernetesPod taskManagerPod =\n-\t\t\tKubernetesTaskManagerFactory.createTaskManagerComponent(kubernetesTaskManagerParameters);\n-\n-\t\tlog.info(\"TaskManager {} will be started with {}.\", podName, taskExecutorProcessSpec);\n-\t\tkubeClient.createTaskManagerPod(taskManagerPod);\n \t}\n \n \t/**\n \t * Request new pod if pending pods cannot satisfy pending slot requests.\n \t */\n-\tprivate void requestKubernetesPodIfRequired() {\n-\t\tfinal int requiredTaskManagers = getNumberRequiredTaskManagers();\n+\tprivate void requestKubernetesPodIfRequired(WorkerResourceSpec workerResourceSpec) {\n+\t\tfinal int requiredTaskManagers = getPendingWorkerNums().get(workerResourceSpec);\n+\t\tfinal int pendingWorkerNum = pendingWorkerCounter.getNum(workerResourceSpec);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d7a0626255f2445cfe20dcb7bab1aede3af4b0d7"}, "originalPosition": 153}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUwMDEwNTQyOnYy", "diffSide": "RIGHT", "path": "flink-kubernetes/src/main/java/org/apache/flink/kubernetes/KubernetesResourceManager.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNTozNDozNlrOGAauLA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNTozNDozNlrOGAauLA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzA5MDk4OA==", "bodyText": "Nit: Changing the return type to Optional<WorkerResourceSpec> could make the contract of this method a bit more explicit.", "url": "https://github.com/apache/flink/pull/11353#discussion_r403090988", "createdAt": "2020-04-03T15:34:36Z", "author": {"login": "tillrohrmann"}, "path": "flink-kubernetes/src/main/java/org/apache/flink/kubernetes/KubernetesResourceManager.java", "diffHunk": "@@ -232,57 +229,75 @@ private void recoverWorkerNodesFromPreviousAttempts() throws ResourceManagerExce\n \t\t\t++currentMaxAttemptId);\n \t}\n \n-\tprivate void requestKubernetesPod() {\n-\t\tnumPendingPodRequests++;\n+\tprivate void requestKubernetesPod(WorkerResourceSpec workerResourceSpec) {\n+\t\tfinal KubernetesTaskManagerParameters parameters =\n+\t\t\tcreateKubernetesTaskManagerParameters(workerResourceSpec);\n+\n+\t\tpodWorkerResources.put(parameters.getPodName(), workerResourceSpec);\n+\t\tfinal int pendingWorkerNum = pendingWorkerCounter.increaseAndGet(workerResourceSpec);\n \n \t\tlog.info(\"Requesting new TaskManager pod with <{},{}>. Number pending requests {}.\",\n-\t\t\tdefaultMemoryMB,\n-\t\t\tdefaultCpus,\n-\t\t\tnumPendingPodRequests);\n+\t\t\tparameters.getTaskManagerMemoryMB(),\n+\t\t\tparameters.getTaskManagerCPU(),\n+\t\t\tpendingWorkerNum);\n+\t\tlog.info(\"TaskManager {} will be started with {}.\", parameters.getPodName(), workerResourceSpec);\n+\n+\t\tfinal KubernetesPod taskManagerPod =\n+\t\t\tKubernetesTaskManagerFactory.createTaskManagerComponent(parameters);\n+\t\tkubeClient.createTaskManagerPod(taskManagerPod);\n+\t}\n+\n+\tprivate KubernetesTaskManagerParameters createKubernetesTaskManagerParameters(WorkerResourceSpec workerResourceSpec) {\n+\t\t// TODO: need to unset process/flink memory size from configuration if dynamic worker resource is activated\n+\t\tfinal TaskExecutorProcessSpec taskExecutorProcessSpec =\n+\t\t\tTaskExecutorProcessUtils.processSpecFromWorkerResourceSpec(flinkConfig, workerResourceSpec);\n \n \t\tfinal String podName = String.format(\n \t\t\tTASK_MANAGER_POD_FORMAT,\n \t\t\tclusterId,\n \t\t\tcurrentMaxAttemptId,\n \t\t\t++currentMaxPodId);\n \n+\t\tfinal ContaineredTaskManagerParameters taskManagerParameters =\n+\t\t\tContaineredTaskManagerParameters.create(flinkConfig, taskExecutorProcessSpec);\n+\n \t\tfinal String dynamicProperties =\n \t\t\tBootstrapTools.getDynamicPropertiesAsString(flinkClientConfig, flinkConfig);\n \n-\t\tfinal KubernetesTaskManagerParameters kubernetesTaskManagerParameters = new KubernetesTaskManagerParameters(\n+\t\treturn new KubernetesTaskManagerParameters(\n \t\t\tflinkConfig,\n \t\t\tpodName,\n \t\t\tdynamicProperties,\n \t\t\ttaskManagerParameters);\n-\n-\t\tfinal KubernetesPod taskManagerPod =\n-\t\t\tKubernetesTaskManagerFactory.createTaskManagerComponent(kubernetesTaskManagerParameters);\n-\n-\t\tlog.info(\"TaskManager {} will be started with {}.\", podName, taskExecutorProcessSpec);\n-\t\tkubeClient.createTaskManagerPod(taskManagerPod);\n \t}\n \n \t/**\n \t * Request new pod if pending pods cannot satisfy pending slot requests.\n \t */\n-\tprivate void requestKubernetesPodIfRequired() {\n-\t\tfinal int requiredTaskManagers = getNumberRequiredTaskManagers();\n+\tprivate void requestKubernetesPodIfRequired(WorkerResourceSpec workerResourceSpec) {\n+\t\tfinal int requiredTaskManagers = getPendingWorkerNums().get(workerResourceSpec);\n+\t\tfinal int pendingWorkerNum = pendingWorkerCounter.getNum(workerResourceSpec);\n \n-\t\tif (requiredTaskManagers > numPendingPodRequests) {\n-\t\t\trequestKubernetesPod();\n+\t\tif (requiredTaskManagers > pendingWorkerNum) {\n+\t\t\trequestKubernetesPod(workerResourceSpec);\n \t\t}\n \t}\n \n \tprivate void removePodIfTerminated(KubernetesPod pod) {\n \t\tif (pod.isTerminated()) {\n \t\t\tkubeClient.stopPod(pod.getName());\n-\t\t\tfinal KubernetesWorkerNode kubernetesWorkerNode = workerNodes.remove(new ResourceID(pod.getName()));\n-\t\t\tif (kubernetesWorkerNode != null) {\n-\t\t\t\trequestKubernetesPodIfRequired();\n+\t\t\tfinal WorkerResourceSpec workerResourceSpec = removeWorkerNodeAndResourceSpec(new ResourceID(pod.getName()));\n+\t\t\tif (workerResourceSpec != null) {\n+\t\t\t\trequestKubernetesPodIfRequired(workerResourceSpec);\n \t\t\t}\n \t\t}\n \t}\n \n+\tprivate WorkerResourceSpec removeWorkerNodeAndResourceSpec(ResourceID resourceId) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d7a0626255f2445cfe20dcb7bab1aede3af4b0d7"}, "originalPosition": 175}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUwMDExNTgyOnYy", "diffSide": "RIGHT", "path": "flink-kubernetes/src/test/java/org/apache/flink/kubernetes/KubernetesResourceManagerTest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNTozNzoyMFrOGAa1JQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNTozNzoyMFrOGAa1JQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzA5Mjc3Mw==", "bodyText": "I think this is pretty much whitebox testing as it strongly relies on internal implementation details. I would recommend to go another way and to rely either on the public APIs of the component or to encapsulate the bookkeeping logic so that it can be tested separately.", "url": "https://github.com/apache/flink/pull/11353#discussion_r403092773", "createdAt": "2020-04-03T15:37:20Z", "author": {"login": "tillrohrmann"}, "path": "flink-kubernetes/src/test/java/org/apache/flink/kubernetes/KubernetesResourceManagerTest.java", "diffHunk": "@@ -176,6 +185,15 @@ MainThreadExecutor getMainThreadExecutorForTesting() {\n \t\tSlotManager getSlotManager() {\n \t\t\treturn this.slotManager;\n \t\t}\n+\n+\t\t@Override\n+\t\tpublic Map<WorkerResourceSpec, Integer> getPendingWorkerNums() {\n+\t\t\treturn customPendingWorkerNums != null ? customPendingWorkerNums : super.getPendingWorkerNums();\n+\t\t}\n+\n+\t\tpublic void setCustomPendingWorkerNums(final Map<WorkerResourceSpec, Integer> customPendingWorkerNums) {\n+\t\t\tthis.customPendingWorkerNums = customPendingWorkerNums;\n+\t\t}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d7a0626255f2445cfe20dcb7bab1aede3af4b0d7"}, "originalPosition": 58}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUwMDEyNDA1OnYy", "diffSide": "RIGHT", "path": "flink-kubernetes/src/test/java/org/apache/flink/kubernetes/KubernetesResourceManagerTest.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNTozOTowOFrOGAa57Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wNlQxNDoxMzoyMVrOGBZvNQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzA5Mzk5Nw==", "bodyText": "I would recommend to not test the component like this. It requires detailed knowledge of the component's internals and makes it harder to evolve it because this test relies on the fact that the KubernetesResourceManager has a map of WorkerResourceSpec to Integers.", "url": "https://github.com/apache/flink/pull/11353#discussion_r403093997", "createdAt": "2020-04-03T15:39:08Z", "author": {"login": "tillrohrmann"}, "path": "flink-kubernetes/src/test/java/org/apache/flink/kubernetes/KubernetesResourceManagerTest.java", "diffHunk": "@@ -321,6 +339,47 @@ public void testGetCpuCoresNumSlots() {\n \t\tassertThat(resourceManager.getCpuCores(configuration), is(3.0));\n \t}\n \n+\t@Test\n+\tpublic void testStartAndRecoverVariousResourceSpec() {\n+\t\t// Start two workers with different resources\n+\t\tfinal WorkerResourceSpec workerResourceSpec1 = new WorkerResourceSpec(1.0, 100, 0, 100, 100);\n+\t\tfinal WorkerResourceSpec workerResourceSpec2 = new WorkerResourceSpec(1.0, 99, 0, 100, 100);\n+\t\tresourceManager.startNewWorker(workerResourceSpec1);\n+\t\tresourceManager.startNewWorker(workerResourceSpec2);\n+\n+\t\t// Verify two pods with both worker resources are started\n+\t\tfinal PodList initialPodList = kubeClient.pods().list();\n+\t\tassertEquals(2, initialPodList.getItems().size());\n+\t\tfinal Pod initialPod1 = getPodContainsStrInArgs(initialPodList, TaskManagerOptions.TASK_HEAP_MEMORY.key() + \"=\" + (100L << 20));\n+\t\tfinal Pod initialPod2 = getPodContainsStrInArgs(initialPodList, TaskManagerOptions.TASK_HEAP_MEMORY.key() + \"=\" + (99L << 20));\n+\n+\t\t// Notify resource manager about pods added.\n+\t\tfinal KubernetesPod initialKubernetesPod1 = new KubernetesPod(initialPod1);\n+\t\tfinal KubernetesPod initialKubernetesPod2 = new KubernetesPod(initialPod2);\n+\t\tresourceManager.onAdded(ImmutableList.of(initialKubernetesPod1, initialKubernetesPod2));\n+\n+\t\t// Terminate pod1.\n+\t\tterminatePod(initialPod1);\n+\t\tresourceManager.setCustomPendingWorkerNums(Collections.singletonMap(workerResourceSpec1, 1));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d7a0626255f2445cfe20dcb7bab1aede3af4b0d7"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzgzNDYwNA==", "bodyText": "OK, I'll try to pass in a testing SlotManager and control what to be returned from getPendingWorkerNums. How does that sound like?", "url": "https://github.com/apache/flink/pull/11353#discussion_r403834604", "createdAt": "2020-04-06T05:17:46Z", "author": {"login": "xintongsong"}, "path": "flink-kubernetes/src/test/java/org/apache/flink/kubernetes/KubernetesResourceManagerTest.java", "diffHunk": "@@ -321,6 +339,47 @@ public void testGetCpuCoresNumSlots() {\n \t\tassertThat(resourceManager.getCpuCores(configuration), is(3.0));\n \t}\n \n+\t@Test\n+\tpublic void testStartAndRecoverVariousResourceSpec() {\n+\t\t// Start two workers with different resources\n+\t\tfinal WorkerResourceSpec workerResourceSpec1 = new WorkerResourceSpec(1.0, 100, 0, 100, 100);\n+\t\tfinal WorkerResourceSpec workerResourceSpec2 = new WorkerResourceSpec(1.0, 99, 0, 100, 100);\n+\t\tresourceManager.startNewWorker(workerResourceSpec1);\n+\t\tresourceManager.startNewWorker(workerResourceSpec2);\n+\n+\t\t// Verify two pods with both worker resources are started\n+\t\tfinal PodList initialPodList = kubeClient.pods().list();\n+\t\tassertEquals(2, initialPodList.getItems().size());\n+\t\tfinal Pod initialPod1 = getPodContainsStrInArgs(initialPodList, TaskManagerOptions.TASK_HEAP_MEMORY.key() + \"=\" + (100L << 20));\n+\t\tfinal Pod initialPod2 = getPodContainsStrInArgs(initialPodList, TaskManagerOptions.TASK_HEAP_MEMORY.key() + \"=\" + (99L << 20));\n+\n+\t\t// Notify resource manager about pods added.\n+\t\tfinal KubernetesPod initialKubernetesPod1 = new KubernetesPod(initialPod1);\n+\t\tfinal KubernetesPod initialKubernetesPod2 = new KubernetesPod(initialPod2);\n+\t\tresourceManager.onAdded(ImmutableList.of(initialKubernetesPod1, initialKubernetesPod2));\n+\n+\t\t// Terminate pod1.\n+\t\tterminatePod(initialPod1);\n+\t\tresourceManager.setCustomPendingWorkerNums(Collections.singletonMap(workerResourceSpec1, 1));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzA5Mzk5Nw=="}, "originalCommit": {"oid": "d7a0626255f2445cfe20dcb7bab1aede3af4b0d7"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDEyMzQ0NQ==", "bodyText": "This sounds good.", "url": "https://github.com/apache/flink/pull/11353#discussion_r404123445", "createdAt": "2020-04-06T14:13:21Z", "author": {"login": "tillrohrmann"}, "path": "flink-kubernetes/src/test/java/org/apache/flink/kubernetes/KubernetesResourceManagerTest.java", "diffHunk": "@@ -321,6 +339,47 @@ public void testGetCpuCoresNumSlots() {\n \t\tassertThat(resourceManager.getCpuCores(configuration), is(3.0));\n \t}\n \n+\t@Test\n+\tpublic void testStartAndRecoverVariousResourceSpec() {\n+\t\t// Start two workers with different resources\n+\t\tfinal WorkerResourceSpec workerResourceSpec1 = new WorkerResourceSpec(1.0, 100, 0, 100, 100);\n+\t\tfinal WorkerResourceSpec workerResourceSpec2 = new WorkerResourceSpec(1.0, 99, 0, 100, 100);\n+\t\tresourceManager.startNewWorker(workerResourceSpec1);\n+\t\tresourceManager.startNewWorker(workerResourceSpec2);\n+\n+\t\t// Verify two pods with both worker resources are started\n+\t\tfinal PodList initialPodList = kubeClient.pods().list();\n+\t\tassertEquals(2, initialPodList.getItems().size());\n+\t\tfinal Pod initialPod1 = getPodContainsStrInArgs(initialPodList, TaskManagerOptions.TASK_HEAP_MEMORY.key() + \"=\" + (100L << 20));\n+\t\tfinal Pod initialPod2 = getPodContainsStrInArgs(initialPodList, TaskManagerOptions.TASK_HEAP_MEMORY.key() + \"=\" + (99L << 20));\n+\n+\t\t// Notify resource manager about pods added.\n+\t\tfinal KubernetesPod initialKubernetesPod1 = new KubernetesPod(initialPod1);\n+\t\tfinal KubernetesPod initialKubernetesPod2 = new KubernetesPod(initialPod2);\n+\t\tresourceManager.onAdded(ImmutableList.of(initialKubernetesPod1, initialKubernetesPod2));\n+\n+\t\t// Terminate pod1.\n+\t\tterminatePod(initialPod1);\n+\t\tresourceManager.setCustomPendingWorkerNums(Collections.singletonMap(workerResourceSpec1, 1));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzA5Mzk5Nw=="}, "originalCommit": {"oid": "d7a0626255f2445cfe20dcb7bab1aede3af4b0d7"}, "originalPosition": 87}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUwMDE0NDcxOnYy", "diffSide": "RIGHT", "path": "flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNTo0NDoyMFrOGAbGuw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNTo0NDoyMFrOGAbGuw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzA5NzI3NQ==", "bodyText": "I think this class is large enough to warrant its own file. This would also decrease the size of this source code file a bit.", "url": "https://github.com/apache/flink/pull/11353#discussion_r403097275", "createdAt": "2020-04-03T15:44:20Z", "author": {"login": "tillrohrmann"}, "path": "flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java", "diffHunk": "@@ -615,4 +632,85 @@ protected double getCpuCores(final Configuration configuration) {\n \t\t//noinspection NumericCastThatLosesPrecision\n \t\treturn cpuCoresLong;\n \t}\n+\n+\t/**\n+\t * Utility class for converting between Flink {@link WorkerResourceSpec} and Yarn {@link Resource}.\n+\t */\n+\t@VisibleForTesting\n+\tstatic class WorkerSpecContainerResourceAdapter {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef959ec03f964ceb0664388b4b72c8eac5773fbd"}, "originalPosition": 68}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUwMDE1ODc2OnYy", "diffSide": "RIGHT", "path": "flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java", "isResolved": true, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNTo0ODowMFrOGAbPxw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QwODo0NjoxN1rOGB5VQA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzA5OTU5MQ==", "bodyText": "I would suggest to add a check state to ensure that we fail once we enable dynamic worker resources.", "url": "https://github.com/apache/flink/pull/11353#discussion_r403099591", "createdAt": "2020-04-03T15:48:00Z", "author": {"login": "tillrohrmann"}, "path": "flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java", "diffHunk": "@@ -615,4 +632,85 @@ protected double getCpuCores(final Configuration configuration) {\n \t\t//noinspection NumericCastThatLosesPrecision\n \t\treturn cpuCoresLong;\n \t}\n+\n+\t/**\n+\t * Utility class for converting between Flink {@link WorkerResourceSpec} and Yarn {@link Resource}.\n+\t */\n+\t@VisibleForTesting\n+\tstatic class WorkerSpecContainerResourceAdapter {\n+\t\tprivate final Configuration flinkConfig;\n+\t\tprivate final int minMemMB;\n+\t\tprivate final int minVcore;\n+\t\tprivate final boolean matchVcores;\n+\t\tprivate final Map<WorkerResourceSpec, Resource> workerSpecToContainerResource;\n+\t\tprivate final Map<Resource, Collection<WorkerResourceSpec>> containerResourceToWorkerSpecs;\n+\t\tprivate final Map<Integer, Collection<Resource>> containerMemoryToContainerResource;\n+\n+\t\t@VisibleForTesting\n+\t\tWorkerSpecContainerResourceAdapter(\n+\t\t\t\tfinal Configuration flinkConfig,\n+\t\t\t\tfinal int minMemMB,\n+\t\t\t\tfinal int minVcore,\n+\t\t\t\tfinal boolean matchVcores) {\n+\t\t\tthis.flinkConfig = Preconditions.checkNotNull(flinkConfig);\n+\t\t\tthis.minMemMB = minMemMB;\n+\t\t\tthis.minVcore = minVcore;\n+\t\t\tthis.matchVcores = matchVcores;\n+\t\t\tworkerSpecToContainerResource = new HashMap<>();\n+\t\t\tcontainerResourceToWorkerSpecs = new HashMap<>();\n+\t\t\tcontainerMemoryToContainerResource = new HashMap<>();\n+\t\t}\n+\n+\t\t@VisibleForTesting\n+\t\tResource getContainerResource(final WorkerResourceSpec workerResourceSpec) {\n+\t\t\treturn workerSpecToContainerResource.computeIfAbsent(\n+\t\t\t\tPreconditions.checkNotNull(workerResourceSpec),\n+\t\t\t\tthis::createAndMapContainerResource);\n+\t\t}\n+\n+\t\t@VisibleForTesting\n+\t\tCollection<WorkerResourceSpec> getWorkerSpecs(final Resource containerResource) {\n+\t\t\treturn getEquivalentContainerResource(containerResource).stream()\n+\t\t\t\t.flatMap(resource -> containerResourceToWorkerSpecs.getOrDefault(resource, Collections.emptyList()).stream())\n+\t\t\t\t.collect(Collectors.toList());\n+\t\t}\n+\n+\t\t@VisibleForTesting\n+\t\tCollection<Resource> getEquivalentContainerResource(final Resource containerResource) {\n+\t\t\t// Yarn might ignore the requested vcores, depending on its configurations.\n+\t\t\t// In such cases, we should also not matching vcores.\n+\t\t\treturn matchVcores ?\n+\t\t\t\tCollections.singletonList(containerResource) :\n+\t\t\t\tcontainerMemoryToContainerResource.getOrDefault(containerResource.getMemory(), Collections.emptyList());\n+\t\t}\n+\n+\t\tprivate Resource createAndMapContainerResource(final WorkerResourceSpec workerResourceSpec) {\n+\t\t\t// TODO: need to unset process/flink memory size from configuration if dynamic worker resource is activated", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef959ec03f964ceb0664388b4b72c8eac5773fbd"}, "originalPosition": 116}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzgzNTUyNA==", "bodyText": "Same here. I think the RM implementations should not be aware of whether the dynamic worker resources is activated or not.", "url": "https://github.com/apache/flink/pull/11353#discussion_r403835524", "createdAt": "2020-04-06T05:21:19Z", "author": {"login": "xintongsong"}, "path": "flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java", "diffHunk": "@@ -615,4 +632,85 @@ protected double getCpuCores(final Configuration configuration) {\n \t\t//noinspection NumericCastThatLosesPrecision\n \t\treturn cpuCoresLong;\n \t}\n+\n+\t/**\n+\t * Utility class for converting between Flink {@link WorkerResourceSpec} and Yarn {@link Resource}.\n+\t */\n+\t@VisibleForTesting\n+\tstatic class WorkerSpecContainerResourceAdapter {\n+\t\tprivate final Configuration flinkConfig;\n+\t\tprivate final int minMemMB;\n+\t\tprivate final int minVcore;\n+\t\tprivate final boolean matchVcores;\n+\t\tprivate final Map<WorkerResourceSpec, Resource> workerSpecToContainerResource;\n+\t\tprivate final Map<Resource, Collection<WorkerResourceSpec>> containerResourceToWorkerSpecs;\n+\t\tprivate final Map<Integer, Collection<Resource>> containerMemoryToContainerResource;\n+\n+\t\t@VisibleForTesting\n+\t\tWorkerSpecContainerResourceAdapter(\n+\t\t\t\tfinal Configuration flinkConfig,\n+\t\t\t\tfinal int minMemMB,\n+\t\t\t\tfinal int minVcore,\n+\t\t\t\tfinal boolean matchVcores) {\n+\t\t\tthis.flinkConfig = Preconditions.checkNotNull(flinkConfig);\n+\t\t\tthis.minMemMB = minMemMB;\n+\t\t\tthis.minVcore = minVcore;\n+\t\t\tthis.matchVcores = matchVcores;\n+\t\t\tworkerSpecToContainerResource = new HashMap<>();\n+\t\t\tcontainerResourceToWorkerSpecs = new HashMap<>();\n+\t\t\tcontainerMemoryToContainerResource = new HashMap<>();\n+\t\t}\n+\n+\t\t@VisibleForTesting\n+\t\tResource getContainerResource(final WorkerResourceSpec workerResourceSpec) {\n+\t\t\treturn workerSpecToContainerResource.computeIfAbsent(\n+\t\t\t\tPreconditions.checkNotNull(workerResourceSpec),\n+\t\t\t\tthis::createAndMapContainerResource);\n+\t\t}\n+\n+\t\t@VisibleForTesting\n+\t\tCollection<WorkerResourceSpec> getWorkerSpecs(final Resource containerResource) {\n+\t\t\treturn getEquivalentContainerResource(containerResource).stream()\n+\t\t\t\t.flatMap(resource -> containerResourceToWorkerSpecs.getOrDefault(resource, Collections.emptyList()).stream())\n+\t\t\t\t.collect(Collectors.toList());\n+\t\t}\n+\n+\t\t@VisibleForTesting\n+\t\tCollection<Resource> getEquivalentContainerResource(final Resource containerResource) {\n+\t\t\t// Yarn might ignore the requested vcores, depending on its configurations.\n+\t\t\t// In such cases, we should also not matching vcores.\n+\t\t\treturn matchVcores ?\n+\t\t\t\tCollections.singletonList(containerResource) :\n+\t\t\t\tcontainerMemoryToContainerResource.getOrDefault(containerResource.getMemory(), Collections.emptyList());\n+\t\t}\n+\n+\t\tprivate Resource createAndMapContainerResource(final WorkerResourceSpec workerResourceSpec) {\n+\t\t\t// TODO: need to unset process/flink memory size from configuration if dynamic worker resource is activated", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzA5OTU5MQ=="}, "originalCommit": {"oid": "ef959ec03f964ceb0664388b4b72c8eac5773fbd"}, "originalPosition": 116}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDEyNDE3MA==", "bodyText": "In general I agree unless it is not yet fully supported by the RM implementation. If it is, then I guess we also don't need this comment. If it is not supported, then the check state will help us to remember what needs to be changed.", "url": "https://github.com/apache/flink/pull/11353#discussion_r404124170", "createdAt": "2020-04-06T14:14:18Z", "author": {"login": "tillrohrmann"}, "path": "flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java", "diffHunk": "@@ -615,4 +632,85 @@ protected double getCpuCores(final Configuration configuration) {\n \t\t//noinspection NumericCastThatLosesPrecision\n \t\treturn cpuCoresLong;\n \t}\n+\n+\t/**\n+\t * Utility class for converting between Flink {@link WorkerResourceSpec} and Yarn {@link Resource}.\n+\t */\n+\t@VisibleForTesting\n+\tstatic class WorkerSpecContainerResourceAdapter {\n+\t\tprivate final Configuration flinkConfig;\n+\t\tprivate final int minMemMB;\n+\t\tprivate final int minVcore;\n+\t\tprivate final boolean matchVcores;\n+\t\tprivate final Map<WorkerResourceSpec, Resource> workerSpecToContainerResource;\n+\t\tprivate final Map<Resource, Collection<WorkerResourceSpec>> containerResourceToWorkerSpecs;\n+\t\tprivate final Map<Integer, Collection<Resource>> containerMemoryToContainerResource;\n+\n+\t\t@VisibleForTesting\n+\t\tWorkerSpecContainerResourceAdapter(\n+\t\t\t\tfinal Configuration flinkConfig,\n+\t\t\t\tfinal int minMemMB,\n+\t\t\t\tfinal int minVcore,\n+\t\t\t\tfinal boolean matchVcores) {\n+\t\t\tthis.flinkConfig = Preconditions.checkNotNull(flinkConfig);\n+\t\t\tthis.minMemMB = minMemMB;\n+\t\t\tthis.minVcore = minVcore;\n+\t\t\tthis.matchVcores = matchVcores;\n+\t\t\tworkerSpecToContainerResource = new HashMap<>();\n+\t\t\tcontainerResourceToWorkerSpecs = new HashMap<>();\n+\t\t\tcontainerMemoryToContainerResource = new HashMap<>();\n+\t\t}\n+\n+\t\t@VisibleForTesting\n+\t\tResource getContainerResource(final WorkerResourceSpec workerResourceSpec) {\n+\t\t\treturn workerSpecToContainerResource.computeIfAbsent(\n+\t\t\t\tPreconditions.checkNotNull(workerResourceSpec),\n+\t\t\t\tthis::createAndMapContainerResource);\n+\t\t}\n+\n+\t\t@VisibleForTesting\n+\t\tCollection<WorkerResourceSpec> getWorkerSpecs(final Resource containerResource) {\n+\t\t\treturn getEquivalentContainerResource(containerResource).stream()\n+\t\t\t\t.flatMap(resource -> containerResourceToWorkerSpecs.getOrDefault(resource, Collections.emptyList()).stream())\n+\t\t\t\t.collect(Collectors.toList());\n+\t\t}\n+\n+\t\t@VisibleForTesting\n+\t\tCollection<Resource> getEquivalentContainerResource(final Resource containerResource) {\n+\t\t\t// Yarn might ignore the requested vcores, depending on its configurations.\n+\t\t\t// In such cases, we should also not matching vcores.\n+\t\t\treturn matchVcores ?\n+\t\t\t\tCollections.singletonList(containerResource) :\n+\t\t\t\tcontainerMemoryToContainerResource.getOrDefault(containerResource.getMemory(), Collections.emptyList());\n+\t\t}\n+\n+\t\tprivate Resource createAndMapContainerResource(final WorkerResourceSpec workerResourceSpec) {\n+\t\t\t// TODO: need to unset process/flink memory size from configuration if dynamic worker resource is activated", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzA5OTU5MQ=="}, "originalCommit": {"oid": "ef959ec03f964ceb0664388b4b72c8eac5773fbd"}, "originalPosition": 116}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQ4MzA1MQ==", "bodyText": "I think after these PRs, the RM implementations should fully support dynamic worker resources. The problem is that we not yet have a SM implementation that supports it. Once we have a SM implementation that supports dynamic worker resources, we should unset the process/flink size from configuration when using it. And if we don't do the unset, Flink might fail at the places of these TODOs. So I added the TODOs as reminders, that if later these codes fail this might be the reason.", "url": "https://github.com/apache/flink/pull/11353#discussion_r404483051", "createdAt": "2020-04-07T01:26:07Z", "author": {"login": "xintongsong"}, "path": "flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java", "diffHunk": "@@ -615,4 +632,85 @@ protected double getCpuCores(final Configuration configuration) {\n \t\t//noinspection NumericCastThatLosesPrecision\n \t\treturn cpuCoresLong;\n \t}\n+\n+\t/**\n+\t * Utility class for converting between Flink {@link WorkerResourceSpec} and Yarn {@link Resource}.\n+\t */\n+\t@VisibleForTesting\n+\tstatic class WorkerSpecContainerResourceAdapter {\n+\t\tprivate final Configuration flinkConfig;\n+\t\tprivate final int minMemMB;\n+\t\tprivate final int minVcore;\n+\t\tprivate final boolean matchVcores;\n+\t\tprivate final Map<WorkerResourceSpec, Resource> workerSpecToContainerResource;\n+\t\tprivate final Map<Resource, Collection<WorkerResourceSpec>> containerResourceToWorkerSpecs;\n+\t\tprivate final Map<Integer, Collection<Resource>> containerMemoryToContainerResource;\n+\n+\t\t@VisibleForTesting\n+\t\tWorkerSpecContainerResourceAdapter(\n+\t\t\t\tfinal Configuration flinkConfig,\n+\t\t\t\tfinal int minMemMB,\n+\t\t\t\tfinal int minVcore,\n+\t\t\t\tfinal boolean matchVcores) {\n+\t\t\tthis.flinkConfig = Preconditions.checkNotNull(flinkConfig);\n+\t\t\tthis.minMemMB = minMemMB;\n+\t\t\tthis.minVcore = minVcore;\n+\t\t\tthis.matchVcores = matchVcores;\n+\t\t\tworkerSpecToContainerResource = new HashMap<>();\n+\t\t\tcontainerResourceToWorkerSpecs = new HashMap<>();\n+\t\t\tcontainerMemoryToContainerResource = new HashMap<>();\n+\t\t}\n+\n+\t\t@VisibleForTesting\n+\t\tResource getContainerResource(final WorkerResourceSpec workerResourceSpec) {\n+\t\t\treturn workerSpecToContainerResource.computeIfAbsent(\n+\t\t\t\tPreconditions.checkNotNull(workerResourceSpec),\n+\t\t\t\tthis::createAndMapContainerResource);\n+\t\t}\n+\n+\t\t@VisibleForTesting\n+\t\tCollection<WorkerResourceSpec> getWorkerSpecs(final Resource containerResource) {\n+\t\t\treturn getEquivalentContainerResource(containerResource).stream()\n+\t\t\t\t.flatMap(resource -> containerResourceToWorkerSpecs.getOrDefault(resource, Collections.emptyList()).stream())\n+\t\t\t\t.collect(Collectors.toList());\n+\t\t}\n+\n+\t\t@VisibleForTesting\n+\t\tCollection<Resource> getEquivalentContainerResource(final Resource containerResource) {\n+\t\t\t// Yarn might ignore the requested vcores, depending on its configurations.\n+\t\t\t// In such cases, we should also not matching vcores.\n+\t\t\treturn matchVcores ?\n+\t\t\t\tCollections.singletonList(containerResource) :\n+\t\t\t\tcontainerMemoryToContainerResource.getOrDefault(containerResource.getMemory(), Collections.emptyList());\n+\t\t}\n+\n+\t\tprivate Resource createAndMapContainerResource(final WorkerResourceSpec workerResourceSpec) {\n+\t\t\t// TODO: need to unset process/flink memory size from configuration if dynamic worker resource is activated", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzA5OTU5MQ=="}, "originalCommit": {"oid": "ef959ec03f964ceb0664388b4b72c8eac5773fbd"}, "originalPosition": 116}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDY0MTA4OA==", "bodyText": "The current SM will be initialized with the proper default WorkerResourceSpec derived from Flink's configuration, right? Hence, we could already resolve this TODO, right? Then there would not be any problems with enabling support for dynamic worker resources.", "url": "https://github.com/apache/flink/pull/11353#discussion_r404641088", "createdAt": "2020-04-07T08:46:17Z", "author": {"login": "tillrohrmann"}, "path": "flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java", "diffHunk": "@@ -615,4 +632,85 @@ protected double getCpuCores(final Configuration configuration) {\n \t\t//noinspection NumericCastThatLosesPrecision\n \t\treturn cpuCoresLong;\n \t}\n+\n+\t/**\n+\t * Utility class for converting between Flink {@link WorkerResourceSpec} and Yarn {@link Resource}.\n+\t */\n+\t@VisibleForTesting\n+\tstatic class WorkerSpecContainerResourceAdapter {\n+\t\tprivate final Configuration flinkConfig;\n+\t\tprivate final int minMemMB;\n+\t\tprivate final int minVcore;\n+\t\tprivate final boolean matchVcores;\n+\t\tprivate final Map<WorkerResourceSpec, Resource> workerSpecToContainerResource;\n+\t\tprivate final Map<Resource, Collection<WorkerResourceSpec>> containerResourceToWorkerSpecs;\n+\t\tprivate final Map<Integer, Collection<Resource>> containerMemoryToContainerResource;\n+\n+\t\t@VisibleForTesting\n+\t\tWorkerSpecContainerResourceAdapter(\n+\t\t\t\tfinal Configuration flinkConfig,\n+\t\t\t\tfinal int minMemMB,\n+\t\t\t\tfinal int minVcore,\n+\t\t\t\tfinal boolean matchVcores) {\n+\t\t\tthis.flinkConfig = Preconditions.checkNotNull(flinkConfig);\n+\t\t\tthis.minMemMB = minMemMB;\n+\t\t\tthis.minVcore = minVcore;\n+\t\t\tthis.matchVcores = matchVcores;\n+\t\t\tworkerSpecToContainerResource = new HashMap<>();\n+\t\t\tcontainerResourceToWorkerSpecs = new HashMap<>();\n+\t\t\tcontainerMemoryToContainerResource = new HashMap<>();\n+\t\t}\n+\n+\t\t@VisibleForTesting\n+\t\tResource getContainerResource(final WorkerResourceSpec workerResourceSpec) {\n+\t\t\treturn workerSpecToContainerResource.computeIfAbsent(\n+\t\t\t\tPreconditions.checkNotNull(workerResourceSpec),\n+\t\t\t\tthis::createAndMapContainerResource);\n+\t\t}\n+\n+\t\t@VisibleForTesting\n+\t\tCollection<WorkerResourceSpec> getWorkerSpecs(final Resource containerResource) {\n+\t\t\treturn getEquivalentContainerResource(containerResource).stream()\n+\t\t\t\t.flatMap(resource -> containerResourceToWorkerSpecs.getOrDefault(resource, Collections.emptyList()).stream())\n+\t\t\t\t.collect(Collectors.toList());\n+\t\t}\n+\n+\t\t@VisibleForTesting\n+\t\tCollection<Resource> getEquivalentContainerResource(final Resource containerResource) {\n+\t\t\t// Yarn might ignore the requested vcores, depending on its configurations.\n+\t\t\t// In such cases, we should also not matching vcores.\n+\t\t\treturn matchVcores ?\n+\t\t\t\tCollections.singletonList(containerResource) :\n+\t\t\t\tcontainerMemoryToContainerResource.getOrDefault(containerResource.getMemory(), Collections.emptyList());\n+\t\t}\n+\n+\t\tprivate Resource createAndMapContainerResource(final WorkerResourceSpec workerResourceSpec) {\n+\t\t\t// TODO: need to unset process/flink memory size from configuration if dynamic worker resource is activated", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzA5OTU5MQ=="}, "originalCommit": {"oid": "ef959ec03f964ceb0664388b4b72c8eac5773fbd"}, "originalPosition": 116}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUwMDE5MTExOnYy", "diffSide": "RIGHT", "path": "flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNTo1Njo0M1rOGAbkaQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNTo1Njo0M1rOGAbkaQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzEwNDg3Mw==", "bodyText": "If we are only interested in the equivalence class, then I would suggest to change the type from Collection to Set.", "url": "https://github.com/apache/flink/pull/11353#discussion_r403104873", "createdAt": "2020-04-03T15:56:43Z", "author": {"login": "tillrohrmann"}, "path": "flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java", "diffHunk": "@@ -615,4 +632,85 @@ protected double getCpuCores(final Configuration configuration) {\n \t\t//noinspection NumericCastThatLosesPrecision\n \t\treturn cpuCoresLong;\n \t}\n+\n+\t/**\n+\t * Utility class for converting between Flink {@link WorkerResourceSpec} and Yarn {@link Resource}.\n+\t */\n+\t@VisibleForTesting\n+\tstatic class WorkerSpecContainerResourceAdapter {\n+\t\tprivate final Configuration flinkConfig;\n+\t\tprivate final int minMemMB;\n+\t\tprivate final int minVcore;\n+\t\tprivate final boolean matchVcores;\n+\t\tprivate final Map<WorkerResourceSpec, Resource> workerSpecToContainerResource;\n+\t\tprivate final Map<Resource, Collection<WorkerResourceSpec>> containerResourceToWorkerSpecs;\n+\t\tprivate final Map<Integer, Collection<Resource>> containerMemoryToContainerResource;\n+\n+\t\t@VisibleForTesting\n+\t\tWorkerSpecContainerResourceAdapter(\n+\t\t\t\tfinal Configuration flinkConfig,\n+\t\t\t\tfinal int minMemMB,\n+\t\t\t\tfinal int minVcore,\n+\t\t\t\tfinal boolean matchVcores) {\n+\t\t\tthis.flinkConfig = Preconditions.checkNotNull(flinkConfig);\n+\t\t\tthis.minMemMB = minMemMB;\n+\t\t\tthis.minVcore = minVcore;\n+\t\t\tthis.matchVcores = matchVcores;\n+\t\t\tworkerSpecToContainerResource = new HashMap<>();\n+\t\t\tcontainerResourceToWorkerSpecs = new HashMap<>();\n+\t\t\tcontainerMemoryToContainerResource = new HashMap<>();\n+\t\t}\n+\n+\t\t@VisibleForTesting\n+\t\tResource getContainerResource(final WorkerResourceSpec workerResourceSpec) {\n+\t\t\treturn workerSpecToContainerResource.computeIfAbsent(\n+\t\t\t\tPreconditions.checkNotNull(workerResourceSpec),\n+\t\t\t\tthis::createAndMapContainerResource);\n+\t\t}\n+\n+\t\t@VisibleForTesting\n+\t\tCollection<WorkerResourceSpec> getWorkerSpecs(final Resource containerResource) {\n+\t\t\treturn getEquivalentContainerResource(containerResource).stream()\n+\t\t\t\t.flatMap(resource -> containerResourceToWorkerSpecs.getOrDefault(resource, Collections.emptyList()).stream())\n+\t\t\t\t.collect(Collectors.toList());\n+\t\t}\n+\n+\t\t@VisibleForTesting\n+\t\tCollection<Resource> getEquivalentContainerResource(final Resource containerResource) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef959ec03f964ceb0664388b4b72c8eac5773fbd"}, "originalPosition": 107}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUwMDE5NjY4OnYy", "diffSide": "RIGHT", "path": "flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNTo1ODoxMVrOGAboJg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNTo1ODoxMVrOGAboJg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzEwNTgzMA==", "bodyText": "If we are interested in every WorkerResourceSpec which ever resulted into a given Resource, then I would suggest to change the value type to List. Otherwise one could instantiate this field with a Set which has different semantics.", "url": "https://github.com/apache/flink/pull/11353#discussion_r403105830", "createdAt": "2020-04-03T15:58:11Z", "author": {"login": "tillrohrmann"}, "path": "flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java", "diffHunk": "@@ -615,4 +632,85 @@ protected double getCpuCores(final Configuration configuration) {\n \t\t//noinspection NumericCastThatLosesPrecision\n \t\treturn cpuCoresLong;\n \t}\n+\n+\t/**\n+\t * Utility class for converting between Flink {@link WorkerResourceSpec} and Yarn {@link Resource}.\n+\t */\n+\t@VisibleForTesting\n+\tstatic class WorkerSpecContainerResourceAdapter {\n+\t\tprivate final Configuration flinkConfig;\n+\t\tprivate final int minMemMB;\n+\t\tprivate final int minVcore;\n+\t\tprivate final boolean matchVcores;\n+\t\tprivate final Map<WorkerResourceSpec, Resource> workerSpecToContainerResource;\n+\t\tprivate final Map<Resource, Collection<WorkerResourceSpec>> containerResourceToWorkerSpecs;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef959ec03f964ceb0664388b4b72c8eac5773fbd"}, "originalPosition": 74}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUwMDIwNDUzOnYy", "diffSide": "RIGHT", "path": "flink-yarn/src/main/java/org/apache/flink/yarn/configuration/YarnConfigOptionsInternal.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNjowMDowMlrOGAbs5g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wNlQxNDoyMDowNVrOGBaEwg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzEwNzA0Ng==", "bodyText": "Does this mean that one has to configure ones Flink cluster depending on the configuration of the Yarn cluster? What happens if one forgets about Flink?", "url": "https://github.com/apache/flink/pull/11353#discussion_r403107046", "createdAt": "2020-04-03T16:00:02Z", "author": {"login": "tillrohrmann"}, "path": "flink-yarn/src/main/java/org/apache/flink/yarn/configuration/YarnConfigOptionsInternal.java", "diffHunk": "@@ -34,4 +37,24 @@\n \t\t\t\t\t.stringType()\n \t\t\t\t\t.noDefaultValue()\n \t\t\t\t\t.withDescription(\"**DO NOT USE** The location of the log config file, e.g. the path to your log4j.properties for log4j.\");\n+\n+\t/**\n+\t * **DO NO USE** Whether {@link YarnResourceManager} should match the vcores of allocated containers with those requested.\n+\t *\n+\t * <p>By default, Yarn ignores vcores in the container requests, and always allocate 1 vcore for each container.\n+\t * Iff 'yarn.scheduler.capacity.resource-calculator' is set to 'DominantResourceCalculator' for Yarn, will it\n+\t * allocate container vcores as requested. Unfortunately, this configuration option is dedicated for Yarn Scheduler,\n+\t * and is only accessible to applications in Hadoop 2.6+.\n+\t *\n+\t * <p>ATM, it should be fine to not match vcores, because with the current {@link SlotManagerImpl} all the TM\n+\t * containers should have the same resources.\n+\t *\n+\t * <p>If later we add another {@link SlotManager} implementation that may have TMs with different resources, we can\n+\t * switch this option on only for the new SM, and the new SM can also be available on Hadoop 2.6+ only.\n+\t */\n+\tpublic static final ConfigOption<Boolean> MATCH_CONTAINER_VCORES =\n+\t\t\tkey(\"$internal.yarn.resourcemanager.enable-vcore-matching\")\n+\t\t\t\t\t.booleanType()\n+\t\t\t\t\t.defaultValue(false)\n+\t\t\t\t\t.withDescription(\"**DO NOT USE** Whether YarnResourceManager should match the container vcores.\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef959ec03f964ceb0664388b4b72c8eac5773fbd"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzg0MDE0Mg==", "bodyText": "For the time being, yes.\nHadoop supports programmatically get this configuration from RegisterApplicationMasterResponse starting from 2.6.x, so we don't need users to manually configure this. But I believe the lowest Hadoop version Flink supports is 2.4.x, so we do not have a good way other than having user configure it.\nThis option should only affect the cases with dynamic worker resources. If the option is not set on a Yarn cluster that matches vcores, then workers with different cpu but same memory may not be schedules correctly. E.g., if Flink wants to start task executor t1 in a container with resources <1GB, 1 vcore>, and t2 in a container with resources <1GB, 2 vcore>, the actually resource available to t1 might be <1GB, 2 vcore> and to t2 might be <1GB, 1 vcore>.", "url": "https://github.com/apache/flink/pull/11353#discussion_r403840142", "createdAt": "2020-04-06T05:39:06Z", "author": {"login": "xintongsong"}, "path": "flink-yarn/src/main/java/org/apache/flink/yarn/configuration/YarnConfigOptionsInternal.java", "diffHunk": "@@ -34,4 +37,24 @@\n \t\t\t\t\t.stringType()\n \t\t\t\t\t.noDefaultValue()\n \t\t\t\t\t.withDescription(\"**DO NOT USE** The location of the log config file, e.g. the path to your log4j.properties for log4j.\");\n+\n+\t/**\n+\t * **DO NO USE** Whether {@link YarnResourceManager} should match the vcores of allocated containers with those requested.\n+\t *\n+\t * <p>By default, Yarn ignores vcores in the container requests, and always allocate 1 vcore for each container.\n+\t * Iff 'yarn.scheduler.capacity.resource-calculator' is set to 'DominantResourceCalculator' for Yarn, will it\n+\t * allocate container vcores as requested. Unfortunately, this configuration option is dedicated for Yarn Scheduler,\n+\t * and is only accessible to applications in Hadoop 2.6+.\n+\t *\n+\t * <p>ATM, it should be fine to not match vcores, because with the current {@link SlotManagerImpl} all the TM\n+\t * containers should have the same resources.\n+\t *\n+\t * <p>If later we add another {@link SlotManager} implementation that may have TMs with different resources, we can\n+\t * switch this option on only for the new SM, and the new SM can also be available on Hadoop 2.6+ only.\n+\t */\n+\tpublic static final ConfigOption<Boolean> MATCH_CONTAINER_VCORES =\n+\t\t\tkey(\"$internal.yarn.resourcemanager.enable-vcore-matching\")\n+\t\t\t\t\t.booleanType()\n+\t\t\t\t\t.defaultValue(false)\n+\t\t\t\t\t.withDescription(\"**DO NOT USE** Whether YarnResourceManager should match the container vcores.\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzEwNzA0Ng=="}, "originalCommit": {"oid": "ef959ec03f964ceb0664388b4b72c8eac5773fbd"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDEyODk2Mg==", "bodyText": "Ok, thanks for the clarification. Automatically configuring this flag with Hadoop >= 2.6 sounds like a good idea.", "url": "https://github.com/apache/flink/pull/11353#discussion_r404128962", "createdAt": "2020-04-06T14:20:05Z", "author": {"login": "tillrohrmann"}, "path": "flink-yarn/src/main/java/org/apache/flink/yarn/configuration/YarnConfigOptionsInternal.java", "diffHunk": "@@ -34,4 +37,24 @@\n \t\t\t\t\t.stringType()\n \t\t\t\t\t.noDefaultValue()\n \t\t\t\t\t.withDescription(\"**DO NOT USE** The location of the log config file, e.g. the path to your log4j.properties for log4j.\");\n+\n+\t/**\n+\t * **DO NO USE** Whether {@link YarnResourceManager} should match the vcores of allocated containers with those requested.\n+\t *\n+\t * <p>By default, Yarn ignores vcores in the container requests, and always allocate 1 vcore for each container.\n+\t * Iff 'yarn.scheduler.capacity.resource-calculator' is set to 'DominantResourceCalculator' for Yarn, will it\n+\t * allocate container vcores as requested. Unfortunately, this configuration option is dedicated for Yarn Scheduler,\n+\t * and is only accessible to applications in Hadoop 2.6+.\n+\t *\n+\t * <p>ATM, it should be fine to not match vcores, because with the current {@link SlotManagerImpl} all the TM\n+\t * containers should have the same resources.\n+\t *\n+\t * <p>If later we add another {@link SlotManager} implementation that may have TMs with different resources, we can\n+\t * switch this option on only for the new SM, and the new SM can also be available on Hadoop 2.6+ only.\n+\t */\n+\tpublic static final ConfigOption<Boolean> MATCH_CONTAINER_VCORES =\n+\t\t\tkey(\"$internal.yarn.resourcemanager.enable-vcore-matching\")\n+\t\t\t\t\t.booleanType()\n+\t\t\t\t\t.defaultValue(false)\n+\t\t\t\t\t.withDescription(\"**DO NOT USE** Whether YarnResourceManager should match the container vcores.\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzEwNzA0Ng=="}, "originalCommit": {"oid": "ef959ec03f964ceb0664388b4b72c8eac5773fbd"}, "originalPosition": 33}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUwMDIxMzM0OnYy", "diffSide": "RIGHT", "path": "flink-yarn/src/test/java/org/apache/flink/yarn/YarnResourceManagerTest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNjowMjozM1rOGAbyvw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNjowMjozM1rOGAbyvw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzEwODU0Mw==", "bodyText": "It is usually easier to understand if one use an enum instead of boolean because one can give the different values expressive names (e.g. MATCH_VCORES, IGNORE_VCORES).", "url": "https://github.com/apache/flink/pull/11353#discussion_r403108543", "createdAt": "2020-04-03T16:02:33Z", "author": {"login": "tillrohrmann"}, "path": "flink-yarn/src/test/java/org/apache/flink/yarn/YarnResourceManagerTest.java", "diffHunk": "@@ -584,6 +590,85 @@ public void testGetCpuExceedMaxInt() throws Exception {\n \t\t}};\n \t}\n \n+\t@Test\n+\tpublic void testWorkerSpecContainerResourceAdapter_MatchVcores() {\n+\t\tfinal int minMemMB = 100;\n+\t\tfinal int minVcore = 10;\n+\t\tfinal YarnResourceManager.WorkerSpecContainerResourceAdapter adapter =\n+\t\t\tnew YarnResourceManager.WorkerSpecContainerResourceAdapter(\n+\t\t\t\tgetConfigProcessSpecEqualsWorkerSpec(), minMemMB, minVcore, true);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef959ec03f964ceb0664388b4b72c8eac5773fbd"}, "originalPosition": 37}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUwMDI0MjU2OnYy", "diffSide": "RIGHT", "path": "flink-yarn/src/test/java/org/apache/flink/yarn/TestingContainer.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNjoxMDo0MVrOGAcFtg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wNlQxNDoyNzoxM1rOGBaadA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzExMzM5OA==", "bodyText": "I like the idea of getting rid of Mockito but how do we ensure that this works with all Hadoop versions? Looking at Hadoop 2.10. https://github.com/apache/hadoop/blob/release-2.10.0-RC1/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/Container.java it looks as if the container has gotten some more methods.", "url": "https://github.com/apache/flink/pull/11353#discussion_r403113398", "createdAt": "2020-04-03T16:10:41Z", "author": {"login": "tillrohrmann"}, "path": "flink-yarn/src/test/java/org/apache/flink/yarn/TestingContainer.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.yarn;\n+\n+import org.apache.hadoop.yarn.api.records.Container;\n+import org.apache.hadoop.yarn.api.records.ContainerId;\n+import org.apache.hadoop.yarn.api.records.NodeId;\n+import org.apache.hadoop.yarn.api.records.Priority;\n+import org.apache.hadoop.yarn.api.records.Resource;\n+import org.apache.hadoop.yarn.api.records.Token;\n+\n+/**\n+ * A {@link Container} implementation for testing.\n+ */\n+class TestingContainer extends Container {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2f2053259d86240c56ad4498eadae237ab30e979"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzg0MzkyMQ==", "bodyText": "Thanks for the pointer. Then I think we can extends ContainerPBImpl instead, similar to TestingYarnAMRMClientAsync.\nI think what we really need is to override the following methods.\n\ngetId\ngetNodeId\ngetResource\ngetPriority\n\nI'm aware that ContainerPBImpl is annotated Unstable, but we need these four methods anyway. If a future Hadoop version removed either of the four methods, then neither the original mockContainer nor the Flink production codes where these methods are used will work anymore.", "url": "https://github.com/apache/flink/pull/11353#discussion_r403843921", "createdAt": "2020-04-06T05:52:51Z", "author": {"login": "xintongsong"}, "path": "flink-yarn/src/test/java/org/apache/flink/yarn/TestingContainer.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.yarn;\n+\n+import org.apache.hadoop.yarn.api.records.Container;\n+import org.apache.hadoop.yarn.api.records.ContainerId;\n+import org.apache.hadoop.yarn.api.records.NodeId;\n+import org.apache.hadoop.yarn.api.records.Priority;\n+import org.apache.hadoop.yarn.api.records.Resource;\n+import org.apache.hadoop.yarn.api.records.Token;\n+\n+/**\n+ * A {@link Container} implementation for testing.\n+ */\n+class TestingContainer extends Container {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzExMzM5OA=="}, "originalCommit": {"oid": "2f2053259d86240c56ad4498eadae237ab30e979"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDEzNDUxNg==", "bodyText": "Using ContainerPBImpl should work.", "url": "https://github.com/apache/flink/pull/11353#discussion_r404134516", "createdAt": "2020-04-06T14:27:13Z", "author": {"login": "tillrohrmann"}, "path": "flink-yarn/src/test/java/org/apache/flink/yarn/TestingContainer.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.yarn;\n+\n+import org.apache.hadoop.yarn.api.records.Container;\n+import org.apache.hadoop.yarn.api.records.ContainerId;\n+import org.apache.hadoop.yarn.api.records.NodeId;\n+import org.apache.hadoop.yarn.api.records.Priority;\n+import org.apache.hadoop.yarn.api.records.Resource;\n+import org.apache.hadoop.yarn.api.records.Token;\n+\n+/**\n+ * A {@link Container} implementation for testing.\n+ */\n+class TestingContainer extends Container {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzExMzM5OA=="}, "originalCommit": {"oid": "2f2053259d86240c56ad4498eadae237ab30e979"}, "originalPosition": 31}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUwMDI1ODcwOnYy", "diffSide": "RIGHT", "path": "flink-yarn/src/test/java/org/apache/flink/yarn/TestingContainerStatus.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNjoxMzoxMlrOGAcO3A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNjoxMzoxMlrOGAcO3A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzExNTc0MA==", "bodyText": "Shouldn't we fail in case someone calls setContainerId here? Otherwise it might go unnoticed and result in some strange behaviour/failure.", "url": "https://github.com/apache/flink/pull/11353#discussion_r403115740", "createdAt": "2020-04-03T16:13:12Z", "author": {"login": "tillrohrmann"}, "path": "flink-yarn/src/test/java/org/apache/flink/yarn/TestingContainerStatus.java", "diffHunk": "@@ -0,0 +1,86 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.yarn;\n+\n+import org.apache.hadoop.yarn.api.records.ContainerId;\n+import org.apache.hadoop.yarn.api.records.ContainerState;\n+import org.apache.hadoop.yarn.api.records.ContainerStatus;\n+\n+/**\n+ * A {@link ContainerStatus} implementation for testing.\n+ */\n+class TestingContainerStatus extends ContainerStatus {\n+\n+\tprivate final ContainerId containerId;\n+\tprivate final ContainerState containerState;\n+\tprivate final String diagnostics;\n+\tprivate final int exitStatus;\n+\n+\tTestingContainerStatus(\n+\t\tfinal ContainerId containerId,\n+\t\tfinal ContainerState containerState,\n+\t\tfinal String diagnostics,\n+\t\tfinal int exitStatus) {\n+\n+\t\tthis.containerId = containerId;\n+\t\tthis.containerState = containerState;\n+\t\tthis.diagnostics = diagnostics;\n+\t\tthis.exitStatus = exitStatus;\n+\t}\n+\n+\t@Override\n+\tpublic ContainerId getContainerId() {\n+\t\treturn containerId;\n+\t}\n+\n+\t@Override\n+\tpublic void setContainerId(ContainerId containerId) {\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dfde526f4f6cea035e7774b59c3b8987fd4ae973"}, "originalPosition": 54}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUwMDI3NzA2OnYy", "diffSide": "RIGHT", "path": "flink-yarn/src/test/java/org/apache/flink/yarn/TestingYarnAMRMClientAsync.java", "isResolved": true, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNjoxNTo0M1rOGAcYjw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QwODozNToyN1rOGB460g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzExODIyMw==", "bodyText": "AMRMClientAsyncImpl is annotated as unstable. I'm not sure how will the testing implementation works across different Yarn versions. Have we tried this out?", "url": "https://github.com/apache/flink/pull/11353#discussion_r403118223", "createdAt": "2020-04-03T16:15:43Z", "author": {"login": "tillrohrmann"}, "path": "flink-yarn/src/test/java/org/apache/flink/yarn/TestingYarnAMRMClientAsync.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.yarn;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.api.java.tuple.Tuple4;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.yarn.api.records.ContainerId;\n+import org.apache.hadoop.yarn.api.records.Priority;\n+import org.apache.hadoop.yarn.api.records.Resource;\n+import org.apache.hadoop.yarn.client.api.AMRMClient;\n+import org.apache.hadoop.yarn.client.api.async.AMRMClientAsync;\n+import org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl;\n+\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+\n+/**\n+ * A Yarn {@link AMRMClientAsync} implementation for testing.\n+ */\n+public class TestingYarnAMRMClientAsync extends AMRMClientAsyncImpl<AMRMClient.ContainerRequest> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "658172cc8265792b710830a5d2749304cd1b10ba"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzg0Njk3MA==", "bodyText": "Same here.\nI'm aware AMRMClientAsyncImpl is annotated as Unstable. But we are only overriding the minimum set of public methods here. If these methods are changed, then neither the original testing codes nor the production codes of Flink could work anymore.\nI have not tried all the Yarn versions. But this should at least work with hadoop 2.4.1 (the default hadoop version if build from sources locally) and 2.8.3 (hadoop version for travis ci tests). I can further verify it with other versions.", "url": "https://github.com/apache/flink/pull/11353#discussion_r403846970", "createdAt": "2020-04-06T06:03:41Z", "author": {"login": "xintongsong"}, "path": "flink-yarn/src/test/java/org/apache/flink/yarn/TestingYarnAMRMClientAsync.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.yarn;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.api.java.tuple.Tuple4;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.yarn.api.records.ContainerId;\n+import org.apache.hadoop.yarn.api.records.Priority;\n+import org.apache.hadoop.yarn.api.records.Resource;\n+import org.apache.hadoop.yarn.client.api.AMRMClient;\n+import org.apache.hadoop.yarn.client.api.async.AMRMClientAsync;\n+import org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl;\n+\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+\n+/**\n+ * A Yarn {@link AMRMClientAsync} implementation for testing.\n+ */\n+public class TestingYarnAMRMClientAsync extends AMRMClientAsyncImpl<AMRMClient.ContainerRequest> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzExODIyMw=="}, "originalCommit": {"oid": "658172cc8265792b710830a5d2749304cd1b10ba"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDE0MjQ0Nw==", "bodyText": "I think my problem with this class is that we are partially overriding an implementation class here. We don't really know how the overridden methods are used internally. Looking at the code, it looks as if they are all forwarding the calls to client. From this perspective it might look fine but if you take a look at getAvailableResources, which has not been overridden, then it also calls client. Do we know that there is no contract between the overridden methods and getAvailableResources? What if the client returns different available resources depending on how many container requests have been added? I'm not saying that this is the case, but I want to make that overriding individual methods of an implementation class can cause failures which are hard to predict and even harder to debug.", "url": "https://github.com/apache/flink/pull/11353#discussion_r404142447", "createdAt": "2020-04-06T14:37:11Z", "author": {"login": "tillrohrmann"}, "path": "flink-yarn/src/test/java/org/apache/flink/yarn/TestingYarnAMRMClientAsync.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.yarn;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.api.java.tuple.Tuple4;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.yarn.api.records.ContainerId;\n+import org.apache.hadoop.yarn.api.records.Priority;\n+import org.apache.hadoop.yarn.api.records.Resource;\n+import org.apache.hadoop.yarn.client.api.AMRMClient;\n+import org.apache.hadoop.yarn.client.api.async.AMRMClientAsync;\n+import org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl;\n+\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+\n+/**\n+ * A Yarn {@link AMRMClientAsync} implementation for testing.\n+ */\n+public class TestingYarnAMRMClientAsync extends AMRMClientAsyncImpl<AMRMClient.ContainerRequest> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzExODIyMw=="}, "originalCommit": {"oid": "658172cc8265792b710830a5d2749304cd1b10ba"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQ4ODAzMg==", "bodyText": "I think we cannot guarantee that there's no contract between the overridden methods and the not-overridden methods. Even we can, these internal things might change in future.\nOn the other hand, what we can guarantee is that all the methods used by Flink are overridden. It won't be a problem if an un-overridden method is internally related to the overridden methods but is never called.\nTo that end, I guess we need to make the following changes.\n\nOverride all the methods used in Flink. That means setHeartbeatInterval and unregisterApplicationMaster in addition to those already overridden.\nExplicitly state in the javadoc that all methods used in Flink should be overridden. That might help debug the tests if later Flink uses more public API of AMRMClientAsync.", "url": "https://github.com/apache/flink/pull/11353#discussion_r404488032", "createdAt": "2020-04-07T01:43:56Z", "author": {"login": "xintongsong"}, "path": "flink-yarn/src/test/java/org/apache/flink/yarn/TestingYarnAMRMClientAsync.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.yarn;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.api.java.tuple.Tuple4;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.yarn.api.records.ContainerId;\n+import org.apache.hadoop.yarn.api.records.Priority;\n+import org.apache.hadoop.yarn.api.records.Resource;\n+import org.apache.hadoop.yarn.client.api.AMRMClient;\n+import org.apache.hadoop.yarn.client.api.async.AMRMClientAsync;\n+import org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl;\n+\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+\n+/**\n+ * A Yarn {@link AMRMClientAsync} implementation for testing.\n+ */\n+public class TestingYarnAMRMClientAsync extends AMRMClientAsyncImpl<AMRMClient.ContainerRequest> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzExODIyMw=="}, "originalCommit": {"oid": "658172cc8265792b710830a5d2749304cd1b10ba"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDYzNDMyMg==", "bodyText": "I think this is the price we have to pay for integrating with another system. It is always painful and tedious to set up the tests. In parts, this always involves reimplementing the other systems behaviour in mocks. I don't see a better atm. Hence, +1 for what you've proposed.", "url": "https://github.com/apache/flink/pull/11353#discussion_r404634322", "createdAt": "2020-04-07T08:35:27Z", "author": {"login": "tillrohrmann"}, "path": "flink-yarn/src/test/java/org/apache/flink/yarn/TestingYarnAMRMClientAsync.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.yarn;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.api.java.tuple.Tuple4;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.yarn.api.records.ContainerId;\n+import org.apache.hadoop.yarn.api.records.Priority;\n+import org.apache.hadoop.yarn.api.records.Resource;\n+import org.apache.hadoop.yarn.client.api.AMRMClient;\n+import org.apache.hadoop.yarn.client.api.async.AMRMClientAsync;\n+import org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl;\n+\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+\n+/**\n+ * A Yarn {@link AMRMClientAsync} implementation for testing.\n+ */\n+public class TestingYarnAMRMClientAsync extends AMRMClientAsyncImpl<AMRMClient.ContainerRequest> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzExODIyMw=="}, "originalCommit": {"oid": "658172cc8265792b710830a5d2749304cd1b10ba"}, "originalPosition": 42}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUwMDI3OTc2OnYy", "diffSide": "RIGHT", "path": "flink-yarn/src/test/java/org/apache/flink/yarn/TestingYarnAMRMClientAsync.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNjoxNjoxMFrOGAcaCg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNjoxNjoxMFrOGAcaCg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzExODYwMg==", "bodyText": "I'd suggest to use the BiConsumer here.", "url": "https://github.com/apache/flink/pull/11353#discussion_r403118602", "createdAt": "2020-04-03T16:16:10Z", "author": {"login": "tillrohrmann"}, "path": "flink-yarn/src/test/java/org/apache/flink/yarn/TestingYarnAMRMClientAsync.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.yarn;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.api.java.tuple.Tuple4;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.yarn.api.records.ContainerId;\n+import org.apache.hadoop.yarn.api.records.Priority;\n+import org.apache.hadoop.yarn.api.records.Resource;\n+import org.apache.hadoop.yarn.client.api.AMRMClient;\n+import org.apache.hadoop.yarn.client.api.async.AMRMClientAsync;\n+import org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl;\n+\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+\n+/**\n+ * A Yarn {@link AMRMClientAsync} implementation for testing.\n+ */\n+public class TestingYarnAMRMClientAsync extends AMRMClientAsyncImpl<AMRMClient.ContainerRequest> {\n+\n+\tprivate Function<Tuple4<Priority, String, Resource, CallbackHandler>, List<? extends Collection<AMRMClient.ContainerRequest>>>\n+\t\tgetMatchingRequestsFunction = ignored -> Collections.emptyList();\n+\tprivate Consumer<Tuple2<AMRMClient.ContainerRequest, CallbackHandler>> addContainerRequestConsumer = ignored -> {};\n+\tprivate Consumer<Tuple2<AMRMClient.ContainerRequest, CallbackHandler>> removeContainerRequestConsumer = ignored -> {};\n+\tprivate Consumer<Tuple2<ContainerId, CallbackHandler>> releaseAssignedContainerConsumer = ignored -> {};", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "658172cc8265792b710830a5d2749304cd1b10ba"}, "originalPosition": 48}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUwMDMwMjIwOnYy", "diffSide": "RIGHT", "path": "flink-yarn/src/test/java/org/apache/flink/yarn/TestingYarnNMClientAsync.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNjoxOTo0NlrOGAcmBw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNjoxOTo0NlrOGAcmBw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzEyMTY3MQ==", "bodyText": "Same here NMClientAsyncImpl seems to be unstable and might change depending on the used Yarn version.", "url": "https://github.com/apache/flink/pull/11353#discussion_r403121671", "createdAt": "2020-04-03T16:19:46Z", "author": {"login": "tillrohrmann"}, "path": "flink-yarn/src/test/java/org/apache/flink/yarn/TestingYarnNMClientAsync.java", "diffHunk": "@@ -0,0 +1,82 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.yarn;\n+\n+import org.apache.flink.api.java.tuple.Tuple3;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.yarn.api.records.Container;\n+import org.apache.hadoop.yarn.api.records.ContainerId;\n+import org.apache.hadoop.yarn.api.records.ContainerLaunchContext;\n+import org.apache.hadoop.yarn.api.records.NodeId;\n+import org.apache.hadoop.yarn.client.api.async.NMClientAsync;\n+import org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl;\n+\n+import java.util.function.Consumer;\n+\n+/**\n+ * A Yarn {@link NMClientAsync} implementation for testing.\n+ */\n+class TestingYarnNMClientAsync extends NMClientAsyncImpl {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8144ec035355decb400352fb6606b55075d13b8b"}, "originalPosition": 37}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUwMDMwMzcwOnYy", "diffSide": "RIGHT", "path": "flink-yarn/src/test/java/org/apache/flink/yarn/TestingYarnNMClientAsync.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNjoyMDowNVrOGAcnFQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNjoyMDowNVrOGAcnFQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzEyMTk0MQ==", "bodyText": "I'd suggest to use the TriConsumer here.", "url": "https://github.com/apache/flink/pull/11353#discussion_r403121941", "createdAt": "2020-04-03T16:20:05Z", "author": {"login": "tillrohrmann"}, "path": "flink-yarn/src/test/java/org/apache/flink/yarn/TestingYarnNMClientAsync.java", "diffHunk": "@@ -0,0 +1,82 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.yarn;\n+\n+import org.apache.flink.api.java.tuple.Tuple3;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.yarn.api.records.Container;\n+import org.apache.hadoop.yarn.api.records.ContainerId;\n+import org.apache.hadoop.yarn.api.records.ContainerLaunchContext;\n+import org.apache.hadoop.yarn.api.records.NodeId;\n+import org.apache.hadoop.yarn.client.api.async.NMClientAsync;\n+import org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl;\n+\n+import java.util.function.Consumer;\n+\n+/**\n+ * A Yarn {@link NMClientAsync} implementation for testing.\n+ */\n+class TestingYarnNMClientAsync extends NMClientAsyncImpl {\n+\n+\tprivate Consumer<Tuple3<Container, ContainerLaunchContext, CallbackHandler>> startContainerAsyncConsumer = ignored -> {};\n+\tprivate Consumer<Tuple3<ContainerId, NodeId, CallbackHandler>> stopContainerAsyncConsumer = ignored -> {};", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8144ec035355decb400352fb6606b55075d13b8b"}, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUwMDMxNjMyOnYy", "diffSide": "RIGHT", "path": "flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNjoyMjoxNlrOGAcumw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNjoyMjoxNlrOGAcumw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzEyMzg2Nw==", "bodyText": "I'd suggest to use the for-each loop.", "url": "https://github.com/apache/flink/pull/11353#discussion_r403123867", "createdAt": "2020-04-03T16:22:16Z", "author": {"login": "tillrohrmann"}, "path": "flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java", "diffHunk": "@@ -363,31 +356,64 @@ public void onContainersCompleted(final List<ContainerStatus> statuses) {\n \t@Override\n \tpublic void onContainersAllocated(List<Container> containers) {\n \t\trunAsync(() -> {\n-\t\t\tlog.info(\"Received {} containers with {} pending container requests.\", containers.size(), numPendingContainerRequests);\n-\t\t\tfinal Collection<AMRMClient.ContainerRequest> pendingRequests = getPendingRequests();\n-\t\t\tfinal Iterator<AMRMClient.ContainerRequest> pendingRequestsIterator = pendingRequests.iterator();\n+\t\t\tlog.info(\"Received {} containers.\", containers.size());\n \n-\t\t\t// number of allocated containers can be larger than the number of pending container requests\n-\t\t\tfinal int numAcceptedContainers = Math.min(containers.size(), numPendingContainerRequests);\n-\t\t\tfinal List<Container> requiredContainers = containers.subList(0, numAcceptedContainers);\n-\t\t\tfinal List<Container> excessContainers = containers.subList(numAcceptedContainers, containers.size());\n-\n-\t\t\tfor (int i = 0; i < requiredContainers.size(); i++) {\n-\t\t\t\tremoveContainerRequest(pendingRequestsIterator.next());\n-\t\t\t}\n-\n-\t\t\texcessContainers.forEach(this::returnExcessContainer);\n-\t\t\trequiredContainers.forEach(this::startTaskExecutorInContainer);\n+\t\t\tgroupContainerByResource(containers).forEach(this::onContainersOfResourceAllocated);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "785558f7c2944cb50e57e191a242c5e981ab42b7"}, "originalPosition": 69}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUwMDMzMTc4OnYy", "diffSide": "RIGHT", "path": "flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNjoyNDo1MlrOGAc2_w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wNlQwNjowNzo1MlrOGBI8RQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzEyNjAxNQ==", "bodyText": "Can it happen that getWorkerSpecs(resource) returns a list which contains a WorkerResourceSpec twice?", "url": "https://github.com/apache/flink/pull/11353#discussion_r403126015", "createdAt": "2020-04-03T16:24:52Z", "author": {"login": "tillrohrmann"}, "path": "flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java", "diffHunk": "@@ -363,31 +356,64 @@ public void onContainersCompleted(final List<ContainerStatus> statuses) {\n \t@Override\n \tpublic void onContainersAllocated(List<Container> containers) {\n \t\trunAsync(() -> {\n-\t\t\tlog.info(\"Received {} containers with {} pending container requests.\", containers.size(), numPendingContainerRequests);\n-\t\t\tfinal Collection<AMRMClient.ContainerRequest> pendingRequests = getPendingRequests();\n-\t\t\tfinal Iterator<AMRMClient.ContainerRequest> pendingRequestsIterator = pendingRequests.iterator();\n+\t\t\tlog.info(\"Received {} containers.\", containers.size());\n \n-\t\t\t// number of allocated containers can be larger than the number of pending container requests\n-\t\t\tfinal int numAcceptedContainers = Math.min(containers.size(), numPendingContainerRequests);\n-\t\t\tfinal List<Container> requiredContainers = containers.subList(0, numAcceptedContainers);\n-\t\t\tfinal List<Container> excessContainers = containers.subList(numAcceptedContainers, containers.size());\n-\n-\t\t\tfor (int i = 0; i < requiredContainers.size(); i++) {\n-\t\t\t\tremoveContainerRequest(pendingRequestsIterator.next());\n-\t\t\t}\n-\n-\t\t\texcessContainers.forEach(this::returnExcessContainer);\n-\t\t\trequiredContainers.forEach(this::startTaskExecutorInContainer);\n+\t\t\tgroupContainerByResource(containers).forEach(this::onContainersOfResourceAllocated);\n \n \t\t\t// if we are waiting for no further containers, we can go to the\n \t\t\t// regular heartbeat interval\n-\t\t\tif (numPendingContainerRequests <= 0) {\n+\t\t\tif (pendingWorkerCounter.getTotalNum() <= 0) {\n \t\t\t\tresourceManagerClient.setHeartbeatInterval(yarnHeartbeatIntervalMillis);\n \t\t\t}\n \t\t});\n \t}\n \n-\tprivate void startTaskExecutorInContainer(Container container) {\n+\tprivate Map<Resource, List<Container>> groupContainerByResource(List<Container> containers) {\n+\t\treturn containers.stream().collect(Collectors.groupingBy(Container::getResource));\n+\t}\n+\n+\tprivate void onContainersOfResourceAllocated(Resource resource, List<Container> containers) {\n+\t\tfinal List<WorkerResourceSpec> pendingWorkerResourceSpecs =\n+\t\t\tworkerSpecContainerResourceAdapter.getWorkerSpecs(resource).stream()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "785558f7c2944cb50e57e191a242c5e981ab42b7"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzg0ODI2MQ==", "bodyText": "It shouldn't. I'll change the return type of getWorkerSpecs to Set.", "url": "https://github.com/apache/flink/pull/11353#discussion_r403848261", "createdAt": "2020-04-06T06:07:52Z", "author": {"login": "xintongsong"}, "path": "flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java", "diffHunk": "@@ -363,31 +356,64 @@ public void onContainersCompleted(final List<ContainerStatus> statuses) {\n \t@Override\n \tpublic void onContainersAllocated(List<Container> containers) {\n \t\trunAsync(() -> {\n-\t\t\tlog.info(\"Received {} containers with {} pending container requests.\", containers.size(), numPendingContainerRequests);\n-\t\t\tfinal Collection<AMRMClient.ContainerRequest> pendingRequests = getPendingRequests();\n-\t\t\tfinal Iterator<AMRMClient.ContainerRequest> pendingRequestsIterator = pendingRequests.iterator();\n+\t\t\tlog.info(\"Received {} containers.\", containers.size());\n \n-\t\t\t// number of allocated containers can be larger than the number of pending container requests\n-\t\t\tfinal int numAcceptedContainers = Math.min(containers.size(), numPendingContainerRequests);\n-\t\t\tfinal List<Container> requiredContainers = containers.subList(0, numAcceptedContainers);\n-\t\t\tfinal List<Container> excessContainers = containers.subList(numAcceptedContainers, containers.size());\n-\n-\t\t\tfor (int i = 0; i < requiredContainers.size(); i++) {\n-\t\t\t\tremoveContainerRequest(pendingRequestsIterator.next());\n-\t\t\t}\n-\n-\t\t\texcessContainers.forEach(this::returnExcessContainer);\n-\t\t\trequiredContainers.forEach(this::startTaskExecutorInContainer);\n+\t\t\tgroupContainerByResource(containers).forEach(this::onContainersOfResourceAllocated);\n \n \t\t\t// if we are waiting for no further containers, we can go to the\n \t\t\t// regular heartbeat interval\n-\t\t\tif (numPendingContainerRequests <= 0) {\n+\t\t\tif (pendingWorkerCounter.getTotalNum() <= 0) {\n \t\t\t\tresourceManagerClient.setHeartbeatInterval(yarnHeartbeatIntervalMillis);\n \t\t\t}\n \t\t});\n \t}\n \n-\tprivate void startTaskExecutorInContainer(Container container) {\n+\tprivate Map<Resource, List<Container>> groupContainerByResource(List<Container> containers) {\n+\t\treturn containers.stream().collect(Collectors.groupingBy(Container::getResource));\n+\t}\n+\n+\tprivate void onContainersOfResourceAllocated(Resource resource, List<Container> containers) {\n+\t\tfinal List<WorkerResourceSpec> pendingWorkerResourceSpecs =\n+\t\t\tworkerSpecContainerResourceAdapter.getWorkerSpecs(resource).stream()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzEyNjAxNQ=="}, "originalCommit": {"oid": "785558f7c2944cb50e57e191a242c5e981ab42b7"}, "originalPosition": 87}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUwMDM0MjMyOnYy", "diffSide": "RIGHT", "path": "flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java", "isResolved": true, "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNjoyNzowM1rOGAc8-g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QwMTo0OTozMFrOGBwFhQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzEyNzU0Ng==", "bodyText": "What about removing it from workerSpecContainerResourceAdapter?", "url": "https://github.com/apache/flink/pull/11353#discussion_r403127546", "createdAt": "2020-04-03T16:27:03Z", "author": {"login": "tillrohrmann"}, "path": "flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java", "diffHunk": "@@ -412,30 +439,32 @@ private void releaseFailedContainerAndRequestNewContainerIfRequired(ContainerId\n \n \t\tfinal ResourceID resourceId = new ResourceID(containerId.toString());\n \t\t// release the failed container\n-\t\tworkerNodeMap.remove(resourceId);\n+\t\tYarnWorkerNode yarnWorkerNode = workerNodeMap.remove(resourceId);\n \t\tresourceManagerClient.releaseAssignedContainer(containerId);\n \t\t// and ask for a new one\n-\t\trequestYarnContainerIfRequired();\n+\t\trequestYarnContainerIfRequired(yarnWorkerNode.getContainer().getResource());\n \t}\n \n \tprivate void returnExcessContainer(Container excessContainer) {\n \t\tlog.info(\"Returning excess container {}.\", excessContainer.getId());\n \t\tresourceManagerClient.releaseAssignedContainer(excessContainer.getId());\n \t}\n \n-\tprivate void removeContainerRequest(AMRMClient.ContainerRequest pendingContainerRequest) {\n-\t\tnumPendingContainerRequests--;\n-\n-\t\tlog.info(\"Removing container request {}. Pending container requests {}.\", pendingContainerRequest, numPendingContainerRequests);\n-\n+\tprivate void removeContainerRequest(AMRMClient.ContainerRequest pendingContainerRequest, WorkerResourceSpec workerResourceSpec) {\n+\t\tlog.info(\"Removing container request {}.\", pendingContainerRequest);\n+\t\tpendingWorkerCounter.decreaseAndGet(workerResourceSpec);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "785558f7c2944cb50e57e191a242c5e981ab42b7"}, "originalPosition": 164}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzEzMTk2OA==", "bodyText": "Differently asked, when do we clean workerSpecContainerResourceAdapter up?", "url": "https://github.com/apache/flink/pull/11353#discussion_r403131968", "createdAt": "2020-04-03T16:34:15Z", "author": {"login": "tillrohrmann"}, "path": "flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java", "diffHunk": "@@ -412,30 +439,32 @@ private void releaseFailedContainerAndRequestNewContainerIfRequired(ContainerId\n \n \t\tfinal ResourceID resourceId = new ResourceID(containerId.toString());\n \t\t// release the failed container\n-\t\tworkerNodeMap.remove(resourceId);\n+\t\tYarnWorkerNode yarnWorkerNode = workerNodeMap.remove(resourceId);\n \t\tresourceManagerClient.releaseAssignedContainer(containerId);\n \t\t// and ask for a new one\n-\t\trequestYarnContainerIfRequired();\n+\t\trequestYarnContainerIfRequired(yarnWorkerNode.getContainer().getResource());\n \t}\n \n \tprivate void returnExcessContainer(Container excessContainer) {\n \t\tlog.info(\"Returning excess container {}.\", excessContainer.getId());\n \t\tresourceManagerClient.releaseAssignedContainer(excessContainer.getId());\n \t}\n \n-\tprivate void removeContainerRequest(AMRMClient.ContainerRequest pendingContainerRequest) {\n-\t\tnumPendingContainerRequests--;\n-\n-\t\tlog.info(\"Removing container request {}. Pending container requests {}.\", pendingContainerRequest, numPendingContainerRequests);\n-\n+\tprivate void removeContainerRequest(AMRMClient.ContainerRequest pendingContainerRequest, WorkerResourceSpec workerResourceSpec) {\n+\t\tlog.info(\"Removing container request {}.\", pendingContainerRequest);\n+\t\tpendingWorkerCounter.decreaseAndGet(workerResourceSpec);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzEyNzU0Ng=="}, "originalCommit": {"oid": "785558f7c2944cb50e57e191a242c5e981ab42b7"}, "originalPosition": 164}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzg1MDQ4OA==", "bodyText": "Unfortunately, we don't clean workerSpecContainerResourceAdapter up.\nI think the upper bond of the amount of records really depends on how many different WorkerResourceSpec do we have. If we want to clean the unused records up, the WorkerSpecContainerResourceAdapter will need YarnResourceManager to tell it which WorkerResourceSpec is no longer needed (all corresponding TMs are completed and no pending ones). I'm not sure whether this complexity is necessary.", "url": "https://github.com/apache/flink/pull/11353#discussion_r403850488", "createdAt": "2020-04-06T06:14:33Z", "author": {"login": "xintongsong"}, "path": "flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java", "diffHunk": "@@ -412,30 +439,32 @@ private void releaseFailedContainerAndRequestNewContainerIfRequired(ContainerId\n \n \t\tfinal ResourceID resourceId = new ResourceID(containerId.toString());\n \t\t// release the failed container\n-\t\tworkerNodeMap.remove(resourceId);\n+\t\tYarnWorkerNode yarnWorkerNode = workerNodeMap.remove(resourceId);\n \t\tresourceManagerClient.releaseAssignedContainer(containerId);\n \t\t// and ask for a new one\n-\t\trequestYarnContainerIfRequired();\n+\t\trequestYarnContainerIfRequired(yarnWorkerNode.getContainer().getResource());\n \t}\n \n \tprivate void returnExcessContainer(Container excessContainer) {\n \t\tlog.info(\"Returning excess container {}.\", excessContainer.getId());\n \t\tresourceManagerClient.releaseAssignedContainer(excessContainer.getId());\n \t}\n \n-\tprivate void removeContainerRequest(AMRMClient.ContainerRequest pendingContainerRequest) {\n-\t\tnumPendingContainerRequests--;\n-\n-\t\tlog.info(\"Removing container request {}. Pending container requests {}.\", pendingContainerRequest, numPendingContainerRequests);\n-\n+\tprivate void removeContainerRequest(AMRMClient.ContainerRequest pendingContainerRequest, WorkerResourceSpec workerResourceSpec) {\n+\t\tlog.info(\"Removing container request {}.\", pendingContainerRequest);\n+\t\tpendingWorkerCounter.decreaseAndGet(workerResourceSpec);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzEyNzU0Ng=="}, "originalCommit": {"oid": "785558f7c2944cb50e57e191a242c5e981ab42b7"}, "originalPosition": 164}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDE0NDkwMA==", "bodyText": "Hmm, would it be possible to have a periodic cleanup task?", "url": "https://github.com/apache/flink/pull/11353#discussion_r404144900", "createdAt": "2020-04-06T14:40:20Z", "author": {"login": "tillrohrmann"}, "path": "flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java", "diffHunk": "@@ -412,30 +439,32 @@ private void releaseFailedContainerAndRequestNewContainerIfRequired(ContainerId\n \n \t\tfinal ResourceID resourceId = new ResourceID(containerId.toString());\n \t\t// release the failed container\n-\t\tworkerNodeMap.remove(resourceId);\n+\t\tYarnWorkerNode yarnWorkerNode = workerNodeMap.remove(resourceId);\n \t\tresourceManagerClient.releaseAssignedContainer(containerId);\n \t\t// and ask for a new one\n-\t\trequestYarnContainerIfRequired();\n+\t\trequestYarnContainerIfRequired(yarnWorkerNode.getContainer().getResource());\n \t}\n \n \tprivate void returnExcessContainer(Container excessContainer) {\n \t\tlog.info(\"Returning excess container {}.\", excessContainer.getId());\n \t\tresourceManagerClient.releaseAssignedContainer(excessContainer.getId());\n \t}\n \n-\tprivate void removeContainerRequest(AMRMClient.ContainerRequest pendingContainerRequest) {\n-\t\tnumPendingContainerRequests--;\n-\n-\t\tlog.info(\"Removing container request {}. Pending container requests {}.\", pendingContainerRequest, numPendingContainerRequests);\n-\n+\tprivate void removeContainerRequest(AMRMClient.ContainerRequest pendingContainerRequest, WorkerResourceSpec workerResourceSpec) {\n+\t\tlog.info(\"Removing container request {}.\", pendingContainerRequest);\n+\t\tpendingWorkerCounter.decreaseAndGet(workerResourceSpec);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzEyNzU0Ng=="}, "originalCommit": {"oid": "785558f7c2944cb50e57e191a242c5e981ab42b7"}, "originalPosition": 164}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDE2MDE1Mw==", "bodyText": "Not saying that we have to implement it right away. I just want to know how one could fix this as a follow-up task.", "url": "https://github.com/apache/flink/pull/11353#discussion_r404160153", "createdAt": "2020-04-06T14:59:32Z", "author": {"login": "tillrohrmann"}, "path": "flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java", "diffHunk": "@@ -412,30 +439,32 @@ private void releaseFailedContainerAndRequestNewContainerIfRequired(ContainerId\n \n \t\tfinal ResourceID resourceId = new ResourceID(containerId.toString());\n \t\t// release the failed container\n-\t\tworkerNodeMap.remove(resourceId);\n+\t\tYarnWorkerNode yarnWorkerNode = workerNodeMap.remove(resourceId);\n \t\tresourceManagerClient.releaseAssignedContainer(containerId);\n \t\t// and ask for a new one\n-\t\trequestYarnContainerIfRequired();\n+\t\trequestYarnContainerIfRequired(yarnWorkerNode.getContainer().getResource());\n \t}\n \n \tprivate void returnExcessContainer(Container excessContainer) {\n \t\tlog.info(\"Returning excess container {}.\", excessContainer.getId());\n \t\tresourceManagerClient.releaseAssignedContainer(excessContainer.getId());\n \t}\n \n-\tprivate void removeContainerRequest(AMRMClient.ContainerRequest pendingContainerRequest) {\n-\t\tnumPendingContainerRequests--;\n-\n-\t\tlog.info(\"Removing container request {}. Pending container requests {}.\", pendingContainerRequest, numPendingContainerRequests);\n-\n+\tprivate void removeContainerRequest(AMRMClient.ContainerRequest pendingContainerRequest, WorkerResourceSpec workerResourceSpec) {\n+\t\tlog.info(\"Removing container request {}.\", pendingContainerRequest);\n+\t\tpendingWorkerCounter.decreaseAndGet(workerResourceSpec);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzEyNzU0Ng=="}, "originalCommit": {"oid": "785558f7c2944cb50e57e191a242c5e981ab42b7"}, "originalPosition": 164}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQ4OTYwNQ==", "bodyText": "True, a periodic cleanup task should work there, or we can trigger a clean-up if the number of entries stored in WorkerSpecContainerResourceAdapter reaches a certain threshold.\nI think the clean-up is definitely doable. It's just not very necessary at the time being.", "url": "https://github.com/apache/flink/pull/11353#discussion_r404489605", "createdAt": "2020-04-07T01:49:30Z", "author": {"login": "xintongsong"}, "path": "flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java", "diffHunk": "@@ -412,30 +439,32 @@ private void releaseFailedContainerAndRequestNewContainerIfRequired(ContainerId\n \n \t\tfinal ResourceID resourceId = new ResourceID(containerId.toString());\n \t\t// release the failed container\n-\t\tworkerNodeMap.remove(resourceId);\n+\t\tYarnWorkerNode yarnWorkerNode = workerNodeMap.remove(resourceId);\n \t\tresourceManagerClient.releaseAssignedContainer(containerId);\n \t\t// and ask for a new one\n-\t\trequestYarnContainerIfRequired();\n+\t\trequestYarnContainerIfRequired(yarnWorkerNode.getContainer().getResource());\n \t}\n \n \tprivate void returnExcessContainer(Container excessContainer) {\n \t\tlog.info(\"Returning excess container {}.\", excessContainer.getId());\n \t\tresourceManagerClient.releaseAssignedContainer(excessContainer.getId());\n \t}\n \n-\tprivate void removeContainerRequest(AMRMClient.ContainerRequest pendingContainerRequest) {\n-\t\tnumPendingContainerRequests--;\n-\n-\t\tlog.info(\"Removing container request {}. Pending container requests {}.\", pendingContainerRequest, numPendingContainerRequests);\n-\n+\tprivate void removeContainerRequest(AMRMClient.ContainerRequest pendingContainerRequest, WorkerResourceSpec workerResourceSpec) {\n+\t\tlog.info(\"Removing container request {}.\", pendingContainerRequest);\n+\t\tpendingWorkerCounter.decreaseAndGet(workerResourceSpec);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzEyNzU0Ng=="}, "originalCommit": {"oid": "785558f7c2944cb50e57e191a242c5e981ab42b7"}, "originalPosition": 164}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUwMDM0OTkzOnYy", "diffSide": "RIGHT", "path": "flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNjoyOTowOFrOGAdB4w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wNlQwNjoxNzoxNVrOGBJIXw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzEyODgwMw==", "bodyText": "Shouldn't we do this for all instead of any?", "url": "https://github.com/apache/flink/pull/11353#discussion_r403128803", "createdAt": "2020-04-03T16:29:08Z", "author": {"login": "tillrohrmann"}, "path": "flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java", "diffHunk": "@@ -540,39 +571,41 @@ private FinalApplicationStatus getYarnStatus(ApplicationStatus status) {\n \t/**\n \t * Request new container if pending containers cannot satisfy pending slot requests.\n \t */\n-\tprivate void requestYarnContainerIfRequired() {\n-\t\tint requiredTaskManagers = getNumberRequiredTaskManagers();\n-\n-\t\tif (requiredTaskManagers > numPendingContainerRequests) {\n-\t\t\trequestYarnContainer();\n-\t\t}\n+\tprivate void requestYarnContainerIfRequired(Resource containerResource) {\n+\t\tgetPendingWorkerNums().entrySet().stream()\n+\t\t\t.filter(entry ->\n+\t\t\t\tgetContainerResource(entry.getKey()).equals(containerResource) &&\n+\t\t\t\tentry.getValue() > pendingWorkerCounter.getNum(entry.getKey()))\n+\t\t\t.findAny()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "785558f7c2944cb50e57e191a242c5e981ab42b7"}, "originalPosition": 213}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzg1MTM1OQ==", "bodyText": "I think this is the same issue as KubernetesResourceManager#requestKubernetesPodIfRequired. I was trying to keep the exactly same behavior as before. But I think you're right, it is nice to request all required pods early.", "url": "https://github.com/apache/flink/pull/11353#discussion_r403851359", "createdAt": "2020-04-06T06:17:15Z", "author": {"login": "xintongsong"}, "path": "flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java", "diffHunk": "@@ -540,39 +571,41 @@ private FinalApplicationStatus getYarnStatus(ApplicationStatus status) {\n \t/**\n \t * Request new container if pending containers cannot satisfy pending slot requests.\n \t */\n-\tprivate void requestYarnContainerIfRequired() {\n-\t\tint requiredTaskManagers = getNumberRequiredTaskManagers();\n-\n-\t\tif (requiredTaskManagers > numPendingContainerRequests) {\n-\t\t\trequestYarnContainer();\n-\t\t}\n+\tprivate void requestYarnContainerIfRequired(Resource containerResource) {\n+\t\tgetPendingWorkerNums().entrySet().stream()\n+\t\t\t.filter(entry ->\n+\t\t\t\tgetContainerResource(entry.getKey()).equals(containerResource) &&\n+\t\t\t\tentry.getValue() > pendingWorkerCounter.getNum(entry.getKey()))\n+\t\t\t.findAny()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzEyODgwMw=="}, "originalCommit": {"oid": "785558f7c2944cb50e57e191a242c5e981ab42b7"}, "originalPosition": 213}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUwMDM1OTA5OnYy", "diffSide": "RIGHT", "path": "flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNjozMjowMFrOGAdH8A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wNlQxNDo0MjozNFrOGBbJpw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzEzMDM1Mg==", "bodyText": "I have to admit that I find getPendingWorkerNums() and pendingWorkerCounter quite confusing. The sound almost the same but the former means the requirements of the SlotManager and the latter the currently pending workers which have been requested by the RM.", "url": "https://github.com/apache/flink/pull/11353#discussion_r403130352", "createdAt": "2020-04-03T16:32:00Z", "author": {"login": "tillrohrmann"}, "path": "flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java", "diffHunk": "@@ -540,39 +571,41 @@ private FinalApplicationStatus getYarnStatus(ApplicationStatus status) {\n \t/**\n \t * Request new container if pending containers cannot satisfy pending slot requests.\n \t */\n-\tprivate void requestYarnContainerIfRequired() {\n-\t\tint requiredTaskManagers = getNumberRequiredTaskManagers();\n-\n-\t\tif (requiredTaskManagers > numPendingContainerRequests) {\n-\t\t\trequestYarnContainer();\n-\t\t}\n+\tprivate void requestYarnContainerIfRequired(Resource containerResource) {\n+\t\tgetPendingWorkerNums().entrySet().stream()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "785558f7c2944cb50e57e191a242c5e981ab42b7"}, "originalPosition": 209}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzg1MjYyMg==", "bodyText": "What's your suggestion here? How about renaming getPendingWorkerNums to getSlotManagerPendingWorkerNums?", "url": "https://github.com/apache/flink/pull/11353#discussion_r403852622", "createdAt": "2020-04-06T06:20:59Z", "author": {"login": "xintongsong"}, "path": "flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java", "diffHunk": "@@ -540,39 +571,41 @@ private FinalApplicationStatus getYarnStatus(ApplicationStatus status) {\n \t/**\n \t * Request new container if pending containers cannot satisfy pending slot requests.\n \t */\n-\tprivate void requestYarnContainerIfRequired() {\n-\t\tint requiredTaskManagers = getNumberRequiredTaskManagers();\n-\n-\t\tif (requiredTaskManagers > numPendingContainerRequests) {\n-\t\t\trequestYarnContainer();\n-\t\t}\n+\tprivate void requestYarnContainerIfRequired(Resource containerResource) {\n+\t\tgetPendingWorkerNums().entrySet().stream()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzEzMDM1Mg=="}, "originalCommit": {"oid": "785558f7c2944cb50e57e191a242c5e981ab42b7"}, "originalPosition": 209}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDE0NjU5OQ==", "bodyText": "maybe we could rename getPendingWorkerNums into getRequiredWorkers or getRequiredResources.", "url": "https://github.com/apache/flink/pull/11353#discussion_r404146599", "createdAt": "2020-04-06T14:42:34Z", "author": {"login": "tillrohrmann"}, "path": "flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java", "diffHunk": "@@ -540,39 +571,41 @@ private FinalApplicationStatus getYarnStatus(ApplicationStatus status) {\n \t/**\n \t * Request new container if pending containers cannot satisfy pending slot requests.\n \t */\n-\tprivate void requestYarnContainerIfRequired() {\n-\t\tint requiredTaskManagers = getNumberRequiredTaskManagers();\n-\n-\t\tif (requiredTaskManagers > numPendingContainerRequests) {\n-\t\t\trequestYarnContainer();\n-\t\t}\n+\tprivate void requestYarnContainerIfRequired(Resource containerResource) {\n+\t\tgetPendingWorkerNums().entrySet().stream()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzEzMDM1Mg=="}, "originalCommit": {"oid": "785558f7c2944cb50e57e191a242c5e981ab42b7"}, "originalPosition": 209}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUwMDM3MjYyOnYy", "diffSide": "RIGHT", "path": "flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNjozNTowNlrOGAdQNw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNjozNTowNlrOGAdQNw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzEzMjQ3MQ==", "bodyText": "Shouldn't we iterate over all resources which are needed instead of restricting it to containerResource?", "url": "https://github.com/apache/flink/pull/11353#discussion_r403132471", "createdAt": "2020-04-03T16:35:06Z", "author": {"login": "tillrohrmann"}, "path": "flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java", "diffHunk": "@@ -540,39 +571,41 @@ private FinalApplicationStatus getYarnStatus(ApplicationStatus status) {\n \t/**\n \t * Request new container if pending containers cannot satisfy pending slot requests.\n \t */\n-\tprivate void requestYarnContainerIfRequired() {\n-\t\tint requiredTaskManagers = getNumberRequiredTaskManagers();\n-\n-\t\tif (requiredTaskManagers > numPendingContainerRequests) {\n-\t\t\trequestYarnContainer();\n-\t\t}\n+\tprivate void requestYarnContainerIfRequired(Resource containerResource) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "785558f7c2944cb50e57e191a242c5e981ab42b7"}, "originalPosition": 208}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUwMDM3NTM5OnYy", "diffSide": "RIGHT", "path": "flink-kubernetes/src/test/java/org/apache/flink/kubernetes/entrypoint/KubernetesResourceManagerFactoryTest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNjozNTo1MVrOGAdR7w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNjozNTo1MVrOGAdR7w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzEzMjkxMQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            public class KubernetesResourceManagerFactoryTest {\n          \n          \n            \n            public class KubernetesResourceManagerFactoryTest extends TestLogger {", "url": "https://github.com/apache/flink/pull/11353#discussion_r403132911", "createdAt": "2020-04-03T16:35:51Z", "author": {"login": "tillrohrmann"}, "path": "flink-kubernetes/src/test/java/org/apache/flink/kubernetes/entrypoint/KubernetesResourceManagerFactoryTest.java", "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.kubernetes.entrypoint;\n+\n+import org.apache.flink.api.common.resources.CPUResource;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.configuration.TaskManagerOptions;\n+import org.apache.flink.kubernetes.configuration.KubernetesConfigOptions;\n+\n+import org.junit.Test;\n+\n+import static org.hamcrest.core.Is.is;\n+import static org.junit.Assert.assertThat;\n+\n+/**\n+ * Tests for {@link KubernetesResourceManagerFactory}.\n+ */\n+public class KubernetesResourceManagerFactoryTest {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c5eb8c3d46d2142f9dd64acfaba9d27102ad3f2"}, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUwMDM3NzUyOnYy", "diffSide": "RIGHT", "path": "flink-yarn/src/test/java/org/apache/flink/yarn/entrypoint/YarnResourceManagerFactoryTest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNjozNjoyOFrOGAdTUQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNjozNjoyOFrOGAdTUQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzEzMzI2NQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            public class YarnResourceManagerFactoryTest {\n          \n          \n            \n            public class YarnResourceManagerFactoryTest extends TestLogger {", "url": "https://github.com/apache/flink/pull/11353#discussion_r403133265", "createdAt": "2020-04-03T16:36:28Z", "author": {"login": "tillrohrmann"}, "path": "flink-yarn/src/test/java/org/apache/flink/yarn/entrypoint/YarnResourceManagerFactoryTest.java", "diffHunk": "@@ -0,0 +1,79 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.yarn.entrypoint;\n+\n+import org.apache.flink.api.common.resources.CPUResource;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.configuration.IllegalConfigurationException;\n+import org.apache.flink.configuration.TaskManagerOptions;\n+import org.apache.flink.yarn.configuration.YarnConfigOptions;\n+\n+import org.junit.Test;\n+\n+import static org.hamcrest.Matchers.is;\n+import static org.junit.Assert.assertThat;\n+\n+/**\n+ * Tests for {@link YarnResourceManagerFactory}.\n+ */\n+public class YarnResourceManagerFactoryTest {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c5eb8c3d46d2142f9dd64acfaba9d27102ad3f2"}, "originalPosition": 35}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0MjkyMzgxOnYy", "diffSide": "RIGHT", "path": "flink-yarn/src/test/java/org/apache/flink/yarn/TestingYarnAMRMClientAsync.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQxMzo1MToxMFrOGGmTtw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMFQxNDoyNTozMVrOGIXMVQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTU3MjI3OQ==", "bodyText": "It's not super important but I would prefer setting these functions in the constructor of the TestingYarnAMRMClientAsync class instead of using setters.", "url": "https://github.com/apache/flink/pull/11353#discussion_r409572279", "createdAt": "2020-04-16T13:51:10Z", "author": {"login": "tillrohrmann"}, "path": "flink-yarn/src/test/java/org/apache/flink/yarn/TestingYarnAMRMClientAsync.java", "diffHunk": "@@ -0,0 +1,158 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.yarn;\n+\n+import org.apache.flink.api.java.tuple.Tuple4;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.flink.util.function.TriConsumer;\n+import org.apache.flink.util.function.TriFunction;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.yarn.api.protocolrecords.RegisterApplicationMasterResponse;\n+import org.apache.hadoop.yarn.api.records.ContainerId;\n+import org.apache.hadoop.yarn.api.records.FinalApplicationStatus;\n+import org.apache.hadoop.yarn.api.records.Priority;\n+import org.apache.hadoop.yarn.api.records.Resource;\n+import org.apache.hadoop.yarn.client.api.AMRMClient;\n+import org.apache.hadoop.yarn.client.api.async.AMRMClientAsync;\n+import org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl;\n+\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.function.BiConsumer;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+\n+/**\n+ * A Yarn {@link AMRMClientAsync} implementation for testing.\n+ */\n+public class TestingYarnAMRMClientAsync extends AMRMClientAsyncImpl<AMRMClient.ContainerRequest> {\n+\n+\tprivate Function<Tuple4<Priority, String, Resource, CallbackHandler>, List<? extends Collection<AMRMClient.ContainerRequest>>>\n+\t\tgetMatchingRequestsFunction = ignored -> Collections.emptyList();\n+\tprivate BiConsumer<AMRMClient.ContainerRequest, CallbackHandler> addContainerRequestConsumer = (ignored1, ignored2) -> {};\n+\tprivate BiConsumer<AMRMClient.ContainerRequest, CallbackHandler> removeContainerRequestConsumer = (ignored1, ignored2) -> {};\n+\tprivate BiConsumer<ContainerId, CallbackHandler> releaseAssignedContainerConsumer = (ignored1, ignored2) -> {};\n+\tprivate Consumer<Integer> setHeartbeatIntervalConsumer = (ignored) -> {};\n+\tprivate TriFunction<String, Integer, String, RegisterApplicationMasterResponse> registerApplicationMasterFunction =\n+\t\t(ignored1, ignored2, ignored3) -> RegisterApplicationMasterResponse.newInstance(\n+\t\t\tResource.newInstance(0, 0),\n+\t\t\tResource.newInstance(Integer.MAX_VALUE, Integer.MAX_VALUE),\n+\t\t\tCollections.emptyMap(),\n+\t\t\tnull,\n+\t\t\tCollections.emptyList(),\n+\t\t\tnull,\n+\t\t\tCollections.emptyList());\n+\tprivate TriConsumer<FinalApplicationStatus, String, String> unregisterApplicationMasterConsumer = (ignored1, ignored2, ignored3) -> {};\n+\n+\tTestingYarnAMRMClientAsync(CallbackHandler callbackHandler) {\n+\t\tsuper(0, callbackHandler);\n+\t}\n+\n+\t@Override\n+\tpublic List<? extends Collection<AMRMClient.ContainerRequest>> getMatchingRequests(Priority priority, String resourceName, Resource capability) {\n+\t\treturn getMatchingRequestsFunction.apply(Tuple4.of(priority, resourceName, capability, handler));\n+\t}\n+\n+\t@Override\n+\tpublic void addContainerRequest(AMRMClient.ContainerRequest req) {\n+\t\taddContainerRequestConsumer.accept(req, handler);\n+\t}\n+\n+\t@Override\n+\tpublic void removeContainerRequest(AMRMClient.ContainerRequest req) {\n+\t\tremoveContainerRequestConsumer.accept(req, handler);\n+\t}\n+\n+\t@Override\n+\tpublic void releaseAssignedContainer(ContainerId containerId) {\n+\t\treleaseAssignedContainerConsumer.accept(containerId, handler);\n+\t}\n+\n+\t@Override\n+\tpublic void setHeartbeatInterval(int interval) {\n+\t\tsetHeartbeatIntervalConsumer.accept(interval);\n+\t}\n+\n+\t@Override\n+\tpublic RegisterApplicationMasterResponse registerApplicationMaster(String appHostName, int appHostPort, String appTrackingUrl) {\n+\t\treturn registerApplicationMasterFunction.apply(appHostName, appHostPort, appTrackingUrl);\n+\t}\n+\n+\t@Override\n+\tpublic void unregisterApplicationMaster(FinalApplicationStatus appStatus, String appMessage, String appTrackingUrl) {\n+\t\tunregisterApplicationMasterConsumer.accept(appStatus, appMessage, appTrackingUrl);\n+\t}\n+\n+\tvoid setGetMatchingRequestsFunction(\n+\t\tFunction<Tuple4<Priority, String, Resource, CallbackHandler>, List<? extends Collection<AMRMClient.ContainerRequest>>>\n+\t\t\tgetMatchingRequestsFunction) {\n+\t\tthis.getMatchingRequestsFunction = Preconditions.checkNotNull(getMatchingRequestsFunction);\n+\t}\n+\n+\tvoid setAddContainerRequestConsumer(\n+\t\tBiConsumer<AMRMClient.ContainerRequest, CallbackHandler> addContainerRequestConsumer) {\n+\t\tthis.addContainerRequestConsumer = Preconditions.checkNotNull(addContainerRequestConsumer);\n+\t}\n+\n+\tvoid setRemoveContainerRequestConsumer(\n+\t\tBiConsumer<AMRMClient.ContainerRequest, CallbackHandler> removeContainerRequestConsumer) {\n+\t\tthis.removeContainerRequestConsumer = Preconditions.checkNotNull(removeContainerRequestConsumer);\n+\t}\n+\n+\tvoid setReleaseAssignedContainerConsumer(\n+\t\tBiConsumer<ContainerId, CallbackHandler> releaseAssignedContainerConsumer) {\n+\t\tthis.releaseAssignedContainerConsumer = Preconditions.checkNotNull(releaseAssignedContainerConsumer);\n+\t}\n+\n+\tvoid setSetHeartbeatIntervalConsumer(\n+\t\tConsumer<Integer> setHeartbeatIntervalConsumer) {\n+\t\tthis.setHeartbeatIntervalConsumer = setHeartbeatIntervalConsumer;\n+\t}\n+\n+\tvoid setRegisterApplicationMasterFunction(\n+\t\tTriFunction<String, Integer, String, RegisterApplicationMasterResponse> registerApplicationMasterFunction) {\n+\t\tthis.registerApplicationMasterFunction = registerApplicationMasterFunction;\n+\t}\n+\n+\tvoid setUnregisterApplicationMasterConsumer(\n+\t\tTriConsumer<FinalApplicationStatus, String, String> unregisterApplicationMasterConsumer) {\n+\t\tthis.unregisterApplicationMasterConsumer = unregisterApplicationMasterConsumer;\n+\t}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7f312629f59d37249600d4e17546a4122cd07589"}, "originalPosition": 138}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTU3NTk2Mw==", "bodyText": "If one uses the setters, then the corresponding fields should be volatile if they are being set from a different thread.", "url": "https://github.com/apache/flink/pull/11353#discussion_r409575963", "createdAt": "2020-04-16T13:55:51Z", "author": {"login": "tillrohrmann"}, "path": "flink-yarn/src/test/java/org/apache/flink/yarn/TestingYarnAMRMClientAsync.java", "diffHunk": "@@ -0,0 +1,158 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.yarn;\n+\n+import org.apache.flink.api.java.tuple.Tuple4;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.flink.util.function.TriConsumer;\n+import org.apache.flink.util.function.TriFunction;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.yarn.api.protocolrecords.RegisterApplicationMasterResponse;\n+import org.apache.hadoop.yarn.api.records.ContainerId;\n+import org.apache.hadoop.yarn.api.records.FinalApplicationStatus;\n+import org.apache.hadoop.yarn.api.records.Priority;\n+import org.apache.hadoop.yarn.api.records.Resource;\n+import org.apache.hadoop.yarn.client.api.AMRMClient;\n+import org.apache.hadoop.yarn.client.api.async.AMRMClientAsync;\n+import org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl;\n+\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.function.BiConsumer;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+\n+/**\n+ * A Yarn {@link AMRMClientAsync} implementation for testing.\n+ */\n+public class TestingYarnAMRMClientAsync extends AMRMClientAsyncImpl<AMRMClient.ContainerRequest> {\n+\n+\tprivate Function<Tuple4<Priority, String, Resource, CallbackHandler>, List<? extends Collection<AMRMClient.ContainerRequest>>>\n+\t\tgetMatchingRequestsFunction = ignored -> Collections.emptyList();\n+\tprivate BiConsumer<AMRMClient.ContainerRequest, CallbackHandler> addContainerRequestConsumer = (ignored1, ignored2) -> {};\n+\tprivate BiConsumer<AMRMClient.ContainerRequest, CallbackHandler> removeContainerRequestConsumer = (ignored1, ignored2) -> {};\n+\tprivate BiConsumer<ContainerId, CallbackHandler> releaseAssignedContainerConsumer = (ignored1, ignored2) -> {};\n+\tprivate Consumer<Integer> setHeartbeatIntervalConsumer = (ignored) -> {};\n+\tprivate TriFunction<String, Integer, String, RegisterApplicationMasterResponse> registerApplicationMasterFunction =\n+\t\t(ignored1, ignored2, ignored3) -> RegisterApplicationMasterResponse.newInstance(\n+\t\t\tResource.newInstance(0, 0),\n+\t\t\tResource.newInstance(Integer.MAX_VALUE, Integer.MAX_VALUE),\n+\t\t\tCollections.emptyMap(),\n+\t\t\tnull,\n+\t\t\tCollections.emptyList(),\n+\t\t\tnull,\n+\t\t\tCollections.emptyList());\n+\tprivate TriConsumer<FinalApplicationStatus, String, String> unregisterApplicationMasterConsumer = (ignored1, ignored2, ignored3) -> {};\n+\n+\tTestingYarnAMRMClientAsync(CallbackHandler callbackHandler) {\n+\t\tsuper(0, callbackHandler);\n+\t}\n+\n+\t@Override\n+\tpublic List<? extends Collection<AMRMClient.ContainerRequest>> getMatchingRequests(Priority priority, String resourceName, Resource capability) {\n+\t\treturn getMatchingRequestsFunction.apply(Tuple4.of(priority, resourceName, capability, handler));\n+\t}\n+\n+\t@Override\n+\tpublic void addContainerRequest(AMRMClient.ContainerRequest req) {\n+\t\taddContainerRequestConsumer.accept(req, handler);\n+\t}\n+\n+\t@Override\n+\tpublic void removeContainerRequest(AMRMClient.ContainerRequest req) {\n+\t\tremoveContainerRequestConsumer.accept(req, handler);\n+\t}\n+\n+\t@Override\n+\tpublic void releaseAssignedContainer(ContainerId containerId) {\n+\t\treleaseAssignedContainerConsumer.accept(containerId, handler);\n+\t}\n+\n+\t@Override\n+\tpublic void setHeartbeatInterval(int interval) {\n+\t\tsetHeartbeatIntervalConsumer.accept(interval);\n+\t}\n+\n+\t@Override\n+\tpublic RegisterApplicationMasterResponse registerApplicationMaster(String appHostName, int appHostPort, String appTrackingUrl) {\n+\t\treturn registerApplicationMasterFunction.apply(appHostName, appHostPort, appTrackingUrl);\n+\t}\n+\n+\t@Override\n+\tpublic void unregisterApplicationMaster(FinalApplicationStatus appStatus, String appMessage, String appTrackingUrl) {\n+\t\tunregisterApplicationMasterConsumer.accept(appStatus, appMessage, appTrackingUrl);\n+\t}\n+\n+\tvoid setGetMatchingRequestsFunction(\n+\t\tFunction<Tuple4<Priority, String, Resource, CallbackHandler>, List<? extends Collection<AMRMClient.ContainerRequest>>>\n+\t\t\tgetMatchingRequestsFunction) {\n+\t\tthis.getMatchingRequestsFunction = Preconditions.checkNotNull(getMatchingRequestsFunction);\n+\t}\n+\n+\tvoid setAddContainerRequestConsumer(\n+\t\tBiConsumer<AMRMClient.ContainerRequest, CallbackHandler> addContainerRequestConsumer) {\n+\t\tthis.addContainerRequestConsumer = Preconditions.checkNotNull(addContainerRequestConsumer);\n+\t}\n+\n+\tvoid setRemoveContainerRequestConsumer(\n+\t\tBiConsumer<AMRMClient.ContainerRequest, CallbackHandler> removeContainerRequestConsumer) {\n+\t\tthis.removeContainerRequestConsumer = Preconditions.checkNotNull(removeContainerRequestConsumer);\n+\t}\n+\n+\tvoid setReleaseAssignedContainerConsumer(\n+\t\tBiConsumer<ContainerId, CallbackHandler> releaseAssignedContainerConsumer) {\n+\t\tthis.releaseAssignedContainerConsumer = Preconditions.checkNotNull(releaseAssignedContainerConsumer);\n+\t}\n+\n+\tvoid setSetHeartbeatIntervalConsumer(\n+\t\tConsumer<Integer> setHeartbeatIntervalConsumer) {\n+\t\tthis.setHeartbeatIntervalConsumer = setHeartbeatIntervalConsumer;\n+\t}\n+\n+\tvoid setRegisterApplicationMasterFunction(\n+\t\tTriFunction<String, Integer, String, RegisterApplicationMasterResponse> registerApplicationMasterFunction) {\n+\t\tthis.registerApplicationMasterFunction = registerApplicationMasterFunction;\n+\t}\n+\n+\tvoid setUnregisterApplicationMasterConsumer(\n+\t\tTriConsumer<FinalApplicationStatus, String, String> unregisterApplicationMasterConsumer) {\n+\t\tthis.unregisterApplicationMasterConsumer = unregisterApplicationMasterConsumer;\n+\t}", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTU3MjI3OQ=="}, "originalCommit": {"oid": "7f312629f59d37249600d4e17546a4122cd07589"}, "originalPosition": 138}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDg2NTc4OQ==", "bodyText": "The reason I did not set these functions in the constructor is that, one would have to construct the entire TestingYarnAMRMClientAsync for defining one of these functions in a test case. We have several such testing classes (RM client, NM client, slot manager), and it would be complicated if some of the test cases need to provide customized instances for a subset of these classes when creating the Context, while some other test cases need to do that for a different subset.\nI think making the fields volatile sounds good to me.", "url": "https://github.com/apache/flink/pull/11353#discussion_r410865789", "createdAt": "2020-04-19T09:50:21Z", "author": {"login": "xintongsong"}, "path": "flink-yarn/src/test/java/org/apache/flink/yarn/TestingYarnAMRMClientAsync.java", "diffHunk": "@@ -0,0 +1,158 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.yarn;\n+\n+import org.apache.flink.api.java.tuple.Tuple4;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.flink.util.function.TriConsumer;\n+import org.apache.flink.util.function.TriFunction;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.yarn.api.protocolrecords.RegisterApplicationMasterResponse;\n+import org.apache.hadoop.yarn.api.records.ContainerId;\n+import org.apache.hadoop.yarn.api.records.FinalApplicationStatus;\n+import org.apache.hadoop.yarn.api.records.Priority;\n+import org.apache.hadoop.yarn.api.records.Resource;\n+import org.apache.hadoop.yarn.client.api.AMRMClient;\n+import org.apache.hadoop.yarn.client.api.async.AMRMClientAsync;\n+import org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl;\n+\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.function.BiConsumer;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+\n+/**\n+ * A Yarn {@link AMRMClientAsync} implementation for testing.\n+ */\n+public class TestingYarnAMRMClientAsync extends AMRMClientAsyncImpl<AMRMClient.ContainerRequest> {\n+\n+\tprivate Function<Tuple4<Priority, String, Resource, CallbackHandler>, List<? extends Collection<AMRMClient.ContainerRequest>>>\n+\t\tgetMatchingRequestsFunction = ignored -> Collections.emptyList();\n+\tprivate BiConsumer<AMRMClient.ContainerRequest, CallbackHandler> addContainerRequestConsumer = (ignored1, ignored2) -> {};\n+\tprivate BiConsumer<AMRMClient.ContainerRequest, CallbackHandler> removeContainerRequestConsumer = (ignored1, ignored2) -> {};\n+\tprivate BiConsumer<ContainerId, CallbackHandler> releaseAssignedContainerConsumer = (ignored1, ignored2) -> {};\n+\tprivate Consumer<Integer> setHeartbeatIntervalConsumer = (ignored) -> {};\n+\tprivate TriFunction<String, Integer, String, RegisterApplicationMasterResponse> registerApplicationMasterFunction =\n+\t\t(ignored1, ignored2, ignored3) -> RegisterApplicationMasterResponse.newInstance(\n+\t\t\tResource.newInstance(0, 0),\n+\t\t\tResource.newInstance(Integer.MAX_VALUE, Integer.MAX_VALUE),\n+\t\t\tCollections.emptyMap(),\n+\t\t\tnull,\n+\t\t\tCollections.emptyList(),\n+\t\t\tnull,\n+\t\t\tCollections.emptyList());\n+\tprivate TriConsumer<FinalApplicationStatus, String, String> unregisterApplicationMasterConsumer = (ignored1, ignored2, ignored3) -> {};\n+\n+\tTestingYarnAMRMClientAsync(CallbackHandler callbackHandler) {\n+\t\tsuper(0, callbackHandler);\n+\t}\n+\n+\t@Override\n+\tpublic List<? extends Collection<AMRMClient.ContainerRequest>> getMatchingRequests(Priority priority, String resourceName, Resource capability) {\n+\t\treturn getMatchingRequestsFunction.apply(Tuple4.of(priority, resourceName, capability, handler));\n+\t}\n+\n+\t@Override\n+\tpublic void addContainerRequest(AMRMClient.ContainerRequest req) {\n+\t\taddContainerRequestConsumer.accept(req, handler);\n+\t}\n+\n+\t@Override\n+\tpublic void removeContainerRequest(AMRMClient.ContainerRequest req) {\n+\t\tremoveContainerRequestConsumer.accept(req, handler);\n+\t}\n+\n+\t@Override\n+\tpublic void releaseAssignedContainer(ContainerId containerId) {\n+\t\treleaseAssignedContainerConsumer.accept(containerId, handler);\n+\t}\n+\n+\t@Override\n+\tpublic void setHeartbeatInterval(int interval) {\n+\t\tsetHeartbeatIntervalConsumer.accept(interval);\n+\t}\n+\n+\t@Override\n+\tpublic RegisterApplicationMasterResponse registerApplicationMaster(String appHostName, int appHostPort, String appTrackingUrl) {\n+\t\treturn registerApplicationMasterFunction.apply(appHostName, appHostPort, appTrackingUrl);\n+\t}\n+\n+\t@Override\n+\tpublic void unregisterApplicationMaster(FinalApplicationStatus appStatus, String appMessage, String appTrackingUrl) {\n+\t\tunregisterApplicationMasterConsumer.accept(appStatus, appMessage, appTrackingUrl);\n+\t}\n+\n+\tvoid setGetMatchingRequestsFunction(\n+\t\tFunction<Tuple4<Priority, String, Resource, CallbackHandler>, List<? extends Collection<AMRMClient.ContainerRequest>>>\n+\t\t\tgetMatchingRequestsFunction) {\n+\t\tthis.getMatchingRequestsFunction = Preconditions.checkNotNull(getMatchingRequestsFunction);\n+\t}\n+\n+\tvoid setAddContainerRequestConsumer(\n+\t\tBiConsumer<AMRMClient.ContainerRequest, CallbackHandler> addContainerRequestConsumer) {\n+\t\tthis.addContainerRequestConsumer = Preconditions.checkNotNull(addContainerRequestConsumer);\n+\t}\n+\n+\tvoid setRemoveContainerRequestConsumer(\n+\t\tBiConsumer<AMRMClient.ContainerRequest, CallbackHandler> removeContainerRequestConsumer) {\n+\t\tthis.removeContainerRequestConsumer = Preconditions.checkNotNull(removeContainerRequestConsumer);\n+\t}\n+\n+\tvoid setReleaseAssignedContainerConsumer(\n+\t\tBiConsumer<ContainerId, CallbackHandler> releaseAssignedContainerConsumer) {\n+\t\tthis.releaseAssignedContainerConsumer = Preconditions.checkNotNull(releaseAssignedContainerConsumer);\n+\t}\n+\n+\tvoid setSetHeartbeatIntervalConsumer(\n+\t\tConsumer<Integer> setHeartbeatIntervalConsumer) {\n+\t\tthis.setHeartbeatIntervalConsumer = setHeartbeatIntervalConsumer;\n+\t}\n+\n+\tvoid setRegisterApplicationMasterFunction(\n+\t\tTriFunction<String, Integer, String, RegisterApplicationMasterResponse> registerApplicationMasterFunction) {\n+\t\tthis.registerApplicationMasterFunction = registerApplicationMasterFunction;\n+\t}\n+\n+\tvoid setUnregisterApplicationMasterConsumer(\n+\t\tTriConsumer<FinalApplicationStatus, String, String> unregisterApplicationMasterConsumer) {\n+\t\tthis.unregisterApplicationMasterConsumer = unregisterApplicationMasterConsumer;\n+\t}", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTU3MjI3OQ=="}, "originalCommit": {"oid": "7f312629f59d37249600d4e17546a4122cd07589"}, "originalPosition": 138}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTQyMTc4MQ==", "bodyText": "Makes sense.", "url": "https://github.com/apache/flink/pull/11353#discussion_r411421781", "createdAt": "2020-04-20T14:25:31Z", "author": {"login": "tillrohrmann"}, "path": "flink-yarn/src/test/java/org/apache/flink/yarn/TestingYarnAMRMClientAsync.java", "diffHunk": "@@ -0,0 +1,158 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.yarn;\n+\n+import org.apache.flink.api.java.tuple.Tuple4;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.flink.util.function.TriConsumer;\n+import org.apache.flink.util.function.TriFunction;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.yarn.api.protocolrecords.RegisterApplicationMasterResponse;\n+import org.apache.hadoop.yarn.api.records.ContainerId;\n+import org.apache.hadoop.yarn.api.records.FinalApplicationStatus;\n+import org.apache.hadoop.yarn.api.records.Priority;\n+import org.apache.hadoop.yarn.api.records.Resource;\n+import org.apache.hadoop.yarn.client.api.AMRMClient;\n+import org.apache.hadoop.yarn.client.api.async.AMRMClientAsync;\n+import org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl;\n+\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.function.BiConsumer;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+\n+/**\n+ * A Yarn {@link AMRMClientAsync} implementation for testing.\n+ */\n+public class TestingYarnAMRMClientAsync extends AMRMClientAsyncImpl<AMRMClient.ContainerRequest> {\n+\n+\tprivate Function<Tuple4<Priority, String, Resource, CallbackHandler>, List<? extends Collection<AMRMClient.ContainerRequest>>>\n+\t\tgetMatchingRequestsFunction = ignored -> Collections.emptyList();\n+\tprivate BiConsumer<AMRMClient.ContainerRequest, CallbackHandler> addContainerRequestConsumer = (ignored1, ignored2) -> {};\n+\tprivate BiConsumer<AMRMClient.ContainerRequest, CallbackHandler> removeContainerRequestConsumer = (ignored1, ignored2) -> {};\n+\tprivate BiConsumer<ContainerId, CallbackHandler> releaseAssignedContainerConsumer = (ignored1, ignored2) -> {};\n+\tprivate Consumer<Integer> setHeartbeatIntervalConsumer = (ignored) -> {};\n+\tprivate TriFunction<String, Integer, String, RegisterApplicationMasterResponse> registerApplicationMasterFunction =\n+\t\t(ignored1, ignored2, ignored3) -> RegisterApplicationMasterResponse.newInstance(\n+\t\t\tResource.newInstance(0, 0),\n+\t\t\tResource.newInstance(Integer.MAX_VALUE, Integer.MAX_VALUE),\n+\t\t\tCollections.emptyMap(),\n+\t\t\tnull,\n+\t\t\tCollections.emptyList(),\n+\t\t\tnull,\n+\t\t\tCollections.emptyList());\n+\tprivate TriConsumer<FinalApplicationStatus, String, String> unregisterApplicationMasterConsumer = (ignored1, ignored2, ignored3) -> {};\n+\n+\tTestingYarnAMRMClientAsync(CallbackHandler callbackHandler) {\n+\t\tsuper(0, callbackHandler);\n+\t}\n+\n+\t@Override\n+\tpublic List<? extends Collection<AMRMClient.ContainerRequest>> getMatchingRequests(Priority priority, String resourceName, Resource capability) {\n+\t\treturn getMatchingRequestsFunction.apply(Tuple4.of(priority, resourceName, capability, handler));\n+\t}\n+\n+\t@Override\n+\tpublic void addContainerRequest(AMRMClient.ContainerRequest req) {\n+\t\taddContainerRequestConsumer.accept(req, handler);\n+\t}\n+\n+\t@Override\n+\tpublic void removeContainerRequest(AMRMClient.ContainerRequest req) {\n+\t\tremoveContainerRequestConsumer.accept(req, handler);\n+\t}\n+\n+\t@Override\n+\tpublic void releaseAssignedContainer(ContainerId containerId) {\n+\t\treleaseAssignedContainerConsumer.accept(containerId, handler);\n+\t}\n+\n+\t@Override\n+\tpublic void setHeartbeatInterval(int interval) {\n+\t\tsetHeartbeatIntervalConsumer.accept(interval);\n+\t}\n+\n+\t@Override\n+\tpublic RegisterApplicationMasterResponse registerApplicationMaster(String appHostName, int appHostPort, String appTrackingUrl) {\n+\t\treturn registerApplicationMasterFunction.apply(appHostName, appHostPort, appTrackingUrl);\n+\t}\n+\n+\t@Override\n+\tpublic void unregisterApplicationMaster(FinalApplicationStatus appStatus, String appMessage, String appTrackingUrl) {\n+\t\tunregisterApplicationMasterConsumer.accept(appStatus, appMessage, appTrackingUrl);\n+\t}\n+\n+\tvoid setGetMatchingRequestsFunction(\n+\t\tFunction<Tuple4<Priority, String, Resource, CallbackHandler>, List<? extends Collection<AMRMClient.ContainerRequest>>>\n+\t\t\tgetMatchingRequestsFunction) {\n+\t\tthis.getMatchingRequestsFunction = Preconditions.checkNotNull(getMatchingRequestsFunction);\n+\t}\n+\n+\tvoid setAddContainerRequestConsumer(\n+\t\tBiConsumer<AMRMClient.ContainerRequest, CallbackHandler> addContainerRequestConsumer) {\n+\t\tthis.addContainerRequestConsumer = Preconditions.checkNotNull(addContainerRequestConsumer);\n+\t}\n+\n+\tvoid setRemoveContainerRequestConsumer(\n+\t\tBiConsumer<AMRMClient.ContainerRequest, CallbackHandler> removeContainerRequestConsumer) {\n+\t\tthis.removeContainerRequestConsumer = Preconditions.checkNotNull(removeContainerRequestConsumer);\n+\t}\n+\n+\tvoid setReleaseAssignedContainerConsumer(\n+\t\tBiConsumer<ContainerId, CallbackHandler> releaseAssignedContainerConsumer) {\n+\t\tthis.releaseAssignedContainerConsumer = Preconditions.checkNotNull(releaseAssignedContainerConsumer);\n+\t}\n+\n+\tvoid setSetHeartbeatIntervalConsumer(\n+\t\tConsumer<Integer> setHeartbeatIntervalConsumer) {\n+\t\tthis.setHeartbeatIntervalConsumer = setHeartbeatIntervalConsumer;\n+\t}\n+\n+\tvoid setRegisterApplicationMasterFunction(\n+\t\tTriFunction<String, Integer, String, RegisterApplicationMasterResponse> registerApplicationMasterFunction) {\n+\t\tthis.registerApplicationMasterFunction = registerApplicationMasterFunction;\n+\t}\n+\n+\tvoid setUnregisterApplicationMasterConsumer(\n+\t\tTriConsumer<FinalApplicationStatus, String, String> unregisterApplicationMasterConsumer) {\n+\t\tthis.unregisterApplicationMasterConsumer = unregisterApplicationMasterConsumer;\n+\t}", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTU3MjI3OQ=="}, "originalCommit": {"oid": "7f312629f59d37249600d4e17546a4122cd07589"}, "originalPosition": 138}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0Mjk1MTU4OnYy", "diffSide": "RIGHT", "path": "flink-yarn/src/test/java/org/apache/flink/yarn/TestingYarnNMClientAsync.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQxMzo1Njo0N1rOGGmk3A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQxMzo1Njo0N1rOGGmk3A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTU3NjY2OA==", "bodyText": "Same here with the setters and volatile fields.", "url": "https://github.com/apache/flink/pull/11353#discussion_r409576668", "createdAt": "2020-04-16T13:56:47Z", "author": {"login": "tillrohrmann"}, "path": "flink-yarn/src/test/java/org/apache/flink/yarn/TestingYarnNMClientAsync.java", "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.yarn;\n+\n+import org.apache.flink.util.Preconditions;\n+import org.apache.flink.util.function.TriConsumer;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.yarn.api.records.Container;\n+import org.apache.hadoop.yarn.api.records.ContainerId;\n+import org.apache.hadoop.yarn.api.records.ContainerLaunchContext;\n+import org.apache.hadoop.yarn.api.records.NodeId;\n+import org.apache.hadoop.yarn.client.api.async.NMClientAsync;\n+import org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl;\n+\n+/**\n+ * A Yarn {@link NMClientAsync} implementation for testing.\n+ */\n+class TestingYarnNMClientAsync extends NMClientAsyncImpl {\n+\n+\tprivate TriConsumer<Container, ContainerLaunchContext, CallbackHandler> startContainerAsyncConsumer = (ignored1, ignored2, ignored3) -> {};\n+\tprivate TriConsumer<ContainerId, NodeId, CallbackHandler> stopContainerAsyncConsumer = (ignored1, ignored2, ignored3) -> {};\n+\n+\tTestingYarnNMClientAsync(final CallbackHandler callbackHandler) {\n+\t\tsuper(callbackHandler);\n+\t}\n+\n+\t@Override\n+\tpublic void startContainerAsync(Container container, ContainerLaunchContext containerLaunchContext) {\n+\t\tthis.startContainerAsyncConsumer.accept(container, containerLaunchContext, callbackHandler);\n+\t}\n+\n+\t@Override\n+\tpublic void stopContainerAsync(ContainerId containerId, NodeId nodeId) {\n+\t\tthis.stopContainerAsyncConsumer.accept(containerId, nodeId, callbackHandler);\n+\t}\n+\n+\tvoid setStartContainerAsyncConsumer(TriConsumer<Container, ContainerLaunchContext, CallbackHandler> startContainerAsyncConsumer) {\n+\t\tthis.startContainerAsyncConsumer = Preconditions.checkNotNull(startContainerAsyncConsumer);\n+\t}\n+\n+\tvoid setStopContainerAsyncConsumer(TriConsumer<ContainerId, NodeId, CallbackHandler> stopContainerAsyncConsumer) {\n+\t\tthis.stopContainerAsyncConsumer = Preconditions.checkNotNull(stopContainerAsyncConsumer);\n+\t}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4b7b0c179879f994f6fd67f41eb690a58751edfa"}, "originalPosition": 60}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0Mjk3MDk4OnYy", "diffSide": "RIGHT", "path": "flink-yarn/src/main/java/org/apache/flink/yarn/WorkerSpecContainerResourceAdapter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQxNDowMDo0MlrOGGmxEA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQxNDowMDo0MlrOGGmxEA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTU3OTc5Mg==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \tprivate final Logger log = LoggerFactory.getLogger(WorkerSpecContainerResourceAdapter.class);\n          \n          \n            \n            \tprivate static final Logger LOG = LoggerFactory.getLogger(WorkerSpecContainerResourceAdapter.class);", "url": "https://github.com/apache/flink/pull/11353#discussion_r409579792", "createdAt": "2020-04-16T14:00:42Z", "author": {"login": "tillrohrmann"}, "path": "flink-yarn/src/main/java/org/apache/flink/yarn/WorkerSpecContainerResourceAdapter.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.yarn;\n+\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.runtime.clusterframework.TaskExecutorProcessSpec;\n+import org.apache.flink.runtime.clusterframework.TaskExecutorProcessUtils;\n+import org.apache.flink.runtime.resourcemanager.WorkerResourceSpec;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.yarn.api.records.Resource;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import javax.annotation.Nullable;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Utility class for converting between Flink {@link WorkerResourceSpec} and Yarn {@link Resource}.\n+ */\n+public class WorkerSpecContainerResourceAdapter {\n+\tprivate final Logger log = LoggerFactory.getLogger(WorkerSpecContainerResourceAdapter.class);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "149f0715bdc453d010789292dcfd8e6de796af55"}, "originalPosition": 48}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0Mjk3MzQxOnYy", "diffSide": "RIGHT", "path": "flink-yarn/src/main/java/org/apache/flink/yarn/WorkerSpecContainerResourceAdapter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQxNDowMToxNFrOGGmyqw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQxNDowMToxNFrOGGmyqw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTU4MDIwMw==", "bodyText": "nit:\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \tprivate final int minMemMB;\n          \n          \n            \n            \tprivate final int minVcore;\n          \n          \n            \n            \tprivate final int maxMemMB;\n          \n          \n            \n            \tprivate final int maxVcore;\n          \n          \n            \n            \tprivate final int minMemMB;\n          \n          \n            \n            \tprivate final int maxMemMB;\n          \n          \n            \n            \tprivate final int minVcore;\n          \n          \n            \n            \tprivate final int maxVcore;", "url": "https://github.com/apache/flink/pull/11353#discussion_r409580203", "createdAt": "2020-04-16T14:01:14Z", "author": {"login": "tillrohrmann"}, "path": "flink-yarn/src/main/java/org/apache/flink/yarn/WorkerSpecContainerResourceAdapter.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.yarn;\n+\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.runtime.clusterframework.TaskExecutorProcessSpec;\n+import org.apache.flink.runtime.clusterframework.TaskExecutorProcessUtils;\n+import org.apache.flink.runtime.resourcemanager.WorkerResourceSpec;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.yarn.api.records.Resource;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import javax.annotation.Nullable;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Utility class for converting between Flink {@link WorkerResourceSpec} and Yarn {@link Resource}.\n+ */\n+public class WorkerSpecContainerResourceAdapter {\n+\tprivate final Logger log = LoggerFactory.getLogger(WorkerSpecContainerResourceAdapter.class);\n+\n+\tprivate final Configuration flinkConfig;\n+\tprivate final int minMemMB;\n+\tprivate final int minVcore;\n+\tprivate final int maxMemMB;\n+\tprivate final int maxVcore;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "149f0715bdc453d010789292dcfd8e6de796af55"}, "originalPosition": 54}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0Mjk4MjgwOnYy", "diffSide": "RIGHT", "path": "flink-yarn/src/main/java/org/apache/flink/yarn/WorkerSpecContainerResourceAdapter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQxNDowMzoxMlrOGGm4sA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQxNDowMzoxMlrOGGm4sA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTU4MTc0NA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \t\tif (value < unitValue) {\n          \n          \n            \n            \t\t\treturn unitValue;\n          \n          \n            \n            \t\t}\n          \n          \n            \n            \n          \n          \n            \n            \t\tif (value % unitValue == 0) {\n          \n          \n            \n            \t\t\treturn value;\n          \n          \n            \n            \t\t}\n          \n          \n            \n            \n          \n          \n            \n            \t\treturn (value / unitValue + 1) * unitValue;\n          \n          \n            \n            \t\treturn MathUtils.divideRoundUp(value, unitValue) * unitValue;", "url": "https://github.com/apache/flink/pull/11353#discussion_r409581744", "createdAt": "2020-04-16T14:03:12Z", "author": {"login": "tillrohrmann"}, "path": "flink-yarn/src/main/java/org/apache/flink/yarn/WorkerSpecContainerResourceAdapter.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.yarn;\n+\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.runtime.clusterframework.TaskExecutorProcessSpec;\n+import org.apache.flink.runtime.clusterframework.TaskExecutorProcessUtils;\n+import org.apache.flink.runtime.resourcemanager.WorkerResourceSpec;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.yarn.api.records.Resource;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import javax.annotation.Nullable;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Utility class for converting between Flink {@link WorkerResourceSpec} and Yarn {@link Resource}.\n+ */\n+public class WorkerSpecContainerResourceAdapter {\n+\tprivate final Logger log = LoggerFactory.getLogger(WorkerSpecContainerResourceAdapter.class);\n+\n+\tprivate final Configuration flinkConfig;\n+\tprivate final int minMemMB;\n+\tprivate final int minVcore;\n+\tprivate final int maxMemMB;\n+\tprivate final int maxVcore;\n+\tprivate final WorkerSpecContainerResourceAdapter.MatchingStrategy matchingStrategy;\n+\tprivate final Map<WorkerResourceSpec, Resource> workerSpecToContainerResource;\n+\tprivate final Map<Resource, List<WorkerResourceSpec>> containerResourceToWorkerSpecs;\n+\tprivate final Map<Integer, Set<Resource>> containerMemoryToContainerResource;\n+\n+\t@VisibleForTesting\n+\tWorkerSpecContainerResourceAdapter(\n+\t\tfinal Configuration flinkConfig,\n+\t\tfinal int minMemMB,\n+\t\tfinal int minVcore,\n+\t\tfinal int maxMemMB,\n+\t\tfinal int maxVcore,\n+\t\tfinal WorkerSpecContainerResourceAdapter.MatchingStrategy matchingStrategy) {\n+\t\tthis.flinkConfig = Preconditions.checkNotNull(flinkConfig);\n+\t\tthis.minMemMB = minMemMB;\n+\t\tthis.minVcore = minVcore;\n+\t\tthis.maxMemMB = maxMemMB;\n+\t\tthis.maxVcore = maxVcore;\n+\t\tthis.matchingStrategy = matchingStrategy;\n+\t\tworkerSpecToContainerResource = new HashMap<>();\n+\t\tcontainerResourceToWorkerSpecs = new HashMap<>();\n+\t\tcontainerMemoryToContainerResource = new HashMap<>();\n+\t}\n+\n+\t@VisibleForTesting\n+\tOptional<Resource> getContainerResource(final WorkerResourceSpec workerResourceSpec) {\n+\t\treturn Optional.ofNullable(workerSpecToContainerResource.computeIfAbsent(\n+\t\t\tPreconditions.checkNotNull(workerResourceSpec),\n+\t\t\tthis::createAndMapContainerResource));\n+\t}\n+\n+\t@VisibleForTesting\n+\tSet<WorkerResourceSpec> getWorkerSpecs(final Resource containerResource) {\n+\t\treturn getEquivalentContainerResource(containerResource).stream()\n+\t\t\t.flatMap(resource -> containerResourceToWorkerSpecs.getOrDefault(resource, Collections.emptyList()).stream())\n+\t\t\t.collect(Collectors.toSet());\n+\t}\n+\n+\t@VisibleForTesting\n+\tSet<Resource> getEquivalentContainerResource(final Resource containerResource) {\n+\t\t// Yarn might ignore the requested vcores, depending on its configurations.\n+\t\t// In such cases, we should also not matching vcores.\n+\t\tfinal Set<Resource> equivalentContainerResources;\n+\t\tswitch (matchingStrategy) {\n+\t\t\tcase MATCH_VCORE:\n+\t\t\t\tequivalentContainerResources = Collections.singleton(containerResource);\n+\t\t\t\tbreak;\n+\t\t\tcase IGNORE_VCORE:\n+\t\t\tdefault:\n+\t\t\t\tequivalentContainerResources = containerMemoryToContainerResource\n+\t\t\t\t\t.getOrDefault(containerResource.getMemory(), Collections.emptySet());\n+\t\t\t\tbreak;\n+\t\t}\n+\t\treturn equivalentContainerResources;\n+\t}\n+\n+\t@Nullable\n+\tprivate Resource createAndMapContainerResource(final WorkerResourceSpec workerResourceSpec) {\n+\t\tfinal TaskExecutorProcessSpec taskExecutorProcessSpec =\n+\t\t\tTaskExecutorProcessUtils.processSpecFromWorkerResourceSpec(flinkConfig, workerResourceSpec);\n+\t\tfinal Resource containerResource = Resource.newInstance(\n+\t\t\tnormalize(taskExecutorProcessSpec.getTotalProcessMemorySize().getMebiBytes(), minMemMB),\n+\t\t\tnormalize(taskExecutorProcessSpec.getCpuCores().getValue().intValue(), minVcore));\n+\n+\t\tif (resourceWithinMaxAllocation(containerResource)) {\n+\t\t\tcontainerResourceToWorkerSpecs.computeIfAbsent(containerResource, ignored -> new ArrayList<>())\n+\t\t\t\t.add(workerResourceSpec);\n+\t\t\tcontainerMemoryToContainerResource.computeIfAbsent(containerResource.getMemory(), ignored -> new HashSet<>())\n+\t\t\t\t.add(containerResource);\n+\t\t\treturn containerResource;\n+\t\t} else {\n+\t\t\tlog.warn(\"Requested container resource {} exceeds yarn max allocation {}. Will not allocate resource.\",\n+\t\t\t\tcontainerResource,\n+\t\t\t\tResource.newInstance(maxMemMB, maxVcore));\n+\t\t\treturn null;\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * Normalize to the minimum integer that is greater or equal to 'value' and is integer multiple of 'unitValue'.\n+\t */\n+\tprivate int normalize(final int value, final int unitValue) {\n+\t\tif (value < unitValue) {\n+\t\t\treturn unitValue;\n+\t\t}\n+\n+\t\tif (value % unitValue == 0) {\n+\t\t\treturn value;\n+\t\t}\n+\n+\t\treturn (value / unitValue + 1) * unitValue;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "149f0715bdc453d010789292dcfd8e6de796af55"}, "originalPosition": 145}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0Mjk5NTEzOnYy", "diffSide": "RIGHT", "path": "flink-yarn/src/main/java/org/apache/flink/yarn/WorkerSpecContainerResourceAdapter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQxNDowNTo0MFrOGGnASw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQxNDowNTo0MFrOGGnASw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTU4MzY5MQ==", "bodyText": "I think we don't need the @VisibleForTesting annotations in this class because we did not increase the visibility of these methods for testing purposes.", "url": "https://github.com/apache/flink/pull/11353#discussion_r409583691", "createdAt": "2020-04-16T14:05:40Z", "author": {"login": "tillrohrmann"}, "path": "flink-yarn/src/main/java/org/apache/flink/yarn/WorkerSpecContainerResourceAdapter.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.yarn;\n+\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.runtime.clusterframework.TaskExecutorProcessSpec;\n+import org.apache.flink.runtime.clusterframework.TaskExecutorProcessUtils;\n+import org.apache.flink.runtime.resourcemanager.WorkerResourceSpec;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.yarn.api.records.Resource;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import javax.annotation.Nullable;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Utility class for converting between Flink {@link WorkerResourceSpec} and Yarn {@link Resource}.\n+ */\n+public class WorkerSpecContainerResourceAdapter {\n+\tprivate final Logger log = LoggerFactory.getLogger(WorkerSpecContainerResourceAdapter.class);\n+\n+\tprivate final Configuration flinkConfig;\n+\tprivate final int minMemMB;\n+\tprivate final int minVcore;\n+\tprivate final int maxMemMB;\n+\tprivate final int maxVcore;\n+\tprivate final WorkerSpecContainerResourceAdapter.MatchingStrategy matchingStrategy;\n+\tprivate final Map<WorkerResourceSpec, Resource> workerSpecToContainerResource;\n+\tprivate final Map<Resource, List<WorkerResourceSpec>> containerResourceToWorkerSpecs;\n+\tprivate final Map<Integer, Set<Resource>> containerMemoryToContainerResource;\n+\n+\t@VisibleForTesting", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "149f0715bdc453d010789292dcfd8e6de796af55"}, "originalPosition": 60}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0MzAyODc1OnYy", "diffSide": "RIGHT", "path": "flink-yarn/src/main/java/org/apache/flink/yarn/configuration/YarnConfigOptionsInternal.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQxNDoxMjozN1rOGGnVYg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQxNDoxMjozN1rOGGnVYg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTU4OTA5MA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \t * **DO NO USE** Whether {@link YarnResourceManager} should match the vcores of allocated containers with those requested.\n          \n          \n            \n            \t * **DO NOT USE** Whether {@link YarnResourceManager} should match the vcores of allocated containers with those requested.", "url": "https://github.com/apache/flink/pull/11353#discussion_r409589090", "createdAt": "2020-04-16T14:12:37Z", "author": {"login": "tillrohrmann"}, "path": "flink-yarn/src/main/java/org/apache/flink/yarn/configuration/YarnConfigOptionsInternal.java", "diffHunk": "@@ -34,4 +37,27 @@\n \t\t\t\t\t.stringType()\n \t\t\t\t\t.noDefaultValue()\n \t\t\t\t\t.withDescription(\"**DO NOT USE** The location of the log config file, e.g. the path to your log4j.properties for log4j.\");\n+\n+\t/**\n+\t * **DO NO USE** Whether {@link YarnResourceManager} should match the vcores of allocated containers with those requested.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "149f0715bdc453d010789292dcfd8e6de796af55"}, "originalPosition": 16}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0MzA0ODI1OnYy", "diffSide": "RIGHT", "path": "flink-yarn/src/main/java/org/apache/flink/yarn/WorkerSpecContainerResourceAdapter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQxNDoxNjoyOFrOGGnhfQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQxNDoxNjoyOFrOGGnhfQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTU5MjE4OQ==", "bodyText": "Maybe rename into tryComputeContainerResource or so because this method is not simply a look up. It is rather a creation call with a caching mechanism.", "url": "https://github.com/apache/flink/pull/11353#discussion_r409592189", "createdAt": "2020-04-16T14:16:28Z", "author": {"login": "tillrohrmann"}, "path": "flink-yarn/src/main/java/org/apache/flink/yarn/WorkerSpecContainerResourceAdapter.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.yarn;\n+\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.runtime.clusterframework.TaskExecutorProcessSpec;\n+import org.apache.flink.runtime.clusterframework.TaskExecutorProcessUtils;\n+import org.apache.flink.runtime.resourcemanager.WorkerResourceSpec;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.yarn.api.records.Resource;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import javax.annotation.Nullable;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Utility class for converting between Flink {@link WorkerResourceSpec} and Yarn {@link Resource}.\n+ */\n+public class WorkerSpecContainerResourceAdapter {\n+\tprivate final Logger log = LoggerFactory.getLogger(WorkerSpecContainerResourceAdapter.class);\n+\n+\tprivate final Configuration flinkConfig;\n+\tprivate final int minMemMB;\n+\tprivate final int minVcore;\n+\tprivate final int maxMemMB;\n+\tprivate final int maxVcore;\n+\tprivate final WorkerSpecContainerResourceAdapter.MatchingStrategy matchingStrategy;\n+\tprivate final Map<WorkerResourceSpec, Resource> workerSpecToContainerResource;\n+\tprivate final Map<Resource, List<WorkerResourceSpec>> containerResourceToWorkerSpecs;\n+\tprivate final Map<Integer, Set<Resource>> containerMemoryToContainerResource;\n+\n+\t@VisibleForTesting\n+\tWorkerSpecContainerResourceAdapter(\n+\t\tfinal Configuration flinkConfig,\n+\t\tfinal int minMemMB,\n+\t\tfinal int minVcore,\n+\t\tfinal int maxMemMB,\n+\t\tfinal int maxVcore,\n+\t\tfinal WorkerSpecContainerResourceAdapter.MatchingStrategy matchingStrategy) {\n+\t\tthis.flinkConfig = Preconditions.checkNotNull(flinkConfig);\n+\t\tthis.minMemMB = minMemMB;\n+\t\tthis.minVcore = minVcore;\n+\t\tthis.maxMemMB = maxMemMB;\n+\t\tthis.maxVcore = maxVcore;\n+\t\tthis.matchingStrategy = matchingStrategy;\n+\t\tworkerSpecToContainerResource = new HashMap<>();\n+\t\tcontainerResourceToWorkerSpecs = new HashMap<>();\n+\t\tcontainerMemoryToContainerResource = new HashMap<>();\n+\t}\n+\n+\t@VisibleForTesting\n+\tOptional<Resource> getContainerResource(final WorkerResourceSpec workerResourceSpec) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "149f0715bdc453d010789292dcfd8e6de796af55"}, "originalPosition": 80}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0MzA2NjEyOnYy", "diffSide": "RIGHT", "path": "flink-yarn/src/test/java/org/apache/flink/yarn/WorkerSpecContainerResourceAdapterTest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQxNDoyMDoxMlrOGGns9g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQxNDoyMDoxMlrOGGns9g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTU5NTEyNg==", "bodyText": "I think contains and containsInAnyOrder already do the size check meaning the that list must contain exactly as many items as matchers are specified.", "url": "https://github.com/apache/flink/pull/11353#discussion_r409595126", "createdAt": "2020-04-16T14:20:12Z", "author": {"login": "tillrohrmann"}, "path": "flink-yarn/src/test/java/org/apache/flink/yarn/WorkerSpecContainerResourceAdapterTest.java", "diffHunk": "@@ -0,0 +1,213 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.yarn;\n+\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.configuration.MemorySize;\n+import org.apache.flink.configuration.TaskManagerOptions;\n+import org.apache.flink.runtime.resourcemanager.WorkerResourceSpec;\n+import org.apache.flink.util.TestLogger;\n+\n+import org.apache.hadoop.yarn.api.records.Resource;\n+import org.junit.Test;\n+\n+import static org.hamcrest.Matchers.contains;\n+import static org.hamcrest.Matchers.containsInAnyOrder;\n+import static org.hamcrest.Matchers.empty;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.is;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertThat;\n+\n+/**\n+ * Tests for {@link WorkerSpecContainerResourceAdapter}.\n+ */\n+public class WorkerSpecContainerResourceAdapterTest extends TestLogger {\n+\n+\t@Test\n+\tpublic void testMatchVcores() {\n+\t\tfinal int minMemMB = 100;\n+\t\tfinal int minVcore = 10;\n+\t\tfinal WorkerSpecContainerResourceAdapter adapter =\n+\t\t\tnew WorkerSpecContainerResourceAdapter(\n+\t\t\t\tgetConfigProcessSpecEqualsWorkerSpec(),\n+\t\t\t\tminMemMB,\n+\t\t\t\tminVcore,\n+\t\t\t\tInteger.MAX_VALUE,\n+\t\t\t\tInteger.MAX_VALUE,\n+\t\t\t\tWorkerSpecContainerResourceAdapter.MatchingStrategy.MATCH_VCORE);\n+\n+\t\tfinal WorkerResourceSpec workerSpec1 = new WorkerResourceSpec.Builder()\n+\t\t\t.setCpuCores(1.0)\n+\t\t\t.setTaskHeapMemoryMB(10)\n+\t\t\t.setTaskOffHeapMemoryMB(10)\n+\t\t\t.setNetworkMemoryMB(10)\n+\t\t\t.setManagedMemoryMB(10)\n+\t\t\t.build();\n+\t\tfinal WorkerResourceSpec workerSpec2 = new WorkerResourceSpec.Builder()\n+\t\t\t.setCpuCores(10.0)\n+\t\t\t.setTaskHeapMemoryMB(25)\n+\t\t\t.setTaskOffHeapMemoryMB(25)\n+\t\t\t.setNetworkMemoryMB(25)\n+\t\t\t.setManagedMemoryMB(25)\n+\t\t\t.build();\n+\t\tfinal WorkerResourceSpec workerSpec3 = new WorkerResourceSpec.Builder()\n+\t\t\t.setCpuCores(5.0)\n+\t\t\t.setTaskHeapMemoryMB(30)\n+\t\t\t.setTaskOffHeapMemoryMB(30)\n+\t\t\t.setNetworkMemoryMB(30)\n+\t\t\t.setManagedMemoryMB(30)\n+\t\t\t.build();\n+\t\tfinal WorkerResourceSpec workerSpec4 = new WorkerResourceSpec.Builder()\n+\t\t\t.setCpuCores(15.0)\n+\t\t\t.setTaskHeapMemoryMB(10)\n+\t\t\t.setTaskOffHeapMemoryMB(10)\n+\t\t\t.setNetworkMemoryMB(10)\n+\t\t\t.setManagedMemoryMB(10)\n+\t\t\t.build();\n+\n+\t\tfinal Resource containerResource1 = Resource.newInstance(100, 10);\n+\t\tfinal Resource containerResource2 = Resource.newInstance(200, 10);\n+\t\tfinal Resource containerResource3 = Resource.newInstance(100, 20);\n+\n+\t\tassertThat(adapter.getWorkerSpecs(containerResource1), empty());\n+\t\tassertThat(adapter.getWorkerSpecs(containerResource2), empty());\n+\n+\t\tassertThat(adapter.getContainerResource(workerSpec1).get(), is(containerResource1));\n+\t\tassertThat(adapter.getContainerResource(workerSpec2).get(), is(containerResource1));\n+\t\tassertThat(adapter.getContainerResource(workerSpec3).get(), is(containerResource2));\n+\t\tassertThat(adapter.getContainerResource(workerSpec4).get(), is(containerResource3));\n+\n+\t\tassertThat(adapter.getWorkerSpecs(containerResource1), hasSize(2));\n+\t\tassertThat(adapter.getWorkerSpecs(containerResource2), hasSize(1));\n+\t\tassertThat(adapter.getWorkerSpecs(containerResource3), hasSize(1));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "149f0715bdc453d010789292dcfd8e6de796af55"}, "originalPosition": 99}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0MzA3NTE5OnYy", "diffSide": "RIGHT", "path": "flink-yarn/src/main/java/org/apache/flink/yarn/WorkerSpecContainerResourceAdapter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQxNDoyMjoxMFrOGGny2g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQxNDoyMjoxMFrOGGny2g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTU5NjYzNA==", "bodyText": "Can the value type be a set?\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \tprivate final Map<Resource, List<WorkerResourceSpec>> containerResourceToWorkerSpecs;\n          \n          \n            \n            \tprivate final Map<Resource, Set<WorkerResourceSpec>> containerResourceToWorkerSpecs;", "url": "https://github.com/apache/flink/pull/11353#discussion_r409596634", "createdAt": "2020-04-16T14:22:10Z", "author": {"login": "tillrohrmann"}, "path": "flink-yarn/src/main/java/org/apache/flink/yarn/WorkerSpecContainerResourceAdapter.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.yarn;\n+\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.runtime.clusterframework.TaskExecutorProcessSpec;\n+import org.apache.flink.runtime.clusterframework.TaskExecutorProcessUtils;\n+import org.apache.flink.runtime.resourcemanager.WorkerResourceSpec;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.yarn.api.records.Resource;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import javax.annotation.Nullable;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Utility class for converting between Flink {@link WorkerResourceSpec} and Yarn {@link Resource}.\n+ */\n+public class WorkerSpecContainerResourceAdapter {\n+\tprivate final Logger log = LoggerFactory.getLogger(WorkerSpecContainerResourceAdapter.class);\n+\n+\tprivate final Configuration flinkConfig;\n+\tprivate final int minMemMB;\n+\tprivate final int minVcore;\n+\tprivate final int maxMemMB;\n+\tprivate final int maxVcore;\n+\tprivate final WorkerSpecContainerResourceAdapter.MatchingStrategy matchingStrategy;\n+\tprivate final Map<WorkerResourceSpec, Resource> workerSpecToContainerResource;\n+\tprivate final Map<Resource, List<WorkerResourceSpec>> containerResourceToWorkerSpecs;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "149f0715bdc453d010789292dcfd8e6de796af55"}, "originalPosition": 57}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0MzE0ODQxOnYy", "diffSide": "RIGHT", "path": "flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQxNDozNzoyMlrOGGohpg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQxNDozNzoyMlrOGGohpg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTYwODYxNA==", "bodyText": "forEach is always an indicator that one could do the same with a while loop. In this case I would suggest to keep it simple and stupid:\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \t\tgetNumberRequiredWorkersPerWorkerResourceSpec().entrySet().stream()\n          \n          \n            \n            \t\t\t.filter(entry -> entry.getValue() > getNumPendingWorkersFor(entry.getKey()))\n          \n          \n            \n            \t\t\t.forEach(entry -> requestYarnContainer(entry.getKey()));\n          \n          \n            \n            \t\tfor (Map.Entry<WorkerResourceSpec, Integer> requiredWorkersPerResourceSpec : getNumberRequiredWorkersPerWorkerResourceSpec().entrySet()) {\n          \n          \n            \n            \t\t\tfinal WorkerResourceSpec workerResourceSpec = requiredWorkersPerResourceSpec.getKey();\n          \n          \n            \n            \t\t\twhile (requiredWorkersPerResourceSpec.getValue() > getNumPendingWorkersFor(workerResourceSpec)) {\n          \n          \n            \n            \t\t\t\trequestYarnContainer(workerResourceSpec);\n          \n          \n            \n            \t\t\t}\n          \n          \n            \n            \t\t}", "url": "https://github.com/apache/flink/pull/11353#discussion_r409608614", "createdAt": "2020-04-16T14:37:22Z", "author": {"login": "tillrohrmann"}, "path": "flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java", "diffHunk": "@@ -552,38 +581,44 @@ private FinalApplicationStatus getYarnStatus(ApplicationStatus status) {\n \t * Request new container if pending containers cannot satisfy pending slot requests.\n \t */\n \tprivate void requestYarnContainerIfRequired() {\n-\t\tint requiredTaskManagers = getNumberRequiredWorkers();\n-\n-\t\twhile (requiredTaskManagers-- > numPendingContainerRequests) {\n-\t\t\trequestYarnContainer();\n-\t\t}\n+\t\tgetNumberRequiredWorkersPerWorkerResourceSpec().entrySet().stream()\n+\t\t\t.filter(entry -> entry.getValue() > getNumPendingWorkersFor(entry.getKey()))\n+\t\t\t.forEach(entry -> requestYarnContainer(entry.getKey()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "112fe6526c4e11aabe35b0a72c310536e9e6c8c0"}, "originalPosition": 220}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0MzE2MDY1OnYy", "diffSide": "RIGHT", "path": "flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQxNDo0MDowMFrOGGopnA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQxNDo0MDowMFrOGGopnA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTYxMDY1Mg==", "bodyText": "I assume that requestYarnContainer should never return false here, right? If this is the case, then let's add a checkState to ensure this invariant.", "url": "https://github.com/apache/flink/pull/11353#discussion_r409610652", "createdAt": "2020-04-16T14:40:00Z", "author": {"login": "tillrohrmann"}, "path": "flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java", "diffHunk": "@@ -552,38 +581,44 @@ private FinalApplicationStatus getYarnStatus(ApplicationStatus status) {\n \t * Request new container if pending containers cannot satisfy pending slot requests.\n \t */\n \tprivate void requestYarnContainerIfRequired() {\n-\t\tint requiredTaskManagers = getNumberRequiredWorkers();\n-\n-\t\twhile (requiredTaskManagers-- > numPendingContainerRequests) {\n-\t\t\trequestYarnContainer();\n-\t\t}\n+\t\tgetNumberRequiredWorkersPerWorkerResourceSpec().entrySet().stream()\n+\t\t\t.filter(entry -> entry.getValue() > getNumPendingWorkersFor(entry.getKey()))\n+\t\t\t.forEach(entry -> requestYarnContainer(entry.getKey()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "112fe6526c4e11aabe35b0a72c310536e9e6c8c0"}, "originalPosition": 220}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0MzE2NzU0OnYy", "diffSide": "RIGHT", "path": "flink-yarn/src/test/java/org/apache/flink/yarn/YarnResourceManagerTest.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQxNDo0MToyOFrOGGouEw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMFQxNDoxNTozNlrOGIWtGw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTYxMTc5NQ==", "bodyText": "Why not moving this block into the default constructor and removing the @Nullable annotation from slotManager?", "url": "https://github.com/apache/flink/pull/11353#discussion_r409611795", "createdAt": "2020-04-16T14:41:28Z", "author": {"login": "tillrohrmann"}, "path": "flink-yarn/src/test/java/org/apache/flink/yarn/YarnResourceManagerTest.java", "diffHunk": "@@ -243,24 +248,33 @@ protected NMClientAsync createAndStartNodeManagerClient(YarnConfiguration yarnCo\n \n \t\t// domain objects for test purposes\n \t\tfinal ResourceProfile resourceProfile1 = ResourceProfile.UNKNOWN;\n+\t\tfinal WorkerResourceSpec workerResourceSpec;\n+\n+\t\tfinal Resource containerResource;\n \n \t\tpublic String taskHost = \"host1\";\n \n \t\tfinal TestingYarnNMClientAsync testingYarnNMClientAsync;\n \n \t\tfinal TestingYarnAMRMClientAsync testingYarnAMRMClientAsync;\n \n+\t\tint containerIdx = 0;\n+\n \t\t/**\n \t\t * Create mock RM dependencies.\n \t\t */\n \t\tContext() throws Exception {\n-\t\t\tthis(flinkConfig);\n+\t\t\tthis(flinkConfig, null);\n \t\t}\n \n-\t\tContext(Configuration configuration) throws  Exception {\n-\t\t\tfinal SlotManager slotManager = SlotManagerBuilder.newBuilder()\n-\t\t\t\t.setDefaultWorkerResourceSpec(YarnWorkerResourceSpecFactory.INSTANCE.createDefaultWorkerResourceSpec(configuration))\n-\t\t\t\t.build();\n+\t\tContext(Configuration configuration, @Nullable SlotManager slotManager) throws  Exception {\n+\n+\t\t\tworkerResourceSpec = YarnWorkerResourceSpecFactory.INSTANCE.createDefaultWorkerResourceSpec(configuration);\n+\t\t\tif (slotManager == null) {\n+\t\t\t\tslotManager = SlotManagerBuilder.newBuilder()\n+\t\t\t\t\t.setDefaultWorkerResourceSpec(workerResourceSpec)\n+\t\t\t\t\t.build();\n+\t\t\t}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "112fe6526c4e11aabe35b0a72c310536e9e6c8c0"}, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDg1MTU2MA==", "bodyText": "It requires workerResourceSpec for creating the default slotManager.\nI was trying to avoid creating workerResourceSpec twice, in both constructors.", "url": "https://github.com/apache/flink/pull/11353#discussion_r410851560", "createdAt": "2020-04-19T08:33:44Z", "author": {"login": "xintongsong"}, "path": "flink-yarn/src/test/java/org/apache/flink/yarn/YarnResourceManagerTest.java", "diffHunk": "@@ -243,24 +248,33 @@ protected NMClientAsync createAndStartNodeManagerClient(YarnConfiguration yarnCo\n \n \t\t// domain objects for test purposes\n \t\tfinal ResourceProfile resourceProfile1 = ResourceProfile.UNKNOWN;\n+\t\tfinal WorkerResourceSpec workerResourceSpec;\n+\n+\t\tfinal Resource containerResource;\n \n \t\tpublic String taskHost = \"host1\";\n \n \t\tfinal TestingYarnNMClientAsync testingYarnNMClientAsync;\n \n \t\tfinal TestingYarnAMRMClientAsync testingYarnAMRMClientAsync;\n \n+\t\tint containerIdx = 0;\n+\n \t\t/**\n \t\t * Create mock RM dependencies.\n \t\t */\n \t\tContext() throws Exception {\n-\t\t\tthis(flinkConfig);\n+\t\t\tthis(flinkConfig, null);\n \t\t}\n \n-\t\tContext(Configuration configuration) throws  Exception {\n-\t\t\tfinal SlotManager slotManager = SlotManagerBuilder.newBuilder()\n-\t\t\t\t.setDefaultWorkerResourceSpec(YarnWorkerResourceSpecFactory.INSTANCE.createDefaultWorkerResourceSpec(configuration))\n-\t\t\t\t.build();\n+\t\tContext(Configuration configuration, @Nullable SlotManager slotManager) throws  Exception {\n+\n+\t\t\tworkerResourceSpec = YarnWorkerResourceSpecFactory.INSTANCE.createDefaultWorkerResourceSpec(configuration);\n+\t\t\tif (slotManager == null) {\n+\t\t\t\tslotManager = SlotManagerBuilder.newBuilder()\n+\t\t\t\t\t.setDefaultWorkerResourceSpec(workerResourceSpec)\n+\t\t\t\t\t.build();\n+\t\t\t}", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTYxMTc5NQ=="}, "originalCommit": {"oid": "112fe6526c4e11aabe35b0a72c310536e9e6c8c0"}, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTQxMzc4Nw==", "bodyText": "I think it would be fine to add a SlotManager.getDefaultWorkerResourceSpec(). This could solve the problem here.", "url": "https://github.com/apache/flink/pull/11353#discussion_r411413787", "createdAt": "2020-04-20T14:15:36Z", "author": {"login": "tillrohrmann"}, "path": "flink-yarn/src/test/java/org/apache/flink/yarn/YarnResourceManagerTest.java", "diffHunk": "@@ -243,24 +248,33 @@ protected NMClientAsync createAndStartNodeManagerClient(YarnConfiguration yarnCo\n \n \t\t// domain objects for test purposes\n \t\tfinal ResourceProfile resourceProfile1 = ResourceProfile.UNKNOWN;\n+\t\tfinal WorkerResourceSpec workerResourceSpec;\n+\n+\t\tfinal Resource containerResource;\n \n \t\tpublic String taskHost = \"host1\";\n \n \t\tfinal TestingYarnNMClientAsync testingYarnNMClientAsync;\n \n \t\tfinal TestingYarnAMRMClientAsync testingYarnAMRMClientAsync;\n \n+\t\tint containerIdx = 0;\n+\n \t\t/**\n \t\t * Create mock RM dependencies.\n \t\t */\n \t\tContext() throws Exception {\n-\t\t\tthis(flinkConfig);\n+\t\t\tthis(flinkConfig, null);\n \t\t}\n \n-\t\tContext(Configuration configuration) throws  Exception {\n-\t\t\tfinal SlotManager slotManager = SlotManagerBuilder.newBuilder()\n-\t\t\t\t.setDefaultWorkerResourceSpec(YarnWorkerResourceSpecFactory.INSTANCE.createDefaultWorkerResourceSpec(configuration))\n-\t\t\t\t.build();\n+\t\tContext(Configuration configuration, @Nullable SlotManager slotManager) throws  Exception {\n+\n+\t\t\tworkerResourceSpec = YarnWorkerResourceSpecFactory.INSTANCE.createDefaultWorkerResourceSpec(configuration);\n+\t\t\tif (slotManager == null) {\n+\t\t\t\tslotManager = SlotManagerBuilder.newBuilder()\n+\t\t\t\t\t.setDefaultWorkerResourceSpec(workerResourceSpec)\n+\t\t\t\t\t.build();\n+\t\t\t}", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTYxMTc5NQ=="}, "originalCommit": {"oid": "112fe6526c4e11aabe35b0a72c310536e9e6c8c0"}, "originalPosition": 68}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0MzIxNjg5OnYy", "diffSide": "RIGHT", "path": "flink-yarn/src/main/java/org/apache/flink/yarn/RegisterApplicationMasterResponseReflector.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQxNDo1MToyNFrOGGpNsA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQxNDo1MToyNFrOGGpNsA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTYxOTg4OA==", "bodyText": "One could think about factoring the lookup of methods and the logging statement out into a another method to avoid code duplication.", "url": "https://github.com/apache/flink/pull/11353#discussion_r409619888", "createdAt": "2020-04-16T14:51:24Z", "author": {"login": "tillrohrmann"}, "path": "flink-yarn/src/main/java/org/apache/flink/yarn/RegisterApplicationMasterResponseReflector.java", "diffHunk": "@@ -53,12 +59,20 @@\n \t\trequireNonNull(clazz);\n \n \t\ttry {\n-\t\t\tmethod = clazz.getMethod(\"getContainersFromPreviousAttempts\");\n+\t\t\tgetContainersFromPreviousAttemptsMethod = clazz.getMethod(\"getContainersFromPreviousAttempts\");\n \t\t} catch (NoSuchMethodException e) {\n \t\t\t// that happens in earlier Hadoop versions (pre 2.2)\n \t\t\tlogger.info(\"Cannot reconnect to previously allocated containers. \" +\n \t\t\t\t\"This YARN version does not support 'getContainersFromPreviousAttempts()'\");\n \t\t}\n+\n+\t\ttry {\n+\t\t\tgetSchedulerResourceTypesMethod = clazz.getMethod(\"getSchedulerResourceTypes\");\n+\t\t} catch (NoSuchMethodException e) {\n+\t\t\t// that happens in earlier Hadoop versions (pre 2.6)\n+\t\t\tlogger.info(\"Cannot get scheduler resource types. \" +\n+\t\t\t\t\"This YARN version does not support 'getSchedulerResourceTypes()'\");\n+\t\t}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "82d5dd9966f887470f20377d618c78a88f25dd82"}, "originalPosition": 55}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0MzIzMTE2OnYy", "diffSide": "RIGHT", "path": "flink-yarn/src/main/java/org/apache/flink/yarn/RegisterApplicationMasterResponseReflector.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQxNDo1NDowNFrOGGpWpA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMFQxNDoyNjozNVrOGIXPrQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTYyMjE4MA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \tOptional<Set<String>> getSchedulerResourceTypeNamesUnsafe(final Object response) {\n          \n          \n            \n            \tprivate Optional<Set<String>> getSchedulerResourceTypeNamesUnsafe(final Object response) {\n          \n      \n    \n    \n  \n\nand remove @VisibleForTesting.", "url": "https://github.com/apache/flink/pull/11353#discussion_r409622180", "createdAt": "2020-04-16T14:54:04Z", "author": {"login": "tillrohrmann"}, "path": "flink-yarn/src/main/java/org/apache/flink/yarn/RegisterApplicationMasterResponseReflector.java", "diffHunk": "@@ -96,7 +110,40 @@\n \t}\n \n \t@VisibleForTesting\n-\tMethod getMethod() {\n-\t\treturn method;\n+\tMethod getGetContainersFromPreviousAttemptsMethod() {\n+\t\treturn getContainersFromPreviousAttemptsMethod;\n+\t}\n+\n+\t/**\n+\t * Get names of resource types that are considered by the Yarn scheduler.\n+\t * @param response The response object from the registration at the ResourceManager.\n+\t * @return A set of resource type names, or {@link Optional#empty()} if the Yarn version does not support this API.\n+\t */\n+\tOptional<Set<String>> getSchedulerResourceTypeNames(final RegisterApplicationMasterResponse response) {\n+\t\treturn getSchedulerResourceTypeNamesUnsafe(response);\n+\t}\n+\n+\t@VisibleForTesting\n+\tOptional<Set<String>> getSchedulerResourceTypeNamesUnsafe(final Object response) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "82d5dd9966f887470f20377d618c78a88f25dd82"}, "originalPosition": 92}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTQyMjYzNw==", "bodyText": "Won't do because of testing purposes.", "url": "https://github.com/apache/flink/pull/11353#discussion_r411422637", "createdAt": "2020-04-20T14:26:35Z", "author": {"login": "tillrohrmann"}, "path": "flink-yarn/src/main/java/org/apache/flink/yarn/RegisterApplicationMasterResponseReflector.java", "diffHunk": "@@ -96,7 +110,40 @@\n \t}\n \n \t@VisibleForTesting\n-\tMethod getMethod() {\n-\t\treturn method;\n+\tMethod getGetContainersFromPreviousAttemptsMethod() {\n+\t\treturn getContainersFromPreviousAttemptsMethod;\n+\t}\n+\n+\t/**\n+\t * Get names of resource types that are considered by the Yarn scheduler.\n+\t * @param response The response object from the registration at the ResourceManager.\n+\t * @return A set of resource type names, or {@link Optional#empty()} if the Yarn version does not support this API.\n+\t */\n+\tOptional<Set<String>> getSchedulerResourceTypeNames(final RegisterApplicationMasterResponse response) {\n+\t\treturn getSchedulerResourceTypeNamesUnsafe(response);\n+\t}\n+\n+\t@VisibleForTesting\n+\tOptional<Set<String>> getSchedulerResourceTypeNamesUnsafe(final Object response) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTYyMjE4MA=="}, "originalCommit": {"oid": "82d5dd9966f887470f20377d618c78a88f25dd82"}, "originalPosition": 92}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0MzI0NDg4OnYy", "diffSide": "RIGHT", "path": "flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQxNDo1Njo1MlrOGGpfRg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQxNTowNDoyM1rOGGp2qQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTYyNDM5MA==", "bodyText": "Either annotate with @Nullable and add the corresponding null checks or initialize with a mock WorkerSpecContainerResourceAdapter which fails on every call with an exception saying that the RM has not been properly initialized.", "url": "https://github.com/apache/flink/pull/11353#discussion_r409624390", "createdAt": "2020-04-16T14:56:52Z", "author": {"login": "tillrohrmann"}, "path": "flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java", "diffHunk": "@@ -117,7 +118,9 @@\n \t/** Client to communicate with the Node manager and launch TaskExecutor processes. */\n \tprivate NMClientAsync nodeManagerClient;\n \n-\tprivate final WorkerSpecContainerResourceAdapter workerSpecContainerResourceAdapter;\n+\tprivate WorkerSpecContainerResourceAdapter workerSpecContainerResourceAdapter = null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "82d5dd9966f887470f20377d618c78a88f25dd82"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTYzMDM3Nw==", "bodyText": "An idea to keep the WorkerSpecContainerResourceAdapter final is to pass in the WorkerSpecContainerResourceAdapter.MatchingStrategy to the method getWorkerSpecs. If you look at the WorkerSpecContainerResourceAdapter class then one also sees that the matching strategy is not really an essential part of it. Only the lookup method changes its behaviour based on it. Everything else stays the same.", "url": "https://github.com/apache/flink/pull/11353#discussion_r409630377", "createdAt": "2020-04-16T15:04:23Z", "author": {"login": "tillrohrmann"}, "path": "flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java", "diffHunk": "@@ -117,7 +118,9 @@\n \t/** Client to communicate with the Node manager and launch TaskExecutor processes. */\n \tprivate NMClientAsync nodeManagerClient;\n \n-\tprivate final WorkerSpecContainerResourceAdapter workerSpecContainerResourceAdapter;\n+\tprivate WorkerSpecContainerResourceAdapter workerSpecContainerResourceAdapter = null;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTYyNDM5MA=="}, "originalCommit": {"oid": "82d5dd9966f887470f20377d618c78a88f25dd82"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0MzI2NDQzOnYy", "diffSide": "RIGHT", "path": "flink-yarn/src/test/java/org/apache/flink/yarn/RegisterApplicationMasterResponseReflectorTest.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQxNTowMDo1M1rOGGpr_w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMFQxNDoyNDozOFrOGIXJiA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTYyNzY0Nw==", "bodyText": "Instead of calling private methods of the RegisterApplicationMasterResponseReflector we could also add an assumeTrue statement based on the Hadoop version. Then we can have two tests for Hadoop >= 2.6 and < 2.6.", "url": "https://github.com/apache/flink/pull/11353#discussion_r409627647", "createdAt": "2020-04-16T15:00:53Z", "author": {"login": "tillrohrmann"}, "path": "flink-yarn/src/test/java/org/apache/flink/yarn/RegisterApplicationMasterResponseReflectorTest.java", "diffHunk": "@@ -88,7 +94,44 @@ public void testGetMethodReflectiveHadoop22() {\n \t\tfinal RegisterApplicationMasterResponseReflector registerApplicationMasterResponseReflector =\n \t\t\tnew RegisterApplicationMasterResponseReflector(LOG);\n \n-\t\tfinal Method method = registerApplicationMasterResponseReflector.getMethod();\n+\t\tfinal Method method = registerApplicationMasterResponseReflector.getGetContainersFromPreviousAttemptsMethod();\n+\t\tassertThat(method, notNullValue());\n+\t}\n+\n+\t@Test\n+\tpublic void testCallsGetSchedulerResourceTypesMethodIfPresent() {\n+\t\tfinal RegisterApplicationMasterResponseReflector registerApplicationMasterResponseReflector =\n+\t\t\tnew RegisterApplicationMasterResponseReflector(LOG, HasMethod.class);\n+\n+\t\tfinal Optional<Set<String>> schedulerResourceTypeNames =\n+\t\t\tregisterApplicationMasterResponseReflector.getSchedulerResourceTypeNamesUnsafe(new HasMethod());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "82d5dd9966f887470f20377d618c78a88f25dd82"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDg1ODQzNQ==", "bodyText": "I'm not sure about this. My concern is that, the <2.6 test case might eventually not executed in most cases.\nTake testDoesntCallGetContainersFromPreviousAttemptsMethodIfAbsent as an example. If we make this test case only executed with Hadoop <2.2, then it's practically not executed. Currently we have Hadoop 2.8 for travis ci test, 2.4 & 2.8 for nightly test, and 2.4 (pom default) for local maven verify unless another version is intentionally specified.", "url": "https://github.com/apache/flink/pull/11353#discussion_r410858435", "createdAt": "2020-04-19T09:12:21Z", "author": {"login": "xintongsong"}, "path": "flink-yarn/src/test/java/org/apache/flink/yarn/RegisterApplicationMasterResponseReflectorTest.java", "diffHunk": "@@ -88,7 +94,44 @@ public void testGetMethodReflectiveHadoop22() {\n \t\tfinal RegisterApplicationMasterResponseReflector registerApplicationMasterResponseReflector =\n \t\t\tnew RegisterApplicationMasterResponseReflector(LOG);\n \n-\t\tfinal Method method = registerApplicationMasterResponseReflector.getMethod();\n+\t\tfinal Method method = registerApplicationMasterResponseReflector.getGetContainersFromPreviousAttemptsMethod();\n+\t\tassertThat(method, notNullValue());\n+\t}\n+\n+\t@Test\n+\tpublic void testCallsGetSchedulerResourceTypesMethodIfPresent() {\n+\t\tfinal RegisterApplicationMasterResponseReflector registerApplicationMasterResponseReflector =\n+\t\t\tnew RegisterApplicationMasterResponseReflector(LOG, HasMethod.class);\n+\n+\t\tfinal Optional<Set<String>> schedulerResourceTypeNames =\n+\t\t\tregisterApplicationMasterResponseReflector.getSchedulerResourceTypeNamesUnsafe(new HasMethod());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTYyNzY0Nw=="}, "originalCommit": {"oid": "82d5dd9966f887470f20377d618c78a88f25dd82"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTQyMTA2NA==", "bodyText": "Makes sense. I agree that your proposal is better.", "url": "https://github.com/apache/flink/pull/11353#discussion_r411421064", "createdAt": "2020-04-20T14:24:38Z", "author": {"login": "tillrohrmann"}, "path": "flink-yarn/src/test/java/org/apache/flink/yarn/RegisterApplicationMasterResponseReflectorTest.java", "diffHunk": "@@ -88,7 +94,44 @@ public void testGetMethodReflectiveHadoop22() {\n \t\tfinal RegisterApplicationMasterResponseReflector registerApplicationMasterResponseReflector =\n \t\t\tnew RegisterApplicationMasterResponseReflector(LOG);\n \n-\t\tfinal Method method = registerApplicationMasterResponseReflector.getMethod();\n+\t\tfinal Method method = registerApplicationMasterResponseReflector.getGetContainersFromPreviousAttemptsMethod();\n+\t\tassertThat(method, notNullValue());\n+\t}\n+\n+\t@Test\n+\tpublic void testCallsGetSchedulerResourceTypesMethodIfPresent() {\n+\t\tfinal RegisterApplicationMasterResponseReflector registerApplicationMasterResponseReflector =\n+\t\t\tnew RegisterApplicationMasterResponseReflector(LOG, HasMethod.class);\n+\n+\t\tfinal Optional<Set<String>> schedulerResourceTypeNames =\n+\t\t\tregisterApplicationMasterResponseReflector.getSchedulerResourceTypeNamesUnsafe(new HasMethod());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTYyNzY0Nw=="}, "originalCommit": {"oid": "82d5dd9966f887470f20377d618c78a88f25dd82"}, "originalPosition": 62}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 899, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}