{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDAzMTMwMzU2", "number": 11735, "reviewThreads": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwNzoxMzo1N1rOD0mWdg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwNzoyMTozOFrOD0mjgg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU2NDgwODg2OnYy", "diffSide": "RIGHT", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveTableInputFormat.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwNzoxMzo1N1rOGJnAUw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwNzo0NTowOVrOGJoO4g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjcyOTQyNw==", "bodyText": "Need this branch?\nWe can just:\njobConf.set(SCHEMA_EVOLUTION_COLUMNS, String.join(\",\", Arrays.copyOfRange(fieldNames, 0, firstPartColIndex)));\njobConf.set(SCHEMA_EVOLUTION_COLUMNS_TYPES, String.join(\",\", typeStrs.subList(0, firstPartColIndex)));", "url": "https://github.com/apache/flink/pull/11735#discussion_r412729427", "createdAt": "2020-04-22T07:13:57Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveTableInputFormat.java", "diffHunk": "@@ -122,12 +131,40 @@ public void open(HiveTableInputSplit split) throws IOException {\n \t\t\tthis.reader = new HiveVectorizedParquetSplitReader(\n \t\t\t\t\thiveVersion, jobConf, fieldNames, fieldTypes, selectedFields, split);\n \t\t} else {\n-\t\t\tthis.reader = new HiveMapredSplitReader(jobConf, partitionKeys, fieldTypes, selectedFields, split,\n+\t\t\tJobConf clonedConf = new JobConf(jobConf);\n+\t\t\taddSchemaToConf(clonedConf);\n+\t\t\tthis.reader = new HiveMapredSplitReader(clonedConf, partitionKeys, fieldTypes, selectedFields, split,\n \t\t\t\t\tHiveShimLoader.loadHiveShim(hiveVersion));\n \t\t}\n \t\tcurrentReadCount = 0L;\n \t}\n \n+\t// Hive readers may rely on the schema info in configuration\n+\tprivate void addSchemaToConf(JobConf jobConf) {\n+\t\t// set columns/types -- including partition cols\n+\t\tList<String> typeStrs = Arrays.stream(fieldTypes)\n+\t\t\t\t.map(t -> HiveTypeUtil.toHiveTypeInfo(t, true).toString())\n+\t\t\t\t.collect(Collectors.toList());\n+\t\tjobConf.set(IOConstants.COLUMNS, String.join(\",\", fieldNames));\n+\t\tjobConf.set(IOConstants.COLUMNS_TYPES, String.join(\",\", typeStrs));\n+\t\t// set schema evolution -- excluding partition cols\n+\t\tint numPartCol = partitionKeys != null ? partitionKeys.size() : 0;\n+\t\tint firstPartColIndex = fieldNames.length - numPartCol;\n+\t\tif (numPartCol == 0) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60d86e4a86f0afefe1bfbf398b1d28f139ca97f2"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjc0OTUzOA==", "bodyText": "Yeah I added the branch to avoid array copies if the table is not partitioned. But perhaps terseness is more desirable here.", "url": "https://github.com/apache/flink/pull/11735#discussion_r412749538", "createdAt": "2020-04-22T07:45:09Z", "author": {"login": "lirui-apache"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveTableInputFormat.java", "diffHunk": "@@ -122,12 +131,40 @@ public void open(HiveTableInputSplit split) throws IOException {\n \t\t\tthis.reader = new HiveVectorizedParquetSplitReader(\n \t\t\t\t\thiveVersion, jobConf, fieldNames, fieldTypes, selectedFields, split);\n \t\t} else {\n-\t\t\tthis.reader = new HiveMapredSplitReader(jobConf, partitionKeys, fieldTypes, selectedFields, split,\n+\t\t\tJobConf clonedConf = new JobConf(jobConf);\n+\t\t\taddSchemaToConf(clonedConf);\n+\t\t\tthis.reader = new HiveMapredSplitReader(clonedConf, partitionKeys, fieldTypes, selectedFields, split,\n \t\t\t\t\tHiveShimLoader.loadHiveShim(hiveVersion));\n \t\t}\n \t\tcurrentReadCount = 0L;\n \t}\n \n+\t// Hive readers may rely on the schema info in configuration\n+\tprivate void addSchemaToConf(JobConf jobConf) {\n+\t\t// set columns/types -- including partition cols\n+\t\tList<String> typeStrs = Arrays.stream(fieldTypes)\n+\t\t\t\t.map(t -> HiveTypeUtil.toHiveTypeInfo(t, true).toString())\n+\t\t\t\t.collect(Collectors.toList());\n+\t\tjobConf.set(IOConstants.COLUMNS, String.join(\",\", fieldNames));\n+\t\tjobConf.set(IOConstants.COLUMNS_TYPES, String.join(\",\", typeStrs));\n+\t\t// set schema evolution -- excluding partition cols\n+\t\tint numPartCol = partitionKeys != null ? partitionKeys.size() : 0;\n+\t\tint firstPartColIndex = fieldNames.length - numPartCol;\n+\t\tif (numPartCol == 0) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjcyOTQyNw=="}, "originalCommit": {"oid": "60d86e4a86f0afefe1bfbf398b1d28f139ca97f2"}, "originalPosition": 60}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU2NDgzNzM3OnYy", "diffSide": "RIGHT", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveTableInputFormat.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwNzoyMDozNVrOGJnQoQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwNzoyMDozNVrOGJnQoQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjczMzYwMQ==", "bodyText": "partitionKeys never null", "url": "https://github.com/apache/flink/pull/11735#discussion_r412733601", "createdAt": "2020-04-22T07:20:35Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveTableInputFormat.java", "diffHunk": "@@ -122,12 +131,40 @@ public void open(HiveTableInputSplit split) throws IOException {\n \t\t\tthis.reader = new HiveVectorizedParquetSplitReader(\n \t\t\t\t\thiveVersion, jobConf, fieldNames, fieldTypes, selectedFields, split);\n \t\t} else {\n-\t\t\tthis.reader = new HiveMapredSplitReader(jobConf, partitionKeys, fieldTypes, selectedFields, split,\n+\t\t\tJobConf clonedConf = new JobConf(jobConf);\n+\t\t\taddSchemaToConf(clonedConf);\n+\t\t\tthis.reader = new HiveMapredSplitReader(clonedConf, partitionKeys, fieldTypes, selectedFields, split,\n \t\t\t\t\tHiveShimLoader.loadHiveShim(hiveVersion));\n \t\t}\n \t\tcurrentReadCount = 0L;\n \t}\n \n+\t// Hive readers may rely on the schema info in configuration\n+\tprivate void addSchemaToConf(JobConf jobConf) {\n+\t\t// set columns/types -- including partition cols\n+\t\tList<String> typeStrs = Arrays.stream(fieldTypes)\n+\t\t\t\t.map(t -> HiveTypeUtil.toHiveTypeInfo(t, true).toString())\n+\t\t\t\t.collect(Collectors.toList());\n+\t\tjobConf.set(IOConstants.COLUMNS, String.join(\",\", fieldNames));\n+\t\tjobConf.set(IOConstants.COLUMNS_TYPES, String.join(\",\", typeStrs));\n+\t\t// set schema evolution -- excluding partition cols\n+\t\tint numPartCol = partitionKeys != null ? partitionKeys.size() : 0;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60d86e4a86f0afefe1bfbf398b1d28f139ca97f2"}, "originalPosition": 58}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU2NDg0MjI2OnYy", "diffSide": "RIGHT", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveTableInputFormat.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwNzoyMTozOFrOGJnTXw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwNzoyMTozOFrOGJnTXw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjczNDMwMw==", "bodyText": "numNonPartCol?", "url": "https://github.com/apache/flink/pull/11735#discussion_r412734303", "createdAt": "2020-04-22T07:21:38Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveTableInputFormat.java", "diffHunk": "@@ -122,12 +131,40 @@ public void open(HiveTableInputSplit split) throws IOException {\n \t\t\tthis.reader = new HiveVectorizedParquetSplitReader(\n \t\t\t\t\thiveVersion, jobConf, fieldNames, fieldTypes, selectedFields, split);\n \t\t} else {\n-\t\t\tthis.reader = new HiveMapredSplitReader(jobConf, partitionKeys, fieldTypes, selectedFields, split,\n+\t\t\tJobConf clonedConf = new JobConf(jobConf);\n+\t\t\taddSchemaToConf(clonedConf);\n+\t\t\tthis.reader = new HiveMapredSplitReader(clonedConf, partitionKeys, fieldTypes, selectedFields, split,\n \t\t\t\t\tHiveShimLoader.loadHiveShim(hiveVersion));\n \t\t}\n \t\tcurrentReadCount = 0L;\n \t}\n \n+\t// Hive readers may rely on the schema info in configuration\n+\tprivate void addSchemaToConf(JobConf jobConf) {\n+\t\t// set columns/types -- including partition cols\n+\t\tList<String> typeStrs = Arrays.stream(fieldTypes)\n+\t\t\t\t.map(t -> HiveTypeUtil.toHiveTypeInfo(t, true).toString())\n+\t\t\t\t.collect(Collectors.toList());\n+\t\tjobConf.set(IOConstants.COLUMNS, String.join(\",\", fieldNames));\n+\t\tjobConf.set(IOConstants.COLUMNS_TYPES, String.join(\",\", typeStrs));\n+\t\t// set schema evolution -- excluding partition cols\n+\t\tint numPartCol = partitionKeys != null ? partitionKeys.size() : 0;\n+\t\tint firstPartColIndex = fieldNames.length - numPartCol;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60d86e4a86f0afefe1bfbf398b1d28f139ca97f2"}, "originalPosition": 59}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1613, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}