{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDMxNTA5OTYw", "number": 12537, "reviewThreads": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQwOTo1NjoxMlrOEDzRpg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQwMzo1MToyNlrOEEiDJA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcyNDIxMjg2OnYy", "diffSide": "RIGHT", "path": "docs/dev/table/hive/hive_streaming.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQwOTo1NjoxMlrOGhB-VQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQwOTo1NjoxMlrOGhB-VQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzI4ODUzMw==", "bodyText": "\u2019true\u2019 -> '", "url": "https://github.com/apache/flink/pull/12537#discussion_r437288533", "createdAt": "2020-06-09T09:56:12Z", "author": {"login": "JingsongLi"}, "path": "docs/dev/table/hive/hive_streaming.md", "diffHunk": "@@ -0,0 +1,164 @@\n+---\n+title: \"Hive Streaming\"\n+nav-parent_id: hive_tableapi\n+nav-pos: 2\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+A typical hive job is scheduled periodically to execute, so there will be a large delay.\n+\n+Flink supports to write, read and join the hive table in the form of streaming.\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+There are three types of streaming:\n+\n+- Writing streaming data into Hive table.\n+- Reading Hive table incrementally in the form of streaming.\n+- Streaming table join Hive table using [Temporal Table]({{ site.baseurl }}/dev/table/streaming/temporal_tables.html#temporal-table).\n+\n+## Streaming Writing\n+\n+The Hive table supports streaming writes, based on [Filesystem Streaming Sink]({{ site.baseurl }}/dev/table/connectors/filesystem.html#streaming-sink).\n+\n+The Hive Streaming Sink re-use Filesystem Streaming Sink to integrate Hadoop OutputFormat/RecordWriter to streaming writing.\n+Hadoop RecordWriters are Bulk-encoded Formats, Bulk Formats rolls files on every checkpoint.\n+\n+By default, now only have renaming committer, this means S3 filesystem can not supports exactly-once,\n+if you want to use Hive streaming sink in S3 filesystem, You can configure the following parameters\n+in `TableConfig` (note that these parameters affect all sinks of the job):\n+\n+<table class=\"table table-bordered\">\n+  <thead>\n+    <tr>\n+        <th class=\"text-left\" style=\"width: 20%\">Key</th>\n+        <th class=\"text-left\" style=\"width: 15%\">Default</th>\n+        <th class=\"text-left\" style=\"width: 10%\">Type</th>\n+        <th class=\"text-left\" style=\"width: 55%\">Description</th>\n+    </tr>\n+  </thead>\n+  <tbody>\n+    <tr>\n+        <td><h5>table.exec.hive.fallback-mapred-writer</h5></td>\n+        <td style=\"word-wrap: break-word;\">true</td>\n+        <td>Boolean</td>\n+        <td>If it is false, using flink native writer to write parquet and orc files; if it is true, using hadoop mapred record writer to write parquet and orc files.</td>\n+    </tr>\n+  </tbody>\n+</table>\n+\n+The below shows how the streaming sink can be used to write a streaming query to write data from Kafka into a Hive table with partition-commit,\n+and runs a batch query to read that data back out. \n+\n+{% highlight sql %}\n+\n+SET table.sql-dialect=hive;\n+CREATE TABLE hive_table (\n+  user_id STRING,\n+  order_amount DOUBLE\n+) PARTITIONED BY (dt STRING, hour STRING) STORED AS parquet TBLPROPERTIES (\n+  'partition.time-extractor.timestamp-pattern'='$dt $hour:00:00',\n+  'sink.partition-commit.trigger'='partition-time',\n+  'sink.partition-commit.delay'='1 h',\n+  'sink.partition-commit.policy.kind'='metastore,success-file'\n+);\n+\n+SET table.sql-dialect=default;\n+CREATE TABLE kafka_table (\n+  user_id STRING,\n+  order_amount DOUBLE,\n+  log_ts TIMESTAMP(3),\n+  WATERMARK FOR log_ts AS log_ts - INTERVAL '5' SECOND\n+) WITH (...);\n+\n+-- streaming sql, insert into hive table\n+INSERT INTO TABLE hive_table SELECT user_id, order_amount, DATE_FORMAT(log_ts, 'yyyy-MM-dd'), DATE_FORMAT(log_ts, 'HH') FROM kafka_table;\n+\n+-- batch sql, select with partition pruning\n+SELECT * FROM hive_table WHERE dt='2020-05-20' and hour='12';\n+\n+{% endhighlight %}\n+\n+## Streaming Reading\n+\n+To improve the real-time performance of hive reading, Flink support real-time Hive table stream read:\n+\n+- Partition table, monitor the generation of partition, and read the new partition incrementally.\n+- Non-partition table, monitor the generation of new files in the folder, and read new files incrementally.\n+\n+You can even use the 10 minute level partition strategy, and use Flink's Hive streaming reading and\n+Hive streaming writing to greatly improve the real-time performance of Hive data warehouse to quasi\n+real-time minute level.\n+\n+<table class=\"table table-bordered\">\n+  <thead>\n+    <tr>\n+        <th class=\"text-left\" style=\"width: 20%\">Key</th>\n+        <th class=\"text-left\" style=\"width: 15%\">Default</th>\n+        <th class=\"text-left\" style=\"width: 10%\">Type</th>\n+        <th class=\"text-left\" style=\"width: 55%\">Description</th>\n+    </tr>\n+  </thead>\n+  <tbody>\n+    <tr>\n+        <td><h5>streaming-source.enable</h5></td>\n+        <td style=\"word-wrap: break-word;\">false</td>\n+        <td>Boolean</td>\n+        <td>Enable streaming source or not. NOTES: For non-partition table, please make sure that each file should be put atomically into the target directory, otherwise the reader may get incomplete data.</td>\n+    </tr>\n+    <tr>\n+        <td><h5>streaming-source.monitor-interval</h5></td>\n+        <td style=\"word-wrap: break-word;\">1 m</td>\n+        <td>Duration</td>\n+        <td>Time interval for consecutively monitoring partition/file.</td>\n+    </tr>\n+    <tr>\n+        <td><h5>streaming-source.consume-order</h5></td>\n+        <td style=\"word-wrap: break-word;\">create-time</td>\n+        <td>String</td>\n+        <td>The consume order of streaming source, support create-time and partition-time. create-time compare partition/file creation time, this is not the partition create time in Hive metaStore, but the folder/file create time in filesystem; partition-time compare time represented by partition name. For non-partition table, this value should always be 'create-time'.</td>\n+    </tr>\n+    <tr>\n+        <td><h5>streaming-source.consume-start-offset</h5></td>\n+        <td style=\"word-wrap: break-word;\">1970-00-00</td>\n+        <td>String</td>\n+        <td>Start offset for streaming consuming. How to parse and compare offsets depends on your order. For create-time and partition-time, should be a timestamp string. For partition-time, will use partition time extractor to extract time from partition.</td>\n+    </tr>\n+  </tbody>\n+</table>\n+\n+Note:\n+\n+- Monitor strategy is scan all directories/files in location path now. If there is too more partitions, will have performance problems.\n+- Streaming reading requires that the file or partition is atomic in the view of hive metastore and does not support append writing.\n+- Streaming reading not support watermark grammar in Flink DDL. So it can not be used for window operators.\n+\n+The below shows how to read Hive table incrementally. \n+\n+{% highlight sql %}\n+\n+SELECT * FROM hive_table /*+ OPTIONS('streaming-source.enable'=\u2019true\u2019, 'streaming-source.consume-start-offset'='2020-05-20') */;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "765b588c806f6b08e3f9116d8ef9817d1ecd14ba"}, "originalPosition": 158}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczMTczODIzOnYy", "diffSide": "RIGHT", "path": "docs/dev/table/hive/hive_streaming.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQwMjoxNzoyOFrOGiMQMA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQwMjoxNzoyOFrOGiMQMA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODUwNTUyMA==", "bodyText": "We should clarify what value should be set for the following parameter. And mention that it only works for Orc and Parquet tables.", "url": "https://github.com/apache/flink/pull/12537#discussion_r438505520", "createdAt": "2020-06-11T02:17:28Z", "author": {"login": "lirui-apache"}, "path": "docs/dev/table/hive/hive_streaming.md", "diffHunk": "@@ -0,0 +1,164 @@\n+---\n+title: \"Hive Streaming\"\n+nav-parent_id: hive_tableapi\n+nav-pos: 2\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+A typical hive job is scheduled periodically to execute, so there will be a large delay.\n+\n+Flink supports to write, read and join the hive table in the form of streaming.\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+There are three types of streaming:\n+\n+- Writing streaming data into Hive table.\n+- Reading Hive table incrementally in the form of streaming.\n+- Streaming table join Hive table using [Temporal Table]({{ site.baseurl }}/dev/table/streaming/temporal_tables.html#temporal-table).\n+\n+## Streaming Writing\n+\n+The Hive table supports streaming writes, based on [Filesystem Streaming Sink]({{ site.baseurl }}/dev/table/connectors/filesystem.html#streaming-sink).\n+\n+The Hive Streaming Sink re-use Filesystem Streaming Sink to integrate Hadoop OutputFormat/RecordWriter to streaming writing.\n+Hadoop RecordWriters are Bulk-encoded Formats, Bulk Formats rolls files on every checkpoint.\n+\n+By default, now only have renaming committer, this means S3 filesystem can not supports exactly-once,\n+if you want to use Hive streaming sink in S3 filesystem, You can configure the following parameters", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "765b588c806f6b08e3f9116d8ef9817d1ecd14ba"}, "originalPosition": 46}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczMTg2NDMwOnYy", "diffSide": "RIGHT", "path": "docs/dev/table/hive/hive_streaming.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQwMzo0Mjo1NVrOGiNfMw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQwNzowNTozNlrOGiRBHQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODUyNTc0Nw==", "bodyText": "We need to mention that new data added to an existing partition won't be consumed", "url": "https://github.com/apache/flink/pull/12537#discussion_r438525747", "createdAt": "2020-06-11T03:42:55Z", "author": {"login": "lirui-apache"}, "path": "docs/dev/table/hive/hive_streaming.md", "diffHunk": "@@ -0,0 +1,164 @@\n+---\n+title: \"Hive Streaming\"\n+nav-parent_id: hive_tableapi\n+nav-pos: 2\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+A typical hive job is scheduled periodically to execute, so there will be a large delay.\n+\n+Flink supports to write, read and join the hive table in the form of streaming.\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+There are three types of streaming:\n+\n+- Writing streaming data into Hive table.\n+- Reading Hive table incrementally in the form of streaming.\n+- Streaming table join Hive table using [Temporal Table]({{ site.baseurl }}/dev/table/streaming/temporal_tables.html#temporal-table).\n+\n+## Streaming Writing\n+\n+The Hive table supports streaming writes, based on [Filesystem Streaming Sink]({{ site.baseurl }}/dev/table/connectors/filesystem.html#streaming-sink).\n+\n+The Hive Streaming Sink re-use Filesystem Streaming Sink to integrate Hadoop OutputFormat/RecordWriter to streaming writing.\n+Hadoop RecordWriters are Bulk-encoded Formats, Bulk Formats rolls files on every checkpoint.\n+\n+By default, now only have renaming committer, this means S3 filesystem can not supports exactly-once,\n+if you want to use Hive streaming sink in S3 filesystem, You can configure the following parameters\n+in `TableConfig` (note that these parameters affect all sinks of the job):\n+\n+<table class=\"table table-bordered\">\n+  <thead>\n+    <tr>\n+        <th class=\"text-left\" style=\"width: 20%\">Key</th>\n+        <th class=\"text-left\" style=\"width: 15%\">Default</th>\n+        <th class=\"text-left\" style=\"width: 10%\">Type</th>\n+        <th class=\"text-left\" style=\"width: 55%\">Description</th>\n+    </tr>\n+  </thead>\n+  <tbody>\n+    <tr>\n+        <td><h5>table.exec.hive.fallback-mapred-writer</h5></td>\n+        <td style=\"word-wrap: break-word;\">true</td>\n+        <td>Boolean</td>\n+        <td>If it is false, using flink native writer to write parquet and orc files; if it is true, using hadoop mapred record writer to write parquet and orc files.</td>\n+    </tr>\n+  </tbody>\n+</table>\n+\n+The below shows how the streaming sink can be used to write a streaming query to write data from Kafka into a Hive table with partition-commit,\n+and runs a batch query to read that data back out. \n+\n+{% highlight sql %}\n+\n+SET table.sql-dialect=hive;\n+CREATE TABLE hive_table (\n+  user_id STRING,\n+  order_amount DOUBLE\n+) PARTITIONED BY (dt STRING, hour STRING) STORED AS parquet TBLPROPERTIES (\n+  'partition.time-extractor.timestamp-pattern'='$dt $hour:00:00',\n+  'sink.partition-commit.trigger'='partition-time',\n+  'sink.partition-commit.delay'='1 h',\n+  'sink.partition-commit.policy.kind'='metastore,success-file'\n+);\n+\n+SET table.sql-dialect=default;\n+CREATE TABLE kafka_table (\n+  user_id STRING,\n+  order_amount DOUBLE,\n+  log_ts TIMESTAMP(3),\n+  WATERMARK FOR log_ts AS log_ts - INTERVAL '5' SECOND\n+) WITH (...);\n+\n+-- streaming sql, insert into hive table\n+INSERT INTO TABLE hive_table SELECT user_id, order_amount, DATE_FORMAT(log_ts, 'yyyy-MM-dd'), DATE_FORMAT(log_ts, 'HH') FROM kafka_table;\n+\n+-- batch sql, select with partition pruning\n+SELECT * FROM hive_table WHERE dt='2020-05-20' and hour='12';\n+\n+{% endhighlight %}\n+\n+## Streaming Reading\n+\n+To improve the real-time performance of hive reading, Flink support real-time Hive table stream read:\n+\n+- Partition table, monitor the generation of partition, and read the new partition incrementally.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "765b588c806f6b08e3f9116d8ef9817d1ecd14ba"}, "originalPosition": 104}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODU4MzU4MQ==", "bodyText": "I will explain more in NOTE.", "url": "https://github.com/apache/flink/pull/12537#discussion_r438583581", "createdAt": "2020-06-11T07:05:36Z", "author": {"login": "JingsongLi"}, "path": "docs/dev/table/hive/hive_streaming.md", "diffHunk": "@@ -0,0 +1,164 @@\n+---\n+title: \"Hive Streaming\"\n+nav-parent_id: hive_tableapi\n+nav-pos: 2\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+A typical hive job is scheduled periodically to execute, so there will be a large delay.\n+\n+Flink supports to write, read and join the hive table in the form of streaming.\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+There are three types of streaming:\n+\n+- Writing streaming data into Hive table.\n+- Reading Hive table incrementally in the form of streaming.\n+- Streaming table join Hive table using [Temporal Table]({{ site.baseurl }}/dev/table/streaming/temporal_tables.html#temporal-table).\n+\n+## Streaming Writing\n+\n+The Hive table supports streaming writes, based on [Filesystem Streaming Sink]({{ site.baseurl }}/dev/table/connectors/filesystem.html#streaming-sink).\n+\n+The Hive Streaming Sink re-use Filesystem Streaming Sink to integrate Hadoop OutputFormat/RecordWriter to streaming writing.\n+Hadoop RecordWriters are Bulk-encoded Formats, Bulk Formats rolls files on every checkpoint.\n+\n+By default, now only have renaming committer, this means S3 filesystem can not supports exactly-once,\n+if you want to use Hive streaming sink in S3 filesystem, You can configure the following parameters\n+in `TableConfig` (note that these parameters affect all sinks of the job):\n+\n+<table class=\"table table-bordered\">\n+  <thead>\n+    <tr>\n+        <th class=\"text-left\" style=\"width: 20%\">Key</th>\n+        <th class=\"text-left\" style=\"width: 15%\">Default</th>\n+        <th class=\"text-left\" style=\"width: 10%\">Type</th>\n+        <th class=\"text-left\" style=\"width: 55%\">Description</th>\n+    </tr>\n+  </thead>\n+  <tbody>\n+    <tr>\n+        <td><h5>table.exec.hive.fallback-mapred-writer</h5></td>\n+        <td style=\"word-wrap: break-word;\">true</td>\n+        <td>Boolean</td>\n+        <td>If it is false, using flink native writer to write parquet and orc files; if it is true, using hadoop mapred record writer to write parquet and orc files.</td>\n+    </tr>\n+  </tbody>\n+</table>\n+\n+The below shows how the streaming sink can be used to write a streaming query to write data from Kafka into a Hive table with partition-commit,\n+and runs a batch query to read that data back out. \n+\n+{% highlight sql %}\n+\n+SET table.sql-dialect=hive;\n+CREATE TABLE hive_table (\n+  user_id STRING,\n+  order_amount DOUBLE\n+) PARTITIONED BY (dt STRING, hour STRING) STORED AS parquet TBLPROPERTIES (\n+  'partition.time-extractor.timestamp-pattern'='$dt $hour:00:00',\n+  'sink.partition-commit.trigger'='partition-time',\n+  'sink.partition-commit.delay'='1 h',\n+  'sink.partition-commit.policy.kind'='metastore,success-file'\n+);\n+\n+SET table.sql-dialect=default;\n+CREATE TABLE kafka_table (\n+  user_id STRING,\n+  order_amount DOUBLE,\n+  log_ts TIMESTAMP(3),\n+  WATERMARK FOR log_ts AS log_ts - INTERVAL '5' SECOND\n+) WITH (...);\n+\n+-- streaming sql, insert into hive table\n+INSERT INTO TABLE hive_table SELECT user_id, order_amount, DATE_FORMAT(log_ts, 'yyyy-MM-dd'), DATE_FORMAT(log_ts, 'HH') FROM kafka_table;\n+\n+-- batch sql, select with partition pruning\n+SELECT * FROM hive_table WHERE dt='2020-05-20' and hour='12';\n+\n+{% endhighlight %}\n+\n+## Streaming Reading\n+\n+To improve the real-time performance of hive reading, Flink support real-time Hive table stream read:\n+\n+- Partition table, monitor the generation of partition, and read the new partition incrementally.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODUyNTc0Nw=="}, "originalCommit": {"oid": "765b588c806f6b08e3f9116d8ef9817d1ecd14ba"}, "originalPosition": 104}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczMTg2NTYyOnYy", "diffSide": "RIGHT", "path": "docs/dev/table/hive/hive_streaming.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQwMzo0NDowN1rOGiNgDA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQwNzowMTo0OFrOGiQ66A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODUyNTk2NA==", "bodyText": "For partitioned table, I think each partition should also be added atomically right?", "url": "https://github.com/apache/flink/pull/12537#discussion_r438525964", "createdAt": "2020-06-11T03:44:07Z", "author": {"login": "lirui-apache"}, "path": "docs/dev/table/hive/hive_streaming.md", "diffHunk": "@@ -0,0 +1,164 @@\n+---\n+title: \"Hive Streaming\"\n+nav-parent_id: hive_tableapi\n+nav-pos: 2\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+A typical hive job is scheduled periodically to execute, so there will be a large delay.\n+\n+Flink supports to write, read and join the hive table in the form of streaming.\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+There are three types of streaming:\n+\n+- Writing streaming data into Hive table.\n+- Reading Hive table incrementally in the form of streaming.\n+- Streaming table join Hive table using [Temporal Table]({{ site.baseurl }}/dev/table/streaming/temporal_tables.html#temporal-table).\n+\n+## Streaming Writing\n+\n+The Hive table supports streaming writes, based on [Filesystem Streaming Sink]({{ site.baseurl }}/dev/table/connectors/filesystem.html#streaming-sink).\n+\n+The Hive Streaming Sink re-use Filesystem Streaming Sink to integrate Hadoop OutputFormat/RecordWriter to streaming writing.\n+Hadoop RecordWriters are Bulk-encoded Formats, Bulk Formats rolls files on every checkpoint.\n+\n+By default, now only have renaming committer, this means S3 filesystem can not supports exactly-once,\n+if you want to use Hive streaming sink in S3 filesystem, You can configure the following parameters\n+in `TableConfig` (note that these parameters affect all sinks of the job):\n+\n+<table class=\"table table-bordered\">\n+  <thead>\n+    <tr>\n+        <th class=\"text-left\" style=\"width: 20%\">Key</th>\n+        <th class=\"text-left\" style=\"width: 15%\">Default</th>\n+        <th class=\"text-left\" style=\"width: 10%\">Type</th>\n+        <th class=\"text-left\" style=\"width: 55%\">Description</th>\n+    </tr>\n+  </thead>\n+  <tbody>\n+    <tr>\n+        <td><h5>table.exec.hive.fallback-mapred-writer</h5></td>\n+        <td style=\"word-wrap: break-word;\">true</td>\n+        <td>Boolean</td>\n+        <td>If it is false, using flink native writer to write parquet and orc files; if it is true, using hadoop mapred record writer to write parquet and orc files.</td>\n+    </tr>\n+  </tbody>\n+</table>\n+\n+The below shows how the streaming sink can be used to write a streaming query to write data from Kafka into a Hive table with partition-commit,\n+and runs a batch query to read that data back out. \n+\n+{% highlight sql %}\n+\n+SET table.sql-dialect=hive;\n+CREATE TABLE hive_table (\n+  user_id STRING,\n+  order_amount DOUBLE\n+) PARTITIONED BY (dt STRING, hour STRING) STORED AS parquet TBLPROPERTIES (\n+  'partition.time-extractor.timestamp-pattern'='$dt $hour:00:00',\n+  'sink.partition-commit.trigger'='partition-time',\n+  'sink.partition-commit.delay'='1 h',\n+  'sink.partition-commit.policy.kind'='metastore,success-file'\n+);\n+\n+SET table.sql-dialect=default;\n+CREATE TABLE kafka_table (\n+  user_id STRING,\n+  order_amount DOUBLE,\n+  log_ts TIMESTAMP(3),\n+  WATERMARK FOR log_ts AS log_ts - INTERVAL '5' SECOND\n+) WITH (...);\n+\n+-- streaming sql, insert into hive table\n+INSERT INTO TABLE hive_table SELECT user_id, order_amount, DATE_FORMAT(log_ts, 'yyyy-MM-dd'), DATE_FORMAT(log_ts, 'HH') FROM kafka_table;\n+\n+-- batch sql, select with partition pruning\n+SELECT * FROM hive_table WHERE dt='2020-05-20' and hour='12';\n+\n+{% endhighlight %}\n+\n+## Streaming Reading\n+\n+To improve the real-time performance of hive reading, Flink support real-time Hive table stream read:\n+\n+- Partition table, monitor the generation of partition, and read the new partition incrementally.\n+- Non-partition table, monitor the generation of new files in the folder, and read new files incrementally.\n+\n+You can even use the 10 minute level partition strategy, and use Flink's Hive streaming reading and\n+Hive streaming writing to greatly improve the real-time performance of Hive data warehouse to quasi\n+real-time minute level.\n+\n+<table class=\"table table-bordered\">\n+  <thead>\n+    <tr>\n+        <th class=\"text-left\" style=\"width: 20%\">Key</th>\n+        <th class=\"text-left\" style=\"width: 15%\">Default</th>\n+        <th class=\"text-left\" style=\"width: 10%\">Type</th>\n+        <th class=\"text-left\" style=\"width: 55%\">Description</th>\n+    </tr>\n+  </thead>\n+  <tbody>\n+    <tr>\n+        <td><h5>streaming-source.enable</h5></td>\n+        <td style=\"word-wrap: break-word;\">false</td>\n+        <td>Boolean</td>\n+        <td>Enable streaming source or not. NOTES: For non-partition table, please make sure that each file should be put atomically into the target directory, otherwise the reader may get incomplete data.</td>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "765b588c806f6b08e3f9116d8ef9817d1ecd14ba"}, "originalPosition": 125}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODU4MTk5Mg==", "bodyText": "Yes, will be modified to:\nEnable streaming source or not.NOTES: Please make sure that each partition/file should be written atomically, otherwise the reader may get incomplete data.", "url": "https://github.com/apache/flink/pull/12537#discussion_r438581992", "createdAt": "2020-06-11T07:01:48Z", "author": {"login": "JingsongLi"}, "path": "docs/dev/table/hive/hive_streaming.md", "diffHunk": "@@ -0,0 +1,164 @@\n+---\n+title: \"Hive Streaming\"\n+nav-parent_id: hive_tableapi\n+nav-pos: 2\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+A typical hive job is scheduled periodically to execute, so there will be a large delay.\n+\n+Flink supports to write, read and join the hive table in the form of streaming.\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+There are three types of streaming:\n+\n+- Writing streaming data into Hive table.\n+- Reading Hive table incrementally in the form of streaming.\n+- Streaming table join Hive table using [Temporal Table]({{ site.baseurl }}/dev/table/streaming/temporal_tables.html#temporal-table).\n+\n+## Streaming Writing\n+\n+The Hive table supports streaming writes, based on [Filesystem Streaming Sink]({{ site.baseurl }}/dev/table/connectors/filesystem.html#streaming-sink).\n+\n+The Hive Streaming Sink re-use Filesystem Streaming Sink to integrate Hadoop OutputFormat/RecordWriter to streaming writing.\n+Hadoop RecordWriters are Bulk-encoded Formats, Bulk Formats rolls files on every checkpoint.\n+\n+By default, now only have renaming committer, this means S3 filesystem can not supports exactly-once,\n+if you want to use Hive streaming sink in S3 filesystem, You can configure the following parameters\n+in `TableConfig` (note that these parameters affect all sinks of the job):\n+\n+<table class=\"table table-bordered\">\n+  <thead>\n+    <tr>\n+        <th class=\"text-left\" style=\"width: 20%\">Key</th>\n+        <th class=\"text-left\" style=\"width: 15%\">Default</th>\n+        <th class=\"text-left\" style=\"width: 10%\">Type</th>\n+        <th class=\"text-left\" style=\"width: 55%\">Description</th>\n+    </tr>\n+  </thead>\n+  <tbody>\n+    <tr>\n+        <td><h5>table.exec.hive.fallback-mapred-writer</h5></td>\n+        <td style=\"word-wrap: break-word;\">true</td>\n+        <td>Boolean</td>\n+        <td>If it is false, using flink native writer to write parquet and orc files; if it is true, using hadoop mapred record writer to write parquet and orc files.</td>\n+    </tr>\n+  </tbody>\n+</table>\n+\n+The below shows how the streaming sink can be used to write a streaming query to write data from Kafka into a Hive table with partition-commit,\n+and runs a batch query to read that data back out. \n+\n+{% highlight sql %}\n+\n+SET table.sql-dialect=hive;\n+CREATE TABLE hive_table (\n+  user_id STRING,\n+  order_amount DOUBLE\n+) PARTITIONED BY (dt STRING, hour STRING) STORED AS parquet TBLPROPERTIES (\n+  'partition.time-extractor.timestamp-pattern'='$dt $hour:00:00',\n+  'sink.partition-commit.trigger'='partition-time',\n+  'sink.partition-commit.delay'='1 h',\n+  'sink.partition-commit.policy.kind'='metastore,success-file'\n+);\n+\n+SET table.sql-dialect=default;\n+CREATE TABLE kafka_table (\n+  user_id STRING,\n+  order_amount DOUBLE,\n+  log_ts TIMESTAMP(3),\n+  WATERMARK FOR log_ts AS log_ts - INTERVAL '5' SECOND\n+) WITH (...);\n+\n+-- streaming sql, insert into hive table\n+INSERT INTO TABLE hive_table SELECT user_id, order_amount, DATE_FORMAT(log_ts, 'yyyy-MM-dd'), DATE_FORMAT(log_ts, 'HH') FROM kafka_table;\n+\n+-- batch sql, select with partition pruning\n+SELECT * FROM hive_table WHERE dt='2020-05-20' and hour='12';\n+\n+{% endhighlight %}\n+\n+## Streaming Reading\n+\n+To improve the real-time performance of hive reading, Flink support real-time Hive table stream read:\n+\n+- Partition table, monitor the generation of partition, and read the new partition incrementally.\n+- Non-partition table, monitor the generation of new files in the folder, and read new files incrementally.\n+\n+You can even use the 10 minute level partition strategy, and use Flink's Hive streaming reading and\n+Hive streaming writing to greatly improve the real-time performance of Hive data warehouse to quasi\n+real-time minute level.\n+\n+<table class=\"table table-bordered\">\n+  <thead>\n+    <tr>\n+        <th class=\"text-left\" style=\"width: 20%\">Key</th>\n+        <th class=\"text-left\" style=\"width: 15%\">Default</th>\n+        <th class=\"text-left\" style=\"width: 10%\">Type</th>\n+        <th class=\"text-left\" style=\"width: 55%\">Description</th>\n+    </tr>\n+  </thead>\n+  <tbody>\n+    <tr>\n+        <td><h5>streaming-source.enable</h5></td>\n+        <td style=\"word-wrap: break-word;\">false</td>\n+        <td>Boolean</td>\n+        <td>Enable streaming source or not. NOTES: For non-partition table, please make sure that each file should be put atomically into the target directory, otherwise the reader may get incomplete data.</td>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODUyNTk2NA=="}, "originalCommit": {"oid": "765b588c806f6b08e3f9116d8ef9817d1ecd14ba"}, "originalPosition": 125}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczMTg2Nzk3OnYy", "diffSide": "RIGHT", "path": "docs/dev/table/hive/hive_streaming.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQwMzo0NTo0MFrOGiNhcg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQwMzo0NTo0MFrOGiNhcg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODUyNjMyMg==", "bodyText": "Mention the supported timestamp format is yyyy-[m]m-[d]d [hh:mm:ss]?", "url": "https://github.com/apache/flink/pull/12537#discussion_r438526322", "createdAt": "2020-06-11T03:45:40Z", "author": {"login": "lirui-apache"}, "path": "docs/dev/table/hive/hive_streaming.md", "diffHunk": "@@ -0,0 +1,164 @@\n+---\n+title: \"Hive Streaming\"\n+nav-parent_id: hive_tableapi\n+nav-pos: 2\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+A typical hive job is scheduled periodically to execute, so there will be a large delay.\n+\n+Flink supports to write, read and join the hive table in the form of streaming.\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+There are three types of streaming:\n+\n+- Writing streaming data into Hive table.\n+- Reading Hive table incrementally in the form of streaming.\n+- Streaming table join Hive table using [Temporal Table]({{ site.baseurl }}/dev/table/streaming/temporal_tables.html#temporal-table).\n+\n+## Streaming Writing\n+\n+The Hive table supports streaming writes, based on [Filesystem Streaming Sink]({{ site.baseurl }}/dev/table/connectors/filesystem.html#streaming-sink).\n+\n+The Hive Streaming Sink re-use Filesystem Streaming Sink to integrate Hadoop OutputFormat/RecordWriter to streaming writing.\n+Hadoop RecordWriters are Bulk-encoded Formats, Bulk Formats rolls files on every checkpoint.\n+\n+By default, now only have renaming committer, this means S3 filesystem can not supports exactly-once,\n+if you want to use Hive streaming sink in S3 filesystem, You can configure the following parameters\n+in `TableConfig` (note that these parameters affect all sinks of the job):\n+\n+<table class=\"table table-bordered\">\n+  <thead>\n+    <tr>\n+        <th class=\"text-left\" style=\"width: 20%\">Key</th>\n+        <th class=\"text-left\" style=\"width: 15%\">Default</th>\n+        <th class=\"text-left\" style=\"width: 10%\">Type</th>\n+        <th class=\"text-left\" style=\"width: 55%\">Description</th>\n+    </tr>\n+  </thead>\n+  <tbody>\n+    <tr>\n+        <td><h5>table.exec.hive.fallback-mapred-writer</h5></td>\n+        <td style=\"word-wrap: break-word;\">true</td>\n+        <td>Boolean</td>\n+        <td>If it is false, using flink native writer to write parquet and orc files; if it is true, using hadoop mapred record writer to write parquet and orc files.</td>\n+    </tr>\n+  </tbody>\n+</table>\n+\n+The below shows how the streaming sink can be used to write a streaming query to write data from Kafka into a Hive table with partition-commit,\n+and runs a batch query to read that data back out. \n+\n+{% highlight sql %}\n+\n+SET table.sql-dialect=hive;\n+CREATE TABLE hive_table (\n+  user_id STRING,\n+  order_amount DOUBLE\n+) PARTITIONED BY (dt STRING, hour STRING) STORED AS parquet TBLPROPERTIES (\n+  'partition.time-extractor.timestamp-pattern'='$dt $hour:00:00',\n+  'sink.partition-commit.trigger'='partition-time',\n+  'sink.partition-commit.delay'='1 h',\n+  'sink.partition-commit.policy.kind'='metastore,success-file'\n+);\n+\n+SET table.sql-dialect=default;\n+CREATE TABLE kafka_table (\n+  user_id STRING,\n+  order_amount DOUBLE,\n+  log_ts TIMESTAMP(3),\n+  WATERMARK FOR log_ts AS log_ts - INTERVAL '5' SECOND\n+) WITH (...);\n+\n+-- streaming sql, insert into hive table\n+INSERT INTO TABLE hive_table SELECT user_id, order_amount, DATE_FORMAT(log_ts, 'yyyy-MM-dd'), DATE_FORMAT(log_ts, 'HH') FROM kafka_table;\n+\n+-- batch sql, select with partition pruning\n+SELECT * FROM hive_table WHERE dt='2020-05-20' and hour='12';\n+\n+{% endhighlight %}\n+\n+## Streaming Reading\n+\n+To improve the real-time performance of hive reading, Flink support real-time Hive table stream read:\n+\n+- Partition table, monitor the generation of partition, and read the new partition incrementally.\n+- Non-partition table, monitor the generation of new files in the folder, and read new files incrementally.\n+\n+You can even use the 10 minute level partition strategy, and use Flink's Hive streaming reading and\n+Hive streaming writing to greatly improve the real-time performance of Hive data warehouse to quasi\n+real-time minute level.\n+\n+<table class=\"table table-bordered\">\n+  <thead>\n+    <tr>\n+        <th class=\"text-left\" style=\"width: 20%\">Key</th>\n+        <th class=\"text-left\" style=\"width: 15%\">Default</th>\n+        <th class=\"text-left\" style=\"width: 10%\">Type</th>\n+        <th class=\"text-left\" style=\"width: 55%\">Description</th>\n+    </tr>\n+  </thead>\n+  <tbody>\n+    <tr>\n+        <td><h5>streaming-source.enable</h5></td>\n+        <td style=\"word-wrap: break-word;\">false</td>\n+        <td>Boolean</td>\n+        <td>Enable streaming source or not. NOTES: For non-partition table, please make sure that each file should be put atomically into the target directory, otherwise the reader may get incomplete data.</td>\n+    </tr>\n+    <tr>\n+        <td><h5>streaming-source.monitor-interval</h5></td>\n+        <td style=\"word-wrap: break-word;\">1 m</td>\n+        <td>Duration</td>\n+        <td>Time interval for consecutively monitoring partition/file.</td>\n+    </tr>\n+    <tr>\n+        <td><h5>streaming-source.consume-order</h5></td>\n+        <td style=\"word-wrap: break-word;\">create-time</td>\n+        <td>String</td>\n+        <td>The consume order of streaming source, support create-time and partition-time. create-time compare partition/file creation time, this is not the partition create time in Hive metaStore, but the folder/file create time in filesystem; partition-time compare time represented by partition name. For non-partition table, this value should always be 'create-time'.</td>\n+    </tr>\n+    <tr>\n+        <td><h5>streaming-source.consume-start-offset</h5></td>\n+        <td style=\"word-wrap: break-word;\">1970-00-00</td>\n+        <td>String</td>\n+        <td>Start offset for streaming consuming. How to parse and compare offsets depends on your order. For create-time and partition-time, should be a timestamp string. For partition-time, will use partition time extractor to extract time from partition.</td>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "765b588c806f6b08e3f9116d8ef9817d1ecd14ba"}, "originalPosition": 143}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczMTg3MzEzOnYy", "diffSide": "RIGHT", "path": "docs/dev/table/hive/hive_streaming.md", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQwMzo0OToxN1rOGiNkkw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQwNzoyMTo0OFrOGiRevA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODUyNzEyMw==", "bodyText": "I think we should warn the users that the create-time here is actually the modification time in Hadoop FS. So if the partition folder somehow gets updated, e.g. changing ACL attributes, it can affect how the data is consumed.\nAnd perhaps we should make partition-time as the default and recommended value for this property?", "url": "https://github.com/apache/flink/pull/12537#discussion_r438527123", "createdAt": "2020-06-11T03:49:17Z", "author": {"login": "lirui-apache"}, "path": "docs/dev/table/hive/hive_streaming.md", "diffHunk": "@@ -0,0 +1,164 @@\n+---\n+title: \"Hive Streaming\"\n+nav-parent_id: hive_tableapi\n+nav-pos: 2\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+A typical hive job is scheduled periodically to execute, so there will be a large delay.\n+\n+Flink supports to write, read and join the hive table in the form of streaming.\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+There are three types of streaming:\n+\n+- Writing streaming data into Hive table.\n+- Reading Hive table incrementally in the form of streaming.\n+- Streaming table join Hive table using [Temporal Table]({{ site.baseurl }}/dev/table/streaming/temporal_tables.html#temporal-table).\n+\n+## Streaming Writing\n+\n+The Hive table supports streaming writes, based on [Filesystem Streaming Sink]({{ site.baseurl }}/dev/table/connectors/filesystem.html#streaming-sink).\n+\n+The Hive Streaming Sink re-use Filesystem Streaming Sink to integrate Hadoop OutputFormat/RecordWriter to streaming writing.\n+Hadoop RecordWriters are Bulk-encoded Formats, Bulk Formats rolls files on every checkpoint.\n+\n+By default, now only have renaming committer, this means S3 filesystem can not supports exactly-once,\n+if you want to use Hive streaming sink in S3 filesystem, You can configure the following parameters\n+in `TableConfig` (note that these parameters affect all sinks of the job):\n+\n+<table class=\"table table-bordered\">\n+  <thead>\n+    <tr>\n+        <th class=\"text-left\" style=\"width: 20%\">Key</th>\n+        <th class=\"text-left\" style=\"width: 15%\">Default</th>\n+        <th class=\"text-left\" style=\"width: 10%\">Type</th>\n+        <th class=\"text-left\" style=\"width: 55%\">Description</th>\n+    </tr>\n+  </thead>\n+  <tbody>\n+    <tr>\n+        <td><h5>table.exec.hive.fallback-mapred-writer</h5></td>\n+        <td style=\"word-wrap: break-word;\">true</td>\n+        <td>Boolean</td>\n+        <td>If it is false, using flink native writer to write parquet and orc files; if it is true, using hadoop mapred record writer to write parquet and orc files.</td>\n+    </tr>\n+  </tbody>\n+</table>\n+\n+The below shows how the streaming sink can be used to write a streaming query to write data from Kafka into a Hive table with partition-commit,\n+and runs a batch query to read that data back out. \n+\n+{% highlight sql %}\n+\n+SET table.sql-dialect=hive;\n+CREATE TABLE hive_table (\n+  user_id STRING,\n+  order_amount DOUBLE\n+) PARTITIONED BY (dt STRING, hour STRING) STORED AS parquet TBLPROPERTIES (\n+  'partition.time-extractor.timestamp-pattern'='$dt $hour:00:00',\n+  'sink.partition-commit.trigger'='partition-time',\n+  'sink.partition-commit.delay'='1 h',\n+  'sink.partition-commit.policy.kind'='metastore,success-file'\n+);\n+\n+SET table.sql-dialect=default;\n+CREATE TABLE kafka_table (\n+  user_id STRING,\n+  order_amount DOUBLE,\n+  log_ts TIMESTAMP(3),\n+  WATERMARK FOR log_ts AS log_ts - INTERVAL '5' SECOND\n+) WITH (...);\n+\n+-- streaming sql, insert into hive table\n+INSERT INTO TABLE hive_table SELECT user_id, order_amount, DATE_FORMAT(log_ts, 'yyyy-MM-dd'), DATE_FORMAT(log_ts, 'HH') FROM kafka_table;\n+\n+-- batch sql, select with partition pruning\n+SELECT * FROM hive_table WHERE dt='2020-05-20' and hour='12';\n+\n+{% endhighlight %}\n+\n+## Streaming Reading\n+\n+To improve the real-time performance of hive reading, Flink support real-time Hive table stream read:\n+\n+- Partition table, monitor the generation of partition, and read the new partition incrementally.\n+- Non-partition table, monitor the generation of new files in the folder, and read new files incrementally.\n+\n+You can even use the 10 minute level partition strategy, and use Flink's Hive streaming reading and\n+Hive streaming writing to greatly improve the real-time performance of Hive data warehouse to quasi\n+real-time minute level.\n+\n+<table class=\"table table-bordered\">\n+  <thead>\n+    <tr>\n+        <th class=\"text-left\" style=\"width: 20%\">Key</th>\n+        <th class=\"text-left\" style=\"width: 15%\">Default</th>\n+        <th class=\"text-left\" style=\"width: 10%\">Type</th>\n+        <th class=\"text-left\" style=\"width: 55%\">Description</th>\n+    </tr>\n+  </thead>\n+  <tbody>\n+    <tr>\n+        <td><h5>streaming-source.enable</h5></td>\n+        <td style=\"word-wrap: break-word;\">false</td>\n+        <td>Boolean</td>\n+        <td>Enable streaming source or not. NOTES: For non-partition table, please make sure that each file should be put atomically into the target directory, otherwise the reader may get incomplete data.</td>\n+    </tr>\n+    <tr>\n+        <td><h5>streaming-source.monitor-interval</h5></td>\n+        <td style=\"word-wrap: break-word;\">1 m</td>\n+        <td>Duration</td>\n+        <td>Time interval for consecutively monitoring partition/file.</td>\n+    </tr>\n+    <tr>\n+        <td><h5>streaming-source.consume-order</h5></td>\n+        <td style=\"word-wrap: break-word;\">create-time</td>\n+        <td>String</td>\n+        <td>The consume order of streaming source, support create-time and partition-time. create-time compare partition/file creation time, this is not the partition create time in Hive metaStore, but the folder/file create time in filesystem; partition-time compare time represented by partition name. For non-partition table, this value should always be 'create-time'.</td>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "765b588c806f6b08e3f9116d8ef9817d1ecd14ba"}, "originalPosition": 137}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODU4NTEyOA==", "bodyText": "Default partition-time +1, but this option unify partition and non-partition. It is a little hard.", "url": "https://github.com/apache/flink/pull/12537#discussion_r438585128", "createdAt": "2020-06-11T07:08:55Z", "author": {"login": "JingsongLi"}, "path": "docs/dev/table/hive/hive_streaming.md", "diffHunk": "@@ -0,0 +1,164 @@\n+---\n+title: \"Hive Streaming\"\n+nav-parent_id: hive_tableapi\n+nav-pos: 2\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+A typical hive job is scheduled periodically to execute, so there will be a large delay.\n+\n+Flink supports to write, read and join the hive table in the form of streaming.\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+There are three types of streaming:\n+\n+- Writing streaming data into Hive table.\n+- Reading Hive table incrementally in the form of streaming.\n+- Streaming table join Hive table using [Temporal Table]({{ site.baseurl }}/dev/table/streaming/temporal_tables.html#temporal-table).\n+\n+## Streaming Writing\n+\n+The Hive table supports streaming writes, based on [Filesystem Streaming Sink]({{ site.baseurl }}/dev/table/connectors/filesystem.html#streaming-sink).\n+\n+The Hive Streaming Sink re-use Filesystem Streaming Sink to integrate Hadoop OutputFormat/RecordWriter to streaming writing.\n+Hadoop RecordWriters are Bulk-encoded Formats, Bulk Formats rolls files on every checkpoint.\n+\n+By default, now only have renaming committer, this means S3 filesystem can not supports exactly-once,\n+if you want to use Hive streaming sink in S3 filesystem, You can configure the following parameters\n+in `TableConfig` (note that these parameters affect all sinks of the job):\n+\n+<table class=\"table table-bordered\">\n+  <thead>\n+    <tr>\n+        <th class=\"text-left\" style=\"width: 20%\">Key</th>\n+        <th class=\"text-left\" style=\"width: 15%\">Default</th>\n+        <th class=\"text-left\" style=\"width: 10%\">Type</th>\n+        <th class=\"text-left\" style=\"width: 55%\">Description</th>\n+    </tr>\n+  </thead>\n+  <tbody>\n+    <tr>\n+        <td><h5>table.exec.hive.fallback-mapred-writer</h5></td>\n+        <td style=\"word-wrap: break-word;\">true</td>\n+        <td>Boolean</td>\n+        <td>If it is false, using flink native writer to write parquet and orc files; if it is true, using hadoop mapred record writer to write parquet and orc files.</td>\n+    </tr>\n+  </tbody>\n+</table>\n+\n+The below shows how the streaming sink can be used to write a streaming query to write data from Kafka into a Hive table with partition-commit,\n+and runs a batch query to read that data back out. \n+\n+{% highlight sql %}\n+\n+SET table.sql-dialect=hive;\n+CREATE TABLE hive_table (\n+  user_id STRING,\n+  order_amount DOUBLE\n+) PARTITIONED BY (dt STRING, hour STRING) STORED AS parquet TBLPROPERTIES (\n+  'partition.time-extractor.timestamp-pattern'='$dt $hour:00:00',\n+  'sink.partition-commit.trigger'='partition-time',\n+  'sink.partition-commit.delay'='1 h',\n+  'sink.partition-commit.policy.kind'='metastore,success-file'\n+);\n+\n+SET table.sql-dialect=default;\n+CREATE TABLE kafka_table (\n+  user_id STRING,\n+  order_amount DOUBLE,\n+  log_ts TIMESTAMP(3),\n+  WATERMARK FOR log_ts AS log_ts - INTERVAL '5' SECOND\n+) WITH (...);\n+\n+-- streaming sql, insert into hive table\n+INSERT INTO TABLE hive_table SELECT user_id, order_amount, DATE_FORMAT(log_ts, 'yyyy-MM-dd'), DATE_FORMAT(log_ts, 'HH') FROM kafka_table;\n+\n+-- batch sql, select with partition pruning\n+SELECT * FROM hive_table WHERE dt='2020-05-20' and hour='12';\n+\n+{% endhighlight %}\n+\n+## Streaming Reading\n+\n+To improve the real-time performance of hive reading, Flink support real-time Hive table stream read:\n+\n+- Partition table, monitor the generation of partition, and read the new partition incrementally.\n+- Non-partition table, monitor the generation of new files in the folder, and read new files incrementally.\n+\n+You can even use the 10 minute level partition strategy, and use Flink's Hive streaming reading and\n+Hive streaming writing to greatly improve the real-time performance of Hive data warehouse to quasi\n+real-time minute level.\n+\n+<table class=\"table table-bordered\">\n+  <thead>\n+    <tr>\n+        <th class=\"text-left\" style=\"width: 20%\">Key</th>\n+        <th class=\"text-left\" style=\"width: 15%\">Default</th>\n+        <th class=\"text-left\" style=\"width: 10%\">Type</th>\n+        <th class=\"text-left\" style=\"width: 55%\">Description</th>\n+    </tr>\n+  </thead>\n+  <tbody>\n+    <tr>\n+        <td><h5>streaming-source.enable</h5></td>\n+        <td style=\"word-wrap: break-word;\">false</td>\n+        <td>Boolean</td>\n+        <td>Enable streaming source or not. NOTES: For non-partition table, please make sure that each file should be put atomically into the target directory, otherwise the reader may get incomplete data.</td>\n+    </tr>\n+    <tr>\n+        <td><h5>streaming-source.monitor-interval</h5></td>\n+        <td style=\"word-wrap: break-word;\">1 m</td>\n+        <td>Duration</td>\n+        <td>Time interval for consecutively monitoring partition/file.</td>\n+    </tr>\n+    <tr>\n+        <td><h5>streaming-source.consume-order</h5></td>\n+        <td style=\"word-wrap: break-word;\">create-time</td>\n+        <td>String</td>\n+        <td>The consume order of streaming source, support create-time and partition-time. create-time compare partition/file creation time, this is not the partition create time in Hive metaStore, but the folder/file create time in filesystem; partition-time compare time represented by partition name. For non-partition table, this value should always be 'create-time'.</td>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODUyNzEyMw=="}, "originalCommit": {"oid": "765b588c806f6b08e3f9116d8ef9817d1ecd14ba"}, "originalPosition": 137}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODU5MTE2NA==", "bodyText": "I see. If we change default value to partition-time and user is using a non-partitioned table then the query will fail, right? If that's the case, then maybe we can go with create-time as default, but with proper warnings.", "url": "https://github.com/apache/flink/pull/12537#discussion_r438591164", "createdAt": "2020-06-11T07:21:48Z", "author": {"login": "lirui-apache"}, "path": "docs/dev/table/hive/hive_streaming.md", "diffHunk": "@@ -0,0 +1,164 @@\n+---\n+title: \"Hive Streaming\"\n+nav-parent_id: hive_tableapi\n+nav-pos: 2\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+A typical hive job is scheduled periodically to execute, so there will be a large delay.\n+\n+Flink supports to write, read and join the hive table in the form of streaming.\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+There are three types of streaming:\n+\n+- Writing streaming data into Hive table.\n+- Reading Hive table incrementally in the form of streaming.\n+- Streaming table join Hive table using [Temporal Table]({{ site.baseurl }}/dev/table/streaming/temporal_tables.html#temporal-table).\n+\n+## Streaming Writing\n+\n+The Hive table supports streaming writes, based on [Filesystem Streaming Sink]({{ site.baseurl }}/dev/table/connectors/filesystem.html#streaming-sink).\n+\n+The Hive Streaming Sink re-use Filesystem Streaming Sink to integrate Hadoop OutputFormat/RecordWriter to streaming writing.\n+Hadoop RecordWriters are Bulk-encoded Formats, Bulk Formats rolls files on every checkpoint.\n+\n+By default, now only have renaming committer, this means S3 filesystem can not supports exactly-once,\n+if you want to use Hive streaming sink in S3 filesystem, You can configure the following parameters\n+in `TableConfig` (note that these parameters affect all sinks of the job):\n+\n+<table class=\"table table-bordered\">\n+  <thead>\n+    <tr>\n+        <th class=\"text-left\" style=\"width: 20%\">Key</th>\n+        <th class=\"text-left\" style=\"width: 15%\">Default</th>\n+        <th class=\"text-left\" style=\"width: 10%\">Type</th>\n+        <th class=\"text-left\" style=\"width: 55%\">Description</th>\n+    </tr>\n+  </thead>\n+  <tbody>\n+    <tr>\n+        <td><h5>table.exec.hive.fallback-mapred-writer</h5></td>\n+        <td style=\"word-wrap: break-word;\">true</td>\n+        <td>Boolean</td>\n+        <td>If it is false, using flink native writer to write parquet and orc files; if it is true, using hadoop mapred record writer to write parquet and orc files.</td>\n+    </tr>\n+  </tbody>\n+</table>\n+\n+The below shows how the streaming sink can be used to write a streaming query to write data from Kafka into a Hive table with partition-commit,\n+and runs a batch query to read that data back out. \n+\n+{% highlight sql %}\n+\n+SET table.sql-dialect=hive;\n+CREATE TABLE hive_table (\n+  user_id STRING,\n+  order_amount DOUBLE\n+) PARTITIONED BY (dt STRING, hour STRING) STORED AS parquet TBLPROPERTIES (\n+  'partition.time-extractor.timestamp-pattern'='$dt $hour:00:00',\n+  'sink.partition-commit.trigger'='partition-time',\n+  'sink.partition-commit.delay'='1 h',\n+  'sink.partition-commit.policy.kind'='metastore,success-file'\n+);\n+\n+SET table.sql-dialect=default;\n+CREATE TABLE kafka_table (\n+  user_id STRING,\n+  order_amount DOUBLE,\n+  log_ts TIMESTAMP(3),\n+  WATERMARK FOR log_ts AS log_ts - INTERVAL '5' SECOND\n+) WITH (...);\n+\n+-- streaming sql, insert into hive table\n+INSERT INTO TABLE hive_table SELECT user_id, order_amount, DATE_FORMAT(log_ts, 'yyyy-MM-dd'), DATE_FORMAT(log_ts, 'HH') FROM kafka_table;\n+\n+-- batch sql, select with partition pruning\n+SELECT * FROM hive_table WHERE dt='2020-05-20' and hour='12';\n+\n+{% endhighlight %}\n+\n+## Streaming Reading\n+\n+To improve the real-time performance of hive reading, Flink support real-time Hive table stream read:\n+\n+- Partition table, monitor the generation of partition, and read the new partition incrementally.\n+- Non-partition table, monitor the generation of new files in the folder, and read new files incrementally.\n+\n+You can even use the 10 minute level partition strategy, and use Flink's Hive streaming reading and\n+Hive streaming writing to greatly improve the real-time performance of Hive data warehouse to quasi\n+real-time minute level.\n+\n+<table class=\"table table-bordered\">\n+  <thead>\n+    <tr>\n+        <th class=\"text-left\" style=\"width: 20%\">Key</th>\n+        <th class=\"text-left\" style=\"width: 15%\">Default</th>\n+        <th class=\"text-left\" style=\"width: 10%\">Type</th>\n+        <th class=\"text-left\" style=\"width: 55%\">Description</th>\n+    </tr>\n+  </thead>\n+  <tbody>\n+    <tr>\n+        <td><h5>streaming-source.enable</h5></td>\n+        <td style=\"word-wrap: break-word;\">false</td>\n+        <td>Boolean</td>\n+        <td>Enable streaming source or not. NOTES: For non-partition table, please make sure that each file should be put atomically into the target directory, otherwise the reader may get incomplete data.</td>\n+    </tr>\n+    <tr>\n+        <td><h5>streaming-source.monitor-interval</h5></td>\n+        <td style=\"word-wrap: break-word;\">1 m</td>\n+        <td>Duration</td>\n+        <td>Time interval for consecutively monitoring partition/file.</td>\n+    </tr>\n+    <tr>\n+        <td><h5>streaming-source.consume-order</h5></td>\n+        <td style=\"word-wrap: break-word;\">create-time</td>\n+        <td>String</td>\n+        <td>The consume order of streaming source, support create-time and partition-time. create-time compare partition/file creation time, this is not the partition create time in Hive metaStore, but the folder/file create time in filesystem; partition-time compare time represented by partition name. For non-partition table, this value should always be 'create-time'.</td>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODUyNzEyMw=="}, "originalCommit": {"oid": "765b588c806f6b08e3f9116d8ef9817d1ecd14ba"}, "originalPosition": 137}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczMTg3NjIwOnYy", "diffSide": "RIGHT", "path": "docs/dev/table/hive/hive_streaming.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQwMzo1MToyNlrOGiNmYQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQwMzo1MToyNlrOGiNmYQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODUyNzU4NQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            - Monitor strategy is scan all directories/files in location path now. If there is too more partitions, will have performance problems.\n          \n          \n            \n            - Monitor strategy is to scan all directories/files in location path now. If there are too many partitions, there will be performance problems.", "url": "https://github.com/apache/flink/pull/12537#discussion_r438527585", "createdAt": "2020-06-11T03:51:26Z", "author": {"login": "lirui-apache"}, "path": "docs/dev/table/hive/hive_streaming.md", "diffHunk": "@@ -0,0 +1,164 @@\n+---\n+title: \"Hive Streaming\"\n+nav-parent_id: hive_tableapi\n+nav-pos: 2\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+A typical hive job is scheduled periodically to execute, so there will be a large delay.\n+\n+Flink supports to write, read and join the hive table in the form of streaming.\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+There are three types of streaming:\n+\n+- Writing streaming data into Hive table.\n+- Reading Hive table incrementally in the form of streaming.\n+- Streaming table join Hive table using [Temporal Table]({{ site.baseurl }}/dev/table/streaming/temporal_tables.html#temporal-table).\n+\n+## Streaming Writing\n+\n+The Hive table supports streaming writes, based on [Filesystem Streaming Sink]({{ site.baseurl }}/dev/table/connectors/filesystem.html#streaming-sink).\n+\n+The Hive Streaming Sink re-use Filesystem Streaming Sink to integrate Hadoop OutputFormat/RecordWriter to streaming writing.\n+Hadoop RecordWriters are Bulk-encoded Formats, Bulk Formats rolls files on every checkpoint.\n+\n+By default, now only have renaming committer, this means S3 filesystem can not supports exactly-once,\n+if you want to use Hive streaming sink in S3 filesystem, You can configure the following parameters\n+in `TableConfig` (note that these parameters affect all sinks of the job):\n+\n+<table class=\"table table-bordered\">\n+  <thead>\n+    <tr>\n+        <th class=\"text-left\" style=\"width: 20%\">Key</th>\n+        <th class=\"text-left\" style=\"width: 15%\">Default</th>\n+        <th class=\"text-left\" style=\"width: 10%\">Type</th>\n+        <th class=\"text-left\" style=\"width: 55%\">Description</th>\n+    </tr>\n+  </thead>\n+  <tbody>\n+    <tr>\n+        <td><h5>table.exec.hive.fallback-mapred-writer</h5></td>\n+        <td style=\"word-wrap: break-word;\">true</td>\n+        <td>Boolean</td>\n+        <td>If it is false, using flink native writer to write parquet and orc files; if it is true, using hadoop mapred record writer to write parquet and orc files.</td>\n+    </tr>\n+  </tbody>\n+</table>\n+\n+The below shows how the streaming sink can be used to write a streaming query to write data from Kafka into a Hive table with partition-commit,\n+and runs a batch query to read that data back out. \n+\n+{% highlight sql %}\n+\n+SET table.sql-dialect=hive;\n+CREATE TABLE hive_table (\n+  user_id STRING,\n+  order_amount DOUBLE\n+) PARTITIONED BY (dt STRING, hour STRING) STORED AS parquet TBLPROPERTIES (\n+  'partition.time-extractor.timestamp-pattern'='$dt $hour:00:00',\n+  'sink.partition-commit.trigger'='partition-time',\n+  'sink.partition-commit.delay'='1 h',\n+  'sink.partition-commit.policy.kind'='metastore,success-file'\n+);\n+\n+SET table.sql-dialect=default;\n+CREATE TABLE kafka_table (\n+  user_id STRING,\n+  order_amount DOUBLE,\n+  log_ts TIMESTAMP(3),\n+  WATERMARK FOR log_ts AS log_ts - INTERVAL '5' SECOND\n+) WITH (...);\n+\n+-- streaming sql, insert into hive table\n+INSERT INTO TABLE hive_table SELECT user_id, order_amount, DATE_FORMAT(log_ts, 'yyyy-MM-dd'), DATE_FORMAT(log_ts, 'HH') FROM kafka_table;\n+\n+-- batch sql, select with partition pruning\n+SELECT * FROM hive_table WHERE dt='2020-05-20' and hour='12';\n+\n+{% endhighlight %}\n+\n+## Streaming Reading\n+\n+To improve the real-time performance of hive reading, Flink support real-time Hive table stream read:\n+\n+- Partition table, monitor the generation of partition, and read the new partition incrementally.\n+- Non-partition table, monitor the generation of new files in the folder, and read new files incrementally.\n+\n+You can even use the 10 minute level partition strategy, and use Flink's Hive streaming reading and\n+Hive streaming writing to greatly improve the real-time performance of Hive data warehouse to quasi\n+real-time minute level.\n+\n+<table class=\"table table-bordered\">\n+  <thead>\n+    <tr>\n+        <th class=\"text-left\" style=\"width: 20%\">Key</th>\n+        <th class=\"text-left\" style=\"width: 15%\">Default</th>\n+        <th class=\"text-left\" style=\"width: 10%\">Type</th>\n+        <th class=\"text-left\" style=\"width: 55%\">Description</th>\n+    </tr>\n+  </thead>\n+  <tbody>\n+    <tr>\n+        <td><h5>streaming-source.enable</h5></td>\n+        <td style=\"word-wrap: break-word;\">false</td>\n+        <td>Boolean</td>\n+        <td>Enable streaming source or not. NOTES: For non-partition table, please make sure that each file should be put atomically into the target directory, otherwise the reader may get incomplete data.</td>\n+    </tr>\n+    <tr>\n+        <td><h5>streaming-source.monitor-interval</h5></td>\n+        <td style=\"word-wrap: break-word;\">1 m</td>\n+        <td>Duration</td>\n+        <td>Time interval for consecutively monitoring partition/file.</td>\n+    </tr>\n+    <tr>\n+        <td><h5>streaming-source.consume-order</h5></td>\n+        <td style=\"word-wrap: break-word;\">create-time</td>\n+        <td>String</td>\n+        <td>The consume order of streaming source, support create-time and partition-time. create-time compare partition/file creation time, this is not the partition create time in Hive metaStore, but the folder/file create time in filesystem; partition-time compare time represented by partition name. For non-partition table, this value should always be 'create-time'.</td>\n+    </tr>\n+    <tr>\n+        <td><h5>streaming-source.consume-start-offset</h5></td>\n+        <td style=\"word-wrap: break-word;\">1970-00-00</td>\n+        <td>String</td>\n+        <td>Start offset for streaming consuming. How to parse and compare offsets depends on your order. For create-time and partition-time, should be a timestamp string. For partition-time, will use partition time extractor to extract time from partition.</td>\n+    </tr>\n+  </tbody>\n+</table>\n+\n+Note:\n+\n+- Monitor strategy is scan all directories/files in location path now. If there is too more partitions, will have performance problems.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "765b588c806f6b08e3f9116d8ef9817d1ecd14ba"}, "originalPosition": 150}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4346, "cost": 1, "resetAt": "2021-11-12T11:57:46Z"}}}