{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDg5MjAzMDky", "number": 13420, "title": "[FLINK-19229][python] Introduce the PythonStreamGroupAggregateOperator for Python UDAF.", "bodyText": "What is the purpose of the change\nThis pull request introduces the PythonStreamGroupAggregateOperator for Python UDAF.\nBrief change log\n\nAdd stateful runners for executing Python UDAF via Beam Fn Execution Framework.\nIntroduce the PythonStreamGroupAggregateOperator for Python UDA\n\nVerifying this change\nThis change is already covered by existing tests.\nDoes this pull request potentially affect one of the following parts:\n\nDependencies (does it add or upgrade a dependency): (no)\nThe public API, i.e., is any changed class annotated with @Public(Evolving): (no)\nThe serializers: (no)\nThe runtime per-record code paths (performance sensitive): (no)\nAnything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (no)\nThe S3 file system connector: (no)\n\nDocumentation\n\nDoes this pull request introduce a new feature? (no)\nIf yes, how is the feature documented? (not applicable)", "createdAt": "2020-09-18T09:46:02Z", "url": "https://github.com/apache/flink/pull/13420", "merged": true, "mergeCommit": {"oid": "452b0437cc1560a60f1961a9f73dd7ef6d4f895a"}, "closed": true, "closedAt": "2020-09-23T00:31:56Z", "author": {"login": "WeiZhong94"}, "timelineItems": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdKS8wiAFqTQ5MTg5MzMyMA==", "endCursor": "Y3Vyc29yOnYyOpPPAAABdLU10IgFqTQ5MzI5MzIzNg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDkxODkzMzIw", "url": "https://github.com/apache/flink/pull/13420#pullrequestreview-491893320", "createdAt": "2020-09-19T03:58:43Z", "commit": {"oid": "c95e880c06847e6ee07c4e6cdf107ce1fa3fcb11"}, "state": "COMMENTED", "comments": {"totalCount": 18, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xOVQwMzo1ODo0M1rOHUgdJA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xOVQwNTowMDo0NFrOHUgviw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTI2NTMxNg==", "bodyText": "Move this function before Schema to group the UserDefinedXXXFunctions together?", "url": "https://github.com/apache/flink/pull/13420#discussion_r491265316", "createdAt": "2020-09-19T03:58:43Z", "author": {"login": "dianfu"}, "path": "flink-python/pyflink/proto/flink-fn-execution.proto", "diffHunk": "@@ -208,3 +208,14 @@ message TypeInfo {\n   }\n   repeated Field field = 1;\n }\n+\n+message UserDefinedAggregateFunctions {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c95e880c06847e6ee07c4e6cdf107ce1fa3fcb11"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTI2NTU2NQ==", "bodyText": "Add some description about these fields?", "url": "https://github.com/apache/flink/pull/13420#discussion_r491265565", "createdAt": "2020-09-19T04:01:54Z", "author": {"login": "dianfu"}, "path": "flink-python/pyflink/proto/flink-fn-execution.proto", "diffHunk": "@@ -208,3 +208,14 @@ message TypeInfo {\n   }\n   repeated Field field = 1;\n }\n+\n+message UserDefinedAggregateFunctions {\n+  repeated UserDefinedFunction udfs = 1;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c95e880c06847e6ee07c4e6cdf107ce1fa3fcb11"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTI2NjcxOA==", "bodyText": "It seems that the abstraction of BeamPythonStatefulFunctionRunner and BeamPythonStatelessFunctionRunner is not necessary. What about merge the functionality of BeamPythonStatefulFunctionRunner into BeamPythonFunctionRunner and remove BeamPythonStatelessFunctionRunner?", "url": "https://github.com/apache/flink/pull/13420#discussion_r491266718", "createdAt": "2020-09-19T04:16:52Z", "author": {"login": "dianfu"}, "path": "flink-python/src/main/java/org/apache/flink/streaming/api/runners/python/beam/BeamPythonStatefulFunctionRunner.java", "diffHunk": "@@ -0,0 +1,368 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.api.runners.python.beam;\n+\n+import org.apache.flink.api.common.ExecutionConfig;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.PrimitiveArrayTypeInfo;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.core.memory.ByteArrayInputStreamWithPos;\n+import org.apache.flink.core.memory.ByteArrayOutputStreamWithPos;\n+import org.apache.flink.core.memory.DataInputViewStreamWrapper;\n+import org.apache.flink.core.memory.DataOutputViewStreamWrapper;\n+import org.apache.flink.python.env.PythonEnvironmentManager;\n+import org.apache.flink.python.metric.FlinkMetricContainer;\n+import org.apache.flink.runtime.state.KeyedStateBackend;\n+import org.apache.flink.runtime.state.VoidNamespace;\n+import org.apache.flink.runtime.state.VoidNamespaceSerializer;\n+\n+import org.apache.beam.model.fnexecution.v1.BeamFnApi;\n+import org.apache.beam.model.pipeline.v1.RunnerApi;\n+import org.apache.beam.runners.core.construction.ModelCoders;\n+import org.apache.beam.runners.core.construction.graph.ExecutableStage;\n+import org.apache.beam.runners.core.construction.graph.ImmutableExecutableStage;\n+import org.apache.beam.runners.core.construction.graph.PipelineNode;\n+import org.apache.beam.runners.core.construction.graph.SideInputReference;\n+import org.apache.beam.runners.core.construction.graph.TimerReference;\n+import org.apache.beam.runners.core.construction.graph.UserStateReference;\n+import org.apache.beam.runners.fnexecution.state.StateRequestHandler;\n+import org.apache.beam.sdk.coders.ByteArrayCoder;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n+import org.apache.beam.sdk.transforms.windowing.GlobalWindow;\n+import org.apache.beam.sdk.util.WindowedValue;\n+import org.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.ByteString;\n+\n+import javax.annotation.Nullable;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+\n+import static org.apache.beam.runners.core.construction.BeamUrns.getUrn;\n+\n+/**\n+ * A {@link BeamPythonStatefulFunctionRunner} used to execute Python stateful functions.\n+ */\n+public abstract class BeamPythonStatefulFunctionRunner extends BeamPythonFunctionRunner {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c95e880c06847e6ee07c4e6cdf107ce1fa3fcb11"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTI2Njg1Nw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \t\t\t\tbyte[] keybytes = bagUserState.getKey().toByteArray();\n          \n          \n            \n            \t\t\t\tbyte[] keyBytes = bagUserState.getKey().toByteArray();", "url": "https://github.com/apache/flink/pull/13420#discussion_r491266857", "createdAt": "2020-09-19T04:19:10Z", "author": {"login": "dianfu"}, "path": "flink-python/src/main/java/org/apache/flink/streaming/api/runners/python/beam/BeamPythonStatefulFunctionRunner.java", "diffHunk": "@@ -0,0 +1,368 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.api.runners.python.beam;\n+\n+import org.apache.flink.api.common.ExecutionConfig;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.PrimitiveArrayTypeInfo;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.core.memory.ByteArrayInputStreamWithPos;\n+import org.apache.flink.core.memory.ByteArrayOutputStreamWithPos;\n+import org.apache.flink.core.memory.DataInputViewStreamWrapper;\n+import org.apache.flink.core.memory.DataOutputViewStreamWrapper;\n+import org.apache.flink.python.env.PythonEnvironmentManager;\n+import org.apache.flink.python.metric.FlinkMetricContainer;\n+import org.apache.flink.runtime.state.KeyedStateBackend;\n+import org.apache.flink.runtime.state.VoidNamespace;\n+import org.apache.flink.runtime.state.VoidNamespaceSerializer;\n+\n+import org.apache.beam.model.fnexecution.v1.BeamFnApi;\n+import org.apache.beam.model.pipeline.v1.RunnerApi;\n+import org.apache.beam.runners.core.construction.ModelCoders;\n+import org.apache.beam.runners.core.construction.graph.ExecutableStage;\n+import org.apache.beam.runners.core.construction.graph.ImmutableExecutableStage;\n+import org.apache.beam.runners.core.construction.graph.PipelineNode;\n+import org.apache.beam.runners.core.construction.graph.SideInputReference;\n+import org.apache.beam.runners.core.construction.graph.TimerReference;\n+import org.apache.beam.runners.core.construction.graph.UserStateReference;\n+import org.apache.beam.runners.fnexecution.state.StateRequestHandler;\n+import org.apache.beam.sdk.coders.ByteArrayCoder;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n+import org.apache.beam.sdk.transforms.windowing.GlobalWindow;\n+import org.apache.beam.sdk.util.WindowedValue;\n+import org.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.ByteString;\n+\n+import javax.annotation.Nullable;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+\n+import static org.apache.beam.runners.core.construction.BeamUrns.getUrn;\n+\n+/**\n+ * A {@link BeamPythonStatefulFunctionRunner} used to execute Python stateful functions.\n+ */\n+public abstract class BeamPythonStatefulFunctionRunner extends BeamPythonFunctionRunner {\n+\n+\tprivate static final String USER_STATE_PREFIX = \"user-state-\";\n+\n+\tprivate static final String INPUT_ID = \"input\";\n+\tprivate static final String OUTPUT_ID = \"output\";\n+\tprivate static final String TRANSFORM_ID = \"transform\";\n+\n+\tprivate static final String MAIN_INPUT_NAME = \"input\";\n+\tprivate static final String MAIN_OUTPUT_NAME = \"output\";\n+\n+\tprivate static final String INPUT_CODER_ID = \"input_coder\";\n+\tprivate static final String OUTPUT_CODER_ID = \"output_coder\";\n+\tprivate static final String WINDOW_CODER_ID = \"window_coder\";\n+\n+\tprivate static final String WINDOW_STRATEGY = \"windowing_strategy\";\n+\n+\tprivate final String functionUrn;\n+\n+\tpublic BeamPythonStatefulFunctionRunner(\n+\t\tString taskName,\n+\t\tPythonEnvironmentManager environmentManager,\n+\t\tString functionUrn,\n+\t\tMap<String, String> jobOptions,\n+\t\tFlinkMetricContainer flinkMetricContainer,\n+\t\t@Nullable KeyedStateBackend keyedStateBackend,\n+\t\t@Nullable TypeSerializer keySerializer) {\n+\t\tsuper(\n+\t\t\ttaskName,\n+\t\t\tenvironmentManager,\n+\t\t\tgetStateRequestHandler(keyedStateBackend, keySerializer),\n+\t\t\tjobOptions,\n+\t\t\tflinkMetricContainer);\n+\t\tthis.functionUrn = functionUrn;\n+\t}\n+\n+\t@Override\n+\t@SuppressWarnings(\"unchecked\")\n+\tpublic ExecutableStage createExecutableStage() throws Exception {\n+\t\tRunnerApi.Components components =\n+\t\t\tRunnerApi.Components.newBuilder()\n+\t\t\t\t.putPcollections(\n+\t\t\t\t\tINPUT_ID,\n+\t\t\t\t\tRunnerApi.PCollection.newBuilder()\n+\t\t\t\t\t\t.setWindowingStrategyId(WINDOW_STRATEGY)\n+\t\t\t\t\t\t.setCoderId(INPUT_CODER_ID)\n+\t\t\t\t\t\t.build())\n+\t\t\t\t.putPcollections(\n+\t\t\t\t\tOUTPUT_ID,\n+\t\t\t\t\tRunnerApi.PCollection.newBuilder()\n+\t\t\t\t\t\t.setWindowingStrategyId(WINDOW_STRATEGY)\n+\t\t\t\t\t\t.setCoderId(OUTPUT_CODER_ID)\n+\t\t\t\t\t\t.build())\n+\t\t\t\t.putTransforms(\n+\t\t\t\t\tTRANSFORM_ID,\n+\t\t\t\t\tRunnerApi.PTransform.newBuilder()\n+\t\t\t\t\t\t.setUniqueName(TRANSFORM_ID)\n+\t\t\t\t\t\t.setSpec(RunnerApi.FunctionSpec.newBuilder()\n+\t\t\t\t\t\t\t.setUrn(functionUrn)\n+\t\t\t\t\t\t\t.setPayload(\n+\t\t\t\t\t\t\t\torg.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.ByteString.copyFrom(\n+\t\t\t\t\t\t\t\t\tgetUserDefinedFunctionsProtoBytes()))\n+\t\t\t\t\t\t\t.build())\n+\t\t\t\t\t\t.putInputs(MAIN_INPUT_NAME, INPUT_ID)\n+\t\t\t\t\t\t.putOutputs(MAIN_OUTPUT_NAME, OUTPUT_ID)\n+\t\t\t\t\t\t.build())\n+\t\t\t\t.putWindowingStrategies(\n+\t\t\t\t\tWINDOW_STRATEGY,\n+\t\t\t\t\tRunnerApi.WindowingStrategy.newBuilder()\n+\t\t\t\t\t\t.setWindowCoderId(WINDOW_CODER_ID)\n+\t\t\t\t\t\t.build())\n+\t\t\t\t.putCoders(\n+\t\t\t\t\tINPUT_CODER_ID,\n+\t\t\t\t\tgetInputCoderProto())\n+\t\t\t\t.putCoders(\n+\t\t\t\t\tOUTPUT_CODER_ID,\n+\t\t\t\t\tgetOutputCoderProto())\n+\t\t\t\t.putCoders(\n+\t\t\t\t\tWINDOW_CODER_ID,\n+\t\t\t\t\tgetWindowCoderProto())\n+\t\t\t\t.build();\n+\n+\t\tPipelineNode.PCollectionNode input =\n+\t\t\tPipelineNode.pCollection(INPUT_ID, components.getPcollectionsOrThrow(INPUT_ID));\n+\t\tList<SideInputReference> sideInputs = Collections.EMPTY_LIST;\n+\t\tList<UserStateReference> userStates = Collections.EMPTY_LIST;\n+\t\tList<TimerReference> timers = Collections.EMPTY_LIST;\n+\t\tList<PipelineNode.PTransformNode> transforms =\n+\t\t\tCollections.singletonList(\n+\t\t\t\tPipelineNode.pTransform(TRANSFORM_ID, components.getTransformsOrThrow(TRANSFORM_ID)));\n+\t\tList<PipelineNode.PCollectionNode> outputs =\n+\t\t\tCollections.singletonList(\n+\t\t\t\tPipelineNode.pCollection(OUTPUT_ID, components.getPcollectionsOrThrow(OUTPUT_ID)));\n+\t\treturn ImmutableExecutableStage.of(\n+\t\t\tcomponents, createPythonExecutionEnvironment(), input, sideInputs, userStates, timers, transforms, outputs, createValueOnlyWireCoderSetting());\n+\t}\n+\n+\tprivate Collection<RunnerApi.ExecutableStagePayload.WireCoderSetting> createValueOnlyWireCoderSetting() throws\n+\t\tIOException {\n+\t\tWindowedValue<byte[]> value = WindowedValue.valueInGlobalWindow(new byte[0]);\n+\t\tCoder<? extends BoundedWindow> windowCoder = GlobalWindow.Coder.INSTANCE;\n+\t\tWindowedValue.FullWindowedValueCoder<byte[]> windowedValueCoder =\n+\t\t\tWindowedValue.FullWindowedValueCoder.of(ByteArrayCoder.of(), windowCoder);\n+\t\tByteArrayOutputStream baos = new ByteArrayOutputStream();\n+\t\twindowedValueCoder.encode(value, baos);\n+\n+\t\treturn Arrays.asList(\n+\t\t\tRunnerApi.ExecutableStagePayload.WireCoderSetting.newBuilder()\n+\t\t\t\t.setUrn(getUrn(RunnerApi.StandardCoders.Enum.PARAM_WINDOWED_VALUE))\n+\t\t\t\t.setPayload(\n+\t\t\t\t\torg.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.ByteString.copyFrom(baos.toByteArray()))\n+\t\t\t\t.setInputOrOutputId(INPUT_ID)\n+\t\t\t\t.build(),\n+\t\t\tRunnerApi.ExecutableStagePayload.WireCoderSetting.newBuilder()\n+\t\t\t\t.setUrn(getUrn(RunnerApi.StandardCoders.Enum.PARAM_WINDOWED_VALUE))\n+\t\t\t\t.setPayload(\n+\t\t\t\t\torg.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.ByteString.copyFrom(baos.toByteArray()))\n+\t\t\t\t.setInputOrOutputId(OUTPUT_ID)\n+\t\t\t\t.build()\n+\t\t);\n+\t}\n+\n+\tprotected abstract byte[] getUserDefinedFunctionsProtoBytes();\n+\n+\tprotected abstract RunnerApi.Coder getInputCoderProto();\n+\n+\tprotected abstract RunnerApi.Coder getOutputCoderProto();\n+\n+\t/**\n+\t * Gets the proto representation of the window coder.\n+\t */\n+\tprivate RunnerApi.Coder getWindowCoderProto() {\n+\t\treturn RunnerApi.Coder.newBuilder()\n+\t\t\t.setSpec(\n+\t\t\t\tRunnerApi.FunctionSpec.newBuilder()\n+\t\t\t\t\t.setUrn(ModelCoders.GLOBAL_WINDOW_CODER_URN)\n+\t\t\t\t\t.build())\n+\t\t\t.build();\n+\t}\n+\n+\tprivate static StateRequestHandler getStateRequestHandler(\n+\t\t\tKeyedStateBackend keyedStateBackend,\n+\t\t\tTypeSerializer keySerializer) {\n+\t\tif (keyedStateBackend == null) {\n+\t\t\treturn StateRequestHandler.unsupported();\n+\t\t} else {\n+\t\t\tassert keySerializer != null;\n+\t\t\treturn new SimpleStateRequestHandler(keyedStateBackend, keySerializer);\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * A state request handler which handles the state request from Python side.\n+\t */\n+\tprivate static class SimpleStateRequestHandler implements StateRequestHandler {\n+\n+\t\tprivate final TypeSerializer keySerializer;\n+\t\tprivate final TypeSerializer<byte[]> valueSerializer;\n+\t\tprivate final KeyedStateBackend keyedStateBackend;\n+\n+\t\t/**\n+\t\t * Reusable OutputStream used to holding the serialized input elements.\n+\t\t */\n+\t\tprotected transient ByteArrayOutputStreamWithPos baos;\n+\n+\t\t/**\n+\t\t * Reusable InputStream used to holding the elements to be deserialized.\n+\t\t */\n+\t\tprotected transient ByteArrayInputStreamWithPos bais;\n+\n+\t\t/**\n+\t\t * OutputStream Wrapper.\n+\t\t */\n+\t\tprotected transient DataOutputViewStreamWrapper baosWrapper;\n+\n+\t\t/**\n+\t\t * InputStream Wrapper.\n+\t\t */\n+\t\tprotected transient DataInputViewStreamWrapper baisWrapper;\n+\n+\t\tpublic SimpleStateRequestHandler(\n+\t\t\t\tKeyedStateBackend keyedStateBackend,\n+\t\t\t\tTypeSerializer keySerializer) {\n+\t\t\tthis.keySerializer = keySerializer;\n+\t\t\tthis.valueSerializer = PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO\n+\t\t\t\t.createSerializer(new ExecutionConfig());\n+\t\t\tthis.keyedStateBackend = keyedStateBackend;\n+\t\t\tbaos = new ByteArrayOutputStreamWithPos();\n+\t\t\tbaosWrapper = new DataOutputViewStreamWrapper(baos);\n+\t\t\tbais = new ByteArrayInputStreamWithPos();\n+\t\t\tbaisWrapper = new DataInputViewStreamWrapper(bais);\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic CompletionStage<BeamFnApi.StateResponse.Builder> handle(BeamFnApi.StateRequest request) throws Exception {\n+\t\t\tBeamFnApi.StateKey.TypeCase typeCase = request.getStateKey().getTypeCase();\n+\t\t\tsynchronized (keyedStateBackend) {\n+\t\t\t\tif (typeCase.equals(BeamFnApi.StateKey.TypeCase.BAG_USER_STATE)) {\n+\t\t\t\t\treturn handleBagState(request);\n+\t\t\t\t} else {\n+\t\t\t\t\tthrow new RuntimeException(\"Unsupported state type: \" + typeCase);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\n+\t\tprivate CompletionStage<BeamFnApi.StateResponse.Builder> handleBagState(BeamFnApi.StateRequest request) throws Exception {\n+\n+\t\t\tif (request.getStateKey().hasBagUserState()) {\n+\t\t\t\tBeamFnApi.StateKey.BagUserState bagUserState = request.getStateKey().getBagUserState();\n+\t\t\t\t// get key\n+\t\t\t\tbyte[] keybytes = bagUserState.getKey().toByteArray();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c95e880c06847e6ee07c4e6cdf107ce1fa3fcb11"}, "originalPosition": 281}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTI2NzA1Mg==", "bodyText": "Isn't ByteArrayInputStream enough?", "url": "https://github.com/apache/flink/pull/13420#discussion_r491267052", "createdAt": "2020-09-19T04:22:06Z", "author": {"login": "dianfu"}, "path": "flink-python/src/main/java/org/apache/flink/streaming/api/runners/python/beam/BeamPythonStatefulFunctionRunner.java", "diffHunk": "@@ -0,0 +1,368 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.api.runners.python.beam;\n+\n+import org.apache.flink.api.common.ExecutionConfig;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.PrimitiveArrayTypeInfo;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.core.memory.ByteArrayInputStreamWithPos;\n+import org.apache.flink.core.memory.ByteArrayOutputStreamWithPos;\n+import org.apache.flink.core.memory.DataInputViewStreamWrapper;\n+import org.apache.flink.core.memory.DataOutputViewStreamWrapper;\n+import org.apache.flink.python.env.PythonEnvironmentManager;\n+import org.apache.flink.python.metric.FlinkMetricContainer;\n+import org.apache.flink.runtime.state.KeyedStateBackend;\n+import org.apache.flink.runtime.state.VoidNamespace;\n+import org.apache.flink.runtime.state.VoidNamespaceSerializer;\n+\n+import org.apache.beam.model.fnexecution.v1.BeamFnApi;\n+import org.apache.beam.model.pipeline.v1.RunnerApi;\n+import org.apache.beam.runners.core.construction.ModelCoders;\n+import org.apache.beam.runners.core.construction.graph.ExecutableStage;\n+import org.apache.beam.runners.core.construction.graph.ImmutableExecutableStage;\n+import org.apache.beam.runners.core.construction.graph.PipelineNode;\n+import org.apache.beam.runners.core.construction.graph.SideInputReference;\n+import org.apache.beam.runners.core.construction.graph.TimerReference;\n+import org.apache.beam.runners.core.construction.graph.UserStateReference;\n+import org.apache.beam.runners.fnexecution.state.StateRequestHandler;\n+import org.apache.beam.sdk.coders.ByteArrayCoder;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n+import org.apache.beam.sdk.transforms.windowing.GlobalWindow;\n+import org.apache.beam.sdk.util.WindowedValue;\n+import org.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.ByteString;\n+\n+import javax.annotation.Nullable;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+\n+import static org.apache.beam.runners.core.construction.BeamUrns.getUrn;\n+\n+/**\n+ * A {@link BeamPythonStatefulFunctionRunner} used to execute Python stateful functions.\n+ */\n+public abstract class BeamPythonStatefulFunctionRunner extends BeamPythonFunctionRunner {\n+\n+\tprivate static final String USER_STATE_PREFIX = \"user-state-\";\n+\n+\tprivate static final String INPUT_ID = \"input\";\n+\tprivate static final String OUTPUT_ID = \"output\";\n+\tprivate static final String TRANSFORM_ID = \"transform\";\n+\n+\tprivate static final String MAIN_INPUT_NAME = \"input\";\n+\tprivate static final String MAIN_OUTPUT_NAME = \"output\";\n+\n+\tprivate static final String INPUT_CODER_ID = \"input_coder\";\n+\tprivate static final String OUTPUT_CODER_ID = \"output_coder\";\n+\tprivate static final String WINDOW_CODER_ID = \"window_coder\";\n+\n+\tprivate static final String WINDOW_STRATEGY = \"windowing_strategy\";\n+\n+\tprivate final String functionUrn;\n+\n+\tpublic BeamPythonStatefulFunctionRunner(\n+\t\tString taskName,\n+\t\tPythonEnvironmentManager environmentManager,\n+\t\tString functionUrn,\n+\t\tMap<String, String> jobOptions,\n+\t\tFlinkMetricContainer flinkMetricContainer,\n+\t\t@Nullable KeyedStateBackend keyedStateBackend,\n+\t\t@Nullable TypeSerializer keySerializer) {\n+\t\tsuper(\n+\t\t\ttaskName,\n+\t\t\tenvironmentManager,\n+\t\t\tgetStateRequestHandler(keyedStateBackend, keySerializer),\n+\t\t\tjobOptions,\n+\t\t\tflinkMetricContainer);\n+\t\tthis.functionUrn = functionUrn;\n+\t}\n+\n+\t@Override\n+\t@SuppressWarnings(\"unchecked\")\n+\tpublic ExecutableStage createExecutableStage() throws Exception {\n+\t\tRunnerApi.Components components =\n+\t\t\tRunnerApi.Components.newBuilder()\n+\t\t\t\t.putPcollections(\n+\t\t\t\t\tINPUT_ID,\n+\t\t\t\t\tRunnerApi.PCollection.newBuilder()\n+\t\t\t\t\t\t.setWindowingStrategyId(WINDOW_STRATEGY)\n+\t\t\t\t\t\t.setCoderId(INPUT_CODER_ID)\n+\t\t\t\t\t\t.build())\n+\t\t\t\t.putPcollections(\n+\t\t\t\t\tOUTPUT_ID,\n+\t\t\t\t\tRunnerApi.PCollection.newBuilder()\n+\t\t\t\t\t\t.setWindowingStrategyId(WINDOW_STRATEGY)\n+\t\t\t\t\t\t.setCoderId(OUTPUT_CODER_ID)\n+\t\t\t\t\t\t.build())\n+\t\t\t\t.putTransforms(\n+\t\t\t\t\tTRANSFORM_ID,\n+\t\t\t\t\tRunnerApi.PTransform.newBuilder()\n+\t\t\t\t\t\t.setUniqueName(TRANSFORM_ID)\n+\t\t\t\t\t\t.setSpec(RunnerApi.FunctionSpec.newBuilder()\n+\t\t\t\t\t\t\t.setUrn(functionUrn)\n+\t\t\t\t\t\t\t.setPayload(\n+\t\t\t\t\t\t\t\torg.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.ByteString.copyFrom(\n+\t\t\t\t\t\t\t\t\tgetUserDefinedFunctionsProtoBytes()))\n+\t\t\t\t\t\t\t.build())\n+\t\t\t\t\t\t.putInputs(MAIN_INPUT_NAME, INPUT_ID)\n+\t\t\t\t\t\t.putOutputs(MAIN_OUTPUT_NAME, OUTPUT_ID)\n+\t\t\t\t\t\t.build())\n+\t\t\t\t.putWindowingStrategies(\n+\t\t\t\t\tWINDOW_STRATEGY,\n+\t\t\t\t\tRunnerApi.WindowingStrategy.newBuilder()\n+\t\t\t\t\t\t.setWindowCoderId(WINDOW_CODER_ID)\n+\t\t\t\t\t\t.build())\n+\t\t\t\t.putCoders(\n+\t\t\t\t\tINPUT_CODER_ID,\n+\t\t\t\t\tgetInputCoderProto())\n+\t\t\t\t.putCoders(\n+\t\t\t\t\tOUTPUT_CODER_ID,\n+\t\t\t\t\tgetOutputCoderProto())\n+\t\t\t\t.putCoders(\n+\t\t\t\t\tWINDOW_CODER_ID,\n+\t\t\t\t\tgetWindowCoderProto())\n+\t\t\t\t.build();\n+\n+\t\tPipelineNode.PCollectionNode input =\n+\t\t\tPipelineNode.pCollection(INPUT_ID, components.getPcollectionsOrThrow(INPUT_ID));\n+\t\tList<SideInputReference> sideInputs = Collections.EMPTY_LIST;\n+\t\tList<UserStateReference> userStates = Collections.EMPTY_LIST;\n+\t\tList<TimerReference> timers = Collections.EMPTY_LIST;\n+\t\tList<PipelineNode.PTransformNode> transforms =\n+\t\t\tCollections.singletonList(\n+\t\t\t\tPipelineNode.pTransform(TRANSFORM_ID, components.getTransformsOrThrow(TRANSFORM_ID)));\n+\t\tList<PipelineNode.PCollectionNode> outputs =\n+\t\t\tCollections.singletonList(\n+\t\t\t\tPipelineNode.pCollection(OUTPUT_ID, components.getPcollectionsOrThrow(OUTPUT_ID)));\n+\t\treturn ImmutableExecutableStage.of(\n+\t\t\tcomponents, createPythonExecutionEnvironment(), input, sideInputs, userStates, timers, transforms, outputs, createValueOnlyWireCoderSetting());\n+\t}\n+\n+\tprivate Collection<RunnerApi.ExecutableStagePayload.WireCoderSetting> createValueOnlyWireCoderSetting() throws\n+\t\tIOException {\n+\t\tWindowedValue<byte[]> value = WindowedValue.valueInGlobalWindow(new byte[0]);\n+\t\tCoder<? extends BoundedWindow> windowCoder = GlobalWindow.Coder.INSTANCE;\n+\t\tWindowedValue.FullWindowedValueCoder<byte[]> windowedValueCoder =\n+\t\t\tWindowedValue.FullWindowedValueCoder.of(ByteArrayCoder.of(), windowCoder);\n+\t\tByteArrayOutputStream baos = new ByteArrayOutputStream();\n+\t\twindowedValueCoder.encode(value, baos);\n+\n+\t\treturn Arrays.asList(\n+\t\t\tRunnerApi.ExecutableStagePayload.WireCoderSetting.newBuilder()\n+\t\t\t\t.setUrn(getUrn(RunnerApi.StandardCoders.Enum.PARAM_WINDOWED_VALUE))\n+\t\t\t\t.setPayload(\n+\t\t\t\t\torg.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.ByteString.copyFrom(baos.toByteArray()))\n+\t\t\t\t.setInputOrOutputId(INPUT_ID)\n+\t\t\t\t.build(),\n+\t\t\tRunnerApi.ExecutableStagePayload.WireCoderSetting.newBuilder()\n+\t\t\t\t.setUrn(getUrn(RunnerApi.StandardCoders.Enum.PARAM_WINDOWED_VALUE))\n+\t\t\t\t.setPayload(\n+\t\t\t\t\torg.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.ByteString.copyFrom(baos.toByteArray()))\n+\t\t\t\t.setInputOrOutputId(OUTPUT_ID)\n+\t\t\t\t.build()\n+\t\t);\n+\t}\n+\n+\tprotected abstract byte[] getUserDefinedFunctionsProtoBytes();\n+\n+\tprotected abstract RunnerApi.Coder getInputCoderProto();\n+\n+\tprotected abstract RunnerApi.Coder getOutputCoderProto();\n+\n+\t/**\n+\t * Gets the proto representation of the window coder.\n+\t */\n+\tprivate RunnerApi.Coder getWindowCoderProto() {\n+\t\treturn RunnerApi.Coder.newBuilder()\n+\t\t\t.setSpec(\n+\t\t\t\tRunnerApi.FunctionSpec.newBuilder()\n+\t\t\t\t\t.setUrn(ModelCoders.GLOBAL_WINDOW_CODER_URN)\n+\t\t\t\t\t.build())\n+\t\t\t.build();\n+\t}\n+\n+\tprivate static StateRequestHandler getStateRequestHandler(\n+\t\t\tKeyedStateBackend keyedStateBackend,\n+\t\t\tTypeSerializer keySerializer) {\n+\t\tif (keyedStateBackend == null) {\n+\t\t\treturn StateRequestHandler.unsupported();\n+\t\t} else {\n+\t\t\tassert keySerializer != null;\n+\t\t\treturn new SimpleStateRequestHandler(keyedStateBackend, keySerializer);\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * A state request handler which handles the state request from Python side.\n+\t */\n+\tprivate static class SimpleStateRequestHandler implements StateRequestHandler {\n+\n+\t\tprivate final TypeSerializer keySerializer;\n+\t\tprivate final TypeSerializer<byte[]> valueSerializer;\n+\t\tprivate final KeyedStateBackend keyedStateBackend;\n+\n+\t\t/**\n+\t\t * Reusable OutputStream used to holding the serialized input elements.\n+\t\t */\n+\t\tprotected transient ByteArrayOutputStreamWithPos baos;\n+\n+\t\t/**\n+\t\t * Reusable InputStream used to holding the elements to be deserialized.\n+\t\t */\n+\t\tprotected transient ByteArrayInputStreamWithPos bais;\n+\n+\t\t/**\n+\t\t * OutputStream Wrapper.\n+\t\t */\n+\t\tprotected transient DataOutputViewStreamWrapper baosWrapper;\n+\n+\t\t/**\n+\t\t * InputStream Wrapper.\n+\t\t */\n+\t\tprotected transient DataInputViewStreamWrapper baisWrapper;\n+\n+\t\tpublic SimpleStateRequestHandler(\n+\t\t\t\tKeyedStateBackend keyedStateBackend,\n+\t\t\t\tTypeSerializer keySerializer) {\n+\t\t\tthis.keySerializer = keySerializer;\n+\t\t\tthis.valueSerializer = PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO\n+\t\t\t\t.createSerializer(new ExecutionConfig());\n+\t\t\tthis.keyedStateBackend = keyedStateBackend;\n+\t\t\tbaos = new ByteArrayOutputStreamWithPos();\n+\t\t\tbaosWrapper = new DataOutputViewStreamWrapper(baos);\n+\t\t\tbais = new ByteArrayInputStreamWithPos();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c95e880c06847e6ee07c4e6cdf107ce1fa3fcb11"}, "originalPosition": 260}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTI2NzMxNA==", "bodyText": "Could we avoid constructing flinkStateDescriptor for each request?", "url": "https://github.com/apache/flink/pull/13420#discussion_r491267314", "createdAt": "2020-09-19T04:25:17Z", "author": {"login": "dianfu"}, "path": "flink-python/src/main/java/org/apache/flink/streaming/api/runners/python/beam/BeamPythonStatefulFunctionRunner.java", "diffHunk": "@@ -0,0 +1,368 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.api.runners.python.beam;\n+\n+import org.apache.flink.api.common.ExecutionConfig;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.PrimitiveArrayTypeInfo;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.core.memory.ByteArrayInputStreamWithPos;\n+import org.apache.flink.core.memory.ByteArrayOutputStreamWithPos;\n+import org.apache.flink.core.memory.DataInputViewStreamWrapper;\n+import org.apache.flink.core.memory.DataOutputViewStreamWrapper;\n+import org.apache.flink.python.env.PythonEnvironmentManager;\n+import org.apache.flink.python.metric.FlinkMetricContainer;\n+import org.apache.flink.runtime.state.KeyedStateBackend;\n+import org.apache.flink.runtime.state.VoidNamespace;\n+import org.apache.flink.runtime.state.VoidNamespaceSerializer;\n+\n+import org.apache.beam.model.fnexecution.v1.BeamFnApi;\n+import org.apache.beam.model.pipeline.v1.RunnerApi;\n+import org.apache.beam.runners.core.construction.ModelCoders;\n+import org.apache.beam.runners.core.construction.graph.ExecutableStage;\n+import org.apache.beam.runners.core.construction.graph.ImmutableExecutableStage;\n+import org.apache.beam.runners.core.construction.graph.PipelineNode;\n+import org.apache.beam.runners.core.construction.graph.SideInputReference;\n+import org.apache.beam.runners.core.construction.graph.TimerReference;\n+import org.apache.beam.runners.core.construction.graph.UserStateReference;\n+import org.apache.beam.runners.fnexecution.state.StateRequestHandler;\n+import org.apache.beam.sdk.coders.ByteArrayCoder;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n+import org.apache.beam.sdk.transforms.windowing.GlobalWindow;\n+import org.apache.beam.sdk.util.WindowedValue;\n+import org.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.ByteString;\n+\n+import javax.annotation.Nullable;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+\n+import static org.apache.beam.runners.core.construction.BeamUrns.getUrn;\n+\n+/**\n+ * A {@link BeamPythonStatefulFunctionRunner} used to execute Python stateful functions.\n+ */\n+public abstract class BeamPythonStatefulFunctionRunner extends BeamPythonFunctionRunner {\n+\n+\tprivate static final String USER_STATE_PREFIX = \"user-state-\";\n+\n+\tprivate static final String INPUT_ID = \"input\";\n+\tprivate static final String OUTPUT_ID = \"output\";\n+\tprivate static final String TRANSFORM_ID = \"transform\";\n+\n+\tprivate static final String MAIN_INPUT_NAME = \"input\";\n+\tprivate static final String MAIN_OUTPUT_NAME = \"output\";\n+\n+\tprivate static final String INPUT_CODER_ID = \"input_coder\";\n+\tprivate static final String OUTPUT_CODER_ID = \"output_coder\";\n+\tprivate static final String WINDOW_CODER_ID = \"window_coder\";\n+\n+\tprivate static final String WINDOW_STRATEGY = \"windowing_strategy\";\n+\n+\tprivate final String functionUrn;\n+\n+\tpublic BeamPythonStatefulFunctionRunner(\n+\t\tString taskName,\n+\t\tPythonEnvironmentManager environmentManager,\n+\t\tString functionUrn,\n+\t\tMap<String, String> jobOptions,\n+\t\tFlinkMetricContainer flinkMetricContainer,\n+\t\t@Nullable KeyedStateBackend keyedStateBackend,\n+\t\t@Nullable TypeSerializer keySerializer) {\n+\t\tsuper(\n+\t\t\ttaskName,\n+\t\t\tenvironmentManager,\n+\t\t\tgetStateRequestHandler(keyedStateBackend, keySerializer),\n+\t\t\tjobOptions,\n+\t\t\tflinkMetricContainer);\n+\t\tthis.functionUrn = functionUrn;\n+\t}\n+\n+\t@Override\n+\t@SuppressWarnings(\"unchecked\")\n+\tpublic ExecutableStage createExecutableStage() throws Exception {\n+\t\tRunnerApi.Components components =\n+\t\t\tRunnerApi.Components.newBuilder()\n+\t\t\t\t.putPcollections(\n+\t\t\t\t\tINPUT_ID,\n+\t\t\t\t\tRunnerApi.PCollection.newBuilder()\n+\t\t\t\t\t\t.setWindowingStrategyId(WINDOW_STRATEGY)\n+\t\t\t\t\t\t.setCoderId(INPUT_CODER_ID)\n+\t\t\t\t\t\t.build())\n+\t\t\t\t.putPcollections(\n+\t\t\t\t\tOUTPUT_ID,\n+\t\t\t\t\tRunnerApi.PCollection.newBuilder()\n+\t\t\t\t\t\t.setWindowingStrategyId(WINDOW_STRATEGY)\n+\t\t\t\t\t\t.setCoderId(OUTPUT_CODER_ID)\n+\t\t\t\t\t\t.build())\n+\t\t\t\t.putTransforms(\n+\t\t\t\t\tTRANSFORM_ID,\n+\t\t\t\t\tRunnerApi.PTransform.newBuilder()\n+\t\t\t\t\t\t.setUniqueName(TRANSFORM_ID)\n+\t\t\t\t\t\t.setSpec(RunnerApi.FunctionSpec.newBuilder()\n+\t\t\t\t\t\t\t.setUrn(functionUrn)\n+\t\t\t\t\t\t\t.setPayload(\n+\t\t\t\t\t\t\t\torg.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.ByteString.copyFrom(\n+\t\t\t\t\t\t\t\t\tgetUserDefinedFunctionsProtoBytes()))\n+\t\t\t\t\t\t\t.build())\n+\t\t\t\t\t\t.putInputs(MAIN_INPUT_NAME, INPUT_ID)\n+\t\t\t\t\t\t.putOutputs(MAIN_OUTPUT_NAME, OUTPUT_ID)\n+\t\t\t\t\t\t.build())\n+\t\t\t\t.putWindowingStrategies(\n+\t\t\t\t\tWINDOW_STRATEGY,\n+\t\t\t\t\tRunnerApi.WindowingStrategy.newBuilder()\n+\t\t\t\t\t\t.setWindowCoderId(WINDOW_CODER_ID)\n+\t\t\t\t\t\t.build())\n+\t\t\t\t.putCoders(\n+\t\t\t\t\tINPUT_CODER_ID,\n+\t\t\t\t\tgetInputCoderProto())\n+\t\t\t\t.putCoders(\n+\t\t\t\t\tOUTPUT_CODER_ID,\n+\t\t\t\t\tgetOutputCoderProto())\n+\t\t\t\t.putCoders(\n+\t\t\t\t\tWINDOW_CODER_ID,\n+\t\t\t\t\tgetWindowCoderProto())\n+\t\t\t\t.build();\n+\n+\t\tPipelineNode.PCollectionNode input =\n+\t\t\tPipelineNode.pCollection(INPUT_ID, components.getPcollectionsOrThrow(INPUT_ID));\n+\t\tList<SideInputReference> sideInputs = Collections.EMPTY_LIST;\n+\t\tList<UserStateReference> userStates = Collections.EMPTY_LIST;\n+\t\tList<TimerReference> timers = Collections.EMPTY_LIST;\n+\t\tList<PipelineNode.PTransformNode> transforms =\n+\t\t\tCollections.singletonList(\n+\t\t\t\tPipelineNode.pTransform(TRANSFORM_ID, components.getTransformsOrThrow(TRANSFORM_ID)));\n+\t\tList<PipelineNode.PCollectionNode> outputs =\n+\t\t\tCollections.singletonList(\n+\t\t\t\tPipelineNode.pCollection(OUTPUT_ID, components.getPcollectionsOrThrow(OUTPUT_ID)));\n+\t\treturn ImmutableExecutableStage.of(\n+\t\t\tcomponents, createPythonExecutionEnvironment(), input, sideInputs, userStates, timers, transforms, outputs, createValueOnlyWireCoderSetting());\n+\t}\n+\n+\tprivate Collection<RunnerApi.ExecutableStagePayload.WireCoderSetting> createValueOnlyWireCoderSetting() throws\n+\t\tIOException {\n+\t\tWindowedValue<byte[]> value = WindowedValue.valueInGlobalWindow(new byte[0]);\n+\t\tCoder<? extends BoundedWindow> windowCoder = GlobalWindow.Coder.INSTANCE;\n+\t\tWindowedValue.FullWindowedValueCoder<byte[]> windowedValueCoder =\n+\t\t\tWindowedValue.FullWindowedValueCoder.of(ByteArrayCoder.of(), windowCoder);\n+\t\tByteArrayOutputStream baos = new ByteArrayOutputStream();\n+\t\twindowedValueCoder.encode(value, baos);\n+\n+\t\treturn Arrays.asList(\n+\t\t\tRunnerApi.ExecutableStagePayload.WireCoderSetting.newBuilder()\n+\t\t\t\t.setUrn(getUrn(RunnerApi.StandardCoders.Enum.PARAM_WINDOWED_VALUE))\n+\t\t\t\t.setPayload(\n+\t\t\t\t\torg.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.ByteString.copyFrom(baos.toByteArray()))\n+\t\t\t\t.setInputOrOutputId(INPUT_ID)\n+\t\t\t\t.build(),\n+\t\t\tRunnerApi.ExecutableStagePayload.WireCoderSetting.newBuilder()\n+\t\t\t\t.setUrn(getUrn(RunnerApi.StandardCoders.Enum.PARAM_WINDOWED_VALUE))\n+\t\t\t\t.setPayload(\n+\t\t\t\t\torg.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.ByteString.copyFrom(baos.toByteArray()))\n+\t\t\t\t.setInputOrOutputId(OUTPUT_ID)\n+\t\t\t\t.build()\n+\t\t);\n+\t}\n+\n+\tprotected abstract byte[] getUserDefinedFunctionsProtoBytes();\n+\n+\tprotected abstract RunnerApi.Coder getInputCoderProto();\n+\n+\tprotected abstract RunnerApi.Coder getOutputCoderProto();\n+\n+\t/**\n+\t * Gets the proto representation of the window coder.\n+\t */\n+\tprivate RunnerApi.Coder getWindowCoderProto() {\n+\t\treturn RunnerApi.Coder.newBuilder()\n+\t\t\t.setSpec(\n+\t\t\t\tRunnerApi.FunctionSpec.newBuilder()\n+\t\t\t\t\t.setUrn(ModelCoders.GLOBAL_WINDOW_CODER_URN)\n+\t\t\t\t\t.build())\n+\t\t\t.build();\n+\t}\n+\n+\tprivate static StateRequestHandler getStateRequestHandler(\n+\t\t\tKeyedStateBackend keyedStateBackend,\n+\t\t\tTypeSerializer keySerializer) {\n+\t\tif (keyedStateBackend == null) {\n+\t\t\treturn StateRequestHandler.unsupported();\n+\t\t} else {\n+\t\t\tassert keySerializer != null;\n+\t\t\treturn new SimpleStateRequestHandler(keyedStateBackend, keySerializer);\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * A state request handler which handles the state request from Python side.\n+\t */\n+\tprivate static class SimpleStateRequestHandler implements StateRequestHandler {\n+\n+\t\tprivate final TypeSerializer keySerializer;\n+\t\tprivate final TypeSerializer<byte[]> valueSerializer;\n+\t\tprivate final KeyedStateBackend keyedStateBackend;\n+\n+\t\t/**\n+\t\t * Reusable OutputStream used to holding the serialized input elements.\n+\t\t */\n+\t\tprotected transient ByteArrayOutputStreamWithPos baos;\n+\n+\t\t/**\n+\t\t * Reusable InputStream used to holding the elements to be deserialized.\n+\t\t */\n+\t\tprotected transient ByteArrayInputStreamWithPos bais;\n+\n+\t\t/**\n+\t\t * OutputStream Wrapper.\n+\t\t */\n+\t\tprotected transient DataOutputViewStreamWrapper baosWrapper;\n+\n+\t\t/**\n+\t\t * InputStream Wrapper.\n+\t\t */\n+\t\tprotected transient DataInputViewStreamWrapper baisWrapper;\n+\n+\t\tpublic SimpleStateRequestHandler(\n+\t\t\t\tKeyedStateBackend keyedStateBackend,\n+\t\t\t\tTypeSerializer keySerializer) {\n+\t\t\tthis.keySerializer = keySerializer;\n+\t\t\tthis.valueSerializer = PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO\n+\t\t\t\t.createSerializer(new ExecutionConfig());\n+\t\t\tthis.keyedStateBackend = keyedStateBackend;\n+\t\t\tbaos = new ByteArrayOutputStreamWithPos();\n+\t\t\tbaosWrapper = new DataOutputViewStreamWrapper(baos);\n+\t\t\tbais = new ByteArrayInputStreamWithPos();\n+\t\t\tbaisWrapper = new DataInputViewStreamWrapper(bais);\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic CompletionStage<BeamFnApi.StateResponse.Builder> handle(BeamFnApi.StateRequest request) throws Exception {\n+\t\t\tBeamFnApi.StateKey.TypeCase typeCase = request.getStateKey().getTypeCase();\n+\t\t\tsynchronized (keyedStateBackend) {\n+\t\t\t\tif (typeCase.equals(BeamFnApi.StateKey.TypeCase.BAG_USER_STATE)) {\n+\t\t\t\t\treturn handleBagState(request);\n+\t\t\t\t} else {\n+\t\t\t\t\tthrow new RuntimeException(\"Unsupported state type: \" + typeCase);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\n+\t\tprivate CompletionStage<BeamFnApi.StateResponse.Builder> handleBagState(BeamFnApi.StateRequest request) throws Exception {\n+\n+\t\t\tif (request.getStateKey().hasBagUserState()) {\n+\t\t\t\tBeamFnApi.StateKey.BagUserState bagUserState = request.getStateKey().getBagUserState();\n+\t\t\t\t// get key\n+\t\t\t\tbyte[] keybytes = bagUserState.getKey().toByteArray();\n+\t\t\t\tbais.setBuffer(keybytes, 0, keybytes.length);\n+\t\t\t\tObject key = keySerializer.deserialize(baisWrapper);\n+\t\t\t\tkeyedStateBackend.setCurrentKey(key);\n+\t\t\t} else {\n+\t\t\t\tthrow new RuntimeException(\"Unsupported bag state request: \" + request);\n+\t\t\t}\n+\n+\t\t\tswitch (request.getRequestCase()) {\n+\t\t\t\tcase GET:\n+\t\t\t\t\treturn handleGetRequest(request);\n+\t\t\t\tcase APPEND:\n+\t\t\t\t\treturn handleAppendRequest(request);\n+\t\t\t\tcase CLEAR:\n+\t\t\t\t\treturn handleClearRequest(request);\n+\t\t\t\tdefault:\n+\t\t\t\t\tthrow new RuntimeException(\n+\t\t\t\t\t\tString.format(\n+\t\t\t\t\t\t\t\"Unsupported request type %s for user state.\", request.getRequestCase()));\n+\t\t\t}\n+\t\t}\n+\n+\t\tprivate List<ByteString> convertToByteString(ListState<byte[]> listState) throws Exception {\n+\t\t\tList<ByteString> ret = new LinkedList<>();\n+\t\t\tif (listState.get() == null) {\n+\t\t\t\treturn ret;\n+\t\t\t}\n+\t\t\tfor (byte[] v: listState.get()) {\n+\t\t\t\tret.add(ByteString.copyFrom(v));\n+\t\t\t}\n+\t\t\treturn ret;\n+\t\t}\n+\n+\t\tprivate CompletionStage<BeamFnApi.StateResponse.Builder> handleGetRequest(\n+\t\t\tBeamFnApi.StateRequest request) throws Exception {\n+\n+\t\t\tListState<byte[]> partitionedState = getListState(request);\n+\t\t\tList<ByteString> byteStrings = convertToByteString(partitionedState);\n+\n+\t\t\treturn CompletableFuture.completedFuture(\n+\t\t\t\tBeamFnApi.StateResponse.newBuilder()\n+\t\t\t\t\t.setId(request.getId())\n+\t\t\t\t\t.setGet(\n+\t\t\t\t\t\tBeamFnApi.StateGetResponse.newBuilder()\n+\t\t\t\t\t\t\t.setData(ByteString.copyFrom(byteStrings))));\n+\t\t}\n+\n+\t\tprivate CompletionStage<BeamFnApi.StateResponse.Builder> handleAppendRequest(\n+\t\t\tBeamFnApi.StateRequest request) throws Exception {\n+\n+\t\t\tListState<byte[]> partitionedState = getListState(request);\n+\t\t\t// get values\n+\t\t\tbyte[] valuebytes = request.getAppend().getData().toByteArray();\n+\t\t\tpartitionedState.add(valuebytes);\n+\n+\t\t\treturn CompletableFuture.completedFuture(\n+\t\t\t\tBeamFnApi.StateResponse.newBuilder()\n+\t\t\t\t\t.setId(request.getId())\n+\t\t\t\t\t.setAppend(BeamFnApi.StateAppendResponse.getDefaultInstance()));\n+\t\t}\n+\n+\t\tprivate CompletionStage<BeamFnApi.StateResponse.Builder> handleClearRequest(\n+\t\t\tBeamFnApi.StateRequest request) throws Exception {\n+\n+\t\t\tListState<byte[]> partitionedState = getListState(request);\n+\n+\t\t\tpartitionedState.clear();\n+\t\t\treturn CompletableFuture.completedFuture(\n+\t\t\t\tBeamFnApi.StateResponse.newBuilder()\n+\t\t\t\t\t.setId(request.getId())\n+\t\t\t\t\t.setClear(BeamFnApi.StateClearResponse.getDefaultInstance()));\n+\t\t}\n+\n+\t\tprivate ListState<byte[]> getListState(BeamFnApi.StateRequest request) throws Exception {\n+\t\t\tBeamFnApi.StateKey.BagUserState bagUserState = request.getStateKey().getBagUserState();\n+\t\t\tListStateDescriptor<byte[]> flinkStateDescriptor =", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c95e880c06847e6ee07c4e6cdf107ce1fa3fcb11"}, "originalPosition": 356}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTI2NzM4Nw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \t\t\tListStateDescriptor<byte[]> flinkStateDescriptor =\n          \n          \n            \n            \t\t\tListStateDescriptor<byte[]> listStateDescriptor =", "url": "https://github.com/apache/flink/pull/13420#discussion_r491267387", "createdAt": "2020-09-19T04:26:25Z", "author": {"login": "dianfu"}, "path": "flink-python/src/main/java/org/apache/flink/streaming/api/runners/python/beam/BeamPythonStatefulFunctionRunner.java", "diffHunk": "@@ -0,0 +1,368 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.api.runners.python.beam;\n+\n+import org.apache.flink.api.common.ExecutionConfig;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.PrimitiveArrayTypeInfo;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.core.memory.ByteArrayInputStreamWithPos;\n+import org.apache.flink.core.memory.ByteArrayOutputStreamWithPos;\n+import org.apache.flink.core.memory.DataInputViewStreamWrapper;\n+import org.apache.flink.core.memory.DataOutputViewStreamWrapper;\n+import org.apache.flink.python.env.PythonEnvironmentManager;\n+import org.apache.flink.python.metric.FlinkMetricContainer;\n+import org.apache.flink.runtime.state.KeyedStateBackend;\n+import org.apache.flink.runtime.state.VoidNamespace;\n+import org.apache.flink.runtime.state.VoidNamespaceSerializer;\n+\n+import org.apache.beam.model.fnexecution.v1.BeamFnApi;\n+import org.apache.beam.model.pipeline.v1.RunnerApi;\n+import org.apache.beam.runners.core.construction.ModelCoders;\n+import org.apache.beam.runners.core.construction.graph.ExecutableStage;\n+import org.apache.beam.runners.core.construction.graph.ImmutableExecutableStage;\n+import org.apache.beam.runners.core.construction.graph.PipelineNode;\n+import org.apache.beam.runners.core.construction.graph.SideInputReference;\n+import org.apache.beam.runners.core.construction.graph.TimerReference;\n+import org.apache.beam.runners.core.construction.graph.UserStateReference;\n+import org.apache.beam.runners.fnexecution.state.StateRequestHandler;\n+import org.apache.beam.sdk.coders.ByteArrayCoder;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n+import org.apache.beam.sdk.transforms.windowing.GlobalWindow;\n+import org.apache.beam.sdk.util.WindowedValue;\n+import org.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.ByteString;\n+\n+import javax.annotation.Nullable;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+\n+import static org.apache.beam.runners.core.construction.BeamUrns.getUrn;\n+\n+/**\n+ * A {@link BeamPythonStatefulFunctionRunner} used to execute Python stateful functions.\n+ */\n+public abstract class BeamPythonStatefulFunctionRunner extends BeamPythonFunctionRunner {\n+\n+\tprivate static final String USER_STATE_PREFIX = \"user-state-\";\n+\n+\tprivate static final String INPUT_ID = \"input\";\n+\tprivate static final String OUTPUT_ID = \"output\";\n+\tprivate static final String TRANSFORM_ID = \"transform\";\n+\n+\tprivate static final String MAIN_INPUT_NAME = \"input\";\n+\tprivate static final String MAIN_OUTPUT_NAME = \"output\";\n+\n+\tprivate static final String INPUT_CODER_ID = \"input_coder\";\n+\tprivate static final String OUTPUT_CODER_ID = \"output_coder\";\n+\tprivate static final String WINDOW_CODER_ID = \"window_coder\";\n+\n+\tprivate static final String WINDOW_STRATEGY = \"windowing_strategy\";\n+\n+\tprivate final String functionUrn;\n+\n+\tpublic BeamPythonStatefulFunctionRunner(\n+\t\tString taskName,\n+\t\tPythonEnvironmentManager environmentManager,\n+\t\tString functionUrn,\n+\t\tMap<String, String> jobOptions,\n+\t\tFlinkMetricContainer flinkMetricContainer,\n+\t\t@Nullable KeyedStateBackend keyedStateBackend,\n+\t\t@Nullable TypeSerializer keySerializer) {\n+\t\tsuper(\n+\t\t\ttaskName,\n+\t\t\tenvironmentManager,\n+\t\t\tgetStateRequestHandler(keyedStateBackend, keySerializer),\n+\t\t\tjobOptions,\n+\t\t\tflinkMetricContainer);\n+\t\tthis.functionUrn = functionUrn;\n+\t}\n+\n+\t@Override\n+\t@SuppressWarnings(\"unchecked\")\n+\tpublic ExecutableStage createExecutableStage() throws Exception {\n+\t\tRunnerApi.Components components =\n+\t\t\tRunnerApi.Components.newBuilder()\n+\t\t\t\t.putPcollections(\n+\t\t\t\t\tINPUT_ID,\n+\t\t\t\t\tRunnerApi.PCollection.newBuilder()\n+\t\t\t\t\t\t.setWindowingStrategyId(WINDOW_STRATEGY)\n+\t\t\t\t\t\t.setCoderId(INPUT_CODER_ID)\n+\t\t\t\t\t\t.build())\n+\t\t\t\t.putPcollections(\n+\t\t\t\t\tOUTPUT_ID,\n+\t\t\t\t\tRunnerApi.PCollection.newBuilder()\n+\t\t\t\t\t\t.setWindowingStrategyId(WINDOW_STRATEGY)\n+\t\t\t\t\t\t.setCoderId(OUTPUT_CODER_ID)\n+\t\t\t\t\t\t.build())\n+\t\t\t\t.putTransforms(\n+\t\t\t\t\tTRANSFORM_ID,\n+\t\t\t\t\tRunnerApi.PTransform.newBuilder()\n+\t\t\t\t\t\t.setUniqueName(TRANSFORM_ID)\n+\t\t\t\t\t\t.setSpec(RunnerApi.FunctionSpec.newBuilder()\n+\t\t\t\t\t\t\t.setUrn(functionUrn)\n+\t\t\t\t\t\t\t.setPayload(\n+\t\t\t\t\t\t\t\torg.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.ByteString.copyFrom(\n+\t\t\t\t\t\t\t\t\tgetUserDefinedFunctionsProtoBytes()))\n+\t\t\t\t\t\t\t.build())\n+\t\t\t\t\t\t.putInputs(MAIN_INPUT_NAME, INPUT_ID)\n+\t\t\t\t\t\t.putOutputs(MAIN_OUTPUT_NAME, OUTPUT_ID)\n+\t\t\t\t\t\t.build())\n+\t\t\t\t.putWindowingStrategies(\n+\t\t\t\t\tWINDOW_STRATEGY,\n+\t\t\t\t\tRunnerApi.WindowingStrategy.newBuilder()\n+\t\t\t\t\t\t.setWindowCoderId(WINDOW_CODER_ID)\n+\t\t\t\t\t\t.build())\n+\t\t\t\t.putCoders(\n+\t\t\t\t\tINPUT_CODER_ID,\n+\t\t\t\t\tgetInputCoderProto())\n+\t\t\t\t.putCoders(\n+\t\t\t\t\tOUTPUT_CODER_ID,\n+\t\t\t\t\tgetOutputCoderProto())\n+\t\t\t\t.putCoders(\n+\t\t\t\t\tWINDOW_CODER_ID,\n+\t\t\t\t\tgetWindowCoderProto())\n+\t\t\t\t.build();\n+\n+\t\tPipelineNode.PCollectionNode input =\n+\t\t\tPipelineNode.pCollection(INPUT_ID, components.getPcollectionsOrThrow(INPUT_ID));\n+\t\tList<SideInputReference> sideInputs = Collections.EMPTY_LIST;\n+\t\tList<UserStateReference> userStates = Collections.EMPTY_LIST;\n+\t\tList<TimerReference> timers = Collections.EMPTY_LIST;\n+\t\tList<PipelineNode.PTransformNode> transforms =\n+\t\t\tCollections.singletonList(\n+\t\t\t\tPipelineNode.pTransform(TRANSFORM_ID, components.getTransformsOrThrow(TRANSFORM_ID)));\n+\t\tList<PipelineNode.PCollectionNode> outputs =\n+\t\t\tCollections.singletonList(\n+\t\t\t\tPipelineNode.pCollection(OUTPUT_ID, components.getPcollectionsOrThrow(OUTPUT_ID)));\n+\t\treturn ImmutableExecutableStage.of(\n+\t\t\tcomponents, createPythonExecutionEnvironment(), input, sideInputs, userStates, timers, transforms, outputs, createValueOnlyWireCoderSetting());\n+\t}\n+\n+\tprivate Collection<RunnerApi.ExecutableStagePayload.WireCoderSetting> createValueOnlyWireCoderSetting() throws\n+\t\tIOException {\n+\t\tWindowedValue<byte[]> value = WindowedValue.valueInGlobalWindow(new byte[0]);\n+\t\tCoder<? extends BoundedWindow> windowCoder = GlobalWindow.Coder.INSTANCE;\n+\t\tWindowedValue.FullWindowedValueCoder<byte[]> windowedValueCoder =\n+\t\t\tWindowedValue.FullWindowedValueCoder.of(ByteArrayCoder.of(), windowCoder);\n+\t\tByteArrayOutputStream baos = new ByteArrayOutputStream();\n+\t\twindowedValueCoder.encode(value, baos);\n+\n+\t\treturn Arrays.asList(\n+\t\t\tRunnerApi.ExecutableStagePayload.WireCoderSetting.newBuilder()\n+\t\t\t\t.setUrn(getUrn(RunnerApi.StandardCoders.Enum.PARAM_WINDOWED_VALUE))\n+\t\t\t\t.setPayload(\n+\t\t\t\t\torg.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.ByteString.copyFrom(baos.toByteArray()))\n+\t\t\t\t.setInputOrOutputId(INPUT_ID)\n+\t\t\t\t.build(),\n+\t\t\tRunnerApi.ExecutableStagePayload.WireCoderSetting.newBuilder()\n+\t\t\t\t.setUrn(getUrn(RunnerApi.StandardCoders.Enum.PARAM_WINDOWED_VALUE))\n+\t\t\t\t.setPayload(\n+\t\t\t\t\torg.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.ByteString.copyFrom(baos.toByteArray()))\n+\t\t\t\t.setInputOrOutputId(OUTPUT_ID)\n+\t\t\t\t.build()\n+\t\t);\n+\t}\n+\n+\tprotected abstract byte[] getUserDefinedFunctionsProtoBytes();\n+\n+\tprotected abstract RunnerApi.Coder getInputCoderProto();\n+\n+\tprotected abstract RunnerApi.Coder getOutputCoderProto();\n+\n+\t/**\n+\t * Gets the proto representation of the window coder.\n+\t */\n+\tprivate RunnerApi.Coder getWindowCoderProto() {\n+\t\treturn RunnerApi.Coder.newBuilder()\n+\t\t\t.setSpec(\n+\t\t\t\tRunnerApi.FunctionSpec.newBuilder()\n+\t\t\t\t\t.setUrn(ModelCoders.GLOBAL_WINDOW_CODER_URN)\n+\t\t\t\t\t.build())\n+\t\t\t.build();\n+\t}\n+\n+\tprivate static StateRequestHandler getStateRequestHandler(\n+\t\t\tKeyedStateBackend keyedStateBackend,\n+\t\t\tTypeSerializer keySerializer) {\n+\t\tif (keyedStateBackend == null) {\n+\t\t\treturn StateRequestHandler.unsupported();\n+\t\t} else {\n+\t\t\tassert keySerializer != null;\n+\t\t\treturn new SimpleStateRequestHandler(keyedStateBackend, keySerializer);\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * A state request handler which handles the state request from Python side.\n+\t */\n+\tprivate static class SimpleStateRequestHandler implements StateRequestHandler {\n+\n+\t\tprivate final TypeSerializer keySerializer;\n+\t\tprivate final TypeSerializer<byte[]> valueSerializer;\n+\t\tprivate final KeyedStateBackend keyedStateBackend;\n+\n+\t\t/**\n+\t\t * Reusable OutputStream used to holding the serialized input elements.\n+\t\t */\n+\t\tprotected transient ByteArrayOutputStreamWithPos baos;\n+\n+\t\t/**\n+\t\t * Reusable InputStream used to holding the elements to be deserialized.\n+\t\t */\n+\t\tprotected transient ByteArrayInputStreamWithPos bais;\n+\n+\t\t/**\n+\t\t * OutputStream Wrapper.\n+\t\t */\n+\t\tprotected transient DataOutputViewStreamWrapper baosWrapper;\n+\n+\t\t/**\n+\t\t * InputStream Wrapper.\n+\t\t */\n+\t\tprotected transient DataInputViewStreamWrapper baisWrapper;\n+\n+\t\tpublic SimpleStateRequestHandler(\n+\t\t\t\tKeyedStateBackend keyedStateBackend,\n+\t\t\t\tTypeSerializer keySerializer) {\n+\t\t\tthis.keySerializer = keySerializer;\n+\t\t\tthis.valueSerializer = PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO\n+\t\t\t\t.createSerializer(new ExecutionConfig());\n+\t\t\tthis.keyedStateBackend = keyedStateBackend;\n+\t\t\tbaos = new ByteArrayOutputStreamWithPos();\n+\t\t\tbaosWrapper = new DataOutputViewStreamWrapper(baos);\n+\t\t\tbais = new ByteArrayInputStreamWithPos();\n+\t\t\tbaisWrapper = new DataInputViewStreamWrapper(bais);\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic CompletionStage<BeamFnApi.StateResponse.Builder> handle(BeamFnApi.StateRequest request) throws Exception {\n+\t\t\tBeamFnApi.StateKey.TypeCase typeCase = request.getStateKey().getTypeCase();\n+\t\t\tsynchronized (keyedStateBackend) {\n+\t\t\t\tif (typeCase.equals(BeamFnApi.StateKey.TypeCase.BAG_USER_STATE)) {\n+\t\t\t\t\treturn handleBagState(request);\n+\t\t\t\t} else {\n+\t\t\t\t\tthrow new RuntimeException(\"Unsupported state type: \" + typeCase);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\n+\t\tprivate CompletionStage<BeamFnApi.StateResponse.Builder> handleBagState(BeamFnApi.StateRequest request) throws Exception {\n+\n+\t\t\tif (request.getStateKey().hasBagUserState()) {\n+\t\t\t\tBeamFnApi.StateKey.BagUserState bagUserState = request.getStateKey().getBagUserState();\n+\t\t\t\t// get key\n+\t\t\t\tbyte[] keybytes = bagUserState.getKey().toByteArray();\n+\t\t\t\tbais.setBuffer(keybytes, 0, keybytes.length);\n+\t\t\t\tObject key = keySerializer.deserialize(baisWrapper);\n+\t\t\t\tkeyedStateBackend.setCurrentKey(key);\n+\t\t\t} else {\n+\t\t\t\tthrow new RuntimeException(\"Unsupported bag state request: \" + request);\n+\t\t\t}\n+\n+\t\t\tswitch (request.getRequestCase()) {\n+\t\t\t\tcase GET:\n+\t\t\t\t\treturn handleGetRequest(request);\n+\t\t\t\tcase APPEND:\n+\t\t\t\t\treturn handleAppendRequest(request);\n+\t\t\t\tcase CLEAR:\n+\t\t\t\t\treturn handleClearRequest(request);\n+\t\t\t\tdefault:\n+\t\t\t\t\tthrow new RuntimeException(\n+\t\t\t\t\t\tString.format(\n+\t\t\t\t\t\t\t\"Unsupported request type %s for user state.\", request.getRequestCase()));\n+\t\t\t}\n+\t\t}\n+\n+\t\tprivate List<ByteString> convertToByteString(ListState<byte[]> listState) throws Exception {\n+\t\t\tList<ByteString> ret = new LinkedList<>();\n+\t\t\tif (listState.get() == null) {\n+\t\t\t\treturn ret;\n+\t\t\t}\n+\t\t\tfor (byte[] v: listState.get()) {\n+\t\t\t\tret.add(ByteString.copyFrom(v));\n+\t\t\t}\n+\t\t\treturn ret;\n+\t\t}\n+\n+\t\tprivate CompletionStage<BeamFnApi.StateResponse.Builder> handleGetRequest(\n+\t\t\tBeamFnApi.StateRequest request) throws Exception {\n+\n+\t\t\tListState<byte[]> partitionedState = getListState(request);\n+\t\t\tList<ByteString> byteStrings = convertToByteString(partitionedState);\n+\n+\t\t\treturn CompletableFuture.completedFuture(\n+\t\t\t\tBeamFnApi.StateResponse.newBuilder()\n+\t\t\t\t\t.setId(request.getId())\n+\t\t\t\t\t.setGet(\n+\t\t\t\t\t\tBeamFnApi.StateGetResponse.newBuilder()\n+\t\t\t\t\t\t\t.setData(ByteString.copyFrom(byteStrings))));\n+\t\t}\n+\n+\t\tprivate CompletionStage<BeamFnApi.StateResponse.Builder> handleAppendRequest(\n+\t\t\tBeamFnApi.StateRequest request) throws Exception {\n+\n+\t\t\tListState<byte[]> partitionedState = getListState(request);\n+\t\t\t// get values\n+\t\t\tbyte[] valuebytes = request.getAppend().getData().toByteArray();\n+\t\t\tpartitionedState.add(valuebytes);\n+\n+\t\t\treturn CompletableFuture.completedFuture(\n+\t\t\t\tBeamFnApi.StateResponse.newBuilder()\n+\t\t\t\t\t.setId(request.getId())\n+\t\t\t\t\t.setAppend(BeamFnApi.StateAppendResponse.getDefaultInstance()));\n+\t\t}\n+\n+\t\tprivate CompletionStage<BeamFnApi.StateResponse.Builder> handleClearRequest(\n+\t\t\tBeamFnApi.StateRequest request) throws Exception {\n+\n+\t\t\tListState<byte[]> partitionedState = getListState(request);\n+\n+\t\t\tpartitionedState.clear();\n+\t\t\treturn CompletableFuture.completedFuture(\n+\t\t\t\tBeamFnApi.StateResponse.newBuilder()\n+\t\t\t\t\t.setId(request.getId())\n+\t\t\t\t\t.setClear(BeamFnApi.StateClearResponse.getDefaultInstance()));\n+\t\t}\n+\n+\t\tprivate ListState<byte[]> getListState(BeamFnApi.StateRequest request) throws Exception {\n+\t\t\tBeamFnApi.StateKey.BagUserState bagUserState = request.getStateKey().getBagUserState();\n+\t\t\tListStateDescriptor<byte[]> flinkStateDescriptor =", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c95e880c06847e6ee07c4e6cdf107ce1fa3fcb11"}, "originalPosition": 356}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTI2NzQ5Nw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \t\t\tbyte[] valuebytes = request.getAppend().getData().toByteArray();\n          \n          \n            \n            \t\t\tbyte[] valueBytes = request.getAppend().getData().toByteArray();", "url": "https://github.com/apache/flink/pull/13420#discussion_r491267497", "createdAt": "2020-09-19T04:27:38Z", "author": {"login": "dianfu"}, "path": "flink-python/src/main/java/org/apache/flink/streaming/api/runners/python/beam/BeamPythonStatefulFunctionRunner.java", "diffHunk": "@@ -0,0 +1,368 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.api.runners.python.beam;\n+\n+import org.apache.flink.api.common.ExecutionConfig;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.PrimitiveArrayTypeInfo;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.core.memory.ByteArrayInputStreamWithPos;\n+import org.apache.flink.core.memory.ByteArrayOutputStreamWithPos;\n+import org.apache.flink.core.memory.DataInputViewStreamWrapper;\n+import org.apache.flink.core.memory.DataOutputViewStreamWrapper;\n+import org.apache.flink.python.env.PythonEnvironmentManager;\n+import org.apache.flink.python.metric.FlinkMetricContainer;\n+import org.apache.flink.runtime.state.KeyedStateBackend;\n+import org.apache.flink.runtime.state.VoidNamespace;\n+import org.apache.flink.runtime.state.VoidNamespaceSerializer;\n+\n+import org.apache.beam.model.fnexecution.v1.BeamFnApi;\n+import org.apache.beam.model.pipeline.v1.RunnerApi;\n+import org.apache.beam.runners.core.construction.ModelCoders;\n+import org.apache.beam.runners.core.construction.graph.ExecutableStage;\n+import org.apache.beam.runners.core.construction.graph.ImmutableExecutableStage;\n+import org.apache.beam.runners.core.construction.graph.PipelineNode;\n+import org.apache.beam.runners.core.construction.graph.SideInputReference;\n+import org.apache.beam.runners.core.construction.graph.TimerReference;\n+import org.apache.beam.runners.core.construction.graph.UserStateReference;\n+import org.apache.beam.runners.fnexecution.state.StateRequestHandler;\n+import org.apache.beam.sdk.coders.ByteArrayCoder;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n+import org.apache.beam.sdk.transforms.windowing.GlobalWindow;\n+import org.apache.beam.sdk.util.WindowedValue;\n+import org.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.ByteString;\n+\n+import javax.annotation.Nullable;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+\n+import static org.apache.beam.runners.core.construction.BeamUrns.getUrn;\n+\n+/**\n+ * A {@link BeamPythonStatefulFunctionRunner} used to execute Python stateful functions.\n+ */\n+public abstract class BeamPythonStatefulFunctionRunner extends BeamPythonFunctionRunner {\n+\n+\tprivate static final String USER_STATE_PREFIX = \"user-state-\";\n+\n+\tprivate static final String INPUT_ID = \"input\";\n+\tprivate static final String OUTPUT_ID = \"output\";\n+\tprivate static final String TRANSFORM_ID = \"transform\";\n+\n+\tprivate static final String MAIN_INPUT_NAME = \"input\";\n+\tprivate static final String MAIN_OUTPUT_NAME = \"output\";\n+\n+\tprivate static final String INPUT_CODER_ID = \"input_coder\";\n+\tprivate static final String OUTPUT_CODER_ID = \"output_coder\";\n+\tprivate static final String WINDOW_CODER_ID = \"window_coder\";\n+\n+\tprivate static final String WINDOW_STRATEGY = \"windowing_strategy\";\n+\n+\tprivate final String functionUrn;\n+\n+\tpublic BeamPythonStatefulFunctionRunner(\n+\t\tString taskName,\n+\t\tPythonEnvironmentManager environmentManager,\n+\t\tString functionUrn,\n+\t\tMap<String, String> jobOptions,\n+\t\tFlinkMetricContainer flinkMetricContainer,\n+\t\t@Nullable KeyedStateBackend keyedStateBackend,\n+\t\t@Nullable TypeSerializer keySerializer) {\n+\t\tsuper(\n+\t\t\ttaskName,\n+\t\t\tenvironmentManager,\n+\t\t\tgetStateRequestHandler(keyedStateBackend, keySerializer),\n+\t\t\tjobOptions,\n+\t\t\tflinkMetricContainer);\n+\t\tthis.functionUrn = functionUrn;\n+\t}\n+\n+\t@Override\n+\t@SuppressWarnings(\"unchecked\")\n+\tpublic ExecutableStage createExecutableStage() throws Exception {\n+\t\tRunnerApi.Components components =\n+\t\t\tRunnerApi.Components.newBuilder()\n+\t\t\t\t.putPcollections(\n+\t\t\t\t\tINPUT_ID,\n+\t\t\t\t\tRunnerApi.PCollection.newBuilder()\n+\t\t\t\t\t\t.setWindowingStrategyId(WINDOW_STRATEGY)\n+\t\t\t\t\t\t.setCoderId(INPUT_CODER_ID)\n+\t\t\t\t\t\t.build())\n+\t\t\t\t.putPcollections(\n+\t\t\t\t\tOUTPUT_ID,\n+\t\t\t\t\tRunnerApi.PCollection.newBuilder()\n+\t\t\t\t\t\t.setWindowingStrategyId(WINDOW_STRATEGY)\n+\t\t\t\t\t\t.setCoderId(OUTPUT_CODER_ID)\n+\t\t\t\t\t\t.build())\n+\t\t\t\t.putTransforms(\n+\t\t\t\t\tTRANSFORM_ID,\n+\t\t\t\t\tRunnerApi.PTransform.newBuilder()\n+\t\t\t\t\t\t.setUniqueName(TRANSFORM_ID)\n+\t\t\t\t\t\t.setSpec(RunnerApi.FunctionSpec.newBuilder()\n+\t\t\t\t\t\t\t.setUrn(functionUrn)\n+\t\t\t\t\t\t\t.setPayload(\n+\t\t\t\t\t\t\t\torg.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.ByteString.copyFrom(\n+\t\t\t\t\t\t\t\t\tgetUserDefinedFunctionsProtoBytes()))\n+\t\t\t\t\t\t\t.build())\n+\t\t\t\t\t\t.putInputs(MAIN_INPUT_NAME, INPUT_ID)\n+\t\t\t\t\t\t.putOutputs(MAIN_OUTPUT_NAME, OUTPUT_ID)\n+\t\t\t\t\t\t.build())\n+\t\t\t\t.putWindowingStrategies(\n+\t\t\t\t\tWINDOW_STRATEGY,\n+\t\t\t\t\tRunnerApi.WindowingStrategy.newBuilder()\n+\t\t\t\t\t\t.setWindowCoderId(WINDOW_CODER_ID)\n+\t\t\t\t\t\t.build())\n+\t\t\t\t.putCoders(\n+\t\t\t\t\tINPUT_CODER_ID,\n+\t\t\t\t\tgetInputCoderProto())\n+\t\t\t\t.putCoders(\n+\t\t\t\t\tOUTPUT_CODER_ID,\n+\t\t\t\t\tgetOutputCoderProto())\n+\t\t\t\t.putCoders(\n+\t\t\t\t\tWINDOW_CODER_ID,\n+\t\t\t\t\tgetWindowCoderProto())\n+\t\t\t\t.build();\n+\n+\t\tPipelineNode.PCollectionNode input =\n+\t\t\tPipelineNode.pCollection(INPUT_ID, components.getPcollectionsOrThrow(INPUT_ID));\n+\t\tList<SideInputReference> sideInputs = Collections.EMPTY_LIST;\n+\t\tList<UserStateReference> userStates = Collections.EMPTY_LIST;\n+\t\tList<TimerReference> timers = Collections.EMPTY_LIST;\n+\t\tList<PipelineNode.PTransformNode> transforms =\n+\t\t\tCollections.singletonList(\n+\t\t\t\tPipelineNode.pTransform(TRANSFORM_ID, components.getTransformsOrThrow(TRANSFORM_ID)));\n+\t\tList<PipelineNode.PCollectionNode> outputs =\n+\t\t\tCollections.singletonList(\n+\t\t\t\tPipelineNode.pCollection(OUTPUT_ID, components.getPcollectionsOrThrow(OUTPUT_ID)));\n+\t\treturn ImmutableExecutableStage.of(\n+\t\t\tcomponents, createPythonExecutionEnvironment(), input, sideInputs, userStates, timers, transforms, outputs, createValueOnlyWireCoderSetting());\n+\t}\n+\n+\tprivate Collection<RunnerApi.ExecutableStagePayload.WireCoderSetting> createValueOnlyWireCoderSetting() throws\n+\t\tIOException {\n+\t\tWindowedValue<byte[]> value = WindowedValue.valueInGlobalWindow(new byte[0]);\n+\t\tCoder<? extends BoundedWindow> windowCoder = GlobalWindow.Coder.INSTANCE;\n+\t\tWindowedValue.FullWindowedValueCoder<byte[]> windowedValueCoder =\n+\t\t\tWindowedValue.FullWindowedValueCoder.of(ByteArrayCoder.of(), windowCoder);\n+\t\tByteArrayOutputStream baos = new ByteArrayOutputStream();\n+\t\twindowedValueCoder.encode(value, baos);\n+\n+\t\treturn Arrays.asList(\n+\t\t\tRunnerApi.ExecutableStagePayload.WireCoderSetting.newBuilder()\n+\t\t\t\t.setUrn(getUrn(RunnerApi.StandardCoders.Enum.PARAM_WINDOWED_VALUE))\n+\t\t\t\t.setPayload(\n+\t\t\t\t\torg.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.ByteString.copyFrom(baos.toByteArray()))\n+\t\t\t\t.setInputOrOutputId(INPUT_ID)\n+\t\t\t\t.build(),\n+\t\t\tRunnerApi.ExecutableStagePayload.WireCoderSetting.newBuilder()\n+\t\t\t\t.setUrn(getUrn(RunnerApi.StandardCoders.Enum.PARAM_WINDOWED_VALUE))\n+\t\t\t\t.setPayload(\n+\t\t\t\t\torg.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.ByteString.copyFrom(baos.toByteArray()))\n+\t\t\t\t.setInputOrOutputId(OUTPUT_ID)\n+\t\t\t\t.build()\n+\t\t);\n+\t}\n+\n+\tprotected abstract byte[] getUserDefinedFunctionsProtoBytes();\n+\n+\tprotected abstract RunnerApi.Coder getInputCoderProto();\n+\n+\tprotected abstract RunnerApi.Coder getOutputCoderProto();\n+\n+\t/**\n+\t * Gets the proto representation of the window coder.\n+\t */\n+\tprivate RunnerApi.Coder getWindowCoderProto() {\n+\t\treturn RunnerApi.Coder.newBuilder()\n+\t\t\t.setSpec(\n+\t\t\t\tRunnerApi.FunctionSpec.newBuilder()\n+\t\t\t\t\t.setUrn(ModelCoders.GLOBAL_WINDOW_CODER_URN)\n+\t\t\t\t\t.build())\n+\t\t\t.build();\n+\t}\n+\n+\tprivate static StateRequestHandler getStateRequestHandler(\n+\t\t\tKeyedStateBackend keyedStateBackend,\n+\t\t\tTypeSerializer keySerializer) {\n+\t\tif (keyedStateBackend == null) {\n+\t\t\treturn StateRequestHandler.unsupported();\n+\t\t} else {\n+\t\t\tassert keySerializer != null;\n+\t\t\treturn new SimpleStateRequestHandler(keyedStateBackend, keySerializer);\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * A state request handler which handles the state request from Python side.\n+\t */\n+\tprivate static class SimpleStateRequestHandler implements StateRequestHandler {\n+\n+\t\tprivate final TypeSerializer keySerializer;\n+\t\tprivate final TypeSerializer<byte[]> valueSerializer;\n+\t\tprivate final KeyedStateBackend keyedStateBackend;\n+\n+\t\t/**\n+\t\t * Reusable OutputStream used to holding the serialized input elements.\n+\t\t */\n+\t\tprotected transient ByteArrayOutputStreamWithPos baos;\n+\n+\t\t/**\n+\t\t * Reusable InputStream used to holding the elements to be deserialized.\n+\t\t */\n+\t\tprotected transient ByteArrayInputStreamWithPos bais;\n+\n+\t\t/**\n+\t\t * OutputStream Wrapper.\n+\t\t */\n+\t\tprotected transient DataOutputViewStreamWrapper baosWrapper;\n+\n+\t\t/**\n+\t\t * InputStream Wrapper.\n+\t\t */\n+\t\tprotected transient DataInputViewStreamWrapper baisWrapper;\n+\n+\t\tpublic SimpleStateRequestHandler(\n+\t\t\t\tKeyedStateBackend keyedStateBackend,\n+\t\t\t\tTypeSerializer keySerializer) {\n+\t\t\tthis.keySerializer = keySerializer;\n+\t\t\tthis.valueSerializer = PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO\n+\t\t\t\t.createSerializer(new ExecutionConfig());\n+\t\t\tthis.keyedStateBackend = keyedStateBackend;\n+\t\t\tbaos = new ByteArrayOutputStreamWithPos();\n+\t\t\tbaosWrapper = new DataOutputViewStreamWrapper(baos);\n+\t\t\tbais = new ByteArrayInputStreamWithPos();\n+\t\t\tbaisWrapper = new DataInputViewStreamWrapper(bais);\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic CompletionStage<BeamFnApi.StateResponse.Builder> handle(BeamFnApi.StateRequest request) throws Exception {\n+\t\t\tBeamFnApi.StateKey.TypeCase typeCase = request.getStateKey().getTypeCase();\n+\t\t\tsynchronized (keyedStateBackend) {\n+\t\t\t\tif (typeCase.equals(BeamFnApi.StateKey.TypeCase.BAG_USER_STATE)) {\n+\t\t\t\t\treturn handleBagState(request);\n+\t\t\t\t} else {\n+\t\t\t\t\tthrow new RuntimeException(\"Unsupported state type: \" + typeCase);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\n+\t\tprivate CompletionStage<BeamFnApi.StateResponse.Builder> handleBagState(BeamFnApi.StateRequest request) throws Exception {\n+\n+\t\t\tif (request.getStateKey().hasBagUserState()) {\n+\t\t\t\tBeamFnApi.StateKey.BagUserState bagUserState = request.getStateKey().getBagUserState();\n+\t\t\t\t// get key\n+\t\t\t\tbyte[] keybytes = bagUserState.getKey().toByteArray();\n+\t\t\t\tbais.setBuffer(keybytes, 0, keybytes.length);\n+\t\t\t\tObject key = keySerializer.deserialize(baisWrapper);\n+\t\t\t\tkeyedStateBackend.setCurrentKey(key);\n+\t\t\t} else {\n+\t\t\t\tthrow new RuntimeException(\"Unsupported bag state request: \" + request);\n+\t\t\t}\n+\n+\t\t\tswitch (request.getRequestCase()) {\n+\t\t\t\tcase GET:\n+\t\t\t\t\treturn handleGetRequest(request);\n+\t\t\t\tcase APPEND:\n+\t\t\t\t\treturn handleAppendRequest(request);\n+\t\t\t\tcase CLEAR:\n+\t\t\t\t\treturn handleClearRequest(request);\n+\t\t\t\tdefault:\n+\t\t\t\t\tthrow new RuntimeException(\n+\t\t\t\t\t\tString.format(\n+\t\t\t\t\t\t\t\"Unsupported request type %s for user state.\", request.getRequestCase()));\n+\t\t\t}\n+\t\t}\n+\n+\t\tprivate List<ByteString> convertToByteString(ListState<byte[]> listState) throws Exception {\n+\t\t\tList<ByteString> ret = new LinkedList<>();\n+\t\t\tif (listState.get() == null) {\n+\t\t\t\treturn ret;\n+\t\t\t}\n+\t\t\tfor (byte[] v: listState.get()) {\n+\t\t\t\tret.add(ByteString.copyFrom(v));\n+\t\t\t}\n+\t\t\treturn ret;\n+\t\t}\n+\n+\t\tprivate CompletionStage<BeamFnApi.StateResponse.Builder> handleGetRequest(\n+\t\t\tBeamFnApi.StateRequest request) throws Exception {\n+\n+\t\t\tListState<byte[]> partitionedState = getListState(request);\n+\t\t\tList<ByteString> byteStrings = convertToByteString(partitionedState);\n+\n+\t\t\treturn CompletableFuture.completedFuture(\n+\t\t\t\tBeamFnApi.StateResponse.newBuilder()\n+\t\t\t\t\t.setId(request.getId())\n+\t\t\t\t\t.setGet(\n+\t\t\t\t\t\tBeamFnApi.StateGetResponse.newBuilder()\n+\t\t\t\t\t\t\t.setData(ByteString.copyFrom(byteStrings))));\n+\t\t}\n+\n+\t\tprivate CompletionStage<BeamFnApi.StateResponse.Builder> handleAppendRequest(\n+\t\t\tBeamFnApi.StateRequest request) throws Exception {\n+\n+\t\t\tListState<byte[]> partitionedState = getListState(request);\n+\t\t\t// get values\n+\t\t\tbyte[] valuebytes = request.getAppend().getData().toByteArray();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c95e880c06847e6ee07c4e6cdf107ce1fa3fcb11"}, "originalPosition": 333}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTI2Nzg1NQ==", "bodyText": "Add Java doc for the following fields", "url": "https://github.com/apache/flink/pull/13420#discussion_r491267855", "createdAt": "2020-09-19T04:32:15Z", "author": {"login": "dianfu"}, "path": "flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/aggregate/AbstractPythonStreamGroupAggregateOperator.java", "diffHunk": "@@ -0,0 +1,222 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.operators.python.aggregate;\n+\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.fnexecution.v1.FlinkFnApi;\n+import org.apache.flink.python.PythonFunctionRunner;\n+import org.apache.flink.streaming.api.operators.python.AbstractOneInputPythonFunctionOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.functions.python.PythonEnv;\n+import org.apache.flink.table.functions.python.PythonFunctionInfo;\n+import org.apache.flink.table.planner.plan.utils.KeySelectorUtil;\n+import org.apache.flink.table.runtime.keyselector.RowDataKeySelector;\n+import org.apache.flink.table.runtime.operators.python.utils.OperatorUtils;\n+import org.apache.flink.table.runtime.runners.python.beam.BeamTablePythonStatefulFunctionRunner;\n+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n+import org.apache.flink.table.types.logical.BigIntType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.TinyIntType;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.runtime.typeutils.PythonTypeUtils.toProtoType;\n+\n+/**\n+ * The base class for Python stream group aggregate operators.\n+ * @param <IN> Type of the input elements.\n+ * @param <OUT> Type of the output elements.\n+ */\n+public abstract class AbstractPythonStreamGroupAggregateOperator<IN, OUT>\n+\textends AbstractOneInputPythonFunctionOperator<IN, OUT> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate static final int DEFAULT_STATE_REF_CACHE_SIZE = 1000;\n+\n+\t/**\n+\t * The input logical type.\n+\t */\n+\tprotected final RowType inputType;\n+\n+\t/**\n+\t * The output logical type.\n+\t */\n+\tprotected final RowType outputType;\n+\n+\t/**\n+\t * The options used to configure the Python worker process.\n+\t */\n+\tprivate final Map<String, String> jobOptions;\n+\n+\tprivate final PythonFunctionInfo[] aggregateFunctions;\n+\n+\tprivate final int[] grouping;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c95e880c06847e6ee07c4e6cdf107ce1fa3fcb11"}, "originalPosition": 77}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTI2ODAzOA==", "bodyText": "What about python.state.cache.size?", "url": "https://github.com/apache/flink/pull/13420#discussion_r491268038", "createdAt": "2020-09-19T04:34:27Z", "author": {"login": "dianfu"}, "path": "flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/aggregate/AbstractPythonStreamGroupAggregateOperator.java", "diffHunk": "@@ -0,0 +1,222 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.operators.python.aggregate;\n+\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.fnexecution.v1.FlinkFnApi;\n+import org.apache.flink.python.PythonFunctionRunner;\n+import org.apache.flink.streaming.api.operators.python.AbstractOneInputPythonFunctionOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.functions.python.PythonEnv;\n+import org.apache.flink.table.functions.python.PythonFunctionInfo;\n+import org.apache.flink.table.planner.plan.utils.KeySelectorUtil;\n+import org.apache.flink.table.runtime.keyselector.RowDataKeySelector;\n+import org.apache.flink.table.runtime.operators.python.utils.OperatorUtils;\n+import org.apache.flink.table.runtime.runners.python.beam.BeamTablePythonStatefulFunctionRunner;\n+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n+import org.apache.flink.table.types.logical.BigIntType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.TinyIntType;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.runtime.typeutils.PythonTypeUtils.toProtoType;\n+\n+/**\n+ * The base class for Python stream group aggregate operators.\n+ * @param <IN> Type of the input elements.\n+ * @param <OUT> Type of the output elements.\n+ */\n+public abstract class AbstractPythonStreamGroupAggregateOperator<IN, OUT>\n+\textends AbstractOneInputPythonFunctionOperator<IN, OUT> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate static final int DEFAULT_STATE_REF_CACHE_SIZE = 1000;\n+\n+\t/**\n+\t * The input logical type.\n+\t */\n+\tprotected final RowType inputType;\n+\n+\t/**\n+\t * The output logical type.\n+\t */\n+\tprotected final RowType outputType;\n+\n+\t/**\n+\t * The options used to configure the Python worker process.\n+\t */\n+\tprivate final Map<String, String> jobOptions;\n+\n+\tprivate final PythonFunctionInfo[] aggregateFunctions;\n+\n+\tprivate final int[] grouping;\n+\n+\tprivate final int indexOfCountStar;\n+\n+\tprivate final boolean generateUpdateBefore;\n+\n+\tprotected final long minRetentionTime;\n+\n+\tprotected final long maxRetentionTime;\n+\n+\tprivate int stateRefCacheSize = DEFAULT_STATE_REF_CACHE_SIZE;\n+\n+\tprotected final boolean stateCleaningEnabled;\n+\n+\tprivate transient Object keyForTimerService;\n+\n+\t/**\n+\t * The user-defined function input logical type.\n+\t */\n+\tprotected transient RowType userDefinedFunctionInputType;\n+\n+\tpublic AbstractPythonStreamGroupAggregateOperator(\n+\t\t\tConfiguration config,\n+\t\t\tRowType inputType,\n+\t\t\tRowType outputType,\n+\t\t\tPythonFunctionInfo[] aggregateFunctions,\n+\t\t\tint[] grouping,\n+\t\t\tint indexOfCountStar,\n+\t\t\tboolean generateUpdateBefore,\n+\t\t\tlong minRetentionTime,\n+\t\t\tlong maxRetentionTime) {\n+\t\tsuper(config);\n+\t\tthis.inputType = Preconditions.checkNotNull(inputType);\n+\t\tthis.outputType = Preconditions.checkNotNull(outputType);\n+\t\tthis.aggregateFunctions = aggregateFunctions;\n+\t\tthis.jobOptions = buildJobOptions(config);\n+\t\tthis.grouping = grouping;\n+\t\tthis.indexOfCountStar = indexOfCountStar;\n+\t\tthis.generateUpdateBefore = generateUpdateBefore;\n+\t\tthis.minRetentionTime = minRetentionTime;\n+\t\tthis.maxRetentionTime = maxRetentionTime;\n+\t\tthis.stateCleaningEnabled = minRetentionTime > 1;\n+\t\tthis.keyForTimerService = null;\n+\t}\n+\n+\tprivate Map<String, String> buildJobOptions(Configuration config) {\n+\t\tMap<String, String> jobOptions = new HashMap<>();\n+\t\tif (config.containsKey(\"table.exec.timezone\")) {\n+\t\t\tjobOptions.put(\"table.exec.timezone\", config.getString(\"table.exec.timezone\", null));\n+\t\t}\n+\t\tif (config.containsKey(\"python.state.ref.size\")) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c95e880c06847e6ee07c4e6cdf107ce1fa3fcb11"}, "originalPosition": 127}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTI2ODE0NA==", "bodyText": "can be removed", "url": "https://github.com/apache/flink/pull/13420#discussion_r491268144", "createdAt": "2020-09-19T04:35:38Z", "author": {"login": "dianfu"}, "path": "flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/aggregate/AbstractPythonStreamGroupAggregateOperator.java", "diffHunk": "@@ -0,0 +1,222 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.operators.python.aggregate;\n+\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.fnexecution.v1.FlinkFnApi;\n+import org.apache.flink.python.PythonFunctionRunner;\n+import org.apache.flink.streaming.api.operators.python.AbstractOneInputPythonFunctionOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.functions.python.PythonEnv;\n+import org.apache.flink.table.functions.python.PythonFunctionInfo;\n+import org.apache.flink.table.planner.plan.utils.KeySelectorUtil;\n+import org.apache.flink.table.runtime.keyselector.RowDataKeySelector;\n+import org.apache.flink.table.runtime.operators.python.utils.OperatorUtils;\n+import org.apache.flink.table.runtime.runners.python.beam.BeamTablePythonStatefulFunctionRunner;\n+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n+import org.apache.flink.table.types.logical.BigIntType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.TinyIntType;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.runtime.typeutils.PythonTypeUtils.toProtoType;\n+\n+/**\n+ * The base class for Python stream group aggregate operators.\n+ * @param <IN> Type of the input elements.\n+ * @param <OUT> Type of the output elements.\n+ */\n+public abstract class AbstractPythonStreamGroupAggregateOperator<IN, OUT>\n+\textends AbstractOneInputPythonFunctionOperator<IN, OUT> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate static final int DEFAULT_STATE_REF_CACHE_SIZE = 1000;\n+\n+\t/**\n+\t * The input logical type.\n+\t */\n+\tprotected final RowType inputType;\n+\n+\t/**\n+\t * The output logical type.\n+\t */\n+\tprotected final RowType outputType;\n+\n+\t/**\n+\t * The options used to configure the Python worker process.\n+\t */\n+\tprivate final Map<String, String> jobOptions;\n+\n+\tprivate final PythonFunctionInfo[] aggregateFunctions;\n+\n+\tprivate final int[] grouping;\n+\n+\tprivate final int indexOfCountStar;\n+\n+\tprivate final boolean generateUpdateBefore;\n+\n+\tprotected final long minRetentionTime;\n+\n+\tprotected final long maxRetentionTime;\n+\n+\tprivate int stateRefCacheSize = DEFAULT_STATE_REF_CACHE_SIZE;\n+\n+\tprotected final boolean stateCleaningEnabled;\n+\n+\tprivate transient Object keyForTimerService;\n+\n+\t/**\n+\t * The user-defined function input logical type.\n+\t */\n+\tprotected transient RowType userDefinedFunctionInputType;\n+\n+\tpublic AbstractPythonStreamGroupAggregateOperator(\n+\t\t\tConfiguration config,\n+\t\t\tRowType inputType,\n+\t\t\tRowType outputType,\n+\t\t\tPythonFunctionInfo[] aggregateFunctions,\n+\t\t\tint[] grouping,\n+\t\t\tint indexOfCountStar,\n+\t\t\tboolean generateUpdateBefore,\n+\t\t\tlong minRetentionTime,\n+\t\t\tlong maxRetentionTime) {\n+\t\tsuper(config);\n+\t\tthis.inputType = Preconditions.checkNotNull(inputType);\n+\t\tthis.outputType = Preconditions.checkNotNull(outputType);\n+\t\tthis.aggregateFunctions = aggregateFunctions;\n+\t\tthis.jobOptions = buildJobOptions(config);\n+\t\tthis.grouping = grouping;\n+\t\tthis.indexOfCountStar = indexOfCountStar;\n+\t\tthis.generateUpdateBefore = generateUpdateBefore;\n+\t\tthis.minRetentionTime = minRetentionTime;\n+\t\tthis.maxRetentionTime = maxRetentionTime;\n+\t\tthis.stateCleaningEnabled = minRetentionTime > 1;\n+\t\tthis.keyForTimerService = null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c95e880c06847e6ee07c4e6cdf107ce1fa3fcb11"}, "originalPosition": 119}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTI2ODIzMQ==", "bodyText": "Add an open method and init userDefinedFunctionInputType in the open method?", "url": "https://github.com/apache/flink/pull/13420#discussion_r491268231", "createdAt": "2020-09-19T04:37:03Z", "author": {"login": "dianfu"}, "path": "flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/aggregate/AbstractPythonStreamGroupAggregateOperator.java", "diffHunk": "@@ -0,0 +1,222 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.operators.python.aggregate;\n+\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.fnexecution.v1.FlinkFnApi;\n+import org.apache.flink.python.PythonFunctionRunner;\n+import org.apache.flink.streaming.api.operators.python.AbstractOneInputPythonFunctionOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.functions.python.PythonEnv;\n+import org.apache.flink.table.functions.python.PythonFunctionInfo;\n+import org.apache.flink.table.planner.plan.utils.KeySelectorUtil;\n+import org.apache.flink.table.runtime.keyselector.RowDataKeySelector;\n+import org.apache.flink.table.runtime.operators.python.utils.OperatorUtils;\n+import org.apache.flink.table.runtime.runners.python.beam.BeamTablePythonStatefulFunctionRunner;\n+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n+import org.apache.flink.table.types.logical.BigIntType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.TinyIntType;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.runtime.typeutils.PythonTypeUtils.toProtoType;\n+\n+/**\n+ * The base class for Python stream group aggregate operators.\n+ * @param <IN> Type of the input elements.\n+ * @param <OUT> Type of the output elements.\n+ */\n+public abstract class AbstractPythonStreamGroupAggregateOperator<IN, OUT>\n+\textends AbstractOneInputPythonFunctionOperator<IN, OUT> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate static final int DEFAULT_STATE_REF_CACHE_SIZE = 1000;\n+\n+\t/**\n+\t * The input logical type.\n+\t */\n+\tprotected final RowType inputType;\n+\n+\t/**\n+\t * The output logical type.\n+\t */\n+\tprotected final RowType outputType;\n+\n+\t/**\n+\t * The options used to configure the Python worker process.\n+\t */\n+\tprivate final Map<String, String> jobOptions;\n+\n+\tprivate final PythonFunctionInfo[] aggregateFunctions;\n+\n+\tprivate final int[] grouping;\n+\n+\tprivate final int indexOfCountStar;\n+\n+\tprivate final boolean generateUpdateBefore;\n+\n+\tprotected final long minRetentionTime;\n+\n+\tprotected final long maxRetentionTime;\n+\n+\tprivate int stateRefCacheSize = DEFAULT_STATE_REF_CACHE_SIZE;\n+\n+\tprotected final boolean stateCleaningEnabled;\n+\n+\tprivate transient Object keyForTimerService;\n+\n+\t/**\n+\t * The user-defined function input logical type.\n+\t */\n+\tprotected transient RowType userDefinedFunctionInputType;\n+\n+\tpublic AbstractPythonStreamGroupAggregateOperator(\n+\t\t\tConfiguration config,\n+\t\t\tRowType inputType,\n+\t\t\tRowType outputType,\n+\t\t\tPythonFunctionInfo[] aggregateFunctions,\n+\t\t\tint[] grouping,\n+\t\t\tint indexOfCountStar,\n+\t\t\tboolean generateUpdateBefore,\n+\t\t\tlong minRetentionTime,\n+\t\t\tlong maxRetentionTime) {\n+\t\tsuper(config);\n+\t\tthis.inputType = Preconditions.checkNotNull(inputType);\n+\t\tthis.outputType = Preconditions.checkNotNull(outputType);\n+\t\tthis.aggregateFunctions = aggregateFunctions;\n+\t\tthis.jobOptions = buildJobOptions(config);\n+\t\tthis.grouping = grouping;\n+\t\tthis.indexOfCountStar = indexOfCountStar;\n+\t\tthis.generateUpdateBefore = generateUpdateBefore;\n+\t\tthis.minRetentionTime = minRetentionTime;\n+\t\tthis.maxRetentionTime = maxRetentionTime;\n+\t\tthis.stateCleaningEnabled = minRetentionTime > 1;\n+\t\tthis.keyForTimerService = null;\n+\t}\n+\n+\tprivate Map<String, String> buildJobOptions(Configuration config) {\n+\t\tMap<String, String> jobOptions = new HashMap<>();\n+\t\tif (config.containsKey(\"table.exec.timezone\")) {\n+\t\t\tjobOptions.put(\"table.exec.timezone\", config.getString(\"table.exec.timezone\", null));\n+\t\t}\n+\t\tif (config.containsKey(\"python.state.ref.size\")) {\n+\t\t\tstateRefCacheSize = Integer.valueOf(config.getString(\"python.state.ref.cache.size\", null));\n+\t\t}\n+\t\treturn jobOptions;\n+\t}\n+\n+\t@Override\n+\tpublic void setCurrentKey(Object key) {\n+\t\tkeyForTimerService = key;\n+\t}\n+\n+\t@Override\n+\tpublic Object getCurrentKey() {\n+\t\treturn keyForTimerService;\n+\t}\n+\n+\t@Override\n+\tpublic void processElement(StreamRecord<IN> element) throws Exception {\n+\t\tIN value = element.getValue();\n+\t\tprocessElementInternal(value);\n+\t\telementCount++;\n+\t\tcheckInvokeFinishBundleByCount();\n+\t\temitResults();\n+\t}\n+\n+\t@Override\n+\tpublic PythonFunctionRunner createPythonFunctionRunner() throws Exception {\n+\n+\t\treturn new BeamTablePythonStatefulFunctionRunner(\n+\t\t\tgetRuntimeContext().getTaskName(),\n+\t\t\tcreatePythonEnvironmentManager(),\n+\t\t\tgetUserDefinedFunctionInputType(),\n+\t\t\toutputType,\n+\t\t\tgetFunctionUrn(),\n+\t\t\tgetUserDefinedFunctionsProto(),\n+\t\t\tgetInputCoderUrn(),\n+\t\t\tgetOutputCoderUrn(),\n+\t\t\tjobOptions,\n+\t\t\tgetFlinkMetricContainer(),\n+\t\t\tgetKeyedStateBackend(),\n+\t\t\tgetKeySerializer());\n+\t}\n+\n+\t@Override\n+\tpublic PythonEnv getPythonEnv() {\n+\t\treturn aggregateFunctions[0].getPythonFunction().getPythonEnv();\n+\t}\n+\n+\tprotected RowType getKeyType() {\n+\t\tRowDataKeySelector selector = KeySelectorUtil.getRowDataSelector(\n+\t\t\tgrouping,\n+\t\t\tInternalTypeInfo.of(inputType));\n+\t\treturn selector.getProducedType().toRowType();\n+\t}\n+\n+\tprotected RowType getUserDefinedFunctionInputType() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c95e880c06847e6ee07c4e6cdf107ce1fa3fcb11"}, "originalPosition": 182}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTI2ODQ1Ng==", "bodyText": "Add Java doc on why we need to override setCurrentKey", "url": "https://github.com/apache/flink/pull/13420#discussion_r491268456", "createdAt": "2020-09-19T04:40:24Z", "author": {"login": "dianfu"}, "path": "flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/aggregate/AbstractPythonStreamGroupAggregateOperator.java", "diffHunk": "@@ -0,0 +1,222 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.operators.python.aggregate;\n+\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.fnexecution.v1.FlinkFnApi;\n+import org.apache.flink.python.PythonFunctionRunner;\n+import org.apache.flink.streaming.api.operators.python.AbstractOneInputPythonFunctionOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.functions.python.PythonEnv;\n+import org.apache.flink.table.functions.python.PythonFunctionInfo;\n+import org.apache.flink.table.planner.plan.utils.KeySelectorUtil;\n+import org.apache.flink.table.runtime.keyselector.RowDataKeySelector;\n+import org.apache.flink.table.runtime.operators.python.utils.OperatorUtils;\n+import org.apache.flink.table.runtime.runners.python.beam.BeamTablePythonStatefulFunctionRunner;\n+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n+import org.apache.flink.table.types.logical.BigIntType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.TinyIntType;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.runtime.typeutils.PythonTypeUtils.toProtoType;\n+\n+/**\n+ * The base class for Python stream group aggregate operators.\n+ * @param <IN> Type of the input elements.\n+ * @param <OUT> Type of the output elements.\n+ */\n+public abstract class AbstractPythonStreamGroupAggregateOperator<IN, OUT>\n+\textends AbstractOneInputPythonFunctionOperator<IN, OUT> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate static final int DEFAULT_STATE_REF_CACHE_SIZE = 1000;\n+\n+\t/**\n+\t * The input logical type.\n+\t */\n+\tprotected final RowType inputType;\n+\n+\t/**\n+\t * The output logical type.\n+\t */\n+\tprotected final RowType outputType;\n+\n+\t/**\n+\t * The options used to configure the Python worker process.\n+\t */\n+\tprivate final Map<String, String> jobOptions;\n+\n+\tprivate final PythonFunctionInfo[] aggregateFunctions;\n+\n+\tprivate final int[] grouping;\n+\n+\tprivate final int indexOfCountStar;\n+\n+\tprivate final boolean generateUpdateBefore;\n+\n+\tprotected final long minRetentionTime;\n+\n+\tprotected final long maxRetentionTime;\n+\n+\tprivate int stateRefCacheSize = DEFAULT_STATE_REF_CACHE_SIZE;\n+\n+\tprotected final boolean stateCleaningEnabled;\n+\n+\tprivate transient Object keyForTimerService;\n+\n+\t/**\n+\t * The user-defined function input logical type.\n+\t */\n+\tprotected transient RowType userDefinedFunctionInputType;\n+\n+\tpublic AbstractPythonStreamGroupAggregateOperator(\n+\t\t\tConfiguration config,\n+\t\t\tRowType inputType,\n+\t\t\tRowType outputType,\n+\t\t\tPythonFunctionInfo[] aggregateFunctions,\n+\t\t\tint[] grouping,\n+\t\t\tint indexOfCountStar,\n+\t\t\tboolean generateUpdateBefore,\n+\t\t\tlong minRetentionTime,\n+\t\t\tlong maxRetentionTime) {\n+\t\tsuper(config);\n+\t\tthis.inputType = Preconditions.checkNotNull(inputType);\n+\t\tthis.outputType = Preconditions.checkNotNull(outputType);\n+\t\tthis.aggregateFunctions = aggregateFunctions;\n+\t\tthis.jobOptions = buildJobOptions(config);\n+\t\tthis.grouping = grouping;\n+\t\tthis.indexOfCountStar = indexOfCountStar;\n+\t\tthis.generateUpdateBefore = generateUpdateBefore;\n+\t\tthis.minRetentionTime = minRetentionTime;\n+\t\tthis.maxRetentionTime = maxRetentionTime;\n+\t\tthis.stateCleaningEnabled = minRetentionTime > 1;\n+\t\tthis.keyForTimerService = null;\n+\t}\n+\n+\tprivate Map<String, String> buildJobOptions(Configuration config) {\n+\t\tMap<String, String> jobOptions = new HashMap<>();\n+\t\tif (config.containsKey(\"table.exec.timezone\")) {\n+\t\t\tjobOptions.put(\"table.exec.timezone\", config.getString(\"table.exec.timezone\", null));\n+\t\t}\n+\t\tif (config.containsKey(\"python.state.ref.size\")) {\n+\t\t\tstateRefCacheSize = Integer.valueOf(config.getString(\"python.state.ref.cache.size\", null));\n+\t\t}\n+\t\treturn jobOptions;\n+\t}\n+\n+\t@Override\n+\tpublic void setCurrentKey(Object key) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c95e880c06847e6ee07c4e6cdf107ce1fa3fcb11"}, "originalPosition": 134}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTI2ODg0OA==", "bodyText": "Is there other classes that will extend AbstractPythonStreamGroupAggregateOperator besides RowDataPythonStreamGroupAggregateOperator? If not, we can merge RowDataPythonStreamGroupAggregateOperator and AbstractPythonStreamGroupAggregateOperator together.", "url": "https://github.com/apache/flink/pull/13420#discussion_r491268848", "createdAt": "2020-09-19T04:45:14Z", "author": {"login": "dianfu"}, "path": "flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/aggregate/AbstractPythonStreamGroupAggregateOperator.java", "diffHunk": "@@ -0,0 +1,222 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.operators.python.aggregate;\n+\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.fnexecution.v1.FlinkFnApi;\n+import org.apache.flink.python.PythonFunctionRunner;\n+import org.apache.flink.streaming.api.operators.python.AbstractOneInputPythonFunctionOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.functions.python.PythonEnv;\n+import org.apache.flink.table.functions.python.PythonFunctionInfo;\n+import org.apache.flink.table.planner.plan.utils.KeySelectorUtil;\n+import org.apache.flink.table.runtime.keyselector.RowDataKeySelector;\n+import org.apache.flink.table.runtime.operators.python.utils.OperatorUtils;\n+import org.apache.flink.table.runtime.runners.python.beam.BeamTablePythonStatefulFunctionRunner;\n+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n+import org.apache.flink.table.types.logical.BigIntType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.TinyIntType;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.runtime.typeutils.PythonTypeUtils.toProtoType;\n+\n+/**\n+ * The base class for Python stream group aggregate operators.\n+ * @param <IN> Type of the input elements.\n+ * @param <OUT> Type of the output elements.\n+ */\n+public abstract class AbstractPythonStreamGroupAggregateOperator<IN, OUT>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c95e880c06847e6ee07c4e6cdf107ce1fa3fcb11"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTI2OTEyNA==", "bodyText": "the name doesn't apply here?", "url": "https://github.com/apache/flink/pull/13420#discussion_r491269124", "createdAt": "2020-09-19T04:49:12Z", "author": {"login": "dianfu"}, "path": "flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/aggregate/RowDataPythonStreamGroupAggregateOperator.java", "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.operators.python.aggregate;\n+\n+import org.apache.flink.api.common.state.ValueState;\n+import org.apache.flink.api.common.state.ValueStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.Types;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.memory.ByteArrayInputStreamWithPos;\n+import org.apache.flink.core.memory.ByteArrayOutputStreamWithPos;\n+import org.apache.flink.core.memory.DataInputViewStreamWrapper;\n+import org.apache.flink.core.memory.DataOutputViewStreamWrapper;\n+import org.apache.flink.runtime.state.VoidNamespace;\n+import org.apache.flink.runtime.state.VoidNamespaceSerializer;\n+import org.apache.flink.streaming.api.SimpleTimerService;\n+import org.apache.flink.streaming.api.TimerService;\n+import org.apache.flink.streaming.api.operators.InternalTimer;\n+import org.apache.flink.streaming.api.operators.Triggerable;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.UpdatableRowData;\n+import org.apache.flink.table.functions.python.PythonFunctionInfo;\n+import org.apache.flink.table.runtime.functions.CleanupState;\n+import org.apache.flink.table.runtime.operators.python.utils.OperatorUtils;\n+import org.apache.flink.table.runtime.typeutils.PythonTypeUtils;\n+import org.apache.flink.table.types.logical.RowType;\n+\n+/**\n+ *  The Python AggregateFunction operator for the blink planner.\n+ */\n+public class RowDataPythonStreamGroupAggregateOperator\n+\textends AbstractPythonStreamGroupAggregateOperator<RowData, RowData>\n+\timplements Triggerable<RowData, VoidNamespace>, CleanupState {\n+\n+\tprivate static final String FLINK_AGGREGATE_FUNCTION_INPUT_SCHEMA_CODER_URN =\n+\t\t\"flink:coder:schema:aggregate_function_input:v1\";\n+\tprivate static final String FLINK_AGGREGATE_FUNCTION_OUTPUT_SCHEMA_CODER_URN =\n+\t\t\"flink:coder:schema:aggregate_function_output:v1\";\n+\tprivate static final String STREAM_GROUP_AGGREGATE_URN = \"flink:transform:stream_group_aggregate:v1\";\n+\tprotected static final byte NORMAL_RECORD = 0;\n+\tprivate static final byte TRIGGER_TIMER = 1;\n+\n+\t/**\n+\t * The TypeSerializer for udf execution results.\n+\t */\n+\tprotected transient TypeSerializer<RowData> udfOutputTypeSerializer;\n+\n+\t/**\n+\t * The TypeSerializer for udf input elements.\n+\t */\n+\tprotected transient TypeSerializer<RowData> udfInputTypeSerializer;\n+\n+\t/**\n+\t * Reusable InputStream used to holding the execution results to be deserialized.\n+\t */\n+\tprivate transient ByteArrayInputStreamWithPos bais;\n+\n+\t/**\n+\t * InputStream Wrapper.\n+\t */\n+\tprivate transient DataInputViewStreamWrapper baisWrapper;\n+\n+\t/**\n+\t * Reusable OutputStream used to holding the serialized input elements.\n+\t */\n+\tprivate transient ByteArrayOutputStreamWithPos baos;\n+\n+\t/**\n+\t * OutputStream Wrapper.\n+\t */\n+\tprivate transient DataOutputViewStreamWrapper baosWrapper;\n+\n+\tprivate transient TimerService timerService;\n+\n+\t// holds the latest registered cleanup timer\n+\tprivate transient ValueState<Long> cleanupTimeState;\n+\n+\tprivate transient UpdatableRowData reuseRowData;\n+\tprivate transient UpdatableRowData reuseTimerRowData;\n+\n+\t/**\n+\t * The collector used to collect records.\n+\t */\n+\tprivate transient OperatorUtils.StreamRecordRowDataWrappingCollector rowDataWrapper;\n+\n+\tpublic RowDataPythonStreamGroupAggregateOperator(\n+\t\t\tConfiguration config,\n+\t\t\tRowType inputType,\n+\t\t\tRowType outputType,\n+\t\t\tPythonFunctionInfo[] aggregateFunctions,\n+\t\t\tint[] grouping,\n+\t\t\tint indexOfCountStar,\n+\t\t\tboolean generateUpdateBefore,\n+\t\t\tlong minRetentionTime,\n+\t\t\tlong maxRetentionTime) {\n+\t\tsuper(\n+\t\t\tconfig,\n+\t\t\tinputType,\n+\t\t\toutputType,\n+\t\t\taggregateFunctions,\n+\t\t\tgrouping,\n+\t\t\tindexOfCountStar,\n+\t\t\tgenerateUpdateBefore,\n+\t\t\tminRetentionTime,\n+\t\t\tmaxRetentionTime);\n+\t}\n+\n+\t@Override\n+\tpublic void open() throws Exception {\n+\t\tudfInputTypeSerializer = PythonTypeUtils.toBlinkTypeSerializer(getUserDefinedFunctionInputType());\n+\t\tudfOutputTypeSerializer = PythonTypeUtils.toBlinkTypeSerializer(outputType);\n+\t\tbais = new ByteArrayInputStreamWithPos();\n+\t\tbaisWrapper = new DataInputViewStreamWrapper(bais);\n+\t\tbaos = new ByteArrayOutputStreamWithPos();\n+\t\tbaosWrapper = new DataOutputViewStreamWrapper(baos);\n+\t\ttimerService = new SimpleTimerService(\n+\t\t\tgetInternalTimerService(\"state-clean-timer\", VoidNamespaceSerializer.INSTANCE, this));\n+\t\treuseRowData = new UpdatableRowData(GenericRowData.of(NORMAL_RECORD, null, null, null), 4);\n+\t\treuseTimerRowData = new UpdatableRowData(GenericRowData.of(TRIGGER_TIMER, null, null, null), 4);\n+\t\trowDataWrapper = new OperatorUtils.StreamRecordRowDataWrappingCollector(output);\n+\t\tinitCleanupTimeState();\n+\t\tsuper.open();\n+\t}\n+\n+\t@Override\n+\tTypeSerializer getKeySerializer() {\n+\t\treturn PythonTypeUtils.toBlinkTypeSerializer(getKeyType());\n+\t}\n+\n+\t@Override\n+\tpublic void processElementInternal(RowData value) throws Exception {\n+\t\tlong currentTime = timerService.currentProcessingTime();\n+\t\tregisterProcessingCleanupTimer(currentTime);\n+\t\treuseRowData.setField(1, value);\n+\t\tudfInputTypeSerializer.serialize(reuseRowData, baosWrapper);\n+\t\tpythonFunctionRunner.process(baos.toByteArray());\n+\t\tbaos.reset();\n+\t}\n+\n+\t@Override\n+\tString getInputCoderUrn() {\n+\t\treturn FLINK_AGGREGATE_FUNCTION_INPUT_SCHEMA_CODER_URN;\n+\t}\n+\n+\t@Override\n+\tString getOutputCoderUrn() {\n+\t\treturn FLINK_AGGREGATE_FUNCTION_OUTPUT_SCHEMA_CODER_URN;\n+\t}\n+\n+\t@Override\n+\tpublic String getFunctionUrn() {\n+\t\treturn STREAM_GROUP_AGGREGATE_URN;\n+\t}\n+\n+\t@Override\n+\tpublic void emitResult(Tuple2<byte[], Integer> resultTuple) throws Exception {\n+\t\tbyte[] rawUdfResult = resultTuple.f0;\n+\t\tint length = resultTuple.f1;\n+\t\tbais.setBuffer(rawUdfResult, 0, length);\n+\t\tRowData udfResult = udfOutputTypeSerializer.deserialize(baisWrapper);\n+\t\trowDataWrapper.collect(udfResult);\n+\t}\n+\n+\t/**\n+\t * Invoked when an event-time timer fires.\n+\t */\n+\t@Override\n+\tpublic void onEventTime(InternalTimer<RowData, VoidNamespace> timer) {\n+\t\t// ignore\n+\t}\n+\n+\t/**\n+\t * Invoked when a processing-time timer fires.\n+\t */\n+\t@Override\n+\tpublic void onProcessingTime(InternalTimer<RowData, VoidNamespace> timer) throws Exception {\n+\t\tif (stateCleaningEnabled) {\n+\t\t\tRowData key = timer.getKey();\n+\t\t\tlong timestamp = timer.getTimestamp();\n+\t\t\treuseTimerRowData.setLong(2, timestamp);\n+\t\t\treuseTimerRowData.setField(3, key);\n+\t\t\tudfInputTypeSerializer.serialize(reuseTimerRowData, baosWrapper);\n+\t\t\tpythonFunctionRunner.process(baos.toByteArray());\n+\t\t\tbaos.reset();\n+\t\t\telementCount++;\n+\t\t}\n+\t}\n+\n+\tprivate void registerProcessingCleanupTimer(long currentTime) throws Exception {\n+\t\tif (stateCleaningEnabled) {\n+\t\t\tsynchronized (getKeyedStateBackend()) {\n+\t\t\t\tgetKeyedStateBackend().setCurrentKey(getCurrentKey());\n+\t\t\t\tregisterProcessingCleanupTimer(\n+\t\t\t\t\tcleanupTimeState,\n+\t\t\t\t\tcurrentTime,\n+\t\t\t\t\tminRetentionTime,\n+\t\t\t\t\tmaxRetentionTime,\n+\t\t\t\t\ttimerService\n+\t\t\t\t);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate void initCleanupTimeState() {\n+\t\tif (stateCleaningEnabled) {\n+\t\t\tValueStateDescriptor<Long> inputCntDescriptor =", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c95e880c06847e6ee07c4e6cdf107ce1fa3fcb11"}, "originalPosition": 224}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTI2OTY5Mw==", "bodyText": "Make this class as a standalone class?", "url": "https://github.com/apache/flink/pull/13420#discussion_r491269693", "createdAt": "2020-09-19T04:56:45Z", "author": {"login": "dianfu"}, "path": "flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/utils/OperatorUtils.java", "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.operators.python.utils;\n+\n+import org.apache.flink.fnexecution.v1.FlinkFnApi;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.functions.python.PythonFunctionInfo;\n+import org.apache.flink.table.runtime.types.CRow;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.util.Collector;\n+\n+import com.google.protobuf.ByteString;\n+\n+/**\n+ * The collectors used to collect Row values.\n+ */\n+public enum OperatorUtils {\n+\t;\n+\n+\tpublic static FlinkFnApi.UserDefinedFunction getUserDefinedFunctionProto(PythonFunctionInfo pythonFunctionInfo) {\n+\t\tFlinkFnApi.UserDefinedFunction.Builder builder = FlinkFnApi.UserDefinedFunction.newBuilder();\n+\t\tbuilder.setPayload(ByteString.copyFrom(pythonFunctionInfo.getPythonFunction().getSerializedPythonFunction()));\n+\t\tfor (Object input : pythonFunctionInfo.getInputs()) {\n+\t\t\tFlinkFnApi.UserDefinedFunction.Input.Builder inputProto =\n+\t\t\t\tFlinkFnApi.UserDefinedFunction.Input.newBuilder();\n+\t\t\tif (input instanceof PythonFunctionInfo) {\n+\t\t\t\tinputProto.setUdf(getUserDefinedFunctionProto((PythonFunctionInfo) input));\n+\t\t\t} else if (input instanceof Integer) {\n+\t\t\t\tinputProto.setInputOffset((Integer) input);\n+\t\t\t} else {\n+\t\t\t\tinputProto.setInputConstant(ByteString.copyFrom((byte[]) input));\n+\t\t\t}\n+\t\t\tbuilder.addInputs(inputProto);\n+\t\t}\n+\t\treturn builder.build();\n+\t}\n+\n+\t/**\n+\t * The collector is used to convert a {@link Row} to a {@link CRow}.\n+\t */\n+\tpublic static class StreamRecordCRowWrappingCollector implements Collector<Row> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c95e880c06847e6ee07c4e6cdf107ce1fa3fcb11"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTI2OTcwMg==", "bodyText": "Make this class as a standalone class?", "url": "https://github.com/apache/flink/pull/13420#discussion_r491269702", "createdAt": "2020-09-19T04:56:50Z", "author": {"login": "dianfu"}, "path": "flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/utils/OperatorUtils.java", "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.operators.python.utils;\n+\n+import org.apache.flink.fnexecution.v1.FlinkFnApi;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.functions.python.PythonFunctionInfo;\n+import org.apache.flink.table.runtime.types.CRow;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.util.Collector;\n+\n+import com.google.protobuf.ByteString;\n+\n+/**\n+ * The collectors used to collect Row values.\n+ */\n+public enum OperatorUtils {\n+\t;\n+\n+\tpublic static FlinkFnApi.UserDefinedFunction getUserDefinedFunctionProto(PythonFunctionInfo pythonFunctionInfo) {\n+\t\tFlinkFnApi.UserDefinedFunction.Builder builder = FlinkFnApi.UserDefinedFunction.newBuilder();\n+\t\tbuilder.setPayload(ByteString.copyFrom(pythonFunctionInfo.getPythonFunction().getSerializedPythonFunction()));\n+\t\tfor (Object input : pythonFunctionInfo.getInputs()) {\n+\t\t\tFlinkFnApi.UserDefinedFunction.Input.Builder inputProto =\n+\t\t\t\tFlinkFnApi.UserDefinedFunction.Input.newBuilder();\n+\t\t\tif (input instanceof PythonFunctionInfo) {\n+\t\t\t\tinputProto.setUdf(getUserDefinedFunctionProto((PythonFunctionInfo) input));\n+\t\t\t} else if (input instanceof Integer) {\n+\t\t\t\tinputProto.setInputOffset((Integer) input);\n+\t\t\t} else {\n+\t\t\t\tinputProto.setInputConstant(ByteString.copyFrom((byte[]) input));\n+\t\t\t}\n+\t\t\tbuilder.addInputs(inputProto);\n+\t\t}\n+\t\treturn builder.build();\n+\t}\n+\n+\t/**\n+\t * The collector is used to convert a {@link Row} to a {@link CRow}.\n+\t */\n+\tpublic static class StreamRecordCRowWrappingCollector implements Collector<Row> {\n+\n+\t\tprivate final Collector<StreamRecord<CRow>> out;\n+\t\tprivate final CRow reuseCRow = new CRow();\n+\n+\t\t/**\n+\t\t * For Table API & SQL jobs, the timestamp field is not used.\n+\t\t */\n+\t\tprivate final StreamRecord<CRow> reuseStreamRecord = new StreamRecord<>(reuseCRow);\n+\n+\t\tpublic StreamRecordCRowWrappingCollector(Collector<StreamRecord<CRow>> out) {\n+\t\t\tthis.out = out;\n+\t\t}\n+\n+\t\tpublic void setChange(boolean change) {\n+\t\t\tthis.reuseCRow.change_$eq(change);\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void collect(Row record) {\n+\t\t\treuseCRow.row_$eq(record);\n+\t\t\tout.collect(reuseStreamRecord);\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void close() {\n+\t\t\tout.close();\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * The collector is used to convert a {@link RowData} to a {@link StreamRecord}.\n+\t */\n+\tpublic static class StreamRecordRowDataWrappingCollector implements Collector<RowData> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c95e880c06847e6ee07c4e6cdf107ce1fa3fcb11"}, "originalPosition": 91}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTI3MDAyNw==", "bodyText": "Rename to PythonOperatorUtils?", "url": "https://github.com/apache/flink/pull/13420#discussion_r491270027", "createdAt": "2020-09-19T05:00:44Z", "author": {"login": "dianfu"}, "path": "flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/utils/OperatorUtils.java", "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.operators.python.utils;\n+\n+import org.apache.flink.fnexecution.v1.FlinkFnApi;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.functions.python.PythonFunctionInfo;\n+import org.apache.flink.table.runtime.types.CRow;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.util.Collector;\n+\n+import com.google.protobuf.ByteString;\n+\n+/**\n+ * The collectors used to collect Row values.\n+ */\n+public enum OperatorUtils {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c95e880c06847e6ee07c4e6cdf107ce1fa3fcb11"}, "originalPosition": 34}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "87967d2198d73dea7a84808d6c1a5076a1c7c317", "author": {"user": {"login": "WeiZhong94", "name": "Wei Zhong"}}, "url": "https://github.com/apache/flink/commit/87967d2198d73dea7a84808d6c1a5076a1c7c317", "committedDate": "2020-09-21T12:01:24Z", "message": "[FLINK-19229][python] Introduce the PythonStreamGroupAggregateOperator for Python UDAF."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "6588762ae70dc0d4a97fa04c38207a10eb6eb566", "author": {"user": {"login": "WeiZhong94", "name": "Wei Zhong"}}, "url": "https://github.com/apache/flink/commit/6588762ae70dc0d4a97fa04c38207a10eb6eb566", "committedDate": "2020-09-21T08:41:45Z", "message": "address comments"}, "afterCommit": {"oid": "87967d2198d73dea7a84808d6c1a5076a1c7c317", "author": {"user": {"login": "WeiZhong94", "name": "Wei Zhong"}}, "url": "https://github.com/apache/flink/commit/87967d2198d73dea7a84808d6c1a5076a1c7c317", "committedDate": "2020-09-21T12:01:24Z", "message": "[FLINK-19229][python] Introduce the PythonStreamGroupAggregateOperator for Python UDAF."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDkyNTM0MDEx", "url": "https://github.com/apache/flink/pull/13420#pullrequestreview-492534011", "createdAt": "2020-09-21T12:13:16Z", "commit": {"oid": "87967d2198d73dea7a84808d6c1a5076a1c7c317"}, "state": "COMMENTED", "comments": {"totalCount": 13, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMVQxMjoxMzoxNlrOHVNAiQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMVQxMjo1ODo1N1rOHVO0QQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTk5NTI3Mw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            // A list of the user-defined aggregate functions to be executed in an group aggregate operation.\n          \n          \n            \n            // A list of the user-defined aggregate functions to be executed in a group aggregate operation.", "url": "https://github.com/apache/flink/pull/13420#discussion_r491995273", "createdAt": "2020-09-21T12:13:16Z", "author": {"login": "dianfu"}, "path": "flink-python/pyflink/proto/flink-fn-execution.proto", "diffHunk": "@@ -71,6 +71,24 @@ message UserDefinedDataStreamFunctions {\n   bool metric_enabled = 2;\n }\n \n+// A list of the user-defined aggregate functions to be executed in an group aggregate operation.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "87967d2198d73dea7a84808d6c1a5076a1c7c317"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjAwMDg2Nw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             * A {@link BeamPythonFunctionRunner} used to execute Python stateful functions.\n          \n          \n            \n             * A {@link BeamPythonFunctionRunner} used to execute Python functions.", "url": "https://github.com/apache/flink/pull/13420#discussion_r492000867", "createdAt": "2020-09-21T12:23:29Z", "author": {"login": "dianfu"}, "path": "flink-python/src/main/java/org/apache/flink/streaming/api/runners/python/beam/BeamPythonFunctionRunner.java", "diffHunk": "@@ -41,30 +60,60 @@\n import org.apache.beam.runners.fnexecution.control.StageBundleFactory;\n import org.apache.beam.runners.fnexecution.provisioning.JobInfo;\n import org.apache.beam.runners.fnexecution.state.StateRequestHandler;\n+import org.apache.beam.sdk.coders.ByteArrayCoder;\n+import org.apache.beam.sdk.coders.Coder;\n import org.apache.beam.sdk.fn.data.FnDataReceiver;\n import org.apache.beam.sdk.options.PipelineOptionsFactory;\n import org.apache.beam.sdk.options.PortablePipelineOptions;\n+import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n+import org.apache.beam.sdk.transforms.windowing.GlobalWindow;\n import org.apache.beam.sdk.util.WindowedValue;\n+import org.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.ByteString;\n import org.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.Struct;\n import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.Iterables;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n import javax.annotation.Nullable;\n \n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.LinkedList;\n+import java.util.List;\n import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n import java.util.concurrent.LinkedBlockingQueue;\n \n+import static org.apache.beam.runners.core.construction.BeamUrns.getUrn;\n+\n /**\n- * An base class for {@link PythonFunctionRunner} based on beam.\n+ * A {@link BeamPythonFunctionRunner} used to execute Python stateful functions.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "87967d2198d73dea7a84808d6c1a5076a1c7c317"}, "originalPosition": 77}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjAwMTQ4Mw==", "bodyText": "What about python-state-?", "url": "https://github.com/apache/flink/pull/13420#discussion_r492001483", "createdAt": "2020-09-21T12:24:35Z", "author": {"login": "dianfu"}, "path": "flink-python/src/main/java/org/apache/flink/streaming/api/runners/python/beam/BeamPythonFunctionRunner.java", "diffHunk": "@@ -41,30 +60,60 @@\n import org.apache.beam.runners.fnexecution.control.StageBundleFactory;\n import org.apache.beam.runners.fnexecution.provisioning.JobInfo;\n import org.apache.beam.runners.fnexecution.state.StateRequestHandler;\n+import org.apache.beam.sdk.coders.ByteArrayCoder;\n+import org.apache.beam.sdk.coders.Coder;\n import org.apache.beam.sdk.fn.data.FnDataReceiver;\n import org.apache.beam.sdk.options.PipelineOptionsFactory;\n import org.apache.beam.sdk.options.PortablePipelineOptions;\n+import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n+import org.apache.beam.sdk.transforms.windowing.GlobalWindow;\n import org.apache.beam.sdk.util.WindowedValue;\n+import org.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.ByteString;\n import org.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.Struct;\n import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.Iterables;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n import javax.annotation.Nullable;\n \n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.LinkedList;\n+import java.util.List;\n import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n import java.util.concurrent.LinkedBlockingQueue;\n \n+import static org.apache.beam.runners.core.construction.BeamUrns.getUrn;\n+\n /**\n- * An base class for {@link PythonFunctionRunner} based on beam.\n+ * A {@link BeamPythonFunctionRunner} used to execute Python stateful functions.\n  */\n @Internal\n public abstract class BeamPythonFunctionRunner implements PythonFunctionRunner {\n \tprotected static final Logger LOG = LoggerFactory.getLogger(BeamPythonFunctionRunner.class);\n \n-\tprivate transient boolean bundleStarted;\n+\tprivate static final String BEAM_STATE_PREFIX = \"beam-state-\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "87967d2198d73dea7a84808d6c1a5076a1c7c317"}, "originalPosition": 84}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjAwNDU1NQ==", "bodyText": "Can be converted to a local variable.", "url": "https://github.com/apache/flink/pull/13420#discussion_r492004555", "createdAt": "2020-09-21T12:29:26Z", "author": {"login": "dianfu"}, "path": "flink-python/src/main/java/org/apache/flink/streaming/api/runners/python/beam/BeamPythonFunctionRunner.java", "diffHunk": "@@ -309,4 +356,287 @@ private void checkInvokeStartBundle() throws Exception {\n \t\t}\n \t}\n \n+\t/**\n+\t * Creates a {@link ExecutableStage} which contains the Python user-defined functions to be executed\n+\t * and all the other information needed to execute them, such as the execution environment, the input\n+\t * and output coder, etc.\n+\t */\n+\t@SuppressWarnings(\"unchecked\")\n+\tprivate ExecutableStage createExecutableStage() throws Exception {\n+\t\tRunnerApi.Components components =\n+\t\t\tRunnerApi.Components.newBuilder()\n+\t\t\t\t.putPcollections(\n+\t\t\t\t\tINPUT_ID,\n+\t\t\t\t\tRunnerApi.PCollection.newBuilder()\n+\t\t\t\t\t\t.setWindowingStrategyId(WINDOW_STRATEGY)\n+\t\t\t\t\t\t.setCoderId(INPUT_CODER_ID)\n+\t\t\t\t\t\t.build())\n+\t\t\t\t.putPcollections(\n+\t\t\t\t\tOUTPUT_ID,\n+\t\t\t\t\tRunnerApi.PCollection.newBuilder()\n+\t\t\t\t\t\t.setWindowingStrategyId(WINDOW_STRATEGY)\n+\t\t\t\t\t\t.setCoderId(OUTPUT_CODER_ID)\n+\t\t\t\t\t\t.build())\n+\t\t\t\t.putTransforms(\n+\t\t\t\t\tTRANSFORM_ID,\n+\t\t\t\t\tRunnerApi.PTransform.newBuilder()\n+\t\t\t\t\t\t.setUniqueName(TRANSFORM_ID)\n+\t\t\t\t\t\t.setSpec(RunnerApi.FunctionSpec.newBuilder()\n+\t\t\t\t\t\t\t.setUrn(functionUrn)\n+\t\t\t\t\t\t\t.setPayload(\n+\t\t\t\t\t\t\t\torg.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.ByteString.copyFrom(\n+\t\t\t\t\t\t\t\t\tgetUserDefinedFunctionsProtoBytes()))\n+\t\t\t\t\t\t\t.build())\n+\t\t\t\t\t\t.putInputs(MAIN_INPUT_NAME, INPUT_ID)\n+\t\t\t\t\t\t.putOutputs(MAIN_OUTPUT_NAME, OUTPUT_ID)\n+\t\t\t\t\t\t.build())\n+\t\t\t\t.putWindowingStrategies(\n+\t\t\t\t\tWINDOW_STRATEGY,\n+\t\t\t\t\tRunnerApi.WindowingStrategy.newBuilder()\n+\t\t\t\t\t\t.setWindowCoderId(WINDOW_CODER_ID)\n+\t\t\t\t\t\t.build())\n+\t\t\t\t.putCoders(\n+\t\t\t\t\tINPUT_CODER_ID,\n+\t\t\t\t\tgetInputCoderProto())\n+\t\t\t\t.putCoders(\n+\t\t\t\t\tOUTPUT_CODER_ID,\n+\t\t\t\t\tgetOutputCoderProto())\n+\t\t\t\t.putCoders(\n+\t\t\t\t\tWINDOW_CODER_ID,\n+\t\t\t\t\tgetWindowCoderProto())\n+\t\t\t\t.build();\n+\n+\t\tPipelineNode.PCollectionNode input =\n+\t\t\tPipelineNode.pCollection(INPUT_ID, components.getPcollectionsOrThrow(INPUT_ID));\n+\t\tList<SideInputReference> sideInputs = Collections.EMPTY_LIST;\n+\t\tList<UserStateReference> userStates = Collections.EMPTY_LIST;\n+\t\tList<TimerReference> timers = Collections.EMPTY_LIST;\n+\t\tList<PipelineNode.PTransformNode> transforms =\n+\t\t\tCollections.singletonList(\n+\t\t\t\tPipelineNode.pTransform(TRANSFORM_ID, components.getTransformsOrThrow(TRANSFORM_ID)));\n+\t\tList<PipelineNode.PCollectionNode> outputs =\n+\t\t\tCollections.singletonList(\n+\t\t\t\tPipelineNode.pCollection(OUTPUT_ID, components.getPcollectionsOrThrow(OUTPUT_ID)));\n+\t\treturn ImmutableExecutableStage.of(\n+\t\t\tcomponents, createPythonExecutionEnvironment(), input, sideInputs, userStates, timers, transforms, outputs, createValueOnlyWireCoderSetting());\n+\t}\n+\n+\tprivate Collection<RunnerApi.ExecutableStagePayload.WireCoderSetting> createValueOnlyWireCoderSetting() throws\n+\t\tIOException {\n+\t\tWindowedValue<byte[]> value = WindowedValue.valueInGlobalWindow(new byte[0]);\n+\t\tCoder<? extends BoundedWindow> windowCoder = GlobalWindow.Coder.INSTANCE;\n+\t\tWindowedValue.FullWindowedValueCoder<byte[]> windowedValueCoder =\n+\t\t\tWindowedValue.FullWindowedValueCoder.of(ByteArrayCoder.of(), windowCoder);\n+\t\tByteArrayOutputStream baos = new ByteArrayOutputStream();\n+\t\twindowedValueCoder.encode(value, baos);\n+\n+\t\treturn Arrays.asList(\n+\t\t\tRunnerApi.ExecutableStagePayload.WireCoderSetting.newBuilder()\n+\t\t\t\t.setUrn(getUrn(RunnerApi.StandardCoders.Enum.PARAM_WINDOWED_VALUE))\n+\t\t\t\t.setPayload(\n+\t\t\t\t\torg.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.ByteString.copyFrom(baos.toByteArray()))\n+\t\t\t\t.setInputOrOutputId(INPUT_ID)\n+\t\t\t\t.build(),\n+\t\t\tRunnerApi.ExecutableStagePayload.WireCoderSetting.newBuilder()\n+\t\t\t\t.setUrn(getUrn(RunnerApi.StandardCoders.Enum.PARAM_WINDOWED_VALUE))\n+\t\t\t\t.setPayload(\n+\t\t\t\t\torg.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.ByteString.copyFrom(baos.toByteArray()))\n+\t\t\t\t.setInputOrOutputId(OUTPUT_ID)\n+\t\t\t\t.build()\n+\t\t);\n+\t}\n+\n+\tprotected abstract byte[] getUserDefinedFunctionsProtoBytes();\n+\n+\tprotected abstract RunnerApi.Coder getInputCoderProto();\n+\n+\tprotected abstract RunnerApi.Coder getOutputCoderProto();\n+\n+\t/**\n+\t * Gets the proto representation of the window coder.\n+\t */\n+\tprivate RunnerApi.Coder getWindowCoderProto() {\n+\t\treturn RunnerApi.Coder.newBuilder()\n+\t\t\t.setSpec(\n+\t\t\t\tRunnerApi.FunctionSpec.newBuilder()\n+\t\t\t\t\t.setUrn(ModelCoders.GLOBAL_WINDOW_CODER_URN)\n+\t\t\t\t\t.build())\n+\t\t\t.build();\n+\t}\n+\n+\tprivate static StateRequestHandler getStateRequestHandler(\n+\t\t\tKeyedStateBackend keyedStateBackend,\n+\t\t\tTypeSerializer keySerializer) {\n+\t\tif (keyedStateBackend == null) {\n+\t\t\treturn StateRequestHandler.unsupported();\n+\t\t} else {\n+\t\t\tassert keySerializer != null;\n+\t\t\treturn new SimpleStateRequestHandler(keyedStateBackend, keySerializer);\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * A state request handler which handles the state request from Python side.\n+\t */\n+\tprivate static class SimpleStateRequestHandler implements StateRequestHandler {\n+\n+\t\tprivate final TypeSerializer keySerializer;\n+\t\tprivate final TypeSerializer<byte[]> valueSerializer;\n+\t\tprivate final KeyedStateBackend keyedStateBackend;\n+\n+\t\t/**\n+\t\t * Reusable OutputStream used to holding the serialized input elements.\n+\t\t */\n+\t\tprivate final ByteArrayOutputStreamWithPos baos;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "87967d2198d73dea7a84808d6c1a5076a1c7c317"}, "originalPosition": 282}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjAwNTAxOQ==", "bodyText": "It's never used?", "url": "https://github.com/apache/flink/pull/13420#discussion_r492005019", "createdAt": "2020-09-21T12:29:49Z", "author": {"login": "dianfu"}, "path": "flink-python/src/main/java/org/apache/flink/streaming/api/runners/python/beam/BeamPythonFunctionRunner.java", "diffHunk": "@@ -309,4 +356,287 @@ private void checkInvokeStartBundle() throws Exception {\n \t\t}\n \t}\n \n+\t/**\n+\t * Creates a {@link ExecutableStage} which contains the Python user-defined functions to be executed\n+\t * and all the other information needed to execute them, such as the execution environment, the input\n+\t * and output coder, etc.\n+\t */\n+\t@SuppressWarnings(\"unchecked\")\n+\tprivate ExecutableStage createExecutableStage() throws Exception {\n+\t\tRunnerApi.Components components =\n+\t\t\tRunnerApi.Components.newBuilder()\n+\t\t\t\t.putPcollections(\n+\t\t\t\t\tINPUT_ID,\n+\t\t\t\t\tRunnerApi.PCollection.newBuilder()\n+\t\t\t\t\t\t.setWindowingStrategyId(WINDOW_STRATEGY)\n+\t\t\t\t\t\t.setCoderId(INPUT_CODER_ID)\n+\t\t\t\t\t\t.build())\n+\t\t\t\t.putPcollections(\n+\t\t\t\t\tOUTPUT_ID,\n+\t\t\t\t\tRunnerApi.PCollection.newBuilder()\n+\t\t\t\t\t\t.setWindowingStrategyId(WINDOW_STRATEGY)\n+\t\t\t\t\t\t.setCoderId(OUTPUT_CODER_ID)\n+\t\t\t\t\t\t.build())\n+\t\t\t\t.putTransforms(\n+\t\t\t\t\tTRANSFORM_ID,\n+\t\t\t\t\tRunnerApi.PTransform.newBuilder()\n+\t\t\t\t\t\t.setUniqueName(TRANSFORM_ID)\n+\t\t\t\t\t\t.setSpec(RunnerApi.FunctionSpec.newBuilder()\n+\t\t\t\t\t\t\t.setUrn(functionUrn)\n+\t\t\t\t\t\t\t.setPayload(\n+\t\t\t\t\t\t\t\torg.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.ByteString.copyFrom(\n+\t\t\t\t\t\t\t\t\tgetUserDefinedFunctionsProtoBytes()))\n+\t\t\t\t\t\t\t.build())\n+\t\t\t\t\t\t.putInputs(MAIN_INPUT_NAME, INPUT_ID)\n+\t\t\t\t\t\t.putOutputs(MAIN_OUTPUT_NAME, OUTPUT_ID)\n+\t\t\t\t\t\t.build())\n+\t\t\t\t.putWindowingStrategies(\n+\t\t\t\t\tWINDOW_STRATEGY,\n+\t\t\t\t\tRunnerApi.WindowingStrategy.newBuilder()\n+\t\t\t\t\t\t.setWindowCoderId(WINDOW_CODER_ID)\n+\t\t\t\t\t\t.build())\n+\t\t\t\t.putCoders(\n+\t\t\t\t\tINPUT_CODER_ID,\n+\t\t\t\t\tgetInputCoderProto())\n+\t\t\t\t.putCoders(\n+\t\t\t\t\tOUTPUT_CODER_ID,\n+\t\t\t\t\tgetOutputCoderProto())\n+\t\t\t\t.putCoders(\n+\t\t\t\t\tWINDOW_CODER_ID,\n+\t\t\t\t\tgetWindowCoderProto())\n+\t\t\t\t.build();\n+\n+\t\tPipelineNode.PCollectionNode input =\n+\t\t\tPipelineNode.pCollection(INPUT_ID, components.getPcollectionsOrThrow(INPUT_ID));\n+\t\tList<SideInputReference> sideInputs = Collections.EMPTY_LIST;\n+\t\tList<UserStateReference> userStates = Collections.EMPTY_LIST;\n+\t\tList<TimerReference> timers = Collections.EMPTY_LIST;\n+\t\tList<PipelineNode.PTransformNode> transforms =\n+\t\t\tCollections.singletonList(\n+\t\t\t\tPipelineNode.pTransform(TRANSFORM_ID, components.getTransformsOrThrow(TRANSFORM_ID)));\n+\t\tList<PipelineNode.PCollectionNode> outputs =\n+\t\t\tCollections.singletonList(\n+\t\t\t\tPipelineNode.pCollection(OUTPUT_ID, components.getPcollectionsOrThrow(OUTPUT_ID)));\n+\t\treturn ImmutableExecutableStage.of(\n+\t\t\tcomponents, createPythonExecutionEnvironment(), input, sideInputs, userStates, timers, transforms, outputs, createValueOnlyWireCoderSetting());\n+\t}\n+\n+\tprivate Collection<RunnerApi.ExecutableStagePayload.WireCoderSetting> createValueOnlyWireCoderSetting() throws\n+\t\tIOException {\n+\t\tWindowedValue<byte[]> value = WindowedValue.valueInGlobalWindow(new byte[0]);\n+\t\tCoder<? extends BoundedWindow> windowCoder = GlobalWindow.Coder.INSTANCE;\n+\t\tWindowedValue.FullWindowedValueCoder<byte[]> windowedValueCoder =\n+\t\t\tWindowedValue.FullWindowedValueCoder.of(ByteArrayCoder.of(), windowCoder);\n+\t\tByteArrayOutputStream baos = new ByteArrayOutputStream();\n+\t\twindowedValueCoder.encode(value, baos);\n+\n+\t\treturn Arrays.asList(\n+\t\t\tRunnerApi.ExecutableStagePayload.WireCoderSetting.newBuilder()\n+\t\t\t\t.setUrn(getUrn(RunnerApi.StandardCoders.Enum.PARAM_WINDOWED_VALUE))\n+\t\t\t\t.setPayload(\n+\t\t\t\t\torg.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.ByteString.copyFrom(baos.toByteArray()))\n+\t\t\t\t.setInputOrOutputId(INPUT_ID)\n+\t\t\t\t.build(),\n+\t\t\tRunnerApi.ExecutableStagePayload.WireCoderSetting.newBuilder()\n+\t\t\t\t.setUrn(getUrn(RunnerApi.StandardCoders.Enum.PARAM_WINDOWED_VALUE))\n+\t\t\t\t.setPayload(\n+\t\t\t\t\torg.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.ByteString.copyFrom(baos.toByteArray()))\n+\t\t\t\t.setInputOrOutputId(OUTPUT_ID)\n+\t\t\t\t.build()\n+\t\t);\n+\t}\n+\n+\tprotected abstract byte[] getUserDefinedFunctionsProtoBytes();\n+\n+\tprotected abstract RunnerApi.Coder getInputCoderProto();\n+\n+\tprotected abstract RunnerApi.Coder getOutputCoderProto();\n+\n+\t/**\n+\t * Gets the proto representation of the window coder.\n+\t */\n+\tprivate RunnerApi.Coder getWindowCoderProto() {\n+\t\treturn RunnerApi.Coder.newBuilder()\n+\t\t\t.setSpec(\n+\t\t\t\tRunnerApi.FunctionSpec.newBuilder()\n+\t\t\t\t\t.setUrn(ModelCoders.GLOBAL_WINDOW_CODER_URN)\n+\t\t\t\t\t.build())\n+\t\t\t.build();\n+\t}\n+\n+\tprivate static StateRequestHandler getStateRequestHandler(\n+\t\t\tKeyedStateBackend keyedStateBackend,\n+\t\t\tTypeSerializer keySerializer) {\n+\t\tif (keyedStateBackend == null) {\n+\t\t\treturn StateRequestHandler.unsupported();\n+\t\t} else {\n+\t\t\tassert keySerializer != null;\n+\t\t\treturn new SimpleStateRequestHandler(keyedStateBackend, keySerializer);\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * A state request handler which handles the state request from Python side.\n+\t */\n+\tprivate static class SimpleStateRequestHandler implements StateRequestHandler {\n+\n+\t\tprivate final TypeSerializer keySerializer;\n+\t\tprivate final TypeSerializer<byte[]> valueSerializer;\n+\t\tprivate final KeyedStateBackend keyedStateBackend;\n+\n+\t\t/**\n+\t\t * Reusable OutputStream used to holding the serialized input elements.\n+\t\t */\n+\t\tprivate final ByteArrayOutputStreamWithPos baos;\n+\n+\t\t/**\n+\t\t * Reusable InputStream used to holding the elements to be deserialized.\n+\t\t */\n+\t\tprivate final ByteArrayInputStreamWithPos bais;\n+\n+\t\t/**\n+\t\t * OutputStream Wrapper.\n+\t\t */\n+\t\tprivate final DataOutputViewStreamWrapper baosWrapper;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "87967d2198d73dea7a84808d6c1a5076a1c7c317"}, "originalPosition": 292}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjAwNzEwMA==", "bodyText": "Move this line to the front of the constructor?", "url": "https://github.com/apache/flink/pull/13420#discussion_r492007100", "createdAt": "2020-09-21T12:33:18Z", "author": {"login": "dianfu"}, "path": "flink-python/src/main/java/org/apache/flink/streaming/api/runners/python/beam/BeamPythonFunctionRunner.java", "diffHunk": "@@ -309,4 +356,287 @@ private void checkInvokeStartBundle() throws Exception {\n \t\t}\n \t}\n \n+\t/**\n+\t * Creates a {@link ExecutableStage} which contains the Python user-defined functions to be executed\n+\t * and all the other information needed to execute them, such as the execution environment, the input\n+\t * and output coder, etc.\n+\t */\n+\t@SuppressWarnings(\"unchecked\")\n+\tprivate ExecutableStage createExecutableStage() throws Exception {\n+\t\tRunnerApi.Components components =\n+\t\t\tRunnerApi.Components.newBuilder()\n+\t\t\t\t.putPcollections(\n+\t\t\t\t\tINPUT_ID,\n+\t\t\t\t\tRunnerApi.PCollection.newBuilder()\n+\t\t\t\t\t\t.setWindowingStrategyId(WINDOW_STRATEGY)\n+\t\t\t\t\t\t.setCoderId(INPUT_CODER_ID)\n+\t\t\t\t\t\t.build())\n+\t\t\t\t.putPcollections(\n+\t\t\t\t\tOUTPUT_ID,\n+\t\t\t\t\tRunnerApi.PCollection.newBuilder()\n+\t\t\t\t\t\t.setWindowingStrategyId(WINDOW_STRATEGY)\n+\t\t\t\t\t\t.setCoderId(OUTPUT_CODER_ID)\n+\t\t\t\t\t\t.build())\n+\t\t\t\t.putTransforms(\n+\t\t\t\t\tTRANSFORM_ID,\n+\t\t\t\t\tRunnerApi.PTransform.newBuilder()\n+\t\t\t\t\t\t.setUniqueName(TRANSFORM_ID)\n+\t\t\t\t\t\t.setSpec(RunnerApi.FunctionSpec.newBuilder()\n+\t\t\t\t\t\t\t.setUrn(functionUrn)\n+\t\t\t\t\t\t\t.setPayload(\n+\t\t\t\t\t\t\t\torg.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.ByteString.copyFrom(\n+\t\t\t\t\t\t\t\t\tgetUserDefinedFunctionsProtoBytes()))\n+\t\t\t\t\t\t\t.build())\n+\t\t\t\t\t\t.putInputs(MAIN_INPUT_NAME, INPUT_ID)\n+\t\t\t\t\t\t.putOutputs(MAIN_OUTPUT_NAME, OUTPUT_ID)\n+\t\t\t\t\t\t.build())\n+\t\t\t\t.putWindowingStrategies(\n+\t\t\t\t\tWINDOW_STRATEGY,\n+\t\t\t\t\tRunnerApi.WindowingStrategy.newBuilder()\n+\t\t\t\t\t\t.setWindowCoderId(WINDOW_CODER_ID)\n+\t\t\t\t\t\t.build())\n+\t\t\t\t.putCoders(\n+\t\t\t\t\tINPUT_CODER_ID,\n+\t\t\t\t\tgetInputCoderProto())\n+\t\t\t\t.putCoders(\n+\t\t\t\t\tOUTPUT_CODER_ID,\n+\t\t\t\t\tgetOutputCoderProto())\n+\t\t\t\t.putCoders(\n+\t\t\t\t\tWINDOW_CODER_ID,\n+\t\t\t\t\tgetWindowCoderProto())\n+\t\t\t\t.build();\n+\n+\t\tPipelineNode.PCollectionNode input =\n+\t\t\tPipelineNode.pCollection(INPUT_ID, components.getPcollectionsOrThrow(INPUT_ID));\n+\t\tList<SideInputReference> sideInputs = Collections.EMPTY_LIST;\n+\t\tList<UserStateReference> userStates = Collections.EMPTY_LIST;\n+\t\tList<TimerReference> timers = Collections.EMPTY_LIST;\n+\t\tList<PipelineNode.PTransformNode> transforms =\n+\t\t\tCollections.singletonList(\n+\t\t\t\tPipelineNode.pTransform(TRANSFORM_ID, components.getTransformsOrThrow(TRANSFORM_ID)));\n+\t\tList<PipelineNode.PCollectionNode> outputs =\n+\t\t\tCollections.singletonList(\n+\t\t\t\tPipelineNode.pCollection(OUTPUT_ID, components.getPcollectionsOrThrow(OUTPUT_ID)));\n+\t\treturn ImmutableExecutableStage.of(\n+\t\t\tcomponents, createPythonExecutionEnvironment(), input, sideInputs, userStates, timers, transforms, outputs, createValueOnlyWireCoderSetting());\n+\t}\n+\n+\tprivate Collection<RunnerApi.ExecutableStagePayload.WireCoderSetting> createValueOnlyWireCoderSetting() throws\n+\t\tIOException {\n+\t\tWindowedValue<byte[]> value = WindowedValue.valueInGlobalWindow(new byte[0]);\n+\t\tCoder<? extends BoundedWindow> windowCoder = GlobalWindow.Coder.INSTANCE;\n+\t\tWindowedValue.FullWindowedValueCoder<byte[]> windowedValueCoder =\n+\t\t\tWindowedValue.FullWindowedValueCoder.of(ByteArrayCoder.of(), windowCoder);\n+\t\tByteArrayOutputStream baos = new ByteArrayOutputStream();\n+\t\twindowedValueCoder.encode(value, baos);\n+\n+\t\treturn Arrays.asList(\n+\t\t\tRunnerApi.ExecutableStagePayload.WireCoderSetting.newBuilder()\n+\t\t\t\t.setUrn(getUrn(RunnerApi.StandardCoders.Enum.PARAM_WINDOWED_VALUE))\n+\t\t\t\t.setPayload(\n+\t\t\t\t\torg.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.ByteString.copyFrom(baos.toByteArray()))\n+\t\t\t\t.setInputOrOutputId(INPUT_ID)\n+\t\t\t\t.build(),\n+\t\t\tRunnerApi.ExecutableStagePayload.WireCoderSetting.newBuilder()\n+\t\t\t\t.setUrn(getUrn(RunnerApi.StandardCoders.Enum.PARAM_WINDOWED_VALUE))\n+\t\t\t\t.setPayload(\n+\t\t\t\t\torg.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.ByteString.copyFrom(baos.toByteArray()))\n+\t\t\t\t.setInputOrOutputId(OUTPUT_ID)\n+\t\t\t\t.build()\n+\t\t);\n+\t}\n+\n+\tprotected abstract byte[] getUserDefinedFunctionsProtoBytes();\n+\n+\tprotected abstract RunnerApi.Coder getInputCoderProto();\n+\n+\tprotected abstract RunnerApi.Coder getOutputCoderProto();\n+\n+\t/**\n+\t * Gets the proto representation of the window coder.\n+\t */\n+\tprivate RunnerApi.Coder getWindowCoderProto() {\n+\t\treturn RunnerApi.Coder.newBuilder()\n+\t\t\t.setSpec(\n+\t\t\t\tRunnerApi.FunctionSpec.newBuilder()\n+\t\t\t\t\t.setUrn(ModelCoders.GLOBAL_WINDOW_CODER_URN)\n+\t\t\t\t\t.build())\n+\t\t\t.build();\n+\t}\n+\n+\tprivate static StateRequestHandler getStateRequestHandler(\n+\t\t\tKeyedStateBackend keyedStateBackend,\n+\t\t\tTypeSerializer keySerializer) {\n+\t\tif (keyedStateBackend == null) {\n+\t\t\treturn StateRequestHandler.unsupported();\n+\t\t} else {\n+\t\t\tassert keySerializer != null;\n+\t\t\treturn new SimpleStateRequestHandler(keyedStateBackend, keySerializer);\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * A state request handler which handles the state request from Python side.\n+\t */\n+\tprivate static class SimpleStateRequestHandler implements StateRequestHandler {\n+\n+\t\tprivate final TypeSerializer keySerializer;\n+\t\tprivate final TypeSerializer<byte[]> valueSerializer;\n+\t\tprivate final KeyedStateBackend keyedStateBackend;\n+\n+\t\t/**\n+\t\t * Reusable OutputStream used to holding the serialized input elements.\n+\t\t */\n+\t\tprivate final ByteArrayOutputStreamWithPos baos;\n+\n+\t\t/**\n+\t\t * Reusable InputStream used to holding the elements to be deserialized.\n+\t\t */\n+\t\tprivate final ByteArrayInputStreamWithPos bais;\n+\n+\t\t/**\n+\t\t * OutputStream Wrapper.\n+\t\t */\n+\t\tprivate final DataOutputViewStreamWrapper baosWrapper;\n+\n+\t\t/**\n+\t\t * InputStream Wrapper.\n+\t\t */\n+\t\tprivate final DataInputViewStreamWrapper baisWrapper;\n+\n+\t\t/**\n+\t\t * The cache of the stateDescriptors.\n+\t\t */\n+\t\tfinal Map<String, StateDescriptor> stateDescriptorCache;\n+\n+\t\tSimpleStateRequestHandler(\n+\t\t\tKeyedStateBackend keyedStateBackend,\n+\t\t\tTypeSerializer keySerializer) {\n+\t\t\tthis.keySerializer = keySerializer;\n+\t\t\tthis.valueSerializer = PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO\n+\t\t\t\t.createSerializer(new ExecutionConfig());\n+\t\t\tthis.keyedStateBackend = keyedStateBackend;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "87967d2198d73dea7a84808d6c1a5076a1c7c317"}, "originalPosition": 310}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjAxMjY5Ng==", "bodyText": "Remove the RowData prefix as we will not support the old planner any more and so the prefix is not necessary.", "url": "https://github.com/apache/flink/pull/13420#discussion_r492012696", "createdAt": "2020-09-21T12:42:19Z", "author": {"login": "dianfu"}, "path": "flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/aggregate/RowDataPythonStreamGroupAggregateOperator.java", "diffHunk": "@@ -0,0 +1,397 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.operators.python.aggregate;\n+\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.api.common.state.ValueState;\n+import org.apache.flink.api.common.state.ValueStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.Types;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.memory.ByteArrayInputStreamWithPos;\n+import org.apache.flink.core.memory.ByteArrayOutputStreamWithPos;\n+import org.apache.flink.core.memory.DataInputViewStreamWrapper;\n+import org.apache.flink.core.memory.DataOutputViewStreamWrapper;\n+import org.apache.flink.fnexecution.v1.FlinkFnApi;\n+import org.apache.flink.python.PythonFunctionRunner;\n+import org.apache.flink.runtime.state.VoidNamespace;\n+import org.apache.flink.runtime.state.VoidNamespaceSerializer;\n+import org.apache.flink.streaming.api.SimpleTimerService;\n+import org.apache.flink.streaming.api.TimerService;\n+import org.apache.flink.streaming.api.operators.InternalTimer;\n+import org.apache.flink.streaming.api.operators.Triggerable;\n+import org.apache.flink.streaming.api.operators.python.AbstractOneInputPythonFunctionOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.UpdatableRowData;\n+import org.apache.flink.table.functions.python.PythonEnv;\n+import org.apache.flink.table.functions.python.PythonFunctionInfo;\n+import org.apache.flink.table.planner.plan.utils.KeySelectorUtil;\n+import org.apache.flink.table.runtime.functions.CleanupState;\n+import org.apache.flink.table.runtime.keyselector.RowDataKeySelector;\n+import org.apache.flink.table.runtime.operators.python.utils.PythonOperatorUtils;\n+import org.apache.flink.table.runtime.operators.python.utils.StreamRecordRowDataWrappingCollector;\n+import org.apache.flink.table.runtime.runners.python.beam.BeamTableStatefulPythonFunctionRunner;\n+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n+import org.apache.flink.table.runtime.typeutils.PythonTypeUtils;\n+import org.apache.flink.table.types.logical.BigIntType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.TinyIntType;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.runtime.typeutils.PythonTypeUtils.toProtoType;\n+\n+/**\n+ *  The Python AggregateFunction operator for the blink planner.\n+ */\n+public class RowDataPythonStreamGroupAggregateOperator", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "87967d2198d73dea7a84808d6c1a5076a1c7c317"}, "originalPosition": 72}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjAxMzYxNw==", "bodyText": "Move this line to the front of the class?", "url": "https://github.com/apache/flink/pull/13420#discussion_r492013617", "createdAt": "2020-09-21T12:43:18Z", "author": {"login": "dianfu"}, "path": "flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/aggregate/RowDataPythonStreamGroupAggregateOperator.java", "diffHunk": "@@ -0,0 +1,397 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.operators.python.aggregate;\n+\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.api.common.state.ValueState;\n+import org.apache.flink.api.common.state.ValueStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.Types;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.memory.ByteArrayInputStreamWithPos;\n+import org.apache.flink.core.memory.ByteArrayOutputStreamWithPos;\n+import org.apache.flink.core.memory.DataInputViewStreamWrapper;\n+import org.apache.flink.core.memory.DataOutputViewStreamWrapper;\n+import org.apache.flink.fnexecution.v1.FlinkFnApi;\n+import org.apache.flink.python.PythonFunctionRunner;\n+import org.apache.flink.runtime.state.VoidNamespace;\n+import org.apache.flink.runtime.state.VoidNamespaceSerializer;\n+import org.apache.flink.streaming.api.SimpleTimerService;\n+import org.apache.flink.streaming.api.TimerService;\n+import org.apache.flink.streaming.api.operators.InternalTimer;\n+import org.apache.flink.streaming.api.operators.Triggerable;\n+import org.apache.flink.streaming.api.operators.python.AbstractOneInputPythonFunctionOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.UpdatableRowData;\n+import org.apache.flink.table.functions.python.PythonEnv;\n+import org.apache.flink.table.functions.python.PythonFunctionInfo;\n+import org.apache.flink.table.planner.plan.utils.KeySelectorUtil;\n+import org.apache.flink.table.runtime.functions.CleanupState;\n+import org.apache.flink.table.runtime.keyselector.RowDataKeySelector;\n+import org.apache.flink.table.runtime.operators.python.utils.PythonOperatorUtils;\n+import org.apache.flink.table.runtime.operators.python.utils.StreamRecordRowDataWrappingCollector;\n+import org.apache.flink.table.runtime.runners.python.beam.BeamTableStatefulPythonFunctionRunner;\n+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n+import org.apache.flink.table.runtime.typeutils.PythonTypeUtils;\n+import org.apache.flink.table.types.logical.BigIntType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.TinyIntType;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.runtime.typeutils.PythonTypeUtils.toProtoType;\n+\n+/**\n+ *  The Python AggregateFunction operator for the blink planner.\n+ */\n+public class RowDataPythonStreamGroupAggregateOperator\n+\textends AbstractOneInputPythonFunctionOperator<RowData, RowData>\n+\timplements Triggerable<RowData, VoidNamespace>, CleanupState {\n+\n+\t@VisibleForTesting\n+\tprotected static final String FLINK_AGGREGATE_FUNCTION_INPUT_SCHEMA_CODER_URN =\n+\t\t\"flink:coder:schema:aggregate_function_input:v1\";\n+\n+\t@VisibleForTesting\n+\tprotected static final String FLINK_AGGREGATE_FUNCTION_OUTPUT_SCHEMA_CODER_URN =\n+\t\t\"flink:coder:schema:aggregate_function_output:v1\";\n+\n+\t@VisibleForTesting\n+\tprotected static final String STREAM_GROUP_AGGREGATE_URN = \"flink:transform:stream_group_aggregate:v1\";\n+\n+\t@VisibleForTesting\n+\tprotected static final byte NORMAL_RECORD = 0;\n+\n+\tprivate static final byte TRIGGER_TIMER = 1;\n+\n+\tprivate static final long serialVersionUID = 1L;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "87967d2198d73dea7a84808d6c1a5076a1c7c317"}, "originalPosition": 92}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjAxNDAyNA==", "bodyText": "private?", "url": "https://github.com/apache/flink/pull/13420#discussion_r492014024", "createdAt": "2020-09-21T12:43:45Z", "author": {"login": "dianfu"}, "path": "flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/aggregate/RowDataPythonStreamGroupAggregateOperator.java", "diffHunk": "@@ -0,0 +1,397 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.operators.python.aggregate;\n+\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.api.common.state.ValueState;\n+import org.apache.flink.api.common.state.ValueStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.Types;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.memory.ByteArrayInputStreamWithPos;\n+import org.apache.flink.core.memory.ByteArrayOutputStreamWithPos;\n+import org.apache.flink.core.memory.DataInputViewStreamWrapper;\n+import org.apache.flink.core.memory.DataOutputViewStreamWrapper;\n+import org.apache.flink.fnexecution.v1.FlinkFnApi;\n+import org.apache.flink.python.PythonFunctionRunner;\n+import org.apache.flink.runtime.state.VoidNamespace;\n+import org.apache.flink.runtime.state.VoidNamespaceSerializer;\n+import org.apache.flink.streaming.api.SimpleTimerService;\n+import org.apache.flink.streaming.api.TimerService;\n+import org.apache.flink.streaming.api.operators.InternalTimer;\n+import org.apache.flink.streaming.api.operators.Triggerable;\n+import org.apache.flink.streaming.api.operators.python.AbstractOneInputPythonFunctionOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.UpdatableRowData;\n+import org.apache.flink.table.functions.python.PythonEnv;\n+import org.apache.flink.table.functions.python.PythonFunctionInfo;\n+import org.apache.flink.table.planner.plan.utils.KeySelectorUtil;\n+import org.apache.flink.table.runtime.functions.CleanupState;\n+import org.apache.flink.table.runtime.keyselector.RowDataKeySelector;\n+import org.apache.flink.table.runtime.operators.python.utils.PythonOperatorUtils;\n+import org.apache.flink.table.runtime.operators.python.utils.StreamRecordRowDataWrappingCollector;\n+import org.apache.flink.table.runtime.runners.python.beam.BeamTableStatefulPythonFunctionRunner;\n+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n+import org.apache.flink.table.runtime.typeutils.PythonTypeUtils;\n+import org.apache.flink.table.types.logical.BigIntType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.TinyIntType;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.runtime.typeutils.PythonTypeUtils.toProtoType;\n+\n+/**\n+ *  The Python AggregateFunction operator for the blink planner.\n+ */\n+public class RowDataPythonStreamGroupAggregateOperator\n+\textends AbstractOneInputPythonFunctionOperator<RowData, RowData>\n+\timplements Triggerable<RowData, VoidNamespace>, CleanupState {\n+\n+\t@VisibleForTesting\n+\tprotected static final String FLINK_AGGREGATE_FUNCTION_INPUT_SCHEMA_CODER_URN =\n+\t\t\"flink:coder:schema:aggregate_function_input:v1\";\n+\n+\t@VisibleForTesting\n+\tprotected static final String FLINK_AGGREGATE_FUNCTION_OUTPUT_SCHEMA_CODER_URN =\n+\t\t\"flink:coder:schema:aggregate_function_output:v1\";\n+\n+\t@VisibleForTesting\n+\tprotected static final String STREAM_GROUP_AGGREGATE_URN = \"flink:transform:stream_group_aggregate:v1\";\n+\n+\t@VisibleForTesting\n+\tprotected static final byte NORMAL_RECORD = 0;\n+\n+\tprivate static final byte TRIGGER_TIMER = 1;\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate static final int DEFAULT_STATE_CACHE_SIZE = 1000;\n+\n+\t/**\n+\t * The input logical type.\n+\t */\n+\tprotected final RowType inputType;\n+\n+\t/**\n+\t * The output logical type.\n+\t */\n+\tprotected final RowType outputType;\n+\n+\t/**\n+\t * The options used to configure the Python worker process.\n+\t */\n+\tprivate final Map<String, String> jobOptions;\n+\n+\tprivate final PythonFunctionInfo[] aggregateFunctions;\n+\n+\t/**\n+\t * The array of the key indexes.\n+\t */\n+\tprivate final int[] grouping;\n+\n+\t/**\n+\t * The index of a count aggregate used to calculate the number of accumulated rows.\n+\t */\n+\tprivate final int indexOfCountStar;\n+\n+\t/**\n+\t * Generate retract messages if true.\n+\t */\n+\tprivate final boolean generateUpdateBefore;\n+\n+\t/**\n+\t * The minimum time in milliseconds until state which was not updated will be retained.\n+\t */\n+\tfinal long minRetentionTime;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "87967d2198d73dea7a84808d6c1a5076a1c7c317"}, "originalPosition": 131}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjAxNDEyMg==", "bodyText": "ditto", "url": "https://github.com/apache/flink/pull/13420#discussion_r492014122", "createdAt": "2020-09-21T12:43:52Z", "author": {"login": "dianfu"}, "path": "flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/aggregate/RowDataPythonStreamGroupAggregateOperator.java", "diffHunk": "@@ -0,0 +1,397 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.operators.python.aggregate;\n+\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.api.common.state.ValueState;\n+import org.apache.flink.api.common.state.ValueStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.Types;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.memory.ByteArrayInputStreamWithPos;\n+import org.apache.flink.core.memory.ByteArrayOutputStreamWithPos;\n+import org.apache.flink.core.memory.DataInputViewStreamWrapper;\n+import org.apache.flink.core.memory.DataOutputViewStreamWrapper;\n+import org.apache.flink.fnexecution.v1.FlinkFnApi;\n+import org.apache.flink.python.PythonFunctionRunner;\n+import org.apache.flink.runtime.state.VoidNamespace;\n+import org.apache.flink.runtime.state.VoidNamespaceSerializer;\n+import org.apache.flink.streaming.api.SimpleTimerService;\n+import org.apache.flink.streaming.api.TimerService;\n+import org.apache.flink.streaming.api.operators.InternalTimer;\n+import org.apache.flink.streaming.api.operators.Triggerable;\n+import org.apache.flink.streaming.api.operators.python.AbstractOneInputPythonFunctionOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.UpdatableRowData;\n+import org.apache.flink.table.functions.python.PythonEnv;\n+import org.apache.flink.table.functions.python.PythonFunctionInfo;\n+import org.apache.flink.table.planner.plan.utils.KeySelectorUtil;\n+import org.apache.flink.table.runtime.functions.CleanupState;\n+import org.apache.flink.table.runtime.keyselector.RowDataKeySelector;\n+import org.apache.flink.table.runtime.operators.python.utils.PythonOperatorUtils;\n+import org.apache.flink.table.runtime.operators.python.utils.StreamRecordRowDataWrappingCollector;\n+import org.apache.flink.table.runtime.runners.python.beam.BeamTableStatefulPythonFunctionRunner;\n+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n+import org.apache.flink.table.runtime.typeutils.PythonTypeUtils;\n+import org.apache.flink.table.types.logical.BigIntType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.TinyIntType;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.runtime.typeutils.PythonTypeUtils.toProtoType;\n+\n+/**\n+ *  The Python AggregateFunction operator for the blink planner.\n+ */\n+public class RowDataPythonStreamGroupAggregateOperator\n+\textends AbstractOneInputPythonFunctionOperator<RowData, RowData>\n+\timplements Triggerable<RowData, VoidNamespace>, CleanupState {\n+\n+\t@VisibleForTesting\n+\tprotected static final String FLINK_AGGREGATE_FUNCTION_INPUT_SCHEMA_CODER_URN =\n+\t\t\"flink:coder:schema:aggregate_function_input:v1\";\n+\n+\t@VisibleForTesting\n+\tprotected static final String FLINK_AGGREGATE_FUNCTION_OUTPUT_SCHEMA_CODER_URN =\n+\t\t\"flink:coder:schema:aggregate_function_output:v1\";\n+\n+\t@VisibleForTesting\n+\tprotected static final String STREAM_GROUP_AGGREGATE_URN = \"flink:transform:stream_group_aggregate:v1\";\n+\n+\t@VisibleForTesting\n+\tprotected static final byte NORMAL_RECORD = 0;\n+\n+\tprivate static final byte TRIGGER_TIMER = 1;\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate static final int DEFAULT_STATE_CACHE_SIZE = 1000;\n+\n+\t/**\n+\t * The input logical type.\n+\t */\n+\tprotected final RowType inputType;\n+\n+\t/**\n+\t * The output logical type.\n+\t */\n+\tprotected final RowType outputType;\n+\n+\t/**\n+\t * The options used to configure the Python worker process.\n+\t */\n+\tprivate final Map<String, String> jobOptions;\n+\n+\tprivate final PythonFunctionInfo[] aggregateFunctions;\n+\n+\t/**\n+\t * The array of the key indexes.\n+\t */\n+\tprivate final int[] grouping;\n+\n+\t/**\n+\t * The index of a count aggregate used to calculate the number of accumulated rows.\n+\t */\n+\tprivate final int indexOfCountStar;\n+\n+\t/**\n+\t * Generate retract messages if true.\n+\t */\n+\tprivate final boolean generateUpdateBefore;\n+\n+\t/**\n+\t * The minimum time in milliseconds until state which was not updated will be retained.\n+\t */\n+\tfinal long minRetentionTime;\n+\n+\t/**\n+\t * The maximum time in milliseconds until state which was not updated will be retained.\n+\t */\n+\tfinal long maxRetentionTime;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "87967d2198d73dea7a84808d6c1a5076a1c7c317"}, "originalPosition": 136}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjAxNDQ1OQ==", "bodyText": "can be private", "url": "https://github.com/apache/flink/pull/13420#discussion_r492014459", "createdAt": "2020-09-21T12:44:23Z", "author": {"login": "dianfu"}, "path": "flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/aggregate/RowDataPythonStreamGroupAggregateOperator.java", "diffHunk": "@@ -0,0 +1,397 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.operators.python.aggregate;\n+\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.api.common.state.ValueState;\n+import org.apache.flink.api.common.state.ValueStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.Types;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.memory.ByteArrayInputStreamWithPos;\n+import org.apache.flink.core.memory.ByteArrayOutputStreamWithPos;\n+import org.apache.flink.core.memory.DataInputViewStreamWrapper;\n+import org.apache.flink.core.memory.DataOutputViewStreamWrapper;\n+import org.apache.flink.fnexecution.v1.FlinkFnApi;\n+import org.apache.flink.python.PythonFunctionRunner;\n+import org.apache.flink.runtime.state.VoidNamespace;\n+import org.apache.flink.runtime.state.VoidNamespaceSerializer;\n+import org.apache.flink.streaming.api.SimpleTimerService;\n+import org.apache.flink.streaming.api.TimerService;\n+import org.apache.flink.streaming.api.operators.InternalTimer;\n+import org.apache.flink.streaming.api.operators.Triggerable;\n+import org.apache.flink.streaming.api.operators.python.AbstractOneInputPythonFunctionOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.UpdatableRowData;\n+import org.apache.flink.table.functions.python.PythonEnv;\n+import org.apache.flink.table.functions.python.PythonFunctionInfo;\n+import org.apache.flink.table.planner.plan.utils.KeySelectorUtil;\n+import org.apache.flink.table.runtime.functions.CleanupState;\n+import org.apache.flink.table.runtime.keyselector.RowDataKeySelector;\n+import org.apache.flink.table.runtime.operators.python.utils.PythonOperatorUtils;\n+import org.apache.flink.table.runtime.operators.python.utils.StreamRecordRowDataWrappingCollector;\n+import org.apache.flink.table.runtime.runners.python.beam.BeamTableStatefulPythonFunctionRunner;\n+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n+import org.apache.flink.table.runtime.typeutils.PythonTypeUtils;\n+import org.apache.flink.table.types.logical.BigIntType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.TinyIntType;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.runtime.typeutils.PythonTypeUtils.toProtoType;\n+\n+/**\n+ *  The Python AggregateFunction operator for the blink planner.\n+ */\n+public class RowDataPythonStreamGroupAggregateOperator\n+\textends AbstractOneInputPythonFunctionOperator<RowData, RowData>\n+\timplements Triggerable<RowData, VoidNamespace>, CleanupState {\n+\n+\t@VisibleForTesting\n+\tprotected static final String FLINK_AGGREGATE_FUNCTION_INPUT_SCHEMA_CODER_URN =\n+\t\t\"flink:coder:schema:aggregate_function_input:v1\";\n+\n+\t@VisibleForTesting\n+\tprotected static final String FLINK_AGGREGATE_FUNCTION_OUTPUT_SCHEMA_CODER_URN =\n+\t\t\"flink:coder:schema:aggregate_function_output:v1\";\n+\n+\t@VisibleForTesting\n+\tprotected static final String STREAM_GROUP_AGGREGATE_URN = \"flink:transform:stream_group_aggregate:v1\";\n+\n+\t@VisibleForTesting\n+\tprotected static final byte NORMAL_RECORD = 0;\n+\n+\tprivate static final byte TRIGGER_TIMER = 1;\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate static final int DEFAULT_STATE_CACHE_SIZE = 1000;\n+\n+\t/**\n+\t * The input logical type.\n+\t */\n+\tprotected final RowType inputType;\n+\n+\t/**\n+\t * The output logical type.\n+\t */\n+\tprotected final RowType outputType;\n+\n+\t/**\n+\t * The options used to configure the Python worker process.\n+\t */\n+\tprivate final Map<String, String> jobOptions;\n+\n+\tprivate final PythonFunctionInfo[] aggregateFunctions;\n+\n+\t/**\n+\t * The array of the key indexes.\n+\t */\n+\tprivate final int[] grouping;\n+\n+\t/**\n+\t * The index of a count aggregate used to calculate the number of accumulated rows.\n+\t */\n+\tprivate final int indexOfCountStar;\n+\n+\t/**\n+\t * Generate retract messages if true.\n+\t */\n+\tprivate final boolean generateUpdateBefore;\n+\n+\t/**\n+\t * The minimum time in milliseconds until state which was not updated will be retained.\n+\t */\n+\tfinal long minRetentionTime;\n+\n+\t/**\n+\t * The maximum time in milliseconds until state which was not updated will be retained.\n+\t */\n+\tfinal long maxRetentionTime;\n+\n+\t/**\n+\t * The maximum NUMBER of the states cached in Python side.\n+\t */\n+\tprivate int stateCacheSize = DEFAULT_STATE_CACHE_SIZE;\n+\n+\t/**\n+\t * Indicates whether state cleaning is enabled. Can be calculated from the `minRetentionTime`.\n+\t */\n+\tprotected final boolean stateCleaningEnabled;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "87967d2198d73dea7a84808d6c1a5076a1c7c317"}, "originalPosition": 146}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjAyMTg0MA==", "bodyText": "Add some comments about the meaning of each column?", "url": "https://github.com/apache/flink/pull/13420#discussion_r492021840", "createdAt": "2020-09-21T12:55:59Z", "author": {"login": "dianfu"}, "path": "flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/aggregate/RowDataPythonStreamGroupAggregateOperator.java", "diffHunk": "@@ -0,0 +1,397 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.operators.python.aggregate;\n+\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.api.common.state.ValueState;\n+import org.apache.flink.api.common.state.ValueStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.Types;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.memory.ByteArrayInputStreamWithPos;\n+import org.apache.flink.core.memory.ByteArrayOutputStreamWithPos;\n+import org.apache.flink.core.memory.DataInputViewStreamWrapper;\n+import org.apache.flink.core.memory.DataOutputViewStreamWrapper;\n+import org.apache.flink.fnexecution.v1.FlinkFnApi;\n+import org.apache.flink.python.PythonFunctionRunner;\n+import org.apache.flink.runtime.state.VoidNamespace;\n+import org.apache.flink.runtime.state.VoidNamespaceSerializer;\n+import org.apache.flink.streaming.api.SimpleTimerService;\n+import org.apache.flink.streaming.api.TimerService;\n+import org.apache.flink.streaming.api.operators.InternalTimer;\n+import org.apache.flink.streaming.api.operators.Triggerable;\n+import org.apache.flink.streaming.api.operators.python.AbstractOneInputPythonFunctionOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.UpdatableRowData;\n+import org.apache.flink.table.functions.python.PythonEnv;\n+import org.apache.flink.table.functions.python.PythonFunctionInfo;\n+import org.apache.flink.table.planner.plan.utils.KeySelectorUtil;\n+import org.apache.flink.table.runtime.functions.CleanupState;\n+import org.apache.flink.table.runtime.keyselector.RowDataKeySelector;\n+import org.apache.flink.table.runtime.operators.python.utils.PythonOperatorUtils;\n+import org.apache.flink.table.runtime.operators.python.utils.StreamRecordRowDataWrappingCollector;\n+import org.apache.flink.table.runtime.runners.python.beam.BeamTableStatefulPythonFunctionRunner;\n+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n+import org.apache.flink.table.runtime.typeutils.PythonTypeUtils;\n+import org.apache.flink.table.types.logical.BigIntType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.TinyIntType;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.runtime.typeutils.PythonTypeUtils.toProtoType;\n+\n+/**\n+ *  The Python AggregateFunction operator for the blink planner.\n+ */\n+public class RowDataPythonStreamGroupAggregateOperator\n+\textends AbstractOneInputPythonFunctionOperator<RowData, RowData>\n+\timplements Triggerable<RowData, VoidNamespace>, CleanupState {\n+\n+\t@VisibleForTesting\n+\tprotected static final String FLINK_AGGREGATE_FUNCTION_INPUT_SCHEMA_CODER_URN =\n+\t\t\"flink:coder:schema:aggregate_function_input:v1\";\n+\n+\t@VisibleForTesting\n+\tprotected static final String FLINK_AGGREGATE_FUNCTION_OUTPUT_SCHEMA_CODER_URN =\n+\t\t\"flink:coder:schema:aggregate_function_output:v1\";\n+\n+\t@VisibleForTesting\n+\tprotected static final String STREAM_GROUP_AGGREGATE_URN = \"flink:transform:stream_group_aggregate:v1\";\n+\n+\t@VisibleForTesting\n+\tprotected static final byte NORMAL_RECORD = 0;\n+\n+\tprivate static final byte TRIGGER_TIMER = 1;\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate static final int DEFAULT_STATE_CACHE_SIZE = 1000;\n+\n+\t/**\n+\t * The input logical type.\n+\t */\n+\tprotected final RowType inputType;\n+\n+\t/**\n+\t * The output logical type.\n+\t */\n+\tprotected final RowType outputType;\n+\n+\t/**\n+\t * The options used to configure the Python worker process.\n+\t */\n+\tprivate final Map<String, String> jobOptions;\n+\n+\tprivate final PythonFunctionInfo[] aggregateFunctions;\n+\n+\t/**\n+\t * The array of the key indexes.\n+\t */\n+\tprivate final int[] grouping;\n+\n+\t/**\n+\t * The index of a count aggregate used to calculate the number of accumulated rows.\n+\t */\n+\tprivate final int indexOfCountStar;\n+\n+\t/**\n+\t * Generate retract messages if true.\n+\t */\n+\tprivate final boolean generateUpdateBefore;\n+\n+\t/**\n+\t * The minimum time in milliseconds until state which was not updated will be retained.\n+\t */\n+\tfinal long minRetentionTime;\n+\n+\t/**\n+\t * The maximum time in milliseconds until state which was not updated will be retained.\n+\t */\n+\tfinal long maxRetentionTime;\n+\n+\t/**\n+\t * The maximum NUMBER of the states cached in Python side.\n+\t */\n+\tprivate int stateCacheSize = DEFAULT_STATE_CACHE_SIZE;\n+\n+\t/**\n+\t * Indicates whether state cleaning is enabled. Can be calculated from the `minRetentionTime`.\n+\t */\n+\tprotected final boolean stateCleaningEnabled;\n+\n+\tprivate transient Object keyForTimerService;\n+\n+\t/**\n+\t * The user-defined function input logical type.\n+\t */\n+\tprotected transient RowType userDefinedFunctionInputType;\n+\n+\t/**\n+\t * The TypeSerializer for udf execution results.\n+\t */\n+\tprotected transient TypeSerializer<RowData> udfOutputTypeSerializer;\n+\n+\t/**\n+\t * The TypeSerializer for udf input elements.\n+\t */\n+\tprotected transient TypeSerializer<RowData> udfInputTypeSerializer;\n+\n+\t/**\n+\t * Reusable InputStream used to holding the execution results to be deserialized.\n+\t */\n+\tprivate transient ByteArrayInputStreamWithPos bais;\n+\n+\t/**\n+\t * InputStream Wrapper.\n+\t */\n+\tprivate transient DataInputViewStreamWrapper baisWrapper;\n+\n+\t/**\n+\t * Reusable OutputStream used to holding the serialized input elements.\n+\t */\n+\tprivate transient ByteArrayOutputStreamWithPos baos;\n+\n+\t/**\n+\t * OutputStream Wrapper.\n+\t */\n+\tprivate transient DataOutputViewStreamWrapper baosWrapper;\n+\n+\tprivate transient TimerService timerService;\n+\n+\t// holds the latest registered cleanup timer\n+\tprivate transient ValueState<Long> cleanupTimeState;\n+\n+\tprivate transient UpdatableRowData reuseRowData;\n+\tprivate transient UpdatableRowData reuseTimerRowData;\n+\n+\t/**\n+\t * The collector used to collect records.\n+\t */\n+\tprivate transient StreamRecordRowDataWrappingCollector rowDataWrapper;\n+\n+\tpublic RowDataPythonStreamGroupAggregateOperator(\n+\t\t\tConfiguration config,\n+\t\t\tRowType inputType,\n+\t\t\tRowType outputType,\n+\t\t\tPythonFunctionInfo[] aggregateFunctions,\n+\t\t\tint[] grouping,\n+\t\t\tint indexOfCountStar,\n+\t\t\tboolean generateUpdateBefore,\n+\t\t\tlong minRetentionTime,\n+\t\t\tlong maxRetentionTime) {\n+\t\tsuper(config);\n+\t\tthis.inputType = Preconditions.checkNotNull(inputType);\n+\t\tthis.outputType = Preconditions.checkNotNull(outputType);\n+\t\tthis.aggregateFunctions = aggregateFunctions;\n+\t\tthis.jobOptions = buildJobOptions(config);\n+\t\tthis.grouping = grouping;\n+\t\tthis.indexOfCountStar = indexOfCountStar;\n+\t\tthis.generateUpdateBefore = generateUpdateBefore;\n+\t\tthis.minRetentionTime = minRetentionTime;\n+\t\tthis.maxRetentionTime = maxRetentionTime;\n+\t\tthis.stateCleaningEnabled = minRetentionTime > 1;\n+\t}\n+\n+\t/**\n+\t * As the beam state gRPC service will access the KeyedStateBackend in parallel with this operator, we must\n+\t * override this method to prevent changing the current key of the KeyedStateBackend while the beam service\n+\t * is handling requests.\n+\t */\n+\t@Override\n+\tpublic void setCurrentKey(Object key) {\n+\t\tkeyForTimerService = key;\n+\t}\n+\n+\t@Override\n+\tpublic Object getCurrentKey() {\n+\t\treturn keyForTimerService;\n+\t}\n+\n+\t@Override\n+\tpublic void open() throws Exception {\n+\t\tList<RowType.RowField> fields = new ArrayList<>();\n+\t\tfields.add(new RowType.RowField(\"record_type\", new TinyIntType()));\n+\t\tfields.add(new RowType.RowField(\"row\", inputType));\n+\t\tfields.add(new RowType.RowField(\"timestamp\", new BigIntType()));\n+\t\tfields.add(new RowType.RowField(\"key\", getKeyType()));\n+\t\tuserDefinedFunctionInputType = new RowType(fields);\n+\t\tudfInputTypeSerializer = PythonTypeUtils.toBlinkTypeSerializer(userDefinedFunctionInputType);\n+\t\tudfOutputTypeSerializer = PythonTypeUtils.toBlinkTypeSerializer(outputType);\n+\t\tbais = new ByteArrayInputStreamWithPos();\n+\t\tbaisWrapper = new DataInputViewStreamWrapper(bais);\n+\t\tbaos = new ByteArrayOutputStreamWithPos();\n+\t\tbaosWrapper = new DataOutputViewStreamWrapper(baos);\n+\t\ttimerService = new SimpleTimerService(\n+\t\t\tgetInternalTimerService(\"state-clean-timer\", VoidNamespaceSerializer.INSTANCE, this));\n+\t\treuseRowData = new UpdatableRowData(GenericRowData.of(NORMAL_RECORD, null, null, null), 4);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "87967d2198d73dea7a84808d6c1a5076a1c7c317"}, "originalPosition": 252}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjAyNDg5Nw==", "bodyText": "How users could know about this config? Should we add it in PythonConfig?", "url": "https://github.com/apache/flink/pull/13420#discussion_r492024897", "createdAt": "2020-09-21T12:58:57Z", "author": {"login": "dianfu"}, "path": "flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/aggregate/RowDataPythonStreamGroupAggregateOperator.java", "diffHunk": "@@ -0,0 +1,397 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.operators.python.aggregate;\n+\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.api.common.state.ValueState;\n+import org.apache.flink.api.common.state.ValueStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.Types;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.memory.ByteArrayInputStreamWithPos;\n+import org.apache.flink.core.memory.ByteArrayOutputStreamWithPos;\n+import org.apache.flink.core.memory.DataInputViewStreamWrapper;\n+import org.apache.flink.core.memory.DataOutputViewStreamWrapper;\n+import org.apache.flink.fnexecution.v1.FlinkFnApi;\n+import org.apache.flink.python.PythonFunctionRunner;\n+import org.apache.flink.runtime.state.VoidNamespace;\n+import org.apache.flink.runtime.state.VoidNamespaceSerializer;\n+import org.apache.flink.streaming.api.SimpleTimerService;\n+import org.apache.flink.streaming.api.TimerService;\n+import org.apache.flink.streaming.api.operators.InternalTimer;\n+import org.apache.flink.streaming.api.operators.Triggerable;\n+import org.apache.flink.streaming.api.operators.python.AbstractOneInputPythonFunctionOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.UpdatableRowData;\n+import org.apache.flink.table.functions.python.PythonEnv;\n+import org.apache.flink.table.functions.python.PythonFunctionInfo;\n+import org.apache.flink.table.planner.plan.utils.KeySelectorUtil;\n+import org.apache.flink.table.runtime.functions.CleanupState;\n+import org.apache.flink.table.runtime.keyselector.RowDataKeySelector;\n+import org.apache.flink.table.runtime.operators.python.utils.PythonOperatorUtils;\n+import org.apache.flink.table.runtime.operators.python.utils.StreamRecordRowDataWrappingCollector;\n+import org.apache.flink.table.runtime.runners.python.beam.BeamTableStatefulPythonFunctionRunner;\n+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n+import org.apache.flink.table.runtime.typeutils.PythonTypeUtils;\n+import org.apache.flink.table.types.logical.BigIntType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.TinyIntType;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.runtime.typeutils.PythonTypeUtils.toProtoType;\n+\n+/**\n+ *  The Python AggregateFunction operator for the blink planner.\n+ */\n+public class RowDataPythonStreamGroupAggregateOperator\n+\textends AbstractOneInputPythonFunctionOperator<RowData, RowData>\n+\timplements Triggerable<RowData, VoidNamespace>, CleanupState {\n+\n+\t@VisibleForTesting\n+\tprotected static final String FLINK_AGGREGATE_FUNCTION_INPUT_SCHEMA_CODER_URN =\n+\t\t\"flink:coder:schema:aggregate_function_input:v1\";\n+\n+\t@VisibleForTesting\n+\tprotected static final String FLINK_AGGREGATE_FUNCTION_OUTPUT_SCHEMA_CODER_URN =\n+\t\t\"flink:coder:schema:aggregate_function_output:v1\";\n+\n+\t@VisibleForTesting\n+\tprotected static final String STREAM_GROUP_AGGREGATE_URN = \"flink:transform:stream_group_aggregate:v1\";\n+\n+\t@VisibleForTesting\n+\tprotected static final byte NORMAL_RECORD = 0;\n+\n+\tprivate static final byte TRIGGER_TIMER = 1;\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate static final int DEFAULT_STATE_CACHE_SIZE = 1000;\n+\n+\t/**\n+\t * The input logical type.\n+\t */\n+\tprotected final RowType inputType;\n+\n+\t/**\n+\t * The output logical type.\n+\t */\n+\tprotected final RowType outputType;\n+\n+\t/**\n+\t * The options used to configure the Python worker process.\n+\t */\n+\tprivate final Map<String, String> jobOptions;\n+\n+\tprivate final PythonFunctionInfo[] aggregateFunctions;\n+\n+\t/**\n+\t * The array of the key indexes.\n+\t */\n+\tprivate final int[] grouping;\n+\n+\t/**\n+\t * The index of a count aggregate used to calculate the number of accumulated rows.\n+\t */\n+\tprivate final int indexOfCountStar;\n+\n+\t/**\n+\t * Generate retract messages if true.\n+\t */\n+\tprivate final boolean generateUpdateBefore;\n+\n+\t/**\n+\t * The minimum time in milliseconds until state which was not updated will be retained.\n+\t */\n+\tfinal long minRetentionTime;\n+\n+\t/**\n+\t * The maximum time in milliseconds until state which was not updated will be retained.\n+\t */\n+\tfinal long maxRetentionTime;\n+\n+\t/**\n+\t * The maximum NUMBER of the states cached in Python side.\n+\t */\n+\tprivate int stateCacheSize = DEFAULT_STATE_CACHE_SIZE;\n+\n+\t/**\n+\t * Indicates whether state cleaning is enabled. Can be calculated from the `minRetentionTime`.\n+\t */\n+\tprotected final boolean stateCleaningEnabled;\n+\n+\tprivate transient Object keyForTimerService;\n+\n+\t/**\n+\t * The user-defined function input logical type.\n+\t */\n+\tprotected transient RowType userDefinedFunctionInputType;\n+\n+\t/**\n+\t * The TypeSerializer for udf execution results.\n+\t */\n+\tprotected transient TypeSerializer<RowData> udfOutputTypeSerializer;\n+\n+\t/**\n+\t * The TypeSerializer for udf input elements.\n+\t */\n+\tprotected transient TypeSerializer<RowData> udfInputTypeSerializer;\n+\n+\t/**\n+\t * Reusable InputStream used to holding the execution results to be deserialized.\n+\t */\n+\tprivate transient ByteArrayInputStreamWithPos bais;\n+\n+\t/**\n+\t * InputStream Wrapper.\n+\t */\n+\tprivate transient DataInputViewStreamWrapper baisWrapper;\n+\n+\t/**\n+\t * Reusable OutputStream used to holding the serialized input elements.\n+\t */\n+\tprivate transient ByteArrayOutputStreamWithPos baos;\n+\n+\t/**\n+\t * OutputStream Wrapper.\n+\t */\n+\tprivate transient DataOutputViewStreamWrapper baosWrapper;\n+\n+\tprivate transient TimerService timerService;\n+\n+\t// holds the latest registered cleanup timer\n+\tprivate transient ValueState<Long> cleanupTimeState;\n+\n+\tprivate transient UpdatableRowData reuseRowData;\n+\tprivate transient UpdatableRowData reuseTimerRowData;\n+\n+\t/**\n+\t * The collector used to collect records.\n+\t */\n+\tprivate transient StreamRecordRowDataWrappingCollector rowDataWrapper;\n+\n+\tpublic RowDataPythonStreamGroupAggregateOperator(\n+\t\t\tConfiguration config,\n+\t\t\tRowType inputType,\n+\t\t\tRowType outputType,\n+\t\t\tPythonFunctionInfo[] aggregateFunctions,\n+\t\t\tint[] grouping,\n+\t\t\tint indexOfCountStar,\n+\t\t\tboolean generateUpdateBefore,\n+\t\t\tlong minRetentionTime,\n+\t\t\tlong maxRetentionTime) {\n+\t\tsuper(config);\n+\t\tthis.inputType = Preconditions.checkNotNull(inputType);\n+\t\tthis.outputType = Preconditions.checkNotNull(outputType);\n+\t\tthis.aggregateFunctions = aggregateFunctions;\n+\t\tthis.jobOptions = buildJobOptions(config);\n+\t\tthis.grouping = grouping;\n+\t\tthis.indexOfCountStar = indexOfCountStar;\n+\t\tthis.generateUpdateBefore = generateUpdateBefore;\n+\t\tthis.minRetentionTime = minRetentionTime;\n+\t\tthis.maxRetentionTime = maxRetentionTime;\n+\t\tthis.stateCleaningEnabled = minRetentionTime > 1;\n+\t}\n+\n+\t/**\n+\t * As the beam state gRPC service will access the KeyedStateBackend in parallel with this operator, we must\n+\t * override this method to prevent changing the current key of the KeyedStateBackend while the beam service\n+\t * is handling requests.\n+\t */\n+\t@Override\n+\tpublic void setCurrentKey(Object key) {\n+\t\tkeyForTimerService = key;\n+\t}\n+\n+\t@Override\n+\tpublic Object getCurrentKey() {\n+\t\treturn keyForTimerService;\n+\t}\n+\n+\t@Override\n+\tpublic void open() throws Exception {\n+\t\tList<RowType.RowField> fields = new ArrayList<>();\n+\t\tfields.add(new RowType.RowField(\"record_type\", new TinyIntType()));\n+\t\tfields.add(new RowType.RowField(\"row\", inputType));\n+\t\tfields.add(new RowType.RowField(\"timestamp\", new BigIntType()));\n+\t\tfields.add(new RowType.RowField(\"key\", getKeyType()));\n+\t\tuserDefinedFunctionInputType = new RowType(fields);\n+\t\tudfInputTypeSerializer = PythonTypeUtils.toBlinkTypeSerializer(userDefinedFunctionInputType);\n+\t\tudfOutputTypeSerializer = PythonTypeUtils.toBlinkTypeSerializer(outputType);\n+\t\tbais = new ByteArrayInputStreamWithPos();\n+\t\tbaisWrapper = new DataInputViewStreamWrapper(bais);\n+\t\tbaos = new ByteArrayOutputStreamWithPos();\n+\t\tbaosWrapper = new DataOutputViewStreamWrapper(baos);\n+\t\ttimerService = new SimpleTimerService(\n+\t\t\tgetInternalTimerService(\"state-clean-timer\", VoidNamespaceSerializer.INSTANCE, this));\n+\t\treuseRowData = new UpdatableRowData(GenericRowData.of(NORMAL_RECORD, null, null, null), 4);\n+\t\treuseTimerRowData = new UpdatableRowData(GenericRowData.of(TRIGGER_TIMER, null, null, null), 4);\n+\t\trowDataWrapper = new StreamRecordRowDataWrappingCollector(output);\n+\t\tinitCleanupTimeState();\n+\t\tsuper.open();\n+\t}\n+\n+\t@Override\n+\tpublic PythonFunctionRunner createPythonFunctionRunner() throws Exception {\n+\t\treturn new BeamTableStatefulPythonFunctionRunner(\n+\t\t\tgetRuntimeContext().getTaskName(),\n+\t\t\tcreatePythonEnvironmentManager(),\n+\t\t\tuserDefinedFunctionInputType,\n+\t\t\toutputType,\n+\t\t\tSTREAM_GROUP_AGGREGATE_URN,\n+\t\t\tgetUserDefinedFunctionsProto(),\n+\t\t\tFLINK_AGGREGATE_FUNCTION_INPUT_SCHEMA_CODER_URN,\n+\t\t\tFLINK_AGGREGATE_FUNCTION_OUTPUT_SCHEMA_CODER_URN,\n+\t\t\tjobOptions,\n+\t\t\tgetFlinkMetricContainer(),\n+\t\t\tgetKeyedStateBackend(),\n+\t\t\tgetKeySerializer());\n+\t}\n+\n+\t@Override\n+\tpublic void processElement(StreamRecord<RowData> element) throws Exception {\n+\t\tRowData value = element.getValue();\n+\t\tprocessElementInternal(value);\n+\t\telementCount++;\n+\t\tcheckInvokeFinishBundleByCount();\n+\t\temitResults();\n+\t}\n+\n+\t@Override\n+\tpublic void emitResult(Tuple2<byte[], Integer> resultTuple) throws Exception {\n+\t\tbyte[] rawUdfResult = resultTuple.f0;\n+\t\tint length = resultTuple.f1;\n+\t\tbais.setBuffer(rawUdfResult, 0, length);\n+\t\tRowData udfResult = udfOutputTypeSerializer.deserialize(baisWrapper);\n+\t\trowDataWrapper.collect(udfResult);\n+\t}\n+\n+\t/**\n+\t * Invoked when an event-time timer fires.\n+\t */\n+\t@Override\n+\tpublic void onEventTime(InternalTimer<RowData, VoidNamespace> timer) {\n+\t\t// ignore\n+\t}\n+\n+\t/**\n+\t * Invoked when a processing-time timer fires.\n+\t */\n+\t@Override\n+\tpublic void onProcessingTime(InternalTimer<RowData, VoidNamespace> timer) throws Exception {\n+\t\tif (stateCleaningEnabled) {\n+\t\t\tRowData key = timer.getKey();\n+\t\t\tlong timestamp = timer.getTimestamp();\n+\t\t\treuseTimerRowData.setLong(2, timestamp);\n+\t\t\treuseTimerRowData.setField(3, key);\n+\t\t\tudfInputTypeSerializer.serialize(reuseTimerRowData, baosWrapper);\n+\t\t\tpythonFunctionRunner.process(baos.toByteArray());\n+\t\t\tbaos.reset();\n+\t\t\telementCount++;\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic PythonEnv getPythonEnv() {\n+\t\treturn aggregateFunctions[0].getPythonFunction().getPythonEnv();\n+\t}\n+\n+\tprivate void registerProcessingCleanupTimer(long currentTime) throws Exception {\n+\t\tif (stateCleaningEnabled) {\n+\t\t\tsynchronized (getKeyedStateBackend()) {\n+\t\t\t\tgetKeyedStateBackend().setCurrentKey(getCurrentKey());\n+\t\t\t\tregisterProcessingCleanupTimer(\n+\t\t\t\t\tcleanupTimeState,\n+\t\t\t\t\tcurrentTime,\n+\t\t\t\t\tminRetentionTime,\n+\t\t\t\t\tmaxRetentionTime,\n+\t\t\t\t\ttimerService\n+\t\t\t\t);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate void initCleanupTimeState() {\n+\t\tif (stateCleaningEnabled) {\n+\t\t\tValueStateDescriptor<Long> inputCntDescriptor =\n+\t\t\t\tnew ValueStateDescriptor<>(\"PythonGroupAggregateCleanupTime\", Types.LONG);\n+\t\t\tcleanupTimeState = getRuntimeContext().getState(inputCntDescriptor);\n+\t\t}\n+\t}\n+\n+\tprivate Map<String, String> buildJobOptions(Configuration config) {\n+\t\tMap<String, String> jobOptions = new HashMap<>();\n+\t\tif (config.containsKey(\"table.exec.timezone\")) {\n+\t\t\tjobOptions.put(\"table.exec.timezone\", config.getString(\"table.exec.timezone\", null));\n+\t\t}\n+\t\tif (config.containsKey(\"python.state.cache.size\")) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "87967d2198d73dea7a84808d6c1a5076a1c7c317"}, "originalPosition": 352}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8a990fb143b847582116fb3178d6350c3e0d38ed", "author": {"user": {"login": "WeiZhong94", "name": "Wei Zhong"}}, "url": "https://github.com/apache/flink/commit/8a990fb143b847582116fb3178d6350c3e0d38ed", "committedDate": "2020-09-22T07:17:57Z", "message": "address comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7571b70b754ee4e740b3a3777c506fb8f5e67c40", "author": {"user": {"login": "WeiZhong94", "name": "Wei Zhong"}}, "url": "https://github.com/apache/flink/commit/7571b70b754ee4e740b3a3777c506fb8f5e67c40", "committedDate": "2020-09-22T09:41:07Z", "message": "address comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDkzMjkzMjM2", "url": "https://github.com/apache/flink/pull/13420#pullrequestreview-493293236", "createdAt": "2020-09-22T09:47:17Z", "commit": {"oid": "7571b70b754ee4e740b3a3777c506fb8f5e67c40"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4085, "cost": 1, "resetAt": "2021-10-28T16:48:13Z"}}}