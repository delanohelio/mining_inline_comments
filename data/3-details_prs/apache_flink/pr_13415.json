{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDg5MDczMDgz", "number": 13415, "title": "[FLINK-19277][python] Introduce BatchArrowPythonGroupWindowAggregateFunctionOperator", "bodyText": "What is the purpose of the change\nThis pull request will introduce BatchArrowPythonGroupWindowAggregateFunctionOperator for supporting Pandas Batch Group Window Aggregation\nBrief change log\n\nadd BatchArrowPythonGroupWindowAggregateFunctionOperator\n\nVerifying this change\nThis change added tests and can be verified as follows:\n\nBatchArrowPythonGroupWindowAggregateFunctionOperatorTest\n\nDoes this pull request potentially affect one of the following parts:\n\nDependencies (does it add or upgrade a dependency): (no)\nThe public API, i.e., is any changed class annotated with @Public(Evolving): (no)\nThe serializers: (no)\nThe runtime per-record code paths (performance sensitive): (no)\nAnything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (no)\nThe S3 file system connector: (no)\n\nDocumentation\n\nDoes this pull request introduce a new feature? (no)\nIf yes, how is the feature documented? (not applicable)", "createdAt": "2020-09-18T05:05:10Z", "url": "https://github.com/apache/flink/pull/13415", "merged": true, "mergeCommit": {"oid": "0007d5e1fa2fa81aededb4c783873046d5e44a23"}, "closed": true, "closedAt": "2020-09-18T13:59:05Z", "author": {"login": "HuangXingBo"}, "timelineItems": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdJ-WK4gH2gAyNDg5MDczMDgzOjRhMTgyYTQ5OWMzODY3NzY1YjZlZjNjMzk1N2ZiZTRjNjNjMmRjZTE=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdKAMFwgFqTQ5MTIwMzUwMQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "4a182a499c3867765b6ef3c3957fbe4c63c2dce1", "author": {"user": {"login": "HuangXingBo", "name": null}}, "url": "https://github.com/apache/flink/commit/4a182a499c3867765b6ef3c3957fbe4c63c2dce1", "committedDate": "2020-09-18T05:00:53Z", "message": "[FLINK-19277][python] Introduce BatchArrowPythonGroupWindowAggregateFunctionOperator"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDkxMTYxMTUw", "url": "https://github.com/apache/flink/pull/13415#pullrequestreview-491161150", "createdAt": "2020-09-18T05:31:02Z", "commit": {"oid": "4a182a499c3867765b6ef3c3957fbe4c63c2dce1"}, "state": "COMMENTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xOFQwNTozMTowMlrOHT-oTA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xOFQwNTo1ODo1N1rOHT_IBg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDcxMTExNg==", "bodyText": "Putting super.open() at the end of this method? Although it doesn't affect the correctness, it makes the code more readable.", "url": "https://github.com/apache/flink/pull/13415#discussion_r490711116", "createdAt": "2020-09-18T05:31:02Z", "author": {"login": "dianfu"}, "path": "flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/aggregate/arrow/batch/BatchArrowPythonGroupWindowAggregateFunctionOperator.java", "diffHunk": "@@ -0,0 +1,216 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.operators.python.aggregate.arrow.batch;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.JoinedRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.data.binary.BinaryRowData;\n+import org.apache.flink.table.functions.AggregateFunction;\n+import org.apache.flink.table.functions.python.PythonFunctionInfo;\n+import org.apache.flink.table.runtime.operators.window.TimeWindow;\n+import org.apache.flink.table.runtime.operators.window.grouping.HeapWindowsGrouping;\n+import org.apache.flink.table.runtime.operators.window.grouping.WindowsGrouping;\n+import org.apache.flink.table.runtime.util.RowIterator;\n+import org.apache.flink.table.types.logical.RowType;\n+\n+import java.util.LinkedList;\n+\n+/**\n+ * The Batch Arrow Python {@link AggregateFunction} Operator for Group Window Aggregation.\n+ */\n+@Internal\n+public class BatchArrowPythonGroupWindowAggregateFunctionOperator\n+\textends AbstractBatchArrowPythonAggregateFunctionOperator {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\t/**\n+\t * The Infos of the Window.\n+\t * 0 -> start of the Window.\n+\t * 1 -> end of the Window.\n+\t * 2 -> row time of the Window.\n+\t */\n+\tprivate final int[] namedProperties;\n+\n+\t/**\n+\t * The row time index of the input data.\n+\t */\n+\tprivate final int inputTimeFieldIndex;\n+\n+\t/**\n+\t * The window elements buffer size limit used in group window agg operator.\n+\t */\n+\tprivate final int maxLimitSize;\n+\n+\t/**\n+\t * The window size of the window.\n+\t */\n+\tprivate final long windowSize;\n+\n+\t/**\n+\t * The sliding size of the sliding window.\n+\t */\n+\tprivate final long slideSize;\n+\n+\tprivate transient WindowsGrouping windowsGrouping;\n+\n+\t/**\n+\t * The GenericRowData reused holding the property of the window, such as window start, window\n+\t * end and window time.\n+\t */\n+\tprivate transient GenericRowData windowProperty;\n+\n+\t/**\n+\t * The JoinedRowData reused holding the window agg execution result.\n+\t */\n+\tprivate transient JoinedRowData windowAggResult;\n+\n+\t/**\n+\t * The queue holding the input groupSet with the TimeWindow for which the execution results\n+\t * have not been received.\n+\t */\n+\tprivate transient LinkedList<Tuple2<RowData, TimeWindow>> inputKeyAndWindow;\n+\n+\tpublic BatchArrowPythonGroupWindowAggregateFunctionOperator(\n+\t\tConfiguration config,\n+\t\tPythonFunctionInfo[] pandasAggFunctions,\n+\t\tRowType inputType,\n+\t\tRowType outputType,\n+\t\tint inputTimeFieldIndex,\n+\t\tint maxLimitSize,\n+\t\tlong windowSize,\n+\t\tlong slideSize,\n+\t\tint[] namedProperties,\n+\t\tint[] groupKey,\n+\t\tint[] groupingSet,\n+\t\tint[] udafInputOffsets) {\n+\t\tsuper(config, pandasAggFunctions, inputType, outputType, groupKey, groupingSet, udafInputOffsets);\n+\t\tthis.namedProperties = namedProperties;\n+\t\tthis.inputTimeFieldIndex = inputTimeFieldIndex;\n+\t\tthis.maxLimitSize = maxLimitSize;\n+\t\tthis.windowSize = windowSize;\n+\t\tthis.slideSize = slideSize;\n+\t}\n+\n+\t@Override\n+\tpublic void open() throws Exception {\n+\t\tuserDefinedFunctionOutputType = new RowType(\n+\t\t\toutputType.getFields().subList(groupingSet.length, outputType.getFieldCount() - namedProperties.length));\n+\t\tsuper.open();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4a182a499c3867765b6ef3c3957fbe4c63c2dce1"}, "originalPosition": 120}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDcxNDEwNw==", "bodyText": "Could we refactor the following code a bit to avoid duplication?", "url": "https://github.com/apache/flink/pull/13415#discussion_r490714107", "createdAt": "2020-09-18T05:41:11Z", "author": {"login": "dianfu"}, "path": "flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/aggregate/arrow/batch/BatchArrowPythonGroupWindowAggregateFunctionOperator.java", "diffHunk": "@@ -0,0 +1,216 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.operators.python.aggregate.arrow.batch;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.JoinedRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.data.binary.BinaryRowData;\n+import org.apache.flink.table.functions.AggregateFunction;\n+import org.apache.flink.table.functions.python.PythonFunctionInfo;\n+import org.apache.flink.table.runtime.operators.window.TimeWindow;\n+import org.apache.flink.table.runtime.operators.window.grouping.HeapWindowsGrouping;\n+import org.apache.flink.table.runtime.operators.window.grouping.WindowsGrouping;\n+import org.apache.flink.table.runtime.util.RowIterator;\n+import org.apache.flink.table.types.logical.RowType;\n+\n+import java.util.LinkedList;\n+\n+/**\n+ * The Batch Arrow Python {@link AggregateFunction} Operator for Group Window Aggregation.\n+ */\n+@Internal\n+public class BatchArrowPythonGroupWindowAggregateFunctionOperator\n+\textends AbstractBatchArrowPythonAggregateFunctionOperator {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\t/**\n+\t * The Infos of the Window.\n+\t * 0 -> start of the Window.\n+\t * 1 -> end of the Window.\n+\t * 2 -> row time of the Window.\n+\t */\n+\tprivate final int[] namedProperties;\n+\n+\t/**\n+\t * The row time index of the input data.\n+\t */\n+\tprivate final int inputTimeFieldIndex;\n+\n+\t/**\n+\t * The window elements buffer size limit used in group window agg operator.\n+\t */\n+\tprivate final int maxLimitSize;\n+\n+\t/**\n+\t * The window size of the window.\n+\t */\n+\tprivate final long windowSize;\n+\n+\t/**\n+\t * The sliding size of the sliding window.\n+\t */\n+\tprivate final long slideSize;\n+\n+\tprivate transient WindowsGrouping windowsGrouping;\n+\n+\t/**\n+\t * The GenericRowData reused holding the property of the window, such as window start, window\n+\t * end and window time.\n+\t */\n+\tprivate transient GenericRowData windowProperty;\n+\n+\t/**\n+\t * The JoinedRowData reused holding the window agg execution result.\n+\t */\n+\tprivate transient JoinedRowData windowAggResult;\n+\n+\t/**\n+\t * The queue holding the input groupSet with the TimeWindow for which the execution results\n+\t * have not been received.\n+\t */\n+\tprivate transient LinkedList<Tuple2<RowData, TimeWindow>> inputKeyAndWindow;\n+\n+\tpublic BatchArrowPythonGroupWindowAggregateFunctionOperator(\n+\t\tConfiguration config,\n+\t\tPythonFunctionInfo[] pandasAggFunctions,\n+\t\tRowType inputType,\n+\t\tRowType outputType,\n+\t\tint inputTimeFieldIndex,\n+\t\tint maxLimitSize,\n+\t\tlong windowSize,\n+\t\tlong slideSize,\n+\t\tint[] namedProperties,\n+\t\tint[] groupKey,\n+\t\tint[] groupingSet,\n+\t\tint[] udafInputOffsets) {\n+\t\tsuper(config, pandasAggFunctions, inputType, outputType, groupKey, groupingSet, udafInputOffsets);\n+\t\tthis.namedProperties = namedProperties;\n+\t\tthis.inputTimeFieldIndex = inputTimeFieldIndex;\n+\t\tthis.maxLimitSize = maxLimitSize;\n+\t\tthis.windowSize = windowSize;\n+\t\tthis.slideSize = slideSize;\n+\t}\n+\n+\t@Override\n+\tpublic void open() throws Exception {\n+\t\tuserDefinedFunctionOutputType = new RowType(\n+\t\t\toutputType.getFields().subList(groupingSet.length, outputType.getFieldCount() - namedProperties.length));\n+\t\tsuper.open();\n+\t\tinputKeyAndWindow = new LinkedList<>();\n+\t\twindowProperty = new GenericRowData(namedProperties.length);\n+\t\twindowAggResult = new JoinedRowData();\n+\t\twindowsGrouping = new HeapWindowsGrouping(\n+\t\t\tmaxLimitSize, windowSize, slideSize, inputTimeFieldIndex, false);\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws Exception {\n+\t\tsuper.close();\n+\t\twindowsGrouping.close();\n+\t}\n+\n+\t@Override\n+\tpublic void bufferInput(RowData input) throws Exception {\n+\t\t// always copy the projection result as the generated Projection reuses the projection result\n+\t\tBinaryRowData currentKey = groupKeyProjection.apply(input).copy();\n+\t\tcurrentKey.setRowKind(input.getRowKind());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4a182a499c3867765b6ef3c3957fbe4c63c2dce1"}, "originalPosition": 138}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDcxNjcwOA==", "bodyText": "Move this line inside if (currentBatchCount > 0)?", "url": "https://github.com/apache/flink/pull/13415#discussion_r490716708", "createdAt": "2020-09-18T05:50:28Z", "author": {"login": "dianfu"}, "path": "flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/aggregate/arrow/batch/BatchArrowPythonGroupWindowAggregateFunctionOperator.java", "diffHunk": "@@ -0,0 +1,216 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.operators.python.aggregate.arrow.batch;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.JoinedRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.data.binary.BinaryRowData;\n+import org.apache.flink.table.functions.AggregateFunction;\n+import org.apache.flink.table.functions.python.PythonFunctionInfo;\n+import org.apache.flink.table.runtime.operators.window.TimeWindow;\n+import org.apache.flink.table.runtime.operators.window.grouping.HeapWindowsGrouping;\n+import org.apache.flink.table.runtime.operators.window.grouping.WindowsGrouping;\n+import org.apache.flink.table.runtime.util.RowIterator;\n+import org.apache.flink.table.types.logical.RowType;\n+\n+import java.util.LinkedList;\n+\n+/**\n+ * The Batch Arrow Python {@link AggregateFunction} Operator for Group Window Aggregation.\n+ */\n+@Internal\n+public class BatchArrowPythonGroupWindowAggregateFunctionOperator\n+\textends AbstractBatchArrowPythonAggregateFunctionOperator {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\t/**\n+\t * The Infos of the Window.\n+\t * 0 -> start of the Window.\n+\t * 1 -> end of the Window.\n+\t * 2 -> row time of the Window.\n+\t */\n+\tprivate final int[] namedProperties;\n+\n+\t/**\n+\t * The row time index of the input data.\n+\t */\n+\tprivate final int inputTimeFieldIndex;\n+\n+\t/**\n+\t * The window elements buffer size limit used in group window agg operator.\n+\t */\n+\tprivate final int maxLimitSize;\n+\n+\t/**\n+\t * The window size of the window.\n+\t */\n+\tprivate final long windowSize;\n+\n+\t/**\n+\t * The sliding size of the sliding window.\n+\t */\n+\tprivate final long slideSize;\n+\n+\tprivate transient WindowsGrouping windowsGrouping;\n+\n+\t/**\n+\t * The GenericRowData reused holding the property of the window, such as window start, window\n+\t * end and window time.\n+\t */\n+\tprivate transient GenericRowData windowProperty;\n+\n+\t/**\n+\t * The JoinedRowData reused holding the window agg execution result.\n+\t */\n+\tprivate transient JoinedRowData windowAggResult;\n+\n+\t/**\n+\t * The queue holding the input groupSet with the TimeWindow for which the execution results\n+\t * have not been received.\n+\t */\n+\tprivate transient LinkedList<Tuple2<RowData, TimeWindow>> inputKeyAndWindow;\n+\n+\tpublic BatchArrowPythonGroupWindowAggregateFunctionOperator(\n+\t\tConfiguration config,\n+\t\tPythonFunctionInfo[] pandasAggFunctions,\n+\t\tRowType inputType,\n+\t\tRowType outputType,\n+\t\tint inputTimeFieldIndex,\n+\t\tint maxLimitSize,\n+\t\tlong windowSize,\n+\t\tlong slideSize,\n+\t\tint[] namedProperties,\n+\t\tint[] groupKey,\n+\t\tint[] groupingSet,\n+\t\tint[] udafInputOffsets) {\n+\t\tsuper(config, pandasAggFunctions, inputType, outputType, groupKey, groupingSet, udafInputOffsets);\n+\t\tthis.namedProperties = namedProperties;\n+\t\tthis.inputTimeFieldIndex = inputTimeFieldIndex;\n+\t\tthis.maxLimitSize = maxLimitSize;\n+\t\tthis.windowSize = windowSize;\n+\t\tthis.slideSize = slideSize;\n+\t}\n+\n+\t@Override\n+\tpublic void open() throws Exception {\n+\t\tuserDefinedFunctionOutputType = new RowType(\n+\t\t\toutputType.getFields().subList(groupingSet.length, outputType.getFieldCount() - namedProperties.length));\n+\t\tsuper.open();\n+\t\tinputKeyAndWindow = new LinkedList<>();\n+\t\twindowProperty = new GenericRowData(namedProperties.length);\n+\t\twindowAggResult = new JoinedRowData();\n+\t\twindowsGrouping = new HeapWindowsGrouping(\n+\t\t\tmaxLimitSize, windowSize, slideSize, inputTimeFieldIndex, false);\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws Exception {\n+\t\tsuper.close();\n+\t\twindowsGrouping.close();\n+\t}\n+\n+\t@Override\n+\tpublic void bufferInput(RowData input) throws Exception {\n+\t\t// always copy the projection result as the generated Projection reuses the projection result\n+\t\tBinaryRowData currentKey = groupKeyProjection.apply(input).copy();\n+\t\tcurrentKey.setRowKind(input.getRowKind());\n+\t\tif (lastGroupKey == null) {\n+\t\t\tlastGroupKey = currentKey;\n+\t\t\tlastGroupSet = groupSetProjection.apply(input).copy();\n+\t\t} else if (isNewKey(currentKey)) {\n+\t\t\tinvokeCurrentBatch();\n+\t\t\tlastGroupKey = currentKey;\n+\t\t\tlastGroupSet = groupSetProjection.apply(input).copy();\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tprotected void invokeCurrentBatch() throws Exception {\n+\t\twindowsGrouping.advanceWatermarkToTriggerAllWindows();\n+\t\ttriggerWindowProcess();\n+\t\twindowsGrouping.reset();\n+\t}\n+\n+\t@Override\n+\tpublic void processElementInternal(RowData value) throws Exception {\n+\t\twindowsGrouping.addInputToBuffer((BinaryRowData) value);\n+\t\ttriggerWindowProcess();\n+\t}\n+\n+\t@Override\n+\t@SuppressWarnings(\"ConstantConditions\")\n+\tpublic void emitResult(Tuple2<byte[], Integer> resultTuple) throws Exception {\n+\t\tbyte[] udafResult = resultTuple.f0;\n+\t\tint length = resultTuple.f1;\n+\t\tbais.setBuffer(udafResult, 0, length);\n+\t\tint rowCount = arrowSerializer.load();\n+\t\tfor (int i = 0; i < rowCount; i++) {\n+\t\t\tTuple2<RowData, TimeWindow> input = inputKeyAndWindow.poll();\n+\t\t\tRowData key = input.f0;\n+\t\t\tTimeWindow window = input.f1;\n+\t\t\tsetWindowProperty(window);\n+\t\t\twindowAggResult.replace(key, arrowSerializer.read(i));\n+\t\t\trowDataWrapper.collect(reuseJoinedRow.replace(windowAggResult, windowProperty));\n+\t\t}\n+\t}\n+\n+\tprivate void triggerWindowProcess() throws Exception {\n+\t\twhile (windowsGrouping.hasTriggerWindow()) {\n+\t\t\tRowIterator<BinaryRowData> elementIterator =\n+\t\t\t\twindowsGrouping.buildTriggerWindowElementsIterator();\n+\t\t\tTimeWindow currentWindow = windowsGrouping.getTriggerWindow();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4a182a499c3867765b6ef3c3957fbe4c63c2dce1"}, "originalPosition": 183}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDcxODQ4NA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             * Test for {@link BatchArrowPythonGroupWindowAggregateFunctionOperatorTest}. These test that:\n          \n          \n            \n             * Test for {@link BatchArrowPythonGroupWindowAggregateFunctionOperator}. These test that:", "url": "https://github.com/apache/flink/pull/13415#discussion_r490718484", "createdAt": "2020-09-18T05:56:25Z", "author": {"login": "dianfu"}, "path": "flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/aggregate/arrow/batch/BatchArrowPythonGroupWindowAggregateFunctionOperatorTest.java", "diffHunk": "@@ -0,0 +1,327 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.operators.python.aggregate.arrow.batch;\n+\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.python.PythonFunctionRunner;\n+import org.apache.flink.python.PythonOptions;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.streaming.util.OneInputStreamOperatorTestHarness;\n+import org.apache.flink.table.api.DataTypes;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.functions.python.PythonFunctionInfo;\n+import org.apache.flink.table.runtime.operators.python.aggregate.arrow.AbstractArrowPythonAggregateFunctionOperator;\n+import org.apache.flink.table.runtime.operators.python.aggregate.arrow.ArrowPythonAggregateFunctionOperatorTestBase;\n+import org.apache.flink.table.runtime.utils.PassThroughPythonAggregateFunctionRunner;\n+import org.apache.flink.table.runtime.utils.PythonTestUtils;\n+import org.apache.flink.table.types.logical.BigIntType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.TimestampType;\n+import org.apache.flink.table.types.logical.VarCharType;\n+\n+import org.junit.Test;\n+\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.concurrent.ConcurrentLinkedQueue;\n+\n+/**\n+ * Test for {@link BatchArrowPythonGroupWindowAggregateFunctionOperatorTest}. These test that:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4a182a499c3867765b6ef3c3957fbe4c63c2dce1"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDcxOTA3Nw==", "bodyText": "BatchArrowPythonGroupWindowAggregateFunctionOperator is running in batch, so I guess there is no checkpoint?", "url": "https://github.com/apache/flink/pull/13415#discussion_r490719077", "createdAt": "2020-09-18T05:58:21Z", "author": {"login": "dianfu"}, "path": "flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/aggregate/arrow/batch/BatchArrowPythonGroupWindowAggregateFunctionOperatorTest.java", "diffHunk": "@@ -0,0 +1,327 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.operators.python.aggregate.arrow.batch;\n+\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.python.PythonFunctionRunner;\n+import org.apache.flink.python.PythonOptions;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.streaming.util.OneInputStreamOperatorTestHarness;\n+import org.apache.flink.table.api.DataTypes;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.functions.python.PythonFunctionInfo;\n+import org.apache.flink.table.runtime.operators.python.aggregate.arrow.AbstractArrowPythonAggregateFunctionOperator;\n+import org.apache.flink.table.runtime.operators.python.aggregate.arrow.ArrowPythonAggregateFunctionOperatorTestBase;\n+import org.apache.flink.table.runtime.utils.PassThroughPythonAggregateFunctionRunner;\n+import org.apache.flink.table.runtime.utils.PythonTestUtils;\n+import org.apache.flink.table.types.logical.BigIntType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.TimestampType;\n+import org.apache.flink.table.types.logical.VarCharType;\n+\n+import org.junit.Test;\n+\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.concurrent.ConcurrentLinkedQueue;\n+\n+/**\n+ * Test for {@link BatchArrowPythonGroupWindowAggregateFunctionOperatorTest}. These test that:\n+ *\n+ * <ul>\n+ * <li>FinishBundle is called when checkpoint is encountered</li>\n+ * <li>Watermarks are buffered and only sent to downstream when finishedBundle is triggered</li>\n+ * </ul>\n+ */\n+public class BatchArrowPythonGroupWindowAggregateFunctionOperatorTest extends ArrowPythonAggregateFunctionOperatorTestBase {\n+\t@Test\n+\tpublic void testGroupAggregateFunction() throws Exception {\n+\t\tOneInputStreamOperatorTestHarness<RowData, RowData> testHarness = getTestHarness(\n+\t\t\tnew Configuration());\n+\t\tlong initialTime = 0L;\n+\t\tConcurrentLinkedQueue<Object> expectedOutput = new ConcurrentLinkedQueue<>();\n+\n+\t\ttestHarness.open();\n+\n+\t\ttestHarness.processElement(new StreamRecord<>(newBinaryRow(true, \"c1\", \"c2\", 0L, 0L), initialTime + 1));\n+\t\ttestHarness.processElement(new StreamRecord<>(newBinaryRow(true, \"c1\", \"c4\", 1L, 6000L), initialTime + 2));\n+\t\ttestHarness.processElement(new StreamRecord<>(newBinaryRow(true, \"c1\", \"c6\", 2L, 10000L), initialTime + 3));\n+\t\ttestHarness.processElement(new StreamRecord<>(newBinaryRow(true, \"c2\", \"c8\", 3L, 0L), initialTime + 3));\n+\n+\t\ttestHarness.close();\n+\n+\t\texpectedOutput.add(new StreamRecord<>(\n+\t\t\tnewRow(true, \"c1\", 0L, TimestampData.fromEpochMillis(-5000L), TimestampData.fromEpochMillis(5000L))));\n+\t\texpectedOutput.add(new StreamRecord<>(\n+\t\t\tnewRow(true, \"c1\", 0L, TimestampData.fromEpochMillis(0L), TimestampData.fromEpochMillis(10000L))));\n+\t\texpectedOutput.add(new StreamRecord<>(\n+\t\t\tnewRow(true, \"c1\", 1L, TimestampData.fromEpochMillis(5000L), TimestampData.fromEpochMillis(15000L))));\n+\t\texpectedOutput.add(new StreamRecord<>(\n+\t\t\tnewRow(true, \"c1\", 2L, TimestampData.fromEpochMillis(10000L), TimestampData.fromEpochMillis(20000L))));\n+\n+\t\texpectedOutput.add(new StreamRecord<>(\n+\t\t\tnewRow(true, \"c2\", 3L, TimestampData.fromEpochMillis(-5000L), TimestampData.fromEpochMillis(5000L))));\n+\t\texpectedOutput.add(new StreamRecord<>(\n+\t\t\tnewRow(true, \"c2\", 3L, TimestampData.fromEpochMillis(0L), TimestampData.fromEpochMillis(10000L))));\n+\n+\t\tassertOutputEquals(\"Output was not correct.\", expectedOutput, testHarness.getOutput());\n+\t}\n+\n+\t@Test\n+\tpublic void testFinishBundleTriggeredOnCheckpoint() throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4a182a499c3867765b6ef3c3957fbe4c63c2dce1"}, "originalPosition": 90}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDcxOTIzOA==", "bodyText": "BatchArrowPythonGroupWindowAggregateFunctionOperator is running in batch and so I guess we don't need to consider watermark?", "url": "https://github.com/apache/flink/pull/13415#discussion_r490719238", "createdAt": "2020-09-18T05:58:57Z", "author": {"login": "dianfu"}, "path": "flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/aggregate/arrow/batch/BatchArrowPythonGroupWindowAggregateFunctionOperatorTest.java", "diffHunk": "@@ -0,0 +1,327 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.operators.python.aggregate.arrow.batch;\n+\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.python.PythonFunctionRunner;\n+import org.apache.flink.python.PythonOptions;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.streaming.util.OneInputStreamOperatorTestHarness;\n+import org.apache.flink.table.api.DataTypes;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.functions.python.PythonFunctionInfo;\n+import org.apache.flink.table.runtime.operators.python.aggregate.arrow.AbstractArrowPythonAggregateFunctionOperator;\n+import org.apache.flink.table.runtime.operators.python.aggregate.arrow.ArrowPythonAggregateFunctionOperatorTestBase;\n+import org.apache.flink.table.runtime.utils.PassThroughPythonAggregateFunctionRunner;\n+import org.apache.flink.table.runtime.utils.PythonTestUtils;\n+import org.apache.flink.table.types.logical.BigIntType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.TimestampType;\n+import org.apache.flink.table.types.logical.VarCharType;\n+\n+import org.junit.Test;\n+\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.concurrent.ConcurrentLinkedQueue;\n+\n+/**\n+ * Test for {@link BatchArrowPythonGroupWindowAggregateFunctionOperatorTest}. These test that:\n+ *\n+ * <ul>\n+ * <li>FinishBundle is called when checkpoint is encountered</li>\n+ * <li>Watermarks are buffered and only sent to downstream when finishedBundle is triggered</li>\n+ * </ul>\n+ */\n+public class BatchArrowPythonGroupWindowAggregateFunctionOperatorTest extends ArrowPythonAggregateFunctionOperatorTestBase {\n+\t@Test\n+\tpublic void testGroupAggregateFunction() throws Exception {\n+\t\tOneInputStreamOperatorTestHarness<RowData, RowData> testHarness = getTestHarness(\n+\t\t\tnew Configuration());\n+\t\tlong initialTime = 0L;\n+\t\tConcurrentLinkedQueue<Object> expectedOutput = new ConcurrentLinkedQueue<>();\n+\n+\t\ttestHarness.open();\n+\n+\t\ttestHarness.processElement(new StreamRecord<>(newBinaryRow(true, \"c1\", \"c2\", 0L, 0L), initialTime + 1));\n+\t\ttestHarness.processElement(new StreamRecord<>(newBinaryRow(true, \"c1\", \"c4\", 1L, 6000L), initialTime + 2));\n+\t\ttestHarness.processElement(new StreamRecord<>(newBinaryRow(true, \"c1\", \"c6\", 2L, 10000L), initialTime + 3));\n+\t\ttestHarness.processElement(new StreamRecord<>(newBinaryRow(true, \"c2\", \"c8\", 3L, 0L), initialTime + 3));\n+\n+\t\ttestHarness.close();\n+\n+\t\texpectedOutput.add(new StreamRecord<>(\n+\t\t\tnewRow(true, \"c1\", 0L, TimestampData.fromEpochMillis(-5000L), TimestampData.fromEpochMillis(5000L))));\n+\t\texpectedOutput.add(new StreamRecord<>(\n+\t\t\tnewRow(true, \"c1\", 0L, TimestampData.fromEpochMillis(0L), TimestampData.fromEpochMillis(10000L))));\n+\t\texpectedOutput.add(new StreamRecord<>(\n+\t\t\tnewRow(true, \"c1\", 1L, TimestampData.fromEpochMillis(5000L), TimestampData.fromEpochMillis(15000L))));\n+\t\texpectedOutput.add(new StreamRecord<>(\n+\t\t\tnewRow(true, \"c1\", 2L, TimestampData.fromEpochMillis(10000L), TimestampData.fromEpochMillis(20000L))));\n+\n+\t\texpectedOutput.add(new StreamRecord<>(\n+\t\t\tnewRow(true, \"c2\", 3L, TimestampData.fromEpochMillis(-5000L), TimestampData.fromEpochMillis(5000L))));\n+\t\texpectedOutput.add(new StreamRecord<>(\n+\t\t\tnewRow(true, \"c2\", 3L, TimestampData.fromEpochMillis(0L), TimestampData.fromEpochMillis(10000L))));\n+\n+\t\tassertOutputEquals(\"Output was not correct.\", expectedOutput, testHarness.getOutput());\n+\t}\n+\n+\t@Test\n+\tpublic void testFinishBundleTriggeredOnCheckpoint() throws Exception {\n+\t\tConfiguration conf = new Configuration();\n+\t\tconf.setInteger(PythonOptions.MAX_BUNDLE_SIZE, 10);\n+\t\tOneInputStreamOperatorTestHarness<RowData, RowData> testHarness = getTestHarness(conf);\n+\n+\t\tlong initialTime = 0L;\n+\t\tConcurrentLinkedQueue<Object> expectedOutput = new ConcurrentLinkedQueue<>();\n+\n+\t\ttestHarness.open();\n+\n+\t\ttestHarness.processElement(new StreamRecord<>(newBinaryRow(true, \"c1\", \"c2\", 0L, 0L), initialTime + 1));\n+\t\ttestHarness.processElement(new StreamRecord<>(newBinaryRow(true, \"c1\", \"c4\", 1L, 6000L), initialTime + 2));\n+\t\ttestHarness.processElement(new StreamRecord<>(newBinaryRow(true, \"c1\", \"c6\", 2L, 10000L), initialTime + 3));\n+\t\ttestHarness.processElement(new StreamRecord<>(newBinaryRow(true, \"c2\", \"c8\", 3L, 0L), initialTime + 3));\n+\t\t// checkpoint trigger finishBundle\n+\t\ttestHarness.prepareSnapshotPreBarrier(0L);\n+\n+\t\texpectedOutput.add(new StreamRecord<>(\n+\t\t\tnewRow(true, \"c1\", 0L, TimestampData.fromEpochMillis(-5000L), TimestampData.fromEpochMillis(5000L))));\n+\t\texpectedOutput.add(new StreamRecord<>(\n+\t\t\tnewRow(true, \"c1\", 0L, TimestampData.fromEpochMillis(0L), TimestampData.fromEpochMillis(10000L))));\n+\t\texpectedOutput.add(new StreamRecord<>(\n+\t\t\tnewRow(true, \"c1\", 1L, TimestampData.fromEpochMillis(5000L), TimestampData.fromEpochMillis(15000L))));\n+\t\texpectedOutput.add(new StreamRecord<>(\n+\t\t\tnewRow(true, \"c1\", 2L, TimestampData.fromEpochMillis(10000L), TimestampData.fromEpochMillis(20000L))));\n+\n+\t\tassertOutputEquals(\"Output was not correct.\", expectedOutput, testHarness.getOutput());\n+\n+\t\ttestHarness.close();\n+\n+\t\texpectedOutput.add(new StreamRecord<>(\n+\t\t\tnewRow(true, \"c2\", 3L, TimestampData.fromEpochMillis(-5000L), TimestampData.fromEpochMillis(5000L))));\n+\t\texpectedOutput.add(new StreamRecord<>(\n+\t\t\tnewRow(true, \"c2\", 3L, TimestampData.fromEpochMillis(0L), TimestampData.fromEpochMillis(10000L))));\n+\n+\t\tassertOutputEquals(\"Output was not correct.\", expectedOutput, testHarness.getOutput());\n+\t}\n+\n+\t@Test\n+\tpublic void testFinishBundleTriggeredByCount() throws Exception {\n+\t\tConfiguration conf = new Configuration();\n+\t\tconf.setInteger(PythonOptions.MAX_BUNDLE_SIZE, 6);\n+\t\tOneInputStreamOperatorTestHarness<RowData, RowData> testHarness = getTestHarness(conf);\n+\n+\t\tlong initialTime = 0L;\n+\t\tConcurrentLinkedQueue<Object> expectedOutput = new ConcurrentLinkedQueue<>();\n+\n+\t\ttestHarness.open();\n+\n+\t\ttestHarness.processElement(new StreamRecord<>(newBinaryRow(true, \"c1\", \"c2\", 0L, 0L), initialTime + 1));\n+\t\ttestHarness.processElement(new StreamRecord<>(newBinaryRow(true, \"c1\", \"c4\", 1L, 6000L), initialTime + 2));\n+\t\ttestHarness.processElement(new StreamRecord<>(newBinaryRow(true, \"c1\", \"c6\", 2L, 10000L), initialTime + 3));\n+\n+\t\tassertOutputEquals(\"FinishBundle should not be triggered.\", expectedOutput, testHarness.getOutput());\n+\n+\t\ttestHarness.processElement(new StreamRecord<>(newBinaryRow(true, \"c2\", \"c8\", 3L, 0L), initialTime + 3));\n+\n+\t\texpectedOutput.add(new StreamRecord<>(\n+\t\t\tnewRow(true, \"c1\", 0L, TimestampData.fromEpochMillis(-5000L), TimestampData.fromEpochMillis(5000L))));\n+\t\texpectedOutput.add(new StreamRecord<>(\n+\t\t\tnewRow(true, \"c1\", 0L, TimestampData.fromEpochMillis(0L), TimestampData.fromEpochMillis(10000L))));\n+\t\texpectedOutput.add(new StreamRecord<>(\n+\t\t\tnewRow(true, \"c1\", 1L, TimestampData.fromEpochMillis(5000L), TimestampData.fromEpochMillis(15000L))));\n+\t\texpectedOutput.add(new StreamRecord<>(\n+\t\t\tnewRow(true, \"c1\", 2L, TimestampData.fromEpochMillis(10000L), TimestampData.fromEpochMillis(20000L))));\n+\n+\t\tassertOutputEquals(\"Output was not correct.\", expectedOutput, testHarness.getOutput());\n+\n+\t\ttestHarness.close();\n+\n+\t\texpectedOutput.add(new StreamRecord<>(\n+\t\t\tnewRow(true, \"c2\", 3L, TimestampData.fromEpochMillis(-5000L), TimestampData.fromEpochMillis(5000L))));\n+\t\texpectedOutput.add(new StreamRecord<>(\n+\t\t\tnewRow(true, \"c2\", 3L, TimestampData.fromEpochMillis(0L), TimestampData.fromEpochMillis(10000L))));\n+\n+\t\tassertOutputEquals(\"Output was not correct.\", expectedOutput, testHarness.getOutput());\n+\t}\n+\n+\t@Test\n+\tpublic void testFinishBundleTriggeredByTime() throws Exception {\n+\t\tConfiguration conf = new Configuration();\n+\t\tconf.setInteger(PythonOptions.MAX_BUNDLE_SIZE, 10);\n+\t\tconf.setLong(PythonOptions.MAX_BUNDLE_TIME_MILLS, 1000L);\n+\t\tOneInputStreamOperatorTestHarness<RowData, RowData> testHarness = getTestHarness(conf);\n+\n+\t\tlong initialTime = 0L;\n+\t\tConcurrentLinkedQueue<Object> expectedOutput = new ConcurrentLinkedQueue<>();\n+\n+\t\ttestHarness.open();\n+\n+\t\ttestHarness.processElement(new StreamRecord<>(newBinaryRow(true, \"c1\", \"c2\", 0L, 0L), initialTime + 1));\n+\t\ttestHarness.processElement(new StreamRecord<>(newBinaryRow(true, \"c1\", \"c4\", 1L, 6000L), initialTime + 2));\n+\t\ttestHarness.processElement(new StreamRecord<>(newBinaryRow(true, \"c1\", \"c6\", 2L, 10000L), initialTime + 3));\n+\t\ttestHarness.processElement(new StreamRecord<>(newBinaryRow(true, \"c2\", \"c8\", 3L, 0L), initialTime + 3));\n+\t\tassertOutputEquals(\"FinishBundle should not be triggered.\", expectedOutput, testHarness.getOutput());\n+\n+\t\ttestHarness.setProcessingTime(1000L);\n+\t\texpectedOutput.add(new StreamRecord<>(\n+\t\t\tnewRow(true, \"c1\", 0L, TimestampData.fromEpochMillis(-5000L), TimestampData.fromEpochMillis(5000L))));\n+\t\texpectedOutput.add(new StreamRecord<>(\n+\t\t\tnewRow(true, \"c1\", 0L, TimestampData.fromEpochMillis(0L), TimestampData.fromEpochMillis(10000L))));\n+\t\texpectedOutput.add(new StreamRecord<>(\n+\t\t\tnewRow(true, \"c1\", 1L, TimestampData.fromEpochMillis(5000L), TimestampData.fromEpochMillis(15000L))));\n+\t\texpectedOutput.add(new StreamRecord<>(\n+\t\t\tnewRow(true, \"c1\", 2L, TimestampData.fromEpochMillis(10000L), TimestampData.fromEpochMillis(20000L))));\n+\t\tassertOutputEquals(\"Output was not correct.\", expectedOutput, testHarness.getOutput());\n+\n+\t\ttestHarness.close();\n+\n+\t\texpectedOutput.add(new StreamRecord<>(\n+\t\t\tnewRow(true, \"c2\", 3L, TimestampData.fromEpochMillis(-5000L), TimestampData.fromEpochMillis(5000L))));\n+\t\texpectedOutput.add(new StreamRecord<>(\n+\t\t\tnewRow(true, \"c2\", 3L, TimestampData.fromEpochMillis(0L), TimestampData.fromEpochMillis(10000L))));\n+\n+\t\tassertOutputEquals(\"Output was not correct.\", expectedOutput, testHarness.getOutput());\n+\t}\n+\n+\t@Test\n+\tpublic void testWatermarkProcessedOnFinishBundle() throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4a182a499c3867765b6ef3c3957fbe4c63c2dce1"}, "originalPosition": 208}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "026263400c716dc9e84d56e93af2b83cc8fe0c06", "author": {"user": {"login": "HuangXingBo", "name": null}}, "url": "https://github.com/apache/flink/commit/026263400c716dc9e84d56e93af2b83cc8fe0c06", "committedDate": "2020-09-18T06:48:14Z", "message": "address comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDkxMjAzNTAx", "url": "https://github.com/apache/flink/pull/13415#pullrequestreview-491203501", "createdAt": "2020-09-18T07:09:41Z", "commit": {"oid": "026263400c716dc9e84d56e93af2b83cc8fe0c06"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4068, "cost": 1, "resetAt": "2021-10-28T16:48:13Z"}}}