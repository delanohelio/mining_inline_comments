{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDQzMjUxOTQy", "number": 12803, "title": "[FLINK-18461][table-planner-blink] Fix Changelog source can't be insert into upsert sink", "bodyText": "What is the purpose of the change\nFix Changelog source can't be insert into upsert sink. Currently, it will throw the following exception:\n[ERROR] Could not execute SQL statement. Reason:\norg.apache.flink.table.api.TableException: Provided trait [BEFORE_AND_AFTER] can't satisfy required trait [ONLY_UPDATE_AFTER]. This is a bug in planner, please file an issue.\nCurrent node is TableSourceScan(table=[[default_catalog, default_database, t_pick_order]], fields=[order_no, status])\n\nBrief change log\n\nIn FlinkChangelogModeInferenceProgram, we should catch the exception when satisfying UPDATE_KIND trait. Because we will may have multiple trait to required. If one of them can'be satisfied, we can fallback to the next one.\n\nVerifying this change\n\nAdded serveral IT cases to verifying reading from changelog source and insert into upsert sink.\n\nDoes this pull request potentially affect one of the following parts:\n\nDependencies (does it add or upgrade a dependency): (yes / no)\nThe public API, i.e., is any changed class annotated with @Public(Evolving): (yes / no)\nThe serializers: (yes / no / don't know)\nThe runtime per-record code paths (performance sensitive): (yes / no / don't know)\nAnything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / no / don't know)\nThe S3 file system connector: (yes / no / don't know)\n\nDocumentation\n\nDoes this pull request introduce a new feature? (yes / no)\nIf yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)", "createdAt": "2020-07-02T03:48:08Z", "url": "https://github.com/apache/flink/pull/12803", "merged": true, "mergeCommit": {"oid": "334f35cbd6da754d8b5b294032cd84c858b1f973"}, "closed": true, "closedAt": "2020-07-03T11:54:51Z", "author": {"login": "wuchong"}, "timelineItems": {"totalCount": 16, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcw2elyAH2gAyNDQzMjUxOTQyOjk1ZjhhYTZlMmRkZTA5OGI3NzI5OGRjYTEzYjczNmJiZTc0MmUwOWE=", "endCursor": "Y3Vyc29yOnYyOpPPAAABd8iIs1AFqTU5NTA1MTc2OQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "95f8aa6e2dde098b77298dca13b736bbe742e09a", "author": {"user": {"login": "wuchong", "name": "Jark Wu"}}, "url": "https://github.com/apache/flink/commit/95f8aa6e2dde098b77298dca13b736bbe742e09a", "committedDate": "2020-07-02T03:42:44Z", "message": "[FLINK-18461][table-planner-blink] Fix Changelog source can't be insert into upsert sink"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQxMzM0OTM1", "url": "https://github.com/apache/flink/pull/12803#pullrequestreview-441334935", "createdAt": "2020-07-02T04:02:47Z", "commit": {"oid": "95f8aa6e2dde098b77298dca13b736bbe742e09a"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQwNDowMjo0N1rOGr8puA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQwNDowMjo0N1rOGr8puA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODczNTY3Mg==", "bodyText": "Should add some comments to explain when/which node will throw exception?", "url": "https://github.com/apache/flink/pull/12803#discussion_r448735672", "createdAt": "2020-07-02T04:02:47Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/optimize/program/FlinkChangelogModeInferenceProgram.scala", "diffHunk": "@@ -66,15 +66,29 @@ class FlinkChangelogModeInferenceProgram extends FlinkOptimizeProgram[StreamOpti\n       // try ONLY_UPDATE_AFTER first, and then BEFORE_AND_AFTER\n       Seq(UpdateKindTrait.ONLY_UPDATE_AFTER, UpdateKindTrait.BEFORE_AND_AFTER)\n     }\n+    var throwable: Throwable = null\n     val finalRoot = requiredUpdateKindTraits.flatMap { requiredUpdateKindTrait =>\n-      SATISFY_UPDATE_KIND_TRAIT_VISITOR.visit(rootWithModifyKindSet, requiredUpdateKindTrait)\n+      try {\n+        SATISFY_UPDATE_KIND_TRAIT_VISITOR.visit(rootWithModifyKindSet, requiredUpdateKindTrait)\n+      } catch {\n+        case t: Throwable =>\n+          // cache exception and return None to", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "95f8aa6e2dde098b77298dca13b736bbe742e09a"}, "originalPosition": 11}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQxMzM4NjAw", "url": "https://github.com/apache/flink/pull/12803#pullrequestreview-441338600", "createdAt": "2020-07-02T04:15:37Z", "commit": {"oid": "95f8aa6e2dde098b77298dca13b736bbe742e09a"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQwNDoxNTozOFrOGr82Mw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQwNDoxNTozOFrOGr82Mw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODczODg2Nw==", "bodyText": "Not covered by tests?", "url": "https://github.com/apache/flink/pull/12803#discussion_r448738867", "createdAt": "2020-07-02T04:15:38Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/optimize/program/FlinkChangelogModeInferenceProgram.scala", "diffHunk": "@@ -66,15 +66,29 @@ class FlinkChangelogModeInferenceProgram extends FlinkOptimizeProgram[StreamOpti\n       // try ONLY_UPDATE_AFTER first, and then BEFORE_AND_AFTER\n       Seq(UpdateKindTrait.ONLY_UPDATE_AFTER, UpdateKindTrait.BEFORE_AND_AFTER)\n     }\n+    var throwable: Throwable = null\n     val finalRoot = requiredUpdateKindTraits.flatMap { requiredUpdateKindTrait =>\n-      SATISFY_UPDATE_KIND_TRAIT_VISITOR.visit(rootWithModifyKindSet, requiredUpdateKindTrait)\n+      try {\n+        SATISFY_UPDATE_KIND_TRAIT_VISITOR.visit(rootWithModifyKindSet, requiredUpdateKindTrait)\n+      } catch {\n+        case t: Throwable =>\n+          // cache exception and return None to", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "95f8aa6e2dde098b77298dca13b736bbe742e09a"}, "originalPosition": 11}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQxMzM4NzQw", "url": "https://github.com/apache/flink/pull/12803#pullrequestreview-441338740", "createdAt": "2020-07-02T04:16:16Z", "commit": {"oid": "95f8aa6e2dde098b77298dca13b736bbe742e09a"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQwNDoxNjoxNlrOGr82qQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQwNDoxNjoxNlrOGr82qQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODczODk4NQ==", "bodyText": "Can we test this branch?", "url": "https://github.com/apache/flink/pull/12803#discussion_r448738985", "createdAt": "2020-07-02T04:16:16Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/optimize/program/FlinkChangelogModeInferenceProgram.scala", "diffHunk": "@@ -66,15 +66,29 @@ class FlinkChangelogModeInferenceProgram extends FlinkOptimizeProgram[StreamOpti\n       // try ONLY_UPDATE_AFTER first, and then BEFORE_AND_AFTER\n       Seq(UpdateKindTrait.ONLY_UPDATE_AFTER, UpdateKindTrait.BEFORE_AND_AFTER)\n     }\n+    var throwable: Throwable = null\n     val finalRoot = requiredUpdateKindTraits.flatMap { requiredUpdateKindTrait =>\n-      SATISFY_UPDATE_KIND_TRAIT_VISITOR.visit(rootWithModifyKindSet, requiredUpdateKindTrait)\n+      try {\n+        SATISFY_UPDATE_KIND_TRAIT_VISITOR.visit(rootWithModifyKindSet, requiredUpdateKindTrait)\n+      } catch {\n+        case t: Throwable =>\n+          // cache exception and return None to\n+          throwable = t\n+          None\n+      }\n     }\n \n     // step3: sanity check and return non-empty root\n     if (finalRoot.isEmpty) {\n       val plan = FlinkRelOptUtil.toString(root, withChangelogTraits = true)\n-      throw new TableException(\n-        \"Can't generate a valid execution plan for the given query:\\n\" + plan)\n+      if (throwable == null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "95f8aa6e2dde098b77298dca13b736bbe742e09a"}, "originalPosition": 22}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQxMzM5NDg5", "url": "https://github.com/apache/flink/pull/12803#pullrequestreview-441339489", "createdAt": "2020-07-02T04:18:48Z", "commit": {"oid": "95f8aa6e2dde098b77298dca13b736bbe742e09a"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQwNDoxODo0OFrOGr85Mg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQwNDoxODo0OFrOGr85Mg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODczOTYzNA==", "bodyText": "case t to flatMap { t => should change to a suitable name instead of t.", "url": "https://github.com/apache/flink/pull/12803#discussion_r448739634", "createdAt": "2020-07-02T04:18:48Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/optimize/program/FlinkChangelogModeInferenceProgram.scala", "diffHunk": "@@ -687,9 +701,24 @@ class FlinkChangelogModeInferenceProgram extends FlinkOptimizeProgram[StreamOpti\n     private def visitSink(\n         sink: StreamPhysicalRel,\n         sinkRequiredTraits: Seq[UpdateKindTrait]): Option[StreamPhysicalRel] = {\n-      val children = sinkRequiredTraits.flatMap(t => visitChildren(sink, t))\n+      var throwable: Throwable = null\n+      val children = sinkRequiredTraits.flatMap { t =>\n+        try {\n+          visitChildren(sink, t)\n+        } catch {\n+          case t: Throwable =>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "95f8aa6e2dde098b77298dca13b736bbe742e09a"}, "originalPosition": 43}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQxMzM5Nzc2", "url": "https://github.com/apache/flink/pull/12803#pullrequestreview-441339776", "createdAt": "2020-07-02T04:20:01Z", "commit": {"oid": "95f8aa6e2dde098b77298dca13b736bbe742e09a"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQwNDoyMDowMVrOGr86Gw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQwNDoyMDowMVrOGr86Gw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODczOTg2Nw==", "bodyText": "Can we change the return type of visitChildren? Instead of catch exception?", "url": "https://github.com/apache/flink/pull/12803#discussion_r448739867", "createdAt": "2020-07-02T04:20:01Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/optimize/program/FlinkChangelogModeInferenceProgram.scala", "diffHunk": "@@ -687,9 +701,24 @@ class FlinkChangelogModeInferenceProgram extends FlinkOptimizeProgram[StreamOpti\n     private def visitSink(\n         sink: StreamPhysicalRel,\n         sinkRequiredTraits: Seq[UpdateKindTrait]): Option[StreamPhysicalRel] = {\n-      val children = sinkRequiredTraits.flatMap(t => visitChildren(sink, t))\n+      var throwable: Throwable = null\n+      val children = sinkRequiredTraits.flatMap { t =>\n+        try {\n+          visitChildren(sink, t)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "95f8aa6e2dde098b77298dca13b736bbe742e09a"}, "originalPosition": 41}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0198d4276d4971f31ec872e185d0261310e3396b", "author": {"user": {"login": "wuchong", "name": "Jark Wu"}}, "url": "https://github.com/apache/flink/commit/0198d4276d4971f31ec872e185d0261310e3396b", "committedDate": "2020-07-02T07:40:54Z", "message": "address comments."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQxNDI5MDA1", "url": "https://github.com/apache/flink/pull/12803#pullrequestreview-441429005", "createdAt": "2020-07-02T07:46:10Z", "commit": {"oid": "0198d4276d4971f31ec872e185d0261310e3396b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQwNzo0NjoxMVrOGsBRAQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQwNzo0NjoxMVrOGsBRAQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODgxMTI2NQ==", "bodyText": "return None?", "url": "https://github.com/apache/flink/pull/12803#discussion_r448811265", "createdAt": "2020-07-02T07:46:11Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/optimize/program/FlinkChangelogModeInferenceProgram.scala", "diffHunk": "@@ -618,12 +616,10 @@ class FlinkChangelogModeInferenceProgram extends FlinkOptimizeProgram[StreamOpti\n             return None\n           case Some(newChild) =>\n             val providedTrait = newChild.getTraitSet.getTrait(UpdateKindTraitDef.INSTANCE)\n-            val childDescription = newChild.getRelDetailedDescription\n             if (!providedTrait.satisfies(requiredChildrenTrait)) {\n-              throw new TableException(s\"Provided trait $providedTrait can't satisfy \" +\n-                s\"required trait $requiredChildrenTrait. \" +\n-                s\"This is a bug in planner, please file an issue. \\n\" +\n-                s\"Current node is $childDescription\")\n+              // the provided trait can't satisfy required trait, thus we should return None.\n+              // for example, the changelog source can't provide ONLY_UPDATE_AFTER.\n+              None", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0198d4276d4971f31ec872e185d0261310e3396b"}, "originalPosition": 25}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "250275b97b69730087e23586a56ae92bca71b7f6", "author": {"user": {"login": "wuchong", "name": "Jark Wu"}}, "url": "https://github.com/apache/flink/commit/250275b97b69730087e23586a56ae92bca71b7f6", "committedDate": "2020-07-02T08:12:14Z", "message": "address comment"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQxNDYzMTUx", "url": "https://github.com/apache/flink/pull/12803#pullrequestreview-441463151", "createdAt": "2020-07-02T08:32:03Z", "commit": {"oid": "250275b97b69730087e23586a56ae92bca71b7f6"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1179adabba184f1203465cd89bbbb055b8fd9c04", "author": {"user": {"login": "wuchong", "name": "Jark Wu"}}, "url": "https://github.com/apache/flink/commit/1179adabba184f1203465cd89bbbb055b8fd9c04", "committedDate": "2020-07-02T17:35:24Z", "message": "fix jdbc bug"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "533492d8b403e3532079bba154f960f8dd66df31", "author": {"user": {"login": "wuchong", "name": "Jark Wu"}}, "url": "https://github.com/apache/flink/commit/533492d8b403e3532079bba154f960f8dd66df31", "committedDate": "2020-07-03T04:44:53Z", "message": "fix tests"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQyMTc5Mzcy", "url": "https://github.com/apache/flink/pull/12803#pullrequestreview-442179372", "createdAt": "2020-07-03T06:41:46Z", "commit": {"oid": "1179adabba184f1203465cd89bbbb055b8fd9c04"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QwNjo0NTo1NVrOGslgHA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QwNjo1MjoyNVrOGslpnw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTQwNDk1Ng==", "bodyText": "We can omit  this local variable?\nreduceBuffer.put(keyExtractor.apply(record),\n                 Tuple2.of(changeFlag(record.getRowKind()), recordExtractor.apply(record)));", "url": "https://github.com/apache/flink/pull/12803#discussion_r449404956", "createdAt": "2020-07-03T06:45:55Z", "author": {"login": "leonardBang"}, "path": "flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/executor/BufferReduceStatementExecutor.java", "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connector.jdbc.internal.executor;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.types.RowKind;\n+\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.function.Function;\n+\n+/**\n+ * Currently, this statement executor is only used for table/sql to buffer insert/update/delete\n+ * events, and reduce them in buffer before submit to external database.\n+ */\n+public class BufferReduceStatementExecutor implements JdbcBatchStatementExecutor<RowData> {\n+\n+\tprivate final JdbcBatchStatementExecutor<RowData> upsertExecutor;\n+\tprivate final JdbcBatchStatementExecutor<RowData> deleteExecutor;\n+\tprivate final Function<RowData, RowData> keyExtractor;\n+\tprivate final Function<RowData, RowData> valueTransform;\n+\t// the mapping is [KEY, <+/-, VALUE>]\n+\tprivate final Map<RowData, Tuple2<Boolean, RowData>> reduceBuffer = new HashMap<>();\n+\n+\tpublic BufferReduceStatementExecutor(\n+\t\t\tJdbcBatchStatementExecutor<RowData> upsertExecutor,\n+\t\t\tJdbcBatchStatementExecutor<RowData> deleteExecutor,\n+\t\t\tFunction<RowData, RowData> keyExtractor,\n+\t\t\tFunction<RowData, RowData> valueTransform) {\n+\t\tthis.upsertExecutor = upsertExecutor;\n+\t\tthis.deleteExecutor = deleteExecutor;\n+\t\tthis.keyExtractor = keyExtractor;\n+\t\tthis.valueTransform = valueTransform;\n+\t}\n+\n+\t@Override\n+\tpublic void prepareStatements(Connection connection) throws SQLException {\n+\t\tupsertExecutor.prepareStatements(connection);\n+\t\tdeleteExecutor.prepareStatements(connection);\n+\t}\n+\n+\t@Override\n+\tpublic void addToBatch(RowData record) throws SQLException {\n+\t\tRowData key = keyExtractor.apply(record);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1179adabba184f1203465cd89bbbb055b8fd9c04"}, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTQwNTQzNA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \t * Returns true if it is INSERT or UPDATE_AFTER, returns false if it is DELETE or UPDATE_BEFORE.\n          \n          \n            \n            \t * Returns true if the row kind is INSERT or UPDATE_AFTER, returns false if the row kind is DELETE or UPDATE_BEFORE.", "url": "https://github.com/apache/flink/pull/12803#discussion_r449405434", "createdAt": "2020-07-03T06:47:09Z", "author": {"login": "leonardBang"}, "path": "flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/executor/BufferReduceStatementExecutor.java", "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connector.jdbc.internal.executor;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.types.RowKind;\n+\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.function.Function;\n+\n+/**\n+ * Currently, this statement executor is only used for table/sql to buffer insert/update/delete\n+ * events, and reduce them in buffer before submit to external database.\n+ */\n+public class BufferReduceStatementExecutor implements JdbcBatchStatementExecutor<RowData> {\n+\n+\tprivate final JdbcBatchStatementExecutor<RowData> upsertExecutor;\n+\tprivate final JdbcBatchStatementExecutor<RowData> deleteExecutor;\n+\tprivate final Function<RowData, RowData> keyExtractor;\n+\tprivate final Function<RowData, RowData> valueTransform;\n+\t// the mapping is [KEY, <+/-, VALUE>]\n+\tprivate final Map<RowData, Tuple2<Boolean, RowData>> reduceBuffer = new HashMap<>();\n+\n+\tpublic BufferReduceStatementExecutor(\n+\t\t\tJdbcBatchStatementExecutor<RowData> upsertExecutor,\n+\t\t\tJdbcBatchStatementExecutor<RowData> deleteExecutor,\n+\t\t\tFunction<RowData, RowData> keyExtractor,\n+\t\t\tFunction<RowData, RowData> valueTransform) {\n+\t\tthis.upsertExecutor = upsertExecutor;\n+\t\tthis.deleteExecutor = deleteExecutor;\n+\t\tthis.keyExtractor = keyExtractor;\n+\t\tthis.valueTransform = valueTransform;\n+\t}\n+\n+\t@Override\n+\tpublic void prepareStatements(Connection connection) throws SQLException {\n+\t\tupsertExecutor.prepareStatements(connection);\n+\t\tdeleteExecutor.prepareStatements(connection);\n+\t}\n+\n+\t@Override\n+\tpublic void addToBatch(RowData record) throws SQLException {\n+\t\tRowData key = keyExtractor.apply(record);\n+\t\treduceBuffer.put(key, Tuple2.of(changeFlag(record.getRowKind()), valueTransform.apply(record)));\n+\t}\n+\n+\t/**\n+\t * Returns true if it is INSERT or UPDATE_AFTER, returns false if it is DELETE or UPDATE_BEFORE.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1179adabba184f1203465cd89bbbb055b8fd9c04"}, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTQwNzM5MQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \t * Creates a {@link JdbcStatementBuilder} for {@link Row} using the provided SQL types array.\n          \n          \n            \n            \t * Create a {@link JdbcStatementBuilder} for {@link RowData} using the provided SQL types array.", "url": "https://github.com/apache/flink/pull/12803#discussion_r449407391", "createdAt": "2020-07-03T06:52:25Z", "author": {"login": "leonardBang"}, "path": "flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcDynamicOutputFormatBuilder.java", "diffHunk": "@@ -0,0 +1,221 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connector.jdbc.table;\n+\n+import org.apache.flink.api.common.functions.RuntimeContext;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.connector.jdbc.JdbcExecutionOptions;\n+import org.apache.flink.connector.jdbc.JdbcStatementBuilder;\n+import org.apache.flink.connector.jdbc.dialect.JdbcDialect;\n+import org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat;\n+import org.apache.flink.connector.jdbc.internal.connection.SimpleJdbcConnectionProvider;\n+import org.apache.flink.connector.jdbc.internal.converter.JdbcRowConverter;\n+import org.apache.flink.connector.jdbc.internal.executor.BufferReduceStatementExecutor;\n+import org.apache.flink.connector.jdbc.internal.executor.InsertOrUpdateJdbcExecutor;\n+import org.apache.flink.connector.jdbc.internal.executor.JdbcBatchStatementExecutor;\n+import org.apache.flink.connector.jdbc.internal.options.JdbcDmlOptions;\n+import org.apache.flink.connector.jdbc.internal.options.JdbcOptions;\n+import org.apache.flink.connector.jdbc.utils.JdbcUtils;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.types.Row;\n+\n+import java.io.Serializable;\n+import java.util.Arrays;\n+import java.util.function.Function;\n+\n+import static org.apache.flink.table.data.RowData.createFieldGetter;\n+import static org.apache.flink.util.Preconditions.checkArgument;\n+import static org.apache.flink.util.Preconditions.checkNotNull;\n+\n+/**\n+ * Builder for {@link JdbcBatchingOutputFormat} for Table/SQL.\n+ */\n+public class JdbcDynamicOutputFormatBuilder implements Serializable {\n+\n+\tprivate JdbcOptions jdbcOptions;\n+\tprivate JdbcExecutionOptions executionOptions;\n+\tprivate JdbcDmlOptions dmlOptions;\n+\tprivate TypeInformation<RowData> rowDataTypeInformation;\n+\tprivate DataType[] fieldDataTypes;\n+\n+\tpublic JdbcDynamicOutputFormatBuilder() {\n+\t}\n+\n+\tpublic JdbcDynamicOutputFormatBuilder setJdbcOptions(JdbcOptions jdbcOptions) {\n+\t\tthis.jdbcOptions = jdbcOptions;\n+\t\treturn this;\n+\t}\n+\n+\tpublic JdbcDynamicOutputFormatBuilder setJdbcExecutionOptions(JdbcExecutionOptions executionOptions) {\n+\t\tthis.executionOptions = executionOptions;\n+\t\treturn this;\n+\t}\n+\n+\tpublic JdbcDynamicOutputFormatBuilder setJdbcDmlOptions(JdbcDmlOptions dmlOptions) {\n+\t\tthis.dmlOptions = dmlOptions;\n+\t\treturn this;\n+\t}\n+\n+\tpublic JdbcDynamicOutputFormatBuilder setRowDataTypeInfo(TypeInformation<RowData> rowDataTypeInfo) {\n+\t\tthis.rowDataTypeInformation = rowDataTypeInfo;\n+\t\treturn this;\n+\t}\n+\n+\tpublic JdbcDynamicOutputFormatBuilder setFieldDataTypes(DataType[] fieldDataTypes) {\n+\t\tthis.fieldDataTypes = fieldDataTypes;\n+\t\treturn this;\n+\t}\n+\n+\tpublic JdbcBatchingOutputFormat<RowData, ?, ?> build() {\n+\t\tcheckNotNull(jdbcOptions, \"jdbc options can not be null\");\n+\t\tcheckNotNull(dmlOptions, \"jdbc dml options can not be null\");\n+\t\tcheckNotNull(executionOptions, \"jdbc execution options can not be null\");\n+\n+\t\tfinal LogicalType[] logicalTypes = Arrays.stream(fieldDataTypes)\n+\t\t\t.map(DataType::getLogicalType)\n+\t\t\t.toArray(LogicalType[]::new);\n+\t\tif (dmlOptions.getKeyFields().isPresent() && dmlOptions.getKeyFields().get().length > 0) {\n+\t\t\t//upsert query\n+\t\t\treturn new JdbcBatchingOutputFormat<>(\n+\t\t\t\tnew SimpleJdbcConnectionProvider(jdbcOptions),\n+\t\t\t\texecutionOptions,\n+\t\t\t\tctx -> createBufferReduceExecutor(dmlOptions, ctx, rowDataTypeInformation, logicalTypes),\n+\t\t\t\tJdbcBatchingOutputFormat.RecordExtractor.identity());\n+\t\t} else {\n+\t\t\t// append only query\n+\t\t\tfinal String sql = dmlOptions\n+\t\t\t\t.getDialect()\n+\t\t\t\t.getInsertIntoStatement(dmlOptions.getTableName(), dmlOptions.getFieldNames());\n+\t\t\treturn new JdbcBatchingOutputFormat<>(\n+\t\t\t\tnew SimpleJdbcConnectionProvider(jdbcOptions),\n+\t\t\t\texecutionOptions,\n+\t\t\t\tctx -> createSimpleRowDataExecutor(dmlOptions.getDialect(), sql, logicalTypes, ctx, rowDataTypeInformation),\n+\t\t\t\tJdbcBatchingOutputFormat.RecordExtractor.identity());\n+\t\t}\n+\t}\n+\n+\tprivate static JdbcBatchStatementExecutor<RowData> createKeyedRowExecutor(\n+\t\tJdbcDialect dialect,\n+\t\tint[] pkFields,\n+\t\tLogicalType[] pkTypes,\n+\t\tString sql,\n+\t\tLogicalType[] logicalTypes) {\n+\t\tfinal JdbcRowConverter rowConverter = dialect.getRowConverter(RowType.of(pkTypes));\n+\t\tfinal Function<RowData, RowData> keyExtractor = createRowKeyExtractor(logicalTypes, pkFields);\n+\t\treturn JdbcBatchStatementExecutor.keyed(\n+\t\t\tsql,\n+\t\t\tkeyExtractor,\n+\t\t\t(st, record) -> rowConverter.toExternal(keyExtractor.apply(record), st));\n+\t}\n+\n+\tprivate static JdbcBatchStatementExecutor<RowData> createBufferReduceExecutor(\n+\t\tJdbcDmlOptions opt,\n+\t\tRuntimeContext ctx,\n+\t\tTypeInformation<RowData> rowDataTypeInfo,\n+\t\tLogicalType[] fieldTypes) {\n+\t\tcheckArgument(opt.getKeyFields().isPresent());\n+\t\tint[] pkFields = Arrays.stream(opt.getKeyFields().get()).mapToInt(Arrays.asList(opt.getFieldNames())::indexOf).toArray();\n+\t\tLogicalType[] pkTypes = Arrays.stream(pkFields).mapToObj(f -> fieldTypes[f]).toArray(LogicalType[]::new);\n+\t\tfinal TypeSerializer<RowData> typeSerializer = rowDataTypeInfo.createSerializer(ctx.getExecutionConfig());\n+\t\tfinal Function<RowData, RowData> valueTransform = ctx.getExecutionConfig().isObjectReuseEnabled() ? typeSerializer::copy : Function.identity();\n+\n+\t\tJdbcBatchStatementExecutor<RowData> upsertExecutor = createUpsertRowExecutor(opt, ctx, rowDataTypeInfo, pkFields, pkTypes, fieldTypes, valueTransform);\n+\t\tJdbcBatchStatementExecutor<RowData> deleteExecutor = createDeleteExecutor(opt, pkFields, pkTypes, fieldTypes);\n+\n+\t\treturn new BufferReduceStatementExecutor(\n+\t\t\tupsertExecutor,\n+\t\t\tdeleteExecutor,\n+\t\t\tcreateRowKeyExtractor(fieldTypes, pkFields),\n+\t\t\tvalueTransform);\n+\t}\n+\n+\tprivate static JdbcBatchStatementExecutor<RowData> createUpsertRowExecutor(\n+\t\tJdbcDmlOptions opt,\n+\t\tRuntimeContext ctx,\n+\t\tTypeInformation<RowData> rowDataTypeInfo,\n+\t\tint[] pkFields,\n+\t\tLogicalType[] pkTypes,\n+\t\tLogicalType[] fieldTypes,\n+\t\tFunction<RowData, RowData> valueTransform) {\n+\t\tcheckArgument(opt.getKeyFields().isPresent());\n+\t\tJdbcDialect dialect = opt.getDialect();\n+\t\treturn opt.getDialect()\n+\t\t\t.getUpsertStatement(opt.getTableName(), opt.getFieldNames(), opt.getKeyFields().get())\n+\t\t\t.map(sql -> createSimpleRowDataExecutor(dialect, sql, fieldTypes, ctx, rowDataTypeInfo))\n+\t\t\t.orElseGet(() ->\n+\t\t\t\tnew InsertOrUpdateJdbcExecutor<>(\n+\t\t\t\t\topt.getDialect().getRowExistsStatement(opt.getTableName(), opt.getKeyFields().get()),\n+\t\t\t\t\topt.getDialect().getInsertIntoStatement(opt.getTableName(), opt.getFieldNames()),\n+\t\t\t\t\topt.getDialect().getUpdateStatement(opt.getTableName(), opt.getFieldNames(), opt.getKeyFields().get()),\n+\t\t\t\t\tcreateRowDataJdbcStatementBuilder(dialect, pkTypes),\n+\t\t\t\t\tcreateRowDataJdbcStatementBuilder(dialect, fieldTypes),\n+\t\t\t\t\tcreateRowDataJdbcStatementBuilder(dialect, fieldTypes),\n+\t\t\t\t\tcreateRowKeyExtractor(fieldTypes, pkFields),\n+\t\t\t\t\tvalueTransform));\n+\t}\n+\n+\tprivate static JdbcBatchStatementExecutor<RowData> createDeleteExecutor(\n+\t\tJdbcDmlOptions dmlOptions,\n+\t\tint[] pkFields,\n+\t\tLogicalType[] pkTypes,\n+\t\tLogicalType[] fieldTypes) {\n+\t\tcheckArgument(dmlOptions.getKeyFields().isPresent());\n+\t\tString[] pkNames = Arrays.stream(pkFields).mapToObj(k -> dmlOptions.getFieldNames()[k]).toArray(String[]::new);\n+\t\tString deleteSql = dmlOptions.getDialect().getDeleteStatement(dmlOptions.getTableName(), pkNames);\n+\t\treturn createKeyedRowExecutor(dmlOptions.getDialect(), pkFields, pkTypes, deleteSql, fieldTypes);\n+\t}\n+\n+\tprivate static Function<RowData, RowData> createRowKeyExtractor(LogicalType[] logicalTypes, int[] pkFields) {\n+\t\tfinal RowData.FieldGetter[] fieldGetters = new RowData.FieldGetter[pkFields.length];\n+\t\tfor (int i = 0; i < pkFields.length; i++) {\n+\t\t\tfieldGetters[i] = createFieldGetter(logicalTypes[pkFields[i]], pkFields[i]);\n+\t\t}\n+\t\treturn row -> getPrimaryKey(row, fieldGetters);\n+\t}\n+\n+\tprivate static JdbcBatchStatementExecutor<RowData> createSimpleRowDataExecutor(JdbcDialect dialect, String sql, LogicalType[] fieldTypes, RuntimeContext ctx, TypeInformation<RowData> rowDataTypeInfo) {\n+\t\tfinal TypeSerializer<RowData> typeSerializer = rowDataTypeInfo.createSerializer(ctx.getExecutionConfig());\n+\t\treturn JdbcBatchStatementExecutor.simple(\n+\t\t\tsql,\n+\t\t\tcreateRowDataJdbcStatementBuilder(dialect, fieldTypes),\n+\t\t\tctx.getExecutionConfig().isObjectReuseEnabled() ? typeSerializer::copy : Function.identity());\n+\t}\n+\n+\t/**\n+\t * Creates a {@link JdbcStatementBuilder} for {@link Row} using the provided SQL types array.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1179adabba184f1203465cd89bbbb055b8fd9c04"}, "originalPosition": 206}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3b212fec4e0c4453bc6da26180643487c01071d0", "author": {"user": {"login": "wuchong", "name": "Jark Wu"}}, "url": "https://github.com/apache/flink/commit/3b212fec4e0c4453bc6da26180643487c01071d0", "committedDate": "2020-07-03T07:20:11Z", "message": "address review comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQyMjE2Mzcx", "url": "https://github.com/apache/flink/pull/12803#pullrequestreview-442216371", "createdAt": "2020-07-03T07:52:06Z", "commit": {"oid": "3b212fec4e0c4453bc6da26180643487c01071d0"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTk1MDUxNzY5", "url": "https://github.com/apache/flink/pull/12803#pullrequestreview-595051769", "createdAt": "2021-02-22T06:58:38Z", "commit": {"oid": "3b212fec4e0c4453bc6da26180643487c01071d0"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yMlQwNjo1ODozOFrOIpJmfw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yMlQwNjo1ODozOFrOIpJmfw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4MDAxOTgzOQ==", "bodyText": "Hi @wuchong , may I know the reason why \"deleteExecutor.executeBatch()\" is placed after \"upsertExecutor.executeBatch()\"? I have found that the deleteExecutor would accidentally delete previously updated records, resulting in loss of data.", "url": "https://github.com/apache/flink/pull/12803#discussion_r580019839", "createdAt": "2021-02-22T06:58:38Z", "author": {"login": "kylemeow"}, "path": "flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/executor/BufferReduceStatementExecutor.java", "diffHunk": "@@ -0,0 +1,108 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connector.jdbc.internal.executor;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.types.RowKind;\n+\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.function.Function;\n+\n+/**\n+ * Currently, this statement executor is only used for table/sql to buffer insert/update/delete\n+ * events, and reduce them in buffer before submit to external database.\n+ */\n+public class BufferReduceStatementExecutor implements JdbcBatchStatementExecutor<RowData> {\n+\n+\tprivate final JdbcBatchStatementExecutor<RowData> upsertExecutor;\n+\tprivate final JdbcBatchStatementExecutor<RowData> deleteExecutor;\n+\tprivate final Function<RowData, RowData> keyExtractor;\n+\tprivate final Function<RowData, RowData> valueTransform;\n+\t// the mapping is [KEY, <+/-, VALUE>]\n+\tprivate final Map<RowData, Tuple2<Boolean, RowData>> reduceBuffer = new HashMap<>();\n+\n+\tpublic BufferReduceStatementExecutor(\n+\t\t\tJdbcBatchStatementExecutor<RowData> upsertExecutor,\n+\t\t\tJdbcBatchStatementExecutor<RowData> deleteExecutor,\n+\t\t\tFunction<RowData, RowData> keyExtractor,\n+\t\t\tFunction<RowData, RowData> valueTransform) {\n+\t\tthis.upsertExecutor = upsertExecutor;\n+\t\tthis.deleteExecutor = deleteExecutor;\n+\t\tthis.keyExtractor = keyExtractor;\n+\t\tthis.valueTransform = valueTransform;\n+\t}\n+\n+\t@Override\n+\tpublic void prepareStatements(Connection connection) throws SQLException {\n+\t\tupsertExecutor.prepareStatements(connection);\n+\t\tdeleteExecutor.prepareStatements(connection);\n+\t}\n+\n+\t@Override\n+\tpublic void addToBatch(RowData record) throws SQLException {\n+\t\tRowData key = keyExtractor.apply(record);\n+\t\tboolean flag = changeFlag(record.getRowKind());\n+\t\tRowData value = valueTransform.apply(record); // copy or not\n+\t\treduceBuffer.put(key, Tuple2.of(flag, value));\n+\t}\n+\n+\t/**\n+\t * Returns true if the row kind is INSERT or UPDATE_AFTER,\n+\t * returns false if the row kind is DELETE or UPDATE_BEFORE.\n+\t */\n+\tprivate boolean changeFlag(RowKind rowKind) {\n+\t\tswitch (rowKind) {\n+\t\t\tcase INSERT:\n+\t\t\tcase UPDATE_AFTER:\n+\t\t\t\treturn true;\n+\t\t\tcase DELETE:\n+\t\t\tcase UPDATE_BEFORE:\n+\t\t\t\treturn false;\n+\t\t\tdefault:\n+\t\t\t\tthrow new UnsupportedOperationException(\n+\t\t\t\t\tString.format(\"Unknown row kind, the supported row kinds is: INSERT, UPDATE_BEFORE, UPDATE_AFTER,\" +\n+\t\t\t\t\t\t\" DELETE, but get: %s.\", rowKind));\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void executeBatch() throws SQLException {\n+\t\t// TODO: reuse the extracted key and avoid value copy in the future\n+\t\tfor (Tuple2<Boolean, RowData> tuple : reduceBuffer.values()) {\n+\t\t\tif (tuple.f0) {\n+\t\t\t\tupsertExecutor.addToBatch(tuple.f1);\n+\t\t\t} else {\n+\t\t\t\tdeleteExecutor.addToBatch(tuple.f1);\n+\t\t\t}\n+\t\t}\n+\t\tupsertExecutor.executeBatch();\n+\t\tdeleteExecutor.executeBatch();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3b212fec4e0c4453bc6da26180643487c01071d0"}, "originalPosition": 99}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3348, "cost": 1, "resetAt": "2021-10-28T16:48:13Z"}}}