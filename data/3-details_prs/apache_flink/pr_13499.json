{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDkzOTU3NjUz", "number": 13499, "title": " [FLINK-16972][network] LocalBufferPool eagerly fetches global segments to ensure proper availability.", "bodyText": "What is the purpose of the change\nBefore this commit, availability of LocalBufferPool depended on a the availability of a shared NetworkBufferPool. However, if multiple LocalBufferPools simultaneously are available only because the NetworkBufferPool becomes available with one segment, only one of the LocalBufferPools is truly available (the one that actually acquires this segment).\nAdditionally, LocalBufferPool would switch to unavailable only after it could not serve a requested memory segment. For requestBufferBuilderBlocking that is too late as it entered the blocking loop already.\nBrief change log\n\nRemoving deprecated BufferPoolOwner and cleaning up exception usages to ease implementation of the following changes.\nClear definition of availability: a LocalBufferPool is only available if it can provide a memory segment to the consumer in a non-blocking fashion.\nIf a LocalBufferPool runs out of local segments it will become unavailable until it receives a segment from the NetworkBufferPool. To minimize unavailability, LocalBufferPool first tries to eagerly fetch new segments before declaring unavailability and if that fails, the local pool subscribes to the availability to the network pool to restore availability asap.\nLocalBufferPool now permanently holds at least one buffer. To reflect that, the number of required segments needs to be at least one, which matches all usages in production code. A few test needed to be adjusted to properly capture the new requirement.\nFixed StreamTask not blocking default action when output becomes unavailable during recovery.\n\nVerifying this change\nAlready covered by plenty of tests.\nDoes this pull request potentially affect one of the following parts:\n\nDependencies (does it add or upgrade a dependency): (yes / no)\nThe public API, i.e., is any changed class annotated with @Public(Evolving): (yes / no)\nThe serializers: (yes / no / don't know)\nThe runtime per-record code paths (performance sensitive): (yes / no / don't know)\nAnything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (yes / no / don't know)\nThe S3 file system connector: (yes / no / don't know)\n\nDocumentation\n\nDoes this pull request introduce a new feature? (yes / no)\nIf yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)", "createdAt": "2020-09-28T07:53:14Z", "url": "https://github.com/apache/flink/pull/13499", "merged": true, "mergeCommit": {"oid": "e17dbab24f4f71c5472d27267e938791686e45c3"}, "closed": true, "closedAt": "2020-10-01T06:39:28Z", "author": {"login": "AHeise"}, "timelineItems": {"totalCount": 16, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdNQ_49gBqjM4MTM3ODgwNDg=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdOD8OGABqjM4MjY1ODcxOTk=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "4a43a68e1704e812557746a9079aa4a4c7f56f00", "author": {"user": {"login": "AHeise", "name": "Arvid Heise"}}, "url": "https://github.com/apache/flink/commit/4a43a68e1704e812557746a9079aa4a4c7f56f00", "committedDate": "2020-09-28T07:49:35Z", "message": "[FLINK-16972][network] LocalBufferPool eagerly fetches global segments to ensure proper availability.\n\nBefore this commit, availability of LocalBufferPool depended on a the availability of a shared NetworkBufferPool. However, if multiple LocalBufferPools simultaneously are available only because the NetworkBufferPool becomes available with one segment, only one of the LocalBufferPools is truly available (the one that actually acquires this segment).\n\nThe solution in this commit is to define availability only through the guaranteed ability to provide a memory segment to the consumer. If a LocalBufferPool runs out of local segments it will become unavailable until it receives a segment from the NetworkBufferPool. To minimize unavailability, LocalBufferPool first tries to eagerly fetch new segments before declaring unavailability and if that fails, the local pool subscribes to the availability to the network pool to restore availability asap.\n\nAdditionally, LocalBufferPool would switch to unavailable only after it could not serve a requested memory segment. For requestBufferBuilderBlocking that is too late as it entered the blocking loop already.\n\nFinally, LocalBufferPool now permanently holds at least one buffer. To reflect that, the number of required segments needs to be at least one, which matches all usages in production code. A few test needed to be adjusted to properly capture the new requirement."}, "afterCommit": {"oid": "378555e68fc64c708f033109422f8701013fdaed", "author": {"user": {"login": "AHeise", "name": "Arvid Heise"}}, "url": "https://github.com/apache/flink/commit/378555e68fc64c708f033109422f8701013fdaed", "committedDate": "2020-09-28T10:25:27Z", "message": "[FLINK-16972][network] LocalBufferPool eagerly fetches global segments to ensure proper availability.\n\nBefore this commit, availability of LocalBufferPool depended on a the availability of a shared NetworkBufferPool. However, if multiple LocalBufferPools simultaneously are available only because the NetworkBufferPool becomes available with one segment, only one of the LocalBufferPools is truly available (the one that actually acquires this segment).\n\nThe solution in this commit is to define availability only through the guaranteed ability to provide a memory segment to the consumer. If a LocalBufferPool runs out of local segments it will become unavailable until it receives a segment from the NetworkBufferPool. To minimize unavailability, LocalBufferPool first tries to eagerly fetch new segments before declaring unavailability and if that fails, the local pool subscribes to the availability to the network pool to restore availability asap.\n\nAdditionally, LocalBufferPool would switch to unavailable only after it could not serve a requested memory segment. For requestBufferBuilderBlocking that is too late as it entered the blocking loop already.\n\nFinally, LocalBufferPool now permanently holds at least one buffer. To reflect that, the number of required segments needs to be at least one, which matches all usages in production code. A few test needed to be adjusted to properly capture the new requirement."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk3NTU0MTE0", "url": "https://github.com/apache/flink/pull/13499#pullrequestreview-497554114", "createdAt": "2020-09-28T14:12:36Z", "commit": {"oid": "68b0c710d4850f79920a584097c0218a1ace6ca8"}, "state": "COMMENTED", "comments": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQxNDoxMjozNlrOHY_p8A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQxNjowNjoyNlrOHZFgbQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTk3MDgwMA==", "bodyText": "Have you checked if the BufferPoolOwner is not part of our shuffle service API? Maybe there can be some 3rd party shuffle services using it?\n@zhijiangW seemed to be fine with removing it in the ticket, so I guess that's not an issue (he was involved in the plugable shuffle service story).", "url": "https://github.com/apache/flink/pull/13499#discussion_r495970800", "createdAt": "2020-09-28T14:12:36Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/buffer/BufferPoolFactory.java", "diffHunk": "@@ -48,8 +48,6 @@\n \t * \t\tminimum number of network buffers in this pool\n \t * @param maxUsedBuffers\n \t * \t\tmaximum number of network buffers this pool offers\n-\t * @param bufferPoolOwner", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "68b0c710d4850f79920a584097c0218a1ace6ca8"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTk5MzE0OQ==", "bodyText": "What's the purpose of this change? Is it actually working? What if getInputOutputJointFuture returns completed future, but it become unavailable during the input recovery?\nAlso, it's missing a test coverage.", "url": "https://github.com/apache/flink/pull/13499#discussion_r495993149", "createdAt": "2020-09-28T14:43:15Z", "author": {"login": "pnowojski"}, "path": "flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/StreamTask.java", "diffHunk": "@@ -518,6 +518,11 @@ private void readRecoveredChannelState() throws IOException, InterruptedExceptio\n \t\t\t\t\t\t\t\"Cannot restore state to a non-checkpointable partition type: \" + writer);\n \t\t\t\t}\n \t\t\t}\n+\n+\t\t\tif (!recordWriter.isAvailable()) {\n+\t\t\t\tMailboxDefaultAction.Suspension suspendedDefaultAction = mailboxProcessor.suspendDefaultAction();\n+\t\t\t\tgetInputOutputJointFuture(InputStatus.NOTHING_AVAILABLE).thenRun(suspendedDefaultAction::resume);\n+\t\t\t}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a59d85b1e23a1c3c904c36c63f6dddae8ccc691c"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjAwNTA0Mw==", "bodyText": "why assert and not checkState()?", "url": "https://github.com/apache/flink/pull/13499#discussion_r496005043", "createdAt": "2020-09-28T14:56:08Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/buffer/LocalBufferPool.java", "diffHunk": "@@ -190,6 +205,12 @@\n \t\t\tsubpartitionBufferRecyclers[i] = new SubpartitionBufferRecycler(i, this);\n \t\t}\n \t\tthis.maxBuffersPerChannel = maxBuffersPerChannel;\n+\n+\t\tif (checkAvailability()) {\n+\t\t\tavailabilityHelper.resetAvailable();\n+\t\t}\n+\n+\t\tassert hasConsistentAvailability();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "378555e68fc64c708f033109422f8701013fdaed"}, "originalPosition": 74}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjAzNzI3Ng==", "bodyText": "doing this under the lock would look more consistent? As it is, can not it yield false results? All of the modifications to the availabilityHelper  are happening under the availableMemorySegments lock , so after moving there it should be perfectly accurate.", "url": "https://github.com/apache/flink/pull/13499#discussion_r496037276", "createdAt": "2020-09-28T15:23:57Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/buffer/LocalBufferPool.java", "diffHunk": "@@ -322,23 +350,71 @@ private MemorySegment requestMemorySegment() {\n \t\treturn requestMemorySegment(UNKNOWN_CHANNEL);\n \t}\n \n-\t@Nullable\n-\tprivate MemorySegment requestMemorySegmentFromGlobal() {\n-\t\tassert Thread.holdsLock(availableMemorySegments);\n+\tprivate boolean requestMemorySegmentFromGlobal() {\n+\t\tif (numberOfRequestedMemorySegments >= currentPoolSize) {\n+\t\t\treturn false;\n+\t\t}\n+\n+\t\tMemorySegment segment = networkBufferPool.requestMemorySegment();\n+\t\tif (segment != null) {\n+\t\t\tavailableMemorySegments.add(segment);\n+\t\t\tnumberOfRequestedMemorySegments++;\n+\t\t\treturn true;\n+\t\t}\n+\t\treturn false;\n+\t}\n \n-\t\tif (isDestroyed) {\n-\t\t\tthrow new IllegalStateException(\"Buffer pool is destroyed.\");\n+\t/**\n+\t * Tries to obtain a buffer from global pool as soon as one pool is available. Note that multiple\n+\t * {@link LocalBufferPool}s might wait on the future of the global pool, hence this method double-check if a new\n+\t * buffer is really needed at the time it becomes available.\n+\t */\n+\tprivate void eagerlyRequestMemorySegmentFromGlobal() {\n+\t\tif (eagerlyRequesting) {\n+\t\t\treturn;\n \t\t}\n+\t\teagerlyRequesting = true;\n+\t\tnetworkBufferPool.getAvailableFuture().thenRun(() -> {\n+\t\t\teagerlyRequesting = false;\n+\t\t\tif (availabilityHelper.isAvailable()) {\n+\t\t\t\t// there is currently no benefit for this pool to obtain buffer from global; give other pools precedent\n+\t\t\t\treturn;\n+\t\t\t}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "378555e68fc64c708f033109422f8701013fdaed"}, "originalPosition": 163}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjAzODQwOA==", "bodyText": "does it need to be volatile if we already have @GuardedBy(\"availableMemorySegments\")? Adding another point of synchronisation makes it more difficult to reason about the concurrency model.", "url": "https://github.com/apache/flink/pull/13499#discussion_r496038408", "createdAt": "2020-09-28T15:25:09Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/buffer/LocalBufferPool.java", "diffHunk": "@@ -92,16 +102,21 @@\n \n \tprivate final int maxBuffersPerChannel;\n \n+\t@GuardedBy(\"availableMemorySegments\")\n \tprivate final int[] subpartitionBuffersCount;\n \n \tprivate final BufferRecycler[] subpartitionBufferRecyclers;\n \n+\t@GuardedBy(\"availableMemorySegments\")\n \tprivate int unavailableSubpartitionsCount = 0;\n \n \tprivate boolean isDestroyed;\n \n+\t@GuardedBy(\"availableMemorySegments\")\n \tprivate final AvailabilityHelper availabilityHelper = new AvailabilityHelper();\n \n+\tprivate volatile boolean eagerlyRequesting;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "378555e68fc64c708f033109422f8701013fdaed"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjA0MTgxNQ==", "bodyText": "nit: for (future) consistency and self documenting code, maybe extract this condition to a private method isRequestedSizeReached()?", "url": "https://github.com/apache/flink/pull/13499#discussion_r496041815", "createdAt": "2020-09-28T15:29:47Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/buffer/LocalBufferPool.java", "diffHunk": "@@ -322,23 +350,71 @@ private MemorySegment requestMemorySegment() {\n \t\treturn requestMemorySegment(UNKNOWN_CHANNEL);\n \t}\n \n-\t@Nullable\n-\tprivate MemorySegment requestMemorySegmentFromGlobal() {\n-\t\tassert Thread.holdsLock(availableMemorySegments);\n+\tprivate boolean requestMemorySegmentFromGlobal() {\n+\t\tif (numberOfRequestedMemorySegments >= currentPoolSize) {\n+\t\t\treturn false;\n+\t\t}\n+\n+\t\tMemorySegment segment = networkBufferPool.requestMemorySegment();\n+\t\tif (segment != null) {\n+\t\t\tavailableMemorySegments.add(segment);\n+\t\t\tnumberOfRequestedMemorySegments++;\n+\t\t\treturn true;\n+\t\t}\n+\t\treturn false;\n+\t}\n \n-\t\tif (isDestroyed) {\n-\t\t\tthrow new IllegalStateException(\"Buffer pool is destroyed.\");\n+\t/**\n+\t * Tries to obtain a buffer from global pool as soon as one pool is available. Note that multiple\n+\t * {@link LocalBufferPool}s might wait on the future of the global pool, hence this method double-check if a new\n+\t * buffer is really needed at the time it becomes available.\n+\t */\n+\tprivate void eagerlyRequestMemorySegmentFromGlobal() {\n+\t\tif (eagerlyRequesting) {\n+\t\t\treturn;\n \t\t}\n+\t\teagerlyRequesting = true;\n+\t\tnetworkBufferPool.getAvailableFuture().thenRun(() -> {\n+\t\t\teagerlyRequesting = false;\n+\t\t\tif (availabilityHelper.isAvailable()) {\n+\t\t\t\t// there is currently no benefit for this pool to obtain buffer from global; give other pools precedent\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tCompletableFuture<?> toNotify = null;\n+\t\t\tsynchronized (availableMemorySegments) {\n+\t\t\t\tif (numberOfRequestedMemorySegments >= currentPoolSize) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "378555e68fc64c708f033109422f8701013fdaed"}, "originalPosition": 166}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjA2MzM5OQ==", "bodyText": "I guess it's not \"eagerly\" after all? Eagerly would mean to me something like request them upon construction, or something like that?\nHere you mean, request the buffers first, before making LocalBufferPool available?", "url": "https://github.com/apache/flink/pull/13499#discussion_r496063399", "createdAt": "2020-09-28T16:01:16Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/buffer/LocalBufferPool.java", "diffHunk": "@@ -322,23 +350,71 @@ private MemorySegment requestMemorySegment() {\n \t\treturn requestMemorySegment(UNKNOWN_CHANNEL);\n \t}\n \n-\t@Nullable\n-\tprivate MemorySegment requestMemorySegmentFromGlobal() {\n-\t\tassert Thread.holdsLock(availableMemorySegments);\n+\tprivate boolean requestMemorySegmentFromGlobal() {\n+\t\tif (numberOfRequestedMemorySegments >= currentPoolSize) {\n+\t\t\treturn false;\n+\t\t}\n+\n+\t\tMemorySegment segment = networkBufferPool.requestMemorySegment();\n+\t\tif (segment != null) {\n+\t\t\tavailableMemorySegments.add(segment);\n+\t\t\tnumberOfRequestedMemorySegments++;\n+\t\t\treturn true;\n+\t\t}\n+\t\treturn false;\n+\t}\n \n-\t\tif (isDestroyed) {\n-\t\t\tthrow new IllegalStateException(\"Buffer pool is destroyed.\");\n+\t/**\n+\t * Tries to obtain a buffer from global pool as soon as one pool is available. Note that multiple\n+\t * {@link LocalBufferPool}s might wait on the future of the global pool, hence this method double-check if a new\n+\t * buffer is really needed at the time it becomes available.\n+\t */\n+\tprivate void eagerlyRequestMemorySegmentFromGlobal() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "378555e68fc64c708f033109422f8701013fdaed"}, "originalPosition": 153}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjA2NDEyNw==", "bodyText": "does it mean we can keep requesting segments despite reaching per sub-partition limit? Is it a pre-existing behaviour? (if so, we could leave it as a future improvement)", "url": "https://github.com/apache/flink/pull/13499#discussion_r496064127", "createdAt": "2020-09-28T16:02:23Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/buffer/LocalBufferPool.java", "diffHunk": "@@ -322,23 +350,71 @@ private MemorySegment requestMemorySegment() {\n \t\treturn requestMemorySegment(UNKNOWN_CHANNEL);\n \t}\n \n-\t@Nullable\n-\tprivate MemorySegment requestMemorySegmentFromGlobal() {\n-\t\tassert Thread.holdsLock(availableMemorySegments);\n+\tprivate boolean requestMemorySegmentFromGlobal() {\n+\t\tif (numberOfRequestedMemorySegments >= currentPoolSize) {\n+\t\t\treturn false;\n+\t\t}\n+\n+\t\tMemorySegment segment = networkBufferPool.requestMemorySegment();\n+\t\tif (segment != null) {\n+\t\t\tavailableMemorySegments.add(segment);\n+\t\t\tnumberOfRequestedMemorySegments++;\n+\t\t\treturn true;\n+\t\t}\n+\t\treturn false;\n+\t}\n \n-\t\tif (isDestroyed) {\n-\t\t\tthrow new IllegalStateException(\"Buffer pool is destroyed.\");\n+\t/**\n+\t * Tries to obtain a buffer from global pool as soon as one pool is available. Note that multiple\n+\t * {@link LocalBufferPool}s might wait on the future of the global pool, hence this method double-check if a new\n+\t * buffer is really needed at the time it becomes available.\n+\t */\n+\tprivate void eagerlyRequestMemorySegmentFromGlobal() {\n+\t\tif (eagerlyRequesting) {\n+\t\t\treturn;\n \t\t}\n+\t\teagerlyRequesting = true;\n+\t\tnetworkBufferPool.getAvailableFuture().thenRun(() -> {\n+\t\t\teagerlyRequesting = false;\n+\t\t\tif (availabilityHelper.isAvailable()) {\n+\t\t\t\t// there is currently no benefit for this pool to obtain buffer from global; give other pools precedent\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tCompletableFuture<?> toNotify = null;\n+\t\t\tsynchronized (availableMemorySegments) {\n+\t\t\t\tif (numberOfRequestedMemorySegments >= currentPoolSize) {\n+\t\t\t\t\treturn;\n+\t\t\t\t}\n+\n+\t\t\t\t// fetch a segment from global pool\n+\t\t\t\tif (requestMemorySegmentFromGlobal()) {\n+\t\t\t\t\ttoNotify = availabilityHelper.getUnavailableToResetAvailable();\n+\t\t\t\t} else {\n+\t\t\t\t\t// segment probably taken by other pool, so retry later\n+\t\t\t\t\teagerlyRequestMemorySegmentFromGlobal();\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tmayNotifyAvailable(toNotify);\n+\t\t});\n+\t}\n \n+\tprivate boolean checkAvailability() {\n+\t\tif (!availableMemorySegments.isEmpty()) {\n+\t\t\treturn unavailableSubpartitionsCount == 0;\n+\t\t}\n \t\tif (numberOfRequestedMemorySegments < currentPoolSize) {\n-\t\t\tfinal MemorySegment segment = networkBufferPool.requestMemorySegment();\n-\t\t\tif (segment != null) {\n-\t\t\t\tnumberOfRequestedMemorySegments++;\n-\t\t\t\treturn segment;\n+\t\t\tif (requestMemorySegmentFromGlobal()) {\n+\t\t\t\treturn unavailableSubpartitionsCount == 0;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "378555e68fc64c708f033109422f8701013fdaed"}, "originalPosition": 192}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjA2NjY2OQ==", "bodyText": "subpartitionBuffersCount[targetChannel]++ vs ++subpartitionBuffersCount[targetChannel], isn't it changing the semantic a bit?", "url": "https://github.com/apache/flink/pull/13499#discussion_r496066669", "createdAt": "2020-09-28T16:06:26Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/buffer/LocalBufferPool.java", "diffHunk": "@@ -292,27 +313,34 @@ private MemorySegment requestMemorySegmentBlocking(int targetChannel) throws Int\n \n \t@Nullable\n \tprivate MemorySegment requestMemorySegment(int targetChannel) {\n-\t\tMemorySegment segment = null;\n+\t\tMemorySegment segment;\n \t\tsynchronized (availableMemorySegments) {\n-\t\t\treturnExcessMemorySegments();\n-\n-\t\t\tif (availableMemorySegments.isEmpty()) {\n-\t\t\t\tsegment = requestMemorySegmentFromGlobal();\n+\t\t\tif (isDestroyed) {\n+\t\t\t\tthrow new IllegalStateException(\"Buffer pool is destroyed.\");\n \t\t\t}\n-\t\t\t// segment may have been released by buffer pool owner\n-\t\t\tif (segment == null) {\n-\t\t\t\tsegment = availableMemorySegments.poll();\n+\n+\t\t\t// target channel over quota; do not return a segment\n+\t\t\tif (targetChannel != UNKNOWN_CHANNEL && subpartitionBuffersCount[targetChannel] >= maxBuffersPerChannel) {\n+\t\t\t\treturn null;\n \t\t\t}\n+\n+\t\t\tsegment = availableMemorySegments.poll();\n+\n \t\t\tif (segment == null) {\n-\t\t\t\tavailabilityHelper.resetUnavailable();\n+\t\t\t\treturn null;\n \t\t\t}\n \n-\t\t\tif (segment != null && targetChannel != UNKNOWN_CHANNEL) {\n-\t\t\t\tif (subpartitionBuffersCount[targetChannel]++ == maxBuffersPerChannel) {\n+\t\t\tif (targetChannel != UNKNOWN_CHANNEL) {\n+\t\t\t\tif (++subpartitionBuffersCount[targetChannel] == maxBuffersPerChannel) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "378555e68fc64c708f033109422f8701013fdaed"}, "originalPosition": 111}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk4MjcxMDk1", "url": "https://github.com/apache/flink/pull/13499#pullrequestreview-498271095", "createdAt": "2020-09-29T09:16:13Z", "commit": {"oid": "378555e68fc64c708f033109422f8701013fdaed"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwOToxNjoxM1rOHZj8VQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwOToyNTozM1rOHZkUhQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjU2NTMzMw==", "bodyText": "maybe change it to while (numberOfRequestedMemorySegments < currentPoolSize) loop? (as a follow up commit?)", "url": "https://github.com/apache/flink/pull/13499#discussion_r496565333", "createdAt": "2020-09-29T09:16:13Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/buffer/LocalBufferPool.java", "diffHunk": "@@ -322,23 +350,71 @@ private MemorySegment requestMemorySegment() {\n \t\treturn requestMemorySegment(UNKNOWN_CHANNEL);\n \t}\n \n-\t@Nullable\n-\tprivate MemorySegment requestMemorySegmentFromGlobal() {\n-\t\tassert Thread.holdsLock(availableMemorySegments);\n+\tprivate boolean requestMemorySegmentFromGlobal() {\n+\t\tif (numberOfRequestedMemorySegments >= currentPoolSize) {\n+\t\t\treturn false;\n+\t\t}\n+\n+\t\tMemorySegment segment = networkBufferPool.requestMemorySegment();\n+\t\tif (segment != null) {\n+\t\t\tavailableMemorySegments.add(segment);\n+\t\t\tnumberOfRequestedMemorySegments++;\n+\t\t\treturn true;\n+\t\t}\n+\t\treturn false;\n+\t}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "378555e68fc64c708f033109422f8701013fdaed"}, "originalPosition": 144}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjU3MDg2Nw==", "bodyText": "I don't think this is a big issue. Just wanted to make sure it's not a regression (that would be an issue)", "url": "https://github.com/apache/flink/pull/13499#discussion_r496570867", "createdAt": "2020-09-29T09:24:40Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/buffer/LocalBufferPool.java", "diffHunk": "@@ -322,23 +350,71 @@ private MemorySegment requestMemorySegment() {\n \t\treturn requestMemorySegment(UNKNOWN_CHANNEL);\n \t}\n \n-\t@Nullable\n-\tprivate MemorySegment requestMemorySegmentFromGlobal() {\n-\t\tassert Thread.holdsLock(availableMemorySegments);\n+\tprivate boolean requestMemorySegmentFromGlobal() {\n+\t\tif (numberOfRequestedMemorySegments >= currentPoolSize) {\n+\t\t\treturn false;\n+\t\t}\n+\n+\t\tMemorySegment segment = networkBufferPool.requestMemorySegment();\n+\t\tif (segment != null) {\n+\t\t\tavailableMemorySegments.add(segment);\n+\t\t\tnumberOfRequestedMemorySegments++;\n+\t\t\treturn true;\n+\t\t}\n+\t\treturn false;\n+\t}\n \n-\t\tif (isDestroyed) {\n-\t\t\tthrow new IllegalStateException(\"Buffer pool is destroyed.\");\n+\t/**\n+\t * Tries to obtain a buffer from global pool as soon as one pool is available. Note that multiple\n+\t * {@link LocalBufferPool}s might wait on the future of the global pool, hence this method double-check if a new\n+\t * buffer is really needed at the time it becomes available.\n+\t */\n+\tprivate void eagerlyRequestMemorySegmentFromGlobal() {\n+\t\tif (eagerlyRequesting) {\n+\t\t\treturn;\n \t\t}\n+\t\teagerlyRequesting = true;\n+\t\tnetworkBufferPool.getAvailableFuture().thenRun(() -> {\n+\t\t\teagerlyRequesting = false;\n+\t\t\tif (availabilityHelper.isAvailable()) {\n+\t\t\t\t// there is currently no benefit for this pool to obtain buffer from global; give other pools precedent\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tCompletableFuture<?> toNotify = null;\n+\t\t\tsynchronized (availableMemorySegments) {\n+\t\t\t\tif (numberOfRequestedMemorySegments >= currentPoolSize) {\n+\t\t\t\t\treturn;\n+\t\t\t\t}\n+\n+\t\t\t\t// fetch a segment from global pool\n+\t\t\t\tif (requestMemorySegmentFromGlobal()) {\n+\t\t\t\t\ttoNotify = availabilityHelper.getUnavailableToResetAvailable();\n+\t\t\t\t} else {\n+\t\t\t\t\t// segment probably taken by other pool, so retry later\n+\t\t\t\t\teagerlyRequestMemorySegmentFromGlobal();\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tmayNotifyAvailable(toNotify);\n+\t\t});\n+\t}\n \n+\tprivate boolean checkAvailability() {\n+\t\tif (!availableMemorySegments.isEmpty()) {\n+\t\t\treturn unavailableSubpartitionsCount == 0;\n+\t\t}\n \t\tif (numberOfRequestedMemorySegments < currentPoolSize) {\n-\t\t\tfinal MemorySegment segment = networkBufferPool.requestMemorySegment();\n-\t\t\tif (segment != null) {\n-\t\t\t\tnumberOfRequestedMemorySegments++;\n-\t\t\t\treturn segment;\n+\t\t\tif (requestMemorySegmentFromGlobal()) {\n+\t\t\t\treturn unavailableSubpartitionsCount == 0;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjA2NDEyNw=="}, "originalCommit": {"oid": "378555e68fc64c708f033109422f8701013fdaed"}, "originalPosition": 192}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjU3MTUyNQ==", "bodyText": "Yes, it would be better to move it to a separate commit. I agree that [0, maxBuffersPerChannel] makes more sense.", "url": "https://github.com/apache/flink/pull/13499#discussion_r496571525", "createdAt": "2020-09-29T09:25:33Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/buffer/LocalBufferPool.java", "diffHunk": "@@ -292,27 +313,34 @@ private MemorySegment requestMemorySegmentBlocking(int targetChannel) throws Int\n \n \t@Nullable\n \tprivate MemorySegment requestMemorySegment(int targetChannel) {\n-\t\tMemorySegment segment = null;\n+\t\tMemorySegment segment;\n \t\tsynchronized (availableMemorySegments) {\n-\t\t\treturnExcessMemorySegments();\n-\n-\t\t\tif (availableMemorySegments.isEmpty()) {\n-\t\t\t\tsegment = requestMemorySegmentFromGlobal();\n+\t\t\tif (isDestroyed) {\n+\t\t\t\tthrow new IllegalStateException(\"Buffer pool is destroyed.\");\n \t\t\t}\n-\t\t\t// segment may have been released by buffer pool owner\n-\t\t\tif (segment == null) {\n-\t\t\t\tsegment = availableMemorySegments.poll();\n+\n+\t\t\t// target channel over quota; do not return a segment\n+\t\t\tif (targetChannel != UNKNOWN_CHANNEL && subpartitionBuffersCount[targetChannel] >= maxBuffersPerChannel) {\n+\t\t\t\treturn null;\n \t\t\t}\n+\n+\t\t\tsegment = availableMemorySegments.poll();\n+\n \t\t\tif (segment == null) {\n-\t\t\t\tavailabilityHelper.resetUnavailable();\n+\t\t\t\treturn null;\n \t\t\t}\n \n-\t\t\tif (segment != null && targetChannel != UNKNOWN_CHANNEL) {\n-\t\t\t\tif (subpartitionBuffersCount[targetChannel]++ == maxBuffersPerChannel) {\n+\t\t\tif (targetChannel != UNKNOWN_CHANNEL) {\n+\t\t\t\tif (++subpartitionBuffersCount[targetChannel] == maxBuffersPerChannel) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjA2NjY2OQ=="}, "originalCommit": {"oid": "378555e68fc64c708f033109422f8701013fdaed"}, "originalPosition": 111}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "378555e68fc64c708f033109422f8701013fdaed", "author": {"user": {"login": "AHeise", "name": "Arvid Heise"}}, "url": "https://github.com/apache/flink/commit/378555e68fc64c708f033109422f8701013fdaed", "committedDate": "2020-09-28T10:25:27Z", "message": "[FLINK-16972][network] LocalBufferPool eagerly fetches global segments to ensure proper availability.\n\nBefore this commit, availability of LocalBufferPool depended on a the availability of a shared NetworkBufferPool. However, if multiple LocalBufferPools simultaneously are available only because the NetworkBufferPool becomes available with one segment, only one of the LocalBufferPools is truly available (the one that actually acquires this segment).\n\nThe solution in this commit is to define availability only through the guaranteed ability to provide a memory segment to the consumer. If a LocalBufferPool runs out of local segments it will become unavailable until it receives a segment from the NetworkBufferPool. To minimize unavailability, LocalBufferPool first tries to eagerly fetch new segments before declaring unavailability and if that fails, the local pool subscribes to the availability to the network pool to restore availability asap.\n\nAdditionally, LocalBufferPool would switch to unavailable only after it could not serve a requested memory segment. For requestBufferBuilderBlocking that is too late as it entered the blocking loop already.\n\nFinally, LocalBufferPool now permanently holds at least one buffer. To reflect that, the number of required segments needs to be at least one, which matches all usages in production code. A few test needed to be adjusted to properly capture the new requirement."}, "afterCommit": {"oid": "f09cce84837e90a6617a4f2d22ad96258f98427b", "author": {"user": {"login": "AHeise", "name": "Arvid Heise"}}, "url": "https://github.com/apache/flink/commit/f09cce84837e90a6617a4f2d22ad96258f98427b", "committedDate": "2020-09-29T19:43:21Z", "message": "[FLINK-16972][network] LocalBufferPool eagerly fetches global segments to ensure proper availability.\n\nBefore this commit, availability of LocalBufferPool depended on a the availability of a shared NetworkBufferPool. However, if multiple LocalBufferPools simultaneously are available only because the NetworkBufferPool becomes available with one segment, only one of the LocalBufferPools is truly available (the one that actually acquires this segment).\n\nThe solution in this commit is to define availability only through the guaranteed ability to provide a memory segment to the consumer. If a LocalBufferPool runs out of local segments it will become unavailable until it receives a segment from the NetworkBufferPool. To minimize unavailability, LocalBufferPool first tries to eagerly fetch new segments before declaring unavailability and if that fails, the local pool subscribes to the availability to the network pool to restore availability asap.\n\nAdditionally, LocalBufferPool would switch to unavailable only after it could not serve a requested memory segment. For requestBufferBuilderBlocking that is too late as it entered the blocking loop already.\n\nFinally, LocalBufferPool now permanently holds at least one buffer. To reflect that, the number of required segments needs to be at least one, which matches all usages in production code. A few test needed to be adjusted to properly capture the new requirement."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "f09cce84837e90a6617a4f2d22ad96258f98427b", "author": {"user": {"login": "AHeise", "name": "Arvid Heise"}}, "url": "https://github.com/apache/flink/commit/f09cce84837e90a6617a4f2d22ad96258f98427b", "committedDate": "2020-09-29T19:43:21Z", "message": "[FLINK-16972][network] LocalBufferPool eagerly fetches global segments to ensure proper availability.\n\nBefore this commit, availability of LocalBufferPool depended on a the availability of a shared NetworkBufferPool. However, if multiple LocalBufferPools simultaneously are available only because the NetworkBufferPool becomes available with one segment, only one of the LocalBufferPools is truly available (the one that actually acquires this segment).\n\nThe solution in this commit is to define availability only through the guaranteed ability to provide a memory segment to the consumer. If a LocalBufferPool runs out of local segments it will become unavailable until it receives a segment from the NetworkBufferPool. To minimize unavailability, LocalBufferPool first tries to eagerly fetch new segments before declaring unavailability and if that fails, the local pool subscribes to the availability to the network pool to restore availability asap.\n\nAdditionally, LocalBufferPool would switch to unavailable only after it could not serve a requested memory segment. For requestBufferBuilderBlocking that is too late as it entered the blocking loop already.\n\nFinally, LocalBufferPool now permanently holds at least one buffer. To reflect that, the number of required segments needs to be at least one, which matches all usages in production code. A few test needed to be adjusted to properly capture the new requirement."}, "afterCommit": {"oid": "f298e8b85569242501d871e326cce21695582caa", "author": {"user": {"login": "AHeise", "name": "Arvid Heise"}}, "url": "https://github.com/apache/flink/commit/f298e8b85569242501d871e326cce21695582caa", "committedDate": "2020-09-29T19:56:00Z", "message": "[FLINK-16972][network] LocalBufferPool eagerly fetches global segments to ensure proper availability.\n\nBefore this commit, availability of LocalBufferPool depended on a the availability of a shared NetworkBufferPool. However, if multiple LocalBufferPools simultaneously are available only because the NetworkBufferPool becomes available with one segment, only one of the LocalBufferPools is truly available (the one that actually acquires this segment).\n\nThe solution in this commit is to define availability only through the guaranteed ability to provide a memory segment to the consumer. If a LocalBufferPool runs out of local segments it will become unavailable until it receives a segment from the NetworkBufferPool. To minimize unavailability, LocalBufferPool first tries to eagerly fetch new segments before declaring unavailability and if that fails, the local pool subscribes to the availability to the network pool to restore availability asap.\n\nAdditionally, LocalBufferPool would switch to unavailable only after it could not serve a requested memory segment. For requestBufferBuilderBlocking that is too late as it entered the blocking loop already.\n\nFinally, LocalBufferPool now permanently holds at least one buffer. To reflect that, the number of required segments needs to be at least one, which matches all usages in production code. A few test needed to be adjusted to properly capture the new requirement."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "f298e8b85569242501d871e326cce21695582caa", "author": {"user": {"login": "AHeise", "name": "Arvid Heise"}}, "url": "https://github.com/apache/flink/commit/f298e8b85569242501d871e326cce21695582caa", "committedDate": "2020-09-29T19:56:00Z", "message": "[FLINK-16972][network] LocalBufferPool eagerly fetches global segments to ensure proper availability.\n\nBefore this commit, availability of LocalBufferPool depended on a the availability of a shared NetworkBufferPool. However, if multiple LocalBufferPools simultaneously are available only because the NetworkBufferPool becomes available with one segment, only one of the LocalBufferPools is truly available (the one that actually acquires this segment).\n\nThe solution in this commit is to define availability only through the guaranteed ability to provide a memory segment to the consumer. If a LocalBufferPool runs out of local segments it will become unavailable until it receives a segment from the NetworkBufferPool. To minimize unavailability, LocalBufferPool first tries to eagerly fetch new segments before declaring unavailability and if that fails, the local pool subscribes to the availability to the network pool to restore availability asap.\n\nAdditionally, LocalBufferPool would switch to unavailable only after it could not serve a requested memory segment. For requestBufferBuilderBlocking that is too late as it entered the blocking loop already.\n\nFinally, LocalBufferPool now permanently holds at least one buffer. To reflect that, the number of required segments needs to be at least one, which matches all usages in production code. A few test needed to be adjusted to properly capture the new requirement."}, "afterCommit": {"oid": "df08363e00e50b79ff2492f344a4cef335699b17", "author": {"user": {"login": "AHeise", "name": "Arvid Heise"}}, "url": "https://github.com/apache/flink/commit/df08363e00e50b79ff2492f344a4cef335699b17", "committedDate": "2020-09-30T07:19:20Z", "message": "[FLINK-16972][network] LocalBufferPool eagerly fetches global segments to ensure proper availability.\n\nBefore this commit, availability of LocalBufferPool depended on a the availability of a shared NetworkBufferPool. However, if multiple LocalBufferPools simultaneously are available only because the NetworkBufferPool becomes available with one segment, only one of the LocalBufferPools is truly available (the one that actually acquires this segment).\n\nThe solution in this commit is to define availability only through the guaranteed ability to provide a memory segment to the consumer. If a LocalBufferPool runs out of local segments it will become unavailable until it receives a segment from the NetworkBufferPool. To minimize unavailability, LocalBufferPool first tries to eagerly fetch new segments before declaring unavailability and if that fails, the local pool subscribes to the availability to the network pool to restore availability asap.\n\nAdditionally, LocalBufferPool would switch to unavailable only after it could not serve a requested memory segment. For requestBufferBuilderBlocking that is too late as it entered the blocking loop already.\n\nFinally, LocalBufferPool now permanently holds at least one buffer. To reflect that, the number of required segments needs to be at least one, which matches all usages in production code. A few test needed to be adjusted to properly capture the new requirement."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk5Mjk5MzU1", "url": "https://github.com/apache/flink/pull/13499#pullrequestreview-499299355", "createdAt": "2020-09-30T10:39:11Z", "commit": {"oid": "0db12e7ec1a83349104a4f6556096f9997c03805"}, "state": "APPROVED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQxMDozOToxMVrOHaXkQQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQxMDo0MjowNFrOHaXqTg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzQxMTEzNw==", "bodyText": "Shouldn't this commit change some tests?", "url": "https://github.com/apache/flink/pull/13499#discussion_r497411137", "createdAt": "2020-09-30T10:39:11Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/buffer/LocalBufferPool.java", "diffHunk": "@@ -296,6 +296,11 @@ private MemorySegment requestMemorySegment(int targetChannel) {\n \t\tsynchronized (availableMemorySegments) {\n \t\t\treturnExcessMemorySegments();\n \n+\t\t\t// target channel over quota; do not return a segment", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0db12e7ec1a83349104a4f6556096f9997c03805"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzQxMjY4Ng==", "bodyText": "nit: there are still two places doing numberOfRequestedMemorySegments > currentPoolSize, but I guess they are off by one in the comparison and can not be easily migrated to isRequestedSizeReached()?", "url": "https://github.com/apache/flink/pull/13499#discussion_r497412686", "createdAt": "2020-09-30T10:42:04Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/buffer/LocalBufferPool.java", "diffHunk": "@@ -327,23 +358,83 @@ private MemorySegment requestMemorySegment() {\n \t\treturn requestMemorySegment(UNKNOWN_CHANNEL);\n \t}\n \n-\t@Nullable\n-\tprivate MemorySegment requestMemorySegmentFromGlobal() {\n+\tprivate boolean requestMemorySegmentFromGlobal() {\n+\t\tassert Thread.holdsLock(availableMemorySegments);\n+\n+\t\tif (isRequestedSizeReached()) {\n+\t\t\treturn false;\n+\t\t}\n+\n+\t\tMemorySegment segment = networkBufferPool.requestMemorySegment();\n+\t\tif (segment != null) {\n+\t\t\tavailableMemorySegments.add(segment);\n+\t\t\tnumberOfRequestedMemorySegments++;\n+\t\t\treturn true;\n+\t\t}\n+\t\treturn false;\n+\t}\n+\n+\tprivate boolean isRequestedSizeReached() {\n+\t\treturn numberOfRequestedMemorySegments >= currentPoolSize;\n+\t}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "df08363e00e50b79ff2492f344a4cef335699b17"}, "originalPosition": 174}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "df08363e00e50b79ff2492f344a4cef335699b17", "author": {"user": {"login": "AHeise", "name": "Arvid Heise"}}, "url": "https://github.com/apache/flink/commit/df08363e00e50b79ff2492f344a4cef335699b17", "committedDate": "2020-09-30T07:19:20Z", "message": "[FLINK-16972][network] LocalBufferPool eagerly fetches global segments to ensure proper availability.\n\nBefore this commit, availability of LocalBufferPool depended on a the availability of a shared NetworkBufferPool. However, if multiple LocalBufferPools simultaneously are available only because the NetworkBufferPool becomes available with one segment, only one of the LocalBufferPools is truly available (the one that actually acquires this segment).\n\nThe solution in this commit is to define availability only through the guaranteed ability to provide a memory segment to the consumer. If a LocalBufferPool runs out of local segments it will become unavailable until it receives a segment from the NetworkBufferPool. To minimize unavailability, LocalBufferPool first tries to eagerly fetch new segments before declaring unavailability and if that fails, the local pool subscribes to the availability to the network pool to restore availability asap.\n\nAdditionally, LocalBufferPool would switch to unavailable only after it could not serve a requested memory segment. For requestBufferBuilderBlocking that is too late as it entered the blocking loop already.\n\nFinally, LocalBufferPool now permanently holds at least one buffer. To reflect that, the number of required segments needs to be at least one, which matches all usages in production code. A few test needed to be adjusted to properly capture the new requirement."}, "afterCommit": {"oid": "8cc24be5102c67d50c2f3555195cab48210339d4", "author": {"user": {"login": "AHeise", "name": "Arvid Heise"}}, "url": "https://github.com/apache/flink/commit/8cc24be5102c67d50c2f3555195cab48210339d4", "committedDate": "2020-09-30T12:51:48Z", "message": "[FLINK-16972][network] LocalBufferPool eagerly fetches global segments to ensure proper availability.\n\nBefore this commit, availability of LocalBufferPool depended on a the availability of a shared NetworkBufferPool. However, if multiple LocalBufferPools simultaneously are available only because the NetworkBufferPool becomes available with one segment, only one of the LocalBufferPools is truly available (the one that actually acquires this segment).\n\nThe solution in this commit is to define availability only through the guaranteed ability to provide a memory segment to the consumer. If a LocalBufferPool runs out of local segments it will become unavailable until it receives a segment from the NetworkBufferPool. To minimize unavailability, LocalBufferPool first tries to eagerly fetch new segments before declaring unavailability and if that fails, the local pool subscribes to the availability to the network pool to restore availability asap.\n\nAdditionally, LocalBufferPool would switch to unavailable only after it could not serve a requested memory segment. For requestBufferBuilderBlocking that is too late as it entered the blocking loop already.\n\nFinally, LocalBufferPool now permanently holds at least one buffer. To reflect that, the number of required segments needs to be at least one, which matches all usages in production code. A few test needed to be adjusted to properly capture the new requirement."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "8cc24be5102c67d50c2f3555195cab48210339d4", "author": {"user": {"login": "AHeise", "name": "Arvid Heise"}}, "url": "https://github.com/apache/flink/commit/8cc24be5102c67d50c2f3555195cab48210339d4", "committedDate": "2020-09-30T12:51:48Z", "message": "[FLINK-16972][network] LocalBufferPool eagerly fetches global segments to ensure proper availability.\n\nBefore this commit, availability of LocalBufferPool depended on a the availability of a shared NetworkBufferPool. However, if multiple LocalBufferPools simultaneously are available only because the NetworkBufferPool becomes available with one segment, only one of the LocalBufferPools is truly available (the one that actually acquires this segment).\n\nThe solution in this commit is to define availability only through the guaranteed ability to provide a memory segment to the consumer. If a LocalBufferPool runs out of local segments it will become unavailable until it receives a segment from the NetworkBufferPool. To minimize unavailability, LocalBufferPool first tries to eagerly fetch new segments before declaring unavailability and if that fails, the local pool subscribes to the availability to the network pool to restore availability asap.\n\nAdditionally, LocalBufferPool would switch to unavailable only after it could not serve a requested memory segment. For requestBufferBuilderBlocking that is too late as it entered the blocking loop already.\n\nFinally, LocalBufferPool now permanently holds at least one buffer. To reflect that, the number of required segments needs to be at least one, which matches all usages in production code. A few test needed to be adjusted to properly capture the new requirement."}, "afterCommit": {"oid": "159747dae3d424c8dfa0a7b6286d446e1a111c61", "author": {"user": {"login": "AHeise", "name": "Arvid Heise"}}, "url": "https://github.com/apache/flink/commit/159747dae3d424c8dfa0a7b6286d446e1a111c61", "committedDate": "2020-09-30T14:04:20Z", "message": "[FLINK-16972][network] LocalBufferPool eagerly fetches global segments to ensure proper availability.\n\nBefore this commit, availability of LocalBufferPool depended on a the availability of a shared NetworkBufferPool. However, if multiple LocalBufferPools simultaneously are available only because the NetworkBufferPool becomes available with one segment, only one of the LocalBufferPools is truly available (the one that actually acquires this segment).\n\nThe solution in this commit is to define availability only through the guaranteed ability to provide a memory segment to the consumer. If a LocalBufferPool runs out of local segments it will become unavailable until it receives a segment from the NetworkBufferPool. To minimize unavailability, LocalBufferPool first tries to eagerly fetch new segments before declaring unavailability and if that fails, the local pool subscribes to the availability to the network pool to restore availability asap.\n\nAdditionally, LocalBufferPool would switch to unavailable only after it could not serve a requested memory segment. For requestBufferBuilderBlocking that is too late as it entered the blocking loop already.\n\nFinally, LocalBufferPool now permanently holds at least one buffer. To reflect that, the number of required segments needs to be at least one, which matches all usages in production code. A few test needed to be adjusted to properly capture the new requirement."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "159747dae3d424c8dfa0a7b6286d446e1a111c61", "author": {"user": {"login": "AHeise", "name": "Arvid Heise"}}, "url": "https://github.com/apache/flink/commit/159747dae3d424c8dfa0a7b6286d446e1a111c61", "committedDate": "2020-09-30T14:04:20Z", "message": "[FLINK-16972][network] LocalBufferPool eagerly fetches global segments to ensure proper availability.\n\nBefore this commit, availability of LocalBufferPool depended on a the availability of a shared NetworkBufferPool. However, if multiple LocalBufferPools simultaneously are available only because the NetworkBufferPool becomes available with one segment, only one of the LocalBufferPools is truly available (the one that actually acquires this segment).\n\nThe solution in this commit is to define availability only through the guaranteed ability to provide a memory segment to the consumer. If a LocalBufferPool runs out of local segments it will become unavailable until it receives a segment from the NetworkBufferPool. To minimize unavailability, LocalBufferPool first tries to eagerly fetch new segments before declaring unavailability and if that fails, the local pool subscribes to the availability to the network pool to restore availability asap.\n\nAdditionally, LocalBufferPool would switch to unavailable only after it could not serve a requested memory segment. For requestBufferBuilderBlocking that is too late as it entered the blocking loop already.\n\nFinally, LocalBufferPool now permanently holds at least one buffer. To reflect that, the number of required segments needs to be at least one, which matches all usages in production code. A few test needed to be adjusted to properly capture the new requirement."}, "afterCommit": {"oid": "01cedbe068a6c964e003e4d5d6b2e179eadd5365", "author": {"user": {"login": "AHeise", "name": "Arvid Heise"}}, "url": "https://github.com/apache/flink/commit/01cedbe068a6c964e003e4d5d6b2e179eadd5365", "committedDate": "2020-09-30T14:05:39Z", "message": "[FLINK-16972][network] LocalBufferPool eagerly fetches global segments to ensure proper availability.\n\nBefore this commit, availability of LocalBufferPool depended on a the availability of a shared NetworkBufferPool. However, if multiple LocalBufferPools simultaneously are available only because the NetworkBufferPool becomes available with one segment, only one of the LocalBufferPools is truly available (the one that actually acquires this segment).\n\nThe solution in this commit is to define availability only through the guaranteed ability to provide a memory segment to the consumer. If a LocalBufferPool runs out of local segments it will become unavailable until it receives a segment from the NetworkBufferPool. To minimize unavailability, LocalBufferPool first tries to eagerly fetch new segments before declaring unavailability and if that fails, the local pool subscribes to the availability to the network pool to restore availability asap.\n\nAdditionally, LocalBufferPool would switch to unavailable only after it could not serve a requested memory segment. For requestBufferBuilderBlocking that is too late as it entered the blocking loop already.\n\nFinally, LocalBufferPool now permanently holds at least one buffer. To reflect that, the number of required segments needs to be at least one, which matches all usages in production code. A few test needed to be adjusted to properly capture the new requirement."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "01cedbe068a6c964e003e4d5d6b2e179eadd5365", "author": {"user": {"login": "AHeise", "name": "Arvid Heise"}}, "url": "https://github.com/apache/flink/commit/01cedbe068a6c964e003e4d5d6b2e179eadd5365", "committedDate": "2020-09-30T14:05:39Z", "message": "[FLINK-16972][network] LocalBufferPool eagerly fetches global segments to ensure proper availability.\n\nBefore this commit, availability of LocalBufferPool depended on a the availability of a shared NetworkBufferPool. However, if multiple LocalBufferPools simultaneously are available only because the NetworkBufferPool becomes available with one segment, only one of the LocalBufferPools is truly available (the one that actually acquires this segment).\n\nThe solution in this commit is to define availability only through the guaranteed ability to provide a memory segment to the consumer. If a LocalBufferPool runs out of local segments it will become unavailable until it receives a segment from the NetworkBufferPool. To minimize unavailability, LocalBufferPool first tries to eagerly fetch new segments before declaring unavailability and if that fails, the local pool subscribes to the availability to the network pool to restore availability asap.\n\nAdditionally, LocalBufferPool would switch to unavailable only after it could not serve a requested memory segment. For requestBufferBuilderBlocking that is too late as it entered the blocking loop already.\n\nFinally, LocalBufferPool now permanently holds at least one buffer. To reflect that, the number of required segments needs to be at least one, which matches all usages in production code. A few test needed to be adjusted to properly capture the new requirement."}, "afterCommit": {"oid": "16935a2bb218146d4f6880f314844cc607197b8b", "author": {"user": {"login": "AHeise", "name": "Arvid Heise"}}, "url": "https://github.com/apache/flink/commit/16935a2bb218146d4f6880f314844cc607197b8b", "committedDate": "2020-09-30T17:22:22Z", "message": "[FLINK-16972][network] LocalBufferPool eagerly fetches global segments to ensure proper availability.\n\nBefore this commit, availability of LocalBufferPool depended on a the availability of a shared NetworkBufferPool. However, if multiple LocalBufferPools simultaneously are available only because the NetworkBufferPool becomes available with one segment, only one of the LocalBufferPools is truly available (the one that actually acquires this segment).\n\nThe solution in this commit is to define availability only through the guaranteed ability to provide a memory segment to the consumer. If a LocalBufferPool runs out of local segments it will become unavailable until it receives a segment from the NetworkBufferPool. To minimize unavailability, LocalBufferPool first tries to eagerly fetch new segments before declaring unavailability and if that fails, the local pool subscribes to the availability to the network pool to restore availability asap.\n\nAdditionally, LocalBufferPool would switch to unavailable only after it could not serve a requested memory segment. For requestBufferBuilderBlocking that is too late as it entered the blocking loop already.\n\nFinally, LocalBufferPool now permanently holds at least one buffer. To reflect that, the number of required segments needs to be at least one, which matches all usages in production code. A few test needed to be adjusted to properly capture the new requirement."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6f8aa8fb436ef7a136003389f006a2d507a4314e", "author": {"user": {"login": "AHeise", "name": "Arvid Heise"}}, "url": "https://github.com/apache/flink/commit/6f8aa8fb436ef7a136003389f006a2d507a4314e", "committedDate": "2020-09-30T21:43:36Z", "message": "[FLINK-19400][network] Removed legacy BufferPoolOwner.\n\nAll implementations are doing only noop operations and it makes implementation of LocalBufferPool seemingly harder.\nThe removal of BufferPoolOwner also eliminates the only source of IOException during NetworkBufferPool#redistributeBuffers and as such respective tests are also removed. The next commit cleans up exception handling."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6eefd3c7e625b1aac0ca9168b5d9e33caa219d53", "author": {"user": {"login": "AHeise", "name": "Arvid Heise"}}, "url": "https://github.com/apache/flink/commit/6eefd3c7e625b1aac0ca9168b5d9e33caa219d53", "committedDate": "2020-09-30T21:43:36Z", "message": "[FLINK-19400][network] Remove superfluous IOExceptions."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "72507dd1819b1f04b6b7c9c7ef2b43582aa4b134", "author": {"user": {"login": "AHeise", "name": "Arvid Heise"}}, "url": "https://github.com/apache/flink/commit/72507dd1819b1f04b6b7c9c7ef2b43582aa4b134", "committedDate": "2020-09-30T21:43:36Z", "message": "[FLINK-16972][network] Correctly enforcing subpartition quota in LocalBufferPool.\n\nPreviously, it was possible for a subpartition to acquire (maxBuffersPerChannel+1) buffers, before LocalBufferPool became unavailable.\nAlso, requestMemorySegment does not return a buffer for a channel over quota at all, making blocking requests blocking for respective subpartitions."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fce4a20dcb376d6c0df3671c228a872045f64606", "author": {"user": {"login": "AHeise", "name": "Arvid Heise"}}, "url": "https://github.com/apache/flink/commit/fce4a20dcb376d6c0df3671c228a872045f64606", "committedDate": "2020-09-30T21:44:47Z", "message": "[FLINK-16972][network] LocalBufferPool eagerly fetches global segments to ensure proper availability.\n\nBefore this commit, availability of LocalBufferPool depended on a the availability of a shared NetworkBufferPool. However, if multiple LocalBufferPools simultaneously are available only because the NetworkBufferPool becomes available with one segment, only one of the LocalBufferPools is truly available (the one that actually acquires this segment).\n\nThe solution in this commit is to define availability only through the guaranteed ability to provide a memory segment to the consumer. If a LocalBufferPool runs out of local segments it will become unavailable until it receives a segment from the NetworkBufferPool. To minimize unavailability, LocalBufferPool first tries to eagerly fetch new segments before declaring unavailability and if that fails, the local pool subscribes to the availability to the network pool to restore availability asap.\n\nAdditionally, LocalBufferPool would switch to unavailable only after it could not serve a requested memory segment. For requestBufferBuilderBlocking that is too late as it entered the blocking loop already.\n\nFinally, LocalBufferPool now permanently holds at least one buffer. To reflect that, the number of required segments needs to be at least one, which matches all usages in production code. A few test needed to be adjusted to properly capture the new requirement."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "16935a2bb218146d4f6880f314844cc607197b8b", "author": {"user": {"login": "AHeise", "name": "Arvid Heise"}}, "url": "https://github.com/apache/flink/commit/16935a2bb218146d4f6880f314844cc607197b8b", "committedDate": "2020-09-30T17:22:22Z", "message": "[FLINK-16972][network] LocalBufferPool eagerly fetches global segments to ensure proper availability.\n\nBefore this commit, availability of LocalBufferPool depended on a the availability of a shared NetworkBufferPool. However, if multiple LocalBufferPools simultaneously are available only because the NetworkBufferPool becomes available with one segment, only one of the LocalBufferPools is truly available (the one that actually acquires this segment).\n\nThe solution in this commit is to define availability only through the guaranteed ability to provide a memory segment to the consumer. If a LocalBufferPool runs out of local segments it will become unavailable until it receives a segment from the NetworkBufferPool. To minimize unavailability, LocalBufferPool first tries to eagerly fetch new segments before declaring unavailability and if that fails, the local pool subscribes to the availability to the network pool to restore availability asap.\n\nAdditionally, LocalBufferPool would switch to unavailable only after it could not serve a requested memory segment. For requestBufferBuilderBlocking that is too late as it entered the blocking loop already.\n\nFinally, LocalBufferPool now permanently holds at least one buffer. To reflect that, the number of required segments needs to be at least one, which matches all usages in production code. A few test needed to be adjusted to properly capture the new requirement."}, "afterCommit": {"oid": "fce4a20dcb376d6c0df3671c228a872045f64606", "author": {"user": {"login": "AHeise", "name": "Arvid Heise"}}, "url": "https://github.com/apache/flink/commit/fce4a20dcb376d6c0df3671c228a872045f64606", "committedDate": "2020-09-30T21:44:47Z", "message": "[FLINK-16972][network] LocalBufferPool eagerly fetches global segments to ensure proper availability.\n\nBefore this commit, availability of LocalBufferPool depended on a the availability of a shared NetworkBufferPool. However, if multiple LocalBufferPools simultaneously are available only because the NetworkBufferPool becomes available with one segment, only one of the LocalBufferPools is truly available (the one that actually acquires this segment).\n\nThe solution in this commit is to define availability only through the guaranteed ability to provide a memory segment to the consumer. If a LocalBufferPool runs out of local segments it will become unavailable until it receives a segment from the NetworkBufferPool. To minimize unavailability, LocalBufferPool first tries to eagerly fetch new segments before declaring unavailability and if that fails, the local pool subscribes to the availability to the network pool to restore availability asap.\n\nAdditionally, LocalBufferPool would switch to unavailable only after it could not serve a requested memory segment. For requestBufferBuilderBlocking that is too late as it entered the blocking loop already.\n\nFinally, LocalBufferPool now permanently holds at least one buffer. To reflect that, the number of required segments needs to be at least one, which matches all usages in production code. A few test needed to be adjusted to properly capture the new requirement."}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3440, "cost": 1, "resetAt": "2021-10-28T16:48:13Z"}}}