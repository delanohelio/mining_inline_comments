{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDE3MjY5NzM1", "number": 12120, "title": "[FLINK-17547] Support unaligned checkpoints for records spilled to files", "bodyText": "What is the purpose of the change\nImplement SpillingAdaptiveSpanningRecordDeserializer.getUnconsumedBuffer for records spilled to file (currently, UnsupportedOperationException is thrown).\nBrief change log\n\nrefactor SpillingAdaptiveSpanningRecordDeserializer; extract SpanningWrapper\nextract RefCountingFile and move it to flink-core\nimplement SpanningWrapper.getUnconsumedBuffer using RefCountingFile\n\nVerifying this change\n\nAdded unit tests:  SpanningWrapperTest, MemorySegmentFactoryTest, CloseableIteratorTest\nExisting integration test: ChannelPersistenceITCase\n\nDoes this pull request potentially affect one of the following parts:\n\nDependencies (does it add or upgrade a dependency): no\nThe public API, i.e., is any changed class annotated with @Public(Evolving): no\nThe serializers: no\nThe runtime per-record code paths (performance sensitive): no\nAnything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: no\nThe S3 file system connector: no\n\nDocumentation\n\nDoes this pull request introduce a new feature? no\nIf yes, how is the feature documented? not applicable", "createdAt": "2020-05-13T10:30:23Z", "url": "https://github.com/apache/flink/pull/12120", "merged": true, "mergeCommit": {"oid": "2aacb62c292bb9e642e792ae99b6c2e6aac9e495"}, "closed": true, "closedAt": "2020-05-19T13:59:44Z", "author": {"login": "rkhachatryan"}, "timelineItems": {"totalCount": 26, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcg8BOXAFqTQxMTAwNDE5Nw==", "endCursor": "Y3Vyc29yOnYyOpPPAAABci05akgFqTQxNDQ3MTI1NA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDExMDA0MTk3", "url": "https://github.com/apache/flink/pull/12120#pullrequestreview-411004197", "createdAt": "2020-05-13T14:49:41Z", "commit": {"oid": "75557c37eec1d0fddebf0f229b926fa884ab7379"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 24, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xM1QxNDo0OTo0MVrOGU1XVA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xM1QxNzowNTowN1rOGU7PlQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDQ5OTAyOA==", "bodyText": "Shouldn't this be the other way around? throwable.addSuppressed(e)", "url": "https://github.com/apache/flink/pull/12120#discussion_r424499028", "createdAt": "2020-05-13T14:49:41Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/ChannelStateWriteRequest.java", "diffHunk": "@@ -46,8 +48,27 @@ static CheckpointInProgressRequest completeOutput(long checkpointId) {\n \t\treturn new CheckpointInProgressRequest(\"completeOutput\", checkpointId, ChannelStateCheckpointWriter::completeOutput, false);\n \t}\n \n-\tstatic ChannelStateWriteRequest write(long checkpointId, InputChannelInfo info, Buffer... flinkBuffers) {\n-\t\treturn new CheckpointInProgressRequest(\"writeInput\", checkpointId, writer -> writer.writeInput(info, flinkBuffers), recycle(flinkBuffers), false);\n+\tstatic ChannelStateWriteRequest write(long checkpointId, InputChannelInfo info, CloseableIterator<Buffer> iterator) {\n+\t\treturn new CheckpointInProgressRequest(\n+\t\t\t\"writeInput\",\n+\t\t\tcheckpointId,\n+\t\t\twriter -> {\n+\t\t\t\twhile (iterator.hasNext()) {\n+\t\t\t\t\tBuffer buffer = iterator.next();\n+\t\t\t\t\tcheckArgument(buffer.isBuffer());\n+\t\t\t\t\twriter.writeInput(info, buffer);\n+\t\t\t\t}\n+\t\t\t},\n+\t\t\tthrowable -> {\n+\t\t\t\ttry {\n+\t\t\t\t\titerator.close();\n+\t\t\t\t} catch (Exception e) {\n+\t\t\t\t\tfinal RuntimeException re = new RuntimeException(\"unable to close iterator\", e);\n+\t\t\t\t\tre.addSuppressed(throwable);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "75557c37eec1d0fddebf0f229b926fa884ab7379"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDQ5OTI2Mg==", "bodyText": "Couldn't this be ThrowingConsumer so that we avoid RuntimeException?", "url": "https://github.com/apache/flink/pull/12120#discussion_r424499262", "createdAt": "2020-05-13T14:49:56Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/ChannelStateWriteRequest.java", "diffHunk": "@@ -46,8 +48,27 @@ static CheckpointInProgressRequest completeOutput(long checkpointId) {\n \t\treturn new CheckpointInProgressRequest(\"completeOutput\", checkpointId, ChannelStateCheckpointWriter::completeOutput, false);\n \t}\n \n-\tstatic ChannelStateWriteRequest write(long checkpointId, InputChannelInfo info, Buffer... flinkBuffers) {\n-\t\treturn new CheckpointInProgressRequest(\"writeInput\", checkpointId, writer -> writer.writeInput(info, flinkBuffers), recycle(flinkBuffers), false);\n+\tstatic ChannelStateWriteRequest write(long checkpointId, InputChannelInfo info, CloseableIterator<Buffer> iterator) {\n+\t\treturn new CheckpointInProgressRequest(\n+\t\t\t\"writeInput\",\n+\t\t\tcheckpointId,\n+\t\t\twriter -> {\n+\t\t\t\twhile (iterator.hasNext()) {\n+\t\t\t\t\tBuffer buffer = iterator.next();\n+\t\t\t\t\tcheckArgument(buffer.isBuffer());\n+\t\t\t\t\twriter.writeInput(info, buffer);\n+\t\t\t\t}\n+\t\t\t},\n+\t\t\tthrowable -> {\n+\t\t\t\ttry {\n+\t\t\t\t\titerator.close();\n+\t\t\t\t} catch (Exception e) {\n+\t\t\t\t\tfinal RuntimeException re = new RuntimeException(\"unable to close iterator\", e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "75557c37eec1d0fddebf0f229b926fa884ab7379"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDUwMDMwNg==", "bodyText": "I think inside ChannelStateWriteRequestExecutorImpl#run exceptions from this consumer, invoked via cleanupRequests() can be swallowed/ignored.", "url": "https://github.com/apache/flink/pull/12120#discussion_r424500306", "createdAt": "2020-05-13T14:51:15Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/ChannelStateWriteRequest.java", "diffHunk": "@@ -46,8 +48,27 @@ static CheckpointInProgressRequest completeOutput(long checkpointId) {\n \t\treturn new CheckpointInProgressRequest(\"completeOutput\", checkpointId, ChannelStateCheckpointWriter::completeOutput, false);\n \t}\n \n-\tstatic ChannelStateWriteRequest write(long checkpointId, InputChannelInfo info, Buffer... flinkBuffers) {\n-\t\treturn new CheckpointInProgressRequest(\"writeInput\", checkpointId, writer -> writer.writeInput(info, flinkBuffers), recycle(flinkBuffers), false);\n+\tstatic ChannelStateWriteRequest write(long checkpointId, InputChannelInfo info, CloseableIterator<Buffer> iterator) {\n+\t\treturn new CheckpointInProgressRequest(\n+\t\t\t\"writeInput\",\n+\t\t\tcheckpointId,\n+\t\t\twriter -> {\n+\t\t\t\twhile (iterator.hasNext()) {\n+\t\t\t\t\tBuffer buffer = iterator.next();\n+\t\t\t\t\tcheckArgument(buffer.isBuffer());\n+\t\t\t\t\twriter.writeInput(info, buffer);\n+\t\t\t\t}\n+\t\t\t},\n+\t\t\tthrowable -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "75557c37eec1d0fddebf0f229b926fa884ab7379"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDUxNTcwMg==", "bodyText": "It looks like a bit of an overkill to introduce flatten for just this single use case. Especially that wrapInt produces unpooled buffer/segment (with no-op recycle).\nCan not we return wrapInt(recordLength) as first buffer from FileBasedBufferIterator?", "url": "https://github.com/apache/flink/pull/12120#discussion_r424515702", "createdAt": "2020-05-13T15:10:53Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/api/serialization/SpanningWrapper.java", "diffHunk": "@@ -183,22 +196,26 @@ private void updateLength(int length) throws IOException {\n \t@Override\n \tpublic CloseableIterator<Buffer> getUnconsumedSegment() throws IOException {\n \t\tif (isReadingLength()) {\n-\t\t\treturn singleBufferIterator(copyLengthBuffer());\n+\t\t\treturn singleBufferIterator(wrapCopy(lengthBuffer.array(), 0, lengthBuffer.position()));\n \t\t} else if (isAboveSpillingThreshold()) {\n-\t\t\tthrow new UnsupportedOperationException(\"Unaligned checkpoint currently do not support spilled records.\");\n+\t\t\treturn createSpilledDataIterator();\n \t\t} else if (recordLength == -1) {\n-\t\t\treturn CloseableIterator.empty(); // no remaining partial length or data\n+\t\t\treturn empty(); // no remaining partial length or data\n \t\t} else {\n \t\t\treturn singleBufferIterator(copyDataBuffer());\n \t\t}\n \t}\n \n-\tprivate MemorySegment copyLengthBuffer() {\n-\t\tint position = lengthBuffer.position();\n-\t\tMemorySegment segment = MemorySegmentFactory.allocateUnpooledSegment(position);\n-\t\tlengthBuffer.position(0);\n-\t\tsegment.put(0, lengthBuffer, position);\n-\t\treturn segment;\n+\t@SuppressWarnings(\"unchecked\")\n+\tprivate CloseableIterator<Buffer> createSpilledDataIterator() throws IOException {\n+\t\tif (spillingChannel != null) {\n+\t\t\tspillingChannel.force(false);\n+\t\t}\n+\t\treturn CloseableIterator.flatten(\n+\t\t\ttoSingleBufferIterator(wrapInt(recordLength)),\n+\t\t\tnew FileBasedBufferIterator(spillFile, min(accumulatedRecordBytes, recordLength), fileBufferSize),\n+\t\t\tleftOverData == null ? empty() : toSingleBufferIterator(wrapCopy(leftOverData.getArray(), leftOverStart, leftOverLimit))\n+\t\t);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "91982f5d59af5b6540ffa4052050b18a56f92b25"}, "originalPosition": 117}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDUxNzk5MQ==", "bodyText": "\ud83d\udc4d", "url": "https://github.com/apache/flink/pull/12120#discussion_r424517991", "createdAt": "2020-05-13T15:13:58Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/api/serialization/SpanningWrapper.java", "diffHunk": "@@ -183,22 +196,26 @@ private void updateLength(int length) throws IOException {\n \t@Override\n \tpublic CloseableIterator<Buffer> getUnconsumedSegment() throws IOException {\n \t\tif (isReadingLength()) {\n-\t\t\treturn singleBufferIterator(copyLengthBuffer());\n+\t\t\treturn singleBufferIterator(wrapCopy(lengthBuffer.array(), 0, lengthBuffer.position()));\n \t\t} else if (isAboveSpillingThreshold()) {\n-\t\t\tthrow new UnsupportedOperationException(\"Unaligned checkpoint currently do not support spilled records.\");\n+\t\t\treturn createSpilledDataIterator();\n \t\t} else if (recordLength == -1) {\n-\t\t\treturn CloseableIterator.empty(); // no remaining partial length or data\n+\t\t\treturn empty(); // no remaining partial length or data\n \t\t} else {\n \t\t\treturn singleBufferIterator(copyDataBuffer());\n \t\t}\n \t}\n \n-\tprivate MemorySegment copyLengthBuffer() {\n-\t\tint position = lengthBuffer.position();\n-\t\tMemorySegment segment = MemorySegmentFactory.allocateUnpooledSegment(position);\n-\t\tlengthBuffer.position(0);\n-\t\tsegment.put(0, lengthBuffer, position);\n-\t\treturn segment;\n+\t@SuppressWarnings(\"unchecked\")\n+\tprivate CloseableIterator<Buffer> createSpilledDataIterator() throws IOException {\n+\t\tif (spillingChannel != null) {\n+\t\t\tspillingChannel.force(false);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "91982f5d59af5b6540ffa4052050b18a56f92b25"}, "originalPosition": 111}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDUzNDc1NA==", "bodyText": "missing iterator.close() on the happy path?", "url": "https://github.com/apache/flink/pull/12120#discussion_r424534754", "createdAt": "2020-05-13T15:36:04Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/ChannelStateWriteRequest.java", "diffHunk": "@@ -46,8 +48,27 @@ static CheckpointInProgressRequest completeOutput(long checkpointId) {\n \t\treturn new CheckpointInProgressRequest(\"completeOutput\", checkpointId, ChannelStateCheckpointWriter::completeOutput, false);\n \t}\n \n-\tstatic ChannelStateWriteRequest write(long checkpointId, InputChannelInfo info, Buffer... flinkBuffers) {\n-\t\treturn new CheckpointInProgressRequest(\"writeInput\", checkpointId, writer -> writer.writeInput(info, flinkBuffers), recycle(flinkBuffers), false);\n+\tstatic ChannelStateWriteRequest write(long checkpointId, InputChannelInfo info, CloseableIterator<Buffer> iterator) {\n+\t\treturn new CheckpointInProgressRequest(\n+\t\t\t\"writeInput\",\n+\t\t\tcheckpointId,\n+\t\t\twriter -> {\n+\t\t\t\twhile (iterator.hasNext()) {\n+\t\t\t\t\tBuffer buffer = iterator.next();\n+\t\t\t\t\tcheckArgument(buffer.isBuffer());\n+\t\t\t\t\twriter.writeInput(info, buffer);\n+\t\t\t\t}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "75557c37eec1d0fddebf0f229b926fa884ab7379"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDUzNjE1MQ==", "bodyText": "512KB? I would expect even smaller values to be working reasonably well, but below 1MB the memory consumption of the extra buffer will be less then 20% of the spilling buffer, while 2MB could increase memory consumption by ~40%.", "url": "https://github.com/apache/flink/pull/12120#discussion_r424536151", "createdAt": "2020-05-13T15:37:45Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/api/serialization/SpanningWrapper.java", "diffHunk": "@@ -43,15 +46,18 @@\n \n import static java.lang.Math.max;\n import static java.lang.Math.min;\n+import static org.apache.flink.core.memory.MemorySegmentFactory.wrapCopy;\n+import static org.apache.flink.core.memory.MemorySegmentFactory.wrapInt;\n import static org.apache.flink.runtime.io.network.api.serialization.NonSpanningWrapper.singleBufferIterator;\n import static org.apache.flink.runtime.io.network.api.serialization.SpillingAdaptiveSpanningRecordDeserializer.LENGTH_BYTES;\n+import static org.apache.flink.util.CloseableIterator.empty;\n import static org.apache.flink.util.FileUtils.writeCompletely;\n import static org.apache.flink.util.IOUtils.closeAllQuietly;\n \n final class SpanningWrapper implements BufferWrapper {\n \n-\tprivate static final int THRESHOLD_FOR_SPILLING = 5 * 1024 * 1024; // 5 MiBytes\n-\tprivate static final int FILE_BUFFER_SIZE = 2 * 1024 * 1024;\n+\tprivate static final int DEFAULT_THRESHOLD_FOR_SPILLING = 5 * 1024 * 1024; // 5 MiBytes\n+\tprivate static final int DEFAULT_FILE_BUFFER_SIZE = 2 * 1024 * 1024;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "91982f5d59af5b6540ffa4052050b18a56f92b25"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDUzODc4Nw==", "bodyText": "What's the point of this interface? (It's not being used anywhere)", "url": "https://github.com/apache/flink/pull/12120#discussion_r424538787", "createdAt": "2020-05-13T15:41:20Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/api/serialization/SpillingAdaptiveSpanningRecordDeserializer.java", "diffHunk": "@@ -158,7 +113,10 @@ public void clear() {\n \n \t@Override\n \tpublic boolean hasUnfinishedData() {\n-\t\treturn this.nonSpanningWrapper.remaining() > 0 || this.spanningWrapper.getNumGatheredBytes() > 0;\n+\t\treturn this.nonSpanningWrapper.hasRemaining() || this.spanningWrapper.getNumGatheredBytes() > 0;\n \t}\n \n+\tinterface BufferWrapper {\n+\t\tOptional<MemorySegment> getUnconsumedSegment() throws IOException;\n+\t}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "670aab1cacb84a6403811dd959dffd2e1d03d889"}, "originalPosition": 142}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDUzOTcwNw==", "bodyText": "It's better if simpler/shorter branch goes first", "url": "https://github.com/apache/flink/pull/12120#discussion_r424539707", "createdAt": "2020-05-13T15:42:33Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/api/serialization/NonSpanningWrapper.java", "diffHunk": "@@ -47,27 +64,33 @@ void clear() {\n \t\tthis.position = 0;\n \t}\n \n-\tvoid initializeFromMemorySegment(MemorySegment seg, int position, int leftOverLimit) {\n+\tvoid initializeFromMemorySegment(MemorySegment seg, int position, int limit) {\n \t\tthis.segment = seg;\n \t\tthis.position = position;\n-\t\tthis.limit = leftOverLimit;\n+\t\tthis.limit = limit;\n \t}\n \n-\tOptional<MemorySegment> getUnconsumedSegment() {\n-\t\tif (remaining() == 0) {\n+\t@Override\n+\tpublic Optional<MemorySegment> getUnconsumedSegment() {\n+\t\tif (hasRemaining()) {\n+\t\t\tMemorySegment target = MemorySegmentFactory.allocateUnpooledSegment(remaining());\n+\t\t\tsegment.copyTo(position, target, 0, remaining());\n+\t\t\treturn Optional.of(target);\n+\t\t} else {\n \t\t\treturn Optional.empty();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "670aab1cacb84a6403811dd959dffd2e1d03d889"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDU0OTYzNQ==", "bodyText": "This onPartialRecord is a bit of spaghetti dependency. Also it doesn't help that's it's a ThrowingConsumer so IDE support for finding implementations wouldn't work.\nWhy don't you return recordLen via some wrapper around DeserializationResult?", "url": "https://github.com/apache/flink/pull/12120#discussion_r424549635", "createdAt": "2020-05-13T15:55:55Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/api/serialization/NonSpanningWrapper.java", "diffHunk": "@@ -290,4 +313,55 @@ public int read(byte[] b, int off, int len) {\n \tpublic int read(byte[] b) {\n \t\treturn read(b, 0, b.length);\n \t}\n+\n+\tTuple2<ByteBuffer, Integer> wrapIntoByteBuffer() {\n+\t\tint numBytes = remaining();\n+\t\treturn Tuple2.of(segment.wrap(position, numBytes), numBytes);\n+\t}\n+\n+\tint copyTo(byte[] dst) {\n+\t\tfinal int numBytesChunk = remaining();\n+\t\tsegment.get(position, dst, 0, numBytesChunk);\n+\t\treturn numBytesChunk;\n+\t}\n+\n+\t/**\n+\t * We have an incomplete length add our part of the length to the length buffer.\n+\t */\n+\tvoid transferTo(ByteBuffer dst) {\n+\t\tsegment.get(position, dst, remaining());\n+\t\tclear();\n+\t}\n+\n+\tprivate DeserializationResult readInto(IOReadableWritable target) throws IOException {\n+\t\ttarget.read(this);\n+\n+\t\tint remaining = remaining();\n+\t\tif (remaining > 0) {\n+\t\t\treturn INTERMEDIATE_RECORD_FROM_BUFFER;\n+\t\t} else if (remaining == 0) {\n+\t\t\treturn LAST_RECORD_FROM_BUFFER;\n+\t\t} else {\n+\t\t\tthrow new IOException(BROKEN_SERIALIZATION_ERROR_MESSAGE, new IndexOutOfBoundsException(\"Remaining = \" + remaining));\n+\t\t}\n+\t}\n+\n+\tDeserializationResult getNextRecord(IOReadableWritable target, ThrowingConsumer<Integer, IOException> onPartialRecord) throws IOException {\n+\t\tint recordLen = readInt();\n+\t\tif (canReadRecord(recordLen)) {\n+\t\t\treturn readInto(target);\n+\t\t} else {\n+\t\t\tonPartialRecord.accept(recordLen);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "670aab1cacb84a6403811dd959dffd2e1d03d889"}, "originalPosition": 133}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDU1NzYwMA==", "bodyText": "Either the methods in the if conditions should be renamed to self-documenting ones, or the inlined comments should be preserved.\nBy looking at this right now, I'm not even sure which condition was mapped to which one (has the order of conditions been preserved?) after couple of minutes I figured it out, that the order was preserved, but since it wasn't obvious for me, it proves the point that something is missing here.", "url": "https://github.com/apache/flink/pull/12120#discussion_r424557600", "createdAt": "2020-05-13T16:06:53Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/api/serialization/SpillingAdaptiveSpanningRecordDeserializer.java", "diffHunk": "@@ -91,62 +86,22 @@ public DeserializationResult getNextRecord(T target) throws IOException {\n \t\t// this should be the majority of the cases for small records\n \t\t// for large records, this portion of the work is very small in comparison anyways\n \n-\t\tint nonSpanningRemaining = this.nonSpanningWrapper.remaining();\n-\n-\t\t// check if we can get a full length;\n-\t\tif (nonSpanningRemaining >= 4) {\n-\t\t\tint len = this.nonSpanningWrapper.readInt();\n-\n-\t\t\tif (len <= nonSpanningRemaining - 4) {\n-\t\t\t\t// we can get a full record from here\n-\t\t\t\ttry {\n-\t\t\t\t\ttarget.read(this.nonSpanningWrapper);\n-\n-\t\t\t\t\tint remaining = this.nonSpanningWrapper.remaining();\n-\t\t\t\t\tif (remaining > 0) {\n-\t\t\t\t\t\treturn DeserializationResult.INTERMEDIATE_RECORD_FROM_BUFFER;\n-\t\t\t\t\t}\n-\t\t\t\t\telse if (remaining == 0) {\n-\t\t\t\t\t\treturn DeserializationResult.LAST_RECORD_FROM_BUFFER;\n-\t\t\t\t\t}\n-\t\t\t\t\telse {\n-\t\t\t\t\t\tthrow new IndexOutOfBoundsException(\"Remaining = \" + remaining);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t\tcatch (IndexOutOfBoundsException e) {\n-\t\t\t\t\tthrow new IOException(BROKEN_SERIALIZATION_ERROR_MESSAGE, e);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\telse {\n-\t\t\t\t// we got the length, but we need the rest from the spanning deserializer\n-\t\t\t\t// and need to wait for more buffers\n-\t\t\t\tthis.spanningWrapper.initializeWithPartialRecord(this.nonSpanningWrapper, len);\n-\t\t\t\tthis.nonSpanningWrapper.clear();\n-\t\t\t\treturn DeserializationResult.PARTIAL_RECORD;\n-\t\t\t}\n-\t\t} else if (nonSpanningRemaining > 0) {\n-\t\t\t// we have an incomplete length\n-\t\t\t// add our part of the length to the length buffer\n-\t\t\tthis.spanningWrapper.initializeWithPartialLength(this.nonSpanningWrapper);\n-\t\t\tthis.nonSpanningWrapper.clear();\n-\t\t\treturn DeserializationResult.PARTIAL_RECORD;\n-\t\t}\n+\t\tif (nonSpanningWrapper.canReadLength()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "670aab1cacb84a6403811dd959dffd2e1d03d889"}, "originalPosition": 101}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDU1OTUzMQ==", "bodyText": "nonSpanningWrapper.canReadLength() -> !nonSpanningWrapper.hasCompleteLength()?\nnonSpanningWrapper.canReadLength() -> nonSpanningWrapper.hasCompleteLength()?", "url": "https://github.com/apache/flink/pull/12120#discussion_r424559531", "createdAt": "2020-05-13T16:09:45Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/api/serialization/SpillingAdaptiveSpanningRecordDeserializer.java", "diffHunk": "@@ -91,62 +86,22 @@ public DeserializationResult getNextRecord(T target) throws IOException {\n \t\t// this should be the majority of the cases for small records\n \t\t// for large records, this portion of the work is very small in comparison anyways\n \n-\t\tint nonSpanningRemaining = this.nonSpanningWrapper.remaining();\n-\n-\t\t// check if we can get a full length;\n-\t\tif (nonSpanningRemaining >= 4) {\n-\t\t\tint len = this.nonSpanningWrapper.readInt();\n-\n-\t\t\tif (len <= nonSpanningRemaining - 4) {\n-\t\t\t\t// we can get a full record from here\n-\t\t\t\ttry {\n-\t\t\t\t\ttarget.read(this.nonSpanningWrapper);\n-\n-\t\t\t\t\tint remaining = this.nonSpanningWrapper.remaining();\n-\t\t\t\t\tif (remaining > 0) {\n-\t\t\t\t\t\treturn DeserializationResult.INTERMEDIATE_RECORD_FROM_BUFFER;\n-\t\t\t\t\t}\n-\t\t\t\t\telse if (remaining == 0) {\n-\t\t\t\t\t\treturn DeserializationResult.LAST_RECORD_FROM_BUFFER;\n-\t\t\t\t\t}\n-\t\t\t\t\telse {\n-\t\t\t\t\t\tthrow new IndexOutOfBoundsException(\"Remaining = \" + remaining);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t\tcatch (IndexOutOfBoundsException e) {\n-\t\t\t\t\tthrow new IOException(BROKEN_SERIALIZATION_ERROR_MESSAGE, e);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\telse {\n-\t\t\t\t// we got the length, but we need the rest from the spanning deserializer\n-\t\t\t\t// and need to wait for more buffers\n-\t\t\t\tthis.spanningWrapper.initializeWithPartialRecord(this.nonSpanningWrapper, len);\n-\t\t\t\tthis.nonSpanningWrapper.clear();\n-\t\t\t\treturn DeserializationResult.PARTIAL_RECORD;\n-\t\t\t}\n-\t\t} else if (nonSpanningRemaining > 0) {\n-\t\t\t// we have an incomplete length\n-\t\t\t// add our part of the length to the length buffer\n-\t\t\tthis.spanningWrapper.initializeWithPartialLength(this.nonSpanningWrapper);\n-\t\t\tthis.nonSpanningWrapper.clear();\n-\t\t\treturn DeserializationResult.PARTIAL_RECORD;\n-\t\t}\n+\t\tif (nonSpanningWrapper.canReadLength()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "670aab1cacb84a6403811dd959dffd2e1d03d889"}, "originalPosition": 101}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDU2MTc1Nw==", "bodyText": "Again it would help quite a lot with understanding the code, if the onPartialRecord would be handled here, instead of inside the getNextRecord(). As it is, it's strange that nonSpanningWrapper is implicitly, but directly using spanningWrapper. That's almost by definition purpose of ***SpanningRecordDeserializer class.", "url": "https://github.com/apache/flink/pull/12120#discussion_r424561757", "createdAt": "2020-05-13T16:13:01Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/api/serialization/SpillingAdaptiveSpanningRecordDeserializer.java", "diffHunk": "@@ -91,62 +86,22 @@ public DeserializationResult getNextRecord(T target) throws IOException {\n \t\t// this should be the majority of the cases for small records\n \t\t// for large records, this portion of the work is very small in comparison anyways\n \n-\t\tint nonSpanningRemaining = this.nonSpanningWrapper.remaining();\n-\n-\t\t// check if we can get a full length;\n-\t\tif (nonSpanningRemaining >= 4) {\n-\t\t\tint len = this.nonSpanningWrapper.readInt();\n-\n-\t\t\tif (len <= nonSpanningRemaining - 4) {\n-\t\t\t\t// we can get a full record from here\n-\t\t\t\ttry {\n-\t\t\t\t\ttarget.read(this.nonSpanningWrapper);\n-\n-\t\t\t\t\tint remaining = this.nonSpanningWrapper.remaining();\n-\t\t\t\t\tif (remaining > 0) {\n-\t\t\t\t\t\treturn DeserializationResult.INTERMEDIATE_RECORD_FROM_BUFFER;\n-\t\t\t\t\t}\n-\t\t\t\t\telse if (remaining == 0) {\n-\t\t\t\t\t\treturn DeserializationResult.LAST_RECORD_FROM_BUFFER;\n-\t\t\t\t\t}\n-\t\t\t\t\telse {\n-\t\t\t\t\t\tthrow new IndexOutOfBoundsException(\"Remaining = \" + remaining);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t\tcatch (IndexOutOfBoundsException e) {\n-\t\t\t\t\tthrow new IOException(BROKEN_SERIALIZATION_ERROR_MESSAGE, e);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\telse {\n-\t\t\t\t// we got the length, but we need the rest from the spanning deserializer\n-\t\t\t\t// and need to wait for more buffers\n-\t\t\t\tthis.spanningWrapper.initializeWithPartialRecord(this.nonSpanningWrapper, len);\n-\t\t\t\tthis.nonSpanningWrapper.clear();\n-\t\t\t\treturn DeserializationResult.PARTIAL_RECORD;\n-\t\t\t}\n-\t\t} else if (nonSpanningRemaining > 0) {\n-\t\t\t// we have an incomplete length\n-\t\t\t// add our part of the length to the length buffer\n-\t\t\tthis.spanningWrapper.initializeWithPartialLength(this.nonSpanningWrapper);\n-\t\t\tthis.nonSpanningWrapper.clear();\n-\t\t\treturn DeserializationResult.PARTIAL_RECORD;\n-\t\t}\n+\t\tif (nonSpanningWrapper.canReadLength()) {\n+\t\t\treturn nonSpanningWrapper.getNextRecord(\n+\t\t\t\ttarget,\n+\t\t\t\trecordLength -> spanningWrapper.transferFrom(nonSpanningWrapper, recordLength));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "670aab1cacb84a6403811dd959dffd2e1d03d889"}, "originalPosition": 104}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDU2NzM5OA==", "bodyText": "hasPartialLength()?", "url": "https://github.com/apache/flink/pull/12120#discussion_r424567398", "createdAt": "2020-05-13T16:21:35Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/api/serialization/SpillingAdaptiveSpanningRecordDeserializer.java", "diffHunk": "@@ -91,62 +86,22 @@ public DeserializationResult getNextRecord(T target) throws IOException {\n \t\t// this should be the majority of the cases for small records\n \t\t// for large records, this portion of the work is very small in comparison anyways\n \n-\t\tint nonSpanningRemaining = this.nonSpanningWrapper.remaining();\n-\n-\t\t// check if we can get a full length;\n-\t\tif (nonSpanningRemaining >= 4) {\n-\t\t\tint len = this.nonSpanningWrapper.readInt();\n-\n-\t\t\tif (len <= nonSpanningRemaining - 4) {\n-\t\t\t\t// we can get a full record from here\n-\t\t\t\ttry {\n-\t\t\t\t\ttarget.read(this.nonSpanningWrapper);\n-\n-\t\t\t\t\tint remaining = this.nonSpanningWrapper.remaining();\n-\t\t\t\t\tif (remaining > 0) {\n-\t\t\t\t\t\treturn DeserializationResult.INTERMEDIATE_RECORD_FROM_BUFFER;\n-\t\t\t\t\t}\n-\t\t\t\t\telse if (remaining == 0) {\n-\t\t\t\t\t\treturn DeserializationResult.LAST_RECORD_FROM_BUFFER;\n-\t\t\t\t\t}\n-\t\t\t\t\telse {\n-\t\t\t\t\t\tthrow new IndexOutOfBoundsException(\"Remaining = \" + remaining);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t\tcatch (IndexOutOfBoundsException e) {\n-\t\t\t\t\tthrow new IOException(BROKEN_SERIALIZATION_ERROR_MESSAGE, e);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\telse {\n-\t\t\t\t// we got the length, but we need the rest from the spanning deserializer\n-\t\t\t\t// and need to wait for more buffers\n-\t\t\t\tthis.spanningWrapper.initializeWithPartialRecord(this.nonSpanningWrapper, len);\n-\t\t\t\tthis.nonSpanningWrapper.clear();\n-\t\t\t\treturn DeserializationResult.PARTIAL_RECORD;\n-\t\t\t}\n-\t\t} else if (nonSpanningRemaining > 0) {\n-\t\t\t// we have an incomplete length\n-\t\t\t// add our part of the length to the length buffer\n-\t\t\tthis.spanningWrapper.initializeWithPartialLength(this.nonSpanningWrapper);\n-\t\t\tthis.nonSpanningWrapper.clear();\n-\t\t\treturn DeserializationResult.PARTIAL_RECORD;\n-\t\t}\n+\t\tif (nonSpanningWrapper.canReadLength()) {\n+\t\t\treturn nonSpanningWrapper.getNextRecord(\n+\t\t\t\ttarget,\n+\t\t\t\trecordLength -> spanningWrapper.transferFrom(nonSpanningWrapper, recordLength));\n \n-\t\t// spanning record case\n-\t\tif (this.spanningWrapper.hasFullRecord()) {\n-\t\t\t// get the full record\n-\t\t\ttarget.read(this.spanningWrapper.getInputView());\n+\t\t} else if (nonSpanningWrapper.hasRemaining()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "670aab1cacb84a6403811dd959dffd2e1d03d889"}, "originalPosition": 110}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDU3MTQ1Nw==", "bodyText": "Unnamed tuples are not very readable. I would say convert to a simple POJO class, but I think you do not need length here at all, right?\nByteBuffer buffer = partial.wrapIntoByteBuffer()\nint length = buffer.remaining()\nwriteCompletely(spillingChannel, buffer);\nreturn length;\n\n?", "url": "https://github.com/apache/flink/pull/12120#discussion_r424571457", "createdAt": "2020-05-13T16:27:34Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/api/serialization/SpanningWrapper.java", "diffHunk": "@@ -294,4 +290,15 @@ private static String randomString(Random random) {\n \t\trandom.nextBytes(bytes);\n \t\treturn StringUtils.byteToHexString(bytes);\n \t}\n+\n+\tprivate int spill(NonSpanningWrapper partial) throws IOException {\n+\t\tTuple2<ByteBuffer, Integer> bufferAndLen = partial.wrapIntoByteBuffer();\n+\t\twriteCompletely(spillingChannel, bufferAndLen.f0);\n+\t\treturn bufferAndLen.f1;\n+\t}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "670aab1cacb84a6403811dd959dffd2e1d03d889"}, "originalPosition": 381}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDU3NjM1NA==", "bodyText": "copyTo -> copyPartialDataTo? -> copyContentTo?", "url": "https://github.com/apache/flink/pull/12120#discussion_r424576354", "createdAt": "2020-05-13T16:35:18Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/api/serialization/NonSpanningWrapper.java", "diffHunk": "@@ -290,4 +313,55 @@ public int read(byte[] b, int off, int len) {\n \tpublic int read(byte[] b) {\n \t\treturn read(b, 0, b.length);\n \t}\n+\n+\tTuple2<ByteBuffer, Integer> wrapIntoByteBuffer() {\n+\t\tint numBytes = remaining();\n+\t\treturn Tuple2.of(segment.wrap(position, numBytes), numBytes);\n+\t}\n+\n+\tint copyTo(byte[] dst) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "670aab1cacb84a6403811dd959dffd2e1d03d889"}, "originalPosition": 101}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDU3ODY4NQ==", "bodyText": "What is the difference between transferTo and copyTo? It's not clear from the signatures.\nAlso it's strange (and unnecessary?) that transferTo is calling clear() from within, but copyTo doesn't. I would unify this and pull the clear() call out of here.", "url": "https://github.com/apache/flink/pull/12120#discussion_r424578685", "createdAt": "2020-05-13T16:38:44Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/api/serialization/NonSpanningWrapper.java", "diffHunk": "@@ -290,4 +313,55 @@ public int read(byte[] b, int off, int len) {\n \tpublic int read(byte[] b) {\n \t\treturn read(b, 0, b.length);\n \t}\n+\n+\tTuple2<ByteBuffer, Integer> wrapIntoByteBuffer() {\n+\t\tint numBytes = remaining();\n+\t\treturn Tuple2.of(segment.wrap(position, numBytes), numBytes);\n+\t}\n+\n+\tint copyTo(byte[] dst) {\n+\t\tfinal int numBytesChunk = remaining();\n+\t\tsegment.get(position, dst, 0, numBytesChunk);\n+\t\treturn numBytesChunk;\n+\t}\n+\n+\t/**\n+\t * We have an incomplete length add our part of the length to the length buffer.\n+\t */\n+\tvoid transferTo(ByteBuffer dst) {\n+\t\tsegment.get(position, dst, remaining());\n+\t\tclear();\n+\t}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "670aab1cacb84a6403811dd959dffd2e1d03d889"}, "originalPosition": 113}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDU3OTQzOQ==", "bodyText": "move this method below getNextRecord so the class could be read top to bottom?", "url": "https://github.com/apache/flink/pull/12120#discussion_r424579439", "createdAt": "2020-05-13T16:39:56Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/api/serialization/NonSpanningWrapper.java", "diffHunk": "@@ -290,4 +313,55 @@ public int read(byte[] b, int off, int len) {\n \tpublic int read(byte[] b) {\n \t\treturn read(b, 0, b.length);\n \t}\n+\n+\tTuple2<ByteBuffer, Integer> wrapIntoByteBuffer() {\n+\t\tint numBytes = remaining();\n+\t\treturn Tuple2.of(segment.wrap(position, numBytes), numBytes);\n+\t}\n+\n+\tint copyTo(byte[] dst) {\n+\t\tfinal int numBytesChunk = remaining();\n+\t\tsegment.get(position, dst, 0, numBytesChunk);\n+\t\treturn numBytesChunk;\n+\t}\n+\n+\t/**\n+\t * We have an incomplete length add our part of the length to the length buffer.\n+\t */\n+\tvoid transferTo(ByteBuffer dst) {\n+\t\tsegment.get(position, dst, remaining());\n+\t\tclear();\n+\t}\n+\n+\tprivate DeserializationResult readInto(IOReadableWritable target) throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "670aab1cacb84a6403811dd959dffd2e1d03d889"}, "originalPosition": 115}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDU4MDUxOA==", "bodyText": "startPosition (abbreviation \ud83d\udc4e )\nedit: using offset and numBytes in the arguments and position + remaining as local variables is confusing. It was less so in the old version, where it was obvious that those things are the same, but here this rename is mangled via readLengthIfNeeded - but that might be simplified if you return a single int from readLength (check comment below)", "url": "https://github.com/apache/flink/pull/12120#discussion_r424580518", "createdAt": "2020-05-13T16:41:36Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/api/serialization/SpanningWrapper.java", "diffHunk": "@@ -82,187 +94,171 @@ public SpanningWrapper(String[] tempDirs) {\n \t\tthis.buffer = initialBuffer;\n \t}\n \n-\tvoid initializeWithPartialRecord(NonSpanningWrapper partial, int nextRecordLength) throws IOException {\n-\t\t// set the length and copy what is available to the buffer\n-\t\tthis.recordLength = nextRecordLength;\n-\n-\t\tfinal int numBytesChunk = partial.remaining();\n-\n-\t\tif (nextRecordLength > THRESHOLD_FOR_SPILLING) {\n-\t\t\t// create a spilling channel and put the data there\n-\t\t\tthis.spillingChannel = createSpillingChannel();\n-\n-\t\t\tByteBuffer toWrite = partial.segment.wrap(partial.position, numBytesChunk);\n-\t\t\tFileUtils.writeCompletely(this.spillingChannel, toWrite);\n-\t\t}\n-\t\telse {\n-\t\t\t// collect in memory\n-\t\t\tensureBufferCapacity(nextRecordLength);\n-\t\t\tpartial.segment.get(partial.position, buffer, 0, numBytesChunk);\n-\t\t}\n-\n-\t\tthis.accumulatedRecordBytes = numBytesChunk;\n+\t/**\n+\t * We got the length, but we need the rest from the spanning deserializer and need to wait for more buffers.\n+\t */\n+\tvoid transferFrom(NonSpanningWrapper partial, int nextRecordLength) throws IOException {\n+\t\tupdateLength(nextRecordLength);\n+\t\taccumulatedRecordBytes = isAboveSpillingThreshold() ? spill(partial) : partial.copyTo(buffer);\n+\t\tpartial.clear();\n \t}\n \n-\tvoid initializeWithPartialLength(NonSpanningWrapper partial) throws IOException {\n-\t\t// copy what we have to the length buffer\n-\t\tpartial.segment.get(partial.position, this.lengthBuffer, partial.remaining());\n+\tprivate boolean isAboveSpillingThreshold() {\n+\t\treturn recordLength > THRESHOLD_FOR_SPILLING;\n \t}\n \n \tvoid addNextChunkFromMemorySegment(MemorySegment segment, int offset, int numBytes) throws IOException {\n-\t\tint segmentPosition = offset;\n-\t\tint segmentRemaining = numBytes;\n-\t\t// check where to go. if we have a partial length, we need to complete it first\n-\t\tif (this.lengthBuffer.position() > 0) {\n-\t\t\tint toPut = Math.min(this.lengthBuffer.remaining(), segmentRemaining);\n-\t\t\tsegment.get(segmentPosition, this.lengthBuffer, toPut);\n-\t\t\t// did we complete the length?\n-\t\t\tif (this.lengthBuffer.hasRemaining()) {\n-\t\t\t\treturn;\n-\t\t\t} else {\n-\t\t\t\tthis.recordLength = this.lengthBuffer.getInt(0);\n-\n-\t\t\t\tthis.lengthBuffer.clear();\n-\t\t\t\tsegmentPosition += toPut;\n-\t\t\t\tsegmentRemaining -= toPut;\n-\t\t\t\tif (this.recordLength > THRESHOLD_FOR_SPILLING) {\n-\t\t\t\t\tthis.spillingChannel = createSpillingChannel();\n-\t\t\t\t} else {\n-\t\t\t\t\tensureBufferCapacity(this.recordLength);\n-\t\t\t\t}\n-\t\t\t}\n+\t\tTuple2<Integer, Integer> positionAndRemaining = readLengthIfNeeded(segment, offset, numBytes);\n+\t\tint startPos = positionAndRemaining.f0;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "670aab1cacb84a6403811dd959dffd2e1d03d889"}, "originalPosition": 125}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDU4MjY1Mg==", "bodyText": "inline this method? One branch is a no-op just returning the arguments, which is quite strange. Also there would be one fewer place with unnamed Tupple2 (it's really difficult to understand what is this method doing looking at it, and the return value is a mystery)", "url": "https://github.com/apache/flink/pull/12120#discussion_r424582652", "createdAt": "2020-05-13T16:45:05Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/api/serialization/SpanningWrapper.java", "diffHunk": "@@ -82,187 +94,171 @@ public SpanningWrapper(String[] tempDirs) {\n \t\tthis.buffer = initialBuffer;\n \t}\n \n-\tvoid initializeWithPartialRecord(NonSpanningWrapper partial, int nextRecordLength) throws IOException {\n-\t\t// set the length and copy what is available to the buffer\n-\t\tthis.recordLength = nextRecordLength;\n-\n-\t\tfinal int numBytesChunk = partial.remaining();\n-\n-\t\tif (nextRecordLength > THRESHOLD_FOR_SPILLING) {\n-\t\t\t// create a spilling channel and put the data there\n-\t\t\tthis.spillingChannel = createSpillingChannel();\n-\n-\t\t\tByteBuffer toWrite = partial.segment.wrap(partial.position, numBytesChunk);\n-\t\t\tFileUtils.writeCompletely(this.spillingChannel, toWrite);\n-\t\t}\n-\t\telse {\n-\t\t\t// collect in memory\n-\t\t\tensureBufferCapacity(nextRecordLength);\n-\t\t\tpartial.segment.get(partial.position, buffer, 0, numBytesChunk);\n-\t\t}\n-\n-\t\tthis.accumulatedRecordBytes = numBytesChunk;\n+\t/**\n+\t * We got the length, but we need the rest from the spanning deserializer and need to wait for more buffers.\n+\t */\n+\tvoid transferFrom(NonSpanningWrapper partial, int nextRecordLength) throws IOException {\n+\t\tupdateLength(nextRecordLength);\n+\t\taccumulatedRecordBytes = isAboveSpillingThreshold() ? spill(partial) : partial.copyTo(buffer);\n+\t\tpartial.clear();\n \t}\n \n-\tvoid initializeWithPartialLength(NonSpanningWrapper partial) throws IOException {\n-\t\t// copy what we have to the length buffer\n-\t\tpartial.segment.get(partial.position, this.lengthBuffer, partial.remaining());\n+\tprivate boolean isAboveSpillingThreshold() {\n+\t\treturn recordLength > THRESHOLD_FOR_SPILLING;\n \t}\n \n \tvoid addNextChunkFromMemorySegment(MemorySegment segment, int offset, int numBytes) throws IOException {\n-\t\tint segmentPosition = offset;\n-\t\tint segmentRemaining = numBytes;\n-\t\t// check where to go. if we have a partial length, we need to complete it first\n-\t\tif (this.lengthBuffer.position() > 0) {\n-\t\t\tint toPut = Math.min(this.lengthBuffer.remaining(), segmentRemaining);\n-\t\t\tsegment.get(segmentPosition, this.lengthBuffer, toPut);\n-\t\t\t// did we complete the length?\n-\t\t\tif (this.lengthBuffer.hasRemaining()) {\n-\t\t\t\treturn;\n-\t\t\t} else {\n-\t\t\t\tthis.recordLength = this.lengthBuffer.getInt(0);\n-\n-\t\t\t\tthis.lengthBuffer.clear();\n-\t\t\t\tsegmentPosition += toPut;\n-\t\t\t\tsegmentRemaining -= toPut;\n-\t\t\t\tif (this.recordLength > THRESHOLD_FOR_SPILLING) {\n-\t\t\t\t\tthis.spillingChannel = createSpillingChannel();\n-\t\t\t\t} else {\n-\t\t\t\t\tensureBufferCapacity(this.recordLength);\n-\t\t\t\t}\n-\t\t\t}\n+\t\tTuple2<Integer, Integer> positionAndRemaining = readLengthIfNeeded(segment, offset, numBytes);\n+\t\tint startPos = positionAndRemaining.f0;\n+\t\tint remaining = positionAndRemaining.f1;\n+\t\tif (remaining == 0) {\n+\t\t\treturn;\n \t\t}\n \n-\t\t// copy as much as we need or can for this next spanning record\n-\t\tint needed = this.recordLength - this.accumulatedRecordBytes;\n-\t\tint toCopy = Math.min(needed, segmentRemaining);\n+\t\tint toCopy = min(recordLength - accumulatedRecordBytes, remaining);\n+\t\tif (toCopy > 0) {\n+\t\t\treadData(segment, startPos, toCopy);\n+\t\t}\n+\t\tif (remaining > toCopy) {\n+\t\t\tleftOverData = segment;\n+\t\t\tleftOverStart = startPos + toCopy;\n+\t\t\tleftOverLimit = offset + numBytes;\n+\t\t}\n+\t}\n \n-\t\tif (spillingChannel != null) {\n-\t\t\t// spill to file\n-\t\t\tByteBuffer toWrite = segment.wrap(segmentPosition, toCopy);\n-\t\t\tFileUtils.writeCompletely(this.spillingChannel, toWrite);\n+\tprivate void readData(MemorySegment segment, int offset, int length) throws IOException {\n+\t\tif (spillingChannel == null) {\n+\t\t\treadIntoBuffer(segment, offset, length);\n \t\t} else {\n-\t\t\tsegment.get(segmentPosition, buffer, this.accumulatedRecordBytes, toCopy);\n+\t\t\treadIntoFile(segment, offset, length);\n \t\t}\n+\t}\n \n-\t\tthis.accumulatedRecordBytes += toCopy;\n+\tprivate void readIntoFile(MemorySegment segment, int offset, int length) throws IOException {\n+\t\twriteCompletely(spillingChannel, segment.wrap(offset, length));\n+\t\taccumulatedRecordBytes += length;\n+\t\tif (hasFullRecord()) {\n+\t\t\tspillingChannel.close();\n+\t\t\tspillFileReader = new DataInputViewStreamWrapper(new BufferedInputStream(new FileInputStream(spillFile), FILE_BUFFER_SIZE));\n+\t\t}\n+\t}\n \n-\t\tif (toCopy < segmentRemaining) {\n-\t\t\t// there is more data in the segment\n-\t\t\tthis.leftOverData = segment;\n-\t\t\tthis.leftOverStart = segmentPosition + toCopy;\n-\t\t\tthis.leftOverLimit = numBytes + offset;\n+\tprivate void readIntoBuffer(MemorySegment segment, int offset, int length) {\n+\t\tsegment.get(offset, buffer, accumulatedRecordBytes, length);\n+\t\taccumulatedRecordBytes += length;\n+\t\tif (hasFullRecord()) {\n+\t\t\tserializationReadBuffer.setBuffer(buffer, 0, recordLength);\n \t\t}\n+\t}\n \n-\t\tif (accumulatedRecordBytes == recordLength) {\n-\t\t\t// we have the full record\n-\t\t\tif (spillingChannel == null) {\n-\t\t\t\tthis.serializationReadBuffer.setBuffer(buffer, 0, recordLength);\n-\t\t\t}\n-\t\t\telse {\n-\t\t\t\tspillingChannel.close();\n+\tprivate Tuple2<Integer, Integer> readLengthIfNeeded(MemorySegment segment, int offset, int numBytes) throws IOException {\n+\t\tif (isReadingLength()) {\n+\t\t\treturn readLength(segment, offset, numBytes);\n+\t\t} else {\n+\t\t\treturn Tuple2.of(offset, numBytes);\n+\t\t}\n+\t}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "670aab1cacb84a6403811dd959dffd2e1d03d889"}, "originalPosition": 194}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDU4NDc3MA==", "bodyText": "Why is it a constant? It's not being used anywhere. It looks like the only value of it is to workaround the problem of unnamed fields of Tuple2?", "url": "https://github.com/apache/flink/pull/12120#discussion_r424584770", "createdAt": "2020-05-13T16:48:17Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/api/serialization/SpanningWrapper.java", "diffHunk": "@@ -82,187 +94,171 @@ public SpanningWrapper(String[] tempDirs) {\n \t\tthis.buffer = initialBuffer;\n \t}\n \n-\tvoid initializeWithPartialRecord(NonSpanningWrapper partial, int nextRecordLength) throws IOException {\n-\t\t// set the length and copy what is available to the buffer\n-\t\tthis.recordLength = nextRecordLength;\n-\n-\t\tfinal int numBytesChunk = partial.remaining();\n-\n-\t\tif (nextRecordLength > THRESHOLD_FOR_SPILLING) {\n-\t\t\t// create a spilling channel and put the data there\n-\t\t\tthis.spillingChannel = createSpillingChannel();\n-\n-\t\t\tByteBuffer toWrite = partial.segment.wrap(partial.position, numBytesChunk);\n-\t\t\tFileUtils.writeCompletely(this.spillingChannel, toWrite);\n-\t\t}\n-\t\telse {\n-\t\t\t// collect in memory\n-\t\t\tensureBufferCapacity(nextRecordLength);\n-\t\t\tpartial.segment.get(partial.position, buffer, 0, numBytesChunk);\n-\t\t}\n-\n-\t\tthis.accumulatedRecordBytes = numBytesChunk;\n+\t/**\n+\t * We got the length, but we need the rest from the spanning deserializer and need to wait for more buffers.\n+\t */\n+\tvoid transferFrom(NonSpanningWrapper partial, int nextRecordLength) throws IOException {\n+\t\tupdateLength(nextRecordLength);\n+\t\taccumulatedRecordBytes = isAboveSpillingThreshold() ? spill(partial) : partial.copyTo(buffer);\n+\t\tpartial.clear();\n \t}\n \n-\tvoid initializeWithPartialLength(NonSpanningWrapper partial) throws IOException {\n-\t\t// copy what we have to the length buffer\n-\t\tpartial.segment.get(partial.position, this.lengthBuffer, partial.remaining());\n+\tprivate boolean isAboveSpillingThreshold() {\n+\t\treturn recordLength > THRESHOLD_FOR_SPILLING;\n \t}\n \n \tvoid addNextChunkFromMemorySegment(MemorySegment segment, int offset, int numBytes) throws IOException {\n-\t\tint segmentPosition = offset;\n-\t\tint segmentRemaining = numBytes;\n-\t\t// check where to go. if we have a partial length, we need to complete it first\n-\t\tif (this.lengthBuffer.position() > 0) {\n-\t\t\tint toPut = Math.min(this.lengthBuffer.remaining(), segmentRemaining);\n-\t\t\tsegment.get(segmentPosition, this.lengthBuffer, toPut);\n-\t\t\t// did we complete the length?\n-\t\t\tif (this.lengthBuffer.hasRemaining()) {\n-\t\t\t\treturn;\n-\t\t\t} else {\n-\t\t\t\tthis.recordLength = this.lengthBuffer.getInt(0);\n-\n-\t\t\t\tthis.lengthBuffer.clear();\n-\t\t\t\tsegmentPosition += toPut;\n-\t\t\t\tsegmentRemaining -= toPut;\n-\t\t\t\tif (this.recordLength > THRESHOLD_FOR_SPILLING) {\n-\t\t\t\t\tthis.spillingChannel = createSpillingChannel();\n-\t\t\t\t} else {\n-\t\t\t\t\tensureBufferCapacity(this.recordLength);\n-\t\t\t\t}\n-\t\t\t}\n+\t\tTuple2<Integer, Integer> positionAndRemaining = readLengthIfNeeded(segment, offset, numBytes);\n+\t\tint startPos = positionAndRemaining.f0;\n+\t\tint remaining = positionAndRemaining.f1;\n+\t\tif (remaining == 0) {\n+\t\t\treturn;\n \t\t}\n \n-\t\t// copy as much as we need or can for this next spanning record\n-\t\tint needed = this.recordLength - this.accumulatedRecordBytes;\n-\t\tint toCopy = Math.min(needed, segmentRemaining);\n+\t\tint toCopy = min(recordLength - accumulatedRecordBytes, remaining);\n+\t\tif (toCopy > 0) {\n+\t\t\treadData(segment, startPos, toCopy);\n+\t\t}\n+\t\tif (remaining > toCopy) {\n+\t\t\tleftOverData = segment;\n+\t\t\tleftOverStart = startPos + toCopy;\n+\t\t\tleftOverLimit = offset + numBytes;\n+\t\t}\n+\t}\n \n-\t\tif (spillingChannel != null) {\n-\t\t\t// spill to file\n-\t\t\tByteBuffer toWrite = segment.wrap(segmentPosition, toCopy);\n-\t\t\tFileUtils.writeCompletely(this.spillingChannel, toWrite);\n+\tprivate void readData(MemorySegment segment, int offset, int length) throws IOException {\n+\t\tif (spillingChannel == null) {\n+\t\t\treadIntoBuffer(segment, offset, length);\n \t\t} else {\n-\t\t\tsegment.get(segmentPosition, buffer, this.accumulatedRecordBytes, toCopy);\n+\t\t\treadIntoFile(segment, offset, length);\n \t\t}\n+\t}\n \n-\t\tthis.accumulatedRecordBytes += toCopy;\n+\tprivate void readIntoFile(MemorySegment segment, int offset, int length) throws IOException {\n+\t\twriteCompletely(spillingChannel, segment.wrap(offset, length));\n+\t\taccumulatedRecordBytes += length;\n+\t\tif (hasFullRecord()) {\n+\t\t\tspillingChannel.close();\n+\t\t\tspillFileReader = new DataInputViewStreamWrapper(new BufferedInputStream(new FileInputStream(spillFile), FILE_BUFFER_SIZE));\n+\t\t}\n+\t}\n \n-\t\tif (toCopy < segmentRemaining) {\n-\t\t\t// there is more data in the segment\n-\t\t\tthis.leftOverData = segment;\n-\t\t\tthis.leftOverStart = segmentPosition + toCopy;\n-\t\t\tthis.leftOverLimit = numBytes + offset;\n+\tprivate void readIntoBuffer(MemorySegment segment, int offset, int length) {\n+\t\tsegment.get(offset, buffer, accumulatedRecordBytes, length);\n+\t\taccumulatedRecordBytes += length;\n+\t\tif (hasFullRecord()) {\n+\t\t\tserializationReadBuffer.setBuffer(buffer, 0, recordLength);\n \t\t}\n+\t}\n \n-\t\tif (accumulatedRecordBytes == recordLength) {\n-\t\t\t// we have the full record\n-\t\t\tif (spillingChannel == null) {\n-\t\t\t\tthis.serializationReadBuffer.setBuffer(buffer, 0, recordLength);\n-\t\t\t}\n-\t\t\telse {\n-\t\t\t\tspillingChannel.close();\n+\tprivate Tuple2<Integer, Integer> readLengthIfNeeded(MemorySegment segment, int offset, int numBytes) throws IOException {\n+\t\tif (isReadingLength()) {\n+\t\t\treturn readLength(segment, offset, numBytes);\n+\t\t} else {\n+\t\t\treturn Tuple2.of(offset, numBytes);\n+\t\t}\n+\t}\n \n-\t\t\t\tBufferedInputStream inStream = new BufferedInputStream(new FileInputStream(spillFile), 2 * 1024 * 1024);\n-\t\t\t\tthis.spillFileReader = new DataInputViewStreamWrapper(inStream);\n-\t\t\t}\n+\tprivate Tuple2<Integer, Integer> readLength(MemorySegment segment, int segmentPosition, int segmentRemaining) throws IOException {\n+\t\tint numBytes = min(lengthBuffer.remaining(), segmentRemaining);\n+\t\tsegment.get(segmentPosition, lengthBuffer, numBytes);\n+\t\tif (lengthBuffer.hasRemaining()) {\n+\t\t\treturn POSITION_AND_SIZE_IF_LENGTH_NOT_FULLY_READ;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "670aab1cacb84a6403811dd959dffd2e1d03d889"}, "originalPosition": 203}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDU5MDU4MA==", "bodyText": "If you return a single numBytes from this method, I think it would simplify the code. Returning number of bytes that was read/copied is quite standard practice and easy to explain/document/understand. While deciphering behaviour of this Tuple2 took me quite a bit of time.", "url": "https://github.com/apache/flink/pull/12120#discussion_r424590580", "createdAt": "2020-05-13T16:57:31Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/api/serialization/SpanningWrapper.java", "diffHunk": "@@ -82,187 +94,171 @@ public SpanningWrapper(String[] tempDirs) {\n \t\tthis.buffer = initialBuffer;\n \t}\n \n-\tvoid initializeWithPartialRecord(NonSpanningWrapper partial, int nextRecordLength) throws IOException {\n-\t\t// set the length and copy what is available to the buffer\n-\t\tthis.recordLength = nextRecordLength;\n-\n-\t\tfinal int numBytesChunk = partial.remaining();\n-\n-\t\tif (nextRecordLength > THRESHOLD_FOR_SPILLING) {\n-\t\t\t// create a spilling channel and put the data there\n-\t\t\tthis.spillingChannel = createSpillingChannel();\n-\n-\t\t\tByteBuffer toWrite = partial.segment.wrap(partial.position, numBytesChunk);\n-\t\t\tFileUtils.writeCompletely(this.spillingChannel, toWrite);\n-\t\t}\n-\t\telse {\n-\t\t\t// collect in memory\n-\t\t\tensureBufferCapacity(nextRecordLength);\n-\t\t\tpartial.segment.get(partial.position, buffer, 0, numBytesChunk);\n-\t\t}\n-\n-\t\tthis.accumulatedRecordBytes = numBytesChunk;\n+\t/**\n+\t * We got the length, but we need the rest from the spanning deserializer and need to wait for more buffers.\n+\t */\n+\tvoid transferFrom(NonSpanningWrapper partial, int nextRecordLength) throws IOException {\n+\t\tupdateLength(nextRecordLength);\n+\t\taccumulatedRecordBytes = isAboveSpillingThreshold() ? spill(partial) : partial.copyTo(buffer);\n+\t\tpartial.clear();\n \t}\n \n-\tvoid initializeWithPartialLength(NonSpanningWrapper partial) throws IOException {\n-\t\t// copy what we have to the length buffer\n-\t\tpartial.segment.get(partial.position, this.lengthBuffer, partial.remaining());\n+\tprivate boolean isAboveSpillingThreshold() {\n+\t\treturn recordLength > THRESHOLD_FOR_SPILLING;\n \t}\n \n \tvoid addNextChunkFromMemorySegment(MemorySegment segment, int offset, int numBytes) throws IOException {\n-\t\tint segmentPosition = offset;\n-\t\tint segmentRemaining = numBytes;\n-\t\t// check where to go. if we have a partial length, we need to complete it first\n-\t\tif (this.lengthBuffer.position() > 0) {\n-\t\t\tint toPut = Math.min(this.lengthBuffer.remaining(), segmentRemaining);\n-\t\t\tsegment.get(segmentPosition, this.lengthBuffer, toPut);\n-\t\t\t// did we complete the length?\n-\t\t\tif (this.lengthBuffer.hasRemaining()) {\n-\t\t\t\treturn;\n-\t\t\t} else {\n-\t\t\t\tthis.recordLength = this.lengthBuffer.getInt(0);\n-\n-\t\t\t\tthis.lengthBuffer.clear();\n-\t\t\t\tsegmentPosition += toPut;\n-\t\t\t\tsegmentRemaining -= toPut;\n-\t\t\t\tif (this.recordLength > THRESHOLD_FOR_SPILLING) {\n-\t\t\t\t\tthis.spillingChannel = createSpillingChannel();\n-\t\t\t\t} else {\n-\t\t\t\t\tensureBufferCapacity(this.recordLength);\n-\t\t\t\t}\n-\t\t\t}\n+\t\tTuple2<Integer, Integer> positionAndRemaining = readLengthIfNeeded(segment, offset, numBytes);\n+\t\tint startPos = positionAndRemaining.f0;\n+\t\tint remaining = positionAndRemaining.f1;\n+\t\tif (remaining == 0) {\n+\t\t\treturn;\n \t\t}\n \n-\t\t// copy as much as we need or can for this next spanning record\n-\t\tint needed = this.recordLength - this.accumulatedRecordBytes;\n-\t\tint toCopy = Math.min(needed, segmentRemaining);\n+\t\tint toCopy = min(recordLength - accumulatedRecordBytes, remaining);\n+\t\tif (toCopy > 0) {\n+\t\t\treadData(segment, startPos, toCopy);\n+\t\t}\n+\t\tif (remaining > toCopy) {\n+\t\t\tleftOverData = segment;\n+\t\t\tleftOverStart = startPos + toCopy;\n+\t\t\tleftOverLimit = offset + numBytes;\n+\t\t}\n+\t}\n \n-\t\tif (spillingChannel != null) {\n-\t\t\t// spill to file\n-\t\t\tByteBuffer toWrite = segment.wrap(segmentPosition, toCopy);\n-\t\t\tFileUtils.writeCompletely(this.spillingChannel, toWrite);\n+\tprivate void readData(MemorySegment segment, int offset, int length) throws IOException {\n+\t\tif (spillingChannel == null) {\n+\t\t\treadIntoBuffer(segment, offset, length);\n \t\t} else {\n-\t\t\tsegment.get(segmentPosition, buffer, this.accumulatedRecordBytes, toCopy);\n+\t\t\treadIntoFile(segment, offset, length);\n \t\t}\n+\t}\n \n-\t\tthis.accumulatedRecordBytes += toCopy;\n+\tprivate void readIntoFile(MemorySegment segment, int offset, int length) throws IOException {\n+\t\twriteCompletely(spillingChannel, segment.wrap(offset, length));\n+\t\taccumulatedRecordBytes += length;\n+\t\tif (hasFullRecord()) {\n+\t\t\tspillingChannel.close();\n+\t\t\tspillFileReader = new DataInputViewStreamWrapper(new BufferedInputStream(new FileInputStream(spillFile), FILE_BUFFER_SIZE));\n+\t\t}\n+\t}\n \n-\t\tif (toCopy < segmentRemaining) {\n-\t\t\t// there is more data in the segment\n-\t\t\tthis.leftOverData = segment;\n-\t\t\tthis.leftOverStart = segmentPosition + toCopy;\n-\t\t\tthis.leftOverLimit = numBytes + offset;\n+\tprivate void readIntoBuffer(MemorySegment segment, int offset, int length) {\n+\t\tsegment.get(offset, buffer, accumulatedRecordBytes, length);\n+\t\taccumulatedRecordBytes += length;\n+\t\tif (hasFullRecord()) {\n+\t\t\tserializationReadBuffer.setBuffer(buffer, 0, recordLength);\n \t\t}\n+\t}\n \n-\t\tif (accumulatedRecordBytes == recordLength) {\n-\t\t\t// we have the full record\n-\t\t\tif (spillingChannel == null) {\n-\t\t\t\tthis.serializationReadBuffer.setBuffer(buffer, 0, recordLength);\n-\t\t\t}\n-\t\t\telse {\n-\t\t\t\tspillingChannel.close();\n+\tprivate Tuple2<Integer, Integer> readLengthIfNeeded(MemorySegment segment, int offset, int numBytes) throws IOException {\n+\t\tif (isReadingLength()) {\n+\t\t\treturn readLength(segment, offset, numBytes);\n+\t\t} else {\n+\t\t\treturn Tuple2.of(offset, numBytes);\n+\t\t}\n+\t}\n \n-\t\t\t\tBufferedInputStream inStream = new BufferedInputStream(new FileInputStream(spillFile), 2 * 1024 * 1024);\n-\t\t\t\tthis.spillFileReader = new DataInputViewStreamWrapper(inStream);\n-\t\t\t}\n+\tprivate Tuple2<Integer, Integer> readLength(MemorySegment segment, int segmentPosition, int segmentRemaining) throws IOException {\n+\t\tint numBytes = min(lengthBuffer.remaining(), segmentRemaining);\n+\t\tsegment.get(segmentPosition, lengthBuffer, numBytes);\n+\t\tif (lengthBuffer.hasRemaining()) {\n+\t\t\treturn POSITION_AND_SIZE_IF_LENGTH_NOT_FULLY_READ;\n+\t\t} else {\n+\t\t\tupdateLength(lengthBuffer.getInt(0));\n+\t\t\treturn Tuple2.of(segmentPosition + numBytes, segmentRemaining - numBytes);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "670aab1cacb84a6403811dd959dffd2e1d03d889"}, "originalPosition": 206}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDU5MzIyMg==", "bodyText": "nit: numBytes has a very different meaning on the call stack in addNextChunkFromMemorySegment() method, so naming (collision?) is a bit unfortunate.", "url": "https://github.com/apache/flink/pull/12120#discussion_r424593222", "createdAt": "2020-05-13T17:01:38Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/api/serialization/SpanningWrapper.java", "diffHunk": "@@ -82,187 +94,171 @@ public SpanningWrapper(String[] tempDirs) {\n \t\tthis.buffer = initialBuffer;\n \t}\n \n-\tvoid initializeWithPartialRecord(NonSpanningWrapper partial, int nextRecordLength) throws IOException {\n-\t\t// set the length and copy what is available to the buffer\n-\t\tthis.recordLength = nextRecordLength;\n-\n-\t\tfinal int numBytesChunk = partial.remaining();\n-\n-\t\tif (nextRecordLength > THRESHOLD_FOR_SPILLING) {\n-\t\t\t// create a spilling channel and put the data there\n-\t\t\tthis.spillingChannel = createSpillingChannel();\n-\n-\t\t\tByteBuffer toWrite = partial.segment.wrap(partial.position, numBytesChunk);\n-\t\t\tFileUtils.writeCompletely(this.spillingChannel, toWrite);\n-\t\t}\n-\t\telse {\n-\t\t\t// collect in memory\n-\t\t\tensureBufferCapacity(nextRecordLength);\n-\t\t\tpartial.segment.get(partial.position, buffer, 0, numBytesChunk);\n-\t\t}\n-\n-\t\tthis.accumulatedRecordBytes = numBytesChunk;\n+\t/**\n+\t * We got the length, but we need the rest from the spanning deserializer and need to wait for more buffers.\n+\t */\n+\tvoid transferFrom(NonSpanningWrapper partial, int nextRecordLength) throws IOException {\n+\t\tupdateLength(nextRecordLength);\n+\t\taccumulatedRecordBytes = isAboveSpillingThreshold() ? spill(partial) : partial.copyTo(buffer);\n+\t\tpartial.clear();\n \t}\n \n-\tvoid initializeWithPartialLength(NonSpanningWrapper partial) throws IOException {\n-\t\t// copy what we have to the length buffer\n-\t\tpartial.segment.get(partial.position, this.lengthBuffer, partial.remaining());\n+\tprivate boolean isAboveSpillingThreshold() {\n+\t\treturn recordLength > THRESHOLD_FOR_SPILLING;\n \t}\n \n \tvoid addNextChunkFromMemorySegment(MemorySegment segment, int offset, int numBytes) throws IOException {\n-\t\tint segmentPosition = offset;\n-\t\tint segmentRemaining = numBytes;\n-\t\t// check where to go. if we have a partial length, we need to complete it first\n-\t\tif (this.lengthBuffer.position() > 0) {\n-\t\t\tint toPut = Math.min(this.lengthBuffer.remaining(), segmentRemaining);\n-\t\t\tsegment.get(segmentPosition, this.lengthBuffer, toPut);\n-\t\t\t// did we complete the length?\n-\t\t\tif (this.lengthBuffer.hasRemaining()) {\n-\t\t\t\treturn;\n-\t\t\t} else {\n-\t\t\t\tthis.recordLength = this.lengthBuffer.getInt(0);\n-\n-\t\t\t\tthis.lengthBuffer.clear();\n-\t\t\t\tsegmentPosition += toPut;\n-\t\t\t\tsegmentRemaining -= toPut;\n-\t\t\t\tif (this.recordLength > THRESHOLD_FOR_SPILLING) {\n-\t\t\t\t\tthis.spillingChannel = createSpillingChannel();\n-\t\t\t\t} else {\n-\t\t\t\t\tensureBufferCapacity(this.recordLength);\n-\t\t\t\t}\n-\t\t\t}\n+\t\tTuple2<Integer, Integer> positionAndRemaining = readLengthIfNeeded(segment, offset, numBytes);\n+\t\tint startPos = positionAndRemaining.f0;\n+\t\tint remaining = positionAndRemaining.f1;\n+\t\tif (remaining == 0) {\n+\t\t\treturn;\n \t\t}\n \n-\t\t// copy as much as we need or can for this next spanning record\n-\t\tint needed = this.recordLength - this.accumulatedRecordBytes;\n-\t\tint toCopy = Math.min(needed, segmentRemaining);\n+\t\tint toCopy = min(recordLength - accumulatedRecordBytes, remaining);\n+\t\tif (toCopy > 0) {\n+\t\t\treadData(segment, startPos, toCopy);\n+\t\t}\n+\t\tif (remaining > toCopy) {\n+\t\t\tleftOverData = segment;\n+\t\t\tleftOverStart = startPos + toCopy;\n+\t\t\tleftOverLimit = offset + numBytes;\n+\t\t}\n+\t}\n \n-\t\tif (spillingChannel != null) {\n-\t\t\t// spill to file\n-\t\t\tByteBuffer toWrite = segment.wrap(segmentPosition, toCopy);\n-\t\t\tFileUtils.writeCompletely(this.spillingChannel, toWrite);\n+\tprivate void readData(MemorySegment segment, int offset, int length) throws IOException {\n+\t\tif (spillingChannel == null) {\n+\t\t\treadIntoBuffer(segment, offset, length);\n \t\t} else {\n-\t\t\tsegment.get(segmentPosition, buffer, this.accumulatedRecordBytes, toCopy);\n+\t\t\treadIntoFile(segment, offset, length);\n \t\t}\n+\t}\n \n-\t\tthis.accumulatedRecordBytes += toCopy;\n+\tprivate void readIntoFile(MemorySegment segment, int offset, int length) throws IOException {\n+\t\twriteCompletely(spillingChannel, segment.wrap(offset, length));\n+\t\taccumulatedRecordBytes += length;\n+\t\tif (hasFullRecord()) {\n+\t\t\tspillingChannel.close();\n+\t\t\tspillFileReader = new DataInputViewStreamWrapper(new BufferedInputStream(new FileInputStream(spillFile), FILE_BUFFER_SIZE));\n+\t\t}\n+\t}\n \n-\t\tif (toCopy < segmentRemaining) {\n-\t\t\t// there is more data in the segment\n-\t\t\tthis.leftOverData = segment;\n-\t\t\tthis.leftOverStart = segmentPosition + toCopy;\n-\t\t\tthis.leftOverLimit = numBytes + offset;\n+\tprivate void readIntoBuffer(MemorySegment segment, int offset, int length) {\n+\t\tsegment.get(offset, buffer, accumulatedRecordBytes, length);\n+\t\taccumulatedRecordBytes += length;\n+\t\tif (hasFullRecord()) {\n+\t\t\tserializationReadBuffer.setBuffer(buffer, 0, recordLength);\n \t\t}\n+\t}\n \n-\t\tif (accumulatedRecordBytes == recordLength) {\n-\t\t\t// we have the full record\n-\t\t\tif (spillingChannel == null) {\n-\t\t\t\tthis.serializationReadBuffer.setBuffer(buffer, 0, recordLength);\n-\t\t\t}\n-\t\t\telse {\n-\t\t\t\tspillingChannel.close();\n+\tprivate Tuple2<Integer, Integer> readLengthIfNeeded(MemorySegment segment, int offset, int numBytes) throws IOException {\n+\t\tif (isReadingLength()) {\n+\t\t\treturn readLength(segment, offset, numBytes);\n+\t\t} else {\n+\t\t\treturn Tuple2.of(offset, numBytes);\n+\t\t}\n+\t}\n \n-\t\t\t\tBufferedInputStream inStream = new BufferedInputStream(new FileInputStream(spillFile), 2 * 1024 * 1024);\n-\t\t\t\tthis.spillFileReader = new DataInputViewStreamWrapper(inStream);\n-\t\t\t}\n+\tprivate Tuple2<Integer, Integer> readLength(MemorySegment segment, int segmentPosition, int segmentRemaining) throws IOException {\n+\t\tint numBytes = min(lengthBuffer.remaining(), segmentRemaining);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "670aab1cacb84a6403811dd959dffd2e1d03d889"}, "originalPosition": 200}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDU5NTM0OQ==", "bodyText": "nit: readData -> copyFromSegment\nreadIntoBuffer -> copyIntoBuffer / copyFromSegmentIntoBuffer\nreadIntoFile -> copyIntoFile / copyFromSegmentIntoFile\n?\nIt wasn't obvious to me what's the direction and purpose of the plain \"readData\". (Inside readData it was already obvious)", "url": "https://github.com/apache/flink/pull/12120#discussion_r424595349", "createdAt": "2020-05-13T17:05:07Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/api/serialization/SpanningWrapper.java", "diffHunk": "@@ -82,187 +94,171 @@ public SpanningWrapper(String[] tempDirs) {\n \t\tthis.buffer = initialBuffer;\n \t}\n \n-\tvoid initializeWithPartialRecord(NonSpanningWrapper partial, int nextRecordLength) throws IOException {\n-\t\t// set the length and copy what is available to the buffer\n-\t\tthis.recordLength = nextRecordLength;\n-\n-\t\tfinal int numBytesChunk = partial.remaining();\n-\n-\t\tif (nextRecordLength > THRESHOLD_FOR_SPILLING) {\n-\t\t\t// create a spilling channel and put the data there\n-\t\t\tthis.spillingChannel = createSpillingChannel();\n-\n-\t\t\tByteBuffer toWrite = partial.segment.wrap(partial.position, numBytesChunk);\n-\t\t\tFileUtils.writeCompletely(this.spillingChannel, toWrite);\n-\t\t}\n-\t\telse {\n-\t\t\t// collect in memory\n-\t\t\tensureBufferCapacity(nextRecordLength);\n-\t\t\tpartial.segment.get(partial.position, buffer, 0, numBytesChunk);\n-\t\t}\n-\n-\t\tthis.accumulatedRecordBytes = numBytesChunk;\n+\t/**\n+\t * We got the length, but we need the rest from the spanning deserializer and need to wait for more buffers.\n+\t */\n+\tvoid transferFrom(NonSpanningWrapper partial, int nextRecordLength) throws IOException {\n+\t\tupdateLength(nextRecordLength);\n+\t\taccumulatedRecordBytes = isAboveSpillingThreshold() ? spill(partial) : partial.copyTo(buffer);\n+\t\tpartial.clear();\n \t}\n \n-\tvoid initializeWithPartialLength(NonSpanningWrapper partial) throws IOException {\n-\t\t// copy what we have to the length buffer\n-\t\tpartial.segment.get(partial.position, this.lengthBuffer, partial.remaining());\n+\tprivate boolean isAboveSpillingThreshold() {\n+\t\treturn recordLength > THRESHOLD_FOR_SPILLING;\n \t}\n \n \tvoid addNextChunkFromMemorySegment(MemorySegment segment, int offset, int numBytes) throws IOException {\n-\t\tint segmentPosition = offset;\n-\t\tint segmentRemaining = numBytes;\n-\t\t// check where to go. if we have a partial length, we need to complete it first\n-\t\tif (this.lengthBuffer.position() > 0) {\n-\t\t\tint toPut = Math.min(this.lengthBuffer.remaining(), segmentRemaining);\n-\t\t\tsegment.get(segmentPosition, this.lengthBuffer, toPut);\n-\t\t\t// did we complete the length?\n-\t\t\tif (this.lengthBuffer.hasRemaining()) {\n-\t\t\t\treturn;\n-\t\t\t} else {\n-\t\t\t\tthis.recordLength = this.lengthBuffer.getInt(0);\n-\n-\t\t\t\tthis.lengthBuffer.clear();\n-\t\t\t\tsegmentPosition += toPut;\n-\t\t\t\tsegmentRemaining -= toPut;\n-\t\t\t\tif (this.recordLength > THRESHOLD_FOR_SPILLING) {\n-\t\t\t\t\tthis.spillingChannel = createSpillingChannel();\n-\t\t\t\t} else {\n-\t\t\t\t\tensureBufferCapacity(this.recordLength);\n-\t\t\t\t}\n-\t\t\t}\n+\t\tTuple2<Integer, Integer> positionAndRemaining = readLengthIfNeeded(segment, offset, numBytes);\n+\t\tint startPos = positionAndRemaining.f0;\n+\t\tint remaining = positionAndRemaining.f1;\n+\t\tif (remaining == 0) {\n+\t\t\treturn;\n \t\t}\n \n-\t\t// copy as much as we need or can for this next spanning record\n-\t\tint needed = this.recordLength - this.accumulatedRecordBytes;\n-\t\tint toCopy = Math.min(needed, segmentRemaining);\n+\t\tint toCopy = min(recordLength - accumulatedRecordBytes, remaining);\n+\t\tif (toCopy > 0) {\n+\t\t\treadData(segment, startPos, toCopy);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "670aab1cacb84a6403811dd959dffd2e1d03d889"}, "originalPosition": 136}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "91982f5d59af5b6540ffa4052050b18a56f92b25", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/91982f5d59af5b6540ffa4052050b18a56f92b25", "committedDate": "2020-05-13T10:20:36Z", "message": "[FLINK-17547][task] Implement getUnconsumedSegment for spilled buffers (incomplete)"}, "afterCommit": {"oid": "b9fe05dbed1b5e67d9495f3619886d7ffd5b101f", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/b9fe05dbed1b5e67d9495f3619886d7ffd5b101f", "committedDate": "2020-05-14T12:30:56Z", "message": "[FLINK-17547][task] Implement getUnconsumedSegment for spilled buffers (incomplete)"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "b9fe05dbed1b5e67d9495f3619886d7ffd5b101f", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/b9fe05dbed1b5e67d9495f3619886d7ffd5b101f", "committedDate": "2020-05-14T12:30:56Z", "message": "[FLINK-17547][task] Implement getUnconsumedSegment for spilled buffers (incomplete)"}, "afterCommit": {"oid": "a83ee4ba9c88ba7a9d3ac8065cd056db57c5f65d", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/a83ee4ba9c88ba7a9d3ac8065cd056db57c5f65d", "committedDate": "2020-05-14T14:35:34Z", "message": "[FLINK-17547][task] Implement getUnconsumedSegment for spilled buffers (incomplete)"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "c6c3a9b0961c20f732e3ff81fdd29d0b7cae3e21", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/c6c3a9b0961c20f732e3ff81fdd29d0b7cae3e21", "committedDate": "2020-05-14T17:08:06Z", "message": "[FLINK-17547][task] Implement getUnconsumedSegment for spilled buffers"}, "afterCommit": {"oid": "2b8d574b50b77c59aeee5f45c796eaa16ef13aeb", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/2b8d574b50b77c59aeee5f45c796eaa16ef13aeb", "committedDate": "2020-05-14T17:10:26Z", "message": "[FLINK-17547][task] Implement getUnconsumedSegment for spilled buffers"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDEyMTMzNjYw", "url": "https://github.com/apache/flink/pull/12120#pullrequestreview-412133660", "createdAt": "2020-05-14T19:50:10Z", "commit": {"oid": "5c01c2037a94b4b236fba20c5facce73351dfd58"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQxOTo1MDoxMFrOGVryKQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQxOTo1MDoxMFrOGVryKQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTM5MDYzMw==", "bodyText": "are you sure this won't be called twice? Or if it will, that it will not double release the buffers/files?", "url": "https://github.com/apache/flink/pull/12120#discussion_r425390633", "createdAt": "2020-05-14T19:50:10Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/ChannelStateWriteRequest.java", "diffHunk": "@@ -46,8 +47,23 @@ static CheckpointInProgressRequest completeOutput(long checkpointId) {\n \t\treturn new CheckpointInProgressRequest(\"completeOutput\", checkpointId, ChannelStateCheckpointWriter::completeOutput, false);\n \t}\n \n-\tstatic ChannelStateWriteRequest write(long checkpointId, InputChannelInfo info, Buffer... flinkBuffers) {\n-\t\treturn new CheckpointInProgressRequest(\"writeInput\", checkpointId, writer -> writer.writeInput(info, flinkBuffers), recycle(flinkBuffers), false);\n+\tstatic ChannelStateWriteRequest write(long checkpointId, InputChannelInfo info, CloseableIterator<Buffer> iterator) {\n+\t\treturn new CheckpointInProgressRequest(\n+\t\t\t\"writeInput\",\n+\t\t\tcheckpointId,\n+\t\t\twriter -> {\n+\t\t\t\ttry {\n+\t\t\t\t\twhile (iterator.hasNext()) {\n+\t\t\t\t\t\tBuffer buffer = iterator.next();\n+\t\t\t\t\t\tcheckArgument(buffer.isBuffer());\n+\t\t\t\t\t\twriter.writeInput(info, buffer);\n+\t\t\t\t\t}\n+\t\t\t\t} finally {\n+\t\t\t\t\titerator.close();\n+\t\t\t\t}\n+\t\t\t},\n+\t\t\tthrowable -> iterator.close(),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5c01c2037a94b4b236fba20c5facce73351dfd58"}, "originalPosition": 48}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "b786134d31ac9c7414dc28b8134fd8140edf6a96", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/b786134d31ac9c7414dc28b8134fd8140edf6a96", "committedDate": "2020-05-14T20:03:54Z", "message": "fixup! [FLINK-17547][task] Use iterator for unconsumed buffers. Motivation: support spilled records Changes: 1. change SpillingAdaptiveSpanningRecordDeserializer.getUnconsumedBuffer signature 2. adapt channel state persistence to new types"}, "afterCommit": {"oid": "16e4c4a0a573d16eeb52692d8ab4af949c80170d", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/16e4c4a0a573d16eeb52692d8ab4af949c80170d", "committedDate": "2020-05-14T20:06:17Z", "message": "fixup! [FLINK-17547][task] Use iterator for unconsumed buffers. Motivation: support spilled records Changes: 1. change SpillingAdaptiveSpanningRecordDeserializer.getUnconsumedBuffer signature 2. adapt channel state persistence to new types"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDEyMzY3NDc5", "url": "https://github.com/apache/flink/pull/12120#pullrequestreview-412367479", "createdAt": "2020-05-15T06:00:38Z", "commit": {"oid": "16e4c4a0a573d16eeb52692d8ab4af949c80170d"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQwNjowMDozOVrOGV3h1g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQwNjoxNjoxN1rOGV31lg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTU4MzA2Mg==", "bodyText": "we had quite a bit of problems with error logging like that polluting the logs failing e2e tests. It would be better to add e as suppressed exception to the thrown, like org.apache.flink.streaming.runtime.tasks.StreamTask#invoke is doing.\n} catch (Exception e) {\n\tdispatcher.fail(ExceptionUtils.firstOrSuppressed(e, thrown));\n}\n\n?", "url": "https://github.com/apache/flink/pull/12120#discussion_r425583062", "createdAt": "2020-05-15T06:00:39Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/ChannelStateWriteRequestExecutorImpl.java", "diffHunk": "@@ -67,8 +70,11 @@ void run() {\n \t\t} catch (Exception ex) {\n \t\t\tthrown = ex;\n \t\t} finally {\n-\t\t\tcleanupRequests();\n-\t\t\tdispatcher.fail(thrown == null ? new CancellationException() : thrown);\n+\t\t\ttry {\n+\t\t\t\tcloseAll(this::cleanupRequests, () -> dispatcher.fail(thrown == null ? new CancellationException() : thrown));\n+\t\t\t} catch (Exception e) {\n+\t\t\t\tLOG.error(\"unable to terminate properly\", e);\n+\t\t\t}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "16e4c4a0a573d16eeb52692d8ab4af949c80170d"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTU4ODExOA==", "bodyText": "nit: by wrapper I meant a POJO not Tuple2, but this is probably fine.\nHowever now I'm a bit worried about performance implications of this - it's a critical/hot path, and we are allocating new object on it.", "url": "https://github.com/apache/flink/pull/12120#discussion_r425588118", "createdAt": "2020-05-15T06:16:17Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/api/serialization/NonSpanningWrapper.java", "diffHunk": "@@ -290,4 +309,49 @@ public int read(byte[] b, int off, int len) {\n \tpublic int read(byte[] b) {\n \t\treturn read(b, 0, b.length);\n \t}\n+\n+\tByteBuffer wrapIntoByteBuffer() {\n+\t\treturn segment.wrap(position, remaining());\n+\t}\n+\n+\tint copyContentTo(byte[] dst) {\n+\t\tfinal int numBytesChunk = remaining();\n+\t\tsegment.get(position, dst, 0, numBytesChunk);\n+\t\treturn numBytesChunk;\n+\t}\n+\n+\t/**\n+\t * Copies the data and transfers the \"ownership\" (i.e. clears current wrapper).\n+\t */\n+\tvoid transferTo(ByteBuffer dst) {\n+\t\tsegment.get(position, dst, remaining());\n+\t\tclear();\n+\t}\n+\n+\tTuple2<DeserializationResult, Integer> getNextRecord(IOReadableWritable target) throws IOException {\n+\t\tint recordLen = readInt();\n+\t\tif (canReadRecord(recordLen)) {\n+\t\t\treturn readInto(target);\n+\t\t} else {\n+\t\t\treturn new Tuple2<>(PARTIAL_RECORD, recordLen);\n+\t\t}\n+\t}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2c35fca3a72306f9f19b85da09e400cd33896cb3"}, "originalPosition": 112}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDEyNzg5NzM1", "url": "https://github.com/apache/flink/pull/12120#pullrequestreview-412789735", "createdAt": "2020-05-15T16:20:55Z", "commit": {"oid": "73e4094cc98004f8dcc51d4c495fceec3a35f964"}, "state": "APPROVED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQxNjoyMDo1NVrOGWLe9w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQxNjoyMDo1NVrOGWLe9w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTkxMDAwNw==", "bodyText": "nit: This is a really hacky piece of code :)\nI'm fine with keeping it as it is to not trigger another re-build over a minor issue.", "url": "https://github.com/apache/flink/pull/12120#discussion_r425910007", "createdAt": "2020-05-15T16:20:55Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/ChannelStateWriteRequestExecutorImpl.java", "diffHunk": "@@ -67,8 +70,12 @@ void run() {\n \t\t} catch (Exception ex) {\n \t\t\tthrown = ex;\n \t\t} finally {\n-\t\t\tcleanupRequests();\n-\t\t\tdispatcher.fail(thrown == null ? new CancellationException() : thrown);\n+\t\t\ttry {\n+\t\t\t\tcloseAll(this::cleanupRequests, () -> dispatcher.fail(thrown == null ? new CancellationException() : thrown));\n+\t\t\t} catch (Exception e) {\n+\t\t\t\t//noinspection NonAtomicOperationOnVolatileField\n+\t\t\t\tthrown = ExceptionUtils.firstOrSuppressed(e, thrown);\n+\t\t\t}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "73e4094cc98004f8dcc51d4c495fceec3a35f964"}, "originalPosition": 21}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDEyNzkwOTgy", "url": "https://github.com/apache/flink/pull/12120#pullrequestreview-412790982", "createdAt": "2020-05-15T16:22:43Z", "commit": {"oid": "73e4094cc98004f8dcc51d4c495fceec3a35f964"}, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "73e4094cc98004f8dcc51d4c495fceec3a35f964", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/73e4094cc98004f8dcc51d4c495fceec3a35f964", "committedDate": "2020-05-15T07:17:41Z", "message": "fixup! [FLINK-17547][task] Use iterator for unconsumed buffers. Motivation: support spilled records Changes: 1. change SpillingAdaptiveSpanningRecordDeserializer.getUnconsumedBuffer signature 2. adapt channel state persistence to new types"}, "afterCommit": {"oid": "3892c6442227362797a73e8cc0fbfb5b11fa7863", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/3892c6442227362797a73e8cc0fbfb5b11fa7863", "committedDate": "2020-05-15T20:27:10Z", "message": "fixup! [FLINK-17547][task] Use iterator for unconsumed buffers. Motivation: support spilled records Changes: 1. change SpillingAdaptiveSpanningRecordDeserializer.getUnconsumedBuffer signature 2. adapt channel state persistence to new types"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "3892c6442227362797a73e8cc0fbfb5b11fa7863", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/3892c6442227362797a73e8cc0fbfb5b11fa7863", "committedDate": "2020-05-15T20:27:10Z", "message": "fixup! [FLINK-17547][task] Use iterator for unconsumed buffers. Motivation: support spilled records Changes: 1. change SpillingAdaptiveSpanningRecordDeserializer.getUnconsumedBuffer signature 2. adapt channel state persistence to new types"}, "afterCommit": {"oid": "4b453c84b4823034f4118e24191339851096097e", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/4b453c84b4823034f4118e24191339851096097e", "committedDate": "2020-05-15T20:32:44Z", "message": "fixup! [FLINK-17547][task] Use iterator for unconsumed buffers. Motivation: support spilled records Changes: 1. change SpillingAdaptiveSpanningRecordDeserializer.getUnconsumedBuffer signature 2. adapt channel state persistence to new types"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDEzMDUwMzA4", "url": "https://github.com/apache/flink/pull/12120#pullrequestreview-413050308", "createdAt": "2020-05-16T05:44:48Z", "commit": {"oid": "4b453c84b4823034f4118e24191339851096097e"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNlQwNTo0NDo0OVrOGWYZfg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNlQwNTo0NDo0OVrOGWYZfg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjEyMTU5OA==", "bodyText": "Why are we recycling buffers at the end, on close and not on the fly? It can mean GBs of extra (unnecessary) memory usage if we keep all unconsumed buffers from different channels. The ownership of the buffers returned from the iterator should belong to the caller of next() method.", "url": "https://github.com/apache/flink/pull/12120#discussion_r426121598", "createdAt": "2020-05-16T05:44:49Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/disk/FileBasedBufferIterator.java", "diffHunk": "@@ -85,6 +90,6 @@ private int read(byte[] buffer) {\n \n \t@Override\n \tpublic void close() throws Exception {\n-\t\tcloseAll(stream, file::release);\n+\t\tcloseAll(stream, file::release, () -> buffersToClose.forEach(Buffer::recycleBuffer));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4b453c84b4823034f4118e24191339851096097e"}, "originalPosition": 33}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "979c1dc26503c67f39af742c46e6df241e5e5ab7", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/979c1dc26503c67f39af742c46e6df241e5e5ab7", "committedDate": "2020-05-16T16:38:34Z", "message": "[FLINK-17547][task][hotfix] Improve error handling\n1 catch one more invalid input in DataOutputSerializer.write\n2 more informative error messages"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2472c7572e86b5c85d71d834a39c51da097fab6a", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/2472c7572e86b5c85d71d834a39c51da097fab6a", "committedDate": "2020-05-16T16:38:34Z", "message": "[FLINK-17547][task][hotfix] Extract NonSpanningWrapper from\nSpillingAdaptiveSpanningRecordDeserializer (static inner class)\nAs it is, no logical changes."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "224c2077305b4930e4ff8776791a16f9fc118782", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/224c2077305b4930e4ff8776791a16f9fc118782", "committedDate": "2020-05-16T16:38:34Z", "message": "[FLINK-17547][task][hotfix] Extract SpanningWrapper\nfrom SpillingAdaptiveSpanningRecordDeserializer (static inner class).\nAs it is, no logical changes."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "09f64114070ff7783e28d7cc273a31a54b1b7e28", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/09f64114070ff7783e28d7cc273a31a54b1b7e28", "committedDate": "2020-05-16T16:38:34Z", "message": "[FLINK-17547][task][hotfix] Fix compiler warnings in NonSpanningWrapper"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "eb63e8ca140a0d6f4a1fff1d4f8fe09fefc2c91f", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/eb63e8ca140a0d6f4a1fff1d4f8fe09fefc2c91f", "committedDate": "2020-05-16T16:38:35Z", "message": "[FLINK-17547][task][hotfix] Extract methods from RecordsDeserializer"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "a60972a16e43987b8fda57c83171a4ca639492cf", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/a60972a16e43987b8fda57c83171a4ca639492cf", "committedDate": "2020-05-16T08:17:08Z", "message": "fixup! [FLINK-17547][task] Use iterator for unconsumed buffers. Motivation: support spilled records Changes: 1. change SpillingAdaptiveSpanningRecordDeserializer.getUnconsumedBuffer signature 2. adapt channel state persistence to new types"}, "afterCommit": {"oid": "95c57fc02f0f4c16e685df54b249f2386177126c", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/95c57fc02f0f4c16e685df54b249f2386177126c", "committedDate": "2020-05-16T17:36:12Z", "message": "[FLINK-17547][task] Implement getUnconsumedSegment for spilled buffers"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "91fd89dff9bdbe4286d64a3c015e891e2199f173", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/91fd89dff9bdbe4286d64a3c015e891e2199f173", "committedDate": "2020-05-18T18:29:05Z", "message": "[FLINK-17547][task] Use iterator for unconsumed buffers.\nMotivation: support spilled records\nChanges:\n1. change SpillingAdaptiveSpanningRecordDeserializer.getUnconsumedBuffer\nsignature\n2. adapt channel state persistence to new types\n\nNo changes in existing logic."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a15a76f78250e35a48975dc91748cff0ba13c999", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/a15a76f78250e35a48975dc91748cff0ba13c999", "committedDate": "2020-05-18T18:29:14Z", "message": "[FLINK-17547][task][hotfix] Extract RefCountedFileWithStream from RefCountedFile\nMotivation: use RefCountedFile for reading as well."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bca823195e558bf3e85ca39356b7075c0a8ef6ec", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/bca823195e558bf3e85ca39356b7075c0a8ef6ec", "committedDate": "2020-05-18T18:29:14Z", "message": "[FLINK-17547][task][hotfix] Move RefCountedFile to flink-core\nto use it in SpanningWrapper"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a863cd18d1b42346eb8d826f8596d1755495b809", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/a863cd18d1b42346eb8d826f8596d1755495b809", "committedDate": "2020-05-18T18:29:14Z", "message": "[FLINK-17547][task] Use RefCountedFile in SpanningWrapper (todo: merge\nwith next?)"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0242c584e370d24eca0329895e81b0a11a4ed7cd", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/0242c584e370d24eca0329895e81b0a11a4ed7cd", "committedDate": "2020-05-18T18:30:23Z", "message": "[FLINK-17547][task] Implement getUnconsumedSegment for spilled buffers"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "95c57fc02f0f4c16e685df54b249f2386177126c", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/95c57fc02f0f4c16e685df54b249f2386177126c", "committedDate": "2020-05-16T17:36:12Z", "message": "[FLINK-17547][task] Implement getUnconsumedSegment for spilled buffers"}, "afterCommit": {"oid": "0242c584e370d24eca0329895e81b0a11a4ed7cd", "author": {"user": {"login": "rkhachatryan", "name": "Roman"}}, "url": "https://github.com/apache/flink/commit/0242c584e370d24eca0329895e81b0a11a4ed7cd", "committedDate": "2020-05-18T18:30:23Z", "message": "[FLINK-17547][task] Implement getUnconsumedSegment for spilled buffers"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE0NDIwNjM0", "url": "https://github.com/apache/flink/pull/12120#pullrequestreview-414420634", "createdAt": "2020-05-19T13:02:09Z", "commit": {"oid": "0242c584e370d24eca0329895e81b0a11a4ed7cd"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE0NDcxMjU0", "url": "https://github.com/apache/flink/pull/12120#pullrequestreview-414471254", "createdAt": "2020-05-19T13:57:17Z", "commit": {"oid": "95c57fc02f0f4c16e685df54b249f2386177126c"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4358, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}