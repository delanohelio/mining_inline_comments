{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDE1ODI3Njg2", "number": 12062, "reviewThreads": {"totalCount": 13, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMVQwNzoyNjo0M1rOD7HZrA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQxMzoyNzo1NVrOD83x4g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMzEzODM2OnYy", "diffSide": "RIGHT", "path": "flink-streaming-java/src/main/java/org/apache/flink/streaming/api/functions/sink/filesystem/StreamingFileSink.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMVQwNzoyNjo0M1rOGTP3PA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQwNDowNDoxNlrOGT1Rcw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjgzNjAyOA==", "bodyText": "Why do we need this change?", "url": "https://github.com/apache/flink/pull/12062#discussion_r422836028", "createdAt": "2020-05-11T07:26:43Z", "author": {"login": "lirui-apache"}, "path": "flink-streaming-java/src/main/java/org/apache/flink/streaming/api/functions/sink/filesystem/StreamingFileSink.java", "diffHunk": "@@ -467,7 +483,11 @@ public void onProcessingTime(long timestamp) throws Exception {\n \n \t@Override\n \tpublic void invoke(IN value, SinkFunction.Context context) throws Exception {\n-\t\tbuckets.onElement(value, context);\n+\t\tbuckets.onElement(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8ea1504837c815c55f8a41a7fa887285333c62a6"}, "originalPosition": 101}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQ0ODk0Nw==", "bodyText": "Will remove.", "url": "https://github.com/apache/flink/pull/12062#discussion_r423448947", "createdAt": "2020-05-12T04:04:16Z", "author": {"login": "JingsongLi"}, "path": "flink-streaming-java/src/main/java/org/apache/flink/streaming/api/functions/sink/filesystem/StreamingFileSink.java", "diffHunk": "@@ -467,7 +483,11 @@ public void onProcessingTime(long timestamp) throws Exception {\n \n \t@Override\n \tpublic void invoke(IN value, SinkFunction.Context context) throws Exception {\n-\t\tbuckets.onElement(value, context);\n+\t\tbuckets.onElement(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjgzNjAyOA=="}, "originalCommit": {"oid": "8ea1504837c815c55f8a41a7fa887285333c62a6"}, "originalPosition": 101}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMzYzMzYyOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemOptions.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMVQwOTozNTo1OFrOGTUgfQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMVQwOTozNTo1OFrOGTUgfQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjkxMjEyNQ==", "bodyText": "add descriptions", "url": "https://github.com/apache/flink/pull/12062#discussion_r422912125", "createdAt": "2020-05-11T09:35:58Z", "author": {"login": "lirui-apache"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemOptions.java", "diffHunk": "@@ -0,0 +1,78 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.configuration.ConfigOption;\n+\n+import java.time.Duration;\n+\n+import static org.apache.flink.configuration.ConfigOptions.key;\n+\n+/**\n+ * This class holds configuration constants used by filesystem(Including hive) connector.\n+ */\n+public class FileSystemOptions {\n+\n+\tpublic static final ConfigOption<String> PARTITION_TIME_EXTRACTOR_TYPE =\n+\t\t\tkey(\"partition.time-extractor.type\")\n+\t\t\t\t\t.stringType()\n+\t\t\t\t\t.defaultValue(\"default\")\n+\t\t\t\t\t.withDescription(\"Time extractor to extract time from partition values. Only be\" +\n+\t\t\t\t\t\t\t\" used if order is set to partition-time. Support default and custom.\" +\n+\t\t\t\t\t\t\t\" For default, can configure timestamp pattern.\" +\n+\t\t\t\t\t\t\t\" For custom, should configure extractor class.\");\n+\n+\tpublic static final ConfigOption<String> PARTITION_TIME_EXTRACTOR_CLASS =\n+\t\t\tkey(\"partition.time-extractor.class\")\n+\t\t\t\t\t.stringType()\n+\t\t\t\t\t.noDefaultValue()\n+\t\t\t\t\t.withDescription(\"The extractor class for implement PartitionTimeExtractor interface.\");\n+\n+\tpublic static final ConfigOption<String> PARTITION_TIME_EXTRACTOR_TIMESTAMP_PATTERN =\n+\t\t\tkey(\"partition.time-extractor.timestamp-pattern\")\n+\t\t\t\t\t.stringType()\n+\t\t\t\t\t.noDefaultValue()\n+\t\t\t\t\t.withDescription(\"The 'default' construction way allows users to use partition\" +\n+\t\t\t\t\t\t\t\" fields to get a legal timestamp pattern.\" +\n+\t\t\t\t\t\t\t\" Default support 'yyyy-mm-dd hh:mm:ss' from first field.\" +\n+\t\t\t\t\t\t\t\" If timestamp in partition is single field 'dt', can configure: '$dt'.\" +\n+\t\t\t\t\t\t\t\" If timestamp in partition is year, month, day, hour,\" +\n+\t\t\t\t\t\t\t\" can configure: '$year-$month-$day $hour:00:00'.\" +\n+\t\t\t\t\t\t\t\" If timestamp in partition is dt and hour, can configure: '$dt $hour:00:00'.\");\n+\n+\tpublic static final ConfigOption<Duration> PARTITION_TIME_INTERVAL =\n+\t\t\tkey(\"partition.time-interval\")\n+\t\t\t\t\t.durationType()\n+\t\t\t\t\t.noDefaultValue()\n+\t\t\t\t\t.withDescription(\"Interval time of partition,\" +\n+\t\t\t\t\t\t\t\" if it is a day partition, should be '1 d',\" +\n+\t\t\t\t\t\t\t\" if it is a hour partition, should be '1 h'\");\n+\n+\tpublic static final ConfigOption<String> PARTITION_COMMIT_POLICY_TYPE =\n+\t\t\tkey(\"partition.commit-policy.type\")\n+\t\t\t\t\t.stringType()\n+\t\t\t\t\t.noDefaultValue()\n+\t\t\t\t\t.withDescription(\"\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e5127c6a73d24f0c9c03deb0a7518fb18832b744"}, "originalPosition": 71}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzNDEyNTY1OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/StreamingFileCommitter.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMVQxMjowMToxNVrOGTZN-A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQwNDoxMTo0OVrOGT1Ycg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjk4OTMwNA==", "bodyText": "Can we have some comments about which partitions should be in this list? My understanding is it should include partitions for which some files should be committed, right?", "url": "https://github.com/apache/flink/pull/12062#discussion_r422989304", "createdAt": "2020-05-11T12:01:15Z", "author": {"login": "lirui-apache"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/StreamingFileCommitter.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem.stream;\n+\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.fs.FileSystem;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.filesystem.TableMetaStoreFactory;\n+\n+import java.io.Serializable;\n+import java.util.HashSet;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.TreeMap;\n+\n+import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_COMMIT_POLICY_SUCCESS_FILE_NAME;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_COMMIT_POLICY_TYPE;\n+import static org.apache.flink.table.utils.PartitionPathUtils.extractPartitionSpecFromPath;\n+import static org.apache.flink.table.utils.PartitionPathUtils.generatePartitionPath;\n+\n+/**\n+ * Committer for {@link StreamingFileWriter}. This is the single (non-parallel) task.\n+ */\n+public class StreamingFileCommitter extends AbstractStreamOperator<Void>\n+\t\timplements OneInputStreamOperator<StreamingFileCommitter.CommitMessage, Void> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate final Configuration conf;\n+\n+\tprivate final List<String> partitionKeys;\n+\n+\tprivate final TableMetaStoreFactory metaStoreFactory;\n+\n+\tprivate transient PartitionCommitManager commitManager;\n+\n+\tprivate transient TaskTracker taskTracker;\n+\n+\tprivate transient long currentWatermark = Long.MIN_VALUE;\n+\n+\tprivate transient List<PartitionCommitPolicy> policies;\n+\n+\tpublic StreamingFileCommitter(\n+\t\t\tList<String> partitionKeys, TableMetaStoreFactory metaStoreFactory, Configuration conf) {\n+\t\tthis.partitionKeys = partitionKeys;\n+\t\tthis.metaStoreFactory = metaStoreFactory;\n+\t\tthis.conf = conf;\n+\t}\n+\n+\t@Override\n+\tpublic void initializeState(StateInitializationContext context) throws Exception {\n+\t\tsuper.initializeState(context);\n+\t\tthis.commitManager = new PartitionCommitManager(\n+\t\t\t\tcontext.isRestored(),\n+\t\t\t\tcontext.getOperatorStateStore(),\n+\t\t\t\tgetUserCodeClassloader(),\n+\t\t\t\tpartitionKeys,\n+\t\t\t\tconf);\n+\t\tthis.policies = PartitionCommitPolicy.createCommitChain(\n+\t\t\t\tconf.get(PARTITION_COMMIT_POLICY_TYPE),\n+\t\t\t\tconf.get(PARTITION_COMMIT_POLICY_SUCCESS_FILE_NAME));\n+\t}\n+\n+\t@Override\n+\tpublic void processElement(StreamRecord<CommitMessage> element) throws Exception {\n+\t\tCommitMessage message = element.getValue();\n+\t\tfor (String partition : message.partitions) {\n+\t\t\tcommitManager.addPartition(partition);\n+\t\t}\n+\n+\t\tif (taskTracker == null) {\n+\t\t\ttaskTracker = new TaskTracker(message.numberOfTasks);\n+\t\t}\n+\t\tboolean needCommit = taskTracker.add(message.checkpointId, message.taskId);\n+\t\tif (needCommit) {\n+\t\t\tcommitPartitions(commitManager.triggerCommit(message.checkpointId));\n+\t\t}\n+\t}\n+\n+\tprivate void commitPartitions(List<String> partitions) throws Exception {\n+\t\ttry (TableMetaStoreFactory.TableMetaStore metaStore = metaStoreFactory.createTableMetaStore()) {\n+\t\t\tFileSystem fs = metaStore.getLocationPath().getFileSystem();\n+\t\t\tfor (String partition : partitions) {\n+\t\t\t\tLinkedHashMap<String, String> partSpec = extractPartitionSpecFromPath(new Path(partition));\n+\t\t\t\tPath path = new Path(metaStore.getLocationPath(), generatePartitionPath(partSpec));\n+\t\t\t\tfor (PartitionCommitPolicy policy : policies) {\n+\t\t\t\t\tpolicy.commit(partSpec, path, fs, metaStore);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void processWatermark(Watermark mark) throws Exception {\n+\t\tsuper.processWatermark(mark);\n+\t\tthis.currentWatermark = mark.getTimestamp();\n+\t}\n+\n+\t@Override\n+\tpublic void snapshotState(StateSnapshotContext context) throws Exception {\n+\t\tsuper.snapshotState(context);\n+\t\tcommitManager.snapshotState(context.getCheckpointId(), currentWatermark);\n+\t}\n+\n+\t/**\n+\t * The message sent upstream.\n+\t */\n+\tpublic static class CommitMessage implements Serializable {\n+\n+\t\tpublic long checkpointId;\n+\t\tpublic int taskId;\n+\t\tpublic int numberOfTasks;\n+\t\tpublic List<String> partitions;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f8bdc31fb33986d81fe96a21019c1beaf0487cfe"}, "originalPosition": 136}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQ1MDczOA==", "bodyText": "Yes.", "url": "https://github.com/apache/flink/pull/12062#discussion_r423450738", "createdAt": "2020-05-12T04:11:49Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/StreamingFileCommitter.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem.stream;\n+\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.fs.FileSystem;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.filesystem.TableMetaStoreFactory;\n+\n+import java.io.Serializable;\n+import java.util.HashSet;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.TreeMap;\n+\n+import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_COMMIT_POLICY_SUCCESS_FILE_NAME;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_COMMIT_POLICY_TYPE;\n+import static org.apache.flink.table.utils.PartitionPathUtils.extractPartitionSpecFromPath;\n+import static org.apache.flink.table.utils.PartitionPathUtils.generatePartitionPath;\n+\n+/**\n+ * Committer for {@link StreamingFileWriter}. This is the single (non-parallel) task.\n+ */\n+public class StreamingFileCommitter extends AbstractStreamOperator<Void>\n+\t\timplements OneInputStreamOperator<StreamingFileCommitter.CommitMessage, Void> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate final Configuration conf;\n+\n+\tprivate final List<String> partitionKeys;\n+\n+\tprivate final TableMetaStoreFactory metaStoreFactory;\n+\n+\tprivate transient PartitionCommitManager commitManager;\n+\n+\tprivate transient TaskTracker taskTracker;\n+\n+\tprivate transient long currentWatermark = Long.MIN_VALUE;\n+\n+\tprivate transient List<PartitionCommitPolicy> policies;\n+\n+\tpublic StreamingFileCommitter(\n+\t\t\tList<String> partitionKeys, TableMetaStoreFactory metaStoreFactory, Configuration conf) {\n+\t\tthis.partitionKeys = partitionKeys;\n+\t\tthis.metaStoreFactory = metaStoreFactory;\n+\t\tthis.conf = conf;\n+\t}\n+\n+\t@Override\n+\tpublic void initializeState(StateInitializationContext context) throws Exception {\n+\t\tsuper.initializeState(context);\n+\t\tthis.commitManager = new PartitionCommitManager(\n+\t\t\t\tcontext.isRestored(),\n+\t\t\t\tcontext.getOperatorStateStore(),\n+\t\t\t\tgetUserCodeClassloader(),\n+\t\t\t\tpartitionKeys,\n+\t\t\t\tconf);\n+\t\tthis.policies = PartitionCommitPolicy.createCommitChain(\n+\t\t\t\tconf.get(PARTITION_COMMIT_POLICY_TYPE),\n+\t\t\t\tconf.get(PARTITION_COMMIT_POLICY_SUCCESS_FILE_NAME));\n+\t}\n+\n+\t@Override\n+\tpublic void processElement(StreamRecord<CommitMessage> element) throws Exception {\n+\t\tCommitMessage message = element.getValue();\n+\t\tfor (String partition : message.partitions) {\n+\t\t\tcommitManager.addPartition(partition);\n+\t\t}\n+\n+\t\tif (taskTracker == null) {\n+\t\t\ttaskTracker = new TaskTracker(message.numberOfTasks);\n+\t\t}\n+\t\tboolean needCommit = taskTracker.add(message.checkpointId, message.taskId);\n+\t\tif (needCommit) {\n+\t\t\tcommitPartitions(commitManager.triggerCommit(message.checkpointId));\n+\t\t}\n+\t}\n+\n+\tprivate void commitPartitions(List<String> partitions) throws Exception {\n+\t\ttry (TableMetaStoreFactory.TableMetaStore metaStore = metaStoreFactory.createTableMetaStore()) {\n+\t\t\tFileSystem fs = metaStore.getLocationPath().getFileSystem();\n+\t\t\tfor (String partition : partitions) {\n+\t\t\t\tLinkedHashMap<String, String> partSpec = extractPartitionSpecFromPath(new Path(partition));\n+\t\t\t\tPath path = new Path(metaStore.getLocationPath(), generatePartitionPath(partSpec));\n+\t\t\t\tfor (PartitionCommitPolicy policy : policies) {\n+\t\t\t\t\tpolicy.commit(partSpec, path, fs, metaStore);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void processWatermark(Watermark mark) throws Exception {\n+\t\tsuper.processWatermark(mark);\n+\t\tthis.currentWatermark = mark.getTimestamp();\n+\t}\n+\n+\t@Override\n+\tpublic void snapshotState(StateSnapshotContext context) throws Exception {\n+\t\tsuper.snapshotState(context);\n+\t\tcommitManager.snapshotState(context.getCheckpointId(), currentWatermark);\n+\t}\n+\n+\t/**\n+\t * The message sent upstream.\n+\t */\n+\tpublic static class CommitMessage implements Serializable {\n+\n+\t\tpublic long checkpointId;\n+\t\tpublic int taskId;\n+\t\tpublic int numberOfTasks;\n+\t\tpublic List<String> partitions;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjk4OTMwNA=="}, "originalCommit": {"oid": "f8bdc31fb33986d81fe96a21019c1beaf0487cfe"}, "originalPosition": 136}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzNDEyOTgzOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/PartitionCommitManager.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMVQxMjowMjozOFrOGTZQnw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQwNDoyMDo0N1rOGT1gmA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjk4OTk4Mw==", "bodyText": "I don't think this method really \"triggers\" the commit. Seems it just decides which partitions should be committed. So maybe rename to getPartitionsToCommit?", "url": "https://github.com/apache/flink/pull/12062#discussion_r422989983", "createdAt": "2020-05-11T12:02:38Z", "author": {"login": "lirui-apache"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/PartitionCommitManager.java", "diffHunk": "@@ -0,0 +1,143 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem.stream;\n+\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.state.OperatorStateStore;\n+import org.apache.flink.api.common.typeutils.base.ListSerializer;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.api.common.typeutils.base.MapSerializer;\n+import org.apache.flink.api.common.typeutils.base.StringSerializer;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.filesystem.PartitionTimeExtractor;\n+import org.apache.flink.util.StringUtils;\n+\n+import java.time.Duration;\n+import java.time.LocalDateTime;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.TreeMap;\n+\n+import static org.apache.flink.table.filesystem.DefaultPartTimeExtractor.toMills;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_CLASS;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_TIMESTAMP_PATTERN;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_KIND;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_INTERVAL;\n+import static org.apache.flink.table.utils.PartitionPathUtils.extractPartitionValues;\n+\n+/**\n+ * Manage partition and watermark information.\n+ */\n+public class PartitionCommitManager {\n+\n+\tprivate static final ListStateDescriptor<Map<Long, Long>> WATERMARKS_STATE_DESC =\n+\t\t\tnew ListStateDescriptor<>(\n+\t\t\t\t\t\"checkpoint-id-to-watermark\",\n+\t\t\t\t\tnew MapSerializer<>(LongSerializer.INSTANCE, LongSerializer.INSTANCE));\n+\tprivate static final ListStateDescriptor<List<String>> PENDING_PARTITIONS_STATE_DESC =\n+\t\t\tnew ListStateDescriptor<>(\n+\t\t\t\t\t\"pending-partitions\",\n+\t\t\t\t\tnew ListSerializer<>(StringSerializer.INSTANCE));\n+\n+\tprivate final ListState<Map<Long, Long>> watermarksState;\n+\tprivate final ListState<List<String>> pendingPartitionsState;\n+\tprivate final TreeMap<Long, Long> watermarks;\n+\tprivate final Set<String> pendingPartitions;\n+\tprivate final PartitionTimeExtractor extractor;\n+\tprivate final long timeIntervalMills;\n+\tprivate final List<String> partitionKeys;\n+\n+\tpublic PartitionCommitManager(\n+\t\t\tboolean isRestored,\n+\t\t\tOperatorStateStore operatorStateStore,\n+\t\t\tClassLoader userCodeClassLoader,\n+\t\t\tList<String> partitionKeys,\n+\t\t\tConfiguration conf) throws Exception {\n+\t\tthis.partitionKeys = partitionKeys;\n+\t\tString extractorKind = conf.get(PARTITION_TIME_EXTRACTOR_KIND);\n+\t\tString extractorClass = conf.get(PARTITION_TIME_EXTRACTOR_CLASS);\n+\t\tString extractorPattern = conf.get(PARTITION_TIME_EXTRACTOR_TIMESTAMP_PATTERN);\n+\t\tthis.timeIntervalMills = conf.getOptional(PARTITION_TIME_INTERVAL)\n+\t\t\t\t.map(Duration::toMillis)\n+\t\t\t\t.orElse(Long.MAX_VALUE);\n+\t\tthis.extractor = PartitionTimeExtractor.create(\n+\t\t\t\tuserCodeClassLoader,\n+\t\t\t\textractorKind,\n+\t\t\t\textractorClass,\n+\t\t\t\textractorPattern);\n+\n+\t\tthis.watermarksState = operatorStateStore.getListState(WATERMARKS_STATE_DESC);\n+\t\tthis.pendingPartitionsState = operatorStateStore.getListState(PENDING_PARTITIONS_STATE_DESC);\n+\n+\t\tthis.watermarks = new TreeMap<>();\n+\t\tthis.pendingPartitions = new HashSet<>();\n+\t\tif (isRestored) {\n+\t\t\twatermarks.putAll(watermarksState.get().iterator().next());\n+\t\t\tpendingPartitions.addAll(pendingPartitionsState.get().iterator().next());\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * Add a pending partition.\n+\t */\n+\tpublic void addPartition(String partition) {\n+\t\tif (!StringUtils.isNullOrWhitespaceOnly(partition)) {\n+\t\t\tthis.pendingPartitions.add(partition);\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * Trigger commit of pending partitions, and cleanup useless watermarks and partitions.\n+\t */\n+\tpublic List<String> triggerCommit(long checkpointId) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f8bdc31fb33986d81fe96a21019c1beaf0487cfe"}, "originalPosition": 115}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQ1MjgyNA==", "bodyText": "committablePartitions", "url": "https://github.com/apache/flink/pull/12062#discussion_r423452824", "createdAt": "2020-05-12T04:20:47Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/PartitionCommitManager.java", "diffHunk": "@@ -0,0 +1,143 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem.stream;\n+\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.state.OperatorStateStore;\n+import org.apache.flink.api.common.typeutils.base.ListSerializer;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.api.common.typeutils.base.MapSerializer;\n+import org.apache.flink.api.common.typeutils.base.StringSerializer;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.filesystem.PartitionTimeExtractor;\n+import org.apache.flink.util.StringUtils;\n+\n+import java.time.Duration;\n+import java.time.LocalDateTime;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.TreeMap;\n+\n+import static org.apache.flink.table.filesystem.DefaultPartTimeExtractor.toMills;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_CLASS;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_TIMESTAMP_PATTERN;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_KIND;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_INTERVAL;\n+import static org.apache.flink.table.utils.PartitionPathUtils.extractPartitionValues;\n+\n+/**\n+ * Manage partition and watermark information.\n+ */\n+public class PartitionCommitManager {\n+\n+\tprivate static final ListStateDescriptor<Map<Long, Long>> WATERMARKS_STATE_DESC =\n+\t\t\tnew ListStateDescriptor<>(\n+\t\t\t\t\t\"checkpoint-id-to-watermark\",\n+\t\t\t\t\tnew MapSerializer<>(LongSerializer.INSTANCE, LongSerializer.INSTANCE));\n+\tprivate static final ListStateDescriptor<List<String>> PENDING_PARTITIONS_STATE_DESC =\n+\t\t\tnew ListStateDescriptor<>(\n+\t\t\t\t\t\"pending-partitions\",\n+\t\t\t\t\tnew ListSerializer<>(StringSerializer.INSTANCE));\n+\n+\tprivate final ListState<Map<Long, Long>> watermarksState;\n+\tprivate final ListState<List<String>> pendingPartitionsState;\n+\tprivate final TreeMap<Long, Long> watermarks;\n+\tprivate final Set<String> pendingPartitions;\n+\tprivate final PartitionTimeExtractor extractor;\n+\tprivate final long timeIntervalMills;\n+\tprivate final List<String> partitionKeys;\n+\n+\tpublic PartitionCommitManager(\n+\t\t\tboolean isRestored,\n+\t\t\tOperatorStateStore operatorStateStore,\n+\t\t\tClassLoader userCodeClassLoader,\n+\t\t\tList<String> partitionKeys,\n+\t\t\tConfiguration conf) throws Exception {\n+\t\tthis.partitionKeys = partitionKeys;\n+\t\tString extractorKind = conf.get(PARTITION_TIME_EXTRACTOR_KIND);\n+\t\tString extractorClass = conf.get(PARTITION_TIME_EXTRACTOR_CLASS);\n+\t\tString extractorPattern = conf.get(PARTITION_TIME_EXTRACTOR_TIMESTAMP_PATTERN);\n+\t\tthis.timeIntervalMills = conf.getOptional(PARTITION_TIME_INTERVAL)\n+\t\t\t\t.map(Duration::toMillis)\n+\t\t\t\t.orElse(Long.MAX_VALUE);\n+\t\tthis.extractor = PartitionTimeExtractor.create(\n+\t\t\t\tuserCodeClassLoader,\n+\t\t\t\textractorKind,\n+\t\t\t\textractorClass,\n+\t\t\t\textractorPattern);\n+\n+\t\tthis.watermarksState = operatorStateStore.getListState(WATERMARKS_STATE_DESC);\n+\t\tthis.pendingPartitionsState = operatorStateStore.getListState(PENDING_PARTITIONS_STATE_DESC);\n+\n+\t\tthis.watermarks = new TreeMap<>();\n+\t\tthis.pendingPartitions = new HashSet<>();\n+\t\tif (isRestored) {\n+\t\t\twatermarks.putAll(watermarksState.get().iterator().next());\n+\t\t\tpendingPartitions.addAll(pendingPartitionsState.get().iterator().next());\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * Add a pending partition.\n+\t */\n+\tpublic void addPartition(String partition) {\n+\t\tif (!StringUtils.isNullOrWhitespaceOnly(partition)) {\n+\t\t\tthis.pendingPartitions.add(partition);\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * Trigger commit of pending partitions, and cleanup useless watermarks and partitions.\n+\t */\n+\tpublic List<String> triggerCommit(long checkpointId) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjk4OTk4Mw=="}, "originalCommit": {"oid": "f8bdc31fb33986d81fe96a21019c1beaf0487cfe"}, "originalPosition": 115}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzNDE0MjMxOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/PartitionCommitPolicy.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMVQxMjowNjo0MFrOGTZYfg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMVQxMjowNjo0MFrOGTZYfg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjk5MTk5OA==", "bodyText": "add constants for them", "url": "https://github.com/apache/flink/pull/12062#discussion_r422991998", "createdAt": "2020-05-11T12:06:40Z", "author": {"login": "lirui-apache"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/PartitionCommitPolicy.java", "diffHunk": "@@ -0,0 +1,63 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem.stream;\n+\n+import org.apache.flink.core.fs.FileSystem;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.filesystem.TableMetaStoreFactory;\n+\n+import java.io.Serializable;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Policy for partition commit.\n+ */\n+public interface PartitionCommitPolicy extends Serializable {\n+\n+\t/**\n+\t * Commit partition by partitionSpec and path.\n+\t */\n+\tvoid commit(\n+\t\t\tLinkedHashMap<String, String> partitionSpec,\n+\t\t\tPath partitionPath,\n+\t\t\tFileSystem fileSystem,\n+\t\t\tTableMetaStoreFactory.TableMetaStore metaStore) throws Exception;\n+\n+\tstatic List<PartitionCommitPolicy> createCommitChain(String policy, String successFileName) {\n+\t\tif (policy == null) {\n+\t\t\treturn Collections.emptyList();\n+\t\t}\n+\t\tString[] policyStrings = policy.split(\",\");\n+\t\treturn Arrays.stream(policyStrings).map(name -> {\n+\t\t\tswitch (name) {\n+\t\t\t\tcase \"metastore\":", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f8bdc31fb33986d81fe96a21019c1beaf0487cfe"}, "originalPosition": 53}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzNDE2MjE5OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/StreamingFileCommitter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMVQxMjoxMzoxNFrOGTZlAQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMVQxMjoxMzoxNFrOGTZlAQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjk5NTIwMQ==", "bodyText": "Does this invoke traffic to HMS? If so maybe we should only do it when partitions is not empty.", "url": "https://github.com/apache/flink/pull/12062#discussion_r422995201", "createdAt": "2020-05-11T12:13:14Z", "author": {"login": "lirui-apache"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/StreamingFileCommitter.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem.stream;\n+\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.fs.FileSystem;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.filesystem.TableMetaStoreFactory;\n+\n+import java.io.Serializable;\n+import java.util.HashSet;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.TreeMap;\n+\n+import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_COMMIT_POLICY_SUCCESS_FILE_NAME;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_COMMIT_POLICY_TYPE;\n+import static org.apache.flink.table.utils.PartitionPathUtils.extractPartitionSpecFromPath;\n+import static org.apache.flink.table.utils.PartitionPathUtils.generatePartitionPath;\n+\n+/**\n+ * Committer for {@link StreamingFileWriter}. This is the single (non-parallel) task.\n+ */\n+public class StreamingFileCommitter extends AbstractStreamOperator<Void>\n+\t\timplements OneInputStreamOperator<StreamingFileCommitter.CommitMessage, Void> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate final Configuration conf;\n+\n+\tprivate final List<String> partitionKeys;\n+\n+\tprivate final TableMetaStoreFactory metaStoreFactory;\n+\n+\tprivate transient PartitionCommitManager commitManager;\n+\n+\tprivate transient TaskTracker taskTracker;\n+\n+\tprivate transient long currentWatermark = Long.MIN_VALUE;\n+\n+\tprivate transient List<PartitionCommitPolicy> policies;\n+\n+\tpublic StreamingFileCommitter(\n+\t\t\tList<String> partitionKeys, TableMetaStoreFactory metaStoreFactory, Configuration conf) {\n+\t\tthis.partitionKeys = partitionKeys;\n+\t\tthis.metaStoreFactory = metaStoreFactory;\n+\t\tthis.conf = conf;\n+\t}\n+\n+\t@Override\n+\tpublic void initializeState(StateInitializationContext context) throws Exception {\n+\t\tsuper.initializeState(context);\n+\t\tthis.commitManager = new PartitionCommitManager(\n+\t\t\t\tcontext.isRestored(),\n+\t\t\t\tcontext.getOperatorStateStore(),\n+\t\t\t\tgetUserCodeClassloader(),\n+\t\t\t\tpartitionKeys,\n+\t\t\t\tconf);\n+\t\tthis.policies = PartitionCommitPolicy.createCommitChain(\n+\t\t\t\tconf.get(PARTITION_COMMIT_POLICY_TYPE),\n+\t\t\t\tconf.get(PARTITION_COMMIT_POLICY_SUCCESS_FILE_NAME));\n+\t}\n+\n+\t@Override\n+\tpublic void processElement(StreamRecord<CommitMessage> element) throws Exception {\n+\t\tCommitMessage message = element.getValue();\n+\t\tfor (String partition : message.partitions) {\n+\t\t\tcommitManager.addPartition(partition);\n+\t\t}\n+\n+\t\tif (taskTracker == null) {\n+\t\t\ttaskTracker = new TaskTracker(message.numberOfTasks);\n+\t\t}\n+\t\tboolean needCommit = taskTracker.add(message.checkpointId, message.taskId);\n+\t\tif (needCommit) {\n+\t\t\tcommitPartitions(commitManager.triggerCommit(message.checkpointId));\n+\t\t}\n+\t}\n+\n+\tprivate void commitPartitions(List<String> partitions) throws Exception {\n+\t\ttry (TableMetaStoreFactory.TableMetaStore metaStore = metaStoreFactory.createTableMetaStore()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f8bdc31fb33986d81fe96a21019c1beaf0487cfe"}, "originalPosition": 104}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzNDE4NTc1OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/StreamingFileWriter.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMVQxMjoyMDoyNVrOGTZz5A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQwNToyMToxMlrOGT2dAg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjk5OTAxMg==", "bodyText": "Is it possible that there's some pending data between close and the last notifyCheckpointComplete?", "url": "https://github.com/apache/flink/pull/12062#discussion_r422999012", "createdAt": "2020-05-11T12:20:25Z", "author": {"login": "lirui-apache"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/StreamingFileWriter.java", "diffHunk": "@@ -0,0 +1,180 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem.stream;\n+\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.state.OperatorStateStore;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.functions.sink.filesystem.Buckets;\n+import org.apache.flink.streaming.api.functions.sink.filesystem.StreamingFileSink;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.streaming.runtime.tasks.ProcessingTimeCallback;\n+import org.apache.flink.streaming.runtime.tasks.ProcessingTimeService;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.filesystem.stream.StreamingFileCommitter.CommitMessage;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.util.ArrayList;\n+import java.util.HashSet;\n+import java.util.Set;\n+\n+/**\n+ * Operator for file system sink.\n+ */\n+public class StreamingFileWriter extends AbstractStreamOperator<CommitMessage>\n+\t\timplements OneInputStreamOperator<RowData, CommitMessage>, ProcessingTimeCallback {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\t// -------------------------- state descriptors ---------------------------\n+\n+\tprivate static final ListStateDescriptor<byte[]> BUCKET_STATE_DESC =\n+\t\t\tnew ListStateDescriptor<>(\"bucket-states\", BytePrimitiveArraySerializer.INSTANCE);\n+\n+\tprivate static final ListStateDescriptor<Long> MAX_PART_COUNTER_STATE_DESC =\n+\t\t\tnew ListStateDescriptor<>(\"max-part-counter\", LongSerializer.INSTANCE);\n+\n+\t// ------------------------ configuration fields --------------------------\n+\n+\tprivate final long bucketCheckInterval;\n+\n+\tprivate final StreamingFileSink.BucketsBuilder<RowData, ?, ? extends\n+\t\t\tStreamingFileSink.BucketsBuilder<RowData, ?, ?>> bucketsBuilder;\n+\n+\tprivate final FileSystemBucketListener listener;\n+\n+\t// --------------------------- runtime fields -----------------------------\n+\n+\tprivate transient Buckets<RowData, ?> buckets;\n+\n+\tprivate transient ProcessingTimeService processingTimeService;\n+\n+\tprivate transient long currentWatermark = Long.MIN_VALUE;\n+\n+\tprivate transient Set<String> inactivePartitions;\n+\n+\t// --------------------------- State Related Fields -----------------------------\n+\n+\tprivate transient ListState<byte[]> bucketStates;\n+\n+\tprivate transient ListState<Long> maxPartCountersState;\n+\n+\tpublic StreamingFileWriter(\n+\t\t\tlong bucketCheckInterval,\n+\t\t\tStreamingFileSink.BucketsBuilder<RowData, ?, ? extends\n+\t\t\t\t\tStreamingFileSink.BucketsBuilder<RowData, ?, ?>> bucketsBuilder,\n+\t\t\tFileSystemBucketListener listener) {\n+\t\tthis.bucketCheckInterval = bucketCheckInterval;\n+\t\tthis.bucketsBuilder = bucketsBuilder;\n+\t\tthis.listener = listener;\n+\t}\n+\n+\t@Override\n+\tpublic void initializeState(StateInitializationContext context) throws Exception {\n+\t\tsuper.initializeState(context);\n+\t\tfinal int subtaskIndex = getRuntimeContext().getIndexOfThisSubtask();\n+\t\tthis.buckets = bucketsBuilder.createBuckets(subtaskIndex);\n+\n+\t\tfinal OperatorStateStore stateStore = context.getOperatorStateStore();\n+\t\tbucketStates = stateStore.getListState(BUCKET_STATE_DESC);\n+\t\tmaxPartCountersState = stateStore.getUnionListState(MAX_PART_COUNTER_STATE_DESC);\n+\n+\t\tif (context.isRestored()) {\n+\t\t\tbuckets.initializeState(bucketStates, maxPartCountersState);\n+\t\t}\n+\t\tinactivePartitions = new HashSet<>();\n+\t\tlistener.setInactiveConsumer(b -> inactivePartitions.add(b));\n+\t}\n+\n+\t@Override\n+\tpublic void snapshotState(StateSnapshotContext context) throws Exception {\n+\t\tsuper.snapshotState(context);\n+\t\tPreconditions.checkState(bucketStates != null && maxPartCountersState != null, \"sink has not been initialized\");\n+\t\tbuckets.snapshotState(\n+\t\t\t\tcontext.getCheckpointId(),\n+\t\t\t\tbucketStates,\n+\t\t\t\tmaxPartCountersState);\n+\t}\n+\n+\t@Override\n+\tpublic void open() throws Exception {\n+\t\tsuper.open();\n+\t\tthis.processingTimeService = getRuntimeContext().getProcessingTimeService();\n+\t\tlong currentProcessingTime = processingTimeService.getCurrentProcessingTime();\n+\t\tprocessingTimeService.registerTimer(currentProcessingTime + bucketCheckInterval, this);\n+\t}\n+\n+\t@Override\n+\tpublic void onProcessingTime(long timestamp) throws Exception {\n+\t\tfinal long currentTime = processingTimeService.getCurrentProcessingTime();\n+\t\tbuckets.onProcessingTime(currentTime);\n+\t\tprocessingTimeService.registerTimer(currentTime + bucketCheckInterval, this);\n+\t}\n+\n+\t@Override\n+\tpublic void processWatermark(Watermark mark) throws Exception {\n+\t\tsuper.processWatermark(mark);\n+\t\tthis.currentWatermark = mark.getTimestamp();\n+\t}\n+\n+\t@Override\n+\tpublic void processElement(StreamRecord<RowData> element) throws Exception {\n+\t\tbuckets.onElement(\n+\t\t\t\telement.getValue(),\n+\t\t\t\tgetProcessingTimeService().getCurrentProcessingTime(),\n+\t\t\t\telement.getTimestamp(),\n+\t\t\t\tcurrentWatermark);\n+\t}\n+\n+\t@Override\n+\tpublic void notifyCheckpointComplete(long checkpointId) throws Exception {\n+\t\tsuper.notifyCheckpointComplete(checkpointId);\n+\t\tbuckets.commitUpToCheckpoint(checkpointId);\n+\t\tCommitMessage message = new CommitMessage(\n+\t\t\t\tcheckpointId,\n+\t\t\t\tgetRuntimeContext().getIndexOfThisSubtask(),\n+\t\t\t\tgetRuntimeContext().getNumberOfParallelSubtasks(),\n+\t\t\t\tnew ArrayList<>(inactivePartitions));\n+\t\toutput.collect(new StreamRecord<>(message));\n+\t\tinactivePartitions.clear();\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f8bdc31fb33986d81fe96a21019c1beaf0487cfe"}, "originalPosition": 166}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQ2ODI5MA==", "bodyText": "I'll modify to endInput and support commit pending data.", "url": "https://github.com/apache/flink/pull/12062#discussion_r423468290", "createdAt": "2020-05-12T05:21:12Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/StreamingFileWriter.java", "diffHunk": "@@ -0,0 +1,180 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem.stream;\n+\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.state.OperatorStateStore;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.functions.sink.filesystem.Buckets;\n+import org.apache.flink.streaming.api.functions.sink.filesystem.StreamingFileSink;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.streaming.runtime.tasks.ProcessingTimeCallback;\n+import org.apache.flink.streaming.runtime.tasks.ProcessingTimeService;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.filesystem.stream.StreamingFileCommitter.CommitMessage;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.util.ArrayList;\n+import java.util.HashSet;\n+import java.util.Set;\n+\n+/**\n+ * Operator for file system sink.\n+ */\n+public class StreamingFileWriter extends AbstractStreamOperator<CommitMessage>\n+\t\timplements OneInputStreamOperator<RowData, CommitMessage>, ProcessingTimeCallback {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\t// -------------------------- state descriptors ---------------------------\n+\n+\tprivate static final ListStateDescriptor<byte[]> BUCKET_STATE_DESC =\n+\t\t\tnew ListStateDescriptor<>(\"bucket-states\", BytePrimitiveArraySerializer.INSTANCE);\n+\n+\tprivate static final ListStateDescriptor<Long> MAX_PART_COUNTER_STATE_DESC =\n+\t\t\tnew ListStateDescriptor<>(\"max-part-counter\", LongSerializer.INSTANCE);\n+\n+\t// ------------------------ configuration fields --------------------------\n+\n+\tprivate final long bucketCheckInterval;\n+\n+\tprivate final StreamingFileSink.BucketsBuilder<RowData, ?, ? extends\n+\t\t\tStreamingFileSink.BucketsBuilder<RowData, ?, ?>> bucketsBuilder;\n+\n+\tprivate final FileSystemBucketListener listener;\n+\n+\t// --------------------------- runtime fields -----------------------------\n+\n+\tprivate transient Buckets<RowData, ?> buckets;\n+\n+\tprivate transient ProcessingTimeService processingTimeService;\n+\n+\tprivate transient long currentWatermark = Long.MIN_VALUE;\n+\n+\tprivate transient Set<String> inactivePartitions;\n+\n+\t// --------------------------- State Related Fields -----------------------------\n+\n+\tprivate transient ListState<byte[]> bucketStates;\n+\n+\tprivate transient ListState<Long> maxPartCountersState;\n+\n+\tpublic StreamingFileWriter(\n+\t\t\tlong bucketCheckInterval,\n+\t\t\tStreamingFileSink.BucketsBuilder<RowData, ?, ? extends\n+\t\t\t\t\tStreamingFileSink.BucketsBuilder<RowData, ?, ?>> bucketsBuilder,\n+\t\t\tFileSystemBucketListener listener) {\n+\t\tthis.bucketCheckInterval = bucketCheckInterval;\n+\t\tthis.bucketsBuilder = bucketsBuilder;\n+\t\tthis.listener = listener;\n+\t}\n+\n+\t@Override\n+\tpublic void initializeState(StateInitializationContext context) throws Exception {\n+\t\tsuper.initializeState(context);\n+\t\tfinal int subtaskIndex = getRuntimeContext().getIndexOfThisSubtask();\n+\t\tthis.buckets = bucketsBuilder.createBuckets(subtaskIndex);\n+\n+\t\tfinal OperatorStateStore stateStore = context.getOperatorStateStore();\n+\t\tbucketStates = stateStore.getListState(BUCKET_STATE_DESC);\n+\t\tmaxPartCountersState = stateStore.getUnionListState(MAX_PART_COUNTER_STATE_DESC);\n+\n+\t\tif (context.isRestored()) {\n+\t\t\tbuckets.initializeState(bucketStates, maxPartCountersState);\n+\t\t}\n+\t\tinactivePartitions = new HashSet<>();\n+\t\tlistener.setInactiveConsumer(b -> inactivePartitions.add(b));\n+\t}\n+\n+\t@Override\n+\tpublic void snapshotState(StateSnapshotContext context) throws Exception {\n+\t\tsuper.snapshotState(context);\n+\t\tPreconditions.checkState(bucketStates != null && maxPartCountersState != null, \"sink has not been initialized\");\n+\t\tbuckets.snapshotState(\n+\t\t\t\tcontext.getCheckpointId(),\n+\t\t\t\tbucketStates,\n+\t\t\t\tmaxPartCountersState);\n+\t}\n+\n+\t@Override\n+\tpublic void open() throws Exception {\n+\t\tsuper.open();\n+\t\tthis.processingTimeService = getRuntimeContext().getProcessingTimeService();\n+\t\tlong currentProcessingTime = processingTimeService.getCurrentProcessingTime();\n+\t\tprocessingTimeService.registerTimer(currentProcessingTime + bucketCheckInterval, this);\n+\t}\n+\n+\t@Override\n+\tpublic void onProcessingTime(long timestamp) throws Exception {\n+\t\tfinal long currentTime = processingTimeService.getCurrentProcessingTime();\n+\t\tbuckets.onProcessingTime(currentTime);\n+\t\tprocessingTimeService.registerTimer(currentTime + bucketCheckInterval, this);\n+\t}\n+\n+\t@Override\n+\tpublic void processWatermark(Watermark mark) throws Exception {\n+\t\tsuper.processWatermark(mark);\n+\t\tthis.currentWatermark = mark.getTimestamp();\n+\t}\n+\n+\t@Override\n+\tpublic void processElement(StreamRecord<RowData> element) throws Exception {\n+\t\tbuckets.onElement(\n+\t\t\t\telement.getValue(),\n+\t\t\t\tgetProcessingTimeService().getCurrentProcessingTime(),\n+\t\t\t\telement.getTimestamp(),\n+\t\t\t\tcurrentWatermark);\n+\t}\n+\n+\t@Override\n+\tpublic void notifyCheckpointComplete(long checkpointId) throws Exception {\n+\t\tsuper.notifyCheckpointComplete(checkpointId);\n+\t\tbuckets.commitUpToCheckpoint(checkpointId);\n+\t\tCommitMessage message = new CommitMessage(\n+\t\t\t\tcheckpointId,\n+\t\t\t\tgetRuntimeContext().getIndexOfThisSubtask(),\n+\t\t\t\tgetRuntimeContext().getNumberOfParallelSubtasks(),\n+\t\t\t\tnew ArrayList<>(inactivePartitions));\n+\t\toutput.collect(new StreamRecord<>(message));\n+\t\tinactivePartitions.clear();\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws Exception {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjk5OTAxMg=="}, "originalCommit": {"oid": "f8bdc31fb33986d81fe96a21019c1beaf0487cfe"}, "originalPosition": 166}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzNDE4OTY5OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/SuccessFileCommitPolicy.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMVQxMjoyMTozOVrOGTZ2gw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMVQxMjoyMTozOVrOGTZ2gw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjk5OTY4Mw==", "bodyText": "incorrect java doc", "url": "https://github.com/apache/flink/pull/12062#discussion_r422999683", "createdAt": "2020-05-11T12:21:39Z", "author": {"login": "lirui-apache"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/SuccessFileCommitPolicy.java", "diffHunk": "@@ -0,0 +1,46 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem.stream;\n+\n+import org.apache.flink.core.fs.FileSystem;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.filesystem.TableMetaStoreFactory;\n+\n+import java.util.LinkedHashMap;\n+\n+/**\n+ * Partition commit policy to update metastore.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f8bdc31fb33986d81fe96a21019c1beaf0487cfe"}, "originalPosition": 28}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzODQ1MzEyOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/StreamingFileWriter.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQxMjoxNjowNlrOGUDtYg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xM1QxMDozNTozNlrOGUrmSw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzY4NTQ3NA==", "bodyText": "This class is copy pasting quite a bit of code/fields from StreamingFileSink. Can not we extract a common abstraction or re-use one in the another?", "url": "https://github.com/apache/flink/pull/12062#discussion_r423685474", "createdAt": "2020-05-12T12:16:06Z", "author": {"login": "pnowojski"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/StreamingFileWriter.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem.stream;\n+\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.state.OperatorStateStore;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.functions.sink.filesystem.Buckets;\n+import org.apache.flink.streaming.api.functions.sink.filesystem.StreamingFileSink;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.BoundedOneInput;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.streaming.runtime.tasks.ProcessingTimeCallback;\n+import org.apache.flink.streaming.runtime.tasks.ProcessingTimeService;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.filesystem.stream.StreamingFileCommitter.CommitMessage;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.util.ArrayList;\n+import java.util.HashSet;\n+import java.util.Set;\n+\n+/**\n+ * Operator for file system sink. It is a operator version of {@link StreamingFileSink}.\n+ * It sends partition commit message to downstream for committing.\n+ *\n+ * <p>See {@link StreamingFileCommitter}.\n+ */\n+public class StreamingFileWriter extends AbstractStreamOperator<CommitMessage> implements", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e7040a4ef029a828ce0ad2584ecadac9cb55522b"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDMzOTAxOQ==", "bodyText": "I've discussed with @gaoyunhaii  offline, mainly because feel that there is too little code, so don't reuse it.\nBut +1 for abstraction. I've extracted a helper from StreamingFileSink. See\n36fa3d2", "url": "https://github.com/apache/flink/pull/12062#discussion_r424339019", "createdAt": "2020-05-13T10:35:36Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/StreamingFileWriter.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem.stream;\n+\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.state.OperatorStateStore;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.functions.sink.filesystem.Buckets;\n+import org.apache.flink.streaming.api.functions.sink.filesystem.StreamingFileSink;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.BoundedOneInput;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.streaming.runtime.tasks.ProcessingTimeCallback;\n+import org.apache.flink.streaming.runtime.tasks.ProcessingTimeService;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.filesystem.stream.StreamingFileCommitter.CommitMessage;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.util.ArrayList;\n+import java.util.HashSet;\n+import java.util.Set;\n+\n+/**\n+ * Operator for file system sink. It is a operator version of {@link StreamingFileSink}.\n+ * It sends partition commit message to downstream for committing.\n+ *\n+ * <p>See {@link StreamingFileCommitter}.\n+ */\n+public class StreamingFileWriter extends AbstractStreamOperator<CommitMessage> implements", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzY4NTQ3NA=="}, "originalCommit": {"oid": "e7040a4ef029a828ce0ad2584ecadac9cb55522b"}, "originalPosition": 51}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1MTQzNjQxOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemOptions.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQxMjo1Nzo0MFrOGWDu3w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQxNDoyMDoxNlrOGWG5mg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTc4MzAwNw==", "bodyText": "I think we should explain how \"partition creation time\" is determined.", "url": "https://github.com/apache/flink/pull/12062#discussion_r425783007", "createdAt": "2020-05-15T12:57:40Z", "author": {"login": "lirui-apache"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemOptions.java", "diffHunk": "@@ -88,4 +88,48 @@\n \t\t\t\t\t\t\t\" If timestamp in partition is year, month, day, hour,\" +\n \t\t\t\t\t\t\t\" can configure: '$year-$month-$day $hour:00:00'.\" +\n \t\t\t\t\t\t\t\" If timestamp in partition is dt and hour, can configure: '$dt $hour:00:00'.\");\n+\n+\tpublic static final ConfigOption<String> SINK_PARTITION_COMMIT_TRIGGER =\n+\t\t\tkey(\"sink.partition-commit.trigger\")\n+\t\t\t\t\t.stringType()\n+\t\t\t\t\t.defaultValue(\"partition-time\")\n+\t\t\t\t\t.withDescription(\"Trigger type for partition commit:\" +\n+\t\t\t\t\t\t\t\" 'partition-time': extract time from partition,\" +\n+\t\t\t\t\t\t\t\" if 'watermark' > 'partition-time' + 'delay', will commit the partition.\" +\n+\t\t\t\t\t\t\t\" 'process-time': use processing time, if 'current processing time' > \" +\n+\t\t\t\t\t\t\t\"'partition creation time' + 'delay', will commit the partition.\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e2b2ede56019d7b04ca9a2623eb552916d598b35"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTgzNDkwNg==", "bodyText": "\"partition directory creation time\"", "url": "https://github.com/apache/flink/pull/12062#discussion_r425834906", "createdAt": "2020-05-15T14:20:16Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemOptions.java", "diffHunk": "@@ -88,4 +88,48 @@\n \t\t\t\t\t\t\t\" If timestamp in partition is year, month, day, hour,\" +\n \t\t\t\t\t\t\t\" can configure: '$year-$month-$day $hour:00:00'.\" +\n \t\t\t\t\t\t\t\" If timestamp in partition is dt and hour, can configure: '$dt $hour:00:00'.\");\n+\n+\tpublic static final ConfigOption<String> SINK_PARTITION_COMMIT_TRIGGER =\n+\t\t\tkey(\"sink.partition-commit.trigger\")\n+\t\t\t\t\t.stringType()\n+\t\t\t\t\t.defaultValue(\"partition-time\")\n+\t\t\t\t\t.withDescription(\"Trigger type for partition commit:\" +\n+\t\t\t\t\t\t\t\" 'partition-time': extract time from partition,\" +\n+\t\t\t\t\t\t\t\" if 'watermark' > 'partition-time' + 'delay', will commit the partition.\" +\n+\t\t\t\t\t\t\t\" 'process-time': use processing time, if 'current processing time' > \" +\n+\t\t\t\t\t\t\t\"'partition creation time' + 'delay', will commit the partition.\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTc4MzAwNw=="}, "originalCommit": {"oid": "e2b2ede56019d7b04ca9a2623eb552916d598b35"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1MTQ4ODI4OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemOptions.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQxMzoxMjozMlrOGWEQQg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQxNDoyMTo0M1rOGWG9UA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTc5MTU1NA==", "bodyText": "Does this only work custom commit policy? If so it should be mentioned in the description and reflected in the config name.", "url": "https://github.com/apache/flink/pull/12062#discussion_r425791554", "createdAt": "2020-05-15T13:12:32Z", "author": {"login": "lirui-apache"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemOptions.java", "diffHunk": "@@ -88,4 +88,48 @@\n \t\t\t\t\t\t\t\" If timestamp in partition is year, month, day, hour,\" +\n \t\t\t\t\t\t\t\" can configure: '$year-$month-$day $hour:00:00'.\" +\n \t\t\t\t\t\t\t\" If timestamp in partition is dt and hour, can configure: '$dt $hour:00:00'.\");\n+\n+\tpublic static final ConfigOption<String> SINK_PARTITION_COMMIT_TRIGGER =\n+\t\t\tkey(\"sink.partition-commit.trigger\")\n+\t\t\t\t\t.stringType()\n+\t\t\t\t\t.defaultValue(\"partition-time\")\n+\t\t\t\t\t.withDescription(\"Trigger type for partition commit:\" +\n+\t\t\t\t\t\t\t\" 'partition-time': extract time from partition,\" +\n+\t\t\t\t\t\t\t\" if 'watermark' > 'partition-time' + 'delay', will commit the partition.\" +\n+\t\t\t\t\t\t\t\" 'process-time': use processing time, if 'current processing time' > \" +\n+\t\t\t\t\t\t\t\"'partition creation time' + 'delay', will commit the partition.\");\n+\n+\tpublic static final ConfigOption<Duration> SINK_PARTITION_COMMIT_DELAY =\n+\t\t\tkey(\"sink.partition-commit.delay\")\n+\t\t\t\t\t.durationType()\n+\t\t\t\t\t.defaultValue(Duration.ofMillis(0))\n+\t\t\t\t\t.withDescription(\"The partition will not commit until the delay time.\" +\n+\t\t\t\t\t\t\t\" if it is a day partition, should be '1 d',\" +\n+\t\t\t\t\t\t\t\" if it is a hour partition, should be '1 h'\");\n+\n+\tpublic static final ConfigOption<String> SINK_PARTITION_COMMIT_POLICY_KIND =\n+\t\t\tkey(\"sink.partition-commit.policy.kind\")\n+\t\t\t\t\t.stringType()\n+\t\t\t\t\t.noDefaultValue()\n+\t\t\t\t\t.withDescription(\"Policy to commit a partition is to notify the downstream\" +\n+\t\t\t\t\t\t\t\" application that the partition has finished writing, the partition\" +\n+\t\t\t\t\t\t\t\" is ready to be read.\" +\n+\t\t\t\t\t\t\t\" metastore: add partition to metastore.\" +\n+\t\t\t\t\t\t\t\" success-file: add '_success' file to directory.\" +\n+\t\t\t\t\t\t\t\" Both can be configured at the same time: 'metastore,success-file'.\" +\n+\t\t\t\t\t\t\t\" custom: use policy class to create a commit policy.\" +\n+\t\t\t\t\t\t\t\" Support to configure multiple policies: 'metastore,success-file'.\");\n+\n+\tpublic static final ConfigOption<String> SINK_PARTITION_COMMIT_POLICY_CLASS =\n+\t\t\tkey(\"sink.partition-commit.policy.class\")\n+\t\t\t\t\t.stringType()\n+\t\t\t\t\t.noDefaultValue()\n+\t\t\t\t\t.withDescription(\"The partition commit policy class for implement PartitionCommitPolicy interface.\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e2b2ede56019d7b04ca9a2623eb552916d598b35"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTgzNTg1Ng==", "bodyText": "I will add comment, it is OK, has been mentioned in policy kind.", "url": "https://github.com/apache/flink/pull/12062#discussion_r425835856", "createdAt": "2020-05-15T14:21:43Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemOptions.java", "diffHunk": "@@ -88,4 +88,48 @@\n \t\t\t\t\t\t\t\" If timestamp in partition is year, month, day, hour,\" +\n \t\t\t\t\t\t\t\" can configure: '$year-$month-$day $hour:00:00'.\" +\n \t\t\t\t\t\t\t\" If timestamp in partition is dt and hour, can configure: '$dt $hour:00:00'.\");\n+\n+\tpublic static final ConfigOption<String> SINK_PARTITION_COMMIT_TRIGGER =\n+\t\t\tkey(\"sink.partition-commit.trigger\")\n+\t\t\t\t\t.stringType()\n+\t\t\t\t\t.defaultValue(\"partition-time\")\n+\t\t\t\t\t.withDescription(\"Trigger type for partition commit:\" +\n+\t\t\t\t\t\t\t\" 'partition-time': extract time from partition,\" +\n+\t\t\t\t\t\t\t\" if 'watermark' > 'partition-time' + 'delay', will commit the partition.\" +\n+\t\t\t\t\t\t\t\" 'process-time': use processing time, if 'current processing time' > \" +\n+\t\t\t\t\t\t\t\"'partition creation time' + 'delay', will commit the partition.\");\n+\n+\tpublic static final ConfigOption<Duration> SINK_PARTITION_COMMIT_DELAY =\n+\t\t\tkey(\"sink.partition-commit.delay\")\n+\t\t\t\t\t.durationType()\n+\t\t\t\t\t.defaultValue(Duration.ofMillis(0))\n+\t\t\t\t\t.withDescription(\"The partition will not commit until the delay time.\" +\n+\t\t\t\t\t\t\t\" if it is a day partition, should be '1 d',\" +\n+\t\t\t\t\t\t\t\" if it is a hour partition, should be '1 h'\");\n+\n+\tpublic static final ConfigOption<String> SINK_PARTITION_COMMIT_POLICY_KIND =\n+\t\t\tkey(\"sink.partition-commit.policy.kind\")\n+\t\t\t\t\t.stringType()\n+\t\t\t\t\t.noDefaultValue()\n+\t\t\t\t\t.withDescription(\"Policy to commit a partition is to notify the downstream\" +\n+\t\t\t\t\t\t\t\" application that the partition has finished writing, the partition\" +\n+\t\t\t\t\t\t\t\" is ready to be read.\" +\n+\t\t\t\t\t\t\t\" metastore: add partition to metastore.\" +\n+\t\t\t\t\t\t\t\" success-file: add '_success' file to directory.\" +\n+\t\t\t\t\t\t\t\" Both can be configured at the same time: 'metastore,success-file'.\" +\n+\t\t\t\t\t\t\t\" custom: use policy class to create a commit policy.\" +\n+\t\t\t\t\t\t\t\" Support to configure multiple policies: 'metastore,success-file'.\");\n+\n+\tpublic static final ConfigOption<String> SINK_PARTITION_COMMIT_POLICY_CLASS =\n+\t\t\tkey(\"sink.partition-commit.policy.class\")\n+\t\t\t\t\t.stringType()\n+\t\t\t\t\t.noDefaultValue()\n+\t\t\t\t\t.withDescription(\"The partition commit policy class for implement PartitionCommitPolicy interface.\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTc5MTU1NA=="}, "originalCommit": {"oid": "e2b2ede56019d7b04ca9a2623eb552916d598b35"}, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1MTUyMjkxOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/PartitionCommitPolicy.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQxMzoyMToxOVrOGWEl3w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQxMzoyMToxOVrOGWEl3w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTc5NzA4Nw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             * <p>The implemented commit method needs to be reentrant because the same partition may be\n          \n          \n            \n             * <p>The implemented commit method needs to be idempotent because the same partition may be", "url": "https://github.com/apache/flink/pull/12062#discussion_r425797087", "createdAt": "2020-05-15T13:21:19Z", "author": {"login": "lirui-apache"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/PartitionCommitPolicy.java", "diffHunk": "@@ -0,0 +1,124 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.annotation.Experimental;\n+import org.apache.flink.core.fs.FileSystem;\n+import org.apache.flink.core.fs.Path;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Policy for commit a partition.\n+ *\n+ * <p>The implemented commit method needs to be reentrant because the same partition may be", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e2b2ede56019d7b04ca9a2623eb552916d598b35"}, "originalPosition": 33}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1MTU1MDQyOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemOptions.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQxMzoyNzo1NVrOGWE2yg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQxNDoyNDowNFrOGWHDzg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTgwMTQxOA==", "bodyText": "If users choose metastore policy, don't they need to specify/provide a TableMetaStoreFactory implementation?", "url": "https://github.com/apache/flink/pull/12062#discussion_r425801418", "createdAt": "2020-05-15T13:27:55Z", "author": {"login": "lirui-apache"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemOptions.java", "diffHunk": "@@ -88,4 +88,48 @@\n \t\t\t\t\t\t\t\" If timestamp in partition is year, month, day, hour,\" +\n \t\t\t\t\t\t\t\" can configure: '$year-$month-$day $hour:00:00'.\" +\n \t\t\t\t\t\t\t\" If timestamp in partition is dt and hour, can configure: '$dt $hour:00:00'.\");\n+\n+\tpublic static final ConfigOption<String> SINK_PARTITION_COMMIT_TRIGGER =\n+\t\t\tkey(\"sink.partition-commit.trigger\")\n+\t\t\t\t\t.stringType()\n+\t\t\t\t\t.defaultValue(\"partition-time\")\n+\t\t\t\t\t.withDescription(\"Trigger type for partition commit:\" +\n+\t\t\t\t\t\t\t\" 'partition-time': extract time from partition,\" +\n+\t\t\t\t\t\t\t\" if 'watermark' > 'partition-time' + 'delay', will commit the partition.\" +\n+\t\t\t\t\t\t\t\" 'process-time': use processing time, if 'current processing time' > \" +\n+\t\t\t\t\t\t\t\"'partition creation time' + 'delay', will commit the partition.\");\n+\n+\tpublic static final ConfigOption<Duration> SINK_PARTITION_COMMIT_DELAY =\n+\t\t\tkey(\"sink.partition-commit.delay\")\n+\t\t\t\t\t.durationType()\n+\t\t\t\t\t.defaultValue(Duration.ofMillis(0))\n+\t\t\t\t\t.withDescription(\"The partition will not commit until the delay time.\" +\n+\t\t\t\t\t\t\t\" if it is a day partition, should be '1 d',\" +\n+\t\t\t\t\t\t\t\" if it is a hour partition, should be '1 h'\");\n+\n+\tpublic static final ConfigOption<String> SINK_PARTITION_COMMIT_POLICY_KIND =\n+\t\t\tkey(\"sink.partition-commit.policy.kind\")\n+\t\t\t\t\t.stringType()\n+\t\t\t\t\t.noDefaultValue()\n+\t\t\t\t\t.withDescription(\"Policy to commit a partition is to notify the downstream\" +\n+\t\t\t\t\t\t\t\" application that the partition has finished writing, the partition\" +\n+\t\t\t\t\t\t\t\" is ready to be read.\" +\n+\t\t\t\t\t\t\t\" metastore: add partition to metastore.\" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e2b2ede56019d7b04ca9a2623eb552916d598b35"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTgzNzUxOA==", "bodyText": "I will add comment: Only work with hive table, it is empty implementation for file system table.", "url": "https://github.com/apache/flink/pull/12062#discussion_r425837518", "createdAt": "2020-05-15T14:24:04Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemOptions.java", "diffHunk": "@@ -88,4 +88,48 @@\n \t\t\t\t\t\t\t\" If timestamp in partition is year, month, day, hour,\" +\n \t\t\t\t\t\t\t\" can configure: '$year-$month-$day $hour:00:00'.\" +\n \t\t\t\t\t\t\t\" If timestamp in partition is dt and hour, can configure: '$dt $hour:00:00'.\");\n+\n+\tpublic static final ConfigOption<String> SINK_PARTITION_COMMIT_TRIGGER =\n+\t\t\tkey(\"sink.partition-commit.trigger\")\n+\t\t\t\t\t.stringType()\n+\t\t\t\t\t.defaultValue(\"partition-time\")\n+\t\t\t\t\t.withDescription(\"Trigger type for partition commit:\" +\n+\t\t\t\t\t\t\t\" 'partition-time': extract time from partition,\" +\n+\t\t\t\t\t\t\t\" if 'watermark' > 'partition-time' + 'delay', will commit the partition.\" +\n+\t\t\t\t\t\t\t\" 'process-time': use processing time, if 'current processing time' > \" +\n+\t\t\t\t\t\t\t\"'partition creation time' + 'delay', will commit the partition.\");\n+\n+\tpublic static final ConfigOption<Duration> SINK_PARTITION_COMMIT_DELAY =\n+\t\t\tkey(\"sink.partition-commit.delay\")\n+\t\t\t\t\t.durationType()\n+\t\t\t\t\t.defaultValue(Duration.ofMillis(0))\n+\t\t\t\t\t.withDescription(\"The partition will not commit until the delay time.\" +\n+\t\t\t\t\t\t\t\" if it is a day partition, should be '1 d',\" +\n+\t\t\t\t\t\t\t\" if it is a hour partition, should be '1 h'\");\n+\n+\tpublic static final ConfigOption<String> SINK_PARTITION_COMMIT_POLICY_KIND =\n+\t\t\tkey(\"sink.partition-commit.policy.kind\")\n+\t\t\t\t\t.stringType()\n+\t\t\t\t\t.noDefaultValue()\n+\t\t\t\t\t.withDescription(\"Policy to commit a partition is to notify the downstream\" +\n+\t\t\t\t\t\t\t\" application that the partition has finished writing, the partition\" +\n+\t\t\t\t\t\t\t\" is ready to be read.\" +\n+\t\t\t\t\t\t\t\" metastore: add partition to metastore.\" +", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTgwMTQxOA=="}, "originalCommit": {"oid": "e2b2ede56019d7b04ca9a2623eb552916d598b35"}, "originalPosition": 30}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1387, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}