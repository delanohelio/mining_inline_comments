{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTI4NDk2NDEy", "number": 14246, "title": "[FLINK-20273][table/kafka] Fix the Kafka round-robin behaviour when k\u2026", "bodyText": "\u2026eys are specified\n\nWhat is the purpose of the change\nFix the kafka round-robin partition behaviour.\nBrief change log\n\nAdd value 'default' for option 'sink.partitioner'. It uses the kafka default partitioner.\nForbid use of the round-robin partitioner when record's key is specified.\nModify the wrong description of the partitioner.\n\nVerifying this change\nThis change added tests and can be verified as follows:\n\nAdd an invalid test when using round-robin partitioner and key.fields together\n\nDoes this pull request potentially affect one of the following parts:\n\nDependencies (does it add or upgrade a dependency): (yes / no)\nThe public API, i.e., is any changed class annotated with @Public(Evolving): (yes / no)\nThe serializers: (yes / no / don't know)\nThe runtime per-record code paths (performance sensitive): (yes / no / don't know)\nAnything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (yes / no / don't know)\nThe S3 file system connector: (yes / no / don't know)\n\nDocumentation\n\nDoes this pull request introduce a new feature? (yes / no)\nIf yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)", "createdAt": "2020-11-27T09:27:31Z", "url": "https://github.com/apache/flink/pull/14246", "merged": true, "mergeCommit": {"oid": "033b230a155ed188ca8051c5defcda016427591a"}, "closed": true, "closedAt": "2020-12-07T15:26:27Z", "author": {"login": "fsk119"}, "timelineItems": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdgj3M3gH2gAyNTI4NDk2NDEyOjlkODU2ODcxZTc5YzNiYmU2NTA1NGMxNDVjNTFlNDEwZjgzZDVjNjU=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdjzDBxgFqTU0NjAxODM3OA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "9d856871e79c3bbe65054c145c51e410f83d5c65", "author": {"user": {"login": "fsk119", "name": "Shengkai "}}, "url": "https://github.com/apache/flink/commit/9d856871e79c3bbe65054c145c51e410f83d5c65", "committedDate": "2020-11-27T09:09:47Z", "message": "[FLINK-20273][table/kafka] Fix the Kafka round-robin behaviour when keys are specified"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ1OTE3NjUw", "url": "https://github.com/apache/flink/pull/14246#pullrequestreview-545917650", "createdAt": "2020-12-07T08:24:48Z", "commit": {"oid": "9d856871e79c3bbe65054c145c51e410f83d5c65"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QwODoyNDo0OFrOIAa-Cg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QwODo0Njo1OFrOIAby-Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzMxMjc3OA==", "bodyText": "We should still need to check the partitioner is in the allowed enums.", "url": "https://github.com/apache/flink/pull/14246#discussion_r537312778", "createdAt": "2020-12-07T08:24:48Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/table/KafkaOptions.java", "diffHunk": "@@ -314,12 +317,12 @@ private static void validateScanStartupMode(ReadableConfig tableOptions) {\n \tprivate static void validateSinkPartitioner(ReadableConfig tableOptions) {\n \t\ttableOptions.getOptional(SINK_PARTITIONER)\n \t\t\t\t.ifPresent(partitioner -> {\n-\t\t\t\t\tif (!SINK_PARTITIONER_ENUMS.contains(partitioner.toLowerCase())) {\n-\t\t\t\t\t\tif (partitioner.isEmpty()) {\n-\t\t\t\t\t\t\tthrow new ValidationException(\n-\t\t\t\t\t\t\t\t\tString.format(\"Option '%s' should be a non-empty string.\",\n-\t\t\t\t\t\t\t\t\t\t\tSINK_PARTITIONER.key()));\n-\t\t\t\t\t\t}\n+\t\t\t\t\tif (partitioner.equals(SINK_PARTITIONER_VALUE_ROUND_ROBIN) && tableOptions.getOptional(KEY_FIELDS).isPresent()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9d856871e79c3bbe65054c145c51e410f83d5c65"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzMyNTkzOA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            By default, Flink uses the [Kafka default partitioner](https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/producer/internals/DefaultPartitioner.java) to parititon records.\n          \n          \n            \n            It uses the [sticky partition strategy](https://www.confluent.io/blog/apache-kafka-producer-improvements-sticky-partitioner/) for records with null keys and uses a murmur2 hash to compute the partition for a record with the key defined.\n          \n          \n            \n            In order to control the routing of rows into partitions, a custom sink partitioner can be provided. The 'fixed' partitioner will write the records in the same Flink partition into the same partition, which could reduce the cost of the network connections.\n          \n          \n            \n            By default, Flink uses the [Kafka default partitioner](https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/producer/internals/DefaultPartitioner.java) to parititon records. It uses the [sticky partition strategy](https://www.confluent.io/blog/apache-kafka-producer-improvements-sticky-partitioner/) for records with null keys and uses a murmur2 hash to compute the partition for a record with the key defined.\n          \n          \n            \n            \n          \n          \n            \n            In order to control the routing of rows into partitions, a custom sink partitioner can be provided. The 'fixed' partitioner will write the records in the same Flink partition into the same Kafka partition, which could reduce the cost of the network connections.", "url": "https://github.com/apache/flink/pull/14246#discussion_r537325938", "createdAt": "2020-12-07T08:46:23Z", "author": {"login": "wuchong"}, "path": "docs/dev/table/connectors/kafka.md", "diffHunk": "@@ -525,9 +527,9 @@ See more about how to use the CDC formats in [debezium-json]({% link dev/table/c\n ### Sink Partitioning\n \n The config option `sink.partitioner` specifies output partitioning from Flink's partitions into Kafka's partitions.\n-By default, a Kafka sink writes to at most as many partitions as its own parallelism (each parallel instance of the sink writes to exactly one partition).\n-In order to distribute the writes to more partitions or control the routing of rows into partitions, a custom sink partitioner can be provided. The `round-robin` partitioner is useful to avoid an unbalanced partitioning.\n-However, it will cause a lot of network connections between all the Flink instances and all the Kafka brokers.\n+By default, Flink uses the [Kafka default partitioner](https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/producer/internals/DefaultPartitioner.java) to parititon records.\n+It uses the [sticky partition strategy](https://www.confluent.io/blog/apache-kafka-producer-improvements-sticky-partitioner/) for records with null keys and uses a murmur2 hash to compute the partition for a record with the key defined.\n+In order to control the routing of rows into partitions, a custom sink partitioner can be provided. The 'fixed' partitioner will write the records in the same Flink partition into the same partition, which could reduce the cost of the network connections.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9d856871e79c3bbe65054c145c51e410f83d5c65"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzMyNjMyOQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            By default, Flink uses the [Kafka default partitioner](https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/producer/internals/DefaultPartitioner.java) to parititon records.\n          \n          \n            \n            It uses the [sticky partition strategy](https://www.confluent.io/blog/apache-kafka-producer-improvements-sticky-partitioner/) for records with null keys and uses a murmur2 hash to compute the partition for a record with the key defined.\n          \n          \n            \n            In order to control the routing of rows into partitions, a custom sink partitioner can be provided. The 'fixed' partitioner will write the records in the same Flink partition into the same partition, which could reduce the cost of the network connections.\n          \n          \n            \n            By default, Flink uses the [Kafka default partitioner](https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/producer/internals/DefaultPartitioner.java) to parititon records. It uses the [sticky partition strategy](https://www.confluent.io/blog/apache-kafka-producer-improvements-sticky-partitioner/) for records with null keys and uses a murmur2 hash to compute the partition for a record with the key defined.\n          \n          \n            \n            \n          \n          \n            \n            In order to control the routing of rows into partitions, a custom sink partitioner can be provided. The 'fixed' partitioner will write the records in the same Flink partition into the same partition, which could reduce the cost of the network connections.", "url": "https://github.com/apache/flink/pull/14246#discussion_r537326329", "createdAt": "2020-12-07T08:46:58Z", "author": {"login": "wuchong"}, "path": "docs/dev/table/connectors/kafka.zh.md", "diffHunk": "@@ -526,9 +528,9 @@ See more about how to use the CDC formats in [debezium-json]({% link dev/table/c\n ### Sink Partitioning\n \n The config option `sink.partitioner` specifies output partitioning from Flink's partitions into Kafka's partitions.\n-By default, a Kafka sink writes to at most as many partitions as its own parallelism (each parallel instance of the sink writes to exactly one partition).\n-In order to distribute the writes to more partitions or control the routing of rows into partitions, a custom sink partitioner can be provided. The `round-robin` partitioner is useful to avoid an unbalanced partitioning.\n-However, it will cause a lot of network connections between all the Flink instances and all the Kafka brokers.\n+By default, Flink uses the [Kafka default partitioner](https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/producer/internals/DefaultPartitioner.java) to parititon records.\n+It uses the [sticky partition strategy](https://www.confluent.io/blog/apache-kafka-producer-improvements-sticky-partitioner/) for records with null keys and uses a murmur2 hash to compute the partition for a record with the key defined.\n+In order to control the routing of rows into partitions, a custom sink partitioner can be provided. The 'fixed' partitioner will write the records in the same Flink partition into the same partition, which could reduce the cost of the network connections.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9d856871e79c3bbe65054c145c51e410f83d5c65"}, "originalPosition": 28}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "96531a0172fef0b4943b198139269a333a6715a6", "author": {"user": {"login": "fsk119", "name": "Shengkai "}}, "url": "https://github.com/apache/flink/commit/96531a0172fef0b4943b198139269a333a6715a6", "committedDate": "2020-12-07T09:26:50Z", "message": "Apply suggestions from code review\n\nCo-authored-by: Jark Wu <imjark@gmail.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "129faf543cd4bb829a3951388ee894521dccd88f", "author": {"user": {"login": "fsk119", "name": "Shengkai "}}, "url": "https://github.com/apache/flink/commit/129faf543cd4bb829a3951388ee894521dccd88f", "committedDate": "2020-12-07T10:22:29Z", "message": "delete useless enum"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ2MDE4Mzc4", "url": "https://github.com/apache/flink/pull/14246#pullrequestreview-546018378", "createdAt": "2020-12-07T10:33:03Z", "commit": {"oid": "129faf543cd4bb829a3951388ee894521dccd88f"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3738, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}