{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDY3MTk1Njcy", "number": 13136, "reviewThreads": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QwODowNjozOVrOEX8Tyw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QwOTowMzozM1rOEX9o-w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNTQwODExOnYy", "diffSide": "RIGHT", "path": "flink-python/pyflink/datastream/stream_execution_environment.py", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QwODowNjozOVrOHAAkzw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QwODowNjozOVrOHAAkzw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTc3MTQ3MQ==", "bodyText": "Duplicate type hint.", "url": "https://github.com/apache/flink/pull/13136#discussion_r469771471", "createdAt": "2020-08-13T08:06:39Z", "author": {"login": "hequn8128"}, "path": "flink-python/pyflink/datastream/stream_execution_environment.py", "diffHunk": "@@ -389,6 +389,174 @@ def get_stream_time_characteristic(self):\n         j_characteristic = self._j_stream_execution_environment.getStreamTimeCharacteristic()\n         return TimeCharacteristic._from_j_time_characteristic(j_characteristic)\n \n+    def add_python_file(self, file_path: str):\n+        \"\"\"\n+        Adds a python dependency which could be python files, python packages or\n+        local directories. They will be added to the PYTHONPATH of the python UDF worker.\n+        Please make sure that these dependencies can be imported.\n+\n+        :param file_path: The path of the python dependency.\n+        :type file_path: str", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6098b6b0563eded5c97787f55094467b82a88593"}, "originalPosition": 11}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNTQxMzQxOnYy", "diffSide": "RIGHT", "path": "flink-python/pyflink/datastream/stream_execution_environment.py", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QwODowODowMlrOHAAn-g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QwODowODowMlrOHAAn-g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTc3MjI4Mg==", "bodyText": "Duplicate type hint.", "url": "https://github.com/apache/flink/pull/13136#discussion_r469772282", "createdAt": "2020-08-13T08:08:02Z", "author": {"login": "hequn8128"}, "path": "flink-python/pyflink/datastream/stream_execution_environment.py", "diffHunk": "@@ -389,6 +389,174 @@ def get_stream_time_characteristic(self):\n         j_characteristic = self._j_stream_execution_environment.getStreamTimeCharacteristic()\n         return TimeCharacteristic._from_j_time_characteristic(j_characteristic)\n \n+    def add_python_file(self, file_path: str):\n+        \"\"\"\n+        Adds a python dependency which could be python files, python packages or\n+        local directories. They will be added to the PYTHONPATH of the python UDF worker.\n+        Please make sure that these dependencies can be imported.\n+\n+        :param file_path: The path of the python dependency.\n+        :type file_path: str\n+        \"\"\"\n+        jvm = get_gateway().jvm\n+        env_config = jvm.org.apache.flink.python.util.PythonConfigUtil\\\n+            .getEnvironmentConfig(self._j_stream_execution_environment)\n+        python_files = env_config.getString(jvm.PythonOptions.PYTHON_FILES.key(), None)\n+        if python_files is not None:\n+            python_files = jvm.PythonDependencyUtils.FILE_DELIMITER.join([python_files, file_path])\n+        else:\n+            python_files = file_path\n+        env_config.setString(jvm.PythonOptions.PYTHON_FILES.key(), python_files)\n+\n+    def set_python_requirements(self, requirements_file_path: str,\n+                                requirements_cache_dir: str = None):\n+        \"\"\"\n+        Specifies a requirements.txt file which defines the third-party dependencies.\n+        These dependencies will be installed to a temporary directory and added to the\n+        PYTHONPATH of the python UDF worker.\n+\n+        For the dependencies which could not be accessed in the cluster, a directory which contains\n+        the installation packages of these dependencies could be specified using the parameter\n+        \"requirements_cached_dir\". It will be uploaded to the cluster to support offline\n+        installation.\n+\n+        Example:\n+        ::\n+\n+            # commands executed in shell\n+            $ echo numpy==1.16.5 > requirements.txt\n+            $ pip download -d cached_dir -r requirements.txt --no-binary :all:\n+\n+            # python code\n+            >>> stream_env.set_python_requirements(\"requirements.txt\", \"cached_dir\")\n+\n+        .. note::\n+\n+            Please make sure the installation packages matches the platform of the cluster\n+            and the python version used. These packages will be installed using pip,\n+            so also make sure the version of Pip (version >= 7.1.0) and the version of\n+            SetupTools (version >= 37.0.0).\n+\n+        :param requirements_file_path: The path of \"requirements.txt\" file.\n+        :type requirements_file_path: str\n+        :param requirements_cache_dir: The path of the local directory which contains the\n+                                       installation packages.\n+        :type requirements_cache_dir: str", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6098b6b0563eded5c97787f55094467b82a88593"}, "originalPosition": 56}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNTQxMzk1OnYy", "diffSide": "RIGHT", "path": "flink-python/pyflink/datastream/stream_execution_environment.py", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QwODowODoxM1rOHAAoUQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QwODowODoxM1rOHAAoUQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTc3MjM2OQ==", "bodyText": "Duplicate type hint.", "url": "https://github.com/apache/flink/pull/13136#discussion_r469772369", "createdAt": "2020-08-13T08:08:13Z", "author": {"login": "hequn8128"}, "path": "flink-python/pyflink/datastream/stream_execution_environment.py", "diffHunk": "@@ -389,6 +389,174 @@ def get_stream_time_characteristic(self):\n         j_characteristic = self._j_stream_execution_environment.getStreamTimeCharacteristic()\n         return TimeCharacteristic._from_j_time_characteristic(j_characteristic)\n \n+    def add_python_file(self, file_path: str):\n+        \"\"\"\n+        Adds a python dependency which could be python files, python packages or\n+        local directories. They will be added to the PYTHONPATH of the python UDF worker.\n+        Please make sure that these dependencies can be imported.\n+\n+        :param file_path: The path of the python dependency.\n+        :type file_path: str\n+        \"\"\"\n+        jvm = get_gateway().jvm\n+        env_config = jvm.org.apache.flink.python.util.PythonConfigUtil\\\n+            .getEnvironmentConfig(self._j_stream_execution_environment)\n+        python_files = env_config.getString(jvm.PythonOptions.PYTHON_FILES.key(), None)\n+        if python_files is not None:\n+            python_files = jvm.PythonDependencyUtils.FILE_DELIMITER.join([python_files, file_path])\n+        else:\n+            python_files = file_path\n+        env_config.setString(jvm.PythonOptions.PYTHON_FILES.key(), python_files)\n+\n+    def set_python_requirements(self, requirements_file_path: str,\n+                                requirements_cache_dir: str = None):\n+        \"\"\"\n+        Specifies a requirements.txt file which defines the third-party dependencies.\n+        These dependencies will be installed to a temporary directory and added to the\n+        PYTHONPATH of the python UDF worker.\n+\n+        For the dependencies which could not be accessed in the cluster, a directory which contains\n+        the installation packages of these dependencies could be specified using the parameter\n+        \"requirements_cached_dir\". It will be uploaded to the cluster to support offline\n+        installation.\n+\n+        Example:\n+        ::\n+\n+            # commands executed in shell\n+            $ echo numpy==1.16.5 > requirements.txt\n+            $ pip download -d cached_dir -r requirements.txt --no-binary :all:\n+\n+            # python code\n+            >>> stream_env.set_python_requirements(\"requirements.txt\", \"cached_dir\")\n+\n+        .. note::\n+\n+            Please make sure the installation packages matches the platform of the cluster\n+            and the python version used. These packages will be installed using pip,\n+            so also make sure the version of Pip (version >= 7.1.0) and the version of\n+            SetupTools (version >= 37.0.0).\n+\n+        :param requirements_file_path: The path of \"requirements.txt\" file.\n+        :type requirements_file_path: str\n+        :param requirements_cache_dir: The path of the local directory which contains the\n+                                       installation packages.\n+        :type requirements_cache_dir: str\n+        \"\"\"\n+        jvm = get_gateway().jvm\n+        python_requirements = requirements_file_path\n+        if requirements_cache_dir is not None:\n+            python_requirements = jvm.PythonDependencyUtils.PARAM_DELIMITER.join(\n+                [python_requirements, requirements_cache_dir])\n+        env_config = jvm.org.apache.flink.python.util.PythonConfigUtil \\\n+            .getEnvironmentConfig(self._j_stream_execution_environment)\n+        env_config.setString(jvm.PythonOptions.PYTHON_REQUIREMENTS.key(), python_requirements)\n+\n+    def add_python_archive(self, archive_path: str, target_dir: str = None):\n+        \"\"\"\n+        Adds a python archive file. The file will be extracted to the working directory of\n+        python UDF worker.\n+\n+        If the parameter \"target_dir\" is specified, the archive file will be extracted to a\n+        directory named ${target_dir}. Otherwise, the archive file will be extracted to a\n+        directory with the same name of the archive file.\n+\n+        If python UDF depends on a specific python version which does not exist in the cluster,\n+        this method can be used to upload the virtual environment.\n+        Note that the path of the python interpreter contained in the uploaded environment\n+        should be specified via the method :func:`pyflink.table.TableConfig.set_python_executable`.\n+\n+        The files uploaded via this method are also accessible in UDFs via relative path.\n+\n+        Example:\n+        ::\n+\n+            # command executed in shell\n+            # assert the relative path of python interpreter is py_env/bin/python\n+            $ zip -r py_env.zip py_env\n+\n+            # python code\n+            >>> stream_env.add_python_archive(\"py_env.zip\")\n+            >>> stream_env.set_python_executable(\"py_env.zip/py_env/bin/python\")\n+\n+            # or\n+            >>> stream_env.add_python_archive(\"py_env.zip\", \"myenv\")\n+            >>> stream_env.set_python_executable(\"myenv/py_env/bin/python\")\n+\n+            # the files contained in the archive file can be accessed in UDF\n+            >>> def my_udf():\n+            ...     with open(\"myenv/py_env/data/data.txt\") as f:\n+            ...         ...\n+\n+        .. note::\n+\n+            Please make sure the uploaded python environment matches the platform that the cluster\n+            is running on and that the python version must be 3.5 or higher.\n+\n+        .. note::\n+\n+            Currently only zip-format is supported. i.e. zip, jar, whl, egg, etc.\n+            The other archive formats such as tar, tar.gz, 7z, rar, etc are not supported.\n+\n+        :param archive_path: The archive file path.\n+        :type archive_path: str\n+        :param target_dir: Optional, the target dir name that the archive file extracted to.\n+        :type target_dir: str", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6098b6b0563eded5c97787f55094467b82a88593"}, "originalPosition": 116}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNTQxNDMyOnYy", "diffSide": "RIGHT", "path": "flink-python/pyflink/datastream/stream_execution_environment.py", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QwODowODoyMVrOHAAomg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QwODowODoyMVrOHAAomg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTc3MjQ0Mg==", "bodyText": "Duplicate type hint.", "url": "https://github.com/apache/flink/pull/13136#discussion_r469772442", "createdAt": "2020-08-13T08:08:21Z", "author": {"login": "hequn8128"}, "path": "flink-python/pyflink/datastream/stream_execution_environment.py", "diffHunk": "@@ -389,6 +389,174 @@ def get_stream_time_characteristic(self):\n         j_characteristic = self._j_stream_execution_environment.getStreamTimeCharacteristic()\n         return TimeCharacteristic._from_j_time_characteristic(j_characteristic)\n \n+    def add_python_file(self, file_path: str):\n+        \"\"\"\n+        Adds a python dependency which could be python files, python packages or\n+        local directories. They will be added to the PYTHONPATH of the python UDF worker.\n+        Please make sure that these dependencies can be imported.\n+\n+        :param file_path: The path of the python dependency.\n+        :type file_path: str\n+        \"\"\"\n+        jvm = get_gateway().jvm\n+        env_config = jvm.org.apache.flink.python.util.PythonConfigUtil\\\n+            .getEnvironmentConfig(self._j_stream_execution_environment)\n+        python_files = env_config.getString(jvm.PythonOptions.PYTHON_FILES.key(), None)\n+        if python_files is not None:\n+            python_files = jvm.PythonDependencyUtils.FILE_DELIMITER.join([python_files, file_path])\n+        else:\n+            python_files = file_path\n+        env_config.setString(jvm.PythonOptions.PYTHON_FILES.key(), python_files)\n+\n+    def set_python_requirements(self, requirements_file_path: str,\n+                                requirements_cache_dir: str = None):\n+        \"\"\"\n+        Specifies a requirements.txt file which defines the third-party dependencies.\n+        These dependencies will be installed to a temporary directory and added to the\n+        PYTHONPATH of the python UDF worker.\n+\n+        For the dependencies which could not be accessed in the cluster, a directory which contains\n+        the installation packages of these dependencies could be specified using the parameter\n+        \"requirements_cached_dir\". It will be uploaded to the cluster to support offline\n+        installation.\n+\n+        Example:\n+        ::\n+\n+            # commands executed in shell\n+            $ echo numpy==1.16.5 > requirements.txt\n+            $ pip download -d cached_dir -r requirements.txt --no-binary :all:\n+\n+            # python code\n+            >>> stream_env.set_python_requirements(\"requirements.txt\", \"cached_dir\")\n+\n+        .. note::\n+\n+            Please make sure the installation packages matches the platform of the cluster\n+            and the python version used. These packages will be installed using pip,\n+            so also make sure the version of Pip (version >= 7.1.0) and the version of\n+            SetupTools (version >= 37.0.0).\n+\n+        :param requirements_file_path: The path of \"requirements.txt\" file.\n+        :type requirements_file_path: str\n+        :param requirements_cache_dir: The path of the local directory which contains the\n+                                       installation packages.\n+        :type requirements_cache_dir: str\n+        \"\"\"\n+        jvm = get_gateway().jvm\n+        python_requirements = requirements_file_path\n+        if requirements_cache_dir is not None:\n+            python_requirements = jvm.PythonDependencyUtils.PARAM_DELIMITER.join(\n+                [python_requirements, requirements_cache_dir])\n+        env_config = jvm.org.apache.flink.python.util.PythonConfigUtil \\\n+            .getEnvironmentConfig(self._j_stream_execution_environment)\n+        env_config.setString(jvm.PythonOptions.PYTHON_REQUIREMENTS.key(), python_requirements)\n+\n+    def add_python_archive(self, archive_path: str, target_dir: str = None):\n+        \"\"\"\n+        Adds a python archive file. The file will be extracted to the working directory of\n+        python UDF worker.\n+\n+        If the parameter \"target_dir\" is specified, the archive file will be extracted to a\n+        directory named ${target_dir}. Otherwise, the archive file will be extracted to a\n+        directory with the same name of the archive file.\n+\n+        If python UDF depends on a specific python version which does not exist in the cluster,\n+        this method can be used to upload the virtual environment.\n+        Note that the path of the python interpreter contained in the uploaded environment\n+        should be specified via the method :func:`pyflink.table.TableConfig.set_python_executable`.\n+\n+        The files uploaded via this method are also accessible in UDFs via relative path.\n+\n+        Example:\n+        ::\n+\n+            # command executed in shell\n+            # assert the relative path of python interpreter is py_env/bin/python\n+            $ zip -r py_env.zip py_env\n+\n+            # python code\n+            >>> stream_env.add_python_archive(\"py_env.zip\")\n+            >>> stream_env.set_python_executable(\"py_env.zip/py_env/bin/python\")\n+\n+            # or\n+            >>> stream_env.add_python_archive(\"py_env.zip\", \"myenv\")\n+            >>> stream_env.set_python_executable(\"myenv/py_env/bin/python\")\n+\n+            # the files contained in the archive file can be accessed in UDF\n+            >>> def my_udf():\n+            ...     with open(\"myenv/py_env/data/data.txt\") as f:\n+            ...         ...\n+\n+        .. note::\n+\n+            Please make sure the uploaded python environment matches the platform that the cluster\n+            is running on and that the python version must be 3.5 or higher.\n+\n+        .. note::\n+\n+            Currently only zip-format is supported. i.e. zip, jar, whl, egg, etc.\n+            The other archive formats such as tar, tar.gz, 7z, rar, etc are not supported.\n+\n+        :param archive_path: The archive file path.\n+        :type archive_path: str\n+        :param target_dir: Optional, the target dir name that the archive file extracted to.\n+        :type target_dir: str\n+        \"\"\"\n+        jvm = get_gateway().jvm\n+        if target_dir is not None:\n+            archive_path = jvm.PythonDependencyUtils.PARAM_DELIMITER.join(\n+                [archive_path, target_dir])\n+        env_config = jvm.org.apache.flink.python.util.PythonConfigUtil \\\n+            .getEnvironmentConfig(self._j_stream_execution_environment)\n+        python_archives = env_config.getString(jvm.PythonOptions.PYTHON_ARCHIVES.key(), None)\n+        if python_archives is not None:\n+            python_files = jvm.PythonDependencyUtils.FILE_DELIMITER.join(\n+                [python_archives, archive_path])\n+        else:\n+            python_files = archive_path\n+        env_config.setString(jvm.PythonOptions.PYTHON_ARCHIVES.key(), python_files)\n+\n+    def set_python_executable(self, python_exec: str):\n+        \"\"\"\n+        Sets the path of the python interpreter which is used to execute the python udf workers.\n+\n+        e.g. \"/usr/local/bin/python3\".\n+\n+        If python UDF depends on a specific python version which does not exist in the cluster,\n+        the method :func:`pyflink.datastream.StreamExecutionEnvironment.add_python_archive` can be\n+        used to upload a virtual environment. The path of the python interpreter contained in the\n+        uploaded environment can be specified via this method.\n+\n+        Example:\n+        ::\n+\n+            # command executed in shell\n+            # assume that the relative path of python interpreter is py_env/bin/python\n+            $ zip -r py_env.zip py_env\n+\n+            # python code\n+            >>> stream_env.add_python_archive(\"py_env.zip\")\n+            >>> stream_env.set_python_executable(\"py_env.zip/py_env/bin/python\")\n+\n+        .. note::\n+\n+            Please make sure the uploaded python environment matches the platform that the cluster\n+            is running on and that the python version must be 3.5 or higher.\n+\n+        .. note::\n+\n+            The python udf worker depends on Apache Beam (version == 2.19.0).\n+            Please ensure that the specified environment meets the above requirements.\n+\n+        :param python_exec: The path of python interpreter.\n+        :type python_exec: str", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6098b6b0563eded5c97787f55094467b82a88593"}, "originalPosition": 165}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNTQyMDA0OnYy", "diffSide": "RIGHT", "path": "flink-python/pyflink/datastream/stream_execution_environment.py", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QwODowOTo1NlrOHAAsNw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QwODowOTo1NlrOHAAsNw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTc3MzM2Nw==", "bodyText": "We can extract a method and reuse it in different methods.", "url": "https://github.com/apache/flink/pull/13136#discussion_r469773367", "createdAt": "2020-08-13T08:09:56Z", "author": {"login": "hequn8128"}, "path": "flink-python/pyflink/datastream/stream_execution_environment.py", "diffHunk": "@@ -416,10 +584,14 @@ def execute(self, job_name=None):\n         :param job_name: Desired name of the job, optional.\n         :return: The result of the job execution, containing elapsed time and accumulators.\n         \"\"\"\n-        if job_name is None:\n-            return JobExecutionResult(self._j_stream_execution_environment.execute())\n-        else:\n-            return JobExecutionResult(self._j_stream_execution_environment.execute(job_name))\n+        j_stream_graph = get_gateway().jvm \\\n+            .org.apache.flink.python.util.PythonConfigUtil.generateStreamGraphWithDependencies(\n+            self._j_stream_execution_environment, True)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6098b6b0563eded5c97787f55094467b82a88593"}, "originalPosition": 185}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNTQ0MDY1OnYy", "diffSide": "RIGHT", "path": "flink-python/src/main/java/org/apache/flink/datastream/runtime/operators/python/DataStreamPythonStatelessFunctionOperator.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QwODoxNToyNFrOHAA4NQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QwODoxNToyNFrOHAA4NQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTc3NjQzNw==", "bodyText": "We don't need this config. Use the PythonConfig in DataStreamPythonStatelessFunctionOperator directly.", "url": "https://github.com/apache/flink/pull/13136#discussion_r469776437", "createdAt": "2020-08-13T08:15:24Z", "author": {"login": "hequn8128"}, "path": "flink-python/src/main/java/org/apache/flink/datastream/runtime/operators/python/DataStreamPythonStatelessFunctionOperator.java", "diffHunk": "@@ -75,6 +75,8 @@\n \n \tprotected transient StreamRecordCollector streamRecordCollector;\n \n+\tprivate Configuration mergedEnvConfig;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6098b6b0563eded5c97787f55094467b82a88593"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNTQ1OTcxOnYy", "diffSide": "RIGHT", "path": "flink-python/pyflink/datastream/data_stream.py", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QwODoyMDoyN1rOHABDsQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QwODoyMDoyN1rOHABDsQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTc3OTM3Nw==", "bodyText": "We need to return an empty config here.", "url": "https://github.com/apache/flink/pull/13136#discussion_r469779377", "createdAt": "2020-08-13T08:20:27Z", "author": {"login": "hequn8128"}, "path": "flink-python/pyflink/datastream/data_stream.py", "diffHunk": "@@ -438,7 +440,7 @@ def _get_java_python_function_operator(self, func: Union[Function, FunctionWrapp\n \n         j_env = self._j_data_stream.getExecutionEnvironment()\n         PythonConfigUtil = gateway.jvm.org.apache.flink.python.util.PythonConfigUtil\n-        j_conf = PythonConfigUtil.getMergedConfig(j_env)\n+        j_conf = PythonConfigUtil.getEnvConfigWithDependencies(j_env)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6098b6b0563eded5c97787f55094467b82a88593"}, "originalPosition": 22}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNTUwMjk5OnYy", "diffSide": "RIGHT", "path": "flink-python/src/main/java/org/apache/flink/python/util/PythonConfigUtil.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QwODozMjowNlrOHABd8w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QwODozMjowNlrOHABd8w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTc4NjA5OQ==", "bodyText": "Another method can be extracted to avoid code duplication.", "url": "https://github.com/apache/flink/pull/13136#discussion_r469786099", "createdAt": "2020-08-13T08:32:06Z", "author": {"login": "hequn8128"}, "path": "flink-python/src/main/java/org/apache/flink/python/util/PythonConfigUtil.java", "diffHunk": "@@ -64,4 +82,103 @@ public static Configuration getEnvironmentConfig(StreamExecutionEnvironment env)\n \t\tConfiguration envConfiguration = (Configuration) getConfigurationMethod.invoke(env);\n \t\treturn envConfiguration;\n \t}\n+\n+\t/**\n+\t * Configure the {@link DataStreamPythonStatelessFunctionOperator} to be chained with the upstream/downstream\n+\t * operator by setting their parallelism, slot sharing group, co-location group to be the same, and applying a\n+\t * {@link ForwardPartitioner}.\n+\t * 1. operator with name \"_keyed_stream_values_operator\" should align with its downstream operator.\n+\t * 2. operator with name \"_stream_key_by_map_operator\" should align with its upstream operator.\n+\t */\n+\tprivate static void alignStreamNode(StreamNode streamNode, StreamGraph streamGraph) {\n+\t\tif (streamNode.getOperatorName().equals(KEYED_STREAM_VALUE_OPERATOR_NAME)) {\n+\t\t\tStreamEdge downStreamEdge = streamNode.getOutEdges().get(0);\n+\t\t\tStreamNode downStreamNode = streamGraph.getStreamNode(downStreamEdge.getTargetId());\n+\t\t\tdownStreamEdge.setPartitioner(new ForwardPartitioner());\n+\t\t\tstreamNode.setParallelism(downStreamNode.getParallelism());\n+\t\t\tstreamNode.setCoLocationGroup(downStreamNode.getCoLocationGroup());\n+\t\t\tstreamNode.setSlotSharingGroup(downStreamNode.getSlotSharingGroup());\n+\t\t}\n+\n+\t\tif (streamNode.getOperatorName().equals(STREAM_KEY_BY_MAP_OPERATOR_NAME)) {\n+\t\t\tStreamEdge upStreamEdge = streamNode.getInEdges().get(0);\n+\t\t\tStreamNode upStreamNode = streamGraph.getStreamNode(upStreamEdge.getSourceId());\n+\t\t\tupStreamEdge.setPartitioner(new ForwardPartitioner<>());\n+\t\t\tstreamNode.setParallelism(upStreamNode.getParallelism());\n+\t\t\tstreamNode.setSlotSharingGroup(upStreamNode.getSlotSharingGroup());\n+\t\t\tstreamNode.setCoLocationGroup(upStreamNode.getCoLocationGroup());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6098b6b0563eded5c97787f55094467b82a88593"}, "originalPosition": 75}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNTU1ODk2OnYy", "diffSide": "RIGHT", "path": "flink-python/src/main/java/org/apache/flink/python/util/PythonConfigUtil.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QwODo0NTozMVrOHACAtg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QwODo0NTozMVrOHACAtg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTc5NDk5OA==", "bodyText": "These methods can be replaced by:\n\tprivate static PythonConfig generateNewPythonConfig(Configuration oldConfig, Configuration newConfig) {\n\t\tnewConfig.clone().addAll(oldConfig);\n\t\treturn new PythonConfig(oldConfig);\n\t}", "url": "https://github.com/apache/flink/pull/13136#discussion_r469794998", "createdAt": "2020-08-13T08:45:31Z", "author": {"login": "hequn8128"}, "path": "flink-python/src/main/java/org/apache/flink/python/util/PythonConfigUtil.java", "diffHunk": "@@ -64,4 +82,103 @@ public static Configuration getEnvironmentConfig(StreamExecutionEnvironment env)\n \t\tConfiguration envConfiguration = (Configuration) getConfigurationMethod.invoke(env);\n \t\treturn envConfiguration;\n \t}\n+\n+\t/**\n+\t * Configure the {@link DataStreamPythonStatelessFunctionOperator} to be chained with the upstream/downstream\n+\t * operator by setting their parallelism, slot sharing group, co-location group to be the same, and applying a\n+\t * {@link ForwardPartitioner}.\n+\t * 1. operator with name \"_keyed_stream_values_operator\" should align with its downstream operator.\n+\t * 2. operator with name \"_stream_key_by_map_operator\" should align with its upstream operator.\n+\t */\n+\tprivate static void alignStreamNode(StreamNode streamNode, StreamGraph streamGraph) {\n+\t\tif (streamNode.getOperatorName().equals(KEYED_STREAM_VALUE_OPERATOR_NAME)) {\n+\t\t\tStreamEdge downStreamEdge = streamNode.getOutEdges().get(0);\n+\t\t\tStreamNode downStreamNode = streamGraph.getStreamNode(downStreamEdge.getTargetId());\n+\t\t\tdownStreamEdge.setPartitioner(new ForwardPartitioner());\n+\t\t\tstreamNode.setParallelism(downStreamNode.getParallelism());\n+\t\t\tstreamNode.setCoLocationGroup(downStreamNode.getCoLocationGroup());\n+\t\t\tstreamNode.setSlotSharingGroup(downStreamNode.getSlotSharingGroup());\n+\t\t}\n+\n+\t\tif (streamNode.getOperatorName().equals(STREAM_KEY_BY_MAP_OPERATOR_NAME)) {\n+\t\t\tStreamEdge upStreamEdge = streamNode.getInEdges().get(0);\n+\t\t\tStreamNode upStreamNode = streamGraph.getStreamNode(upStreamEdge.getSourceId());\n+\t\t\tupStreamEdge.setPartitioner(new ForwardPartitioner<>());\n+\t\t\tstreamNode.setParallelism(upStreamNode.getParallelism());\n+\t\t\tstreamNode.setSlotSharingGroup(upStreamNode.getSlotSharingGroup());\n+\t\t\tstreamNode.setCoLocationGroup(upStreamNode.getCoLocationGroup());\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * Generate a {@link StreamGraph} for transformations maintained by current {@link StreamExecutionEnvironment}, and\n+\t * reset the merged env configurations with dependencies to every {@link DataStreamPythonStatelessFunctionOperator}.\n+\t * It is an idempotent operation that can be call multiple times. Remember that only when need to execute the\n+\t * StreamGraph can we set the clearTransformations to be True.\n+\t */\n+\tpublic static StreamGraph generateStreamGraphWithDependencies(\n+\t\tStreamExecutionEnvironment env, boolean clearTransformations) throws IllegalAccessException,\n+\t\tNoSuchMethodException, InvocationTargetException {\n+\n+\t\tConfiguration mergedConfig = getEnvConfigWithDependencies(env);\n+\t\tStreamGraph streamGraph = env.getStreamGraph(StreamExecutionEnvironment.DEFAULT_JOB_NAME, clearTransformations);\n+\t\tCollection<StreamNode> streamNodes = streamGraph.getStreamNodes();\n+\t\tfor (StreamNode streamNode : streamNodes) {\n+\n+\t\t\talignStreamNode(streamNode, streamGraph);\n+\n+\t\t\tStreamOperatorFactory streamOperatorFactory = streamNode.getOperatorFactory();\n+\t\t\tif (streamOperatorFactory instanceof SimpleOperatorFactory) {\n+\t\t\t\tStreamOperator streamOperator = ((SimpleOperatorFactory) streamOperatorFactory).getOperator();\n+\t\t\t\tif (streamOperator instanceof DataStreamPythonStatelessFunctionOperator) {\n+\t\t\t\t\tDataStreamPythonStatelessFunctionOperator dataStreamPythonStatelessFunctionOperator =\n+\t\t\t\t\t\t(DataStreamPythonStatelessFunctionOperator) streamOperator;\n+\t\t\t\t\tConfiguration oldConfig = dataStreamPythonStatelessFunctionOperator.getMergedEnvConfig();\n+\t\t\t\t\tdataStreamPythonStatelessFunctionOperator.setPythonConfig(generateNewPythonConfig(oldConfig,\n+\t\t\t\t\t\tmergedConfig));\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t\treturn streamGraph;\n+\t}\n+\n+\t/**\n+\t * Generator a new {@link  PythonConfig} with the combined config which is derived from oldConfig.\n+\t */\n+\tprivate static PythonConfig generateNewPythonConfig(Configuration oldConfig, Configuration newConfig) {\n+\t\tsetIfNotExist(PythonOptions.MAX_BUNDLE_SIZE, oldConfig, newConfig);\n+\t\tsetIfNotExist(PythonOptions.MAX_BUNDLE_TIME_MILLS, oldConfig, newConfig);\n+\t\tsetIfNotExist(PythonOptions.MAX_BUNDLE_TIME_MILLS, oldConfig, newConfig);\n+\t\tsetIfNotExist(PythonOptions.PYTHON_FRAMEWORK_MEMORY_SIZE, oldConfig, newConfig);\n+\t\tsetIfNotExist(PythonOptions.PYTHON_DATA_BUFFER_MEMORY_SIZE, oldConfig, newConfig);\n+\t\tsetIfNotExist(PythonOptions.PYTHON_EXECUTABLE, oldConfig, newConfig);\n+\t\tsetIfNotExist(PythonOptions.PYTHON_METRIC_ENABLED, oldConfig, newConfig);\n+\t\tsetIfNotExist(PythonOptions.USE_MANAGED_MEMORY, oldConfig, newConfig);\n+\n+\t\tcombineConfigValue(PythonDependencyUtils.PYTHON_FILES, oldConfig, newConfig);\n+\t\tcombineConfigValue(PythonDependencyUtils.PYTHON_REQUIREMENTS_FILE, oldConfig, newConfig);\n+\t\tcombineConfigValue(PythonDependencyUtils.PYTHON_ARCHIVES, oldConfig, newConfig);\n+\n+\t\treturn new PythonConfig(oldConfig);\n+\t}\n+\n+\t/**\n+\t * Make sure new configuration not overriding the previously configured value. For example, the MAX_BUNDLE_SIZE of\n+\t * {@link org.apache.flink.datastream.runtime.operators.python.DataStreamPythonReduceFunctionOperator} is\n+\t * pre-configured to be 1, we must not to change it.\n+\t */\n+\tprivate static void setIfNotExist(ConfigOption configOption, Configuration oldConfig, Configuration newConfig) {\n+\t\tif (!oldConfig.containsKey(configOption.key())) {\n+\t\t\toldConfig.set(configOption, newConfig.get(configOption));\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * Dependency file information maintained by a Map in old config can be combined with new config.\n+\t */\n+\tprivate static void combineConfigValue(ConfigOption<Map<String, String>> configOption, Configuration oldConfig, Configuration newConfig) {\n+\t\tMap<String, String> oldConfigValue = oldConfig.getOptional(configOption).orElse(new HashMap<>());\n+\t\toldConfigValue.putAll(newConfig.getOptional(configOption).orElse(new HashMap<>()));\n+\t\toldConfig.set(configOption, oldConfigValue);\n+\t}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6098b6b0563eded5c97787f55094467b82a88593"}, "originalPosition": 149}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNTU2MjMyOnYy", "diffSide": "RIGHT", "path": "flink-python/src/main/java/org/apache/flink/streaming/api/operators/python/AbstractPythonFunctionOperator.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QwODo0NjoyNFrOHACCsQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QwODo0NjoyNFrOHACCsQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTc5NTUwNQ==", "bodyText": "Put the set method close to the corresponding get method.", "url": "https://github.com/apache/flink/pull/13136#discussion_r469795505", "createdAt": "2020-08-13T08:46:24Z", "author": {"login": "hequn8128"}, "path": "flink-python/src/main/java/org/apache/flink/streaming/api/operators/python/AbstractPythonFunctionOperator.java", "diffHunk": "@@ -240,6 +240,14 @@ public void processWatermark(Watermark mark) throws Exception {\n \t\t}\n \t}\n \n+\t/**\n+\t * Reset the {@link PythonConfig} if needed.\n+\t * */\n+\t@Internal\n+\tpublic void setPythonConfig(PythonConfig pythonConfig) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6098b6b0563eded5c97787f55094467b82a88593"}, "originalPosition": 17}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNTYyNjE5OnYy", "diffSide": "RIGHT", "path": "flink-python/pyflink/datastream/tests/test_stream_execution_environment.py", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QwOTowMzozM1rOHACqlQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QwOTowMzozM1rOHACqlQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTgwNTcxNw==", "bodyText": "Duplicated check since it has been tested by other tests.", "url": "https://github.com/apache/flink/pull/13136#discussion_r469805717", "createdAt": "2020-08-13T09:03:33Z", "author": {"login": "hequn8128"}, "path": "flink-python/pyflink/datastream/tests/test_stream_execution_environment.py", "diffHunk": "@@ -280,5 +283,222 @@ def test_execute_async(self):\n         execution_result = job_client.get_job_execution_result().result()\n         self.assertEqual(str(job_id), str(execution_result.get_job_id()))\n \n+    def test_add_python_file(self):\n+        import uuid\n+        python_file_dir = os.path.join(self.tempdir, \"python_file_dir_\" + str(uuid.uuid4()))\n+        os.mkdir(python_file_dir)\n+        python_file_path = os.path.join(python_file_dir, \"test_stream_dependency_manage_lib.py\")\n+        with open(python_file_path, 'w') as f:\n+            f.write(\"def add_two(a):\\n    return a + 2\")\n+\n+        def plus_two_map(value):\n+            from test_stream_dependency_manage_lib import add_two\n+            return add_two(value)\n+\n+        self.env.add_python_file(python_file_path)\n+        ds = self.env.from_collection([1, 2, 3, 4, 5])\n+        ds.map(plus_two_map).add_sink(self.test_sink)\n+        self.env.execute(\"test add python file\")\n+        result = self.test_sink.get_results(True)\n+        expected = ['3', '4', '5', '6', '7']\n+        result.sort()\n+        expected.sort()\n+        self.assertEqual(expected, result)\n+\n+    def test_set_requirements_without_cached_directory(self):\n+        import uuid\n+        requirements_txt_path = os.path.join(self.tempdir, str(uuid.uuid4()))\n+        with open(requirements_txt_path, 'w') as f:\n+            f.write(\"cloudpickle==1.2.2\")\n+        self.env.set_python_requirements(requirements_txt_path)\n+\n+        def check_requirements(i):\n+            import cloudpickle\n+            assert os.path.abspath(cloudpickle.__file__).startswith(\n+                os.environ['_PYTHON_REQUIREMENTS_INSTALL_DIR'])\n+            return i\n+\n+        ds = self.env.from_collection([1, 2, 3, 4, 5])\n+        ds.map(check_requirements).add_sink(self.test_sink)\n+        self.env.execute(\"test set requirements without cache dir\")\n+        result = self.test_sink.get_results(True)\n+        expected = ['1', '2', '3', '4', '5']\n+        result.sort()\n+        expected.sort()\n+        self.assertEqual(expected, result)\n+\n+    def test_set_requirements_with_cached_directory(self):\n+        import uuid\n+        tmp_dir = self.tempdir\n+        requirements_txt_path = os.path.join(tmp_dir, \"requirements_txt_\" + str(uuid.uuid4()))\n+        with open(requirements_txt_path, 'w') as f:\n+            f.write(\"python-package1==0.0.0\")\n+\n+        requirements_dir_path = os.path.join(tmp_dir, \"requirements_dir_\" + str(uuid.uuid4()))\n+        os.mkdir(requirements_dir_path)\n+        package_file_name = \"python-package1-0.0.0.tar.gz\"\n+        with open(os.path.join(requirements_dir_path, package_file_name), 'wb') as f:\n+            import base64\n+            # This base64 data is encoded from a python package file which includes a\n+            # \"python_package1\" module. The module contains a \"plus(a, b)\" function.\n+            # The base64 can be recomputed by following code:\n+            # base64.b64encode(open(\"python-package1-0.0.0.tar.gz\", \"rb\").read()).decode(\"utf-8\")\n+            f.write(base64.b64decode(\n+                \"H4sICNefrV0C/2Rpc3QvcHl0aG9uLXBhY2thZ2UxLTAuMC4wLnRhcgDtmVtv2jAYhnPtX2H1CrRCY+ckI\"\n+                \"XEx7axuUA11u5imyICTRc1JiVnHfv1MKKWjYxwKEdPehws7xkmUfH5f+3PyqfqWpa1cjG5EKFnLbOvfhX\"\n+                \"FQTI3nOPPSdavS5Pa8nGMwy3Esi3ke9wyTObbnGNQxamBSKlFQavzUryG8ldG6frpbEGx4yNmDLMp/hPy\"\n+                \"P8b+6fNN613vdP1z8XdteG3+ug/17/F3Hcw1qIv5H54NUYiyUaH2SRRllaYeytkl6IpEdujI2yH2XapCQ\"\n+                \"wSRJRDHt0OveZa//uUfeZonUvUO5bHo+0ZcoVo9bMhFRvGx9H41kWj447aUsR0WUq+pui8arWKggK5Jli\"\n+                \"wGOo/95q79ovXi6/nfyf246Dof/n078fT9KI+X77Xx6BP83bX4Xf5NxT7dz7toO/L8OxjKgeTwpG+KcDp\"\n+                \"sdQjWFVJMipYI+o0MCk4X/t2UYtqI0yPabCHb3f861XcD/Ty/+Y5nLdCzT0dSPo/SmbKsf6un+b7KV+Ls\"\n+                \"W4/D/OoC9w/930P9eGwM75//csrD+Q/6P/P/k9D/oX3988Wqw1bS/tf6tR+s/m3EG/ddBqXO9XKf15C8p\"\n+                \"P9k4HZBtBgzZaVW5vrfKcj+W32W82ygEB9D/Xu9+4/qfP9L/rBv0X1v87yONKRX61/qfzwqjIDzIPTbv/\"\n+                \"7or3/88i0H/tfBFW7s/s/avRInQH06ieEy7tDrQeYHUdRN7wP+n/vf62LOH/pld7f9xz7a5Pfufedy0oP\"\n+                \"86iJI8KxStAq6yLC4JWdbbVbWRikR2z1ZGytk5vauW3QdnBFE6XqwmykazCesAAAAAAAAAAAAAAAAAAAA\"\n+                \"AAAAAAAAAAAAAAOBw/AJw5CHBAFAAAA==\"))\n+        self.env.set_python_requirements(requirements_txt_path, requirements_dir_path)\n+\n+        def add_one(i):\n+            from python_package1 import plus\n+            return plus(i, 1)\n+\n+        ds = self.env.from_collection([1, 2, 3, 4, 5])\n+        ds.map(add_one).add_sink(self.test_sink)\n+        self.env.execute(\"test set requirements with cachd dir\")\n+        result = self.test_sink.get_results(True)\n+        expected = ['2', '3', '4', '5', '6']\n+        result.sort()\n+        expected.sort()\n+        self.assertEqual(expected, result)\n+\n+    def test_add_python_archive(self):\n+        import uuid\n+        import shutil\n+        tmp_dir = self.tempdir\n+        archive_dir_path = os.path.join(tmp_dir, \"archive_\" + str(uuid.uuid4()))\n+        os.mkdir(archive_dir_path)\n+        with open(os.path.join(archive_dir_path, \"data.txt\"), 'w') as f:\n+            f.write(\"2\")\n+        archive_file_path = \\\n+            shutil.make_archive(os.path.dirname(archive_dir_path), 'zip', archive_dir_path)\n+        self.env.add_python_archive(archive_file_path, \"data\")\n+\n+        def add_from_file(i):\n+            with open(\"data/data.txt\", 'r') as f:\n+                return i + int(f.read())\n+\n+        ds = self.env.from_collection([1, 2, 3, 4, 5])\n+        ds.map(add_from_file).add_sink(self.test_sink)\n+        self.env.execute(\"test set python archive\")\n+        result = self.test_sink.get_results(True)\n+        expected = ['3', '4', '5', '6', '7']\n+        result.sort()\n+        expected.sort()\n+        self.assertEqual(expected, result)\n+\n+    def test_set_stream_env(self):\n+        import sys\n+        python_exec = sys.executable\n+        tmp_dir = self.tempdir\n+        python_exec_link_path = os.path.join(tmp_dir, \"py_exec\")\n+        os.symlink(python_exec, python_exec_link_path)\n+        self.env.set_python_executable(python_exec_link_path)\n+\n+        def check_python_exec(i):\n+            import os\n+            assert os.environ[\"python\"] == python_exec_link_path\n+            return i\n+\n+        def check_pyflink_gateway_disabled(i):\n+            try:\n+                from pyflink.java_gateway import get_gateway\n+                get_gateway()\n+            except Exception as e:\n+                assert str(e).startswith(\"It's launching the PythonGatewayServer during Python UDF\"\n+                                         \" execution which is unexpected.\")\n+            else:\n+                raise Exception(\"The gateway server is not disabled!\")\n+            return i", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6098b6b0563eded5c97787f55094467b82a88593"}, "originalPosition": 161}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 546, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}