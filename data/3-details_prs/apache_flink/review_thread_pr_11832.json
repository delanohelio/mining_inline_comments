{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDA2MTE3NTE2", "number": 11832, "reviewThreads": {"totalCount": 16, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQwNzozMjo1NVrOD2RN8w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQxMToyNjo1NFrOD3lXvQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4MjMxNzk1OnYy", "diffSide": "RIGHT", "path": "flink-python/pyflink/table/tests/test_pandas_conversion.py", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQwNzozMjo1NVrOGMAXyg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQwMjo0NjoxOFrOGNt9-w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTI0MjE4Ng==", "bodyText": "I found we should use lowercase for these test methods. However, it is not related to this PR. Maybe we can create another jira to address the problem.", "url": "https://github.com/apache/flink/pull/11832#discussion_r415242186", "createdAt": "2020-04-26T07:32:55Z", "author": {"login": "hequn8128"}, "path": "flink-python/pyflink/table/tests/test_pandas_conversion.py", "diffHunk": "@@ -0,0 +1,147 @@\n+################################################################################\n+#  Licensed to the Apache Software Foundation (ASF) under one\n+#  or more contributor license agreements.  See the NOTICE file\n+#  distributed with this work for additional information\n+#  regarding copyright ownership.  The ASF licenses this file\n+#  to you under the Apache License, Version 2.0 (the\n+#  \"License\"); you may not use this file except in compliance\n+#  with the License.  You may obtain a copy of the License at\n+#\n+#      http://www.apache.org/licenses/LICENSE-2.0\n+#\n+#  Unless required by applicable law or agreed to in writing, software\n+#  distributed under the License is distributed on an \"AS IS\" BASIS,\n+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+#  See the License for the specific language governing permissions and\n+# limitations under the License.\n+################################################################################\n+import datetime\n+import decimal\n+\n+from pyflink.table.types import DataTypes, Row\n+from pyflink.testing import source_sink_utils\n+from pyflink.testing.test_case_utils import PyFlinkBlinkBatchTableTestCase, \\\n+    PyFlinkBlinkStreamTableTestCase, PyFlinkStreamTableTestCase\n+\n+\n+class PandasConversionTestBase(object):\n+\n+    @classmethod\n+    def setUpClass(cls):", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f427a2215c9bb6ecb5458e8693887cafffbef491"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzAzNzgxOQ==", "bodyText": "The name setUpClass is from unittest.TestCase and I guess we can not change it.", "url": "https://github.com/apache/flink/pull/11832#discussion_r417037819", "createdAt": "2020-04-29T02:46:18Z", "author": {"login": "dianfu"}, "path": "flink-python/pyflink/table/tests/test_pandas_conversion.py", "diffHunk": "@@ -0,0 +1,147 @@\n+################################################################################\n+#  Licensed to the Apache Software Foundation (ASF) under one\n+#  or more contributor license agreements.  See the NOTICE file\n+#  distributed with this work for additional information\n+#  regarding copyright ownership.  The ASF licenses this file\n+#  to you under the Apache License, Version 2.0 (the\n+#  \"License\"); you may not use this file except in compliance\n+#  with the License.  You may obtain a copy of the License at\n+#\n+#      http://www.apache.org/licenses/LICENSE-2.0\n+#\n+#  Unless required by applicable law or agreed to in writing, software\n+#  distributed under the License is distributed on an \"AS IS\" BASIS,\n+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+#  See the License for the specific language governing permissions and\n+# limitations under the License.\n+################################################################################\n+import datetime\n+import decimal\n+\n+from pyflink.table.types import DataTypes, Row\n+from pyflink.testing import source_sink_utils\n+from pyflink.testing.test_case_utils import PyFlinkBlinkBatchTableTestCase, \\\n+    PyFlinkBlinkStreamTableTestCase, PyFlinkStreamTableTestCase\n+\n+\n+class PandasConversionTestBase(object):\n+\n+    @classmethod\n+    def setUpClass(cls):", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTI0MjE4Ng=="}, "originalCommit": {"oid": "f427a2215c9bb6ecb5458e8693887cafffbef491"}, "originalPosition": 30}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4MjQwOTkyOnYy", "diffSide": "RIGHT", "path": "flink-python/pyflink/table/tests/test_pandas_conversion.py", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQwODoyODozNFrOGMBB3Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQwMjo1MzoyNlrOGNuEXg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTI1Mjk1Nw==", "bodyText": "Why we need to convert to NumPy types?", "url": "https://github.com/apache/flink/pull/11832#discussion_r415252957", "createdAt": "2020-04-26T08:28:34Z", "author": {"login": "hequn8128"}, "path": "flink-python/pyflink/table/tests/test_pandas_conversion.py", "diffHunk": "@@ -0,0 +1,147 @@\n+################################################################################\n+#  Licensed to the Apache Software Foundation (ASF) under one\n+#  or more contributor license agreements.  See the NOTICE file\n+#  distributed with this work for additional information\n+#  regarding copyright ownership.  The ASF licenses this file\n+#  to you under the Apache License, Version 2.0 (the\n+#  \"License\"); you may not use this file except in compliance\n+#  with the License.  You may obtain a copy of the License at\n+#\n+#      http://www.apache.org/licenses/LICENSE-2.0\n+#\n+#  Unless required by applicable law or agreed to in writing, software\n+#  distributed under the License is distributed on an \"AS IS\" BASIS,\n+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+#  See the License for the specific language governing permissions and\n+# limitations under the License.\n+################################################################################\n+import datetime\n+import decimal\n+\n+from pyflink.table.types import DataTypes, Row\n+from pyflink.testing import source_sink_utils\n+from pyflink.testing.test_case_utils import PyFlinkBlinkBatchTableTestCase, \\\n+    PyFlinkBlinkStreamTableTestCase, PyFlinkStreamTableTestCase\n+\n+\n+class PandasConversionTestBase(object):\n+\n+    @classmethod\n+    def setUpClass(cls):\n+        super(PandasConversionTestBase, cls).setUpClass()\n+        cls.data = [(1, 1, 1, 1, True, 1.1, 1.2, 'hello', bytearray(b\"aaa\"),\n+                     decimal.Decimal('1000000000000000000.01'), datetime.date(2014, 9, 13),\n+                     datetime.time(hour=1, minute=0, second=1),\n+                     datetime.datetime(1970, 1, 1, 0, 0, 0, 123000), ['hello', '\u4e2d\u6587'],\n+                     Row(a=1, b='hello', c=datetime.datetime(1970, 1, 1, 0, 0, 0, 123000),\n+                         d=[1, 2])),\n+                    (2, 2, 2, 2, False, 2.1, 2.2, 'world', bytearray(b\"bbb\"),\n+                     decimal.Decimal('1000000000000000000.02'), datetime.date(2014, 9, 13),\n+                     datetime.time(hour=1, minute=0, second=1),\n+                     datetime.datetime(1970, 1, 1, 0, 0, 0, 123000), ['hello', '\u4e2d\u6587'],\n+                     Row(a=1, b='hello', c=datetime.datetime(1970, 1, 1, 0, 0, 0, 123000),\n+                         d=[1, 2]))]\n+        cls.data_type = DataTypes.ROW(\n+            [DataTypes.FIELD(\"f1\", DataTypes.TINYINT()),\n+             DataTypes.FIELD(\"f2\", DataTypes.SMALLINT()),\n+             DataTypes.FIELD(\"f3\", DataTypes.INT()),\n+             DataTypes.FIELD(\"f4\", DataTypes.BIGINT()),\n+             DataTypes.FIELD(\"f5\", DataTypes.BOOLEAN()),\n+             DataTypes.FIELD(\"f6\", DataTypes.FLOAT()),\n+             DataTypes.FIELD(\"f7\", DataTypes.DOUBLE()),\n+             DataTypes.FIELD(\"f8\", DataTypes.STRING()),\n+             DataTypes.FIELD(\"f9\", DataTypes.BYTES()),\n+             DataTypes.FIELD(\"f10\", DataTypes.DECIMAL(38, 18)),\n+             DataTypes.FIELD(\"f11\", DataTypes.DATE()),\n+             DataTypes.FIELD(\"f12\", DataTypes.TIME()),\n+             DataTypes.FIELD(\"f13\", DataTypes.TIMESTAMP(3)),\n+             DataTypes.FIELD(\"f14\", DataTypes.ARRAY(DataTypes.STRING())),\n+             DataTypes.FIELD(\"f15\", DataTypes.ROW(\n+                 [DataTypes.FIELD(\"a\", DataTypes.INT()),\n+                  DataTypes.FIELD(\"b\", DataTypes.STRING()),\n+                  DataTypes.FIELD(\"c\", DataTypes.TIMESTAMP(3)),\n+                  DataTypes.FIELD(\"d\", DataTypes.ARRAY(DataTypes.INT()))]))])\n+        cls.pdf = cls.create_pandas_data_frame()\n+\n+    @classmethod\n+    def create_pandas_data_frame(cls):\n+        data_dict = {}\n+        for j, name in enumerate(cls.data_type.names):\n+            data_dict[name] = [cls.data[i][j] for i in range(len(cls.data))]\n+        # need convert to numpy types", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f427a2215c9bb6ecb5458e8693887cafffbef491"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzAzOTQ1NA==", "bodyText": "The integer types will be parsed as int64 by default and so we need to specify it explicitly.", "url": "https://github.com/apache/flink/pull/11832#discussion_r417039454", "createdAt": "2020-04-29T02:53:26Z", "author": {"login": "dianfu"}, "path": "flink-python/pyflink/table/tests/test_pandas_conversion.py", "diffHunk": "@@ -0,0 +1,147 @@\n+################################################################################\n+#  Licensed to the Apache Software Foundation (ASF) under one\n+#  or more contributor license agreements.  See the NOTICE file\n+#  distributed with this work for additional information\n+#  regarding copyright ownership.  The ASF licenses this file\n+#  to you under the Apache License, Version 2.0 (the\n+#  \"License\"); you may not use this file except in compliance\n+#  with the License.  You may obtain a copy of the License at\n+#\n+#      http://www.apache.org/licenses/LICENSE-2.0\n+#\n+#  Unless required by applicable law or agreed to in writing, software\n+#  distributed under the License is distributed on an \"AS IS\" BASIS,\n+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+#  See the License for the specific language governing permissions and\n+# limitations under the License.\n+################################################################################\n+import datetime\n+import decimal\n+\n+from pyflink.table.types import DataTypes, Row\n+from pyflink.testing import source_sink_utils\n+from pyflink.testing.test_case_utils import PyFlinkBlinkBatchTableTestCase, \\\n+    PyFlinkBlinkStreamTableTestCase, PyFlinkStreamTableTestCase\n+\n+\n+class PandasConversionTestBase(object):\n+\n+    @classmethod\n+    def setUpClass(cls):\n+        super(PandasConversionTestBase, cls).setUpClass()\n+        cls.data = [(1, 1, 1, 1, True, 1.1, 1.2, 'hello', bytearray(b\"aaa\"),\n+                     decimal.Decimal('1000000000000000000.01'), datetime.date(2014, 9, 13),\n+                     datetime.time(hour=1, minute=0, second=1),\n+                     datetime.datetime(1970, 1, 1, 0, 0, 0, 123000), ['hello', '\u4e2d\u6587'],\n+                     Row(a=1, b='hello', c=datetime.datetime(1970, 1, 1, 0, 0, 0, 123000),\n+                         d=[1, 2])),\n+                    (2, 2, 2, 2, False, 2.1, 2.2, 'world', bytearray(b\"bbb\"),\n+                     decimal.Decimal('1000000000000000000.02'), datetime.date(2014, 9, 13),\n+                     datetime.time(hour=1, minute=0, second=1),\n+                     datetime.datetime(1970, 1, 1, 0, 0, 0, 123000), ['hello', '\u4e2d\u6587'],\n+                     Row(a=1, b='hello', c=datetime.datetime(1970, 1, 1, 0, 0, 0, 123000),\n+                         d=[1, 2]))]\n+        cls.data_type = DataTypes.ROW(\n+            [DataTypes.FIELD(\"f1\", DataTypes.TINYINT()),\n+             DataTypes.FIELD(\"f2\", DataTypes.SMALLINT()),\n+             DataTypes.FIELD(\"f3\", DataTypes.INT()),\n+             DataTypes.FIELD(\"f4\", DataTypes.BIGINT()),\n+             DataTypes.FIELD(\"f5\", DataTypes.BOOLEAN()),\n+             DataTypes.FIELD(\"f6\", DataTypes.FLOAT()),\n+             DataTypes.FIELD(\"f7\", DataTypes.DOUBLE()),\n+             DataTypes.FIELD(\"f8\", DataTypes.STRING()),\n+             DataTypes.FIELD(\"f9\", DataTypes.BYTES()),\n+             DataTypes.FIELD(\"f10\", DataTypes.DECIMAL(38, 18)),\n+             DataTypes.FIELD(\"f11\", DataTypes.DATE()),\n+             DataTypes.FIELD(\"f12\", DataTypes.TIME()),\n+             DataTypes.FIELD(\"f13\", DataTypes.TIMESTAMP(3)),\n+             DataTypes.FIELD(\"f14\", DataTypes.ARRAY(DataTypes.STRING())),\n+             DataTypes.FIELD(\"f15\", DataTypes.ROW(\n+                 [DataTypes.FIELD(\"a\", DataTypes.INT()),\n+                  DataTypes.FIELD(\"b\", DataTypes.STRING()),\n+                  DataTypes.FIELD(\"c\", DataTypes.TIMESTAMP(3)),\n+                  DataTypes.FIELD(\"d\", DataTypes.ARRAY(DataTypes.INT()))]))])\n+        cls.pdf = cls.create_pandas_data_frame()\n+\n+    @classmethod\n+    def create_pandas_data_frame(cls):\n+        data_dict = {}\n+        for j, name in enumerate(cls.data_type.names):\n+            data_dict[name] = [cls.data[i][j] for i in range(len(cls.data))]\n+        # need convert to numpy types", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTI1Mjk1Nw=="}, "originalCommit": {"oid": "f427a2215c9bb6ecb5458e8693887cafffbef491"}, "originalPosition": 71}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4MjQyNTIzOnYy", "diffSide": "RIGHT", "path": "flink-python/pyflink/table/tests/test_pandas_conversion.py", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQwODozNzo0NVrOGMBI8w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQwNzoyNzozM1rOGNyy3Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTI1NDc3MQ==", "bodyText": "Can we also cover the batch mode for the old planner?", "url": "https://github.com/apache/flink/pull/11832#discussion_r415254771", "createdAt": "2020-04-26T08:37:45Z", "author": {"login": "hequn8128"}, "path": "flink-python/pyflink/table/tests/test_pandas_conversion.py", "diffHunk": "@@ -0,0 +1,147 @@\n+################################################################################\n+#  Licensed to the Apache Software Foundation (ASF) under one\n+#  or more contributor license agreements.  See the NOTICE file\n+#  distributed with this work for additional information\n+#  regarding copyright ownership.  The ASF licenses this file\n+#  to you under the Apache License, Version 2.0 (the\n+#  \"License\"); you may not use this file except in compliance\n+#  with the License.  You may obtain a copy of the License at\n+#\n+#      http://www.apache.org/licenses/LICENSE-2.0\n+#\n+#  Unless required by applicable law or agreed to in writing, software\n+#  distributed under the License is distributed on an \"AS IS\" BASIS,\n+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+#  See the License for the specific language governing permissions and\n+# limitations under the License.\n+################################################################################\n+import datetime\n+import decimal\n+\n+from pyflink.table.types import DataTypes, Row\n+from pyflink.testing import source_sink_utils\n+from pyflink.testing.test_case_utils import PyFlinkBlinkBatchTableTestCase, \\\n+    PyFlinkBlinkStreamTableTestCase, PyFlinkStreamTableTestCase\n+\n+\n+class PandasConversionTestBase(object):\n+\n+    @classmethod\n+    def setUpClass(cls):\n+        super(PandasConversionTestBase, cls).setUpClass()\n+        cls.data = [(1, 1, 1, 1, True, 1.1, 1.2, 'hello', bytearray(b\"aaa\"),\n+                     decimal.Decimal('1000000000000000000.01'), datetime.date(2014, 9, 13),\n+                     datetime.time(hour=1, minute=0, second=1),\n+                     datetime.datetime(1970, 1, 1, 0, 0, 0, 123000), ['hello', '\u4e2d\u6587'],\n+                     Row(a=1, b='hello', c=datetime.datetime(1970, 1, 1, 0, 0, 0, 123000),\n+                         d=[1, 2])),\n+                    (2, 2, 2, 2, False, 2.1, 2.2, 'world', bytearray(b\"bbb\"),\n+                     decimal.Decimal('1000000000000000000.02'), datetime.date(2014, 9, 13),\n+                     datetime.time(hour=1, minute=0, second=1),\n+                     datetime.datetime(1970, 1, 1, 0, 0, 0, 123000), ['hello', '\u4e2d\u6587'],\n+                     Row(a=1, b='hello', c=datetime.datetime(1970, 1, 1, 0, 0, 0, 123000),\n+                         d=[1, 2]))]\n+        cls.data_type = DataTypes.ROW(\n+            [DataTypes.FIELD(\"f1\", DataTypes.TINYINT()),\n+             DataTypes.FIELD(\"f2\", DataTypes.SMALLINT()),\n+             DataTypes.FIELD(\"f3\", DataTypes.INT()),\n+             DataTypes.FIELD(\"f4\", DataTypes.BIGINT()),\n+             DataTypes.FIELD(\"f5\", DataTypes.BOOLEAN()),\n+             DataTypes.FIELD(\"f6\", DataTypes.FLOAT()),\n+             DataTypes.FIELD(\"f7\", DataTypes.DOUBLE()),\n+             DataTypes.FIELD(\"f8\", DataTypes.STRING()),\n+             DataTypes.FIELD(\"f9\", DataTypes.BYTES()),\n+             DataTypes.FIELD(\"f10\", DataTypes.DECIMAL(38, 18)),\n+             DataTypes.FIELD(\"f11\", DataTypes.DATE()),\n+             DataTypes.FIELD(\"f12\", DataTypes.TIME()),\n+             DataTypes.FIELD(\"f13\", DataTypes.TIMESTAMP(3)),\n+             DataTypes.FIELD(\"f14\", DataTypes.ARRAY(DataTypes.STRING())),\n+             DataTypes.FIELD(\"f15\", DataTypes.ROW(\n+                 [DataTypes.FIELD(\"a\", DataTypes.INT()),\n+                  DataTypes.FIELD(\"b\", DataTypes.STRING()),\n+                  DataTypes.FIELD(\"c\", DataTypes.TIMESTAMP(3)),\n+                  DataTypes.FIELD(\"d\", DataTypes.ARRAY(DataTypes.INT()))]))])\n+        cls.pdf = cls.create_pandas_data_frame()\n+\n+    @classmethod\n+    def create_pandas_data_frame(cls):\n+        data_dict = {}\n+        for j, name in enumerate(cls.data_type.names):\n+            data_dict[name] = [cls.data[i][j] for i in range(len(cls.data))]\n+        # need convert to numpy types\n+        import numpy as np\n+        data_dict[\"f1\"] = np.int8(data_dict[\"f1\"])\n+        data_dict[\"f2\"] = np.int16(data_dict[\"f2\"])\n+        data_dict[\"f3\"] = np.int32(data_dict[\"f3\"])\n+        data_dict[\"f4\"] = np.int64(data_dict[\"f4\"])\n+        data_dict[\"f6\"] = np.float32(data_dict[\"f6\"])\n+        data_dict[\"f7\"] = np.float64(data_dict[\"f7\"])\n+        data_dict[\"f15\"] = [row.as_dict() for row in data_dict[\"f15\"]]\n+        import pandas as pd\n+        return pd.DataFrame(data=data_dict,\n+                            columns=['f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9',\n+                                     'f10', 'f11', 'f12', 'f13', 'f14', 'f15'])\n+\n+\n+class PandasConversionTests(PandasConversionTestBase):\n+\n+    def test_from_pandas_with_incorrect_schema(self):\n+        fields = self.data_type.fields.copy()\n+        fields[0], fields[7] = fields[7], fields[0]  # swap str with tinyint\n+        wrong_schema = DataTypes.ROW(fields)  # should be DataTypes.STRING()\n+        with self.assertRaisesRegex(Exception, \"Expected a string.*got int8\"):\n+            self.t_env.from_pandas(self.pdf, schema=wrong_schema)\n+\n+    def test_from_pandas_with_names(self):\n+        # skip decimal as currently only decimal(38, 18) is supported\n+        pdf = self.pdf.drop(['f10', 'f11', 'f12', 'f13', 'f14', 'f15'], axis=1)\n+        new_names = list(map(str, range(len(pdf.columns))))\n+        table = self.t_env.from_pandas(pdf, schema=new_names)\n+        self.assertEqual(new_names, table.get_schema().get_field_names())\n+        table = self.t_env.from_pandas(pdf, schema=tuple(new_names))\n+        self.assertEqual(new_names, table.get_schema().get_field_names())\n+\n+    def test_from_pandas_with_types(self):\n+        new_types = self.data_type.field_types()\n+        new_types[0] = DataTypes.BIGINT()\n+        table = self.t_env.from_pandas(self.pdf, schema=new_types)\n+        self.assertEqual(new_types, table.get_schema().get_field_data_types())\n+        table = self.t_env.from_pandas(self.pdf, schema=tuple(new_types))\n+        self.assertEqual(new_types, table.get_schema().get_field_data_types())\n+\n+\n+class PandasConversionITTests(PandasConversionTestBase):\n+\n+    def test_from_pandas(self):\n+        table = self.t_env.from_pandas(self.pdf, self.data_type, 5)\n+        self.assertEqual(self.data_type, table.get_schema().to_row_data_type())\n+\n+        table = table.filter(\"f1 < 2\")\n+        table_sink = source_sink_utils.TestAppendSink(\n+            self.data_type.field_names(),\n+            self.data_type.field_types())\n+        self.t_env.register_table_sink(\"Results\", table_sink)\n+        table.insert_into(\"Results\")\n+        self.t_env.execute(\"test\")\n+        actual = source_sink_utils.results()\n+        self.assert_equals(actual,\n+                           [\"1,1,1,1,true,1.1,1.2,hello,[97, 97, 97],\"\n+                            \"1000000000000000000.010000000000000000,2014-09-13,01:00:01,\"\n+                            \"1970-01-01 00:00:00.123,[hello, \u4e2d\u6587],1,hello,\"\n+                            \"1970-01-01 00:00:00.123,[1, 2]\"])\n+\n+\n+class StreamPandasConversionTests(PandasConversionITTests,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f427a2215c9bb6ecb5458e8693887cafffbef491"}, "originalPosition": 134}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzExNjg5Mw==", "bodyText": "Most code could not be reusable for the batch mode of the old planner. So I'd like to handle it in a separate JIRA if needed.", "url": "https://github.com/apache/flink/pull/11832#discussion_r417116893", "createdAt": "2020-04-29T07:27:33Z", "author": {"login": "dianfu"}, "path": "flink-python/pyflink/table/tests/test_pandas_conversion.py", "diffHunk": "@@ -0,0 +1,147 @@\n+################################################################################\n+#  Licensed to the Apache Software Foundation (ASF) under one\n+#  or more contributor license agreements.  See the NOTICE file\n+#  distributed with this work for additional information\n+#  regarding copyright ownership.  The ASF licenses this file\n+#  to you under the Apache License, Version 2.0 (the\n+#  \"License\"); you may not use this file except in compliance\n+#  with the License.  You may obtain a copy of the License at\n+#\n+#      http://www.apache.org/licenses/LICENSE-2.0\n+#\n+#  Unless required by applicable law or agreed to in writing, software\n+#  distributed under the License is distributed on an \"AS IS\" BASIS,\n+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+#  See the License for the specific language governing permissions and\n+# limitations under the License.\n+################################################################################\n+import datetime\n+import decimal\n+\n+from pyflink.table.types import DataTypes, Row\n+from pyflink.testing import source_sink_utils\n+from pyflink.testing.test_case_utils import PyFlinkBlinkBatchTableTestCase, \\\n+    PyFlinkBlinkStreamTableTestCase, PyFlinkStreamTableTestCase\n+\n+\n+class PandasConversionTestBase(object):\n+\n+    @classmethod\n+    def setUpClass(cls):\n+        super(PandasConversionTestBase, cls).setUpClass()\n+        cls.data = [(1, 1, 1, 1, True, 1.1, 1.2, 'hello', bytearray(b\"aaa\"),\n+                     decimal.Decimal('1000000000000000000.01'), datetime.date(2014, 9, 13),\n+                     datetime.time(hour=1, minute=0, second=1),\n+                     datetime.datetime(1970, 1, 1, 0, 0, 0, 123000), ['hello', '\u4e2d\u6587'],\n+                     Row(a=1, b='hello', c=datetime.datetime(1970, 1, 1, 0, 0, 0, 123000),\n+                         d=[1, 2])),\n+                    (2, 2, 2, 2, False, 2.1, 2.2, 'world', bytearray(b\"bbb\"),\n+                     decimal.Decimal('1000000000000000000.02'), datetime.date(2014, 9, 13),\n+                     datetime.time(hour=1, minute=0, second=1),\n+                     datetime.datetime(1970, 1, 1, 0, 0, 0, 123000), ['hello', '\u4e2d\u6587'],\n+                     Row(a=1, b='hello', c=datetime.datetime(1970, 1, 1, 0, 0, 0, 123000),\n+                         d=[1, 2]))]\n+        cls.data_type = DataTypes.ROW(\n+            [DataTypes.FIELD(\"f1\", DataTypes.TINYINT()),\n+             DataTypes.FIELD(\"f2\", DataTypes.SMALLINT()),\n+             DataTypes.FIELD(\"f3\", DataTypes.INT()),\n+             DataTypes.FIELD(\"f4\", DataTypes.BIGINT()),\n+             DataTypes.FIELD(\"f5\", DataTypes.BOOLEAN()),\n+             DataTypes.FIELD(\"f6\", DataTypes.FLOAT()),\n+             DataTypes.FIELD(\"f7\", DataTypes.DOUBLE()),\n+             DataTypes.FIELD(\"f8\", DataTypes.STRING()),\n+             DataTypes.FIELD(\"f9\", DataTypes.BYTES()),\n+             DataTypes.FIELD(\"f10\", DataTypes.DECIMAL(38, 18)),\n+             DataTypes.FIELD(\"f11\", DataTypes.DATE()),\n+             DataTypes.FIELD(\"f12\", DataTypes.TIME()),\n+             DataTypes.FIELD(\"f13\", DataTypes.TIMESTAMP(3)),\n+             DataTypes.FIELD(\"f14\", DataTypes.ARRAY(DataTypes.STRING())),\n+             DataTypes.FIELD(\"f15\", DataTypes.ROW(\n+                 [DataTypes.FIELD(\"a\", DataTypes.INT()),\n+                  DataTypes.FIELD(\"b\", DataTypes.STRING()),\n+                  DataTypes.FIELD(\"c\", DataTypes.TIMESTAMP(3)),\n+                  DataTypes.FIELD(\"d\", DataTypes.ARRAY(DataTypes.INT()))]))])\n+        cls.pdf = cls.create_pandas_data_frame()\n+\n+    @classmethod\n+    def create_pandas_data_frame(cls):\n+        data_dict = {}\n+        for j, name in enumerate(cls.data_type.names):\n+            data_dict[name] = [cls.data[i][j] for i in range(len(cls.data))]\n+        # need convert to numpy types\n+        import numpy as np\n+        data_dict[\"f1\"] = np.int8(data_dict[\"f1\"])\n+        data_dict[\"f2\"] = np.int16(data_dict[\"f2\"])\n+        data_dict[\"f3\"] = np.int32(data_dict[\"f3\"])\n+        data_dict[\"f4\"] = np.int64(data_dict[\"f4\"])\n+        data_dict[\"f6\"] = np.float32(data_dict[\"f6\"])\n+        data_dict[\"f7\"] = np.float64(data_dict[\"f7\"])\n+        data_dict[\"f15\"] = [row.as_dict() for row in data_dict[\"f15\"]]\n+        import pandas as pd\n+        return pd.DataFrame(data=data_dict,\n+                            columns=['f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9',\n+                                     'f10', 'f11', 'f12', 'f13', 'f14', 'f15'])\n+\n+\n+class PandasConversionTests(PandasConversionTestBase):\n+\n+    def test_from_pandas_with_incorrect_schema(self):\n+        fields = self.data_type.fields.copy()\n+        fields[0], fields[7] = fields[7], fields[0]  # swap str with tinyint\n+        wrong_schema = DataTypes.ROW(fields)  # should be DataTypes.STRING()\n+        with self.assertRaisesRegex(Exception, \"Expected a string.*got int8\"):\n+            self.t_env.from_pandas(self.pdf, schema=wrong_schema)\n+\n+    def test_from_pandas_with_names(self):\n+        # skip decimal as currently only decimal(38, 18) is supported\n+        pdf = self.pdf.drop(['f10', 'f11', 'f12', 'f13', 'f14', 'f15'], axis=1)\n+        new_names = list(map(str, range(len(pdf.columns))))\n+        table = self.t_env.from_pandas(pdf, schema=new_names)\n+        self.assertEqual(new_names, table.get_schema().get_field_names())\n+        table = self.t_env.from_pandas(pdf, schema=tuple(new_names))\n+        self.assertEqual(new_names, table.get_schema().get_field_names())\n+\n+    def test_from_pandas_with_types(self):\n+        new_types = self.data_type.field_types()\n+        new_types[0] = DataTypes.BIGINT()\n+        table = self.t_env.from_pandas(self.pdf, schema=new_types)\n+        self.assertEqual(new_types, table.get_schema().get_field_data_types())\n+        table = self.t_env.from_pandas(self.pdf, schema=tuple(new_types))\n+        self.assertEqual(new_types, table.get_schema().get_field_data_types())\n+\n+\n+class PandasConversionITTests(PandasConversionTestBase):\n+\n+    def test_from_pandas(self):\n+        table = self.t_env.from_pandas(self.pdf, self.data_type, 5)\n+        self.assertEqual(self.data_type, table.get_schema().to_row_data_type())\n+\n+        table = table.filter(\"f1 < 2\")\n+        table_sink = source_sink_utils.TestAppendSink(\n+            self.data_type.field_names(),\n+            self.data_type.field_types())\n+        self.t_env.register_table_sink(\"Results\", table_sink)\n+        table.insert_into(\"Results\")\n+        self.t_env.execute(\"test\")\n+        actual = source_sink_utils.results()\n+        self.assert_equals(actual,\n+                           [\"1,1,1,1,true,1.1,1.2,hello,[97, 97, 97],\"\n+                            \"1000000000000000000.010000000000000000,2014-09-13,01:00:01,\"\n+                            \"1970-01-01 00:00:00.123,[hello, \u4e2d\u6587],1,hello,\"\n+                            \"1970-01-01 00:00:00.123,[1, 2]\"])\n+\n+\n+class StreamPandasConversionTests(PandasConversionITTests,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTI1NDc3MQ=="}, "originalCommit": {"oid": "f427a2215c9bb6ecb5458e8693887cafffbef491"}, "originalPosition": 134}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4MjQzODA4OnYy", "diffSide": "RIGHT", "path": "flink-python/pyflink/table/table_environment.py", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQwODo0NTowNlrOGMBOnw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQwODo0NTowNlrOGMBOnw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTI1NjIyMw==", "bodyText": "Add detailed python docs for the API.\nBTW, do we plan to add Flink document for this API in another PR? If so, we can first create a jira to address it under FLINK-17146", "url": "https://github.com/apache/flink/pull/11832#discussion_r415256223", "createdAt": "2020-04-26T08:45:06Z", "author": {"login": "hequn8128"}, "path": "flink-python/pyflink/table/table_environment.py", "diffHunk": "@@ -1107,6 +1107,63 @@ def _from_elements(self, elements, schema):\n         finally:\n             os.unlink(temp_file.name)\n \n+    def from_pandas(self, pdf,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f427a2215c9bb6ecb5458e8693887cafffbef491"}, "originalPosition": 42}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4NDc4ODg2OnYy", "diffSide": "RIGHT", "path": "flink-python/src/main/java/org/apache/flink/table/runtime/arrow/sources/AbstractArrowSourceFunction.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QwNjoxNTo1OFrOGMSc0Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QwNjoxNTo1OFrOGMSc0Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTUzODM4NQ==", "bodyText": "protected", "url": "https://github.com/apache/flink/pull/11832#discussion_r415538385", "createdAt": "2020-04-27T06:15:58Z", "author": {"login": "hequn8128"}, "path": "flink-python/src/main/java/org/apache/flink/table/runtime/arrow/sources/AbstractArrowSourceFunction.java", "diffHunk": "@@ -0,0 +1,214 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.arrow.sources;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.common.typeutils.base.IntSerializer;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.api.java.typeutils.ResultTypeQueryable;\n+import org.apache.flink.api.java.typeutils.runtime.TupleSerializer;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.runtime.state.FunctionInitializationContext;\n+import org.apache.flink.runtime.state.FunctionSnapshotContext;\n+import org.apache.flink.streaming.api.checkpoint.CheckpointedFunction;\n+import org.apache.flink.streaming.api.functions.source.RichParallelSourceFunction;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.table.runtime.arrow.ArrowReader;\n+import org.apache.flink.table.runtime.arrow.ArrowUtils;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.arrow.memory.BufferAllocator;\n+import org.apache.arrow.vector.VectorLoader;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ReadChannel;\n+import org.apache.arrow.vector.ipc.message.ArrowRecordBatch;\n+import org.apache.arrow.vector.ipc.message.MessageSerializer;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.nio.channels.Channels;\n+import java.util.ArrayDeque;\n+import java.util.Deque;\n+\n+/**\n+ * An Arrow {@link SourceFunction} which takes the serialized arrow record batch data as input.\n+ *\n+ * @param <OUT> The type of the records produced by this source.\n+ */\n+@Internal\n+public abstract class AbstractArrowSourceFunction<OUT>\n+\t\textends RichParallelSourceFunction<OUT>\n+\t\timplements ResultTypeQueryable<OUT>, CheckpointedFunction {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tstatic {\n+\t\tArrowUtils.checkArrowUsable();\n+\t}\n+\n+\t/**\n+\t * The type of the records produced by this source.\n+\t */\n+\tfinal DataType dataType;\n+\n+\t/**\n+\t * The array of byte array of the source data. Each element is an array\n+\t * representing an arrow batch.\n+\t */\n+\tprivate final byte[][] arrowData;\n+\n+\t/**\n+\t * Allocator which is used for byte buffer allocation.\n+\t */\n+\tprivate transient BufferAllocator allocator;\n+\n+\t/**\n+\t * Container that holds a set of vectors for the source data to emit.\n+\t */\n+\tprivate transient VectorSchemaRoot root;\n+\n+\tprivate transient volatile boolean running;\n+\n+\t/**\n+\t * The indexes of the collection of source data to emit. Each element is a tuple of\n+\t * the index of the arrow batch and the staring index inside the arrow batch.\n+\t */\n+\tprivate transient Deque<Tuple2<Integer, Integer>> indexesToEmit;\n+\n+\t/**\n+\t * The indexes of the source data which have not been emitted.\n+\t */\n+\tprivate transient ListState<Tuple2<Integer, Integer>> checkpointedState;\n+\n+\tAbstractArrowSourceFunction(DataType dataType, byte[][] arrowData) {\n+\t\tthis.dataType = Preconditions.checkNotNull(dataType);\n+\t\tthis.arrowData = Preconditions.checkNotNull(arrowData);\n+\t}\n+\n+\t@Override\n+\tpublic void open(Configuration parameters) throws Exception {\n+\t\tallocator = ArrowUtils.getRootAllocator().newChildAllocator(\"ArrowSourceFunction\", 0, Long.MAX_VALUE);\n+\t\troot = VectorSchemaRoot.create(ArrowUtils.toArrowSchema((RowType) dataType.getLogicalType()), allocator);\n+\t\trunning = true;\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws Exception {\n+\t\ttry {\n+\t\t\tsuper.close();\n+\t\t} finally {\n+\t\t\tif (root != null) {\n+\t\t\t\troot.close();\n+\t\t\t\troot = null;\n+\t\t\t}\n+\t\t\tif (allocator != null) {\n+\t\t\t\tallocator.close();\n+\t\t\t\tallocator = null;\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void initializeState(FunctionInitializationContext context) throws Exception {\n+\t\tPreconditions.checkState(this.checkpointedState == null,\n+\t\t\t\"The \" + getClass().getSimpleName() + \" has already been initialized.\");\n+\n+\t\tthis.checkpointedState = context.getOperatorStateStore().getListState(\n+\t\t\tnew ListStateDescriptor<>(\n+\t\t\t\t\"arrow-source-state\",\n+\t\t\t\tnew TupleSerializer<>(\n+\t\t\t\t\t(Class<Tuple2<Integer, Integer>>) (Class<?>) Tuple2.class,\n+\t\t\t\t\tnew TypeSerializer[]{IntSerializer.INSTANCE, IntSerializer.INSTANCE})\n+\t\t\t)\n+\t\t);\n+\n+\t\tthis.indexesToEmit = new ArrayDeque<>();\n+\t\tif (context.isRestored()) {\n+\t\t\t// upon restoring\n+\t\t\tfor (Tuple2<Integer, Integer> v : this.checkpointedState.get()) {\n+\t\t\t\tthis.indexesToEmit.add(v);\n+\t\t\t}\n+\t\t} else {\n+\t\t\t// the first time the job is executed\n+\t\t\tfinal int stepSize = getRuntimeContext().getNumberOfParallelSubtasks();\n+\t\t\tfinal int taskIdx = getRuntimeContext().getIndexOfThisSubtask();\n+\n+\t\t\tfor (int i = taskIdx; i < arrowData.length; i += stepSize) {\n+\t\t\t\tthis.indexesToEmit.add(Tuple2.of(i, 0));\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void snapshotState(FunctionSnapshotContext context) throws Exception {\n+\t\tPreconditions.checkState(this.checkpointedState != null,\n+\t\t\t\"The \" + getClass().getSimpleName() + \" state has not been properly initialized.\");\n+\n+\t\tthis.checkpointedState.clear();\n+\t\tfor (Tuple2<Integer, Integer> v : indexesToEmit) {\n+\t\t\tthis.checkpointedState.add(v);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void run(SourceContext<OUT> ctx) throws Exception {\n+\t\tVectorLoader vectorLoader = new VectorLoader(root);\n+\t\twhile (running && !indexesToEmit.isEmpty()) {\n+\t\t\tTuple2<Integer, Integer> indexToEmit = indexesToEmit.peek();\n+\t\t\tArrowRecordBatch arrowRecordBatch = loadBatch(indexToEmit.f0);\n+\t\t\tvectorLoader.load(arrowRecordBatch);\n+\t\t\tarrowRecordBatch.close();\n+\n+\t\t\tArrowReader<OUT> arrowReader = createArrowReader(root);\n+\t\t\tint rowCount = root.getRowCount();\n+\t\t\tint nextRowId = indexToEmit.f1;\n+\t\t\twhile (nextRowId < rowCount) {\n+\t\t\t\tOUT element = arrowReader.read(nextRowId);\n+\t\t\t\tsynchronized (ctx.getCheckpointLock()) {\n+\t\t\t\t\tctx.collect(element);\n+\t\t\t\t\tindexToEmit.setField(++nextRowId, 1);\n+\t\t\t\t}\n+\t\t\t}\n+\n+\t\t\tsynchronized (ctx.getCheckpointLock()) {\n+\t\t\t\tindexesToEmit.pop();\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void cancel() {\n+\t\trunning = false;\n+\t}\n+\n+\tpublic abstract ArrowReader<OUT> createArrowReader(VectorSchemaRoot root);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f427a2215c9bb6ecb5458e8693887cafffbef491"}, "originalPosition": 205}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4NDgzNjAwOnYy", "diffSide": "RIGHT", "path": "flink-python/src/main/java/org/apache/flink/table/runtime/arrow/sources/AbstractArrowSourceFunction.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QwNjozMDowN1rOGMS1vQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QwNjozMDowN1rOGMS1vQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTU0NDc2NQ==", "bodyText": "Maybe add some log in this method? For example, LOG.info the restored information.", "url": "https://github.com/apache/flink/pull/11832#discussion_r415544765", "createdAt": "2020-04-27T06:30:07Z", "author": {"login": "hequn8128"}, "path": "flink-python/src/main/java/org/apache/flink/table/runtime/arrow/sources/AbstractArrowSourceFunction.java", "diffHunk": "@@ -0,0 +1,214 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.arrow.sources;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.common.typeutils.base.IntSerializer;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.api.java.typeutils.ResultTypeQueryable;\n+import org.apache.flink.api.java.typeutils.runtime.TupleSerializer;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.runtime.state.FunctionInitializationContext;\n+import org.apache.flink.runtime.state.FunctionSnapshotContext;\n+import org.apache.flink.streaming.api.checkpoint.CheckpointedFunction;\n+import org.apache.flink.streaming.api.functions.source.RichParallelSourceFunction;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.table.runtime.arrow.ArrowReader;\n+import org.apache.flink.table.runtime.arrow.ArrowUtils;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.arrow.memory.BufferAllocator;\n+import org.apache.arrow.vector.VectorLoader;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ReadChannel;\n+import org.apache.arrow.vector.ipc.message.ArrowRecordBatch;\n+import org.apache.arrow.vector.ipc.message.MessageSerializer;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.nio.channels.Channels;\n+import java.util.ArrayDeque;\n+import java.util.Deque;\n+\n+/**\n+ * An Arrow {@link SourceFunction} which takes the serialized arrow record batch data as input.\n+ *\n+ * @param <OUT> The type of the records produced by this source.\n+ */\n+@Internal\n+public abstract class AbstractArrowSourceFunction<OUT>\n+\t\textends RichParallelSourceFunction<OUT>\n+\t\timplements ResultTypeQueryable<OUT>, CheckpointedFunction {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tstatic {\n+\t\tArrowUtils.checkArrowUsable();\n+\t}\n+\n+\t/**\n+\t * The type of the records produced by this source.\n+\t */\n+\tfinal DataType dataType;\n+\n+\t/**\n+\t * The array of byte array of the source data. Each element is an array\n+\t * representing an arrow batch.\n+\t */\n+\tprivate final byte[][] arrowData;\n+\n+\t/**\n+\t * Allocator which is used for byte buffer allocation.\n+\t */\n+\tprivate transient BufferAllocator allocator;\n+\n+\t/**\n+\t * Container that holds a set of vectors for the source data to emit.\n+\t */\n+\tprivate transient VectorSchemaRoot root;\n+\n+\tprivate transient volatile boolean running;\n+\n+\t/**\n+\t * The indexes of the collection of source data to emit. Each element is a tuple of\n+\t * the index of the arrow batch and the staring index inside the arrow batch.\n+\t */\n+\tprivate transient Deque<Tuple2<Integer, Integer>> indexesToEmit;\n+\n+\t/**\n+\t * The indexes of the source data which have not been emitted.\n+\t */\n+\tprivate transient ListState<Tuple2<Integer, Integer>> checkpointedState;\n+\n+\tAbstractArrowSourceFunction(DataType dataType, byte[][] arrowData) {\n+\t\tthis.dataType = Preconditions.checkNotNull(dataType);\n+\t\tthis.arrowData = Preconditions.checkNotNull(arrowData);\n+\t}\n+\n+\t@Override\n+\tpublic void open(Configuration parameters) throws Exception {\n+\t\tallocator = ArrowUtils.getRootAllocator().newChildAllocator(\"ArrowSourceFunction\", 0, Long.MAX_VALUE);\n+\t\troot = VectorSchemaRoot.create(ArrowUtils.toArrowSchema((RowType) dataType.getLogicalType()), allocator);\n+\t\trunning = true;\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws Exception {\n+\t\ttry {\n+\t\t\tsuper.close();\n+\t\t} finally {\n+\t\t\tif (root != null) {\n+\t\t\t\troot.close();\n+\t\t\t\troot = null;\n+\t\t\t}\n+\t\t\tif (allocator != null) {\n+\t\t\t\tallocator.close();\n+\t\t\t\tallocator = null;\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void initializeState(FunctionInitializationContext context) throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f427a2215c9bb6ecb5458e8693887cafffbef491"}, "originalPosition": 133}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4NDk3ODI0OnYy", "diffSide": "RIGHT", "path": "flink-python/src/test/java/org/apache/flink/table/runtime/arrow/sources/ArrowSourceFunctionTestBase.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QwNzoxMDo0OVrOGMUEeQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QwNzoxMDo0OVrOGMUEeQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTU2NDkyMQ==", "bodyText": "Also verify the content of the data?", "url": "https://github.com/apache/flink/pull/11832#discussion_r415564921", "createdAt": "2020-04-27T07:10:49Z", "author": {"login": "hequn8128"}, "path": "flink-python/src/test/java/org/apache/flink/table/runtime/arrow/sources/ArrowSourceFunctionTestBase.java", "diffHunk": "@@ -0,0 +1,279 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.arrow.sources;\n+\n+import org.apache.flink.api.common.functions.RuntimeContext;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.testutils.MultiShotLatch;\n+import org.apache.flink.core.testutils.OneShotLatch;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.operators.StreamSource;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.util.AbstractStreamOperatorTestHarness;\n+import org.apache.flink.table.api.DataTypes;\n+import org.apache.flink.table.runtime.arrow.ArrowUtils;\n+import org.apache.flink.table.runtime.arrow.ArrowWriter;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+\n+import org.apache.flink.shaded.guava18.com.google.common.collect.Lists;\n+\n+import org.apache.arrow.memory.BufferAllocator;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowStreamWriter;\n+import org.junit.Assert;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+import org.mockito.Mockito;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.nio.channels.Channels;\n+import java.util.List;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+/**\n+ * Abstract test base for the Arrow source function processing.\n+ */\n+public abstract class ArrowSourceFunctionTestBase<T> {\n+\n+\tstatic DataType dataType;\n+\tprivate static BufferAllocator allocator;\n+\n+\t@BeforeClass\n+\tpublic static void init() {\n+\t\tdataType = DataTypes.ROW(DataTypes.FIELD(\"f0\", DataTypes.STRING()));\n+\t\tallocator = ArrowUtils.getRootAllocator().newChildAllocator(\"stdout\", 0, Long.MAX_VALUE);\n+\t}\n+\n+\t@Test\n+\tpublic void testRestore() throws Exception {\n+\t\tTuple2<List<T>, Integer> testData = getTestData();\n+\t\tfinal AbstractArrowSourceFunction<T> arrowSourceFunction =\n+\t\t\tcreateTestArrowSourceFunction(testData.f0, testData.f1);\n+\n+\t\tfinal AbstractStreamOperatorTestHarness<T> testHarness =\n+\t\t\tnew AbstractStreamOperatorTestHarness<>(new StreamSource<>(arrowSourceFunction), 1, 1, 0);\n+\t\ttestHarness.open();\n+\n+\t\tfinal Throwable[] error = new Throwable[1];\n+\t\tfinal MultiShotLatch latch = new MultiShotLatch();\n+\t\tfinal AtomicInteger numOfEmittedElements = new AtomicInteger(0);\n+\n+\t\tfinal DummySourceContext<T> sourceContext = new DummySourceContext<T>() {\n+\t\t\t@Override\n+\t\t\tpublic void collect(T element) {\n+\t\t\t\tif (numOfEmittedElements.get() == 2) {\n+\t\t\t\t\tlatch.trigger();\n+\t\t\t\t\t// fail the source function at the the second element\n+\t\t\t\t\tthrow new RuntimeException(\"Fail the arrow source\");\n+\t\t\t\t}\n+\t\t\t\tnumOfEmittedElements.incrementAndGet();\n+\t\t\t}\n+\t\t};\n+\n+\t\t// run the source asynchronously\n+\t\tThread runner = new Thread(() -> {\n+\t\t\ttry {\n+\t\t\t\tarrowSourceFunction.run(sourceContext);\n+\t\t\t} catch (Throwable t) {\n+\t\t\t\tif (!t.getMessage().equals(\"Fail the arrow source\")) {\n+\t\t\t\t\terror[0] = t;\n+\t\t\t\t}\n+\t\t\t}\n+\t\t});\n+\t\trunner.start();\n+\n+\t\tif (!latch.isTriggered()) {\n+\t\t\tlatch.await();\n+\t\t}\n+\n+\t\tOperatorSubtaskState snapshot;\n+\t\tsynchronized (sourceContext.getCheckpointLock()) {\n+\t\t\tsnapshot = testHarness.snapshot(0, 0);\n+\t\t}\n+\n+\t\trunner.join();\n+\t\ttestHarness.close();\n+\n+\t\tfinal AbstractArrowSourceFunction<T> arrowSourceFunction2 =\n+\t\t\tcreateTestArrowSourceFunction(testData.f0, testData.f1);\n+\t\tAbstractStreamOperatorTestHarness<T> testHarnessCopy =\n+\t\t\tnew AbstractStreamOperatorTestHarness<>(new StreamSource<>(arrowSourceFunction2), 1, 1, 0);\n+\t\ttestHarnessCopy.initializeState(snapshot);\n+\t\ttestHarnessCopy.open();\n+\n+\t\t// run the source asynchronously\n+\t\tThread runner2 = new Thread(() -> {\n+\t\t\ttry {\n+\t\t\t\tarrowSourceFunction2.run(new DummySourceContext<T>() {\n+\t\t\t\t\t@Override\n+\t\t\t\t\tpublic void collect(T element) {\n+\t\t\t\t\t\tif (numOfEmittedElements.incrementAndGet() == testData.f0.size()) {\n+\t\t\t\t\t\t\tlatch.trigger();\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n+\t\t\t\t});\n+\t\t\t} catch (Throwable t) {\n+\t\t\t\terror[0] = t;\n+\t\t\t}\n+\t\t});\n+\t\trunner2.start();\n+\n+\t\tif (!latch.isTriggered()) {\n+\t\t\tlatch.await();\n+\t\t}\n+\t\trunner2.join();\n+\n+\t\tAssert.assertNull(error[0]);\n+\t\tAssert.assertEquals(testData.f0.size(), numOfEmittedElements.get());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f427a2215c9bb6ecb5458e8693887cafffbef491"}, "originalPosition": 147}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4NDk3ODU1OnYy", "diffSide": "RIGHT", "path": "flink-python/src/test/java/org/apache/flink/table/runtime/arrow/sources/ArrowSourceFunctionTestBase.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QwNzoxMDo1NVrOGMUEpA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QwNzoxMDo1NVrOGMUEpA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTU2NDk2NA==", "bodyText": "Also verify the content of the data?", "url": "https://github.com/apache/flink/pull/11832#discussion_r415564964", "createdAt": "2020-04-27T07:10:55Z", "author": {"login": "hequn8128"}, "path": "flink-python/src/test/java/org/apache/flink/table/runtime/arrow/sources/ArrowSourceFunctionTestBase.java", "diffHunk": "@@ -0,0 +1,279 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.arrow.sources;\n+\n+import org.apache.flink.api.common.functions.RuntimeContext;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.testutils.MultiShotLatch;\n+import org.apache.flink.core.testutils.OneShotLatch;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.operators.StreamSource;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.util.AbstractStreamOperatorTestHarness;\n+import org.apache.flink.table.api.DataTypes;\n+import org.apache.flink.table.runtime.arrow.ArrowUtils;\n+import org.apache.flink.table.runtime.arrow.ArrowWriter;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+\n+import org.apache.flink.shaded.guava18.com.google.common.collect.Lists;\n+\n+import org.apache.arrow.memory.BufferAllocator;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowStreamWriter;\n+import org.junit.Assert;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+import org.mockito.Mockito;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.nio.channels.Channels;\n+import java.util.List;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+/**\n+ * Abstract test base for the Arrow source function processing.\n+ */\n+public abstract class ArrowSourceFunctionTestBase<T> {\n+\n+\tstatic DataType dataType;\n+\tprivate static BufferAllocator allocator;\n+\n+\t@BeforeClass\n+\tpublic static void init() {\n+\t\tdataType = DataTypes.ROW(DataTypes.FIELD(\"f0\", DataTypes.STRING()));\n+\t\tallocator = ArrowUtils.getRootAllocator().newChildAllocator(\"stdout\", 0, Long.MAX_VALUE);\n+\t}\n+\n+\t@Test\n+\tpublic void testRestore() throws Exception {\n+\t\tTuple2<List<T>, Integer> testData = getTestData();\n+\t\tfinal AbstractArrowSourceFunction<T> arrowSourceFunction =\n+\t\t\tcreateTestArrowSourceFunction(testData.f0, testData.f1);\n+\n+\t\tfinal AbstractStreamOperatorTestHarness<T> testHarness =\n+\t\t\tnew AbstractStreamOperatorTestHarness<>(new StreamSource<>(arrowSourceFunction), 1, 1, 0);\n+\t\ttestHarness.open();\n+\n+\t\tfinal Throwable[] error = new Throwable[1];\n+\t\tfinal MultiShotLatch latch = new MultiShotLatch();\n+\t\tfinal AtomicInteger numOfEmittedElements = new AtomicInteger(0);\n+\n+\t\tfinal DummySourceContext<T> sourceContext = new DummySourceContext<T>() {\n+\t\t\t@Override\n+\t\t\tpublic void collect(T element) {\n+\t\t\t\tif (numOfEmittedElements.get() == 2) {\n+\t\t\t\t\tlatch.trigger();\n+\t\t\t\t\t// fail the source function at the the second element\n+\t\t\t\t\tthrow new RuntimeException(\"Fail the arrow source\");\n+\t\t\t\t}\n+\t\t\t\tnumOfEmittedElements.incrementAndGet();\n+\t\t\t}\n+\t\t};\n+\n+\t\t// run the source asynchronously\n+\t\tThread runner = new Thread(() -> {\n+\t\t\ttry {\n+\t\t\t\tarrowSourceFunction.run(sourceContext);\n+\t\t\t} catch (Throwable t) {\n+\t\t\t\tif (!t.getMessage().equals(\"Fail the arrow source\")) {\n+\t\t\t\t\terror[0] = t;\n+\t\t\t\t}\n+\t\t\t}\n+\t\t});\n+\t\trunner.start();\n+\n+\t\tif (!latch.isTriggered()) {\n+\t\t\tlatch.await();\n+\t\t}\n+\n+\t\tOperatorSubtaskState snapshot;\n+\t\tsynchronized (sourceContext.getCheckpointLock()) {\n+\t\t\tsnapshot = testHarness.snapshot(0, 0);\n+\t\t}\n+\n+\t\trunner.join();\n+\t\ttestHarness.close();\n+\n+\t\tfinal AbstractArrowSourceFunction<T> arrowSourceFunction2 =\n+\t\t\tcreateTestArrowSourceFunction(testData.f0, testData.f1);\n+\t\tAbstractStreamOperatorTestHarness<T> testHarnessCopy =\n+\t\t\tnew AbstractStreamOperatorTestHarness<>(new StreamSource<>(arrowSourceFunction2), 1, 1, 0);\n+\t\ttestHarnessCopy.initializeState(snapshot);\n+\t\ttestHarnessCopy.open();\n+\n+\t\t// run the source asynchronously\n+\t\tThread runner2 = new Thread(() -> {\n+\t\t\ttry {\n+\t\t\t\tarrowSourceFunction2.run(new DummySourceContext<T>() {\n+\t\t\t\t\t@Override\n+\t\t\t\t\tpublic void collect(T element) {\n+\t\t\t\t\t\tif (numOfEmittedElements.incrementAndGet() == testData.f0.size()) {\n+\t\t\t\t\t\t\tlatch.trigger();\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n+\t\t\t\t});\n+\t\t\t} catch (Throwable t) {\n+\t\t\t\terror[0] = t;\n+\t\t\t}\n+\t\t});\n+\t\trunner2.start();\n+\n+\t\tif (!latch.isTriggered()) {\n+\t\t\tlatch.await();\n+\t\t}\n+\t\trunner2.join();\n+\n+\t\tAssert.assertNull(error[0]);\n+\t\tAssert.assertEquals(testData.f0.size(), numOfEmittedElements.get());\n+\t}\n+\n+\t@Test\n+\tpublic void testParallelProcessing() throws Exception {\n+\t\tTuple2<List<T>, Integer> testData = getTestData();\n+\t\tfinal AbstractArrowSourceFunction<T> arrowSourceFunction =\n+\t\t\tcreateTestArrowSourceFunction(testData.f0, testData.f1);\n+\n+\t\tfinal AbstractStreamOperatorTestHarness<T> testHarness =\n+\t\t\tnew AbstractStreamOperatorTestHarness<>(new StreamSource<>(arrowSourceFunction), 2, 2, 0);\n+\t\ttestHarness.open();\n+\n+\t\tfinal Throwable[] error = new Throwable[2];\n+\t\tfinal OneShotLatch latch = new OneShotLatch();\n+\t\tfinal AtomicInteger numOfEmittedElements = new AtomicInteger(0);\n+\n+\t\t// run the source asynchronously\n+\t\tThread runner = new Thread(() -> {\n+\t\t\ttry {\n+\t\t\t\tarrowSourceFunction.run(new DummySourceContext<T>() {\n+\t\t\t\t\t@Override\n+\t\t\t\t\tpublic void collect(T element) {\n+\t\t\t\t\t\tif (numOfEmittedElements.incrementAndGet() == testData.f0.size()) {\n+\t\t\t\t\t\t\tlatch.trigger();\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n+\t\t\t\t});\n+\t\t\t} catch (Throwable t) {\n+\t\t\t\terror[0] = t;\n+\t\t\t}\n+\t\t});\n+\t\trunner.start();\n+\n+\t\tfinal AbstractArrowSourceFunction<T> arrowSourceFunction2 =\n+\t\t\tcreateTestArrowSourceFunction(testData.f0, testData.f1);\n+\t\tfinal AbstractStreamOperatorTestHarness<T> testHarness2 =\n+\t\t\tnew AbstractStreamOperatorTestHarness<>(new StreamSource<>(arrowSourceFunction2), 2, 2, 1);\n+\t\ttestHarness2.open();\n+\n+\t\t// run the source asynchronously\n+\t\tThread runner2 = new Thread(() -> {\n+\t\t\ttry {\n+\t\t\t\tarrowSourceFunction2.run(new DummySourceContext<T>() {\n+\t\t\t\t\t@Override\n+\t\t\t\t\tpublic void collect(T element) {\n+\t\t\t\t\t\tif (numOfEmittedElements.incrementAndGet() == testData.f0.size()) {\n+\t\t\t\t\t\t\tlatch.trigger();\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n+\t\t\t\t});\n+\t\t\t} catch (Throwable t) {\n+\t\t\t\terror[1] = t;\n+\t\t\t}\n+\t\t});\n+\t\trunner2.start();\n+\n+\t\tif (!latch.isTriggered()) {\n+\t\t\tlatch.await();\n+\t\t}\n+\n+\t\trunner.join();\n+\t\trunner2.join();\n+\t\ttestHarness.close();\n+\t\ttestHarness2.close();\n+\n+\t\tAssert.assertNull(error[0]);\n+\t\tAssert.assertNull(error[1]);\n+\t\tAssert.assertEquals(testData.f0.size(), numOfEmittedElements.get());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f427a2215c9bb6ecb5458e8693887cafffbef491"}, "originalPosition": 215}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4NTAxNTM0OnYy", "diffSide": "RIGHT", "path": "flink-python/src/test/java/org/apache/flink/table/runtime/arrow/sources/ArrowSourceFunctionTestBase.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QwNzoyMDowM1rOGMUZPQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQwNzoxMzoyMlrOGNyY9g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTU3MDIzNw==", "bodyText": "Add the corresponding assert to verify that error[0] is not null?", "url": "https://github.com/apache/flink/pull/11832#discussion_r415570237", "createdAt": "2020-04-27T07:20:03Z", "author": {"login": "hequn8128"}, "path": "flink-python/src/test/java/org/apache/flink/table/runtime/arrow/sources/ArrowSourceFunctionTestBase.java", "diffHunk": "@@ -0,0 +1,279 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.arrow.sources;\n+\n+import org.apache.flink.api.common.functions.RuntimeContext;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.testutils.MultiShotLatch;\n+import org.apache.flink.core.testutils.OneShotLatch;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.operators.StreamSource;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.util.AbstractStreamOperatorTestHarness;\n+import org.apache.flink.table.api.DataTypes;\n+import org.apache.flink.table.runtime.arrow.ArrowUtils;\n+import org.apache.flink.table.runtime.arrow.ArrowWriter;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+\n+import org.apache.flink.shaded.guava18.com.google.common.collect.Lists;\n+\n+import org.apache.arrow.memory.BufferAllocator;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowStreamWriter;\n+import org.junit.Assert;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+import org.mockito.Mockito;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.nio.channels.Channels;\n+import java.util.List;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+/**\n+ * Abstract test base for the Arrow source function processing.\n+ */\n+public abstract class ArrowSourceFunctionTestBase<T> {\n+\n+\tstatic DataType dataType;\n+\tprivate static BufferAllocator allocator;\n+\n+\t@BeforeClass\n+\tpublic static void init() {\n+\t\tdataType = DataTypes.ROW(DataTypes.FIELD(\"f0\", DataTypes.STRING()));\n+\t\tallocator = ArrowUtils.getRootAllocator().newChildAllocator(\"stdout\", 0, Long.MAX_VALUE);\n+\t}\n+\n+\t@Test\n+\tpublic void testRestore() throws Exception {\n+\t\tTuple2<List<T>, Integer> testData = getTestData();\n+\t\tfinal AbstractArrowSourceFunction<T> arrowSourceFunction =\n+\t\t\tcreateTestArrowSourceFunction(testData.f0, testData.f1);\n+\n+\t\tfinal AbstractStreamOperatorTestHarness<T> testHarness =\n+\t\t\tnew AbstractStreamOperatorTestHarness<>(new StreamSource<>(arrowSourceFunction), 1, 1, 0);\n+\t\ttestHarness.open();\n+\n+\t\tfinal Throwable[] error = new Throwable[1];\n+\t\tfinal MultiShotLatch latch = new MultiShotLatch();\n+\t\tfinal AtomicInteger numOfEmittedElements = new AtomicInteger(0);\n+\n+\t\tfinal DummySourceContext<T> sourceContext = new DummySourceContext<T>() {\n+\t\t\t@Override\n+\t\t\tpublic void collect(T element) {\n+\t\t\t\tif (numOfEmittedElements.get() == 2) {\n+\t\t\t\t\tlatch.trigger();\n+\t\t\t\t\t// fail the source function at the the second element\n+\t\t\t\t\tthrow new RuntimeException(\"Fail the arrow source\");\n+\t\t\t\t}\n+\t\t\t\tnumOfEmittedElements.incrementAndGet();\n+\t\t\t}\n+\t\t};\n+\n+\t\t// run the source asynchronously\n+\t\tThread runner = new Thread(() -> {\n+\t\t\ttry {\n+\t\t\t\tarrowSourceFunction.run(sourceContext);\n+\t\t\t} catch (Throwable t) {\n+\t\t\t\tif (!t.getMessage().equals(\"Fail the arrow source\")) {\n+\t\t\t\t\terror[0] = t;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f427a2215c9bb6ecb5458e8693887cafffbef491"}, "originalPosition": 99}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzA3NzUwMw==", "bodyText": "error[0] should always be null and it has been asserted at the end of the test. I'm not sure what do you mean about verify that error[0] is not null?", "url": "https://github.com/apache/flink/pull/11832#discussion_r417077503", "createdAt": "2020-04-29T05:39:51Z", "author": {"login": "dianfu"}, "path": "flink-python/src/test/java/org/apache/flink/table/runtime/arrow/sources/ArrowSourceFunctionTestBase.java", "diffHunk": "@@ -0,0 +1,279 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.arrow.sources;\n+\n+import org.apache.flink.api.common.functions.RuntimeContext;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.testutils.MultiShotLatch;\n+import org.apache.flink.core.testutils.OneShotLatch;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.operators.StreamSource;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.util.AbstractStreamOperatorTestHarness;\n+import org.apache.flink.table.api.DataTypes;\n+import org.apache.flink.table.runtime.arrow.ArrowUtils;\n+import org.apache.flink.table.runtime.arrow.ArrowWriter;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+\n+import org.apache.flink.shaded.guava18.com.google.common.collect.Lists;\n+\n+import org.apache.arrow.memory.BufferAllocator;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowStreamWriter;\n+import org.junit.Assert;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+import org.mockito.Mockito;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.nio.channels.Channels;\n+import java.util.List;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+/**\n+ * Abstract test base for the Arrow source function processing.\n+ */\n+public abstract class ArrowSourceFunctionTestBase<T> {\n+\n+\tstatic DataType dataType;\n+\tprivate static BufferAllocator allocator;\n+\n+\t@BeforeClass\n+\tpublic static void init() {\n+\t\tdataType = DataTypes.ROW(DataTypes.FIELD(\"f0\", DataTypes.STRING()));\n+\t\tallocator = ArrowUtils.getRootAllocator().newChildAllocator(\"stdout\", 0, Long.MAX_VALUE);\n+\t}\n+\n+\t@Test\n+\tpublic void testRestore() throws Exception {\n+\t\tTuple2<List<T>, Integer> testData = getTestData();\n+\t\tfinal AbstractArrowSourceFunction<T> arrowSourceFunction =\n+\t\t\tcreateTestArrowSourceFunction(testData.f0, testData.f1);\n+\n+\t\tfinal AbstractStreamOperatorTestHarness<T> testHarness =\n+\t\t\tnew AbstractStreamOperatorTestHarness<>(new StreamSource<>(arrowSourceFunction), 1, 1, 0);\n+\t\ttestHarness.open();\n+\n+\t\tfinal Throwable[] error = new Throwable[1];\n+\t\tfinal MultiShotLatch latch = new MultiShotLatch();\n+\t\tfinal AtomicInteger numOfEmittedElements = new AtomicInteger(0);\n+\n+\t\tfinal DummySourceContext<T> sourceContext = new DummySourceContext<T>() {\n+\t\t\t@Override\n+\t\t\tpublic void collect(T element) {\n+\t\t\t\tif (numOfEmittedElements.get() == 2) {\n+\t\t\t\t\tlatch.trigger();\n+\t\t\t\t\t// fail the source function at the the second element\n+\t\t\t\t\tthrow new RuntimeException(\"Fail the arrow source\");\n+\t\t\t\t}\n+\t\t\t\tnumOfEmittedElements.incrementAndGet();\n+\t\t\t}\n+\t\t};\n+\n+\t\t// run the source asynchronously\n+\t\tThread runner = new Thread(() -> {\n+\t\t\ttry {\n+\t\t\t\tarrowSourceFunction.run(sourceContext);\n+\t\t\t} catch (Throwable t) {\n+\t\t\t\tif (!t.getMessage().equals(\"Fail the arrow source\")) {\n+\t\t\t\t\terror[0] = t;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTU3MDIzNw=="}, "originalCommit": {"oid": "f427a2215c9bb6ecb5458e8693887cafffbef491"}, "originalPosition": 99}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzExMDI2Mg==", "bodyText": "Ignore my comment here. You are right.", "url": "https://github.com/apache/flink/pull/11832#discussion_r417110262", "createdAt": "2020-04-29T07:13:22Z", "author": {"login": "hequn8128"}, "path": "flink-python/src/test/java/org/apache/flink/table/runtime/arrow/sources/ArrowSourceFunctionTestBase.java", "diffHunk": "@@ -0,0 +1,279 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.arrow.sources;\n+\n+import org.apache.flink.api.common.functions.RuntimeContext;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.testutils.MultiShotLatch;\n+import org.apache.flink.core.testutils.OneShotLatch;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.operators.StreamSource;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.util.AbstractStreamOperatorTestHarness;\n+import org.apache.flink.table.api.DataTypes;\n+import org.apache.flink.table.runtime.arrow.ArrowUtils;\n+import org.apache.flink.table.runtime.arrow.ArrowWriter;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+\n+import org.apache.flink.shaded.guava18.com.google.common.collect.Lists;\n+\n+import org.apache.arrow.memory.BufferAllocator;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowStreamWriter;\n+import org.junit.Assert;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+import org.mockito.Mockito;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.nio.channels.Channels;\n+import java.util.List;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+/**\n+ * Abstract test base for the Arrow source function processing.\n+ */\n+public abstract class ArrowSourceFunctionTestBase<T> {\n+\n+\tstatic DataType dataType;\n+\tprivate static BufferAllocator allocator;\n+\n+\t@BeforeClass\n+\tpublic static void init() {\n+\t\tdataType = DataTypes.ROW(DataTypes.FIELD(\"f0\", DataTypes.STRING()));\n+\t\tallocator = ArrowUtils.getRootAllocator().newChildAllocator(\"stdout\", 0, Long.MAX_VALUE);\n+\t}\n+\n+\t@Test\n+\tpublic void testRestore() throws Exception {\n+\t\tTuple2<List<T>, Integer> testData = getTestData();\n+\t\tfinal AbstractArrowSourceFunction<T> arrowSourceFunction =\n+\t\t\tcreateTestArrowSourceFunction(testData.f0, testData.f1);\n+\n+\t\tfinal AbstractStreamOperatorTestHarness<T> testHarness =\n+\t\t\tnew AbstractStreamOperatorTestHarness<>(new StreamSource<>(arrowSourceFunction), 1, 1, 0);\n+\t\ttestHarness.open();\n+\n+\t\tfinal Throwable[] error = new Throwable[1];\n+\t\tfinal MultiShotLatch latch = new MultiShotLatch();\n+\t\tfinal AtomicInteger numOfEmittedElements = new AtomicInteger(0);\n+\n+\t\tfinal DummySourceContext<T> sourceContext = new DummySourceContext<T>() {\n+\t\t\t@Override\n+\t\t\tpublic void collect(T element) {\n+\t\t\t\tif (numOfEmittedElements.get() == 2) {\n+\t\t\t\t\tlatch.trigger();\n+\t\t\t\t\t// fail the source function at the the second element\n+\t\t\t\t\tthrow new RuntimeException(\"Fail the arrow source\");\n+\t\t\t\t}\n+\t\t\t\tnumOfEmittedElements.incrementAndGet();\n+\t\t\t}\n+\t\t};\n+\n+\t\t// run the source asynchronously\n+\t\tThread runner = new Thread(() -> {\n+\t\t\ttry {\n+\t\t\t\tarrowSourceFunction.run(sourceContext);\n+\t\t\t} catch (Throwable t) {\n+\t\t\t\tif (!t.getMessage().equals(\"Fail the arrow source\")) {\n+\t\t\t\t\terror[0] = t;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTU3MDIzNw=="}, "originalCommit": {"oid": "f427a2215c9bb6ecb5458e8693887cafffbef491"}, "originalPosition": 99}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU5NTQyNDY2OnYy", "diffSide": "RIGHT", "path": "flink-python/pyflink/table/table_environment.py", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQwODowNDoyOFrOGNz9QQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQwODowNDoyOFrOGNz9QQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzEzNTkzNw==", "bodyText": "Move this comment after the creation of DataFrame.", "url": "https://github.com/apache/flink/pull/11832#discussion_r417135937", "createdAt": "2020-04-29T08:04:28Z", "author": {"login": "hequn8128"}, "path": "flink-python/pyflink/table/table_environment.py", "diffHunk": "@@ -1119,6 +1119,90 @@ def _from_elements(self, elements, schema):\n         finally:\n             os.unlink(temp_file.name)\n \n+    def from_pandas(self, pdf,\n+                    schema: Union[RowType, List[str], Tuple[str], List[DataType],\n+                                  Tuple[DataType]] = None,\n+                    splits_num: int = 1) -> Table:\n+        \"\"\"\n+        Creates a table from a pandas DataFrame.\n+\n+        Example:\n+        ::\n+\n+            # use the second parameter to specify custom field names", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a30c4d84d218a3ab91a232c648f0a586df75eaa9"}, "originalPosition": 52}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU5NTQzMzU5OnYy", "diffSide": "RIGHT", "path": "flink-python/pyflink/table/table_environment.py", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQwODowNzowNlrOGN0Cnw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQwODowNzowNlrOGN0Cnw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzEzNzMxMQ==", "bodyText": "duplicate type hint.", "url": "https://github.com/apache/flink/pull/11832#discussion_r417137311", "createdAt": "2020-04-29T08:07:06Z", "author": {"login": "hequn8128"}, "path": "flink-python/pyflink/table/table_environment.py", "diffHunk": "@@ -1119,6 +1119,90 @@ def _from_elements(self, elements, schema):\n         finally:\n             os.unlink(temp_file.name)\n \n+    def from_pandas(self, pdf,\n+                    schema: Union[RowType, List[str], Tuple[str], List[DataType],\n+                                  Tuple[DataType]] = None,\n+                    splits_num: int = 1) -> Table:\n+        \"\"\"\n+        Creates a table from a pandas DataFrame.\n+\n+        Example:\n+        ::\n+\n+            # use the second parameter to specify custom field names\n+            >>> pdf = pd.DataFrame(np.random.rand(1000, 2))\n+            >>> table_env.from_pandas(pdf, [\"a\", \"b\"])\n+            # use the second parameter to specify custom field types\n+            >>> table_env.from_pandas(pdf, [DataTypes.DOUBLE(), DataTypes.DOUBLE()]))\n+            # use the second parameter to specify custom table schema\n+            >>> table_env.from_pandas(pdf,\n+            ...                       DataTypes.ROW([DataTypes.FIELD(\"a\", DataTypes.DOUBLE()),\n+            ...                                      DataTypes.FIELD(\"b\", DataTypes.DOUBLE())]))\n+\n+        :param pdf: The pandas DataFrame.\n+        :param schema: The schema of the converted table.\n+        :type schema: RowType or list[str] or list[DataType]", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a30c4d84d218a3ab91a232c648f0a586df75eaa9"}, "originalPosition": 64}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU5NTQzNDE4OnYy", "diffSide": "RIGHT", "path": "flink-python/pyflink/table/table_environment.py", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQwODowNzoxNFrOGN0C9w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQwODowNzoxNFrOGN0C9w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzEzNzM5OQ==", "bodyText": "duplicate type hint.", "url": "https://github.com/apache/flink/pull/11832#discussion_r417137399", "createdAt": "2020-04-29T08:07:14Z", "author": {"login": "hequn8128"}, "path": "flink-python/pyflink/table/table_environment.py", "diffHunk": "@@ -1119,6 +1119,90 @@ def _from_elements(self, elements, schema):\n         finally:\n             os.unlink(temp_file.name)\n \n+    def from_pandas(self, pdf,\n+                    schema: Union[RowType, List[str], Tuple[str], List[DataType],\n+                                  Tuple[DataType]] = None,\n+                    splits_num: int = 1) -> Table:\n+        \"\"\"\n+        Creates a table from a pandas DataFrame.\n+\n+        Example:\n+        ::\n+\n+            # use the second parameter to specify custom field names\n+            >>> pdf = pd.DataFrame(np.random.rand(1000, 2))\n+            >>> table_env.from_pandas(pdf, [\"a\", \"b\"])\n+            # use the second parameter to specify custom field types\n+            >>> table_env.from_pandas(pdf, [DataTypes.DOUBLE(), DataTypes.DOUBLE()]))\n+            # use the second parameter to specify custom table schema\n+            >>> table_env.from_pandas(pdf,\n+            ...                       DataTypes.ROW([DataTypes.FIELD(\"a\", DataTypes.DOUBLE()),\n+            ...                                      DataTypes.FIELD(\"b\", DataTypes.DOUBLE())]))\n+\n+        :param pdf: The pandas DataFrame.\n+        :param schema: The schema of the converted table.\n+        :type schema: RowType or list[str] or list[DataType]\n+        :param splits_num: The number of splits the given Pandas DataFrame will be split into. It\n+                           determines the number of parallel source tasks.\n+                           If not specified, the default parallelism will be used.\n+        :type splits_num: int\n+        :return: The result table.\n+        :rtype: Table", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a30c4d84d218a3ab91a232c648f0a586df75eaa9"}, "originalPosition": 70}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU5NTQzNDkyOnYy", "diffSide": "RIGHT", "path": "flink-python/pyflink/table/table_environment.py", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQwODowNzoyOVrOGN0DeQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQwODowNzoyOVrOGN0DeQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzEzNzUyOQ==", "bodyText": "duplicate type hint.", "url": "https://github.com/apache/flink/pull/11832#discussion_r417137529", "createdAt": "2020-04-29T08:07:29Z", "author": {"login": "hequn8128"}, "path": "flink-python/pyflink/table/table_environment.py", "diffHunk": "@@ -1119,6 +1119,90 @@ def _from_elements(self, elements, schema):\n         finally:\n             os.unlink(temp_file.name)\n \n+    def from_pandas(self, pdf,\n+                    schema: Union[RowType, List[str], Tuple[str], List[DataType],\n+                                  Tuple[DataType]] = None,\n+                    splits_num: int = 1) -> Table:\n+        \"\"\"\n+        Creates a table from a pandas DataFrame.\n+\n+        Example:\n+        ::\n+\n+            # use the second parameter to specify custom field names\n+            >>> pdf = pd.DataFrame(np.random.rand(1000, 2))\n+            >>> table_env.from_pandas(pdf, [\"a\", \"b\"])\n+            # use the second parameter to specify custom field types\n+            >>> table_env.from_pandas(pdf, [DataTypes.DOUBLE(), DataTypes.DOUBLE()]))\n+            # use the second parameter to specify custom table schema\n+            >>> table_env.from_pandas(pdf,\n+            ...                       DataTypes.ROW([DataTypes.FIELD(\"a\", DataTypes.DOUBLE()),\n+            ...                                      DataTypes.FIELD(\"b\", DataTypes.DOUBLE())]))\n+\n+        :param pdf: The pandas DataFrame.\n+        :param schema: The schema of the converted table.\n+        :type schema: RowType or list[str] or list[DataType]\n+        :param splits_num: The number of splits the given Pandas DataFrame will be split into. It\n+                           determines the number of parallel source tasks.\n+                           If not specified, the default parallelism will be used.\n+        :type splits_num: int", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a30c4d84d218a3ab91a232c648f0a586df75eaa9"}, "originalPosition": 68}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU5NTQ0NzQwOnYy", "diffSide": "RIGHT", "path": "docs/dev/table/python/index.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQwODoxMTowN1rOGN0LEA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQwODoxMTowN1rOGN0LEA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzEzOTQ3Mg==", "bodyText": "Conversions?", "url": "https://github.com/apache/flink/pull/11832#discussion_r417139472", "createdAt": "2020-04-29T08:11:07Z", "author": {"login": "hequn8128"}, "path": "docs/dev/table/python/index.md", "diffHunk": "@@ -32,5 +32,6 @@ Apache Flink has provided Python Table API support since 1.9.0.\n - [Installation]({{ site.baseurl }}/dev/table/python/installation.html): Introduction of how to set up the Python Table API execution environment.\n - [User-defined Functions]({{ site.baseurl }}/dev/table/python/python_udfs.html): Explanation of how to define Python user-defined functions.\n - [Vectorized User-defined Functions]({{ site.baseurl }}/dev/table/python/vectorized_python_udfs.html): Explanation of how to define vectorized Python user-defined functions.\n+- [Conversion between PyFlink Table and Pandas DataFrame]({{ site.baseurl }}/dev/table/python/conversion_of_pandas.html): Explanation of how to convert between PyFlink Table and Pandas DataFrame.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a30c4d84d218a3ab91a232c648f0a586df75eaa9"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU5NTkwOTQxOnYy", "diffSide": "RIGHT", "path": "docs/dev/table/python/index.zh.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQxMDoyNDozMVrOGN4pcQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQxMDoyNDozMVrOGN4pcQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzIxMjc4NQ==", "bodyText": "According to most copywriting guidelines, it's better to leave a blank between an English word and a Chinese word.", "url": "https://github.com/apache/flink/pull/11832#discussion_r417212785", "createdAt": "2020-04-29T10:24:31Z", "author": {"login": "hequn8128"}, "path": "docs/dev/table/python/index.zh.md", "diffHunk": "@@ -32,5 +32,6 @@ Apache Flink has provided Python Table API support since 1.9.0.\n - [\u73af\u5883\u5b89\u88c5]({{ site.baseurl }}/zh/dev/table/python/installation.html): Introduction of how to set up the Python Table API execution environment.\n - [\u81ea\u5b9a\u4e49\u51fd\u6570]({{ site.baseurl }}/zh/dev/table/python/python_udfs.html): Explanation of how to define Python user-defined functions.\n - [\u81ea\u5b9a\u4e49\u5411\u91cf\u5316\u51fd\u6570]({{ site.baseurl }}/zh/dev/table/python/vectorized_python_udfs.html): Explanation of how to define vectorized Python user-defined functions.\n+- [PyFlink Table\u548cPandas DataFrame\u4e92\u8f6c]({{ site.baseurl }}/zh/dev/table/python/conversion_of_pandas.html): Explanation of how to convert between PyFlink Table and Pandas DataFrame.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a30c4d84d218a3ab91a232c648f0a586df75eaa9"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU5NjEwNTU3OnYy", "diffSide": "RIGHT", "path": "docs/dev/table/python/conversion_of_pandas.zh.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQxMToyNjo1NFrOGN6iuA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQxMToyNjo1NFrOGN6iuA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzI0MzgzMg==", "bodyText": "Maybe add more examples here. For example, how to specify table names, which is commonly required.", "url": "https://github.com/apache/flink/pull/11832#discussion_r417243832", "createdAt": "2020-04-29T11:26:54Z", "author": {"login": "hequn8128"}, "path": "docs/dev/table/python/conversion_of_pandas.zh.md", "diffHunk": "@@ -0,0 +1,48 @@\n+---\n+title: \"PyFlink Table\u548cPandas DataFrame\u4e92\u8f6c\"\n+nav-parent_id: python_tableapi\n+nav-pos: 50\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+It supports to convert between PyFlink Table and Pandas DataFrame.\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+## Convert Pandas DataFrame to PyFlink Table\n+\n+It supports to create a PyFlink Table from a Pandas DataFrame. Internally, it will serialize the Pandas DataFrame\n+using Arrow columnar format at client side and the serialized data will be processed and deserialized in Arrow source\n+during execution. The Arrow source could also be used in streaming jobs and it will properly handle the checkpoint\n+and provides the exactly once guarantees.\n+\n+The following example shows how to create a PyFlink Table from a Pandas DataFrame:\n+\n+{% highlight python %}\n+import pandas as pd\n+import numpy as np\n+\n+# Create a Pandas DataFrame\n+pdf = pd.DataFrame(np.random.rand(1000, 2))\n+\n+# Create a PyFlink Table from a Pandas DataFrame\n+table = t_env.from_pandas(pdf)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a30c4d84d218a3ab91a232c648f0a586df75eaa9"}, "originalPosition": 47}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1684, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}