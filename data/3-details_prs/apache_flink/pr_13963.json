{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTE2NjQ5MTYw", "number": 13963, "title": "[FLINK-19888][hive] Migrate Hive source to FLIP-27 source interface for streaming", "bodyText": "What is the purpose of the change\nMigrate hive streaming read to FLIP-27 source.\nBrief change log\n\nMake file source more extensible by changing method/field access modifiers of AbstractFileSource, ContinuousEnumerationSettings and PendingSplitsCheckpoint.\nImplement ContinuousHivePendingSplitsCheckpoint as sub-class of PendingSplitsCheckpoint. It keeps the max current partition read timestamp, and the processed partitions for this timestamp.\nImplement ContinuousHivePendingSplitsCheckpointSerializer as the SerDe for ContinuousHivePendingSplitsCheckpoint.\nImplement ContinuousHiveSplitEnumerator as the SplitEnumerator for reading partitioned table in streaming fashion.\nChange HiveSource to create/restore ContinuousHiveSplitEnumerator to do streaming read on partitioned tables.\nChange HiveTableSource to use HiveSource for streaming read.\n\nVerifying this change\nExisting streaming read tests\nDoes this pull request potentially affect one of the following parts:\n\nDependencies (does it add or upgrade a dependency): no\nThe public API, i.e., is any changed class annotated with @Public(Evolving): yes\nThe serializers: yes\nThe runtime per-record code paths (performance sensitive): no\nAnything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: no\nThe S3 file system connector: no\n\nDocumentation\n\nDoes this pull request introduce a new feature? no\nIf yes, how is the feature documented? NA", "createdAt": "2020-11-06T10:23:18Z", "url": "https://github.com/apache/flink/pull/13963", "merged": true, "mergeCommit": {"oid": "fc581da73fcb9bbaa443604d36498c9a5dbdf0cd"}, "closed": true, "closedAt": "2020-11-08T13:13:38Z", "author": {"login": "lirui-apache"}, "timelineItems": {"totalCount": 19, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdaIaYFgBqjM5Njk3NTY1NDM=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdaaxtxAFqTUyNTc2NjIyNQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "9d3fc1f4ae609c7ecf60505058490fc527ddaa71", "author": {"user": {"login": "lirui-apache", "name": "Rui Li"}}, "url": "https://github.com/apache/flink/commit/9d3fc1f4ae609c7ecf60505058490fc527ddaa71", "committedDate": "2020-11-06T10:13:18Z", "message": "[FLINK-19888][hive] Migrate Hive source to FLIP-27 source interface for streaming"}, "afterCommit": {"oid": "cfd0be434e9ef7b045dc0b418d68b1ad0665d186", "author": {"user": {"login": "lirui-apache", "name": "Rui Li"}}, "url": "https://github.com/apache/flink/commit/cfd0be434e9ef7b045dc0b418d68b1ad0665d186", "committedDate": "2020-11-07T09:45:47Z", "message": "[FLINK-19888][hive] Migrate Hive source to FLIP-27 source interface for streaming"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI1NjkwNzc1", "url": "https://github.com/apache/flink/pull/13963#pullrequestreview-525690775", "createdAt": "2020-11-07T23:12:52Z", "commit": {"oid": "55caff0edf1e89c58d896468e8b4c8a814726aac"}, "state": "APPROVED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wN1QyMzoxMjo1M1rOHvLX6A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wN1QyMzoxMzowMFrOHvLX8A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTIzMTQ2NA==", "bodyText": "I have a slight preference to keep these fields private and add a public getter (public FileSplitAssigner.Provider getAssigner().", "url": "https://github.com/apache/flink/pull/13963#discussion_r519231464", "createdAt": "2020-11-07T23:12:53Z", "author": {"login": "StephanEwen"}, "path": "flink-connectors/flink-connector-files/src/main/java/org/apache/flink/connector/file/src/AbstractFileSource.java", "diffHunk": "@@ -75,12 +75,12 @@\n \n \tprivate final FileEnumerator.Provider enumeratorFactory;\n \n-\tprivate final FileSplitAssigner.Provider assignerFactory;\n+\tprotected final FileSplitAssigner.Provider assignerFactory;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "55caff0edf1e89c58d896468e8b4c8a814726aac"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTIzMTQ3Mg==", "bodyText": "Similar as above.", "url": "https://github.com/apache/flink/pull/13963#discussion_r519231472", "createdAt": "2020-11-07T23:13:00Z", "author": {"login": "StephanEwen"}, "path": "flink-connectors/flink-connector-files/src/main/java/org/apache/flink/connector/file/src/AbstractFileSource.java", "diffHunk": "@@ -75,12 +75,12 @@\n \n \tprivate final FileEnumerator.Provider enumeratorFactory;\n \n-\tprivate final FileSplitAssigner.Provider assignerFactory;\n+\tprotected final FileSplitAssigner.Provider assignerFactory;\n \n \tprivate final BulkFormat<T, SplitT> readerFormat;\n \n \t@Nullable\n-\tprivate final ContinuousEnumerationSettings continuousEnumerationSettings;\n+\tprotected final ContinuousEnumerationSettings continuousEnumerationSettings;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "55caff0edf1e89c58d896468e8b4c8a814726aac"}, "originalPosition": 11}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI1Njk1OTQ2", "url": "https://github.com/apache/flink/pull/13963#pullrequestreview-525695946", "createdAt": "2020-11-08T00:53:26Z", "commit": {"oid": "cfd0be434e9ef7b045dc0b418d68b1ad0665d186"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOFQwMDo1MzoyNlrOHvL6YQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOFQwMDo1MzoyNlrOHvL6YQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTI0MDI4OQ==", "bodyText": "Code style:\npublic ContinuousHivePendingSplitsCheckpoint(\n         Collection<HiveSourceSplit> splits,\n         Comparable<?> currentReadOffset,\n         Collection<List<String>> seenPartitions) {", "url": "https://github.com/apache/flink/pull/13963#discussion_r519240289", "createdAt": "2020-11-08T00:53:26Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/ContinuousHivePendingSplitsCheckpoint.java", "diffHunk": "@@ -0,0 +1,51 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+import org.apache.flink.connector.file.src.PendingSplitsCheckpoint;\n+import org.apache.flink.connectors.hive.read.HiveSourceSplit;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.List;\n+\n+/**\n+ * The checkpoint of current state of continuous hive source reading.\n+ */\n+public class ContinuousHivePendingSplitsCheckpoint extends PendingSplitsCheckpoint<HiveSourceSplit> {\n+\n+\tprivate final Comparable<?> currentReadOffset;\n+\tprivate final Collection<List<String>> seenPartitions;\n+\n+\tpublic ContinuousHivePendingSplitsCheckpoint(Collection<HiveSourceSplit> splits,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cfd0be434e9ef7b045dc0b418d68b1ad0665d186"}, "originalPosition": 37}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI1Njk2MjIy", "url": "https://github.com/apache/flink/pull/13963#pullrequestreview-525696222", "createdAt": "2020-11-08T01:00:41Z", "commit": {"oid": "cfd0be434e9ef7b045dc0b418d68b1ad0665d186"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOFQwMTowMDo0MVrOHvL8Wg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOFQwMTowMDo0MVrOHvL8Wg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTI0MDc5NA==", "bodyText": "Why not return newSplits and pass to handleNewSplits and call assignSplits?", "url": "https://github.com/apache/flink/pull/13963#discussion_r519240794", "createdAt": "2020-11-08T01:00:41Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/ContinuousHiveSplitEnumerator.java", "diffHunk": "@@ -0,0 +1,230 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+import org.apache.flink.api.connector.source.SourceEvent;\n+import org.apache.flink.api.connector.source.SplitEnumerator;\n+import org.apache.flink.api.connector.source.SplitEnumeratorContext;\n+import org.apache.flink.api.connector.source.event.RequestSplitEvent;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.connector.file.src.FileSourceSplit;\n+import org.apache.flink.connector.file.src.PendingSplitsCheckpoint;\n+import org.apache.flink.connector.file.src.assigners.FileSplitAssigner;\n+import org.apache.flink.connectors.hive.read.HiveSourceSplit;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.filesystem.ContinuousPartitionFetcher;\n+\n+import org.apache.hadoop.hive.metastore.api.Partition;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.locks.ReadWriteLock;\n+import java.util.concurrent.locks.ReentrantReadWriteLock;\n+\n+/**\n+ * A continuously monitoring {@link SplitEnumerator} for hive source.\n+ */\n+public class ContinuousHiveSplitEnumerator<T extends Comparable<T>> implements SplitEnumerator<HiveSourceSplit, PendingSplitsCheckpoint<HiveSourceSplit>> {\n+\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(ContinuousHiveSplitEnumerator.class);\n+\n+\tprivate final SplitEnumeratorContext<HiveSourceSplit> enumeratorContext;\n+\tprivate final LinkedHashMap<Integer, String> readersAwaitingSplit;\n+\tprivate final FileSplitAssigner splitAssigner;\n+\tprivate final long discoveryInterval;\n+\n+\tprivate final JobConf jobConf;\n+\tprivate final ObjectPath tablePath;\n+\n+\tprivate final ContinuousPartitionFetcher<Partition, T> fetcher;\n+\tprivate final HiveTableSource.HiveContinuousPartitionFetcherContext<T> fetcherContext;\n+\n+\tprivate final ReadWriteLock stateLock = new ReentrantReadWriteLock();\n+\t// the maximum partition read offset seen so far\n+\tprivate volatile T currentReadOffset;\n+\t// the partitions that have been processed for a given read offset\n+\tprivate final Set<List<String>> seenPartitions;\n+\n+\tpublic ContinuousHiveSplitEnumerator(\n+\t\t\tSplitEnumeratorContext<HiveSourceSplit> enumeratorContext,\n+\t\t\tT currentReadOffset,\n+\t\t\tCollection<List<String>> seenPartitions,\n+\t\t\tFileSplitAssigner splitAssigner,\n+\t\t\tlong discoveryInterval,\n+\t\t\tJobConf jobConf,\n+\t\t\tObjectPath tablePath,\n+\t\t\tContinuousPartitionFetcher<Partition, T> fetcher,\n+\t\t\tHiveTableSource.HiveContinuousPartitionFetcherContext<T> fetcherContext) {\n+\t\tthis.enumeratorContext = enumeratorContext;\n+\t\tthis.currentReadOffset = currentReadOffset;\n+\t\tthis.seenPartitions = new HashSet<>(seenPartitions);\n+\t\tthis.splitAssigner = splitAssigner;\n+\t\tthis.discoveryInterval = discoveryInterval;\n+\t\tthis.jobConf = jobConf;\n+\t\tthis.tablePath = tablePath;\n+\t\tthis.fetcher = fetcher;\n+\t\tthis.fetcherContext = fetcherContext;\n+\t\treadersAwaitingSplit = new LinkedHashMap<>();\n+\t}\n+\n+\t@Override\n+\tpublic void start() {\n+\t\ttry {\n+\t\t\tfetcherContext.open();\n+\t\t\tenumeratorContext.callAsync(\n+\t\t\t\t\tthis::monitorAndGetSplits,\n+\t\t\t\t\tthis::handleNewSplits,\n+\t\t\t\t\tdiscoveryInterval,\n+\t\t\t\t\tdiscoveryInterval);\n+\t\t} catch (Exception e) {\n+\t\t\tthrow new FlinkHiveException(\"Failed to start continuous split enumerator\", e);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void handleSourceEvent(int subtaskId, SourceEvent sourceEvent) {\n+\t\tif (sourceEvent instanceof RequestSplitEvent) {\n+\t\t\treadersAwaitingSplit.put(subtaskId, ((RequestSplitEvent) sourceEvent).hostName());\n+\t\t\tassignSplits();\n+\t\t} else {\n+\t\t\tLOG.error(\"Received unrecognized event: {}\", sourceEvent);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void addSplitsBack(List<HiveSourceSplit> splits, int subtaskId) {\n+\t\tLOG.debug(\"Continuous Hive Source Enumerator adds splits back: {}\", splits);\n+\t\tstateLock.writeLock().lock();\n+\t\ttry {\n+\t\t\tsplitAssigner.addSplits(new ArrayList<>(splits));\n+\t\t} finally {\n+\t\t\tstateLock.writeLock().unlock();\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void addReader(int subtaskId) {\n+\t\t// this source is purely lazy-pull-based, nothing to do upon registration\n+\t}\n+\n+\t@Override\n+\tpublic PendingSplitsCheckpoint<HiveSourceSplit> snapshotState() throws Exception {\n+\t\tstateLock.readLock().lock();\n+\t\ttry {\n+\t\t\tCollection<HiveSourceSplit> remainingSplits = (Collection<HiveSourceSplit>) (Collection<?>) splitAssigner.remainingSplits();\n+\t\t\treturn new ContinuousHivePendingSplitsCheckpoint(remainingSplits, currentReadOffset, seenPartitions);\n+\t\t} finally {\n+\t\t\tstateLock.readLock().unlock();\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws IOException {\n+\t\ttry {\n+\t\t\tfetcherContext.close();\n+\t\t} catch (Exception e) {\n+\t\t\tthrow new IOException(e);\n+\t\t}\n+\t}\n+\n+\tprivate Void monitorAndGetSplits() throws Exception {\n+\t\tstateLock.writeLock().lock();\n+\t\ttry {\n+\t\t\tList<Tuple2<Partition, T>> partitions = fetcher.fetchPartitions(fetcherContext, currentReadOffset);\n+\t\t\tif (partitions.isEmpty()) {\n+\t\t\t\treturn null;\n+\t\t\t}\n+\n+\t\t\tpartitions.sort(Comparator.comparing(o -> o.f1));\n+\t\t\tList<HiveSourceSplit> newSplits = new ArrayList<>();\n+\t\t\t// the max offset of new partitions\n+\t\t\tT maxOffset = currentReadOffset;\n+\t\t\tSet<List<String>> nextSeen = new HashSet<>();\n+\t\t\tfor (Tuple2<Partition, T> tuple2 : partitions) {\n+\t\t\t\tPartition partition = tuple2.f0;\n+\t\t\t\tList<String> partSpec = partition.getValues();\n+\t\t\t\tif (seenPartitions.add(partSpec)) {\n+\t\t\t\t\tT offset = tuple2.f1;\n+\t\t\t\t\tif (offset.compareTo(currentReadOffset) > 0) {\n+\t\t\t\t\t\tnextSeen.add(partSpec);\n+\t\t\t\t\t}\n+\t\t\t\t\tif (offset.compareTo(maxOffset) > 0) {\n+\t\t\t\t\t\tmaxOffset = offset;\n+\t\t\t\t\t}\n+\t\t\t\t\tLOG.info(\"Found new partition {} of table {}, generating splits for it\",\n+\t\t\t\t\t\t\tpartSpec, tablePath.getFullName());\n+\t\t\t\t\tnewSplits.addAll(HiveSourceFileEnumerator.createInputSplits(\n+\t\t\t\t\t\t\t0, Collections.singletonList(fetcherContext.toHiveTablePartition(partition)), jobConf));\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tcurrentReadOffset = maxOffset;\n+\t\t\tsplitAssigner.addSplits(new ArrayList<>(newSplits));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cfd0be434e9ef7b045dc0b418d68b1ad0665d186"}, "originalPosition": 190}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI1Njk2MjYw", "url": "https://github.com/apache/flink/pull/13963#pullrequestreview-525696260", "createdAt": "2020-11-08T01:02:06Z", "commit": {"oid": "cfd0be434e9ef7b045dc0b418d68b1ad0665d186"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOFQwMTowMjowNlrOHvL8rw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOFQwMTowMjowNlrOHvL8rw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTI0MDg3OQ==", "bodyText": "Why not DEFAULT_SPLIT_ASSIGNER ?", "url": "https://github.com/apache/flink/pull/13963#discussion_r519240879", "createdAt": "2020-11-08T01:02:06Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveSource.java", "diffHunk": "@@ -46,29 +58,98 @@\n \n \tprivate static final long serialVersionUID = 1L;\n \n+\tprivate final JobConfWrapper jobConfWrapper;\n+\tprivate final List<String> partitionKeys;\n+\tprivate final ContinuousPartitionFetcher<Partition, ?> fetcher;\n+\tprivate final HiveTableSource.HiveContinuousPartitionFetcherContext<?> fetcherContext;\n+\tprivate final ObjectPath tablePath;\n+\n \tHiveSource(\n \t\t\tJobConf jobConf,\n+\t\t\tObjectPath tablePath,\n \t\t\tCatalogTable catalogTable,\n \t\t\tList<HiveTablePartition> partitions,\n \t\t\t@Nullable Long limit,\n \t\t\tString hiveVersion,\n \t\t\tboolean useMapRedReader,\n-\t\t\tboolean isStreamingSource,\n+\t\t\t@Nullable ContinuousEnumerationSettings continuousEnumerationSettings,\n+\t\t\tContinuousPartitionFetcher<Partition, ?> fetcher,\n+\t\t\tHiveTableSource.HiveContinuousPartitionFetcherContext<?> fetcherContext,\n \t\t\tRowType producedRowType) {\n \t\tsuper(\n \t\t\t\tnew org.apache.flink.core.fs.Path[1],\n \t\t\t\tnew HiveSourceFileEnumerator.Provider(partitions, new JobConfWrapper(jobConf)),\n-\t\t\t\tDEFAULT_SPLIT_ASSIGNER,\n+\t\t\t\tcontinuousEnumerationSettings == null ? DEFAULT_SPLIT_ASSIGNER : SimpleSplitAssigner::new,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cfd0be434e9ef7b045dc0b418d68b1ad0665d186"}, "originalPosition": 62}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI1Njk2MzI0", "url": "https://github.com/apache/flink/pull/13963#pullrequestreview-525696324", "createdAt": "2020-11-08T01:03:51Z", "commit": {"oid": "cfd0be434e9ef7b045dc0b418d68b1ad0665d186"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOFQwMTowMzo1MVrOHvL9Dw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOFQwMTowMzo1MVrOHvL9Dw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTI0MDk3NQ==", "bodyText": "continuousPartitionedEnumerator?", "url": "https://github.com/apache/flink/pull/13963#discussion_r519240975", "createdAt": "2020-11-08T01:03:51Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveSource.java", "diffHunk": "@@ -46,29 +58,98 @@\n \n \tprivate static final long serialVersionUID = 1L;\n \n+\tprivate final JobConfWrapper jobConfWrapper;\n+\tprivate final List<String> partitionKeys;\n+\tprivate final ContinuousPartitionFetcher<Partition, ?> fetcher;\n+\tprivate final HiveTableSource.HiveContinuousPartitionFetcherContext<?> fetcherContext;\n+\tprivate final ObjectPath tablePath;\n+\n \tHiveSource(\n \t\t\tJobConf jobConf,\n+\t\t\tObjectPath tablePath,\n \t\t\tCatalogTable catalogTable,\n \t\t\tList<HiveTablePartition> partitions,\n \t\t\t@Nullable Long limit,\n \t\t\tString hiveVersion,\n \t\t\tboolean useMapRedReader,\n-\t\t\tboolean isStreamingSource,\n+\t\t\t@Nullable ContinuousEnumerationSettings continuousEnumerationSettings,\n+\t\t\tContinuousPartitionFetcher<Partition, ?> fetcher,\n+\t\t\tHiveTableSource.HiveContinuousPartitionFetcherContext<?> fetcherContext,\n \t\t\tRowType producedRowType) {\n \t\tsuper(\n \t\t\t\tnew org.apache.flink.core.fs.Path[1],\n \t\t\t\tnew HiveSourceFileEnumerator.Provider(partitions, new JobConfWrapper(jobConf)),\n-\t\t\t\tDEFAULT_SPLIT_ASSIGNER,\n+\t\t\t\tcontinuousEnumerationSettings == null ? DEFAULT_SPLIT_ASSIGNER : SimpleSplitAssigner::new,\n \t\t\t\tcreateBulkFormat(new JobConf(jobConf), catalogTable, hiveVersion, producedRowType, useMapRedReader, limit),\n-\t\t\t\tnull);\n-\t\tPreconditions.checkArgument(!isStreamingSource, \"HiveSource currently only supports bounded mode\");\n+\t\t\t\tcontinuousEnumerationSettings);\n+\t\tthis.jobConfWrapper = new JobConfWrapper(jobConf);\n+\t\tthis.tablePath = tablePath;\n+\t\tthis.partitionKeys = catalogTable.getPartitionKeys();\n+\t\tthis.fetcher = fetcher;\n+\t\tthis.fetcherContext = fetcherContext;\n \t}\n \n \t@Override\n \tpublic SimpleVersionedSerializer<HiveSourceSplit> getSplitSerializer() {\n \t\treturn HiveSourceSplitSerializer.INSTANCE;\n \t}\n \n+\t@Override\n+\tpublic SimpleVersionedSerializer<PendingSplitsCheckpoint<HiveSourceSplit>> getEnumeratorCheckpointSerializer() {\n+\t\tif (needContinuousSplitEnumerator()) {\n+\t\t\treturn new ContinuousHivePendingSplitsCheckpointSerializer(getSplitSerializer());\n+\t\t} else {\n+\t\t\treturn super.getEnumeratorCheckpointSerializer();\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic SplitEnumerator<HiveSourceSplit, PendingSplitsCheckpoint<HiveSourceSplit>> createEnumerator(\n+\t\t\tSplitEnumeratorContext<HiveSourceSplit> enumContext) {\n+\t\tif (needContinuousSplitEnumerator()) {\n+\t\t\treturn createContinuousSplitEnumerator(\n+\t\t\t\t\tenumContext, fetcherContext.getConsumeStartOffset(), Collections.emptyList(), Collections.emptyList());\n+\t\t} else {\n+\t\t\treturn super.createEnumerator(enumContext);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic SplitEnumerator<HiveSourceSplit, PendingSplitsCheckpoint<HiveSourceSplit>> restoreEnumerator(\n+\t\t\tSplitEnumeratorContext<HiveSourceSplit> enumContext, PendingSplitsCheckpoint<HiveSourceSplit> checkpoint) {\n+\t\tif (needContinuousSplitEnumerator()) {\n+\t\t\tPreconditions.checkState(checkpoint instanceof ContinuousHivePendingSplitsCheckpoint,\n+\t\t\t\t\t\"Illegal type of splits checkpoint %s for streaming read partitioned table\", checkpoint.getClass().getName());\n+\t\t\tContinuousHivePendingSplitsCheckpoint hiveCheckpoint = (ContinuousHivePendingSplitsCheckpoint) checkpoint;\n+\t\t\treturn createContinuousSplitEnumerator(\n+\t\t\t\t\tenumContext, hiveCheckpoint.getCurrentReadOffset(), hiveCheckpoint.getSeenPartitions(), hiveCheckpoint.getSplits());\n+\t\t} else {\n+\t\t\treturn super.restoreEnumerator(enumContext, checkpoint);\n+\t\t}\n+\t}\n+\n+\tprivate boolean needContinuousSplitEnumerator() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cfd0be434e9ef7b045dc0b418d68b1ad0665d186"}, "originalPosition": 113}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI1Njk2MzM4", "url": "https://github.com/apache/flink/pull/13963#pullrequestreview-525696338", "createdAt": "2020-11-08T01:04:05Z", "commit": {"oid": "cfd0be434e9ef7b045dc0b418d68b1ad0665d186"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOFQwMTowNDowNVrOHvL9Kg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOFQwMTowNDowNVrOHvL9Kg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTI0MTAwMg==", "bodyText": "Ditto", "url": "https://github.com/apache/flink/pull/13963#discussion_r519241002", "createdAt": "2020-11-08T01:04:05Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveSource.java", "diffHunk": "@@ -46,29 +58,98 @@\n \n \tprivate static final long serialVersionUID = 1L;\n \n+\tprivate final JobConfWrapper jobConfWrapper;\n+\tprivate final List<String> partitionKeys;\n+\tprivate final ContinuousPartitionFetcher<Partition, ?> fetcher;\n+\tprivate final HiveTableSource.HiveContinuousPartitionFetcherContext<?> fetcherContext;\n+\tprivate final ObjectPath tablePath;\n+\n \tHiveSource(\n \t\t\tJobConf jobConf,\n+\t\t\tObjectPath tablePath,\n \t\t\tCatalogTable catalogTable,\n \t\t\tList<HiveTablePartition> partitions,\n \t\t\t@Nullable Long limit,\n \t\t\tString hiveVersion,\n \t\t\tboolean useMapRedReader,\n-\t\t\tboolean isStreamingSource,\n+\t\t\t@Nullable ContinuousEnumerationSettings continuousEnumerationSettings,\n+\t\t\tContinuousPartitionFetcher<Partition, ?> fetcher,\n+\t\t\tHiveTableSource.HiveContinuousPartitionFetcherContext<?> fetcherContext,\n \t\t\tRowType producedRowType) {\n \t\tsuper(\n \t\t\t\tnew org.apache.flink.core.fs.Path[1],\n \t\t\t\tnew HiveSourceFileEnumerator.Provider(partitions, new JobConfWrapper(jobConf)),\n-\t\t\t\tDEFAULT_SPLIT_ASSIGNER,\n+\t\t\t\tcontinuousEnumerationSettings == null ? DEFAULT_SPLIT_ASSIGNER : SimpleSplitAssigner::new,\n \t\t\t\tcreateBulkFormat(new JobConf(jobConf), catalogTable, hiveVersion, producedRowType, useMapRedReader, limit),\n-\t\t\t\tnull);\n-\t\tPreconditions.checkArgument(!isStreamingSource, \"HiveSource currently only supports bounded mode\");\n+\t\t\t\tcontinuousEnumerationSettings);\n+\t\tthis.jobConfWrapper = new JobConfWrapper(jobConf);\n+\t\tthis.tablePath = tablePath;\n+\t\tthis.partitionKeys = catalogTable.getPartitionKeys();\n+\t\tthis.fetcher = fetcher;\n+\t\tthis.fetcherContext = fetcherContext;\n \t}\n \n \t@Override\n \tpublic SimpleVersionedSerializer<HiveSourceSplit> getSplitSerializer() {\n \t\treturn HiveSourceSplitSerializer.INSTANCE;\n \t}\n \n+\t@Override\n+\tpublic SimpleVersionedSerializer<PendingSplitsCheckpoint<HiveSourceSplit>> getEnumeratorCheckpointSerializer() {\n+\t\tif (needContinuousSplitEnumerator()) {\n+\t\t\treturn new ContinuousHivePendingSplitsCheckpointSerializer(getSplitSerializer());\n+\t\t} else {\n+\t\t\treturn super.getEnumeratorCheckpointSerializer();\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic SplitEnumerator<HiveSourceSplit, PendingSplitsCheckpoint<HiveSourceSplit>> createEnumerator(\n+\t\t\tSplitEnumeratorContext<HiveSourceSplit> enumContext) {\n+\t\tif (needContinuousSplitEnumerator()) {\n+\t\t\treturn createContinuousSplitEnumerator(\n+\t\t\t\t\tenumContext, fetcherContext.getConsumeStartOffset(), Collections.emptyList(), Collections.emptyList());\n+\t\t} else {\n+\t\t\treturn super.createEnumerator(enumContext);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic SplitEnumerator<HiveSourceSplit, PendingSplitsCheckpoint<HiveSourceSplit>> restoreEnumerator(\n+\t\t\tSplitEnumeratorContext<HiveSourceSplit> enumContext, PendingSplitsCheckpoint<HiveSourceSplit> checkpoint) {\n+\t\tif (needContinuousSplitEnumerator()) {\n+\t\t\tPreconditions.checkState(checkpoint instanceof ContinuousHivePendingSplitsCheckpoint,\n+\t\t\t\t\t\"Illegal type of splits checkpoint %s for streaming read partitioned table\", checkpoint.getClass().getName());\n+\t\t\tContinuousHivePendingSplitsCheckpoint hiveCheckpoint = (ContinuousHivePendingSplitsCheckpoint) checkpoint;\n+\t\t\treturn createContinuousSplitEnumerator(\n+\t\t\t\t\tenumContext, hiveCheckpoint.getCurrentReadOffset(), hiveCheckpoint.getSeenPartitions(), hiveCheckpoint.getSplits());\n+\t\t} else {\n+\t\t\treturn super.restoreEnumerator(enumContext, checkpoint);\n+\t\t}\n+\t}\n+\n+\tprivate boolean needContinuousSplitEnumerator() {\n+\t\treturn getBoundedness() == Boundedness.CONTINUOUS_UNBOUNDED && !partitionKeys.isEmpty();\n+\t}\n+\n+\tprivate SplitEnumerator<HiveSourceSplit, PendingSplitsCheckpoint<HiveSourceSplit>> createContinuousSplitEnumerator(\n+\t\t\tSplitEnumeratorContext<HiveSourceSplit> enumContext,\n+\t\t\tComparable<?> currentReadOffset,\n+\t\t\tCollection<List<String>> seenPartitions,\n+\t\t\tCollection<HiveSourceSplit> splits) {\n+\t\treturn new ContinuousHiveSplitEnumerator(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cfd0be434e9ef7b045dc0b418d68b1ad0665d186"}, "originalPosition": 122}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI1Njk2NTA3", "url": "https://github.com/apache/flink/pull/13963#pullrequestreview-525696507", "createdAt": "2020-11-08T01:09:11Z", "commit": {"oid": "cfd0be434e9ef7b045dc0b418d68b1ad0665d186"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOFQwMTowOToxMVrOHvL-mQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOFQwMTowOToxMVrOHvL-mQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTI0MTM2OQ==", "bodyText": "It is better to create a builder for HiveSource instead of passing all arguments.", "url": "https://github.com/apache/flink/pull/13963#discussion_r519241369", "createdAt": "2020-11-08T01:09:11Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveSource.java", "diffHunk": "@@ -46,29 +58,98 @@\n \n \tprivate static final long serialVersionUID = 1L;\n \n+\tprivate final JobConfWrapper jobConfWrapper;\n+\tprivate final List<String> partitionKeys;\n+\tprivate final ContinuousPartitionFetcher<Partition, ?> fetcher;\n+\tprivate final HiveTableSource.HiveContinuousPartitionFetcherContext<?> fetcherContext;\n+\tprivate final ObjectPath tablePath;\n+\n \tHiveSource(\n \t\t\tJobConf jobConf,\n+\t\t\tObjectPath tablePath,\n \t\t\tCatalogTable catalogTable,\n \t\t\tList<HiveTablePartition> partitions,\n \t\t\t@Nullable Long limit,\n \t\t\tString hiveVersion,\n \t\t\tboolean useMapRedReader,\n-\t\t\tboolean isStreamingSource,\n+\t\t\t@Nullable ContinuousEnumerationSettings continuousEnumerationSettings,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cfd0be434e9ef7b045dc0b418d68b1ad0665d186"}, "originalPosition": 54}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI1Njk2NTU0", "url": "https://github.com/apache/flink/pull/13963#pullrequestreview-525696554", "createdAt": "2020-11-08T01:10:32Z", "commit": {"oid": "cfd0be434e9ef7b045dc0b418d68b1ad0665d186"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOFQwMToxMDozMlrOHvL--g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOFQwMToxMDozMlrOHvL--g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTI0MTQ2Ng==", "bodyText": "Why not move these check to upper?", "url": "https://github.com/apache/flink/pull/13963#discussion_r519241466", "createdAt": "2020-11-08T01:10:32Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSource.java", "diffHunk": "@@ -151,23 +139,63 @@ public boolean isBounded() {\n \t\t\t\tcatalogTable,\n \t\t\t\thiveShim,\n \t\t\t\tremainingPartitions);\n+\t\tConfiguration configuration = Configuration.fromMap(catalogTable.getOptions());\n \n-\t\t@SuppressWarnings(\"unchecked\")\n-\t\tTypeInformation<RowData> typeInfo =\n-\t\t\t\t(TypeInformation<RowData>) TypeInfoDataTypeConverter.fromDataTypeToTypeInfo(getProducedDataType());\n+\t\tDuration monitorInterval = null;\n+\t\tContinuousPartitionFetcher<Partition, ?> fetcher = null;\n+\t\tHiveContinuousPartitionFetcherContext<?> fetcherContext = null;\n+\t\tif (isStreamingSource()) {\n+\t\t\tmonitorInterval = configuration.get(STREAMING_SOURCE_MONITOR_INTERVAL) == null\n+\t\t\t\t\t? DEFAULT_SCAN_MONITOR_INTERVAL\n+\t\t\t\t\t: configuration.get(STREAMING_SOURCE_MONITOR_INTERVAL);\n+\t\t\tfetcher = new HiveContinuousPartitionFetcher();\n+\n+\t\t\tfinal String defaultPartitionName = jobConf.get(HiveConf.ConfVars.DEFAULTPARTITIONNAME.varname,\n+\t\t\t\t\tHiveConf.ConfVars.DEFAULTPARTITIONNAME.defaultStrVal);\n+\t\t\tfetcherContext = new HiveContinuousPartitionFetcherContext(\n+\t\t\t\t\ttablePath,\n+\t\t\t\t\thiveShim,\n+\t\t\t\t\tnew JobConfWrapper(jobConf),\n+\t\t\t\t\tcatalogTable.getPartitionKeys(),\n+\t\t\t\t\tgetProducedTableSchema().getFieldDataTypes(),\n+\t\t\t\t\tgetProducedTableSchema().getFieldNames(),\n+\t\t\t\t\tconfiguration,\n+\t\t\t\t\tdefaultPartitionName);\n+\t\t}\n \n-\t\tHiveTableInputFormat inputFormat = getInputFormat(\n+\t\tHiveSource hiveSource = new HiveSource(\n+\t\t\t\tjobConf,\n+\t\t\t\ttablePath,\n+\t\t\t\tcatalogTable,\n \t\t\t\tallHivePartitions,\n-\t\t\t\tflinkConf.get(HiveOptions.TABLE_EXEC_HIVE_FALLBACK_MAPRED_READER));\n+\t\t\t\tlimit,\n+\t\t\t\thiveVersion,\n+\t\t\t\tflinkConf.get(HiveOptions.TABLE_EXEC_HIVE_FALLBACK_MAPRED_READER),\n+\t\t\t\tisStreamingSource() ? new ContinuousEnumerationSettings(monitorInterval) : null,\n+\t\t\t\tfetcher,\n+\t\t\t\tfetcherContext,\n+\t\t\t\t(RowType) getProducedDataType().getLogicalType()\n+\t\t);\n+\t\tDataStreamSource<RowData> source = execEnv.fromSource(\n+\t\t\t\thiveSource, WatermarkStrategy.noWatermarks(), \"HiveSource-\" + tablePath.getFullName());\n \n \t\tif (isStreamingSource()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cfd0be434e9ef7b045dc0b418d68b1ad0665d186"}, "originalPosition": 100}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI1Njk2NjQz", "url": "https://github.com/apache/flink/pull/13963#pullrequestreview-525696643", "createdAt": "2020-11-08T01:12:28Z", "commit": {"oid": "cfd0be434e9ef7b045dc0b418d68b1ad0665d186"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOFQwMToxMjoyOFrOHvL_gw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOFQwMToxMjoyOFrOHvL_gw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTI0MTYwMw==", "bodyText": "Do we really need this lock? I think thread safety should be ensured by framework", "url": "https://github.com/apache/flink/pull/13963#discussion_r519241603", "createdAt": "2020-11-08T01:12:28Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/ContinuousHiveSplitEnumerator.java", "diffHunk": "@@ -0,0 +1,230 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+import org.apache.flink.api.connector.source.SourceEvent;\n+import org.apache.flink.api.connector.source.SplitEnumerator;\n+import org.apache.flink.api.connector.source.SplitEnumeratorContext;\n+import org.apache.flink.api.connector.source.event.RequestSplitEvent;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.connector.file.src.FileSourceSplit;\n+import org.apache.flink.connector.file.src.PendingSplitsCheckpoint;\n+import org.apache.flink.connector.file.src.assigners.FileSplitAssigner;\n+import org.apache.flink.connectors.hive.read.HiveSourceSplit;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.filesystem.ContinuousPartitionFetcher;\n+\n+import org.apache.hadoop.hive.metastore.api.Partition;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.locks.ReadWriteLock;\n+import java.util.concurrent.locks.ReentrantReadWriteLock;\n+\n+/**\n+ * A continuously monitoring {@link SplitEnumerator} for hive source.\n+ */\n+public class ContinuousHiveSplitEnumerator<T extends Comparable<T>> implements SplitEnumerator<HiveSourceSplit, PendingSplitsCheckpoint<HiveSourceSplit>> {\n+\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(ContinuousHiveSplitEnumerator.class);\n+\n+\tprivate final SplitEnumeratorContext<HiveSourceSplit> enumeratorContext;\n+\tprivate final LinkedHashMap<Integer, String> readersAwaitingSplit;\n+\tprivate final FileSplitAssigner splitAssigner;\n+\tprivate final long discoveryInterval;\n+\n+\tprivate final JobConf jobConf;\n+\tprivate final ObjectPath tablePath;\n+\n+\tprivate final ContinuousPartitionFetcher<Partition, T> fetcher;\n+\tprivate final HiveTableSource.HiveContinuousPartitionFetcherContext<T> fetcherContext;\n+\n+\tprivate final ReadWriteLock stateLock = new ReentrantReadWriteLock();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cfd0be434e9ef7b045dc0b418d68b1ad0665d186"}, "originalPosition": 71}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI1Njk2NzQw", "url": "https://github.com/apache/flink/pull/13963#pullrequestreview-525696740", "createdAt": "2020-11-08T01:15:33Z", "commit": {"oid": "cfd0be434e9ef7b045dc0b418d68b1ad0665d186"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOFQwMToxNTozM1rOHvMAYQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOFQwMToxNTozM1rOHvMAYQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTI0MTgyNQ==", "bodyText": "seenPartitions vs distinctPartitions\nI prefer the latter, because it is only used to remove duplication, and does not save all the partitions we have seen", "url": "https://github.com/apache/flink/pull/13963#discussion_r519241825", "createdAt": "2020-11-08T01:15:33Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/ContinuousHiveSplitEnumerator.java", "diffHunk": "@@ -0,0 +1,230 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+import org.apache.flink.api.connector.source.SourceEvent;\n+import org.apache.flink.api.connector.source.SplitEnumerator;\n+import org.apache.flink.api.connector.source.SplitEnumeratorContext;\n+import org.apache.flink.api.connector.source.event.RequestSplitEvent;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.connector.file.src.FileSourceSplit;\n+import org.apache.flink.connector.file.src.PendingSplitsCheckpoint;\n+import org.apache.flink.connector.file.src.assigners.FileSplitAssigner;\n+import org.apache.flink.connectors.hive.read.HiveSourceSplit;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.filesystem.ContinuousPartitionFetcher;\n+\n+import org.apache.hadoop.hive.metastore.api.Partition;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.locks.ReadWriteLock;\n+import java.util.concurrent.locks.ReentrantReadWriteLock;\n+\n+/**\n+ * A continuously monitoring {@link SplitEnumerator} for hive source.\n+ */\n+public class ContinuousHiveSplitEnumerator<T extends Comparable<T>> implements SplitEnumerator<HiveSourceSplit, PendingSplitsCheckpoint<HiveSourceSplit>> {\n+\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(ContinuousHiveSplitEnumerator.class);\n+\n+\tprivate final SplitEnumeratorContext<HiveSourceSplit> enumeratorContext;\n+\tprivate final LinkedHashMap<Integer, String> readersAwaitingSplit;\n+\tprivate final FileSplitAssigner splitAssigner;\n+\tprivate final long discoveryInterval;\n+\n+\tprivate final JobConf jobConf;\n+\tprivate final ObjectPath tablePath;\n+\n+\tprivate final ContinuousPartitionFetcher<Partition, T> fetcher;\n+\tprivate final HiveTableSource.HiveContinuousPartitionFetcherContext<T> fetcherContext;\n+\n+\tprivate final ReadWriteLock stateLock = new ReentrantReadWriteLock();\n+\t// the maximum partition read offset seen so far\n+\tprivate volatile T currentReadOffset;\n+\t// the partitions that have been processed for a given read offset\n+\tprivate final Set<List<String>> seenPartitions;\n+\n+\tpublic ContinuousHiveSplitEnumerator(\n+\t\t\tSplitEnumeratorContext<HiveSourceSplit> enumeratorContext,\n+\t\t\tT currentReadOffset,\n+\t\t\tCollection<List<String>> seenPartitions,\n+\t\t\tFileSplitAssigner splitAssigner,\n+\t\t\tlong discoveryInterval,\n+\t\t\tJobConf jobConf,\n+\t\t\tObjectPath tablePath,\n+\t\t\tContinuousPartitionFetcher<Partition, T> fetcher,\n+\t\t\tHiveTableSource.HiveContinuousPartitionFetcherContext<T> fetcherContext) {\n+\t\tthis.enumeratorContext = enumeratorContext;\n+\t\tthis.currentReadOffset = currentReadOffset;\n+\t\tthis.seenPartitions = new HashSet<>(seenPartitions);\n+\t\tthis.splitAssigner = splitAssigner;\n+\t\tthis.discoveryInterval = discoveryInterval;\n+\t\tthis.jobConf = jobConf;\n+\t\tthis.tablePath = tablePath;\n+\t\tthis.fetcher = fetcher;\n+\t\tthis.fetcherContext = fetcherContext;\n+\t\treadersAwaitingSplit = new LinkedHashMap<>();\n+\t}\n+\n+\t@Override\n+\tpublic void start() {\n+\t\ttry {\n+\t\t\tfetcherContext.open();\n+\t\t\tenumeratorContext.callAsync(\n+\t\t\t\t\tthis::monitorAndGetSplits,\n+\t\t\t\t\tthis::handleNewSplits,\n+\t\t\t\t\tdiscoveryInterval,\n+\t\t\t\t\tdiscoveryInterval);\n+\t\t} catch (Exception e) {\n+\t\t\tthrow new FlinkHiveException(\"Failed to start continuous split enumerator\", e);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void handleSourceEvent(int subtaskId, SourceEvent sourceEvent) {\n+\t\tif (sourceEvent instanceof RequestSplitEvent) {\n+\t\t\treadersAwaitingSplit.put(subtaskId, ((RequestSplitEvent) sourceEvent).hostName());\n+\t\t\tassignSplits();\n+\t\t} else {\n+\t\t\tLOG.error(\"Received unrecognized event: {}\", sourceEvent);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void addSplitsBack(List<HiveSourceSplit> splits, int subtaskId) {\n+\t\tLOG.debug(\"Continuous Hive Source Enumerator adds splits back: {}\", splits);\n+\t\tstateLock.writeLock().lock();\n+\t\ttry {\n+\t\t\tsplitAssigner.addSplits(new ArrayList<>(splits));\n+\t\t} finally {\n+\t\t\tstateLock.writeLock().unlock();\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void addReader(int subtaskId) {\n+\t\t// this source is purely lazy-pull-based, nothing to do upon registration\n+\t}\n+\n+\t@Override\n+\tpublic PendingSplitsCheckpoint<HiveSourceSplit> snapshotState() throws Exception {\n+\t\tstateLock.readLock().lock();\n+\t\ttry {\n+\t\t\tCollection<HiveSourceSplit> remainingSplits = (Collection<HiveSourceSplit>) (Collection<?>) splitAssigner.remainingSplits();\n+\t\t\treturn new ContinuousHivePendingSplitsCheckpoint(remainingSplits, currentReadOffset, seenPartitions);\n+\t\t} finally {\n+\t\t\tstateLock.readLock().unlock();\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws IOException {\n+\t\ttry {\n+\t\t\tfetcherContext.close();\n+\t\t} catch (Exception e) {\n+\t\t\tthrow new IOException(e);\n+\t\t}\n+\t}\n+\n+\tprivate Void monitorAndGetSplits() throws Exception {\n+\t\tstateLock.writeLock().lock();\n+\t\ttry {\n+\t\t\tList<Tuple2<Partition, T>> partitions = fetcher.fetchPartitions(fetcherContext, currentReadOffset);\n+\t\t\tif (partitions.isEmpty()) {\n+\t\t\t\treturn null;\n+\t\t\t}\n+\n+\t\t\tpartitions.sort(Comparator.comparing(o -> o.f1));\n+\t\t\tList<HiveSourceSplit> newSplits = new ArrayList<>();\n+\t\t\t// the max offset of new partitions\n+\t\t\tT maxOffset = currentReadOffset;\n+\t\t\tSet<List<String>> nextSeen = new HashSet<>();\n+\t\t\tfor (Tuple2<Partition, T> tuple2 : partitions) {\n+\t\t\t\tPartition partition = tuple2.f0;\n+\t\t\t\tList<String> partSpec = partition.getValues();\n+\t\t\t\tif (seenPartitions.add(partSpec)) {\n+\t\t\t\t\tT offset = tuple2.f1;\n+\t\t\t\t\tif (offset.compareTo(currentReadOffset) > 0) {\n+\t\t\t\t\t\tnextSeen.add(partSpec);\n+\t\t\t\t\t}\n+\t\t\t\t\tif (offset.compareTo(maxOffset) > 0) {\n+\t\t\t\t\t\tmaxOffset = offset;\n+\t\t\t\t\t}\n+\t\t\t\t\tLOG.info(\"Found new partition {} of table {}, generating splits for it\",\n+\t\t\t\t\t\t\tpartSpec, tablePath.getFullName());\n+\t\t\t\t\tnewSplits.addAll(HiveSourceFileEnumerator.createInputSplits(\n+\t\t\t\t\t\t\t0, Collections.singletonList(fetcherContext.toHiveTablePartition(partition)), jobConf));\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tcurrentReadOffset = maxOffset;\n+\t\t\tsplitAssigner.addSplits(new ArrayList<>(newSplits));\n+\t\t\tif (!nextSeen.isEmpty()) {\n+\t\t\t\tseenPartitions.clear();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cfd0be434e9ef7b045dc0b418d68b1ad0665d186"}, "originalPosition": 192}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI1Njk2ODE2", "url": "https://github.com/apache/flink/pull/13963#pullrequestreview-525696816", "createdAt": "2020-11-08T01:17:25Z", "commit": {"oid": "cfd0be434e9ef7b045dc0b418d68b1ad0665d186"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOFQwMToxNzoyNlrOHvMA6w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOFQwMToxNzoyNlrOHvMA6w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTI0MTk2Mw==", "bodyText": "It is better to pass serializer here", "url": "https://github.com/apache/flink/pull/13963#discussion_r519241963", "createdAt": "2020-11-08T01:17:26Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/ContinuousHivePendingSplitsCheckpointSerializer.java", "diffHunk": "@@ -0,0 +1,101 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+import org.apache.flink.connector.file.src.PendingSplitsCheckpoint;\n+import org.apache.flink.connector.file.src.PendingSplitsCheckpointSerializer;\n+import org.apache.flink.connectors.hive.read.HiveSourceSplit;\n+import org.apache.flink.core.io.SimpleVersionedSerializer;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.ObjectInputStream;\n+import java.io.ObjectOutputStream;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * SerDe for {@link ContinuousHivePendingSplitsCheckpoint}.\n+ */\n+public class ContinuousHivePendingSplitsCheckpointSerializer implements SimpleVersionedSerializer<PendingSplitsCheckpoint<HiveSourceSplit>> {\n+\n+\tprivate static final int VERSION = 1;\n+\n+\tprivate final PendingSplitsCheckpointSerializer<HiveSourceSplit> superSerDe;\n+\n+\tpublic ContinuousHivePendingSplitsCheckpointSerializer(SimpleVersionedSerializer<HiveSourceSplit> splitSerDe) {\n+\t\tsuperSerDe = new PendingSplitsCheckpointSerializer<>(splitSerDe);\n+\t}\n+\n+\t@Override\n+\tpublic int getVersion() {\n+\t\treturn VERSION;\n+\t}\n+\n+\t@Override\n+\tpublic byte[] serialize(PendingSplitsCheckpoint<HiveSourceSplit> checkpoint) throws IOException {\n+\t\tPreconditions.checkArgument(checkpoint.getClass() == ContinuousHivePendingSplitsCheckpoint.class,\n+\t\t\t\t\"Only supports %s\", ContinuousHivePendingSplitsCheckpoint.class.getName());\n+\n+\t\tContinuousHivePendingSplitsCheckpoint hiveCheckpoint = (ContinuousHivePendingSplitsCheckpoint) checkpoint;\n+\t\tPendingSplitsCheckpoint<HiveSourceSplit> superCP = PendingSplitsCheckpoint.fromCollectionSnapshot(\n+\t\t\t\thiveCheckpoint.getSplits(), hiveCheckpoint.getAlreadyProcessedPaths());\n+\t\tbyte[] superBytes = superSerDe.serialize(superCP);\n+\t\tByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream();\n+\t\ttry (ObjectOutputStream outputStream = new ObjectOutputStream(byteArrayOutputStream)) {\n+\t\t\toutputStream.writeInt(superBytes.length);\n+\t\t\toutputStream.write(superBytes);\n+\t\t\toutputStream.writeObject(hiveCheckpoint.getCurrentReadOffset());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cfd0be434e9ef7b045dc0b418d68b1ad0665d186"}, "originalPosition": 66}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI1Njk2ODI5", "url": "https://github.com/apache/flink/pull/13963#pullrequestreview-525696829", "createdAt": "2020-11-08T01:17:47Z", "commit": {"oid": "cfd0be434e9ef7b045dc0b418d68b1ad0665d186"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOFQwMToxNzo0N1rOHvMA_g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOFQwMToxNzo0N1rOHvMA_g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTI0MTk4Mg==", "bodyText": "It is better to use ListSer", "url": "https://github.com/apache/flink/pull/13963#discussion_r519241982", "createdAt": "2020-11-08T01:17:47Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/ContinuousHivePendingSplitsCheckpointSerializer.java", "diffHunk": "@@ -0,0 +1,101 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+import org.apache.flink.connector.file.src.PendingSplitsCheckpoint;\n+import org.apache.flink.connector.file.src.PendingSplitsCheckpointSerializer;\n+import org.apache.flink.connectors.hive.read.HiveSourceSplit;\n+import org.apache.flink.core.io.SimpleVersionedSerializer;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.ObjectInputStream;\n+import java.io.ObjectOutputStream;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * SerDe for {@link ContinuousHivePendingSplitsCheckpoint}.\n+ */\n+public class ContinuousHivePendingSplitsCheckpointSerializer implements SimpleVersionedSerializer<PendingSplitsCheckpoint<HiveSourceSplit>> {\n+\n+\tprivate static final int VERSION = 1;\n+\n+\tprivate final PendingSplitsCheckpointSerializer<HiveSourceSplit> superSerDe;\n+\n+\tpublic ContinuousHivePendingSplitsCheckpointSerializer(SimpleVersionedSerializer<HiveSourceSplit> splitSerDe) {\n+\t\tsuperSerDe = new PendingSplitsCheckpointSerializer<>(splitSerDe);\n+\t}\n+\n+\t@Override\n+\tpublic int getVersion() {\n+\t\treturn VERSION;\n+\t}\n+\n+\t@Override\n+\tpublic byte[] serialize(PendingSplitsCheckpoint<HiveSourceSplit> checkpoint) throws IOException {\n+\t\tPreconditions.checkArgument(checkpoint.getClass() == ContinuousHivePendingSplitsCheckpoint.class,\n+\t\t\t\t\"Only supports %s\", ContinuousHivePendingSplitsCheckpoint.class.getName());\n+\n+\t\tContinuousHivePendingSplitsCheckpoint hiveCheckpoint = (ContinuousHivePendingSplitsCheckpoint) checkpoint;\n+\t\tPendingSplitsCheckpoint<HiveSourceSplit> superCP = PendingSplitsCheckpoint.fromCollectionSnapshot(\n+\t\t\t\thiveCheckpoint.getSplits(), hiveCheckpoint.getAlreadyProcessedPaths());\n+\t\tbyte[] superBytes = superSerDe.serialize(superCP);\n+\t\tByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream();\n+\t\ttry (ObjectOutputStream outputStream = new ObjectOutputStream(byteArrayOutputStream)) {\n+\t\t\toutputStream.writeInt(superBytes.length);\n+\t\t\toutputStream.write(superBytes);\n+\t\t\toutputStream.writeObject(hiveCheckpoint.getCurrentReadOffset());\n+\t\t\toutputStream.writeInt(hiveCheckpoint.getSeenPartitions().size());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cfd0be434e9ef7b045dc0b418d68b1ad0665d186"}, "originalPosition": 67}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI1Njk2ODQz", "url": "https://github.com/apache/flink/pull/13963#pullrequestreview-525696843", "createdAt": "2020-11-08T01:18:25Z", "commit": {"oid": "cfd0be434e9ef7b045dc0b418d68b1ad0665d186"}, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a7187f1e884c1ba4fec3d3630a3f157d633d0c48", "author": {"user": {"login": "lirui-apache", "name": "Rui Li"}}, "url": "https://github.com/apache/flink/commit/a7187f1e884c1ba4fec3d3630a3f157d633d0c48", "committedDate": "2020-11-08T02:12:25Z", "message": "[FLINK-19888][connector-files] Make file source more extensible"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "cfd0be434e9ef7b045dc0b418d68b1ad0665d186", "author": {"user": {"login": "lirui-apache", "name": "Rui Li"}}, "url": "https://github.com/apache/flink/commit/cfd0be434e9ef7b045dc0b418d68b1ad0665d186", "committedDate": "2020-11-07T09:45:47Z", "message": "[FLINK-19888][hive] Migrate Hive source to FLIP-27 source interface for streaming"}, "afterCommit": {"oid": "873f52f02d69a32b1f78822fa6e0fe850833807f", "author": {"user": {"login": "lirui-apache", "name": "Rui Li"}}, "url": "https://github.com/apache/flink/commit/873f52f02d69a32b1f78822fa6e0fe850833807f", "committedDate": "2020-11-08T04:38:55Z", "message": "address comments for hive connector"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "292f7debbab2b02b317b2578421edb9eed5cdb48", "author": {"user": {"login": "lirui-apache", "name": "Rui Li"}}, "url": "https://github.com/apache/flink/commit/292f7debbab2b02b317b2578421edb9eed5cdb48", "committedDate": "2020-11-08T07:05:14Z", "message": "[FLINK-19888][hive] Migrate Hive source to FLIP-27 source interface for streaming"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "873f52f02d69a32b1f78822fa6e0fe850833807f", "author": {"user": {"login": "lirui-apache", "name": "Rui Li"}}, "url": "https://github.com/apache/flink/commit/873f52f02d69a32b1f78822fa6e0fe850833807f", "committedDate": "2020-11-08T04:38:55Z", "message": "address comments for hive connector"}, "afterCommit": {"oid": "292f7debbab2b02b317b2578421edb9eed5cdb48", "author": {"user": {"login": "lirui-apache", "name": "Rui Li"}}, "url": "https://github.com/apache/flink/commit/292f7debbab2b02b317b2578421edb9eed5cdb48", "committedDate": "2020-11-08T07:05:14Z", "message": "[FLINK-19888][hive] Migrate Hive source to FLIP-27 source interface for streaming"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI1NzY2MjI1", "url": "https://github.com/apache/flink/pull/13963#pullrequestreview-525766225", "createdAt": "2020-11-08T07:11:07Z", "commit": {"oid": "292f7debbab2b02b317b2578421edb9eed5cdb48"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4441, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}