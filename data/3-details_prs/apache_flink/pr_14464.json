{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTQ0MTIxNzU0", "number": 14464, "title": "[FLINK-20385][canal][json] Allow to read metadata for canal-json format", "bodyText": "What is the purpose of the change\nCurrently FLIP-107 supports reading meta from the Debezium format. According to FLIP-107, metadata should support to be exposed for the Canal JSON format.\nBrief change log\n\nLet CanalJsonDeserializationSchema access and convert those additional fields to metadata columns.\n\nVerifying this change\n\nCanalJsonSerDeSchemaTest adds testDeserializationWithMetadata to whether deserialization of CanalJsonDeserializationSchema could read metadata for the Canal JSON format.\n\nDoes this pull request potentially affect one of the following parts:\n\nDependencies (does it add or upgrade a dependency): (yes / no)\nThe public API, i.e., is any changed class annotated with @Public(Evolving): (yes / no)\nThe serializers: (yes / no / don't know)\nThe runtime per-record code paths (performance sensitive): (yes / no / don't know)\nAnything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (yes / no / don't know)\nThe S3 file system connector: (yes / no / don't know)\n\nDocumentation\n\nDoes this pull request introduce a new feature? (yes / no)\nIf yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)", "createdAt": "2020-12-22T13:24:34Z", "url": "https://github.com/apache/flink/pull/14464", "merged": true, "mergeCommit": {"oid": "814fe0eb06f1941247742de27e8150b7c9274b43"}, "closed": true, "closedAt": "2020-12-31T02:34:01Z", "author": {"login": "SteNicholas"}, "timelineItems": {"totalCount": 15, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdpPp3VAFqTU1ODQwMDI4Ng==", "endCursor": "Y3Vyc29yOnYyOpPPAAABdrORt0AFqTU2MDAyNDE2NQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU4NDAwMjg2", "url": "https://github.com/apache/flink/pull/14464#pullrequestreview-558400286", "createdAt": "2020-12-24T08:12:24Z", "commit": {"oid": "ddcee436fcf2904fb1b3241eb4eccfd36f6553be"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yNFQwODoxMjoyNFrOILCMBw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yNFQwODozMzo0NVrOILClmg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODQ0MTA5NQ==", "bodyText": "Add indent.", "url": "https://github.com/apache/flink/pull/14464#discussion_r548441095", "createdAt": "2020-12-24T08:12:24Z", "author": {"login": "wuchong"}, "path": "flink-formats/flink-json/src/main/java/org/apache/flink/formats/json/canal/CanalJsonDecodingFormat.java", "diffHunk": "@@ -0,0 +1,226 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.json.canal;\n+\n+import org.apache.flink.api.common.serialization.DeserializationSchema;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.formats.json.TimestampFormat;\n+import org.apache.flink.formats.json.canal.CanalJsonDeserializationSchema.MetadataConverter;\n+import org.apache.flink.table.api.DataTypes;\n+import org.apache.flink.table.connector.ChangelogMode;\n+import org.apache.flink.table.connector.format.DecodingFormat;\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.utils.DataTypeUtils;\n+import org.apache.flink.types.RowKind;\n+\n+import javax.annotation.Nullable;\n+\n+import java.util.Collections;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * {@link DecodingFormat} for Canal using JSON encoding.\n+ */\n+public class CanalJsonDecodingFormat implements DecodingFormat<DeserializationSchema<RowData>> {\n+\n+\t// --------------------------------------------------------------------------------------------\n+\t// Mutable attributes\n+\t// --------------------------------------------------------------------------------------------\n+\n+\tprivate List<String> metadataKeys;\n+\n+\t// --------------------------------------------------------------------------------------------\n+\t// Canal-specific attributes\n+\t// --------------------------------------------------------------------------------------------\n+\n+\tprivate final @Nullable String database;\n+\n+\tprivate final @Nullable String table;\n+\n+\tprivate final boolean ignoreParseErrors;\n+\n+\tprivate final TimestampFormat timestampFormat;\n+\n+\tpublic CanalJsonDecodingFormat(\n+\t\tString database,\n+\t\tString table,\n+\t\tboolean ignoreParseErrors,\n+\t\tTimestampFormat timestampFormat) {\n+\t\tthis.database = database;\n+\t\tthis.table = table;\n+\t\tthis.ignoreParseErrors = ignoreParseErrors;\n+\t\tthis.timestampFormat = timestampFormat;\n+\t\tthis.metadataKeys = Collections.emptyList();\n+\t}\n+\n+\t@Override\n+\tpublic DeserializationSchema<RowData> createRuntimeDecoder(\n+\t\tDynamicTableSource.Context context,\n+\t\tDataType physicalDataType) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ddcee436fcf2904fb1b3241eb4eccfd36f6553be"}, "originalPosition": 83}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODQ0NjA4OA==", "bodyText": "Add indent.", "url": "https://github.com/apache/flink/pull/14464#discussion_r548446088", "createdAt": "2020-12-24T08:28:35Z", "author": {"login": "wuchong"}, "path": "flink-formats/flink-json/src/main/java/org/apache/flink/formats/json/canal/CanalJsonDecodingFormat.java", "diffHunk": "@@ -0,0 +1,226 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.json.canal;\n+\n+import org.apache.flink.api.common.serialization.DeserializationSchema;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.formats.json.TimestampFormat;\n+import org.apache.flink.formats.json.canal.CanalJsonDeserializationSchema.MetadataConverter;\n+import org.apache.flink.table.api.DataTypes;\n+import org.apache.flink.table.connector.ChangelogMode;\n+import org.apache.flink.table.connector.format.DecodingFormat;\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.utils.DataTypeUtils;\n+import org.apache.flink.types.RowKind;\n+\n+import javax.annotation.Nullable;\n+\n+import java.util.Collections;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * {@link DecodingFormat} for Canal using JSON encoding.\n+ */\n+public class CanalJsonDecodingFormat implements DecodingFormat<DeserializationSchema<RowData>> {\n+\n+\t// --------------------------------------------------------------------------------------------\n+\t// Mutable attributes\n+\t// --------------------------------------------------------------------------------------------\n+\n+\tprivate List<String> metadataKeys;\n+\n+\t// --------------------------------------------------------------------------------------------\n+\t// Canal-specific attributes\n+\t// --------------------------------------------------------------------------------------------\n+\n+\tprivate final @Nullable String database;\n+\n+\tprivate final @Nullable String table;\n+\n+\tprivate final boolean ignoreParseErrors;\n+\n+\tprivate final TimestampFormat timestampFormat;\n+\n+\tpublic CanalJsonDecodingFormat(\n+\t\tString database,\n+\t\tString table,\n+\t\tboolean ignoreParseErrors,\n+\t\tTimestampFormat timestampFormat) {\n+\t\tthis.database = database;\n+\t\tthis.table = table;\n+\t\tthis.ignoreParseErrors = ignoreParseErrors;\n+\t\tthis.timestampFormat = timestampFormat;\n+\t\tthis.metadataKeys = Collections.emptyList();\n+\t}\n+\n+\t@Override\n+\tpublic DeserializationSchema<RowData> createRuntimeDecoder(\n+\t\tDynamicTableSource.Context context,\n+\t\tDataType physicalDataType) {\n+\t\tfinal List<ReadableMetadata> readableMetadata = metadataKeys.stream()\n+\t\t\t.map(k ->\n+\t\t\t\tStream.of(ReadableMetadata.values())\n+\t\t\t\t\t.filter(rm -> rm.key.equals(k))\n+\t\t\t\t\t.findFirst()\n+\t\t\t\t\t.orElseThrow(IllegalStateException::new))\n+\t\t\t.collect(Collectors.toList());\n+\t\tfinal List<DataTypes.Field> metadataFields = readableMetadata.stream()\n+\t\t\t.map(m -> DataTypes.FIELD(m.key, m.dataType))\n+\t\t\t.collect(Collectors.toList());\n+\t\tfinal DataType producedDataType = DataTypeUtils.appendRowFields(physicalDataType, metadataFields);\n+\t\tfinal TypeInformation<RowData> producedTypeInfo = context.createTypeInformation(producedDataType);\n+\t\treturn CanalJsonDeserializationSchema.builder(physicalDataType, readableMetadata, producedTypeInfo)\n+\t\t\t.setDatabase(database)\n+\t\t\t.setTable(table)\n+\t\t\t.setIgnoreParseErrors(ignoreParseErrors)\n+\t\t\t.setTimestampFormat(timestampFormat)\n+\t\t\t.build();\n+\t}\n+\n+\t@Override\n+\tpublic Map<String, DataType> listReadableMetadata() {\n+\t\tfinal Map<String, DataType> metadataMap = new LinkedHashMap<>();\n+\t\tStream.of(ReadableMetadata.values()).forEachOrdered(m -> metadataMap.put(m.key, m.dataType));\n+\t\treturn metadataMap;\n+\t}\n+\n+\t@Override\n+\tpublic void applyReadableMetadata(List<String> metadataKeys) {\n+\t\tthis.metadataKeys = metadataKeys;\n+\t}\n+\n+\t@Override\n+\tpublic ChangelogMode getChangelogMode() {\n+\t\treturn ChangelogMode.newBuilder()\n+\t\t\t.addContainedKind(RowKind.INSERT)\n+\t\t\t.addContainedKind(RowKind.UPDATE_BEFORE)\n+\t\t\t.addContainedKind(RowKind.UPDATE_AFTER)\n+\t\t\t.addContainedKind(RowKind.DELETE)\n+\t\t\t.build();\n+\t}\n+\n+\t// --------------------------------------------------------------------------------------------\n+\t// Metadata handling\n+\t// --------------------------------------------------------------------------------------------\n+\n+\t/**\n+\t * List of metadata that can be read with this format.\n+\t */\n+\tenum ReadableMetadata {\n+\t\tDATABASE(\n+\t\t\t\"database\",\n+\t\t\tDataTypes.STRING().nullable(),\n+\t\t\tDataTypes.FIELD(\"database\", DataTypes.STRING()),\n+\t\t\tnew MetadataConverter() {\n+\t\t\t\tprivate static final long serialVersionUID = 1L;\n+\n+\t\t\t\t@Override\n+\t\t\t\tpublic Object convert(GenericRowData row, int pos) {\n+\t\t\t\t\treturn row.getString(pos);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t),\n+\n+\t\tTABLE(\n+\t\t\t\"table\",\n+\t\t\tDataTypes.STRING().nullable(),\n+\t\t\tDataTypes.FIELD(\"table\", DataTypes.STRING()),\n+\t\t\tnew MetadataConverter() {\n+\t\t\t\tprivate static final long serialVersionUID = 1L;\n+\n+\t\t\t\t@Override\n+\t\t\t\tpublic Object convert(GenericRowData row, int pos) {\n+\t\t\t\t\treturn row.getString(pos);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t),\n+\n+\t\tSQL_TYPE(\n+\t\t\t\"sql-type\",\n+\t\t\tDataTypes.STRING().nullable(),\n+\t\t\tDataTypes.FIELD(\"sqlType\", DataTypes.STRING()),\n+\t\t\tnew MetadataConverter() {\n+\t\t\t\tprivate static final long serialVersionUID = 1L;\n+\n+\t\t\t\t@Override\n+\t\t\t\tpublic Object convert(GenericRowData row, int pos) {\n+\t\t\t\t\treturn row.getString(pos);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t),\n+\n+\t\tPK_NAMES(\n+\t\t\t\"pk-names\",\n+\t\t\tDataTypes.STRING().nullable(),\n+\t\t\tDataTypes.FIELD(\"pkNames\", DataTypes.ARRAY(DataTypes.STRING())),\n+\t\t\tnew MetadataConverter() {\n+\t\t\t\tprivate static final long serialVersionUID = 1L;\n+\n+\t\t\t\t@Override\n+\t\t\t\tpublic Object convert(GenericRowData row, int pos) {\n+\t\t\t\t\treturn row.getArray(pos);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t),\n+\n+\t\tINGESTION_TIMESTAMP(\n+\t\t\t\"ingestion-timestamp\",\n+\t\t\tDataTypes.TIMESTAMP_WITH_LOCAL_TIME_ZONE(3).nullable(),\n+\t\t\tDataTypes.FIELD(\"ts\", DataTypes.BIGINT()),\n+\t\t\tnew MetadataConverter() {\n+\t\t\t\tprivate static final long serialVersionUID = 1L;\n+\n+\t\t\t\t@Override\n+\t\t\t\tpublic Object convert(GenericRowData row, int pos) {\n+\t\t\t\t\tif (row.isNullAt(pos)) {\n+\t\t\t\t\t\treturn null;\n+\t\t\t\t\t}\n+\t\t\t\t\treturn TimestampData.fromEpochMillis(row.getLong(pos));\n+\t\t\t\t}\n+\t\t\t}\n+\t\t);\n+\n+\t\tfinal String key;\n+\n+\t\tfinal DataType dataType;\n+\n+\t\tfinal DataTypes.Field requiredJsonField;\n+\n+\t\tfinal MetadataConverter converter;\n+\n+\t\tReadableMetadata(\n+\t\t\tString key,\n+\t\t\tDataType dataType,\n+\t\t\tDataTypes.Field requiredJsonField,\n+\t\t\tMetadataConverter converter) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ddcee436fcf2904fb1b3241eb4eccfd36f6553be"}, "originalPosition": 219}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODQ0NzY0Mg==", "bodyText": "Add indent.", "url": "https://github.com/apache/flink/pull/14464#discussion_r548447642", "createdAt": "2020-12-24T08:33:45Z", "author": {"login": "wuchong"}, "path": "flink-formats/flink-json/src/main/java/org/apache/flink/formats/json/canal/CanalJsonDeserializationSchema.java", "diffHunk": "@@ -262,24 +296,71 @@ public boolean equals(Object o) {\n \t\t\treturn false;\n \t\t}\n \t\tCanalJsonDeserializationSchema that = (CanalJsonDeserializationSchema) o;\n-\t\treturn ignoreParseErrors == that.ignoreParseErrors &&\n-\t\t\tfieldCount == that.fieldCount &&\n-\t\t\tObjects.equals(jsonDeserializer, that.jsonDeserializer) &&\n-\t\t\tObjects.equals(resultTypeInfo, that.resultTypeInfo);\n+\t\treturn Objects.equals(jsonDeserializer, that.jsonDeserializer)\n+\t\t\t&& hasMetadata == that.hasMetadata\n+\t\t\t&& Objects.equals(producedTypeInfo, that.producedTypeInfo)\n+\t\t\t&& Objects.equals(database, that.database)\n+\t\t\t&& Objects.equals(table, that.table)\n+\t\t\t&& ignoreParseErrors == that.ignoreParseErrors\n+\t\t\t&& fieldCount == that.fieldCount;\n \t}\n \n \t@Override\n \tpublic int hashCode() {\n-\t\treturn Objects.hash(jsonDeserializer, resultTypeInfo, ignoreParseErrors, fieldCount);\n+\t\treturn Objects.hash(jsonDeserializer, hasMetadata, producedTypeInfo, database, table, ignoreParseErrors, fieldCount);\n \t}\n \n-\tprivate static RowType createJsonRowType(DataType databaseSchema) {\n+\t// --------------------------------------------------------------------------------------------\n+\n+\tprivate static RowType createJsonRowType(DataType physicalDataType, List<ReadableMetadata> readableMetadata) {\n \t\t// Canal JSON contains other information, e.g. \"ts\", \"sql\", but we don't need them\n-\t\treturn (RowType) DataTypes.ROW(\n-\t\t\tDataTypes.FIELD(\"data\", DataTypes.ARRAY(databaseSchema)),\n-\t\t\tDataTypes.FIELD(\"old\", DataTypes.ARRAY(databaseSchema)),\n+\t\tDataType root = DataTypes.ROW(\n+\t\t\tDataTypes.FIELD(\"data\", DataTypes.ARRAY(physicalDataType)),\n+\t\t\tDataTypes.FIELD(\"old\", DataTypes.ARRAY(physicalDataType)),\n \t\t\tDataTypes.FIELD(\"type\", DataTypes.STRING()),\n-\t\t\tDataTypes.FIELD(\"database\", DataTypes.STRING()),\n-\t\t\tDataTypes.FIELD(\"table\", DataTypes.STRING())).getLogicalType();\n+\t\t\tReadableMetadata.DATABASE.requiredJsonField,\n+\t\t\tReadableMetadata.TABLE.requiredJsonField);\n+\t\t// append fields that are required for reading metadata in the root\n+\t\tfinal List<DataTypes.Field> rootMetadataFields = readableMetadata.stream()\n+\t\t\t.filter(m -> m != ReadableMetadata.DATABASE && m != ReadableMetadata.TABLE)\n+\t\t\t.map(m -> m.requiredJsonField)\n+\t\t\t.distinct()\n+\t\t\t.collect(Collectors.toList());\n+\t\treturn (RowType) DataTypeUtils.appendRowFields(root, rootMetadataFields).getLogicalType();\n+\t}\n+\n+\tprivate static MetadataConverter[] createMetadataConverters(\n+\t\tRowType jsonRowType,\n+\t\tList<ReadableMetadata> requestedMetadata) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ddcee436fcf2904fb1b3241eb4eccfd36f6553be"}, "originalPosition": 272}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "4ded1f32d30bf98ad7ac4753a2831acf2e78a4f5", "author": {"user": {"login": "SteNicholas", "name": "SteNicholas"}}, "url": "https://github.com/apache/flink/commit/4ded1f32d30bf98ad7ac4753a2831acf2e78a4f5", "committedDate": "2020-12-27T15:58:30Z", "message": "[FLINK-20385][canal][json] Allow to read metadata for canal-json format"}, "afterCommit": {"oid": "9abb79a2bb6e487d3f1deb0cfc4ecfff74a7d780", "author": {"user": {"login": "SteNicholas", "name": "SteNicholas"}}, "url": "https://github.com/apache/flink/commit/9abb79a2bb6e487d3f1deb0cfc4ecfff74a7d780", "committedDate": "2020-12-27T23:18:40Z", "message": "[FLINK-20385][canal][json] Allow to read metadata for canal-json format"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU4OTg4NTU4", "url": "https://github.com/apache/flink/pull/14464#pullrequestreview-558988558", "createdAt": "2020-12-28T04:04:34Z", "commit": {"oid": "9abb79a2bb6e487d3f1deb0cfc4ecfff74a7d780"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOFQwNDowNDozNFrOILw8Zw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOFQwNDowNTo0OFrOILw88A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTIwNzE0Mw==", "bodyText": "Could you list all the metadata columns? I think that would be helpful, esp. for the complex types.", "url": "https://github.com/apache/flink/pull/14464#discussion_r549207143", "createdAt": "2020-12-28T04:04:34Z", "author": {"login": "wuchong"}, "path": "docs/dev/table/connectors/formats/canal.md", "diffHunk": "@@ -142,6 +142,79 @@ SELECT * FROM topic_products;\n </div>\n </div>\n \n+Available Metadata\n+------------------\n+\n+The following format metadata can be exposed as read-only (`VIRTUAL`) columns in a table definition.\n+\n+<span class=\"label label-danger\">Attention</span> Format metadata fields are only available if the\n+corresponding connector forwards format metadata. Currently, only the Kafka connector is able to expose\n+metadata fields for its value format.\n+\n+<table class=\"table table-bordered\">\n+    <thead>\n+    <tr>\n+      <th class=\"text-left\" style=\"width: 25%\">Key</th>\n+      <th class=\"text-center\" style=\"width: 40%\">Data Type</th>\n+      <th class=\"text-center\" style=\"width: 40%\">Description</th>\n+    </tr>\n+    </thead>\n+    <tbody>\n+    <tr>\n+      <td><code>database</code></td>\n+      <td><code>STRING NULL</code></td>\n+      <td>The originating database. Corresponds to the <code>database</code> field in the\n+      Canal record if available.</td>\n+    </tr>\n+    <tr>\n+      <td><code>table</code></td>\n+      <td><code>STRING NULL</code></td>\n+      <td>The originating database table. Corresponds to the <code>table</code> field in the\n+      Canal record if available.</td>\n+    </tr>\n+    <tr>\n+      <td><code>sql-type</code></td>\n+      <td><code>MAP&lt;STRING, INT&gt; NULL</code></td>\n+      <td>Map of various sql types. Corresponds to the <code>sqlType</code> field in the \n+      Canal record if available.</td>\n+    </tr>\n+    <tr>\n+      <td><code>pk-names</code></td>\n+      <td><code>ARRAY&lt;STRING&gt; NULL</code></td>\n+      <td>Array of primary key names. Corresponds to the <code>pkNames</code> field in the \n+      Canal record if available.</td>\n+    </tr>\n+    <tr>\n+      <td><code>ingestion-timestamp</code></td>\n+      <td><code>TIMESTAMP(3) WITH LOCAL TIME ZONE NULL</code></td>\n+      <td>The timestamp at which the connector processed the event. Corresponds to the <code>ts</code>\n+      field in the Canal record.</td>\n+    </tr>\n+    </tbody>\n+</table>\n+\n+The following example shows how to access Canal metadata fields in Kafka:\n+\n+<div class=\"codetabs\" markdown=\"1\">\n+<div data-lang=\"SQL\" markdown=\"1\">\n+{% highlight sql %}\n+CREATE TABLE KafkaTable (\n+  `origin_database` STRING METADATA FROM 'value.database' VIRTUAL,\n+  `origin_table` STRING METADATA FROM 'value.table' VIRTUAL,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9abb79a2bb6e487d3f1deb0cfc4ecfff74a7d780"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTIwNzE4OA==", "bodyText": "I think we don't need to add backquotes around the column names, because they are not keywords.", "url": "https://github.com/apache/flink/pull/14464#discussion_r549207188", "createdAt": "2020-12-28T04:04:59Z", "author": {"login": "wuchong"}, "path": "docs/dev/table/connectors/formats/canal.md", "diffHunk": "@@ -142,6 +142,79 @@ SELECT * FROM topic_products;\n </div>\n </div>\n \n+Available Metadata\n+------------------\n+\n+The following format metadata can be exposed as read-only (`VIRTUAL`) columns in a table definition.\n+\n+<span class=\"label label-danger\">Attention</span> Format metadata fields are only available if the\n+corresponding connector forwards format metadata. Currently, only the Kafka connector is able to expose\n+metadata fields for its value format.\n+\n+<table class=\"table table-bordered\">\n+    <thead>\n+    <tr>\n+      <th class=\"text-left\" style=\"width: 25%\">Key</th>\n+      <th class=\"text-center\" style=\"width: 40%\">Data Type</th>\n+      <th class=\"text-center\" style=\"width: 40%\">Description</th>\n+    </tr>\n+    </thead>\n+    <tbody>\n+    <tr>\n+      <td><code>database</code></td>\n+      <td><code>STRING NULL</code></td>\n+      <td>The originating database. Corresponds to the <code>database</code> field in the\n+      Canal record if available.</td>\n+    </tr>\n+    <tr>\n+      <td><code>table</code></td>\n+      <td><code>STRING NULL</code></td>\n+      <td>The originating database table. Corresponds to the <code>table</code> field in the\n+      Canal record if available.</td>\n+    </tr>\n+    <tr>\n+      <td><code>sql-type</code></td>\n+      <td><code>MAP&lt;STRING, INT&gt; NULL</code></td>\n+      <td>Map of various sql types. Corresponds to the <code>sqlType</code> field in the \n+      Canal record if available.</td>\n+    </tr>\n+    <tr>\n+      <td><code>pk-names</code></td>\n+      <td><code>ARRAY&lt;STRING&gt; NULL</code></td>\n+      <td>Array of primary key names. Corresponds to the <code>pkNames</code> field in the \n+      Canal record if available.</td>\n+    </tr>\n+    <tr>\n+      <td><code>ingestion-timestamp</code></td>\n+      <td><code>TIMESTAMP(3) WITH LOCAL TIME ZONE NULL</code></td>\n+      <td>The timestamp at which the connector processed the event. Corresponds to the <code>ts</code>\n+      field in the Canal record.</td>\n+    </tr>\n+    </tbody>\n+</table>\n+\n+The following example shows how to access Canal metadata fields in Kafka:\n+\n+<div class=\"codetabs\" markdown=\"1\">\n+<div data-lang=\"SQL\" markdown=\"1\">\n+{% highlight sql %}\n+CREATE TABLE KafkaTable (\n+  `origin_database` STRING METADATA FROM 'value.database' VIRTUAL,\n+  `origin_table` STRING METADATA FROM 'value.table' VIRTUAL,\n+  `user_id` BIGINT,\n+  `item_id` BIGINT,\n+  `behavior` STRING", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9abb79a2bb6e487d3f1deb0cfc4ecfff74a7d780"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTIwNzI4MA==", "bodyText": "Could you also test the Map and Array metadatas?", "url": "https://github.com/apache/flink/pull/14464#discussion_r549207280", "createdAt": "2020-12-28T04:05:48Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/table/KafkaChangelogTableITCase.java", "diffHunk": "@@ -198,4 +198,130 @@ public void testKafkaDebeziumChangelogSource() throws Exception {\n \t\ttableResult.getJobClient().get().cancel().get(); // stop the job\n \t\tdeleteTestTopic(topic);\n \t}\n+\n+\t@Test\n+\tpublic void testKafkaCanalChangelogSource() throws Exception {\n+\t\tfinal String topic = \"changelog_canal\";\n+\t\tcreateTestTopic(topic, 1, 1);\n+\n+\t\t// enables MiniBatch processing to verify MiniBatch + FLIP-95, see FLINK-18769\n+\t\tConfiguration tableConf = tEnv.getConfig().getConfiguration();\n+\t\ttableConf.setString(\"table.exec.mini-batch.enabled\", \"true\");\n+\t\ttableConf.setString(\"table.exec.mini-batch.allow-latency\", \"1s\");\n+\t\ttableConf.setString(\"table.exec.mini-batch.size\", \"5000\");\n+\t\ttableConf.setString(\"table.optimizer.agg-phase-strategy\", \"TWO_PHASE\");\n+\n+\t\t// ---------- Write the Canal json into Kafka -------------------\n+\t\tList<String> lines = readLines(\"canal-data.txt\");\n+\t\tDataStreamSource<String> stream = env.fromCollection(lines);\n+\t\tSerializationSchema<String> serSchema = new SimpleStringSchema();\n+\t\tFlinkKafkaPartitioner<String> partitioner = new FlinkFixedPartitioner<>();\n+\n+\t\t// the producer must not produce duplicates\n+\t\tProperties producerProperties = FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings);\n+\t\tproducerProperties.setProperty(\"retries\", \"0\");\n+\t\tproducerProperties.putAll(secureProps);\n+\t\tkafkaServer.produceIntoKafka(stream, topic, serSchema, producerProperties, partitioner);\n+\t\ttry {\n+\t\t\tenv.execute(\"Write sequence\");\n+\t\t} catch (Exception e) {\n+\t\t\tthrow new Exception(\"Failed to write canal data to Kafka.\", e);\n+\t\t}\n+\n+\t\t// ---------- Produce an event time stream into Kafka -------------------\n+\t\tString bootstraps = standardProps.getProperty(\"bootstrap.servers\");\n+\t\tString sourceDDL = String.format(\n+\t\t\t\"CREATE TABLE canal_source (\" +\n+\t\t\t\t// test format metadata\n+\t\t\t\t\" origin_ts STRING METADATA FROM 'value.ingestion-timestamp' VIRTUAL,\" + // unused\n+\t\t\t\t\" origin_table STRING METADATA FROM 'value.table' VIRTUAL,\" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9abb79a2bb6e487d3f1deb0cfc4ecfff74a7d780"}, "originalPosition": 40}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU5MTA4NTcy", "url": "https://github.com/apache/flink/pull/14464#pullrequestreview-559108572", "createdAt": "2020-12-28T11:35:32Z", "commit": {"oid": "44f3c38cdbc31df14dafa54cf9b80264da281256"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOFQxMTozNTozMlrOIL3mAQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOFQxMTozNTozMlrOIL3mAQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTMxNjA5Nw==", "bodyText": "Don't need to escape the character.", "url": "https://github.com/apache/flink/pull/14464#discussion_r549316097", "createdAt": "2020-12-28T11:35:32Z", "author": {"login": "wuchong"}, "path": "docs/dev/table/connectors/formats/canal.zh.md", "diffHunk": "@@ -198,11 +198,14 @@ The following example shows how to access Canal metadata fields in Kafka:\n <div data-lang=\"SQL\" markdown=\"1\">\n {% highlight sql %}\n CREATE TABLE KafkaTable (\n-  `origin_database` STRING METADATA FROM 'value.database' VIRTUAL,\n-  `origin_table` STRING METADATA FROM 'value.table' VIRTUAL,\n-  `user_id` BIGINT,\n-  `item_id` BIGINT,\n-  `behavior` STRING\n+  origin_database STRING METADATA FROM 'value.database' VIRTUAL,\n+  origin_table STRING METADATA FROM 'value.table' VIRTUAL,\n+  origin_sql_type MAP&lt;STRING, INT&gt; METADATA FROM 'value.sql-type' VIRTUAL,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "44f3c38cdbc31df14dafa54cf9b80264da281256"}, "originalPosition": 11}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU5MTI2MDk5", "url": "https://github.com/apache/flink/pull/14464#pullrequestreview-559126099", "createdAt": "2020-12-28T12:31:41Z", "commit": {"oid": "2aa16a8e76b7ebf4705e2554d13ea014af50e446"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOFQxMjozMTo0MVrOIL4h-w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOFQxMjozMTo0MVrOIL4h-w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTMzMTQ1MQ==", "bodyText": "Please also modify others and preview the page in local.", "url": "https://github.com/apache/flink/pull/14464#discussion_r549331451", "createdAt": "2020-12-28T12:31:41Z", "author": {"login": "wuchong"}, "path": "docs/dev/table/connectors/formats/canal.zh.md", "diffHunk": "@@ -200,7 +200,7 @@ The following example shows how to access Canal metadata fields in Kafka:\n CREATE TABLE KafkaTable (\n   origin_database STRING METADATA FROM 'value.database' VIRTUAL,\n   origin_table STRING METADATA FROM 'value.table' VIRTUAL,\n-  origin_sql_type MAP&lt;STRING, INT&gt; METADATA FROM 'value.sql-type' VIRTUAL,\n+  origin_sql_type MAP<STRING, INT> METADATA FROM 'value.sql-type' VIRTUAL,\n   origin_pk_names ARRAY&lt;STRING&gt; METADATA FROM 'value.pk-names' VIRTUAL,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2aa16a8e76b7ebf4705e2554d13ea014af50e446"}, "originalPosition": 15}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "2aa16a8e76b7ebf4705e2554d13ea014af50e446", "author": {"user": {"login": "SteNicholas", "name": "SteNicholas"}}, "url": "https://github.com/apache/flink/commit/2aa16a8e76b7ebf4705e2554d13ea014af50e446", "committedDate": "2020-12-28T12:09:16Z", "message": "[FLINK-20385][canal][json] Allow to read metadata for canal-json format"}, "afterCommit": {"oid": "e81abd70d44d6ce7221d4cd2f44d31deab78ea9a", "author": {"user": {"login": "SteNicholas", "name": "SteNicholas"}}, "url": "https://github.com/apache/flink/commit/e81abd70d44d6ce7221d4cd2f44d31deab78ea9a", "committedDate": "2020-12-28T13:26:32Z", "message": "[FLINK-20385][canal][json] Allow to read metadata for canal-json format"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "e81abd70d44d6ce7221d4cd2f44d31deab78ea9a", "author": {"user": {"login": "SteNicholas", "name": "SteNicholas"}}, "url": "https://github.com/apache/flink/commit/e81abd70d44d6ce7221d4cd2f44d31deab78ea9a", "committedDate": "2020-12-28T13:26:32Z", "message": "[FLINK-20385][canal][json] Allow to read metadata for canal-json format"}, "afterCommit": {"oid": "6956309e3928df5f4e7717a978bd91eee472f6ed", "author": {"user": {"login": "SteNicholas", "name": "SteNicholas"}}, "url": "https://github.com/apache/flink/commit/6956309e3928df5f4e7717a978bd91eee472f6ed", "committedDate": "2020-12-28T13:33:45Z", "message": "[FLINK-20385][canal][json] Allow to read metadata for canal-json format"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "6956309e3928df5f4e7717a978bd91eee472f6ed", "author": {"user": {"login": "SteNicholas", "name": "SteNicholas"}}, "url": "https://github.com/apache/flink/commit/6956309e3928df5f4e7717a978bd91eee472f6ed", "committedDate": "2020-12-28T13:33:45Z", "message": "[FLINK-20385][canal][json] Allow to read metadata for canal-json format"}, "afterCommit": {"oid": "f868d49d3c71f7b55d1c6a3dc601ac1561bdd9e8", "author": {"user": {"login": "SteNicholas", "name": "SteNicholas"}}, "url": "https://github.com/apache/flink/commit/f868d49d3c71f7b55d1c6a3dc601ac1561bdd9e8", "committedDate": "2020-12-29T06:19:35Z", "message": "[FLINK-20385][canal][json] Allow to read metadata for canal-json format"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU5NDAxNTIw", "url": "https://github.com/apache/flink/pull/14464#pullrequestreview-559401520", "createdAt": "2020-12-29T07:01:09Z", "commit": {"oid": "f868d49d3c71f7b55d1c6a3dc601ac1561bdd9e8"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwNzowMTowOVrOIMIchA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwNzowMTowOVrOIMIchA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU5MjE5Ng==", "bodyText": "Could you beautify the format? The same to the debezium one.", "url": "https://github.com/apache/flink/pull/14464#discussion_r549592196", "createdAt": "2020-12-29T07:01:09Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/table/KafkaChangelogTableITCase.java", "diffHunk": "@@ -218,4 +218,168 @@ public void testKafkaDebeziumChangelogSource() throws Exception {\n         tableResult.getJobClient().get().cancel().get(); // stop the job\n         deleteTestTopic(topic);\n     }\n+\n+    @Test\n+    public void testKafkaCanalChangelogSource() throws Exception {\n+        final String topic = \"changelog_canal\";\n+        createTestTopic(topic, 1, 1);\n+\n+        // enables MiniBatch processing to verify MiniBatch + FLIP-95, see FLINK-18769\n+        Configuration tableConf = tEnv.getConfig().getConfiguration();\n+        tableConf.setString(\"table.exec.mini-batch.enabled\", \"true\");\n+        tableConf.setString(\"table.exec.mini-batch.allow-latency\", \"1s\");\n+        tableConf.setString(\"table.exec.mini-batch.size\", \"5000\");\n+        tableConf.setString(\"table.optimizer.agg-phase-strategy\", \"TWO_PHASE\");\n+\n+        // ---------- Write the Canal json into Kafka -------------------\n+        List<String> lines = readLines(\"canal-data.txt\");\n+        DataStreamSource<String> stream = env.fromCollection(lines);\n+        SerializationSchema<String> serSchema = new SimpleStringSchema();\n+        FlinkKafkaPartitioner<String> partitioner = new FlinkFixedPartitioner<>();\n+\n+        // the producer must not produce duplicates\n+        Properties producerProperties =\n+                FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings);\n+        producerProperties.setProperty(\"retries\", \"0\");\n+        producerProperties.putAll(secureProps);\n+        kafkaServer.produceIntoKafka(stream, topic, serSchema, producerProperties, partitioner);\n+        try {\n+            env.execute(\"Write sequence\");\n+        } catch (Exception e) {\n+            throw new Exception(\"Failed to write canal data to Kafka.\", e);\n+        }\n+\n+        // ---------- Produce an event time stream into Kafka -------------------\n+        String bootstraps = standardProps.getProperty(\"bootstrap.servers\");\n+        String sourceDDL =\n+                String.format(\n+                        \"CREATE TABLE canal_source (\"\n+                                +\n+                                // test format metadata\n+                                \" origin_database STRING METADATA FROM 'value.database' VIRTUAL,\"\n+                                + \" origin_table STRING METADATA FROM 'value.table' VIRTUAL,\"\n+                                + \" origin_sql_type MAP<STRING, INT> METADATA FROM 'value.sql-type' VIRTUAL,\"\n+                                + \" origin_pk_names ARRAY<STRING> METADATA FROM 'value.pk-names' VIRTUAL,\"\n+                                + \" origin_ts TIMESTAMP(3) METADATA FROM 'value.ingestion-timestamp' VIRTUAL,\"\n+                                + \" id INT NOT NULL,\"\n+                                + \" name STRING,\"\n+                                + \" description STRING,\"\n+                                + \" weight DECIMAL(10,3),\"\n+                                +\n+                                // test connector metadata\n+                                \" origin_topic STRING METADATA FROM 'topic' VIRTUAL,\"\n+                                + \" origin_partition STRING METADATA FROM 'partition' VIRTUAL\"\n+                                + // unused\n+                                \") WITH (\"\n+                                + \" 'connector' = 'kafka',\"\n+                                + \" 'topic' = '%s',\"\n+                                + \" 'properties.bootstrap.servers' = '%s',\"\n+                                + \" 'scan.startup.mode' = 'earliest-offset',\"\n+                                + \" 'value.format' = 'canal-json'\"\n+                                + \")\",\n+                        topic, bootstraps);\n+        String sinkDDL =\n+                \"CREATE TABLE sink (\"\n+                        + \" origin_topic STRING,\"\n+                        + \" origin_database STRING,\"\n+                        + \" origin_table STRING,\"\n+                        + \" origin_sql_type MAP<STRING, INT>,\"\n+                        + \" origin_pk_names ARRAY<STRING>,\"\n+                        + \" origin_ts TIMESTAMP(3),\"\n+                        + \" name STRING,\"\n+                        + \" PRIMARY KEY (name) NOT ENFORCED\"\n+                        + \") WITH (\"\n+                        + \" 'connector' = 'values',\"\n+                        + \" 'sink-insert-only' = 'false'\"\n+                        + \")\";\n+        tEnv.executeSql(sourceDDL);\n+        tEnv.executeSql(sinkDDL);\n+        TableResult tableResult =\n+                tEnv.executeSql(\n+                        \"INSERT INTO sink \"\n+                                + \"SELECT origin_topic, origin_database, origin_table, origin_sql_type, \"\n+                                + \"origin_pk_names, origin_ts, name \"\n+                                + \"FROM canal_source\");\n+\n+        // Canal captures change data on the `products2` table:\n+        //\n+        // CREATE TABLE products2 (\n+        //  id INTEGER NOT NULL AUTO_INCREMENT PRIMARY KEY,\n+        //  name VARCHAR(255),\n+        //  description VARCHAR(512),\n+        //  weight FLOAT\n+        // );\n+        // ALTER TABLE products2 AUTO_INCREMENT = 101;\n+        //\n+        // INSERT INTO products2\n+        // VALUES (default,\"scooter\",\"Small 2-wheel scooter\",3.14),\n+        //        (default,\"car battery\",\"12V car battery\",8.1),\n+        //        (default,\"12-pack drill bits\",\"12-pack of drill bits with sizes ranging from #40\n+        // to\n+        // #3\",0.8),\n+        //        (default,\"hammer\",\"12oz carpenter's hammer\",0.75),\n+        //        (default,\"hammer\",\"14oz carpenter's hammer\",0.875),\n+        //        (default,\"hammer\",\"16oz carpenter's hammer\",1.0),\n+        //        (default,\"rocks\",\"box of assorted rocks\",5.3),\n+        //        (default,\"jacket\",\"water resistent black wind breaker\",0.1),\n+        //        (default,\"spare tire\",\"24 inch spare tire\",22.2);\n+        // UPDATE products2 SET description='18oz carpenter hammer' WHERE id=106;\n+        // UPDATE products2 SET weight='5.1' WHERE id=107;\n+        // INSERT INTO products2 VALUES (default,\"jacket\",\"water resistent white wind breaker\",0.2);\n+        // INSERT INTO products2 VALUES (default,\"scooter\",\"Big 2-wheel scooter \",5.18);\n+        // UPDATE products2 SET description='new water resistent white wind breaker', weight='0.5'\n+        // WHERE\n+        // id=110;\n+        // UPDATE products2 SET weight='5.17' WHERE id=111;\n+        // DELETE FROM products2 WHERE id=111;\n+        // UPDATE products2 SET weight='5.17' WHERE id=102 or id = 101;\n+        // DELETE FROM products2 WHERE id=102 or id = 103;\n+        //\n+        // > SELECT * FROM products2;\n+        // +-----+--------------------+---------------------------------------------------------+--------+\n+        // | id  | name               | description                                             |\n+        // weight\n+        // |\n+        // +-----+--------------------+---------------------------------------------------------+--------+\n+        // | 101 | scooter            | Small 2-wheel scooter                                   |\n+        // 5.17\n+        // |\n+        // | 104 | hammer             | 12oz carpenter's hammer                                 |\n+        // 0.75\n+        // |\n+        // | 105 | hammer             | 14oz carpenter's hammer                                 |\n+        // 0.875\n+        // |\n+        // | 106 | hammer             | 18oz carpenter hammer                                   |\n+        //   1\n+        // |\n+        // | 107 | rocks              | box of assorted rocks                                   |\n+        // 5.1\n+        // |\n+        // | 108 | jacket             | water resistent black wind breaker                      |", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f868d49d3c71f7b55d1c6a3dc601ac1561bdd9e8"}, "originalPosition": 151}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "73d00ec389bcf20486ae71758ab613e8cfe7cb03", "author": {"user": {"login": "SteNicholas", "name": "SteNicholas"}}, "url": "https://github.com/apache/flink/commit/73d00ec389bcf20486ae71758ab613e8cfe7cb03", "committedDate": "2020-12-29T07:35:33Z", "message": "[FLINK-20385][canal][json] Allow to read metadata for canal-json format"}, "afterCommit": {"oid": "554e0a2c2ada42836d65397b163bad657806e8c2", "author": {"user": {"login": "SteNicholas", "name": "SteNicholas"}}, "url": "https://github.com/apache/flink/commit/554e0a2c2ada42836d65397b163bad657806e8c2", "committedDate": "2020-12-29T12:02:57Z", "message": "[FLINK-20385][canal][json] Allow to read metadata for canal-json format"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "554e0a2c2ada42836d65397b163bad657806e8c2", "author": {"user": {"login": "SteNicholas", "name": "SteNicholas"}}, "url": "https://github.com/apache/flink/commit/554e0a2c2ada42836d65397b163bad657806e8c2", "committedDate": "2020-12-29T12:02:57Z", "message": "[FLINK-20385][canal][json] Allow to read metadata for canal-json format"}, "afterCommit": {"oid": "3f3b88d98c525b635179b6a9098a2634a1ffb42c", "author": {"user": {"login": "SteNicholas", "name": "SteNicholas"}}, "url": "https://github.com/apache/flink/commit/3f3b88d98c525b635179b6a9098a2634a1ffb42c", "committedDate": "2020-12-30T06:52:25Z", "message": "[FLINK-20385][canal][json] Allow to read metadata for canal-json format"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5a5ef1f0bb5e1dcc094ad5454890d13ddf55c646", "author": {"user": {"login": "SteNicholas", "name": "SteNicholas"}}, "url": "https://github.com/apache/flink/commit/5a5ef1f0bb5e1dcc094ad5454890d13ddf55c646", "committedDate": "2020-12-30T07:33:03Z", "message": "[FLINK-20385][canal-json] Allow to read metadata for canal-json format\n\nThis closes #14464"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "3f3b88d98c525b635179b6a9098a2634a1ffb42c", "author": {"user": {"login": "SteNicholas", "name": "SteNicholas"}}, "url": "https://github.com/apache/flink/commit/3f3b88d98c525b635179b6a9098a2634a1ffb42c", "committedDate": "2020-12-30T06:52:25Z", "message": "[FLINK-20385][canal][json] Allow to read metadata for canal-json format"}, "afterCommit": {"oid": "5a5ef1f0bb5e1dcc094ad5454890d13ddf55c646", "author": {"user": {"login": "SteNicholas", "name": "SteNicholas"}}, "url": "https://github.com/apache/flink/commit/5a5ef1f0bb5e1dcc094ad5454890d13ddf55c646", "committedDate": "2020-12-30T07:33:03Z", "message": "[FLINK-20385][canal-json] Allow to read metadata for canal-json format\n\nThis closes #14464"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d6ec8319ec7906aeda62d9c60887f6b9fb2f505f", "author": {"user": {"login": "SteNicholas", "name": "SteNicholas"}}, "url": "https://github.com/apache/flink/commit/d6ec8319ec7906aeda62d9c60887f6b9fb2f505f", "committedDate": "2020-12-30T10:58:13Z", "message": "[FLINK-20385][canal-json] Allow to read metadata for canal-json format"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTYwMDI0MTY1", "url": "https://github.com/apache/flink/pull/14464#pullrequestreview-560024165", "createdAt": "2020-12-30T12:14:00Z", "commit": {"oid": "d6ec8319ec7906aeda62d9c60887f6b9fb2f505f"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3359, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}