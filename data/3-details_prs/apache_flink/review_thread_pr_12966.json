{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDU1NTE5MzE1", "number": 12966, "reviewThreads": {"totalCount": 56, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QwMzo0Mzo1NVrOESOb3g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQwNzowMjoxN1rOEZVszg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg3NTQ2MzM0OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QwMzo0Mzo1NVrOG3SsfA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QwMzo0Mzo1NVrOG3SsfA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDYzMTE2NA==", "bodyText": "!Arrays.stream(tableSourceTable.extraDigests()).anyMatch(digest -> digest.startsWith(\"source: [partitions=\")) can be simplified as Arrays.stream(tableSourceTable.extraDigests()).noneMatch(digest -> digest.startsWith(\"source: [partitions=\"))", "url": "https://github.com/apache/flink/pull/12966#discussion_r460631164", "createdAt": "2020-07-27T03:43:55Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsPartitionPushDown;\n+import org.apache.flink.table.planner.calcite.FlinkContext;\n+import org.apache.flink.table.planner.calcite.FlinkTypeFactory;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+import org.apache.flink.table.planner.plan.utils.FlinkRelOptUtil;\n+import org.apache.flink.table.planner.plan.utils.PartitionPruner;\n+import org.apache.flink.table.planner.plan.utils.RexNodeExtractor;\n+import org.apache.flink.table.types.logical.LogicalType;\n+\n+import org.apache.calcite.plan.Context;\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Filter;\n+import org.apache.calcite.rel.logical.LogicalTableScan;\n+import org.apache.calcite.rel.type.RelDataType;\n+import org.apache.calcite.rex.RexBuilder;\n+import org.apache.calcite.rex.RexInputRef;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.calcite.rex.RexShuttle;\n+import org.apache.calcite.rex.RexUtil;\n+import org.apache.calcite.tools.RelBuilder;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.Seq;\n+\n+/**\n+ * Planner rule that tries to push partition evaluated by filter condition into a {@link LogicalTableScan}.\n+*/\n+public class PushPartitionIntoTableSourceScanRule extends RelOptRule {\n+\tpublic static final PushPartitionIntoTableSourceScanRule INSTANCE = new PushPartitionIntoTableSourceScanRule();\n+\n+\tpublic PushPartitionIntoTableSourceScanRule(){\n+\t\tsuper(operand(Filter.class,\n+\t\t\t\toperand(LogicalTableScan.class, none())),\n+\t\t\t\"PushPartitionTableSourceScanRule\");\n+\t}\n+\n+\t@Override\n+\tpublic boolean matches(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tif (filter.getCondition() == null) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tTableSourceTable tableSourceTable = call.rel(1).getTable().unwrap(TableSourceTable.class);\n+\t\tif (tableSourceTable == null){\n+\t\t\treturn false;\n+\t\t}\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource();\n+\t\tif (!(dynamicTableSource instanceof SupportsPartitionPushDown)) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tOptional<List<Map<String, String>>> partitions = ((SupportsPartitionPushDown) dynamicTableSource).listPartitions();\n+\t\treturn partitions.isPresent()\n+\t\t\t&& !partitions.get().isEmpty()\n+\t\t\t&& !Arrays.stream(tableSourceTable.extraDigests()).anyMatch(digest -> digest.startsWith(\"source: [partitions=\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0f19df4d1fcbb4093b69d772628b67b81ebb443a"}, "originalPosition": 84}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg3NzIzMDE0OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QxMzo0NjoxMlrOG3jKaA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QxMzo0NjoxMlrOG3jKaA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDkwMDk2OA==", "bodyText": "I think we can return false if partitioned field names is empty", "url": "https://github.com/apache/flink/pull/12966#discussion_r460900968", "createdAt": "2020-07-27T13:46:12Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsPartitionPushDown;\n+import org.apache.flink.table.planner.calcite.FlinkContext;\n+import org.apache.flink.table.planner.calcite.FlinkTypeFactory;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+import org.apache.flink.table.planner.plan.utils.FlinkRelOptUtil;\n+import org.apache.flink.table.planner.plan.utils.PartitionPruner;\n+import org.apache.flink.table.planner.plan.utils.RexNodeExtractor;\n+import org.apache.flink.table.types.logical.LogicalType;\n+\n+import org.apache.calcite.plan.Context;\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Filter;\n+import org.apache.calcite.rel.logical.LogicalTableScan;\n+import org.apache.calcite.rel.type.RelDataType;\n+import org.apache.calcite.rex.RexBuilder;\n+import org.apache.calcite.rex.RexInputRef;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.calcite.rex.RexShuttle;\n+import org.apache.calcite.rex.RexUtil;\n+import org.apache.calcite.tools.RelBuilder;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.Seq;\n+\n+/**\n+ * Planner rule that tries to push partition evaluated by filter condition into a {@link LogicalTableScan}.\n+*/\n+public class PushPartitionIntoTableSourceScanRule extends RelOptRule {\n+\tpublic static final PushPartitionIntoTableSourceScanRule INSTANCE = new PushPartitionIntoTableSourceScanRule();\n+\n+\tpublic PushPartitionIntoTableSourceScanRule(){\n+\t\tsuper(operand(Filter.class,\n+\t\t\t\toperand(LogicalTableScan.class, none())),\n+\t\t\t\"PushPartitionTableSourceScanRule\");\n+\t}\n+\n+\t@Override\n+\tpublic boolean matches(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tif (filter.getCondition() == null) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tTableSourceTable tableSourceTable = call.rel(1).getTable().unwrap(TableSourceTable.class);\n+\t\tif (tableSourceTable == null){\n+\t\t\treturn false;\n+\t\t}\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource();\n+\t\tif (!(dynamicTableSource instanceof SupportsPartitionPushDown)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0f19df4d1fcbb4093b69d772628b67b81ebb443a"}, "originalPosition": 78}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg3NzIzNTQ3OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QxMzo0NzoyOFrOG3jN4A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QxMzo0NzoyOFrOG3jN4A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDkwMTg1Ng==", "bodyText": "listPartitions  is a very heavy operation, the onMatch method will also use the partition values. so it's better we get the result once. We can remove the validation whether the partitions is present.", "url": "https://github.com/apache/flink/pull/12966#discussion_r460901856", "createdAt": "2020-07-27T13:47:28Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsPartitionPushDown;\n+import org.apache.flink.table.planner.calcite.FlinkContext;\n+import org.apache.flink.table.planner.calcite.FlinkTypeFactory;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+import org.apache.flink.table.planner.plan.utils.FlinkRelOptUtil;\n+import org.apache.flink.table.planner.plan.utils.PartitionPruner;\n+import org.apache.flink.table.planner.plan.utils.RexNodeExtractor;\n+import org.apache.flink.table.types.logical.LogicalType;\n+\n+import org.apache.calcite.plan.Context;\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Filter;\n+import org.apache.calcite.rel.logical.LogicalTableScan;\n+import org.apache.calcite.rel.type.RelDataType;\n+import org.apache.calcite.rex.RexBuilder;\n+import org.apache.calcite.rex.RexInputRef;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.calcite.rex.RexShuttle;\n+import org.apache.calcite.rex.RexUtil;\n+import org.apache.calcite.tools.RelBuilder;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.Seq;\n+\n+/**\n+ * Planner rule that tries to push partition evaluated by filter condition into a {@link LogicalTableScan}.\n+*/\n+public class PushPartitionIntoTableSourceScanRule extends RelOptRule {\n+\tpublic static final PushPartitionIntoTableSourceScanRule INSTANCE = new PushPartitionIntoTableSourceScanRule();\n+\n+\tpublic PushPartitionIntoTableSourceScanRule(){\n+\t\tsuper(operand(Filter.class,\n+\t\t\t\toperand(LogicalTableScan.class, none())),\n+\t\t\t\"PushPartitionTableSourceScanRule\");\n+\t}\n+\n+\t@Override\n+\tpublic boolean matches(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tif (filter.getCondition() == null) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tTableSourceTable tableSourceTable = call.rel(1).getTable().unwrap(TableSourceTable.class);\n+\t\tif (tableSourceTable == null){\n+\t\t\treturn false;\n+\t\t}\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource();\n+\t\tif (!(dynamicTableSource instanceof SupportsPartitionPushDown)) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tOptional<List<Map<String, String>>> partitions = ((SupportsPartitionPushDown) dynamicTableSource).listPartitions();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0f19df4d1fcbb4093b69d772628b67b81ebb443a"}, "originalPosition": 81}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg3NzI0Mjc2OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QxMzo0OToxMFrOG3jSaA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QxMzo0OToxMFrOG3jSaA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDkwMzAxNg==", "bodyText": "nit: replace argument of toArray with empty array", "url": "https://github.com/apache/flink/pull/12966#discussion_r460903016", "createdAt": "2020-07-27T13:49:10Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsPartitionPushDown;\n+import org.apache.flink.table.planner.calcite.FlinkContext;\n+import org.apache.flink.table.planner.calcite.FlinkTypeFactory;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+import org.apache.flink.table.planner.plan.utils.FlinkRelOptUtil;\n+import org.apache.flink.table.planner.plan.utils.PartitionPruner;\n+import org.apache.flink.table.planner.plan.utils.RexNodeExtractor;\n+import org.apache.flink.table.types.logical.LogicalType;\n+\n+import org.apache.calcite.plan.Context;\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Filter;\n+import org.apache.calcite.rel.logical.LogicalTableScan;\n+import org.apache.calcite.rel.type.RelDataType;\n+import org.apache.calcite.rex.RexBuilder;\n+import org.apache.calcite.rex.RexInputRef;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.calcite.rex.RexShuttle;\n+import org.apache.calcite.rex.RexUtil;\n+import org.apache.calcite.tools.RelBuilder;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.Seq;\n+\n+/**\n+ * Planner rule that tries to push partition evaluated by filter condition into a {@link LogicalTableScan}.\n+*/\n+public class PushPartitionIntoTableSourceScanRule extends RelOptRule {\n+\tpublic static final PushPartitionIntoTableSourceScanRule INSTANCE = new PushPartitionIntoTableSourceScanRule();\n+\n+\tpublic PushPartitionIntoTableSourceScanRule(){\n+\t\tsuper(operand(Filter.class,\n+\t\t\t\toperand(LogicalTableScan.class, none())),\n+\t\t\t\"PushPartitionTableSourceScanRule\");\n+\t}\n+\n+\t@Override\n+\tpublic boolean matches(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tif (filter.getCondition() == null) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tTableSourceTable tableSourceTable = call.rel(1).getTable().unwrap(TableSourceTable.class);\n+\t\tif (tableSourceTable == null){\n+\t\t\treturn false;\n+\t\t}\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource();\n+\t\tif (!(dynamicTableSource instanceof SupportsPartitionPushDown)) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tOptional<List<Map<String, String>>> partitions = ((SupportsPartitionPushDown) dynamicTableSource).listPartitions();\n+\t\treturn partitions.isPresent()\n+\t\t\t&& !partitions.get().isEmpty()\n+\t\t\t&& !Arrays.stream(tableSourceTable.extraDigests()).anyMatch(digest -> digest.startsWith(\"source: [partitions=\"));\n+\t}\n+\n+\t@Override\n+\tpublic void onMatch(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tLogicalTableScan scan = call.rel(1);\n+\t\tContext context = call.getPlanner().getContext().unwrap(FlinkContext.class);\n+\t\tTableSourceTable tableSourceTable = scan.getTable().unwrap(TableSourceTable.class);\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource();\n+\t\tRelDataType inputFieldType = filter.getInput().getRowType();\n+\t\tList<String> inputFieldName = inputFieldType.getFieldNames();\n+\n+\t\tList<String> partitionedFieldNames = tableSourceTable.catalogTable().getPartitionKeys();\n+\n+\t\tRelBuilder relBuilder = call.builder();\n+\t\tRexBuilder rexBuilder = relBuilder.getRexBuilder();\n+\n+\t\tTuple2<Seq<RexNode>, Seq<RexNode>> predicate = RexNodeExtractor.extractPartitionPredicateList(\n+\t\t\tfilter.getCondition(),\n+\t\t\tFlinkRelOptUtil.getMaxCnfNodeCount(scan),\n+\t\t\tinputFieldName.toArray(new String[inputFieldName.size()]),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0f19df4d1fcbb4093b69d772628b67b81ebb443a"}, "originalPosition": 105}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg3NzI0NTA0OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QxMzo0OTozOVrOG3jT0Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QxMzo0OTozOVrOG3jT0Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDkwMzM3Nw==", "bodyText": "ditto", "url": "https://github.com/apache/flink/pull/12966#discussion_r460903377", "createdAt": "2020-07-27T13:49:39Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsPartitionPushDown;\n+import org.apache.flink.table.planner.calcite.FlinkContext;\n+import org.apache.flink.table.planner.calcite.FlinkTypeFactory;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+import org.apache.flink.table.planner.plan.utils.FlinkRelOptUtil;\n+import org.apache.flink.table.planner.plan.utils.PartitionPruner;\n+import org.apache.flink.table.planner.plan.utils.RexNodeExtractor;\n+import org.apache.flink.table.types.logical.LogicalType;\n+\n+import org.apache.calcite.plan.Context;\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Filter;\n+import org.apache.calcite.rel.logical.LogicalTableScan;\n+import org.apache.calcite.rel.type.RelDataType;\n+import org.apache.calcite.rex.RexBuilder;\n+import org.apache.calcite.rex.RexInputRef;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.calcite.rex.RexShuttle;\n+import org.apache.calcite.rex.RexUtil;\n+import org.apache.calcite.tools.RelBuilder;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.Seq;\n+\n+/**\n+ * Planner rule that tries to push partition evaluated by filter condition into a {@link LogicalTableScan}.\n+*/\n+public class PushPartitionIntoTableSourceScanRule extends RelOptRule {\n+\tpublic static final PushPartitionIntoTableSourceScanRule INSTANCE = new PushPartitionIntoTableSourceScanRule();\n+\n+\tpublic PushPartitionIntoTableSourceScanRule(){\n+\t\tsuper(operand(Filter.class,\n+\t\t\t\toperand(LogicalTableScan.class, none())),\n+\t\t\t\"PushPartitionTableSourceScanRule\");\n+\t}\n+\n+\t@Override\n+\tpublic boolean matches(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tif (filter.getCondition() == null) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tTableSourceTable tableSourceTable = call.rel(1).getTable().unwrap(TableSourceTable.class);\n+\t\tif (tableSourceTable == null){\n+\t\t\treturn false;\n+\t\t}\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource();\n+\t\tif (!(dynamicTableSource instanceof SupportsPartitionPushDown)) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tOptional<List<Map<String, String>>> partitions = ((SupportsPartitionPushDown) dynamicTableSource).listPartitions();\n+\t\treturn partitions.isPresent()\n+\t\t\t&& !partitions.get().isEmpty()\n+\t\t\t&& !Arrays.stream(tableSourceTable.extraDigests()).anyMatch(digest -> digest.startsWith(\"source: [partitions=\"));\n+\t}\n+\n+\t@Override\n+\tpublic void onMatch(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tLogicalTableScan scan = call.rel(1);\n+\t\tContext context = call.getPlanner().getContext().unwrap(FlinkContext.class);\n+\t\tTableSourceTable tableSourceTable = scan.getTable().unwrap(TableSourceTable.class);\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource();\n+\t\tRelDataType inputFieldType = filter.getInput().getRowType();\n+\t\tList<String> inputFieldName = inputFieldType.getFieldNames();\n+\n+\t\tList<String> partitionedFieldNames = tableSourceTable.catalogTable().getPartitionKeys();\n+\n+\t\tRelBuilder relBuilder = call.builder();\n+\t\tRexBuilder rexBuilder = relBuilder.getRexBuilder();\n+\n+\t\tTuple2<Seq<RexNode>, Seq<RexNode>> predicate = RexNodeExtractor.extractPartitionPredicateList(\n+\t\t\tfilter.getCondition(),\n+\t\t\tFlinkRelOptUtil.getMaxCnfNodeCount(scan),\n+\t\t\tinputFieldName.toArray(new String[inputFieldName.size()]),\n+\t\t\trexBuilder,\n+\t\t\tpartitionedFieldNames.toArray(new String[partitionedFieldNames.size()])", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0f19df4d1fcbb4093b69d772628b67b81ebb443a"}, "originalPosition": 107}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg3NzI5MjA1OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QxMzo1OTo0OFrOG3jwng==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QxMzo1OTo0OFrOG3jwng==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDkxMDc1MA==", "bodyText": "rename partitionFieldType to partitionFieldTypes ?\nchange the type definition to LogicalType[]? since prunePartitions method use Array[LogicalType] as argument type.", "url": "https://github.com/apache/flink/pull/12966#discussion_r460910750", "createdAt": "2020-07-27T13:59:48Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsPartitionPushDown;\n+import org.apache.flink.table.planner.calcite.FlinkContext;\n+import org.apache.flink.table.planner.calcite.FlinkTypeFactory;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+import org.apache.flink.table.planner.plan.utils.FlinkRelOptUtil;\n+import org.apache.flink.table.planner.plan.utils.PartitionPruner;\n+import org.apache.flink.table.planner.plan.utils.RexNodeExtractor;\n+import org.apache.flink.table.types.logical.LogicalType;\n+\n+import org.apache.calcite.plan.Context;\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Filter;\n+import org.apache.calcite.rel.logical.LogicalTableScan;\n+import org.apache.calcite.rel.type.RelDataType;\n+import org.apache.calcite.rex.RexBuilder;\n+import org.apache.calcite.rex.RexInputRef;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.calcite.rex.RexShuttle;\n+import org.apache.calcite.rex.RexUtil;\n+import org.apache.calcite.tools.RelBuilder;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.Seq;\n+\n+/**\n+ * Planner rule that tries to push partition evaluated by filter condition into a {@link LogicalTableScan}.\n+*/\n+public class PushPartitionIntoTableSourceScanRule extends RelOptRule {\n+\tpublic static final PushPartitionIntoTableSourceScanRule INSTANCE = new PushPartitionIntoTableSourceScanRule();\n+\n+\tpublic PushPartitionIntoTableSourceScanRule(){\n+\t\tsuper(operand(Filter.class,\n+\t\t\t\toperand(LogicalTableScan.class, none())),\n+\t\t\t\"PushPartitionTableSourceScanRule\");\n+\t}\n+\n+\t@Override\n+\tpublic boolean matches(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tif (filter.getCondition() == null) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tTableSourceTable tableSourceTable = call.rel(1).getTable().unwrap(TableSourceTable.class);\n+\t\tif (tableSourceTable == null){\n+\t\t\treturn false;\n+\t\t}\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource();\n+\t\tif (!(dynamicTableSource instanceof SupportsPartitionPushDown)) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tOptional<List<Map<String, String>>> partitions = ((SupportsPartitionPushDown) dynamicTableSource).listPartitions();\n+\t\treturn partitions.isPresent()\n+\t\t\t&& !partitions.get().isEmpty()\n+\t\t\t&& !Arrays.stream(tableSourceTable.extraDigests()).anyMatch(digest -> digest.startsWith(\"source: [partitions=\"));\n+\t}\n+\n+\t@Override\n+\tpublic void onMatch(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tLogicalTableScan scan = call.rel(1);\n+\t\tContext context = call.getPlanner().getContext().unwrap(FlinkContext.class);\n+\t\tTableSourceTable tableSourceTable = scan.getTable().unwrap(TableSourceTable.class);\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource();\n+\t\tRelDataType inputFieldType = filter.getInput().getRowType();\n+\t\tList<String> inputFieldName = inputFieldType.getFieldNames();\n+\n+\t\tList<String> partitionedFieldNames = tableSourceTable.catalogTable().getPartitionKeys();\n+\n+\t\tRelBuilder relBuilder = call.builder();\n+\t\tRexBuilder rexBuilder = relBuilder.getRexBuilder();\n+\n+\t\tTuple2<Seq<RexNode>, Seq<RexNode>> predicate = RexNodeExtractor.extractPartitionPredicateList(\n+\t\t\tfilter.getCondition(),\n+\t\t\tFlinkRelOptUtil.getMaxCnfNodeCount(scan),\n+\t\t\tinputFieldName.toArray(new String[inputFieldName.size()]),\n+\t\t\trexBuilder,\n+\t\t\tpartitionedFieldNames.toArray(new String[partitionedFieldNames.size()])\n+\t\t\t);\n+\n+\t\tRexNode partitionPredicate = RexUtil.composeConjunction(rexBuilder, JavaConversions.seqAsJavaList(predicate._1));\n+\n+\t\tif (partitionPredicate.isAlwaysTrue()){\n+\t\t\treturn;\n+\t\t}\n+\n+\t\tList<LogicalType> partitionFieldType = partitionedFieldNames.stream().map(name -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0f19df4d1fcbb4093b69d772628b67b81ebb443a"}, "originalPosition": 116}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg3NzI5NTA4OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QxNDowMDoyOFrOG3jycA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QxNDowMDoyOFrOG3jycA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDkxMTIxNg==", "bodyText": "Context context -> FlinkContext context", "url": "https://github.com/apache/flink/pull/12966#discussion_r460911216", "createdAt": "2020-07-27T14:00:28Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsPartitionPushDown;\n+import org.apache.flink.table.planner.calcite.FlinkContext;\n+import org.apache.flink.table.planner.calcite.FlinkTypeFactory;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+import org.apache.flink.table.planner.plan.utils.FlinkRelOptUtil;\n+import org.apache.flink.table.planner.plan.utils.PartitionPruner;\n+import org.apache.flink.table.planner.plan.utils.RexNodeExtractor;\n+import org.apache.flink.table.types.logical.LogicalType;\n+\n+import org.apache.calcite.plan.Context;\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Filter;\n+import org.apache.calcite.rel.logical.LogicalTableScan;\n+import org.apache.calcite.rel.type.RelDataType;\n+import org.apache.calcite.rex.RexBuilder;\n+import org.apache.calcite.rex.RexInputRef;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.calcite.rex.RexShuttle;\n+import org.apache.calcite.rex.RexUtil;\n+import org.apache.calcite.tools.RelBuilder;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.Seq;\n+\n+/**\n+ * Planner rule that tries to push partition evaluated by filter condition into a {@link LogicalTableScan}.\n+*/\n+public class PushPartitionIntoTableSourceScanRule extends RelOptRule {\n+\tpublic static final PushPartitionIntoTableSourceScanRule INSTANCE = new PushPartitionIntoTableSourceScanRule();\n+\n+\tpublic PushPartitionIntoTableSourceScanRule(){\n+\t\tsuper(operand(Filter.class,\n+\t\t\t\toperand(LogicalTableScan.class, none())),\n+\t\t\t\"PushPartitionTableSourceScanRule\");\n+\t}\n+\n+\t@Override\n+\tpublic boolean matches(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tif (filter.getCondition() == null) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tTableSourceTable tableSourceTable = call.rel(1).getTable().unwrap(TableSourceTable.class);\n+\t\tif (tableSourceTable == null){\n+\t\t\treturn false;\n+\t\t}\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource();\n+\t\tif (!(dynamicTableSource instanceof SupportsPartitionPushDown)) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tOptional<List<Map<String, String>>> partitions = ((SupportsPartitionPushDown) dynamicTableSource).listPartitions();\n+\t\treturn partitions.isPresent()\n+\t\t\t&& !partitions.get().isEmpty()\n+\t\t\t&& !Arrays.stream(tableSourceTable.extraDigests()).anyMatch(digest -> digest.startsWith(\"source: [partitions=\"));\n+\t}\n+\n+\t@Override\n+\tpublic void onMatch(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tLogicalTableScan scan = call.rel(1);\n+\t\tContext context = call.getPlanner().getContext().unwrap(FlinkContext.class);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0f19df4d1fcbb4093b69d772628b67b81ebb443a"}, "originalPosition": 91}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg3NzI5NTg5OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QxNDowMDozOFrOG3jy7w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QxNDowMDozOFrOG3jy7w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDkxMTM0Mw==", "bodyText": "remove the cast", "url": "https://github.com/apache/flink/pull/12966#discussion_r460911343", "createdAt": "2020-07-27T14:00:38Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsPartitionPushDown;\n+import org.apache.flink.table.planner.calcite.FlinkContext;\n+import org.apache.flink.table.planner.calcite.FlinkTypeFactory;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+import org.apache.flink.table.planner.plan.utils.FlinkRelOptUtil;\n+import org.apache.flink.table.planner.plan.utils.PartitionPruner;\n+import org.apache.flink.table.planner.plan.utils.RexNodeExtractor;\n+import org.apache.flink.table.types.logical.LogicalType;\n+\n+import org.apache.calcite.plan.Context;\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Filter;\n+import org.apache.calcite.rel.logical.LogicalTableScan;\n+import org.apache.calcite.rel.type.RelDataType;\n+import org.apache.calcite.rex.RexBuilder;\n+import org.apache.calcite.rex.RexInputRef;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.calcite.rex.RexShuttle;\n+import org.apache.calcite.rex.RexUtil;\n+import org.apache.calcite.tools.RelBuilder;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.Seq;\n+\n+/**\n+ * Planner rule that tries to push partition evaluated by filter condition into a {@link LogicalTableScan}.\n+*/\n+public class PushPartitionIntoTableSourceScanRule extends RelOptRule {\n+\tpublic static final PushPartitionIntoTableSourceScanRule INSTANCE = new PushPartitionIntoTableSourceScanRule();\n+\n+\tpublic PushPartitionIntoTableSourceScanRule(){\n+\t\tsuper(operand(Filter.class,\n+\t\t\t\toperand(LogicalTableScan.class, none())),\n+\t\t\t\"PushPartitionTableSourceScanRule\");\n+\t}\n+\n+\t@Override\n+\tpublic boolean matches(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tif (filter.getCondition() == null) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tTableSourceTable tableSourceTable = call.rel(1).getTable().unwrap(TableSourceTable.class);\n+\t\tif (tableSourceTable == null){\n+\t\t\treturn false;\n+\t\t}\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource();\n+\t\tif (!(dynamicTableSource instanceof SupportsPartitionPushDown)) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tOptional<List<Map<String, String>>> partitions = ((SupportsPartitionPushDown) dynamicTableSource).listPartitions();\n+\t\treturn partitions.isPresent()\n+\t\t\t&& !partitions.get().isEmpty()\n+\t\t\t&& !Arrays.stream(tableSourceTable.extraDigests()).anyMatch(digest -> digest.startsWith(\"source: [partitions=\"));\n+\t}\n+\n+\t@Override\n+\tpublic void onMatch(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tLogicalTableScan scan = call.rel(1);\n+\t\tContext context = call.getPlanner().getContext().unwrap(FlinkContext.class);\n+\t\tTableSourceTable tableSourceTable = scan.getTable().unwrap(TableSourceTable.class);\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource();\n+\t\tRelDataType inputFieldType = filter.getInput().getRowType();\n+\t\tList<String> inputFieldName = inputFieldType.getFieldNames();\n+\n+\t\tList<String> partitionedFieldNames = tableSourceTable.catalogTable().getPartitionKeys();\n+\n+\t\tRelBuilder relBuilder = call.builder();\n+\t\tRexBuilder rexBuilder = relBuilder.getRexBuilder();\n+\n+\t\tTuple2<Seq<RexNode>, Seq<RexNode>> predicate = RexNodeExtractor.extractPartitionPredicateList(\n+\t\t\tfilter.getCondition(),\n+\t\t\tFlinkRelOptUtil.getMaxCnfNodeCount(scan),\n+\t\t\tinputFieldName.toArray(new String[inputFieldName.size()]),\n+\t\t\trexBuilder,\n+\t\t\tpartitionedFieldNames.toArray(new String[partitionedFieldNames.size()])\n+\t\t\t);\n+\n+\t\tRexNode partitionPredicate = RexUtil.composeConjunction(rexBuilder, JavaConversions.seqAsJavaList(predicate._1));\n+\n+\t\tif (partitionPredicate.isAlwaysTrue()){\n+\t\t\treturn;\n+\t\t}\n+\n+\t\tList<LogicalType> partitionFieldType = partitionedFieldNames.stream().map(name -> {\n+\t\t\tint index = inputFieldName.indexOf(name);\n+\t\t\tif (index < 0) {\n+\t\t\t\tthrow new RuntimeException(String.format(\"Partitioned key '%s' isn't found in input columns. \" +\n+\t\t\t\t\t\"Validator should have checked that.\", name));\n+\t\t\t}\n+\t\t\treturn inputFieldType.getFieldList().get(index).getType(); })\n+\t\t\t.map(FlinkTypeFactory::toLogicalType).collect(Collectors.toList());\n+\n+\t\t// checked in matches(call) and listPartitions() is not empty.\n+\t\tList<Map<String, String>> allPartitions = ((SupportsPartitionPushDown) dynamicTableSource).listPartitions().get();\n+\t\tRexNode finalPartitionPredicate = adjustPartitionPredicate(inputFieldName, partitionedFieldNames, partitionPredicate);\n+\t\t// get partitions\n+\t\tList<Map<String, String>> remainingPartitions = PartitionPruner.prunePartitions(\n+\t\t\t((FlinkContext) context).getTableConfig(),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0f19df4d1fcbb4093b69d772628b67b81ebb443a"}, "originalPosition": 130}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg3NzI5ODU3OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QxNDowMToxOVrOG3j0qw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QxNDowMToxOVrOG3j0qw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDkxMTc4Nw==", "bodyText": "nit: move to the last line?", "url": "https://github.com/apache/flink/pull/12966#discussion_r460911787", "createdAt": "2020-07-27T14:01:19Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsPartitionPushDown;\n+import org.apache.flink.table.planner.calcite.FlinkContext;\n+import org.apache.flink.table.planner.calcite.FlinkTypeFactory;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+import org.apache.flink.table.planner.plan.utils.FlinkRelOptUtil;\n+import org.apache.flink.table.planner.plan.utils.PartitionPruner;\n+import org.apache.flink.table.planner.plan.utils.RexNodeExtractor;\n+import org.apache.flink.table.types.logical.LogicalType;\n+\n+import org.apache.calcite.plan.Context;\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Filter;\n+import org.apache.calcite.rel.logical.LogicalTableScan;\n+import org.apache.calcite.rel.type.RelDataType;\n+import org.apache.calcite.rex.RexBuilder;\n+import org.apache.calcite.rex.RexInputRef;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.calcite.rex.RexShuttle;\n+import org.apache.calcite.rex.RexUtil;\n+import org.apache.calcite.tools.RelBuilder;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.Seq;\n+\n+/**\n+ * Planner rule that tries to push partition evaluated by filter condition into a {@link LogicalTableScan}.\n+*/\n+public class PushPartitionIntoTableSourceScanRule extends RelOptRule {\n+\tpublic static final PushPartitionIntoTableSourceScanRule INSTANCE = new PushPartitionIntoTableSourceScanRule();\n+\n+\tpublic PushPartitionIntoTableSourceScanRule(){\n+\t\tsuper(operand(Filter.class,\n+\t\t\t\toperand(LogicalTableScan.class, none())),\n+\t\t\t\"PushPartitionTableSourceScanRule\");\n+\t}\n+\n+\t@Override\n+\tpublic boolean matches(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tif (filter.getCondition() == null) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tTableSourceTable tableSourceTable = call.rel(1).getTable().unwrap(TableSourceTable.class);\n+\t\tif (tableSourceTable == null){\n+\t\t\treturn false;\n+\t\t}\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource();\n+\t\tif (!(dynamicTableSource instanceof SupportsPartitionPushDown)) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tOptional<List<Map<String, String>>> partitions = ((SupportsPartitionPushDown) dynamicTableSource).listPartitions();\n+\t\treturn partitions.isPresent()\n+\t\t\t&& !partitions.get().isEmpty()\n+\t\t\t&& !Arrays.stream(tableSourceTable.extraDigests()).anyMatch(digest -> digest.startsWith(\"source: [partitions=\"));\n+\t}\n+\n+\t@Override\n+\tpublic void onMatch(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tLogicalTableScan scan = call.rel(1);\n+\t\tContext context = call.getPlanner().getContext().unwrap(FlinkContext.class);\n+\t\tTableSourceTable tableSourceTable = scan.getTable().unwrap(TableSourceTable.class);\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource();\n+\t\tRelDataType inputFieldType = filter.getInput().getRowType();\n+\t\tList<String> inputFieldName = inputFieldType.getFieldNames();\n+\n+\t\tList<String> partitionedFieldNames = tableSourceTable.catalogTable().getPartitionKeys();\n+\n+\t\tRelBuilder relBuilder = call.builder();\n+\t\tRexBuilder rexBuilder = relBuilder.getRexBuilder();\n+\n+\t\tTuple2<Seq<RexNode>, Seq<RexNode>> predicate = RexNodeExtractor.extractPartitionPredicateList(\n+\t\t\tfilter.getCondition(),\n+\t\t\tFlinkRelOptUtil.getMaxCnfNodeCount(scan),\n+\t\t\tinputFieldName.toArray(new String[inputFieldName.size()]),\n+\t\t\trexBuilder,\n+\t\t\tpartitionedFieldNames.toArray(new String[partitionedFieldNames.size()])\n+\t\t\t);\n+\n+\t\tRexNode partitionPredicate = RexUtil.composeConjunction(rexBuilder, JavaConversions.seqAsJavaList(predicate._1));\n+\n+\t\tif (partitionPredicate.isAlwaysTrue()){\n+\t\t\treturn;\n+\t\t}\n+\n+\t\tList<LogicalType> partitionFieldType = partitionedFieldNames.stream().map(name -> {\n+\t\t\tint index = inputFieldName.indexOf(name);\n+\t\t\tif (index < 0) {\n+\t\t\t\tthrow new RuntimeException(String.format(\"Partitioned key '%s' isn't found in input columns. \" +\n+\t\t\t\t\t\"Validator should have checked that.\", name));\n+\t\t\t}\n+\t\t\treturn inputFieldType.getFieldList().get(index).getType(); })\n+\t\t\t.map(FlinkTypeFactory::toLogicalType).collect(Collectors.toList());\n+\n+\t\t// checked in matches(call) and listPartitions() is not empty.\n+\t\tList<Map<String, String>> allPartitions = ((SupportsPartitionPushDown) dynamicTableSource).listPartitions().get();\n+\t\tRexNode finalPartitionPredicate = adjustPartitionPredicate(inputFieldName, partitionedFieldNames, partitionPredicate);\n+\t\t// get partitions\n+\t\tList<Map<String, String>> remainingPartitions = PartitionPruner.prunePartitions(\n+\t\t\t((FlinkContext) context).getTableConfig(),\n+\t\t\tpartitionedFieldNames.toArray(new String[partitionedFieldNames.size()]),\n+\t\t\tpartitionFieldType.toArray(new LogicalType[partitionFieldType.size()]),\n+\t\t\tallPartitions,\n+\t\t\tfinalPartitionPredicate\n+\t\t\t);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0f19df4d1fcbb4093b69d772628b67b81ebb443a"}, "originalPosition": 135}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg3NzMwNjcwOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QxNDowMzoxMlrOG3j5lA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QxNDowMzoxMlrOG3j5lA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDkxMzA0NA==", "bodyText": "please throw TableException instead", "url": "https://github.com/apache/flink/pull/12966#discussion_r460913044", "createdAt": "2020-07-27T14:03:12Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsPartitionPushDown;\n+import org.apache.flink.table.planner.calcite.FlinkContext;\n+import org.apache.flink.table.planner.calcite.FlinkTypeFactory;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+import org.apache.flink.table.planner.plan.utils.FlinkRelOptUtil;\n+import org.apache.flink.table.planner.plan.utils.PartitionPruner;\n+import org.apache.flink.table.planner.plan.utils.RexNodeExtractor;\n+import org.apache.flink.table.types.logical.LogicalType;\n+\n+import org.apache.calcite.plan.Context;\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Filter;\n+import org.apache.calcite.rel.logical.LogicalTableScan;\n+import org.apache.calcite.rel.type.RelDataType;\n+import org.apache.calcite.rex.RexBuilder;\n+import org.apache.calcite.rex.RexInputRef;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.calcite.rex.RexShuttle;\n+import org.apache.calcite.rex.RexUtil;\n+import org.apache.calcite.tools.RelBuilder;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.Seq;\n+\n+/**\n+ * Planner rule that tries to push partition evaluated by filter condition into a {@link LogicalTableScan}.\n+*/\n+public class PushPartitionIntoTableSourceScanRule extends RelOptRule {\n+\tpublic static final PushPartitionIntoTableSourceScanRule INSTANCE = new PushPartitionIntoTableSourceScanRule();\n+\n+\tpublic PushPartitionIntoTableSourceScanRule(){\n+\t\tsuper(operand(Filter.class,\n+\t\t\t\toperand(LogicalTableScan.class, none())),\n+\t\t\t\"PushPartitionTableSourceScanRule\");\n+\t}\n+\n+\t@Override\n+\tpublic boolean matches(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tif (filter.getCondition() == null) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tTableSourceTable tableSourceTable = call.rel(1).getTable().unwrap(TableSourceTable.class);\n+\t\tif (tableSourceTable == null){\n+\t\t\treturn false;\n+\t\t}\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource();\n+\t\tif (!(dynamicTableSource instanceof SupportsPartitionPushDown)) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tOptional<List<Map<String, String>>> partitions = ((SupportsPartitionPushDown) dynamicTableSource).listPartitions();\n+\t\treturn partitions.isPresent()\n+\t\t\t&& !partitions.get().isEmpty()\n+\t\t\t&& !Arrays.stream(tableSourceTable.extraDigests()).anyMatch(digest -> digest.startsWith(\"source: [partitions=\"));\n+\t}\n+\n+\t@Override\n+\tpublic void onMatch(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tLogicalTableScan scan = call.rel(1);\n+\t\tContext context = call.getPlanner().getContext().unwrap(FlinkContext.class);\n+\t\tTableSourceTable tableSourceTable = scan.getTable().unwrap(TableSourceTable.class);\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource();\n+\t\tRelDataType inputFieldType = filter.getInput().getRowType();\n+\t\tList<String> inputFieldName = inputFieldType.getFieldNames();\n+\n+\t\tList<String> partitionedFieldNames = tableSourceTable.catalogTable().getPartitionKeys();\n+\n+\t\tRelBuilder relBuilder = call.builder();\n+\t\tRexBuilder rexBuilder = relBuilder.getRexBuilder();\n+\n+\t\tTuple2<Seq<RexNode>, Seq<RexNode>> predicate = RexNodeExtractor.extractPartitionPredicateList(\n+\t\t\tfilter.getCondition(),\n+\t\t\tFlinkRelOptUtil.getMaxCnfNodeCount(scan),\n+\t\t\tinputFieldName.toArray(new String[inputFieldName.size()]),\n+\t\t\trexBuilder,\n+\t\t\tpartitionedFieldNames.toArray(new String[partitionedFieldNames.size()])\n+\t\t\t);\n+\n+\t\tRexNode partitionPredicate = RexUtil.composeConjunction(rexBuilder, JavaConversions.seqAsJavaList(predicate._1));\n+\n+\t\tif (partitionPredicate.isAlwaysTrue()){\n+\t\t\treturn;\n+\t\t}\n+\n+\t\tList<LogicalType> partitionFieldType = partitionedFieldNames.stream().map(name -> {\n+\t\t\tint index = inputFieldName.indexOf(name);\n+\t\t\tif (index < 0) {\n+\t\t\t\tthrow new RuntimeException(String.format(\"Partitioned key '%s' isn't found in input columns. \" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0f19df4d1fcbb4093b69d772628b67b81ebb443a"}, "originalPosition": 119}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg3NzMxMDc4OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QxNDowNDowNVrOG3j8AA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QxNDowNDowNVrOG3j8AA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDkxMzY2NA==", "bodyText": "reuse the validation logic with line 117-120 ?", "url": "https://github.com/apache/flink/pull/12966#discussion_r460913664", "createdAt": "2020-07-27T14:04:05Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsPartitionPushDown;\n+import org.apache.flink.table.planner.calcite.FlinkContext;\n+import org.apache.flink.table.planner.calcite.FlinkTypeFactory;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+import org.apache.flink.table.planner.plan.utils.FlinkRelOptUtil;\n+import org.apache.flink.table.planner.plan.utils.PartitionPruner;\n+import org.apache.flink.table.planner.plan.utils.RexNodeExtractor;\n+import org.apache.flink.table.types.logical.LogicalType;\n+\n+import org.apache.calcite.plan.Context;\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Filter;\n+import org.apache.calcite.rel.logical.LogicalTableScan;\n+import org.apache.calcite.rel.type.RelDataType;\n+import org.apache.calcite.rex.RexBuilder;\n+import org.apache.calcite.rex.RexInputRef;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.calcite.rex.RexShuttle;\n+import org.apache.calcite.rex.RexUtil;\n+import org.apache.calcite.tools.RelBuilder;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.Seq;\n+\n+/**\n+ * Planner rule that tries to push partition evaluated by filter condition into a {@link LogicalTableScan}.\n+*/\n+public class PushPartitionIntoTableSourceScanRule extends RelOptRule {\n+\tpublic static final PushPartitionIntoTableSourceScanRule INSTANCE = new PushPartitionIntoTableSourceScanRule();\n+\n+\tpublic PushPartitionIntoTableSourceScanRule(){\n+\t\tsuper(operand(Filter.class,\n+\t\t\t\toperand(LogicalTableScan.class, none())),\n+\t\t\t\"PushPartitionTableSourceScanRule\");\n+\t}\n+\n+\t@Override\n+\tpublic boolean matches(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tif (filter.getCondition() == null) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tTableSourceTable tableSourceTable = call.rel(1).getTable().unwrap(TableSourceTable.class);\n+\t\tif (tableSourceTable == null){\n+\t\t\treturn false;\n+\t\t}\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource();\n+\t\tif (!(dynamicTableSource instanceof SupportsPartitionPushDown)) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tOptional<List<Map<String, String>>> partitions = ((SupportsPartitionPushDown) dynamicTableSource).listPartitions();\n+\t\treturn partitions.isPresent()\n+\t\t\t&& !partitions.get().isEmpty()\n+\t\t\t&& !Arrays.stream(tableSourceTable.extraDigests()).anyMatch(digest -> digest.startsWith(\"source: [partitions=\"));\n+\t}\n+\n+\t@Override\n+\tpublic void onMatch(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tLogicalTableScan scan = call.rel(1);\n+\t\tContext context = call.getPlanner().getContext().unwrap(FlinkContext.class);\n+\t\tTableSourceTable tableSourceTable = scan.getTable().unwrap(TableSourceTable.class);\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource();\n+\t\tRelDataType inputFieldType = filter.getInput().getRowType();\n+\t\tList<String> inputFieldName = inputFieldType.getFieldNames();\n+\n+\t\tList<String> partitionedFieldNames = tableSourceTable.catalogTable().getPartitionKeys();\n+\n+\t\tRelBuilder relBuilder = call.builder();\n+\t\tRexBuilder rexBuilder = relBuilder.getRexBuilder();\n+\n+\t\tTuple2<Seq<RexNode>, Seq<RexNode>> predicate = RexNodeExtractor.extractPartitionPredicateList(\n+\t\t\tfilter.getCondition(),\n+\t\t\tFlinkRelOptUtil.getMaxCnfNodeCount(scan),\n+\t\t\tinputFieldName.toArray(new String[inputFieldName.size()]),\n+\t\t\trexBuilder,\n+\t\t\tpartitionedFieldNames.toArray(new String[partitionedFieldNames.size()])\n+\t\t\t);\n+\n+\t\tRexNode partitionPredicate = RexUtil.composeConjunction(rexBuilder, JavaConversions.seqAsJavaList(predicate._1));\n+\n+\t\tif (partitionPredicate.isAlwaysTrue()){\n+\t\t\treturn;\n+\t\t}\n+\n+\t\tList<LogicalType> partitionFieldType = partitionedFieldNames.stream().map(name -> {\n+\t\t\tint index = inputFieldName.indexOf(name);\n+\t\t\tif (index < 0) {\n+\t\t\t\tthrow new RuntimeException(String.format(\"Partitioned key '%s' isn't found in input columns. \" +\n+\t\t\t\t\t\"Validator should have checked that.\", name));\n+\t\t\t}\n+\t\t\treturn inputFieldType.getFieldList().get(index).getType(); })\n+\t\t\t.map(FlinkTypeFactory::toLogicalType).collect(Collectors.toList());\n+\n+\t\t// checked in matches(call) and listPartitions() is not empty.\n+\t\tList<Map<String, String>> allPartitions = ((SupportsPartitionPushDown) dynamicTableSource).listPartitions().get();\n+\t\tRexNode finalPartitionPredicate = adjustPartitionPredicate(inputFieldName, partitionedFieldNames, partitionPredicate);\n+\t\t// get partitions\n+\t\tList<Map<String, String>> remainingPartitions = PartitionPruner.prunePartitions(\n+\t\t\t((FlinkContext) context).getTableConfig(),\n+\t\t\tpartitionedFieldNames.toArray(new String[partitionedFieldNames.size()]),\n+\t\t\tpartitionFieldType.toArray(new LogicalType[partitionFieldType.size()]),\n+\t\t\tallPartitions,\n+\t\t\tfinalPartitionPredicate\n+\t\t\t);\n+\t\t((SupportsPartitionPushDown) dynamicTableSource).applyPartitions(remainingPartitions);\n+\n+\t\t// build new table scan\n+\t\tFlinkStatistic statistic = tableSourceTable.getStatistic();\n+\t\tString extraDigest = \"source: [partitions=\" +\n+\t\t\tString.join(\", \", ((SupportsPartitionPushDown) dynamicTableSource).listPartitions()\n+\t\t\t\t.get()\n+\t\t\t\t.stream()\n+\t\t\t\t.map(partition -> partition.toString())\n+\t\t\t\t.collect(Collectors.toList())\n+\t\t\t\t.toArray(new String[1])) +\n+\t\t\t\"]\";\n+\t\tTableSourceTable newTableSourceTable = tableSourceTable.copy(dynamicTableSource, statistic, new String[]{extraDigest});\n+\t\tLogicalTableScan newScan = new LogicalTableScan(\n+\t\t\tscan.getCluster(), scan.getTraitSet(), scan.getHints(), newTableSourceTable);\n+\n+\t\tRexNode nonPartitionPredicate = RexUtil.composeConjunction(rexBuilder, JavaConversions.seqAsJavaList(predicate._2()));\n+\t\tif (nonPartitionPredicate.isAlwaysTrue()) {\n+\t\t\tcall.transformTo(newScan);\n+\t\t} else {\n+\t\t\tcall.transformTo(filter.copy(filter.getTraitSet(), newScan, nonPartitionPredicate));\n+\t\t}\n+\t}\n+\n+\tprivate RexNode adjustPartitionPredicate(List<String> inputFieldNames, List<String> partitionFieldNames, RexNode partitionPredicate) {\n+\t\treturn partitionPredicate.accept(new RexShuttle(){\n+\t\t\t@Override\n+\t\t\tpublic RexNode visitInputRef(RexInputRef inputRef) {\n+\t\t\t\tint index = inputRef.getIndex();\n+\t\t\t\tString fieldName = inputFieldNames.get(index);\n+\t\t\t\tint newIndex = partitionFieldNames.indexOf(fieldName);\n+\t\t\t\tif (newIndex < 0) {\n+\t\t\t\t\tthrow new RuntimeException(String.format(\"Field name '%s' isn't found in partitioned columns.\" +\n+\t\t\t\t\t\t\" Validator should have checked that.\", fieldName));\n+\t\t\t\t}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0f19df4d1fcbb4093b69d772628b67b81ebb443a"}, "originalPosition": 170}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg3NzMxODczOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QxNDowNTo0M1rOG3kAsg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QxNDowNTo0M1rOG3kAsg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDkxNDg2Ng==", "bodyText": "please copy the comments from PushPartitionIntoLegacyTableSourceScanRule#adjustPartitionPredicate", "url": "https://github.com/apache/flink/pull/12966#discussion_r460914866", "createdAt": "2020-07-27T14:05:43Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsPartitionPushDown;\n+import org.apache.flink.table.planner.calcite.FlinkContext;\n+import org.apache.flink.table.planner.calcite.FlinkTypeFactory;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+import org.apache.flink.table.planner.plan.utils.FlinkRelOptUtil;\n+import org.apache.flink.table.planner.plan.utils.PartitionPruner;\n+import org.apache.flink.table.planner.plan.utils.RexNodeExtractor;\n+import org.apache.flink.table.types.logical.LogicalType;\n+\n+import org.apache.calcite.plan.Context;\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Filter;\n+import org.apache.calcite.rel.logical.LogicalTableScan;\n+import org.apache.calcite.rel.type.RelDataType;\n+import org.apache.calcite.rex.RexBuilder;\n+import org.apache.calcite.rex.RexInputRef;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.calcite.rex.RexShuttle;\n+import org.apache.calcite.rex.RexUtil;\n+import org.apache.calcite.tools.RelBuilder;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.Seq;\n+\n+/**\n+ * Planner rule that tries to push partition evaluated by filter condition into a {@link LogicalTableScan}.\n+*/\n+public class PushPartitionIntoTableSourceScanRule extends RelOptRule {\n+\tpublic static final PushPartitionIntoTableSourceScanRule INSTANCE = new PushPartitionIntoTableSourceScanRule();\n+\n+\tpublic PushPartitionIntoTableSourceScanRule(){\n+\t\tsuper(operand(Filter.class,\n+\t\t\t\toperand(LogicalTableScan.class, none())),\n+\t\t\t\"PushPartitionTableSourceScanRule\");\n+\t}\n+\n+\t@Override\n+\tpublic boolean matches(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tif (filter.getCondition() == null) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tTableSourceTable tableSourceTable = call.rel(1).getTable().unwrap(TableSourceTable.class);\n+\t\tif (tableSourceTable == null){\n+\t\t\treturn false;\n+\t\t}\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource();\n+\t\tif (!(dynamicTableSource instanceof SupportsPartitionPushDown)) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tOptional<List<Map<String, String>>> partitions = ((SupportsPartitionPushDown) dynamicTableSource).listPartitions();\n+\t\treturn partitions.isPresent()\n+\t\t\t&& !partitions.get().isEmpty()\n+\t\t\t&& !Arrays.stream(tableSourceTable.extraDigests()).anyMatch(digest -> digest.startsWith(\"source: [partitions=\"));\n+\t}\n+\n+\t@Override\n+\tpublic void onMatch(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tLogicalTableScan scan = call.rel(1);\n+\t\tContext context = call.getPlanner().getContext().unwrap(FlinkContext.class);\n+\t\tTableSourceTable tableSourceTable = scan.getTable().unwrap(TableSourceTable.class);\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource();\n+\t\tRelDataType inputFieldType = filter.getInput().getRowType();\n+\t\tList<String> inputFieldName = inputFieldType.getFieldNames();\n+\n+\t\tList<String> partitionedFieldNames = tableSourceTable.catalogTable().getPartitionKeys();\n+\n+\t\tRelBuilder relBuilder = call.builder();\n+\t\tRexBuilder rexBuilder = relBuilder.getRexBuilder();\n+\n+\t\tTuple2<Seq<RexNode>, Seq<RexNode>> predicate = RexNodeExtractor.extractPartitionPredicateList(\n+\t\t\tfilter.getCondition(),\n+\t\t\tFlinkRelOptUtil.getMaxCnfNodeCount(scan),\n+\t\t\tinputFieldName.toArray(new String[inputFieldName.size()]),\n+\t\t\trexBuilder,\n+\t\t\tpartitionedFieldNames.toArray(new String[partitionedFieldNames.size()])\n+\t\t\t);\n+\n+\t\tRexNode partitionPredicate = RexUtil.composeConjunction(rexBuilder, JavaConversions.seqAsJavaList(predicate._1));\n+\n+\t\tif (partitionPredicate.isAlwaysTrue()){\n+\t\t\treturn;\n+\t\t}\n+\n+\t\tList<LogicalType> partitionFieldType = partitionedFieldNames.stream().map(name -> {\n+\t\t\tint index = inputFieldName.indexOf(name);\n+\t\t\tif (index < 0) {\n+\t\t\t\tthrow new RuntimeException(String.format(\"Partitioned key '%s' isn't found in input columns. \" +\n+\t\t\t\t\t\"Validator should have checked that.\", name));\n+\t\t\t}\n+\t\t\treturn inputFieldType.getFieldList().get(index).getType(); })\n+\t\t\t.map(FlinkTypeFactory::toLogicalType).collect(Collectors.toList());\n+\n+\t\t// checked in matches(call) and listPartitions() is not empty.\n+\t\tList<Map<String, String>> allPartitions = ((SupportsPartitionPushDown) dynamicTableSource).listPartitions().get();\n+\t\tRexNode finalPartitionPredicate = adjustPartitionPredicate(inputFieldName, partitionedFieldNames, partitionPredicate);\n+\t\t// get partitions\n+\t\tList<Map<String, String>> remainingPartitions = PartitionPruner.prunePartitions(\n+\t\t\t((FlinkContext) context).getTableConfig(),\n+\t\t\tpartitionedFieldNames.toArray(new String[partitionedFieldNames.size()]),\n+\t\t\tpartitionFieldType.toArray(new LogicalType[partitionFieldType.size()]),\n+\t\t\tallPartitions,\n+\t\t\tfinalPartitionPredicate\n+\t\t\t);\n+\t\t((SupportsPartitionPushDown) dynamicTableSource).applyPartitions(remainingPartitions);\n+\n+\t\t// build new table scan\n+\t\tFlinkStatistic statistic = tableSourceTable.getStatistic();\n+\t\tString extraDigest = \"source: [partitions=\" +\n+\t\t\tString.join(\", \", ((SupportsPartitionPushDown) dynamicTableSource).listPartitions()\n+\t\t\t\t.get()\n+\t\t\t\t.stream()\n+\t\t\t\t.map(partition -> partition.toString())\n+\t\t\t\t.collect(Collectors.toList())\n+\t\t\t\t.toArray(new String[1])) +\n+\t\t\t\"]\";\n+\t\tTableSourceTable newTableSourceTable = tableSourceTable.copy(dynamicTableSource, statistic, new String[]{extraDigest});\n+\t\tLogicalTableScan newScan = new LogicalTableScan(\n+\t\t\tscan.getCluster(), scan.getTraitSet(), scan.getHints(), newTableSourceTable);\n+\n+\t\tRexNode nonPartitionPredicate = RexUtil.composeConjunction(rexBuilder, JavaConversions.seqAsJavaList(predicate._2()));\n+\t\tif (nonPartitionPredicate.isAlwaysTrue()) {\n+\t\t\tcall.transformTo(newScan);\n+\t\t} else {\n+\t\t\tcall.transformTo(filter.copy(filter.getTraitSet(), newScan, nonPartitionPredicate));\n+\t\t}\n+\t}\n+\n+\tprivate RexNode adjustPartitionPredicate(List<String> inputFieldNames, List<String> partitionFieldNames, RexNode partitionPredicate) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0f19df4d1fcbb4093b69d772628b67b81ebb443a"}, "originalPosition": 160}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg3NzMyNTU0OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QxNDowNzoxNFrOG3kEwg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QxNDowNzoxNFrOG3kEwg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDkxNTkwNg==", "bodyText": "please reuse the listPartitions result", "url": "https://github.com/apache/flink/pull/12966#discussion_r460915906", "createdAt": "2020-07-27T14:07:14Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsPartitionPushDown;\n+import org.apache.flink.table.planner.calcite.FlinkContext;\n+import org.apache.flink.table.planner.calcite.FlinkTypeFactory;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+import org.apache.flink.table.planner.plan.utils.FlinkRelOptUtil;\n+import org.apache.flink.table.planner.plan.utils.PartitionPruner;\n+import org.apache.flink.table.planner.plan.utils.RexNodeExtractor;\n+import org.apache.flink.table.types.logical.LogicalType;\n+\n+import org.apache.calcite.plan.Context;\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Filter;\n+import org.apache.calcite.rel.logical.LogicalTableScan;\n+import org.apache.calcite.rel.type.RelDataType;\n+import org.apache.calcite.rex.RexBuilder;\n+import org.apache.calcite.rex.RexInputRef;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.calcite.rex.RexShuttle;\n+import org.apache.calcite.rex.RexUtil;\n+import org.apache.calcite.tools.RelBuilder;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.Seq;\n+\n+/**\n+ * Planner rule that tries to push partition evaluated by filter condition into a {@link LogicalTableScan}.\n+*/\n+public class PushPartitionIntoTableSourceScanRule extends RelOptRule {\n+\tpublic static final PushPartitionIntoTableSourceScanRule INSTANCE = new PushPartitionIntoTableSourceScanRule();\n+\n+\tpublic PushPartitionIntoTableSourceScanRule(){\n+\t\tsuper(operand(Filter.class,\n+\t\t\t\toperand(LogicalTableScan.class, none())),\n+\t\t\t\"PushPartitionTableSourceScanRule\");\n+\t}\n+\n+\t@Override\n+\tpublic boolean matches(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tif (filter.getCondition() == null) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tTableSourceTable tableSourceTable = call.rel(1).getTable().unwrap(TableSourceTable.class);\n+\t\tif (tableSourceTable == null){\n+\t\t\treturn false;\n+\t\t}\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource();\n+\t\tif (!(dynamicTableSource instanceof SupportsPartitionPushDown)) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tOptional<List<Map<String, String>>> partitions = ((SupportsPartitionPushDown) dynamicTableSource).listPartitions();\n+\t\treturn partitions.isPresent()\n+\t\t\t&& !partitions.get().isEmpty()\n+\t\t\t&& !Arrays.stream(tableSourceTable.extraDigests()).anyMatch(digest -> digest.startsWith(\"source: [partitions=\"));\n+\t}\n+\n+\t@Override\n+\tpublic void onMatch(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tLogicalTableScan scan = call.rel(1);\n+\t\tContext context = call.getPlanner().getContext().unwrap(FlinkContext.class);\n+\t\tTableSourceTable tableSourceTable = scan.getTable().unwrap(TableSourceTable.class);\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource();\n+\t\tRelDataType inputFieldType = filter.getInput().getRowType();\n+\t\tList<String> inputFieldName = inputFieldType.getFieldNames();\n+\n+\t\tList<String> partitionedFieldNames = tableSourceTable.catalogTable().getPartitionKeys();\n+\n+\t\tRelBuilder relBuilder = call.builder();\n+\t\tRexBuilder rexBuilder = relBuilder.getRexBuilder();\n+\n+\t\tTuple2<Seq<RexNode>, Seq<RexNode>> predicate = RexNodeExtractor.extractPartitionPredicateList(\n+\t\t\tfilter.getCondition(),\n+\t\t\tFlinkRelOptUtil.getMaxCnfNodeCount(scan),\n+\t\t\tinputFieldName.toArray(new String[inputFieldName.size()]),\n+\t\t\trexBuilder,\n+\t\t\tpartitionedFieldNames.toArray(new String[partitionedFieldNames.size()])\n+\t\t\t);\n+\n+\t\tRexNode partitionPredicate = RexUtil.composeConjunction(rexBuilder, JavaConversions.seqAsJavaList(predicate._1));\n+\n+\t\tif (partitionPredicate.isAlwaysTrue()){\n+\t\t\treturn;\n+\t\t}\n+\n+\t\tList<LogicalType> partitionFieldType = partitionedFieldNames.stream().map(name -> {\n+\t\t\tint index = inputFieldName.indexOf(name);\n+\t\t\tif (index < 0) {\n+\t\t\t\tthrow new RuntimeException(String.format(\"Partitioned key '%s' isn't found in input columns. \" +\n+\t\t\t\t\t\"Validator should have checked that.\", name));\n+\t\t\t}\n+\t\t\treturn inputFieldType.getFieldList().get(index).getType(); })\n+\t\t\t.map(FlinkTypeFactory::toLogicalType).collect(Collectors.toList());\n+\n+\t\t// checked in matches(call) and listPartitions() is not empty.\n+\t\tList<Map<String, String>> allPartitions = ((SupportsPartitionPushDown) dynamicTableSource).listPartitions().get();\n+\t\tRexNode finalPartitionPredicate = adjustPartitionPredicate(inputFieldName, partitionedFieldNames, partitionPredicate);\n+\t\t// get partitions\n+\t\tList<Map<String, String>> remainingPartitions = PartitionPruner.prunePartitions(\n+\t\t\t((FlinkContext) context).getTableConfig(),\n+\t\t\tpartitionedFieldNames.toArray(new String[partitionedFieldNames.size()]),\n+\t\t\tpartitionFieldType.toArray(new LogicalType[partitionFieldType.size()]),\n+\t\t\tallPartitions,\n+\t\t\tfinalPartitionPredicate\n+\t\t\t);\n+\t\t((SupportsPartitionPushDown) dynamicTableSource).applyPartitions(remainingPartitions);\n+\n+\t\t// build new table scan\n+\t\tFlinkStatistic statistic = tableSourceTable.getStatistic();\n+\t\tString extraDigest = \"source: [partitions=\" +\n+\t\t\tString.join(\", \", ((SupportsPartitionPushDown) dynamicTableSource).listPartitions()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0f19df4d1fcbb4093b69d772628b67b81ebb443a"}, "originalPosition": 141}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg3NzMyNzU1OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QxNDowNzo0M1rOG3kGBQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QxNDowNzo0M1rOG3kGBQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDkxNjIyOQ==", "bodyText": "use Object::toString instead", "url": "https://github.com/apache/flink/pull/12966#discussion_r460916229", "createdAt": "2020-07-27T14:07:43Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsPartitionPushDown;\n+import org.apache.flink.table.planner.calcite.FlinkContext;\n+import org.apache.flink.table.planner.calcite.FlinkTypeFactory;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+import org.apache.flink.table.planner.plan.utils.FlinkRelOptUtil;\n+import org.apache.flink.table.planner.plan.utils.PartitionPruner;\n+import org.apache.flink.table.planner.plan.utils.RexNodeExtractor;\n+import org.apache.flink.table.types.logical.LogicalType;\n+\n+import org.apache.calcite.plan.Context;\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Filter;\n+import org.apache.calcite.rel.logical.LogicalTableScan;\n+import org.apache.calcite.rel.type.RelDataType;\n+import org.apache.calcite.rex.RexBuilder;\n+import org.apache.calcite.rex.RexInputRef;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.calcite.rex.RexShuttle;\n+import org.apache.calcite.rex.RexUtil;\n+import org.apache.calcite.tools.RelBuilder;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.Seq;\n+\n+/**\n+ * Planner rule that tries to push partition evaluated by filter condition into a {@link LogicalTableScan}.\n+*/\n+public class PushPartitionIntoTableSourceScanRule extends RelOptRule {\n+\tpublic static final PushPartitionIntoTableSourceScanRule INSTANCE = new PushPartitionIntoTableSourceScanRule();\n+\n+\tpublic PushPartitionIntoTableSourceScanRule(){\n+\t\tsuper(operand(Filter.class,\n+\t\t\t\toperand(LogicalTableScan.class, none())),\n+\t\t\t\"PushPartitionTableSourceScanRule\");\n+\t}\n+\n+\t@Override\n+\tpublic boolean matches(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tif (filter.getCondition() == null) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tTableSourceTable tableSourceTable = call.rel(1).getTable().unwrap(TableSourceTable.class);\n+\t\tif (tableSourceTable == null){\n+\t\t\treturn false;\n+\t\t}\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource();\n+\t\tif (!(dynamicTableSource instanceof SupportsPartitionPushDown)) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tOptional<List<Map<String, String>>> partitions = ((SupportsPartitionPushDown) dynamicTableSource).listPartitions();\n+\t\treturn partitions.isPresent()\n+\t\t\t&& !partitions.get().isEmpty()\n+\t\t\t&& !Arrays.stream(tableSourceTable.extraDigests()).anyMatch(digest -> digest.startsWith(\"source: [partitions=\"));\n+\t}\n+\n+\t@Override\n+\tpublic void onMatch(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tLogicalTableScan scan = call.rel(1);\n+\t\tContext context = call.getPlanner().getContext().unwrap(FlinkContext.class);\n+\t\tTableSourceTable tableSourceTable = scan.getTable().unwrap(TableSourceTable.class);\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource();\n+\t\tRelDataType inputFieldType = filter.getInput().getRowType();\n+\t\tList<String> inputFieldName = inputFieldType.getFieldNames();\n+\n+\t\tList<String> partitionedFieldNames = tableSourceTable.catalogTable().getPartitionKeys();\n+\n+\t\tRelBuilder relBuilder = call.builder();\n+\t\tRexBuilder rexBuilder = relBuilder.getRexBuilder();\n+\n+\t\tTuple2<Seq<RexNode>, Seq<RexNode>> predicate = RexNodeExtractor.extractPartitionPredicateList(\n+\t\t\tfilter.getCondition(),\n+\t\t\tFlinkRelOptUtil.getMaxCnfNodeCount(scan),\n+\t\t\tinputFieldName.toArray(new String[inputFieldName.size()]),\n+\t\t\trexBuilder,\n+\t\t\tpartitionedFieldNames.toArray(new String[partitionedFieldNames.size()])\n+\t\t\t);\n+\n+\t\tRexNode partitionPredicate = RexUtil.composeConjunction(rexBuilder, JavaConversions.seqAsJavaList(predicate._1));\n+\n+\t\tif (partitionPredicate.isAlwaysTrue()){\n+\t\t\treturn;\n+\t\t}\n+\n+\t\tList<LogicalType> partitionFieldType = partitionedFieldNames.stream().map(name -> {\n+\t\t\tint index = inputFieldName.indexOf(name);\n+\t\t\tif (index < 0) {\n+\t\t\t\tthrow new RuntimeException(String.format(\"Partitioned key '%s' isn't found in input columns. \" +\n+\t\t\t\t\t\"Validator should have checked that.\", name));\n+\t\t\t}\n+\t\t\treturn inputFieldType.getFieldList().get(index).getType(); })\n+\t\t\t.map(FlinkTypeFactory::toLogicalType).collect(Collectors.toList());\n+\n+\t\t// checked in matches(call) and listPartitions() is not empty.\n+\t\tList<Map<String, String>> allPartitions = ((SupportsPartitionPushDown) dynamicTableSource).listPartitions().get();\n+\t\tRexNode finalPartitionPredicate = adjustPartitionPredicate(inputFieldName, partitionedFieldNames, partitionPredicate);\n+\t\t// get partitions\n+\t\tList<Map<String, String>> remainingPartitions = PartitionPruner.prunePartitions(\n+\t\t\t((FlinkContext) context).getTableConfig(),\n+\t\t\tpartitionedFieldNames.toArray(new String[partitionedFieldNames.size()]),\n+\t\t\tpartitionFieldType.toArray(new LogicalType[partitionFieldType.size()]),\n+\t\t\tallPartitions,\n+\t\t\tfinalPartitionPredicate\n+\t\t\t);\n+\t\t((SupportsPartitionPushDown) dynamicTableSource).applyPartitions(remainingPartitions);\n+\n+\t\t// build new table scan\n+\t\tFlinkStatistic statistic = tableSourceTable.getStatistic();\n+\t\tString extraDigest = \"source: [partitions=\" +\n+\t\t\tString.join(\", \", ((SupportsPartitionPushDown) dynamicTableSource).listPartitions()\n+\t\t\t\t.get()\n+\t\t\t\t.stream()\n+\t\t\t\t.map(partition -> partition.toString())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0f19df4d1fcbb4093b69d772628b67b81ebb443a"}, "originalPosition": 144}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg3NzMzMTk4OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QxNDowODo0NFrOG3kIvA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QxNDowODo0NFrOG3kIvA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDkxNjkyNA==", "bodyText": "The statistic should be updated", "url": "https://github.com/apache/flink/pull/12966#discussion_r460916924", "createdAt": "2020-07-27T14:08:44Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsPartitionPushDown;\n+import org.apache.flink.table.planner.calcite.FlinkContext;\n+import org.apache.flink.table.planner.calcite.FlinkTypeFactory;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+import org.apache.flink.table.planner.plan.utils.FlinkRelOptUtil;\n+import org.apache.flink.table.planner.plan.utils.PartitionPruner;\n+import org.apache.flink.table.planner.plan.utils.RexNodeExtractor;\n+import org.apache.flink.table.types.logical.LogicalType;\n+\n+import org.apache.calcite.plan.Context;\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Filter;\n+import org.apache.calcite.rel.logical.LogicalTableScan;\n+import org.apache.calcite.rel.type.RelDataType;\n+import org.apache.calcite.rex.RexBuilder;\n+import org.apache.calcite.rex.RexInputRef;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.calcite.rex.RexShuttle;\n+import org.apache.calcite.rex.RexUtil;\n+import org.apache.calcite.tools.RelBuilder;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.Seq;\n+\n+/**\n+ * Planner rule that tries to push partition evaluated by filter condition into a {@link LogicalTableScan}.\n+*/\n+public class PushPartitionIntoTableSourceScanRule extends RelOptRule {\n+\tpublic static final PushPartitionIntoTableSourceScanRule INSTANCE = new PushPartitionIntoTableSourceScanRule();\n+\n+\tpublic PushPartitionIntoTableSourceScanRule(){\n+\t\tsuper(operand(Filter.class,\n+\t\t\t\toperand(LogicalTableScan.class, none())),\n+\t\t\t\"PushPartitionTableSourceScanRule\");\n+\t}\n+\n+\t@Override\n+\tpublic boolean matches(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tif (filter.getCondition() == null) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tTableSourceTable tableSourceTable = call.rel(1).getTable().unwrap(TableSourceTable.class);\n+\t\tif (tableSourceTable == null){\n+\t\t\treturn false;\n+\t\t}\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource();\n+\t\tif (!(dynamicTableSource instanceof SupportsPartitionPushDown)) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tOptional<List<Map<String, String>>> partitions = ((SupportsPartitionPushDown) dynamicTableSource).listPartitions();\n+\t\treturn partitions.isPresent()\n+\t\t\t&& !partitions.get().isEmpty()\n+\t\t\t&& !Arrays.stream(tableSourceTable.extraDigests()).anyMatch(digest -> digest.startsWith(\"source: [partitions=\"));\n+\t}\n+\n+\t@Override\n+\tpublic void onMatch(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tLogicalTableScan scan = call.rel(1);\n+\t\tContext context = call.getPlanner().getContext().unwrap(FlinkContext.class);\n+\t\tTableSourceTable tableSourceTable = scan.getTable().unwrap(TableSourceTable.class);\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource();\n+\t\tRelDataType inputFieldType = filter.getInput().getRowType();\n+\t\tList<String> inputFieldName = inputFieldType.getFieldNames();\n+\n+\t\tList<String> partitionedFieldNames = tableSourceTable.catalogTable().getPartitionKeys();\n+\n+\t\tRelBuilder relBuilder = call.builder();\n+\t\tRexBuilder rexBuilder = relBuilder.getRexBuilder();\n+\n+\t\tTuple2<Seq<RexNode>, Seq<RexNode>> predicate = RexNodeExtractor.extractPartitionPredicateList(\n+\t\t\tfilter.getCondition(),\n+\t\t\tFlinkRelOptUtil.getMaxCnfNodeCount(scan),\n+\t\t\tinputFieldName.toArray(new String[inputFieldName.size()]),\n+\t\t\trexBuilder,\n+\t\t\tpartitionedFieldNames.toArray(new String[partitionedFieldNames.size()])\n+\t\t\t);\n+\n+\t\tRexNode partitionPredicate = RexUtil.composeConjunction(rexBuilder, JavaConversions.seqAsJavaList(predicate._1));\n+\n+\t\tif (partitionPredicate.isAlwaysTrue()){\n+\t\t\treturn;\n+\t\t}\n+\n+\t\tList<LogicalType> partitionFieldType = partitionedFieldNames.stream().map(name -> {\n+\t\t\tint index = inputFieldName.indexOf(name);\n+\t\t\tif (index < 0) {\n+\t\t\t\tthrow new RuntimeException(String.format(\"Partitioned key '%s' isn't found in input columns. \" +\n+\t\t\t\t\t\"Validator should have checked that.\", name));\n+\t\t\t}\n+\t\t\treturn inputFieldType.getFieldList().get(index).getType(); })\n+\t\t\t.map(FlinkTypeFactory::toLogicalType).collect(Collectors.toList());\n+\n+\t\t// checked in matches(call) and listPartitions() is not empty.\n+\t\tList<Map<String, String>> allPartitions = ((SupportsPartitionPushDown) dynamicTableSource).listPartitions().get();\n+\t\tRexNode finalPartitionPredicate = adjustPartitionPredicate(inputFieldName, partitionedFieldNames, partitionPredicate);\n+\t\t// get partitions\n+\t\tList<Map<String, String>> remainingPartitions = PartitionPruner.prunePartitions(\n+\t\t\t((FlinkContext) context).getTableConfig(),\n+\t\t\tpartitionedFieldNames.toArray(new String[partitionedFieldNames.size()]),\n+\t\t\tpartitionFieldType.toArray(new LogicalType[partitionFieldType.size()]),\n+\t\t\tallPartitions,\n+\t\t\tfinalPartitionPredicate\n+\t\t\t);\n+\t\t((SupportsPartitionPushDown) dynamicTableSource).applyPartitions(remainingPartitions);\n+\n+\t\t// build new table scan\n+\t\tFlinkStatistic statistic = tableSourceTable.getStatistic();\n+\t\tString extraDigest = \"source: [partitions=\" +\n+\t\t\tString.join(\", \", ((SupportsPartitionPushDown) dynamicTableSource).listPartitions()\n+\t\t\t\t.get()\n+\t\t\t\t.stream()\n+\t\t\t\t.map(partition -> partition.toString())\n+\t\t\t\t.collect(Collectors.toList())\n+\t\t\t\t.toArray(new String[1])) +\n+\t\t\t\"]\";\n+\t\tTableSourceTable newTableSourceTable = tableSourceTable.copy(dynamicTableSource, statistic, new String[]{extraDigest});", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0f19df4d1fcbb4093b69d772628b67b81ebb443a"}, "originalPosition": 148}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg3NzMzNTEwOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QxNDowOToyM1rOG3kKpg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QxNDowOToyM1rOG3kKpg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDkxNzQxNA==", "bodyText": "\"source: [partitions=\" -> \"partitions=[\" ?", "url": "https://github.com/apache/flink/pull/12966#discussion_r460917414", "createdAt": "2020-07-27T14:09:23Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsPartitionPushDown;\n+import org.apache.flink.table.planner.calcite.FlinkContext;\n+import org.apache.flink.table.planner.calcite.FlinkTypeFactory;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+import org.apache.flink.table.planner.plan.utils.FlinkRelOptUtil;\n+import org.apache.flink.table.planner.plan.utils.PartitionPruner;\n+import org.apache.flink.table.planner.plan.utils.RexNodeExtractor;\n+import org.apache.flink.table.types.logical.LogicalType;\n+\n+import org.apache.calcite.plan.Context;\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Filter;\n+import org.apache.calcite.rel.logical.LogicalTableScan;\n+import org.apache.calcite.rel.type.RelDataType;\n+import org.apache.calcite.rex.RexBuilder;\n+import org.apache.calcite.rex.RexInputRef;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.calcite.rex.RexShuttle;\n+import org.apache.calcite.rex.RexUtil;\n+import org.apache.calcite.tools.RelBuilder;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.Seq;\n+\n+/**\n+ * Planner rule that tries to push partition evaluated by filter condition into a {@link LogicalTableScan}.\n+*/\n+public class PushPartitionIntoTableSourceScanRule extends RelOptRule {\n+\tpublic static final PushPartitionIntoTableSourceScanRule INSTANCE = new PushPartitionIntoTableSourceScanRule();\n+\n+\tpublic PushPartitionIntoTableSourceScanRule(){\n+\t\tsuper(operand(Filter.class,\n+\t\t\t\toperand(LogicalTableScan.class, none())),\n+\t\t\t\"PushPartitionTableSourceScanRule\");\n+\t}\n+\n+\t@Override\n+\tpublic boolean matches(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tif (filter.getCondition() == null) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tTableSourceTable tableSourceTable = call.rel(1).getTable().unwrap(TableSourceTable.class);\n+\t\tif (tableSourceTable == null){\n+\t\t\treturn false;\n+\t\t}\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource();\n+\t\tif (!(dynamicTableSource instanceof SupportsPartitionPushDown)) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tOptional<List<Map<String, String>>> partitions = ((SupportsPartitionPushDown) dynamicTableSource).listPartitions();\n+\t\treturn partitions.isPresent()\n+\t\t\t&& !partitions.get().isEmpty()\n+\t\t\t&& !Arrays.stream(tableSourceTable.extraDigests()).anyMatch(digest -> digest.startsWith(\"source: [partitions=\"));\n+\t}\n+\n+\t@Override\n+\tpublic void onMatch(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tLogicalTableScan scan = call.rel(1);\n+\t\tContext context = call.getPlanner().getContext().unwrap(FlinkContext.class);\n+\t\tTableSourceTable tableSourceTable = scan.getTable().unwrap(TableSourceTable.class);\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource();\n+\t\tRelDataType inputFieldType = filter.getInput().getRowType();\n+\t\tList<String> inputFieldName = inputFieldType.getFieldNames();\n+\n+\t\tList<String> partitionedFieldNames = tableSourceTable.catalogTable().getPartitionKeys();\n+\n+\t\tRelBuilder relBuilder = call.builder();\n+\t\tRexBuilder rexBuilder = relBuilder.getRexBuilder();\n+\n+\t\tTuple2<Seq<RexNode>, Seq<RexNode>> predicate = RexNodeExtractor.extractPartitionPredicateList(\n+\t\t\tfilter.getCondition(),\n+\t\t\tFlinkRelOptUtil.getMaxCnfNodeCount(scan),\n+\t\t\tinputFieldName.toArray(new String[inputFieldName.size()]),\n+\t\t\trexBuilder,\n+\t\t\tpartitionedFieldNames.toArray(new String[partitionedFieldNames.size()])\n+\t\t\t);\n+\n+\t\tRexNode partitionPredicate = RexUtil.composeConjunction(rexBuilder, JavaConversions.seqAsJavaList(predicate._1));\n+\n+\t\tif (partitionPredicate.isAlwaysTrue()){\n+\t\t\treturn;\n+\t\t}\n+\n+\t\tList<LogicalType> partitionFieldType = partitionedFieldNames.stream().map(name -> {\n+\t\t\tint index = inputFieldName.indexOf(name);\n+\t\t\tif (index < 0) {\n+\t\t\t\tthrow new RuntimeException(String.format(\"Partitioned key '%s' isn't found in input columns. \" +\n+\t\t\t\t\t\"Validator should have checked that.\", name));\n+\t\t\t}\n+\t\t\treturn inputFieldType.getFieldList().get(index).getType(); })\n+\t\t\t.map(FlinkTypeFactory::toLogicalType).collect(Collectors.toList());\n+\n+\t\t// checked in matches(call) and listPartitions() is not empty.\n+\t\tList<Map<String, String>> allPartitions = ((SupportsPartitionPushDown) dynamicTableSource).listPartitions().get();\n+\t\tRexNode finalPartitionPredicate = adjustPartitionPredicate(inputFieldName, partitionedFieldNames, partitionPredicate);\n+\t\t// get partitions\n+\t\tList<Map<String, String>> remainingPartitions = PartitionPruner.prunePartitions(\n+\t\t\t((FlinkContext) context).getTableConfig(),\n+\t\t\tpartitionedFieldNames.toArray(new String[partitionedFieldNames.size()]),\n+\t\t\tpartitionFieldType.toArray(new LogicalType[partitionFieldType.size()]),\n+\t\t\tallPartitions,\n+\t\t\tfinalPartitionPredicate\n+\t\t\t);\n+\t\t((SupportsPartitionPushDown) dynamicTableSource).applyPartitions(remainingPartitions);\n+\n+\t\t// build new table scan\n+\t\tFlinkStatistic statistic = tableSourceTable.getStatistic();\n+\t\tString extraDigest = \"source: [partitions=\" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0f19df4d1fcbb4093b69d772628b67b81ebb443a"}, "originalPosition": 140}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg3NzMzOTI5OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QxNDoxMDoyMlrOG3kNLg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QxNDoxMDoyMlrOG3kNLg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDkxODA2Mg==", "bodyText": "give a proper name for predicate ?", "url": "https://github.com/apache/flink/pull/12966#discussion_r460918062", "createdAt": "2020-07-27T14:10:22Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsPartitionPushDown;\n+import org.apache.flink.table.planner.calcite.FlinkContext;\n+import org.apache.flink.table.planner.calcite.FlinkTypeFactory;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+import org.apache.flink.table.planner.plan.utils.FlinkRelOptUtil;\n+import org.apache.flink.table.planner.plan.utils.PartitionPruner;\n+import org.apache.flink.table.planner.plan.utils.RexNodeExtractor;\n+import org.apache.flink.table.types.logical.LogicalType;\n+\n+import org.apache.calcite.plan.Context;\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Filter;\n+import org.apache.calcite.rel.logical.LogicalTableScan;\n+import org.apache.calcite.rel.type.RelDataType;\n+import org.apache.calcite.rex.RexBuilder;\n+import org.apache.calcite.rex.RexInputRef;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.calcite.rex.RexShuttle;\n+import org.apache.calcite.rex.RexUtil;\n+import org.apache.calcite.tools.RelBuilder;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.Seq;\n+\n+/**\n+ * Planner rule that tries to push partition evaluated by filter condition into a {@link LogicalTableScan}.\n+*/\n+public class PushPartitionIntoTableSourceScanRule extends RelOptRule {\n+\tpublic static final PushPartitionIntoTableSourceScanRule INSTANCE = new PushPartitionIntoTableSourceScanRule();\n+\n+\tpublic PushPartitionIntoTableSourceScanRule(){\n+\t\tsuper(operand(Filter.class,\n+\t\t\t\toperand(LogicalTableScan.class, none())),\n+\t\t\t\"PushPartitionTableSourceScanRule\");\n+\t}\n+\n+\t@Override\n+\tpublic boolean matches(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tif (filter.getCondition() == null) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tTableSourceTable tableSourceTable = call.rel(1).getTable().unwrap(TableSourceTable.class);\n+\t\tif (tableSourceTable == null){\n+\t\t\treturn false;\n+\t\t}\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource();\n+\t\tif (!(dynamicTableSource instanceof SupportsPartitionPushDown)) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tOptional<List<Map<String, String>>> partitions = ((SupportsPartitionPushDown) dynamicTableSource).listPartitions();\n+\t\treturn partitions.isPresent()\n+\t\t\t&& !partitions.get().isEmpty()\n+\t\t\t&& !Arrays.stream(tableSourceTable.extraDigests()).anyMatch(digest -> digest.startsWith(\"source: [partitions=\"));\n+\t}\n+\n+\t@Override\n+\tpublic void onMatch(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tLogicalTableScan scan = call.rel(1);\n+\t\tContext context = call.getPlanner().getContext().unwrap(FlinkContext.class);\n+\t\tTableSourceTable tableSourceTable = scan.getTable().unwrap(TableSourceTable.class);\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource();\n+\t\tRelDataType inputFieldType = filter.getInput().getRowType();\n+\t\tList<String> inputFieldName = inputFieldType.getFieldNames();\n+\n+\t\tList<String> partitionedFieldNames = tableSourceTable.catalogTable().getPartitionKeys();\n+\n+\t\tRelBuilder relBuilder = call.builder();\n+\t\tRexBuilder rexBuilder = relBuilder.getRexBuilder();\n+\n+\t\tTuple2<Seq<RexNode>, Seq<RexNode>> predicate = RexNodeExtractor.extractPartitionPredicateList(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0f19df4d1fcbb4093b69d772628b67b81ebb443a"}, "originalPosition": 102}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg3NzM1MTUxOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QxNDoxMzowMFrOG3kUhg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QxNDoxMzowMFrOG3kUhg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDkxOTk0Mg==", "bodyText": "move filter.copy(filter.getTraitSet(), newScan, nonPartitionPredicate) into a new line for more easy debugging", "url": "https://github.com/apache/flink/pull/12966#discussion_r460919942", "createdAt": "2020-07-27T14:13:00Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsPartitionPushDown;\n+import org.apache.flink.table.planner.calcite.FlinkContext;\n+import org.apache.flink.table.planner.calcite.FlinkTypeFactory;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+import org.apache.flink.table.planner.plan.utils.FlinkRelOptUtil;\n+import org.apache.flink.table.planner.plan.utils.PartitionPruner;\n+import org.apache.flink.table.planner.plan.utils.RexNodeExtractor;\n+import org.apache.flink.table.types.logical.LogicalType;\n+\n+import org.apache.calcite.plan.Context;\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Filter;\n+import org.apache.calcite.rel.logical.LogicalTableScan;\n+import org.apache.calcite.rel.type.RelDataType;\n+import org.apache.calcite.rex.RexBuilder;\n+import org.apache.calcite.rex.RexInputRef;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.calcite.rex.RexShuttle;\n+import org.apache.calcite.rex.RexUtil;\n+import org.apache.calcite.tools.RelBuilder;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.Seq;\n+\n+/**\n+ * Planner rule that tries to push partition evaluated by filter condition into a {@link LogicalTableScan}.\n+*/\n+public class PushPartitionIntoTableSourceScanRule extends RelOptRule {\n+\tpublic static final PushPartitionIntoTableSourceScanRule INSTANCE = new PushPartitionIntoTableSourceScanRule();\n+\n+\tpublic PushPartitionIntoTableSourceScanRule(){\n+\t\tsuper(operand(Filter.class,\n+\t\t\t\toperand(LogicalTableScan.class, none())),\n+\t\t\t\"PushPartitionTableSourceScanRule\");\n+\t}\n+\n+\t@Override\n+\tpublic boolean matches(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tif (filter.getCondition() == null) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tTableSourceTable tableSourceTable = call.rel(1).getTable().unwrap(TableSourceTable.class);\n+\t\tif (tableSourceTable == null){\n+\t\t\treturn false;\n+\t\t}\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource();\n+\t\tif (!(dynamicTableSource instanceof SupportsPartitionPushDown)) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tOptional<List<Map<String, String>>> partitions = ((SupportsPartitionPushDown) dynamicTableSource).listPartitions();\n+\t\treturn partitions.isPresent()\n+\t\t\t&& !partitions.get().isEmpty()\n+\t\t\t&& !Arrays.stream(tableSourceTable.extraDigests()).anyMatch(digest -> digest.startsWith(\"source: [partitions=\"));\n+\t}\n+\n+\t@Override\n+\tpublic void onMatch(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tLogicalTableScan scan = call.rel(1);\n+\t\tContext context = call.getPlanner().getContext().unwrap(FlinkContext.class);\n+\t\tTableSourceTable tableSourceTable = scan.getTable().unwrap(TableSourceTable.class);\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource();\n+\t\tRelDataType inputFieldType = filter.getInput().getRowType();\n+\t\tList<String> inputFieldName = inputFieldType.getFieldNames();\n+\n+\t\tList<String> partitionedFieldNames = tableSourceTable.catalogTable().getPartitionKeys();\n+\n+\t\tRelBuilder relBuilder = call.builder();\n+\t\tRexBuilder rexBuilder = relBuilder.getRexBuilder();\n+\n+\t\tTuple2<Seq<RexNode>, Seq<RexNode>> predicate = RexNodeExtractor.extractPartitionPredicateList(\n+\t\t\tfilter.getCondition(),\n+\t\t\tFlinkRelOptUtil.getMaxCnfNodeCount(scan),\n+\t\t\tinputFieldName.toArray(new String[inputFieldName.size()]),\n+\t\t\trexBuilder,\n+\t\t\tpartitionedFieldNames.toArray(new String[partitionedFieldNames.size()])\n+\t\t\t);\n+\n+\t\tRexNode partitionPredicate = RexUtil.composeConjunction(rexBuilder, JavaConversions.seqAsJavaList(predicate._1));\n+\n+\t\tif (partitionPredicate.isAlwaysTrue()){\n+\t\t\treturn;\n+\t\t}\n+\n+\t\tList<LogicalType> partitionFieldType = partitionedFieldNames.stream().map(name -> {\n+\t\t\tint index = inputFieldName.indexOf(name);\n+\t\t\tif (index < 0) {\n+\t\t\t\tthrow new RuntimeException(String.format(\"Partitioned key '%s' isn't found in input columns. \" +\n+\t\t\t\t\t\"Validator should have checked that.\", name));\n+\t\t\t}\n+\t\t\treturn inputFieldType.getFieldList().get(index).getType(); })\n+\t\t\t.map(FlinkTypeFactory::toLogicalType).collect(Collectors.toList());\n+\n+\t\t// checked in matches(call) and listPartitions() is not empty.\n+\t\tList<Map<String, String>> allPartitions = ((SupportsPartitionPushDown) dynamicTableSource).listPartitions().get();\n+\t\tRexNode finalPartitionPredicate = adjustPartitionPredicate(inputFieldName, partitionedFieldNames, partitionPredicate);\n+\t\t// get partitions\n+\t\tList<Map<String, String>> remainingPartitions = PartitionPruner.prunePartitions(\n+\t\t\t((FlinkContext) context).getTableConfig(),\n+\t\t\tpartitionedFieldNames.toArray(new String[partitionedFieldNames.size()]),\n+\t\t\tpartitionFieldType.toArray(new LogicalType[partitionFieldType.size()]),\n+\t\t\tallPartitions,\n+\t\t\tfinalPartitionPredicate\n+\t\t\t);\n+\t\t((SupportsPartitionPushDown) dynamicTableSource).applyPartitions(remainingPartitions);\n+\n+\t\t// build new table scan\n+\t\tFlinkStatistic statistic = tableSourceTable.getStatistic();\n+\t\tString extraDigest = \"source: [partitions=\" +\n+\t\t\tString.join(\", \", ((SupportsPartitionPushDown) dynamicTableSource).listPartitions()\n+\t\t\t\t.get()\n+\t\t\t\t.stream()\n+\t\t\t\t.map(partition -> partition.toString())\n+\t\t\t\t.collect(Collectors.toList())\n+\t\t\t\t.toArray(new String[1])) +\n+\t\t\t\"]\";\n+\t\tTableSourceTable newTableSourceTable = tableSourceTable.copy(dynamicTableSource, statistic, new String[]{extraDigest});\n+\t\tLogicalTableScan newScan = new LogicalTableScan(\n+\t\t\tscan.getCluster(), scan.getTraitSet(), scan.getHints(), newTableSourceTable);\n+\n+\t\tRexNode nonPartitionPredicate = RexUtil.composeConjunction(rexBuilder, JavaConversions.seqAsJavaList(predicate._2()));\n+\t\tif (nonPartitionPredicate.isAlwaysTrue()) {\n+\t\t\tcall.transformTo(newScan);\n+\t\t} else {\n+\t\t\tcall.transformTo(filter.copy(filter.getTraitSet(), newScan, nonPartitionPredicate));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0f19df4d1fcbb4093b69d772628b67b81ebb443a"}, "originalPosition": 156}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg3NzM1Mzk0OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QxNDoxMzozNlrOG3kWDQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QxNDoxMzozNlrOG3kWDQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDkyMDMzMw==", "bodyText": "it's better to use LogicalTableScan.create method", "url": "https://github.com/apache/flink/pull/12966#discussion_r460920333", "createdAt": "2020-07-27T14:13:36Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsPartitionPushDown;\n+import org.apache.flink.table.planner.calcite.FlinkContext;\n+import org.apache.flink.table.planner.calcite.FlinkTypeFactory;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+import org.apache.flink.table.planner.plan.utils.FlinkRelOptUtil;\n+import org.apache.flink.table.planner.plan.utils.PartitionPruner;\n+import org.apache.flink.table.planner.plan.utils.RexNodeExtractor;\n+import org.apache.flink.table.types.logical.LogicalType;\n+\n+import org.apache.calcite.plan.Context;\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Filter;\n+import org.apache.calcite.rel.logical.LogicalTableScan;\n+import org.apache.calcite.rel.type.RelDataType;\n+import org.apache.calcite.rex.RexBuilder;\n+import org.apache.calcite.rex.RexInputRef;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.calcite.rex.RexShuttle;\n+import org.apache.calcite.rex.RexUtil;\n+import org.apache.calcite.tools.RelBuilder;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.Seq;\n+\n+/**\n+ * Planner rule that tries to push partition evaluated by filter condition into a {@link LogicalTableScan}.\n+*/\n+public class PushPartitionIntoTableSourceScanRule extends RelOptRule {\n+\tpublic static final PushPartitionIntoTableSourceScanRule INSTANCE = new PushPartitionIntoTableSourceScanRule();\n+\n+\tpublic PushPartitionIntoTableSourceScanRule(){\n+\t\tsuper(operand(Filter.class,\n+\t\t\t\toperand(LogicalTableScan.class, none())),\n+\t\t\t\"PushPartitionTableSourceScanRule\");\n+\t}\n+\n+\t@Override\n+\tpublic boolean matches(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tif (filter.getCondition() == null) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tTableSourceTable tableSourceTable = call.rel(1).getTable().unwrap(TableSourceTable.class);\n+\t\tif (tableSourceTable == null){\n+\t\t\treturn false;\n+\t\t}\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource();\n+\t\tif (!(dynamicTableSource instanceof SupportsPartitionPushDown)) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tOptional<List<Map<String, String>>> partitions = ((SupportsPartitionPushDown) dynamicTableSource).listPartitions();\n+\t\treturn partitions.isPresent()\n+\t\t\t&& !partitions.get().isEmpty()\n+\t\t\t&& !Arrays.stream(tableSourceTable.extraDigests()).anyMatch(digest -> digest.startsWith(\"source: [partitions=\"));\n+\t}\n+\n+\t@Override\n+\tpublic void onMatch(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tLogicalTableScan scan = call.rel(1);\n+\t\tContext context = call.getPlanner().getContext().unwrap(FlinkContext.class);\n+\t\tTableSourceTable tableSourceTable = scan.getTable().unwrap(TableSourceTable.class);\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource();\n+\t\tRelDataType inputFieldType = filter.getInput().getRowType();\n+\t\tList<String> inputFieldName = inputFieldType.getFieldNames();\n+\n+\t\tList<String> partitionedFieldNames = tableSourceTable.catalogTable().getPartitionKeys();\n+\n+\t\tRelBuilder relBuilder = call.builder();\n+\t\tRexBuilder rexBuilder = relBuilder.getRexBuilder();\n+\n+\t\tTuple2<Seq<RexNode>, Seq<RexNode>> predicate = RexNodeExtractor.extractPartitionPredicateList(\n+\t\t\tfilter.getCondition(),\n+\t\t\tFlinkRelOptUtil.getMaxCnfNodeCount(scan),\n+\t\t\tinputFieldName.toArray(new String[inputFieldName.size()]),\n+\t\t\trexBuilder,\n+\t\t\tpartitionedFieldNames.toArray(new String[partitionedFieldNames.size()])\n+\t\t\t);\n+\n+\t\tRexNode partitionPredicate = RexUtil.composeConjunction(rexBuilder, JavaConversions.seqAsJavaList(predicate._1));\n+\n+\t\tif (partitionPredicate.isAlwaysTrue()){\n+\t\t\treturn;\n+\t\t}\n+\n+\t\tList<LogicalType> partitionFieldType = partitionedFieldNames.stream().map(name -> {\n+\t\t\tint index = inputFieldName.indexOf(name);\n+\t\t\tif (index < 0) {\n+\t\t\t\tthrow new RuntimeException(String.format(\"Partitioned key '%s' isn't found in input columns. \" +\n+\t\t\t\t\t\"Validator should have checked that.\", name));\n+\t\t\t}\n+\t\t\treturn inputFieldType.getFieldList().get(index).getType(); })\n+\t\t\t.map(FlinkTypeFactory::toLogicalType).collect(Collectors.toList());\n+\n+\t\t// checked in matches(call) and listPartitions() is not empty.\n+\t\tList<Map<String, String>> allPartitions = ((SupportsPartitionPushDown) dynamicTableSource).listPartitions().get();\n+\t\tRexNode finalPartitionPredicate = adjustPartitionPredicate(inputFieldName, partitionedFieldNames, partitionPredicate);\n+\t\t// get partitions\n+\t\tList<Map<String, String>> remainingPartitions = PartitionPruner.prunePartitions(\n+\t\t\t((FlinkContext) context).getTableConfig(),\n+\t\t\tpartitionedFieldNames.toArray(new String[partitionedFieldNames.size()]),\n+\t\t\tpartitionFieldType.toArray(new LogicalType[partitionFieldType.size()]),\n+\t\t\tallPartitions,\n+\t\t\tfinalPartitionPredicate\n+\t\t\t);\n+\t\t((SupportsPartitionPushDown) dynamicTableSource).applyPartitions(remainingPartitions);\n+\n+\t\t// build new table scan\n+\t\tFlinkStatistic statistic = tableSourceTable.getStatistic();\n+\t\tString extraDigest = \"source: [partitions=\" +\n+\t\t\tString.join(\", \", ((SupportsPartitionPushDown) dynamicTableSource).listPartitions()\n+\t\t\t\t.get()\n+\t\t\t\t.stream()\n+\t\t\t\t.map(partition -> partition.toString())\n+\t\t\t\t.collect(Collectors.toList())\n+\t\t\t\t.toArray(new String[1])) +\n+\t\t\t\"]\";\n+\t\tTableSourceTable newTableSourceTable = tableSourceTable.copy(dynamicTableSource, statistic, new String[]{extraDigest});\n+\t\tLogicalTableScan newScan = new LogicalTableScan(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0f19df4d1fcbb4093b69d772628b67b81ebb443a"}, "originalPosition": 149}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg3NzM1NTQxOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QxNDoxMzo1NFrOG3kW6g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QxNDoxMzo1NFrOG3kW6g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDkyMDU1NA==", "bodyText": "new String[0]", "url": "https://github.com/apache/flink/pull/12966#discussion_r460920554", "createdAt": "2020-07-27T14:13:54Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsPartitionPushDown;\n+import org.apache.flink.table.planner.calcite.FlinkContext;\n+import org.apache.flink.table.planner.calcite.FlinkTypeFactory;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+import org.apache.flink.table.planner.plan.utils.FlinkRelOptUtil;\n+import org.apache.flink.table.planner.plan.utils.PartitionPruner;\n+import org.apache.flink.table.planner.plan.utils.RexNodeExtractor;\n+import org.apache.flink.table.types.logical.LogicalType;\n+\n+import org.apache.calcite.plan.Context;\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Filter;\n+import org.apache.calcite.rel.logical.LogicalTableScan;\n+import org.apache.calcite.rel.type.RelDataType;\n+import org.apache.calcite.rex.RexBuilder;\n+import org.apache.calcite.rex.RexInputRef;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.calcite.rex.RexShuttle;\n+import org.apache.calcite.rex.RexUtil;\n+import org.apache.calcite.tools.RelBuilder;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.Seq;\n+\n+/**\n+ * Planner rule that tries to push partition evaluated by filter condition into a {@link LogicalTableScan}.\n+*/\n+public class PushPartitionIntoTableSourceScanRule extends RelOptRule {\n+\tpublic static final PushPartitionIntoTableSourceScanRule INSTANCE = new PushPartitionIntoTableSourceScanRule();\n+\n+\tpublic PushPartitionIntoTableSourceScanRule(){\n+\t\tsuper(operand(Filter.class,\n+\t\t\t\toperand(LogicalTableScan.class, none())),\n+\t\t\t\"PushPartitionTableSourceScanRule\");\n+\t}\n+\n+\t@Override\n+\tpublic boolean matches(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tif (filter.getCondition() == null) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tTableSourceTable tableSourceTable = call.rel(1).getTable().unwrap(TableSourceTable.class);\n+\t\tif (tableSourceTable == null){\n+\t\t\treturn false;\n+\t\t}\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource();\n+\t\tif (!(dynamicTableSource instanceof SupportsPartitionPushDown)) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tOptional<List<Map<String, String>>> partitions = ((SupportsPartitionPushDown) dynamicTableSource).listPartitions();\n+\t\treturn partitions.isPresent()\n+\t\t\t&& !partitions.get().isEmpty()\n+\t\t\t&& !Arrays.stream(tableSourceTable.extraDigests()).anyMatch(digest -> digest.startsWith(\"source: [partitions=\"));\n+\t}\n+\n+\t@Override\n+\tpublic void onMatch(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tLogicalTableScan scan = call.rel(1);\n+\t\tContext context = call.getPlanner().getContext().unwrap(FlinkContext.class);\n+\t\tTableSourceTable tableSourceTable = scan.getTable().unwrap(TableSourceTable.class);\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource();\n+\t\tRelDataType inputFieldType = filter.getInput().getRowType();\n+\t\tList<String> inputFieldName = inputFieldType.getFieldNames();\n+\n+\t\tList<String> partitionedFieldNames = tableSourceTable.catalogTable().getPartitionKeys();\n+\n+\t\tRelBuilder relBuilder = call.builder();\n+\t\tRexBuilder rexBuilder = relBuilder.getRexBuilder();\n+\n+\t\tTuple2<Seq<RexNode>, Seq<RexNode>> predicate = RexNodeExtractor.extractPartitionPredicateList(\n+\t\t\tfilter.getCondition(),\n+\t\t\tFlinkRelOptUtil.getMaxCnfNodeCount(scan),\n+\t\t\tinputFieldName.toArray(new String[inputFieldName.size()]),\n+\t\t\trexBuilder,\n+\t\t\tpartitionedFieldNames.toArray(new String[partitionedFieldNames.size()])\n+\t\t\t);\n+\n+\t\tRexNode partitionPredicate = RexUtil.composeConjunction(rexBuilder, JavaConversions.seqAsJavaList(predicate._1));\n+\n+\t\tif (partitionPredicate.isAlwaysTrue()){\n+\t\t\treturn;\n+\t\t}\n+\n+\t\tList<LogicalType> partitionFieldType = partitionedFieldNames.stream().map(name -> {\n+\t\t\tint index = inputFieldName.indexOf(name);\n+\t\t\tif (index < 0) {\n+\t\t\t\tthrow new RuntimeException(String.format(\"Partitioned key '%s' isn't found in input columns. \" +\n+\t\t\t\t\t\"Validator should have checked that.\", name));\n+\t\t\t}\n+\t\t\treturn inputFieldType.getFieldList().get(index).getType(); })\n+\t\t\t.map(FlinkTypeFactory::toLogicalType).collect(Collectors.toList());\n+\n+\t\t// checked in matches(call) and listPartitions() is not empty.\n+\t\tList<Map<String, String>> allPartitions = ((SupportsPartitionPushDown) dynamicTableSource).listPartitions().get();\n+\t\tRexNode finalPartitionPredicate = adjustPartitionPredicate(inputFieldName, partitionedFieldNames, partitionPredicate);\n+\t\t// get partitions\n+\t\tList<Map<String, String>> remainingPartitions = PartitionPruner.prunePartitions(\n+\t\t\t((FlinkContext) context).getTableConfig(),\n+\t\t\tpartitionedFieldNames.toArray(new String[partitionedFieldNames.size()]),\n+\t\t\tpartitionFieldType.toArray(new LogicalType[partitionFieldType.size()]),\n+\t\t\tallPartitions,\n+\t\t\tfinalPartitionPredicate\n+\t\t\t);\n+\t\t((SupportsPartitionPushDown) dynamicTableSource).applyPartitions(remainingPartitions);\n+\n+\t\t// build new table scan\n+\t\tFlinkStatistic statistic = tableSourceTable.getStatistic();\n+\t\tString extraDigest = \"source: [partitions=\" +\n+\t\t\tString.join(\", \", ((SupportsPartitionPushDown) dynamicTableSource).listPartitions()\n+\t\t\t\t.get()\n+\t\t\t\t.stream()\n+\t\t\t\t.map(partition -> partition.toString())\n+\t\t\t\t.collect(Collectors.toList())\n+\t\t\t\t.toArray(new String[1])) +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0f19df4d1fcbb4093b69d772628b67b81ebb443a"}, "originalPosition": 146}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg3NzM3MjM0OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QxNDoxNzoxMVrOG3kgww==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QxNDoxNzoxMVrOG3kgww==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDkyMzA3NQ==", "bodyText": "we must copy a new table source here, because we should create a new DynamicTableSource instance to hold the partition info and the original DynamicTableSource instance should not changed", "url": "https://github.com/apache/flink/pull/12966#discussion_r460923075", "createdAt": "2020-07-27T14:17:11Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsPartitionPushDown;\n+import org.apache.flink.table.planner.calcite.FlinkContext;\n+import org.apache.flink.table.planner.calcite.FlinkTypeFactory;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+import org.apache.flink.table.planner.plan.utils.FlinkRelOptUtil;\n+import org.apache.flink.table.planner.plan.utils.PartitionPruner;\n+import org.apache.flink.table.planner.plan.utils.RexNodeExtractor;\n+import org.apache.flink.table.types.logical.LogicalType;\n+\n+import org.apache.calcite.plan.Context;\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Filter;\n+import org.apache.calcite.rel.logical.LogicalTableScan;\n+import org.apache.calcite.rel.type.RelDataType;\n+import org.apache.calcite.rex.RexBuilder;\n+import org.apache.calcite.rex.RexInputRef;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.calcite.rex.RexShuttle;\n+import org.apache.calcite.rex.RexUtil;\n+import org.apache.calcite.tools.RelBuilder;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.Seq;\n+\n+/**\n+ * Planner rule that tries to push partition evaluated by filter condition into a {@link LogicalTableScan}.\n+*/\n+public class PushPartitionIntoTableSourceScanRule extends RelOptRule {\n+\tpublic static final PushPartitionIntoTableSourceScanRule INSTANCE = new PushPartitionIntoTableSourceScanRule();\n+\n+\tpublic PushPartitionIntoTableSourceScanRule(){\n+\t\tsuper(operand(Filter.class,\n+\t\t\t\toperand(LogicalTableScan.class, none())),\n+\t\t\t\"PushPartitionTableSourceScanRule\");\n+\t}\n+\n+\t@Override\n+\tpublic boolean matches(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tif (filter.getCondition() == null) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tTableSourceTable tableSourceTable = call.rel(1).getTable().unwrap(TableSourceTable.class);\n+\t\tif (tableSourceTable == null){\n+\t\t\treturn false;\n+\t\t}\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource();\n+\t\tif (!(dynamicTableSource instanceof SupportsPartitionPushDown)) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tOptional<List<Map<String, String>>> partitions = ((SupportsPartitionPushDown) dynamicTableSource).listPartitions();\n+\t\treturn partitions.isPresent()\n+\t\t\t&& !partitions.get().isEmpty()\n+\t\t\t&& !Arrays.stream(tableSourceTable.extraDigests()).anyMatch(digest -> digest.startsWith(\"source: [partitions=\"));\n+\t}\n+\n+\t@Override\n+\tpublic void onMatch(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tLogicalTableScan scan = call.rel(1);\n+\t\tContext context = call.getPlanner().getContext().unwrap(FlinkContext.class);\n+\t\tTableSourceTable tableSourceTable = scan.getTable().unwrap(TableSourceTable.class);\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0f19df4d1fcbb4093b69d772628b67b81ebb443a"}, "originalPosition": 93}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg3NzM5MjMxOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/factories/TestValuesTableFactory.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QxNDoyMToyNFrOG3kszw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QxNDoyMToyNFrOG3kszw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDkyNjE1OQ==", "bodyText": "please update the convertToRowData method based on the values of remainingPartitions", "url": "https://github.com/apache/flink/pull/12966#discussion_r460926159", "createdAt": "2020-07-27T14:21:24Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/factories/TestValuesTableFactory.java", "diffHunk": "@@ -644,6 +670,16 @@ public String asSummaryString() {\n \t\t\t}\n \t\t\treturn result;\n \t\t}\n+\n+\t\t@Override\n+\t\tpublic Optional<List<Map<String, String>>> listPartitions() {\n+\t\t\treturn Optional.of(allPartitions);\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void applyPartitions(List<Map<String, String>> remainingPartitions) {\n+\t\t\tthis.allPartitions = remainingPartitions;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0f19df4d1fcbb4093b69d772628b67b81ebb443a"}, "originalPosition": 102}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5OTIyMTIwOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QwNzowNjoxOVrOG6uX9g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QwNzowNjoxOVrOG6uX9g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDIzMDM5MA==", "bodyText": "nit: catalogTable.getPartitionKeys().isEmpty() ?", "url": "https://github.com/apache/flink/pull/12966#discussion_r464230390", "createdAt": "2020-08-03T07:06:19Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,320 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.api.TableConfig;\n+import org.apache.flink.table.api.TableException;\n+import org.apache.flink.table.catalog.Catalog;\n+import org.apache.flink.table.catalog.CatalogPartitionSpec;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.catalog.ObjectIdentifier;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.catalog.exceptions.PartitionNotExistException;\n+import org.apache.flink.table.catalog.exceptions.TableNotExistException;\n+import org.apache.flink.table.catalog.exceptions.TableNotPartitionedException;\n+import org.apache.flink.table.catalog.stats.CatalogColumnStatistics;\n+import org.apache.flink.table.catalog.stats.CatalogTableStatistics;\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsPartitionPushDown;\n+import org.apache.flink.table.expressions.Expression;\n+import org.apache.flink.table.expressions.ResolvedExpression;\n+import org.apache.flink.table.plan.stats.TableStats;\n+import org.apache.flink.table.planner.calcite.FlinkContext;\n+import org.apache.flink.table.planner.calcite.FlinkTypeFactory;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+import org.apache.flink.table.planner.plan.utils.FlinkRelOptUtil;\n+import org.apache.flink.table.planner.plan.utils.PartitionPruner;\n+import org.apache.flink.table.planner.plan.utils.RexNodeExtractor;\n+import org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter;\n+import org.apache.flink.table.planner.utils.CatalogTableStatisticsConverter;\n+import org.apache.flink.table.types.logical.LogicalType;\n+\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Filter;\n+import org.apache.calcite.rel.logical.LogicalTableScan;\n+import org.apache.calcite.rel.type.RelDataType;\n+import org.apache.calcite.rex.RexBuilder;\n+import org.apache.calcite.rex.RexInputRef;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.calcite.rex.RexShuttle;\n+import org.apache.calcite.rex.RexUtil;\n+import org.apache.calcite.tools.RelBuilder;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.TimeZone;\n+import java.util.stream.Collectors;\n+\n+import scala.Option;\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.Seq;\n+\n+/**\n+ * Planner rule that tries to push partition evaluated by filter condition into a {@link LogicalTableScan}.\n+*/\n+public class PushPartitionIntoTableSourceScanRule extends RelOptRule {\n+\tpublic static final PushPartitionIntoTableSourceScanRule INSTANCE = new PushPartitionIntoTableSourceScanRule();\n+\n+\tpublic PushPartitionIntoTableSourceScanRule(){\n+\t\tsuper(operand(Filter.class,\n+\t\t\t\toperand(LogicalTableScan.class, none())),\n+\t\t\t\"PushPartitionTableSourceScanRule\");\n+\t}\n+\n+\t@Override\n+\tpublic boolean matches(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tif (filter.getCondition() == null) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tTableSourceTable tableSourceTable = call.rel(1).getTable().unwrap(TableSourceTable.class);\n+\t\tif (tableSourceTable == null){\n+\t\t\treturn false;\n+\t\t}\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource();\n+\t\tif (!(dynamicTableSource instanceof SupportsPartitionPushDown)) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tCatalogTable catalogTable = tableSourceTable.catalogTable();\n+\t\tif (!catalogTable.isPartitioned() || catalogTable.getPartitionKeys().size() == 0) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6697110fb14fef778707f74b66227df2953c487c"}, "originalPosition": 101}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5OTI3OTAzOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QwNzoyODoxOVrOG6u6Ig==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QwNzoyODoxOVrOG6u6Ig==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDIzOTEzOA==", "bodyText": "Initialize variables as close as possible to the point where you use them, please refer to The Principle of Proximity for more detailes", "url": "https://github.com/apache/flink/pull/12966#discussion_r464239138", "createdAt": "2020-08-03T07:28:19Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,320 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.api.TableConfig;\n+import org.apache.flink.table.api.TableException;\n+import org.apache.flink.table.catalog.Catalog;\n+import org.apache.flink.table.catalog.CatalogPartitionSpec;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.catalog.ObjectIdentifier;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.catalog.exceptions.PartitionNotExistException;\n+import org.apache.flink.table.catalog.exceptions.TableNotExistException;\n+import org.apache.flink.table.catalog.exceptions.TableNotPartitionedException;\n+import org.apache.flink.table.catalog.stats.CatalogColumnStatistics;\n+import org.apache.flink.table.catalog.stats.CatalogTableStatistics;\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsPartitionPushDown;\n+import org.apache.flink.table.expressions.Expression;\n+import org.apache.flink.table.expressions.ResolvedExpression;\n+import org.apache.flink.table.plan.stats.TableStats;\n+import org.apache.flink.table.planner.calcite.FlinkContext;\n+import org.apache.flink.table.planner.calcite.FlinkTypeFactory;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+import org.apache.flink.table.planner.plan.utils.FlinkRelOptUtil;\n+import org.apache.flink.table.planner.plan.utils.PartitionPruner;\n+import org.apache.flink.table.planner.plan.utils.RexNodeExtractor;\n+import org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter;\n+import org.apache.flink.table.planner.utils.CatalogTableStatisticsConverter;\n+import org.apache.flink.table.types.logical.LogicalType;\n+\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Filter;\n+import org.apache.calcite.rel.logical.LogicalTableScan;\n+import org.apache.calcite.rel.type.RelDataType;\n+import org.apache.calcite.rex.RexBuilder;\n+import org.apache.calcite.rex.RexInputRef;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.calcite.rex.RexShuttle;\n+import org.apache.calcite.rex.RexUtil;\n+import org.apache.calcite.tools.RelBuilder;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.TimeZone;\n+import java.util.stream.Collectors;\n+\n+import scala.Option;\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.Seq;\n+\n+/**\n+ * Planner rule that tries to push partition evaluated by filter condition into a {@link LogicalTableScan}.\n+*/\n+public class PushPartitionIntoTableSourceScanRule extends RelOptRule {\n+\tpublic static final PushPartitionIntoTableSourceScanRule INSTANCE = new PushPartitionIntoTableSourceScanRule();\n+\n+\tpublic PushPartitionIntoTableSourceScanRule(){\n+\t\tsuper(operand(Filter.class,\n+\t\t\t\toperand(LogicalTableScan.class, none())),\n+\t\t\t\"PushPartitionTableSourceScanRule\");\n+\t}\n+\n+\t@Override\n+\tpublic boolean matches(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tif (filter.getCondition() == null) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tTableSourceTable tableSourceTable = call.rel(1).getTable().unwrap(TableSourceTable.class);\n+\t\tif (tableSourceTable == null){\n+\t\t\treturn false;\n+\t\t}\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource();\n+\t\tif (!(dynamicTableSource instanceof SupportsPartitionPushDown)) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tCatalogTable catalogTable = tableSourceTable.catalogTable();\n+\t\tif (!catalogTable.isPartitioned() || catalogTable.getPartitionKeys().size() == 0) {\n+\t\t\treturn false;\n+\t\t}\n+\t\treturn Arrays.stream(tableSourceTable.extraDigests()).noneMatch(digest -> digest.startsWith(\"partitions=[\"));\n+\t}\n+\n+\t@Override\n+\tpublic void onMatch(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tLogicalTableScan scan = call.rel(1);\n+\t\tFlinkContext context = call.getPlanner().getContext().unwrap(FlinkContext.class);\n+\t\tTableSourceTable tableSourceTable = scan.getTable().unwrap(TableSourceTable.class);\n+\t\t// use new dynamic table source to push down\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource().copy();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6697110fb14fef778707f74b66227df2953c487c"}, "originalPosition": 114}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5OTI4NTg0OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QwNzozMDo0NFrOG6u-Nw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QwNzozMDo0NFrOG6u-Nw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDI0MDE4Mw==", "bodyText": "nit: move them into one line ?", "url": "https://github.com/apache/flink/pull/12966#discussion_r464240183", "createdAt": "2020-08-03T07:30:44Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,320 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.api.TableConfig;\n+import org.apache.flink.table.api.TableException;\n+import org.apache.flink.table.catalog.Catalog;\n+import org.apache.flink.table.catalog.CatalogPartitionSpec;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.catalog.ObjectIdentifier;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.catalog.exceptions.PartitionNotExistException;\n+import org.apache.flink.table.catalog.exceptions.TableNotExistException;\n+import org.apache.flink.table.catalog.exceptions.TableNotPartitionedException;\n+import org.apache.flink.table.catalog.stats.CatalogColumnStatistics;\n+import org.apache.flink.table.catalog.stats.CatalogTableStatistics;\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsPartitionPushDown;\n+import org.apache.flink.table.expressions.Expression;\n+import org.apache.flink.table.expressions.ResolvedExpression;\n+import org.apache.flink.table.plan.stats.TableStats;\n+import org.apache.flink.table.planner.calcite.FlinkContext;\n+import org.apache.flink.table.planner.calcite.FlinkTypeFactory;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+import org.apache.flink.table.planner.plan.utils.FlinkRelOptUtil;\n+import org.apache.flink.table.planner.plan.utils.PartitionPruner;\n+import org.apache.flink.table.planner.plan.utils.RexNodeExtractor;\n+import org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter;\n+import org.apache.flink.table.planner.utils.CatalogTableStatisticsConverter;\n+import org.apache.flink.table.types.logical.LogicalType;\n+\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Filter;\n+import org.apache.calcite.rel.logical.LogicalTableScan;\n+import org.apache.calcite.rel.type.RelDataType;\n+import org.apache.calcite.rex.RexBuilder;\n+import org.apache.calcite.rex.RexInputRef;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.calcite.rex.RexShuttle;\n+import org.apache.calcite.rex.RexUtil;\n+import org.apache.calcite.tools.RelBuilder;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.TimeZone;\n+import java.util.stream.Collectors;\n+\n+import scala.Option;\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.Seq;\n+\n+/**\n+ * Planner rule that tries to push partition evaluated by filter condition into a {@link LogicalTableScan}.\n+*/\n+public class PushPartitionIntoTableSourceScanRule extends RelOptRule {\n+\tpublic static final PushPartitionIntoTableSourceScanRule INSTANCE = new PushPartitionIntoTableSourceScanRule();\n+\n+\tpublic PushPartitionIntoTableSourceScanRule(){\n+\t\tsuper(operand(Filter.class,\n+\t\t\t\toperand(LogicalTableScan.class, none())),\n+\t\t\t\"PushPartitionTableSourceScanRule\");\n+\t}\n+\n+\t@Override\n+\tpublic boolean matches(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tif (filter.getCondition() == null) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tTableSourceTable tableSourceTable = call.rel(1).getTable().unwrap(TableSourceTable.class);\n+\t\tif (tableSourceTable == null){\n+\t\t\treturn false;\n+\t\t}\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource();\n+\t\tif (!(dynamicTableSource instanceof SupportsPartitionPushDown)) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tCatalogTable catalogTable = tableSourceTable.catalogTable();\n+\t\tif (!catalogTable.isPartitioned() || catalogTable.getPartitionKeys().size() == 0) {\n+\t\t\treturn false;\n+\t\t}\n+\t\treturn Arrays.stream(tableSourceTable.extraDigests()).noneMatch(digest -> digest.startsWith(\"partitions=[\"));\n+\t}\n+\n+\t@Override\n+\tpublic void onMatch(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tLogicalTableScan scan = call.rel(1);\n+\t\tFlinkContext context = call.getPlanner().getContext().unwrap(FlinkContext.class);\n+\t\tTableSourceTable tableSourceTable = scan.getTable().unwrap(TableSourceTable.class);\n+\t\t// use new dynamic table source to push down\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource().copy();\n+\t\t// fields to read partitions from catalog and build new statistic\n+\t\tOptional<Catalog> catalogOptional = context.getCatalogManager().getCatalog(tableSourceTable.tableIdentifier().getCatalogName());\n+\t\tObjectIdentifier identifier = tableSourceTable.tableIdentifier();\n+\t\tObjectPath tablePath = identifier.toObjectPath();\n+\t\t// fields used to prune\n+\t\tRelDataType inputFieldTypes = filter.getInput().getRowType();\n+\t\tList<String> inputFieldNames = inputFieldTypes.getFieldNames();\n+\n+\t\tList<String> partitionFieldNames = tableSourceTable.catalogTable().getPartitionKeys();\n+\n+\t\tRelBuilder relBuilder = call.builder();\n+\t\tRexBuilder rexBuilder = relBuilder.getRexBuilder();\n+\n+\t\tTuple2<Seq<RexNode>, Seq<RexNode>> allPredicates = RexNodeExtractor.extractPartitionPredicateList(\n+\t\t\tfilter.getCondition(),\n+\t\t\tFlinkRelOptUtil.getMaxCnfNodeCount(scan),\n+\t\t\tinputFieldNames.toArray(new String[0]),\n+\t\t\trexBuilder,\n+\t\t\tpartitionFieldNames.toArray(new String[0])\n+\t\t\t);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6697110fb14fef778707f74b66227df2953c487c"}, "originalPosition": 134}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5OTM1MzA3OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QwNzo1MzozMVrOG6vmBQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QwNzo1MzozMVrOG6vmBQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDI1MDM3Mw==", "bodyText": "I think it's better we can split the logic to different sub-methods, and simplify this part logic as:\n\nget partition from TableSource, if succeed, do partition pruning and build new table scan, else fallback to step 2\ncheck whether the catalog exists. if not existing, return. else go to step 2.1\n2.1. try to get partitions through listPartitionsByFilter method. if succeed, build new table scan. else go to step 2.2\n2.2. try to get partitions through listPartitions method. if failed, return. else do partition pruning and build new table scan.", "url": "https://github.com/apache/flink/pull/12966#discussion_r464250373", "createdAt": "2020-08-03T07:53:31Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,320 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.api.TableConfig;\n+import org.apache.flink.table.api.TableException;\n+import org.apache.flink.table.catalog.Catalog;\n+import org.apache.flink.table.catalog.CatalogPartitionSpec;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.catalog.ObjectIdentifier;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.catalog.exceptions.PartitionNotExistException;\n+import org.apache.flink.table.catalog.exceptions.TableNotExistException;\n+import org.apache.flink.table.catalog.exceptions.TableNotPartitionedException;\n+import org.apache.flink.table.catalog.stats.CatalogColumnStatistics;\n+import org.apache.flink.table.catalog.stats.CatalogTableStatistics;\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsPartitionPushDown;\n+import org.apache.flink.table.expressions.Expression;\n+import org.apache.flink.table.expressions.ResolvedExpression;\n+import org.apache.flink.table.plan.stats.TableStats;\n+import org.apache.flink.table.planner.calcite.FlinkContext;\n+import org.apache.flink.table.planner.calcite.FlinkTypeFactory;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+import org.apache.flink.table.planner.plan.utils.FlinkRelOptUtil;\n+import org.apache.flink.table.planner.plan.utils.PartitionPruner;\n+import org.apache.flink.table.planner.plan.utils.RexNodeExtractor;\n+import org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter;\n+import org.apache.flink.table.planner.utils.CatalogTableStatisticsConverter;\n+import org.apache.flink.table.types.logical.LogicalType;\n+\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Filter;\n+import org.apache.calcite.rel.logical.LogicalTableScan;\n+import org.apache.calcite.rel.type.RelDataType;\n+import org.apache.calcite.rex.RexBuilder;\n+import org.apache.calcite.rex.RexInputRef;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.calcite.rex.RexShuttle;\n+import org.apache.calcite.rex.RexUtil;\n+import org.apache.calcite.tools.RelBuilder;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.TimeZone;\n+import java.util.stream.Collectors;\n+\n+import scala.Option;\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.Seq;\n+\n+/**\n+ * Planner rule that tries to push partition evaluated by filter condition into a {@link LogicalTableScan}.\n+*/\n+public class PushPartitionIntoTableSourceScanRule extends RelOptRule {\n+\tpublic static final PushPartitionIntoTableSourceScanRule INSTANCE = new PushPartitionIntoTableSourceScanRule();\n+\n+\tpublic PushPartitionIntoTableSourceScanRule(){\n+\t\tsuper(operand(Filter.class,\n+\t\t\t\toperand(LogicalTableScan.class, none())),\n+\t\t\t\"PushPartitionTableSourceScanRule\");\n+\t}\n+\n+\t@Override\n+\tpublic boolean matches(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tif (filter.getCondition() == null) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tTableSourceTable tableSourceTable = call.rel(1).getTable().unwrap(TableSourceTable.class);\n+\t\tif (tableSourceTable == null){\n+\t\t\treturn false;\n+\t\t}\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource();\n+\t\tif (!(dynamicTableSource instanceof SupportsPartitionPushDown)) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tCatalogTable catalogTable = tableSourceTable.catalogTable();\n+\t\tif (!catalogTable.isPartitioned() || catalogTable.getPartitionKeys().size() == 0) {\n+\t\t\treturn false;\n+\t\t}\n+\t\treturn Arrays.stream(tableSourceTable.extraDigests()).noneMatch(digest -> digest.startsWith(\"partitions=[\"));\n+\t}\n+\n+\t@Override\n+\tpublic void onMatch(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tLogicalTableScan scan = call.rel(1);\n+\t\tFlinkContext context = call.getPlanner().getContext().unwrap(FlinkContext.class);\n+\t\tTableSourceTable tableSourceTable = scan.getTable().unwrap(TableSourceTable.class);\n+\t\t// use new dynamic table source to push down\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource().copy();\n+\t\t// fields to read partitions from catalog and build new statistic\n+\t\tOptional<Catalog> catalogOptional = context.getCatalogManager().getCatalog(tableSourceTable.tableIdentifier().getCatalogName());\n+\t\tObjectIdentifier identifier = tableSourceTable.tableIdentifier();\n+\t\tObjectPath tablePath = identifier.toObjectPath();\n+\t\t// fields used to prune\n+\t\tRelDataType inputFieldTypes = filter.getInput().getRowType();\n+\t\tList<String> inputFieldNames = inputFieldTypes.getFieldNames();\n+\n+\t\tList<String> partitionFieldNames = tableSourceTable.catalogTable().getPartitionKeys();\n+\n+\t\tRelBuilder relBuilder = call.builder();\n+\t\tRexBuilder rexBuilder = relBuilder.getRexBuilder();\n+\n+\t\tTuple2<Seq<RexNode>, Seq<RexNode>> allPredicates = RexNodeExtractor.extractPartitionPredicateList(\n+\t\t\tfilter.getCondition(),\n+\t\t\tFlinkRelOptUtil.getMaxCnfNodeCount(scan),\n+\t\t\tinputFieldNames.toArray(new String[0]),\n+\t\t\trexBuilder,\n+\t\t\tpartitionFieldNames.toArray(new String[0])\n+\t\t\t);\n+\n+\t\tRexNode partitionPredicate = RexUtil.composeConjunction(rexBuilder, JavaConversions.seqAsJavaList(allPredicates._1));\n+\n+\t\tif (partitionPredicate.isAlwaysTrue()){\n+\t\t\treturn;\n+\t\t}\n+\n+\t\tList<LogicalType> partitionFieldTypes = partitionFieldNames.stream().map(name -> {\n+\t\t\tint index = inputFieldNames.indexOf(name);\n+\t\t\tif (index < 0) {\n+\t\t\t\tthrow new TableException(String.format(\"Partitioned key '%s' isn't found in input columns. \" +\n+\t\t\t\t\t\"Validator should have checked that.\", name));\n+\t\t\t}\n+\t\t\treturn inputFieldTypes.getFieldList().get(index).getType(); })\n+\t\t\t.map(FlinkTypeFactory::toLogicalType).collect(Collectors.toList());\n+\n+\t\t// get partitions from table source and prune\n+\t\tList<Map<String, String>> remainingPartitions = null;\n+\t\tOptional<List<Map<String, String>>> optionalPartitions = null;\n+\t\ttry {\n+\t\t\toptionalPartitions = ((SupportsPartitionPushDown) dynamicTableSource).listPartitions();\n+\t\t} catch (UnsupportedOperationException e) {\n+\t\t\t// read partitions from catalog if table source doesn't support listPartitions operation.\n+\t\t\tif (!catalogOptional.isPresent()){\n+\t\t\t\tthrow new TableException(\n+\t\t\t\t\tString.format(\"Table %s must from a catalog, but %s is not a catalog\",\n+\t\t\t\t\t\tidentifier.asSummaryString(), identifier.getCatalogName()), e);\n+\t\t\t}\n+\t\t}\n+\t\tif (optionalPartitions != null) {\n+\t\t\tif (!optionalPartitions.isPresent() || optionalPartitions.get().size() == 0) {\n+\t\t\t\t// return if no partitions\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\t// get remaining partitions\n+\t\t\tremainingPartitions = internalPrunePartitions(\n+\t\t\t\toptionalPartitions.get(),\n+\t\t\t\tinputFieldNames,\n+\t\t\t\tpartitionFieldNames,\n+\t\t\t\tpartitionFieldTypes,\n+\t\t\t\tpartitionPredicate,\n+\t\t\t\tcontext.getTableConfig());\n+\t\t} else {\n+\t\t\tRexNodeToExpressionConverter converter = new RexNodeToExpressionConverter(\n+\t\t\t\tinputFieldNames.toArray(new String[0]),\n+\t\t\t\tcontext.getFunctionCatalog(),\n+\t\t\t\tcontext.getCatalogManager(),\n+\t\t\t\tTimeZone.getTimeZone(context.getTableConfig().getLocalTimeZone()));\n+\t\t\tArrayList<Expression> exprs = new ArrayList<>();\n+\t\t\tOption<ResolvedExpression> subExpr;\n+\t\t\tfor (RexNode node: JavaConversions.seqAsJavaList(allPredicates._1)) {\n+\t\t\t\tsubExpr = node.accept(converter);\n+\t\t\t\tif (!subExpr.isEmpty()) {\n+\t\t\t\t\texprs.add(subExpr.get());\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\ttry {\n+\t\t\t\tif (exprs.size() > 0) {\n+\t\t\t\t\tremainingPartitions = catalogOptional.get().listPartitionsByFilter(tablePath, exprs)\n+\t\t\t\t\t\t.stream()\n+\t\t\t\t\t\t.map(CatalogPartitionSpec::getPartitionSpec)\n+\t\t\t\t\t\t.collect(Collectors.toList());\n+\t\t\t\t} else {\n+\t\t\t\t\t// no filter and get all partitions\n+\t\t\t\t\tList<Map<String, String>> partitions = catalogOptional.get().listPartitions(tablePath)\n+\t\t\t\t\t\t.stream()\n+\t\t\t\t\t\t.map(CatalogPartitionSpec::getPartitionSpec)\n+\t\t\t\t\t\t.collect(Collectors.toList());\n+\t\t\t\t\t// prune partitions\n+\t\t\t\t\tif (partitions.size() > 0) {\n+\t\t\t\t\t\tremainingPartitions = internalPrunePartitions(\n+\t\t\t\t\t\t\tpartitions,\n+\t\t\t\t\t\t\tinputFieldNames,\n+\t\t\t\t\t\t\tpartitionFieldNames,\n+\t\t\t\t\t\t\tpartitionFieldTypes,\n+\t\t\t\t\t\t\tpartitionPredicate,\n+\t\t\t\t\t\t\tcontext.getTableConfig());\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\treturn;\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t} catch (TableNotExistException e) {\n+\t\t\t\tthrow new TableException(String.format(\"Table %s is not found in catalog.\", identifier.asSummaryString()), e);\n+\t\t\t} catch (TableNotPartitionedException e) {\n+\t\t\t\t// no partitions in table source\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6697110fb14fef778707f74b66227df2953c487c"}, "originalPosition": 222}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkyMjE0NTA0OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQwNzowNjo1NFrOG-DaKA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQwODoyMDozMFrOG-FUuA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzcyMDc0NA==", "bodyText": "what if the subExpr is empty ?", "url": "https://github.com/apache/flink/pull/12966#discussion_r467720744", "createdAt": "2020-08-10T07:06:54Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,325 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.api.TableException;\n+import org.apache.flink.table.catalog.Catalog;\n+import org.apache.flink.table.catalog.CatalogPartitionSpec;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.catalog.ObjectIdentifier;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.catalog.exceptions.CatalogException;\n+import org.apache.flink.table.catalog.exceptions.PartitionNotExistException;\n+import org.apache.flink.table.catalog.exceptions.TableNotExistException;\n+import org.apache.flink.table.catalog.exceptions.TableNotPartitionedException;\n+import org.apache.flink.table.catalog.stats.CatalogColumnStatistics;\n+import org.apache.flink.table.catalog.stats.CatalogTableStatistics;\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsPartitionPushDown;\n+import org.apache.flink.table.expressions.Expression;\n+import org.apache.flink.table.expressions.ResolvedExpression;\n+import org.apache.flink.table.plan.stats.TableStats;\n+import org.apache.flink.table.planner.calcite.FlinkContext;\n+import org.apache.flink.table.planner.calcite.FlinkTypeFactory;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+import org.apache.flink.table.planner.plan.utils.FlinkRelOptUtil;\n+import org.apache.flink.table.planner.plan.utils.PartitionPruner;\n+import org.apache.flink.table.planner.plan.utils.RexNodeExtractor;\n+import org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter;\n+import org.apache.flink.table.planner.utils.CatalogTableStatisticsConverter;\n+import org.apache.flink.table.types.logical.LogicalType;\n+\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Filter;\n+import org.apache.calcite.rel.logical.LogicalTableScan;\n+import org.apache.calcite.rel.type.RelDataType;\n+import org.apache.calcite.rex.RexBuilder;\n+import org.apache.calcite.rex.RexInputRef;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.calcite.rex.RexShuttle;\n+import org.apache.calcite.rex.RexUtil;\n+import org.apache.calcite.tools.RelBuilder;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.TimeZone;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import scala.Option;\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.Seq;\n+\n+/**\n+ * Planner rule that tries to push partition evaluated by filter condition into a {@link LogicalTableScan}.\n+*/\n+public class PushPartitionIntoTableSourceScanRule extends RelOptRule {\n+\tpublic static final PushPartitionIntoTableSourceScanRule INSTANCE = new PushPartitionIntoTableSourceScanRule();\n+\n+\tpublic PushPartitionIntoTableSourceScanRule(){\n+\t\tsuper(operand(Filter.class,\n+\t\t\t\toperand(LogicalTableScan.class, none())),\n+\t\t\t\"PushPartitionTableSourceScanRule\");\n+\t}\n+\n+\t@Override\n+\tpublic boolean matches(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tif (filter.getCondition() == null) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tTableSourceTable tableSourceTable = call.rel(1).getTable().unwrap(TableSourceTable.class);\n+\t\tif (tableSourceTable == null){\n+\t\t\treturn false;\n+\t\t}\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource();\n+\t\tif (!(dynamicTableSource instanceof SupportsPartitionPushDown)) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tCatalogTable catalogTable = tableSourceTable.catalogTable();\n+\t\tif (!catalogTable.isPartitioned() || catalogTable.getPartitionKeys().isEmpty()) {\n+\t\t\treturn false;\n+\t\t}\n+\t\treturn Arrays.stream(tableSourceTable.extraDigests()).noneMatch(digest -> digest.startsWith(\"partitions=[\"));\n+\t}\n+\n+\t@Override\n+\tpublic void onMatch(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tLogicalTableScan scan = call.rel(1);\n+\t\tFlinkContext context = call.getPlanner().getContext().unwrap(FlinkContext.class);\n+\t\tTableSourceTable tableSourceTable = scan.getTable().unwrap(TableSourceTable.class);\n+\n+\t\t// build pruner\n+\t\tRelDataType inputFieldTypes = filter.getInput().getRowType();\n+\t\tList<String> inputFieldNames = inputFieldTypes.getFieldNames();\n+\t\tList<String> partitionFieldNames = tableSourceTable.catalogTable().getPartitionKeys();\n+\t\tRelBuilder relBuilder = call.builder();\n+\t\tRexBuilder rexBuilder = relBuilder.getRexBuilder();\n+\t\tTuple2<Seq<RexNode>, Seq<RexNode>> allPredicates = RexNodeExtractor.extractPartitionPredicateList(\n+\t\t\tfilter.getCondition(),\n+\t\t\tFlinkRelOptUtil.getMaxCnfNodeCount(scan),\n+\t\t\tinputFieldNames.toArray(new String[0]),\n+\t\t\trexBuilder,\n+\t\t\tpartitionFieldNames.toArray(new String[0])\n+\t\t\t);\n+\t\tRexNode partitionPredicate = RexUtil.composeConjunction(rexBuilder, JavaConversions.seqAsJavaList(allPredicates._1));\n+\n+\t\tif (partitionPredicate.isAlwaysTrue()){\n+\t\t\treturn;\n+\t\t}\n+\n+\t\tList<LogicalType> partitionFieldTypes = partitionFieldNames.stream().map(name -> {\n+\t\t\tint index  = inputFieldNames.indexOf(name);\n+\t\t\tif (index < 0) {\n+\t\t\t\tthrow new TableException(String.format(\"Partitioned key '%s' isn't found in input columns. \" +\n+\t\t\t\t\t\"Validator should have checked that.\", name));\n+\t\t\t}\t\t\treturn inputFieldTypes.getFieldList().get(index).getType(); })\n+\t\t\t.map(FlinkTypeFactory::toLogicalType).collect(Collectors.toList());\n+\n+\t\tRexNode finalPartitionPredicate = adjustPartitionPredicate(inputFieldNames, partitionFieldNames, partitionPredicate);\n+\t\tFunction<List<Map<String, String>>, List<Map<String, String>>> defaultPruner = partitions -> PartitionPruner.prunePartitions(\n+\t\t\tcontext.getTableConfig(),\n+\t\t\tpartitionFieldNames.toArray(new String[0]),\n+\t\t\tpartitionFieldTypes.toArray(new LogicalType[0]),\n+\t\t\tpartitions,\n+\t\t\tfinalPartitionPredicate);\n+\n+\t\t// get partitions from table/catalog and prune\n+\t\tOptional<Catalog> catalogOptional = context.getCatalogManager().getCatalog(tableSourceTable.tableIdentifier().getCatalogName());\n+\t\tList<Map<String, String>> remainingPartitions = null;\n+\t\tOptional<List<Map<String, String>>> optionalPartitions;\n+\t\t// fields to read partitions from catalog and build new statistic\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource().copy();\n+\t\tObjectIdentifier identifier = tableSourceTable.tableIdentifier();\n+\t\tObjectPath tablePath = identifier.toObjectPath();\n+\t\ttry {\n+\t\t\toptionalPartitions = ((SupportsPartitionPushDown) dynamicTableSource).listPartitions();\n+\t\t\tif (optionalPartitions.isPresent() && !optionalPartitions.get().isEmpty()) {\n+\t\t\t\tremainingPartitions = defaultPruner.apply(optionalPartitions.get());\n+\t\t\t}\n+\t\t} catch (UnsupportedOperationException e) {\n+\t\t\t// check catalog whether is available\n+\t\t\t// we will read partitions from catalog if table doesn't support listPartitions.\n+\t\t\tif (!catalogOptional.isPresent()){\n+\t\t\t\tthrow new TableException(\n+\t\t\t\t\tString.format(\"Table %s must from a catalog, but %s is not a catalog\",\n+\t\t\t\t\t\tidentifier.asSummaryString(), identifier.getCatalogName()), e);\n+\t\t\t}\n+\t\t\ttry {\n+\t\t\t\toptionalPartitions = readPartitionFromCatalogAndPrune(\n+\t\t\t\t\tcontext,\n+\t\t\t\t\tcatalogOptional.get(),\n+\t\t\t\t\tidentifier,\n+\t\t\t\t\tinputFieldNames,\n+\t\t\t\t\tallPredicates._1(),\n+\t\t\t\t\tdefaultPruner\n+\t\t\t\t);\n+\t\t\t\tif (optionalPartitions.isPresent() && !optionalPartitions.get().isEmpty()) {\n+\t\t\t\t\tremainingPartitions = optionalPartitions.get();\n+\t\t\t\t}\n+\t\t\t} catch (TableNotExistException tableNotExistException) {\n+\t\t\t\tthrow new TableException(String.format(\"Table %s is not found in catalog.\", identifier.asSummaryString()), e);\n+\t\t\t} catch (TableNotPartitionedException tableNotPartitionedException) {\n+\t\t\t\tremainingPartitions = null;\n+\t\t\t}\n+\t\t}\n+\t\tif (remainingPartitions != null) {\n+\t\t\t((SupportsPartitionPushDown) dynamicTableSource).applyPartitions(remainingPartitions);\n+\t\t}\n+\n+\t\t// build new statistic\n+\t\tTableStats newTableStat = null;\n+\t\tOptional<TableStats> partitionStats;\n+\t\tif (remainingPartitions != null && catalogOptional.isPresent()) {\n+\t\t\tfor (Map<String, String> partition: remainingPartitions) {\n+\t\t\t\tpartitionStats = getPartitionStats(catalogOptional.get(), tablePath, partition);\n+\t\t\t\tif (!partitionStats.isPresent()) {\n+\t\t\t\t\t// clear all information before\n+\t\t\t\t\tnewTableStat = null;\n+\t\t\t\t\tbreak;\n+\t\t\t\t} else {\n+\t\t\t\t\tnewTableStat = newTableStat == null ? partitionStats.get() : newTableStat.merge(partitionStats.get());\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t\tFlinkStatistic newStatistic = FlinkStatistic.builder()\n+\t\t\t.statistic(tableSourceTable.getStatistic())\n+\t\t\t.tableStats(newTableStat)\n+\t\t\t.build();\n+\n+\t\tString extraDigest = remainingPartitions == null ? \"partitions=[]\" :\n+\t\t\t(\"partitions=[\" +\n+\t\t\t\tString.join(\", \", remainingPartitions\n+\t\t\t\t\t.stream()\n+\t\t\t\t\t.map(Object::toString)\n+\t\t\t\t\t.toArray(String[]::new)) +\n+\t\t\t\t\"]\");\n+\t\tTableSourceTable newTableSourceTable = tableSourceTable.copy(dynamicTableSource, newStatistic, new String[]{extraDigest});\n+\n+\t\tLogicalTableScan newScan = LogicalTableScan.create(scan.getCluster(), newTableSourceTable, scan.getHints());\n+\n+\t\tRexNode nonPartitionPredicate = RexUtil.composeConjunction(rexBuilder, JavaConversions.seqAsJavaList(allPredicates._2()));\n+\t\tif (nonPartitionPredicate.isAlwaysTrue()) {\n+\t\t\tcall.transformTo(newScan);\n+\t\t} else {\n+\t\t\tFilter newFilter = filter.copy(filter.getTraitSet(), newScan, nonPartitionPredicate);\n+\t\t\tcall.transformTo(newFilter);\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * adjust the partition field reference index to evaluate the partition values.\n+\t * e.g. the original input fields is: a, b, c, p, and p is partition field. the partition values\n+\t * are: List(Map(\"p\"->\"1\"), Map(\"p\" -> \"2\"), Map(\"p\" -> \"3\")). If the original partition\n+\t * predicate is $3 > 1. after adjusting, the new predicate is ($0 > 1).\n+\t * and use ($0 > 1) to evaluate partition values (row(1), row(2), row(3)).\n+\t */\n+\tprivate RexNode adjustPartitionPredicate(List<String> inputFieldNames, List<String> partitionFieldNames, RexNode partitionPredicate) {\n+\t\treturn partitionPredicate.accept(new RexShuttle(){\n+\t\t\t@Override\n+\t\t\tpublic RexNode visitInputRef(RexInputRef inputRef) {\n+\t\t\t\tint index = inputRef.getIndex();\n+\t\t\t\tString fieldName = inputFieldNames.get(index);\n+\t\t\t\tint newIndex = partitionFieldNames.indexOf(fieldName);\n+\t\t\t\tif (newIndex < 0) {\n+\t\t\t\t\tthrow new TableException(String.format(\"Field name '%s' isn't found in partitioned columns.\" +\n+\t\t\t\t\t\t\" Validator should have checked that.\", fieldName));\n+\t\t\t\t}\n+\t\t\t\tif (newIndex == index){\n+\t\t\t\t\treturn inputRef;\n+\t\t\t\t} else {\n+\t\t\t\t\treturn new RexInputRef(newIndex, inputRef.getType());\n+\t\t\t\t}\n+\t\t\t}\n+\t\t});\n+\t}\n+\n+\tprivate Optional<List<Map<String, String>>> readPartitionFromCatalogAndPrune(\n+\t\t\tFlinkContext context,\n+\t\t\tCatalog catalog,\n+\t\t\tObjectIdentifier tableIdentifier,\n+\t\t\tList<String> allFieldNames,\n+\t\t\tSeq<RexNode> partitionPredicate,\n+\t\t\tFunction<List<Map<String, String>>, List<Map<String, String>>> pruner) throws TableNotExistException, TableNotPartitionedException{\n+\t\tRexNodeToExpressionConverter converter = new RexNodeToExpressionConverter(\n+\t\t\tallFieldNames.toArray(new String[0]),\n+\t\t\tcontext.getFunctionCatalog(),\n+\t\t\tcontext.getCatalogManager(),\n+\t\t\tTimeZone.getTimeZone(context.getTableConfig().getLocalTimeZone()));\n+\t\tArrayList<Expression> partitionFilters = new ArrayList<>();\n+\t\tOption<ResolvedExpression> subExpr;\n+\t\tfor (RexNode node: JavaConversions.seqAsJavaList(partitionPredicate)) {\n+\t\t\tsubExpr = node.accept(converter);\n+\t\t\tif (!subExpr.isEmpty()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9a05a030c661def4a45b36370dbcfa5e786ed8dc"}, "originalPosition": 276}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Nzc0NTQ4Nw==", "bodyText": "we only use subExpr to accept value that is calculated from node. we have checked whether subExpr isEmpty in line 276", "url": "https://github.com/apache/flink/pull/12966#discussion_r467745487", "createdAt": "2020-08-10T08:07:08Z", "author": {"login": "fsk119"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,325 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.api.TableException;\n+import org.apache.flink.table.catalog.Catalog;\n+import org.apache.flink.table.catalog.CatalogPartitionSpec;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.catalog.ObjectIdentifier;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.catalog.exceptions.CatalogException;\n+import org.apache.flink.table.catalog.exceptions.PartitionNotExistException;\n+import org.apache.flink.table.catalog.exceptions.TableNotExistException;\n+import org.apache.flink.table.catalog.exceptions.TableNotPartitionedException;\n+import org.apache.flink.table.catalog.stats.CatalogColumnStatistics;\n+import org.apache.flink.table.catalog.stats.CatalogTableStatistics;\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsPartitionPushDown;\n+import org.apache.flink.table.expressions.Expression;\n+import org.apache.flink.table.expressions.ResolvedExpression;\n+import org.apache.flink.table.plan.stats.TableStats;\n+import org.apache.flink.table.planner.calcite.FlinkContext;\n+import org.apache.flink.table.planner.calcite.FlinkTypeFactory;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+import org.apache.flink.table.planner.plan.utils.FlinkRelOptUtil;\n+import org.apache.flink.table.planner.plan.utils.PartitionPruner;\n+import org.apache.flink.table.planner.plan.utils.RexNodeExtractor;\n+import org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter;\n+import org.apache.flink.table.planner.utils.CatalogTableStatisticsConverter;\n+import org.apache.flink.table.types.logical.LogicalType;\n+\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Filter;\n+import org.apache.calcite.rel.logical.LogicalTableScan;\n+import org.apache.calcite.rel.type.RelDataType;\n+import org.apache.calcite.rex.RexBuilder;\n+import org.apache.calcite.rex.RexInputRef;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.calcite.rex.RexShuttle;\n+import org.apache.calcite.rex.RexUtil;\n+import org.apache.calcite.tools.RelBuilder;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.TimeZone;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import scala.Option;\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.Seq;\n+\n+/**\n+ * Planner rule that tries to push partition evaluated by filter condition into a {@link LogicalTableScan}.\n+*/\n+public class PushPartitionIntoTableSourceScanRule extends RelOptRule {\n+\tpublic static final PushPartitionIntoTableSourceScanRule INSTANCE = new PushPartitionIntoTableSourceScanRule();\n+\n+\tpublic PushPartitionIntoTableSourceScanRule(){\n+\t\tsuper(operand(Filter.class,\n+\t\t\t\toperand(LogicalTableScan.class, none())),\n+\t\t\t\"PushPartitionTableSourceScanRule\");\n+\t}\n+\n+\t@Override\n+\tpublic boolean matches(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tif (filter.getCondition() == null) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tTableSourceTable tableSourceTable = call.rel(1).getTable().unwrap(TableSourceTable.class);\n+\t\tif (tableSourceTable == null){\n+\t\t\treturn false;\n+\t\t}\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource();\n+\t\tif (!(dynamicTableSource instanceof SupportsPartitionPushDown)) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tCatalogTable catalogTable = tableSourceTable.catalogTable();\n+\t\tif (!catalogTable.isPartitioned() || catalogTable.getPartitionKeys().isEmpty()) {\n+\t\t\treturn false;\n+\t\t}\n+\t\treturn Arrays.stream(tableSourceTable.extraDigests()).noneMatch(digest -> digest.startsWith(\"partitions=[\"));\n+\t}\n+\n+\t@Override\n+\tpublic void onMatch(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tLogicalTableScan scan = call.rel(1);\n+\t\tFlinkContext context = call.getPlanner().getContext().unwrap(FlinkContext.class);\n+\t\tTableSourceTable tableSourceTable = scan.getTable().unwrap(TableSourceTable.class);\n+\n+\t\t// build pruner\n+\t\tRelDataType inputFieldTypes = filter.getInput().getRowType();\n+\t\tList<String> inputFieldNames = inputFieldTypes.getFieldNames();\n+\t\tList<String> partitionFieldNames = tableSourceTable.catalogTable().getPartitionKeys();\n+\t\tRelBuilder relBuilder = call.builder();\n+\t\tRexBuilder rexBuilder = relBuilder.getRexBuilder();\n+\t\tTuple2<Seq<RexNode>, Seq<RexNode>> allPredicates = RexNodeExtractor.extractPartitionPredicateList(\n+\t\t\tfilter.getCondition(),\n+\t\t\tFlinkRelOptUtil.getMaxCnfNodeCount(scan),\n+\t\t\tinputFieldNames.toArray(new String[0]),\n+\t\t\trexBuilder,\n+\t\t\tpartitionFieldNames.toArray(new String[0])\n+\t\t\t);\n+\t\tRexNode partitionPredicate = RexUtil.composeConjunction(rexBuilder, JavaConversions.seqAsJavaList(allPredicates._1));\n+\n+\t\tif (partitionPredicate.isAlwaysTrue()){\n+\t\t\treturn;\n+\t\t}\n+\n+\t\tList<LogicalType> partitionFieldTypes = partitionFieldNames.stream().map(name -> {\n+\t\t\tint index  = inputFieldNames.indexOf(name);\n+\t\t\tif (index < 0) {\n+\t\t\t\tthrow new TableException(String.format(\"Partitioned key '%s' isn't found in input columns. \" +\n+\t\t\t\t\t\"Validator should have checked that.\", name));\n+\t\t\t}\t\t\treturn inputFieldTypes.getFieldList().get(index).getType(); })\n+\t\t\t.map(FlinkTypeFactory::toLogicalType).collect(Collectors.toList());\n+\n+\t\tRexNode finalPartitionPredicate = adjustPartitionPredicate(inputFieldNames, partitionFieldNames, partitionPredicate);\n+\t\tFunction<List<Map<String, String>>, List<Map<String, String>>> defaultPruner = partitions -> PartitionPruner.prunePartitions(\n+\t\t\tcontext.getTableConfig(),\n+\t\t\tpartitionFieldNames.toArray(new String[0]),\n+\t\t\tpartitionFieldTypes.toArray(new LogicalType[0]),\n+\t\t\tpartitions,\n+\t\t\tfinalPartitionPredicate);\n+\n+\t\t// get partitions from table/catalog and prune\n+\t\tOptional<Catalog> catalogOptional = context.getCatalogManager().getCatalog(tableSourceTable.tableIdentifier().getCatalogName());\n+\t\tList<Map<String, String>> remainingPartitions = null;\n+\t\tOptional<List<Map<String, String>>> optionalPartitions;\n+\t\t// fields to read partitions from catalog and build new statistic\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource().copy();\n+\t\tObjectIdentifier identifier = tableSourceTable.tableIdentifier();\n+\t\tObjectPath tablePath = identifier.toObjectPath();\n+\t\ttry {\n+\t\t\toptionalPartitions = ((SupportsPartitionPushDown) dynamicTableSource).listPartitions();\n+\t\t\tif (optionalPartitions.isPresent() && !optionalPartitions.get().isEmpty()) {\n+\t\t\t\tremainingPartitions = defaultPruner.apply(optionalPartitions.get());\n+\t\t\t}\n+\t\t} catch (UnsupportedOperationException e) {\n+\t\t\t// check catalog whether is available\n+\t\t\t// we will read partitions from catalog if table doesn't support listPartitions.\n+\t\t\tif (!catalogOptional.isPresent()){\n+\t\t\t\tthrow new TableException(\n+\t\t\t\t\tString.format(\"Table %s must from a catalog, but %s is not a catalog\",\n+\t\t\t\t\t\tidentifier.asSummaryString(), identifier.getCatalogName()), e);\n+\t\t\t}\n+\t\t\ttry {\n+\t\t\t\toptionalPartitions = readPartitionFromCatalogAndPrune(\n+\t\t\t\t\tcontext,\n+\t\t\t\t\tcatalogOptional.get(),\n+\t\t\t\t\tidentifier,\n+\t\t\t\t\tinputFieldNames,\n+\t\t\t\t\tallPredicates._1(),\n+\t\t\t\t\tdefaultPruner\n+\t\t\t\t);\n+\t\t\t\tif (optionalPartitions.isPresent() && !optionalPartitions.get().isEmpty()) {\n+\t\t\t\t\tremainingPartitions = optionalPartitions.get();\n+\t\t\t\t}\n+\t\t\t} catch (TableNotExistException tableNotExistException) {\n+\t\t\t\tthrow new TableException(String.format(\"Table %s is not found in catalog.\", identifier.asSummaryString()), e);\n+\t\t\t} catch (TableNotPartitionedException tableNotPartitionedException) {\n+\t\t\t\tremainingPartitions = null;\n+\t\t\t}\n+\t\t}\n+\t\tif (remainingPartitions != null) {\n+\t\t\t((SupportsPartitionPushDown) dynamicTableSource).applyPartitions(remainingPartitions);\n+\t\t}\n+\n+\t\t// build new statistic\n+\t\tTableStats newTableStat = null;\n+\t\tOptional<TableStats> partitionStats;\n+\t\tif (remainingPartitions != null && catalogOptional.isPresent()) {\n+\t\t\tfor (Map<String, String> partition: remainingPartitions) {\n+\t\t\t\tpartitionStats = getPartitionStats(catalogOptional.get(), tablePath, partition);\n+\t\t\t\tif (!partitionStats.isPresent()) {\n+\t\t\t\t\t// clear all information before\n+\t\t\t\t\tnewTableStat = null;\n+\t\t\t\t\tbreak;\n+\t\t\t\t} else {\n+\t\t\t\t\tnewTableStat = newTableStat == null ? partitionStats.get() : newTableStat.merge(partitionStats.get());\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t\tFlinkStatistic newStatistic = FlinkStatistic.builder()\n+\t\t\t.statistic(tableSourceTable.getStatistic())\n+\t\t\t.tableStats(newTableStat)\n+\t\t\t.build();\n+\n+\t\tString extraDigest = remainingPartitions == null ? \"partitions=[]\" :\n+\t\t\t(\"partitions=[\" +\n+\t\t\t\tString.join(\", \", remainingPartitions\n+\t\t\t\t\t.stream()\n+\t\t\t\t\t.map(Object::toString)\n+\t\t\t\t\t.toArray(String[]::new)) +\n+\t\t\t\t\"]\");\n+\t\tTableSourceTable newTableSourceTable = tableSourceTable.copy(dynamicTableSource, newStatistic, new String[]{extraDigest});\n+\n+\t\tLogicalTableScan newScan = LogicalTableScan.create(scan.getCluster(), newTableSourceTable, scan.getHints());\n+\n+\t\tRexNode nonPartitionPredicate = RexUtil.composeConjunction(rexBuilder, JavaConversions.seqAsJavaList(allPredicates._2()));\n+\t\tif (nonPartitionPredicate.isAlwaysTrue()) {\n+\t\t\tcall.transformTo(newScan);\n+\t\t} else {\n+\t\t\tFilter newFilter = filter.copy(filter.getTraitSet(), newScan, nonPartitionPredicate);\n+\t\t\tcall.transformTo(newFilter);\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * adjust the partition field reference index to evaluate the partition values.\n+\t * e.g. the original input fields is: a, b, c, p, and p is partition field. the partition values\n+\t * are: List(Map(\"p\"->\"1\"), Map(\"p\" -> \"2\"), Map(\"p\" -> \"3\")). If the original partition\n+\t * predicate is $3 > 1. after adjusting, the new predicate is ($0 > 1).\n+\t * and use ($0 > 1) to evaluate partition values (row(1), row(2), row(3)).\n+\t */\n+\tprivate RexNode adjustPartitionPredicate(List<String> inputFieldNames, List<String> partitionFieldNames, RexNode partitionPredicate) {\n+\t\treturn partitionPredicate.accept(new RexShuttle(){\n+\t\t\t@Override\n+\t\t\tpublic RexNode visitInputRef(RexInputRef inputRef) {\n+\t\t\t\tint index = inputRef.getIndex();\n+\t\t\t\tString fieldName = inputFieldNames.get(index);\n+\t\t\t\tint newIndex = partitionFieldNames.indexOf(fieldName);\n+\t\t\t\tif (newIndex < 0) {\n+\t\t\t\t\tthrow new TableException(String.format(\"Field name '%s' isn't found in partitioned columns.\" +\n+\t\t\t\t\t\t\" Validator should have checked that.\", fieldName));\n+\t\t\t\t}\n+\t\t\t\tif (newIndex == index){\n+\t\t\t\t\treturn inputRef;\n+\t\t\t\t} else {\n+\t\t\t\t\treturn new RexInputRef(newIndex, inputRef.getType());\n+\t\t\t\t}\n+\t\t\t}\n+\t\t});\n+\t}\n+\n+\tprivate Optional<List<Map<String, String>>> readPartitionFromCatalogAndPrune(\n+\t\t\tFlinkContext context,\n+\t\t\tCatalog catalog,\n+\t\t\tObjectIdentifier tableIdentifier,\n+\t\t\tList<String> allFieldNames,\n+\t\t\tSeq<RexNode> partitionPredicate,\n+\t\t\tFunction<List<Map<String, String>>, List<Map<String, String>>> pruner) throws TableNotExistException, TableNotPartitionedException{\n+\t\tRexNodeToExpressionConverter converter = new RexNodeToExpressionConverter(\n+\t\t\tallFieldNames.toArray(new String[0]),\n+\t\t\tcontext.getFunctionCatalog(),\n+\t\t\tcontext.getCatalogManager(),\n+\t\t\tTimeZone.getTimeZone(context.getTableConfig().getLocalTimeZone()));\n+\t\tArrayList<Expression> partitionFilters = new ArrayList<>();\n+\t\tOption<ResolvedExpression> subExpr;\n+\t\tfor (RexNode node: JavaConversions.seqAsJavaList(partitionPredicate)) {\n+\t\t\tsubExpr = node.accept(converter);\n+\t\t\tif (!subExpr.isEmpty()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzcyMDc0NA=="}, "originalCommit": {"oid": "9a05a030c661def4a45b36370dbcfa5e786ed8dc"}, "originalPosition": 276}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Nzc1MjEyMA==", "bodyText": "if subExpr is empty, its corresponding sub-filter is dropped, then the result is incorrect. There is no test coverage...", "url": "https://github.com/apache/flink/pull/12966#discussion_r467752120", "createdAt": "2020-08-10T08:20:30Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,325 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.api.TableException;\n+import org.apache.flink.table.catalog.Catalog;\n+import org.apache.flink.table.catalog.CatalogPartitionSpec;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.catalog.ObjectIdentifier;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.catalog.exceptions.CatalogException;\n+import org.apache.flink.table.catalog.exceptions.PartitionNotExistException;\n+import org.apache.flink.table.catalog.exceptions.TableNotExistException;\n+import org.apache.flink.table.catalog.exceptions.TableNotPartitionedException;\n+import org.apache.flink.table.catalog.stats.CatalogColumnStatistics;\n+import org.apache.flink.table.catalog.stats.CatalogTableStatistics;\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsPartitionPushDown;\n+import org.apache.flink.table.expressions.Expression;\n+import org.apache.flink.table.expressions.ResolvedExpression;\n+import org.apache.flink.table.plan.stats.TableStats;\n+import org.apache.flink.table.planner.calcite.FlinkContext;\n+import org.apache.flink.table.planner.calcite.FlinkTypeFactory;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+import org.apache.flink.table.planner.plan.utils.FlinkRelOptUtil;\n+import org.apache.flink.table.planner.plan.utils.PartitionPruner;\n+import org.apache.flink.table.planner.plan.utils.RexNodeExtractor;\n+import org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter;\n+import org.apache.flink.table.planner.utils.CatalogTableStatisticsConverter;\n+import org.apache.flink.table.types.logical.LogicalType;\n+\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Filter;\n+import org.apache.calcite.rel.logical.LogicalTableScan;\n+import org.apache.calcite.rel.type.RelDataType;\n+import org.apache.calcite.rex.RexBuilder;\n+import org.apache.calcite.rex.RexInputRef;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.calcite.rex.RexShuttle;\n+import org.apache.calcite.rex.RexUtil;\n+import org.apache.calcite.tools.RelBuilder;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.TimeZone;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import scala.Option;\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.Seq;\n+\n+/**\n+ * Planner rule that tries to push partition evaluated by filter condition into a {@link LogicalTableScan}.\n+*/\n+public class PushPartitionIntoTableSourceScanRule extends RelOptRule {\n+\tpublic static final PushPartitionIntoTableSourceScanRule INSTANCE = new PushPartitionIntoTableSourceScanRule();\n+\n+\tpublic PushPartitionIntoTableSourceScanRule(){\n+\t\tsuper(operand(Filter.class,\n+\t\t\t\toperand(LogicalTableScan.class, none())),\n+\t\t\t\"PushPartitionTableSourceScanRule\");\n+\t}\n+\n+\t@Override\n+\tpublic boolean matches(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tif (filter.getCondition() == null) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tTableSourceTable tableSourceTable = call.rel(1).getTable().unwrap(TableSourceTable.class);\n+\t\tif (tableSourceTable == null){\n+\t\t\treturn false;\n+\t\t}\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource();\n+\t\tif (!(dynamicTableSource instanceof SupportsPartitionPushDown)) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tCatalogTable catalogTable = tableSourceTable.catalogTable();\n+\t\tif (!catalogTable.isPartitioned() || catalogTable.getPartitionKeys().isEmpty()) {\n+\t\t\treturn false;\n+\t\t}\n+\t\treturn Arrays.stream(tableSourceTable.extraDigests()).noneMatch(digest -> digest.startsWith(\"partitions=[\"));\n+\t}\n+\n+\t@Override\n+\tpublic void onMatch(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tLogicalTableScan scan = call.rel(1);\n+\t\tFlinkContext context = call.getPlanner().getContext().unwrap(FlinkContext.class);\n+\t\tTableSourceTable tableSourceTable = scan.getTable().unwrap(TableSourceTable.class);\n+\n+\t\t// build pruner\n+\t\tRelDataType inputFieldTypes = filter.getInput().getRowType();\n+\t\tList<String> inputFieldNames = inputFieldTypes.getFieldNames();\n+\t\tList<String> partitionFieldNames = tableSourceTable.catalogTable().getPartitionKeys();\n+\t\tRelBuilder relBuilder = call.builder();\n+\t\tRexBuilder rexBuilder = relBuilder.getRexBuilder();\n+\t\tTuple2<Seq<RexNode>, Seq<RexNode>> allPredicates = RexNodeExtractor.extractPartitionPredicateList(\n+\t\t\tfilter.getCondition(),\n+\t\t\tFlinkRelOptUtil.getMaxCnfNodeCount(scan),\n+\t\t\tinputFieldNames.toArray(new String[0]),\n+\t\t\trexBuilder,\n+\t\t\tpartitionFieldNames.toArray(new String[0])\n+\t\t\t);\n+\t\tRexNode partitionPredicate = RexUtil.composeConjunction(rexBuilder, JavaConversions.seqAsJavaList(allPredicates._1));\n+\n+\t\tif (partitionPredicate.isAlwaysTrue()){\n+\t\t\treturn;\n+\t\t}\n+\n+\t\tList<LogicalType> partitionFieldTypes = partitionFieldNames.stream().map(name -> {\n+\t\t\tint index  = inputFieldNames.indexOf(name);\n+\t\t\tif (index < 0) {\n+\t\t\t\tthrow new TableException(String.format(\"Partitioned key '%s' isn't found in input columns. \" +\n+\t\t\t\t\t\"Validator should have checked that.\", name));\n+\t\t\t}\t\t\treturn inputFieldTypes.getFieldList().get(index).getType(); })\n+\t\t\t.map(FlinkTypeFactory::toLogicalType).collect(Collectors.toList());\n+\n+\t\tRexNode finalPartitionPredicate = adjustPartitionPredicate(inputFieldNames, partitionFieldNames, partitionPredicate);\n+\t\tFunction<List<Map<String, String>>, List<Map<String, String>>> defaultPruner = partitions -> PartitionPruner.prunePartitions(\n+\t\t\tcontext.getTableConfig(),\n+\t\t\tpartitionFieldNames.toArray(new String[0]),\n+\t\t\tpartitionFieldTypes.toArray(new LogicalType[0]),\n+\t\t\tpartitions,\n+\t\t\tfinalPartitionPredicate);\n+\n+\t\t// get partitions from table/catalog and prune\n+\t\tOptional<Catalog> catalogOptional = context.getCatalogManager().getCatalog(tableSourceTable.tableIdentifier().getCatalogName());\n+\t\tList<Map<String, String>> remainingPartitions = null;\n+\t\tOptional<List<Map<String, String>>> optionalPartitions;\n+\t\t// fields to read partitions from catalog and build new statistic\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource().copy();\n+\t\tObjectIdentifier identifier = tableSourceTable.tableIdentifier();\n+\t\tObjectPath tablePath = identifier.toObjectPath();\n+\t\ttry {\n+\t\t\toptionalPartitions = ((SupportsPartitionPushDown) dynamicTableSource).listPartitions();\n+\t\t\tif (optionalPartitions.isPresent() && !optionalPartitions.get().isEmpty()) {\n+\t\t\t\tremainingPartitions = defaultPruner.apply(optionalPartitions.get());\n+\t\t\t}\n+\t\t} catch (UnsupportedOperationException e) {\n+\t\t\t// check catalog whether is available\n+\t\t\t// we will read partitions from catalog if table doesn't support listPartitions.\n+\t\t\tif (!catalogOptional.isPresent()){\n+\t\t\t\tthrow new TableException(\n+\t\t\t\t\tString.format(\"Table %s must from a catalog, but %s is not a catalog\",\n+\t\t\t\t\t\tidentifier.asSummaryString(), identifier.getCatalogName()), e);\n+\t\t\t}\n+\t\t\ttry {\n+\t\t\t\toptionalPartitions = readPartitionFromCatalogAndPrune(\n+\t\t\t\t\tcontext,\n+\t\t\t\t\tcatalogOptional.get(),\n+\t\t\t\t\tidentifier,\n+\t\t\t\t\tinputFieldNames,\n+\t\t\t\t\tallPredicates._1(),\n+\t\t\t\t\tdefaultPruner\n+\t\t\t\t);\n+\t\t\t\tif (optionalPartitions.isPresent() && !optionalPartitions.get().isEmpty()) {\n+\t\t\t\t\tremainingPartitions = optionalPartitions.get();\n+\t\t\t\t}\n+\t\t\t} catch (TableNotExistException tableNotExistException) {\n+\t\t\t\tthrow new TableException(String.format(\"Table %s is not found in catalog.\", identifier.asSummaryString()), e);\n+\t\t\t} catch (TableNotPartitionedException tableNotPartitionedException) {\n+\t\t\t\tremainingPartitions = null;\n+\t\t\t}\n+\t\t}\n+\t\tif (remainingPartitions != null) {\n+\t\t\t((SupportsPartitionPushDown) dynamicTableSource).applyPartitions(remainingPartitions);\n+\t\t}\n+\n+\t\t// build new statistic\n+\t\tTableStats newTableStat = null;\n+\t\tOptional<TableStats> partitionStats;\n+\t\tif (remainingPartitions != null && catalogOptional.isPresent()) {\n+\t\t\tfor (Map<String, String> partition: remainingPartitions) {\n+\t\t\t\tpartitionStats = getPartitionStats(catalogOptional.get(), tablePath, partition);\n+\t\t\t\tif (!partitionStats.isPresent()) {\n+\t\t\t\t\t// clear all information before\n+\t\t\t\t\tnewTableStat = null;\n+\t\t\t\t\tbreak;\n+\t\t\t\t} else {\n+\t\t\t\t\tnewTableStat = newTableStat == null ? partitionStats.get() : newTableStat.merge(partitionStats.get());\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t\tFlinkStatistic newStatistic = FlinkStatistic.builder()\n+\t\t\t.statistic(tableSourceTable.getStatistic())\n+\t\t\t.tableStats(newTableStat)\n+\t\t\t.build();\n+\n+\t\tString extraDigest = remainingPartitions == null ? \"partitions=[]\" :\n+\t\t\t(\"partitions=[\" +\n+\t\t\t\tString.join(\", \", remainingPartitions\n+\t\t\t\t\t.stream()\n+\t\t\t\t\t.map(Object::toString)\n+\t\t\t\t\t.toArray(String[]::new)) +\n+\t\t\t\t\"]\");\n+\t\tTableSourceTable newTableSourceTable = tableSourceTable.copy(dynamicTableSource, newStatistic, new String[]{extraDigest});\n+\n+\t\tLogicalTableScan newScan = LogicalTableScan.create(scan.getCluster(), newTableSourceTable, scan.getHints());\n+\n+\t\tRexNode nonPartitionPredicate = RexUtil.composeConjunction(rexBuilder, JavaConversions.seqAsJavaList(allPredicates._2()));\n+\t\tif (nonPartitionPredicate.isAlwaysTrue()) {\n+\t\t\tcall.transformTo(newScan);\n+\t\t} else {\n+\t\t\tFilter newFilter = filter.copy(filter.getTraitSet(), newScan, nonPartitionPredicate);\n+\t\t\tcall.transformTo(newFilter);\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * adjust the partition field reference index to evaluate the partition values.\n+\t * e.g. the original input fields is: a, b, c, p, and p is partition field. the partition values\n+\t * are: List(Map(\"p\"->\"1\"), Map(\"p\" -> \"2\"), Map(\"p\" -> \"3\")). If the original partition\n+\t * predicate is $3 > 1. after adjusting, the new predicate is ($0 > 1).\n+\t * and use ($0 > 1) to evaluate partition values (row(1), row(2), row(3)).\n+\t */\n+\tprivate RexNode adjustPartitionPredicate(List<String> inputFieldNames, List<String> partitionFieldNames, RexNode partitionPredicate) {\n+\t\treturn partitionPredicate.accept(new RexShuttle(){\n+\t\t\t@Override\n+\t\t\tpublic RexNode visitInputRef(RexInputRef inputRef) {\n+\t\t\t\tint index = inputRef.getIndex();\n+\t\t\t\tString fieldName = inputFieldNames.get(index);\n+\t\t\t\tint newIndex = partitionFieldNames.indexOf(fieldName);\n+\t\t\t\tif (newIndex < 0) {\n+\t\t\t\t\tthrow new TableException(String.format(\"Field name '%s' isn't found in partitioned columns.\" +\n+\t\t\t\t\t\t\" Validator should have checked that.\", fieldName));\n+\t\t\t\t}\n+\t\t\t\tif (newIndex == index){\n+\t\t\t\t\treturn inputRef;\n+\t\t\t\t} else {\n+\t\t\t\t\treturn new RexInputRef(newIndex, inputRef.getType());\n+\t\t\t\t}\n+\t\t\t}\n+\t\t});\n+\t}\n+\n+\tprivate Optional<List<Map<String, String>>> readPartitionFromCatalogAndPrune(\n+\t\t\tFlinkContext context,\n+\t\t\tCatalog catalog,\n+\t\t\tObjectIdentifier tableIdentifier,\n+\t\t\tList<String> allFieldNames,\n+\t\t\tSeq<RexNode> partitionPredicate,\n+\t\t\tFunction<List<Map<String, String>>, List<Map<String, String>>> pruner) throws TableNotExistException, TableNotPartitionedException{\n+\t\tRexNodeToExpressionConverter converter = new RexNodeToExpressionConverter(\n+\t\t\tallFieldNames.toArray(new String[0]),\n+\t\t\tcontext.getFunctionCatalog(),\n+\t\t\tcontext.getCatalogManager(),\n+\t\t\tTimeZone.getTimeZone(context.getTableConfig().getLocalTimeZone()));\n+\t\tArrayList<Expression> partitionFilters = new ArrayList<>();\n+\t\tOption<ResolvedExpression> subExpr;\n+\t\tfor (RexNode node: JavaConversions.seqAsJavaList(partitionPredicate)) {\n+\t\t\tsubExpr = node.accept(converter);\n+\t\t\tif (!subExpr.isEmpty()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzcyMDc0NA=="}, "originalCommit": {"oid": "9a05a030c661def4a45b36370dbcfa5e786ed8dc"}, "originalPosition": 276}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkyMjE1Mzc2OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQwNzoxMDoyOVrOG-DfJw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQwNzoxMDoyOVrOG-DfJw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzcyMjAyMw==", "bodyText": "extract those code to a method, then the logic of onMatch  will be more clean, including 4 steps:\n\nextract partition predicate\ndo partition prune, and return the remaining partitions\nre-build statistic\nbuild new table scan and transform the result", "url": "https://github.com/apache/flink/pull/12966#discussion_r467722023", "createdAt": "2020-08-10T07:10:29Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,325 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.api.TableException;\n+import org.apache.flink.table.catalog.Catalog;\n+import org.apache.flink.table.catalog.CatalogPartitionSpec;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.catalog.ObjectIdentifier;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.catalog.exceptions.CatalogException;\n+import org.apache.flink.table.catalog.exceptions.PartitionNotExistException;\n+import org.apache.flink.table.catalog.exceptions.TableNotExistException;\n+import org.apache.flink.table.catalog.exceptions.TableNotPartitionedException;\n+import org.apache.flink.table.catalog.stats.CatalogColumnStatistics;\n+import org.apache.flink.table.catalog.stats.CatalogTableStatistics;\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsPartitionPushDown;\n+import org.apache.flink.table.expressions.Expression;\n+import org.apache.flink.table.expressions.ResolvedExpression;\n+import org.apache.flink.table.plan.stats.TableStats;\n+import org.apache.flink.table.planner.calcite.FlinkContext;\n+import org.apache.flink.table.planner.calcite.FlinkTypeFactory;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+import org.apache.flink.table.planner.plan.utils.FlinkRelOptUtil;\n+import org.apache.flink.table.planner.plan.utils.PartitionPruner;\n+import org.apache.flink.table.planner.plan.utils.RexNodeExtractor;\n+import org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter;\n+import org.apache.flink.table.planner.utils.CatalogTableStatisticsConverter;\n+import org.apache.flink.table.types.logical.LogicalType;\n+\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Filter;\n+import org.apache.calcite.rel.logical.LogicalTableScan;\n+import org.apache.calcite.rel.type.RelDataType;\n+import org.apache.calcite.rex.RexBuilder;\n+import org.apache.calcite.rex.RexInputRef;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.calcite.rex.RexShuttle;\n+import org.apache.calcite.rex.RexUtil;\n+import org.apache.calcite.tools.RelBuilder;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.TimeZone;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import scala.Option;\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.Seq;\n+\n+/**\n+ * Planner rule that tries to push partition evaluated by filter condition into a {@link LogicalTableScan}.\n+*/\n+public class PushPartitionIntoTableSourceScanRule extends RelOptRule {\n+\tpublic static final PushPartitionIntoTableSourceScanRule INSTANCE = new PushPartitionIntoTableSourceScanRule();\n+\n+\tpublic PushPartitionIntoTableSourceScanRule(){\n+\t\tsuper(operand(Filter.class,\n+\t\t\t\toperand(LogicalTableScan.class, none())),\n+\t\t\t\"PushPartitionTableSourceScanRule\");\n+\t}\n+\n+\t@Override\n+\tpublic boolean matches(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tif (filter.getCondition() == null) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tTableSourceTable tableSourceTable = call.rel(1).getTable().unwrap(TableSourceTable.class);\n+\t\tif (tableSourceTable == null){\n+\t\t\treturn false;\n+\t\t}\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource();\n+\t\tif (!(dynamicTableSource instanceof SupportsPartitionPushDown)) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tCatalogTable catalogTable = tableSourceTable.catalogTable();\n+\t\tif (!catalogTable.isPartitioned() || catalogTable.getPartitionKeys().isEmpty()) {\n+\t\t\treturn false;\n+\t\t}\n+\t\treturn Arrays.stream(tableSourceTable.extraDigests()).noneMatch(digest -> digest.startsWith(\"partitions=[\"));\n+\t}\n+\n+\t@Override\n+\tpublic void onMatch(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tLogicalTableScan scan = call.rel(1);\n+\t\tFlinkContext context = call.getPlanner().getContext().unwrap(FlinkContext.class);\n+\t\tTableSourceTable tableSourceTable = scan.getTable().unwrap(TableSourceTable.class);\n+\n+\t\t// build pruner\n+\t\tRelDataType inputFieldTypes = filter.getInput().getRowType();\n+\t\tList<String> inputFieldNames = inputFieldTypes.getFieldNames();\n+\t\tList<String> partitionFieldNames = tableSourceTable.catalogTable().getPartitionKeys();\n+\t\tRelBuilder relBuilder = call.builder();\n+\t\tRexBuilder rexBuilder = relBuilder.getRexBuilder();\n+\t\tTuple2<Seq<RexNode>, Seq<RexNode>> allPredicates = RexNodeExtractor.extractPartitionPredicateList(\n+\t\t\tfilter.getCondition(),\n+\t\t\tFlinkRelOptUtil.getMaxCnfNodeCount(scan),\n+\t\t\tinputFieldNames.toArray(new String[0]),\n+\t\t\trexBuilder,\n+\t\t\tpartitionFieldNames.toArray(new String[0])\n+\t\t\t);\n+\t\tRexNode partitionPredicate = RexUtil.composeConjunction(rexBuilder, JavaConversions.seqAsJavaList(allPredicates._1));\n+\n+\t\tif (partitionPredicate.isAlwaysTrue()){\n+\t\t\treturn;\n+\t\t}\n+\n+\t\tList<LogicalType> partitionFieldTypes = partitionFieldNames.stream().map(name -> {\n+\t\t\tint index  = inputFieldNames.indexOf(name);\n+\t\t\tif (index < 0) {\n+\t\t\t\tthrow new TableException(String.format(\"Partitioned key '%s' isn't found in input columns. \" +\n+\t\t\t\t\t\"Validator should have checked that.\", name));\n+\t\t\t}\t\t\treturn inputFieldTypes.getFieldList().get(index).getType(); })\n+\t\t\t.map(FlinkTypeFactory::toLogicalType).collect(Collectors.toList());\n+\n+\t\tRexNode finalPartitionPredicate = adjustPartitionPredicate(inputFieldNames, partitionFieldNames, partitionPredicate);\n+\t\tFunction<List<Map<String, String>>, List<Map<String, String>>> defaultPruner = partitions -> PartitionPruner.prunePartitions(\n+\t\t\tcontext.getTableConfig(),\n+\t\t\tpartitionFieldNames.toArray(new String[0]),\n+\t\t\tpartitionFieldTypes.toArray(new LogicalType[0]),\n+\t\t\tpartitions,\n+\t\t\tfinalPartitionPredicate);\n+\n+\t\t// get partitions from table/catalog and prune\n+\t\tOptional<Catalog> catalogOptional = context.getCatalogManager().getCatalog(tableSourceTable.tableIdentifier().getCatalogName());\n+\t\tList<Map<String, String>> remainingPartitions = null;\n+\t\tOptional<List<Map<String, String>>> optionalPartitions;\n+\t\t// fields to read partitions from catalog and build new statistic\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource().copy();\n+\t\tObjectIdentifier identifier = tableSourceTable.tableIdentifier();\n+\t\tObjectPath tablePath = identifier.toObjectPath();\n+\t\ttry {\n+\t\t\toptionalPartitions = ((SupportsPartitionPushDown) dynamicTableSource).listPartitions();\n+\t\t\tif (optionalPartitions.isPresent() && !optionalPartitions.get().isEmpty()) {\n+\t\t\t\tremainingPartitions = defaultPruner.apply(optionalPartitions.get());\n+\t\t\t}\n+\t\t} catch (UnsupportedOperationException e) {\n+\t\t\t// check catalog whether is available\n+\t\t\t// we will read partitions from catalog if table doesn't support listPartitions.\n+\t\t\tif (!catalogOptional.isPresent()){\n+\t\t\t\tthrow new TableException(\n+\t\t\t\t\tString.format(\"Table %s must from a catalog, but %s is not a catalog\",\n+\t\t\t\t\t\tidentifier.asSummaryString(), identifier.getCatalogName()), e);\n+\t\t\t}\n+\t\t\ttry {\n+\t\t\t\toptionalPartitions = readPartitionFromCatalogAndPrune(\n+\t\t\t\t\tcontext,\n+\t\t\t\t\tcatalogOptional.get(),\n+\t\t\t\t\tidentifier,\n+\t\t\t\t\tinputFieldNames,\n+\t\t\t\t\tallPredicates._1(),\n+\t\t\t\t\tdefaultPruner\n+\t\t\t\t);\n+\t\t\t\tif (optionalPartitions.isPresent() && !optionalPartitions.get().isEmpty()) {\n+\t\t\t\t\tremainingPartitions = optionalPartitions.get();\n+\t\t\t\t}\n+\t\t\t} catch (TableNotExistException tableNotExistException) {\n+\t\t\t\tthrow new TableException(String.format(\"Table %s is not found in catalog.\", identifier.asSummaryString()), e);\n+\t\t\t} catch (TableNotPartitionedException tableNotPartitionedException) {\n+\t\t\t\tremainingPartitions = null;\n+\t\t\t}\n+\t\t}\n+\t\tif (remainingPartitions != null) {\n+\t\t\t((SupportsPartitionPushDown) dynamicTableSource).applyPartitions(remainingPartitions);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9a05a030c661def4a45b36370dbcfa5e786ed8dc"}, "originalPosition": 190}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkyMjE2NDMyOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQwNzoxNDo0OFrOG-DlJA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQwNzoxNDo0OFrOG-DlJA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzcyMzU1Ng==", "bodyText": "ditto", "url": "https://github.com/apache/flink/pull/12966#discussion_r467723556", "createdAt": "2020-08-10T07:14:48Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,325 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.api.TableException;\n+import org.apache.flink.table.catalog.Catalog;\n+import org.apache.flink.table.catalog.CatalogPartitionSpec;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.catalog.ObjectIdentifier;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.catalog.exceptions.CatalogException;\n+import org.apache.flink.table.catalog.exceptions.PartitionNotExistException;\n+import org.apache.flink.table.catalog.exceptions.TableNotExistException;\n+import org.apache.flink.table.catalog.exceptions.TableNotPartitionedException;\n+import org.apache.flink.table.catalog.stats.CatalogColumnStatistics;\n+import org.apache.flink.table.catalog.stats.CatalogTableStatistics;\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsPartitionPushDown;\n+import org.apache.flink.table.expressions.Expression;\n+import org.apache.flink.table.expressions.ResolvedExpression;\n+import org.apache.flink.table.plan.stats.TableStats;\n+import org.apache.flink.table.planner.calcite.FlinkContext;\n+import org.apache.flink.table.planner.calcite.FlinkTypeFactory;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+import org.apache.flink.table.planner.plan.utils.FlinkRelOptUtil;\n+import org.apache.flink.table.planner.plan.utils.PartitionPruner;\n+import org.apache.flink.table.planner.plan.utils.RexNodeExtractor;\n+import org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter;\n+import org.apache.flink.table.planner.utils.CatalogTableStatisticsConverter;\n+import org.apache.flink.table.types.logical.LogicalType;\n+\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Filter;\n+import org.apache.calcite.rel.logical.LogicalTableScan;\n+import org.apache.calcite.rel.type.RelDataType;\n+import org.apache.calcite.rex.RexBuilder;\n+import org.apache.calcite.rex.RexInputRef;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.calcite.rex.RexShuttle;\n+import org.apache.calcite.rex.RexUtil;\n+import org.apache.calcite.tools.RelBuilder;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.TimeZone;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import scala.Option;\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.Seq;\n+\n+/**\n+ * Planner rule that tries to push partition evaluated by filter condition into a {@link LogicalTableScan}.\n+*/\n+public class PushPartitionIntoTableSourceScanRule extends RelOptRule {\n+\tpublic static final PushPartitionIntoTableSourceScanRule INSTANCE = new PushPartitionIntoTableSourceScanRule();\n+\n+\tpublic PushPartitionIntoTableSourceScanRule(){\n+\t\tsuper(operand(Filter.class,\n+\t\t\t\toperand(LogicalTableScan.class, none())),\n+\t\t\t\"PushPartitionTableSourceScanRule\");\n+\t}\n+\n+\t@Override\n+\tpublic boolean matches(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tif (filter.getCondition() == null) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tTableSourceTable tableSourceTable = call.rel(1).getTable().unwrap(TableSourceTable.class);\n+\t\tif (tableSourceTable == null){\n+\t\t\treturn false;\n+\t\t}\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource();\n+\t\tif (!(dynamicTableSource instanceof SupportsPartitionPushDown)) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tCatalogTable catalogTable = tableSourceTable.catalogTable();\n+\t\tif (!catalogTable.isPartitioned() || catalogTable.getPartitionKeys().isEmpty()) {\n+\t\t\treturn false;\n+\t\t}\n+\t\treturn Arrays.stream(tableSourceTable.extraDigests()).noneMatch(digest -> digest.startsWith(\"partitions=[\"));\n+\t}\n+\n+\t@Override\n+\tpublic void onMatch(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tLogicalTableScan scan = call.rel(1);\n+\t\tFlinkContext context = call.getPlanner().getContext().unwrap(FlinkContext.class);\n+\t\tTableSourceTable tableSourceTable = scan.getTable().unwrap(TableSourceTable.class);\n+\n+\t\t// build pruner\n+\t\tRelDataType inputFieldTypes = filter.getInput().getRowType();\n+\t\tList<String> inputFieldNames = inputFieldTypes.getFieldNames();\n+\t\tList<String> partitionFieldNames = tableSourceTable.catalogTable().getPartitionKeys();\n+\t\tRelBuilder relBuilder = call.builder();\n+\t\tRexBuilder rexBuilder = relBuilder.getRexBuilder();\n+\t\tTuple2<Seq<RexNode>, Seq<RexNode>> allPredicates = RexNodeExtractor.extractPartitionPredicateList(\n+\t\t\tfilter.getCondition(),\n+\t\t\tFlinkRelOptUtil.getMaxCnfNodeCount(scan),\n+\t\t\tinputFieldNames.toArray(new String[0]),\n+\t\t\trexBuilder,\n+\t\t\tpartitionFieldNames.toArray(new String[0])\n+\t\t\t);\n+\t\tRexNode partitionPredicate = RexUtil.composeConjunction(rexBuilder, JavaConversions.seqAsJavaList(allPredicates._1));\n+\n+\t\tif (partitionPredicate.isAlwaysTrue()){\n+\t\t\treturn;\n+\t\t}\n+\n+\t\tList<LogicalType> partitionFieldTypes = partitionFieldNames.stream().map(name -> {\n+\t\t\tint index  = inputFieldNames.indexOf(name);\n+\t\t\tif (index < 0) {\n+\t\t\t\tthrow new TableException(String.format(\"Partitioned key '%s' isn't found in input columns. \" +\n+\t\t\t\t\t\"Validator should have checked that.\", name));\n+\t\t\t}\t\t\treturn inputFieldTypes.getFieldList().get(index).getType(); })\n+\t\t\t.map(FlinkTypeFactory::toLogicalType).collect(Collectors.toList());\n+\n+\t\tRexNode finalPartitionPredicate = adjustPartitionPredicate(inputFieldNames, partitionFieldNames, partitionPredicate);\n+\t\tFunction<List<Map<String, String>>, List<Map<String, String>>> defaultPruner = partitions -> PartitionPruner.prunePartitions(\n+\t\t\tcontext.getTableConfig(),\n+\t\t\tpartitionFieldNames.toArray(new String[0]),\n+\t\t\tpartitionFieldTypes.toArray(new LogicalType[0]),\n+\t\t\tpartitions,\n+\t\t\tfinalPartitionPredicate);\n+\n+\t\t// get partitions from table/catalog and prune\n+\t\tOptional<Catalog> catalogOptional = context.getCatalogManager().getCatalog(tableSourceTable.tableIdentifier().getCatalogName());\n+\t\tList<Map<String, String>> remainingPartitions = null;\n+\t\tOptional<List<Map<String, String>>> optionalPartitions;\n+\t\t// fields to read partitions from catalog and build new statistic\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource().copy();\n+\t\tObjectIdentifier identifier = tableSourceTable.tableIdentifier();\n+\t\tObjectPath tablePath = identifier.toObjectPath();\n+\t\ttry {\n+\t\t\toptionalPartitions = ((SupportsPartitionPushDown) dynamicTableSource).listPartitions();\n+\t\t\tif (optionalPartitions.isPresent() && !optionalPartitions.get().isEmpty()) {\n+\t\t\t\tremainingPartitions = defaultPruner.apply(optionalPartitions.get());\n+\t\t\t}\n+\t\t} catch (UnsupportedOperationException e) {\n+\t\t\t// check catalog whether is available\n+\t\t\t// we will read partitions from catalog if table doesn't support listPartitions.\n+\t\t\tif (!catalogOptional.isPresent()){\n+\t\t\t\tthrow new TableException(\n+\t\t\t\t\tString.format(\"Table %s must from a catalog, but %s is not a catalog\",\n+\t\t\t\t\t\tidentifier.asSummaryString(), identifier.getCatalogName()), e);\n+\t\t\t}\n+\t\t\ttry {\n+\t\t\t\toptionalPartitions = readPartitionFromCatalogAndPrune(\n+\t\t\t\t\tcontext,\n+\t\t\t\t\tcatalogOptional.get(),\n+\t\t\t\t\tidentifier,\n+\t\t\t\t\tinputFieldNames,\n+\t\t\t\t\tallPredicates._1(),\n+\t\t\t\t\tdefaultPruner\n+\t\t\t\t);\n+\t\t\t\tif (optionalPartitions.isPresent() && !optionalPartitions.get().isEmpty()) {\n+\t\t\t\t\tremainingPartitions = optionalPartitions.get();\n+\t\t\t\t}\n+\t\t\t} catch (TableNotExistException tableNotExistException) {\n+\t\t\t\tthrow new TableException(String.format(\"Table %s is not found in catalog.\", identifier.asSummaryString()), e);\n+\t\t\t} catch (TableNotPartitionedException tableNotPartitionedException) {\n+\t\t\t\tremainingPartitions = null;\n+\t\t\t}\n+\t\t}\n+\t\tif (remainingPartitions != null) {\n+\t\t\t((SupportsPartitionPushDown) dynamicTableSource).applyPartitions(remainingPartitions);\n+\t\t}\n+\n+\t\t// build new statistic\n+\t\tTableStats newTableStat = null;\n+\t\tOptional<TableStats> partitionStats;\n+\t\tif (remainingPartitions != null && catalogOptional.isPresent()) {\n+\t\t\tfor (Map<String, String> partition: remainingPartitions) {\n+\t\t\t\tpartitionStats = getPartitionStats(catalogOptional.get(), tablePath, partition);\n+\t\t\t\tif (!partitionStats.isPresent()) {\n+\t\t\t\t\t// clear all information before\n+\t\t\t\t\tnewTableStat = null;\n+\t\t\t\t\tbreak;\n+\t\t\t\t} else {\n+\t\t\t\t\tnewTableStat = newTableStat == null ? partitionStats.get() : newTableStat.merge(partitionStats.get());\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t\tFlinkStatistic newStatistic = FlinkStatistic.builder()\n+\t\t\t.statistic(tableSourceTable.getStatistic())\n+\t\t\t.tableStats(newTableStat)\n+\t\t\t.build();\n+\n+\t\tString extraDigest = remainingPartitions == null ? \"partitions=[]\" :\n+\t\t\t(\"partitions=[\" +\n+\t\t\t\tString.join(\", \", remainingPartitions\n+\t\t\t\t\t.stream()\n+\t\t\t\t\t.map(Object::toString)\n+\t\t\t\t\t.toArray(String[]::new)) +\n+\t\t\t\t\"]\");\n+\t\tTableSourceTable newTableSourceTable = tableSourceTable.copy(dynamicTableSource, newStatistic, new String[]{extraDigest});\n+\n+\t\tLogicalTableScan newScan = LogicalTableScan.create(scan.getCluster(), newTableSourceTable, scan.getHints());\n+\n+\t\tRexNode nonPartitionPredicate = RexUtil.composeConjunction(rexBuilder, JavaConversions.seqAsJavaList(allPredicates._2()));\n+\t\tif (nonPartitionPredicate.isAlwaysTrue()) {\n+\t\t\tcall.transformTo(newScan);\n+\t\t} else {\n+\t\t\tFilter newFilter = filter.copy(filter.getTraitSet(), newScan, nonPartitionPredicate);\n+\t\t\tcall.transformTo(newFilter);\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * adjust the partition field reference index to evaluate the partition values.\n+\t * e.g. the original input fields is: a, b, c, p, and p is partition field. the partition values\n+\t * are: List(Map(\"p\"->\"1\"), Map(\"p\" -> \"2\"), Map(\"p\" -> \"3\")). If the original partition\n+\t * predicate is $3 > 1. after adjusting, the new predicate is ($0 > 1).\n+\t * and use ($0 > 1) to evaluate partition values (row(1), row(2), row(3)).\n+\t */\n+\tprivate RexNode adjustPartitionPredicate(List<String> inputFieldNames, List<String> partitionFieldNames, RexNode partitionPredicate) {\n+\t\treturn partitionPredicate.accept(new RexShuttle(){\n+\t\t\t@Override\n+\t\t\tpublic RexNode visitInputRef(RexInputRef inputRef) {\n+\t\t\t\tint index = inputRef.getIndex();\n+\t\t\t\tString fieldName = inputFieldNames.get(index);\n+\t\t\t\tint newIndex = partitionFieldNames.indexOf(fieldName);\n+\t\t\t\tif (newIndex < 0) {\n+\t\t\t\t\tthrow new TableException(String.format(\"Field name '%s' isn't found in partitioned columns.\" +\n+\t\t\t\t\t\t\" Validator should have checked that.\", fieldName));\n+\t\t\t\t}\n+\t\t\t\tif (newIndex == index){\n+\t\t\t\t\treturn inputRef;\n+\t\t\t\t} else {\n+\t\t\t\t\treturn new RexInputRef(newIndex, inputRef.getType());\n+\t\t\t\t}\n+\t\t\t}\n+\t\t});\n+\t}\n+\n+\tprivate Optional<List<Map<String, String>>> readPartitionFromCatalogAndPrune(\n+\t\t\tFlinkContext context,\n+\t\t\tCatalog catalog,\n+\t\t\tObjectIdentifier tableIdentifier,\n+\t\t\tList<String> allFieldNames,\n+\t\t\tSeq<RexNode> partitionPredicate,\n+\t\t\tFunction<List<Map<String, String>>, List<Map<String, String>>> pruner) throws TableNotExistException, TableNotPartitionedException{\n+\t\tRexNodeToExpressionConverter converter = new RexNodeToExpressionConverter(\n+\t\t\tallFieldNames.toArray(new String[0]),\n+\t\t\tcontext.getFunctionCatalog(),\n+\t\t\tcontext.getCatalogManager(),\n+\t\t\tTimeZone.getTimeZone(context.getTableConfig().getLocalTimeZone()));\n+\t\tArrayList<Expression> partitionFilters = new ArrayList<>();\n+\t\tOption<ResolvedExpression> subExpr;\n+\t\tfor (RexNode node: JavaConversions.seqAsJavaList(partitionPredicate)) {\n+\t\t\tsubExpr = node.accept(converter);\n+\t\t\tif (!subExpr.isEmpty()) {\n+\t\t\t\tpartitionFilters.add(subExpr.get());\n+\t\t\t}\n+\t\t}\n+\t\tObjectPath tablePath = tableIdentifier.toObjectPath();\n+\t\tif (partitionFilters.size() > 0) {\n+\t\t\ttry {\n+\t\t\t\tList<Map<String, String>> remainingPartitions = catalog.listPartitionsByFilter(tablePath, partitionFilters)\n+\t\t\t\t\t.stream()\n+\t\t\t\t\t.map(CatalogPartitionSpec::getPartitionSpec)\n+\t\t\t\t\t.collect(Collectors.toList());\n+\t\t\t\treturn Optional.of(remainingPartitions);\n+\t\t\t} catch (UnsupportedOperationException e) {\n+\t\t\t\treturn readPartitionFromCatalogWithoutFilterAndPrune(catalog, tablePath, pruner);\n+\t\t\t}\n+\t\t} else {\n+\t\t\treturn readPartitionFromCatalogWithoutFilterAndPrune(catalog, tablePath, pruner);\n+\t\t}\n+\t}\n+\n+\tprivate Optional<List<Map<String, String>>> readPartitionFromCatalogWithoutFilterAndPrune(\n+\t\t\tCatalog catalog,\n+\t\t\tObjectPath tablePath,\n+\t\t\tFunction<List<Map<String, String>>, List<Map<String, String>>> pruner) throws TableNotExistException, TableNotPartitionedException, CatalogException {\n+\t\tList<Map<String, String>> remainingPartitions;\n+\t\tList<Map<String, String>> partitions = catalog.listPartitions(tablePath)\n+\t\t\t.stream()\n+\t\t\t.map(CatalogPartitionSpec::getPartitionSpec)\n+\t\t\t.collect(Collectors.toList());\n+\t\t// prune partitions\n+\t\tif (partitions.size() > 0) {\n+\t\t\tremainingPartitions = pruner.apply(partitions);\n+\t\t\treturn Optional.of(remainingPartitions);\n+\t\t} else {\n+\t\t\treturn Optional.empty();\n+\t\t}\n+\t}\n+\n+\tprivate Optional<TableStats> getPartitionStats(Catalog catalog, ObjectPath tablePath, Map<String, String> partition) {\n+\t\ttry {\n+\t\t\tCatalogPartitionSpec spec = new CatalogPartitionSpec(partition);\n+\t\t\tCatalogTableStatistics partitionStat = catalog.getPartitionStatistics(tablePath, spec);\n+\t\t\tCatalogColumnStatistics\tpartitionColStat = catalog.getPartitionColumnStatistics(tablePath, spec);\n+\t\t\tTableStats\tstats = CatalogTableStatisticsConverter.convertToTableStats(partitionStat, partitionColStat);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9a05a030c661def4a45b36370dbcfa5e786ed8dc"}, "originalPosition": 319}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkyMjE3NDQ4OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQwNzoxODo0OVrOG-Dq4Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQwNzoxODo0OVrOG-Dq4Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzcyNTAyNQ==", "bodyText": "we should not throw TableNotPartitionedException because we had check whether the table is partitioned", "url": "https://github.com/apache/flink/pull/12966#discussion_r467725025", "createdAt": "2020-08-10T07:18:49Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,325 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.api.TableException;\n+import org.apache.flink.table.catalog.Catalog;\n+import org.apache.flink.table.catalog.CatalogPartitionSpec;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.catalog.ObjectIdentifier;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.catalog.exceptions.CatalogException;\n+import org.apache.flink.table.catalog.exceptions.PartitionNotExistException;\n+import org.apache.flink.table.catalog.exceptions.TableNotExistException;\n+import org.apache.flink.table.catalog.exceptions.TableNotPartitionedException;\n+import org.apache.flink.table.catalog.stats.CatalogColumnStatistics;\n+import org.apache.flink.table.catalog.stats.CatalogTableStatistics;\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsPartitionPushDown;\n+import org.apache.flink.table.expressions.Expression;\n+import org.apache.flink.table.expressions.ResolvedExpression;\n+import org.apache.flink.table.plan.stats.TableStats;\n+import org.apache.flink.table.planner.calcite.FlinkContext;\n+import org.apache.flink.table.planner.calcite.FlinkTypeFactory;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+import org.apache.flink.table.planner.plan.utils.FlinkRelOptUtil;\n+import org.apache.flink.table.planner.plan.utils.PartitionPruner;\n+import org.apache.flink.table.planner.plan.utils.RexNodeExtractor;\n+import org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter;\n+import org.apache.flink.table.planner.utils.CatalogTableStatisticsConverter;\n+import org.apache.flink.table.types.logical.LogicalType;\n+\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Filter;\n+import org.apache.calcite.rel.logical.LogicalTableScan;\n+import org.apache.calcite.rel.type.RelDataType;\n+import org.apache.calcite.rex.RexBuilder;\n+import org.apache.calcite.rex.RexInputRef;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.calcite.rex.RexShuttle;\n+import org.apache.calcite.rex.RexUtil;\n+import org.apache.calcite.tools.RelBuilder;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.TimeZone;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import scala.Option;\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.Seq;\n+\n+/**\n+ * Planner rule that tries to push partition evaluated by filter condition into a {@link LogicalTableScan}.\n+*/\n+public class PushPartitionIntoTableSourceScanRule extends RelOptRule {\n+\tpublic static final PushPartitionIntoTableSourceScanRule INSTANCE = new PushPartitionIntoTableSourceScanRule();\n+\n+\tpublic PushPartitionIntoTableSourceScanRule(){\n+\t\tsuper(operand(Filter.class,\n+\t\t\t\toperand(LogicalTableScan.class, none())),\n+\t\t\t\"PushPartitionTableSourceScanRule\");\n+\t}\n+\n+\t@Override\n+\tpublic boolean matches(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tif (filter.getCondition() == null) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tTableSourceTable tableSourceTable = call.rel(1).getTable().unwrap(TableSourceTable.class);\n+\t\tif (tableSourceTable == null){\n+\t\t\treturn false;\n+\t\t}\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource();\n+\t\tif (!(dynamicTableSource instanceof SupportsPartitionPushDown)) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tCatalogTable catalogTable = tableSourceTable.catalogTable();\n+\t\tif (!catalogTable.isPartitioned() || catalogTable.getPartitionKeys().isEmpty()) {\n+\t\t\treturn false;\n+\t\t}\n+\t\treturn Arrays.stream(tableSourceTable.extraDigests()).noneMatch(digest -> digest.startsWith(\"partitions=[\"));\n+\t}\n+\n+\t@Override\n+\tpublic void onMatch(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tLogicalTableScan scan = call.rel(1);\n+\t\tFlinkContext context = call.getPlanner().getContext().unwrap(FlinkContext.class);\n+\t\tTableSourceTable tableSourceTable = scan.getTable().unwrap(TableSourceTable.class);\n+\n+\t\t// build pruner\n+\t\tRelDataType inputFieldTypes = filter.getInput().getRowType();\n+\t\tList<String> inputFieldNames = inputFieldTypes.getFieldNames();\n+\t\tList<String> partitionFieldNames = tableSourceTable.catalogTable().getPartitionKeys();\n+\t\tRelBuilder relBuilder = call.builder();\n+\t\tRexBuilder rexBuilder = relBuilder.getRexBuilder();\n+\t\tTuple2<Seq<RexNode>, Seq<RexNode>> allPredicates = RexNodeExtractor.extractPartitionPredicateList(\n+\t\t\tfilter.getCondition(),\n+\t\t\tFlinkRelOptUtil.getMaxCnfNodeCount(scan),\n+\t\t\tinputFieldNames.toArray(new String[0]),\n+\t\t\trexBuilder,\n+\t\t\tpartitionFieldNames.toArray(new String[0])\n+\t\t\t);\n+\t\tRexNode partitionPredicate = RexUtil.composeConjunction(rexBuilder, JavaConversions.seqAsJavaList(allPredicates._1));\n+\n+\t\tif (partitionPredicate.isAlwaysTrue()){\n+\t\t\treturn;\n+\t\t}\n+\n+\t\tList<LogicalType> partitionFieldTypes = partitionFieldNames.stream().map(name -> {\n+\t\t\tint index  = inputFieldNames.indexOf(name);\n+\t\t\tif (index < 0) {\n+\t\t\t\tthrow new TableException(String.format(\"Partitioned key '%s' isn't found in input columns. \" +\n+\t\t\t\t\t\"Validator should have checked that.\", name));\n+\t\t\t}\t\t\treturn inputFieldTypes.getFieldList().get(index).getType(); })\n+\t\t\t.map(FlinkTypeFactory::toLogicalType).collect(Collectors.toList());\n+\n+\t\tRexNode finalPartitionPredicate = adjustPartitionPredicate(inputFieldNames, partitionFieldNames, partitionPredicate);\n+\t\tFunction<List<Map<String, String>>, List<Map<String, String>>> defaultPruner = partitions -> PartitionPruner.prunePartitions(\n+\t\t\tcontext.getTableConfig(),\n+\t\t\tpartitionFieldNames.toArray(new String[0]),\n+\t\t\tpartitionFieldTypes.toArray(new LogicalType[0]),\n+\t\t\tpartitions,\n+\t\t\tfinalPartitionPredicate);\n+\n+\t\t// get partitions from table/catalog and prune\n+\t\tOptional<Catalog> catalogOptional = context.getCatalogManager().getCatalog(tableSourceTable.tableIdentifier().getCatalogName());\n+\t\tList<Map<String, String>> remainingPartitions = null;\n+\t\tOptional<List<Map<String, String>>> optionalPartitions;\n+\t\t// fields to read partitions from catalog and build new statistic\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource().copy();\n+\t\tObjectIdentifier identifier = tableSourceTable.tableIdentifier();\n+\t\tObjectPath tablePath = identifier.toObjectPath();\n+\t\ttry {\n+\t\t\toptionalPartitions = ((SupportsPartitionPushDown) dynamicTableSource).listPartitions();\n+\t\t\tif (optionalPartitions.isPresent() && !optionalPartitions.get().isEmpty()) {\n+\t\t\t\tremainingPartitions = defaultPruner.apply(optionalPartitions.get());\n+\t\t\t}\n+\t\t} catch (UnsupportedOperationException e) {\n+\t\t\t// check catalog whether is available\n+\t\t\t// we will read partitions from catalog if table doesn't support listPartitions.\n+\t\t\tif (!catalogOptional.isPresent()){\n+\t\t\t\tthrow new TableException(\n+\t\t\t\t\tString.format(\"Table %s must from a catalog, but %s is not a catalog\",\n+\t\t\t\t\t\tidentifier.asSummaryString(), identifier.getCatalogName()), e);\n+\t\t\t}\n+\t\t\ttry {\n+\t\t\t\toptionalPartitions = readPartitionFromCatalogAndPrune(\n+\t\t\t\t\tcontext,\n+\t\t\t\t\tcatalogOptional.get(),\n+\t\t\t\t\tidentifier,\n+\t\t\t\t\tinputFieldNames,\n+\t\t\t\t\tallPredicates._1(),\n+\t\t\t\t\tdefaultPruner\n+\t\t\t\t);\n+\t\t\t\tif (optionalPartitions.isPresent() && !optionalPartitions.get().isEmpty()) {\n+\t\t\t\t\tremainingPartitions = optionalPartitions.get();\n+\t\t\t\t}\n+\t\t\t} catch (TableNotExistException tableNotExistException) {\n+\t\t\t\tthrow new TableException(String.format(\"Table %s is not found in catalog.\", identifier.asSummaryString()), e);\n+\t\t\t} catch (TableNotPartitionedException tableNotPartitionedException) {\n+\t\t\t\tremainingPartitions = null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9a05a030c661def4a45b36370dbcfa5e786ed8dc"}, "originalPosition": 186}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkyMjE4NTk4OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQwNzoyMzoxMlrOG-Dxqw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQwNzoyMzoxMlrOG-Dxqw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzcyNjc2Mw==", "bodyText": "nit: please add a blank between )and {, there are many similar case: line 240, line 250, etc", "url": "https://github.com/apache/flink/pull/12966#discussion_r467726763", "createdAt": "2020-08-10T07:23:12Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,325 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.api.TableException;\n+import org.apache.flink.table.catalog.Catalog;\n+import org.apache.flink.table.catalog.CatalogPartitionSpec;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.catalog.ObjectIdentifier;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.catalog.exceptions.CatalogException;\n+import org.apache.flink.table.catalog.exceptions.PartitionNotExistException;\n+import org.apache.flink.table.catalog.exceptions.TableNotExistException;\n+import org.apache.flink.table.catalog.exceptions.TableNotPartitionedException;\n+import org.apache.flink.table.catalog.stats.CatalogColumnStatistics;\n+import org.apache.flink.table.catalog.stats.CatalogTableStatistics;\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsPartitionPushDown;\n+import org.apache.flink.table.expressions.Expression;\n+import org.apache.flink.table.expressions.ResolvedExpression;\n+import org.apache.flink.table.plan.stats.TableStats;\n+import org.apache.flink.table.planner.calcite.FlinkContext;\n+import org.apache.flink.table.planner.calcite.FlinkTypeFactory;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+import org.apache.flink.table.planner.plan.utils.FlinkRelOptUtil;\n+import org.apache.flink.table.planner.plan.utils.PartitionPruner;\n+import org.apache.flink.table.planner.plan.utils.RexNodeExtractor;\n+import org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter;\n+import org.apache.flink.table.planner.utils.CatalogTableStatisticsConverter;\n+import org.apache.flink.table.types.logical.LogicalType;\n+\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Filter;\n+import org.apache.calcite.rel.logical.LogicalTableScan;\n+import org.apache.calcite.rel.type.RelDataType;\n+import org.apache.calcite.rex.RexBuilder;\n+import org.apache.calcite.rex.RexInputRef;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.calcite.rex.RexShuttle;\n+import org.apache.calcite.rex.RexUtil;\n+import org.apache.calcite.tools.RelBuilder;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.TimeZone;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import scala.Option;\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.Seq;\n+\n+/**\n+ * Planner rule that tries to push partition evaluated by filter condition into a {@link LogicalTableScan}.\n+*/\n+public class PushPartitionIntoTableSourceScanRule extends RelOptRule {\n+\tpublic static final PushPartitionIntoTableSourceScanRule INSTANCE = new PushPartitionIntoTableSourceScanRule();\n+\n+\tpublic PushPartitionIntoTableSourceScanRule(){\n+\t\tsuper(operand(Filter.class,\n+\t\t\t\toperand(LogicalTableScan.class, none())),\n+\t\t\t\"PushPartitionTableSourceScanRule\");\n+\t}\n+\n+\t@Override\n+\tpublic boolean matches(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tif (filter.getCondition() == null) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tTableSourceTable tableSourceTable = call.rel(1).getTable().unwrap(TableSourceTable.class);\n+\t\tif (tableSourceTable == null){\n+\t\t\treturn false;\n+\t\t}\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource();\n+\t\tif (!(dynamicTableSource instanceof SupportsPartitionPushDown)) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tCatalogTable catalogTable = tableSourceTable.catalogTable();\n+\t\tif (!catalogTable.isPartitioned() || catalogTable.getPartitionKeys().isEmpty()) {\n+\t\t\treturn false;\n+\t\t}\n+\t\treturn Arrays.stream(tableSourceTable.extraDigests()).noneMatch(digest -> digest.startsWith(\"partitions=[\"));\n+\t}\n+\n+\t@Override\n+\tpublic void onMatch(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tLogicalTableScan scan = call.rel(1);\n+\t\tFlinkContext context = call.getPlanner().getContext().unwrap(FlinkContext.class);\n+\t\tTableSourceTable tableSourceTable = scan.getTable().unwrap(TableSourceTable.class);\n+\n+\t\t// build pruner\n+\t\tRelDataType inputFieldTypes = filter.getInput().getRowType();\n+\t\tList<String> inputFieldNames = inputFieldTypes.getFieldNames();\n+\t\tList<String> partitionFieldNames = tableSourceTable.catalogTable().getPartitionKeys();\n+\t\tRelBuilder relBuilder = call.builder();\n+\t\tRexBuilder rexBuilder = relBuilder.getRexBuilder();\n+\t\tTuple2<Seq<RexNode>, Seq<RexNode>> allPredicates = RexNodeExtractor.extractPartitionPredicateList(\n+\t\t\tfilter.getCondition(),\n+\t\t\tFlinkRelOptUtil.getMaxCnfNodeCount(scan),\n+\t\t\tinputFieldNames.toArray(new String[0]),\n+\t\t\trexBuilder,\n+\t\t\tpartitionFieldNames.toArray(new String[0])\n+\t\t\t);\n+\t\tRexNode partitionPredicate = RexUtil.composeConjunction(rexBuilder, JavaConversions.seqAsJavaList(allPredicates._1));\n+\n+\t\tif (partitionPredicate.isAlwaysTrue()){\n+\t\t\treturn;\n+\t\t}\n+\n+\t\tList<LogicalType> partitionFieldTypes = partitionFieldNames.stream().map(name -> {\n+\t\t\tint index  = inputFieldNames.indexOf(name);\n+\t\t\tif (index < 0) {\n+\t\t\t\tthrow new TableException(String.format(\"Partitioned key '%s' isn't found in input columns. \" +\n+\t\t\t\t\t\"Validator should have checked that.\", name));\n+\t\t\t}\t\t\treturn inputFieldTypes.getFieldList().get(index).getType(); })\n+\t\t\t.map(FlinkTypeFactory::toLogicalType).collect(Collectors.toList());\n+\n+\t\tRexNode finalPartitionPredicate = adjustPartitionPredicate(inputFieldNames, partitionFieldNames, partitionPredicate);\n+\t\tFunction<List<Map<String, String>>, List<Map<String, String>>> defaultPruner = partitions -> PartitionPruner.prunePartitions(\n+\t\t\tcontext.getTableConfig(),\n+\t\t\tpartitionFieldNames.toArray(new String[0]),\n+\t\t\tpartitionFieldTypes.toArray(new LogicalType[0]),\n+\t\t\tpartitions,\n+\t\t\tfinalPartitionPredicate);\n+\n+\t\t// get partitions from table/catalog and prune\n+\t\tOptional<Catalog> catalogOptional = context.getCatalogManager().getCatalog(tableSourceTable.tableIdentifier().getCatalogName());\n+\t\tList<Map<String, String>> remainingPartitions = null;\n+\t\tOptional<List<Map<String, String>>> optionalPartitions;\n+\t\t// fields to read partitions from catalog and build new statistic\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource().copy();\n+\t\tObjectIdentifier identifier = tableSourceTable.tableIdentifier();\n+\t\tObjectPath tablePath = identifier.toObjectPath();\n+\t\ttry {\n+\t\t\toptionalPartitions = ((SupportsPartitionPushDown) dynamicTableSource).listPartitions();\n+\t\t\tif (optionalPartitions.isPresent() && !optionalPartitions.get().isEmpty()) {\n+\t\t\t\tremainingPartitions = defaultPruner.apply(optionalPartitions.get());\n+\t\t\t}\n+\t\t} catch (UnsupportedOperationException e) {\n+\t\t\t// check catalog whether is available\n+\t\t\t// we will read partitions from catalog if table doesn't support listPartitions.\n+\t\t\tif (!catalogOptional.isPresent()){", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9a05a030c661def4a45b36370dbcfa5e786ed8dc"}, "originalPosition": 166}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkyMjE4NzU5OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQwNzoyMzo0NVrOG-Dykg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQwNzoyMzo0NVrOG-Dykg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzcyNjk5NA==", "bodyText": "nit: tab -> blank", "url": "https://github.com/apache/flink/pull/12966#discussion_r467726994", "createdAt": "2020-08-10T07:23:45Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,325 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.api.TableException;\n+import org.apache.flink.table.catalog.Catalog;\n+import org.apache.flink.table.catalog.CatalogPartitionSpec;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.catalog.ObjectIdentifier;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.catalog.exceptions.CatalogException;\n+import org.apache.flink.table.catalog.exceptions.PartitionNotExistException;\n+import org.apache.flink.table.catalog.exceptions.TableNotExistException;\n+import org.apache.flink.table.catalog.exceptions.TableNotPartitionedException;\n+import org.apache.flink.table.catalog.stats.CatalogColumnStatistics;\n+import org.apache.flink.table.catalog.stats.CatalogTableStatistics;\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsPartitionPushDown;\n+import org.apache.flink.table.expressions.Expression;\n+import org.apache.flink.table.expressions.ResolvedExpression;\n+import org.apache.flink.table.plan.stats.TableStats;\n+import org.apache.flink.table.planner.calcite.FlinkContext;\n+import org.apache.flink.table.planner.calcite.FlinkTypeFactory;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+import org.apache.flink.table.planner.plan.utils.FlinkRelOptUtil;\n+import org.apache.flink.table.planner.plan.utils.PartitionPruner;\n+import org.apache.flink.table.planner.plan.utils.RexNodeExtractor;\n+import org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter;\n+import org.apache.flink.table.planner.utils.CatalogTableStatisticsConverter;\n+import org.apache.flink.table.types.logical.LogicalType;\n+\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Filter;\n+import org.apache.calcite.rel.logical.LogicalTableScan;\n+import org.apache.calcite.rel.type.RelDataType;\n+import org.apache.calcite.rex.RexBuilder;\n+import org.apache.calcite.rex.RexInputRef;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.calcite.rex.RexShuttle;\n+import org.apache.calcite.rex.RexUtil;\n+import org.apache.calcite.tools.RelBuilder;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.TimeZone;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import scala.Option;\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.Seq;\n+\n+/**\n+ * Planner rule that tries to push partition evaluated by filter condition into a {@link LogicalTableScan}.\n+*/\n+public class PushPartitionIntoTableSourceScanRule extends RelOptRule {\n+\tpublic static final PushPartitionIntoTableSourceScanRule INSTANCE = new PushPartitionIntoTableSourceScanRule();\n+\n+\tpublic PushPartitionIntoTableSourceScanRule(){\n+\t\tsuper(operand(Filter.class,\n+\t\t\t\toperand(LogicalTableScan.class, none())),\n+\t\t\t\"PushPartitionTableSourceScanRule\");\n+\t}\n+\n+\t@Override\n+\tpublic boolean matches(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tif (filter.getCondition() == null) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tTableSourceTable tableSourceTable = call.rel(1).getTable().unwrap(TableSourceTable.class);\n+\t\tif (tableSourceTable == null){\n+\t\t\treturn false;\n+\t\t}\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource();\n+\t\tif (!(dynamicTableSource instanceof SupportsPartitionPushDown)) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tCatalogTable catalogTable = tableSourceTable.catalogTable();\n+\t\tif (!catalogTable.isPartitioned() || catalogTable.getPartitionKeys().isEmpty()) {\n+\t\t\treturn false;\n+\t\t}\n+\t\treturn Arrays.stream(tableSourceTable.extraDigests()).noneMatch(digest -> digest.startsWith(\"partitions=[\"));\n+\t}\n+\n+\t@Override\n+\tpublic void onMatch(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tLogicalTableScan scan = call.rel(1);\n+\t\tFlinkContext context = call.getPlanner().getContext().unwrap(FlinkContext.class);\n+\t\tTableSourceTable tableSourceTable = scan.getTable().unwrap(TableSourceTable.class);\n+\n+\t\t// build pruner\n+\t\tRelDataType inputFieldTypes = filter.getInput().getRowType();\n+\t\tList<String> inputFieldNames = inputFieldTypes.getFieldNames();\n+\t\tList<String> partitionFieldNames = tableSourceTable.catalogTable().getPartitionKeys();\n+\t\tRelBuilder relBuilder = call.builder();\n+\t\tRexBuilder rexBuilder = relBuilder.getRexBuilder();\n+\t\tTuple2<Seq<RexNode>, Seq<RexNode>> allPredicates = RexNodeExtractor.extractPartitionPredicateList(\n+\t\t\tfilter.getCondition(),\n+\t\t\tFlinkRelOptUtil.getMaxCnfNodeCount(scan),\n+\t\t\tinputFieldNames.toArray(new String[0]),\n+\t\t\trexBuilder,\n+\t\t\tpartitionFieldNames.toArray(new String[0])\n+\t\t\t);\n+\t\tRexNode partitionPredicate = RexUtil.composeConjunction(rexBuilder, JavaConversions.seqAsJavaList(allPredicates._1));\n+\n+\t\tif (partitionPredicate.isAlwaysTrue()){\n+\t\t\treturn;\n+\t\t}\n+\n+\t\tList<LogicalType> partitionFieldTypes = partitionFieldNames.stream().map(name -> {\n+\t\t\tint index  = inputFieldNames.indexOf(name);\n+\t\t\tif (index < 0) {\n+\t\t\t\tthrow new TableException(String.format(\"Partitioned key '%s' isn't found in input columns. \" +\n+\t\t\t\t\t\"Validator should have checked that.\", name));\n+\t\t\t}\t\t\treturn inputFieldTypes.getFieldList().get(index).getType(); })\n+\t\t\t.map(FlinkTypeFactory::toLogicalType).collect(Collectors.toList());\n+\n+\t\tRexNode finalPartitionPredicate = adjustPartitionPredicate(inputFieldNames, partitionFieldNames, partitionPredicate);\n+\t\tFunction<List<Map<String, String>>, List<Map<String, String>>> defaultPruner = partitions -> PartitionPruner.prunePartitions(\n+\t\t\tcontext.getTableConfig(),\n+\t\t\tpartitionFieldNames.toArray(new String[0]),\n+\t\t\tpartitionFieldTypes.toArray(new LogicalType[0]),\n+\t\t\tpartitions,\n+\t\t\tfinalPartitionPredicate);\n+\n+\t\t// get partitions from table/catalog and prune\n+\t\tOptional<Catalog> catalogOptional = context.getCatalogManager().getCatalog(tableSourceTable.tableIdentifier().getCatalogName());\n+\t\tList<Map<String, String>> remainingPartitions = null;\n+\t\tOptional<List<Map<String, String>>> optionalPartitions;\n+\t\t// fields to read partitions from catalog and build new statistic\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource().copy();\n+\t\tObjectIdentifier identifier = tableSourceTable.tableIdentifier();\n+\t\tObjectPath tablePath = identifier.toObjectPath();\n+\t\ttry {\n+\t\t\toptionalPartitions = ((SupportsPartitionPushDown) dynamicTableSource).listPartitions();\n+\t\t\tif (optionalPartitions.isPresent() && !optionalPartitions.get().isEmpty()) {\n+\t\t\t\tremainingPartitions = defaultPruner.apply(optionalPartitions.get());\n+\t\t\t}\n+\t\t} catch (UnsupportedOperationException e) {\n+\t\t\t// check catalog whether is available\n+\t\t\t// we will read partitions from catalog if table doesn't support listPartitions.\n+\t\t\tif (!catalogOptional.isPresent()){\n+\t\t\t\tthrow new TableException(\n+\t\t\t\t\tString.format(\"Table %s must from a catalog, but %s is not a catalog\",\n+\t\t\t\t\t\tidentifier.asSummaryString(), identifier.getCatalogName()), e);\n+\t\t\t}\n+\t\t\ttry {\n+\t\t\t\toptionalPartitions = readPartitionFromCatalogAndPrune(\n+\t\t\t\t\tcontext,\n+\t\t\t\t\tcatalogOptional.get(),\n+\t\t\t\t\tidentifier,\n+\t\t\t\t\tinputFieldNames,\n+\t\t\t\t\tallPredicates._1(),\n+\t\t\t\t\tdefaultPruner\n+\t\t\t\t);\n+\t\t\t\tif (optionalPartitions.isPresent() && !optionalPartitions.get().isEmpty()) {\n+\t\t\t\t\tremainingPartitions = optionalPartitions.get();\n+\t\t\t\t}\n+\t\t\t} catch (TableNotExistException tableNotExistException) {\n+\t\t\t\tthrow new TableException(String.format(\"Table %s is not found in catalog.\", identifier.asSummaryString()), e);\n+\t\t\t} catch (TableNotPartitionedException tableNotPartitionedException) {\n+\t\t\t\tremainingPartitions = null;\n+\t\t\t}\n+\t\t}\n+\t\tif (remainingPartitions != null) {\n+\t\t\t((SupportsPartitionPushDown) dynamicTableSource).applyPartitions(remainingPartitions);\n+\t\t}\n+\n+\t\t// build new statistic\n+\t\tTableStats newTableStat = null;\n+\t\tOptional<TableStats> partitionStats;\n+\t\tif (remainingPartitions != null && catalogOptional.isPresent()) {\n+\t\t\tfor (Map<String, String> partition: remainingPartitions) {\n+\t\t\t\tpartitionStats = getPartitionStats(catalogOptional.get(), tablePath, partition);\n+\t\t\t\tif (!partitionStats.isPresent()) {\n+\t\t\t\t\t// clear all information before\n+\t\t\t\t\tnewTableStat = null;\n+\t\t\t\t\tbreak;\n+\t\t\t\t} else {\n+\t\t\t\t\tnewTableStat = newTableStat == null ? partitionStats.get() : newTableStat.merge(partitionStats.get());\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t\tFlinkStatistic newStatistic = FlinkStatistic.builder()\n+\t\t\t.statistic(tableSourceTable.getStatistic())\n+\t\t\t.tableStats(newTableStat)\n+\t\t\t.build();\n+\n+\t\tString extraDigest = remainingPartitions == null ? \"partitions=[]\" :\n+\t\t\t(\"partitions=[\" +\n+\t\t\t\tString.join(\", \", remainingPartitions\n+\t\t\t\t\t.stream()\n+\t\t\t\t\t.map(Object::toString)\n+\t\t\t\t\t.toArray(String[]::new)) +\n+\t\t\t\t\"]\");\n+\t\tTableSourceTable newTableSourceTable = tableSourceTable.copy(dynamicTableSource, newStatistic, new String[]{extraDigest});\n+\n+\t\tLogicalTableScan newScan = LogicalTableScan.create(scan.getCluster(), newTableSourceTable, scan.getHints());\n+\n+\t\tRexNode nonPartitionPredicate = RexUtil.composeConjunction(rexBuilder, JavaConversions.seqAsJavaList(allPredicates._2()));\n+\t\tif (nonPartitionPredicate.isAlwaysTrue()) {\n+\t\t\tcall.transformTo(newScan);\n+\t\t} else {\n+\t\t\tFilter newFilter = filter.copy(filter.getTraitSet(), newScan, nonPartitionPredicate);\n+\t\t\tcall.transformTo(newFilter);\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * adjust the partition field reference index to evaluate the partition values.\n+\t * e.g. the original input fields is: a, b, c, p, and p is partition field. the partition values\n+\t * are: List(Map(\"p\"->\"1\"), Map(\"p\" -> \"2\"), Map(\"p\" -> \"3\")). If the original partition\n+\t * predicate is $3 > 1. after adjusting, the new predicate is ($0 > 1).\n+\t * and use ($0 > 1) to evaluate partition values (row(1), row(2), row(3)).\n+\t */\n+\tprivate RexNode adjustPartitionPredicate(List<String> inputFieldNames, List<String> partitionFieldNames, RexNode partitionPredicate) {\n+\t\treturn partitionPredicate.accept(new RexShuttle(){\n+\t\t\t@Override\n+\t\t\tpublic RexNode visitInputRef(RexInputRef inputRef) {\n+\t\t\t\tint index = inputRef.getIndex();\n+\t\t\t\tString fieldName = inputFieldNames.get(index);\n+\t\t\t\tint newIndex = partitionFieldNames.indexOf(fieldName);\n+\t\t\t\tif (newIndex < 0) {\n+\t\t\t\t\tthrow new TableException(String.format(\"Field name '%s' isn't found in partitioned columns.\" +\n+\t\t\t\t\t\t\" Validator should have checked that.\", fieldName));\n+\t\t\t\t}\n+\t\t\t\tif (newIndex == index){\n+\t\t\t\t\treturn inputRef;\n+\t\t\t\t} else {\n+\t\t\t\t\treturn new RexInputRef(newIndex, inputRef.getType());\n+\t\t\t\t}\n+\t\t\t}\n+\t\t});\n+\t}\n+\n+\tprivate Optional<List<Map<String, String>>> readPartitionFromCatalogAndPrune(\n+\t\t\tFlinkContext context,\n+\t\t\tCatalog catalog,\n+\t\t\tObjectIdentifier tableIdentifier,\n+\t\t\tList<String> allFieldNames,\n+\t\t\tSeq<RexNode> partitionPredicate,\n+\t\t\tFunction<List<Map<String, String>>, List<Map<String, String>>> pruner) throws TableNotExistException, TableNotPartitionedException{\n+\t\tRexNodeToExpressionConverter converter = new RexNodeToExpressionConverter(\n+\t\t\tallFieldNames.toArray(new String[0]),\n+\t\t\tcontext.getFunctionCatalog(),\n+\t\t\tcontext.getCatalogManager(),\n+\t\t\tTimeZone.getTimeZone(context.getTableConfig().getLocalTimeZone()));\n+\t\tArrayList<Expression> partitionFilters = new ArrayList<>();\n+\t\tOption<ResolvedExpression> subExpr;\n+\t\tfor (RexNode node: JavaConversions.seqAsJavaList(partitionPredicate)) {\n+\t\t\tsubExpr = node.accept(converter);\n+\t\t\tif (!subExpr.isEmpty()) {\n+\t\t\t\tpartitionFilters.add(subExpr.get());\n+\t\t\t}\n+\t\t}\n+\t\tObjectPath tablePath = tableIdentifier.toObjectPath();\n+\t\tif (partitionFilters.size() > 0) {\n+\t\t\ttry {\n+\t\t\t\tList<Map<String, String>> remainingPartitions = catalog.listPartitionsByFilter(tablePath, partitionFilters)\n+\t\t\t\t\t.stream()\n+\t\t\t\t\t.map(CatalogPartitionSpec::getPartitionSpec)\n+\t\t\t\t\t.collect(Collectors.toList());\n+\t\t\t\treturn Optional.of(remainingPartitions);\n+\t\t\t} catch (UnsupportedOperationException e) {\n+\t\t\t\treturn readPartitionFromCatalogWithoutFilterAndPrune(catalog, tablePath, pruner);\n+\t\t\t}\n+\t\t} else {\n+\t\t\treturn readPartitionFromCatalogWithoutFilterAndPrune(catalog, tablePath, pruner);\n+\t\t}\n+\t}\n+\n+\tprivate Optional<List<Map<String, String>>> readPartitionFromCatalogWithoutFilterAndPrune(\n+\t\t\tCatalog catalog,\n+\t\t\tObjectPath tablePath,\n+\t\t\tFunction<List<Map<String, String>>, List<Map<String, String>>> pruner) throws TableNotExistException, TableNotPartitionedException, CatalogException {\n+\t\tList<Map<String, String>> remainingPartitions;\n+\t\tList<Map<String, String>> partitions = catalog.listPartitions(tablePath)\n+\t\t\t.stream()\n+\t\t\t.map(CatalogPartitionSpec::getPartitionSpec)\n+\t\t\t.collect(Collectors.toList());\n+\t\t// prune partitions\n+\t\tif (partitions.size() > 0) {\n+\t\t\tremainingPartitions = pruner.apply(partitions);\n+\t\t\treturn Optional.of(remainingPartitions);\n+\t\t} else {\n+\t\t\treturn Optional.empty();\n+\t\t}\n+\t}\n+\n+\tprivate Optional<TableStats> getPartitionStats(Catalog catalog, ObjectPath tablePath, Map<String, String> partition) {\n+\t\ttry {\n+\t\t\tCatalogPartitionSpec spec = new CatalogPartitionSpec(partition);\n+\t\t\tCatalogTableStatistics partitionStat = catalog.getPartitionStatistics(tablePath, spec);\n+\t\t\tCatalogColumnStatistics\tpartitionColStat = catalog.getPartitionColumnStatistics(tablePath, spec);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9a05a030c661def4a45b36370dbcfa5e786ed8dc"}, "originalPosition": 318}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkyMjE5Mzg2OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQwNzoyNTo1NVrOG-D2JQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQwNzoyNTo1NVrOG-D2JQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzcyNzkwOQ==", "bodyText": "change List<LogicalType> to LogicalType[] ?", "url": "https://github.com/apache/flink/pull/12966#discussion_r467727909", "createdAt": "2020-08-10T07:25:55Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,325 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.api.TableException;\n+import org.apache.flink.table.catalog.Catalog;\n+import org.apache.flink.table.catalog.CatalogPartitionSpec;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.catalog.ObjectIdentifier;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.catalog.exceptions.CatalogException;\n+import org.apache.flink.table.catalog.exceptions.PartitionNotExistException;\n+import org.apache.flink.table.catalog.exceptions.TableNotExistException;\n+import org.apache.flink.table.catalog.exceptions.TableNotPartitionedException;\n+import org.apache.flink.table.catalog.stats.CatalogColumnStatistics;\n+import org.apache.flink.table.catalog.stats.CatalogTableStatistics;\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsPartitionPushDown;\n+import org.apache.flink.table.expressions.Expression;\n+import org.apache.flink.table.expressions.ResolvedExpression;\n+import org.apache.flink.table.plan.stats.TableStats;\n+import org.apache.flink.table.planner.calcite.FlinkContext;\n+import org.apache.flink.table.planner.calcite.FlinkTypeFactory;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+import org.apache.flink.table.planner.plan.utils.FlinkRelOptUtil;\n+import org.apache.flink.table.planner.plan.utils.PartitionPruner;\n+import org.apache.flink.table.planner.plan.utils.RexNodeExtractor;\n+import org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter;\n+import org.apache.flink.table.planner.utils.CatalogTableStatisticsConverter;\n+import org.apache.flink.table.types.logical.LogicalType;\n+\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Filter;\n+import org.apache.calcite.rel.logical.LogicalTableScan;\n+import org.apache.calcite.rel.type.RelDataType;\n+import org.apache.calcite.rex.RexBuilder;\n+import org.apache.calcite.rex.RexInputRef;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.calcite.rex.RexShuttle;\n+import org.apache.calcite.rex.RexUtil;\n+import org.apache.calcite.tools.RelBuilder;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.TimeZone;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import scala.Option;\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.Seq;\n+\n+/**\n+ * Planner rule that tries to push partition evaluated by filter condition into a {@link LogicalTableScan}.\n+*/\n+public class PushPartitionIntoTableSourceScanRule extends RelOptRule {\n+\tpublic static final PushPartitionIntoTableSourceScanRule INSTANCE = new PushPartitionIntoTableSourceScanRule();\n+\n+\tpublic PushPartitionIntoTableSourceScanRule(){\n+\t\tsuper(operand(Filter.class,\n+\t\t\t\toperand(LogicalTableScan.class, none())),\n+\t\t\t\"PushPartitionTableSourceScanRule\");\n+\t}\n+\n+\t@Override\n+\tpublic boolean matches(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tif (filter.getCondition() == null) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tTableSourceTable tableSourceTable = call.rel(1).getTable().unwrap(TableSourceTable.class);\n+\t\tif (tableSourceTable == null){\n+\t\t\treturn false;\n+\t\t}\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource();\n+\t\tif (!(dynamicTableSource instanceof SupportsPartitionPushDown)) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tCatalogTable catalogTable = tableSourceTable.catalogTable();\n+\t\tif (!catalogTable.isPartitioned() || catalogTable.getPartitionKeys().isEmpty()) {\n+\t\t\treturn false;\n+\t\t}\n+\t\treturn Arrays.stream(tableSourceTable.extraDigests()).noneMatch(digest -> digest.startsWith(\"partitions=[\"));\n+\t}\n+\n+\t@Override\n+\tpublic void onMatch(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tLogicalTableScan scan = call.rel(1);\n+\t\tFlinkContext context = call.getPlanner().getContext().unwrap(FlinkContext.class);\n+\t\tTableSourceTable tableSourceTable = scan.getTable().unwrap(TableSourceTable.class);\n+\n+\t\t// build pruner\n+\t\tRelDataType inputFieldTypes = filter.getInput().getRowType();\n+\t\tList<String> inputFieldNames = inputFieldTypes.getFieldNames();\n+\t\tList<String> partitionFieldNames = tableSourceTable.catalogTable().getPartitionKeys();\n+\t\tRelBuilder relBuilder = call.builder();\n+\t\tRexBuilder rexBuilder = relBuilder.getRexBuilder();\n+\t\tTuple2<Seq<RexNode>, Seq<RexNode>> allPredicates = RexNodeExtractor.extractPartitionPredicateList(\n+\t\t\tfilter.getCondition(),\n+\t\t\tFlinkRelOptUtil.getMaxCnfNodeCount(scan),\n+\t\t\tinputFieldNames.toArray(new String[0]),\n+\t\t\trexBuilder,\n+\t\t\tpartitionFieldNames.toArray(new String[0])\n+\t\t\t);\n+\t\tRexNode partitionPredicate = RexUtil.composeConjunction(rexBuilder, JavaConversions.seqAsJavaList(allPredicates._1));\n+\n+\t\tif (partitionPredicate.isAlwaysTrue()){\n+\t\t\treturn;\n+\t\t}\n+\n+\t\tList<LogicalType> partitionFieldTypes = partitionFieldNames.stream().map(name -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9a05a030c661def4a45b36370dbcfa5e786ed8dc"}, "originalPosition": 134}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkyMjIxNTE0OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRuleTest.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQwNzozMzo0MVrOG-ECcw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQwNzozMzo0MVrOG-ECcw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzczMTA1OQ==", "bodyText": "It seems a lot of logic in PushPartitionIntoTableSourceScanRule is not covered, such as: listing partitions by filter, listing partition without filter, etc", "url": "https://github.com/apache/flink/pull/12966#discussion_r467731059", "createdAt": "2020-08-10T07:33:41Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRuleTest.java", "diffHunk": "@@ -0,0 +1,85 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.planner.calcite.CalciteConfig;\n+import org.apache.flink.table.planner.plan.optimize.program.BatchOptimizeContext;\n+import org.apache.flink.table.planner.plan.optimize.program.FlinkBatchProgram;\n+import org.apache.flink.table.planner.plan.optimize.program.FlinkHepRuleSetProgramBuilder;\n+import org.apache.flink.table.planner.plan.optimize.program.HEP_RULES_EXECUTION_TYPE;\n+import org.apache.flink.table.planner.utils.TableConfigUtils;\n+\n+import org.apache.calcite.plan.hep.HepMatchOrder;\n+import org.apache.calcite.rel.rules.FilterProjectTransposeRule;\n+import org.apache.calcite.tools.RuleSets;\n+\n+/**\n+ * Test for {@link PushPartitionIntoTableSourceScanRule}.\n+ */\n+public class PushPartitionIntoTableSourceScanRuleTest extends PushPartitionIntoLegacyTableSourceScanRuleTest{", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9a05a030c661def4a45b36370dbcfa5e786ed8dc"}, "originalPosition": 35}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkyMjIxODQ4OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/batch/sql/TableSourceITCase.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQwNzozNDo1M1rOG-EEVQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQwNzozNDo1M1rOG-EEVQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzczMTU0MQ==", "bodyText": "useless import", "url": "https://github.com/apache/flink/pull/12966#discussion_r467731541", "createdAt": "2020-08-10T07:34:53Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/batch/sql/TableSourceITCase.scala", "diffHunk": "@@ -20,14 +20,15 @@ package org.apache.flink.table.planner.runtime.batch.sql\n \n import org.apache.flink.table.planner.factories.TestValuesTableFactory\n import org.apache.flink.table.planner.runtime.utils.BatchTestBase.row\n-import org.apache.flink.table.planner.runtime.utils.{BatchTestBase, TestData}\n+import org.apache.flink.table.planner.runtime.utils.{BatchTestBase, TestData, TestingAppendSink}\n import org.apache.flink.table.planner.utils._\n import org.apache.flink.types.Row\n-\n import org.junit.{Before, Test}\n-\n import java.lang.{Boolean => JBool, Integer => JInt, Long => JLong}\n \n+import org.apache.flink.table.planner.JString", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9a05a030c661def4a45b36370dbcfa5e786ed8dc"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkyMjIzODc1OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/factories/TestValuesTableFactory.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQwNzo0MjowM1rOG-EQOQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQwNzo0MjowM1rOG-EQOQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzczNDU4NQ==", "bodyText": "move this line to line#394", "url": "https://github.com/apache/flink/pull/12966#discussion_r467734585", "createdAt": "2020-08-10T07:42:03Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/factories/TestValuesTableFactory.java", "diffHunk": "@@ -328,7 +357,59 @@ public DynamicTableSink createDynamicTableSink(Context context) {\n \t\t\tRUNTIME_SINK,\n \t\t\tSINK_EXPECTED_MESSAGES_NUM,\n \t\t\tNESTED_PROJECTION_SUPPORTED,\n-\t\t\tFILTERABLE_FIELDS));\n+\t\t\tFILTERABLE_FIELDS,\n+\t\t\tUSE_PARTITION_PUSH_DOWN,\n+\t\t\tPARTITION_LIST));\n+\t}\n+\n+\tprivate List<Map<String, String>> parsePartitionList(String partitionString) {\n+\t\treturn Arrays.stream(partitionString.split(\";\")).map(\n+\t\t\tpartition -> {\n+\t\t\t\tMap<String, String> spec = new HashMap<>();\n+\t\t\t\tArrays.stream(partition.split(\",\")).forEach(pair -> {\n+\t\t\t\t\tString[] split = pair.split(\":\");\n+\t\t\t\t\tspec.put(split[0].trim(), split[1].trim());\n+\t\t\t\t});\n+\t\t\t\treturn spec;\n+\t\t\t}\n+\t\t).collect(Collectors.toList());\n+\t}\n+\n+\tprivate Map<Map<String, String>, Collection<Row>> mapRowsToPartitions(\n+\t\t\tTableSchema schema,\n+\t\t\tCollection<Row> rows,\n+\t\t\tList<Map<String, String>> partitions) {\n+\t\tif (!rows.isEmpty() && partitions.isEmpty()) {\n+\t\t\tthrow new IllegalArgumentException(\n+\t\t\t\t\"Please add partition list if use partition push down. Currently TestValuesTableSource doesn't support create partition list automatically.\");\n+\t\t}\n+\t\tMap<Map<String, String>, Collection<Row>> map = new HashMap<>();\n+\t\tfor (Map<String, String> partition: partitions) {\n+\t\t\tmap.put(partition, new ArrayList<>());\n+\t\t}\n+\t\tString[] fieldnames = schema.getFieldNames();\n+\t\tboolean match = true;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9a05a030c661def4a45b36370dbcfa5e786ed8dc"}, "originalPosition": 117}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkyMjI0ODk2OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/factories/TestValuesTableFactory.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQwNzo0NTozNlrOG-EWHA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQwNzo0NTozNlrOG-EWHA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzczNjA5Mg==", "bodyText": "Map.Entry<?, ?> -> Map.Entry<String, String>", "url": "https://github.com/apache/flink/pull/12966#discussion_r467736092", "createdAt": "2020-08-10T07:45:36Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/factories/TestValuesTableFactory.java", "diffHunk": "@@ -328,7 +357,59 @@ public DynamicTableSink createDynamicTableSink(Context context) {\n \t\t\tRUNTIME_SINK,\n \t\t\tSINK_EXPECTED_MESSAGES_NUM,\n \t\t\tNESTED_PROJECTION_SUPPORTED,\n-\t\t\tFILTERABLE_FIELDS));\n+\t\t\tFILTERABLE_FIELDS,\n+\t\t\tUSE_PARTITION_PUSH_DOWN,\n+\t\t\tPARTITION_LIST));\n+\t}\n+\n+\tprivate List<Map<String, String>> parsePartitionList(String partitionString) {\n+\t\treturn Arrays.stream(partitionString.split(\";\")).map(\n+\t\t\tpartition -> {\n+\t\t\t\tMap<String, String> spec = new HashMap<>();\n+\t\t\t\tArrays.stream(partition.split(\",\")).forEach(pair -> {\n+\t\t\t\t\tString[] split = pair.split(\":\");\n+\t\t\t\t\tspec.put(split[0].trim(), split[1].trim());\n+\t\t\t\t});\n+\t\t\t\treturn spec;\n+\t\t\t}\n+\t\t).collect(Collectors.toList());\n+\t}\n+\n+\tprivate Map<Map<String, String>, Collection<Row>> mapRowsToPartitions(\n+\t\t\tTableSchema schema,\n+\t\t\tCollection<Row> rows,\n+\t\t\tList<Map<String, String>> partitions) {\n+\t\tif (!rows.isEmpty() && partitions.isEmpty()) {\n+\t\t\tthrow new IllegalArgumentException(\n+\t\t\t\t\"Please add partition list if use partition push down. Currently TestValuesTableSource doesn't support create partition list automatically.\");\n+\t\t}\n+\t\tMap<Map<String, String>, Collection<Row>> map = new HashMap<>();\n+\t\tfor (Map<String, String> partition: partitions) {\n+\t\t\tmap.put(partition, new ArrayList<>());\n+\t\t}\n+\t\tString[] fieldnames = schema.getFieldNames();\n+\t\tboolean match = true;\n+\t\tfor (Row row: rows) {\n+\t\t\tfor (Map<String, String> partition: partitions) {\n+\t\t\t\tmatch = true;\n+\t\t\t\tfor (Map.Entry<?, ?> entry: partition.entrySet()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9a05a030c661def4a45b36370dbcfa5e786ed8dc"}, "originalPosition": 121}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkyMjI1MzQ3OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/factories/TestValuesTableFactory.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQwNzo0NzoyNVrOG-EYww==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQwNzo0NzoyNVrOG-EYww==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzczNjc3MQ==", "bodyText": "Collections.EMPTY_LIST.equals(allPartitions) -> allPartitions.isEmpty()", "url": "https://github.com/apache/flink/pull/12966#discussion_r467736771", "createdAt": "2020-08-10T07:47:25Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/factories/TestValuesTableFactory.java", "diffHunk": "@@ -629,35 +728,53 @@ public String asSummaryString() {\n \t\t}\n \n \t\tprivate Collection<RowData> convertToRowData(\n-\t\t\t\tCollection<Row> data,\n+\t\t\t\tMap<Map<String, String>, Collection<Row>> data,\n \t\t\t\tint[] projectedFields,\n \t\t\t\tDataStructureConverter converter) {\n \t\t\tList<RowData> result = new ArrayList<>();\n-\t\t\tfor (Row value : data) {\n-\t\t\t\tif (result.size() >= limit) {\n-\t\t\t\t\treturn result;\n-\t\t\t\t}\n-\t\t\t\tif (isRetainedAfterApplyingFilterPredicates(value)) {\n-\t\t\t\t\tRow projectedRow;\n-\t\t\t\t\tif (projectedFields == null) {\n-\t\t\t\t\t\tprojectedRow = value;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\tObject[] newValues = new Object[projectedFields.length];\n-\t\t\t\t\t\tfor (int i = 0; i < projectedFields.length; ++i) {\n-\t\t\t\t\t\t\tnewValues[i] = value.getField(projectedFields[i]);\n-\t\t\t\t\t\t}\n-\t\t\t\t\t\tprojectedRow = Row.of(newValues);\n+\t\t\tList<Map<String, String>> keys = Collections.EMPTY_LIST.equals(allPartitions) ?", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9a05a030c661def4a45b36370dbcfa5e786ed8dc"}, "originalPosition": 263}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkyMjI1OTI0OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/factories/TestValuesTableFactory.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQwNzo0OTo0MVrOG-EcHg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQwNzo0OTo0MVrOG-EcHg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzczNzYzMA==", "bodyText": "nit: .stream() is unnecessary", "url": "https://github.com/apache/flink/pull/12966#discussion_r467737630", "createdAt": "2020-08-10T07:49:41Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/factories/TestValuesTableFactory.java", "diffHunk": "@@ -459,7 +547,15 @@ public LookupRuntimeProvider getLookupRuntimeProvider(LookupContext context) {\n \t\t\t\t.mapToInt(k -> k[0])\n \t\t\t\t.toArray();\n \t\t\tMap<Row, List<Row>> mapping = new HashMap<>();\n-\t\t\tdata.forEach(record -> {\n+\t\t\tCollection<Row> rows;\n+\t\t\tif (allPartitions.equals(Collections.EMPTY_LIST)) {\n+\t\t\t\trows = data.getOrDefault(Collections.EMPTY_MAP, Collections.EMPTY_LIST);\n+\t\t\t} else {\n+\t\t\t\trows = new ArrayList<>();\n+\t\t\t\tallPartitions.stream()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9a05a030c661def4a45b36370dbcfa5e786ed8dc"}, "originalPosition": 205}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkyMjI2NTY2OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/factories/TestValuesTableFactory.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQwNzo1MjowMFrOG-Efww==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQwNzo1MjowMFrOG-Efww==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzczODU2Mw==", "bodyText": "whether we can use PARTITION_LIST is not empty (including null) to represent use-partition-push-down=true ? then this config can be removed", "url": "https://github.com/apache/flink/pull/12966#discussion_r467738563", "createdAt": "2020-08-10T07:52:00Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/factories/TestValuesTableFactory.java", "diffHunk": "@@ -243,6 +245,19 @@ private static RowKind parseRowKind(String rowKindShortString) {\n \t\t.asList()\n \t\t.noDefaultValue();\n \n+\tprivate static final ConfigOption<Boolean> USE_PARTITION_PUSH_DOWN = ConfigOptions", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9a05a030c661def4a45b36370dbcfa5e786ed8dc"}, "originalPosition": 20}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNTY2Mjk3OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QwOToxMjowOFrOHAC_pQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QwOToxMjowOFrOHAC_pQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTgxMTEwOQ==", "bodyText": "\"PushPartitionTableSourceScanRule\" -> \"PushPartitionIntoTableSourceScanRule\"", "url": "https://github.com/apache/flink/pull/12966#discussion_r469811109", "createdAt": "2020-08-13T09:12:08Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,343 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.api.TableException;\n+import org.apache.flink.table.catalog.Catalog;\n+import org.apache.flink.table.catalog.CatalogPartitionSpec;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.catalog.ObjectIdentifier;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.catalog.exceptions.CatalogException;\n+import org.apache.flink.table.catalog.exceptions.PartitionNotExistException;\n+import org.apache.flink.table.catalog.exceptions.TableNotExistException;\n+import org.apache.flink.table.catalog.exceptions.TableNotPartitionedException;\n+import org.apache.flink.table.catalog.stats.CatalogColumnStatistics;\n+import org.apache.flink.table.catalog.stats.CatalogTableStatistics;\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsPartitionPushDown;\n+import org.apache.flink.table.expressions.Expression;\n+import org.apache.flink.table.expressions.ResolvedExpression;\n+import org.apache.flink.table.plan.stats.TableStats;\n+import org.apache.flink.table.planner.calcite.FlinkContext;\n+import org.apache.flink.table.planner.calcite.FlinkTypeFactory;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+import org.apache.flink.table.planner.plan.utils.FlinkRelOptUtil;\n+import org.apache.flink.table.planner.plan.utils.PartitionPruner;\n+import org.apache.flink.table.planner.plan.utils.RexNodeExtractor;\n+import org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter;\n+import org.apache.flink.table.planner.utils.CatalogTableStatisticsConverter;\n+import org.apache.flink.table.types.logical.LogicalType;\n+\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Filter;\n+import org.apache.calcite.rel.logical.LogicalTableScan;\n+import org.apache.calcite.rel.type.RelDataType;\n+import org.apache.calcite.rex.RexBuilder;\n+import org.apache.calcite.rex.RexInputRef;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.calcite.rex.RexShuttle;\n+import org.apache.calcite.rex.RexUtil;\n+import org.apache.calcite.tools.RelBuilder;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.TimeZone;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import scala.Option;\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.Seq;\n+\n+/**\n+ * Planner rule that tries to push partition evaluated by filter condition into a {@link LogicalTableScan}.\n+*/\n+public class PushPartitionIntoTableSourceScanRule extends RelOptRule {\n+\tpublic static final PushPartitionIntoTableSourceScanRule INSTANCE = new PushPartitionIntoTableSourceScanRule();\n+\n+\tpublic PushPartitionIntoTableSourceScanRule() {\n+\t\tsuper(operand(Filter.class,\n+\t\t\t\toperand(LogicalTableScan.class, none())),\n+\t\t\t\"PushPartitionTableSourceScanRule\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b1b4efe76749cd0dfa6b4aea94ca2ce9f0d15be1"}, "originalPosition": 84}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNTc0NzU0OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QwOTozNDoxNlrOHADzKQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QwOTozNDoxNlrOHADzKQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTgyNDI5Nw==", "bodyText": "partitionFilters.size() should not be empty, because line#128 has checked it", "url": "https://github.com/apache/flink/pull/12966#discussion_r469824297", "createdAt": "2020-08-13T09:34:16Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,343 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.api.TableException;\n+import org.apache.flink.table.catalog.Catalog;\n+import org.apache.flink.table.catalog.CatalogPartitionSpec;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.catalog.ObjectIdentifier;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.catalog.exceptions.CatalogException;\n+import org.apache.flink.table.catalog.exceptions.PartitionNotExistException;\n+import org.apache.flink.table.catalog.exceptions.TableNotExistException;\n+import org.apache.flink.table.catalog.exceptions.TableNotPartitionedException;\n+import org.apache.flink.table.catalog.stats.CatalogColumnStatistics;\n+import org.apache.flink.table.catalog.stats.CatalogTableStatistics;\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsPartitionPushDown;\n+import org.apache.flink.table.expressions.Expression;\n+import org.apache.flink.table.expressions.ResolvedExpression;\n+import org.apache.flink.table.plan.stats.TableStats;\n+import org.apache.flink.table.planner.calcite.FlinkContext;\n+import org.apache.flink.table.planner.calcite.FlinkTypeFactory;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+import org.apache.flink.table.planner.plan.utils.FlinkRelOptUtil;\n+import org.apache.flink.table.planner.plan.utils.PartitionPruner;\n+import org.apache.flink.table.planner.plan.utils.RexNodeExtractor;\n+import org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter;\n+import org.apache.flink.table.planner.utils.CatalogTableStatisticsConverter;\n+import org.apache.flink.table.types.logical.LogicalType;\n+\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Filter;\n+import org.apache.calcite.rel.logical.LogicalTableScan;\n+import org.apache.calcite.rel.type.RelDataType;\n+import org.apache.calcite.rex.RexBuilder;\n+import org.apache.calcite.rex.RexInputRef;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.calcite.rex.RexShuttle;\n+import org.apache.calcite.rex.RexUtil;\n+import org.apache.calcite.tools.RelBuilder;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.TimeZone;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import scala.Option;\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.Seq;\n+\n+/**\n+ * Planner rule that tries to push partition evaluated by filter condition into a {@link LogicalTableScan}.\n+*/\n+public class PushPartitionIntoTableSourceScanRule extends RelOptRule {\n+\tpublic static final PushPartitionIntoTableSourceScanRule INSTANCE = new PushPartitionIntoTableSourceScanRule();\n+\n+\tpublic PushPartitionIntoTableSourceScanRule() {\n+\t\tsuper(operand(Filter.class,\n+\t\t\t\toperand(LogicalTableScan.class, none())),\n+\t\t\t\"PushPartitionTableSourceScanRule\");\n+\t}\n+\n+\t@Override\n+\tpublic boolean matches(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tif (filter.getCondition() == null) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tTableSourceTable tableSourceTable = call.rel(1).getTable().unwrap(TableSourceTable.class);\n+\t\tif (tableSourceTable == null) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource();\n+\t\tif (!(dynamicTableSource instanceof SupportsPartitionPushDown)) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tCatalogTable catalogTable = tableSourceTable.catalogTable();\n+\t\tif (!catalogTable.isPartitioned() || catalogTable.getPartitionKeys().isEmpty()) {\n+\t\t\treturn false;\n+\t\t}\n+\t\treturn Arrays.stream(tableSourceTable.extraDigests()).noneMatch(digest -> digest.startsWith(\"partitions=[\"));\n+\t}\n+\n+\t@Override\n+\tpublic void onMatch(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tLogicalTableScan scan = call.rel(1);\n+\t\tTableSourceTable tableSourceTable = scan.getTable().unwrap(TableSourceTable.class);\n+\n+\t\tRelDataType inputFieldTypes = filter.getInput().getRowType();\n+\t\tList<String> inputFieldNames = inputFieldTypes.getFieldNames();\n+\t\tList<String> partitionFieldNames = tableSourceTable.catalogTable().getPartitionKeys();\n+\t\t// extract partition predicates\n+\t\tRelBuilder relBuilder = call.builder();\n+\t\tRexBuilder rexBuilder = relBuilder.getRexBuilder();\n+\t\tTuple2<Seq<RexNode>, Seq<RexNode>> allPredicates = RexNodeExtractor.extractPartitionPredicateList(\n+\t\t\tfilter.getCondition(),\n+\t\t\tFlinkRelOptUtil.getMaxCnfNodeCount(scan),\n+\t\t\tinputFieldNames.toArray(new String[0]),\n+\t\t\trexBuilder,\n+\t\t\tpartitionFieldNames.toArray(new String[0]));\n+\t\tRexNode partitionPredicate = RexUtil.composeConjunction(rexBuilder, JavaConversions.seqAsJavaList(allPredicates._1));\n+\n+\t\tif (partitionPredicate.isAlwaysTrue()) {\n+\t\t\treturn;\n+\t\t}\n+\t\t// build pruner\n+\t\tLogicalType[] partitionFieldTypes = partitionFieldNames.stream()\n+\t\t\t.map(name -> {\n+\t\t\t\tint index  = inputFieldNames.indexOf(name);\n+\t\t\t\tif (index < 0) {\n+\t\t\t\t\tthrow new TableException(String.format(\"Partitioned key '%s' isn't found in input columns. \" +\n+\t\t\t\t\t\t\"Validator should have checked that.\", name));\n+\t\t\t\t}\n+\t\t\t\treturn inputFieldTypes.getFieldList().get(index).getType(); })\n+\t\t\t.map(FlinkTypeFactory::toLogicalType)\n+\t\t\t.toArray(LogicalType[]::new);\n+\t\tRexNode finalPartitionPredicate = adjustPartitionPredicate(inputFieldNames, partitionFieldNames, partitionPredicate);\n+\t\tFlinkContext context = call.getPlanner().getContext().unwrap(FlinkContext.class);\n+\t\tFunction<List<Map<String, String>>, List<Map<String, String>>> defaultPruner = partitions -> PartitionPruner.prunePartitions(\n+\t\t\tcontext.getTableConfig(),\n+\t\t\tpartitionFieldNames.toArray(new String[0]),\n+\t\t\tpartitionFieldTypes,\n+\t\t\tpartitions,\n+\t\t\tfinalPartitionPredicate);\n+\t\t// prune partitions\n+\t\tOptional<List<Map<String, String>>> remainingPartitions =\n+\t\t\treadPartitionsAndPrune(context, tableSourceTable, defaultPruner, allPredicates._1(), inputFieldNames);\n+\t\t// apply push down\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource().copy();\n+\t\tremainingPartitions.ifPresent(((SupportsPartitionPushDown) dynamicTableSource)::applyPartitions);\n+\n+\t\t// build new statistic\n+\t\tTableStats newTableStat = null;\n+\t\tObjectIdentifier identifier = tableSourceTable.tableIdentifier();\n+\t\tObjectPath tablePath = identifier.toObjectPath();\n+\t\tOptional<Catalog> catalogOptional = context.getCatalogManager().getCatalog(identifier.getCatalogName());\n+\t\tOptional<TableStats> partitionStats;\n+\t\tif (remainingPartitions.isPresent() && catalogOptional.isPresent()) {\n+\t\t\tfor (Map<String, String> partition: remainingPartitions.get()) {\n+\t\t\t\tpartitionStats = getPartitionStats(catalogOptional.get(), tablePath, partition);\n+\t\t\t\tif (!partitionStats.isPresent()) {\n+\t\t\t\t\t// clear all information before\n+\t\t\t\t\tnewTableStat = null;\n+\t\t\t\t\tbreak;\n+\t\t\t\t} else {\n+\t\t\t\t\tnewTableStat = newTableStat == null ? partitionStats.get() : newTableStat.merge(partitionStats.get());\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t\tFlinkStatistic newStatistic = FlinkStatistic.builder()\n+\t\t\t.statistic(tableSourceTable.getStatistic())\n+\t\t\t.tableStats(newTableStat)\n+\t\t\t.build();\n+\n+\t\tString extraDigest = remainingPartitions.map(partition -> (\"partitions=[\" +\n+\t\t\tString.join(\", \", partition\n+\t\t\t\t.stream()\n+\t\t\t\t.map(Object::toString)\n+\t\t\t\t.toArray(String[]::new)) +\n+\t\t\t\"]\")).orElse(\"partitions=[]\");\n+\t\tTableSourceTable newTableSourceTable = tableSourceTable.copy(dynamicTableSource, newStatistic, new String[]{extraDigest});\n+\t\tLogicalTableScan newScan = LogicalTableScan.create(scan.getCluster(), newTableSourceTable, scan.getHints());\n+\n+\t\t// transform to new node\n+\t\tRexNode nonPartitionPredicate = RexUtil.composeConjunction(rexBuilder, JavaConversions.seqAsJavaList(allPredicates._2()));\n+\t\tif (nonPartitionPredicate.isAlwaysTrue()) {\n+\t\t\tcall.transformTo(newScan);\n+\t\t} else {\n+\t\t\tFilter newFilter = filter.copy(filter.getTraitSet(), newScan, nonPartitionPredicate);\n+\t\t\tcall.transformTo(newFilter);\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * adjust the partition field reference index to evaluate the partition values.\n+\t * e.g. the original input fields is: a, b, c, p, and p is partition field. the partition values\n+\t * are: List(Map(\"p\"->\"1\"), Map(\"p\" -> \"2\"), Map(\"p\" -> \"3\")). If the original partition\n+\t * predicate is $3 > 1. after adjusting, the new predicate is ($0 > 1).\n+\t * and use ($0 > 1) to evaluate partition values (row(1), row(2), row(3)).\n+\t */\n+\tprivate RexNode adjustPartitionPredicate(List<String> inputFieldNames, List<String> partitionFieldNames, RexNode partitionPredicate) {\n+\t\treturn partitionPredicate.accept(new RexShuttle() {\n+\t\t\t@Override\n+\t\t\tpublic RexNode visitInputRef(RexInputRef inputRef) {\n+\t\t\t\tint index = inputRef.getIndex();\n+\t\t\t\tString fieldName = inputFieldNames.get(index);\n+\t\t\t\tint newIndex = partitionFieldNames.indexOf(fieldName);\n+\t\t\t\tif (newIndex < 0) {\n+\t\t\t\t\tthrow new TableException(String.format(\"Field name '%s' isn't found in partitioned columns.\" +\n+\t\t\t\t\t\t\" Validator should have checked that.\", fieldName));\n+\t\t\t\t}\n+\t\t\t\tif (newIndex == index) {\n+\t\t\t\t\treturn inputRef;\n+\t\t\t\t} else {\n+\t\t\t\t\treturn new RexInputRef(newIndex, inputRef.getType());\n+\t\t\t\t}\n+\t\t\t}\n+\t\t});\n+\t}\n+\n+\tprivate Optional<List<Map<String, String>>> readPartitionsAndPrune(\n+\t\t\tFlinkContext context,\n+\t\t\tTableSourceTable tableSourceTable,\n+\t\t\tFunction<List<Map<String, String>>, List<Map<String, String>>> pruner,\n+\t\t\tSeq<RexNode> partitionPredicate,\n+\t\t\tList<String> inputFieldNames) {\n+\t\t// get partitions from table/catalog and prune\n+\t\tOptional<Catalog> catalogOptional = context.getCatalogManager().getCatalog(tableSourceTable.tableIdentifier().getCatalogName());\n+\t\tList<Map<String, String>> remainingPartitions;\n+\t\tOptional<List<Map<String, String>>> optionalPartitions;\n+\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource();\n+\t\tObjectIdentifier identifier = tableSourceTable.tableIdentifier();\n+\t\ttry {\n+\t\t\toptionalPartitions = ((SupportsPartitionPushDown) dynamicTableSource).listPartitions();\n+\t\t\tif (optionalPartitions.isPresent() && !optionalPartitions.get().isEmpty()) {\n+\t\t\t\tremainingPartitions = pruner.apply(optionalPartitions.get());\n+\t\t\t\treturn remainingPartitions != null ? Optional.of(remainingPartitions) : Optional.empty();\n+\t\t\t} else {\n+\t\t\t\treturn Optional.empty();\n+\t\t\t}\n+\t\t} catch (UnsupportedOperationException e) {\n+\t\t\t// check catalog whether is available\n+\t\t\t// we will read partitions from catalog if table doesn't support listPartitions.\n+\t\t\tif (!catalogOptional.isPresent()) {\n+\t\t\t\tthrow new TableException(\n+\t\t\t\t\tString.format(\"Table %s must from a catalog, but %s is not a catalog\",\n+\t\t\t\t\t\tidentifier.asSummaryString(), identifier.getCatalogName()), e);\n+\t\t\t}\n+\t\t\ttry {\n+\t\t\t\treturn readPartitionFromCatalogAndPrune(\n+\t\t\t\t\tcontext,\n+\t\t\t\t\tcatalogOptional.get(),\n+\t\t\t\t\tidentifier,\n+\t\t\t\t\tinputFieldNames,\n+\t\t\t\t\tpartitionPredicate,\n+\t\t\t\t\tpruner);\n+\t\t\t} catch (TableNotExistException tableNotExistException) {\n+\t\t\t\tthrow new TableException(String.format(\"Table %s is not found in catalog.\", identifier.asSummaryString()), e);\n+\t\t\t} catch (TableNotPartitionedException tableNotPartitionedException) {\n+\t\t\t\tthrow new TableException(\n+\t\t\t\t\tString.format(\"Table %s is not a partitionable source. Validator should have checked it.\", identifier.asSummaryString()),\n+\t\t\t\t\ttableNotPartitionedException);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate Optional<List<Map<String, String>>> readPartitionFromCatalogAndPrune(\n+\t\t\tFlinkContext context,\n+\t\t\tCatalog catalog,\n+\t\t\tObjectIdentifier tableIdentifier,\n+\t\t\tList<String> allFieldNames,\n+\t\t\tSeq<RexNode> partitionPredicate,\n+\t\t\tFunction<List<Map<String, String>>, List<Map<String, String>>> pruner)\n+\t\t\tthrows TableNotExistException, TableNotPartitionedException {\n+\t\tObjectPath tablePath = tableIdentifier.toObjectPath();\n+\t\t// build filters\n+\t\tRexNodeToExpressionConverter converter = new RexNodeToExpressionConverter(\n+\t\t\tallFieldNames.toArray(new String[0]),\n+\t\t\tcontext.getFunctionCatalog(),\n+\t\t\tcontext.getCatalogManager(),\n+\t\t\tTimeZone.getTimeZone(context.getTableConfig().getLocalTimeZone()));\n+\t\tArrayList<Expression> partitionFilters = new ArrayList<>();\n+\t\tOption<ResolvedExpression> subExpr;\n+\t\tfor (RexNode node: JavaConversions.seqAsJavaList(partitionPredicate)) {\n+\t\t\tsubExpr = node.accept(converter);\n+\t\t\tif (!subExpr.isEmpty()) {\n+\t\t\t\tpartitionFilters.add(subExpr.get());\n+\t\t\t} else {\n+\t\t\t\t// if part of expr is unresolved, we read all partitions and prune.\n+\t\t\t\treturn readPartitionFromCatalogWithoutFilterAndPrune(catalog, tablePath, pruner);\n+\t\t\t}\n+\t\t}\n+\t\tif (partitionFilters.size() > 0) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b1b4efe76749cd0dfa6b4aea94ca2ce9f0d15be1"}, "originalPosition": 299}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNTc1Njg0OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QwOTozNjo0MlrOHAD42w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QwOTozNjo0MlrOHAD42w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTgyNTc1NQ==", "bodyText": "nit:  wrap the line at throws  ?", "url": "https://github.com/apache/flink/pull/12966#discussion_r469825755", "createdAt": "2020-08-13T09:36:42Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,343 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.api.TableException;\n+import org.apache.flink.table.catalog.Catalog;\n+import org.apache.flink.table.catalog.CatalogPartitionSpec;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.catalog.ObjectIdentifier;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.catalog.exceptions.CatalogException;\n+import org.apache.flink.table.catalog.exceptions.PartitionNotExistException;\n+import org.apache.flink.table.catalog.exceptions.TableNotExistException;\n+import org.apache.flink.table.catalog.exceptions.TableNotPartitionedException;\n+import org.apache.flink.table.catalog.stats.CatalogColumnStatistics;\n+import org.apache.flink.table.catalog.stats.CatalogTableStatistics;\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsPartitionPushDown;\n+import org.apache.flink.table.expressions.Expression;\n+import org.apache.flink.table.expressions.ResolvedExpression;\n+import org.apache.flink.table.plan.stats.TableStats;\n+import org.apache.flink.table.planner.calcite.FlinkContext;\n+import org.apache.flink.table.planner.calcite.FlinkTypeFactory;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+import org.apache.flink.table.planner.plan.utils.FlinkRelOptUtil;\n+import org.apache.flink.table.planner.plan.utils.PartitionPruner;\n+import org.apache.flink.table.planner.plan.utils.RexNodeExtractor;\n+import org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter;\n+import org.apache.flink.table.planner.utils.CatalogTableStatisticsConverter;\n+import org.apache.flink.table.types.logical.LogicalType;\n+\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Filter;\n+import org.apache.calcite.rel.logical.LogicalTableScan;\n+import org.apache.calcite.rel.type.RelDataType;\n+import org.apache.calcite.rex.RexBuilder;\n+import org.apache.calcite.rex.RexInputRef;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.calcite.rex.RexShuttle;\n+import org.apache.calcite.rex.RexUtil;\n+import org.apache.calcite.tools.RelBuilder;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.TimeZone;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import scala.Option;\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.Seq;\n+\n+/**\n+ * Planner rule that tries to push partition evaluated by filter condition into a {@link LogicalTableScan}.\n+*/\n+public class PushPartitionIntoTableSourceScanRule extends RelOptRule {\n+\tpublic static final PushPartitionIntoTableSourceScanRule INSTANCE = new PushPartitionIntoTableSourceScanRule();\n+\n+\tpublic PushPartitionIntoTableSourceScanRule() {\n+\t\tsuper(operand(Filter.class,\n+\t\t\t\toperand(LogicalTableScan.class, none())),\n+\t\t\t\"PushPartitionTableSourceScanRule\");\n+\t}\n+\n+\t@Override\n+\tpublic boolean matches(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tif (filter.getCondition() == null) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tTableSourceTable tableSourceTable = call.rel(1).getTable().unwrap(TableSourceTable.class);\n+\t\tif (tableSourceTable == null) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource();\n+\t\tif (!(dynamicTableSource instanceof SupportsPartitionPushDown)) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tCatalogTable catalogTable = tableSourceTable.catalogTable();\n+\t\tif (!catalogTable.isPartitioned() || catalogTable.getPartitionKeys().isEmpty()) {\n+\t\t\treturn false;\n+\t\t}\n+\t\treturn Arrays.stream(tableSourceTable.extraDigests()).noneMatch(digest -> digest.startsWith(\"partitions=[\"));\n+\t}\n+\n+\t@Override\n+\tpublic void onMatch(RelOptRuleCall call) {\n+\t\tFilter filter = call.rel(0);\n+\t\tLogicalTableScan scan = call.rel(1);\n+\t\tTableSourceTable tableSourceTable = scan.getTable().unwrap(TableSourceTable.class);\n+\n+\t\tRelDataType inputFieldTypes = filter.getInput().getRowType();\n+\t\tList<String> inputFieldNames = inputFieldTypes.getFieldNames();\n+\t\tList<String> partitionFieldNames = tableSourceTable.catalogTable().getPartitionKeys();\n+\t\t// extract partition predicates\n+\t\tRelBuilder relBuilder = call.builder();\n+\t\tRexBuilder rexBuilder = relBuilder.getRexBuilder();\n+\t\tTuple2<Seq<RexNode>, Seq<RexNode>> allPredicates = RexNodeExtractor.extractPartitionPredicateList(\n+\t\t\tfilter.getCondition(),\n+\t\t\tFlinkRelOptUtil.getMaxCnfNodeCount(scan),\n+\t\t\tinputFieldNames.toArray(new String[0]),\n+\t\t\trexBuilder,\n+\t\t\tpartitionFieldNames.toArray(new String[0]));\n+\t\tRexNode partitionPredicate = RexUtil.composeConjunction(rexBuilder, JavaConversions.seqAsJavaList(allPredicates._1));\n+\n+\t\tif (partitionPredicate.isAlwaysTrue()) {\n+\t\t\treturn;\n+\t\t}\n+\t\t// build pruner\n+\t\tLogicalType[] partitionFieldTypes = partitionFieldNames.stream()\n+\t\t\t.map(name -> {\n+\t\t\t\tint index  = inputFieldNames.indexOf(name);\n+\t\t\t\tif (index < 0) {\n+\t\t\t\t\tthrow new TableException(String.format(\"Partitioned key '%s' isn't found in input columns. \" +\n+\t\t\t\t\t\t\"Validator should have checked that.\", name));\n+\t\t\t\t}\n+\t\t\t\treturn inputFieldTypes.getFieldList().get(index).getType(); })\n+\t\t\t.map(FlinkTypeFactory::toLogicalType)\n+\t\t\t.toArray(LogicalType[]::new);\n+\t\tRexNode finalPartitionPredicate = adjustPartitionPredicate(inputFieldNames, partitionFieldNames, partitionPredicate);\n+\t\tFlinkContext context = call.getPlanner().getContext().unwrap(FlinkContext.class);\n+\t\tFunction<List<Map<String, String>>, List<Map<String, String>>> defaultPruner = partitions -> PartitionPruner.prunePartitions(\n+\t\t\tcontext.getTableConfig(),\n+\t\t\tpartitionFieldNames.toArray(new String[0]),\n+\t\t\tpartitionFieldTypes,\n+\t\t\tpartitions,\n+\t\t\tfinalPartitionPredicate);\n+\t\t// prune partitions\n+\t\tOptional<List<Map<String, String>>> remainingPartitions =\n+\t\t\treadPartitionsAndPrune(context, tableSourceTable, defaultPruner, allPredicates._1(), inputFieldNames);\n+\t\t// apply push down\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource().copy();\n+\t\tremainingPartitions.ifPresent(((SupportsPartitionPushDown) dynamicTableSource)::applyPartitions);\n+\n+\t\t// build new statistic\n+\t\tTableStats newTableStat = null;\n+\t\tObjectIdentifier identifier = tableSourceTable.tableIdentifier();\n+\t\tObjectPath tablePath = identifier.toObjectPath();\n+\t\tOptional<Catalog> catalogOptional = context.getCatalogManager().getCatalog(identifier.getCatalogName());\n+\t\tOptional<TableStats> partitionStats;\n+\t\tif (remainingPartitions.isPresent() && catalogOptional.isPresent()) {\n+\t\t\tfor (Map<String, String> partition: remainingPartitions.get()) {\n+\t\t\t\tpartitionStats = getPartitionStats(catalogOptional.get(), tablePath, partition);\n+\t\t\t\tif (!partitionStats.isPresent()) {\n+\t\t\t\t\t// clear all information before\n+\t\t\t\t\tnewTableStat = null;\n+\t\t\t\t\tbreak;\n+\t\t\t\t} else {\n+\t\t\t\t\tnewTableStat = newTableStat == null ? partitionStats.get() : newTableStat.merge(partitionStats.get());\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t\tFlinkStatistic newStatistic = FlinkStatistic.builder()\n+\t\t\t.statistic(tableSourceTable.getStatistic())\n+\t\t\t.tableStats(newTableStat)\n+\t\t\t.build();\n+\n+\t\tString extraDigest = remainingPartitions.map(partition -> (\"partitions=[\" +\n+\t\t\tString.join(\", \", partition\n+\t\t\t\t.stream()\n+\t\t\t\t.map(Object::toString)\n+\t\t\t\t.toArray(String[]::new)) +\n+\t\t\t\"]\")).orElse(\"partitions=[]\");\n+\t\tTableSourceTable newTableSourceTable = tableSourceTable.copy(dynamicTableSource, newStatistic, new String[]{extraDigest});\n+\t\tLogicalTableScan newScan = LogicalTableScan.create(scan.getCluster(), newTableSourceTable, scan.getHints());\n+\n+\t\t// transform to new node\n+\t\tRexNode nonPartitionPredicate = RexUtil.composeConjunction(rexBuilder, JavaConversions.seqAsJavaList(allPredicates._2()));\n+\t\tif (nonPartitionPredicate.isAlwaysTrue()) {\n+\t\t\tcall.transformTo(newScan);\n+\t\t} else {\n+\t\t\tFilter newFilter = filter.copy(filter.getTraitSet(), newScan, nonPartitionPredicate);\n+\t\t\tcall.transformTo(newFilter);\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * adjust the partition field reference index to evaluate the partition values.\n+\t * e.g. the original input fields is: a, b, c, p, and p is partition field. the partition values\n+\t * are: List(Map(\"p\"->\"1\"), Map(\"p\" -> \"2\"), Map(\"p\" -> \"3\")). If the original partition\n+\t * predicate is $3 > 1. after adjusting, the new predicate is ($0 > 1).\n+\t * and use ($0 > 1) to evaluate partition values (row(1), row(2), row(3)).\n+\t */\n+\tprivate RexNode adjustPartitionPredicate(List<String> inputFieldNames, List<String> partitionFieldNames, RexNode partitionPredicate) {\n+\t\treturn partitionPredicate.accept(new RexShuttle() {\n+\t\t\t@Override\n+\t\t\tpublic RexNode visitInputRef(RexInputRef inputRef) {\n+\t\t\t\tint index = inputRef.getIndex();\n+\t\t\t\tString fieldName = inputFieldNames.get(index);\n+\t\t\t\tint newIndex = partitionFieldNames.indexOf(fieldName);\n+\t\t\t\tif (newIndex < 0) {\n+\t\t\t\t\tthrow new TableException(String.format(\"Field name '%s' isn't found in partitioned columns.\" +\n+\t\t\t\t\t\t\" Validator should have checked that.\", fieldName));\n+\t\t\t\t}\n+\t\t\t\tif (newIndex == index) {\n+\t\t\t\t\treturn inputRef;\n+\t\t\t\t} else {\n+\t\t\t\t\treturn new RexInputRef(newIndex, inputRef.getType());\n+\t\t\t\t}\n+\t\t\t}\n+\t\t});\n+\t}\n+\n+\tprivate Optional<List<Map<String, String>>> readPartitionsAndPrune(\n+\t\t\tFlinkContext context,\n+\t\t\tTableSourceTable tableSourceTable,\n+\t\t\tFunction<List<Map<String, String>>, List<Map<String, String>>> pruner,\n+\t\t\tSeq<RexNode> partitionPredicate,\n+\t\t\tList<String> inputFieldNames) {\n+\t\t// get partitions from table/catalog and prune\n+\t\tOptional<Catalog> catalogOptional = context.getCatalogManager().getCatalog(tableSourceTable.tableIdentifier().getCatalogName());\n+\t\tList<Map<String, String>> remainingPartitions;\n+\t\tOptional<List<Map<String, String>>> optionalPartitions;\n+\n+\t\tDynamicTableSource dynamicTableSource = tableSourceTable.tableSource();\n+\t\tObjectIdentifier identifier = tableSourceTable.tableIdentifier();\n+\t\ttry {\n+\t\t\toptionalPartitions = ((SupportsPartitionPushDown) dynamicTableSource).listPartitions();\n+\t\t\tif (optionalPartitions.isPresent() && !optionalPartitions.get().isEmpty()) {\n+\t\t\t\tremainingPartitions = pruner.apply(optionalPartitions.get());\n+\t\t\t\treturn remainingPartitions != null ? Optional.of(remainingPartitions) : Optional.empty();\n+\t\t\t} else {\n+\t\t\t\treturn Optional.empty();\n+\t\t\t}\n+\t\t} catch (UnsupportedOperationException e) {\n+\t\t\t// check catalog whether is available\n+\t\t\t// we will read partitions from catalog if table doesn't support listPartitions.\n+\t\t\tif (!catalogOptional.isPresent()) {\n+\t\t\t\tthrow new TableException(\n+\t\t\t\t\tString.format(\"Table %s must from a catalog, but %s is not a catalog\",\n+\t\t\t\t\t\tidentifier.asSummaryString(), identifier.getCatalogName()), e);\n+\t\t\t}\n+\t\t\ttry {\n+\t\t\t\treturn readPartitionFromCatalogAndPrune(\n+\t\t\t\t\tcontext,\n+\t\t\t\t\tcatalogOptional.get(),\n+\t\t\t\t\tidentifier,\n+\t\t\t\t\tinputFieldNames,\n+\t\t\t\t\tpartitionPredicate,\n+\t\t\t\t\tpruner);\n+\t\t\t} catch (TableNotExistException tableNotExistException) {\n+\t\t\t\tthrow new TableException(String.format(\"Table %s is not found in catalog.\", identifier.asSummaryString()), e);\n+\t\t\t} catch (TableNotPartitionedException tableNotPartitionedException) {\n+\t\t\t\tthrow new TableException(\n+\t\t\t\t\tString.format(\"Table %s is not a partitionable source. Validator should have checked it.\", identifier.asSummaryString()),\n+\t\t\t\t\ttableNotPartitionedException);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate Optional<List<Map<String, String>>> readPartitionFromCatalogAndPrune(\n+\t\t\tFlinkContext context,\n+\t\t\tCatalog catalog,\n+\t\t\tObjectIdentifier tableIdentifier,\n+\t\t\tList<String> allFieldNames,\n+\t\t\tSeq<RexNode> partitionPredicate,\n+\t\t\tFunction<List<Map<String, String>>, List<Map<String, String>>> pruner)\n+\t\t\tthrows TableNotExistException, TableNotPartitionedException {\n+\t\tObjectPath tablePath = tableIdentifier.toObjectPath();\n+\t\t// build filters\n+\t\tRexNodeToExpressionConverter converter = new RexNodeToExpressionConverter(\n+\t\t\tallFieldNames.toArray(new String[0]),\n+\t\t\tcontext.getFunctionCatalog(),\n+\t\t\tcontext.getCatalogManager(),\n+\t\t\tTimeZone.getTimeZone(context.getTableConfig().getLocalTimeZone()));\n+\t\tArrayList<Expression> partitionFilters = new ArrayList<>();\n+\t\tOption<ResolvedExpression> subExpr;\n+\t\tfor (RexNode node: JavaConversions.seqAsJavaList(partitionPredicate)) {\n+\t\t\tsubExpr = node.accept(converter);\n+\t\t\tif (!subExpr.isEmpty()) {\n+\t\t\t\tpartitionFilters.add(subExpr.get());\n+\t\t\t} else {\n+\t\t\t\t// if part of expr is unresolved, we read all partitions and prune.\n+\t\t\t\treturn readPartitionFromCatalogWithoutFilterAndPrune(catalog, tablePath, pruner);\n+\t\t\t}\n+\t\t}\n+\t\tif (partitionFilters.size() > 0) {\n+\t\t\ttry {\n+\t\t\t\tList<Map<String, String>> remainingPartitions = catalog.listPartitionsByFilter(tablePath, partitionFilters)\n+\t\t\t\t\t.stream()\n+\t\t\t\t\t.map(CatalogPartitionSpec::getPartitionSpec)\n+\t\t\t\t\t.collect(Collectors.toList());\n+\t\t\t\treturn Optional.of(remainingPartitions);\n+\t\t\t} catch (UnsupportedOperationException e) {\n+\t\t\t\treturn readPartitionFromCatalogWithoutFilterAndPrune(catalog, tablePath, pruner);\n+\t\t\t}\n+\t\t} else {\n+\t\t\treturn readPartitionFromCatalogWithoutFilterAndPrune(catalog, tablePath, pruner);\n+\t\t}\n+\t}\n+\n+\tprivate Optional<List<Map<String, String>>> readPartitionFromCatalogWithoutFilterAndPrune(\n+\t\t\tCatalog catalog,\n+\t\t\tObjectPath tablePath,\n+\t\t\tFunction<List<Map<String, String>>, List<Map<String, String>>> pruner) throws TableNotExistException, CatalogException, TableNotPartitionedException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b1b4efe76749cd0dfa6b4aea94ca2ce9f0d15be1"}, "originalPosition": 317}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNTc4MzQ1OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/factories/TestValuesTableFactory.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QwOTo0Mzo1NVrOHAEI2w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QwOTo0Mzo1NVrOHAEI2w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTgyOTg1MQ==", "bodyText": "move this class to org.apache.flink.table.planner.utils ? which could make TestValuesTableFactory more lightweight.", "url": "https://github.com/apache/flink/pull/12966#discussion_r469829851", "createdAt": "2020-08-13T09:43:55Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/factories/TestValuesTableFactory.java", "diffHunk": "@@ -821,4 +848,125 @@ public String asSummaryString() {\n \t\t}\n \t}\n \n+\t// --------------------------------------------------------------------------------------------\n+\t// Table utils\n+\t// --------------------------------------------------------------------------------------------\n+\n+\t/**\n+\t * Utils for catalog and source to filter partition or row.\n+\t * */\n+\tpublic static class FilterUtil {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b1b4efe76749cd0dfa6b4aea94ca2ce9f0d15be1"}, "originalPosition": 456}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNTc4ODAxOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/factories/TestValuesTableFactory.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QwOTo0NTowMVrOHAELbQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QwOTo0NTowMVrOHAELbQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTgzMDUwOQ==", "bodyText": "just make these utility methods static ? then we can remove this field.", "url": "https://github.com/apache/flink/pull/12966#discussion_r469830509", "createdAt": "2020-08-13T09:45:01Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/factories/TestValuesTableFactory.java", "diffHunk": "@@ -821,4 +848,125 @@ public String asSummaryString() {\n \t\t}\n \t}\n \n+\t// --------------------------------------------------------------------------------------------\n+\t// Table utils\n+\t// --------------------------------------------------------------------------------------------\n+\n+\t/**\n+\t * Utils for catalog and source to filter partition or row.\n+\t * */\n+\tpublic static class FilterUtil {\n+\t\tpublic static final FilterUtil INSTANCE = new FilterUtil();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b1b4efe76749cd0dfa6b4aea94ca2ce9f0d15be1"}, "originalPosition": 457}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNTgwOTA1OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/factories/TestValuesTableFactory.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QwOTo1MDoyM1rOHAEXuQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QwOTo1MDoyM1rOHAEXuQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTgzMzY1Nw==", "bodyText": "nit: use Collections.emptyMap() to make IDE happy ?", "url": "https://github.com/apache/flink/pull/12966#discussion_r469833657", "createdAt": "2020-08-13T09:50:23Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/factories/TestValuesTableFactory.java", "diffHunk": "@@ -629,35 +618,73 @@ public String asSummaryString() {\n \t\t}\n \n \t\tprivate Collection<RowData> convertToRowData(\n-\t\t\t\tCollection<Row> data,\n+\t\t\t\tMap<Map<String, String>, Collection<Row>> data,\n \t\t\t\tint[] projectedFields,\n \t\t\t\tDataStructureConverter converter) {\n \t\t\tList<RowData> result = new ArrayList<>();\n-\t\t\tfor (Row value : data) {\n-\t\t\t\tif (result.size() >= limit) {\n-\t\t\t\t\treturn result;\n-\t\t\t\t}\n-\t\t\t\tif (isRetainedAfterApplyingFilterPredicates(value)) {\n-\t\t\t\t\tRow projectedRow;\n-\t\t\t\t\tif (projectedFields == null) {\n-\t\t\t\t\t\tprojectedRow = value;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\tObject[] newValues = new Object[projectedFields.length];\n-\t\t\t\t\t\tfor (int i = 0; i < projectedFields.length; ++i) {\n-\t\t\t\t\t\t\tnewValues[i] = value.getField(projectedFields[i]);\n+\t\t\tList<Map<String, String>> keys = allPartitions.isEmpty() ?\n+\t\t\t\tCollections.singletonList(Collections.EMPTY_MAP) :", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b1b4efe76749cd0dfa6b4aea94ca2ce9f0d15be1"}, "originalPosition": 375}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNTgzNTYwOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/factories/TestValuesTableFactory.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QwOTo1NzozNVrOHAEn2Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QwOTo1NzozNVrOHAEn2Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTgzNzc4NQ==", "bodyText": "it's better isRetainedAfterApplyingFilterPredicates can accept multiple predicates as parameter, because both parts who use this method are predicate list", "url": "https://github.com/apache/flink/pull/12966#discussion_r469837785", "createdAt": "2020-08-13T09:57:35Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/factories/TestValuesTableFactory.java", "diffHunk": "@@ -629,35 +618,73 @@ public String asSummaryString() {\n \t\t}\n \n \t\tprivate Collection<RowData> convertToRowData(\n-\t\t\t\tCollection<Row> data,\n+\t\t\t\tMap<Map<String, String>, Collection<Row>> data,\n \t\t\t\tint[] projectedFields,\n \t\t\t\tDataStructureConverter converter) {\n \t\t\tList<RowData> result = new ArrayList<>();\n-\t\t\tfor (Row value : data) {\n-\t\t\t\tif (result.size() >= limit) {\n-\t\t\t\t\treturn result;\n-\t\t\t\t}\n-\t\t\t\tif (isRetainedAfterApplyingFilterPredicates(value)) {\n-\t\t\t\t\tRow projectedRow;\n-\t\t\t\t\tif (projectedFields == null) {\n-\t\t\t\t\t\tprojectedRow = value;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\tObject[] newValues = new Object[projectedFields.length];\n-\t\t\t\t\t\tfor (int i = 0; i < projectedFields.length; ++i) {\n-\t\t\t\t\t\t\tnewValues[i] = value.getField(projectedFields[i]);\n+\t\t\tList<Map<String, String>> keys = allPartitions.isEmpty() ?\n+\t\t\t\tCollections.singletonList(Collections.EMPTY_MAP) :\n+\t\t\t\tallPartitions;\n+\t\t\tFilterUtil util = FilterUtil.INSTANCE;\n+\t\t\tboolean isRetained = true;\n+\t\t\tfor (Map<String, String> partition: keys) {\n+\t\t\t\tfor (Row value : data.get(partition)) {\n+\t\t\t\t\tif (result.size() >= limit) {\n+\t\t\t\t\t\treturn result;\n+\t\t\t\t\t}\n+\t\t\t\t\tif (filterPredicates != null && !filterPredicates.isEmpty()) {\n+\t\t\t\t\t\tfor (ResolvedExpression predicate: filterPredicates) {\n+\t\t\t\t\t\t\tisRetained = util.isRetainedAfterApplyingFilterPredicates(predicate, getGetter(value));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b1b4efe76749cd0dfa6b4aea94ca2ce9f0d15be1"}, "originalPosition": 386}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNTgzNzM2OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/factories/TestValuesTableFactory.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QwOTo1ODowNlrOHAEpAA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QwOTo1ODowNlrOHAEpAA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTgzODA4MA==", "bodyText": "nit: move this variable into for loop ?", "url": "https://github.com/apache/flink/pull/12966#discussion_r469838080", "createdAt": "2020-08-13T09:58:06Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/factories/TestValuesTableFactory.java", "diffHunk": "@@ -629,35 +618,73 @@ public String asSummaryString() {\n \t\t}\n \n \t\tprivate Collection<RowData> convertToRowData(\n-\t\t\t\tCollection<Row> data,\n+\t\t\t\tMap<Map<String, String>, Collection<Row>> data,\n \t\t\t\tint[] projectedFields,\n \t\t\t\tDataStructureConverter converter) {\n \t\t\tList<RowData> result = new ArrayList<>();\n-\t\t\tfor (Row value : data) {\n-\t\t\t\tif (result.size() >= limit) {\n-\t\t\t\t\treturn result;\n-\t\t\t\t}\n-\t\t\t\tif (isRetainedAfterApplyingFilterPredicates(value)) {\n-\t\t\t\t\tRow projectedRow;\n-\t\t\t\t\tif (projectedFields == null) {\n-\t\t\t\t\t\tprojectedRow = value;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\tObject[] newValues = new Object[projectedFields.length];\n-\t\t\t\t\t\tfor (int i = 0; i < projectedFields.length; ++i) {\n-\t\t\t\t\t\t\tnewValues[i] = value.getField(projectedFields[i]);\n+\t\t\tList<Map<String, String>> keys = allPartitions.isEmpty() ?\n+\t\t\t\tCollections.singletonList(Collections.EMPTY_MAP) :\n+\t\t\t\tallPartitions;\n+\t\t\tFilterUtil util = FilterUtil.INSTANCE;\n+\t\t\tboolean isRetained = true;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b1b4efe76749cd0dfa6b4aea94ca2ce9f0d15be1"}, "originalPosition": 378}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNTg1MTM3OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/factories/TestValuesCatalog.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxMDowMTo1MlrOHAExfA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxMDowMTo1MlrOHAExfA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTg0MDI1Mg==", "bodyText": "getValueGetter ?", "url": "https://github.com/apache/flink/pull/12966#discussion_r469840252", "createdAt": "2020-08-13T10:01:52Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/factories/TestValuesCatalog.java", "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.factories;\n+\n+import org.apache.flink.table.api.TableException;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.catalog.CatalogBaseTable;\n+import org.apache.flink.table.catalog.CatalogPartitionSpec;\n+import org.apache.flink.table.catalog.GenericInMemoryCatalog;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.catalog.exceptions.CatalogException;\n+import org.apache.flink.table.catalog.exceptions.TableNotExistException;\n+import org.apache.flink.table.catalog.exceptions.TableNotPartitionedException;\n+import org.apache.flink.table.expressions.Expression;\n+import org.apache.flink.table.expressions.ResolvedExpression;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.BooleanType;\n+import org.apache.flink.table.types.logical.CharType;\n+import org.apache.flink.table.types.logical.DoubleType;\n+import org.apache.flink.table.types.logical.IntType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.VarCharType;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.function.Function;\n+\n+\n+/**\n+ * Use TestValuesCatalog to test partition push down.\n+ * */\n+public class TestValuesCatalog extends GenericInMemoryCatalog {\n+\tprivate boolean supportListPartitionByFilter;\n+\tpublic TestValuesCatalog(String name, String defaultDatabase, boolean supportListPartitionByFilter) {\n+\t\tsuper(name, defaultDatabase);\n+\t\tthis.supportListPartitionByFilter = supportListPartitionByFilter;\n+\t}\n+\n+\t@Override\n+\tpublic List<CatalogPartitionSpec> listPartitionsByFilter(ObjectPath tablePath, List<Expression> filters)\n+\t\t\tthrows TableNotExistException, TableNotPartitionedException, CatalogException {\n+\t\tif (!supportListPartitionByFilter) {\n+\t\t\tthrow new UnsupportedOperationException(\"TestValuesCatalog doesn't support list partition by filters\");\n+\t\t}\n+\n+\t\tList<CatalogPartitionSpec> partitions = listPartitions(tablePath);\n+\t\tif (partitions.isEmpty()) {\n+\t\t\treturn partitions;\n+\t\t}\n+\n+\t\tCatalogBaseTable table = this.getTable(tablePath);\n+\t\tTableSchema schema = table.getSchema();\n+\t\tTestValuesTableFactory.FilterUtil util = TestValuesTableFactory.FilterUtil.INSTANCE;\n+\t\tList<CatalogPartitionSpec> remainingPartitions = new ArrayList<>();\n+\t\tfor (CatalogPartitionSpec partition : partitions) {\n+\t\t\tboolean isRetained = true;\n+\t\t\tFunction<String, Comparable<?>> gettter = getGetter(partition.getPartitionSpec(), schema);\n+\t\t\tfor (Expression predicate : filters) {\n+\t\t\t\tisRetained = util.isRetainedAfterApplyingFilterPredicates((ResolvedExpression) predicate, gettter);\n+\t\t\t\tif (!isRetained) {\n+\t\t\t\t\tbreak;\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tif (isRetained) {\n+\t\t\t\tremainingPartitions.add(partition);\n+\t\t\t}\n+\t\t}\n+\t\treturn remainingPartitions;\n+\t}\n+\n+\tprivate Function<String, Comparable<?>> getGetter(Map<String, String> spec, TableSchema schema) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b1b4efe76749cd0dfa6b4aea94ca2ce9f0d15be1"}, "originalPosition": 89}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNTg1OTc2OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/factories/TestValuesTableFactory.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxMDowNDoyNVrOHAE2zg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxMDowNDoyNVrOHAE2zg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTg0MTYxNA==", "bodyText": "what if remainingPartitions  is empty ?", "url": "https://github.com/apache/flink/pull/12966#discussion_r469841614", "createdAt": "2020-08-13T10:04:25Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/factories/TestValuesTableFactory.java", "diffHunk": "@@ -629,35 +618,73 @@ public String asSummaryString() {\n \t\t}\n \n \t\tprivate Collection<RowData> convertToRowData(\n-\t\t\t\tCollection<Row> data,\n+\t\t\t\tMap<Map<String, String>, Collection<Row>> data,\n \t\t\t\tint[] projectedFields,\n \t\t\t\tDataStructureConverter converter) {\n \t\t\tList<RowData> result = new ArrayList<>();\n-\t\t\tfor (Row value : data) {\n-\t\t\t\tif (result.size() >= limit) {\n-\t\t\t\t\treturn result;\n-\t\t\t\t}\n-\t\t\t\tif (isRetainedAfterApplyingFilterPredicates(value)) {\n-\t\t\t\t\tRow projectedRow;\n-\t\t\t\t\tif (projectedFields == null) {\n-\t\t\t\t\t\tprojectedRow = value;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\tObject[] newValues = new Object[projectedFields.length];\n-\t\t\t\t\t\tfor (int i = 0; i < projectedFields.length; ++i) {\n-\t\t\t\t\t\t\tnewValues[i] = value.getField(projectedFields[i]);\n+\t\t\tList<Map<String, String>> keys = allPartitions.isEmpty() ?\n+\t\t\t\tCollections.singletonList(Collections.EMPTY_MAP) :\n+\t\t\t\tallPartitions;\n+\t\t\tFilterUtil util = FilterUtil.INSTANCE;\n+\t\t\tboolean isRetained = true;\n+\t\t\tfor (Map<String, String> partition: keys) {\n+\t\t\t\tfor (Row value : data.get(partition)) {\n+\t\t\t\t\tif (result.size() >= limit) {\n+\t\t\t\t\t\treturn result;\n+\t\t\t\t\t}\n+\t\t\t\t\tif (filterPredicates != null && !filterPredicates.isEmpty()) {\n+\t\t\t\t\t\tfor (ResolvedExpression predicate: filterPredicates) {\n+\t\t\t\t\t\t\tisRetained = util.isRetainedAfterApplyingFilterPredicates(predicate, getGetter(value));\n+\t\t\t\t\t\t\tif (!isRetained) {\n+\t\t\t\t\t\t\t\tbreak;\n+\t\t\t\t\t\t\t}\n \t\t\t\t\t\t}\n-\t\t\t\t\t\tprojectedRow = Row.of(newValues);\n \t\t\t\t\t}\n-\t\t\t\t\tRowData rowData = (RowData) converter.toInternal(projectedRow);\n-\t\t\t\t\tif (rowData != null) {\n-\t\t\t\t\t\trowData.setRowKind(value.getKind());\n-\t\t\t\t\t\tresult.add(rowData);\n+\t\t\t\t\tif (isRetained) {\n+\t\t\t\t\t\tRow projectedRow;\n+\t\t\t\t\t\tif (projectedFields == null) {\n+\t\t\t\t\t\t\tprojectedRow = value;\n+\t\t\t\t\t\t} else {\n+\t\t\t\t\t\t\tObject[] newValues = new Object[projectedFields.length];\n+\t\t\t\t\t\t\tfor (int i = 0; i < projectedFields.length; ++i) {\n+\t\t\t\t\t\t\t\tnewValues[i] = value.getField(projectedFields[i]);\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t\tprojectedRow = Row.of(newValues);\n+\t\t\t\t\t\t}\n+\t\t\t\t\t\tRowData rowData = (RowData) converter.toInternal(projectedRow);\n+\t\t\t\t\t\tif (rowData != null) {\n+\t\t\t\t\t\t\trowData.setRowKind(value.getKind());\n+\t\t\t\t\t\t\tresult.add(rowData);\n+\t\t\t\t\t\t}\n \t\t\t\t\t}\n \t\t\t\t}\n \t\t\t}\n \t\t\treturn result;\n \t\t}\n \n+\t\t@Override\n+\t\tpublic Optional<List<Map<String, String>>> listPartitions() {\n+\t\t\tif (allPartitions.isEmpty()) {\n+\t\t\t\tthrow new UnsupportedOperationException(\"Please use catalog to read partitions\");\n+\t\t\t}\n+\t\t\treturn Optional.of(allPartitions);\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void applyPartitions(List<Map<String, String>> remainingPartitions) {\n+\t\t\t// remainingPartition is non-nullable.\n+\t\t\tif (allPartitions.isEmpty()) {\n+\t\t\t\t// read partitions from catalog\n+\t\t\t\tif (!remainingPartitions.isEmpty()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b1b4efe76749cd0dfa6b4aea94ca2ce9f0d15be1"}, "originalPosition": 432}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNTg2MzA0OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/factories/TestValuesTableFactory.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxMDowNToxNVrOHAE4vw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxMDowNToxNVrOHAE4vw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTg0MjExMQ==", "bodyText": "these lines can be simpler", "url": "https://github.com/apache/flink/pull/12966#discussion_r469842111", "createdAt": "2020-08-13T10:05:15Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/factories/TestValuesTableFactory.java", "diffHunk": "@@ -821,4 +848,125 @@ public String asSummaryString() {\n \t\t}\n \t}\n \n+\t// --------------------------------------------------------------------------------------------\n+\t// Table utils\n+\t// --------------------------------------------------------------------------------------------\n+\n+\t/**\n+\t * Utils for catalog and source to filter partition or row.\n+\t * */\n+\tpublic static class FilterUtil {\n+\t\tpublic static final FilterUtil INSTANCE = new FilterUtil();\n+\n+\t\tprivate FilterUtil() {}\n+\n+\t\tpublic boolean shouldPushDown(ResolvedExpression expr, Set<String> filterableFields) {\n+\t\t\tif (expr instanceof CallExpression && expr.getChildren().size() == 2) {\n+\t\t\t\treturn shouldPushDownUnaryExpression(expr.getResolvedChildren().get(0), filterableFields)\n+\t\t\t\t\t&& shouldPushDownUnaryExpression(expr.getResolvedChildren().get(1), filterableFields);\n+\t\t\t}\n+\t\t\treturn false;\n+\t\t}\n+\n+\t\tpublic boolean isRetainedAfterApplyingFilterPredicates(ResolvedExpression predicate, Function<String, Comparable<?>> getter) {\n+\t\t\tif (predicate instanceof CallExpression) {\n+\t\t\t\tFunctionDefinition definition = ((CallExpression) predicate).getFunctionDefinition();\n+\t\t\t\tif (definition.equals(BuiltInFunctionDefinitions.OR)) {\n+\t\t\t\t\t// nested filter, such as (key1 > 2 or key2 > 3)\n+\t\t\t\t\tboolean result = false;\n+\t\t\t\t\tfor (Expression expr: predicate.getChildren()) {\n+\t\t\t\t\t\tif (!(expr instanceof CallExpression && expr.getChildren().size() == 2)) {\n+\t\t\t\t\t\t\tthrow new TableException(expr + \" not supported!\");\n+\t\t\t\t\t\t}\n+\t\t\t\t\t\tresult |= binaryFilterApplies((CallExpression) expr, getter);\n+\t\t\t\t\t\tif (result) {\n+\t\t\t\t\t\t\treturn result;\n+\t\t\t\t\t\t}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b1b4efe76749cd0dfa6b4aea94ca2ce9f0d15be1"}, "originalPosition": 482}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNTg3MDEyOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/factories/TestValuesCatalog.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxMDowNzoxNFrOHAE8_g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxMDowNzoxNFrOHAE8_g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTg0MzE5OA==", "bodyText": "nit: make supportListPartitionByFilter final?", "url": "https://github.com/apache/flink/pull/12966#discussion_r469843198", "createdAt": "2020-08-13T10:07:14Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/factories/TestValuesCatalog.java", "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.factories;\n+\n+import org.apache.flink.table.api.TableException;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.catalog.CatalogBaseTable;\n+import org.apache.flink.table.catalog.CatalogPartitionSpec;\n+import org.apache.flink.table.catalog.GenericInMemoryCatalog;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.catalog.exceptions.CatalogException;\n+import org.apache.flink.table.catalog.exceptions.TableNotExistException;\n+import org.apache.flink.table.catalog.exceptions.TableNotPartitionedException;\n+import org.apache.flink.table.expressions.Expression;\n+import org.apache.flink.table.expressions.ResolvedExpression;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.BooleanType;\n+import org.apache.flink.table.types.logical.CharType;\n+import org.apache.flink.table.types.logical.DoubleType;\n+import org.apache.flink.table.types.logical.IntType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.VarCharType;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.function.Function;\n+\n+\n+/**\n+ * Use TestValuesCatalog to test partition push down.\n+ * */\n+public class TestValuesCatalog extends GenericInMemoryCatalog {\n+\tprivate boolean supportListPartitionByFilter;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b1b4efe76749cd0dfa6b4aea94ca2ce9f0d15be1"}, "originalPosition": 51}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNTg3Mjk1OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/factories/TestValuesCatalog.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxMDowODowMFrOHAE-nA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxMDowODowMFrOHAE-nA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTg0MzYxMg==", "bodyText": "typo", "url": "https://github.com/apache/flink/pull/12966#discussion_r469843612", "createdAt": "2020-08-13T10:08:00Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/factories/TestValuesCatalog.java", "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.factories;\n+\n+import org.apache.flink.table.api.TableException;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.catalog.CatalogBaseTable;\n+import org.apache.flink.table.catalog.CatalogPartitionSpec;\n+import org.apache.flink.table.catalog.GenericInMemoryCatalog;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.catalog.exceptions.CatalogException;\n+import org.apache.flink.table.catalog.exceptions.TableNotExistException;\n+import org.apache.flink.table.catalog.exceptions.TableNotPartitionedException;\n+import org.apache.flink.table.expressions.Expression;\n+import org.apache.flink.table.expressions.ResolvedExpression;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.BooleanType;\n+import org.apache.flink.table.types.logical.CharType;\n+import org.apache.flink.table.types.logical.DoubleType;\n+import org.apache.flink.table.types.logical.IntType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.VarCharType;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.function.Function;\n+\n+\n+/**\n+ * Use TestValuesCatalog to test partition push down.\n+ * */\n+public class TestValuesCatalog extends GenericInMemoryCatalog {\n+\tprivate boolean supportListPartitionByFilter;\n+\tpublic TestValuesCatalog(String name, String defaultDatabase, boolean supportListPartitionByFilter) {\n+\t\tsuper(name, defaultDatabase);\n+\t\tthis.supportListPartitionByFilter = supportListPartitionByFilter;\n+\t}\n+\n+\t@Override\n+\tpublic List<CatalogPartitionSpec> listPartitionsByFilter(ObjectPath tablePath, List<Expression> filters)\n+\t\t\tthrows TableNotExistException, TableNotPartitionedException, CatalogException {\n+\t\tif (!supportListPartitionByFilter) {\n+\t\t\tthrow new UnsupportedOperationException(\"TestValuesCatalog doesn't support list partition by filters\");\n+\t\t}\n+\n+\t\tList<CatalogPartitionSpec> partitions = listPartitions(tablePath);\n+\t\tif (partitions.isEmpty()) {\n+\t\t\treturn partitions;\n+\t\t}\n+\n+\t\tCatalogBaseTable table = this.getTable(tablePath);\n+\t\tTableSchema schema = table.getSchema();\n+\t\tTestValuesTableFactory.FilterUtil util = TestValuesTableFactory.FilterUtil.INSTANCE;\n+\t\tList<CatalogPartitionSpec> remainingPartitions = new ArrayList<>();\n+\t\tfor (CatalogPartitionSpec partition : partitions) {\n+\t\t\tboolean isRetained = true;\n+\t\t\tFunction<String, Comparable<?>> gettter = getGetter(partition.getPartitionSpec(), schema);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b1b4efe76749cd0dfa6b4aea94ca2ce9f0d15be1"}, "originalPosition": 75}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNTg5MjQ0OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRuleTest.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxMDoxMzo0MVrOHAFKKw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxMDoxMzo0MVrOHAFKKw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTg0NjU3MQ==", "bodyText": "just register a new catalog, and change it as default ?", "url": "https://github.com/apache/flink/pull/12966#discussion_r469846571", "createdAt": "2020-08-13T10:13:41Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoTableSourceScanRuleTest.java", "diffHunk": "@@ -0,0 +1,142 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.catalog.CatalogPartition;\n+import org.apache.flink.table.catalog.CatalogPartitionImpl;\n+import org.apache.flink.table.catalog.CatalogPartitionSpec;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.planner.calcite.CalciteConfig;\n+import org.apache.flink.table.planner.factories.TestValuesCatalog;\n+import org.apache.flink.table.planner.plan.optimize.program.BatchOptimizeContext;\n+import org.apache.flink.table.planner.plan.optimize.program.FlinkBatchProgram;\n+import org.apache.flink.table.planner.plan.optimize.program.FlinkHepRuleSetProgramBuilder;\n+import org.apache.flink.table.planner.plan.optimize.program.HEP_RULES_EXECUTION_TYPE;\n+import org.apache.flink.table.planner.utils.TableConfigUtils;\n+\n+import org.apache.calcite.plan.hep.HepMatchOrder;\n+import org.apache.calcite.rel.rules.FilterProjectTransposeRule;\n+import org.apache.calcite.tools.RuleSets;\n+import org.junit.Test;\n+\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * Test for {@link PushPartitionIntoTableSourceScanRule}.\n+ */\n+public class PushPartitionIntoTableSourceScanRuleTest extends PushPartitionIntoLegacyTableSourceScanRuleTest{\n+\tpublic PushPartitionIntoTableSourceScanRuleTest(boolean sourceFetchPartitions, boolean useFilter) {\n+\t\tsuper(sourceFetchPartitions, useFilter);\n+\t}\n+\n+\t@Override\n+\tpublic void setup() throws Exception {\n+\t\tutil().buildBatchProgram(FlinkBatchProgram.DEFAULT_REWRITE());\n+\t\tCalciteConfig calciteConfig = TableConfigUtils.getCalciteConfig(util().tableEnv().getConfig());\n+\t\tcalciteConfig.getBatchProgram().get().addLast(\n+\t\t\t\"rules\",\n+\t\t\tFlinkHepRuleSetProgramBuilder.<BatchOptimizeContext>newBuilder()\n+\t\t\t\t.setHepRulesExecutionType(HEP_RULES_EXECUTION_TYPE.RULE_SEQUENCE())\n+\t\t\t\t.setHepMatchOrder(HepMatchOrder.BOTTOM_UP)\n+\t\t\t\t.add(RuleSets.ofList(FilterProjectTransposeRule.INSTANCE,\n+\t\t\t\t\tPushPartitionIntoTableSourceScanRule.INSTANCE))\n+\t\t\t\t.build());\n+\n+\t\t// define ddl\n+\t\tString ddlTemp =\n+\t\t\t\"CREATE TABLE MyTable (\\n\" +\n+\t\t\t\t\"  id int,\\n\" +\n+\t\t\t\t\"  name string,\\n\" +\n+\t\t\t\t\"  part1 string,\\n\" +\n+\t\t\t\t\"  part2 int)\\n\" +\n+\t\t\t\t\"  partitioned by (part1, part2)\\n\" +\n+\t\t\t\t\"  WITH (\\n\" +\n+\t\t\t\t\" 'connector' = 'values',\\n\" +\n+\t\t\t\t\" 'bounded' = 'true',\\n\" +\n+\t\t\t\t\" 'partition-list' = '%s'\" +\n+\t\t\t\t\")\";\n+\n+\t\tString ddlTempWithVirtualColumn =\n+\t\t\t\"CREATE TABLE VirtualTable (\\n\" +\n+\t\t\t\t\"  id int,\\n\" +\n+\t\t\t\t\"  name string,\\n\" +\n+\t\t\t\t\"  part1 string,\\n\" +\n+\t\t\t\t\"  part2 int,\\n\" +\n+\t\t\t\t\"  virtualField AS part2 + 1)\\n\" +\n+\t\t\t\t\"  partitioned by (part1, part2)\\n\" +\n+\t\t\t\t\"  WITH (\\n\" +\n+\t\t\t\t\" 'connector' = 'values',\\n\" +\n+\t\t\t\t\" 'bounded' = 'true',\\n\" +\n+\t\t\t\t\" 'partition-list' = '%s'\" +\n+\t\t\t\t\")\";\n+\n+\t\tif (sourceFetchPartitions()) {\n+\t\t\tString partitionString = \"part1:A,part2:1;part1:A,part2:2;part1:B,part2:3;part1:C,part2:1\";\n+\t\t\tutil().tableEnv().executeSql(String.format(ddlTemp, partitionString));\n+\t\t\tutil().tableEnv().executeSql(String.format(ddlTempWithVirtualColumn, partitionString));\n+\t\t} else {\n+\t\t\t// replace catalog with TestValuesCatalog\n+\t\t\tutil().tableEnv().executeSql(\"drop catalog default_catalog\");\n+\t\t\tTestValuesCatalog catalog =\n+\t\t\t\tnew TestValuesCatalog(\"default_catalog\", \"default_database\", useCatalogFilter());\n+\t\t\tutil().tableEnv().registerCatalog(\"default_catalog\", catalog);\n+\t\t\tutil().tableEnv().useCatalog(\"default_catalog\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b1b4efe76749cd0dfa6b4aea94ca2ce9f0d15be1"}, "originalPosition": 102}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzOTc1Mzk4OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/batch/sql/PartitionableSourceITCase.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQwNjo1MDoxOFrOHApf4w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQwNjo1MDoxOFrOHApf4w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDQ0MTk1NQ==", "bodyText": "just create a new catalog with different catalog name and set it as default", "url": "https://github.com/apache/flink/pull/12966#discussion_r470441955", "createdAt": "2020-08-14T06:50:18Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/batch/sql/PartitionableSourceITCase.scala", "diffHunk": "@@ -0,0 +1,135 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.runtime.batch.sql\n+\n+import java.util\n+\n+import org.apache.flink.table.catalog.{CatalogPartitionImpl, CatalogPartitionSpec, ObjectPath}\n+import org.apache.flink.table.planner.factories.{TestValuesCatalog, TestValuesTableFactory}\n+import org.apache.flink.table.planner.runtime.utils.BatchTestBase\n+import org.apache.flink.table.planner.runtime.utils.BatchTestBase.row\n+import org.junit.{Before, Test}\n+import org.junit.runner.RunWith\n+import org.junit.runners.Parameterized\n+\n+import scala.collection.JavaConversions._\n+\n+@RunWith(classOf[Parameterized])\n+class PartitionableSourceITCase(\n+  val sourceFetchPartitions: Boolean,\n+  val useCatalogFilter: Boolean) extends BatchTestBase{\n+\n+  @Before\n+  override def before() : Unit = {\n+    super.before()\n+    env.setParallelism(1) // set sink parallelism to 1\n+    val data = Seq(\n+      row(1, \"ZhangSan\", \"A\", 1),\n+      row(2, \"LiSi\", \"A\", 1),\n+      row(3, \"Jack\", \"A\", 2),\n+      row(4, \"Tom\", \"B\", 3),\n+      row(5, \"Vivi\", \"C\", 1)\n+    )\n+    val myTableDataId = TestValuesTableFactory.registerData(data)\n+\n+    val ddlTemp =\n+      s\"\"\"\n+        |CREATE TABLE MyTable (\n+        |  id int,\n+        |  name string,\n+        |  part1 string,\n+        |  part2 int,\n+        |  virtualField as part2 + 1)\n+        |  partitioned by (part1, part2)\n+        |  with (\n+        |    'connector' = 'values',\n+        |    'data-id' = '$myTableDataId',\n+        |    'bounded' = 'true',\n+        |    'partition-list' = '%s'\n+        |)\n+        |\"\"\".stripMargin\n+\n+    if (sourceFetchPartitions) {\n+      val partitions = \"part1:A,part2:1;part1:A,part2:2;part1:B,part2:3;part1:C,part2:1\"\n+      tEnv.executeSql(String.format(ddlTemp, partitions))\n+    } else {\n+      tEnv.executeSql(\"drop catalog default_catalog\")\n+      val catalog =\n+        new TestValuesCatalog(\"default_catalog\", \"default_database\", useCatalogFilter);\n+      tEnv.registerCatalog(\"default_catalog\", catalog)\n+      tEnv.useCatalog(\"default_catalog\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "984744723761b8124aa003f23e65d4bb484a73c7"}, "originalPosition": 76}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk1MDA1MzkwOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoLegacyTableSourceScanRuleTest.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQwNzowMjoxN1rOHCF_3w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQwNzowMjoxN1rOHCF_3w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTk1NzQ3MQ==", "bodyText": "nit: reorder the imports", "url": "https://github.com/apache/flink/pull/12966#discussion_r471957471", "createdAt": "2020-08-18T07:02:17Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/rules/logical/PushPartitionIntoLegacyTableSourceScanRuleTest.scala", "diffHunk": "@@ -17,26 +17,30 @@\n  */\n package org.apache.flink.table.planner.plan.rules.logical\n \n+import java.util", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e91a84cc20e3655749b8cf9b69ed79d855aaedaf"}, "originalPosition": 4}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4985, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}