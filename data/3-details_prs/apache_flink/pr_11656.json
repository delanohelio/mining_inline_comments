{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDAwMTkyNTY2", "number": 11656, "title": "[FLINK-16983][python] Support RowType in vectorized Python UDF", "bodyText": "What is the purpose of the change\nThis pull request add support of RowType in vectorized Python UDF.\nBrief change log\n\nIntroduce row column vector in blink\nIntroduce ArrowRowColumnVector, RowWriter, etc to support RowType in vectorized Python UDF\n\nVerifying this change\nThis change added tests and can be verified as follows:\n\nJava test cases ArrowUtilsTest, BaseRowArrowReaderWriterTest and RowArrowReaderWriterTest\nPython test case test_pandas_udf.py\n\nDoes this pull request potentially affect one of the following parts:\n\nDependencies (does it add or upgrade a dependency): (no)\nThe public API, i.e., is any changed class annotated with @Public(Evolving): (no)\nThe serializers: (no)\nThe runtime per-record code paths (performance sensitive): (no)\nAnything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (no)\nThe S3 file system connector: (no)\n\nDocumentation\n\nDoes this pull request introduce a new feature? (no)\nIf yes, how is the feature documented? (not applicable)", "createdAt": "2020-04-07T10:54:51Z", "url": "https://github.com/apache/flink/pull/11656", "merged": true, "mergeCommit": {"oid": "177976b8941a4996d82efbcc1b60b8898aae0775"}, "closed": true, "closedAt": "2020-04-16T03:21:33Z", "author": {"login": "dianfu"}, "timelineItems": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcV4sReAFqTM5MDU4NTUwNA==", "endCursor": "Y3Vyc29yOnYyOpPPAAABcYD8nHgFqTM5NDI3MTI2MA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzkwNTg1NTA0", "url": "https://github.com/apache/flink/pull/11656#pullrequestreview-390585504", "createdAt": "2020-04-09T08:21:39Z", "commit": {"oid": "4597eea52db61944a6fd122f55392eb4a165ea32"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQwODoyMTozOVrOGDOhgw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQwODo1NTowN1rOGDPtzQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjAzNjg2Nw==", "bodyText": "It is strange to return dict type while the input type is Row. And there is no order guarantee for the field in dict which maybe not consistent with the semantic of Row.\nIs it possible to convert the pd.Series into a Series contains Row?", "url": "https://github.com/apache/flink/pull/11656#discussion_r406036867", "createdAt": "2020-04-09T08:21:39Z", "author": {"login": "hequn8128"}, "path": "flink-python/pyflink/table/tests/test_pandas_udf.py", "diffHunk": "@@ -165,6 +165,12 @@ def nested_array_func(nested_array_param):\n                 'nested_array_param of wrong type %s !' % type(nested_array_param[0])\n             return pd.Series(nested_array_param[0])\n \n+        def row_func(row_param):\n+            assert isinstance(row_param, pd.Series)\n+            assert isinstance(row_param[0], dict), \\", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4597eea52db61944a6fd122f55392eb4a165ea32"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjA1MTU3Ng==", "bodyText": "new Row(n)?", "url": "https://github.com/apache/flink/pull/11656#discussion_r406051576", "createdAt": "2020-04-09T08:46:46Z", "author": {"login": "hequn8128"}, "path": "flink-python/src/test/java/org/apache/flink/table/runtime/arrow/RowArrowReaderWriterTest.java", "diffHunk": "@@ -118,18 +127,20 @@ public static void init() {\n \t\t\tSqlDateTimeUtils.internalToTime(3600000), SqlDateTimeUtils.internalToTime(3600000), SqlDateTimeUtils.internalToTime(3600000), SqlDateTimeUtils.internalToTime(3600000),\n \t\t\tnew Timestamp(3600000), new Timestamp(3600000), new Timestamp(3600000), new Timestamp(3600000),\n \t\t\tnew Timestamp(3600000), new Timestamp(3600000), new Timestamp(3600000), new Timestamp(3600000),\n-\t\t\tnew String[] {null, null, null});\n+\t\t\tnew String[] {null, null, null},\n+\t\t\tRow.of(1, \"hello\", new String[] {null, null, null}, new Timestamp(3600000), Row.of(1, \"hello\")));\n \t\tRow row2 = Row.of(null, (short) 2, 3, 4L, false, 1.0f, 1.0, \"\u4e2d\u6587\", \"\u4e2d\u6587\".getBytes(), new BigDecimal(1), SqlDateTimeUtils.internalToDate(100),\n \t\t\tSqlDateTimeUtils.internalToTime(3600000), SqlDateTimeUtils.internalToTime(3600000), SqlDateTimeUtils.internalToTime(3600000), SqlDateTimeUtils.internalToTime(3600000),\n \t\t\tnew Timestamp(3600000), new Timestamp(3600000), new Timestamp(3600000), new Timestamp(3600000),\n \t\t\tnew Timestamp(3600000), new Timestamp(3600000), new Timestamp(3600000), new Timestamp(3600000),\n-\t\t\tnew String[] {\"hello\", \"\u4e2d\u6587\", null});\n+\t\t\tnew String[] {\"hello\", \"\u4e2d\u6587\", null},\n+\t\t\tRow.of(1, \"hello\", new String[] {\"hello\", \"\u4e2d\u6587\", null}, new Timestamp(3600000), Row.of(1, \"hello\")));\n \t\tRow row3 = Row.of((byte) 1, null, 3, 4L, true, 1.0f, 1.0, \"hello\", \"hello\".getBytes(), new BigDecimal(1), SqlDateTimeUtils.internalToDate(100),\n \t\t\tSqlDateTimeUtils.internalToTime(3600000), SqlDateTimeUtils.internalToTime(3600000), SqlDateTimeUtils.internalToTime(3600000), SqlDateTimeUtils.internalToTime(3600000),\n \t\t\tnew Timestamp(3600000), new Timestamp(3600000), new Timestamp(3600000), new Timestamp(3600000),\n \t\t\tnew Timestamp(3600000), new Timestamp(3600000), new Timestamp(3600000), new Timestamp(3600000),\n-\t\t\tnull);\n-\t\tRow row4 = Row.of(null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null);\n+\t\t\tnull, null);\n+\t\tRow row4 = Row.of(null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4597eea52db61944a6fd122f55392eb4a165ea32"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjA1NjM5Nw==", "bodyText": "StreamRecordUtils.baserow(new Object[n]) ?", "url": "https://github.com/apache/flink/pull/11656#discussion_r406056397", "createdAt": "2020-04-09T08:55:07Z", "author": {"login": "hequn8128"}, "path": "flink-python/src/test/java/org/apache/flink/table/runtime/arrow/BaseRowArrowReaderWriterTest.java", "diffHunk": "@@ -152,21 +164,25 @@ public static void init() {\n \t\tBaseRow row1 = StreamRecordUtils.baserow((byte) 1, (short) 2, 3, 4L, true, 1.0f, 1.0, \"hello\", \"hello\".getBytes(), Decimal.fromLong(1, 10, 3), 100, 3600000, 3600000, 3600000, 3600000,\n \t\t\tSqlTimestamp.fromEpochMillis(3600000), SqlTimestamp.fromEpochMillis(3600000), SqlTimestamp.fromEpochMillis(3600000, 100000), SqlTimestamp.fromEpochMillis(3600000, 100000),\n \t\t\tSqlTimestamp.fromEpochMillis(3600000), SqlTimestamp.fromEpochMillis(3600000), SqlTimestamp.fromEpochMillis(3600000, 100000), SqlTimestamp.fromEpochMillis(3600000, 100000),\n-\t\t\tnew GenericArray(new BinaryString[] {BinaryString.fromString(\"hello\"), BinaryString.fromString(\"\u4e2d\u6587\"), null}, 3));\n+\t\t\tnew GenericArray(new BinaryString[] {BinaryString.fromString(\"hello\"), BinaryString.fromString(\"\u4e2d\u6587\"), null}, 3),\n+\t\t\tGenericRow.of(1, BinaryString.fromString(\"hello\"), new GenericArray(new BinaryString[] {BinaryString.fromString(\"hello\")}, 1), SqlTimestamp.fromEpochMillis(3600000), GenericRow.of(1, BinaryString.fromString(\"hello\"))));\n \t\tBinaryRow row2 = StreamRecordUtils.binaryrow((byte) 1, (short) 2, 3, 4L, false, 1.0f, 1.0, \"\u4e2d\u6587\", \"\u4e2d\u6587\".getBytes(), Decimal.fromLong(1, 10, 3), 100, 3600000, 3600000, 3600000, 3600000,\n \t\t\tTuple2.of(SqlTimestamp.fromEpochMillis(3600000), 0), Tuple2.of(SqlTimestamp.fromEpochMillis(3600000), 2), Tuple2.of(SqlTimestamp.fromEpochMillis(3600000, 100000), 4), Tuple2.of(SqlTimestamp.fromEpochMillis(3600000, 100000), 8),\n \t\t\tTuple2.of(SqlTimestamp.fromEpochMillis(3600000), 0), Tuple2.of(SqlTimestamp.fromEpochMillis(3600000), 2), Tuple2.of(SqlTimestamp.fromEpochMillis(3600000, 100000), 4), Tuple2.of(SqlTimestamp.fromEpochMillis(3600000, 100000), 8),\n-\t\t\tTuple2.of(new GenericArray(new String[] {null, null, null}, 3), new BaseArraySerializer(new VarCharType(), null)));\n+\t\t\tTuple2.of(new GenericArray(new String[] {null, null, null}, 3), new BaseArraySerializer(new VarCharType(), null)),\n+\t\t\tTuple2.of(GenericRow.of(1, null, new GenericArray(new BinaryString[] {BinaryString.fromString(\"hello\")}, 1), null, GenericRow.of(1, BinaryString.fromString(\"hello\"))), new BaseRowSerializer(new ExecutionConfig(), rowFieldType)));\n \t\tBaseRow row3 = StreamRecordUtils.baserow(null, (short) 2, 3, 4L, false, 1.0f, 1.0, \"\u4e2d\u6587\", \"\u4e2d\u6587\".getBytes(), Decimal.fromLong(1, 10, 3), 100, 3600000, 3600000, 3600000, 3600000,\n \t\t\tSqlTimestamp.fromEpochMillis(3600000), SqlTimestamp.fromEpochMillis(3600000), SqlTimestamp.fromEpochMillis(3600000, 100000), SqlTimestamp.fromEpochMillis(3600000, 100000),\n \t\t\tSqlTimestamp.fromEpochMillis(3600000), SqlTimestamp.fromEpochMillis(3600000), SqlTimestamp.fromEpochMillis(3600000, 100000), SqlTimestamp.fromEpochMillis(3600000, 100000),\n-\t\t\tnew GenericArray(new String[] {null, null, null}, 3));\n+\t\t\tnew GenericArray(new String[] {null, null, null}, 3),\n+\t\t\tGenericRow.of(1, null, new GenericArray(new BinaryString[] {BinaryString.fromString(\"hello\")}, 1), null, null));\n \t\tBinaryRow row4 = StreamRecordUtils.binaryrow((byte) 1, null, 3, 4L, true, 1.0f, 1.0, \"hello\", \"hello\".getBytes(), Decimal.fromLong(1, 10, 3), 100, 3600000, 3600000, 3600000, 3600000,\n \t\t\tTuple2.of(SqlTimestamp.fromEpochMillis(3600000), 0), Tuple2.of(SqlTimestamp.fromEpochMillis(3600000), 2), Tuple2.of(SqlTimestamp.fromEpochMillis(3600000, 100000), 4), Tuple2.of(SqlTimestamp.fromEpochMillis(3600000, 100000), 8),\n \t\t\tTuple2.of(SqlTimestamp.fromEpochMillis(3600000), 0), Tuple2.of(SqlTimestamp.fromEpochMillis(3600000), 2), Tuple2.of(SqlTimestamp.fromEpochMillis(3600000, 100000), 4), Tuple2.of(SqlTimestamp.fromEpochMillis(3600000, 100000), 8),\n-\t\t\tTuple2.of(new GenericArray(new BinaryString[] {BinaryString.fromString(\"hello\"), BinaryString.fromString(\"\u4e2d\u6587\"), null}, 3), new BaseArraySerializer(new VarCharType(), null)));\n-\t\tBaseRow row5 = StreamRecordUtils.baserow(null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null);\n-\t\tBinaryRow row6 = StreamRecordUtils.binaryrow(null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null);\n+\t\t\tTuple2.of(new GenericArray(new BinaryString[] {BinaryString.fromString(\"hello\"), BinaryString.fromString(\"\u4e2d\u6587\"), null}, 3), new BaseArraySerializer(new VarCharType(), null)),\n+\t\t\tTuple2.of(GenericRow.of(1, null, new GenericArray(new BinaryString[] {BinaryString.fromString(\"hello\")}, 1), null, null), new BaseRowSerializer(new ExecutionConfig(), rowFieldType)));\n+\t\tBaseRow row5 = StreamRecordUtils.baserow(null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4597eea52db61944a6fd122f55392eb4a165ea32"}, "originalPosition": 67}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "eeb34a5dcd83ce70e0863b003f4eced6a2e2a353", "author": {"user": {"login": "dianfu", "name": "Dian Fu"}}, "url": "https://github.com/apache/flink/commit/eeb34a5dcd83ce70e0863b003f4eced6a2e2a353", "committedDate": "2020-04-10T06:48:49Z", "message": "[FLINK-16983][table-runtime-blink] Introduce row column vector in blink"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ed4b87dfa57d1b0dca1623ddd40e29de6037839b", "author": {"user": {"login": "dianfu", "name": "Dian Fu"}}, "url": "https://github.com/apache/flink/commit/ed4b87dfa57d1b0dca1623ddd40e29de6037839b", "committedDate": "2020-04-10T06:49:05Z", "message": "[FLINK-16983][python] Support RowType in vectorized Python UDF"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "4597eea52db61944a6fd122f55392eb4a165ea32", "author": {"user": {"login": "dianfu", "name": "Dian Fu"}}, "url": "https://github.com/apache/flink/commit/4597eea52db61944a6fd122f55392eb4a165ea32", "committedDate": "2020-04-07T10:07:07Z", "message": "[FLINK-16983][python] Support RowType in vectorized Python UDF"}, "afterCommit": {"oid": "ed4b87dfa57d1b0dca1623ddd40e29de6037839b", "author": {"user": {"login": "dianfu", "name": "Dian Fu"}}, "url": "https://github.com/apache/flink/commit/ed4b87dfa57d1b0dca1623ddd40e29de6037839b", "committedDate": "2020-04-10T06:49:05Z", "message": "[FLINK-16983][python] Support RowType in vectorized Python UDF"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk0MjcxMjYw", "url": "https://github.com/apache/flink/pull/11656#pullrequestreview-394271260", "createdAt": "2020-04-16T03:16:11Z", "commit": {"oid": "ed4b87dfa57d1b0dca1623ddd40e29de6037839b"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2364, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}