{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDEzODc2Nzk2", "number": 12004, "reviewThreads": {"totalCount": 22, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNlQxMjoxMzozMlrOD5yYfw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQxMTo1MzozNVrOD620Ig==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYxOTIwODk1OnYy", "diffSide": "RIGHT", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveOptions.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNlQxMjoxMzozM1rOGRQEVw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNlQxMjoxMzozM1rOGRQEVw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDc0MjIzMQ==", "bodyText": "I think these properties are user-facing right? Need to add descriptions for them.", "url": "https://github.com/apache/flink/pull/12004#discussion_r420742231", "createdAt": "2020-05-06T12:13:33Z", "author": {"login": "lirui-apache"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveOptions.java", "diffHunk": "@@ -45,4 +47,34 @@\n \t\t\tkey(\"table.exec.hive.infer-source-parallelism.max\")\n \t\t\t\t\t.defaultValue(1000)\n \t\t\t\t\t.withDescription(\"Sets max infer parallelism for source operator.\");\n+\n+\tpublic static final ConfigOption<Boolean> HIVE_STREAMING_SOURCE_ENABLE =\n+\t\t\tkey(\"hive.streaming-source.enable\")\n+\t\t\t\t\t.booleanType()\n+\t\t\t\t\t.defaultValue(false)\n+\t\t\t\t\t.withDescription(\"\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4223a26640c7243a61135e803aa1dd21888114a5"}, "originalPosition": 18}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYxOTQ1MTg0OnYy", "diffSide": "RIGHT", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveContinuousMonitoringFunction.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNlQxMzoxNToyOFrOGRSZhw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNlQxMzoxNToyOFrOGRSZhw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDc4MDQyMw==", "bodyText": "Need to close the HMS client somewhere.", "url": "https://github.com/apache/flink/pull/12004#discussion_r420780423", "createdAt": "2020-05-06T13:15:28Z", "author": {"login": "lirui-apache"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveContinuousMonitoringFunction.java", "diffHunk": "@@ -0,0 +1,280 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive.read;\n+\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeutils.base.ListSerializer;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.api.common.typeutils.base.StringSerializer;\n+import org.apache.flink.connectors.hive.HiveTablePartition;\n+import org.apache.flink.connectors.hive.HiveTableSource;\n+import org.apache.flink.connectors.hive.JobConfWrapper;\n+import org.apache.flink.connectors.hive.read.PartitionStrategy.PartitionStrategyFactory;\n+import org.apache.flink.runtime.state.FunctionInitializationContext;\n+import org.apache.flink.runtime.state.FunctionSnapshotContext;\n+import org.apache.flink.streaming.api.checkpoint.CheckpointedFunction;\n+import org.apache.flink.streaming.api.functions.source.ContinuousFileReaderOperator;\n+import org.apache.flink.streaming.api.functions.source.RichSourceFunction;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.catalog.hive.client.HiveShim;\n+import org.apache.flink.table.catalog.hive.util.HiveReflectionUtils;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.metastore.IMetaStoreClient;\n+import org.apache.hadoop.hive.metastore.api.Partition;\n+import org.apache.hadoop.hive.metastore.api.Table;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.thrift.TException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.Set;\n+\n+/**\n+ * This is the single (non-parallel) monitoring task which takes a {@link HiveTableInputFormat},\n+ * it is responsible for:\n+ *\n+ * <ol>\n+ *     <li>Monitoring partitions of hive meta store.</li>\n+ *     <li>Deciding which partitions should be further read and processed.</li>\n+ *     <li>Creating the {@link HiveTableInputSplit splits} corresponding to those partitions.</li>\n+ *     <li>Assigning them to downstream tasks for further processing.</li>\n+ * </ol>\n+ *\n+ * <p>The splits to be read are forwarded to the downstream {@link ContinuousFileReaderOperator}\n+ * which can have parallelism greater than one.\n+ *\n+ * <p><b>IMPORTANT NOTE: </b> Splits are forwarded downstream for reading in ascending partition time order,\n+ * based on the partition time of the partitions they belong to.\n+ */\n+public class HiveContinuousMonitoringFunction\n+\t\textends RichSourceFunction<TimestampedHiveInputSplit>\n+\t\timplements CheckpointedFunction {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(HiveContinuousMonitoringFunction.class);\n+\n+\t/** The parallelism of the downstream readers. */\n+\tprivate final int readerParallelism;\n+\n+\t/** The interval between consecutive path scans. */\n+\tprivate final long interval;\n+\n+\tprivate final HiveShim hiveShim;\n+\n+\tprivate final JobConfWrapper conf;\n+\n+\tprivate final ObjectPath tablePath;\n+\n+\tprivate final List<String> partitionKeys;\n+\n+\tprivate final String[] fieldNames;\n+\n+\tprivate final DataType[] fieldTypes;\n+\n+\tprivate final long startupTimestampMillis;\n+\n+\tprivate final PartitionStrategyFactory partStrategyFactory;\n+\n+\tprivate volatile boolean isRunning = true;\n+\n+\t/** The maximum partition read time seen so far. */\n+\tprivate volatile long currentReadTime;\n+\n+\tprivate transient PartitionStrategy strategy;\n+\n+\tprivate transient Object checkpointLock;\n+\n+\tprivate transient ListState<Long> currReadTimeState;\n+\n+\tprivate transient ListState<List<List<String>>> distinctPartsState;\n+\n+\tprivate transient IMetaStoreClient client;\n+\n+\tprivate transient Properties tableProps;\n+\n+\tprivate transient String defaultPartitionName;\n+\n+\tprivate transient Set<List<String>> distinctPartitions;\n+\n+\tpublic HiveContinuousMonitoringFunction(\n+\t\t\tHiveShim hiveShim,\n+\t\t\tJobConf conf,\n+\t\t\tObjectPath tablePath,\n+\t\t\tCatalogTable catalogTable,\n+\t\t\tlong startupTimestampMillis,\n+\t\t\tPartitionStrategyFactory partStrategyFactory,\n+\t\t\tint readerParallelism,\n+\t\t\tlong interval) {\n+\t\tthis.hiveShim = hiveShim;\n+\t\tthis.conf = new JobConfWrapper(conf);\n+\t\tthis.tablePath = tablePath;\n+\t\tthis.partitionKeys = catalogTable.getPartitionKeys();\n+\t\tthis.fieldNames = catalogTable.getSchema().getFieldNames();\n+\t\tthis.fieldTypes = catalogTable.getSchema().getFieldDataTypes();\n+\t\tthis.startupTimestampMillis = startupTimestampMillis;\n+\t\tthis.partStrategyFactory = partStrategyFactory;\n+\n+\t\tthis.interval = interval;\n+\t\tthis.readerParallelism = Math.max(readerParallelism, 1);\n+\t\tthis.currentReadTime = Long.MIN_VALUE;\n+\t}\n+\n+\t@Override\n+\tpublic void initializeState(FunctionInitializationContext context) throws Exception {\n+\t\tthis.strategy = partStrategyFactory.createStrategy(getRuntimeContext().getUserCodeClassLoader());\n+\n+\t\tthis.currReadTimeState = context.getOperatorStateStore().getListState(\n+\t\t\tnew ListStateDescriptor<>(\n+\t\t\t\t\"partition-monitoring-state\",\n+\t\t\t\tLongSerializer.INSTANCE\n+\t\t\t)\n+\t\t);\n+\t\tthis.distinctPartsState = context.getOperatorStateStore().getListState(\n+\t\t\tnew ListStateDescriptor<>(\n+\t\t\t\t\"partition-monitoring-state\",\n+\t\t\t\tnew ListSerializer<>(new ListSerializer<>(StringSerializer.INSTANCE))\n+\t\t\t)\n+\t\t);\n+\n+\t\tthis.client = this.hiveShim.getHiveMetastoreClient(new HiveConf(conf.conf(), HiveConf.class));\n+\n+\t\tTable hiveTable = client.getTable(tablePath.getDatabaseName(), tablePath.getObjectName());\n+\t\tthis.tableProps = HiveReflectionUtils.getTableMetadata(hiveShim, hiveTable);\n+\t\tthis.defaultPartitionName = conf.conf().get(HiveConf.ConfVars.DEFAULTPARTITIONNAME.varname,\n+\t\t\t\tHiveConf.ConfVars.DEFAULTPARTITIONNAME.defaultStrVal);\n+\n+\t\tthis.distinctPartitions = new HashSet<>();\n+\t\tif (context.isRestored()) {\n+\t\t\tLOG.info(\"Restoring state for the {}.\", getClass().getSimpleName());\n+\t\t\tthis.currentReadTime = this.currReadTimeState.get().iterator().next();\n+\t\t\tthis.distinctPartitions.addAll(this.distinctPartsState.get().iterator().next());\n+\t\t} else {\n+\t\t\tLOG.info(\"No state to restore for the {}.\", getClass().getSimpleName());\n+\t\t\tthis.currentReadTime = this.startupTimestampMillis;\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void run(SourceContext<TimestampedHiveInputSplit> context) throws Exception {\n+\t\tcheckpointLock = context.getCheckpointLock();\n+\t\twhile (isRunning) {\n+\t\t\tsynchronized (checkpointLock) {\n+\t\t\t\tmonitorAndForwardSplits(context);\n+\t\t\t}\n+\t\t\tThread.sleep(interval);\n+\t\t}\n+\t}\n+\n+\tprivate void monitorAndForwardSplits(\n+\t\t\tSourceContext<TimestampedHiveInputSplit> context) throws IOException, TException {\n+\t\tassert (Thread.holdsLock(checkpointLock));\n+\n+\t\tList<Partition> partitions = client.listPartitionsByFilter(\n+\t\t\t\ttablePath.getDatabaseName(),\n+\t\t\t\ttablePath.getObjectName(),\n+\t\t\t\tstrategy.generateFetchFilter(partitionKeys, currentReadTime),\n+\t\t\t\t(short) -1);\n+\n+\t\tif (partitions.isEmpty()) {\n+\t\t\treturn;\n+\t\t}\n+\n+\t\tlong maxTime = Long.MIN_VALUE;\n+\t\tfor (Partition partition : partitions) {\n+\t\t\tList<String> partSpec = partition.getValues();\n+\t\t\tif (!this.distinctPartitions.contains(partSpec)) {\n+\t\t\t\tthis.distinctPartitions.add(partition.getValues());\n+\t\t\t\tlong time = this.strategy.extractPartTime(partitionKeys, partition.getValues());\n+\t\t\t\tif (time > maxTime) {\n+\t\t\t\t\tmaxTime = time;\n+\t\t\t\t}\n+\t\t\t\tHiveTableInputSplit[] splits = HiveTableInputFormat.createInputSplits(\n+\t\t\t\t\t\tthis.readerParallelism,\n+\t\t\t\t\t\tCollections.singletonList(toHiveTablePartition(partition)),\n+\t\t\t\t\t\tthis.conf.conf());\n+\t\t\t\tfor (HiveTableInputSplit split : splits) {\n+\t\t\t\t\tcontext.collect(new TimestampedHiveInputSplit(time, split));\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t\tthis.currentReadTime = maxTime;\n+\n+\t\tthis.distinctPartitions.removeIf(partSpec -> this.strategy.canExpireForDistinct(\n+\t\t\t\tthis.strategy.extractPartTime(partitionKeys, partSpec),\n+\t\t\t\tthis.currentReadTime));\n+\t}\n+\n+\tprivate HiveTablePartition toHiveTablePartition(Partition p) {\n+\t\treturn HiveTableSource.toHiveTablePartition(\n+\t\t\t\tpartitionKeys, fieldNames, fieldTypes, hiveShim, tableProps, defaultPartitionName, p);\n+\t}\n+\n+\t@Override\n+\tpublic void snapshotState(FunctionSnapshotContext context) throws Exception {\n+\t\tPreconditions.checkState(this.currReadTimeState != null,\n+\t\t\t\t\"The \" + getClass().getSimpleName() + \" state has not been properly initialized.\");\n+\n+\t\tthis.currReadTimeState.clear();\n+\t\tthis.currReadTimeState.add(this.currentReadTime);\n+\n+\t\tthis.distinctPartsState.clear();\n+\t\tthis.distinctPartsState.add(new ArrayList<>(this.distinctPartitions));\n+\n+\t\tif (LOG.isDebugEnabled()) {\n+\t\t\tLOG.debug(\"{} checkpointed {}.\", getClass().getSimpleName(), currentReadTime);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws Exception {\n+\t\tsuper.close();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4223a26640c7243a61135e803aa1dd21888114a5"}, "originalPosition": 257}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYxOTQ4ODYyOnYy", "diffSide": "RIGHT", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveContinuousMonitoringFunction.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNlQxMzoyNDowMVrOGRSwsA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQxMTowMjo1M1rOGS6VRw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDc4NjM1Mg==", "bodyText": "The checkpointLock  is assigned in run(). So if cancel is called before run, the checkpointLock  should always be null?", "url": "https://github.com/apache/flink/pull/12004#discussion_r420786352", "createdAt": "2020-05-06T13:24:01Z", "author": {"login": "lirui-apache"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveContinuousMonitoringFunction.java", "diffHunk": "@@ -0,0 +1,280 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive.read;\n+\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeutils.base.ListSerializer;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.api.common.typeutils.base.StringSerializer;\n+import org.apache.flink.connectors.hive.HiveTablePartition;\n+import org.apache.flink.connectors.hive.HiveTableSource;\n+import org.apache.flink.connectors.hive.JobConfWrapper;\n+import org.apache.flink.connectors.hive.read.PartitionStrategy.PartitionStrategyFactory;\n+import org.apache.flink.runtime.state.FunctionInitializationContext;\n+import org.apache.flink.runtime.state.FunctionSnapshotContext;\n+import org.apache.flink.streaming.api.checkpoint.CheckpointedFunction;\n+import org.apache.flink.streaming.api.functions.source.ContinuousFileReaderOperator;\n+import org.apache.flink.streaming.api.functions.source.RichSourceFunction;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.catalog.hive.client.HiveShim;\n+import org.apache.flink.table.catalog.hive.util.HiveReflectionUtils;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.metastore.IMetaStoreClient;\n+import org.apache.hadoop.hive.metastore.api.Partition;\n+import org.apache.hadoop.hive.metastore.api.Table;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.thrift.TException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.Set;\n+\n+/**\n+ * This is the single (non-parallel) monitoring task which takes a {@link HiveTableInputFormat},\n+ * it is responsible for:\n+ *\n+ * <ol>\n+ *     <li>Monitoring partitions of hive meta store.</li>\n+ *     <li>Deciding which partitions should be further read and processed.</li>\n+ *     <li>Creating the {@link HiveTableInputSplit splits} corresponding to those partitions.</li>\n+ *     <li>Assigning them to downstream tasks for further processing.</li>\n+ * </ol>\n+ *\n+ * <p>The splits to be read are forwarded to the downstream {@link ContinuousFileReaderOperator}\n+ * which can have parallelism greater than one.\n+ *\n+ * <p><b>IMPORTANT NOTE: </b> Splits are forwarded downstream for reading in ascending partition time order,\n+ * based on the partition time of the partitions they belong to.\n+ */\n+public class HiveContinuousMonitoringFunction\n+\t\textends RichSourceFunction<TimestampedHiveInputSplit>\n+\t\timplements CheckpointedFunction {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(HiveContinuousMonitoringFunction.class);\n+\n+\t/** The parallelism of the downstream readers. */\n+\tprivate final int readerParallelism;\n+\n+\t/** The interval between consecutive path scans. */\n+\tprivate final long interval;\n+\n+\tprivate final HiveShim hiveShim;\n+\n+\tprivate final JobConfWrapper conf;\n+\n+\tprivate final ObjectPath tablePath;\n+\n+\tprivate final List<String> partitionKeys;\n+\n+\tprivate final String[] fieldNames;\n+\n+\tprivate final DataType[] fieldTypes;\n+\n+\tprivate final long startupTimestampMillis;\n+\n+\tprivate final PartitionStrategyFactory partStrategyFactory;\n+\n+\tprivate volatile boolean isRunning = true;\n+\n+\t/** The maximum partition read time seen so far. */\n+\tprivate volatile long currentReadTime;\n+\n+\tprivate transient PartitionStrategy strategy;\n+\n+\tprivate transient Object checkpointLock;\n+\n+\tprivate transient ListState<Long> currReadTimeState;\n+\n+\tprivate transient ListState<List<List<String>>> distinctPartsState;\n+\n+\tprivate transient IMetaStoreClient client;\n+\n+\tprivate transient Properties tableProps;\n+\n+\tprivate transient String defaultPartitionName;\n+\n+\tprivate transient Set<List<String>> distinctPartitions;\n+\n+\tpublic HiveContinuousMonitoringFunction(\n+\t\t\tHiveShim hiveShim,\n+\t\t\tJobConf conf,\n+\t\t\tObjectPath tablePath,\n+\t\t\tCatalogTable catalogTable,\n+\t\t\tlong startupTimestampMillis,\n+\t\t\tPartitionStrategyFactory partStrategyFactory,\n+\t\t\tint readerParallelism,\n+\t\t\tlong interval) {\n+\t\tthis.hiveShim = hiveShim;\n+\t\tthis.conf = new JobConfWrapper(conf);\n+\t\tthis.tablePath = tablePath;\n+\t\tthis.partitionKeys = catalogTable.getPartitionKeys();\n+\t\tthis.fieldNames = catalogTable.getSchema().getFieldNames();\n+\t\tthis.fieldTypes = catalogTable.getSchema().getFieldDataTypes();\n+\t\tthis.startupTimestampMillis = startupTimestampMillis;\n+\t\tthis.partStrategyFactory = partStrategyFactory;\n+\n+\t\tthis.interval = interval;\n+\t\tthis.readerParallelism = Math.max(readerParallelism, 1);\n+\t\tthis.currentReadTime = Long.MIN_VALUE;\n+\t}\n+\n+\t@Override\n+\tpublic void initializeState(FunctionInitializationContext context) throws Exception {\n+\t\tthis.strategy = partStrategyFactory.createStrategy(getRuntimeContext().getUserCodeClassLoader());\n+\n+\t\tthis.currReadTimeState = context.getOperatorStateStore().getListState(\n+\t\t\tnew ListStateDescriptor<>(\n+\t\t\t\t\"partition-monitoring-state\",\n+\t\t\t\tLongSerializer.INSTANCE\n+\t\t\t)\n+\t\t);\n+\t\tthis.distinctPartsState = context.getOperatorStateStore().getListState(\n+\t\t\tnew ListStateDescriptor<>(\n+\t\t\t\t\"partition-monitoring-state\",\n+\t\t\t\tnew ListSerializer<>(new ListSerializer<>(StringSerializer.INSTANCE))\n+\t\t\t)\n+\t\t);\n+\n+\t\tthis.client = this.hiveShim.getHiveMetastoreClient(new HiveConf(conf.conf(), HiveConf.class));\n+\n+\t\tTable hiveTable = client.getTable(tablePath.getDatabaseName(), tablePath.getObjectName());\n+\t\tthis.tableProps = HiveReflectionUtils.getTableMetadata(hiveShim, hiveTable);\n+\t\tthis.defaultPartitionName = conf.conf().get(HiveConf.ConfVars.DEFAULTPARTITIONNAME.varname,\n+\t\t\t\tHiveConf.ConfVars.DEFAULTPARTITIONNAME.defaultStrVal);\n+\n+\t\tthis.distinctPartitions = new HashSet<>();\n+\t\tif (context.isRestored()) {\n+\t\t\tLOG.info(\"Restoring state for the {}.\", getClass().getSimpleName());\n+\t\t\tthis.currentReadTime = this.currReadTimeState.get().iterator().next();\n+\t\t\tthis.distinctPartitions.addAll(this.distinctPartsState.get().iterator().next());\n+\t\t} else {\n+\t\t\tLOG.info(\"No state to restore for the {}.\", getClass().getSimpleName());\n+\t\t\tthis.currentReadTime = this.startupTimestampMillis;\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void run(SourceContext<TimestampedHiveInputSplit> context) throws Exception {\n+\t\tcheckpointLock = context.getCheckpointLock();\n+\t\twhile (isRunning) {\n+\t\t\tsynchronized (checkpointLock) {\n+\t\t\t\tmonitorAndForwardSplits(context);\n+\t\t\t}\n+\t\t\tThread.sleep(interval);\n+\t\t}\n+\t}\n+\n+\tprivate void monitorAndForwardSplits(\n+\t\t\tSourceContext<TimestampedHiveInputSplit> context) throws IOException, TException {\n+\t\tassert (Thread.holdsLock(checkpointLock));\n+\n+\t\tList<Partition> partitions = client.listPartitionsByFilter(\n+\t\t\t\ttablePath.getDatabaseName(),\n+\t\t\t\ttablePath.getObjectName(),\n+\t\t\t\tstrategy.generateFetchFilter(partitionKeys, currentReadTime),\n+\t\t\t\t(short) -1);\n+\n+\t\tif (partitions.isEmpty()) {\n+\t\t\treturn;\n+\t\t}\n+\n+\t\tlong maxTime = Long.MIN_VALUE;\n+\t\tfor (Partition partition : partitions) {\n+\t\t\tList<String> partSpec = partition.getValues();\n+\t\t\tif (!this.distinctPartitions.contains(partSpec)) {\n+\t\t\t\tthis.distinctPartitions.add(partition.getValues());\n+\t\t\t\tlong time = this.strategy.extractPartTime(partitionKeys, partition.getValues());\n+\t\t\t\tif (time > maxTime) {\n+\t\t\t\t\tmaxTime = time;\n+\t\t\t\t}\n+\t\t\t\tHiveTableInputSplit[] splits = HiveTableInputFormat.createInputSplits(\n+\t\t\t\t\t\tthis.readerParallelism,\n+\t\t\t\t\t\tCollections.singletonList(toHiveTablePartition(partition)),\n+\t\t\t\t\t\tthis.conf.conf());\n+\t\t\t\tfor (HiveTableInputSplit split : splits) {\n+\t\t\t\t\tcontext.collect(new TimestampedHiveInputSplit(time, split));\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t\tthis.currentReadTime = maxTime;\n+\n+\t\tthis.distinctPartitions.removeIf(partSpec -> this.strategy.canExpireForDistinct(\n+\t\t\t\tthis.strategy.extractPartTime(partitionKeys, partSpec),\n+\t\t\t\tthis.currentReadTime));\n+\t}\n+\n+\tprivate HiveTablePartition toHiveTablePartition(Partition p) {\n+\t\treturn HiveTableSource.toHiveTablePartition(\n+\t\t\t\tpartitionKeys, fieldNames, fieldTypes, hiveShim, tableProps, defaultPartitionName, p);\n+\t}\n+\n+\t@Override\n+\tpublic void snapshotState(FunctionSnapshotContext context) throws Exception {\n+\t\tPreconditions.checkState(this.currReadTimeState != null,\n+\t\t\t\t\"The \" + getClass().getSimpleName() + \" state has not been properly initialized.\");\n+\n+\t\tthis.currReadTimeState.clear();\n+\t\tthis.currReadTimeState.add(this.currentReadTime);\n+\n+\t\tthis.distinctPartsState.clear();\n+\t\tthis.distinctPartsState.add(new ArrayList<>(this.distinctPartitions));\n+\n+\t\tif (LOG.isDebugEnabled()) {\n+\t\t\tLOG.debug(\"{} checkpointed {}.\", getClass().getSimpleName(), currentReadTime);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws Exception {\n+\t\tsuper.close();\n+\n+\t\tif (checkpointLock != null) {\n+\t\t\tsynchronized (checkpointLock) {\n+\t\t\t\tcurrentReadTime = Long.MAX_VALUE;\n+\t\t\t\tisRunning = false;\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void cancel() {\n+\t\tif (checkpointLock != null) {\n+\t\t\t// this is to cover the case where cancel() is called before the run()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4223a26640c7243a61135e803aa1dd21888114a5"}, "originalPosition": 270}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ1NDQzMg==", "bodyText": "this is from another thread.", "url": "https://github.com/apache/flink/pull/12004#discussion_r422454432", "createdAt": "2020-05-09T05:16:10Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveContinuousMonitoringFunction.java", "diffHunk": "@@ -0,0 +1,280 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive.read;\n+\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeutils.base.ListSerializer;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.api.common.typeutils.base.StringSerializer;\n+import org.apache.flink.connectors.hive.HiveTablePartition;\n+import org.apache.flink.connectors.hive.HiveTableSource;\n+import org.apache.flink.connectors.hive.JobConfWrapper;\n+import org.apache.flink.connectors.hive.read.PartitionStrategy.PartitionStrategyFactory;\n+import org.apache.flink.runtime.state.FunctionInitializationContext;\n+import org.apache.flink.runtime.state.FunctionSnapshotContext;\n+import org.apache.flink.streaming.api.checkpoint.CheckpointedFunction;\n+import org.apache.flink.streaming.api.functions.source.ContinuousFileReaderOperator;\n+import org.apache.flink.streaming.api.functions.source.RichSourceFunction;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.catalog.hive.client.HiveShim;\n+import org.apache.flink.table.catalog.hive.util.HiveReflectionUtils;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.metastore.IMetaStoreClient;\n+import org.apache.hadoop.hive.metastore.api.Partition;\n+import org.apache.hadoop.hive.metastore.api.Table;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.thrift.TException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.Set;\n+\n+/**\n+ * This is the single (non-parallel) monitoring task which takes a {@link HiveTableInputFormat},\n+ * it is responsible for:\n+ *\n+ * <ol>\n+ *     <li>Monitoring partitions of hive meta store.</li>\n+ *     <li>Deciding which partitions should be further read and processed.</li>\n+ *     <li>Creating the {@link HiveTableInputSplit splits} corresponding to those partitions.</li>\n+ *     <li>Assigning them to downstream tasks for further processing.</li>\n+ * </ol>\n+ *\n+ * <p>The splits to be read are forwarded to the downstream {@link ContinuousFileReaderOperator}\n+ * which can have parallelism greater than one.\n+ *\n+ * <p><b>IMPORTANT NOTE: </b> Splits are forwarded downstream for reading in ascending partition time order,\n+ * based on the partition time of the partitions they belong to.\n+ */\n+public class HiveContinuousMonitoringFunction\n+\t\textends RichSourceFunction<TimestampedHiveInputSplit>\n+\t\timplements CheckpointedFunction {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(HiveContinuousMonitoringFunction.class);\n+\n+\t/** The parallelism of the downstream readers. */\n+\tprivate final int readerParallelism;\n+\n+\t/** The interval between consecutive path scans. */\n+\tprivate final long interval;\n+\n+\tprivate final HiveShim hiveShim;\n+\n+\tprivate final JobConfWrapper conf;\n+\n+\tprivate final ObjectPath tablePath;\n+\n+\tprivate final List<String> partitionKeys;\n+\n+\tprivate final String[] fieldNames;\n+\n+\tprivate final DataType[] fieldTypes;\n+\n+\tprivate final long startupTimestampMillis;\n+\n+\tprivate final PartitionStrategyFactory partStrategyFactory;\n+\n+\tprivate volatile boolean isRunning = true;\n+\n+\t/** The maximum partition read time seen so far. */\n+\tprivate volatile long currentReadTime;\n+\n+\tprivate transient PartitionStrategy strategy;\n+\n+\tprivate transient Object checkpointLock;\n+\n+\tprivate transient ListState<Long> currReadTimeState;\n+\n+\tprivate transient ListState<List<List<String>>> distinctPartsState;\n+\n+\tprivate transient IMetaStoreClient client;\n+\n+\tprivate transient Properties tableProps;\n+\n+\tprivate transient String defaultPartitionName;\n+\n+\tprivate transient Set<List<String>> distinctPartitions;\n+\n+\tpublic HiveContinuousMonitoringFunction(\n+\t\t\tHiveShim hiveShim,\n+\t\t\tJobConf conf,\n+\t\t\tObjectPath tablePath,\n+\t\t\tCatalogTable catalogTable,\n+\t\t\tlong startupTimestampMillis,\n+\t\t\tPartitionStrategyFactory partStrategyFactory,\n+\t\t\tint readerParallelism,\n+\t\t\tlong interval) {\n+\t\tthis.hiveShim = hiveShim;\n+\t\tthis.conf = new JobConfWrapper(conf);\n+\t\tthis.tablePath = tablePath;\n+\t\tthis.partitionKeys = catalogTable.getPartitionKeys();\n+\t\tthis.fieldNames = catalogTable.getSchema().getFieldNames();\n+\t\tthis.fieldTypes = catalogTable.getSchema().getFieldDataTypes();\n+\t\tthis.startupTimestampMillis = startupTimestampMillis;\n+\t\tthis.partStrategyFactory = partStrategyFactory;\n+\n+\t\tthis.interval = interval;\n+\t\tthis.readerParallelism = Math.max(readerParallelism, 1);\n+\t\tthis.currentReadTime = Long.MIN_VALUE;\n+\t}\n+\n+\t@Override\n+\tpublic void initializeState(FunctionInitializationContext context) throws Exception {\n+\t\tthis.strategy = partStrategyFactory.createStrategy(getRuntimeContext().getUserCodeClassLoader());\n+\n+\t\tthis.currReadTimeState = context.getOperatorStateStore().getListState(\n+\t\t\tnew ListStateDescriptor<>(\n+\t\t\t\t\"partition-monitoring-state\",\n+\t\t\t\tLongSerializer.INSTANCE\n+\t\t\t)\n+\t\t);\n+\t\tthis.distinctPartsState = context.getOperatorStateStore().getListState(\n+\t\t\tnew ListStateDescriptor<>(\n+\t\t\t\t\"partition-monitoring-state\",\n+\t\t\t\tnew ListSerializer<>(new ListSerializer<>(StringSerializer.INSTANCE))\n+\t\t\t)\n+\t\t);\n+\n+\t\tthis.client = this.hiveShim.getHiveMetastoreClient(new HiveConf(conf.conf(), HiveConf.class));\n+\n+\t\tTable hiveTable = client.getTable(tablePath.getDatabaseName(), tablePath.getObjectName());\n+\t\tthis.tableProps = HiveReflectionUtils.getTableMetadata(hiveShim, hiveTable);\n+\t\tthis.defaultPartitionName = conf.conf().get(HiveConf.ConfVars.DEFAULTPARTITIONNAME.varname,\n+\t\t\t\tHiveConf.ConfVars.DEFAULTPARTITIONNAME.defaultStrVal);\n+\n+\t\tthis.distinctPartitions = new HashSet<>();\n+\t\tif (context.isRestored()) {\n+\t\t\tLOG.info(\"Restoring state for the {}.\", getClass().getSimpleName());\n+\t\t\tthis.currentReadTime = this.currReadTimeState.get().iterator().next();\n+\t\t\tthis.distinctPartitions.addAll(this.distinctPartsState.get().iterator().next());\n+\t\t} else {\n+\t\t\tLOG.info(\"No state to restore for the {}.\", getClass().getSimpleName());\n+\t\t\tthis.currentReadTime = this.startupTimestampMillis;\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void run(SourceContext<TimestampedHiveInputSplit> context) throws Exception {\n+\t\tcheckpointLock = context.getCheckpointLock();\n+\t\twhile (isRunning) {\n+\t\t\tsynchronized (checkpointLock) {\n+\t\t\t\tmonitorAndForwardSplits(context);\n+\t\t\t}\n+\t\t\tThread.sleep(interval);\n+\t\t}\n+\t}\n+\n+\tprivate void monitorAndForwardSplits(\n+\t\t\tSourceContext<TimestampedHiveInputSplit> context) throws IOException, TException {\n+\t\tassert (Thread.holdsLock(checkpointLock));\n+\n+\t\tList<Partition> partitions = client.listPartitionsByFilter(\n+\t\t\t\ttablePath.getDatabaseName(),\n+\t\t\t\ttablePath.getObjectName(),\n+\t\t\t\tstrategy.generateFetchFilter(partitionKeys, currentReadTime),\n+\t\t\t\t(short) -1);\n+\n+\t\tif (partitions.isEmpty()) {\n+\t\t\treturn;\n+\t\t}\n+\n+\t\tlong maxTime = Long.MIN_VALUE;\n+\t\tfor (Partition partition : partitions) {\n+\t\t\tList<String> partSpec = partition.getValues();\n+\t\t\tif (!this.distinctPartitions.contains(partSpec)) {\n+\t\t\t\tthis.distinctPartitions.add(partition.getValues());\n+\t\t\t\tlong time = this.strategy.extractPartTime(partitionKeys, partition.getValues());\n+\t\t\t\tif (time > maxTime) {\n+\t\t\t\t\tmaxTime = time;\n+\t\t\t\t}\n+\t\t\t\tHiveTableInputSplit[] splits = HiveTableInputFormat.createInputSplits(\n+\t\t\t\t\t\tthis.readerParallelism,\n+\t\t\t\t\t\tCollections.singletonList(toHiveTablePartition(partition)),\n+\t\t\t\t\t\tthis.conf.conf());\n+\t\t\t\tfor (HiveTableInputSplit split : splits) {\n+\t\t\t\t\tcontext.collect(new TimestampedHiveInputSplit(time, split));\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t\tthis.currentReadTime = maxTime;\n+\n+\t\tthis.distinctPartitions.removeIf(partSpec -> this.strategy.canExpireForDistinct(\n+\t\t\t\tthis.strategy.extractPartTime(partitionKeys, partSpec),\n+\t\t\t\tthis.currentReadTime));\n+\t}\n+\n+\tprivate HiveTablePartition toHiveTablePartition(Partition p) {\n+\t\treturn HiveTableSource.toHiveTablePartition(\n+\t\t\t\tpartitionKeys, fieldNames, fieldTypes, hiveShim, tableProps, defaultPartitionName, p);\n+\t}\n+\n+\t@Override\n+\tpublic void snapshotState(FunctionSnapshotContext context) throws Exception {\n+\t\tPreconditions.checkState(this.currReadTimeState != null,\n+\t\t\t\t\"The \" + getClass().getSimpleName() + \" state has not been properly initialized.\");\n+\n+\t\tthis.currReadTimeState.clear();\n+\t\tthis.currReadTimeState.add(this.currentReadTime);\n+\n+\t\tthis.distinctPartsState.clear();\n+\t\tthis.distinctPartsState.add(new ArrayList<>(this.distinctPartitions));\n+\n+\t\tif (LOG.isDebugEnabled()) {\n+\t\t\tLOG.debug(\"{} checkpointed {}.\", getClass().getSimpleName(), currentReadTime);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws Exception {\n+\t\tsuper.close();\n+\n+\t\tif (checkpointLock != null) {\n+\t\t\tsynchronized (checkpointLock) {\n+\t\t\t\tcurrentReadTime = Long.MAX_VALUE;\n+\t\t\t\tisRunning = false;\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void cancel() {\n+\t\tif (checkpointLock != null) {\n+\t\t\t// this is to cover the case where cancel() is called before the run()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDc4NjM1Mg=="}, "originalCommit": {"oid": "4223a26640c7243a61135e803aa1dd21888114a5"}, "originalPosition": 270}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ4MzI3MQ==", "bodyText": "Let's move this comment to the outside of the if block, so that it's easier to understand.", "url": "https://github.com/apache/flink/pull/12004#discussion_r422483271", "createdAt": "2020-05-09T11:02:53Z", "author": {"login": "lirui-apache"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveContinuousMonitoringFunction.java", "diffHunk": "@@ -0,0 +1,280 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive.read;\n+\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeutils.base.ListSerializer;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.api.common.typeutils.base.StringSerializer;\n+import org.apache.flink.connectors.hive.HiveTablePartition;\n+import org.apache.flink.connectors.hive.HiveTableSource;\n+import org.apache.flink.connectors.hive.JobConfWrapper;\n+import org.apache.flink.connectors.hive.read.PartitionStrategy.PartitionStrategyFactory;\n+import org.apache.flink.runtime.state.FunctionInitializationContext;\n+import org.apache.flink.runtime.state.FunctionSnapshotContext;\n+import org.apache.flink.streaming.api.checkpoint.CheckpointedFunction;\n+import org.apache.flink.streaming.api.functions.source.ContinuousFileReaderOperator;\n+import org.apache.flink.streaming.api.functions.source.RichSourceFunction;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.catalog.hive.client.HiveShim;\n+import org.apache.flink.table.catalog.hive.util.HiveReflectionUtils;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.metastore.IMetaStoreClient;\n+import org.apache.hadoop.hive.metastore.api.Partition;\n+import org.apache.hadoop.hive.metastore.api.Table;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.thrift.TException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.Set;\n+\n+/**\n+ * This is the single (non-parallel) monitoring task which takes a {@link HiveTableInputFormat},\n+ * it is responsible for:\n+ *\n+ * <ol>\n+ *     <li>Monitoring partitions of hive meta store.</li>\n+ *     <li>Deciding which partitions should be further read and processed.</li>\n+ *     <li>Creating the {@link HiveTableInputSplit splits} corresponding to those partitions.</li>\n+ *     <li>Assigning them to downstream tasks for further processing.</li>\n+ * </ol>\n+ *\n+ * <p>The splits to be read are forwarded to the downstream {@link ContinuousFileReaderOperator}\n+ * which can have parallelism greater than one.\n+ *\n+ * <p><b>IMPORTANT NOTE: </b> Splits are forwarded downstream for reading in ascending partition time order,\n+ * based on the partition time of the partitions they belong to.\n+ */\n+public class HiveContinuousMonitoringFunction\n+\t\textends RichSourceFunction<TimestampedHiveInputSplit>\n+\t\timplements CheckpointedFunction {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(HiveContinuousMonitoringFunction.class);\n+\n+\t/** The parallelism of the downstream readers. */\n+\tprivate final int readerParallelism;\n+\n+\t/** The interval between consecutive path scans. */\n+\tprivate final long interval;\n+\n+\tprivate final HiveShim hiveShim;\n+\n+\tprivate final JobConfWrapper conf;\n+\n+\tprivate final ObjectPath tablePath;\n+\n+\tprivate final List<String> partitionKeys;\n+\n+\tprivate final String[] fieldNames;\n+\n+\tprivate final DataType[] fieldTypes;\n+\n+\tprivate final long startupTimestampMillis;\n+\n+\tprivate final PartitionStrategyFactory partStrategyFactory;\n+\n+\tprivate volatile boolean isRunning = true;\n+\n+\t/** The maximum partition read time seen so far. */\n+\tprivate volatile long currentReadTime;\n+\n+\tprivate transient PartitionStrategy strategy;\n+\n+\tprivate transient Object checkpointLock;\n+\n+\tprivate transient ListState<Long> currReadTimeState;\n+\n+\tprivate transient ListState<List<List<String>>> distinctPartsState;\n+\n+\tprivate transient IMetaStoreClient client;\n+\n+\tprivate transient Properties tableProps;\n+\n+\tprivate transient String defaultPartitionName;\n+\n+\tprivate transient Set<List<String>> distinctPartitions;\n+\n+\tpublic HiveContinuousMonitoringFunction(\n+\t\t\tHiveShim hiveShim,\n+\t\t\tJobConf conf,\n+\t\t\tObjectPath tablePath,\n+\t\t\tCatalogTable catalogTable,\n+\t\t\tlong startupTimestampMillis,\n+\t\t\tPartitionStrategyFactory partStrategyFactory,\n+\t\t\tint readerParallelism,\n+\t\t\tlong interval) {\n+\t\tthis.hiveShim = hiveShim;\n+\t\tthis.conf = new JobConfWrapper(conf);\n+\t\tthis.tablePath = tablePath;\n+\t\tthis.partitionKeys = catalogTable.getPartitionKeys();\n+\t\tthis.fieldNames = catalogTable.getSchema().getFieldNames();\n+\t\tthis.fieldTypes = catalogTable.getSchema().getFieldDataTypes();\n+\t\tthis.startupTimestampMillis = startupTimestampMillis;\n+\t\tthis.partStrategyFactory = partStrategyFactory;\n+\n+\t\tthis.interval = interval;\n+\t\tthis.readerParallelism = Math.max(readerParallelism, 1);\n+\t\tthis.currentReadTime = Long.MIN_VALUE;\n+\t}\n+\n+\t@Override\n+\tpublic void initializeState(FunctionInitializationContext context) throws Exception {\n+\t\tthis.strategy = partStrategyFactory.createStrategy(getRuntimeContext().getUserCodeClassLoader());\n+\n+\t\tthis.currReadTimeState = context.getOperatorStateStore().getListState(\n+\t\t\tnew ListStateDescriptor<>(\n+\t\t\t\t\"partition-monitoring-state\",\n+\t\t\t\tLongSerializer.INSTANCE\n+\t\t\t)\n+\t\t);\n+\t\tthis.distinctPartsState = context.getOperatorStateStore().getListState(\n+\t\t\tnew ListStateDescriptor<>(\n+\t\t\t\t\"partition-monitoring-state\",\n+\t\t\t\tnew ListSerializer<>(new ListSerializer<>(StringSerializer.INSTANCE))\n+\t\t\t)\n+\t\t);\n+\n+\t\tthis.client = this.hiveShim.getHiveMetastoreClient(new HiveConf(conf.conf(), HiveConf.class));\n+\n+\t\tTable hiveTable = client.getTable(tablePath.getDatabaseName(), tablePath.getObjectName());\n+\t\tthis.tableProps = HiveReflectionUtils.getTableMetadata(hiveShim, hiveTable);\n+\t\tthis.defaultPartitionName = conf.conf().get(HiveConf.ConfVars.DEFAULTPARTITIONNAME.varname,\n+\t\t\t\tHiveConf.ConfVars.DEFAULTPARTITIONNAME.defaultStrVal);\n+\n+\t\tthis.distinctPartitions = new HashSet<>();\n+\t\tif (context.isRestored()) {\n+\t\t\tLOG.info(\"Restoring state for the {}.\", getClass().getSimpleName());\n+\t\t\tthis.currentReadTime = this.currReadTimeState.get().iterator().next();\n+\t\t\tthis.distinctPartitions.addAll(this.distinctPartsState.get().iterator().next());\n+\t\t} else {\n+\t\t\tLOG.info(\"No state to restore for the {}.\", getClass().getSimpleName());\n+\t\t\tthis.currentReadTime = this.startupTimestampMillis;\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void run(SourceContext<TimestampedHiveInputSplit> context) throws Exception {\n+\t\tcheckpointLock = context.getCheckpointLock();\n+\t\twhile (isRunning) {\n+\t\t\tsynchronized (checkpointLock) {\n+\t\t\t\tmonitorAndForwardSplits(context);\n+\t\t\t}\n+\t\t\tThread.sleep(interval);\n+\t\t}\n+\t}\n+\n+\tprivate void monitorAndForwardSplits(\n+\t\t\tSourceContext<TimestampedHiveInputSplit> context) throws IOException, TException {\n+\t\tassert (Thread.holdsLock(checkpointLock));\n+\n+\t\tList<Partition> partitions = client.listPartitionsByFilter(\n+\t\t\t\ttablePath.getDatabaseName(),\n+\t\t\t\ttablePath.getObjectName(),\n+\t\t\t\tstrategy.generateFetchFilter(partitionKeys, currentReadTime),\n+\t\t\t\t(short) -1);\n+\n+\t\tif (partitions.isEmpty()) {\n+\t\t\treturn;\n+\t\t}\n+\n+\t\tlong maxTime = Long.MIN_VALUE;\n+\t\tfor (Partition partition : partitions) {\n+\t\t\tList<String> partSpec = partition.getValues();\n+\t\t\tif (!this.distinctPartitions.contains(partSpec)) {\n+\t\t\t\tthis.distinctPartitions.add(partition.getValues());\n+\t\t\t\tlong time = this.strategy.extractPartTime(partitionKeys, partition.getValues());\n+\t\t\t\tif (time > maxTime) {\n+\t\t\t\t\tmaxTime = time;\n+\t\t\t\t}\n+\t\t\t\tHiveTableInputSplit[] splits = HiveTableInputFormat.createInputSplits(\n+\t\t\t\t\t\tthis.readerParallelism,\n+\t\t\t\t\t\tCollections.singletonList(toHiveTablePartition(partition)),\n+\t\t\t\t\t\tthis.conf.conf());\n+\t\t\t\tfor (HiveTableInputSplit split : splits) {\n+\t\t\t\t\tcontext.collect(new TimestampedHiveInputSplit(time, split));\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t\tthis.currentReadTime = maxTime;\n+\n+\t\tthis.distinctPartitions.removeIf(partSpec -> this.strategy.canExpireForDistinct(\n+\t\t\t\tthis.strategy.extractPartTime(partitionKeys, partSpec),\n+\t\t\t\tthis.currentReadTime));\n+\t}\n+\n+\tprivate HiveTablePartition toHiveTablePartition(Partition p) {\n+\t\treturn HiveTableSource.toHiveTablePartition(\n+\t\t\t\tpartitionKeys, fieldNames, fieldTypes, hiveShim, tableProps, defaultPartitionName, p);\n+\t}\n+\n+\t@Override\n+\tpublic void snapshotState(FunctionSnapshotContext context) throws Exception {\n+\t\tPreconditions.checkState(this.currReadTimeState != null,\n+\t\t\t\t\"The \" + getClass().getSimpleName() + \" state has not been properly initialized.\");\n+\n+\t\tthis.currReadTimeState.clear();\n+\t\tthis.currReadTimeState.add(this.currentReadTime);\n+\n+\t\tthis.distinctPartsState.clear();\n+\t\tthis.distinctPartsState.add(new ArrayList<>(this.distinctPartitions));\n+\n+\t\tif (LOG.isDebugEnabled()) {\n+\t\t\tLOG.debug(\"{} checkpointed {}.\", getClass().getSimpleName(), currentReadTime);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws Exception {\n+\t\tsuper.close();\n+\n+\t\tif (checkpointLock != null) {\n+\t\t\tsynchronized (checkpointLock) {\n+\t\t\t\tcurrentReadTime = Long.MAX_VALUE;\n+\t\t\t\tisRunning = false;\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void cancel() {\n+\t\tif (checkpointLock != null) {\n+\t\t\t// this is to cover the case where cancel() is called before the run()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDc4NjM1Mg=="}, "originalCommit": {"oid": "4223a26640c7243a61135e803aa1dd21888114a5"}, "originalPosition": 270}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYxOTUwMzgwOnYy", "diffSide": "RIGHT", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveContinuousMonitoringFunction.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNlQxMzoyNzoxMVrOGRS5zg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQwNzoxMTozNlrOGS5D3w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDc4ODY4Ng==", "bodyText": "The java doc mentions the splits should be forwarded in a specific order. But it seems we're not sorting the partitions here?", "url": "https://github.com/apache/flink/pull/12004#discussion_r420788686", "createdAt": "2020-05-06T13:27:11Z", "author": {"login": "lirui-apache"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveContinuousMonitoringFunction.java", "diffHunk": "@@ -0,0 +1,280 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive.read;\n+\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeutils.base.ListSerializer;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.api.common.typeutils.base.StringSerializer;\n+import org.apache.flink.connectors.hive.HiveTablePartition;\n+import org.apache.flink.connectors.hive.HiveTableSource;\n+import org.apache.flink.connectors.hive.JobConfWrapper;\n+import org.apache.flink.connectors.hive.read.PartitionStrategy.PartitionStrategyFactory;\n+import org.apache.flink.runtime.state.FunctionInitializationContext;\n+import org.apache.flink.runtime.state.FunctionSnapshotContext;\n+import org.apache.flink.streaming.api.checkpoint.CheckpointedFunction;\n+import org.apache.flink.streaming.api.functions.source.ContinuousFileReaderOperator;\n+import org.apache.flink.streaming.api.functions.source.RichSourceFunction;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.catalog.hive.client.HiveShim;\n+import org.apache.flink.table.catalog.hive.util.HiveReflectionUtils;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.metastore.IMetaStoreClient;\n+import org.apache.hadoop.hive.metastore.api.Partition;\n+import org.apache.hadoop.hive.metastore.api.Table;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.thrift.TException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.Set;\n+\n+/**\n+ * This is the single (non-parallel) monitoring task which takes a {@link HiveTableInputFormat},\n+ * it is responsible for:\n+ *\n+ * <ol>\n+ *     <li>Monitoring partitions of hive meta store.</li>\n+ *     <li>Deciding which partitions should be further read and processed.</li>\n+ *     <li>Creating the {@link HiveTableInputSplit splits} corresponding to those partitions.</li>\n+ *     <li>Assigning them to downstream tasks for further processing.</li>\n+ * </ol>\n+ *\n+ * <p>The splits to be read are forwarded to the downstream {@link ContinuousFileReaderOperator}\n+ * which can have parallelism greater than one.\n+ *\n+ * <p><b>IMPORTANT NOTE: </b> Splits are forwarded downstream for reading in ascending partition time order,\n+ * based on the partition time of the partitions they belong to.\n+ */\n+public class HiveContinuousMonitoringFunction\n+\t\textends RichSourceFunction<TimestampedHiveInputSplit>\n+\t\timplements CheckpointedFunction {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(HiveContinuousMonitoringFunction.class);\n+\n+\t/** The parallelism of the downstream readers. */\n+\tprivate final int readerParallelism;\n+\n+\t/** The interval between consecutive path scans. */\n+\tprivate final long interval;\n+\n+\tprivate final HiveShim hiveShim;\n+\n+\tprivate final JobConfWrapper conf;\n+\n+\tprivate final ObjectPath tablePath;\n+\n+\tprivate final List<String> partitionKeys;\n+\n+\tprivate final String[] fieldNames;\n+\n+\tprivate final DataType[] fieldTypes;\n+\n+\tprivate final long startupTimestampMillis;\n+\n+\tprivate final PartitionStrategyFactory partStrategyFactory;\n+\n+\tprivate volatile boolean isRunning = true;\n+\n+\t/** The maximum partition read time seen so far. */\n+\tprivate volatile long currentReadTime;\n+\n+\tprivate transient PartitionStrategy strategy;\n+\n+\tprivate transient Object checkpointLock;\n+\n+\tprivate transient ListState<Long> currReadTimeState;\n+\n+\tprivate transient ListState<List<List<String>>> distinctPartsState;\n+\n+\tprivate transient IMetaStoreClient client;\n+\n+\tprivate transient Properties tableProps;\n+\n+\tprivate transient String defaultPartitionName;\n+\n+\tprivate transient Set<List<String>> distinctPartitions;\n+\n+\tpublic HiveContinuousMonitoringFunction(\n+\t\t\tHiveShim hiveShim,\n+\t\t\tJobConf conf,\n+\t\t\tObjectPath tablePath,\n+\t\t\tCatalogTable catalogTable,\n+\t\t\tlong startupTimestampMillis,\n+\t\t\tPartitionStrategyFactory partStrategyFactory,\n+\t\t\tint readerParallelism,\n+\t\t\tlong interval) {\n+\t\tthis.hiveShim = hiveShim;\n+\t\tthis.conf = new JobConfWrapper(conf);\n+\t\tthis.tablePath = tablePath;\n+\t\tthis.partitionKeys = catalogTable.getPartitionKeys();\n+\t\tthis.fieldNames = catalogTable.getSchema().getFieldNames();\n+\t\tthis.fieldTypes = catalogTable.getSchema().getFieldDataTypes();\n+\t\tthis.startupTimestampMillis = startupTimestampMillis;\n+\t\tthis.partStrategyFactory = partStrategyFactory;\n+\n+\t\tthis.interval = interval;\n+\t\tthis.readerParallelism = Math.max(readerParallelism, 1);\n+\t\tthis.currentReadTime = Long.MIN_VALUE;\n+\t}\n+\n+\t@Override\n+\tpublic void initializeState(FunctionInitializationContext context) throws Exception {\n+\t\tthis.strategy = partStrategyFactory.createStrategy(getRuntimeContext().getUserCodeClassLoader());\n+\n+\t\tthis.currReadTimeState = context.getOperatorStateStore().getListState(\n+\t\t\tnew ListStateDescriptor<>(\n+\t\t\t\t\"partition-monitoring-state\",\n+\t\t\t\tLongSerializer.INSTANCE\n+\t\t\t)\n+\t\t);\n+\t\tthis.distinctPartsState = context.getOperatorStateStore().getListState(\n+\t\t\tnew ListStateDescriptor<>(\n+\t\t\t\t\"partition-monitoring-state\",\n+\t\t\t\tnew ListSerializer<>(new ListSerializer<>(StringSerializer.INSTANCE))\n+\t\t\t)\n+\t\t);\n+\n+\t\tthis.client = this.hiveShim.getHiveMetastoreClient(new HiveConf(conf.conf(), HiveConf.class));\n+\n+\t\tTable hiveTable = client.getTable(tablePath.getDatabaseName(), tablePath.getObjectName());\n+\t\tthis.tableProps = HiveReflectionUtils.getTableMetadata(hiveShim, hiveTable);\n+\t\tthis.defaultPartitionName = conf.conf().get(HiveConf.ConfVars.DEFAULTPARTITIONNAME.varname,\n+\t\t\t\tHiveConf.ConfVars.DEFAULTPARTITIONNAME.defaultStrVal);\n+\n+\t\tthis.distinctPartitions = new HashSet<>();\n+\t\tif (context.isRestored()) {\n+\t\t\tLOG.info(\"Restoring state for the {}.\", getClass().getSimpleName());\n+\t\t\tthis.currentReadTime = this.currReadTimeState.get().iterator().next();\n+\t\t\tthis.distinctPartitions.addAll(this.distinctPartsState.get().iterator().next());\n+\t\t} else {\n+\t\t\tLOG.info(\"No state to restore for the {}.\", getClass().getSimpleName());\n+\t\t\tthis.currentReadTime = this.startupTimestampMillis;\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void run(SourceContext<TimestampedHiveInputSplit> context) throws Exception {\n+\t\tcheckpointLock = context.getCheckpointLock();\n+\t\twhile (isRunning) {\n+\t\t\tsynchronized (checkpointLock) {\n+\t\t\t\tmonitorAndForwardSplits(context);\n+\t\t\t}\n+\t\t\tThread.sleep(interval);\n+\t\t}\n+\t}\n+\n+\tprivate void monitorAndForwardSplits(\n+\t\t\tSourceContext<TimestampedHiveInputSplit> context) throws IOException, TException {\n+\t\tassert (Thread.holdsLock(checkpointLock));\n+\n+\t\tList<Partition> partitions = client.listPartitionsByFilter(\n+\t\t\t\ttablePath.getDatabaseName(),\n+\t\t\t\ttablePath.getObjectName(),\n+\t\t\t\tstrategy.generateFetchFilter(partitionKeys, currentReadTime),\n+\t\t\t\t(short) -1);\n+\n+\t\tif (partitions.isEmpty()) {\n+\t\t\treturn;\n+\t\t}\n+\n+\t\tlong maxTime = Long.MIN_VALUE;\n+\t\tfor (Partition partition : partitions) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4223a26640c7243a61135e803aa1dd21888114a5"}, "originalPosition": 210}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ2MjQzMQ==", "bodyText": "Good point, yes, will sort.", "url": "https://github.com/apache/flink/pull/12004#discussion_r422462431", "createdAt": "2020-05-09T07:11:36Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveContinuousMonitoringFunction.java", "diffHunk": "@@ -0,0 +1,280 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive.read;\n+\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeutils.base.ListSerializer;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.api.common.typeutils.base.StringSerializer;\n+import org.apache.flink.connectors.hive.HiveTablePartition;\n+import org.apache.flink.connectors.hive.HiveTableSource;\n+import org.apache.flink.connectors.hive.JobConfWrapper;\n+import org.apache.flink.connectors.hive.read.PartitionStrategy.PartitionStrategyFactory;\n+import org.apache.flink.runtime.state.FunctionInitializationContext;\n+import org.apache.flink.runtime.state.FunctionSnapshotContext;\n+import org.apache.flink.streaming.api.checkpoint.CheckpointedFunction;\n+import org.apache.flink.streaming.api.functions.source.ContinuousFileReaderOperator;\n+import org.apache.flink.streaming.api.functions.source.RichSourceFunction;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.catalog.hive.client.HiveShim;\n+import org.apache.flink.table.catalog.hive.util.HiveReflectionUtils;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.metastore.IMetaStoreClient;\n+import org.apache.hadoop.hive.metastore.api.Partition;\n+import org.apache.hadoop.hive.metastore.api.Table;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.thrift.TException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.Set;\n+\n+/**\n+ * This is the single (non-parallel) monitoring task which takes a {@link HiveTableInputFormat},\n+ * it is responsible for:\n+ *\n+ * <ol>\n+ *     <li>Monitoring partitions of hive meta store.</li>\n+ *     <li>Deciding which partitions should be further read and processed.</li>\n+ *     <li>Creating the {@link HiveTableInputSplit splits} corresponding to those partitions.</li>\n+ *     <li>Assigning them to downstream tasks for further processing.</li>\n+ * </ol>\n+ *\n+ * <p>The splits to be read are forwarded to the downstream {@link ContinuousFileReaderOperator}\n+ * which can have parallelism greater than one.\n+ *\n+ * <p><b>IMPORTANT NOTE: </b> Splits are forwarded downstream for reading in ascending partition time order,\n+ * based on the partition time of the partitions they belong to.\n+ */\n+public class HiveContinuousMonitoringFunction\n+\t\textends RichSourceFunction<TimestampedHiveInputSplit>\n+\t\timplements CheckpointedFunction {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(HiveContinuousMonitoringFunction.class);\n+\n+\t/** The parallelism of the downstream readers. */\n+\tprivate final int readerParallelism;\n+\n+\t/** The interval between consecutive path scans. */\n+\tprivate final long interval;\n+\n+\tprivate final HiveShim hiveShim;\n+\n+\tprivate final JobConfWrapper conf;\n+\n+\tprivate final ObjectPath tablePath;\n+\n+\tprivate final List<String> partitionKeys;\n+\n+\tprivate final String[] fieldNames;\n+\n+\tprivate final DataType[] fieldTypes;\n+\n+\tprivate final long startupTimestampMillis;\n+\n+\tprivate final PartitionStrategyFactory partStrategyFactory;\n+\n+\tprivate volatile boolean isRunning = true;\n+\n+\t/** The maximum partition read time seen so far. */\n+\tprivate volatile long currentReadTime;\n+\n+\tprivate transient PartitionStrategy strategy;\n+\n+\tprivate transient Object checkpointLock;\n+\n+\tprivate transient ListState<Long> currReadTimeState;\n+\n+\tprivate transient ListState<List<List<String>>> distinctPartsState;\n+\n+\tprivate transient IMetaStoreClient client;\n+\n+\tprivate transient Properties tableProps;\n+\n+\tprivate transient String defaultPartitionName;\n+\n+\tprivate transient Set<List<String>> distinctPartitions;\n+\n+\tpublic HiveContinuousMonitoringFunction(\n+\t\t\tHiveShim hiveShim,\n+\t\t\tJobConf conf,\n+\t\t\tObjectPath tablePath,\n+\t\t\tCatalogTable catalogTable,\n+\t\t\tlong startupTimestampMillis,\n+\t\t\tPartitionStrategyFactory partStrategyFactory,\n+\t\t\tint readerParallelism,\n+\t\t\tlong interval) {\n+\t\tthis.hiveShim = hiveShim;\n+\t\tthis.conf = new JobConfWrapper(conf);\n+\t\tthis.tablePath = tablePath;\n+\t\tthis.partitionKeys = catalogTable.getPartitionKeys();\n+\t\tthis.fieldNames = catalogTable.getSchema().getFieldNames();\n+\t\tthis.fieldTypes = catalogTable.getSchema().getFieldDataTypes();\n+\t\tthis.startupTimestampMillis = startupTimestampMillis;\n+\t\tthis.partStrategyFactory = partStrategyFactory;\n+\n+\t\tthis.interval = interval;\n+\t\tthis.readerParallelism = Math.max(readerParallelism, 1);\n+\t\tthis.currentReadTime = Long.MIN_VALUE;\n+\t}\n+\n+\t@Override\n+\tpublic void initializeState(FunctionInitializationContext context) throws Exception {\n+\t\tthis.strategy = partStrategyFactory.createStrategy(getRuntimeContext().getUserCodeClassLoader());\n+\n+\t\tthis.currReadTimeState = context.getOperatorStateStore().getListState(\n+\t\t\tnew ListStateDescriptor<>(\n+\t\t\t\t\"partition-monitoring-state\",\n+\t\t\t\tLongSerializer.INSTANCE\n+\t\t\t)\n+\t\t);\n+\t\tthis.distinctPartsState = context.getOperatorStateStore().getListState(\n+\t\t\tnew ListStateDescriptor<>(\n+\t\t\t\t\"partition-monitoring-state\",\n+\t\t\t\tnew ListSerializer<>(new ListSerializer<>(StringSerializer.INSTANCE))\n+\t\t\t)\n+\t\t);\n+\n+\t\tthis.client = this.hiveShim.getHiveMetastoreClient(new HiveConf(conf.conf(), HiveConf.class));\n+\n+\t\tTable hiveTable = client.getTable(tablePath.getDatabaseName(), tablePath.getObjectName());\n+\t\tthis.tableProps = HiveReflectionUtils.getTableMetadata(hiveShim, hiveTable);\n+\t\tthis.defaultPartitionName = conf.conf().get(HiveConf.ConfVars.DEFAULTPARTITIONNAME.varname,\n+\t\t\t\tHiveConf.ConfVars.DEFAULTPARTITIONNAME.defaultStrVal);\n+\n+\t\tthis.distinctPartitions = new HashSet<>();\n+\t\tif (context.isRestored()) {\n+\t\t\tLOG.info(\"Restoring state for the {}.\", getClass().getSimpleName());\n+\t\t\tthis.currentReadTime = this.currReadTimeState.get().iterator().next();\n+\t\t\tthis.distinctPartitions.addAll(this.distinctPartsState.get().iterator().next());\n+\t\t} else {\n+\t\t\tLOG.info(\"No state to restore for the {}.\", getClass().getSimpleName());\n+\t\t\tthis.currentReadTime = this.startupTimestampMillis;\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void run(SourceContext<TimestampedHiveInputSplit> context) throws Exception {\n+\t\tcheckpointLock = context.getCheckpointLock();\n+\t\twhile (isRunning) {\n+\t\t\tsynchronized (checkpointLock) {\n+\t\t\t\tmonitorAndForwardSplits(context);\n+\t\t\t}\n+\t\t\tThread.sleep(interval);\n+\t\t}\n+\t}\n+\n+\tprivate void monitorAndForwardSplits(\n+\t\t\tSourceContext<TimestampedHiveInputSplit> context) throws IOException, TException {\n+\t\tassert (Thread.holdsLock(checkpointLock));\n+\n+\t\tList<Partition> partitions = client.listPartitionsByFilter(\n+\t\t\t\ttablePath.getDatabaseName(),\n+\t\t\t\ttablePath.getObjectName(),\n+\t\t\t\tstrategy.generateFetchFilter(partitionKeys, currentReadTime),\n+\t\t\t\t(short) -1);\n+\n+\t\tif (partitions.isEmpty()) {\n+\t\t\treturn;\n+\t\t}\n+\n+\t\tlong maxTime = Long.MIN_VALUE;\n+\t\tfor (Partition partition : partitions) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDc4ODY4Ng=="}, "originalCommit": {"oid": "4223a26640c7243a61135e803aa1dd21888114a5"}, "originalPosition": 210}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYyMjE1NjY1OnYy", "diffSide": "RIGHT", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveTableInputFormat.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wN1QwMjo0Mzo0MVrOGRsYlg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMVQwMjozMTozNVrOGTKgYA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTIwNjE2Ng==", "bodyText": "What's the difference between close and closeInputFormat? And can we have some comments explaining why we don't have to do anything in close?", "url": "https://github.com/apache/flink/pull/12004#discussion_r421206166", "createdAt": "2020-05-07T02:43:41Z", "author": {"login": "lirui-apache"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveTableInputFormat.java", "diffHunk": "@@ -248,7 +263,11 @@ public RowData nextRecord(RowData reuse) throws IOException {\n \t}\n \n \t@Override\n-\tpublic void close() throws IOException {\n+\tpublic void close() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4223a26640c7243a61135e803aa1dd21888114a5"}, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjc0ODI1Ng==", "bodyText": "I am wrong, should in close.", "url": "https://github.com/apache/flink/pull/12004#discussion_r422748256", "createdAt": "2020-05-11T02:31:35Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveTableInputFormat.java", "diffHunk": "@@ -248,7 +263,11 @@ public RowData nextRecord(RowData reuse) throws IOException {\n \t}\n \n \t@Override\n-\tpublic void close() throws IOException {\n+\tpublic void close() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTIwNjE2Ng=="}, "originalCommit": {"oid": "4223a26640c7243a61135e803aa1dd21888114a5"}, "originalPosition": 68}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYyNzkzNDQ4OnYy", "diffSide": "RIGHT", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSource.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOFQxMjoxMzo1MlrOGSje6w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOFQxMjoxMzo1MlrOGSje6w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjEwODkwNw==", "bodyText": "not -> does not .", "url": "https://github.com/apache/flink/pull/12004#discussion_r422108907", "createdAt": "2020-05-08T12:13:52Z", "author": {"login": "openinx"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSource.java", "diffHunk": "@@ -150,6 +164,25 @@ public boolean isBounded() {\n \t\t\t\tallHivePartitions,\n \t\t\t\tflinkConf.get(HiveOptions.TABLE_EXEC_HIVE_FALLBACK_MAPRED_READER));\n \n+\t\tif (isStreamingSource()) {\n+\t\t\tif (catalogTable.getPartitionKeys().isEmpty()) {\n+\t\t\t\tthrow new UnsupportedOperationException(\n+\t\t\t\t\t\t\"Non-partition table not support streaming read now.\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "42953325bb1d6a447ef53e020d3cc311a8c986c6"}, "originalPosition": 62}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYyODAxODgyOnYy", "diffSide": "RIGHT", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSource.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOFQxMjo0NDo1M1rOGSkRSQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQwNzoxNDo1MVrOGS5FBw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjEyMTgwMQ==", "bodyText": "I've one question here, what's the parallelism of the ContinuousFileReaderOperator will be ?   should we use the default  parallelism in ExecutionConfig ?\nOK, seems the InputFormat will generate the expected parallelism...", "url": "https://github.com/apache/flink/pull/12004#discussion_r422121801", "createdAt": "2020-05-08T12:44:53Z", "author": {"login": "openinx"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSource.java", "diffHunk": "@@ -181,6 +214,47 @@ public boolean isBounded() {\n \t\treturn source.name(explainSource());\n \t}\n \n+\tprivate DataStream<RowData> createStreamSource(\n+\t\t\tStreamExecutionEnvironment execEnv,\n+\t\t\tTypeInformation<RowData> typeInfo,\n+\t\t\tHiveTableInputFormat inputFormat) {\n+\t\tfinal Map<String, String> properties = catalogTable.getProperties();\n+\t\tPartitionFetcherFactory strategyFactory = cl -> PartitionFetcher.createStrategy(\n+\t\t\t\tproperties.get(HIVE_STREAMING_SOURCE_PARTITION_STRATEGY.key()),\n+\t\t\t\tproperties.get(HIVE_STREAMING_SOURCE_PARTITION_STRATEGY_CLASS.key()),\n+\t\t\t\tcl);\n+\n+\t\tString monitorIntervalStr = properties.get(HIVE_STREAMING_SOURCE_MONITOR_INTERVAL.key());\n+\t\tDuration monitorInterval = monitorIntervalStr != null ?\n+\t\t\t\tTimeUtils.parseDuration(monitorIntervalStr) :\n+\t\t\t\tHIVE_STREAMING_SOURCE_MONITOR_INTERVAL.defaultValue();\n+\n+\t\tHiveContinuousMonitoringFunction monitoringFunction = new HiveContinuousMonitoringFunction(\n+\t\t\t\thiveShim,\n+\t\t\t\tjobConf,\n+\t\t\t\ttablePath,\n+\t\t\t\tcatalogTable,\n+\t\t\t\tgetStartupPartition(),\n+\t\t\t\tstrategyFactory,\n+\t\t\t\texecEnv.getParallelism(),\n+\t\t\t\tmonitorInterval.toMillis());\n+\n+\t\tContinuousFileReaderOperatorFactory<RowData, TimestampedHiveInputSplit> factory =\n+\t\t\t\tnew ContinuousFileReaderOperatorFactory<>(inputFormat);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "42953325bb1d6a447ef53e020d3cc311a8c986c6"}, "originalPosition": 111}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjEyNDMyNQ==", "bodyText": "Besides  seems we don't do any split inside the file which may introduce the unbalanced data consuming in the upstream operator ?  Iceberg seems handle this well because it will split the larger data file into small balanced tasks  and dispatch to the executing task..", "url": "https://github.com/apache/flink/pull/12004#discussion_r422124325", "createdAt": "2020-05-08T12:50:25Z", "author": {"login": "openinx"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSource.java", "diffHunk": "@@ -181,6 +214,47 @@ public boolean isBounded() {\n \t\treturn source.name(explainSource());\n \t}\n \n+\tprivate DataStream<RowData> createStreamSource(\n+\t\t\tStreamExecutionEnvironment execEnv,\n+\t\t\tTypeInformation<RowData> typeInfo,\n+\t\t\tHiveTableInputFormat inputFormat) {\n+\t\tfinal Map<String, String> properties = catalogTable.getProperties();\n+\t\tPartitionFetcherFactory strategyFactory = cl -> PartitionFetcher.createStrategy(\n+\t\t\t\tproperties.get(HIVE_STREAMING_SOURCE_PARTITION_STRATEGY.key()),\n+\t\t\t\tproperties.get(HIVE_STREAMING_SOURCE_PARTITION_STRATEGY_CLASS.key()),\n+\t\t\t\tcl);\n+\n+\t\tString monitorIntervalStr = properties.get(HIVE_STREAMING_SOURCE_MONITOR_INTERVAL.key());\n+\t\tDuration monitorInterval = monitorIntervalStr != null ?\n+\t\t\t\tTimeUtils.parseDuration(monitorIntervalStr) :\n+\t\t\t\tHIVE_STREAMING_SOURCE_MONITOR_INTERVAL.defaultValue();\n+\n+\t\tHiveContinuousMonitoringFunction monitoringFunction = new HiveContinuousMonitoringFunction(\n+\t\t\t\thiveShim,\n+\t\t\t\tjobConf,\n+\t\t\t\ttablePath,\n+\t\t\t\tcatalogTable,\n+\t\t\t\tgetStartupPartition(),\n+\t\t\t\tstrategyFactory,\n+\t\t\t\texecEnv.getParallelism(),\n+\t\t\t\tmonitorInterval.toMillis());\n+\n+\t\tContinuousFileReaderOperatorFactory<RowData, TimestampedHiveInputSplit> factory =\n+\t\t\t\tnew ContinuousFileReaderOperatorFactory<>(inputFormat);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjEyMTgwMQ=="}, "originalCommit": {"oid": "42953325bb1d6a447ef53e020d3cc311a8c986c6"}, "originalPosition": 111}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ2MjcyNw==", "bodyText": "we don't do any split inside the file\n\nWe will split one file to many splits.", "url": "https://github.com/apache/flink/pull/12004#discussion_r422462727", "createdAt": "2020-05-09T07:14:51Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSource.java", "diffHunk": "@@ -181,6 +214,47 @@ public boolean isBounded() {\n \t\treturn source.name(explainSource());\n \t}\n \n+\tprivate DataStream<RowData> createStreamSource(\n+\t\t\tStreamExecutionEnvironment execEnv,\n+\t\t\tTypeInformation<RowData> typeInfo,\n+\t\t\tHiveTableInputFormat inputFormat) {\n+\t\tfinal Map<String, String> properties = catalogTable.getProperties();\n+\t\tPartitionFetcherFactory strategyFactory = cl -> PartitionFetcher.createStrategy(\n+\t\t\t\tproperties.get(HIVE_STREAMING_SOURCE_PARTITION_STRATEGY.key()),\n+\t\t\t\tproperties.get(HIVE_STREAMING_SOURCE_PARTITION_STRATEGY_CLASS.key()),\n+\t\t\t\tcl);\n+\n+\t\tString monitorIntervalStr = properties.get(HIVE_STREAMING_SOURCE_MONITOR_INTERVAL.key());\n+\t\tDuration monitorInterval = monitorIntervalStr != null ?\n+\t\t\t\tTimeUtils.parseDuration(monitorIntervalStr) :\n+\t\t\t\tHIVE_STREAMING_SOURCE_MONITOR_INTERVAL.defaultValue();\n+\n+\t\tHiveContinuousMonitoringFunction monitoringFunction = new HiveContinuousMonitoringFunction(\n+\t\t\t\thiveShim,\n+\t\t\t\tjobConf,\n+\t\t\t\ttablePath,\n+\t\t\t\tcatalogTable,\n+\t\t\t\tgetStartupPartition(),\n+\t\t\t\tstrategyFactory,\n+\t\t\t\texecEnv.getParallelism(),\n+\t\t\t\tmonitorInterval.toMillis());\n+\n+\t\tContinuousFileReaderOperatorFactory<RowData, TimestampedHiveInputSplit> factory =\n+\t\t\t\tnew ContinuousFileReaderOperatorFactory<>(inputFormat);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjEyMTgwMQ=="}, "originalCommit": {"oid": "42953325bb1d6a447ef53e020d3cc311a8c986c6"}, "originalPosition": 111}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYyODA1ODg3OnYy", "diffSide": "RIGHT", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveContinuousMonitoringFunction.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOFQxMjo1ODoxNFrOGSkpFg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOFQxMjo1ODoxNFrOGSkpFg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjEyNzg5NA==", "bodyText": "Seems the useless imported class can be removed", "url": "https://github.com/apache/flink/pull/12004#discussion_r422127894", "createdAt": "2020-05-08T12:58:14Z", "author": {"login": "openinx"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveContinuousMonitoringFunction.java", "diffHunk": "@@ -0,0 +1,331 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive.read;\n+\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeutils.base.ListSerializer;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.api.common.typeutils.base.StringSerializer;\n+import org.apache.flink.connectors.hive.HiveTablePartition;\n+import org.apache.flink.connectors.hive.HiveTableSource;\n+import org.apache.flink.connectors.hive.JobConfWrapper;\n+import org.apache.flink.connectors.hive.read.PartitionFetcher.PartitionFetcherFactory;\n+import org.apache.flink.runtime.state.FunctionInitializationContext;\n+import org.apache.flink.runtime.state.FunctionSnapshotContext;\n+import org.apache.flink.streaming.api.checkpoint.CheckpointedFunction;\n+import org.apache.flink.streaming.api.functions.source.ContinuousFileReaderOperator;\n+import org.apache.flink.streaming.api.functions.source.RichSourceFunction;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.catalog.hive.client.HiveShim;\n+import org.apache.flink.table.catalog.hive.util.HiveReflectionUtils;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.metastore.IMetaStoreClient;\n+import org.apache.hadoop.hive.metastore.api.Partition;\n+import org.apache.hadoop.hive.metastore.api.Table;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.thrift.TException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "42953325bb1d6a447ef53e020d3cc311a8c986c6"}, "originalPosition": 52}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYyODE0NDU0OnYy", "diffSide": "RIGHT", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveContinuousMonitoringFunction.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOFQxMzoyNTo0NVrOGSldBg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQwNzoxMzo1NFrOGS5ErQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjE0MTE5MA==", "bodyText": "Just ask the similar question as @lirui-apache said,   if the partitions is not sorted by timestamp ascending, then it seems have problems,  say we have partitions with timestamp like [(p0, 5), (p1, 1), (p2, 3)] ( the first part of (p0, 5) is partition name and the second is timestamp ),   now we have emitted the timestamp=5 successfully, then we will update the currentReadTime = 5   I think. Once the operator restored, we will just start from the timestamp = 5 to read the following partitions...it mistakenly skipped other timestamps such as 1 and 3 , finally we will lost part of the data to consume....\nPls correct me if I'm wrong.", "url": "https://github.com/apache/flink/pull/12004#discussion_r422141190", "createdAt": "2020-05-08T13:25:45Z", "author": {"login": "openinx"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveContinuousMonitoringFunction.java", "diffHunk": "@@ -0,0 +1,331 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive.read;\n+\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeutils.base.ListSerializer;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.api.common.typeutils.base.StringSerializer;\n+import org.apache.flink.connectors.hive.HiveTablePartition;\n+import org.apache.flink.connectors.hive.HiveTableSource;\n+import org.apache.flink.connectors.hive.JobConfWrapper;\n+import org.apache.flink.connectors.hive.read.PartitionFetcher.PartitionFetcherFactory;\n+import org.apache.flink.runtime.state.FunctionInitializationContext;\n+import org.apache.flink.runtime.state.FunctionSnapshotContext;\n+import org.apache.flink.streaming.api.checkpoint.CheckpointedFunction;\n+import org.apache.flink.streaming.api.functions.source.ContinuousFileReaderOperator;\n+import org.apache.flink.streaming.api.functions.source.RichSourceFunction;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.catalog.hive.client.HiveShim;\n+import org.apache.flink.table.catalog.hive.util.HiveReflectionUtils;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.metastore.IMetaStoreClient;\n+import org.apache.hadoop.hive.metastore.api.Partition;\n+import org.apache.hadoop.hive.metastore.api.Table;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.thrift.TException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.Set;\n+\n+import static org.apache.flink.table.filesystem.PartitionPathUtils.escapePathName;\n+\n+/**\n+ * This is the single (non-parallel) monitoring task which takes a {@link HiveTableInputFormat},\n+ * it is responsible for:\n+ *\n+ * <ol>\n+ *     <li>Monitoring partitions of hive meta store.</li>\n+ *     <li>Deciding which partitions should be further read and processed.</li>\n+ *     <li>Creating the {@link HiveTableInputSplit splits} corresponding to those partitions.</li>\n+ *     <li>Assigning them to downstream tasks for further processing.</li>\n+ * </ol>\n+ *\n+ * <p>The splits to be read are forwarded to the downstream {@link ContinuousFileReaderOperator}\n+ * which can have parallelism greater than one.\n+ *\n+ * <p><b>IMPORTANT NOTE: </b> Splits are forwarded downstream for reading in ascending partition time order,\n+ * based on the partition time of the partitions they belong to.\n+ */\n+public class HiveContinuousMonitoringFunction\n+\t\textends RichSourceFunction<TimestampedHiveInputSplit>\n+\t\timplements CheckpointedFunction {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(HiveContinuousMonitoringFunction.class);\n+\n+\t/** The parallelism of the downstream readers. */\n+\tprivate final int readerParallelism;\n+\n+\t/** The interval between consecutive path scans. */\n+\tprivate final long interval;\n+\n+\tprivate final HiveShim hiveShim;\n+\n+\tprivate final JobConfWrapper conf;\n+\n+\tprivate final ObjectPath tablePath;\n+\n+\tprivate final List<String> partitionKeys;\n+\n+\tprivate final String[] fieldNames;\n+\n+\tprivate final DataType[] fieldTypes;\n+\n+\tprivate final String startupPartition;\n+\n+\tprivate final PartitionFetcherFactory fetcherFactory;\n+\n+\tprivate volatile boolean isRunning = true;\n+\n+\t/** The maximum partition read time seen so far. */\n+\tprivate volatile long currentReadTime;\n+\n+\tprivate transient PartitionFetcher.Context fetcherContext;\n+\n+\tprivate transient PartitionFetcher fetcher;\n+\n+\tprivate transient Object checkpointLock;\n+\n+\tprivate transient ListState<Long> currReadTimeState;\n+\n+\tprivate transient ListState<List<List<String>>> distinctPartsState;\n+\n+\tprivate transient IMetaStoreClient client;\n+\n+\tprivate transient Properties tableProps;\n+\n+\tprivate transient String defaultPartitionName;\n+\n+\tprivate transient Set<List<String>> distinctPartitions;\n+\n+\tpublic HiveContinuousMonitoringFunction(\n+\t\t\tHiveShim hiveShim,\n+\t\t\tJobConf conf,\n+\t\t\tObjectPath tablePath,\n+\t\t\tCatalogTable catalogTable,\n+\t\t\tString startupPartition,\n+\t\t\tPartitionFetcherFactory fetcherFactory,\n+\t\t\tint readerParallelism,\n+\t\t\tlong interval) {\n+\t\tthis.hiveShim = hiveShim;\n+\t\tthis.conf = new JobConfWrapper(conf);\n+\t\tthis.tablePath = tablePath;\n+\t\tthis.partitionKeys = catalogTable.getPartitionKeys();\n+\t\tthis.fieldNames = catalogTable.getSchema().getFieldNames();\n+\t\tthis.fieldTypes = catalogTable.getSchema().getFieldDataTypes();\n+\t\tthis.startupPartition = startupPartition;\n+\t\tthis.fetcherFactory = fetcherFactory;\n+\n+\t\tthis.interval = interval;\n+\t\tthis.readerParallelism = Math.max(readerParallelism, 1);\n+\t\tthis.currentReadTime = 0;\n+\t}\n+\n+\t@Override\n+\tpublic void initializeState(FunctionInitializationContext context) throws Exception {\n+\t\tthis.currReadTimeState = context.getOperatorStateStore().getListState(\n+\t\t\tnew ListStateDescriptor<>(\n+\t\t\t\t\"partition-monitoring-state\",\n+\t\t\t\tLongSerializer.INSTANCE\n+\t\t\t)\n+\t\t);\n+\t\tthis.distinctPartsState = context.getOperatorStateStore().getListState(\n+\t\t\tnew ListStateDescriptor<>(\n+\t\t\t\t\"partition-monitoring-state\",\n+\t\t\t\tnew ListSerializer<>(new ListSerializer<>(StringSerializer.INSTANCE))\n+\t\t\t)\n+\t\t);\n+\n+\t\tthis.client = this.hiveShim.getHiveMetastoreClient(new HiveConf(conf.conf(), HiveConf.class));\n+\n+\t\tTable hiveTable = client.getTable(tablePath.getDatabaseName(), tablePath.getObjectName());\n+\t\tthis.tableProps = HiveReflectionUtils.getTableMetadata(hiveShim, hiveTable);\n+\t\tthis.defaultPartitionName = conf.conf().get(HiveConf.ConfVars.DEFAULTPARTITIONNAME.varname,\n+\t\t\t\tHiveConf.ConfVars.DEFAULTPARTITIONNAME.defaultStrVal);\n+\n+\t\tthis.fetcher = fetcherFactory.createStrategy(getRuntimeContext().getUserCodeClassLoader());\n+\n+\t\tPath location = new Path(hiveTable.getSd().getLocation());\n+\t\tFileSystem fs = location.getFileSystem(conf.conf());\n+\t\tthis.fetcherContext = new PartitionFetcher.Context() {\n+\n+\t\t\t@Override\n+\t\t\tpublic List<String> partitionKeys() {\n+\t\t\t\treturn partitionKeys;\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic Partition getPartition(String escapeName) throws TException {\n+\t\t\t\treturn client.getPartition(\n+\t\t\t\t\t\ttablePath.getDatabaseName(),\n+\t\t\t\t\t\ttablePath.getObjectName(),\n+\t\t\t\t\t\tescapeName);\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic List<Partition> listPartitionsByFilter(String filter) throws TException {\n+\t\t\t\treturn client.listPartitionsByFilter(\n+\t\t\t\t\t\ttablePath.getDatabaseName(),\n+\t\t\t\t\t\ttablePath.getObjectName(),\n+\t\t\t\t\t\tfilter,\n+\t\t\t\t\t\t(short) -1);\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic FileSystem fileSystem() {\n+\t\t\t\treturn fs;\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic Path tableLocation() {\n+\t\t\t\treturn new Path(hiveTable.getSd().getLocation());\n+\t\t\t}\n+\t\t};\n+\n+\t\tthis.distinctPartitions = new HashSet<>();\n+\t\tif (context.isRestored()) {\n+\t\t\tLOG.info(\"Restoring state for the {}.\", getClass().getSimpleName());\n+\t\t\tthis.currentReadTime = this.currReadTimeState.get().iterator().next();\n+\t\t\tthis.distinctPartitions.addAll(this.distinctPartsState.get().iterator().next());\n+\t\t} else {\n+\t\t\tLOG.info(\"No state to restore for the {}.\", getClass().getSimpleName());\n+\t\t\tif (startupPartition != null) {\n+\t\t\t\tthis.currentReadTime = fetcher.extractTimestamp(\n+\t\t\t\t\t\tfetcherContext,\n+\t\t\t\t\t\tclient.getPartition(\n+\t\t\t\t\t\t\t\ttablePath.getDatabaseName(),\n+\t\t\t\t\t\t\t\ttablePath.getObjectName(),\n+\t\t\t\t\t\t\t\tescapePathName(startupPartition)));\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void run(SourceContext<TimestampedHiveInputSplit> context) throws Exception {\n+\t\tcheckpointLock = context.getCheckpointLock();\n+\t\twhile (isRunning) {\n+\t\t\tsynchronized (checkpointLock) {\n+\t\t\t\tmonitorAndForwardSplits(context);\n+\t\t\t}\n+\t\t\tThread.sleep(interval);\n+\t\t}\n+\t}\n+\n+\tprivate void monitorAndForwardSplits(\n+\t\t\tSourceContext<TimestampedHiveInputSplit> context) throws Exception {\n+\t\tassert (Thread.holdsLock(checkpointLock));\n+\n+\t\tList<Partition> partitions = fetcher.fetchPartitions(fetcherContext, currentReadTime);\n+\n+\t\tif (partitions.isEmpty()) {\n+\t\t\treturn;\n+\t\t}\n+\n+\t\tlong maxTimestamp = Long.MIN_VALUE;\n+\t\tSet<List<String>> nextDistinctParts = new HashSet<>();\n+\t\tfor (Partition partition : partitions) {\n+\t\t\tList<String> partSpec = partition.getValues();\n+\t\t\tif (!this.distinctPartitions.contains(partSpec)) {\n+\t\t\t\tthis.distinctPartitions.add(partition.getValues());\n+\t\t\t\tlong timestamp = this.fetcher.extractTimestamp(fetcherContext, partition);\n+\t\t\t\tif (timestamp > currentReadTime) {\n+\t\t\t\t\tnextDistinctParts.add(partSpec);\n+\t\t\t\t}\n+\t\t\t\tif (timestamp > maxTimestamp) {\n+\t\t\t\t\tmaxTimestamp = timestamp;\n+\t\t\t\t}\n+\t\t\t\tHiveTableInputSplit[] splits = HiveTableInputFormat.createInputSplits(\n+\t\t\t\t\t\tthis.readerParallelism,\n+\t\t\t\t\t\tCollections.singletonList(toHiveTablePartition(partition)),\n+\t\t\t\t\t\tthis.conf.conf());\n+\t\t\t\tfor (HiveTableInputSplit split : splits) {\n+\t\t\t\t\tcontext.collect(new TimestampedHiveInputSplit(timestamp, split));\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\n+\t\tif (maxTimestamp > currentReadTime) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "42953325bb1d6a447ef53e020d3cc311a8c986c6"}, "originalPosition": 278}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ2MjUyNw==", "bodyText": "First, will sort this list.", "url": "https://github.com/apache/flink/pull/12004#discussion_r422462527", "createdAt": "2020-05-09T07:12:52Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveContinuousMonitoringFunction.java", "diffHunk": "@@ -0,0 +1,331 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive.read;\n+\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeutils.base.ListSerializer;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.api.common.typeutils.base.StringSerializer;\n+import org.apache.flink.connectors.hive.HiveTablePartition;\n+import org.apache.flink.connectors.hive.HiveTableSource;\n+import org.apache.flink.connectors.hive.JobConfWrapper;\n+import org.apache.flink.connectors.hive.read.PartitionFetcher.PartitionFetcherFactory;\n+import org.apache.flink.runtime.state.FunctionInitializationContext;\n+import org.apache.flink.runtime.state.FunctionSnapshotContext;\n+import org.apache.flink.streaming.api.checkpoint.CheckpointedFunction;\n+import org.apache.flink.streaming.api.functions.source.ContinuousFileReaderOperator;\n+import org.apache.flink.streaming.api.functions.source.RichSourceFunction;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.catalog.hive.client.HiveShim;\n+import org.apache.flink.table.catalog.hive.util.HiveReflectionUtils;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.metastore.IMetaStoreClient;\n+import org.apache.hadoop.hive.metastore.api.Partition;\n+import org.apache.hadoop.hive.metastore.api.Table;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.thrift.TException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.Set;\n+\n+import static org.apache.flink.table.filesystem.PartitionPathUtils.escapePathName;\n+\n+/**\n+ * This is the single (non-parallel) monitoring task which takes a {@link HiveTableInputFormat},\n+ * it is responsible for:\n+ *\n+ * <ol>\n+ *     <li>Monitoring partitions of hive meta store.</li>\n+ *     <li>Deciding which partitions should be further read and processed.</li>\n+ *     <li>Creating the {@link HiveTableInputSplit splits} corresponding to those partitions.</li>\n+ *     <li>Assigning them to downstream tasks for further processing.</li>\n+ * </ol>\n+ *\n+ * <p>The splits to be read are forwarded to the downstream {@link ContinuousFileReaderOperator}\n+ * which can have parallelism greater than one.\n+ *\n+ * <p><b>IMPORTANT NOTE: </b> Splits are forwarded downstream for reading in ascending partition time order,\n+ * based on the partition time of the partitions they belong to.\n+ */\n+public class HiveContinuousMonitoringFunction\n+\t\textends RichSourceFunction<TimestampedHiveInputSplit>\n+\t\timplements CheckpointedFunction {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(HiveContinuousMonitoringFunction.class);\n+\n+\t/** The parallelism of the downstream readers. */\n+\tprivate final int readerParallelism;\n+\n+\t/** The interval between consecutive path scans. */\n+\tprivate final long interval;\n+\n+\tprivate final HiveShim hiveShim;\n+\n+\tprivate final JobConfWrapper conf;\n+\n+\tprivate final ObjectPath tablePath;\n+\n+\tprivate final List<String> partitionKeys;\n+\n+\tprivate final String[] fieldNames;\n+\n+\tprivate final DataType[] fieldTypes;\n+\n+\tprivate final String startupPartition;\n+\n+\tprivate final PartitionFetcherFactory fetcherFactory;\n+\n+\tprivate volatile boolean isRunning = true;\n+\n+\t/** The maximum partition read time seen so far. */\n+\tprivate volatile long currentReadTime;\n+\n+\tprivate transient PartitionFetcher.Context fetcherContext;\n+\n+\tprivate transient PartitionFetcher fetcher;\n+\n+\tprivate transient Object checkpointLock;\n+\n+\tprivate transient ListState<Long> currReadTimeState;\n+\n+\tprivate transient ListState<List<List<String>>> distinctPartsState;\n+\n+\tprivate transient IMetaStoreClient client;\n+\n+\tprivate transient Properties tableProps;\n+\n+\tprivate transient String defaultPartitionName;\n+\n+\tprivate transient Set<List<String>> distinctPartitions;\n+\n+\tpublic HiveContinuousMonitoringFunction(\n+\t\t\tHiveShim hiveShim,\n+\t\t\tJobConf conf,\n+\t\t\tObjectPath tablePath,\n+\t\t\tCatalogTable catalogTable,\n+\t\t\tString startupPartition,\n+\t\t\tPartitionFetcherFactory fetcherFactory,\n+\t\t\tint readerParallelism,\n+\t\t\tlong interval) {\n+\t\tthis.hiveShim = hiveShim;\n+\t\tthis.conf = new JobConfWrapper(conf);\n+\t\tthis.tablePath = tablePath;\n+\t\tthis.partitionKeys = catalogTable.getPartitionKeys();\n+\t\tthis.fieldNames = catalogTable.getSchema().getFieldNames();\n+\t\tthis.fieldTypes = catalogTable.getSchema().getFieldDataTypes();\n+\t\tthis.startupPartition = startupPartition;\n+\t\tthis.fetcherFactory = fetcherFactory;\n+\n+\t\tthis.interval = interval;\n+\t\tthis.readerParallelism = Math.max(readerParallelism, 1);\n+\t\tthis.currentReadTime = 0;\n+\t}\n+\n+\t@Override\n+\tpublic void initializeState(FunctionInitializationContext context) throws Exception {\n+\t\tthis.currReadTimeState = context.getOperatorStateStore().getListState(\n+\t\t\tnew ListStateDescriptor<>(\n+\t\t\t\t\"partition-monitoring-state\",\n+\t\t\t\tLongSerializer.INSTANCE\n+\t\t\t)\n+\t\t);\n+\t\tthis.distinctPartsState = context.getOperatorStateStore().getListState(\n+\t\t\tnew ListStateDescriptor<>(\n+\t\t\t\t\"partition-monitoring-state\",\n+\t\t\t\tnew ListSerializer<>(new ListSerializer<>(StringSerializer.INSTANCE))\n+\t\t\t)\n+\t\t);\n+\n+\t\tthis.client = this.hiveShim.getHiveMetastoreClient(new HiveConf(conf.conf(), HiveConf.class));\n+\n+\t\tTable hiveTable = client.getTable(tablePath.getDatabaseName(), tablePath.getObjectName());\n+\t\tthis.tableProps = HiveReflectionUtils.getTableMetadata(hiveShim, hiveTable);\n+\t\tthis.defaultPartitionName = conf.conf().get(HiveConf.ConfVars.DEFAULTPARTITIONNAME.varname,\n+\t\t\t\tHiveConf.ConfVars.DEFAULTPARTITIONNAME.defaultStrVal);\n+\n+\t\tthis.fetcher = fetcherFactory.createStrategy(getRuntimeContext().getUserCodeClassLoader());\n+\n+\t\tPath location = new Path(hiveTable.getSd().getLocation());\n+\t\tFileSystem fs = location.getFileSystem(conf.conf());\n+\t\tthis.fetcherContext = new PartitionFetcher.Context() {\n+\n+\t\t\t@Override\n+\t\t\tpublic List<String> partitionKeys() {\n+\t\t\t\treturn partitionKeys;\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic Partition getPartition(String escapeName) throws TException {\n+\t\t\t\treturn client.getPartition(\n+\t\t\t\t\t\ttablePath.getDatabaseName(),\n+\t\t\t\t\t\ttablePath.getObjectName(),\n+\t\t\t\t\t\tescapeName);\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic List<Partition> listPartitionsByFilter(String filter) throws TException {\n+\t\t\t\treturn client.listPartitionsByFilter(\n+\t\t\t\t\t\ttablePath.getDatabaseName(),\n+\t\t\t\t\t\ttablePath.getObjectName(),\n+\t\t\t\t\t\tfilter,\n+\t\t\t\t\t\t(short) -1);\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic FileSystem fileSystem() {\n+\t\t\t\treturn fs;\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic Path tableLocation() {\n+\t\t\t\treturn new Path(hiveTable.getSd().getLocation());\n+\t\t\t}\n+\t\t};\n+\n+\t\tthis.distinctPartitions = new HashSet<>();\n+\t\tif (context.isRestored()) {\n+\t\t\tLOG.info(\"Restoring state for the {}.\", getClass().getSimpleName());\n+\t\t\tthis.currentReadTime = this.currReadTimeState.get().iterator().next();\n+\t\t\tthis.distinctPartitions.addAll(this.distinctPartsState.get().iterator().next());\n+\t\t} else {\n+\t\t\tLOG.info(\"No state to restore for the {}.\", getClass().getSimpleName());\n+\t\t\tif (startupPartition != null) {\n+\t\t\t\tthis.currentReadTime = fetcher.extractTimestamp(\n+\t\t\t\t\t\tfetcherContext,\n+\t\t\t\t\t\tclient.getPartition(\n+\t\t\t\t\t\t\t\ttablePath.getDatabaseName(),\n+\t\t\t\t\t\t\t\ttablePath.getObjectName(),\n+\t\t\t\t\t\t\t\tescapePathName(startupPartition)));\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void run(SourceContext<TimestampedHiveInputSplit> context) throws Exception {\n+\t\tcheckpointLock = context.getCheckpointLock();\n+\t\twhile (isRunning) {\n+\t\t\tsynchronized (checkpointLock) {\n+\t\t\t\tmonitorAndForwardSplits(context);\n+\t\t\t}\n+\t\t\tThread.sleep(interval);\n+\t\t}\n+\t}\n+\n+\tprivate void monitorAndForwardSplits(\n+\t\t\tSourceContext<TimestampedHiveInputSplit> context) throws Exception {\n+\t\tassert (Thread.holdsLock(checkpointLock));\n+\n+\t\tList<Partition> partitions = fetcher.fetchPartitions(fetcherContext, currentReadTime);\n+\n+\t\tif (partitions.isEmpty()) {\n+\t\t\treturn;\n+\t\t}\n+\n+\t\tlong maxTimestamp = Long.MIN_VALUE;\n+\t\tSet<List<String>> nextDistinctParts = new HashSet<>();\n+\t\tfor (Partition partition : partitions) {\n+\t\t\tList<String> partSpec = partition.getValues();\n+\t\t\tif (!this.distinctPartitions.contains(partSpec)) {\n+\t\t\t\tthis.distinctPartitions.add(partition.getValues());\n+\t\t\t\tlong timestamp = this.fetcher.extractTimestamp(fetcherContext, partition);\n+\t\t\t\tif (timestamp > currentReadTime) {\n+\t\t\t\t\tnextDistinctParts.add(partSpec);\n+\t\t\t\t}\n+\t\t\t\tif (timestamp > maxTimestamp) {\n+\t\t\t\t\tmaxTimestamp = timestamp;\n+\t\t\t\t}\n+\t\t\t\tHiveTableInputSplit[] splits = HiveTableInputFormat.createInputSplits(\n+\t\t\t\t\t\tthis.readerParallelism,\n+\t\t\t\t\t\tCollections.singletonList(toHiveTablePartition(partition)),\n+\t\t\t\t\t\tthis.conf.conf());\n+\t\t\t\tfor (HiveTableInputSplit split : splits) {\n+\t\t\t\t\tcontext.collect(new TimestampedHiveInputSplit(timestamp, split));\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\n+\t\tif (maxTimestamp > currentReadTime) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjE0MTE5MA=="}, "originalCommit": {"oid": "42953325bb1d6a447ef53e020d3cc311a8c986c6"}, "originalPosition": 278}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ2MjYzNw==", "bodyText": "we will lost part of the data to consume\n\nNo, these partitions will be all emitted to downstream, there is no checkpoint in them.", "url": "https://github.com/apache/flink/pull/12004#discussion_r422462637", "createdAt": "2020-05-09T07:13:54Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveContinuousMonitoringFunction.java", "diffHunk": "@@ -0,0 +1,331 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive.read;\n+\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeutils.base.ListSerializer;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.api.common.typeutils.base.StringSerializer;\n+import org.apache.flink.connectors.hive.HiveTablePartition;\n+import org.apache.flink.connectors.hive.HiveTableSource;\n+import org.apache.flink.connectors.hive.JobConfWrapper;\n+import org.apache.flink.connectors.hive.read.PartitionFetcher.PartitionFetcherFactory;\n+import org.apache.flink.runtime.state.FunctionInitializationContext;\n+import org.apache.flink.runtime.state.FunctionSnapshotContext;\n+import org.apache.flink.streaming.api.checkpoint.CheckpointedFunction;\n+import org.apache.flink.streaming.api.functions.source.ContinuousFileReaderOperator;\n+import org.apache.flink.streaming.api.functions.source.RichSourceFunction;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.catalog.hive.client.HiveShim;\n+import org.apache.flink.table.catalog.hive.util.HiveReflectionUtils;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.metastore.IMetaStoreClient;\n+import org.apache.hadoop.hive.metastore.api.Partition;\n+import org.apache.hadoop.hive.metastore.api.Table;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.thrift.TException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.Set;\n+\n+import static org.apache.flink.table.filesystem.PartitionPathUtils.escapePathName;\n+\n+/**\n+ * This is the single (non-parallel) monitoring task which takes a {@link HiveTableInputFormat},\n+ * it is responsible for:\n+ *\n+ * <ol>\n+ *     <li>Monitoring partitions of hive meta store.</li>\n+ *     <li>Deciding which partitions should be further read and processed.</li>\n+ *     <li>Creating the {@link HiveTableInputSplit splits} corresponding to those partitions.</li>\n+ *     <li>Assigning them to downstream tasks for further processing.</li>\n+ * </ol>\n+ *\n+ * <p>The splits to be read are forwarded to the downstream {@link ContinuousFileReaderOperator}\n+ * which can have parallelism greater than one.\n+ *\n+ * <p><b>IMPORTANT NOTE: </b> Splits are forwarded downstream for reading in ascending partition time order,\n+ * based on the partition time of the partitions they belong to.\n+ */\n+public class HiveContinuousMonitoringFunction\n+\t\textends RichSourceFunction<TimestampedHiveInputSplit>\n+\t\timplements CheckpointedFunction {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(HiveContinuousMonitoringFunction.class);\n+\n+\t/** The parallelism of the downstream readers. */\n+\tprivate final int readerParallelism;\n+\n+\t/** The interval between consecutive path scans. */\n+\tprivate final long interval;\n+\n+\tprivate final HiveShim hiveShim;\n+\n+\tprivate final JobConfWrapper conf;\n+\n+\tprivate final ObjectPath tablePath;\n+\n+\tprivate final List<String> partitionKeys;\n+\n+\tprivate final String[] fieldNames;\n+\n+\tprivate final DataType[] fieldTypes;\n+\n+\tprivate final String startupPartition;\n+\n+\tprivate final PartitionFetcherFactory fetcherFactory;\n+\n+\tprivate volatile boolean isRunning = true;\n+\n+\t/** The maximum partition read time seen so far. */\n+\tprivate volatile long currentReadTime;\n+\n+\tprivate transient PartitionFetcher.Context fetcherContext;\n+\n+\tprivate transient PartitionFetcher fetcher;\n+\n+\tprivate transient Object checkpointLock;\n+\n+\tprivate transient ListState<Long> currReadTimeState;\n+\n+\tprivate transient ListState<List<List<String>>> distinctPartsState;\n+\n+\tprivate transient IMetaStoreClient client;\n+\n+\tprivate transient Properties tableProps;\n+\n+\tprivate transient String defaultPartitionName;\n+\n+\tprivate transient Set<List<String>> distinctPartitions;\n+\n+\tpublic HiveContinuousMonitoringFunction(\n+\t\t\tHiveShim hiveShim,\n+\t\t\tJobConf conf,\n+\t\t\tObjectPath tablePath,\n+\t\t\tCatalogTable catalogTable,\n+\t\t\tString startupPartition,\n+\t\t\tPartitionFetcherFactory fetcherFactory,\n+\t\t\tint readerParallelism,\n+\t\t\tlong interval) {\n+\t\tthis.hiveShim = hiveShim;\n+\t\tthis.conf = new JobConfWrapper(conf);\n+\t\tthis.tablePath = tablePath;\n+\t\tthis.partitionKeys = catalogTable.getPartitionKeys();\n+\t\tthis.fieldNames = catalogTable.getSchema().getFieldNames();\n+\t\tthis.fieldTypes = catalogTable.getSchema().getFieldDataTypes();\n+\t\tthis.startupPartition = startupPartition;\n+\t\tthis.fetcherFactory = fetcherFactory;\n+\n+\t\tthis.interval = interval;\n+\t\tthis.readerParallelism = Math.max(readerParallelism, 1);\n+\t\tthis.currentReadTime = 0;\n+\t}\n+\n+\t@Override\n+\tpublic void initializeState(FunctionInitializationContext context) throws Exception {\n+\t\tthis.currReadTimeState = context.getOperatorStateStore().getListState(\n+\t\t\tnew ListStateDescriptor<>(\n+\t\t\t\t\"partition-monitoring-state\",\n+\t\t\t\tLongSerializer.INSTANCE\n+\t\t\t)\n+\t\t);\n+\t\tthis.distinctPartsState = context.getOperatorStateStore().getListState(\n+\t\t\tnew ListStateDescriptor<>(\n+\t\t\t\t\"partition-monitoring-state\",\n+\t\t\t\tnew ListSerializer<>(new ListSerializer<>(StringSerializer.INSTANCE))\n+\t\t\t)\n+\t\t);\n+\n+\t\tthis.client = this.hiveShim.getHiveMetastoreClient(new HiveConf(conf.conf(), HiveConf.class));\n+\n+\t\tTable hiveTable = client.getTable(tablePath.getDatabaseName(), tablePath.getObjectName());\n+\t\tthis.tableProps = HiveReflectionUtils.getTableMetadata(hiveShim, hiveTable);\n+\t\tthis.defaultPartitionName = conf.conf().get(HiveConf.ConfVars.DEFAULTPARTITIONNAME.varname,\n+\t\t\t\tHiveConf.ConfVars.DEFAULTPARTITIONNAME.defaultStrVal);\n+\n+\t\tthis.fetcher = fetcherFactory.createStrategy(getRuntimeContext().getUserCodeClassLoader());\n+\n+\t\tPath location = new Path(hiveTable.getSd().getLocation());\n+\t\tFileSystem fs = location.getFileSystem(conf.conf());\n+\t\tthis.fetcherContext = new PartitionFetcher.Context() {\n+\n+\t\t\t@Override\n+\t\t\tpublic List<String> partitionKeys() {\n+\t\t\t\treturn partitionKeys;\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic Partition getPartition(String escapeName) throws TException {\n+\t\t\t\treturn client.getPartition(\n+\t\t\t\t\t\ttablePath.getDatabaseName(),\n+\t\t\t\t\t\ttablePath.getObjectName(),\n+\t\t\t\t\t\tescapeName);\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic List<Partition> listPartitionsByFilter(String filter) throws TException {\n+\t\t\t\treturn client.listPartitionsByFilter(\n+\t\t\t\t\t\ttablePath.getDatabaseName(),\n+\t\t\t\t\t\ttablePath.getObjectName(),\n+\t\t\t\t\t\tfilter,\n+\t\t\t\t\t\t(short) -1);\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic FileSystem fileSystem() {\n+\t\t\t\treturn fs;\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic Path tableLocation() {\n+\t\t\t\treturn new Path(hiveTable.getSd().getLocation());\n+\t\t\t}\n+\t\t};\n+\n+\t\tthis.distinctPartitions = new HashSet<>();\n+\t\tif (context.isRestored()) {\n+\t\t\tLOG.info(\"Restoring state for the {}.\", getClass().getSimpleName());\n+\t\t\tthis.currentReadTime = this.currReadTimeState.get().iterator().next();\n+\t\t\tthis.distinctPartitions.addAll(this.distinctPartsState.get().iterator().next());\n+\t\t} else {\n+\t\t\tLOG.info(\"No state to restore for the {}.\", getClass().getSimpleName());\n+\t\t\tif (startupPartition != null) {\n+\t\t\t\tthis.currentReadTime = fetcher.extractTimestamp(\n+\t\t\t\t\t\tfetcherContext,\n+\t\t\t\t\t\tclient.getPartition(\n+\t\t\t\t\t\t\t\ttablePath.getDatabaseName(),\n+\t\t\t\t\t\t\t\ttablePath.getObjectName(),\n+\t\t\t\t\t\t\t\tescapePathName(startupPartition)));\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void run(SourceContext<TimestampedHiveInputSplit> context) throws Exception {\n+\t\tcheckpointLock = context.getCheckpointLock();\n+\t\twhile (isRunning) {\n+\t\t\tsynchronized (checkpointLock) {\n+\t\t\t\tmonitorAndForwardSplits(context);\n+\t\t\t}\n+\t\t\tThread.sleep(interval);\n+\t\t}\n+\t}\n+\n+\tprivate void monitorAndForwardSplits(\n+\t\t\tSourceContext<TimestampedHiveInputSplit> context) throws Exception {\n+\t\tassert (Thread.holdsLock(checkpointLock));\n+\n+\t\tList<Partition> partitions = fetcher.fetchPartitions(fetcherContext, currentReadTime);\n+\n+\t\tif (partitions.isEmpty()) {\n+\t\t\treturn;\n+\t\t}\n+\n+\t\tlong maxTimestamp = Long.MIN_VALUE;\n+\t\tSet<List<String>> nextDistinctParts = new HashSet<>();\n+\t\tfor (Partition partition : partitions) {\n+\t\t\tList<String> partSpec = partition.getValues();\n+\t\t\tif (!this.distinctPartitions.contains(partSpec)) {\n+\t\t\t\tthis.distinctPartitions.add(partition.getValues());\n+\t\t\t\tlong timestamp = this.fetcher.extractTimestamp(fetcherContext, partition);\n+\t\t\t\tif (timestamp > currentReadTime) {\n+\t\t\t\t\tnextDistinctParts.add(partSpec);\n+\t\t\t\t}\n+\t\t\t\tif (timestamp > maxTimestamp) {\n+\t\t\t\t\tmaxTimestamp = timestamp;\n+\t\t\t\t}\n+\t\t\t\tHiveTableInputSplit[] splits = HiveTableInputFormat.createInputSplits(\n+\t\t\t\t\t\tthis.readerParallelism,\n+\t\t\t\t\t\tCollections.singletonList(toHiveTablePartition(partition)),\n+\t\t\t\t\t\tthis.conf.conf());\n+\t\t\t\tfor (HiveTableInputSplit split : splits) {\n+\t\t\t\t\tcontext.collect(new TimestampedHiveInputSplit(timestamp, split));\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\n+\t\tif (maxTimestamp > currentReadTime) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjE0MTE5MA=="}, "originalCommit": {"oid": "42953325bb1d6a447ef53e020d3cc311a8c986c6"}, "originalPosition": 278}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMDMwOTEzOnYy", "diffSide": "RIGHT", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveContinuousMonitoringFunction.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQwODo1OTowMFrOGS5pPQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQwODo1OTowMFrOGS5pPQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ3MTk5Nw==", "bodyText": "I think it's possible that a partition folder exists in FS but not in HMS, in which case that folder should be simply ignored instead of throwing an exception.", "url": "https://github.com/apache/flink/pull/12004#discussion_r422471997", "createdAt": "2020-05-09T08:59:00Z", "author": {"login": "lirui-apache"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveContinuousMonitoringFunction.java", "diffHunk": "@@ -0,0 +1,358 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive.read;\n+\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeutils.base.ListSerializer;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.api.common.typeutils.base.StringSerializer;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.connectors.hive.HiveTablePartition;\n+import org.apache.flink.connectors.hive.HiveTableSource;\n+import org.apache.flink.connectors.hive.JobConfWrapper;\n+import org.apache.flink.runtime.state.FunctionInitializationContext;\n+import org.apache.flink.runtime.state.FunctionSnapshotContext;\n+import org.apache.flink.streaming.api.checkpoint.CheckpointedFunction;\n+import org.apache.flink.streaming.api.functions.source.ContinuousFileReaderOperator;\n+import org.apache.flink.streaming.api.functions.source.RichSourceFunction;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.catalog.hive.client.HiveShim;\n+import org.apache.flink.table.catalog.hive.util.HiveReflectionUtils;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.filesystem.PartitionTimeExtractor;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.metastore.IMetaStoreClient;\n+import org.apache.hadoop.hive.metastore.api.Partition;\n+import org.apache.hadoop.hive.metastore.api.Table;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.thrift.TException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.time.LocalDateTime;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.function.Supplier;\n+\n+import static org.apache.flink.table.filesystem.TimestampPartTimeExtractor.toLocalDateTime;\n+\n+/**\n+ * This is the single (non-parallel) monitoring task which takes a {@link HiveTableInputFormat},\n+ * it is responsible for:\n+ *\n+ * <ol>\n+ *     <li>Monitoring partitions of hive meta store.</li>\n+ *     <li>Deciding which partitions should be further read and processed.</li>\n+ *     <li>Creating the {@link HiveTableInputSplit splits} corresponding to those partitions.</li>\n+ *     <li>Assigning them to downstream tasks for further processing.</li>\n+ * </ol>\n+ *\n+ * <p>The splits to be read are forwarded to the downstream {@link ContinuousFileReaderOperator}\n+ * which can have parallelism greater than one.\n+ *\n+ * <p><b>IMPORTANT NOTE: </b> Splits are forwarded downstream for reading in ascending partition time order,\n+ * based on the partition time of the partitions they belong to.\n+ */\n+public class HiveContinuousMonitoringFunction\n+\t\textends RichSourceFunction<TimestampedHiveInputSplit>\n+\t\timplements CheckpointedFunction {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(HiveContinuousMonitoringFunction.class);\n+\n+\t/** The parallelism of the downstream readers. */\n+\tprivate final int readerParallelism;\n+\n+\t/** The interval between consecutive path scans. */\n+\tprivate final long interval;\n+\n+\tprivate final HiveShim hiveShim;\n+\n+\tprivate final JobConfWrapper conf;\n+\n+\tprivate final ObjectPath tablePath;\n+\n+\tprivate final List<String> partitionKeys;\n+\n+\tprivate final String[] fieldNames;\n+\n+\tprivate final DataType[] fieldTypes;\n+\n+\t// consumer variables\n+\tprivate final String consumeOrder;\n+\tprivate final String consumeOffset;\n+\n+\t// extractor variables\n+\tprivate final String extractorType;\n+\tprivate final String extractorClass;\n+\tprivate final String extractorPattern;\n+\n+\tprivate volatile boolean isRunning = true;\n+\n+\t/** The maximum partition read time seen so far. */\n+\tprivate volatile long currentReadTime;\n+\n+\tprivate transient PartitionDiscovery.Context context;\n+\n+\tprivate transient PartitionDiscovery fetcher;\n+\n+\tprivate transient Object checkpointLock;\n+\n+\tprivate transient ListState<Long> currReadTimeState;\n+\n+\tprivate transient ListState<List<List<String>>> distinctPartsState;\n+\n+\tprivate transient IMetaStoreClient client;\n+\n+\tprivate transient Properties tableProps;\n+\n+\tprivate transient String defaultPartitionName;\n+\n+\tprivate transient Set<List<String>> distinctPartitions;\n+\n+\tpublic HiveContinuousMonitoringFunction(\n+\t\t\tHiveShim hiveShim,\n+\t\t\tJobConf conf,\n+\t\t\tObjectPath tablePath,\n+\t\t\tCatalogTable catalogTable,\n+\t\t\tint readerParallelism,\n+\t\t\tString consumeOrder,\n+\t\t\tString consumeOffset,\n+\t\t\tString extractorType,\n+\t\t\tString extractorClass,\n+\t\t\tString extractorPattern,\n+\t\t\tlong interval) {\n+\t\tthis.hiveShim = hiveShim;\n+\t\tthis.conf = new JobConfWrapper(conf);\n+\t\tthis.tablePath = tablePath;\n+\t\tthis.partitionKeys = catalogTable.getPartitionKeys();\n+\t\tthis.fieldNames = catalogTable.getSchema().getFieldNames();\n+\t\tthis.fieldTypes = catalogTable.getSchema().getFieldDataTypes();\n+\t\tthis.consumeOrder = consumeOrder;\n+\t\tthis.extractorType = extractorType;\n+\t\tthis.extractorClass = extractorClass;\n+\t\tthis.extractorPattern = extractorPattern;\n+\t\tthis.consumeOffset = consumeOffset;\n+\n+\t\tthis.interval = interval;\n+\t\tthis.readerParallelism = Math.max(readerParallelism, 1);\n+\t\tthis.currentReadTime = 0;\n+\t}\n+\n+\t@Override\n+\tpublic void initializeState(FunctionInitializationContext context) throws Exception {\n+\t\tthis.currReadTimeState = context.getOperatorStateStore().getListState(\n+\t\t\tnew ListStateDescriptor<>(\n+\t\t\t\t\"partition-monitoring-state\",\n+\t\t\t\tLongSerializer.INSTANCE\n+\t\t\t)\n+\t\t);\n+\t\tthis.distinctPartsState = context.getOperatorStateStore().getListState(\n+\t\t\tnew ListStateDescriptor<>(\n+\t\t\t\t\"partition-monitoring-state\",\n+\t\t\t\tnew ListSerializer<>(new ListSerializer<>(StringSerializer.INSTANCE))\n+\t\t\t)\n+\t\t);\n+\n+\t\tthis.client = this.hiveShim.getHiveMetastoreClient(new HiveConf(conf.conf(), HiveConf.class));\n+\n+\t\tTable hiveTable = client.getTable(tablePath.getDatabaseName(), tablePath.getObjectName());\n+\t\tthis.tableProps = HiveReflectionUtils.getTableMetadata(hiveShim, hiveTable);\n+\t\tthis.defaultPartitionName = conf.conf().get(HiveConf.ConfVars.DEFAULTPARTITIONNAME.varname,\n+\t\t\t\tHiveConf.ConfVars.DEFAULTPARTITIONNAME.defaultStrVal);\n+\n+\t\tPartitionTimeExtractor extractor = PartitionTimeExtractor.create(\n+\t\t\t\tgetRuntimeContext().getUserCodeClassLoader(),\n+\t\t\t\textractorType,\n+\t\t\t\textractorClass,\n+\t\t\t\textractorPattern);\n+\n+\t\tthis.fetcher = new DirectoryMonitorDiscovery();\n+\n+\t\tPath location = new Path(hiveTable.getSd().getLocation());\n+\t\tFileSystem fs = location.getFileSystem(conf.conf());\n+\t\tthis.context = new PartitionDiscovery.Context() {\n+\n+\t\t\t@Override\n+\t\t\tpublic List<String> partitionKeys() {\n+\t\t\t\treturn partitionKeys;\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic Partition getPartition(List<String> partValues) throws TException {\n+\t\t\t\treturn client.getPartition(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "44df52a0ff62e1b4ec65efd6a14a8b07d7712e75"}, "originalPosition": 210}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMDMwOTI4OnYy", "diffSide": "RIGHT", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveContinuousMonitoringFunction.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQwODo1OToyNVrOGS5pVA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQwODo1OToyNVrOGS5pVA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ3MjAyMA==", "bodyText": "Should add constants for create-time and partition-time.", "url": "https://github.com/apache/flink/pull/12004#discussion_r422472020", "createdAt": "2020-05-09T08:59:25Z", "author": {"login": "lirui-apache"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveContinuousMonitoringFunction.java", "diffHunk": "@@ -0,0 +1,358 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive.read;\n+\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeutils.base.ListSerializer;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.api.common.typeutils.base.StringSerializer;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.connectors.hive.HiveTablePartition;\n+import org.apache.flink.connectors.hive.HiveTableSource;\n+import org.apache.flink.connectors.hive.JobConfWrapper;\n+import org.apache.flink.runtime.state.FunctionInitializationContext;\n+import org.apache.flink.runtime.state.FunctionSnapshotContext;\n+import org.apache.flink.streaming.api.checkpoint.CheckpointedFunction;\n+import org.apache.flink.streaming.api.functions.source.ContinuousFileReaderOperator;\n+import org.apache.flink.streaming.api.functions.source.RichSourceFunction;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.catalog.hive.client.HiveShim;\n+import org.apache.flink.table.catalog.hive.util.HiveReflectionUtils;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.filesystem.PartitionTimeExtractor;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.metastore.IMetaStoreClient;\n+import org.apache.hadoop.hive.metastore.api.Partition;\n+import org.apache.hadoop.hive.metastore.api.Table;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.thrift.TException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.time.LocalDateTime;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.function.Supplier;\n+\n+import static org.apache.flink.table.filesystem.TimestampPartTimeExtractor.toLocalDateTime;\n+\n+/**\n+ * This is the single (non-parallel) monitoring task which takes a {@link HiveTableInputFormat},\n+ * it is responsible for:\n+ *\n+ * <ol>\n+ *     <li>Monitoring partitions of hive meta store.</li>\n+ *     <li>Deciding which partitions should be further read and processed.</li>\n+ *     <li>Creating the {@link HiveTableInputSplit splits} corresponding to those partitions.</li>\n+ *     <li>Assigning them to downstream tasks for further processing.</li>\n+ * </ol>\n+ *\n+ * <p>The splits to be read are forwarded to the downstream {@link ContinuousFileReaderOperator}\n+ * which can have parallelism greater than one.\n+ *\n+ * <p><b>IMPORTANT NOTE: </b> Splits are forwarded downstream for reading in ascending partition time order,\n+ * based on the partition time of the partitions they belong to.\n+ */\n+public class HiveContinuousMonitoringFunction\n+\t\textends RichSourceFunction<TimestampedHiveInputSplit>\n+\t\timplements CheckpointedFunction {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(HiveContinuousMonitoringFunction.class);\n+\n+\t/** The parallelism of the downstream readers. */\n+\tprivate final int readerParallelism;\n+\n+\t/** The interval between consecutive path scans. */\n+\tprivate final long interval;\n+\n+\tprivate final HiveShim hiveShim;\n+\n+\tprivate final JobConfWrapper conf;\n+\n+\tprivate final ObjectPath tablePath;\n+\n+\tprivate final List<String> partitionKeys;\n+\n+\tprivate final String[] fieldNames;\n+\n+\tprivate final DataType[] fieldTypes;\n+\n+\t// consumer variables\n+\tprivate final String consumeOrder;\n+\tprivate final String consumeOffset;\n+\n+\t// extractor variables\n+\tprivate final String extractorType;\n+\tprivate final String extractorClass;\n+\tprivate final String extractorPattern;\n+\n+\tprivate volatile boolean isRunning = true;\n+\n+\t/** The maximum partition read time seen so far. */\n+\tprivate volatile long currentReadTime;\n+\n+\tprivate transient PartitionDiscovery.Context context;\n+\n+\tprivate transient PartitionDiscovery fetcher;\n+\n+\tprivate transient Object checkpointLock;\n+\n+\tprivate transient ListState<Long> currReadTimeState;\n+\n+\tprivate transient ListState<List<List<String>>> distinctPartsState;\n+\n+\tprivate transient IMetaStoreClient client;\n+\n+\tprivate transient Properties tableProps;\n+\n+\tprivate transient String defaultPartitionName;\n+\n+\tprivate transient Set<List<String>> distinctPartitions;\n+\n+\tpublic HiveContinuousMonitoringFunction(\n+\t\t\tHiveShim hiveShim,\n+\t\t\tJobConf conf,\n+\t\t\tObjectPath tablePath,\n+\t\t\tCatalogTable catalogTable,\n+\t\t\tint readerParallelism,\n+\t\t\tString consumeOrder,\n+\t\t\tString consumeOffset,\n+\t\t\tString extractorType,\n+\t\t\tString extractorClass,\n+\t\t\tString extractorPattern,\n+\t\t\tlong interval) {\n+\t\tthis.hiveShim = hiveShim;\n+\t\tthis.conf = new JobConfWrapper(conf);\n+\t\tthis.tablePath = tablePath;\n+\t\tthis.partitionKeys = catalogTable.getPartitionKeys();\n+\t\tthis.fieldNames = catalogTable.getSchema().getFieldNames();\n+\t\tthis.fieldTypes = catalogTable.getSchema().getFieldDataTypes();\n+\t\tthis.consumeOrder = consumeOrder;\n+\t\tthis.extractorType = extractorType;\n+\t\tthis.extractorClass = extractorClass;\n+\t\tthis.extractorPattern = extractorPattern;\n+\t\tthis.consumeOffset = consumeOffset;\n+\n+\t\tthis.interval = interval;\n+\t\tthis.readerParallelism = Math.max(readerParallelism, 1);\n+\t\tthis.currentReadTime = 0;\n+\t}\n+\n+\t@Override\n+\tpublic void initializeState(FunctionInitializationContext context) throws Exception {\n+\t\tthis.currReadTimeState = context.getOperatorStateStore().getListState(\n+\t\t\tnew ListStateDescriptor<>(\n+\t\t\t\t\"partition-monitoring-state\",\n+\t\t\t\tLongSerializer.INSTANCE\n+\t\t\t)\n+\t\t);\n+\t\tthis.distinctPartsState = context.getOperatorStateStore().getListState(\n+\t\t\tnew ListStateDescriptor<>(\n+\t\t\t\t\"partition-monitoring-state\",\n+\t\t\t\tnew ListSerializer<>(new ListSerializer<>(StringSerializer.INSTANCE))\n+\t\t\t)\n+\t\t);\n+\n+\t\tthis.client = this.hiveShim.getHiveMetastoreClient(new HiveConf(conf.conf(), HiveConf.class));\n+\n+\t\tTable hiveTable = client.getTable(tablePath.getDatabaseName(), tablePath.getObjectName());\n+\t\tthis.tableProps = HiveReflectionUtils.getTableMetadata(hiveShim, hiveTable);\n+\t\tthis.defaultPartitionName = conf.conf().get(HiveConf.ConfVars.DEFAULTPARTITIONNAME.varname,\n+\t\t\t\tHiveConf.ConfVars.DEFAULTPARTITIONNAME.defaultStrVal);\n+\n+\t\tPartitionTimeExtractor extractor = PartitionTimeExtractor.create(\n+\t\t\t\tgetRuntimeContext().getUserCodeClassLoader(),\n+\t\t\t\textractorType,\n+\t\t\t\textractorClass,\n+\t\t\t\textractorPattern);\n+\n+\t\tthis.fetcher = new DirectoryMonitorDiscovery();\n+\n+\t\tPath location = new Path(hiveTable.getSd().getLocation());\n+\t\tFileSystem fs = location.getFileSystem(conf.conf());\n+\t\tthis.context = new PartitionDiscovery.Context() {\n+\n+\t\t\t@Override\n+\t\t\tpublic List<String> partitionKeys() {\n+\t\t\t\treturn partitionKeys;\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic Partition getPartition(List<String> partValues) throws TException {\n+\t\t\t\treturn client.getPartition(\n+\t\t\t\t\t\ttablePath.getDatabaseName(),\n+\t\t\t\t\t\ttablePath.getObjectName(),\n+\t\t\t\t\t\tpartValues);\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic FileSystem fileSystem() {\n+\t\t\t\treturn fs;\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic Path tableLocation() {\n+\t\t\t\treturn new Path(hiveTable.getSd().getLocation());\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic long extractTimestamp(\n+\t\t\t\t\tList<String> partKeys,\n+\t\t\t\t\tList<String> partValues,\n+\t\t\t\t\tSupplier<Long> fileTime) {\n+\t\t\t\tswitch (consumeOrder) {\n+\t\t\t\t\tcase \"create-time\":", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "44df52a0ff62e1b4ec65efd6a14a8b07d7712e75"}, "originalPosition": 232}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMDMyMDgxOnYy", "diffSide": "RIGHT", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveContinuousMonitoringFunction.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQwOToxNjoxMlrOGS5vcQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQwOToxNjoxMlrOGS5vcQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ3MzU4NQ==", "bodyText": "this.distinctPartitions.add(partSpec)?", "url": "https://github.com/apache/flink/pull/12004#discussion_r422473585", "createdAt": "2020-05-09T09:16:12Z", "author": {"login": "lirui-apache"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveContinuousMonitoringFunction.java", "diffHunk": "@@ -0,0 +1,358 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive.read;\n+\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeutils.base.ListSerializer;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.api.common.typeutils.base.StringSerializer;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.connectors.hive.HiveTablePartition;\n+import org.apache.flink.connectors.hive.HiveTableSource;\n+import org.apache.flink.connectors.hive.JobConfWrapper;\n+import org.apache.flink.runtime.state.FunctionInitializationContext;\n+import org.apache.flink.runtime.state.FunctionSnapshotContext;\n+import org.apache.flink.streaming.api.checkpoint.CheckpointedFunction;\n+import org.apache.flink.streaming.api.functions.source.ContinuousFileReaderOperator;\n+import org.apache.flink.streaming.api.functions.source.RichSourceFunction;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.catalog.hive.client.HiveShim;\n+import org.apache.flink.table.catalog.hive.util.HiveReflectionUtils;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.filesystem.PartitionTimeExtractor;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.metastore.IMetaStoreClient;\n+import org.apache.hadoop.hive.metastore.api.Partition;\n+import org.apache.hadoop.hive.metastore.api.Table;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.thrift.TException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.time.LocalDateTime;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.function.Supplier;\n+\n+import static org.apache.flink.table.filesystem.TimestampPartTimeExtractor.toLocalDateTime;\n+\n+/**\n+ * This is the single (non-parallel) monitoring task which takes a {@link HiveTableInputFormat},\n+ * it is responsible for:\n+ *\n+ * <ol>\n+ *     <li>Monitoring partitions of hive meta store.</li>\n+ *     <li>Deciding which partitions should be further read and processed.</li>\n+ *     <li>Creating the {@link HiveTableInputSplit splits} corresponding to those partitions.</li>\n+ *     <li>Assigning them to downstream tasks for further processing.</li>\n+ * </ol>\n+ *\n+ * <p>The splits to be read are forwarded to the downstream {@link ContinuousFileReaderOperator}\n+ * which can have parallelism greater than one.\n+ *\n+ * <p><b>IMPORTANT NOTE: </b> Splits are forwarded downstream for reading in ascending partition time order,\n+ * based on the partition time of the partitions they belong to.\n+ */\n+public class HiveContinuousMonitoringFunction\n+\t\textends RichSourceFunction<TimestampedHiveInputSplit>\n+\t\timplements CheckpointedFunction {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(HiveContinuousMonitoringFunction.class);\n+\n+\t/** The parallelism of the downstream readers. */\n+\tprivate final int readerParallelism;\n+\n+\t/** The interval between consecutive path scans. */\n+\tprivate final long interval;\n+\n+\tprivate final HiveShim hiveShim;\n+\n+\tprivate final JobConfWrapper conf;\n+\n+\tprivate final ObjectPath tablePath;\n+\n+\tprivate final List<String> partitionKeys;\n+\n+\tprivate final String[] fieldNames;\n+\n+\tprivate final DataType[] fieldTypes;\n+\n+\t// consumer variables\n+\tprivate final String consumeOrder;\n+\tprivate final String consumeOffset;\n+\n+\t// extractor variables\n+\tprivate final String extractorType;\n+\tprivate final String extractorClass;\n+\tprivate final String extractorPattern;\n+\n+\tprivate volatile boolean isRunning = true;\n+\n+\t/** The maximum partition read time seen so far. */\n+\tprivate volatile long currentReadTime;\n+\n+\tprivate transient PartitionDiscovery.Context context;\n+\n+\tprivate transient PartitionDiscovery fetcher;\n+\n+\tprivate transient Object checkpointLock;\n+\n+\tprivate transient ListState<Long> currReadTimeState;\n+\n+\tprivate transient ListState<List<List<String>>> distinctPartsState;\n+\n+\tprivate transient IMetaStoreClient client;\n+\n+\tprivate transient Properties tableProps;\n+\n+\tprivate transient String defaultPartitionName;\n+\n+\tprivate transient Set<List<String>> distinctPartitions;\n+\n+\tpublic HiveContinuousMonitoringFunction(\n+\t\t\tHiveShim hiveShim,\n+\t\t\tJobConf conf,\n+\t\t\tObjectPath tablePath,\n+\t\t\tCatalogTable catalogTable,\n+\t\t\tint readerParallelism,\n+\t\t\tString consumeOrder,\n+\t\t\tString consumeOffset,\n+\t\t\tString extractorType,\n+\t\t\tString extractorClass,\n+\t\t\tString extractorPattern,\n+\t\t\tlong interval) {\n+\t\tthis.hiveShim = hiveShim;\n+\t\tthis.conf = new JobConfWrapper(conf);\n+\t\tthis.tablePath = tablePath;\n+\t\tthis.partitionKeys = catalogTable.getPartitionKeys();\n+\t\tthis.fieldNames = catalogTable.getSchema().getFieldNames();\n+\t\tthis.fieldTypes = catalogTable.getSchema().getFieldDataTypes();\n+\t\tthis.consumeOrder = consumeOrder;\n+\t\tthis.extractorType = extractorType;\n+\t\tthis.extractorClass = extractorClass;\n+\t\tthis.extractorPattern = extractorPattern;\n+\t\tthis.consumeOffset = consumeOffset;\n+\n+\t\tthis.interval = interval;\n+\t\tthis.readerParallelism = Math.max(readerParallelism, 1);\n+\t\tthis.currentReadTime = 0;\n+\t}\n+\n+\t@Override\n+\tpublic void initializeState(FunctionInitializationContext context) throws Exception {\n+\t\tthis.currReadTimeState = context.getOperatorStateStore().getListState(\n+\t\t\tnew ListStateDescriptor<>(\n+\t\t\t\t\"partition-monitoring-state\",\n+\t\t\t\tLongSerializer.INSTANCE\n+\t\t\t)\n+\t\t);\n+\t\tthis.distinctPartsState = context.getOperatorStateStore().getListState(\n+\t\t\tnew ListStateDescriptor<>(\n+\t\t\t\t\"partition-monitoring-state\",\n+\t\t\t\tnew ListSerializer<>(new ListSerializer<>(StringSerializer.INSTANCE))\n+\t\t\t)\n+\t\t);\n+\n+\t\tthis.client = this.hiveShim.getHiveMetastoreClient(new HiveConf(conf.conf(), HiveConf.class));\n+\n+\t\tTable hiveTable = client.getTable(tablePath.getDatabaseName(), tablePath.getObjectName());\n+\t\tthis.tableProps = HiveReflectionUtils.getTableMetadata(hiveShim, hiveTable);\n+\t\tthis.defaultPartitionName = conf.conf().get(HiveConf.ConfVars.DEFAULTPARTITIONNAME.varname,\n+\t\t\t\tHiveConf.ConfVars.DEFAULTPARTITIONNAME.defaultStrVal);\n+\n+\t\tPartitionTimeExtractor extractor = PartitionTimeExtractor.create(\n+\t\t\t\tgetRuntimeContext().getUserCodeClassLoader(),\n+\t\t\t\textractorType,\n+\t\t\t\textractorClass,\n+\t\t\t\textractorPattern);\n+\n+\t\tthis.fetcher = new DirectoryMonitorDiscovery();\n+\n+\t\tPath location = new Path(hiveTable.getSd().getLocation());\n+\t\tFileSystem fs = location.getFileSystem(conf.conf());\n+\t\tthis.context = new PartitionDiscovery.Context() {\n+\n+\t\t\t@Override\n+\t\t\tpublic List<String> partitionKeys() {\n+\t\t\t\treturn partitionKeys;\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic Partition getPartition(List<String> partValues) throws TException {\n+\t\t\t\treturn client.getPartition(\n+\t\t\t\t\t\ttablePath.getDatabaseName(),\n+\t\t\t\t\t\ttablePath.getObjectName(),\n+\t\t\t\t\t\tpartValues);\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic FileSystem fileSystem() {\n+\t\t\t\treturn fs;\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic Path tableLocation() {\n+\t\t\t\treturn new Path(hiveTable.getSd().getLocation());\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic long extractTimestamp(\n+\t\t\t\t\tList<String> partKeys,\n+\t\t\t\t\tList<String> partValues,\n+\t\t\t\t\tSupplier<Long> fileTime) {\n+\t\t\t\tswitch (consumeOrder) {\n+\t\t\t\t\tcase \"create-time\":\n+\t\t\t\t\t\treturn fileTime.get();\n+\t\t\t\t\tcase \"partition-time\":\n+\t\t\t\t\t\treturn toMills(extractor.extract(partKeys, partValues));\n+\t\t\t\t\tdefault:\n+\t\t\t\t\t\tthrow new UnsupportedOperationException(\n+\t\t\t\t\t\t\t\t\"Unsupported consumer order: \" + consumeOrder);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t};\n+\n+\t\tthis.distinctPartitions = new HashSet<>();\n+\t\tif (context.isRestored()) {\n+\t\t\tLOG.info(\"Restoring state for the {}.\", getClass().getSimpleName());\n+\t\t\tthis.currentReadTime = this.currReadTimeState.get().iterator().next();\n+\t\t\tthis.distinctPartitions.addAll(this.distinctPartsState.get().iterator().next());\n+\t\t} else {\n+\t\t\tLOG.info(\"No state to restore for the {}.\", getClass().getSimpleName());\n+\t\t\tif (consumeOffset != null) {\n+\t\t\t\tthis.currentReadTime = toMills(toLocalDateTime(consumeOffset));\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void run(SourceContext<TimestampedHiveInputSplit> context) throws Exception {\n+\t\tcheckpointLock = context.getCheckpointLock();\n+\t\twhile (isRunning) {\n+\t\t\tsynchronized (checkpointLock) {\n+\t\t\t\tmonitorAndForwardSplits(context);\n+\t\t\t}\n+\t\t\tThread.sleep(interval);\n+\t\t}\n+\t}\n+\n+\tprivate void monitorAndForwardSplits(\n+\t\t\tSourceContext<TimestampedHiveInputSplit> context) throws Exception {\n+\t\tassert (Thread.holdsLock(checkpointLock));\n+\n+\t\tList<Tuple2<Partition, Long>> partitions = fetcher.fetchPartitions(this.context, currentReadTime);\n+\n+\t\tif (partitions.isEmpty()) {\n+\t\t\treturn;\n+\t\t}\n+\n+\t\tpartitions.sort((o1, o2) -> (int) (o1.f1 - o2.f1));\n+\n+\t\tlong maxTimestamp = Long.MIN_VALUE;\n+\t\tSet<List<String>> nextDistinctParts = new HashSet<>();\n+\t\tfor (Tuple2<Partition, Long> tuple2 : partitions) {\n+\t\t\tPartition partition = tuple2.f0;\n+\t\t\tList<String> partSpec = partition.getValues();\n+\t\t\tif (!this.distinctPartitions.contains(partSpec)) {\n+\t\t\t\tthis.distinctPartitions.add(partition.getValues());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "44df52a0ff62e1b4ec65efd6a14a8b07d7712e75"}, "originalPosition": 285}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMDM4NDE0OnYy", "diffSide": "RIGHT", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/SplitReader.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQxMDo1MjoyNVrOGS6RLQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQxMDo1MjoyNVrOGS6RLQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ4MjIyMQ==", "bodyText": "\"Seek too many rows.\" ?", "url": "https://github.com/apache/flink/pull/12004#discussion_r422482221", "createdAt": "2020-05-09T10:52:25Z", "author": {"login": "lirui-apache"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/SplitReader.java", "diffHunk": "@@ -46,4 +46,17 @@\n \t * @throws IOException Thrown, if an I/O error occurred.\n \t */\n \tRowData nextRecord(RowData reuse) throws IOException;\n+\n+\t/**\n+\t * Seek to a particular row number.\n+\t */\n+\tdefault void seekToRow(long rowCount, RowData reuse) throws IOException {\n+\t\tfor (int i = 0; i < rowCount; i++) {\n+\t\t\tboolean end = reachedEnd();\n+\t\t\tif (end) {\n+\t\t\t\tthrow new RuntimeException(\"Seek to many rows.\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "44df52a0ff62e1b4ec65efd6a14a8b07d7712e75"}, "originalPosition": 12}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMDQwMjc1OnYy", "diffSide": "RIGHT", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/TimestampedHiveInputSplit.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQxMToyMDoxNVrOGS6aQQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMVQwMzoxODoxNVrOGTLFoQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ4NDU0NQ==", "bodyText": "For two split instances, this means equals can return false while compareTo returns 0. Is this intended behavior?", "url": "https://github.com/apache/flink/pull/12004#discussion_r422484545", "createdAt": "2020-05-09T11:20:15Z", "author": {"login": "lirui-apache"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/TimestampedHiveInputSplit.java", "diffHunk": "@@ -0,0 +1,232 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive.read;\n+\n+import org.apache.flink.api.common.typeinfo.BasicTypeInfo;\n+import org.apache.flink.api.common.typeinfo.TypeInfo;\n+import org.apache.flink.api.common.typeinfo.TypeInfoFactory;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeutils.SimpleTypeSerializerSnapshot;\n+import org.apache.flink.api.common.typeutils.TypeSerializerSnapshot;\n+import org.apache.flink.api.common.typeutils.base.TypeSerializerSingleton;\n+import org.apache.flink.api.java.typeutils.runtime.DataInputViewStream;\n+import org.apache.flink.api.java.typeutils.runtime.DataOutputViewStream;\n+import org.apache.flink.core.memory.DataInputView;\n+import org.apache.flink.core.memory.DataOutputView;\n+import org.apache.flink.streaming.api.functions.source.TimestampedInputSplit;\n+import org.apache.flink.util.FlinkRuntimeException;\n+import org.apache.flink.util.InstantiationUtil;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.lang.reflect.Type;\n+import java.util.Map;\n+import java.util.Objects;\n+\n+/**\n+ * A {@link HiveTableInputSplit} with {@link TimestampedInputSplit}.\n+ * Kryo serializer can not deal with hadoop split, need specific type information factory.\n+ */\n+@TypeInfo(TimestampedHiveInputSplit.SplitTypeInfoFactory.class)\n+public class TimestampedHiveInputSplit extends HiveTableInputSplit implements TimestampedInputSplit {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\t/** The modification time of the file this split belongs to. */\n+\tprivate final long modificationTime;\n+\n+\t/**\n+\t * The state of the split. This information is used when\n+\t * restoring from a checkpoint and allows to resume reading the\n+\t * underlying file from the point we left off.\n+\t * */\n+\tprivate Serializable splitState;\n+\n+\tpublic TimestampedHiveInputSplit(\n+\t\t\tlong modificationTime,\n+\t\t\tHiveTableInputSplit split) {\n+\t\tsuper(\n+\t\t\t\tsplit.getSplitNumber(),\n+\t\t\t\tsplit.getHadoopInputSplit(),\n+\t\t\t\tsplit.getJobConf(),\n+\t\t\t\tsplit.getHiveTablePartition());\n+\t\tthis.modificationTime = modificationTime;\n+\t}\n+\n+\t@Override\n+\tpublic void setSplitState(Serializable state) {\n+\t\tthis.splitState = state;\n+\t}\n+\n+\t@Override\n+\tpublic Serializable getSplitState() {\n+\t\treturn this.splitState;\n+\t}\n+\n+\t@Override\n+\tpublic long getModificationTime() {\n+\t\treturn modificationTime;\n+\t}\n+\n+\t@Override\n+\tpublic int compareTo(TimestampedInputSplit o) {\n+\t\tTimestampedHiveInputSplit split = (TimestampedHiveInputSplit) o;\n+\t\tint modTimeComp = Long.compare(this.modificationTime, split.modificationTime);\n+\t\tif (modTimeComp != 0L) {\n+\t\t\treturn modTimeComp;\n+\t\t}\n+\n+\t\tint pathComp = this.hiveTablePartition.getStorageDescriptor().compareTo(\n+\t\t\t\tsplit.hiveTablePartition.getStorageDescriptor());\n+\n+\t\treturn pathComp != 0 ? pathComp :\n+\t\t\t\tthis.getSplitNumber() - o.getSplitNumber();\n+\t}\n+\n+\t@Override\n+\tpublic boolean equals(Object o) {\n+\t\tif (this == o) {\n+\t\t\treturn true;\n+\t\t}\n+\t\tif (o == null || getClass() != o.getClass()) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tif (!super.equals(o)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "44df52a0ff62e1b4ec65efd6a14a8b07d7712e75"}, "originalPosition": 109}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjc1MjY1NA==", "bodyText": "Yes, we can create the case like you said, but in read world, it should not happen.", "url": "https://github.com/apache/flink/pull/12004#discussion_r422752654", "createdAt": "2020-05-11T02:53:10Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/TimestampedHiveInputSplit.java", "diffHunk": "@@ -0,0 +1,232 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive.read;\n+\n+import org.apache.flink.api.common.typeinfo.BasicTypeInfo;\n+import org.apache.flink.api.common.typeinfo.TypeInfo;\n+import org.apache.flink.api.common.typeinfo.TypeInfoFactory;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeutils.SimpleTypeSerializerSnapshot;\n+import org.apache.flink.api.common.typeutils.TypeSerializerSnapshot;\n+import org.apache.flink.api.common.typeutils.base.TypeSerializerSingleton;\n+import org.apache.flink.api.java.typeutils.runtime.DataInputViewStream;\n+import org.apache.flink.api.java.typeutils.runtime.DataOutputViewStream;\n+import org.apache.flink.core.memory.DataInputView;\n+import org.apache.flink.core.memory.DataOutputView;\n+import org.apache.flink.streaming.api.functions.source.TimestampedInputSplit;\n+import org.apache.flink.util.FlinkRuntimeException;\n+import org.apache.flink.util.InstantiationUtil;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.lang.reflect.Type;\n+import java.util.Map;\n+import java.util.Objects;\n+\n+/**\n+ * A {@link HiveTableInputSplit} with {@link TimestampedInputSplit}.\n+ * Kryo serializer can not deal with hadoop split, need specific type information factory.\n+ */\n+@TypeInfo(TimestampedHiveInputSplit.SplitTypeInfoFactory.class)\n+public class TimestampedHiveInputSplit extends HiveTableInputSplit implements TimestampedInputSplit {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\t/** The modification time of the file this split belongs to. */\n+\tprivate final long modificationTime;\n+\n+\t/**\n+\t * The state of the split. This information is used when\n+\t * restoring from a checkpoint and allows to resume reading the\n+\t * underlying file from the point we left off.\n+\t * */\n+\tprivate Serializable splitState;\n+\n+\tpublic TimestampedHiveInputSplit(\n+\t\t\tlong modificationTime,\n+\t\t\tHiveTableInputSplit split) {\n+\t\tsuper(\n+\t\t\t\tsplit.getSplitNumber(),\n+\t\t\t\tsplit.getHadoopInputSplit(),\n+\t\t\t\tsplit.getJobConf(),\n+\t\t\t\tsplit.getHiveTablePartition());\n+\t\tthis.modificationTime = modificationTime;\n+\t}\n+\n+\t@Override\n+\tpublic void setSplitState(Serializable state) {\n+\t\tthis.splitState = state;\n+\t}\n+\n+\t@Override\n+\tpublic Serializable getSplitState() {\n+\t\treturn this.splitState;\n+\t}\n+\n+\t@Override\n+\tpublic long getModificationTime() {\n+\t\treturn modificationTime;\n+\t}\n+\n+\t@Override\n+\tpublic int compareTo(TimestampedInputSplit o) {\n+\t\tTimestampedHiveInputSplit split = (TimestampedHiveInputSplit) o;\n+\t\tint modTimeComp = Long.compare(this.modificationTime, split.modificationTime);\n+\t\tif (modTimeComp != 0L) {\n+\t\t\treturn modTimeComp;\n+\t\t}\n+\n+\t\tint pathComp = this.hiveTablePartition.getStorageDescriptor().compareTo(\n+\t\t\t\tsplit.hiveTablePartition.getStorageDescriptor());\n+\n+\t\treturn pathComp != 0 ? pathComp :\n+\t\t\t\tthis.getSplitNumber() - o.getSplitNumber();\n+\t}\n+\n+\t@Override\n+\tpublic boolean equals(Object o) {\n+\t\tif (this == o) {\n+\t\t\treturn true;\n+\t\t}\n+\t\tif (o == null || getClass() != o.getClass()) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tif (!super.equals(o)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ4NDU0NQ=="}, "originalCommit": {"oid": "44df52a0ff62e1b4ec65efd6a14a8b07d7712e75"}, "originalPosition": 109}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjc1NjQxNw==", "bodyText": "I think the best practice is to avoid such implementations. And if for some reason it has to be done this way, we should clarify it in the java doc, according to the suggestions of Comparable:\n     * <p>It is strongly recommended, but <i>not</i> strictly required that\n     * <tt>(x.compareTo(y)==0) == (x.equals(y))</tt>.  Generally speaking, any\n     * class that implements the <tt>Comparable</tt> interface and violates\n     * this condition should clearly indicate this fact.  The recommended\n     * language is \"Note: this class has a natural ordering that is\n     * inconsistent with equals.\"", "url": "https://github.com/apache/flink/pull/12004#discussion_r422756417", "createdAt": "2020-05-11T03:11:20Z", "author": {"login": "lirui-apache"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/TimestampedHiveInputSplit.java", "diffHunk": "@@ -0,0 +1,232 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive.read;\n+\n+import org.apache.flink.api.common.typeinfo.BasicTypeInfo;\n+import org.apache.flink.api.common.typeinfo.TypeInfo;\n+import org.apache.flink.api.common.typeinfo.TypeInfoFactory;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeutils.SimpleTypeSerializerSnapshot;\n+import org.apache.flink.api.common.typeutils.TypeSerializerSnapshot;\n+import org.apache.flink.api.common.typeutils.base.TypeSerializerSingleton;\n+import org.apache.flink.api.java.typeutils.runtime.DataInputViewStream;\n+import org.apache.flink.api.java.typeutils.runtime.DataOutputViewStream;\n+import org.apache.flink.core.memory.DataInputView;\n+import org.apache.flink.core.memory.DataOutputView;\n+import org.apache.flink.streaming.api.functions.source.TimestampedInputSplit;\n+import org.apache.flink.util.FlinkRuntimeException;\n+import org.apache.flink.util.InstantiationUtil;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.lang.reflect.Type;\n+import java.util.Map;\n+import java.util.Objects;\n+\n+/**\n+ * A {@link HiveTableInputSplit} with {@link TimestampedInputSplit}.\n+ * Kryo serializer can not deal with hadoop split, need specific type information factory.\n+ */\n+@TypeInfo(TimestampedHiveInputSplit.SplitTypeInfoFactory.class)\n+public class TimestampedHiveInputSplit extends HiveTableInputSplit implements TimestampedInputSplit {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\t/** The modification time of the file this split belongs to. */\n+\tprivate final long modificationTime;\n+\n+\t/**\n+\t * The state of the split. This information is used when\n+\t * restoring from a checkpoint and allows to resume reading the\n+\t * underlying file from the point we left off.\n+\t * */\n+\tprivate Serializable splitState;\n+\n+\tpublic TimestampedHiveInputSplit(\n+\t\t\tlong modificationTime,\n+\t\t\tHiveTableInputSplit split) {\n+\t\tsuper(\n+\t\t\t\tsplit.getSplitNumber(),\n+\t\t\t\tsplit.getHadoopInputSplit(),\n+\t\t\t\tsplit.getJobConf(),\n+\t\t\t\tsplit.getHiveTablePartition());\n+\t\tthis.modificationTime = modificationTime;\n+\t}\n+\n+\t@Override\n+\tpublic void setSplitState(Serializable state) {\n+\t\tthis.splitState = state;\n+\t}\n+\n+\t@Override\n+\tpublic Serializable getSplitState() {\n+\t\treturn this.splitState;\n+\t}\n+\n+\t@Override\n+\tpublic long getModificationTime() {\n+\t\treturn modificationTime;\n+\t}\n+\n+\t@Override\n+\tpublic int compareTo(TimestampedInputSplit o) {\n+\t\tTimestampedHiveInputSplit split = (TimestampedHiveInputSplit) o;\n+\t\tint modTimeComp = Long.compare(this.modificationTime, split.modificationTime);\n+\t\tif (modTimeComp != 0L) {\n+\t\t\treturn modTimeComp;\n+\t\t}\n+\n+\t\tint pathComp = this.hiveTablePartition.getStorageDescriptor().compareTo(\n+\t\t\t\tsplit.hiveTablePartition.getStorageDescriptor());\n+\n+\t\treturn pathComp != 0 ? pathComp :\n+\t\t\t\tthis.getSplitNumber() - o.getSplitNumber();\n+\t}\n+\n+\t@Override\n+\tpublic boolean equals(Object o) {\n+\t\tif (this == o) {\n+\t\t\treturn true;\n+\t\t}\n+\t\tif (o == null || getClass() != o.getClass()) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tif (!super.equals(o)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ4NDU0NQ=="}, "originalCommit": {"oid": "44df52a0ff62e1b4ec65efd6a14a8b07d7712e75"}, "originalPosition": 109}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjc1Nzc5Mw==", "bodyText": "TimestampedHiveInputSplit is only used in ContinuousFileReaderOperator. I will add comments.", "url": "https://github.com/apache/flink/pull/12004#discussion_r422757793", "createdAt": "2020-05-11T03:18:15Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/TimestampedHiveInputSplit.java", "diffHunk": "@@ -0,0 +1,232 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive.read;\n+\n+import org.apache.flink.api.common.typeinfo.BasicTypeInfo;\n+import org.apache.flink.api.common.typeinfo.TypeInfo;\n+import org.apache.flink.api.common.typeinfo.TypeInfoFactory;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeutils.SimpleTypeSerializerSnapshot;\n+import org.apache.flink.api.common.typeutils.TypeSerializerSnapshot;\n+import org.apache.flink.api.common.typeutils.base.TypeSerializerSingleton;\n+import org.apache.flink.api.java.typeutils.runtime.DataInputViewStream;\n+import org.apache.flink.api.java.typeutils.runtime.DataOutputViewStream;\n+import org.apache.flink.core.memory.DataInputView;\n+import org.apache.flink.core.memory.DataOutputView;\n+import org.apache.flink.streaming.api.functions.source.TimestampedInputSplit;\n+import org.apache.flink.util.FlinkRuntimeException;\n+import org.apache.flink.util.InstantiationUtil;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.lang.reflect.Type;\n+import java.util.Map;\n+import java.util.Objects;\n+\n+/**\n+ * A {@link HiveTableInputSplit} with {@link TimestampedInputSplit}.\n+ * Kryo serializer can not deal with hadoop split, need specific type information factory.\n+ */\n+@TypeInfo(TimestampedHiveInputSplit.SplitTypeInfoFactory.class)\n+public class TimestampedHiveInputSplit extends HiveTableInputSplit implements TimestampedInputSplit {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\t/** The modification time of the file this split belongs to. */\n+\tprivate final long modificationTime;\n+\n+\t/**\n+\t * The state of the split. This information is used when\n+\t * restoring from a checkpoint and allows to resume reading the\n+\t * underlying file from the point we left off.\n+\t * */\n+\tprivate Serializable splitState;\n+\n+\tpublic TimestampedHiveInputSplit(\n+\t\t\tlong modificationTime,\n+\t\t\tHiveTableInputSplit split) {\n+\t\tsuper(\n+\t\t\t\tsplit.getSplitNumber(),\n+\t\t\t\tsplit.getHadoopInputSplit(),\n+\t\t\t\tsplit.getJobConf(),\n+\t\t\t\tsplit.getHiveTablePartition());\n+\t\tthis.modificationTime = modificationTime;\n+\t}\n+\n+\t@Override\n+\tpublic void setSplitState(Serializable state) {\n+\t\tthis.splitState = state;\n+\t}\n+\n+\t@Override\n+\tpublic Serializable getSplitState() {\n+\t\treturn this.splitState;\n+\t}\n+\n+\t@Override\n+\tpublic long getModificationTime() {\n+\t\treturn modificationTime;\n+\t}\n+\n+\t@Override\n+\tpublic int compareTo(TimestampedInputSplit o) {\n+\t\tTimestampedHiveInputSplit split = (TimestampedHiveInputSplit) o;\n+\t\tint modTimeComp = Long.compare(this.modificationTime, split.modificationTime);\n+\t\tif (modTimeComp != 0L) {\n+\t\t\treturn modTimeComp;\n+\t\t}\n+\n+\t\tint pathComp = this.hiveTablePartition.getStorageDescriptor().compareTo(\n+\t\t\t\tsplit.hiveTablePartition.getStorageDescriptor());\n+\n+\t\treturn pathComp != 0 ? pathComp :\n+\t\t\t\tthis.getSplitNumber() - o.getSplitNumber();\n+\t}\n+\n+\t@Override\n+\tpublic boolean equals(Object o) {\n+\t\tif (this == o) {\n+\t\t\treturn true;\n+\t\t}\n+\t\tif (o == null || getClass() != o.getClass()) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tif (!super.equals(o)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ4NDU0NQ=="}, "originalCommit": {"oid": "44df52a0ff62e1b4ec65efd6a14a8b07d7712e75"}, "originalPosition": 109}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMDQwMjkwOnYy", "diffSide": "RIGHT", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/TimestampedHiveInputSplit.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQxMToyMDozM1rOGS6aWQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMVQwMjo1MjoxOFrOGTKw2Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ4NDU2OQ==", "bodyText": "I think this compares more than just paths.", "url": "https://github.com/apache/flink/pull/12004#discussion_r422484569", "createdAt": "2020-05-09T11:20:33Z", "author": {"login": "lirui-apache"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/TimestampedHiveInputSplit.java", "diffHunk": "@@ -0,0 +1,232 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive.read;\n+\n+import org.apache.flink.api.common.typeinfo.BasicTypeInfo;\n+import org.apache.flink.api.common.typeinfo.TypeInfo;\n+import org.apache.flink.api.common.typeinfo.TypeInfoFactory;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeutils.SimpleTypeSerializerSnapshot;\n+import org.apache.flink.api.common.typeutils.TypeSerializerSnapshot;\n+import org.apache.flink.api.common.typeutils.base.TypeSerializerSingleton;\n+import org.apache.flink.api.java.typeutils.runtime.DataInputViewStream;\n+import org.apache.flink.api.java.typeutils.runtime.DataOutputViewStream;\n+import org.apache.flink.core.memory.DataInputView;\n+import org.apache.flink.core.memory.DataOutputView;\n+import org.apache.flink.streaming.api.functions.source.TimestampedInputSplit;\n+import org.apache.flink.util.FlinkRuntimeException;\n+import org.apache.flink.util.InstantiationUtil;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.lang.reflect.Type;\n+import java.util.Map;\n+import java.util.Objects;\n+\n+/**\n+ * A {@link HiveTableInputSplit} with {@link TimestampedInputSplit}.\n+ * Kryo serializer can not deal with hadoop split, need specific type information factory.\n+ */\n+@TypeInfo(TimestampedHiveInputSplit.SplitTypeInfoFactory.class)\n+public class TimestampedHiveInputSplit extends HiveTableInputSplit implements TimestampedInputSplit {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\t/** The modification time of the file this split belongs to. */\n+\tprivate final long modificationTime;\n+\n+\t/**\n+\t * The state of the split. This information is used when\n+\t * restoring from a checkpoint and allows to resume reading the\n+\t * underlying file from the point we left off.\n+\t * */\n+\tprivate Serializable splitState;\n+\n+\tpublic TimestampedHiveInputSplit(\n+\t\t\tlong modificationTime,\n+\t\t\tHiveTableInputSplit split) {\n+\t\tsuper(\n+\t\t\t\tsplit.getSplitNumber(),\n+\t\t\t\tsplit.getHadoopInputSplit(),\n+\t\t\t\tsplit.getJobConf(),\n+\t\t\t\tsplit.getHiveTablePartition());\n+\t\tthis.modificationTime = modificationTime;\n+\t}\n+\n+\t@Override\n+\tpublic void setSplitState(Serializable state) {\n+\t\tthis.splitState = state;\n+\t}\n+\n+\t@Override\n+\tpublic Serializable getSplitState() {\n+\t\treturn this.splitState;\n+\t}\n+\n+\t@Override\n+\tpublic long getModificationTime() {\n+\t\treturn modificationTime;\n+\t}\n+\n+\t@Override\n+\tpublic int compareTo(TimestampedInputSplit o) {\n+\t\tTimestampedHiveInputSplit split = (TimestampedHiveInputSplit) o;\n+\t\tint modTimeComp = Long.compare(this.modificationTime, split.modificationTime);\n+\t\tif (modTimeComp != 0L) {\n+\t\t\treturn modTimeComp;\n+\t\t}\n+\n+\t\tint pathComp = this.hiveTablePartition.getStorageDescriptor().compareTo(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "44df52a0ff62e1b4ec65efd6a14a8b07d7712e75"}, "originalPosition": 94}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjc1MjQ3Mw==", "bodyText": "It is a short cut for comparing. If the sd is same, SplitNumber must be different.", "url": "https://github.com/apache/flink/pull/12004#discussion_r422752473", "createdAt": "2020-05-11T02:52:18Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/TimestampedHiveInputSplit.java", "diffHunk": "@@ -0,0 +1,232 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive.read;\n+\n+import org.apache.flink.api.common.typeinfo.BasicTypeInfo;\n+import org.apache.flink.api.common.typeinfo.TypeInfo;\n+import org.apache.flink.api.common.typeinfo.TypeInfoFactory;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeutils.SimpleTypeSerializerSnapshot;\n+import org.apache.flink.api.common.typeutils.TypeSerializerSnapshot;\n+import org.apache.flink.api.common.typeutils.base.TypeSerializerSingleton;\n+import org.apache.flink.api.java.typeutils.runtime.DataInputViewStream;\n+import org.apache.flink.api.java.typeutils.runtime.DataOutputViewStream;\n+import org.apache.flink.core.memory.DataInputView;\n+import org.apache.flink.core.memory.DataOutputView;\n+import org.apache.flink.streaming.api.functions.source.TimestampedInputSplit;\n+import org.apache.flink.util.FlinkRuntimeException;\n+import org.apache.flink.util.InstantiationUtil;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.lang.reflect.Type;\n+import java.util.Map;\n+import java.util.Objects;\n+\n+/**\n+ * A {@link HiveTableInputSplit} with {@link TimestampedInputSplit}.\n+ * Kryo serializer can not deal with hadoop split, need specific type information factory.\n+ */\n+@TypeInfo(TimestampedHiveInputSplit.SplitTypeInfoFactory.class)\n+public class TimestampedHiveInputSplit extends HiveTableInputSplit implements TimestampedInputSplit {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\t/** The modification time of the file this split belongs to. */\n+\tprivate final long modificationTime;\n+\n+\t/**\n+\t * The state of the split. This information is used when\n+\t * restoring from a checkpoint and allows to resume reading the\n+\t * underlying file from the point we left off.\n+\t * */\n+\tprivate Serializable splitState;\n+\n+\tpublic TimestampedHiveInputSplit(\n+\t\t\tlong modificationTime,\n+\t\t\tHiveTableInputSplit split) {\n+\t\tsuper(\n+\t\t\t\tsplit.getSplitNumber(),\n+\t\t\t\tsplit.getHadoopInputSplit(),\n+\t\t\t\tsplit.getJobConf(),\n+\t\t\t\tsplit.getHiveTablePartition());\n+\t\tthis.modificationTime = modificationTime;\n+\t}\n+\n+\t@Override\n+\tpublic void setSplitState(Serializable state) {\n+\t\tthis.splitState = state;\n+\t}\n+\n+\t@Override\n+\tpublic Serializable getSplitState() {\n+\t\treturn this.splitState;\n+\t}\n+\n+\t@Override\n+\tpublic long getModificationTime() {\n+\t\treturn modificationTime;\n+\t}\n+\n+\t@Override\n+\tpublic int compareTo(TimestampedInputSplit o) {\n+\t\tTimestampedHiveInputSplit split = (TimestampedHiveInputSplit) o;\n+\t\tint modTimeComp = Long.compare(this.modificationTime, split.modificationTime);\n+\t\tif (modTimeComp != 0L) {\n+\t\t\treturn modTimeComp;\n+\t\t}\n+\n+\t\tint pathComp = this.hiveTablePartition.getStorageDescriptor().compareTo(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ4NDU2OQ=="}, "originalCommit": {"oid": "44df52a0ff62e1b4ec65efd6a14a8b07d7712e75"}, "originalPosition": 94}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMDQwODMwOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemOptions.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQxMToyOTo0MVrOGS6dDw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMVQwMjo0OTo0OVrOGTKu5g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ4NTI2Mw==", "bodyText": "Wondering what's the relationship between this and the execution mode? E.g. can I enable streaming source with batch execution mode?", "url": "https://github.com/apache/flink/pull/12004#discussion_r422485263", "createdAt": "2020-05-09T11:29:41Z", "author": {"login": "lirui-apache"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemOptions.java", "diffHunk": "@@ -0,0 +1,95 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.configuration.ConfigOption;\n+\n+import java.time.Duration;\n+\n+import static org.apache.flink.configuration.ConfigOptions.key;\n+\n+/**\n+ * This class holds configuration constants used by filesystem(Including hive) connector.\n+ */\n+public class FileSystemOptions {\n+\n+\tpublic static final ConfigOption<Boolean> STREAMING_SOURCE_ENABLE =", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "44df52a0ff62e1b4ec65efd6a14a8b07d7712e75"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjc1MTk3NA==", "bodyText": "No, but streaming execution mode can run both streaming source and batch source.\nBatch execution mode is a subset of streaming execution.", "url": "https://github.com/apache/flink/pull/12004#discussion_r422751974", "createdAt": "2020-05-11T02:49:49Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemOptions.java", "diffHunk": "@@ -0,0 +1,95 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.configuration.ConfigOption;\n+\n+import java.time.Duration;\n+\n+import static org.apache.flink.configuration.ConfigOptions.key;\n+\n+/**\n+ * This class holds configuration constants used by filesystem(Including hive) connector.\n+ */\n+public class FileSystemOptions {\n+\n+\tpublic static final ConfigOption<Boolean> STREAMING_SOURCE_ENABLE =", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ4NTI2Mw=="}, "originalCommit": {"oid": "44df52a0ff62e1b4ec65efd6a14a8b07d7712e75"}, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMDQxMDg5OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemOptions.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQxMTozMzozM1rOGS6eVg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMVQwMjo1NToxN1rOGTKy-w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ4NTU5MA==", "bodyText": "Why it's the minimum interval? It may seem confusing to users.", "url": "https://github.com/apache/flink/pull/12004#discussion_r422485590", "createdAt": "2020-05-09T11:33:33Z", "author": {"login": "lirui-apache"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemOptions.java", "diffHunk": "@@ -0,0 +1,95 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.configuration.ConfigOption;\n+\n+import java.time.Duration;\n+\n+import static org.apache.flink.configuration.ConfigOptions.key;\n+\n+/**\n+ * This class holds configuration constants used by filesystem(Including hive) connector.\n+ */\n+public class FileSystemOptions {\n+\n+\tpublic static final ConfigOption<Boolean> STREAMING_SOURCE_ENABLE =\n+\t\t\tkey(\"streaming-source.enable\")\n+\t\t\t\t\t.booleanType()\n+\t\t\t\t\t.defaultValue(false)\n+\t\t\t\t\t.withDescription(\"Enable streaming source or not.\");\n+\n+\tpublic static final ConfigOption<Duration> STREAMING_SOURCE_MONITOR_INTERVAL =\n+\t\t\tkey(\"streaming-source.monitor-interval\")\n+\t\t\t\t\t.durationType()\n+\t\t\t\t\t.defaultValue(Duration.ofMillis(1))\n+\t\t\t\t\t.withDescription(\"The minimum interval allowed between consecutive partition/file discovery.\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "44df52a0ff62e1b4ec65efd6a14a8b07d7712e75"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjc1MzAxOQ==", "bodyText": "Time interval for consecutively monitoring partition/file.", "url": "https://github.com/apache/flink/pull/12004#discussion_r422753019", "createdAt": "2020-05-11T02:55:17Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemOptions.java", "diffHunk": "@@ -0,0 +1,95 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.configuration.ConfigOption;\n+\n+import java.time.Duration;\n+\n+import static org.apache.flink.configuration.ConfigOptions.key;\n+\n+/**\n+ * This class holds configuration constants used by filesystem(Including hive) connector.\n+ */\n+public class FileSystemOptions {\n+\n+\tpublic static final ConfigOption<Boolean> STREAMING_SOURCE_ENABLE =\n+\t\t\tkey(\"streaming-source.enable\")\n+\t\t\t\t\t.booleanType()\n+\t\t\t\t\t.defaultValue(false)\n+\t\t\t\t\t.withDescription(\"Enable streaming source or not.\");\n+\n+\tpublic static final ConfigOption<Duration> STREAMING_SOURCE_MONITOR_INTERVAL =\n+\t\t\tkey(\"streaming-source.monitor-interval\")\n+\t\t\t\t\t.durationType()\n+\t\t\t\t\t.defaultValue(Duration.ofMillis(1))\n+\t\t\t\t\t.withDescription(\"The minimum interval allowed between consecutive partition/file discovery.\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ4NTU5MA=="}, "originalCommit": {"oid": "44df52a0ff62e1b4ec65efd6a14a8b07d7712e75"}, "originalPosition": 42}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMDQxMzcxOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemOptions.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQxMTozOTowNFrOGS6f0g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQxMTozOTowNFrOGS6f0g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ4NTk3MA==", "bodyText": "I think we'd better clarify this is not the partition create time in HMS, but the folder/file create time in filesystem?", "url": "https://github.com/apache/flink/pull/12004#discussion_r422485970", "createdAt": "2020-05-09T11:39:04Z", "author": {"login": "lirui-apache"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemOptions.java", "diffHunk": "@@ -0,0 +1,95 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.configuration.ConfigOption;\n+\n+import java.time.Duration;\n+\n+import static org.apache.flink.configuration.ConfigOptions.key;\n+\n+/**\n+ * This class holds configuration constants used by filesystem(Including hive) connector.\n+ */\n+public class FileSystemOptions {\n+\n+\tpublic static final ConfigOption<Boolean> STREAMING_SOURCE_ENABLE =\n+\t\t\tkey(\"streaming-source.enable\")\n+\t\t\t\t\t.booleanType()\n+\t\t\t\t\t.defaultValue(false)\n+\t\t\t\t\t.withDescription(\"Enable streaming source or not.\");\n+\n+\tpublic static final ConfigOption<Duration> STREAMING_SOURCE_MONITOR_INTERVAL =\n+\t\t\tkey(\"streaming-source.monitor-interval\")\n+\t\t\t\t\t.durationType()\n+\t\t\t\t\t.defaultValue(Duration.ofMillis(1))\n+\t\t\t\t\t.withDescription(\"The minimum interval allowed between consecutive partition/file discovery.\");\n+\n+\tpublic static final ConfigOption<String> STREAMING_SOURCE_CONSUME_ORDER =\n+\t\t\tkey(\"streaming-source.consume-order\")\n+\t\t\t\t\t.stringType()\n+\t\t\t\t\t.defaultValue(\"create-time\")\n+\t\t\t\t\t.withDescription(\"The consume order of streaming source,\" +\n+\t\t\t\t\t\t\t\" support create-time and partition-time.\" +\n+\t\t\t\t\t\t\t\" create-time compare partition/file creation time;\" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "44df52a0ff62e1b4ec65efd6a14a8b07d7712e75"}, "originalPosition": 50}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMDQxNjAwOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemOptions.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQxMTo0NDowOVrOGS6hHw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMVQwMjo1NzoyOFrOGTK0ng==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ4NjMwMw==", "bodyText": "Clarify this should only be used if STREAMING_SOURCE_CONSUME_ORDER is set to partition-time?", "url": "https://github.com/apache/flink/pull/12004#discussion_r422486303", "createdAt": "2020-05-09T11:44:09Z", "author": {"login": "lirui-apache"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemOptions.java", "diffHunk": "@@ -0,0 +1,95 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.configuration.ConfigOption;\n+\n+import java.time.Duration;\n+\n+import static org.apache.flink.configuration.ConfigOptions.key;\n+\n+/**\n+ * This class holds configuration constants used by filesystem(Including hive) connector.\n+ */\n+public class FileSystemOptions {\n+\n+\tpublic static final ConfigOption<Boolean> STREAMING_SOURCE_ENABLE =\n+\t\t\tkey(\"streaming-source.enable\")\n+\t\t\t\t\t.booleanType()\n+\t\t\t\t\t.defaultValue(false)\n+\t\t\t\t\t.withDescription(\"Enable streaming source or not.\");\n+\n+\tpublic static final ConfigOption<Duration> STREAMING_SOURCE_MONITOR_INTERVAL =\n+\t\t\tkey(\"streaming-source.monitor-interval\")\n+\t\t\t\t\t.durationType()\n+\t\t\t\t\t.defaultValue(Duration.ofMillis(1))\n+\t\t\t\t\t.withDescription(\"The minimum interval allowed between consecutive partition/file discovery.\");\n+\n+\tpublic static final ConfigOption<String> STREAMING_SOURCE_CONSUME_ORDER =\n+\t\t\tkey(\"streaming-source.consume-order\")\n+\t\t\t\t\t.stringType()\n+\t\t\t\t\t.defaultValue(\"create-time\")\n+\t\t\t\t\t.withDescription(\"The consume order of streaming source,\" +\n+\t\t\t\t\t\t\t\" support create-time and partition-time.\" +\n+\t\t\t\t\t\t\t\" create-time compare partition/file creation time;\" +\n+\t\t\t\t\t\t\t\" partition-time compare time represented by partition name.\");\n+\n+\tpublic static final ConfigOption<String> STREAMING_SOURCE_CONSUME_START_OFFSET =\n+\t\t\tkey(\"streaming-source.consume-start-offset\")\n+\t\t\t\t\t.stringType()\n+\t\t\t\t\t.defaultValue(\"1970-00-00\")\n+\t\t\t\t\t.withDescription(\"Start offset for streaming consuming.\" +\n+\t\t\t\t\t\t\t\" How to parse and compare offsets depends on your order.\" +\n+\t\t\t\t\t\t\t\" For create-time and partition-time, should be a timestamp string.\");\n+\n+\tpublic static final ConfigOption<String> PARTITION_TIME_EXTRACTOR_TYPE =\n+\t\t\tkey(\"partition.time-extractor.type\")\n+\t\t\t\t\t.stringType()\n+\t\t\t\t\t.defaultValue(\"timestamp\")\n+\t\t\t\t\t.withDescription(\"Time extractor to extract time from partition values.\" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "44df52a0ff62e1b4ec65efd6a14a8b07d7712e75"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjc1MzQzOA==", "bodyText": "Yes, will add comments.", "url": "https://github.com/apache/flink/pull/12004#discussion_r422753438", "createdAt": "2020-05-11T02:57:28Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemOptions.java", "diffHunk": "@@ -0,0 +1,95 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.configuration.ConfigOption;\n+\n+import java.time.Duration;\n+\n+import static org.apache.flink.configuration.ConfigOptions.key;\n+\n+/**\n+ * This class holds configuration constants used by filesystem(Including hive) connector.\n+ */\n+public class FileSystemOptions {\n+\n+\tpublic static final ConfigOption<Boolean> STREAMING_SOURCE_ENABLE =\n+\t\t\tkey(\"streaming-source.enable\")\n+\t\t\t\t\t.booleanType()\n+\t\t\t\t\t.defaultValue(false)\n+\t\t\t\t\t.withDescription(\"Enable streaming source or not.\");\n+\n+\tpublic static final ConfigOption<Duration> STREAMING_SOURCE_MONITOR_INTERVAL =\n+\t\t\tkey(\"streaming-source.monitor-interval\")\n+\t\t\t\t\t.durationType()\n+\t\t\t\t\t.defaultValue(Duration.ofMillis(1))\n+\t\t\t\t\t.withDescription(\"The minimum interval allowed between consecutive partition/file discovery.\");\n+\n+\tpublic static final ConfigOption<String> STREAMING_SOURCE_CONSUME_ORDER =\n+\t\t\tkey(\"streaming-source.consume-order\")\n+\t\t\t\t\t.stringType()\n+\t\t\t\t\t.defaultValue(\"create-time\")\n+\t\t\t\t\t.withDescription(\"The consume order of streaming source,\" +\n+\t\t\t\t\t\t\t\" support create-time and partition-time.\" +\n+\t\t\t\t\t\t\t\" create-time compare partition/file creation time;\" +\n+\t\t\t\t\t\t\t\" partition-time compare time represented by partition name.\");\n+\n+\tpublic static final ConfigOption<String> STREAMING_SOURCE_CONSUME_START_OFFSET =\n+\t\t\tkey(\"streaming-source.consume-start-offset\")\n+\t\t\t\t\t.stringType()\n+\t\t\t\t\t.defaultValue(\"1970-00-00\")\n+\t\t\t\t\t.withDescription(\"Start offset for streaming consuming.\" +\n+\t\t\t\t\t\t\t\" How to parse and compare offsets depends on your order.\" +\n+\t\t\t\t\t\t\t\" For create-time and partition-time, should be a timestamp string.\");\n+\n+\tpublic static final ConfigOption<String> PARTITION_TIME_EXTRACTOR_TYPE =\n+\t\t\tkey(\"partition.time-extractor.type\")\n+\t\t\t\t\t.stringType()\n+\t\t\t\t\t.defaultValue(\"timestamp\")\n+\t\t\t\t\t.withDescription(\"Time extractor to extract time from partition values.\" +", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ4NjMwMw=="}, "originalCommit": {"oid": "44df52a0ff62e1b4ec65efd6a14a8b07d7712e75"}, "originalPosition": 65}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMDQxNjg1OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemOptions.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQxMTo0NTo0MVrOGS6hig==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQxMTo0NTo0MVrOGS6hig==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ4NjQxMA==", "bodyText": "Seems unused", "url": "https://github.com/apache/flink/pull/12004#discussion_r422486410", "createdAt": "2020-05-09T11:45:41Z", "author": {"login": "lirui-apache"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemOptions.java", "diffHunk": "@@ -0,0 +1,95 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.configuration.ConfigOption;\n+\n+import java.time.Duration;\n+\n+import static org.apache.flink.configuration.ConfigOptions.key;\n+\n+/**\n+ * This class holds configuration constants used by filesystem(Including hive) connector.\n+ */\n+public class FileSystemOptions {\n+\n+\tpublic static final ConfigOption<Boolean> STREAMING_SOURCE_ENABLE =\n+\t\t\tkey(\"streaming-source.enable\")\n+\t\t\t\t\t.booleanType()\n+\t\t\t\t\t.defaultValue(false)\n+\t\t\t\t\t.withDescription(\"Enable streaming source or not.\");\n+\n+\tpublic static final ConfigOption<Duration> STREAMING_SOURCE_MONITOR_INTERVAL =\n+\t\t\tkey(\"streaming-source.monitor-interval\")\n+\t\t\t\t\t.durationType()\n+\t\t\t\t\t.defaultValue(Duration.ofMillis(1))\n+\t\t\t\t\t.withDescription(\"The minimum interval allowed between consecutive partition/file discovery.\");\n+\n+\tpublic static final ConfigOption<String> STREAMING_SOURCE_CONSUME_ORDER =\n+\t\t\tkey(\"streaming-source.consume-order\")\n+\t\t\t\t\t.stringType()\n+\t\t\t\t\t.defaultValue(\"create-time\")\n+\t\t\t\t\t.withDescription(\"The consume order of streaming source,\" +\n+\t\t\t\t\t\t\t\" support create-time and partition-time.\" +\n+\t\t\t\t\t\t\t\" create-time compare partition/file creation time;\" +\n+\t\t\t\t\t\t\t\" partition-time compare time represented by partition name.\");\n+\n+\tpublic static final ConfigOption<String> STREAMING_SOURCE_CONSUME_START_OFFSET =\n+\t\t\tkey(\"streaming-source.consume-start-offset\")\n+\t\t\t\t\t.stringType()\n+\t\t\t\t\t.defaultValue(\"1970-00-00\")\n+\t\t\t\t\t.withDescription(\"Start offset for streaming consuming.\" +\n+\t\t\t\t\t\t\t\" How to parse and compare offsets depends on your order.\" +\n+\t\t\t\t\t\t\t\" For create-time and partition-time, should be a timestamp string.\");\n+\n+\tpublic static final ConfigOption<String> PARTITION_TIME_EXTRACTOR_TYPE =\n+\t\t\tkey(\"partition.time-extractor.type\")\n+\t\t\t\t\t.stringType()\n+\t\t\t\t\t.defaultValue(\"timestamp\")\n+\t\t\t\t\t.withDescription(\"Time extractor to extract time from partition values.\" +\n+\t\t\t\t\t\t\t\" Support timestamp and custom.\" +\n+\t\t\t\t\t\t\t\" For timestamp, can configure timestamp pattern.\" +\n+\t\t\t\t\t\t\t\" For custom, should configure extractor class.\");\n+\n+\tpublic static final ConfigOption<String> PARTITION_TIME_EXTRACTOR_CLASS =\n+\t\t\tkey(\"partition.time-extractor.class\")\n+\t\t\t\t\t.stringType()\n+\t\t\t\t\t.noDefaultValue()\n+\t\t\t\t\t.withDescription(\"The extractor class for implement PartitionTimeExtractor interface.\");\n+\n+\tpublic static final ConfigOption<String> PARTITION_TIME_EXTRACTOR_TIMESTAMP_PATTERN =\n+\t\t\tkey(\"partition.time-extractor.timestamp-pattern\")\n+\t\t\t\t\t.stringType()\n+\t\t\t\t\t.noDefaultValue()\n+\t\t\t\t\t.withDescription(\"The 'timestamp' construction way allows users to use partition\" +\n+\t\t\t\t\t\t\t\" fields to get a legal timestamp pattern.\" +\n+\t\t\t\t\t\t\t\" Default support 'yyyy-mm-dd hh:mm:ss' from first field.\" +\n+\t\t\t\t\t\t\t\" If timestamp in partition is single field 'dt', can configure: '$dt'.\" +\n+\t\t\t\t\t\t\t\" If timestamp in partition is year, month, day, hour,\" +\n+\t\t\t\t\t\t\t\" can configure: '$year-$month-$day $hour:00:00'.\" +\n+\t\t\t\t\t\t\t\" If timestamp in partition is dt and hour, can configure: '$dt $hour:00:00'.\");\n+\n+\tpublic static final ConfigOption<Duration> PARTITION_TIME_INTERVAL =", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "44df52a0ff62e1b4ec65efd6a14a8b07d7712e75"}, "originalPosition": 88}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMDQxNzY2OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/PartitionTimeExtractor.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQxMTo0NzozM1rOGS6iBQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQxMTo0NzozM1rOGS6iBQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ4NjUzMw==", "bodyText": "Should add constants for these values", "url": "https://github.com/apache/flink/pull/12004#discussion_r422486533", "createdAt": "2020-05-09T11:47:33Z", "author": {"login": "lirui-apache"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/PartitionTimeExtractor.java", "diffHunk": "@@ -0,0 +1,58 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.annotation.Experimental;\n+\n+import java.io.Serializable;\n+import java.time.LocalDateTime;\n+import java.util.List;\n+\n+/**\n+ * Time extractor to extract time from partition values.\n+ */\n+@Experimental\n+public interface PartitionTimeExtractor extends Serializable {\n+\n+\t/**\n+\t * Extract time from partition keys and values.\n+\t */\n+\tLocalDateTime extract(List<String> partitionKeys, List<String> partitionValues);\n+\n+\tstatic PartitionTimeExtractor create(\n+\t\t\tClassLoader userClassLoader,\n+\t\t\tString extractorType,\n+\t\t\tString extractorClass,\n+\t\t\tString extractorPattern) {\n+\t\tswitch (extractorType) {\n+\t\t\tcase \"timestamp\":", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "44df52a0ff62e1b4ec65efd6a14a8b07d7712e75"}, "originalPosition": 44}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMDQyMDgyOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemOptions.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQxMTo1MzozNVrOGS6jtw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMVQwMjo1ODo1OVrOGTK15w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ4Njk2Nw==", "bodyText": "I find timestamp somehow difficult to understand. According to the PartitionTimeExtractor interface, any implementation should extract a \"timestamp\". We may as well just call it default?", "url": "https://github.com/apache/flink/pull/12004#discussion_r422486967", "createdAt": "2020-05-09T11:53:35Z", "author": {"login": "lirui-apache"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemOptions.java", "diffHunk": "@@ -0,0 +1,95 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.configuration.ConfigOption;\n+\n+import java.time.Duration;\n+\n+import static org.apache.flink.configuration.ConfigOptions.key;\n+\n+/**\n+ * This class holds configuration constants used by filesystem(Including hive) connector.\n+ */\n+public class FileSystemOptions {\n+\n+\tpublic static final ConfigOption<Boolean> STREAMING_SOURCE_ENABLE =\n+\t\t\tkey(\"streaming-source.enable\")\n+\t\t\t\t\t.booleanType()\n+\t\t\t\t\t.defaultValue(false)\n+\t\t\t\t\t.withDescription(\"Enable streaming source or not.\");\n+\n+\tpublic static final ConfigOption<Duration> STREAMING_SOURCE_MONITOR_INTERVAL =\n+\t\t\tkey(\"streaming-source.monitor-interval\")\n+\t\t\t\t\t.durationType()\n+\t\t\t\t\t.defaultValue(Duration.ofMillis(1))\n+\t\t\t\t\t.withDescription(\"The minimum interval allowed between consecutive partition/file discovery.\");\n+\n+\tpublic static final ConfigOption<String> STREAMING_SOURCE_CONSUME_ORDER =\n+\t\t\tkey(\"streaming-source.consume-order\")\n+\t\t\t\t\t.stringType()\n+\t\t\t\t\t.defaultValue(\"create-time\")\n+\t\t\t\t\t.withDescription(\"The consume order of streaming source,\" +\n+\t\t\t\t\t\t\t\" support create-time and partition-time.\" +\n+\t\t\t\t\t\t\t\" create-time compare partition/file creation time;\" +\n+\t\t\t\t\t\t\t\" partition-time compare time represented by partition name.\");\n+\n+\tpublic static final ConfigOption<String> STREAMING_SOURCE_CONSUME_START_OFFSET =\n+\t\t\tkey(\"streaming-source.consume-start-offset\")\n+\t\t\t\t\t.stringType()\n+\t\t\t\t\t.defaultValue(\"1970-00-00\")\n+\t\t\t\t\t.withDescription(\"Start offset for streaming consuming.\" +\n+\t\t\t\t\t\t\t\" How to parse and compare offsets depends on your order.\" +\n+\t\t\t\t\t\t\t\" For create-time and partition-time, should be a timestamp string.\");\n+\n+\tpublic static final ConfigOption<String> PARTITION_TIME_EXTRACTOR_TYPE =\n+\t\t\tkey(\"partition.time-extractor.type\")\n+\t\t\t\t\t.stringType()\n+\t\t\t\t\t.defaultValue(\"timestamp\")\n+\t\t\t\t\t.withDescription(\"Time extractor to extract time from partition values.\" +\n+\t\t\t\t\t\t\t\" Support timestamp and custom.\" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "44df52a0ff62e1b4ec65efd6a14a8b07d7712e75"}, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjc1Mzc2Nw==", "bodyText": "+1", "url": "https://github.com/apache/flink/pull/12004#discussion_r422753767", "createdAt": "2020-05-11T02:58:59Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemOptions.java", "diffHunk": "@@ -0,0 +1,95 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.configuration.ConfigOption;\n+\n+import java.time.Duration;\n+\n+import static org.apache.flink.configuration.ConfigOptions.key;\n+\n+/**\n+ * This class holds configuration constants used by filesystem(Including hive) connector.\n+ */\n+public class FileSystemOptions {\n+\n+\tpublic static final ConfigOption<Boolean> STREAMING_SOURCE_ENABLE =\n+\t\t\tkey(\"streaming-source.enable\")\n+\t\t\t\t\t.booleanType()\n+\t\t\t\t\t.defaultValue(false)\n+\t\t\t\t\t.withDescription(\"Enable streaming source or not.\");\n+\n+\tpublic static final ConfigOption<Duration> STREAMING_SOURCE_MONITOR_INTERVAL =\n+\t\t\tkey(\"streaming-source.monitor-interval\")\n+\t\t\t\t\t.durationType()\n+\t\t\t\t\t.defaultValue(Duration.ofMillis(1))\n+\t\t\t\t\t.withDescription(\"The minimum interval allowed between consecutive partition/file discovery.\");\n+\n+\tpublic static final ConfigOption<String> STREAMING_SOURCE_CONSUME_ORDER =\n+\t\t\tkey(\"streaming-source.consume-order\")\n+\t\t\t\t\t.stringType()\n+\t\t\t\t\t.defaultValue(\"create-time\")\n+\t\t\t\t\t.withDescription(\"The consume order of streaming source,\" +\n+\t\t\t\t\t\t\t\" support create-time and partition-time.\" +\n+\t\t\t\t\t\t\t\" create-time compare partition/file creation time;\" +\n+\t\t\t\t\t\t\t\" partition-time compare time represented by partition name.\");\n+\n+\tpublic static final ConfigOption<String> STREAMING_SOURCE_CONSUME_START_OFFSET =\n+\t\t\tkey(\"streaming-source.consume-start-offset\")\n+\t\t\t\t\t.stringType()\n+\t\t\t\t\t.defaultValue(\"1970-00-00\")\n+\t\t\t\t\t.withDescription(\"Start offset for streaming consuming.\" +\n+\t\t\t\t\t\t\t\" How to parse and compare offsets depends on your order.\" +\n+\t\t\t\t\t\t\t\" For create-time and partition-time, should be a timestamp string.\");\n+\n+\tpublic static final ConfigOption<String> PARTITION_TIME_EXTRACTOR_TYPE =\n+\t\t\tkey(\"partition.time-extractor.type\")\n+\t\t\t\t\t.stringType()\n+\t\t\t\t\t.defaultValue(\"timestamp\")\n+\t\t\t\t\t.withDescription(\"Time extractor to extract time from partition values.\" +\n+\t\t\t\t\t\t\t\" Support timestamp and custom.\" +", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ4Njk2Nw=="}, "originalCommit": {"oid": "44df52a0ff62e1b4ec65efd6a14a8b07d7712e75"}, "originalPosition": 66}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1511, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}