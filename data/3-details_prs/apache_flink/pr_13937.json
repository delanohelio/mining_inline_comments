{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTE1ODkxMzcw", "number": 13937, "title": "[FLINK-19886][hive] Integrate file compaction to Hive connector", "bodyText": "What is the purpose of the change\nIntegrate file compaction to Hive connector\nBrief change log\n\nIntroduce HivePartitionUtils\nIntroduce HiveCompactReader.java\nIntegrate file compaction to HiveTableSink and refactor it.\n\nVerifying this change\nHiveSinkCompactionITCase\nDoes this pull request potentially affect one of the following parts:\n\nDependencies (does it add or upgrade a dependency): no\nThe public API, i.e., is any changed class annotated with @Public(Evolving): no\nThe serializers: no\nThe runtime per-record code paths (performance sensitive): no\nAnything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: no\nThe S3 file system connector: no\n\nDocumentation\n\nDoes this pull request introduce a new feature? yes\nIf yes, how is the feature documented? JavaDocs", "createdAt": "2020-11-05T08:40:47Z", "url": "https://github.com/apache/flink/pull/13937", "merged": true, "mergeCommit": {"oid": "f1f25e012b3fc7d338f169871addbd78b86f33bf"}, "closed": true, "closedAt": "2020-11-07T03:39:00Z", "author": {"login": "JingsongLi"}, "timelineItems": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdZeNzLAH2gAyNTE1ODkxMzcwOjc5OWFiNmEzNjlmY2ZhYThkZjE3NjVjNjhiZjc3YTk2MGFiZDA5Nzc=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdaCmkMAFqTUyNTU4NzAwNQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "799ab6a369fcfaa8df1765c68bf77a960abd0977", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/flink/commit/799ab6a369fcfaa8df1765c68bf77a960abd0977", "committedDate": "2020-11-05T08:37:34Z", "message": "[FLINK-19886][hive] Integrate file compaction to Hive connector"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0d6b4e7fbd9948ee6c4db339bc49b93a823c3b88", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/flink/commit/0d6b4e7fbd9948ee6c4db339bc49b93a823c3b88", "committedDate": "2020-11-05T08:41:03Z", "message": "Remove restorePartitionValueFromType in HiveTableSource"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI1MDUxNjA0", "url": "https://github.com/apache/flink/pull/13937#pullrequestreview-525051604", "createdAt": "2020-11-06T10:59:02Z", "commit": {"oid": "0d6b4e7fbd9948ee6c4db339bc49b93a823c3b88"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQxMDo1OTowMlrOHupgwA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQxMTo0Mzo1MFrOHuq5kA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODY3NjY3Mg==", "bodyText": "Why do we need a CompactBulkReader? It seems all we need is just a CompactReader.Factory?", "url": "https://github.com/apache/flink/pull/13937#discussion_r518676672", "createdAt": "2020-11-06T10:59:02Z", "author": {"login": "lirui-apache"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveCompactReader.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive.read;\n+\n+import org.apache.flink.connector.file.src.reader.BulkFormat;\n+import org.apache.flink.connectors.hive.HiveTablePartition;\n+import org.apache.flink.connectors.hive.JobConfWrapper;\n+import org.apache.flink.core.fs.FileSystem;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.catalog.hive.client.HiveShim;\n+import org.apache.flink.table.catalog.hive.client.HiveShimLoader;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.filesystem.stream.compact.CompactBulkReader;\n+import org.apache.flink.table.filesystem.stream.compact.CompactContext;\n+import org.apache.flink.table.filesystem.stream.compact.CompactReader;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.mapred.JobConf;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+import static org.apache.flink.connectors.hive.util.HivePartitionUtils.restorePartitionValueFromType;\n+import static org.apache.flink.table.utils.PartitionPathUtils.extractPartitionSpecFromPath;\n+\n+/**\n+ * The {@link CompactReader} to delegate hive bulk format.\n+ */\n+public class HiveCompactReader extends CompactBulkReader<RowData> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0d6b4e7fbd9948ee6c4db339bc49b93a823c3b88"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODY4MTczNg==", "bodyText": "Is it possible that partition spec cannot be extracted from path? E.g. when we write to a static partition and the partition location is different from the default one?", "url": "https://github.com/apache/flink/pull/13937#discussion_r518681736", "createdAt": "2020-11-06T11:07:02Z", "author": {"login": "lirui-apache"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveCompactReader.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive.read;\n+\n+import org.apache.flink.connector.file.src.reader.BulkFormat;\n+import org.apache.flink.connectors.hive.HiveTablePartition;\n+import org.apache.flink.connectors.hive.JobConfWrapper;\n+import org.apache.flink.core.fs.FileSystem;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.catalog.hive.client.HiveShim;\n+import org.apache.flink.table.catalog.hive.client.HiveShimLoader;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.filesystem.stream.compact.CompactBulkReader;\n+import org.apache.flink.table.filesystem.stream.compact.CompactContext;\n+import org.apache.flink.table.filesystem.stream.compact.CompactReader;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.mapred.JobConf;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+import static org.apache.flink.connectors.hive.util.HivePartitionUtils.restorePartitionValueFromType;\n+import static org.apache.flink.table.utils.PartitionPathUtils.extractPartitionSpecFromPath;\n+\n+/**\n+ * The {@link CompactReader} to delegate hive bulk format.\n+ */\n+public class HiveCompactReader extends CompactBulkReader<RowData> {\n+\n+\tprivate HiveCompactReader(BulkFormat.Reader<RowData> reader) throws IOException {\n+\t\tsuper(reader);\n+\t}\n+\n+\tpublic static CompactReader.Factory<RowData> factory(\n+\t\t\tStorageDescriptor sd,\n+\t\t\tProperties properties,\n+\t\t\tJobConf jobConf,\n+\t\t\tCatalogTable catalogTable,\n+\t\t\tString hiveVersion,\n+\t\t\tRowType producedRowType,\n+\t\t\tboolean useMapRedReader) {\n+\t\treturn new Factory(\n+\t\t\t\tsd,\n+\t\t\t\tproperties,\n+\t\t\t\tnew JobConfWrapper(jobConf),\n+\t\t\t\tcatalogTable.getPartitionKeys(),\n+\t\t\t\tcatalogTable.getSchema().getFieldNames(),\n+\t\t\t\tcatalogTable.getSchema().getFieldDataTypes(),\n+\t\t\t\thiveVersion,\n+\t\t\t\tproducedRowType,\n+\t\t\t\tuseMapRedReader);\n+\t}\n+\n+\t/**\n+\t * Factory to create {@link HiveCompactReader}.\n+\t */\n+\tprivate static class Factory implements CompactReader.Factory<RowData> {\n+\n+\t\tprivate static final long serialVersionUID = 1L;\n+\n+\t\tprivate final StorageDescriptor sd;\n+\t\tprivate final Properties properties;\n+\t\tprivate final JobConfWrapper jobConfWrapper;\n+\t\tprivate final List<String> partitionKeys;\n+\t\tprivate final String[] fieldNames;\n+\t\tprivate final DataType[] fieldTypes;\n+\t\tprivate final String hiveVersion;\n+\t\tprivate final HiveShim shim;\n+\t\tprivate final RowType producedRowType;\n+\t\tprivate final boolean useMapRedReader;\n+\n+\t\tprivate Factory(\n+\t\t\t\tStorageDescriptor sd,\n+\t\t\t\tProperties properties,\n+\t\t\t\tJobConfWrapper jobConfWrapper,\n+\t\t\t\tList<String> partitionKeys,\n+\t\t\t\tString[] fieldNames,\n+\t\t\t\tDataType[] fieldTypes,\n+\t\t\t\tString hiveVersion,\n+\t\t\t\tRowType producedRowType,\n+\t\t\t\tboolean useMapRedReader) {\n+\t\t\tthis.sd = sd;\n+\t\t\tthis.properties = properties;\n+\t\t\tthis.jobConfWrapper = jobConfWrapper;\n+\t\t\tthis.partitionKeys = partitionKeys;\n+\t\t\tthis.fieldNames = fieldNames;\n+\t\t\tthis.fieldTypes = fieldTypes;\n+\t\t\tthis.hiveVersion = hiveVersion;\n+\t\t\tthis.shim = HiveShimLoader.loadHiveShim(hiveVersion);\n+\t\t\tthis.producedRowType = producedRowType;\n+\t\t\tthis.useMapRedReader = useMapRedReader;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic CompactReader<RowData> create(CompactContext context) throws IOException {\n+\t\t\tHiveSourceSplit split = createSplit(context.getPath(), context.getFileSystem());\n+\t\t\tHiveBulkFormatAdapter format = new HiveBulkFormatAdapter(\n+\t\t\t\t\tjobConfWrapper, partitionKeys, fieldNames, fieldTypes, hiveVersion, producedRowType, useMapRedReader);\n+\t\t\tBulkFormat.Reader<RowData> reader = format.createReader(context.getConfig(), split);\n+\t\t\treturn new HiveCompactReader(reader);\n+\t\t}\n+\n+\t\tprivate HiveSourceSplit createSplit(Path path, FileSystem fs) throws IOException {\n+\t\t\tlong len = fs.getFileStatus(path).getLen();\n+\t\t\treturn new HiveSourceSplit(\"id\", path, 0, len, new String[0], null, createPartition(path));\n+\t\t}\n+\n+\t\tprivate HiveTablePartition createPartition(Path path) {\n+\t\t\tMap<String, Object> partitionSpec = new LinkedHashMap<>();\n+\t\t\tMap<String, DataType> nameToTypes = new HashMap<>();\n+\t\t\tfor (int i = 0; i < fieldNames.length; i++) {\n+\t\t\t\tnameToTypes.put(fieldNames[i], fieldTypes[i]);\n+\t\t\t}\n+\t\t\tfor (Map.Entry<String, String> entry : extractPartitionSpecFromPath(path).entrySet()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0d6b4e7fbd9948ee6c4db339bc49b93a823c3b88"}, "originalPosition": 138}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODY5OTQwOA==", "bodyText": "I think only HiveTableSource performs parallelism inference. But this test won't use HiveTableSource, right?", "url": "https://github.com/apache/flink/pull/13937#discussion_r518699408", "createdAt": "2020-11-06T11:43:50Z", "author": {"login": "lirui-apache"}, "path": "flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveSinkCompactionITCase.java", "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+import org.apache.flink.table.api.SqlDialect;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.catalog.exceptions.TableNotExistException;\n+import org.apache.flink.table.catalog.hive.HiveCatalog;\n+import org.apache.flink.table.catalog.hive.HiveTestUtils;\n+import org.apache.flink.table.planner.runtime.stream.sql.CompactionITCaseBase;\n+\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Collection;\n+\n+/**\n+ * Test sink file compaction of hive tables.\n+ */\n+@RunWith(Parameterized.class)\n+public class HiveSinkCompactionITCase extends CompactionITCaseBase {\n+\n+\t@Parameterized.Parameters(name = \"format = {0}\")\n+\tpublic static Collection<String> parameters() {\n+\t\treturn Arrays.asList(\"sequencefile\", \"parquet\");\n+\t}\n+\n+\t@Parameterized.Parameter\n+\tpublic String format;\n+\n+\tprivate HiveCatalog hiveCatalog;\n+\n+\t@Override\n+\t@Before\n+\tpublic void init() throws IOException {\n+\t\thiveCatalog = HiveTestUtils.createHiveCatalog();\n+\t\ttEnv().registerCatalog(hiveCatalog.getName(), hiveCatalog);\n+\t\ttEnv().useCatalog(hiveCatalog.getName());\n+\n+\t\t// avoid too large parallelism lead to scheduler dead lock in streaming mode\n+\t\ttEnv().getConfig().getConfiguration().set(\n+\t\t\t\tHiveOptions.TABLE_EXEC_HIVE_INFER_SOURCE_PARALLELISM, false);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0d6b4e7fbd9948ee6c4db339bc49b93a823c3b88"}, "originalPosition": 62}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ac28db597b7f045cd6e749baf6ea3ae86d2ad2a2", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/flink/commit/ac28db597b7f045cd6e749baf6ea3ae86d2ad2a2", "committedDate": "2020-11-06T14:43:23Z", "message": "Address comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI1NTg3MDA1", "url": "https://github.com/apache/flink/pull/13937#pullrequestreview-525587005", "createdAt": "2020-11-07T03:01:12Z", "commit": {"oid": "ac28db597b7f045cd6e749baf6ea3ae86d2ad2a2"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4818, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}