{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDAwMTkyNTY2", "number": 11656, "reviewThreads": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQwODoyMTozOVrODwTe6Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQwODo1NTowN1rODwUOiA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxOTc3NDQ5OnYy", "diffSide": "RIGHT", "path": "flink-python/pyflink/table/tests/test_pandas_udf.py", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQwODoyMTozOVrOGDOhgw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMFQwNjo1NDoxNlrOGDyrFQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjAzNjg2Nw==", "bodyText": "It is strange to return dict type while the input type is Row. And there is no order guarantee for the field in dict which maybe not consistent with the semantic of Row.\nIs it possible to convert the pd.Series into a Series contains Row?", "url": "https://github.com/apache/flink/pull/11656#discussion_r406036867", "createdAt": "2020-04-09T08:21:39Z", "author": {"login": "hequn8128"}, "path": "flink-python/pyflink/table/tests/test_pandas_udf.py", "diffHunk": "@@ -165,6 +165,12 @@ def nested_array_func(nested_array_param):\n                 'nested_array_param of wrong type %s !' % type(nested_array_param[0])\n             return pd.Series(nested_array_param[0])\n \n+        def row_func(row_param):\n+            assert isinstance(row_param, pd.Series)\n+            assert isinstance(row_param[0], dict), \\", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4597eea52db61944a6fd122f55392eb4a165ea32"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjYyOTE0MQ==", "bodyText": "I think this will be handled internally by Arrow, e.g. it could identify the order of the columns according to the field names and so I think there is no order problems. What do you think?", "url": "https://github.com/apache/flink/pull/11656#discussion_r406629141", "createdAt": "2020-04-10T06:54:16Z", "author": {"login": "dianfu"}, "path": "flink-python/pyflink/table/tests/test_pandas_udf.py", "diffHunk": "@@ -165,6 +165,12 @@ def nested_array_func(nested_array_param):\n                 'nested_array_param of wrong type %s !' % type(nested_array_param[0])\n             return pd.Series(nested_array_param[0])\n \n+        def row_func(row_param):\n+            assert isinstance(row_param, pd.Series)\n+            assert isinstance(row_param[0], dict), \\", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjAzNjg2Nw=="}, "originalCommit": {"oid": "4597eea52db61944a6fd122f55392eb4a165ea32"}, "originalPosition": 15}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxOTg2NjUwOnYy", "diffSide": "RIGHT", "path": "flink-python/src/test/java/org/apache/flink/table/runtime/arrow/RowArrowReaderWriterTest.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQwODo0Njo0NlrOGDPa-A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQwODo0Njo0NlrOGDPa-A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjA1MTU3Ng==", "bodyText": "new Row(n)?", "url": "https://github.com/apache/flink/pull/11656#discussion_r406051576", "createdAt": "2020-04-09T08:46:46Z", "author": {"login": "hequn8128"}, "path": "flink-python/src/test/java/org/apache/flink/table/runtime/arrow/RowArrowReaderWriterTest.java", "diffHunk": "@@ -118,18 +127,20 @@ public static void init() {\n \t\t\tSqlDateTimeUtils.internalToTime(3600000), SqlDateTimeUtils.internalToTime(3600000), SqlDateTimeUtils.internalToTime(3600000), SqlDateTimeUtils.internalToTime(3600000),\n \t\t\tnew Timestamp(3600000), new Timestamp(3600000), new Timestamp(3600000), new Timestamp(3600000),\n \t\t\tnew Timestamp(3600000), new Timestamp(3600000), new Timestamp(3600000), new Timestamp(3600000),\n-\t\t\tnew String[] {null, null, null});\n+\t\t\tnew String[] {null, null, null},\n+\t\t\tRow.of(1, \"hello\", new String[] {null, null, null}, new Timestamp(3600000), Row.of(1, \"hello\")));\n \t\tRow row2 = Row.of(null, (short) 2, 3, 4L, false, 1.0f, 1.0, \"\u4e2d\u6587\", \"\u4e2d\u6587\".getBytes(), new BigDecimal(1), SqlDateTimeUtils.internalToDate(100),\n \t\t\tSqlDateTimeUtils.internalToTime(3600000), SqlDateTimeUtils.internalToTime(3600000), SqlDateTimeUtils.internalToTime(3600000), SqlDateTimeUtils.internalToTime(3600000),\n \t\t\tnew Timestamp(3600000), new Timestamp(3600000), new Timestamp(3600000), new Timestamp(3600000),\n \t\t\tnew Timestamp(3600000), new Timestamp(3600000), new Timestamp(3600000), new Timestamp(3600000),\n-\t\t\tnew String[] {\"hello\", \"\u4e2d\u6587\", null});\n+\t\t\tnew String[] {\"hello\", \"\u4e2d\u6587\", null},\n+\t\t\tRow.of(1, \"hello\", new String[] {\"hello\", \"\u4e2d\u6587\", null}, new Timestamp(3600000), Row.of(1, \"hello\")));\n \t\tRow row3 = Row.of((byte) 1, null, 3, 4L, true, 1.0f, 1.0, \"hello\", \"hello\".getBytes(), new BigDecimal(1), SqlDateTimeUtils.internalToDate(100),\n \t\t\tSqlDateTimeUtils.internalToTime(3600000), SqlDateTimeUtils.internalToTime(3600000), SqlDateTimeUtils.internalToTime(3600000), SqlDateTimeUtils.internalToTime(3600000),\n \t\t\tnew Timestamp(3600000), new Timestamp(3600000), new Timestamp(3600000), new Timestamp(3600000),\n \t\t\tnew Timestamp(3600000), new Timestamp(3600000), new Timestamp(3600000), new Timestamp(3600000),\n-\t\t\tnull);\n-\t\tRow row4 = Row.of(null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null);\n+\t\t\tnull, null);\n+\t\tRow row4 = Row.of(null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4597eea52db61944a6fd122f55392eb4a165ea32"}, "originalPosition": 44}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxOTg5NjQwOnYy", "diffSide": "RIGHT", "path": "flink-python/src/test/java/org/apache/flink/table/runtime/arrow/BaseRowArrowReaderWriterTest.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQwODo1NTowN1rOGDPtzQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQwODo1NTowN1rOGDPtzQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjA1NjM5Nw==", "bodyText": "StreamRecordUtils.baserow(new Object[n]) ?", "url": "https://github.com/apache/flink/pull/11656#discussion_r406056397", "createdAt": "2020-04-09T08:55:07Z", "author": {"login": "hequn8128"}, "path": "flink-python/src/test/java/org/apache/flink/table/runtime/arrow/BaseRowArrowReaderWriterTest.java", "diffHunk": "@@ -152,21 +164,25 @@ public static void init() {\n \t\tBaseRow row1 = StreamRecordUtils.baserow((byte) 1, (short) 2, 3, 4L, true, 1.0f, 1.0, \"hello\", \"hello\".getBytes(), Decimal.fromLong(1, 10, 3), 100, 3600000, 3600000, 3600000, 3600000,\n \t\t\tSqlTimestamp.fromEpochMillis(3600000), SqlTimestamp.fromEpochMillis(3600000), SqlTimestamp.fromEpochMillis(3600000, 100000), SqlTimestamp.fromEpochMillis(3600000, 100000),\n \t\t\tSqlTimestamp.fromEpochMillis(3600000), SqlTimestamp.fromEpochMillis(3600000), SqlTimestamp.fromEpochMillis(3600000, 100000), SqlTimestamp.fromEpochMillis(3600000, 100000),\n-\t\t\tnew GenericArray(new BinaryString[] {BinaryString.fromString(\"hello\"), BinaryString.fromString(\"\u4e2d\u6587\"), null}, 3));\n+\t\t\tnew GenericArray(new BinaryString[] {BinaryString.fromString(\"hello\"), BinaryString.fromString(\"\u4e2d\u6587\"), null}, 3),\n+\t\t\tGenericRow.of(1, BinaryString.fromString(\"hello\"), new GenericArray(new BinaryString[] {BinaryString.fromString(\"hello\")}, 1), SqlTimestamp.fromEpochMillis(3600000), GenericRow.of(1, BinaryString.fromString(\"hello\"))));\n \t\tBinaryRow row2 = StreamRecordUtils.binaryrow((byte) 1, (short) 2, 3, 4L, false, 1.0f, 1.0, \"\u4e2d\u6587\", \"\u4e2d\u6587\".getBytes(), Decimal.fromLong(1, 10, 3), 100, 3600000, 3600000, 3600000, 3600000,\n \t\t\tTuple2.of(SqlTimestamp.fromEpochMillis(3600000), 0), Tuple2.of(SqlTimestamp.fromEpochMillis(3600000), 2), Tuple2.of(SqlTimestamp.fromEpochMillis(3600000, 100000), 4), Tuple2.of(SqlTimestamp.fromEpochMillis(3600000, 100000), 8),\n \t\t\tTuple2.of(SqlTimestamp.fromEpochMillis(3600000), 0), Tuple2.of(SqlTimestamp.fromEpochMillis(3600000), 2), Tuple2.of(SqlTimestamp.fromEpochMillis(3600000, 100000), 4), Tuple2.of(SqlTimestamp.fromEpochMillis(3600000, 100000), 8),\n-\t\t\tTuple2.of(new GenericArray(new String[] {null, null, null}, 3), new BaseArraySerializer(new VarCharType(), null)));\n+\t\t\tTuple2.of(new GenericArray(new String[] {null, null, null}, 3), new BaseArraySerializer(new VarCharType(), null)),\n+\t\t\tTuple2.of(GenericRow.of(1, null, new GenericArray(new BinaryString[] {BinaryString.fromString(\"hello\")}, 1), null, GenericRow.of(1, BinaryString.fromString(\"hello\"))), new BaseRowSerializer(new ExecutionConfig(), rowFieldType)));\n \t\tBaseRow row3 = StreamRecordUtils.baserow(null, (short) 2, 3, 4L, false, 1.0f, 1.0, \"\u4e2d\u6587\", \"\u4e2d\u6587\".getBytes(), Decimal.fromLong(1, 10, 3), 100, 3600000, 3600000, 3600000, 3600000,\n \t\t\tSqlTimestamp.fromEpochMillis(3600000), SqlTimestamp.fromEpochMillis(3600000), SqlTimestamp.fromEpochMillis(3600000, 100000), SqlTimestamp.fromEpochMillis(3600000, 100000),\n \t\t\tSqlTimestamp.fromEpochMillis(3600000), SqlTimestamp.fromEpochMillis(3600000), SqlTimestamp.fromEpochMillis(3600000, 100000), SqlTimestamp.fromEpochMillis(3600000, 100000),\n-\t\t\tnew GenericArray(new String[] {null, null, null}, 3));\n+\t\t\tnew GenericArray(new String[] {null, null, null}, 3),\n+\t\t\tGenericRow.of(1, null, new GenericArray(new BinaryString[] {BinaryString.fromString(\"hello\")}, 1), null, null));\n \t\tBinaryRow row4 = StreamRecordUtils.binaryrow((byte) 1, null, 3, 4L, true, 1.0f, 1.0, \"hello\", \"hello\".getBytes(), Decimal.fromLong(1, 10, 3), 100, 3600000, 3600000, 3600000, 3600000,\n \t\t\tTuple2.of(SqlTimestamp.fromEpochMillis(3600000), 0), Tuple2.of(SqlTimestamp.fromEpochMillis(3600000), 2), Tuple2.of(SqlTimestamp.fromEpochMillis(3600000, 100000), 4), Tuple2.of(SqlTimestamp.fromEpochMillis(3600000, 100000), 8),\n \t\t\tTuple2.of(SqlTimestamp.fromEpochMillis(3600000), 0), Tuple2.of(SqlTimestamp.fromEpochMillis(3600000), 2), Tuple2.of(SqlTimestamp.fromEpochMillis(3600000, 100000), 4), Tuple2.of(SqlTimestamp.fromEpochMillis(3600000, 100000), 8),\n-\t\t\tTuple2.of(new GenericArray(new BinaryString[] {BinaryString.fromString(\"hello\"), BinaryString.fromString(\"\u4e2d\u6587\"), null}, 3), new BaseArraySerializer(new VarCharType(), null)));\n-\t\tBaseRow row5 = StreamRecordUtils.baserow(null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null);\n-\t\tBinaryRow row6 = StreamRecordUtils.binaryrow(null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null);\n+\t\t\tTuple2.of(new GenericArray(new BinaryString[] {BinaryString.fromString(\"hello\"), BinaryString.fromString(\"\u4e2d\u6587\"), null}, 3), new BaseArraySerializer(new VarCharType(), null)),\n+\t\t\tTuple2.of(GenericRow.of(1, null, new GenericArray(new BinaryString[] {BinaryString.fromString(\"hello\")}, 1), null, null), new BaseRowSerializer(new ExecutionConfig(), rowFieldType)));\n+\t\tBaseRow row5 = StreamRecordUtils.baserow(null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4597eea52db61944a6fd122f55392eb4a165ea32"}, "originalPosition": 67}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 657, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}