{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTQ0MTIxNzU0", "number": 14464, "reviewThreads": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yNFQwODoxMjoyNFrOFI31YA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwNzowMTowOVrOFJsopg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ0ODQ3NzEyOnYy", "diffSide": "RIGHT", "path": "flink-formats/flink-json/src/main/java/org/apache/flink/formats/json/canal/CanalJsonDecodingFormat.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yNFQwODoxMjoyNFrOILCMBw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yNFQwODoxMjoyNFrOILCMBw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODQ0MTA5NQ==", "bodyText": "Add indent.", "url": "https://github.com/apache/flink/pull/14464#discussion_r548441095", "createdAt": "2020-12-24T08:12:24Z", "author": {"login": "wuchong"}, "path": "flink-formats/flink-json/src/main/java/org/apache/flink/formats/json/canal/CanalJsonDecodingFormat.java", "diffHunk": "@@ -0,0 +1,226 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.json.canal;\n+\n+import org.apache.flink.api.common.serialization.DeserializationSchema;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.formats.json.TimestampFormat;\n+import org.apache.flink.formats.json.canal.CanalJsonDeserializationSchema.MetadataConverter;\n+import org.apache.flink.table.api.DataTypes;\n+import org.apache.flink.table.connector.ChangelogMode;\n+import org.apache.flink.table.connector.format.DecodingFormat;\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.utils.DataTypeUtils;\n+import org.apache.flink.types.RowKind;\n+\n+import javax.annotation.Nullable;\n+\n+import java.util.Collections;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * {@link DecodingFormat} for Canal using JSON encoding.\n+ */\n+public class CanalJsonDecodingFormat implements DecodingFormat<DeserializationSchema<RowData>> {\n+\n+\t// --------------------------------------------------------------------------------------------\n+\t// Mutable attributes\n+\t// --------------------------------------------------------------------------------------------\n+\n+\tprivate List<String> metadataKeys;\n+\n+\t// --------------------------------------------------------------------------------------------\n+\t// Canal-specific attributes\n+\t// --------------------------------------------------------------------------------------------\n+\n+\tprivate final @Nullable String database;\n+\n+\tprivate final @Nullable String table;\n+\n+\tprivate final boolean ignoreParseErrors;\n+\n+\tprivate final TimestampFormat timestampFormat;\n+\n+\tpublic CanalJsonDecodingFormat(\n+\t\tString database,\n+\t\tString table,\n+\t\tboolean ignoreParseErrors,\n+\t\tTimestampFormat timestampFormat) {\n+\t\tthis.database = database;\n+\t\tthis.table = table;\n+\t\tthis.ignoreParseErrors = ignoreParseErrors;\n+\t\tthis.timestampFormat = timestampFormat;\n+\t\tthis.metadataKeys = Collections.emptyList();\n+\t}\n+\n+\t@Override\n+\tpublic DeserializationSchema<RowData> createRuntimeDecoder(\n+\t\tDynamicTableSource.Context context,\n+\t\tDataType physicalDataType) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ddcee436fcf2904fb1b3241eb4eccfd36f6553be"}, "originalPosition": 83}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ0ODUxMTc1OnYy", "diffSide": "RIGHT", "path": "flink-formats/flink-json/src/main/java/org/apache/flink/formats/json/canal/CanalJsonDecodingFormat.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yNFQwODoyODozNVrOILCfiA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yNFQwODoyODozNVrOILCfiA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODQ0NjA4OA==", "bodyText": "Add indent.", "url": "https://github.com/apache/flink/pull/14464#discussion_r548446088", "createdAt": "2020-12-24T08:28:35Z", "author": {"login": "wuchong"}, "path": "flink-formats/flink-json/src/main/java/org/apache/flink/formats/json/canal/CanalJsonDecodingFormat.java", "diffHunk": "@@ -0,0 +1,226 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.json.canal;\n+\n+import org.apache.flink.api.common.serialization.DeserializationSchema;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.formats.json.TimestampFormat;\n+import org.apache.flink.formats.json.canal.CanalJsonDeserializationSchema.MetadataConverter;\n+import org.apache.flink.table.api.DataTypes;\n+import org.apache.flink.table.connector.ChangelogMode;\n+import org.apache.flink.table.connector.format.DecodingFormat;\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.utils.DataTypeUtils;\n+import org.apache.flink.types.RowKind;\n+\n+import javax.annotation.Nullable;\n+\n+import java.util.Collections;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * {@link DecodingFormat} for Canal using JSON encoding.\n+ */\n+public class CanalJsonDecodingFormat implements DecodingFormat<DeserializationSchema<RowData>> {\n+\n+\t// --------------------------------------------------------------------------------------------\n+\t// Mutable attributes\n+\t// --------------------------------------------------------------------------------------------\n+\n+\tprivate List<String> metadataKeys;\n+\n+\t// --------------------------------------------------------------------------------------------\n+\t// Canal-specific attributes\n+\t// --------------------------------------------------------------------------------------------\n+\n+\tprivate final @Nullable String database;\n+\n+\tprivate final @Nullable String table;\n+\n+\tprivate final boolean ignoreParseErrors;\n+\n+\tprivate final TimestampFormat timestampFormat;\n+\n+\tpublic CanalJsonDecodingFormat(\n+\t\tString database,\n+\t\tString table,\n+\t\tboolean ignoreParseErrors,\n+\t\tTimestampFormat timestampFormat) {\n+\t\tthis.database = database;\n+\t\tthis.table = table;\n+\t\tthis.ignoreParseErrors = ignoreParseErrors;\n+\t\tthis.timestampFormat = timestampFormat;\n+\t\tthis.metadataKeys = Collections.emptyList();\n+\t}\n+\n+\t@Override\n+\tpublic DeserializationSchema<RowData> createRuntimeDecoder(\n+\t\tDynamicTableSource.Context context,\n+\t\tDataType physicalDataType) {\n+\t\tfinal List<ReadableMetadata> readableMetadata = metadataKeys.stream()\n+\t\t\t.map(k ->\n+\t\t\t\tStream.of(ReadableMetadata.values())\n+\t\t\t\t\t.filter(rm -> rm.key.equals(k))\n+\t\t\t\t\t.findFirst()\n+\t\t\t\t\t.orElseThrow(IllegalStateException::new))\n+\t\t\t.collect(Collectors.toList());\n+\t\tfinal List<DataTypes.Field> metadataFields = readableMetadata.stream()\n+\t\t\t.map(m -> DataTypes.FIELD(m.key, m.dataType))\n+\t\t\t.collect(Collectors.toList());\n+\t\tfinal DataType producedDataType = DataTypeUtils.appendRowFields(physicalDataType, metadataFields);\n+\t\tfinal TypeInformation<RowData> producedTypeInfo = context.createTypeInformation(producedDataType);\n+\t\treturn CanalJsonDeserializationSchema.builder(physicalDataType, readableMetadata, producedTypeInfo)\n+\t\t\t.setDatabase(database)\n+\t\t\t.setTable(table)\n+\t\t\t.setIgnoreParseErrors(ignoreParseErrors)\n+\t\t\t.setTimestampFormat(timestampFormat)\n+\t\t\t.build();\n+\t}\n+\n+\t@Override\n+\tpublic Map<String, DataType> listReadableMetadata() {\n+\t\tfinal Map<String, DataType> metadataMap = new LinkedHashMap<>();\n+\t\tStream.of(ReadableMetadata.values()).forEachOrdered(m -> metadataMap.put(m.key, m.dataType));\n+\t\treturn metadataMap;\n+\t}\n+\n+\t@Override\n+\tpublic void applyReadableMetadata(List<String> metadataKeys) {\n+\t\tthis.metadataKeys = metadataKeys;\n+\t}\n+\n+\t@Override\n+\tpublic ChangelogMode getChangelogMode() {\n+\t\treturn ChangelogMode.newBuilder()\n+\t\t\t.addContainedKind(RowKind.INSERT)\n+\t\t\t.addContainedKind(RowKind.UPDATE_BEFORE)\n+\t\t\t.addContainedKind(RowKind.UPDATE_AFTER)\n+\t\t\t.addContainedKind(RowKind.DELETE)\n+\t\t\t.build();\n+\t}\n+\n+\t// --------------------------------------------------------------------------------------------\n+\t// Metadata handling\n+\t// --------------------------------------------------------------------------------------------\n+\n+\t/**\n+\t * List of metadata that can be read with this format.\n+\t */\n+\tenum ReadableMetadata {\n+\t\tDATABASE(\n+\t\t\t\"database\",\n+\t\t\tDataTypes.STRING().nullable(),\n+\t\t\tDataTypes.FIELD(\"database\", DataTypes.STRING()),\n+\t\t\tnew MetadataConverter() {\n+\t\t\t\tprivate static final long serialVersionUID = 1L;\n+\n+\t\t\t\t@Override\n+\t\t\t\tpublic Object convert(GenericRowData row, int pos) {\n+\t\t\t\t\treturn row.getString(pos);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t),\n+\n+\t\tTABLE(\n+\t\t\t\"table\",\n+\t\t\tDataTypes.STRING().nullable(),\n+\t\t\tDataTypes.FIELD(\"table\", DataTypes.STRING()),\n+\t\t\tnew MetadataConverter() {\n+\t\t\t\tprivate static final long serialVersionUID = 1L;\n+\n+\t\t\t\t@Override\n+\t\t\t\tpublic Object convert(GenericRowData row, int pos) {\n+\t\t\t\t\treturn row.getString(pos);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t),\n+\n+\t\tSQL_TYPE(\n+\t\t\t\"sql-type\",\n+\t\t\tDataTypes.STRING().nullable(),\n+\t\t\tDataTypes.FIELD(\"sqlType\", DataTypes.STRING()),\n+\t\t\tnew MetadataConverter() {\n+\t\t\t\tprivate static final long serialVersionUID = 1L;\n+\n+\t\t\t\t@Override\n+\t\t\t\tpublic Object convert(GenericRowData row, int pos) {\n+\t\t\t\t\treturn row.getString(pos);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t),\n+\n+\t\tPK_NAMES(\n+\t\t\t\"pk-names\",\n+\t\t\tDataTypes.STRING().nullable(),\n+\t\t\tDataTypes.FIELD(\"pkNames\", DataTypes.ARRAY(DataTypes.STRING())),\n+\t\t\tnew MetadataConverter() {\n+\t\t\t\tprivate static final long serialVersionUID = 1L;\n+\n+\t\t\t\t@Override\n+\t\t\t\tpublic Object convert(GenericRowData row, int pos) {\n+\t\t\t\t\treturn row.getArray(pos);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t),\n+\n+\t\tINGESTION_TIMESTAMP(\n+\t\t\t\"ingestion-timestamp\",\n+\t\t\tDataTypes.TIMESTAMP_WITH_LOCAL_TIME_ZONE(3).nullable(),\n+\t\t\tDataTypes.FIELD(\"ts\", DataTypes.BIGINT()),\n+\t\t\tnew MetadataConverter() {\n+\t\t\t\tprivate static final long serialVersionUID = 1L;\n+\n+\t\t\t\t@Override\n+\t\t\t\tpublic Object convert(GenericRowData row, int pos) {\n+\t\t\t\t\tif (row.isNullAt(pos)) {\n+\t\t\t\t\t\treturn null;\n+\t\t\t\t\t}\n+\t\t\t\t\treturn TimestampData.fromEpochMillis(row.getLong(pos));\n+\t\t\t\t}\n+\t\t\t}\n+\t\t);\n+\n+\t\tfinal String key;\n+\n+\t\tfinal DataType dataType;\n+\n+\t\tfinal DataTypes.Field requiredJsonField;\n+\n+\t\tfinal MetadataConverter converter;\n+\n+\t\tReadableMetadata(\n+\t\t\tString key,\n+\t\t\tDataType dataType,\n+\t\t\tDataTypes.Field requiredJsonField,\n+\t\t\tMetadataConverter converter) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ddcee436fcf2904fb1b3241eb4eccfd36f6553be"}, "originalPosition": 219}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ0ODUyMjg3OnYy", "diffSide": "RIGHT", "path": "flink-formats/flink-json/src/main/java/org/apache/flink/formats/json/canal/CanalJsonDeserializationSchema.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yNFQwODozMzo0NVrOILClmg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yNFQwODozMzo0NVrOILClmg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODQ0NzY0Mg==", "bodyText": "Add indent.", "url": "https://github.com/apache/flink/pull/14464#discussion_r548447642", "createdAt": "2020-12-24T08:33:45Z", "author": {"login": "wuchong"}, "path": "flink-formats/flink-json/src/main/java/org/apache/flink/formats/json/canal/CanalJsonDeserializationSchema.java", "diffHunk": "@@ -262,24 +296,71 @@ public boolean equals(Object o) {\n \t\t\treturn false;\n \t\t}\n \t\tCanalJsonDeserializationSchema that = (CanalJsonDeserializationSchema) o;\n-\t\treturn ignoreParseErrors == that.ignoreParseErrors &&\n-\t\t\tfieldCount == that.fieldCount &&\n-\t\t\tObjects.equals(jsonDeserializer, that.jsonDeserializer) &&\n-\t\t\tObjects.equals(resultTypeInfo, that.resultTypeInfo);\n+\t\treturn Objects.equals(jsonDeserializer, that.jsonDeserializer)\n+\t\t\t&& hasMetadata == that.hasMetadata\n+\t\t\t&& Objects.equals(producedTypeInfo, that.producedTypeInfo)\n+\t\t\t&& Objects.equals(database, that.database)\n+\t\t\t&& Objects.equals(table, that.table)\n+\t\t\t&& ignoreParseErrors == that.ignoreParseErrors\n+\t\t\t&& fieldCount == that.fieldCount;\n \t}\n \n \t@Override\n \tpublic int hashCode() {\n-\t\treturn Objects.hash(jsonDeserializer, resultTypeInfo, ignoreParseErrors, fieldCount);\n+\t\treturn Objects.hash(jsonDeserializer, hasMetadata, producedTypeInfo, database, table, ignoreParseErrors, fieldCount);\n \t}\n \n-\tprivate static RowType createJsonRowType(DataType databaseSchema) {\n+\t// --------------------------------------------------------------------------------------------\n+\n+\tprivate static RowType createJsonRowType(DataType physicalDataType, List<ReadableMetadata> readableMetadata) {\n \t\t// Canal JSON contains other information, e.g. \"ts\", \"sql\", but we don't need them\n-\t\treturn (RowType) DataTypes.ROW(\n-\t\t\tDataTypes.FIELD(\"data\", DataTypes.ARRAY(databaseSchema)),\n-\t\t\tDataTypes.FIELD(\"old\", DataTypes.ARRAY(databaseSchema)),\n+\t\tDataType root = DataTypes.ROW(\n+\t\t\tDataTypes.FIELD(\"data\", DataTypes.ARRAY(physicalDataType)),\n+\t\t\tDataTypes.FIELD(\"old\", DataTypes.ARRAY(physicalDataType)),\n \t\t\tDataTypes.FIELD(\"type\", DataTypes.STRING()),\n-\t\t\tDataTypes.FIELD(\"database\", DataTypes.STRING()),\n-\t\t\tDataTypes.FIELD(\"table\", DataTypes.STRING())).getLogicalType();\n+\t\t\tReadableMetadata.DATABASE.requiredJsonField,\n+\t\t\tReadableMetadata.TABLE.requiredJsonField);\n+\t\t// append fields that are required for reading metadata in the root\n+\t\tfinal List<DataTypes.Field> rootMetadataFields = readableMetadata.stream()\n+\t\t\t.filter(m -> m != ReadableMetadata.DATABASE && m != ReadableMetadata.TABLE)\n+\t\t\t.map(m -> m.requiredJsonField)\n+\t\t\t.distinct()\n+\t\t\t.collect(Collectors.toList());\n+\t\treturn (RowType) DataTypeUtils.appendRowFields(root, rootMetadataFields).getLogicalType();\n+\t}\n+\n+\tprivate static MetadataConverter[] createMetadataConverters(\n+\t\tRowType jsonRowType,\n+\t\tList<ReadableMetadata> requestedMetadata) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ddcee436fcf2904fb1b3241eb4eccfd36f6553be"}, "originalPosition": 272}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ1NDMyMjMwOnYy", "diffSide": "RIGHT", "path": "docs/dev/table/connectors/formats/canal.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOFQwNDowNDozNFrOILw8Zw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOFQwNDowNDozNFrOILw8Zw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTIwNzE0Mw==", "bodyText": "Could you list all the metadata columns? I think that would be helpful, esp. for the complex types.", "url": "https://github.com/apache/flink/pull/14464#discussion_r549207143", "createdAt": "2020-12-28T04:04:34Z", "author": {"login": "wuchong"}, "path": "docs/dev/table/connectors/formats/canal.md", "diffHunk": "@@ -142,6 +142,79 @@ SELECT * FROM topic_products;\n </div>\n </div>\n \n+Available Metadata\n+------------------\n+\n+The following format metadata can be exposed as read-only (`VIRTUAL`) columns in a table definition.\n+\n+<span class=\"label label-danger\">Attention</span> Format metadata fields are only available if the\n+corresponding connector forwards format metadata. Currently, only the Kafka connector is able to expose\n+metadata fields for its value format.\n+\n+<table class=\"table table-bordered\">\n+    <thead>\n+    <tr>\n+      <th class=\"text-left\" style=\"width: 25%\">Key</th>\n+      <th class=\"text-center\" style=\"width: 40%\">Data Type</th>\n+      <th class=\"text-center\" style=\"width: 40%\">Description</th>\n+    </tr>\n+    </thead>\n+    <tbody>\n+    <tr>\n+      <td><code>database</code></td>\n+      <td><code>STRING NULL</code></td>\n+      <td>The originating database. Corresponds to the <code>database</code> field in the\n+      Canal record if available.</td>\n+    </tr>\n+    <tr>\n+      <td><code>table</code></td>\n+      <td><code>STRING NULL</code></td>\n+      <td>The originating database table. Corresponds to the <code>table</code> field in the\n+      Canal record if available.</td>\n+    </tr>\n+    <tr>\n+      <td><code>sql-type</code></td>\n+      <td><code>MAP&lt;STRING, INT&gt; NULL</code></td>\n+      <td>Map of various sql types. Corresponds to the <code>sqlType</code> field in the \n+      Canal record if available.</td>\n+    </tr>\n+    <tr>\n+      <td><code>pk-names</code></td>\n+      <td><code>ARRAY&lt;STRING&gt; NULL</code></td>\n+      <td>Array of primary key names. Corresponds to the <code>pkNames</code> field in the \n+      Canal record if available.</td>\n+    </tr>\n+    <tr>\n+      <td><code>ingestion-timestamp</code></td>\n+      <td><code>TIMESTAMP(3) WITH LOCAL TIME ZONE NULL</code></td>\n+      <td>The timestamp at which the connector processed the event. Corresponds to the <code>ts</code>\n+      field in the Canal record.</td>\n+    </tr>\n+    </tbody>\n+</table>\n+\n+The following example shows how to access Canal metadata fields in Kafka:\n+\n+<div class=\"codetabs\" markdown=\"1\">\n+<div data-lang=\"SQL\" markdown=\"1\">\n+{% highlight sql %}\n+CREATE TABLE KafkaTable (\n+  `origin_database` STRING METADATA FROM 'value.database' VIRTUAL,\n+  `origin_table` STRING METADATA FROM 'value.table' VIRTUAL,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9abb79a2bb6e487d3f1deb0cfc4ecfff74a7d780"}, "originalPosition": 62}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ1NDMyMjcwOnYy", "diffSide": "RIGHT", "path": "docs/dev/table/connectors/formats/canal.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOFQwNDowNDo1OVrOILw8lA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOFQwNDowNDo1OVrOILw8lA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTIwNzE4OA==", "bodyText": "I think we don't need to add backquotes around the column names, because they are not keywords.", "url": "https://github.com/apache/flink/pull/14464#discussion_r549207188", "createdAt": "2020-12-28T04:04:59Z", "author": {"login": "wuchong"}, "path": "docs/dev/table/connectors/formats/canal.md", "diffHunk": "@@ -142,6 +142,79 @@ SELECT * FROM topic_products;\n </div>\n </div>\n \n+Available Metadata\n+------------------\n+\n+The following format metadata can be exposed as read-only (`VIRTUAL`) columns in a table definition.\n+\n+<span class=\"label label-danger\">Attention</span> Format metadata fields are only available if the\n+corresponding connector forwards format metadata. Currently, only the Kafka connector is able to expose\n+metadata fields for its value format.\n+\n+<table class=\"table table-bordered\">\n+    <thead>\n+    <tr>\n+      <th class=\"text-left\" style=\"width: 25%\">Key</th>\n+      <th class=\"text-center\" style=\"width: 40%\">Data Type</th>\n+      <th class=\"text-center\" style=\"width: 40%\">Description</th>\n+    </tr>\n+    </thead>\n+    <tbody>\n+    <tr>\n+      <td><code>database</code></td>\n+      <td><code>STRING NULL</code></td>\n+      <td>The originating database. Corresponds to the <code>database</code> field in the\n+      Canal record if available.</td>\n+    </tr>\n+    <tr>\n+      <td><code>table</code></td>\n+      <td><code>STRING NULL</code></td>\n+      <td>The originating database table. Corresponds to the <code>table</code> field in the\n+      Canal record if available.</td>\n+    </tr>\n+    <tr>\n+      <td><code>sql-type</code></td>\n+      <td><code>MAP&lt;STRING, INT&gt; NULL</code></td>\n+      <td>Map of various sql types. Corresponds to the <code>sqlType</code> field in the \n+      Canal record if available.</td>\n+    </tr>\n+    <tr>\n+      <td><code>pk-names</code></td>\n+      <td><code>ARRAY&lt;STRING&gt; NULL</code></td>\n+      <td>Array of primary key names. Corresponds to the <code>pkNames</code> field in the \n+      Canal record if available.</td>\n+    </tr>\n+    <tr>\n+      <td><code>ingestion-timestamp</code></td>\n+      <td><code>TIMESTAMP(3) WITH LOCAL TIME ZONE NULL</code></td>\n+      <td>The timestamp at which the connector processed the event. Corresponds to the <code>ts</code>\n+      field in the Canal record.</td>\n+    </tr>\n+    </tbody>\n+</table>\n+\n+The following example shows how to access Canal metadata fields in Kafka:\n+\n+<div class=\"codetabs\" markdown=\"1\">\n+<div data-lang=\"SQL\" markdown=\"1\">\n+{% highlight sql %}\n+CREATE TABLE KafkaTable (\n+  `origin_database` STRING METADATA FROM 'value.database' VIRTUAL,\n+  `origin_table` STRING METADATA FROM 'value.table' VIRTUAL,\n+  `user_id` BIGINT,\n+  `item_id` BIGINT,\n+  `behavior` STRING", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9abb79a2bb6e487d3f1deb0cfc4ecfff74a7d780"}, "originalPosition": 65}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ1NDMyMzQ5OnYy", "diffSide": "RIGHT", "path": "flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/table/KafkaChangelogTableITCase.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOFQwNDowNTo0OFrOILw88A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOFQwNDowNTo0OFrOILw88A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTIwNzI4MA==", "bodyText": "Could you also test the Map and Array metadatas?", "url": "https://github.com/apache/flink/pull/14464#discussion_r549207280", "createdAt": "2020-12-28T04:05:48Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/table/KafkaChangelogTableITCase.java", "diffHunk": "@@ -198,4 +198,130 @@ public void testKafkaDebeziumChangelogSource() throws Exception {\n \t\ttableResult.getJobClient().get().cancel().get(); // stop the job\n \t\tdeleteTestTopic(topic);\n \t}\n+\n+\t@Test\n+\tpublic void testKafkaCanalChangelogSource() throws Exception {\n+\t\tfinal String topic = \"changelog_canal\";\n+\t\tcreateTestTopic(topic, 1, 1);\n+\n+\t\t// enables MiniBatch processing to verify MiniBatch + FLIP-95, see FLINK-18769\n+\t\tConfiguration tableConf = tEnv.getConfig().getConfiguration();\n+\t\ttableConf.setString(\"table.exec.mini-batch.enabled\", \"true\");\n+\t\ttableConf.setString(\"table.exec.mini-batch.allow-latency\", \"1s\");\n+\t\ttableConf.setString(\"table.exec.mini-batch.size\", \"5000\");\n+\t\ttableConf.setString(\"table.optimizer.agg-phase-strategy\", \"TWO_PHASE\");\n+\n+\t\t// ---------- Write the Canal json into Kafka -------------------\n+\t\tList<String> lines = readLines(\"canal-data.txt\");\n+\t\tDataStreamSource<String> stream = env.fromCollection(lines);\n+\t\tSerializationSchema<String> serSchema = new SimpleStringSchema();\n+\t\tFlinkKafkaPartitioner<String> partitioner = new FlinkFixedPartitioner<>();\n+\n+\t\t// the producer must not produce duplicates\n+\t\tProperties producerProperties = FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings);\n+\t\tproducerProperties.setProperty(\"retries\", \"0\");\n+\t\tproducerProperties.putAll(secureProps);\n+\t\tkafkaServer.produceIntoKafka(stream, topic, serSchema, producerProperties, partitioner);\n+\t\ttry {\n+\t\t\tenv.execute(\"Write sequence\");\n+\t\t} catch (Exception e) {\n+\t\t\tthrow new Exception(\"Failed to write canal data to Kafka.\", e);\n+\t\t}\n+\n+\t\t// ---------- Produce an event time stream into Kafka -------------------\n+\t\tString bootstraps = standardProps.getProperty(\"bootstrap.servers\");\n+\t\tString sourceDDL = String.format(\n+\t\t\t\"CREATE TABLE canal_source (\" +\n+\t\t\t\t// test format metadata\n+\t\t\t\t\" origin_ts STRING METADATA FROM 'value.ingestion-timestamp' VIRTUAL,\" + // unused\n+\t\t\t\t\" origin_table STRING METADATA FROM 'value.table' VIRTUAL,\" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9abb79a2bb6e487d3f1deb0cfc4ecfff74a7d780"}, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ1NTExNjA0OnYy", "diffSide": "RIGHT", "path": "docs/dev/table/connectors/formats/canal.zh.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOFQxMTozNTozMlrOIL3mAQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOFQxMTozNTozMlrOIL3mAQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTMxNjA5Nw==", "bodyText": "Don't need to escape the character.", "url": "https://github.com/apache/flink/pull/14464#discussion_r549316097", "createdAt": "2020-12-28T11:35:32Z", "author": {"login": "wuchong"}, "path": "docs/dev/table/connectors/formats/canal.zh.md", "diffHunk": "@@ -198,11 +198,14 @@ The following example shows how to access Canal metadata fields in Kafka:\n <div data-lang=\"SQL\" markdown=\"1\">\n {% highlight sql %}\n CREATE TABLE KafkaTable (\n-  `origin_database` STRING METADATA FROM 'value.database' VIRTUAL,\n-  `origin_table` STRING METADATA FROM 'value.table' VIRTUAL,\n-  `user_id` BIGINT,\n-  `item_id` BIGINT,\n-  `behavior` STRING\n+  origin_database STRING METADATA FROM 'value.database' VIRTUAL,\n+  origin_table STRING METADATA FROM 'value.table' VIRTUAL,\n+  origin_sql_type MAP&lt;STRING, INT&gt; METADATA FROM 'value.sql-type' VIRTUAL,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "44f3c38cdbc31df14dafa54cf9b80264da281256"}, "originalPosition": 11}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ1NTIyNjE1OnYy", "diffSide": "RIGHT", "path": "docs/dev/table/connectors/formats/canal.zh.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOFQxMjozMTo0MVrOIL4h-w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOFQxMjozMTo0MVrOIL4h-w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTMzMTQ1MQ==", "bodyText": "Please also modify others and preview the page in local.", "url": "https://github.com/apache/flink/pull/14464#discussion_r549331451", "createdAt": "2020-12-28T12:31:41Z", "author": {"login": "wuchong"}, "path": "docs/dev/table/connectors/formats/canal.zh.md", "diffHunk": "@@ -200,7 +200,7 @@ The following example shows how to access Canal metadata fields in Kafka:\n CREATE TABLE KafkaTable (\n   origin_database STRING METADATA FROM 'value.database' VIRTUAL,\n   origin_table STRING METADATA FROM 'value.table' VIRTUAL,\n-  origin_sql_type MAP&lt;STRING, INT&gt; METADATA FROM 'value.sql-type' VIRTUAL,\n+  origin_sql_type MAP<STRING, INT> METADATA FROM 'value.sql-type' VIRTUAL,\n   origin_pk_names ARRAY&lt;STRING&gt; METADATA FROM 'value.pk-names' VIRTUAL,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2aa16a8e76b7ebf4705e2554d13ea014af50e446"}, "originalPosition": 15}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ1NzEyODA2OnYy", "diffSide": "RIGHT", "path": "flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/table/KafkaChangelogTableITCase.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwNzowMTowOVrOIMIchA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwNzowMTowOVrOIMIchA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU5MjE5Ng==", "bodyText": "Could you beautify the format? The same to the debezium one.", "url": "https://github.com/apache/flink/pull/14464#discussion_r549592196", "createdAt": "2020-12-29T07:01:09Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/table/KafkaChangelogTableITCase.java", "diffHunk": "@@ -218,4 +218,168 @@ public void testKafkaDebeziumChangelogSource() throws Exception {\n         tableResult.getJobClient().get().cancel().get(); // stop the job\n         deleteTestTopic(topic);\n     }\n+\n+    @Test\n+    public void testKafkaCanalChangelogSource() throws Exception {\n+        final String topic = \"changelog_canal\";\n+        createTestTopic(topic, 1, 1);\n+\n+        // enables MiniBatch processing to verify MiniBatch + FLIP-95, see FLINK-18769\n+        Configuration tableConf = tEnv.getConfig().getConfiguration();\n+        tableConf.setString(\"table.exec.mini-batch.enabled\", \"true\");\n+        tableConf.setString(\"table.exec.mini-batch.allow-latency\", \"1s\");\n+        tableConf.setString(\"table.exec.mini-batch.size\", \"5000\");\n+        tableConf.setString(\"table.optimizer.agg-phase-strategy\", \"TWO_PHASE\");\n+\n+        // ---------- Write the Canal json into Kafka -------------------\n+        List<String> lines = readLines(\"canal-data.txt\");\n+        DataStreamSource<String> stream = env.fromCollection(lines);\n+        SerializationSchema<String> serSchema = new SimpleStringSchema();\n+        FlinkKafkaPartitioner<String> partitioner = new FlinkFixedPartitioner<>();\n+\n+        // the producer must not produce duplicates\n+        Properties producerProperties =\n+                FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings);\n+        producerProperties.setProperty(\"retries\", \"0\");\n+        producerProperties.putAll(secureProps);\n+        kafkaServer.produceIntoKafka(stream, topic, serSchema, producerProperties, partitioner);\n+        try {\n+            env.execute(\"Write sequence\");\n+        } catch (Exception e) {\n+            throw new Exception(\"Failed to write canal data to Kafka.\", e);\n+        }\n+\n+        // ---------- Produce an event time stream into Kafka -------------------\n+        String bootstraps = standardProps.getProperty(\"bootstrap.servers\");\n+        String sourceDDL =\n+                String.format(\n+                        \"CREATE TABLE canal_source (\"\n+                                +\n+                                // test format metadata\n+                                \" origin_database STRING METADATA FROM 'value.database' VIRTUAL,\"\n+                                + \" origin_table STRING METADATA FROM 'value.table' VIRTUAL,\"\n+                                + \" origin_sql_type MAP<STRING, INT> METADATA FROM 'value.sql-type' VIRTUAL,\"\n+                                + \" origin_pk_names ARRAY<STRING> METADATA FROM 'value.pk-names' VIRTUAL,\"\n+                                + \" origin_ts TIMESTAMP(3) METADATA FROM 'value.ingestion-timestamp' VIRTUAL,\"\n+                                + \" id INT NOT NULL,\"\n+                                + \" name STRING,\"\n+                                + \" description STRING,\"\n+                                + \" weight DECIMAL(10,3),\"\n+                                +\n+                                // test connector metadata\n+                                \" origin_topic STRING METADATA FROM 'topic' VIRTUAL,\"\n+                                + \" origin_partition STRING METADATA FROM 'partition' VIRTUAL\"\n+                                + // unused\n+                                \") WITH (\"\n+                                + \" 'connector' = 'kafka',\"\n+                                + \" 'topic' = '%s',\"\n+                                + \" 'properties.bootstrap.servers' = '%s',\"\n+                                + \" 'scan.startup.mode' = 'earliest-offset',\"\n+                                + \" 'value.format' = 'canal-json'\"\n+                                + \")\",\n+                        topic, bootstraps);\n+        String sinkDDL =\n+                \"CREATE TABLE sink (\"\n+                        + \" origin_topic STRING,\"\n+                        + \" origin_database STRING,\"\n+                        + \" origin_table STRING,\"\n+                        + \" origin_sql_type MAP<STRING, INT>,\"\n+                        + \" origin_pk_names ARRAY<STRING>,\"\n+                        + \" origin_ts TIMESTAMP(3),\"\n+                        + \" name STRING,\"\n+                        + \" PRIMARY KEY (name) NOT ENFORCED\"\n+                        + \") WITH (\"\n+                        + \" 'connector' = 'values',\"\n+                        + \" 'sink-insert-only' = 'false'\"\n+                        + \")\";\n+        tEnv.executeSql(sourceDDL);\n+        tEnv.executeSql(sinkDDL);\n+        TableResult tableResult =\n+                tEnv.executeSql(\n+                        \"INSERT INTO sink \"\n+                                + \"SELECT origin_topic, origin_database, origin_table, origin_sql_type, \"\n+                                + \"origin_pk_names, origin_ts, name \"\n+                                + \"FROM canal_source\");\n+\n+        // Canal captures change data on the `products2` table:\n+        //\n+        // CREATE TABLE products2 (\n+        //  id INTEGER NOT NULL AUTO_INCREMENT PRIMARY KEY,\n+        //  name VARCHAR(255),\n+        //  description VARCHAR(512),\n+        //  weight FLOAT\n+        // );\n+        // ALTER TABLE products2 AUTO_INCREMENT = 101;\n+        //\n+        // INSERT INTO products2\n+        // VALUES (default,\"scooter\",\"Small 2-wheel scooter\",3.14),\n+        //        (default,\"car battery\",\"12V car battery\",8.1),\n+        //        (default,\"12-pack drill bits\",\"12-pack of drill bits with sizes ranging from #40\n+        // to\n+        // #3\",0.8),\n+        //        (default,\"hammer\",\"12oz carpenter's hammer\",0.75),\n+        //        (default,\"hammer\",\"14oz carpenter's hammer\",0.875),\n+        //        (default,\"hammer\",\"16oz carpenter's hammer\",1.0),\n+        //        (default,\"rocks\",\"box of assorted rocks\",5.3),\n+        //        (default,\"jacket\",\"water resistent black wind breaker\",0.1),\n+        //        (default,\"spare tire\",\"24 inch spare tire\",22.2);\n+        // UPDATE products2 SET description='18oz carpenter hammer' WHERE id=106;\n+        // UPDATE products2 SET weight='5.1' WHERE id=107;\n+        // INSERT INTO products2 VALUES (default,\"jacket\",\"water resistent white wind breaker\",0.2);\n+        // INSERT INTO products2 VALUES (default,\"scooter\",\"Big 2-wheel scooter \",5.18);\n+        // UPDATE products2 SET description='new water resistent white wind breaker', weight='0.5'\n+        // WHERE\n+        // id=110;\n+        // UPDATE products2 SET weight='5.17' WHERE id=111;\n+        // DELETE FROM products2 WHERE id=111;\n+        // UPDATE products2 SET weight='5.17' WHERE id=102 or id = 101;\n+        // DELETE FROM products2 WHERE id=102 or id = 103;\n+        //\n+        // > SELECT * FROM products2;\n+        // +-----+--------------------+---------------------------------------------------------+--------+\n+        // | id  | name               | description                                             |\n+        // weight\n+        // |\n+        // +-----+--------------------+---------------------------------------------------------+--------+\n+        // | 101 | scooter            | Small 2-wheel scooter                                   |\n+        // 5.17\n+        // |\n+        // | 104 | hammer             | 12oz carpenter's hammer                                 |\n+        // 0.75\n+        // |\n+        // | 105 | hammer             | 14oz carpenter's hammer                                 |\n+        // 0.875\n+        // |\n+        // | 106 | hammer             | 18oz carpenter hammer                                   |\n+        //   1\n+        // |\n+        // | 107 | rocks              | box of assorted rocks                                   |\n+        // 5.1\n+        // |\n+        // | 108 | jacket             | water resistent black wind breaker                      |", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f868d49d3c71f7b55d1c6a3dc601ac1561bdd9e8"}, "originalPosition": 151}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4570, "cost": 1, "resetAt": "2021-11-12T11:57:46Z"}}}