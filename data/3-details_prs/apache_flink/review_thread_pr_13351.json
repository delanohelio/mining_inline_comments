{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDgxODk1MDk0", "number": 13351, "reviewThreads": {"totalCount": 35, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwODoyOTozMVrOEm7u3Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOVQwOTo0MDozOVrOEr-jkg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5MjU5OTk3OnYy", "diffSide": "RIGHT", "path": "flink-runtime/src/test/java/org/apache/flink/runtime/operators/testutils/DummyEnvironment.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwODoyOTozMVrOHXPg1g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwODoyOTozMVrOHXPg1g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDEzMzQ2Mg==", "bodyText": "Good fix. I'm starting to wonder if we should phase out DummyEnvironment. It seems like it lacks so much.", "url": "https://github.com/apache/flink/pull/13351#discussion_r494133462", "createdAt": "2020-09-24T08:29:31Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/test/java/org/apache/flink/runtime/operators/testutils/DummyEnvironment.java", "diffHunk": "@@ -230,7 +230,7 @@ public ResultPartitionWriter getWriter(int index) {\n \n \t@Override\n \tpublic IndexedInputGate getInputGate(int index) {\n-\t\treturn null;\n+\t\tthrow new ArrayIndexOutOfBoundsException(0);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "77ec70dc867605d7855ea4a8aad21a1bfc5743bd"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5MzU1NDAwOnYy", "diffSide": "RIGHT", "path": "flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/StreamTask.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxMjo0MjowOFrOHXYwXQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxNzo0Mjo0MVrOHXlxsg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDI4NDg5Mw==", "bodyText": "Could we unify both code paths by always using\nfor (InputGate inputGate : getEnvironment().getAllInputGates()) {\n\tinputGate\n\t\t.getStateConsumedFuture()\n\t\t.thenRun(() -> mainMailboxExecutor.execute(inputGate::requestPartitions, \"Input gate request partitions\"));\n}\n\nand set getStateConsumedFuture to done if no data is available?", "url": "https://github.com/apache/flink/pull/13351#discussion_r494284893", "createdAt": "2020-09-24T12:42:08Z", "author": {"login": "AHeise"}, "path": "flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/StreamTask.java", "diffHunk": "@@ -502,33 +503,24 @@ protected void beforeInvoke() throws Exception {\n \t}\n \n \tprivate void readRecoveredChannelState() throws IOException, InterruptedException {\n-\t\tChannelStateReader reader = getEnvironment().getTaskStateManager().getChannelStateReader();\n-\t\tif (!reader.hasChannelStates()) {\n-\t\t\trequestPartitions();\n-\t\t\treturn;\n-\t\t}\n-\n-\t\tResultPartitionWriter[] writers = getEnvironment().getAllWriters();\n-\t\tif (writers != null) {\n-\t\t\tfor (ResultPartitionWriter writer : writers) {\n-\t\t\t\tif (writer instanceof CheckpointedResultPartition) {\n-\t\t\t\t\t((CheckpointedResultPartition) writer).readRecoveredState(reader);\n-\t\t\t\t} else {\n-\t\t\t\t\tthrow new IllegalStateException(\n-\t\t\t\t\t\t\t\"Cannot restore state to a non-checkpointable partition type: \" + writer);\n+\t\tSequentialChannelStateReader reader = getEnvironment().getTaskStateManager().getSequentialChannelStateReader();\n+\t\tif (reader.hasChannelStates()) {\n+\t\t\treader.readOutputData(getEnvironment().getAllWriters());\n+\t\t\tchannelIOExecutor.execute(() -> {\n+\t\t\t\ttry {\n+\t\t\t\t\treader.readInputData(getEnvironment().getAllInputGates());\n+\t\t\t\t} catch (Exception e) {\n+\t\t\t\t\tasyncExceptionHandler.handleAsyncException(\"Unable to read channel state\", e);\n \t\t\t\t}\n-\t\t\t}\n-\t\t}\n+\t\t\t});\n \n-\t\t// It would get possible benefits to recovery input side after output side, which guarantees the\n-\t\t// output can request more floating buffers from global firstly.\n-\t\tInputGate[] inputGates = getEnvironment().getAllInputGates();\n-\t\tif (inputGates != null && inputGates.length > 0) {\n-\t\t\tfor (InputGate inputGate : inputGates) {\n+\t\t\tfor (InputGate inputGate : getEnvironment().getAllInputGates()) {\n \t\t\t\tinputGate\n-\t\t\t\t\t.readRecoveredState(channelIOExecutor, reader)\n+\t\t\t\t\t.getStateConsumedFuture()\n \t\t\t\t\t.thenRun(() -> mainMailboxExecutor.execute(inputGate::requestPartitions, \"Input gate request partitions\"));\n \t\t\t}\n+\t\t} else {\n+\t\t\trequestPartitions();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 77}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDQ5ODIyNg==", "bodyText": "Yes, this is what I'm doing in [FLINK-18989][task] Read channel state unconditionally (but I'm trying to move this commit to #13467).", "url": "https://github.com/apache/flink/pull/13351#discussion_r494498226", "createdAt": "2020-09-24T17:42:41Z", "author": {"login": "rkhachatryan"}, "path": "flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/StreamTask.java", "diffHunk": "@@ -502,33 +503,24 @@ protected void beforeInvoke() throws Exception {\n \t}\n \n \tprivate void readRecoveredChannelState() throws IOException, InterruptedException {\n-\t\tChannelStateReader reader = getEnvironment().getTaskStateManager().getChannelStateReader();\n-\t\tif (!reader.hasChannelStates()) {\n-\t\t\trequestPartitions();\n-\t\t\treturn;\n-\t\t}\n-\n-\t\tResultPartitionWriter[] writers = getEnvironment().getAllWriters();\n-\t\tif (writers != null) {\n-\t\t\tfor (ResultPartitionWriter writer : writers) {\n-\t\t\t\tif (writer instanceof CheckpointedResultPartition) {\n-\t\t\t\t\t((CheckpointedResultPartition) writer).readRecoveredState(reader);\n-\t\t\t\t} else {\n-\t\t\t\t\tthrow new IllegalStateException(\n-\t\t\t\t\t\t\t\"Cannot restore state to a non-checkpointable partition type: \" + writer);\n+\t\tSequentialChannelStateReader reader = getEnvironment().getTaskStateManager().getSequentialChannelStateReader();\n+\t\tif (reader.hasChannelStates()) {\n+\t\t\treader.readOutputData(getEnvironment().getAllWriters());\n+\t\t\tchannelIOExecutor.execute(() -> {\n+\t\t\t\ttry {\n+\t\t\t\t\treader.readInputData(getEnvironment().getAllInputGates());\n+\t\t\t\t} catch (Exception e) {\n+\t\t\t\t\tasyncExceptionHandler.handleAsyncException(\"Unable to read channel state\", e);\n \t\t\t\t}\n-\t\t\t}\n-\t\t}\n+\t\t\t});\n \n-\t\t// It would get possible benefits to recovery input side after output side, which guarantees the\n-\t\t// output can request more floating buffers from global firstly.\n-\t\tInputGate[] inputGates = getEnvironment().getAllInputGates();\n-\t\tif (inputGates != null && inputGates.length > 0) {\n-\t\t\tfor (InputGate inputGate : inputGates) {\n+\t\t\tfor (InputGate inputGate : getEnvironment().getAllInputGates()) {\n \t\t\t\tinputGate\n-\t\t\t\t\t.readRecoveredState(channelIOExecutor, reader)\n+\t\t\t\t\t.getStateConsumedFuture()\n \t\t\t\t\t.thenRun(() -> mainMailboxExecutor.execute(inputGate::requestPartitions, \"Input gate request partitions\"));\n \t\t\t}\n+\t\t} else {\n+\t\t\trequestPartitions();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDI4NDg5Mw=="}, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 77}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5MzU2MjE5OnYy", "diffSide": "RIGHT", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReader.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxMjo0NDoyM1rOHXY1fw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQwODoyNzo1NlrOHX6EMQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDI4NjIwNw==", "bodyText": "Do we have a use for the old ChannelStateReader? I had expected to see all changes inside the existing class.", "url": "https://github.com/apache/flink/pull/13351#discussion_r494286207", "createdAt": "2020-09-24T12:44:23Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReader.java", "diffHunk": "@@ -0,0 +1,62 @@\n+package org.apache.flink.runtime.checkpoint.channel;\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.runtime.io.network.api.writer.ResultPartitionWriter;\n+import org.apache.flink.runtime.io.network.partition.consumer.InputGate;\n+\n+import java.io.IOException;\n+\n+/**\n+ * Reads channel state saved during checkpoint/savepoint.\n+ */\n+@Internal\n+public interface SequentialChannelStateReader extends AutoCloseable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDgzMDY0MQ==", "bodyText": "Became apparent with later commits that it's going to be removed. Would be good to see it reflected in commit message.", "url": "https://github.com/apache/flink/pull/13351#discussion_r494830641", "createdAt": "2020-09-25T08:27:56Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReader.java", "diffHunk": "@@ -0,0 +1,62 @@\n+package org.apache.flink.runtime.checkpoint.channel;\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.runtime.io.network.api.writer.ResultPartitionWriter;\n+import org.apache.flink.runtime.io.network.partition.consumer.InputGate;\n+\n+import java.io.IOException;\n+\n+/**\n+ * Reads channel state saved during checkpoint/savepoint.\n+ */\n+@Internal\n+public interface SequentialChannelStateReader extends AutoCloseable {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDI4NjIwNw=="}, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 29}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5MzYxNjE0OnYy", "diffSide": "RIGHT", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImpl.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxMjo1Njo0NFrOHXZVrg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxMjo1Njo0NFrOHXZVrg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDI5NDQ0Ng==", "bodyText": "It's probably easier to understand (at least for me) if you apply the extractor on call side and just pass Stream<StateObjectCollection<Handle>> into read.\nread(OperatorSubtaskState::getInputChannelState, stateHandler)\n\n->\nread(streamSubtaskStates().map(OperatorSubtaskState::getInputChannelState), stateHandler)", "url": "https://github.com/apache/flink/pull/13351#discussion_r494294446", "createdAt": "2020-09-24T12:56:44Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImpl.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.fs.FSDataInputStream;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.runtime.checkpoint.StateObjectCollection;\n+import org.apache.flink.runtime.checkpoint.TaskStateSnapshot;\n+import org.apache.flink.runtime.io.network.api.writer.ResultPartitionWriter;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.BufferBuilder;\n+import org.apache.flink.runtime.io.network.buffer.BufferConsumer;\n+import org.apache.flink.runtime.io.network.partition.consumer.InputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel;\n+import org.apache.flink.runtime.state.AbstractChannelStateHandle;\n+import org.apache.flink.runtime.state.StreamStateHandle;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+import java.util.stream.Stream;\n+\n+import static java.util.Comparator.comparingLong;\n+import static java.util.stream.Collectors.groupingBy;\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.flink.runtime.checkpoint.channel.ChannelStateByteBuffer.wrap;\n+\n+/**\n+ * {@link SequentialChannelStateReader} implementation.\n+ */\n+public class SequentialChannelStateReaderImpl implements SequentialChannelStateReader {\n+\n+\tprivate final TaskStateSnapshot taskStateSnapshot;\n+\tprivate final ChannelStateSerializer serializer;\n+\tprivate final ChannelStateChunkReader chunkReader;\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot) {\n+\t\tthis(taskStateSnapshot, new ChannelStateSerializerImpl());\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = new ChannelStateChunkReader(serializer);\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer, ChannelStateChunkReader chunkReader) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = chunkReader;\n+\t}\n+\n+\t@Override\n+\tpublic boolean hasChannelStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().anyMatch(subtaskStateEntry ->\n+\t\t\tsubtaskStateEntry.getValue().getInputChannelState().stream().anyMatch(h -> !h.getOffsets().isEmpty()) ||\n+\t\t\t\tsubtaskStateEntry.getValue().getResultSubpartitionState().stream().anyMatch(h -> !h.getOffsets().isEmpty()));\n+\t}\n+\n+\t@Override\n+\tpublic void readInputData(InputGate[] inputGates) throws IOException {\n+\t\ttry (InputChannelRecoveredStateHandler stateHandler = new InputChannelRecoveredStateHandler(inputGates)) {\n+\t\t\tread(OperatorSubtaskState::getInputChannelState, stateHandler);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void readOutputData(ResultPartitionWriter[] writers) throws IOException {\n+\t\ttry (ResultSubpartitionRecoveredStateHandler stateHandler = new ResultSubpartitionRecoveredStateHandler(writers)) {\n+\t\t\tread(OperatorSubtaskState::getResultSubpartitionState, stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void read(\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 97}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5MzYyNTY2OnYy", "diffSide": "RIGHT", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImpl.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxMjo1ODo1OVrOHXZbew==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxNzo1Mjo0MFrOHXmIDA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDI5NTkzMQ==", "bodyText": "Okay noob question: What is the relation between delegate and handles?\nIs that necessary because of the optimization to have small state in metadata?\nI thought that we only have one state file per subtask and now I'm a bit confused.", "url": "https://github.com/apache/flink/pull/13351#discussion_r494295931", "createdAt": "2020-09-24T12:58:59Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImpl.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.fs.FSDataInputStream;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.runtime.checkpoint.StateObjectCollection;\n+import org.apache.flink.runtime.checkpoint.TaskStateSnapshot;\n+import org.apache.flink.runtime.io.network.api.writer.ResultPartitionWriter;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.BufferBuilder;\n+import org.apache.flink.runtime.io.network.buffer.BufferConsumer;\n+import org.apache.flink.runtime.io.network.partition.consumer.InputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel;\n+import org.apache.flink.runtime.state.AbstractChannelStateHandle;\n+import org.apache.flink.runtime.state.StreamStateHandle;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+import java.util.stream.Stream;\n+\n+import static java.util.Comparator.comparingLong;\n+import static java.util.stream.Collectors.groupingBy;\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.flink.runtime.checkpoint.channel.ChannelStateByteBuffer.wrap;\n+\n+/**\n+ * {@link SequentialChannelStateReader} implementation.\n+ */\n+public class SequentialChannelStateReaderImpl implements SequentialChannelStateReader {\n+\n+\tprivate final TaskStateSnapshot taskStateSnapshot;\n+\tprivate final ChannelStateSerializer serializer;\n+\tprivate final ChannelStateChunkReader chunkReader;\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot) {\n+\t\tthis(taskStateSnapshot, new ChannelStateSerializerImpl());\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = new ChannelStateChunkReader(serializer);\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer, ChannelStateChunkReader chunkReader) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = chunkReader;\n+\t}\n+\n+\t@Override\n+\tpublic boolean hasChannelStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().anyMatch(subtaskStateEntry ->\n+\t\t\tsubtaskStateEntry.getValue().getInputChannelState().stream().anyMatch(h -> !h.getOffsets().isEmpty()) ||\n+\t\t\t\tsubtaskStateEntry.getValue().getResultSubpartitionState().stream().anyMatch(h -> !h.getOffsets().isEmpty()));\n+\t}\n+\n+\t@Override\n+\tpublic void readInputData(InputGate[] inputGates) throws IOException {\n+\t\ttry (InputChannelRecoveredStateHandler stateHandler = new InputChannelRecoveredStateHandler(inputGates)) {\n+\t\t\tread(OperatorSubtaskState::getInputChannelState, stateHandler);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void readOutputData(ResultPartitionWriter[] writers) throws IOException {\n+\t\ttry (ResultSubpartitionRecoveredStateHandler stateHandler = new ResultSubpartitionRecoveredStateHandler(writers)) {\n+\t\t\tread(OperatorSubtaskState::getResultSubpartitionState, stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void read(\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\tfor (Map.Entry<StreamStateHandle, List<Handle>> delegateAndHandles : groupByDelegate(streamSubtaskStates(), stateHandleExtractor).entrySet()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 99}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDUwMzk0OA==", "bodyText": "What is the relation between delegate and handles\n\n1:M, that is one underlying delegate can be referenced by 1 or more handles.\n\nwe only have one state file per subtask\n\nYes, but on downscaling we can have more.\nDoes this answer your question?", "url": "https://github.com/apache/flink/pull/13351#discussion_r494503948", "createdAt": "2020-09-24T17:52:40Z", "author": {"login": "rkhachatryan"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImpl.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.fs.FSDataInputStream;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.runtime.checkpoint.StateObjectCollection;\n+import org.apache.flink.runtime.checkpoint.TaskStateSnapshot;\n+import org.apache.flink.runtime.io.network.api.writer.ResultPartitionWriter;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.BufferBuilder;\n+import org.apache.flink.runtime.io.network.buffer.BufferConsumer;\n+import org.apache.flink.runtime.io.network.partition.consumer.InputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel;\n+import org.apache.flink.runtime.state.AbstractChannelStateHandle;\n+import org.apache.flink.runtime.state.StreamStateHandle;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+import java.util.stream.Stream;\n+\n+import static java.util.Comparator.comparingLong;\n+import static java.util.stream.Collectors.groupingBy;\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.flink.runtime.checkpoint.channel.ChannelStateByteBuffer.wrap;\n+\n+/**\n+ * {@link SequentialChannelStateReader} implementation.\n+ */\n+public class SequentialChannelStateReaderImpl implements SequentialChannelStateReader {\n+\n+\tprivate final TaskStateSnapshot taskStateSnapshot;\n+\tprivate final ChannelStateSerializer serializer;\n+\tprivate final ChannelStateChunkReader chunkReader;\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot) {\n+\t\tthis(taskStateSnapshot, new ChannelStateSerializerImpl());\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = new ChannelStateChunkReader(serializer);\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer, ChannelStateChunkReader chunkReader) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = chunkReader;\n+\t}\n+\n+\t@Override\n+\tpublic boolean hasChannelStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().anyMatch(subtaskStateEntry ->\n+\t\t\tsubtaskStateEntry.getValue().getInputChannelState().stream().anyMatch(h -> !h.getOffsets().isEmpty()) ||\n+\t\t\t\tsubtaskStateEntry.getValue().getResultSubpartitionState().stream().anyMatch(h -> !h.getOffsets().isEmpty()));\n+\t}\n+\n+\t@Override\n+\tpublic void readInputData(InputGate[] inputGates) throws IOException {\n+\t\ttry (InputChannelRecoveredStateHandler stateHandler = new InputChannelRecoveredStateHandler(inputGates)) {\n+\t\t\tread(OperatorSubtaskState::getInputChannelState, stateHandler);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void readOutputData(ResultPartitionWriter[] writers) throws IOException {\n+\t\ttry (ResultSubpartitionRecoveredStateHandler stateHandler = new ResultSubpartitionRecoveredStateHandler(writers)) {\n+\t\t\tread(OperatorSubtaskState::getResultSubpartitionState, stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void read(\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\tfor (Map.Entry<StreamStateHandle, List<Handle>> delegateAndHandles : groupByDelegate(streamSubtaskStates(), stateHandleExtractor).entrySet()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDI5NTkzMQ=="}, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 99}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5MzYzNzgxOnYy", "diffSide": "RIGHT", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImpl.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxMzowMjowMFrOHXZjGA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxNzo1ODozNVrOHXmVhA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDI5Nzg4MA==", "bodyText": "When would we have length == 0? Shouldn't that be already skipped while writing? I wonder if it's worth throwing.", "url": "https://github.com/apache/flink/pull/13351#discussion_r494297880", "createdAt": "2020-09-24T13:02:00Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImpl.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.fs.FSDataInputStream;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.runtime.checkpoint.StateObjectCollection;\n+import org.apache.flink.runtime.checkpoint.TaskStateSnapshot;\n+import org.apache.flink.runtime.io.network.api.writer.ResultPartitionWriter;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.BufferBuilder;\n+import org.apache.flink.runtime.io.network.buffer.BufferConsumer;\n+import org.apache.flink.runtime.io.network.partition.consumer.InputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel;\n+import org.apache.flink.runtime.state.AbstractChannelStateHandle;\n+import org.apache.flink.runtime.state.StreamStateHandle;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+import java.util.stream.Stream;\n+\n+import static java.util.Comparator.comparingLong;\n+import static java.util.stream.Collectors.groupingBy;\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.flink.runtime.checkpoint.channel.ChannelStateByteBuffer.wrap;\n+\n+/**\n+ * {@link SequentialChannelStateReader} implementation.\n+ */\n+public class SequentialChannelStateReaderImpl implements SequentialChannelStateReader {\n+\n+\tprivate final TaskStateSnapshot taskStateSnapshot;\n+\tprivate final ChannelStateSerializer serializer;\n+\tprivate final ChannelStateChunkReader chunkReader;\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot) {\n+\t\tthis(taskStateSnapshot, new ChannelStateSerializerImpl());\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = new ChannelStateChunkReader(serializer);\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer, ChannelStateChunkReader chunkReader) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = chunkReader;\n+\t}\n+\n+\t@Override\n+\tpublic boolean hasChannelStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().anyMatch(subtaskStateEntry ->\n+\t\t\tsubtaskStateEntry.getValue().getInputChannelState().stream().anyMatch(h -> !h.getOffsets().isEmpty()) ||\n+\t\t\t\tsubtaskStateEntry.getValue().getResultSubpartitionState().stream().anyMatch(h -> !h.getOffsets().isEmpty()));\n+\t}\n+\n+\t@Override\n+\tpublic void readInputData(InputGate[] inputGates) throws IOException {\n+\t\ttry (InputChannelRecoveredStateHandler stateHandler = new InputChannelRecoveredStateHandler(inputGates)) {\n+\t\t\tread(OperatorSubtaskState::getInputChannelState, stateHandler);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void readOutputData(ResultPartitionWriter[] writers) throws IOException {\n+\t\ttry (ResultSubpartitionRecoveredStateHandler stateHandler = new ResultSubpartitionRecoveredStateHandler(writers)) {\n+\t\t\tread(OperatorSubtaskState::getResultSubpartitionState, stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void read(\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\tfor (Map.Entry<StreamStateHandle, List<Handle>> delegateAndHandles : groupByDelegate(streamSubtaskStates(), stateHandleExtractor).entrySet()) {\n+\t\t\treadSequentially(delegateAndHandles.getKey(), delegateAndHandles.getValue(), stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void readSequentially(\n+\t\t\tStreamStateHandle streamStateHandle,\n+\t\t\tList<Handle> channelStateHandles,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\ttry (FSDataInputStream is = streamStateHandle.openInputStream()) {\n+\t\t\tserializer.readHeader(is);\n+\t\t\tfor (Tuple2<Long, Info> offsetAndChannelInfo : extractOffsetsSorted(channelStateHandles)) {\n+\t\t\t\tchunkReader.readChunk(is, offsetAndChannelInfo.f0, stateHandler, offsetAndChannelInfo.f1);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate Stream<OperatorSubtaskState> streamSubtaskStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().map(Map.Entry::getValue);\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> Map<StreamStateHandle, List<Handle>> groupByDelegate(\n+\t\t\tStream<OperatorSubtaskState> states,\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor) {\n+\t\treturn states\n+\t\t\t.map(stateHandleExtractor).flatMap(Collection::stream)\n+\t\t\t.peek(validate())\n+\t\t\t.collect(groupingBy(AbstractChannelStateHandle::getDelegate));\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> Consumer<Handle> validate() {\n+\t\tSet<Info> seen = new HashSet<>();\n+\t\t// expect each channel to be described only once; otherwise, buffers in channel could be re-ordered\n+\t\treturn handle -> Preconditions.checkState(seen.add(handle.getInfo()), \"duplicate channel info: %s\");\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> List<Tuple2<Long, Info>> extractOffsetsSorted(List<Handle> channelStateHandles) {\n+\t\treturn channelStateHandles\n+\t\t\t.stream()\n+\t\t\t.flatMap(SequentialChannelStateReaderImpl::extractOffsets)\n+\t\t\t.sorted(comparingLong(offsetAndInfo -> offsetAndInfo.f0))\n+\t\t\t.collect(toList());\n+\t}\n+\n+\tprivate static  <Info, Handle extends AbstractChannelStateHandle<Info>> Stream<Tuple2<Long, Info>> extractOffsets(Handle handle) {\n+\t\treturn handle.getOffsets().stream().map(offset -> Tuple2.of(offset, handle.getInfo()));\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws Exception {\n+\t}\n+\n+}\n+\n+interface RecoveredChannelStateHandler<Info, Context> extends AutoCloseable {\n+\tTuple2<ChannelStateByteBuffer, Context> getBuffer(Info info) throws IOException;\n+\n+\tvoid recover(Info info, Context context) throws IOException;\n+}\n+\n+class InputChannelRecoveredStateHandler implements RecoveredChannelStateHandler<InputChannelInfo, Buffer> {\n+\tprivate final InputGate[] inputGates;\n+\n+\tInputChannelRecoveredStateHandler(InputGate[] inputGates) {\n+\t\tthis.inputGates = inputGates;\n+\t}\n+\n+\t@Override\n+\tpublic Tuple2<ChannelStateByteBuffer, Buffer> getBuffer(InputChannelInfo channelInfo) throws IOException {\n+\t\tRecoveredInputChannel channel = getChannel(channelInfo);\n+\t\tBuffer buffer;\n+\t\ttry {\n+\t\t\tbuffer = channel.getBuffer();\n+\t\t\treturn Tuple2.of(wrap(buffer), buffer);\n+\t\t} catch (InterruptedException e) {\n+\t\t\tthrow new IOException(e);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void recover(InputChannelInfo channelInfo, Buffer buffer) {\n+\t\tif (buffer.readableBytes() > 0) {\n+\t\t\tgetChannel(channelInfo).onRecoveredStateBuffer(buffer);\n+\t\t} else {\n+\t\t\tbuffer.recycleBuffer();\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws IOException {\n+\t\t// note that we need to finish all RecoveredInputChannels, not just those with state\n+\t\tfor (final InputGate inputGate : inputGates) {\n+\t\t\tinputGate.finishReadRecoveredState();\n+\t\t}\n+\t}\n+\n+\tprivate RecoveredInputChannel getChannel(InputChannelInfo info) {\n+\t\treturn (RecoveredInputChannel) inputGates[info.getGateIdx()].getChannel(info.getInputChannelIdx());\n+\t}\n+}\n+\n+class ResultSubpartitionRecoveredStateHandler implements RecoveredChannelStateHandler<ResultSubpartitionInfo, Tuple2<BufferBuilder, BufferConsumer>> {\n+\n+\tprivate final ResultPartitionWriter[] writers;\n+\n+\tResultSubpartitionRecoveredStateHandler(ResultPartitionWriter[] writers) {\n+\t\tthis.writers = writers;\n+\t}\n+\n+\t@Override\n+\tpublic Tuple2<ChannelStateByteBuffer, Tuple2<BufferBuilder, BufferConsumer>> getBuffer(ResultSubpartitionInfo subpartitionInfo) throws IOException {\n+\t\tBufferBuilder bufferBuilder;\n+\t\ttry {\n+\t\t\tbufferBuilder = getWriter(subpartitionInfo).getBufferBuilder(subpartitionInfo.getSubPartitionIdx());\n+\t\t} catch (InterruptedException e) {\n+\t\t\tthrow new IOException(e);\n+\t\t}\n+\t\treturn Tuple2.of(wrap(bufferBuilder), Tuple2.of(bufferBuilder, bufferBuilder.createBufferConsumer()));\n+\t}\n+\n+\t@Override\n+\tpublic void recover(ResultSubpartitionInfo subpartitionInfo, Tuple2<BufferBuilder, BufferConsumer> bufferConsumer) throws IOException {\n+\t\tbufferConsumer.f0.finish();\n+\t\tgetWriter(subpartitionInfo).addBufferConsumer(bufferConsumer.f1, subpartitionInfo.getSubPartitionIdx());\n+\t}\n+\n+\tprivate ResultPartitionWriter getWriter(ResultSubpartitionInfo info) {\n+\t\treturn writers[info.getPartitionIdx()];\n+\t}\n+\n+\t@Override\n+\tpublic void close() {\n+\t}\n+}\n+\n+class ChannelStateChunkReader {\n+\tprivate final ChannelStateSerializer serializer;\n+\n+\tChannelStateChunkReader(ChannelStateSerializer serializer) {\n+\t\tthis.serializer = serializer;\n+\t}\n+\n+\t<Info, Context, Handle extends AbstractChannelStateHandle<Info>> void readChunk(\n+\t\t\tFSDataInputStream source,\n+\t\t\tlong sourceOffset,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler,\n+\t\t\tInfo channelInfo) throws IOException {\n+\t\tif (source.getPos() != sourceOffset) {\n+\t\t\tsource.seek(sourceOffset);\n+\t\t}\n+\t\tint length = serializer.readLength(source);\n+\t\twhile (length > 0) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 250}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDUwNzM5Ng==", "bodyText": "length is decremented below: length -= serializer.readData", "url": "https://github.com/apache/flink/pull/13351#discussion_r494507396", "createdAt": "2020-09-24T17:58:35Z", "author": {"login": "rkhachatryan"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImpl.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.fs.FSDataInputStream;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.runtime.checkpoint.StateObjectCollection;\n+import org.apache.flink.runtime.checkpoint.TaskStateSnapshot;\n+import org.apache.flink.runtime.io.network.api.writer.ResultPartitionWriter;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.BufferBuilder;\n+import org.apache.flink.runtime.io.network.buffer.BufferConsumer;\n+import org.apache.flink.runtime.io.network.partition.consumer.InputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel;\n+import org.apache.flink.runtime.state.AbstractChannelStateHandle;\n+import org.apache.flink.runtime.state.StreamStateHandle;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+import java.util.stream.Stream;\n+\n+import static java.util.Comparator.comparingLong;\n+import static java.util.stream.Collectors.groupingBy;\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.flink.runtime.checkpoint.channel.ChannelStateByteBuffer.wrap;\n+\n+/**\n+ * {@link SequentialChannelStateReader} implementation.\n+ */\n+public class SequentialChannelStateReaderImpl implements SequentialChannelStateReader {\n+\n+\tprivate final TaskStateSnapshot taskStateSnapshot;\n+\tprivate final ChannelStateSerializer serializer;\n+\tprivate final ChannelStateChunkReader chunkReader;\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot) {\n+\t\tthis(taskStateSnapshot, new ChannelStateSerializerImpl());\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = new ChannelStateChunkReader(serializer);\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer, ChannelStateChunkReader chunkReader) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = chunkReader;\n+\t}\n+\n+\t@Override\n+\tpublic boolean hasChannelStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().anyMatch(subtaskStateEntry ->\n+\t\t\tsubtaskStateEntry.getValue().getInputChannelState().stream().anyMatch(h -> !h.getOffsets().isEmpty()) ||\n+\t\t\t\tsubtaskStateEntry.getValue().getResultSubpartitionState().stream().anyMatch(h -> !h.getOffsets().isEmpty()));\n+\t}\n+\n+\t@Override\n+\tpublic void readInputData(InputGate[] inputGates) throws IOException {\n+\t\ttry (InputChannelRecoveredStateHandler stateHandler = new InputChannelRecoveredStateHandler(inputGates)) {\n+\t\t\tread(OperatorSubtaskState::getInputChannelState, stateHandler);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void readOutputData(ResultPartitionWriter[] writers) throws IOException {\n+\t\ttry (ResultSubpartitionRecoveredStateHandler stateHandler = new ResultSubpartitionRecoveredStateHandler(writers)) {\n+\t\t\tread(OperatorSubtaskState::getResultSubpartitionState, stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void read(\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\tfor (Map.Entry<StreamStateHandle, List<Handle>> delegateAndHandles : groupByDelegate(streamSubtaskStates(), stateHandleExtractor).entrySet()) {\n+\t\t\treadSequentially(delegateAndHandles.getKey(), delegateAndHandles.getValue(), stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void readSequentially(\n+\t\t\tStreamStateHandle streamStateHandle,\n+\t\t\tList<Handle> channelStateHandles,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\ttry (FSDataInputStream is = streamStateHandle.openInputStream()) {\n+\t\t\tserializer.readHeader(is);\n+\t\t\tfor (Tuple2<Long, Info> offsetAndChannelInfo : extractOffsetsSorted(channelStateHandles)) {\n+\t\t\t\tchunkReader.readChunk(is, offsetAndChannelInfo.f0, stateHandler, offsetAndChannelInfo.f1);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate Stream<OperatorSubtaskState> streamSubtaskStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().map(Map.Entry::getValue);\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> Map<StreamStateHandle, List<Handle>> groupByDelegate(\n+\t\t\tStream<OperatorSubtaskState> states,\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor) {\n+\t\treturn states\n+\t\t\t.map(stateHandleExtractor).flatMap(Collection::stream)\n+\t\t\t.peek(validate())\n+\t\t\t.collect(groupingBy(AbstractChannelStateHandle::getDelegate));\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> Consumer<Handle> validate() {\n+\t\tSet<Info> seen = new HashSet<>();\n+\t\t// expect each channel to be described only once; otherwise, buffers in channel could be re-ordered\n+\t\treturn handle -> Preconditions.checkState(seen.add(handle.getInfo()), \"duplicate channel info: %s\");\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> List<Tuple2<Long, Info>> extractOffsetsSorted(List<Handle> channelStateHandles) {\n+\t\treturn channelStateHandles\n+\t\t\t.stream()\n+\t\t\t.flatMap(SequentialChannelStateReaderImpl::extractOffsets)\n+\t\t\t.sorted(comparingLong(offsetAndInfo -> offsetAndInfo.f0))\n+\t\t\t.collect(toList());\n+\t}\n+\n+\tprivate static  <Info, Handle extends AbstractChannelStateHandle<Info>> Stream<Tuple2<Long, Info>> extractOffsets(Handle handle) {\n+\t\treturn handle.getOffsets().stream().map(offset -> Tuple2.of(offset, handle.getInfo()));\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws Exception {\n+\t}\n+\n+}\n+\n+interface RecoveredChannelStateHandler<Info, Context> extends AutoCloseable {\n+\tTuple2<ChannelStateByteBuffer, Context> getBuffer(Info info) throws IOException;\n+\n+\tvoid recover(Info info, Context context) throws IOException;\n+}\n+\n+class InputChannelRecoveredStateHandler implements RecoveredChannelStateHandler<InputChannelInfo, Buffer> {\n+\tprivate final InputGate[] inputGates;\n+\n+\tInputChannelRecoveredStateHandler(InputGate[] inputGates) {\n+\t\tthis.inputGates = inputGates;\n+\t}\n+\n+\t@Override\n+\tpublic Tuple2<ChannelStateByteBuffer, Buffer> getBuffer(InputChannelInfo channelInfo) throws IOException {\n+\t\tRecoveredInputChannel channel = getChannel(channelInfo);\n+\t\tBuffer buffer;\n+\t\ttry {\n+\t\t\tbuffer = channel.getBuffer();\n+\t\t\treturn Tuple2.of(wrap(buffer), buffer);\n+\t\t} catch (InterruptedException e) {\n+\t\t\tthrow new IOException(e);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void recover(InputChannelInfo channelInfo, Buffer buffer) {\n+\t\tif (buffer.readableBytes() > 0) {\n+\t\t\tgetChannel(channelInfo).onRecoveredStateBuffer(buffer);\n+\t\t} else {\n+\t\t\tbuffer.recycleBuffer();\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws IOException {\n+\t\t// note that we need to finish all RecoveredInputChannels, not just those with state\n+\t\tfor (final InputGate inputGate : inputGates) {\n+\t\t\tinputGate.finishReadRecoveredState();\n+\t\t}\n+\t}\n+\n+\tprivate RecoveredInputChannel getChannel(InputChannelInfo info) {\n+\t\treturn (RecoveredInputChannel) inputGates[info.getGateIdx()].getChannel(info.getInputChannelIdx());\n+\t}\n+}\n+\n+class ResultSubpartitionRecoveredStateHandler implements RecoveredChannelStateHandler<ResultSubpartitionInfo, Tuple2<BufferBuilder, BufferConsumer>> {\n+\n+\tprivate final ResultPartitionWriter[] writers;\n+\n+\tResultSubpartitionRecoveredStateHandler(ResultPartitionWriter[] writers) {\n+\t\tthis.writers = writers;\n+\t}\n+\n+\t@Override\n+\tpublic Tuple2<ChannelStateByteBuffer, Tuple2<BufferBuilder, BufferConsumer>> getBuffer(ResultSubpartitionInfo subpartitionInfo) throws IOException {\n+\t\tBufferBuilder bufferBuilder;\n+\t\ttry {\n+\t\t\tbufferBuilder = getWriter(subpartitionInfo).getBufferBuilder(subpartitionInfo.getSubPartitionIdx());\n+\t\t} catch (InterruptedException e) {\n+\t\t\tthrow new IOException(e);\n+\t\t}\n+\t\treturn Tuple2.of(wrap(bufferBuilder), Tuple2.of(bufferBuilder, bufferBuilder.createBufferConsumer()));\n+\t}\n+\n+\t@Override\n+\tpublic void recover(ResultSubpartitionInfo subpartitionInfo, Tuple2<BufferBuilder, BufferConsumer> bufferConsumer) throws IOException {\n+\t\tbufferConsumer.f0.finish();\n+\t\tgetWriter(subpartitionInfo).addBufferConsumer(bufferConsumer.f1, subpartitionInfo.getSubPartitionIdx());\n+\t}\n+\n+\tprivate ResultPartitionWriter getWriter(ResultSubpartitionInfo info) {\n+\t\treturn writers[info.getPartitionIdx()];\n+\t}\n+\n+\t@Override\n+\tpublic void close() {\n+\t}\n+}\n+\n+class ChannelStateChunkReader {\n+\tprivate final ChannelStateSerializer serializer;\n+\n+\tChannelStateChunkReader(ChannelStateSerializer serializer) {\n+\t\tthis.serializer = serializer;\n+\t}\n+\n+\t<Info, Context, Handle extends AbstractChannelStateHandle<Info>> void readChunk(\n+\t\t\tFSDataInputStream source,\n+\t\t\tlong sourceOffset,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler,\n+\t\t\tInfo channelInfo) throws IOException {\n+\t\tif (source.getPos() != sourceOffset) {\n+\t\t\tsource.seek(sourceOffset);\n+\t\t}\n+\t\tint length = serializer.readLength(source);\n+\t\twhile (length > 0) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDI5Nzg4MA=="}, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 250}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5MzY3MDU2OnYy", "diffSide": "RIGHT", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImpl.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxMzowOTo1NVrOHXZ3KA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxODowMTo0N1rOHXmcig==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDMwMzAxNg==", "bodyText": "What is the main motivation for keeping readChunk in a separate class from SeqReaderImpl? Is it for unit tests?", "url": "https://github.com/apache/flink/pull/13351#discussion_r494303016", "createdAt": "2020-09-24T13:09:55Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImpl.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.fs.FSDataInputStream;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.runtime.checkpoint.StateObjectCollection;\n+import org.apache.flink.runtime.checkpoint.TaskStateSnapshot;\n+import org.apache.flink.runtime.io.network.api.writer.ResultPartitionWriter;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.BufferBuilder;\n+import org.apache.flink.runtime.io.network.buffer.BufferConsumer;\n+import org.apache.flink.runtime.io.network.partition.consumer.InputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel;\n+import org.apache.flink.runtime.state.AbstractChannelStateHandle;\n+import org.apache.flink.runtime.state.StreamStateHandle;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+import java.util.stream.Stream;\n+\n+import static java.util.Comparator.comparingLong;\n+import static java.util.stream.Collectors.groupingBy;\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.flink.runtime.checkpoint.channel.ChannelStateByteBuffer.wrap;\n+\n+/**\n+ * {@link SequentialChannelStateReader} implementation.\n+ */\n+public class SequentialChannelStateReaderImpl implements SequentialChannelStateReader {\n+\n+\tprivate final TaskStateSnapshot taskStateSnapshot;\n+\tprivate final ChannelStateSerializer serializer;\n+\tprivate final ChannelStateChunkReader chunkReader;\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot) {\n+\t\tthis(taskStateSnapshot, new ChannelStateSerializerImpl());\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = new ChannelStateChunkReader(serializer);\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer, ChannelStateChunkReader chunkReader) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = chunkReader;\n+\t}\n+\n+\t@Override\n+\tpublic boolean hasChannelStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().anyMatch(subtaskStateEntry ->\n+\t\t\tsubtaskStateEntry.getValue().getInputChannelState().stream().anyMatch(h -> !h.getOffsets().isEmpty()) ||\n+\t\t\t\tsubtaskStateEntry.getValue().getResultSubpartitionState().stream().anyMatch(h -> !h.getOffsets().isEmpty()));\n+\t}\n+\n+\t@Override\n+\tpublic void readInputData(InputGate[] inputGates) throws IOException {\n+\t\ttry (InputChannelRecoveredStateHandler stateHandler = new InputChannelRecoveredStateHandler(inputGates)) {\n+\t\t\tread(OperatorSubtaskState::getInputChannelState, stateHandler);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void readOutputData(ResultPartitionWriter[] writers) throws IOException {\n+\t\ttry (ResultSubpartitionRecoveredStateHandler stateHandler = new ResultSubpartitionRecoveredStateHandler(writers)) {\n+\t\t\tread(OperatorSubtaskState::getResultSubpartitionState, stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void read(\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\tfor (Map.Entry<StreamStateHandle, List<Handle>> delegateAndHandles : groupByDelegate(streamSubtaskStates(), stateHandleExtractor).entrySet()) {\n+\t\t\treadSequentially(delegateAndHandles.getKey(), delegateAndHandles.getValue(), stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void readSequentially(\n+\t\t\tStreamStateHandle streamStateHandle,\n+\t\t\tList<Handle> channelStateHandles,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\ttry (FSDataInputStream is = streamStateHandle.openInputStream()) {\n+\t\t\tserializer.readHeader(is);\n+\t\t\tfor (Tuple2<Long, Info> offsetAndChannelInfo : extractOffsetsSorted(channelStateHandles)) {\n+\t\t\t\tchunkReader.readChunk(is, offsetAndChannelInfo.f0, stateHandler, offsetAndChannelInfo.f1);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate Stream<OperatorSubtaskState> streamSubtaskStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().map(Map.Entry::getValue);\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> Map<StreamStateHandle, List<Handle>> groupByDelegate(\n+\t\t\tStream<OperatorSubtaskState> states,\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor) {\n+\t\treturn states\n+\t\t\t.map(stateHandleExtractor).flatMap(Collection::stream)\n+\t\t\t.peek(validate())\n+\t\t\t.collect(groupingBy(AbstractChannelStateHandle::getDelegate));\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> Consumer<Handle> validate() {\n+\t\tSet<Info> seen = new HashSet<>();\n+\t\t// expect each channel to be described only once; otherwise, buffers in channel could be re-ordered\n+\t\treturn handle -> Preconditions.checkState(seen.add(handle.getInfo()), \"duplicate channel info: %s\");\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> List<Tuple2<Long, Info>> extractOffsetsSorted(List<Handle> channelStateHandles) {\n+\t\treturn channelStateHandles\n+\t\t\t.stream()\n+\t\t\t.flatMap(SequentialChannelStateReaderImpl::extractOffsets)\n+\t\t\t.sorted(comparingLong(offsetAndInfo -> offsetAndInfo.f0))\n+\t\t\t.collect(toList());\n+\t}\n+\n+\tprivate static  <Info, Handle extends AbstractChannelStateHandle<Info>> Stream<Tuple2<Long, Info>> extractOffsets(Handle handle) {\n+\t\treturn handle.getOffsets().stream().map(offset -> Tuple2.of(offset, handle.getInfo()));\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws Exception {\n+\t}\n+\n+}\n+\n+interface RecoveredChannelStateHandler<Info, Context> extends AutoCloseable {\n+\tTuple2<ChannelStateByteBuffer, Context> getBuffer(Info info) throws IOException;\n+\n+\tvoid recover(Info info, Context context) throws IOException;\n+}\n+\n+class InputChannelRecoveredStateHandler implements RecoveredChannelStateHandler<InputChannelInfo, Buffer> {\n+\tprivate final InputGate[] inputGates;\n+\n+\tInputChannelRecoveredStateHandler(InputGate[] inputGates) {\n+\t\tthis.inputGates = inputGates;\n+\t}\n+\n+\t@Override\n+\tpublic Tuple2<ChannelStateByteBuffer, Buffer> getBuffer(InputChannelInfo channelInfo) throws IOException {\n+\t\tRecoveredInputChannel channel = getChannel(channelInfo);\n+\t\tBuffer buffer;\n+\t\ttry {\n+\t\t\tbuffer = channel.getBuffer();\n+\t\t\treturn Tuple2.of(wrap(buffer), buffer);\n+\t\t} catch (InterruptedException e) {\n+\t\t\tthrow new IOException(e);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void recover(InputChannelInfo channelInfo, Buffer buffer) {\n+\t\tif (buffer.readableBytes() > 0) {\n+\t\t\tgetChannel(channelInfo).onRecoveredStateBuffer(buffer);\n+\t\t} else {\n+\t\t\tbuffer.recycleBuffer();\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws IOException {\n+\t\t// note that we need to finish all RecoveredInputChannels, not just those with state\n+\t\tfor (final InputGate inputGate : inputGates) {\n+\t\t\tinputGate.finishReadRecoveredState();\n+\t\t}\n+\t}\n+\n+\tprivate RecoveredInputChannel getChannel(InputChannelInfo info) {\n+\t\treturn (RecoveredInputChannel) inputGates[info.getGateIdx()].getChannel(info.getInputChannelIdx());\n+\t}\n+}\n+\n+class ResultSubpartitionRecoveredStateHandler implements RecoveredChannelStateHandler<ResultSubpartitionInfo, Tuple2<BufferBuilder, BufferConsumer>> {\n+\n+\tprivate final ResultPartitionWriter[] writers;\n+\n+\tResultSubpartitionRecoveredStateHandler(ResultPartitionWriter[] writers) {\n+\t\tthis.writers = writers;\n+\t}\n+\n+\t@Override\n+\tpublic Tuple2<ChannelStateByteBuffer, Tuple2<BufferBuilder, BufferConsumer>> getBuffer(ResultSubpartitionInfo subpartitionInfo) throws IOException {\n+\t\tBufferBuilder bufferBuilder;\n+\t\ttry {\n+\t\t\tbufferBuilder = getWriter(subpartitionInfo).getBufferBuilder(subpartitionInfo.getSubPartitionIdx());\n+\t\t} catch (InterruptedException e) {\n+\t\t\tthrow new IOException(e);\n+\t\t}\n+\t\treturn Tuple2.of(wrap(bufferBuilder), Tuple2.of(bufferBuilder, bufferBuilder.createBufferConsumer()));\n+\t}\n+\n+\t@Override\n+\tpublic void recover(ResultSubpartitionInfo subpartitionInfo, Tuple2<BufferBuilder, BufferConsumer> bufferConsumer) throws IOException {\n+\t\tbufferConsumer.f0.finish();\n+\t\tgetWriter(subpartitionInfo).addBufferConsumer(bufferConsumer.f1, subpartitionInfo.getSubPartitionIdx());\n+\t}\n+\n+\tprivate ResultPartitionWriter getWriter(ResultSubpartitionInfo info) {\n+\t\treturn writers[info.getPartitionIdx()];\n+\t}\n+\n+\t@Override\n+\tpublic void close() {\n+\t}\n+}\n+\n+class ChannelStateChunkReader {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 234}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDUwOTE5NA==", "bodyText": "Yes, mostly for testing.", "url": "https://github.com/apache/flink/pull/13351#discussion_r494509194", "createdAt": "2020-09-24T18:01:47Z", "author": {"login": "rkhachatryan"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImpl.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.fs.FSDataInputStream;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.runtime.checkpoint.StateObjectCollection;\n+import org.apache.flink.runtime.checkpoint.TaskStateSnapshot;\n+import org.apache.flink.runtime.io.network.api.writer.ResultPartitionWriter;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.BufferBuilder;\n+import org.apache.flink.runtime.io.network.buffer.BufferConsumer;\n+import org.apache.flink.runtime.io.network.partition.consumer.InputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel;\n+import org.apache.flink.runtime.state.AbstractChannelStateHandle;\n+import org.apache.flink.runtime.state.StreamStateHandle;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+import java.util.stream.Stream;\n+\n+import static java.util.Comparator.comparingLong;\n+import static java.util.stream.Collectors.groupingBy;\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.flink.runtime.checkpoint.channel.ChannelStateByteBuffer.wrap;\n+\n+/**\n+ * {@link SequentialChannelStateReader} implementation.\n+ */\n+public class SequentialChannelStateReaderImpl implements SequentialChannelStateReader {\n+\n+\tprivate final TaskStateSnapshot taskStateSnapshot;\n+\tprivate final ChannelStateSerializer serializer;\n+\tprivate final ChannelStateChunkReader chunkReader;\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot) {\n+\t\tthis(taskStateSnapshot, new ChannelStateSerializerImpl());\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = new ChannelStateChunkReader(serializer);\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer, ChannelStateChunkReader chunkReader) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = chunkReader;\n+\t}\n+\n+\t@Override\n+\tpublic boolean hasChannelStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().anyMatch(subtaskStateEntry ->\n+\t\t\tsubtaskStateEntry.getValue().getInputChannelState().stream().anyMatch(h -> !h.getOffsets().isEmpty()) ||\n+\t\t\t\tsubtaskStateEntry.getValue().getResultSubpartitionState().stream().anyMatch(h -> !h.getOffsets().isEmpty()));\n+\t}\n+\n+\t@Override\n+\tpublic void readInputData(InputGate[] inputGates) throws IOException {\n+\t\ttry (InputChannelRecoveredStateHandler stateHandler = new InputChannelRecoveredStateHandler(inputGates)) {\n+\t\t\tread(OperatorSubtaskState::getInputChannelState, stateHandler);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void readOutputData(ResultPartitionWriter[] writers) throws IOException {\n+\t\ttry (ResultSubpartitionRecoveredStateHandler stateHandler = new ResultSubpartitionRecoveredStateHandler(writers)) {\n+\t\t\tread(OperatorSubtaskState::getResultSubpartitionState, stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void read(\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\tfor (Map.Entry<StreamStateHandle, List<Handle>> delegateAndHandles : groupByDelegate(streamSubtaskStates(), stateHandleExtractor).entrySet()) {\n+\t\t\treadSequentially(delegateAndHandles.getKey(), delegateAndHandles.getValue(), stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void readSequentially(\n+\t\t\tStreamStateHandle streamStateHandle,\n+\t\t\tList<Handle> channelStateHandles,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\ttry (FSDataInputStream is = streamStateHandle.openInputStream()) {\n+\t\t\tserializer.readHeader(is);\n+\t\t\tfor (Tuple2<Long, Info> offsetAndChannelInfo : extractOffsetsSorted(channelStateHandles)) {\n+\t\t\t\tchunkReader.readChunk(is, offsetAndChannelInfo.f0, stateHandler, offsetAndChannelInfo.f1);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate Stream<OperatorSubtaskState> streamSubtaskStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().map(Map.Entry::getValue);\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> Map<StreamStateHandle, List<Handle>> groupByDelegate(\n+\t\t\tStream<OperatorSubtaskState> states,\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor) {\n+\t\treturn states\n+\t\t\t.map(stateHandleExtractor).flatMap(Collection::stream)\n+\t\t\t.peek(validate())\n+\t\t\t.collect(groupingBy(AbstractChannelStateHandle::getDelegate));\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> Consumer<Handle> validate() {\n+\t\tSet<Info> seen = new HashSet<>();\n+\t\t// expect each channel to be described only once; otherwise, buffers in channel could be re-ordered\n+\t\treturn handle -> Preconditions.checkState(seen.add(handle.getInfo()), \"duplicate channel info: %s\");\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> List<Tuple2<Long, Info>> extractOffsetsSorted(List<Handle> channelStateHandles) {\n+\t\treturn channelStateHandles\n+\t\t\t.stream()\n+\t\t\t.flatMap(SequentialChannelStateReaderImpl::extractOffsets)\n+\t\t\t.sorted(comparingLong(offsetAndInfo -> offsetAndInfo.f0))\n+\t\t\t.collect(toList());\n+\t}\n+\n+\tprivate static  <Info, Handle extends AbstractChannelStateHandle<Info>> Stream<Tuple2<Long, Info>> extractOffsets(Handle handle) {\n+\t\treturn handle.getOffsets().stream().map(offset -> Tuple2.of(offset, handle.getInfo()));\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws Exception {\n+\t}\n+\n+}\n+\n+interface RecoveredChannelStateHandler<Info, Context> extends AutoCloseable {\n+\tTuple2<ChannelStateByteBuffer, Context> getBuffer(Info info) throws IOException;\n+\n+\tvoid recover(Info info, Context context) throws IOException;\n+}\n+\n+class InputChannelRecoveredStateHandler implements RecoveredChannelStateHandler<InputChannelInfo, Buffer> {\n+\tprivate final InputGate[] inputGates;\n+\n+\tInputChannelRecoveredStateHandler(InputGate[] inputGates) {\n+\t\tthis.inputGates = inputGates;\n+\t}\n+\n+\t@Override\n+\tpublic Tuple2<ChannelStateByteBuffer, Buffer> getBuffer(InputChannelInfo channelInfo) throws IOException {\n+\t\tRecoveredInputChannel channel = getChannel(channelInfo);\n+\t\tBuffer buffer;\n+\t\ttry {\n+\t\t\tbuffer = channel.getBuffer();\n+\t\t\treturn Tuple2.of(wrap(buffer), buffer);\n+\t\t} catch (InterruptedException e) {\n+\t\t\tthrow new IOException(e);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void recover(InputChannelInfo channelInfo, Buffer buffer) {\n+\t\tif (buffer.readableBytes() > 0) {\n+\t\t\tgetChannel(channelInfo).onRecoveredStateBuffer(buffer);\n+\t\t} else {\n+\t\t\tbuffer.recycleBuffer();\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws IOException {\n+\t\t// note that we need to finish all RecoveredInputChannels, not just those with state\n+\t\tfor (final InputGate inputGate : inputGates) {\n+\t\t\tinputGate.finishReadRecoveredState();\n+\t\t}\n+\t}\n+\n+\tprivate RecoveredInputChannel getChannel(InputChannelInfo info) {\n+\t\treturn (RecoveredInputChannel) inputGates[info.getGateIdx()].getChannel(info.getInputChannelIdx());\n+\t}\n+}\n+\n+class ResultSubpartitionRecoveredStateHandler implements RecoveredChannelStateHandler<ResultSubpartitionInfo, Tuple2<BufferBuilder, BufferConsumer>> {\n+\n+\tprivate final ResultPartitionWriter[] writers;\n+\n+\tResultSubpartitionRecoveredStateHandler(ResultPartitionWriter[] writers) {\n+\t\tthis.writers = writers;\n+\t}\n+\n+\t@Override\n+\tpublic Tuple2<ChannelStateByteBuffer, Tuple2<BufferBuilder, BufferConsumer>> getBuffer(ResultSubpartitionInfo subpartitionInfo) throws IOException {\n+\t\tBufferBuilder bufferBuilder;\n+\t\ttry {\n+\t\t\tbufferBuilder = getWriter(subpartitionInfo).getBufferBuilder(subpartitionInfo.getSubPartitionIdx());\n+\t\t} catch (InterruptedException e) {\n+\t\t\tthrow new IOException(e);\n+\t\t}\n+\t\treturn Tuple2.of(wrap(bufferBuilder), Tuple2.of(bufferBuilder, bufferBuilder.createBufferConsumer()));\n+\t}\n+\n+\t@Override\n+\tpublic void recover(ResultSubpartitionInfo subpartitionInfo, Tuple2<BufferBuilder, BufferConsumer> bufferConsumer) throws IOException {\n+\t\tbufferConsumer.f0.finish();\n+\t\tgetWriter(subpartitionInfo).addBufferConsumer(bufferConsumer.f1, subpartitionInfo.getSubPartitionIdx());\n+\t}\n+\n+\tprivate ResultPartitionWriter getWriter(ResultSubpartitionInfo info) {\n+\t\treturn writers[info.getPartitionIdx()];\n+\t}\n+\n+\t@Override\n+\tpublic void close() {\n+\t}\n+}\n+\n+class ChannelStateChunkReader {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDMwMzAxNg=="}, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 234}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5MzY3Njg0OnYy", "diffSide": "RIGHT", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImpl.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxMzoxMToyM1rOHXZ7Cg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxMzoxMToyM1rOHXZ7Cg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDMwNDAxMA==", "bodyText": "It would easier to read if you add a small POJO instead of using Tuple2.", "url": "https://github.com/apache/flink/pull/13351#discussion_r494304010", "createdAt": "2020-09-24T13:11:23Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImpl.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.fs.FSDataInputStream;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.runtime.checkpoint.StateObjectCollection;\n+import org.apache.flink.runtime.checkpoint.TaskStateSnapshot;\n+import org.apache.flink.runtime.io.network.api.writer.ResultPartitionWriter;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.BufferBuilder;\n+import org.apache.flink.runtime.io.network.buffer.BufferConsumer;\n+import org.apache.flink.runtime.io.network.partition.consumer.InputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel;\n+import org.apache.flink.runtime.state.AbstractChannelStateHandle;\n+import org.apache.flink.runtime.state.StreamStateHandle;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+import java.util.stream.Stream;\n+\n+import static java.util.Comparator.comparingLong;\n+import static java.util.stream.Collectors.groupingBy;\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.flink.runtime.checkpoint.channel.ChannelStateByteBuffer.wrap;\n+\n+/**\n+ * {@link SequentialChannelStateReader} implementation.\n+ */\n+public class SequentialChannelStateReaderImpl implements SequentialChannelStateReader {\n+\n+\tprivate final TaskStateSnapshot taskStateSnapshot;\n+\tprivate final ChannelStateSerializer serializer;\n+\tprivate final ChannelStateChunkReader chunkReader;\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot) {\n+\t\tthis(taskStateSnapshot, new ChannelStateSerializerImpl());\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = new ChannelStateChunkReader(serializer);\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer, ChannelStateChunkReader chunkReader) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = chunkReader;\n+\t}\n+\n+\t@Override\n+\tpublic boolean hasChannelStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().anyMatch(subtaskStateEntry ->\n+\t\t\tsubtaskStateEntry.getValue().getInputChannelState().stream().anyMatch(h -> !h.getOffsets().isEmpty()) ||\n+\t\t\t\tsubtaskStateEntry.getValue().getResultSubpartitionState().stream().anyMatch(h -> !h.getOffsets().isEmpty()));\n+\t}\n+\n+\t@Override\n+\tpublic void readInputData(InputGate[] inputGates) throws IOException {\n+\t\ttry (InputChannelRecoveredStateHandler stateHandler = new InputChannelRecoveredStateHandler(inputGates)) {\n+\t\t\tread(OperatorSubtaskState::getInputChannelState, stateHandler);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void readOutputData(ResultPartitionWriter[] writers) throws IOException {\n+\t\ttry (ResultSubpartitionRecoveredStateHandler stateHandler = new ResultSubpartitionRecoveredStateHandler(writers)) {\n+\t\t\tread(OperatorSubtaskState::getResultSubpartitionState, stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void read(\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\tfor (Map.Entry<StreamStateHandle, List<Handle>> delegateAndHandles : groupByDelegate(streamSubtaskStates(), stateHandleExtractor).entrySet()) {\n+\t\t\treadSequentially(delegateAndHandles.getKey(), delegateAndHandles.getValue(), stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void readSequentially(\n+\t\t\tStreamStateHandle streamStateHandle,\n+\t\t\tList<Handle> channelStateHandles,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\ttry (FSDataInputStream is = streamStateHandle.openInputStream()) {\n+\t\t\tserializer.readHeader(is);\n+\t\t\tfor (Tuple2<Long, Info> offsetAndChannelInfo : extractOffsetsSorted(channelStateHandles)) {\n+\t\t\t\tchunkReader.readChunk(is, offsetAndChannelInfo.f0, stateHandler, offsetAndChannelInfo.f1);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate Stream<OperatorSubtaskState> streamSubtaskStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().map(Map.Entry::getValue);\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> Map<StreamStateHandle, List<Handle>> groupByDelegate(\n+\t\t\tStream<OperatorSubtaskState> states,\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor) {\n+\t\treturn states\n+\t\t\t.map(stateHandleExtractor).flatMap(Collection::stream)\n+\t\t\t.peek(validate())\n+\t\t\t.collect(groupingBy(AbstractChannelStateHandle::getDelegate));\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> Consumer<Handle> validate() {\n+\t\tSet<Info> seen = new HashSet<>();\n+\t\t// expect each channel to be described only once; otherwise, buffers in channel could be re-ordered\n+\t\treturn handle -> Preconditions.checkState(seen.add(handle.getInfo()), \"duplicate channel info: %s\");\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> List<Tuple2<Long, Info>> extractOffsetsSorted(List<Handle> channelStateHandles) {\n+\t\treturn channelStateHandles\n+\t\t\t.stream()\n+\t\t\t.flatMap(SequentialChannelStateReaderImpl::extractOffsets)\n+\t\t\t.sorted(comparingLong(offsetAndInfo -> offsetAndInfo.f0))\n+\t\t\t.collect(toList());\n+\t}\n+\n+\tprivate static  <Info, Handle extends AbstractChannelStateHandle<Info>> Stream<Tuple2<Long, Info>> extractOffsets(Handle handle) {\n+\t\treturn handle.getOffsets().stream().map(offset -> Tuple2.of(offset, handle.getInfo()));\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws Exception {\n+\t}\n+\n+}\n+\n+interface RecoveredChannelStateHandler<Info, Context> extends AutoCloseable {\n+\tTuple2<ChannelStateByteBuffer, Context> getBuffer(Info info) throws IOException;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 154}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5MzY5MzE3OnYy", "diffSide": "RIGHT", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImpl.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxMzoxNDo1M1rOHXaFFg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxMzoxNDo1M1rOHXaFFg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDMwNjU4Mg==", "bodyText": "rename parameter to bufferBuilderAndConsumer?", "url": "https://github.com/apache/flink/pull/13351#discussion_r494306582", "createdAt": "2020-09-24T13:14:53Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImpl.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.fs.FSDataInputStream;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.runtime.checkpoint.StateObjectCollection;\n+import org.apache.flink.runtime.checkpoint.TaskStateSnapshot;\n+import org.apache.flink.runtime.io.network.api.writer.ResultPartitionWriter;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.BufferBuilder;\n+import org.apache.flink.runtime.io.network.buffer.BufferConsumer;\n+import org.apache.flink.runtime.io.network.partition.consumer.InputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel;\n+import org.apache.flink.runtime.state.AbstractChannelStateHandle;\n+import org.apache.flink.runtime.state.StreamStateHandle;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+import java.util.stream.Stream;\n+\n+import static java.util.Comparator.comparingLong;\n+import static java.util.stream.Collectors.groupingBy;\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.flink.runtime.checkpoint.channel.ChannelStateByteBuffer.wrap;\n+\n+/**\n+ * {@link SequentialChannelStateReader} implementation.\n+ */\n+public class SequentialChannelStateReaderImpl implements SequentialChannelStateReader {\n+\n+\tprivate final TaskStateSnapshot taskStateSnapshot;\n+\tprivate final ChannelStateSerializer serializer;\n+\tprivate final ChannelStateChunkReader chunkReader;\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot) {\n+\t\tthis(taskStateSnapshot, new ChannelStateSerializerImpl());\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = new ChannelStateChunkReader(serializer);\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer, ChannelStateChunkReader chunkReader) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = chunkReader;\n+\t}\n+\n+\t@Override\n+\tpublic boolean hasChannelStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().anyMatch(subtaskStateEntry ->\n+\t\t\tsubtaskStateEntry.getValue().getInputChannelState().stream().anyMatch(h -> !h.getOffsets().isEmpty()) ||\n+\t\t\t\tsubtaskStateEntry.getValue().getResultSubpartitionState().stream().anyMatch(h -> !h.getOffsets().isEmpty()));\n+\t}\n+\n+\t@Override\n+\tpublic void readInputData(InputGate[] inputGates) throws IOException {\n+\t\ttry (InputChannelRecoveredStateHandler stateHandler = new InputChannelRecoveredStateHandler(inputGates)) {\n+\t\t\tread(OperatorSubtaskState::getInputChannelState, stateHandler);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void readOutputData(ResultPartitionWriter[] writers) throws IOException {\n+\t\ttry (ResultSubpartitionRecoveredStateHandler stateHandler = new ResultSubpartitionRecoveredStateHandler(writers)) {\n+\t\t\tread(OperatorSubtaskState::getResultSubpartitionState, stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void read(\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\tfor (Map.Entry<StreamStateHandle, List<Handle>> delegateAndHandles : groupByDelegate(streamSubtaskStates(), stateHandleExtractor).entrySet()) {\n+\t\t\treadSequentially(delegateAndHandles.getKey(), delegateAndHandles.getValue(), stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void readSequentially(\n+\t\t\tStreamStateHandle streamStateHandle,\n+\t\t\tList<Handle> channelStateHandles,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\ttry (FSDataInputStream is = streamStateHandle.openInputStream()) {\n+\t\t\tserializer.readHeader(is);\n+\t\t\tfor (Tuple2<Long, Info> offsetAndChannelInfo : extractOffsetsSorted(channelStateHandles)) {\n+\t\t\t\tchunkReader.readChunk(is, offsetAndChannelInfo.f0, stateHandler, offsetAndChannelInfo.f1);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate Stream<OperatorSubtaskState> streamSubtaskStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().map(Map.Entry::getValue);\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> Map<StreamStateHandle, List<Handle>> groupByDelegate(\n+\t\t\tStream<OperatorSubtaskState> states,\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor) {\n+\t\treturn states\n+\t\t\t.map(stateHandleExtractor).flatMap(Collection::stream)\n+\t\t\t.peek(validate())\n+\t\t\t.collect(groupingBy(AbstractChannelStateHandle::getDelegate));\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> Consumer<Handle> validate() {\n+\t\tSet<Info> seen = new HashSet<>();\n+\t\t// expect each channel to be described only once; otherwise, buffers in channel could be re-ordered\n+\t\treturn handle -> Preconditions.checkState(seen.add(handle.getInfo()), \"duplicate channel info: %s\");\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> List<Tuple2<Long, Info>> extractOffsetsSorted(List<Handle> channelStateHandles) {\n+\t\treturn channelStateHandles\n+\t\t\t.stream()\n+\t\t\t.flatMap(SequentialChannelStateReaderImpl::extractOffsets)\n+\t\t\t.sorted(comparingLong(offsetAndInfo -> offsetAndInfo.f0))\n+\t\t\t.collect(toList());\n+\t}\n+\n+\tprivate static  <Info, Handle extends AbstractChannelStateHandle<Info>> Stream<Tuple2<Long, Info>> extractOffsets(Handle handle) {\n+\t\treturn handle.getOffsets().stream().map(offset -> Tuple2.of(offset, handle.getInfo()));\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws Exception {\n+\t}\n+\n+}\n+\n+interface RecoveredChannelStateHandler<Info, Context> extends AutoCloseable {\n+\tTuple2<ChannelStateByteBuffer, Context> getBuffer(Info info) throws IOException;\n+\n+\tvoid recover(Info info, Context context) throws IOException;\n+}\n+\n+class InputChannelRecoveredStateHandler implements RecoveredChannelStateHandler<InputChannelInfo, Buffer> {\n+\tprivate final InputGate[] inputGates;\n+\n+\tInputChannelRecoveredStateHandler(InputGate[] inputGates) {\n+\t\tthis.inputGates = inputGates;\n+\t}\n+\n+\t@Override\n+\tpublic Tuple2<ChannelStateByteBuffer, Buffer> getBuffer(InputChannelInfo channelInfo) throws IOException {\n+\t\tRecoveredInputChannel channel = getChannel(channelInfo);\n+\t\tBuffer buffer;\n+\t\ttry {\n+\t\t\tbuffer = channel.getBuffer();\n+\t\t\treturn Tuple2.of(wrap(buffer), buffer);\n+\t\t} catch (InterruptedException e) {\n+\t\t\tthrow new IOException(e);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void recover(InputChannelInfo channelInfo, Buffer buffer) {\n+\t\tif (buffer.readableBytes() > 0) {\n+\t\t\tgetChannel(channelInfo).onRecoveredStateBuffer(buffer);\n+\t\t} else {\n+\t\t\tbuffer.recycleBuffer();\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws IOException {\n+\t\t// note that we need to finish all RecoveredInputChannels, not just those with state\n+\t\tfor (final InputGate inputGate : inputGates) {\n+\t\t\tinputGate.finishReadRecoveredState();\n+\t\t}\n+\t}\n+\n+\tprivate RecoveredInputChannel getChannel(InputChannelInfo info) {\n+\t\treturn (RecoveredInputChannel) inputGates[info.getGateIdx()].getChannel(info.getInputChannelIdx());\n+\t}\n+}\n+\n+class ResultSubpartitionRecoveredStateHandler implements RecoveredChannelStateHandler<ResultSubpartitionInfo, Tuple2<BufferBuilder, BufferConsumer>> {\n+\n+\tprivate final ResultPartitionWriter[] writers;\n+\n+\tResultSubpartitionRecoveredStateHandler(ResultPartitionWriter[] writers) {\n+\t\tthis.writers = writers;\n+\t}\n+\n+\t@Override\n+\tpublic Tuple2<ChannelStateByteBuffer, Tuple2<BufferBuilder, BufferConsumer>> getBuffer(ResultSubpartitionInfo subpartitionInfo) throws IOException {\n+\t\tBufferBuilder bufferBuilder;\n+\t\ttry {\n+\t\t\tbufferBuilder = getWriter(subpartitionInfo).getBufferBuilder(subpartitionInfo.getSubPartitionIdx());\n+\t\t} catch (InterruptedException e) {\n+\t\t\tthrow new IOException(e);\n+\t\t}\n+\t\treturn Tuple2.of(wrap(bufferBuilder), Tuple2.of(bufferBuilder, bufferBuilder.createBufferConsumer()));\n+\t}\n+\n+\t@Override\n+\tpublic void recover(ResultSubpartitionInfo subpartitionInfo, Tuple2<BufferBuilder, BufferConsumer> bufferConsumer) throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 220}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5MzcwMzI4OnYy", "diffSide": "RIGHT", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImpl.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxMzoxNjo1N1rOHXaLKQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxODoyNjoxNFrOHXnVgQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDMwODEzNw==", "bodyText": "Wouldn't it be enough to just create the consumer in recover? Then you also could go with BufferBuilder as Context.", "url": "https://github.com/apache/flink/pull/13351#discussion_r494308137", "createdAt": "2020-09-24T13:16:57Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImpl.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.fs.FSDataInputStream;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.runtime.checkpoint.StateObjectCollection;\n+import org.apache.flink.runtime.checkpoint.TaskStateSnapshot;\n+import org.apache.flink.runtime.io.network.api.writer.ResultPartitionWriter;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.BufferBuilder;\n+import org.apache.flink.runtime.io.network.buffer.BufferConsumer;\n+import org.apache.flink.runtime.io.network.partition.consumer.InputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel;\n+import org.apache.flink.runtime.state.AbstractChannelStateHandle;\n+import org.apache.flink.runtime.state.StreamStateHandle;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+import java.util.stream.Stream;\n+\n+import static java.util.Comparator.comparingLong;\n+import static java.util.stream.Collectors.groupingBy;\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.flink.runtime.checkpoint.channel.ChannelStateByteBuffer.wrap;\n+\n+/**\n+ * {@link SequentialChannelStateReader} implementation.\n+ */\n+public class SequentialChannelStateReaderImpl implements SequentialChannelStateReader {\n+\n+\tprivate final TaskStateSnapshot taskStateSnapshot;\n+\tprivate final ChannelStateSerializer serializer;\n+\tprivate final ChannelStateChunkReader chunkReader;\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot) {\n+\t\tthis(taskStateSnapshot, new ChannelStateSerializerImpl());\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = new ChannelStateChunkReader(serializer);\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer, ChannelStateChunkReader chunkReader) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = chunkReader;\n+\t}\n+\n+\t@Override\n+\tpublic boolean hasChannelStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().anyMatch(subtaskStateEntry ->\n+\t\t\tsubtaskStateEntry.getValue().getInputChannelState().stream().anyMatch(h -> !h.getOffsets().isEmpty()) ||\n+\t\t\t\tsubtaskStateEntry.getValue().getResultSubpartitionState().stream().anyMatch(h -> !h.getOffsets().isEmpty()));\n+\t}\n+\n+\t@Override\n+\tpublic void readInputData(InputGate[] inputGates) throws IOException {\n+\t\ttry (InputChannelRecoveredStateHandler stateHandler = new InputChannelRecoveredStateHandler(inputGates)) {\n+\t\t\tread(OperatorSubtaskState::getInputChannelState, stateHandler);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void readOutputData(ResultPartitionWriter[] writers) throws IOException {\n+\t\ttry (ResultSubpartitionRecoveredStateHandler stateHandler = new ResultSubpartitionRecoveredStateHandler(writers)) {\n+\t\t\tread(OperatorSubtaskState::getResultSubpartitionState, stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void read(\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\tfor (Map.Entry<StreamStateHandle, List<Handle>> delegateAndHandles : groupByDelegate(streamSubtaskStates(), stateHandleExtractor).entrySet()) {\n+\t\t\treadSequentially(delegateAndHandles.getKey(), delegateAndHandles.getValue(), stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void readSequentially(\n+\t\t\tStreamStateHandle streamStateHandle,\n+\t\t\tList<Handle> channelStateHandles,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\ttry (FSDataInputStream is = streamStateHandle.openInputStream()) {\n+\t\t\tserializer.readHeader(is);\n+\t\t\tfor (Tuple2<Long, Info> offsetAndChannelInfo : extractOffsetsSorted(channelStateHandles)) {\n+\t\t\t\tchunkReader.readChunk(is, offsetAndChannelInfo.f0, stateHandler, offsetAndChannelInfo.f1);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate Stream<OperatorSubtaskState> streamSubtaskStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().map(Map.Entry::getValue);\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> Map<StreamStateHandle, List<Handle>> groupByDelegate(\n+\t\t\tStream<OperatorSubtaskState> states,\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor) {\n+\t\treturn states\n+\t\t\t.map(stateHandleExtractor).flatMap(Collection::stream)\n+\t\t\t.peek(validate())\n+\t\t\t.collect(groupingBy(AbstractChannelStateHandle::getDelegate));\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> Consumer<Handle> validate() {\n+\t\tSet<Info> seen = new HashSet<>();\n+\t\t// expect each channel to be described only once; otherwise, buffers in channel could be re-ordered\n+\t\treturn handle -> Preconditions.checkState(seen.add(handle.getInfo()), \"duplicate channel info: %s\");\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> List<Tuple2<Long, Info>> extractOffsetsSorted(List<Handle> channelStateHandles) {\n+\t\treturn channelStateHandles\n+\t\t\t.stream()\n+\t\t\t.flatMap(SequentialChannelStateReaderImpl::extractOffsets)\n+\t\t\t.sorted(comparingLong(offsetAndInfo -> offsetAndInfo.f0))\n+\t\t\t.collect(toList());\n+\t}\n+\n+\tprivate static  <Info, Handle extends AbstractChannelStateHandle<Info>> Stream<Tuple2<Long, Info>> extractOffsets(Handle handle) {\n+\t\treturn handle.getOffsets().stream().map(offset -> Tuple2.of(offset, handle.getInfo()));\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws Exception {\n+\t}\n+\n+}\n+\n+interface RecoveredChannelStateHandler<Info, Context> extends AutoCloseable {\n+\tTuple2<ChannelStateByteBuffer, Context> getBuffer(Info info) throws IOException;\n+\n+\tvoid recover(Info info, Context context) throws IOException;\n+}\n+\n+class InputChannelRecoveredStateHandler implements RecoveredChannelStateHandler<InputChannelInfo, Buffer> {\n+\tprivate final InputGate[] inputGates;\n+\n+\tInputChannelRecoveredStateHandler(InputGate[] inputGates) {\n+\t\tthis.inputGates = inputGates;\n+\t}\n+\n+\t@Override\n+\tpublic Tuple2<ChannelStateByteBuffer, Buffer> getBuffer(InputChannelInfo channelInfo) throws IOException {\n+\t\tRecoveredInputChannel channel = getChannel(channelInfo);\n+\t\tBuffer buffer;\n+\t\ttry {\n+\t\t\tbuffer = channel.getBuffer();\n+\t\t\treturn Tuple2.of(wrap(buffer), buffer);\n+\t\t} catch (InterruptedException e) {\n+\t\t\tthrow new IOException(e);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void recover(InputChannelInfo channelInfo, Buffer buffer) {\n+\t\tif (buffer.readableBytes() > 0) {\n+\t\t\tgetChannel(channelInfo).onRecoveredStateBuffer(buffer);\n+\t\t} else {\n+\t\t\tbuffer.recycleBuffer();\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws IOException {\n+\t\t// note that we need to finish all RecoveredInputChannels, not just those with state\n+\t\tfor (final InputGate inputGate : inputGates) {\n+\t\t\tinputGate.finishReadRecoveredState();\n+\t\t}\n+\t}\n+\n+\tprivate RecoveredInputChannel getChannel(InputChannelInfo info) {\n+\t\treturn (RecoveredInputChannel) inputGates[info.getGateIdx()].getChannel(info.getInputChannelIdx());\n+\t}\n+}\n+\n+class ResultSubpartitionRecoveredStateHandler implements RecoveredChannelStateHandler<ResultSubpartitionInfo, Tuple2<BufferBuilder, BufferConsumer>> {\n+\n+\tprivate final ResultPartitionWriter[] writers;\n+\n+\tResultSubpartitionRecoveredStateHandler(ResultPartitionWriter[] writers) {\n+\t\tthis.writers = writers;\n+\t}\n+\n+\t@Override\n+\tpublic Tuple2<ChannelStateByteBuffer, Tuple2<BufferBuilder, BufferConsumer>> getBuffer(ResultSubpartitionInfo subpartitionInfo) throws IOException {\n+\t\tBufferBuilder bufferBuilder;\n+\t\ttry {\n+\t\t\tbufferBuilder = getWriter(subpartitionInfo).getBufferBuilder(subpartitionInfo.getSubPartitionIdx());\n+\t\t} catch (InterruptedException e) {\n+\t\t\tthrow new IOException(e);\n+\t\t}\n+\t\treturn Tuple2.of(wrap(bufferBuilder), Tuple2.of(bufferBuilder, bufferBuilder.createBufferConsumer()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 216}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDUyMzc3Nw==", "bodyText": "Here, it's important to create consumer before writing data.\nFrom BufferBuilder.createBufferConsumer javadoc:\n\nData written to BufferBuilder before creation of BufferConsumer won't be visible for that BufferConsumer", "url": "https://github.com/apache/flink/pull/13351#discussion_r494523777", "createdAt": "2020-09-24T18:26:14Z", "author": {"login": "rkhachatryan"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImpl.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.fs.FSDataInputStream;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.runtime.checkpoint.StateObjectCollection;\n+import org.apache.flink.runtime.checkpoint.TaskStateSnapshot;\n+import org.apache.flink.runtime.io.network.api.writer.ResultPartitionWriter;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.BufferBuilder;\n+import org.apache.flink.runtime.io.network.buffer.BufferConsumer;\n+import org.apache.flink.runtime.io.network.partition.consumer.InputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel;\n+import org.apache.flink.runtime.state.AbstractChannelStateHandle;\n+import org.apache.flink.runtime.state.StreamStateHandle;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+import java.util.stream.Stream;\n+\n+import static java.util.Comparator.comparingLong;\n+import static java.util.stream.Collectors.groupingBy;\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.flink.runtime.checkpoint.channel.ChannelStateByteBuffer.wrap;\n+\n+/**\n+ * {@link SequentialChannelStateReader} implementation.\n+ */\n+public class SequentialChannelStateReaderImpl implements SequentialChannelStateReader {\n+\n+\tprivate final TaskStateSnapshot taskStateSnapshot;\n+\tprivate final ChannelStateSerializer serializer;\n+\tprivate final ChannelStateChunkReader chunkReader;\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot) {\n+\t\tthis(taskStateSnapshot, new ChannelStateSerializerImpl());\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = new ChannelStateChunkReader(serializer);\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer, ChannelStateChunkReader chunkReader) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = chunkReader;\n+\t}\n+\n+\t@Override\n+\tpublic boolean hasChannelStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().anyMatch(subtaskStateEntry ->\n+\t\t\tsubtaskStateEntry.getValue().getInputChannelState().stream().anyMatch(h -> !h.getOffsets().isEmpty()) ||\n+\t\t\t\tsubtaskStateEntry.getValue().getResultSubpartitionState().stream().anyMatch(h -> !h.getOffsets().isEmpty()));\n+\t}\n+\n+\t@Override\n+\tpublic void readInputData(InputGate[] inputGates) throws IOException {\n+\t\ttry (InputChannelRecoveredStateHandler stateHandler = new InputChannelRecoveredStateHandler(inputGates)) {\n+\t\t\tread(OperatorSubtaskState::getInputChannelState, stateHandler);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void readOutputData(ResultPartitionWriter[] writers) throws IOException {\n+\t\ttry (ResultSubpartitionRecoveredStateHandler stateHandler = new ResultSubpartitionRecoveredStateHandler(writers)) {\n+\t\t\tread(OperatorSubtaskState::getResultSubpartitionState, stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void read(\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\tfor (Map.Entry<StreamStateHandle, List<Handle>> delegateAndHandles : groupByDelegate(streamSubtaskStates(), stateHandleExtractor).entrySet()) {\n+\t\t\treadSequentially(delegateAndHandles.getKey(), delegateAndHandles.getValue(), stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void readSequentially(\n+\t\t\tStreamStateHandle streamStateHandle,\n+\t\t\tList<Handle> channelStateHandles,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\ttry (FSDataInputStream is = streamStateHandle.openInputStream()) {\n+\t\t\tserializer.readHeader(is);\n+\t\t\tfor (Tuple2<Long, Info> offsetAndChannelInfo : extractOffsetsSorted(channelStateHandles)) {\n+\t\t\t\tchunkReader.readChunk(is, offsetAndChannelInfo.f0, stateHandler, offsetAndChannelInfo.f1);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate Stream<OperatorSubtaskState> streamSubtaskStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().map(Map.Entry::getValue);\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> Map<StreamStateHandle, List<Handle>> groupByDelegate(\n+\t\t\tStream<OperatorSubtaskState> states,\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor) {\n+\t\treturn states\n+\t\t\t.map(stateHandleExtractor).flatMap(Collection::stream)\n+\t\t\t.peek(validate())\n+\t\t\t.collect(groupingBy(AbstractChannelStateHandle::getDelegate));\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> Consumer<Handle> validate() {\n+\t\tSet<Info> seen = new HashSet<>();\n+\t\t// expect each channel to be described only once; otherwise, buffers in channel could be re-ordered\n+\t\treturn handle -> Preconditions.checkState(seen.add(handle.getInfo()), \"duplicate channel info: %s\");\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> List<Tuple2<Long, Info>> extractOffsetsSorted(List<Handle> channelStateHandles) {\n+\t\treturn channelStateHandles\n+\t\t\t.stream()\n+\t\t\t.flatMap(SequentialChannelStateReaderImpl::extractOffsets)\n+\t\t\t.sorted(comparingLong(offsetAndInfo -> offsetAndInfo.f0))\n+\t\t\t.collect(toList());\n+\t}\n+\n+\tprivate static  <Info, Handle extends AbstractChannelStateHandle<Info>> Stream<Tuple2<Long, Info>> extractOffsets(Handle handle) {\n+\t\treturn handle.getOffsets().stream().map(offset -> Tuple2.of(offset, handle.getInfo()));\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws Exception {\n+\t}\n+\n+}\n+\n+interface RecoveredChannelStateHandler<Info, Context> extends AutoCloseable {\n+\tTuple2<ChannelStateByteBuffer, Context> getBuffer(Info info) throws IOException;\n+\n+\tvoid recover(Info info, Context context) throws IOException;\n+}\n+\n+class InputChannelRecoveredStateHandler implements RecoveredChannelStateHandler<InputChannelInfo, Buffer> {\n+\tprivate final InputGate[] inputGates;\n+\n+\tInputChannelRecoveredStateHandler(InputGate[] inputGates) {\n+\t\tthis.inputGates = inputGates;\n+\t}\n+\n+\t@Override\n+\tpublic Tuple2<ChannelStateByteBuffer, Buffer> getBuffer(InputChannelInfo channelInfo) throws IOException {\n+\t\tRecoveredInputChannel channel = getChannel(channelInfo);\n+\t\tBuffer buffer;\n+\t\ttry {\n+\t\t\tbuffer = channel.getBuffer();\n+\t\t\treturn Tuple2.of(wrap(buffer), buffer);\n+\t\t} catch (InterruptedException e) {\n+\t\t\tthrow new IOException(e);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void recover(InputChannelInfo channelInfo, Buffer buffer) {\n+\t\tif (buffer.readableBytes() > 0) {\n+\t\t\tgetChannel(channelInfo).onRecoveredStateBuffer(buffer);\n+\t\t} else {\n+\t\t\tbuffer.recycleBuffer();\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws IOException {\n+\t\t// note that we need to finish all RecoveredInputChannels, not just those with state\n+\t\tfor (final InputGate inputGate : inputGates) {\n+\t\t\tinputGate.finishReadRecoveredState();\n+\t\t}\n+\t}\n+\n+\tprivate RecoveredInputChannel getChannel(InputChannelInfo info) {\n+\t\treturn (RecoveredInputChannel) inputGates[info.getGateIdx()].getChannel(info.getInputChannelIdx());\n+\t}\n+}\n+\n+class ResultSubpartitionRecoveredStateHandler implements RecoveredChannelStateHandler<ResultSubpartitionInfo, Tuple2<BufferBuilder, BufferConsumer>> {\n+\n+\tprivate final ResultPartitionWriter[] writers;\n+\n+\tResultSubpartitionRecoveredStateHandler(ResultPartitionWriter[] writers) {\n+\t\tthis.writers = writers;\n+\t}\n+\n+\t@Override\n+\tpublic Tuple2<ChannelStateByteBuffer, Tuple2<BufferBuilder, BufferConsumer>> getBuffer(ResultSubpartitionInfo subpartitionInfo) throws IOException {\n+\t\tBufferBuilder bufferBuilder;\n+\t\ttry {\n+\t\t\tbufferBuilder = getWriter(subpartitionInfo).getBufferBuilder(subpartitionInfo.getSubPartitionIdx());\n+\t\t} catch (InterruptedException e) {\n+\t\t\tthrow new IOException(e);\n+\t\t}\n+\t\treturn Tuple2.of(wrap(bufferBuilder), Tuple2.of(bufferBuilder, bufferBuilder.createBufferConsumer()));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDMwODEzNw=="}, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 216}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5Mzc0NDcyOnYy", "diffSide": "RIGHT", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImpl.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxMzoyNjowN1rOHXakmw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxODozMzo1N1rOHXnw4A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDMxNDY1MQ==", "bodyText": "Should the context also be cleaned up or doesn't it matter because it's failing anyways?", "url": "https://github.com/apache/flink/pull/13351#discussion_r494314651", "createdAt": "2020-09-24T13:26:07Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImpl.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.fs.FSDataInputStream;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.runtime.checkpoint.StateObjectCollection;\n+import org.apache.flink.runtime.checkpoint.TaskStateSnapshot;\n+import org.apache.flink.runtime.io.network.api.writer.ResultPartitionWriter;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.BufferBuilder;\n+import org.apache.flink.runtime.io.network.buffer.BufferConsumer;\n+import org.apache.flink.runtime.io.network.partition.consumer.InputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel;\n+import org.apache.flink.runtime.state.AbstractChannelStateHandle;\n+import org.apache.flink.runtime.state.StreamStateHandle;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+import java.util.stream.Stream;\n+\n+import static java.util.Comparator.comparingLong;\n+import static java.util.stream.Collectors.groupingBy;\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.flink.runtime.checkpoint.channel.ChannelStateByteBuffer.wrap;\n+\n+/**\n+ * {@link SequentialChannelStateReader} implementation.\n+ */\n+public class SequentialChannelStateReaderImpl implements SequentialChannelStateReader {\n+\n+\tprivate final TaskStateSnapshot taskStateSnapshot;\n+\tprivate final ChannelStateSerializer serializer;\n+\tprivate final ChannelStateChunkReader chunkReader;\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot) {\n+\t\tthis(taskStateSnapshot, new ChannelStateSerializerImpl());\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = new ChannelStateChunkReader(serializer);\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer, ChannelStateChunkReader chunkReader) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = chunkReader;\n+\t}\n+\n+\t@Override\n+\tpublic boolean hasChannelStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().anyMatch(subtaskStateEntry ->\n+\t\t\tsubtaskStateEntry.getValue().getInputChannelState().stream().anyMatch(h -> !h.getOffsets().isEmpty()) ||\n+\t\t\t\tsubtaskStateEntry.getValue().getResultSubpartitionState().stream().anyMatch(h -> !h.getOffsets().isEmpty()));\n+\t}\n+\n+\t@Override\n+\tpublic void readInputData(InputGate[] inputGates) throws IOException {\n+\t\ttry (InputChannelRecoveredStateHandler stateHandler = new InputChannelRecoveredStateHandler(inputGates)) {\n+\t\t\tread(OperatorSubtaskState::getInputChannelState, stateHandler);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void readOutputData(ResultPartitionWriter[] writers) throws IOException {\n+\t\ttry (ResultSubpartitionRecoveredStateHandler stateHandler = new ResultSubpartitionRecoveredStateHandler(writers)) {\n+\t\t\tread(OperatorSubtaskState::getResultSubpartitionState, stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void read(\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\tfor (Map.Entry<StreamStateHandle, List<Handle>> delegateAndHandles : groupByDelegate(streamSubtaskStates(), stateHandleExtractor).entrySet()) {\n+\t\t\treadSequentially(delegateAndHandles.getKey(), delegateAndHandles.getValue(), stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void readSequentially(\n+\t\t\tStreamStateHandle streamStateHandle,\n+\t\t\tList<Handle> channelStateHandles,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\ttry (FSDataInputStream is = streamStateHandle.openInputStream()) {\n+\t\t\tserializer.readHeader(is);\n+\t\t\tfor (Tuple2<Long, Info> offsetAndChannelInfo : extractOffsetsSorted(channelStateHandles)) {\n+\t\t\t\tchunkReader.readChunk(is, offsetAndChannelInfo.f0, stateHandler, offsetAndChannelInfo.f1);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate Stream<OperatorSubtaskState> streamSubtaskStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().map(Map.Entry::getValue);\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> Map<StreamStateHandle, List<Handle>> groupByDelegate(\n+\t\t\tStream<OperatorSubtaskState> states,\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor) {\n+\t\treturn states\n+\t\t\t.map(stateHandleExtractor).flatMap(Collection::stream)\n+\t\t\t.peek(validate())\n+\t\t\t.collect(groupingBy(AbstractChannelStateHandle::getDelegate));\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> Consumer<Handle> validate() {\n+\t\tSet<Info> seen = new HashSet<>();\n+\t\t// expect each channel to be described only once; otherwise, buffers in channel could be re-ordered\n+\t\treturn handle -> Preconditions.checkState(seen.add(handle.getInfo()), \"duplicate channel info: %s\");\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> List<Tuple2<Long, Info>> extractOffsetsSorted(List<Handle> channelStateHandles) {\n+\t\treturn channelStateHandles\n+\t\t\t.stream()\n+\t\t\t.flatMap(SequentialChannelStateReaderImpl::extractOffsets)\n+\t\t\t.sorted(comparingLong(offsetAndInfo -> offsetAndInfo.f0))\n+\t\t\t.collect(toList());\n+\t}\n+\n+\tprivate static  <Info, Handle extends AbstractChannelStateHandle<Info>> Stream<Tuple2<Long, Info>> extractOffsets(Handle handle) {\n+\t\treturn handle.getOffsets().stream().map(offset -> Tuple2.of(offset, handle.getInfo()));\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws Exception {\n+\t}\n+\n+}\n+\n+interface RecoveredChannelStateHandler<Info, Context> extends AutoCloseable {\n+\tTuple2<ChannelStateByteBuffer, Context> getBuffer(Info info) throws IOException;\n+\n+\tvoid recover(Info info, Context context) throws IOException;\n+}\n+\n+class InputChannelRecoveredStateHandler implements RecoveredChannelStateHandler<InputChannelInfo, Buffer> {\n+\tprivate final InputGate[] inputGates;\n+\n+\tInputChannelRecoveredStateHandler(InputGate[] inputGates) {\n+\t\tthis.inputGates = inputGates;\n+\t}\n+\n+\t@Override\n+\tpublic Tuple2<ChannelStateByteBuffer, Buffer> getBuffer(InputChannelInfo channelInfo) throws IOException {\n+\t\tRecoveredInputChannel channel = getChannel(channelInfo);\n+\t\tBuffer buffer;\n+\t\ttry {\n+\t\t\tbuffer = channel.getBuffer();\n+\t\t\treturn Tuple2.of(wrap(buffer), buffer);\n+\t\t} catch (InterruptedException e) {\n+\t\t\tthrow new IOException(e);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void recover(InputChannelInfo channelInfo, Buffer buffer) {\n+\t\tif (buffer.readableBytes() > 0) {\n+\t\t\tgetChannel(channelInfo).onRecoveredStateBuffer(buffer);\n+\t\t} else {\n+\t\t\tbuffer.recycleBuffer();\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws IOException {\n+\t\t// note that we need to finish all RecoveredInputChannels, not just those with state\n+\t\tfor (final InputGate inputGate : inputGates) {\n+\t\t\tinputGate.finishReadRecoveredState();\n+\t\t}\n+\t}\n+\n+\tprivate RecoveredInputChannel getChannel(InputChannelInfo info) {\n+\t\treturn (RecoveredInputChannel) inputGates[info.getGateIdx()].getChannel(info.getInputChannelIdx());\n+\t}\n+}\n+\n+class ResultSubpartitionRecoveredStateHandler implements RecoveredChannelStateHandler<ResultSubpartitionInfo, Tuple2<BufferBuilder, BufferConsumer>> {\n+\n+\tprivate final ResultPartitionWriter[] writers;\n+\n+\tResultSubpartitionRecoveredStateHandler(ResultPartitionWriter[] writers) {\n+\t\tthis.writers = writers;\n+\t}\n+\n+\t@Override\n+\tpublic Tuple2<ChannelStateByteBuffer, Tuple2<BufferBuilder, BufferConsumer>> getBuffer(ResultSubpartitionInfo subpartitionInfo) throws IOException {\n+\t\tBufferBuilder bufferBuilder;\n+\t\ttry {\n+\t\t\tbufferBuilder = getWriter(subpartitionInfo).getBufferBuilder(subpartitionInfo.getSubPartitionIdx());\n+\t\t} catch (InterruptedException e) {\n+\t\t\tthrow new IOException(e);\n+\t\t}\n+\t\treturn Tuple2.of(wrap(bufferBuilder), Tuple2.of(bufferBuilder, bufferBuilder.createBufferConsumer()));\n+\t}\n+\n+\t@Override\n+\tpublic void recover(ResultSubpartitionInfo subpartitionInfo, Tuple2<BufferBuilder, BufferConsumer> bufferConsumer) throws IOException {\n+\t\tbufferConsumer.f0.finish();\n+\t\tgetWriter(subpartitionInfo).addBufferConsumer(bufferConsumer.f1, subpartitionInfo.getSubPartitionIdx());\n+\t}\n+\n+\tprivate ResultPartitionWriter getWriter(ResultSubpartitionInfo info) {\n+\t\treturn writers[info.getPartitionIdx()];\n+\t}\n+\n+\t@Override\n+\tpublic void close() {\n+\t}\n+}\n+\n+class ChannelStateChunkReader {\n+\tprivate final ChannelStateSerializer serializer;\n+\n+\tChannelStateChunkReader(ChannelStateSerializer serializer) {\n+\t\tthis.serializer = serializer;\n+\t}\n+\n+\t<Info, Context, Handle extends AbstractChannelStateHandle<Info>> void readChunk(\n+\t\t\tFSDataInputStream source,\n+\t\t\tlong sourceOffset,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler,\n+\t\t\tInfo channelInfo) throws IOException {\n+\t\tif (source.getPos() != sourceOffset) {\n+\t\t\tsource.seek(sourceOffset);\n+\t\t}\n+\t\tint length = serializer.readLength(source);\n+\t\twhile (length > 0) {\n+\t\t\tTuple2<ChannelStateByteBuffer, Context> bufferWithContext = stateHandler.getBuffer(channelInfo);\n+\t\t\ttry {\n+\t\t\t\twhile (length > 0 && bufferWithContext.f0.isWritable()) {\n+\t\t\t\t\tlength -= serializer.readData(source, bufferWithContext.f0, length);\n+\t\t\t\t}\n+\t\t\t} catch (Exception e) {\n+\t\t\t\tbufferWithContext.f0.recycle();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 257}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDUzMDc4NA==", "bodyText": "It's enough to clean buffer, context is not supposed to hold any resources on its own.", "url": "https://github.com/apache/flink/pull/13351#discussion_r494530784", "createdAt": "2020-09-24T18:33:57Z", "author": {"login": "rkhachatryan"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImpl.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.fs.FSDataInputStream;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.runtime.checkpoint.StateObjectCollection;\n+import org.apache.flink.runtime.checkpoint.TaskStateSnapshot;\n+import org.apache.flink.runtime.io.network.api.writer.ResultPartitionWriter;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.BufferBuilder;\n+import org.apache.flink.runtime.io.network.buffer.BufferConsumer;\n+import org.apache.flink.runtime.io.network.partition.consumer.InputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel;\n+import org.apache.flink.runtime.state.AbstractChannelStateHandle;\n+import org.apache.flink.runtime.state.StreamStateHandle;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+import java.util.stream.Stream;\n+\n+import static java.util.Comparator.comparingLong;\n+import static java.util.stream.Collectors.groupingBy;\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.flink.runtime.checkpoint.channel.ChannelStateByteBuffer.wrap;\n+\n+/**\n+ * {@link SequentialChannelStateReader} implementation.\n+ */\n+public class SequentialChannelStateReaderImpl implements SequentialChannelStateReader {\n+\n+\tprivate final TaskStateSnapshot taskStateSnapshot;\n+\tprivate final ChannelStateSerializer serializer;\n+\tprivate final ChannelStateChunkReader chunkReader;\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot) {\n+\t\tthis(taskStateSnapshot, new ChannelStateSerializerImpl());\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = new ChannelStateChunkReader(serializer);\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer, ChannelStateChunkReader chunkReader) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = chunkReader;\n+\t}\n+\n+\t@Override\n+\tpublic boolean hasChannelStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().anyMatch(subtaskStateEntry ->\n+\t\t\tsubtaskStateEntry.getValue().getInputChannelState().stream().anyMatch(h -> !h.getOffsets().isEmpty()) ||\n+\t\t\t\tsubtaskStateEntry.getValue().getResultSubpartitionState().stream().anyMatch(h -> !h.getOffsets().isEmpty()));\n+\t}\n+\n+\t@Override\n+\tpublic void readInputData(InputGate[] inputGates) throws IOException {\n+\t\ttry (InputChannelRecoveredStateHandler stateHandler = new InputChannelRecoveredStateHandler(inputGates)) {\n+\t\t\tread(OperatorSubtaskState::getInputChannelState, stateHandler);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void readOutputData(ResultPartitionWriter[] writers) throws IOException {\n+\t\ttry (ResultSubpartitionRecoveredStateHandler stateHandler = new ResultSubpartitionRecoveredStateHandler(writers)) {\n+\t\t\tread(OperatorSubtaskState::getResultSubpartitionState, stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void read(\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\tfor (Map.Entry<StreamStateHandle, List<Handle>> delegateAndHandles : groupByDelegate(streamSubtaskStates(), stateHandleExtractor).entrySet()) {\n+\t\t\treadSequentially(delegateAndHandles.getKey(), delegateAndHandles.getValue(), stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void readSequentially(\n+\t\t\tStreamStateHandle streamStateHandle,\n+\t\t\tList<Handle> channelStateHandles,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\ttry (FSDataInputStream is = streamStateHandle.openInputStream()) {\n+\t\t\tserializer.readHeader(is);\n+\t\t\tfor (Tuple2<Long, Info> offsetAndChannelInfo : extractOffsetsSorted(channelStateHandles)) {\n+\t\t\t\tchunkReader.readChunk(is, offsetAndChannelInfo.f0, stateHandler, offsetAndChannelInfo.f1);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate Stream<OperatorSubtaskState> streamSubtaskStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().map(Map.Entry::getValue);\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> Map<StreamStateHandle, List<Handle>> groupByDelegate(\n+\t\t\tStream<OperatorSubtaskState> states,\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor) {\n+\t\treturn states\n+\t\t\t.map(stateHandleExtractor).flatMap(Collection::stream)\n+\t\t\t.peek(validate())\n+\t\t\t.collect(groupingBy(AbstractChannelStateHandle::getDelegate));\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> Consumer<Handle> validate() {\n+\t\tSet<Info> seen = new HashSet<>();\n+\t\t// expect each channel to be described only once; otherwise, buffers in channel could be re-ordered\n+\t\treturn handle -> Preconditions.checkState(seen.add(handle.getInfo()), \"duplicate channel info: %s\");\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> List<Tuple2<Long, Info>> extractOffsetsSorted(List<Handle> channelStateHandles) {\n+\t\treturn channelStateHandles\n+\t\t\t.stream()\n+\t\t\t.flatMap(SequentialChannelStateReaderImpl::extractOffsets)\n+\t\t\t.sorted(comparingLong(offsetAndInfo -> offsetAndInfo.f0))\n+\t\t\t.collect(toList());\n+\t}\n+\n+\tprivate static  <Info, Handle extends AbstractChannelStateHandle<Info>> Stream<Tuple2<Long, Info>> extractOffsets(Handle handle) {\n+\t\treturn handle.getOffsets().stream().map(offset -> Tuple2.of(offset, handle.getInfo()));\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws Exception {\n+\t}\n+\n+}\n+\n+interface RecoveredChannelStateHandler<Info, Context> extends AutoCloseable {\n+\tTuple2<ChannelStateByteBuffer, Context> getBuffer(Info info) throws IOException;\n+\n+\tvoid recover(Info info, Context context) throws IOException;\n+}\n+\n+class InputChannelRecoveredStateHandler implements RecoveredChannelStateHandler<InputChannelInfo, Buffer> {\n+\tprivate final InputGate[] inputGates;\n+\n+\tInputChannelRecoveredStateHandler(InputGate[] inputGates) {\n+\t\tthis.inputGates = inputGates;\n+\t}\n+\n+\t@Override\n+\tpublic Tuple2<ChannelStateByteBuffer, Buffer> getBuffer(InputChannelInfo channelInfo) throws IOException {\n+\t\tRecoveredInputChannel channel = getChannel(channelInfo);\n+\t\tBuffer buffer;\n+\t\ttry {\n+\t\t\tbuffer = channel.getBuffer();\n+\t\t\treturn Tuple2.of(wrap(buffer), buffer);\n+\t\t} catch (InterruptedException e) {\n+\t\t\tthrow new IOException(e);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void recover(InputChannelInfo channelInfo, Buffer buffer) {\n+\t\tif (buffer.readableBytes() > 0) {\n+\t\t\tgetChannel(channelInfo).onRecoveredStateBuffer(buffer);\n+\t\t} else {\n+\t\t\tbuffer.recycleBuffer();\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws IOException {\n+\t\t// note that we need to finish all RecoveredInputChannels, not just those with state\n+\t\tfor (final InputGate inputGate : inputGates) {\n+\t\t\tinputGate.finishReadRecoveredState();\n+\t\t}\n+\t}\n+\n+\tprivate RecoveredInputChannel getChannel(InputChannelInfo info) {\n+\t\treturn (RecoveredInputChannel) inputGates[info.getGateIdx()].getChannel(info.getInputChannelIdx());\n+\t}\n+}\n+\n+class ResultSubpartitionRecoveredStateHandler implements RecoveredChannelStateHandler<ResultSubpartitionInfo, Tuple2<BufferBuilder, BufferConsumer>> {\n+\n+\tprivate final ResultPartitionWriter[] writers;\n+\n+\tResultSubpartitionRecoveredStateHandler(ResultPartitionWriter[] writers) {\n+\t\tthis.writers = writers;\n+\t}\n+\n+\t@Override\n+\tpublic Tuple2<ChannelStateByteBuffer, Tuple2<BufferBuilder, BufferConsumer>> getBuffer(ResultSubpartitionInfo subpartitionInfo) throws IOException {\n+\t\tBufferBuilder bufferBuilder;\n+\t\ttry {\n+\t\t\tbufferBuilder = getWriter(subpartitionInfo).getBufferBuilder(subpartitionInfo.getSubPartitionIdx());\n+\t\t} catch (InterruptedException e) {\n+\t\t\tthrow new IOException(e);\n+\t\t}\n+\t\treturn Tuple2.of(wrap(bufferBuilder), Tuple2.of(bufferBuilder, bufferBuilder.createBufferConsumer()));\n+\t}\n+\n+\t@Override\n+\tpublic void recover(ResultSubpartitionInfo subpartitionInfo, Tuple2<BufferBuilder, BufferConsumer> bufferConsumer) throws IOException {\n+\t\tbufferConsumer.f0.finish();\n+\t\tgetWriter(subpartitionInfo).addBufferConsumer(bufferConsumer.f1, subpartitionInfo.getSubPartitionIdx());\n+\t}\n+\n+\tprivate ResultPartitionWriter getWriter(ResultSubpartitionInfo info) {\n+\t\treturn writers[info.getPartitionIdx()];\n+\t}\n+\n+\t@Override\n+\tpublic void close() {\n+\t}\n+}\n+\n+class ChannelStateChunkReader {\n+\tprivate final ChannelStateSerializer serializer;\n+\n+\tChannelStateChunkReader(ChannelStateSerializer serializer) {\n+\t\tthis.serializer = serializer;\n+\t}\n+\n+\t<Info, Context, Handle extends AbstractChannelStateHandle<Info>> void readChunk(\n+\t\t\tFSDataInputStream source,\n+\t\t\tlong sourceOffset,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler,\n+\t\t\tInfo channelInfo) throws IOException {\n+\t\tif (source.getPos() != sourceOffset) {\n+\t\t\tsource.seek(sourceOffset);\n+\t\t}\n+\t\tint length = serializer.readLength(source);\n+\t\twhile (length > 0) {\n+\t\t\tTuple2<ChannelStateByteBuffer, Context> bufferWithContext = stateHandler.getBuffer(channelInfo);\n+\t\t\ttry {\n+\t\t\t\twhile (length > 0 && bufferWithContext.f0.isWritable()) {\n+\t\t\t\t\tlength -= serializer.readData(source, bufferWithContext.f0, length);\n+\t\t\t\t}\n+\t\t\t} catch (Exception e) {\n+\t\t\t\tbufferWithContext.f0.recycle();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDMxNDY1MQ=="}, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 257}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5Mzc3ODM4OnYy", "diffSide": "RIGHT", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImpl.java", "isResolved": true, "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxMzozMzowNFrOHXa5GQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwNjo1NDoyMFrOHZdYkA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDMxOTg5Nw==", "bodyText": "This exception handling looks suspicious to me. I'd probably let the exception bubble up as is.\nIf you want to translate for some reason, I'd still use Thread.currentThread().interrupt(); to restore the interruption flag of the thread.", "url": "https://github.com/apache/flink/pull/13351#discussion_r494319897", "createdAt": "2020-09-24T13:33:04Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImpl.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.fs.FSDataInputStream;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.runtime.checkpoint.StateObjectCollection;\n+import org.apache.flink.runtime.checkpoint.TaskStateSnapshot;\n+import org.apache.flink.runtime.io.network.api.writer.ResultPartitionWriter;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.BufferBuilder;\n+import org.apache.flink.runtime.io.network.buffer.BufferConsumer;\n+import org.apache.flink.runtime.io.network.partition.consumer.InputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel;\n+import org.apache.flink.runtime.state.AbstractChannelStateHandle;\n+import org.apache.flink.runtime.state.StreamStateHandle;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+import java.util.stream.Stream;\n+\n+import static java.util.Comparator.comparingLong;\n+import static java.util.stream.Collectors.groupingBy;\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.flink.runtime.checkpoint.channel.ChannelStateByteBuffer.wrap;\n+\n+/**\n+ * {@link SequentialChannelStateReader} implementation.\n+ */\n+public class SequentialChannelStateReaderImpl implements SequentialChannelStateReader {\n+\n+\tprivate final TaskStateSnapshot taskStateSnapshot;\n+\tprivate final ChannelStateSerializer serializer;\n+\tprivate final ChannelStateChunkReader chunkReader;\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot) {\n+\t\tthis(taskStateSnapshot, new ChannelStateSerializerImpl());\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = new ChannelStateChunkReader(serializer);\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer, ChannelStateChunkReader chunkReader) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = chunkReader;\n+\t}\n+\n+\t@Override\n+\tpublic boolean hasChannelStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().anyMatch(subtaskStateEntry ->\n+\t\t\tsubtaskStateEntry.getValue().getInputChannelState().stream().anyMatch(h -> !h.getOffsets().isEmpty()) ||\n+\t\t\t\tsubtaskStateEntry.getValue().getResultSubpartitionState().stream().anyMatch(h -> !h.getOffsets().isEmpty()));\n+\t}\n+\n+\t@Override\n+\tpublic void readInputData(InputGate[] inputGates) throws IOException {\n+\t\ttry (InputChannelRecoveredStateHandler stateHandler = new InputChannelRecoveredStateHandler(inputGates)) {\n+\t\t\tread(OperatorSubtaskState::getInputChannelState, stateHandler);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void readOutputData(ResultPartitionWriter[] writers) throws IOException {\n+\t\ttry (ResultSubpartitionRecoveredStateHandler stateHandler = new ResultSubpartitionRecoveredStateHandler(writers)) {\n+\t\t\tread(OperatorSubtaskState::getResultSubpartitionState, stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void read(\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\tfor (Map.Entry<StreamStateHandle, List<Handle>> delegateAndHandles : groupByDelegate(streamSubtaskStates(), stateHandleExtractor).entrySet()) {\n+\t\t\treadSequentially(delegateAndHandles.getKey(), delegateAndHandles.getValue(), stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void readSequentially(\n+\t\t\tStreamStateHandle streamStateHandle,\n+\t\t\tList<Handle> channelStateHandles,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\ttry (FSDataInputStream is = streamStateHandle.openInputStream()) {\n+\t\t\tserializer.readHeader(is);\n+\t\t\tfor (Tuple2<Long, Info> offsetAndChannelInfo : extractOffsetsSorted(channelStateHandles)) {\n+\t\t\t\tchunkReader.readChunk(is, offsetAndChannelInfo.f0, stateHandler, offsetAndChannelInfo.f1);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate Stream<OperatorSubtaskState> streamSubtaskStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().map(Map.Entry::getValue);\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> Map<StreamStateHandle, List<Handle>> groupByDelegate(\n+\t\t\tStream<OperatorSubtaskState> states,\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor) {\n+\t\treturn states\n+\t\t\t.map(stateHandleExtractor).flatMap(Collection::stream)\n+\t\t\t.peek(validate())\n+\t\t\t.collect(groupingBy(AbstractChannelStateHandle::getDelegate));\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> Consumer<Handle> validate() {\n+\t\tSet<Info> seen = new HashSet<>();\n+\t\t// expect each channel to be described only once; otherwise, buffers in channel could be re-ordered\n+\t\treturn handle -> Preconditions.checkState(seen.add(handle.getInfo()), \"duplicate channel info: %s\");\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> List<Tuple2<Long, Info>> extractOffsetsSorted(List<Handle> channelStateHandles) {\n+\t\treturn channelStateHandles\n+\t\t\t.stream()\n+\t\t\t.flatMap(SequentialChannelStateReaderImpl::extractOffsets)\n+\t\t\t.sorted(comparingLong(offsetAndInfo -> offsetAndInfo.f0))\n+\t\t\t.collect(toList());\n+\t}\n+\n+\tprivate static  <Info, Handle extends AbstractChannelStateHandle<Info>> Stream<Tuple2<Long, Info>> extractOffsets(Handle handle) {\n+\t\treturn handle.getOffsets().stream().map(offset -> Tuple2.of(offset, handle.getInfo()));\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws Exception {\n+\t}\n+\n+}\n+\n+interface RecoveredChannelStateHandler<Info, Context> extends AutoCloseable {\n+\tTuple2<ChannelStateByteBuffer, Context> getBuffer(Info info) throws IOException;\n+\n+\tvoid recover(Info info, Context context) throws IOException;\n+}\n+\n+class InputChannelRecoveredStateHandler implements RecoveredChannelStateHandler<InputChannelInfo, Buffer> {\n+\tprivate final InputGate[] inputGates;\n+\n+\tInputChannelRecoveredStateHandler(InputGate[] inputGates) {\n+\t\tthis.inputGates = inputGates;\n+\t}\n+\n+\t@Override\n+\tpublic Tuple2<ChannelStateByteBuffer, Buffer> getBuffer(InputChannelInfo channelInfo) throws IOException {\n+\t\tRecoveredInputChannel channel = getChannel(channelInfo);\n+\t\tBuffer buffer;\n+\t\ttry {\n+\t\t\tbuffer = channel.getBuffer();\n+\t\t\treturn Tuple2.of(wrap(buffer), buffer);\n+\t\t} catch (InterruptedException e) {\n+\t\t\tthrow new IOException(e);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void recover(InputChannelInfo channelInfo, Buffer buffer) {\n+\t\tif (buffer.readableBytes() > 0) {\n+\t\t\tgetChannel(channelInfo).onRecoveredStateBuffer(buffer);\n+\t\t} else {\n+\t\t\tbuffer.recycleBuffer();\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws IOException {\n+\t\t// note that we need to finish all RecoveredInputChannels, not just those with state\n+\t\tfor (final InputGate inputGate : inputGates) {\n+\t\t\tinputGate.finishReadRecoveredState();\n+\t\t}\n+\t}\n+\n+\tprivate RecoveredInputChannel getChannel(InputChannelInfo info) {\n+\t\treturn (RecoveredInputChannel) inputGates[info.getGateIdx()].getChannel(info.getInputChannelIdx());\n+\t}\n+}\n+\n+class ResultSubpartitionRecoveredStateHandler implements RecoveredChannelStateHandler<ResultSubpartitionInfo, Tuple2<BufferBuilder, BufferConsumer>> {\n+\n+\tprivate final ResultPartitionWriter[] writers;\n+\n+\tResultSubpartitionRecoveredStateHandler(ResultPartitionWriter[] writers) {\n+\t\tthis.writers = writers;\n+\t}\n+\n+\t@Override\n+\tpublic Tuple2<ChannelStateByteBuffer, Tuple2<BufferBuilder, BufferConsumer>> getBuffer(ResultSubpartitionInfo subpartitionInfo) throws IOException {\n+\t\tBufferBuilder bufferBuilder;\n+\t\ttry {\n+\t\t\tbufferBuilder = getWriter(subpartitionInfo).getBufferBuilder(subpartitionInfo.getSubPartitionIdx());\n+\t\t} catch (InterruptedException e) {\n+\t\t\tthrow new IOException(e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 214}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDUzNDQ5MA==", "bodyText": "InterruptedException is a checked exception so I'd had to declare it if not wrapping.\nBut it's not appropriate for this layer (IO).\nThread.currentThread().interrupt(); only sets the status, so the caller can skip interrupt status check.", "url": "https://github.com/apache/flink/pull/13351#discussion_r494534490", "createdAt": "2020-09-24T18:40:29Z", "author": {"login": "rkhachatryan"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImpl.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.fs.FSDataInputStream;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.runtime.checkpoint.StateObjectCollection;\n+import org.apache.flink.runtime.checkpoint.TaskStateSnapshot;\n+import org.apache.flink.runtime.io.network.api.writer.ResultPartitionWriter;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.BufferBuilder;\n+import org.apache.flink.runtime.io.network.buffer.BufferConsumer;\n+import org.apache.flink.runtime.io.network.partition.consumer.InputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel;\n+import org.apache.flink.runtime.state.AbstractChannelStateHandle;\n+import org.apache.flink.runtime.state.StreamStateHandle;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+import java.util.stream.Stream;\n+\n+import static java.util.Comparator.comparingLong;\n+import static java.util.stream.Collectors.groupingBy;\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.flink.runtime.checkpoint.channel.ChannelStateByteBuffer.wrap;\n+\n+/**\n+ * {@link SequentialChannelStateReader} implementation.\n+ */\n+public class SequentialChannelStateReaderImpl implements SequentialChannelStateReader {\n+\n+\tprivate final TaskStateSnapshot taskStateSnapshot;\n+\tprivate final ChannelStateSerializer serializer;\n+\tprivate final ChannelStateChunkReader chunkReader;\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot) {\n+\t\tthis(taskStateSnapshot, new ChannelStateSerializerImpl());\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = new ChannelStateChunkReader(serializer);\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer, ChannelStateChunkReader chunkReader) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = chunkReader;\n+\t}\n+\n+\t@Override\n+\tpublic boolean hasChannelStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().anyMatch(subtaskStateEntry ->\n+\t\t\tsubtaskStateEntry.getValue().getInputChannelState().stream().anyMatch(h -> !h.getOffsets().isEmpty()) ||\n+\t\t\t\tsubtaskStateEntry.getValue().getResultSubpartitionState().stream().anyMatch(h -> !h.getOffsets().isEmpty()));\n+\t}\n+\n+\t@Override\n+\tpublic void readInputData(InputGate[] inputGates) throws IOException {\n+\t\ttry (InputChannelRecoveredStateHandler stateHandler = new InputChannelRecoveredStateHandler(inputGates)) {\n+\t\t\tread(OperatorSubtaskState::getInputChannelState, stateHandler);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void readOutputData(ResultPartitionWriter[] writers) throws IOException {\n+\t\ttry (ResultSubpartitionRecoveredStateHandler stateHandler = new ResultSubpartitionRecoveredStateHandler(writers)) {\n+\t\t\tread(OperatorSubtaskState::getResultSubpartitionState, stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void read(\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\tfor (Map.Entry<StreamStateHandle, List<Handle>> delegateAndHandles : groupByDelegate(streamSubtaskStates(), stateHandleExtractor).entrySet()) {\n+\t\t\treadSequentially(delegateAndHandles.getKey(), delegateAndHandles.getValue(), stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void readSequentially(\n+\t\t\tStreamStateHandle streamStateHandle,\n+\t\t\tList<Handle> channelStateHandles,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\ttry (FSDataInputStream is = streamStateHandle.openInputStream()) {\n+\t\t\tserializer.readHeader(is);\n+\t\t\tfor (Tuple2<Long, Info> offsetAndChannelInfo : extractOffsetsSorted(channelStateHandles)) {\n+\t\t\t\tchunkReader.readChunk(is, offsetAndChannelInfo.f0, stateHandler, offsetAndChannelInfo.f1);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate Stream<OperatorSubtaskState> streamSubtaskStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().map(Map.Entry::getValue);\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> Map<StreamStateHandle, List<Handle>> groupByDelegate(\n+\t\t\tStream<OperatorSubtaskState> states,\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor) {\n+\t\treturn states\n+\t\t\t.map(stateHandleExtractor).flatMap(Collection::stream)\n+\t\t\t.peek(validate())\n+\t\t\t.collect(groupingBy(AbstractChannelStateHandle::getDelegate));\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> Consumer<Handle> validate() {\n+\t\tSet<Info> seen = new HashSet<>();\n+\t\t// expect each channel to be described only once; otherwise, buffers in channel could be re-ordered\n+\t\treturn handle -> Preconditions.checkState(seen.add(handle.getInfo()), \"duplicate channel info: %s\");\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> List<Tuple2<Long, Info>> extractOffsetsSorted(List<Handle> channelStateHandles) {\n+\t\treturn channelStateHandles\n+\t\t\t.stream()\n+\t\t\t.flatMap(SequentialChannelStateReaderImpl::extractOffsets)\n+\t\t\t.sorted(comparingLong(offsetAndInfo -> offsetAndInfo.f0))\n+\t\t\t.collect(toList());\n+\t}\n+\n+\tprivate static  <Info, Handle extends AbstractChannelStateHandle<Info>> Stream<Tuple2<Long, Info>> extractOffsets(Handle handle) {\n+\t\treturn handle.getOffsets().stream().map(offset -> Tuple2.of(offset, handle.getInfo()));\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws Exception {\n+\t}\n+\n+}\n+\n+interface RecoveredChannelStateHandler<Info, Context> extends AutoCloseable {\n+\tTuple2<ChannelStateByteBuffer, Context> getBuffer(Info info) throws IOException;\n+\n+\tvoid recover(Info info, Context context) throws IOException;\n+}\n+\n+class InputChannelRecoveredStateHandler implements RecoveredChannelStateHandler<InputChannelInfo, Buffer> {\n+\tprivate final InputGate[] inputGates;\n+\n+\tInputChannelRecoveredStateHandler(InputGate[] inputGates) {\n+\t\tthis.inputGates = inputGates;\n+\t}\n+\n+\t@Override\n+\tpublic Tuple2<ChannelStateByteBuffer, Buffer> getBuffer(InputChannelInfo channelInfo) throws IOException {\n+\t\tRecoveredInputChannel channel = getChannel(channelInfo);\n+\t\tBuffer buffer;\n+\t\ttry {\n+\t\t\tbuffer = channel.getBuffer();\n+\t\t\treturn Tuple2.of(wrap(buffer), buffer);\n+\t\t} catch (InterruptedException e) {\n+\t\t\tthrow new IOException(e);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void recover(InputChannelInfo channelInfo, Buffer buffer) {\n+\t\tif (buffer.readableBytes() > 0) {\n+\t\t\tgetChannel(channelInfo).onRecoveredStateBuffer(buffer);\n+\t\t} else {\n+\t\t\tbuffer.recycleBuffer();\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws IOException {\n+\t\t// note that we need to finish all RecoveredInputChannels, not just those with state\n+\t\tfor (final InputGate inputGate : inputGates) {\n+\t\t\tinputGate.finishReadRecoveredState();\n+\t\t}\n+\t}\n+\n+\tprivate RecoveredInputChannel getChannel(InputChannelInfo info) {\n+\t\treturn (RecoveredInputChannel) inputGates[info.getGateIdx()].getChannel(info.getInputChannelIdx());\n+\t}\n+}\n+\n+class ResultSubpartitionRecoveredStateHandler implements RecoveredChannelStateHandler<ResultSubpartitionInfo, Tuple2<BufferBuilder, BufferConsumer>> {\n+\n+\tprivate final ResultPartitionWriter[] writers;\n+\n+\tResultSubpartitionRecoveredStateHandler(ResultPartitionWriter[] writers) {\n+\t\tthis.writers = writers;\n+\t}\n+\n+\t@Override\n+\tpublic Tuple2<ChannelStateByteBuffer, Tuple2<BufferBuilder, BufferConsumer>> getBuffer(ResultSubpartitionInfo subpartitionInfo) throws IOException {\n+\t\tBufferBuilder bufferBuilder;\n+\t\ttry {\n+\t\t\tbufferBuilder = getWriter(subpartitionInfo).getBufferBuilder(subpartitionInfo.getSubPartitionIdx());\n+\t\t} catch (InterruptedException e) {\n+\t\t\tthrow new IOException(e);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDMxOTg5Nw=="}, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 214}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDg0ODQzNA==", "bodyText": "InterruptedException quite often should be handled differently and it's good to know that the InterruptedException has happened.\nFor example, when caller received IOException, it might decide to retry or just keep going if that was not a critical part. InterruptedException should on the other hand shut down the code semi gracefully and as quickly as possible.\nHere I think adding throws InterruptedException would be much better. But I didn't look how deep in a call stack are we and how difficult would it be to propagate it. Whether it is appropriate on this layer or not is kind of irrelevant. It can happen, the question is how should you handle it?\nre Thread.currentThread().interrupt();, the crowd wisdom is to always set it when wrapping/hiding/rethrowing InterruptedException. I think we are using interrupted flag in only a couple of places, but it's better to keep it clean, just in case we would had to use it some important place and then we would discover we need to fix interrupted flag everywhere to make it work.", "url": "https://github.com/apache/flink/pull/13351#discussion_r494848434", "createdAt": "2020-09-25T08:59:16Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImpl.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.fs.FSDataInputStream;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.runtime.checkpoint.StateObjectCollection;\n+import org.apache.flink.runtime.checkpoint.TaskStateSnapshot;\n+import org.apache.flink.runtime.io.network.api.writer.ResultPartitionWriter;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.BufferBuilder;\n+import org.apache.flink.runtime.io.network.buffer.BufferConsumer;\n+import org.apache.flink.runtime.io.network.partition.consumer.InputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel;\n+import org.apache.flink.runtime.state.AbstractChannelStateHandle;\n+import org.apache.flink.runtime.state.StreamStateHandle;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+import java.util.stream.Stream;\n+\n+import static java.util.Comparator.comparingLong;\n+import static java.util.stream.Collectors.groupingBy;\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.flink.runtime.checkpoint.channel.ChannelStateByteBuffer.wrap;\n+\n+/**\n+ * {@link SequentialChannelStateReader} implementation.\n+ */\n+public class SequentialChannelStateReaderImpl implements SequentialChannelStateReader {\n+\n+\tprivate final TaskStateSnapshot taskStateSnapshot;\n+\tprivate final ChannelStateSerializer serializer;\n+\tprivate final ChannelStateChunkReader chunkReader;\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot) {\n+\t\tthis(taskStateSnapshot, new ChannelStateSerializerImpl());\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = new ChannelStateChunkReader(serializer);\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer, ChannelStateChunkReader chunkReader) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = chunkReader;\n+\t}\n+\n+\t@Override\n+\tpublic boolean hasChannelStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().anyMatch(subtaskStateEntry ->\n+\t\t\tsubtaskStateEntry.getValue().getInputChannelState().stream().anyMatch(h -> !h.getOffsets().isEmpty()) ||\n+\t\t\t\tsubtaskStateEntry.getValue().getResultSubpartitionState().stream().anyMatch(h -> !h.getOffsets().isEmpty()));\n+\t}\n+\n+\t@Override\n+\tpublic void readInputData(InputGate[] inputGates) throws IOException {\n+\t\ttry (InputChannelRecoveredStateHandler stateHandler = new InputChannelRecoveredStateHandler(inputGates)) {\n+\t\t\tread(OperatorSubtaskState::getInputChannelState, stateHandler);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void readOutputData(ResultPartitionWriter[] writers) throws IOException {\n+\t\ttry (ResultSubpartitionRecoveredStateHandler stateHandler = new ResultSubpartitionRecoveredStateHandler(writers)) {\n+\t\t\tread(OperatorSubtaskState::getResultSubpartitionState, stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void read(\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\tfor (Map.Entry<StreamStateHandle, List<Handle>> delegateAndHandles : groupByDelegate(streamSubtaskStates(), stateHandleExtractor).entrySet()) {\n+\t\t\treadSequentially(delegateAndHandles.getKey(), delegateAndHandles.getValue(), stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void readSequentially(\n+\t\t\tStreamStateHandle streamStateHandle,\n+\t\t\tList<Handle> channelStateHandles,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\ttry (FSDataInputStream is = streamStateHandle.openInputStream()) {\n+\t\t\tserializer.readHeader(is);\n+\t\t\tfor (Tuple2<Long, Info> offsetAndChannelInfo : extractOffsetsSorted(channelStateHandles)) {\n+\t\t\t\tchunkReader.readChunk(is, offsetAndChannelInfo.f0, stateHandler, offsetAndChannelInfo.f1);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate Stream<OperatorSubtaskState> streamSubtaskStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().map(Map.Entry::getValue);\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> Map<StreamStateHandle, List<Handle>> groupByDelegate(\n+\t\t\tStream<OperatorSubtaskState> states,\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor) {\n+\t\treturn states\n+\t\t\t.map(stateHandleExtractor).flatMap(Collection::stream)\n+\t\t\t.peek(validate())\n+\t\t\t.collect(groupingBy(AbstractChannelStateHandle::getDelegate));\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> Consumer<Handle> validate() {\n+\t\tSet<Info> seen = new HashSet<>();\n+\t\t// expect each channel to be described only once; otherwise, buffers in channel could be re-ordered\n+\t\treturn handle -> Preconditions.checkState(seen.add(handle.getInfo()), \"duplicate channel info: %s\");\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> List<Tuple2<Long, Info>> extractOffsetsSorted(List<Handle> channelStateHandles) {\n+\t\treturn channelStateHandles\n+\t\t\t.stream()\n+\t\t\t.flatMap(SequentialChannelStateReaderImpl::extractOffsets)\n+\t\t\t.sorted(comparingLong(offsetAndInfo -> offsetAndInfo.f0))\n+\t\t\t.collect(toList());\n+\t}\n+\n+\tprivate static  <Info, Handle extends AbstractChannelStateHandle<Info>> Stream<Tuple2<Long, Info>> extractOffsets(Handle handle) {\n+\t\treturn handle.getOffsets().stream().map(offset -> Tuple2.of(offset, handle.getInfo()));\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws Exception {\n+\t}\n+\n+}\n+\n+interface RecoveredChannelStateHandler<Info, Context> extends AutoCloseable {\n+\tTuple2<ChannelStateByteBuffer, Context> getBuffer(Info info) throws IOException;\n+\n+\tvoid recover(Info info, Context context) throws IOException;\n+}\n+\n+class InputChannelRecoveredStateHandler implements RecoveredChannelStateHandler<InputChannelInfo, Buffer> {\n+\tprivate final InputGate[] inputGates;\n+\n+\tInputChannelRecoveredStateHandler(InputGate[] inputGates) {\n+\t\tthis.inputGates = inputGates;\n+\t}\n+\n+\t@Override\n+\tpublic Tuple2<ChannelStateByteBuffer, Buffer> getBuffer(InputChannelInfo channelInfo) throws IOException {\n+\t\tRecoveredInputChannel channel = getChannel(channelInfo);\n+\t\tBuffer buffer;\n+\t\ttry {\n+\t\t\tbuffer = channel.getBuffer();\n+\t\t\treturn Tuple2.of(wrap(buffer), buffer);\n+\t\t} catch (InterruptedException e) {\n+\t\t\tthrow new IOException(e);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void recover(InputChannelInfo channelInfo, Buffer buffer) {\n+\t\tif (buffer.readableBytes() > 0) {\n+\t\t\tgetChannel(channelInfo).onRecoveredStateBuffer(buffer);\n+\t\t} else {\n+\t\t\tbuffer.recycleBuffer();\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws IOException {\n+\t\t// note that we need to finish all RecoveredInputChannels, not just those with state\n+\t\tfor (final InputGate inputGate : inputGates) {\n+\t\t\tinputGate.finishReadRecoveredState();\n+\t\t}\n+\t}\n+\n+\tprivate RecoveredInputChannel getChannel(InputChannelInfo info) {\n+\t\treturn (RecoveredInputChannel) inputGates[info.getGateIdx()].getChannel(info.getInputChannelIdx());\n+\t}\n+}\n+\n+class ResultSubpartitionRecoveredStateHandler implements RecoveredChannelStateHandler<ResultSubpartitionInfo, Tuple2<BufferBuilder, BufferConsumer>> {\n+\n+\tprivate final ResultPartitionWriter[] writers;\n+\n+\tResultSubpartitionRecoveredStateHandler(ResultPartitionWriter[] writers) {\n+\t\tthis.writers = writers;\n+\t}\n+\n+\t@Override\n+\tpublic Tuple2<ChannelStateByteBuffer, Tuple2<BufferBuilder, BufferConsumer>> getBuffer(ResultSubpartitionInfo subpartitionInfo) throws IOException {\n+\t\tBufferBuilder bufferBuilder;\n+\t\ttry {\n+\t\t\tbufferBuilder = getWriter(subpartitionInfo).getBufferBuilder(subpartitionInfo.getSubPartitionIdx());\n+\t\t} catch (InterruptedException e) {\n+\t\t\tthrow new IOException(e);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDMxOTg5Nw=="}, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 214}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTA4NzQxNQ==", "bodyText": "You're right about currentThread().interrupt() and that handling of IOException likely differs from that of InterruptedException.\nBut for the latter, I think it's better to wrap it into RuntimeException.", "url": "https://github.com/apache/flink/pull/13351#discussion_r495087415", "createdAt": "2020-09-25T16:04:15Z", "author": {"login": "rkhachatryan"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImpl.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.fs.FSDataInputStream;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.runtime.checkpoint.StateObjectCollection;\n+import org.apache.flink.runtime.checkpoint.TaskStateSnapshot;\n+import org.apache.flink.runtime.io.network.api.writer.ResultPartitionWriter;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.BufferBuilder;\n+import org.apache.flink.runtime.io.network.buffer.BufferConsumer;\n+import org.apache.flink.runtime.io.network.partition.consumer.InputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel;\n+import org.apache.flink.runtime.state.AbstractChannelStateHandle;\n+import org.apache.flink.runtime.state.StreamStateHandle;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+import java.util.stream.Stream;\n+\n+import static java.util.Comparator.comparingLong;\n+import static java.util.stream.Collectors.groupingBy;\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.flink.runtime.checkpoint.channel.ChannelStateByteBuffer.wrap;\n+\n+/**\n+ * {@link SequentialChannelStateReader} implementation.\n+ */\n+public class SequentialChannelStateReaderImpl implements SequentialChannelStateReader {\n+\n+\tprivate final TaskStateSnapshot taskStateSnapshot;\n+\tprivate final ChannelStateSerializer serializer;\n+\tprivate final ChannelStateChunkReader chunkReader;\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot) {\n+\t\tthis(taskStateSnapshot, new ChannelStateSerializerImpl());\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = new ChannelStateChunkReader(serializer);\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer, ChannelStateChunkReader chunkReader) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = chunkReader;\n+\t}\n+\n+\t@Override\n+\tpublic boolean hasChannelStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().anyMatch(subtaskStateEntry ->\n+\t\t\tsubtaskStateEntry.getValue().getInputChannelState().stream().anyMatch(h -> !h.getOffsets().isEmpty()) ||\n+\t\t\t\tsubtaskStateEntry.getValue().getResultSubpartitionState().stream().anyMatch(h -> !h.getOffsets().isEmpty()));\n+\t}\n+\n+\t@Override\n+\tpublic void readInputData(InputGate[] inputGates) throws IOException {\n+\t\ttry (InputChannelRecoveredStateHandler stateHandler = new InputChannelRecoveredStateHandler(inputGates)) {\n+\t\t\tread(OperatorSubtaskState::getInputChannelState, stateHandler);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void readOutputData(ResultPartitionWriter[] writers) throws IOException {\n+\t\ttry (ResultSubpartitionRecoveredStateHandler stateHandler = new ResultSubpartitionRecoveredStateHandler(writers)) {\n+\t\t\tread(OperatorSubtaskState::getResultSubpartitionState, stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void read(\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\tfor (Map.Entry<StreamStateHandle, List<Handle>> delegateAndHandles : groupByDelegate(streamSubtaskStates(), stateHandleExtractor).entrySet()) {\n+\t\t\treadSequentially(delegateAndHandles.getKey(), delegateAndHandles.getValue(), stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void readSequentially(\n+\t\t\tStreamStateHandle streamStateHandle,\n+\t\t\tList<Handle> channelStateHandles,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\ttry (FSDataInputStream is = streamStateHandle.openInputStream()) {\n+\t\t\tserializer.readHeader(is);\n+\t\t\tfor (Tuple2<Long, Info> offsetAndChannelInfo : extractOffsetsSorted(channelStateHandles)) {\n+\t\t\t\tchunkReader.readChunk(is, offsetAndChannelInfo.f0, stateHandler, offsetAndChannelInfo.f1);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate Stream<OperatorSubtaskState> streamSubtaskStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().map(Map.Entry::getValue);\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> Map<StreamStateHandle, List<Handle>> groupByDelegate(\n+\t\t\tStream<OperatorSubtaskState> states,\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor) {\n+\t\treturn states\n+\t\t\t.map(stateHandleExtractor).flatMap(Collection::stream)\n+\t\t\t.peek(validate())\n+\t\t\t.collect(groupingBy(AbstractChannelStateHandle::getDelegate));\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> Consumer<Handle> validate() {\n+\t\tSet<Info> seen = new HashSet<>();\n+\t\t// expect each channel to be described only once; otherwise, buffers in channel could be re-ordered\n+\t\treturn handle -> Preconditions.checkState(seen.add(handle.getInfo()), \"duplicate channel info: %s\");\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> List<Tuple2<Long, Info>> extractOffsetsSorted(List<Handle> channelStateHandles) {\n+\t\treturn channelStateHandles\n+\t\t\t.stream()\n+\t\t\t.flatMap(SequentialChannelStateReaderImpl::extractOffsets)\n+\t\t\t.sorted(comparingLong(offsetAndInfo -> offsetAndInfo.f0))\n+\t\t\t.collect(toList());\n+\t}\n+\n+\tprivate static  <Info, Handle extends AbstractChannelStateHandle<Info>> Stream<Tuple2<Long, Info>> extractOffsets(Handle handle) {\n+\t\treturn handle.getOffsets().stream().map(offset -> Tuple2.of(offset, handle.getInfo()));\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws Exception {\n+\t}\n+\n+}\n+\n+interface RecoveredChannelStateHandler<Info, Context> extends AutoCloseable {\n+\tTuple2<ChannelStateByteBuffer, Context> getBuffer(Info info) throws IOException;\n+\n+\tvoid recover(Info info, Context context) throws IOException;\n+}\n+\n+class InputChannelRecoveredStateHandler implements RecoveredChannelStateHandler<InputChannelInfo, Buffer> {\n+\tprivate final InputGate[] inputGates;\n+\n+\tInputChannelRecoveredStateHandler(InputGate[] inputGates) {\n+\t\tthis.inputGates = inputGates;\n+\t}\n+\n+\t@Override\n+\tpublic Tuple2<ChannelStateByteBuffer, Buffer> getBuffer(InputChannelInfo channelInfo) throws IOException {\n+\t\tRecoveredInputChannel channel = getChannel(channelInfo);\n+\t\tBuffer buffer;\n+\t\ttry {\n+\t\t\tbuffer = channel.getBuffer();\n+\t\t\treturn Tuple2.of(wrap(buffer), buffer);\n+\t\t} catch (InterruptedException e) {\n+\t\t\tthrow new IOException(e);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void recover(InputChannelInfo channelInfo, Buffer buffer) {\n+\t\tif (buffer.readableBytes() > 0) {\n+\t\t\tgetChannel(channelInfo).onRecoveredStateBuffer(buffer);\n+\t\t} else {\n+\t\t\tbuffer.recycleBuffer();\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws IOException {\n+\t\t// note that we need to finish all RecoveredInputChannels, not just those with state\n+\t\tfor (final InputGate inputGate : inputGates) {\n+\t\t\tinputGate.finishReadRecoveredState();\n+\t\t}\n+\t}\n+\n+\tprivate RecoveredInputChannel getChannel(InputChannelInfo info) {\n+\t\treturn (RecoveredInputChannel) inputGates[info.getGateIdx()].getChannel(info.getInputChannelIdx());\n+\t}\n+}\n+\n+class ResultSubpartitionRecoveredStateHandler implements RecoveredChannelStateHandler<ResultSubpartitionInfo, Tuple2<BufferBuilder, BufferConsumer>> {\n+\n+\tprivate final ResultPartitionWriter[] writers;\n+\n+\tResultSubpartitionRecoveredStateHandler(ResultPartitionWriter[] writers) {\n+\t\tthis.writers = writers;\n+\t}\n+\n+\t@Override\n+\tpublic Tuple2<ChannelStateByteBuffer, Tuple2<BufferBuilder, BufferConsumer>> getBuffer(ResultSubpartitionInfo subpartitionInfo) throws IOException {\n+\t\tBufferBuilder bufferBuilder;\n+\t\ttry {\n+\t\t\tbufferBuilder = getWriter(subpartitionInfo).getBufferBuilder(subpartitionInfo.getSubPartitionIdx());\n+\t\t} catch (InterruptedException e) {\n+\t\t\tthrow new IOException(e);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDMxOTg5Nw=="}, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 214}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTc2NDk5OA==", "bodyText": "Personally I don't like declared exceptions, but even in that case special treatment of the InterruptedException means  it makes sense to declare it explicitly. But the issue is that in Flink trend is to have explicit exceptions handling/declarations. Here it looks like the InterruptedException is already present on the call site (StreamTask#readRecoveredChannelState), so it shouldn't be an issue to conform with this.", "url": "https://github.com/apache/flink/pull/13351#discussion_r495764998", "createdAt": "2020-09-28T08:17:26Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImpl.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.fs.FSDataInputStream;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.runtime.checkpoint.StateObjectCollection;\n+import org.apache.flink.runtime.checkpoint.TaskStateSnapshot;\n+import org.apache.flink.runtime.io.network.api.writer.ResultPartitionWriter;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.BufferBuilder;\n+import org.apache.flink.runtime.io.network.buffer.BufferConsumer;\n+import org.apache.flink.runtime.io.network.partition.consumer.InputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel;\n+import org.apache.flink.runtime.state.AbstractChannelStateHandle;\n+import org.apache.flink.runtime.state.StreamStateHandle;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+import java.util.stream.Stream;\n+\n+import static java.util.Comparator.comparingLong;\n+import static java.util.stream.Collectors.groupingBy;\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.flink.runtime.checkpoint.channel.ChannelStateByteBuffer.wrap;\n+\n+/**\n+ * {@link SequentialChannelStateReader} implementation.\n+ */\n+public class SequentialChannelStateReaderImpl implements SequentialChannelStateReader {\n+\n+\tprivate final TaskStateSnapshot taskStateSnapshot;\n+\tprivate final ChannelStateSerializer serializer;\n+\tprivate final ChannelStateChunkReader chunkReader;\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot) {\n+\t\tthis(taskStateSnapshot, new ChannelStateSerializerImpl());\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = new ChannelStateChunkReader(serializer);\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer, ChannelStateChunkReader chunkReader) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = chunkReader;\n+\t}\n+\n+\t@Override\n+\tpublic boolean hasChannelStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().anyMatch(subtaskStateEntry ->\n+\t\t\tsubtaskStateEntry.getValue().getInputChannelState().stream().anyMatch(h -> !h.getOffsets().isEmpty()) ||\n+\t\t\t\tsubtaskStateEntry.getValue().getResultSubpartitionState().stream().anyMatch(h -> !h.getOffsets().isEmpty()));\n+\t}\n+\n+\t@Override\n+\tpublic void readInputData(InputGate[] inputGates) throws IOException {\n+\t\ttry (InputChannelRecoveredStateHandler stateHandler = new InputChannelRecoveredStateHandler(inputGates)) {\n+\t\t\tread(OperatorSubtaskState::getInputChannelState, stateHandler);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void readOutputData(ResultPartitionWriter[] writers) throws IOException {\n+\t\ttry (ResultSubpartitionRecoveredStateHandler stateHandler = new ResultSubpartitionRecoveredStateHandler(writers)) {\n+\t\t\tread(OperatorSubtaskState::getResultSubpartitionState, stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void read(\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\tfor (Map.Entry<StreamStateHandle, List<Handle>> delegateAndHandles : groupByDelegate(streamSubtaskStates(), stateHandleExtractor).entrySet()) {\n+\t\t\treadSequentially(delegateAndHandles.getKey(), delegateAndHandles.getValue(), stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void readSequentially(\n+\t\t\tStreamStateHandle streamStateHandle,\n+\t\t\tList<Handle> channelStateHandles,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\ttry (FSDataInputStream is = streamStateHandle.openInputStream()) {\n+\t\t\tserializer.readHeader(is);\n+\t\t\tfor (Tuple2<Long, Info> offsetAndChannelInfo : extractOffsetsSorted(channelStateHandles)) {\n+\t\t\t\tchunkReader.readChunk(is, offsetAndChannelInfo.f0, stateHandler, offsetAndChannelInfo.f1);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate Stream<OperatorSubtaskState> streamSubtaskStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().map(Map.Entry::getValue);\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> Map<StreamStateHandle, List<Handle>> groupByDelegate(\n+\t\t\tStream<OperatorSubtaskState> states,\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor) {\n+\t\treturn states\n+\t\t\t.map(stateHandleExtractor).flatMap(Collection::stream)\n+\t\t\t.peek(validate())\n+\t\t\t.collect(groupingBy(AbstractChannelStateHandle::getDelegate));\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> Consumer<Handle> validate() {\n+\t\tSet<Info> seen = new HashSet<>();\n+\t\t// expect each channel to be described only once; otherwise, buffers in channel could be re-ordered\n+\t\treturn handle -> Preconditions.checkState(seen.add(handle.getInfo()), \"duplicate channel info: %s\");\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> List<Tuple2<Long, Info>> extractOffsetsSorted(List<Handle> channelStateHandles) {\n+\t\treturn channelStateHandles\n+\t\t\t.stream()\n+\t\t\t.flatMap(SequentialChannelStateReaderImpl::extractOffsets)\n+\t\t\t.sorted(comparingLong(offsetAndInfo -> offsetAndInfo.f0))\n+\t\t\t.collect(toList());\n+\t}\n+\n+\tprivate static  <Info, Handle extends AbstractChannelStateHandle<Info>> Stream<Tuple2<Long, Info>> extractOffsets(Handle handle) {\n+\t\treturn handle.getOffsets().stream().map(offset -> Tuple2.of(offset, handle.getInfo()));\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws Exception {\n+\t}\n+\n+}\n+\n+interface RecoveredChannelStateHandler<Info, Context> extends AutoCloseable {\n+\tTuple2<ChannelStateByteBuffer, Context> getBuffer(Info info) throws IOException;\n+\n+\tvoid recover(Info info, Context context) throws IOException;\n+}\n+\n+class InputChannelRecoveredStateHandler implements RecoveredChannelStateHandler<InputChannelInfo, Buffer> {\n+\tprivate final InputGate[] inputGates;\n+\n+\tInputChannelRecoveredStateHandler(InputGate[] inputGates) {\n+\t\tthis.inputGates = inputGates;\n+\t}\n+\n+\t@Override\n+\tpublic Tuple2<ChannelStateByteBuffer, Buffer> getBuffer(InputChannelInfo channelInfo) throws IOException {\n+\t\tRecoveredInputChannel channel = getChannel(channelInfo);\n+\t\tBuffer buffer;\n+\t\ttry {\n+\t\t\tbuffer = channel.getBuffer();\n+\t\t\treturn Tuple2.of(wrap(buffer), buffer);\n+\t\t} catch (InterruptedException e) {\n+\t\t\tthrow new IOException(e);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void recover(InputChannelInfo channelInfo, Buffer buffer) {\n+\t\tif (buffer.readableBytes() > 0) {\n+\t\t\tgetChannel(channelInfo).onRecoveredStateBuffer(buffer);\n+\t\t} else {\n+\t\t\tbuffer.recycleBuffer();\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws IOException {\n+\t\t// note that we need to finish all RecoveredInputChannels, not just those with state\n+\t\tfor (final InputGate inputGate : inputGates) {\n+\t\t\tinputGate.finishReadRecoveredState();\n+\t\t}\n+\t}\n+\n+\tprivate RecoveredInputChannel getChannel(InputChannelInfo info) {\n+\t\treturn (RecoveredInputChannel) inputGates[info.getGateIdx()].getChannel(info.getInputChannelIdx());\n+\t}\n+}\n+\n+class ResultSubpartitionRecoveredStateHandler implements RecoveredChannelStateHandler<ResultSubpartitionInfo, Tuple2<BufferBuilder, BufferConsumer>> {\n+\n+\tprivate final ResultPartitionWriter[] writers;\n+\n+\tResultSubpartitionRecoveredStateHandler(ResultPartitionWriter[] writers) {\n+\t\tthis.writers = writers;\n+\t}\n+\n+\t@Override\n+\tpublic Tuple2<ChannelStateByteBuffer, Tuple2<BufferBuilder, BufferConsumer>> getBuffer(ResultSubpartitionInfo subpartitionInfo) throws IOException {\n+\t\tBufferBuilder bufferBuilder;\n+\t\ttry {\n+\t\t\tbufferBuilder = getWriter(subpartitionInfo).getBufferBuilder(subpartitionInfo.getSubPartitionIdx());\n+\t\t} catch (InterruptedException e) {\n+\t\t\tthrow new IOException(e);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDMxOTg5Nw=="}, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 214}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjQ1Nzg3Mg==", "bodyText": "Removed catch InterruptedException and added to the method signature after not converging in an offline discussion.", "url": "https://github.com/apache/flink/pull/13351#discussion_r496457872", "createdAt": "2020-09-29T06:54:20Z", "author": {"login": "rkhachatryan"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImpl.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.fs.FSDataInputStream;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.runtime.checkpoint.StateObjectCollection;\n+import org.apache.flink.runtime.checkpoint.TaskStateSnapshot;\n+import org.apache.flink.runtime.io.network.api.writer.ResultPartitionWriter;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.BufferBuilder;\n+import org.apache.flink.runtime.io.network.buffer.BufferConsumer;\n+import org.apache.flink.runtime.io.network.partition.consumer.InputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel;\n+import org.apache.flink.runtime.state.AbstractChannelStateHandle;\n+import org.apache.flink.runtime.state.StreamStateHandle;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+import java.util.stream.Stream;\n+\n+import static java.util.Comparator.comparingLong;\n+import static java.util.stream.Collectors.groupingBy;\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.flink.runtime.checkpoint.channel.ChannelStateByteBuffer.wrap;\n+\n+/**\n+ * {@link SequentialChannelStateReader} implementation.\n+ */\n+public class SequentialChannelStateReaderImpl implements SequentialChannelStateReader {\n+\n+\tprivate final TaskStateSnapshot taskStateSnapshot;\n+\tprivate final ChannelStateSerializer serializer;\n+\tprivate final ChannelStateChunkReader chunkReader;\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot) {\n+\t\tthis(taskStateSnapshot, new ChannelStateSerializerImpl());\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = new ChannelStateChunkReader(serializer);\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer, ChannelStateChunkReader chunkReader) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = chunkReader;\n+\t}\n+\n+\t@Override\n+\tpublic boolean hasChannelStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().anyMatch(subtaskStateEntry ->\n+\t\t\tsubtaskStateEntry.getValue().getInputChannelState().stream().anyMatch(h -> !h.getOffsets().isEmpty()) ||\n+\t\t\t\tsubtaskStateEntry.getValue().getResultSubpartitionState().stream().anyMatch(h -> !h.getOffsets().isEmpty()));\n+\t}\n+\n+\t@Override\n+\tpublic void readInputData(InputGate[] inputGates) throws IOException {\n+\t\ttry (InputChannelRecoveredStateHandler stateHandler = new InputChannelRecoveredStateHandler(inputGates)) {\n+\t\t\tread(OperatorSubtaskState::getInputChannelState, stateHandler);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void readOutputData(ResultPartitionWriter[] writers) throws IOException {\n+\t\ttry (ResultSubpartitionRecoveredStateHandler stateHandler = new ResultSubpartitionRecoveredStateHandler(writers)) {\n+\t\t\tread(OperatorSubtaskState::getResultSubpartitionState, stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void read(\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\tfor (Map.Entry<StreamStateHandle, List<Handle>> delegateAndHandles : groupByDelegate(streamSubtaskStates(), stateHandleExtractor).entrySet()) {\n+\t\t\treadSequentially(delegateAndHandles.getKey(), delegateAndHandles.getValue(), stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void readSequentially(\n+\t\t\tStreamStateHandle streamStateHandle,\n+\t\t\tList<Handle> channelStateHandles,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\ttry (FSDataInputStream is = streamStateHandle.openInputStream()) {\n+\t\t\tserializer.readHeader(is);\n+\t\t\tfor (Tuple2<Long, Info> offsetAndChannelInfo : extractOffsetsSorted(channelStateHandles)) {\n+\t\t\t\tchunkReader.readChunk(is, offsetAndChannelInfo.f0, stateHandler, offsetAndChannelInfo.f1);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate Stream<OperatorSubtaskState> streamSubtaskStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().map(Map.Entry::getValue);\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> Map<StreamStateHandle, List<Handle>> groupByDelegate(\n+\t\t\tStream<OperatorSubtaskState> states,\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor) {\n+\t\treturn states\n+\t\t\t.map(stateHandleExtractor).flatMap(Collection::stream)\n+\t\t\t.peek(validate())\n+\t\t\t.collect(groupingBy(AbstractChannelStateHandle::getDelegate));\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> Consumer<Handle> validate() {\n+\t\tSet<Info> seen = new HashSet<>();\n+\t\t// expect each channel to be described only once; otherwise, buffers in channel could be re-ordered\n+\t\treturn handle -> Preconditions.checkState(seen.add(handle.getInfo()), \"duplicate channel info: %s\");\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> List<Tuple2<Long, Info>> extractOffsetsSorted(List<Handle> channelStateHandles) {\n+\t\treturn channelStateHandles\n+\t\t\t.stream()\n+\t\t\t.flatMap(SequentialChannelStateReaderImpl::extractOffsets)\n+\t\t\t.sorted(comparingLong(offsetAndInfo -> offsetAndInfo.f0))\n+\t\t\t.collect(toList());\n+\t}\n+\n+\tprivate static  <Info, Handle extends AbstractChannelStateHandle<Info>> Stream<Tuple2<Long, Info>> extractOffsets(Handle handle) {\n+\t\treturn handle.getOffsets().stream().map(offset -> Tuple2.of(offset, handle.getInfo()));\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws Exception {\n+\t}\n+\n+}\n+\n+interface RecoveredChannelStateHandler<Info, Context> extends AutoCloseable {\n+\tTuple2<ChannelStateByteBuffer, Context> getBuffer(Info info) throws IOException;\n+\n+\tvoid recover(Info info, Context context) throws IOException;\n+}\n+\n+class InputChannelRecoveredStateHandler implements RecoveredChannelStateHandler<InputChannelInfo, Buffer> {\n+\tprivate final InputGate[] inputGates;\n+\n+\tInputChannelRecoveredStateHandler(InputGate[] inputGates) {\n+\t\tthis.inputGates = inputGates;\n+\t}\n+\n+\t@Override\n+\tpublic Tuple2<ChannelStateByteBuffer, Buffer> getBuffer(InputChannelInfo channelInfo) throws IOException {\n+\t\tRecoveredInputChannel channel = getChannel(channelInfo);\n+\t\tBuffer buffer;\n+\t\ttry {\n+\t\t\tbuffer = channel.getBuffer();\n+\t\t\treturn Tuple2.of(wrap(buffer), buffer);\n+\t\t} catch (InterruptedException e) {\n+\t\t\tthrow new IOException(e);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void recover(InputChannelInfo channelInfo, Buffer buffer) {\n+\t\tif (buffer.readableBytes() > 0) {\n+\t\t\tgetChannel(channelInfo).onRecoveredStateBuffer(buffer);\n+\t\t} else {\n+\t\t\tbuffer.recycleBuffer();\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws IOException {\n+\t\t// note that we need to finish all RecoveredInputChannels, not just those with state\n+\t\tfor (final InputGate inputGate : inputGates) {\n+\t\t\tinputGate.finishReadRecoveredState();\n+\t\t}\n+\t}\n+\n+\tprivate RecoveredInputChannel getChannel(InputChannelInfo info) {\n+\t\treturn (RecoveredInputChannel) inputGates[info.getGateIdx()].getChannel(info.getInputChannelIdx());\n+\t}\n+}\n+\n+class ResultSubpartitionRecoveredStateHandler implements RecoveredChannelStateHandler<ResultSubpartitionInfo, Tuple2<BufferBuilder, BufferConsumer>> {\n+\n+\tprivate final ResultPartitionWriter[] writers;\n+\n+\tResultSubpartitionRecoveredStateHandler(ResultPartitionWriter[] writers) {\n+\t\tthis.writers = writers;\n+\t}\n+\n+\t@Override\n+\tpublic Tuple2<ChannelStateByteBuffer, Tuple2<BufferBuilder, BufferConsumer>> getBuffer(ResultSubpartitionInfo subpartitionInfo) throws IOException {\n+\t\tBufferBuilder bufferBuilder;\n+\t\ttry {\n+\t\t\tbufferBuilder = getWriter(subpartitionInfo).getBufferBuilder(subpartitionInfo.getSubPartitionIdx());\n+\t\t} catch (InterruptedException e) {\n+\t\t\tthrow new IOException(e);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDMxOTg5Nw=="}, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 214}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5Mzc4OTAyOnYy", "diffSide": "RIGHT", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImpl.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxMzozNToxOFrOHXa_pA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxMzozNToxOFrOHXa_pA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDMyMTU3Mg==", "bodyText": "This check is missing on output side. Add there for symmetry?\nHowever, I'd probably rather skip writing empty buffers altogether and just use a checkState to verify it here (and remove else branch).", "url": "https://github.com/apache/flink/pull/13351#discussion_r494321572", "createdAt": "2020-09-24T13:35:18Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImpl.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.fs.FSDataInputStream;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.runtime.checkpoint.StateObjectCollection;\n+import org.apache.flink.runtime.checkpoint.TaskStateSnapshot;\n+import org.apache.flink.runtime.io.network.api.writer.ResultPartitionWriter;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.BufferBuilder;\n+import org.apache.flink.runtime.io.network.buffer.BufferConsumer;\n+import org.apache.flink.runtime.io.network.partition.consumer.InputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel;\n+import org.apache.flink.runtime.state.AbstractChannelStateHandle;\n+import org.apache.flink.runtime.state.StreamStateHandle;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+import java.util.stream.Stream;\n+\n+import static java.util.Comparator.comparingLong;\n+import static java.util.stream.Collectors.groupingBy;\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.flink.runtime.checkpoint.channel.ChannelStateByteBuffer.wrap;\n+\n+/**\n+ * {@link SequentialChannelStateReader} implementation.\n+ */\n+public class SequentialChannelStateReaderImpl implements SequentialChannelStateReader {\n+\n+\tprivate final TaskStateSnapshot taskStateSnapshot;\n+\tprivate final ChannelStateSerializer serializer;\n+\tprivate final ChannelStateChunkReader chunkReader;\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot) {\n+\t\tthis(taskStateSnapshot, new ChannelStateSerializerImpl());\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = new ChannelStateChunkReader(serializer);\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer, ChannelStateChunkReader chunkReader) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = chunkReader;\n+\t}\n+\n+\t@Override\n+\tpublic boolean hasChannelStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().anyMatch(subtaskStateEntry ->\n+\t\t\tsubtaskStateEntry.getValue().getInputChannelState().stream().anyMatch(h -> !h.getOffsets().isEmpty()) ||\n+\t\t\t\tsubtaskStateEntry.getValue().getResultSubpartitionState().stream().anyMatch(h -> !h.getOffsets().isEmpty()));\n+\t}\n+\n+\t@Override\n+\tpublic void readInputData(InputGate[] inputGates) throws IOException {\n+\t\ttry (InputChannelRecoveredStateHandler stateHandler = new InputChannelRecoveredStateHandler(inputGates)) {\n+\t\t\tread(OperatorSubtaskState::getInputChannelState, stateHandler);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void readOutputData(ResultPartitionWriter[] writers) throws IOException {\n+\t\ttry (ResultSubpartitionRecoveredStateHandler stateHandler = new ResultSubpartitionRecoveredStateHandler(writers)) {\n+\t\t\tread(OperatorSubtaskState::getResultSubpartitionState, stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void read(\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\tfor (Map.Entry<StreamStateHandle, List<Handle>> delegateAndHandles : groupByDelegate(streamSubtaskStates(), stateHandleExtractor).entrySet()) {\n+\t\t\treadSequentially(delegateAndHandles.getKey(), delegateAndHandles.getValue(), stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void readSequentially(\n+\t\t\tStreamStateHandle streamStateHandle,\n+\t\t\tList<Handle> channelStateHandles,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\ttry (FSDataInputStream is = streamStateHandle.openInputStream()) {\n+\t\t\tserializer.readHeader(is);\n+\t\t\tfor (Tuple2<Long, Info> offsetAndChannelInfo : extractOffsetsSorted(channelStateHandles)) {\n+\t\t\t\tchunkReader.readChunk(is, offsetAndChannelInfo.f0, stateHandler, offsetAndChannelInfo.f1);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate Stream<OperatorSubtaskState> streamSubtaskStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().map(Map.Entry::getValue);\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> Map<StreamStateHandle, List<Handle>> groupByDelegate(\n+\t\t\tStream<OperatorSubtaskState> states,\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor) {\n+\t\treturn states\n+\t\t\t.map(stateHandleExtractor).flatMap(Collection::stream)\n+\t\t\t.peek(validate())\n+\t\t\t.collect(groupingBy(AbstractChannelStateHandle::getDelegate));\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> Consumer<Handle> validate() {\n+\t\tSet<Info> seen = new HashSet<>();\n+\t\t// expect each channel to be described only once; otherwise, buffers in channel could be re-ordered\n+\t\treturn handle -> Preconditions.checkState(seen.add(handle.getInfo()), \"duplicate channel info: %s\");\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> List<Tuple2<Long, Info>> extractOffsetsSorted(List<Handle> channelStateHandles) {\n+\t\treturn channelStateHandles\n+\t\t\t.stream()\n+\t\t\t.flatMap(SequentialChannelStateReaderImpl::extractOffsets)\n+\t\t\t.sorted(comparingLong(offsetAndInfo -> offsetAndInfo.f0))\n+\t\t\t.collect(toList());\n+\t}\n+\n+\tprivate static  <Info, Handle extends AbstractChannelStateHandle<Info>> Stream<Tuple2<Long, Info>> extractOffsets(Handle handle) {\n+\t\treturn handle.getOffsets().stream().map(offset -> Tuple2.of(offset, handle.getInfo()));\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws Exception {\n+\t}\n+\n+}\n+\n+interface RecoveredChannelStateHandler<Info, Context> extends AutoCloseable {\n+\tTuple2<ChannelStateByteBuffer, Context> getBuffer(Info info) throws IOException;\n+\n+\tvoid recover(Info info, Context context) throws IOException;\n+}\n+\n+class InputChannelRecoveredStateHandler implements RecoveredChannelStateHandler<InputChannelInfo, Buffer> {\n+\tprivate final InputGate[] inputGates;\n+\n+\tInputChannelRecoveredStateHandler(InputGate[] inputGates) {\n+\t\tthis.inputGates = inputGates;\n+\t}\n+\n+\t@Override\n+\tpublic Tuple2<ChannelStateByteBuffer, Buffer> getBuffer(InputChannelInfo channelInfo) throws IOException {\n+\t\tRecoveredInputChannel channel = getChannel(channelInfo);\n+\t\tBuffer buffer;\n+\t\ttry {\n+\t\t\tbuffer = channel.getBuffer();\n+\t\t\treturn Tuple2.of(wrap(buffer), buffer);\n+\t\t} catch (InterruptedException e) {\n+\t\t\tthrow new IOException(e);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void recover(InputChannelInfo channelInfo, Buffer buffer) {\n+\t\tif (buffer.readableBytes() > 0) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 180}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5Mzc5NTE5OnYy", "diffSide": "RIGHT", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImpl.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxMzozNjo0M1rOHXbDsg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxMzozNjo0M1rOHXbDsg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDMyMjYxMA==", "bodyText": "nit: empty line.", "url": "https://github.com/apache/flink/pull/13351#discussion_r494322610", "createdAt": "2020-09-24T13:36:43Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImpl.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.fs.FSDataInputStream;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.runtime.checkpoint.StateObjectCollection;\n+import org.apache.flink.runtime.checkpoint.TaskStateSnapshot;\n+import org.apache.flink.runtime.io.network.api.writer.ResultPartitionWriter;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.BufferBuilder;\n+import org.apache.flink.runtime.io.network.buffer.BufferConsumer;\n+import org.apache.flink.runtime.io.network.partition.consumer.InputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel;\n+import org.apache.flink.runtime.state.AbstractChannelStateHandle;\n+import org.apache.flink.runtime.state.StreamStateHandle;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+import java.util.stream.Stream;\n+\n+import static java.util.Comparator.comparingLong;\n+import static java.util.stream.Collectors.groupingBy;\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.flink.runtime.checkpoint.channel.ChannelStateByteBuffer.wrap;\n+\n+/**\n+ * {@link SequentialChannelStateReader} implementation.\n+ */\n+public class SequentialChannelStateReaderImpl implements SequentialChannelStateReader {\n+\n+\tprivate final TaskStateSnapshot taskStateSnapshot;\n+\tprivate final ChannelStateSerializer serializer;\n+\tprivate final ChannelStateChunkReader chunkReader;\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot) {\n+\t\tthis(taskStateSnapshot, new ChannelStateSerializerImpl());\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = new ChannelStateChunkReader(serializer);\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer, ChannelStateChunkReader chunkReader) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = chunkReader;\n+\t}\n+\n+\t@Override\n+\tpublic boolean hasChannelStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().anyMatch(subtaskStateEntry ->\n+\t\t\tsubtaskStateEntry.getValue().getInputChannelState().stream().anyMatch(h -> !h.getOffsets().isEmpty()) ||\n+\t\t\t\tsubtaskStateEntry.getValue().getResultSubpartitionState().stream().anyMatch(h -> !h.getOffsets().isEmpty()));\n+\t}\n+\n+\t@Override\n+\tpublic void readInputData(InputGate[] inputGates) throws IOException {\n+\t\ttry (InputChannelRecoveredStateHandler stateHandler = new InputChannelRecoveredStateHandler(inputGates)) {\n+\t\t\tread(OperatorSubtaskState::getInputChannelState, stateHandler);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void readOutputData(ResultPartitionWriter[] writers) throws IOException {\n+\t\ttry (ResultSubpartitionRecoveredStateHandler stateHandler = new ResultSubpartitionRecoveredStateHandler(writers)) {\n+\t\t\tread(OperatorSubtaskState::getResultSubpartitionState, stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void read(\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\tfor (Map.Entry<StreamStateHandle, List<Handle>> delegateAndHandles : groupByDelegate(streamSubtaskStates(), stateHandleExtractor).entrySet()) {\n+\t\t\treadSequentially(delegateAndHandles.getKey(), delegateAndHandles.getValue(), stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void readSequentially(\n+\t\t\tStreamStateHandle streamStateHandle,\n+\t\t\tList<Handle> channelStateHandles,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\ttry (FSDataInputStream is = streamStateHandle.openInputStream()) {\n+\t\t\tserializer.readHeader(is);\n+\t\t\tfor (Tuple2<Long, Info> offsetAndChannelInfo : extractOffsetsSorted(channelStateHandles)) {\n+\t\t\t\tchunkReader.readChunk(is, offsetAndChannelInfo.f0, stateHandler, offsetAndChannelInfo.f1);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate Stream<OperatorSubtaskState> streamSubtaskStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().map(Map.Entry::getValue);\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> Map<StreamStateHandle, List<Handle>> groupByDelegate(\n+\t\t\tStream<OperatorSubtaskState> states,\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor) {\n+\t\treturn states\n+\t\t\t.map(stateHandleExtractor).flatMap(Collection::stream)\n+\t\t\t.peek(validate())\n+\t\t\t.collect(groupingBy(AbstractChannelStateHandle::getDelegate));\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> Consumer<Handle> validate() {\n+\t\tSet<Info> seen = new HashSet<>();\n+\t\t// expect each channel to be described only once; otherwise, buffers in channel could be re-ordered\n+\t\treturn handle -> Preconditions.checkState(seen.add(handle.getInfo()), \"duplicate channel info: %s\");\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> List<Tuple2<Long, Info>> extractOffsetsSorted(List<Handle> channelStateHandles) {\n+\t\treturn channelStateHandles\n+\t\t\t.stream()\n+\t\t\t.flatMap(SequentialChannelStateReaderImpl::extractOffsets)\n+\t\t\t.sorted(comparingLong(offsetAndInfo -> offsetAndInfo.f0))\n+\t\t\t.collect(toList());\n+\t}\n+\n+\tprivate static  <Info, Handle extends AbstractChannelStateHandle<Info>> Stream<Tuple2<Long, Info>> extractOffsets(Handle handle) {\n+\t\treturn handle.getOffsets().stream().map(offset -> Tuple2.of(offset, handle.getInfo()));\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws Exception {\n+\t}\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 150}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5MzgwMDUzOnYy", "diffSide": "RIGHT", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImpl.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxMzozNzo1NVrOHXbG9w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQxNjowNTowOVrOHYJxQw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDMyMzQ0Nw==", "bodyText": "I was wondering if 5 classes per file are not too many. I haven't seen that in the code base so far. One option would be to extract RecoveredChannelStateHandler and its implementation to a separate file.", "url": "https://github.com/apache/flink/pull/13351#discussion_r494323447", "createdAt": "2020-09-24T13:37:55Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImpl.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.fs.FSDataInputStream;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.runtime.checkpoint.StateObjectCollection;\n+import org.apache.flink.runtime.checkpoint.TaskStateSnapshot;\n+import org.apache.flink.runtime.io.network.api.writer.ResultPartitionWriter;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.BufferBuilder;\n+import org.apache.flink.runtime.io.network.buffer.BufferConsumer;\n+import org.apache.flink.runtime.io.network.partition.consumer.InputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel;\n+import org.apache.flink.runtime.state.AbstractChannelStateHandle;\n+import org.apache.flink.runtime.state.StreamStateHandle;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+import java.util.stream.Stream;\n+\n+import static java.util.Comparator.comparingLong;\n+import static java.util.stream.Collectors.groupingBy;\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.flink.runtime.checkpoint.channel.ChannelStateByteBuffer.wrap;\n+\n+/**\n+ * {@link SequentialChannelStateReader} implementation.\n+ */\n+public class SequentialChannelStateReaderImpl implements SequentialChannelStateReader {\n+\n+\tprivate final TaskStateSnapshot taskStateSnapshot;\n+\tprivate final ChannelStateSerializer serializer;\n+\tprivate final ChannelStateChunkReader chunkReader;\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot) {\n+\t\tthis(taskStateSnapshot, new ChannelStateSerializerImpl());\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = new ChannelStateChunkReader(serializer);\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer, ChannelStateChunkReader chunkReader) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = chunkReader;\n+\t}\n+\n+\t@Override\n+\tpublic boolean hasChannelStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().anyMatch(subtaskStateEntry ->\n+\t\t\tsubtaskStateEntry.getValue().getInputChannelState().stream().anyMatch(h -> !h.getOffsets().isEmpty()) ||\n+\t\t\t\tsubtaskStateEntry.getValue().getResultSubpartitionState().stream().anyMatch(h -> !h.getOffsets().isEmpty()));\n+\t}\n+\n+\t@Override\n+\tpublic void readInputData(InputGate[] inputGates) throws IOException {\n+\t\ttry (InputChannelRecoveredStateHandler stateHandler = new InputChannelRecoveredStateHandler(inputGates)) {\n+\t\t\tread(OperatorSubtaskState::getInputChannelState, stateHandler);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void readOutputData(ResultPartitionWriter[] writers) throws IOException {\n+\t\ttry (ResultSubpartitionRecoveredStateHandler stateHandler = new ResultSubpartitionRecoveredStateHandler(writers)) {\n+\t\t\tread(OperatorSubtaskState::getResultSubpartitionState, stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void read(\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\tfor (Map.Entry<StreamStateHandle, List<Handle>> delegateAndHandles : groupByDelegate(streamSubtaskStates(), stateHandleExtractor).entrySet()) {\n+\t\t\treadSequentially(delegateAndHandles.getKey(), delegateAndHandles.getValue(), stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void readSequentially(\n+\t\t\tStreamStateHandle streamStateHandle,\n+\t\t\tList<Handle> channelStateHandles,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\ttry (FSDataInputStream is = streamStateHandle.openInputStream()) {\n+\t\t\tserializer.readHeader(is);\n+\t\t\tfor (Tuple2<Long, Info> offsetAndChannelInfo : extractOffsetsSorted(channelStateHandles)) {\n+\t\t\t\tchunkReader.readChunk(is, offsetAndChannelInfo.f0, stateHandler, offsetAndChannelInfo.f1);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate Stream<OperatorSubtaskState> streamSubtaskStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().map(Map.Entry::getValue);\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> Map<StreamStateHandle, List<Handle>> groupByDelegate(\n+\t\t\tStream<OperatorSubtaskState> states,\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor) {\n+\t\treturn states\n+\t\t\t.map(stateHandleExtractor).flatMap(Collection::stream)\n+\t\t\t.peek(validate())\n+\t\t\t.collect(groupingBy(AbstractChannelStateHandle::getDelegate));\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> Consumer<Handle> validate() {\n+\t\tSet<Info> seen = new HashSet<>();\n+\t\t// expect each channel to be described only once; otherwise, buffers in channel could be re-ordered\n+\t\treturn handle -> Preconditions.checkState(seen.add(handle.getInfo()), \"duplicate channel info: %s\");\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> List<Tuple2<Long, Info>> extractOffsetsSorted(List<Handle> channelStateHandles) {\n+\t\treturn channelStateHandles\n+\t\t\t.stream()\n+\t\t\t.flatMap(SequentialChannelStateReaderImpl::extractOffsets)\n+\t\t\t.sorted(comparingLong(offsetAndInfo -> offsetAndInfo.f0))\n+\t\t\t.collect(toList());\n+\t}\n+\n+\tprivate static  <Info, Handle extends AbstractChannelStateHandle<Info>> Stream<Tuple2<Long, Info>> extractOffsets(Handle handle) {\n+\t\treturn handle.getOffsets().stream().map(offset -> Tuple2.of(offset, handle.getInfo()));\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws Exception {\n+\t}\n+\n+}\n+\n+interface RecoveredChannelStateHandler<Info, Context> extends AutoCloseable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 153}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTA4NzkzOQ==", "bodyText": "Good point, I'll move them to a separate file.", "url": "https://github.com/apache/flink/pull/13351#discussion_r495087939", "createdAt": "2020-09-25T16:05:09Z", "author": {"login": "rkhachatryan"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImpl.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.fs.FSDataInputStream;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.runtime.checkpoint.StateObjectCollection;\n+import org.apache.flink.runtime.checkpoint.TaskStateSnapshot;\n+import org.apache.flink.runtime.io.network.api.writer.ResultPartitionWriter;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.BufferBuilder;\n+import org.apache.flink.runtime.io.network.buffer.BufferConsumer;\n+import org.apache.flink.runtime.io.network.partition.consumer.InputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel;\n+import org.apache.flink.runtime.state.AbstractChannelStateHandle;\n+import org.apache.flink.runtime.state.StreamStateHandle;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+import java.util.stream.Stream;\n+\n+import static java.util.Comparator.comparingLong;\n+import static java.util.stream.Collectors.groupingBy;\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.flink.runtime.checkpoint.channel.ChannelStateByteBuffer.wrap;\n+\n+/**\n+ * {@link SequentialChannelStateReader} implementation.\n+ */\n+public class SequentialChannelStateReaderImpl implements SequentialChannelStateReader {\n+\n+\tprivate final TaskStateSnapshot taskStateSnapshot;\n+\tprivate final ChannelStateSerializer serializer;\n+\tprivate final ChannelStateChunkReader chunkReader;\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot) {\n+\t\tthis(taskStateSnapshot, new ChannelStateSerializerImpl());\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = new ChannelStateChunkReader(serializer);\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer, ChannelStateChunkReader chunkReader) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = chunkReader;\n+\t}\n+\n+\t@Override\n+\tpublic boolean hasChannelStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().anyMatch(subtaskStateEntry ->\n+\t\t\tsubtaskStateEntry.getValue().getInputChannelState().stream().anyMatch(h -> !h.getOffsets().isEmpty()) ||\n+\t\t\t\tsubtaskStateEntry.getValue().getResultSubpartitionState().stream().anyMatch(h -> !h.getOffsets().isEmpty()));\n+\t}\n+\n+\t@Override\n+\tpublic void readInputData(InputGate[] inputGates) throws IOException {\n+\t\ttry (InputChannelRecoveredStateHandler stateHandler = new InputChannelRecoveredStateHandler(inputGates)) {\n+\t\t\tread(OperatorSubtaskState::getInputChannelState, stateHandler);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void readOutputData(ResultPartitionWriter[] writers) throws IOException {\n+\t\ttry (ResultSubpartitionRecoveredStateHandler stateHandler = new ResultSubpartitionRecoveredStateHandler(writers)) {\n+\t\t\tread(OperatorSubtaskState::getResultSubpartitionState, stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void read(\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\tfor (Map.Entry<StreamStateHandle, List<Handle>> delegateAndHandles : groupByDelegate(streamSubtaskStates(), stateHandleExtractor).entrySet()) {\n+\t\t\treadSequentially(delegateAndHandles.getKey(), delegateAndHandles.getValue(), stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void readSequentially(\n+\t\t\tStreamStateHandle streamStateHandle,\n+\t\t\tList<Handle> channelStateHandles,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\ttry (FSDataInputStream is = streamStateHandle.openInputStream()) {\n+\t\t\tserializer.readHeader(is);\n+\t\t\tfor (Tuple2<Long, Info> offsetAndChannelInfo : extractOffsetsSorted(channelStateHandles)) {\n+\t\t\t\tchunkReader.readChunk(is, offsetAndChannelInfo.f0, stateHandler, offsetAndChannelInfo.f1);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate Stream<OperatorSubtaskState> streamSubtaskStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().map(Map.Entry::getValue);\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> Map<StreamStateHandle, List<Handle>> groupByDelegate(\n+\t\t\tStream<OperatorSubtaskState> states,\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor) {\n+\t\treturn states\n+\t\t\t.map(stateHandleExtractor).flatMap(Collection::stream)\n+\t\t\t.peek(validate())\n+\t\t\t.collect(groupingBy(AbstractChannelStateHandle::getDelegate));\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> Consumer<Handle> validate() {\n+\t\tSet<Info> seen = new HashSet<>();\n+\t\t// expect each channel to be described only once; otherwise, buffers in channel could be re-ordered\n+\t\treturn handle -> Preconditions.checkState(seen.add(handle.getInfo()), \"duplicate channel info: %s\");\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> List<Tuple2<Long, Info>> extractOffsetsSorted(List<Handle> channelStateHandles) {\n+\t\treturn channelStateHandles\n+\t\t\t.stream()\n+\t\t\t.flatMap(SequentialChannelStateReaderImpl::extractOffsets)\n+\t\t\t.sorted(comparingLong(offsetAndInfo -> offsetAndInfo.f0))\n+\t\t\t.collect(toList());\n+\t}\n+\n+\tprivate static  <Info, Handle extends AbstractChannelStateHandle<Info>> Stream<Tuple2<Long, Info>> extractOffsets(Handle handle) {\n+\t\treturn handle.getOffsets().stream().map(offset -> Tuple2.of(offset, handle.getInfo()));\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws Exception {\n+\t}\n+\n+}\n+\n+interface RecoveredChannelStateHandler<Info, Context> extends AutoCloseable {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDMyMzQ0Nw=="}, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 153}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5MzgwNjcyOnYy", "diffSide": "RIGHT", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImpl.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxMzozOToxNlrOHXbKxw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxMzozOToxNlrOHXbKxw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDMyNDQyMw==", "bodyText": "package-default to make clear that it is only used for testing?", "url": "https://github.com/apache/flink/pull/13351#discussion_r494324423", "createdAt": "2020-09-24T13:39:16Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImpl.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.fs.FSDataInputStream;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.runtime.checkpoint.StateObjectCollection;\n+import org.apache.flink.runtime.checkpoint.TaskStateSnapshot;\n+import org.apache.flink.runtime.io.network.api.writer.ResultPartitionWriter;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.BufferBuilder;\n+import org.apache.flink.runtime.io.network.buffer.BufferConsumer;\n+import org.apache.flink.runtime.io.network.partition.consumer.InputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel;\n+import org.apache.flink.runtime.state.AbstractChannelStateHandle;\n+import org.apache.flink.runtime.state.StreamStateHandle;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+import java.util.stream.Stream;\n+\n+import static java.util.Comparator.comparingLong;\n+import static java.util.stream.Collectors.groupingBy;\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.flink.runtime.checkpoint.channel.ChannelStateByteBuffer.wrap;\n+\n+/**\n+ * {@link SequentialChannelStateReader} implementation.\n+ */\n+public class SequentialChannelStateReaderImpl implements SequentialChannelStateReader {\n+\n+\tprivate final TaskStateSnapshot taskStateSnapshot;\n+\tprivate final ChannelStateSerializer serializer;\n+\tprivate final ChannelStateChunkReader chunkReader;\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot) {\n+\t\tthis(taskStateSnapshot, new ChannelStateSerializerImpl());\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 63}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5MzgwNzIwOnYy", "diffSide": "RIGHT", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImpl.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxMzozOToyMVrOHXbLDw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQxNjowNTo0MVrOHYJyXg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDMyNDQ5NQ==", "bodyText": "package-default to make clear that it is only used for testing?", "url": "https://github.com/apache/flink/pull/13351#discussion_r494324495", "createdAt": "2020-09-24T13:39:21Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImpl.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.fs.FSDataInputStream;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.runtime.checkpoint.StateObjectCollection;\n+import org.apache.flink.runtime.checkpoint.TaskStateSnapshot;\n+import org.apache.flink.runtime.io.network.api.writer.ResultPartitionWriter;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.BufferBuilder;\n+import org.apache.flink.runtime.io.network.buffer.BufferConsumer;\n+import org.apache.flink.runtime.io.network.partition.consumer.InputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel;\n+import org.apache.flink.runtime.state.AbstractChannelStateHandle;\n+import org.apache.flink.runtime.state.StreamStateHandle;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+import java.util.stream.Stream;\n+\n+import static java.util.Comparator.comparingLong;\n+import static java.util.stream.Collectors.groupingBy;\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.flink.runtime.checkpoint.channel.ChannelStateByteBuffer.wrap;\n+\n+/**\n+ * {@link SequentialChannelStateReader} implementation.\n+ */\n+public class SequentialChannelStateReaderImpl implements SequentialChannelStateReader {\n+\n+\tprivate final TaskStateSnapshot taskStateSnapshot;\n+\tprivate final ChannelStateSerializer serializer;\n+\tprivate final ChannelStateChunkReader chunkReader;\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot) {\n+\t\tthis(taskStateSnapshot, new ChannelStateSerializerImpl());\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = new ChannelStateChunkReader(serializer);\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer, ChannelStateChunkReader chunkReader) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTA4ODIyMg==", "bodyText": "I found this constructor can even be inlined.", "url": "https://github.com/apache/flink/pull/13351#discussion_r495088222", "createdAt": "2020-09-25T16:05:41Z", "author": {"login": "rkhachatryan"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImpl.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.fs.FSDataInputStream;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.runtime.checkpoint.StateObjectCollection;\n+import org.apache.flink.runtime.checkpoint.TaskStateSnapshot;\n+import org.apache.flink.runtime.io.network.api.writer.ResultPartitionWriter;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.BufferBuilder;\n+import org.apache.flink.runtime.io.network.buffer.BufferConsumer;\n+import org.apache.flink.runtime.io.network.partition.consumer.InputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel;\n+import org.apache.flink.runtime.state.AbstractChannelStateHandle;\n+import org.apache.flink.runtime.state.StreamStateHandle;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+import java.util.stream.Stream;\n+\n+import static java.util.Comparator.comparingLong;\n+import static java.util.stream.Collectors.groupingBy;\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.flink.runtime.checkpoint.channel.ChannelStateByteBuffer.wrap;\n+\n+/**\n+ * {@link SequentialChannelStateReader} implementation.\n+ */\n+public class SequentialChannelStateReaderImpl implements SequentialChannelStateReader {\n+\n+\tprivate final TaskStateSnapshot taskStateSnapshot;\n+\tprivate final ChannelStateSerializer serializer;\n+\tprivate final ChannelStateChunkReader chunkReader;\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot) {\n+\t\tthis(taskStateSnapshot, new ChannelStateSerializerImpl());\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = new ChannelStateChunkReader(serializer);\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer, ChannelStateChunkReader chunkReader) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDMyNDQ5NQ=="}, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 69}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5MzgzMTY3OnYy", "diffSide": "RIGHT", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImpl.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxMzo0Mzo1N1rOHXbaPg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQxNjoxOTowNlrOHYKPGQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDMyODM4Mg==", "bodyText": "When would the offsets be unsorted? Is this more of a precaution or is it common?\nIs it again the small file optimization?", "url": "https://github.com/apache/flink/pull/13351#discussion_r494328382", "createdAt": "2020-09-24T13:43:57Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImpl.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.fs.FSDataInputStream;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.runtime.checkpoint.StateObjectCollection;\n+import org.apache.flink.runtime.checkpoint.TaskStateSnapshot;\n+import org.apache.flink.runtime.io.network.api.writer.ResultPartitionWriter;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.BufferBuilder;\n+import org.apache.flink.runtime.io.network.buffer.BufferConsumer;\n+import org.apache.flink.runtime.io.network.partition.consumer.InputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel;\n+import org.apache.flink.runtime.state.AbstractChannelStateHandle;\n+import org.apache.flink.runtime.state.StreamStateHandle;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+import java.util.stream.Stream;\n+\n+import static java.util.Comparator.comparingLong;\n+import static java.util.stream.Collectors.groupingBy;\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.flink.runtime.checkpoint.channel.ChannelStateByteBuffer.wrap;\n+\n+/**\n+ * {@link SequentialChannelStateReader} implementation.\n+ */\n+public class SequentialChannelStateReaderImpl implements SequentialChannelStateReader {\n+\n+\tprivate final TaskStateSnapshot taskStateSnapshot;\n+\tprivate final ChannelStateSerializer serializer;\n+\tprivate final ChannelStateChunkReader chunkReader;\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot) {\n+\t\tthis(taskStateSnapshot, new ChannelStateSerializerImpl());\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = new ChannelStateChunkReader(serializer);\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer, ChannelStateChunkReader chunkReader) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = chunkReader;\n+\t}\n+\n+\t@Override\n+\tpublic boolean hasChannelStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().anyMatch(subtaskStateEntry ->\n+\t\t\tsubtaskStateEntry.getValue().getInputChannelState().stream().anyMatch(h -> !h.getOffsets().isEmpty()) ||\n+\t\t\t\tsubtaskStateEntry.getValue().getResultSubpartitionState().stream().anyMatch(h -> !h.getOffsets().isEmpty()));\n+\t}\n+\n+\t@Override\n+\tpublic void readInputData(InputGate[] inputGates) throws IOException {\n+\t\ttry (InputChannelRecoveredStateHandler stateHandler = new InputChannelRecoveredStateHandler(inputGates)) {\n+\t\t\tread(OperatorSubtaskState::getInputChannelState, stateHandler);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void readOutputData(ResultPartitionWriter[] writers) throws IOException {\n+\t\ttry (ResultSubpartitionRecoveredStateHandler stateHandler = new ResultSubpartitionRecoveredStateHandler(writers)) {\n+\t\t\tread(OperatorSubtaskState::getResultSubpartitionState, stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void read(\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\tfor (Map.Entry<StreamStateHandle, List<Handle>> delegateAndHandles : groupByDelegate(streamSubtaskStates(), stateHandleExtractor).entrySet()) {\n+\t\t\treadSequentially(delegateAndHandles.getKey(), delegateAndHandles.getValue(), stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void readSequentially(\n+\t\t\tStreamStateHandle streamStateHandle,\n+\t\t\tList<Handle> channelStateHandles,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\ttry (FSDataInputStream is = streamStateHandle.openInputStream()) {\n+\t\t\tserializer.readHeader(is);\n+\t\t\tfor (Tuple2<Long, Info> offsetAndChannelInfo : extractOffsetsSorted(channelStateHandles)) {\n+\t\t\t\tchunkReader.readChunk(is, offsetAndChannelInfo.f0, stateHandler, offsetAndChannelInfo.f1);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate Stream<OperatorSubtaskState> streamSubtaskStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().map(Map.Entry::getValue);\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> Map<StreamStateHandle, List<Handle>> groupByDelegate(\n+\t\t\tStream<OperatorSubtaskState> states,\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor) {\n+\t\treturn states\n+\t\t\t.map(stateHandleExtractor).flatMap(Collection::stream)\n+\t\t\t.peek(validate())\n+\t\t\t.collect(groupingBy(AbstractChannelStateHandle::getDelegate));\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> Consumer<Handle> validate() {\n+\t\tSet<Info> seen = new HashSet<>();\n+\t\t// expect each channel to be described only once; otherwise, buffers in channel could be re-ordered\n+\t\treturn handle -> Preconditions.checkState(seen.add(handle.getInfo()), \"duplicate channel info: %s\");\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> List<Tuple2<Long, Info>> extractOffsetsSorted(List<Handle> channelStateHandles) {\n+\t\treturn channelStateHandles", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 136}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTA5NTU3Nw==", "bodyText": "Offsets are sorted only across single ChannelStateHandle.\nThey are mostly sorted across all ChannelStateHandles referring to the same file (if handles are sorted in the same way as during writing).\nSo sorting is necessary here.", "url": "https://github.com/apache/flink/pull/13351#discussion_r495095577", "createdAt": "2020-09-25T16:19:06Z", "author": {"login": "rkhachatryan"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImpl.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.fs.FSDataInputStream;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.runtime.checkpoint.StateObjectCollection;\n+import org.apache.flink.runtime.checkpoint.TaskStateSnapshot;\n+import org.apache.flink.runtime.io.network.api.writer.ResultPartitionWriter;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.BufferBuilder;\n+import org.apache.flink.runtime.io.network.buffer.BufferConsumer;\n+import org.apache.flink.runtime.io.network.partition.consumer.InputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel;\n+import org.apache.flink.runtime.state.AbstractChannelStateHandle;\n+import org.apache.flink.runtime.state.StreamStateHandle;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+import java.util.stream.Stream;\n+\n+import static java.util.Comparator.comparingLong;\n+import static java.util.stream.Collectors.groupingBy;\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.flink.runtime.checkpoint.channel.ChannelStateByteBuffer.wrap;\n+\n+/**\n+ * {@link SequentialChannelStateReader} implementation.\n+ */\n+public class SequentialChannelStateReaderImpl implements SequentialChannelStateReader {\n+\n+\tprivate final TaskStateSnapshot taskStateSnapshot;\n+\tprivate final ChannelStateSerializer serializer;\n+\tprivate final ChannelStateChunkReader chunkReader;\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot) {\n+\t\tthis(taskStateSnapshot, new ChannelStateSerializerImpl());\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = new ChannelStateChunkReader(serializer);\n+\t}\n+\n+\tpublic SequentialChannelStateReaderImpl(TaskStateSnapshot taskStateSnapshot, ChannelStateSerializer serializer, ChannelStateChunkReader chunkReader) {\n+\t\tthis.taskStateSnapshot = taskStateSnapshot;\n+\t\tthis.serializer = serializer;\n+\t\tthis.chunkReader = chunkReader;\n+\t}\n+\n+\t@Override\n+\tpublic boolean hasChannelStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().anyMatch(subtaskStateEntry ->\n+\t\t\tsubtaskStateEntry.getValue().getInputChannelState().stream().anyMatch(h -> !h.getOffsets().isEmpty()) ||\n+\t\t\t\tsubtaskStateEntry.getValue().getResultSubpartitionState().stream().anyMatch(h -> !h.getOffsets().isEmpty()));\n+\t}\n+\n+\t@Override\n+\tpublic void readInputData(InputGate[] inputGates) throws IOException {\n+\t\ttry (InputChannelRecoveredStateHandler stateHandler = new InputChannelRecoveredStateHandler(inputGates)) {\n+\t\t\tread(OperatorSubtaskState::getInputChannelState, stateHandler);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void readOutputData(ResultPartitionWriter[] writers) throws IOException {\n+\t\ttry (ResultSubpartitionRecoveredStateHandler stateHandler = new ResultSubpartitionRecoveredStateHandler(writers)) {\n+\t\t\tread(OperatorSubtaskState::getResultSubpartitionState, stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void read(\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\tfor (Map.Entry<StreamStateHandle, List<Handle>> delegateAndHandles : groupByDelegate(streamSubtaskStates(), stateHandleExtractor).entrySet()) {\n+\t\t\treadSequentially(delegateAndHandles.getKey(), delegateAndHandles.getValue(), stateHandler);\n+\t\t}\n+\t}\n+\n+\tprivate <Info, Context, Handle extends AbstractChannelStateHandle<Info>> void readSequentially(\n+\t\t\tStreamStateHandle streamStateHandle,\n+\t\t\tList<Handle> channelStateHandles,\n+\t\t\tRecoveredChannelStateHandler<Info, Context> stateHandler) throws IOException {\n+\t\ttry (FSDataInputStream is = streamStateHandle.openInputStream()) {\n+\t\t\tserializer.readHeader(is);\n+\t\t\tfor (Tuple2<Long, Info> offsetAndChannelInfo : extractOffsetsSorted(channelStateHandles)) {\n+\t\t\t\tchunkReader.readChunk(is, offsetAndChannelInfo.f0, stateHandler, offsetAndChannelInfo.f1);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate Stream<OperatorSubtaskState> streamSubtaskStates() {\n+\t\treturn taskStateSnapshot.getSubtaskStateMappings().stream().map(Map.Entry::getValue);\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> Map<StreamStateHandle, List<Handle>> groupByDelegate(\n+\t\t\tStream<OperatorSubtaskState> states,\n+\t\t\tFunction<OperatorSubtaskState, StateObjectCollection<Handle>> stateHandleExtractor) {\n+\t\treturn states\n+\t\t\t.map(stateHandleExtractor).flatMap(Collection::stream)\n+\t\t\t.peek(validate())\n+\t\t\t.collect(groupingBy(AbstractChannelStateHandle::getDelegate));\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> Consumer<Handle> validate() {\n+\t\tSet<Info> seen = new HashSet<>();\n+\t\t// expect each channel to be described only once; otherwise, buffers in channel could be re-ordered\n+\t\treturn handle -> Preconditions.checkState(seen.add(handle.getInfo()), \"duplicate channel info: %s\");\n+\t}\n+\n+\tprivate static <Info, Handle extends AbstractChannelStateHandle<Info>> List<Tuple2<Long, Info>> extractOffsetsSorted(List<Handle> channelStateHandles) {\n+\t\treturn channelStateHandles", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDMyODM4Mg=="}, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 136}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5Mzg0MzIzOnYy", "diffSide": "RIGHT", "path": "flink-runtime/src/test/java/org/apache/flink/runtime/checkpoint/channel/ChannelStateChunkReaderTest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxMzo0NToyOFrOHXbhlg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxMzo0NToyOFrOHXbhlg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDMzMDI2Mg==", "bodyText": "getStream?", "url": "https://github.com/apache/flink/pull/13351#discussion_r494330262", "createdAt": "2020-09-24T13:45:28Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/test/java/org/apache/flink/runtime/checkpoint/channel/ChannelStateChunkReaderTest.java", "diffHunk": "@@ -0,0 +1,162 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.fs.FSDataInputStream;\n+import org.apache.flink.core.memory.MemorySegmentFactory;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.FreeingBufferRecycler;\n+import org.apache.flink.runtime.io.network.buffer.NetworkBuffer;\n+import org.apache.flink.runtime.state.memory.ByteStreamStateHandle;\n+\n+import org.junit.Test;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+import static org.apache.flink.util.Preconditions.checkArgument;\n+import static org.apache.flink.util.Preconditions.checkState;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n+\n+/**\n+ * {@link ChannelStateChunkReader} test.\n+ */\n+public class ChannelStateChunkReaderTest {\n+\n+\t@Test(expected = TestException.class)\n+\tpublic void testBufferRecycledOnFailure() throws IOException {\n+\t\tFailingChannelStateSerializer serializer = new FailingChannelStateSerializer();\n+\t\tTestRecoveredChannelStateHandler handler = new TestRecoveredChannelStateHandler();\n+\n+\t\ttry (FSDataInputStream stream = geStream(serializer, 10)) {\n+\t\t\tnew ChannelStateChunkReader(serializer).readChunk(stream, serializer.getHeaderLength(), handler, \"channelInfo\");\n+\t\t\tcheckState(serializer.failed);\n+\t\t\tcheckState(!handler.requestedBuffers.isEmpty());\n+\t\t} finally {\n+\t\t\tassertTrue(handler.requestedBuffers.stream().allMatch(TestChannelStateByteBuffer::isRecycled));\n+\t\t}\n+\t}\n+\n+\t@Test\n+\tpublic void testBuffersNotRequestedForEmptyStream() throws IOException {\n+\t\tChannelStateSerializer serializer = new ChannelStateSerializerImpl();\n+\t\tTestRecoveredChannelStateHandler handler = new TestRecoveredChannelStateHandler();\n+\n+\t\ttry (FSDataInputStream stream = geStream(serializer, 0)) {\n+\t\t\tnew ChannelStateChunkReader(serializer).readChunk(stream, serializer.getHeaderLength(), handler, \"channelInfo\");\n+\t\t} finally {\n+\t\t\tassertTrue(handler.requestedBuffers.isEmpty());\n+\t\t}\n+\t}\n+\n+\t@Test\n+\tpublic void testNoSeekUnnecessarily() throws IOException {\n+\t\tfinal int offset = 123;\n+\t\tfinal FSDataInputStream stream = new FSDataInputStream() {\n+\t\t\t@Override\n+\t\t\tpublic long getPos() {\n+\t\t\t\treturn offset;\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic void seek(long ignored) {\n+\t\t\t\tfail();\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic int read() {\n+\t\t\t\treturn 0;\n+\t\t\t}\n+\t\t};\n+\n+\t\tnew ChannelStateChunkReader(new ChannelStateSerializerImpl())\n+\t\t\t.readChunk(stream, offset, new TestRecoveredChannelStateHandler(), \"channelInfo\");\n+\t}\n+\n+\tprivate static class TestRecoveredChannelStateHandler implements RecoveredChannelStateHandler<Object, Object> {\n+\t\tprivate final List<TestChannelStateByteBuffer> requestedBuffers = new ArrayList<>();\n+\n+\t\t@Override\n+\t\tpublic Tuple2<ChannelStateByteBuffer, Object> getBuffer(Object o) {\n+\t\t\tTestChannelStateByteBuffer buffer = new TestChannelStateByteBuffer();\n+\t\t\trequestedBuffers.add(buffer);\n+\t\t\treturn Tuple2.of(buffer, null);\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void recover(Object o, Object o2) {\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void close() throws Exception {\n+\t\t}\n+\n+\t}\n+\n+\tprivate static class FailingChannelStateSerializer extends ChannelStateSerializerImpl {\n+\t\tprivate boolean failed;\n+\n+\t\t@Override\n+\t\tpublic int readData(InputStream stream, ChannelStateByteBuffer buffer, int bytes) {\n+\t\t\tfailed = true;\n+\t\t\tthrow new TestException();\n+\t\t}\n+\t}\n+\n+\tprivate static FSDataInputStream geStream(ChannelStateSerializer serializer, int size) throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 127}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5Mzg2NTg0OnYy", "diffSide": "RIGHT", "path": "flink-runtime/src/test/java/org/apache/flink/runtime/checkpoint/channel/ChannelStateChunkReaderTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxMzo0ODozOFrOHXbwNw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQxNjoyNTowM1rOHYKcQQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDMzNDAwNw==", "bodyText": "I haven't understood the control flow here. Is this line really executed? Did you want to put it into finally?", "url": "https://github.com/apache/flink/pull/13351#discussion_r494334007", "createdAt": "2020-09-24T13:48:38Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/test/java/org/apache/flink/runtime/checkpoint/channel/ChannelStateChunkReaderTest.java", "diffHunk": "@@ -0,0 +1,162 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.fs.FSDataInputStream;\n+import org.apache.flink.core.memory.MemorySegmentFactory;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.FreeingBufferRecycler;\n+import org.apache.flink.runtime.io.network.buffer.NetworkBuffer;\n+import org.apache.flink.runtime.state.memory.ByteStreamStateHandle;\n+\n+import org.junit.Test;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+import static org.apache.flink.util.Preconditions.checkArgument;\n+import static org.apache.flink.util.Preconditions.checkState;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n+\n+/**\n+ * {@link ChannelStateChunkReader} test.\n+ */\n+public class ChannelStateChunkReaderTest {\n+\n+\t@Test(expected = TestException.class)\n+\tpublic void testBufferRecycledOnFailure() throws IOException {\n+\t\tFailingChannelStateSerializer serializer = new FailingChannelStateSerializer();\n+\t\tTestRecoveredChannelStateHandler handler = new TestRecoveredChannelStateHandler();\n+\n+\t\ttry (FSDataInputStream stream = geStream(serializer, 10)) {\n+\t\t\tnew ChannelStateChunkReader(serializer).readChunk(stream, serializer.getHeaderLength(), handler, \"channelInfo\");\n+\t\t\tcheckState(serializer.failed);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTA5ODk0NQ==", "bodyText": "Yes, these checkStates should be in finally. Thanks!", "url": "https://github.com/apache/flink/pull/13351#discussion_r495098945", "createdAt": "2020-09-25T16:25:03Z", "author": {"login": "rkhachatryan"}, "path": "flink-runtime/src/test/java/org/apache/flink/runtime/checkpoint/channel/ChannelStateChunkReaderTest.java", "diffHunk": "@@ -0,0 +1,162 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.fs.FSDataInputStream;\n+import org.apache.flink.core.memory.MemorySegmentFactory;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.FreeingBufferRecycler;\n+import org.apache.flink.runtime.io.network.buffer.NetworkBuffer;\n+import org.apache.flink.runtime.state.memory.ByteStreamStateHandle;\n+\n+import org.junit.Test;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+import static org.apache.flink.util.Preconditions.checkArgument;\n+import static org.apache.flink.util.Preconditions.checkState;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n+\n+/**\n+ * {@link ChannelStateChunkReader} test.\n+ */\n+public class ChannelStateChunkReaderTest {\n+\n+\t@Test(expected = TestException.class)\n+\tpublic void testBufferRecycledOnFailure() throws IOException {\n+\t\tFailingChannelStateSerializer serializer = new FailingChannelStateSerializer();\n+\t\tTestRecoveredChannelStateHandler handler = new TestRecoveredChannelStateHandler();\n+\n+\t\ttry (FSDataInputStream stream = geStream(serializer, 10)) {\n+\t\t\tnew ChannelStateChunkReader(serializer).readChunk(stream, serializer.getHeaderLength(), handler, \"channelInfo\");\n+\t\t\tcheckState(serializer.failed);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDMzNDAwNw=="}, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 54}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5Mzg4MTIxOnYy", "diffSide": "RIGHT", "path": "flink-runtime/src/test/java/org/apache/flink/runtime/checkpoint/channel/ChannelStateChunkReaderTest.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxMzo1MToyMlrOHXb5sw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQxMjowNzo1MlrOHZp6fQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDMzNjQzNQ==", "bodyText": "nit: empty line. (Could you check in all classes?)", "url": "https://github.com/apache/flink/pull/13351#discussion_r494336435", "createdAt": "2020-09-24T13:51:22Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/test/java/org/apache/flink/runtime/checkpoint/channel/ChannelStateChunkReaderTest.java", "diffHunk": "@@ -0,0 +1,162 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.fs.FSDataInputStream;\n+import org.apache.flink.core.memory.MemorySegmentFactory;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.FreeingBufferRecycler;\n+import org.apache.flink.runtime.io.network.buffer.NetworkBuffer;\n+import org.apache.flink.runtime.state.memory.ByteStreamStateHandle;\n+\n+import org.junit.Test;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+import static org.apache.flink.util.Preconditions.checkArgument;\n+import static org.apache.flink.util.Preconditions.checkState;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n+\n+/**\n+ * {@link ChannelStateChunkReader} test.\n+ */\n+public class ChannelStateChunkReaderTest {\n+\n+\t@Test(expected = TestException.class)\n+\tpublic void testBufferRecycledOnFailure() throws IOException {\n+\t\tFailingChannelStateSerializer serializer = new FailingChannelStateSerializer();\n+\t\tTestRecoveredChannelStateHandler handler = new TestRecoveredChannelStateHandler();\n+\n+\t\ttry (FSDataInputStream stream = geStream(serializer, 10)) {\n+\t\t\tnew ChannelStateChunkReader(serializer).readChunk(stream, serializer.getHeaderLength(), handler, \"channelInfo\");\n+\t\t\tcheckState(serializer.failed);\n+\t\t\tcheckState(!handler.requestedBuffers.isEmpty());\n+\t\t} finally {\n+\t\t\tassertTrue(handler.requestedBuffers.stream().allMatch(TestChannelStateByteBuffer::isRecycled));\n+\t\t}\n+\t}\n+\n+\t@Test\n+\tpublic void testBuffersNotRequestedForEmptyStream() throws IOException {\n+\t\tChannelStateSerializer serializer = new ChannelStateSerializerImpl();\n+\t\tTestRecoveredChannelStateHandler handler = new TestRecoveredChannelStateHandler();\n+\n+\t\ttry (FSDataInputStream stream = geStream(serializer, 0)) {\n+\t\t\tnew ChannelStateChunkReader(serializer).readChunk(stream, serializer.getHeaderLength(), handler, \"channelInfo\");\n+\t\t} finally {\n+\t\t\tassertTrue(handler.requestedBuffers.isEmpty());\n+\t\t}\n+\t}\n+\n+\t@Test\n+\tpublic void testNoSeekUnnecessarily() throws IOException {\n+\t\tfinal int offset = 123;\n+\t\tfinal FSDataInputStream stream = new FSDataInputStream() {\n+\t\t\t@Override\n+\t\t\tpublic long getPos() {\n+\t\t\t\treturn offset;\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic void seek(long ignored) {\n+\t\t\t\tfail();\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic int read() {\n+\t\t\t\treturn 0;\n+\t\t\t}\n+\t\t};\n+\n+\t\tnew ChannelStateChunkReader(new ChannelStateSerializerImpl())\n+\t\t\t.readChunk(stream, offset, new TestRecoveredChannelStateHandler(), \"channelInfo\");\n+\t}\n+\n+\tprivate static class TestRecoveredChannelStateHandler implements RecoveredChannelStateHandler<Object, Object> {\n+\t\tprivate final List<TestChannelStateByteBuffer> requestedBuffers = new ArrayList<>();\n+\n+\t\t@Override\n+\t\tpublic Tuple2<ChannelStateByteBuffer, Object> getBuffer(Object o) {\n+\t\t\tTestChannelStateByteBuffer buffer = new TestChannelStateByteBuffer();\n+\t\t\trequestedBuffers.add(buffer);\n+\t\t\treturn Tuple2.of(buffer, null);\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void recover(Object o, Object o2) {\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void close() throws Exception {\n+\t\t}\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 114}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTEwMTEyMg==", "bodyText": "Do you mean double newline between two }?\nWhy is it a problem?\nIt's very convenient for me to select it using vim select block :)", "url": "https://github.com/apache/flink/pull/13351#discussion_r495101122", "createdAt": "2020-09-25T16:28:56Z", "author": {"login": "rkhachatryan"}, "path": "flink-runtime/src/test/java/org/apache/flink/runtime/checkpoint/channel/ChannelStateChunkReaderTest.java", "diffHunk": "@@ -0,0 +1,162 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.fs.FSDataInputStream;\n+import org.apache.flink.core.memory.MemorySegmentFactory;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.FreeingBufferRecycler;\n+import org.apache.flink.runtime.io.network.buffer.NetworkBuffer;\n+import org.apache.flink.runtime.state.memory.ByteStreamStateHandle;\n+\n+import org.junit.Test;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+import static org.apache.flink.util.Preconditions.checkArgument;\n+import static org.apache.flink.util.Preconditions.checkState;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n+\n+/**\n+ * {@link ChannelStateChunkReader} test.\n+ */\n+public class ChannelStateChunkReaderTest {\n+\n+\t@Test(expected = TestException.class)\n+\tpublic void testBufferRecycledOnFailure() throws IOException {\n+\t\tFailingChannelStateSerializer serializer = new FailingChannelStateSerializer();\n+\t\tTestRecoveredChannelStateHandler handler = new TestRecoveredChannelStateHandler();\n+\n+\t\ttry (FSDataInputStream stream = geStream(serializer, 10)) {\n+\t\t\tnew ChannelStateChunkReader(serializer).readChunk(stream, serializer.getHeaderLength(), handler, \"channelInfo\");\n+\t\t\tcheckState(serializer.failed);\n+\t\t\tcheckState(!handler.requestedBuffers.isEmpty());\n+\t\t} finally {\n+\t\t\tassertTrue(handler.requestedBuffers.stream().allMatch(TestChannelStateByteBuffer::isRecycled));\n+\t\t}\n+\t}\n+\n+\t@Test\n+\tpublic void testBuffersNotRequestedForEmptyStream() throws IOException {\n+\t\tChannelStateSerializer serializer = new ChannelStateSerializerImpl();\n+\t\tTestRecoveredChannelStateHandler handler = new TestRecoveredChannelStateHandler();\n+\n+\t\ttry (FSDataInputStream stream = geStream(serializer, 0)) {\n+\t\t\tnew ChannelStateChunkReader(serializer).readChunk(stream, serializer.getHeaderLength(), handler, \"channelInfo\");\n+\t\t} finally {\n+\t\t\tassertTrue(handler.requestedBuffers.isEmpty());\n+\t\t}\n+\t}\n+\n+\t@Test\n+\tpublic void testNoSeekUnnecessarily() throws IOException {\n+\t\tfinal int offset = 123;\n+\t\tfinal FSDataInputStream stream = new FSDataInputStream() {\n+\t\t\t@Override\n+\t\t\tpublic long getPos() {\n+\t\t\t\treturn offset;\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic void seek(long ignored) {\n+\t\t\t\tfail();\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic int read() {\n+\t\t\t\treturn 0;\n+\t\t\t}\n+\t\t};\n+\n+\t\tnew ChannelStateChunkReader(new ChannelStateSerializerImpl())\n+\t\t\t.readChunk(stream, offset, new TestRecoveredChannelStateHandler(), \"channelInfo\");\n+\t}\n+\n+\tprivate static class TestRecoveredChannelStateHandler implements RecoveredChannelStateHandler<Object, Object> {\n+\t\tprivate final List<TestChannelStateByteBuffer> requestedBuffers = new ArrayList<>();\n+\n+\t\t@Override\n+\t\tpublic Tuple2<ChannelStateByteBuffer, Object> getBuffer(Object o) {\n+\t\t\tTestChannelStateByteBuffer buffer = new TestChannelStateByteBuffer();\n+\t\t\trequestedBuffers.add(buffer);\n+\t\t\treturn Tuple2.of(buffer, null);\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void recover(Object o, Object o2) {\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void close() throws Exception {\n+\t\t}\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDMzNjQzNQ=="}, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 114}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjU3NDQ2Mw==", "bodyText": "When looking at formatting, I usually just look for consistency and there are a few classes with this extra new line and many without. But which way is better is probably not worth debating ;).", "url": "https://github.com/apache/flink/pull/13351#discussion_r496574463", "createdAt": "2020-09-29T09:29:59Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/test/java/org/apache/flink/runtime/checkpoint/channel/ChannelStateChunkReaderTest.java", "diffHunk": "@@ -0,0 +1,162 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.fs.FSDataInputStream;\n+import org.apache.flink.core.memory.MemorySegmentFactory;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.FreeingBufferRecycler;\n+import org.apache.flink.runtime.io.network.buffer.NetworkBuffer;\n+import org.apache.flink.runtime.state.memory.ByteStreamStateHandle;\n+\n+import org.junit.Test;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+import static org.apache.flink.util.Preconditions.checkArgument;\n+import static org.apache.flink.util.Preconditions.checkState;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n+\n+/**\n+ * {@link ChannelStateChunkReader} test.\n+ */\n+public class ChannelStateChunkReaderTest {\n+\n+\t@Test(expected = TestException.class)\n+\tpublic void testBufferRecycledOnFailure() throws IOException {\n+\t\tFailingChannelStateSerializer serializer = new FailingChannelStateSerializer();\n+\t\tTestRecoveredChannelStateHandler handler = new TestRecoveredChannelStateHandler();\n+\n+\t\ttry (FSDataInputStream stream = geStream(serializer, 10)) {\n+\t\t\tnew ChannelStateChunkReader(serializer).readChunk(stream, serializer.getHeaderLength(), handler, \"channelInfo\");\n+\t\t\tcheckState(serializer.failed);\n+\t\t\tcheckState(!handler.requestedBuffers.isEmpty());\n+\t\t} finally {\n+\t\t\tassertTrue(handler.requestedBuffers.stream().allMatch(TestChannelStateByteBuffer::isRecycled));\n+\t\t}\n+\t}\n+\n+\t@Test\n+\tpublic void testBuffersNotRequestedForEmptyStream() throws IOException {\n+\t\tChannelStateSerializer serializer = new ChannelStateSerializerImpl();\n+\t\tTestRecoveredChannelStateHandler handler = new TestRecoveredChannelStateHandler();\n+\n+\t\ttry (FSDataInputStream stream = geStream(serializer, 0)) {\n+\t\t\tnew ChannelStateChunkReader(serializer).readChunk(stream, serializer.getHeaderLength(), handler, \"channelInfo\");\n+\t\t} finally {\n+\t\t\tassertTrue(handler.requestedBuffers.isEmpty());\n+\t\t}\n+\t}\n+\n+\t@Test\n+\tpublic void testNoSeekUnnecessarily() throws IOException {\n+\t\tfinal int offset = 123;\n+\t\tfinal FSDataInputStream stream = new FSDataInputStream() {\n+\t\t\t@Override\n+\t\t\tpublic long getPos() {\n+\t\t\t\treturn offset;\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic void seek(long ignored) {\n+\t\t\t\tfail();\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic int read() {\n+\t\t\t\treturn 0;\n+\t\t\t}\n+\t\t};\n+\n+\t\tnew ChannelStateChunkReader(new ChannelStateSerializerImpl())\n+\t\t\t.readChunk(stream, offset, new TestRecoveredChannelStateHandler(), \"channelInfo\");\n+\t}\n+\n+\tprivate static class TestRecoveredChannelStateHandler implements RecoveredChannelStateHandler<Object, Object> {\n+\t\tprivate final List<TestChannelStateByteBuffer> requestedBuffers = new ArrayList<>();\n+\n+\t\t@Override\n+\t\tpublic Tuple2<ChannelStateByteBuffer, Object> getBuffer(Object o) {\n+\t\t\tTestChannelStateByteBuffer buffer = new TestChannelStateByteBuffer();\n+\t\t\trequestedBuffers.add(buffer);\n+\t\t\treturn Tuple2.of(buffer, null);\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void recover(Object o, Object o2) {\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void close() throws Exception {\n+\t\t}\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDMzNjQzNQ=="}, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 114}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjY2MzE2NQ==", "bodyText": "Removed newline.", "url": "https://github.com/apache/flink/pull/13351#discussion_r496663165", "createdAt": "2020-09-29T12:07:52Z", "author": {"login": "rkhachatryan"}, "path": "flink-runtime/src/test/java/org/apache/flink/runtime/checkpoint/channel/ChannelStateChunkReaderTest.java", "diffHunk": "@@ -0,0 +1,162 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.fs.FSDataInputStream;\n+import org.apache.flink.core.memory.MemorySegmentFactory;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.FreeingBufferRecycler;\n+import org.apache.flink.runtime.io.network.buffer.NetworkBuffer;\n+import org.apache.flink.runtime.state.memory.ByteStreamStateHandle;\n+\n+import org.junit.Test;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+import static org.apache.flink.util.Preconditions.checkArgument;\n+import static org.apache.flink.util.Preconditions.checkState;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n+\n+/**\n+ * {@link ChannelStateChunkReader} test.\n+ */\n+public class ChannelStateChunkReaderTest {\n+\n+\t@Test(expected = TestException.class)\n+\tpublic void testBufferRecycledOnFailure() throws IOException {\n+\t\tFailingChannelStateSerializer serializer = new FailingChannelStateSerializer();\n+\t\tTestRecoveredChannelStateHandler handler = new TestRecoveredChannelStateHandler();\n+\n+\t\ttry (FSDataInputStream stream = geStream(serializer, 10)) {\n+\t\t\tnew ChannelStateChunkReader(serializer).readChunk(stream, serializer.getHeaderLength(), handler, \"channelInfo\");\n+\t\t\tcheckState(serializer.failed);\n+\t\t\tcheckState(!handler.requestedBuffers.isEmpty());\n+\t\t} finally {\n+\t\t\tassertTrue(handler.requestedBuffers.stream().allMatch(TestChannelStateByteBuffer::isRecycled));\n+\t\t}\n+\t}\n+\n+\t@Test\n+\tpublic void testBuffersNotRequestedForEmptyStream() throws IOException {\n+\t\tChannelStateSerializer serializer = new ChannelStateSerializerImpl();\n+\t\tTestRecoveredChannelStateHandler handler = new TestRecoveredChannelStateHandler();\n+\n+\t\ttry (FSDataInputStream stream = geStream(serializer, 0)) {\n+\t\t\tnew ChannelStateChunkReader(serializer).readChunk(stream, serializer.getHeaderLength(), handler, \"channelInfo\");\n+\t\t} finally {\n+\t\t\tassertTrue(handler.requestedBuffers.isEmpty());\n+\t\t}\n+\t}\n+\n+\t@Test\n+\tpublic void testNoSeekUnnecessarily() throws IOException {\n+\t\tfinal int offset = 123;\n+\t\tfinal FSDataInputStream stream = new FSDataInputStream() {\n+\t\t\t@Override\n+\t\t\tpublic long getPos() {\n+\t\t\t\treturn offset;\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic void seek(long ignored) {\n+\t\t\t\tfail();\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic int read() {\n+\t\t\t\treturn 0;\n+\t\t\t}\n+\t\t};\n+\n+\t\tnew ChannelStateChunkReader(new ChannelStateSerializerImpl())\n+\t\t\t.readChunk(stream, offset, new TestRecoveredChannelStateHandler(), \"channelInfo\");\n+\t}\n+\n+\tprivate static class TestRecoveredChannelStateHandler implements RecoveredChannelStateHandler<Object, Object> {\n+\t\tprivate final List<TestChannelStateByteBuffer> requestedBuffers = new ArrayList<>();\n+\n+\t\t@Override\n+\t\tpublic Tuple2<ChannelStateByteBuffer, Object> getBuffer(Object o) {\n+\t\t\tTestChannelStateByteBuffer buffer = new TestChannelStateByteBuffer();\n+\t\t\trequestedBuffers.add(buffer);\n+\t\t\treturn Tuple2.of(buffer, null);\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void recover(Object o, Object o2) {\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void close() throws Exception {\n+\t\t}\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDMzNjQzNQ=="}, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 114}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5Mzg5MjY2OnYy", "diffSide": "RIGHT", "path": "flink-runtime/src/test/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImplTest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxMzo1Mzo0MVrOHXcBDA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxMzo1Mzo0MVrOHXcBDA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDMzODMxNg==", "bodyText": "Please check format string.", "url": "https://github.com/apache/flink/pull/13351#discussion_r494338316", "createdAt": "2020-09-24T13:53:41Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/test/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImplTest.java", "diffHunk": "@@ -0,0 +1,321 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.memory.MemorySegmentFactory;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.runtime.checkpoint.StateObjectCollection;\n+import org.apache.flink.runtime.checkpoint.TaskStateSnapshot;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.FreeingBufferRecycler;\n+import org.apache.flink.runtime.io.network.buffer.NetworkBuffer;\n+import org.apache.flink.runtime.io.network.buffer.NetworkBufferPool;\n+import org.apache.flink.runtime.io.network.partition.ResultPartition;\n+import org.apache.flink.runtime.io.network.partition.ResultPartitionBuilder;\n+import org.apache.flink.runtime.io.network.partition.ResultSubpartition.BufferAndBacklog;\n+import org.apache.flink.runtime.io.network.partition.ResultSubpartitionView;\n+import org.apache.flink.runtime.io.network.partition.consumer.BufferOrEvent;\n+import org.apache.flink.runtime.io.network.partition.consumer.InputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateBuilder;\n+import org.apache.flink.runtime.jobgraph.OperatorID;\n+import org.apache.flink.runtime.state.InputChannelStateHandle;\n+import org.apache.flink.runtime.state.ResultSubpartitionStateHandle;\n+import org.apache.flink.runtime.state.memory.ByteStreamStateHandle;\n+import org.apache.flink.util.function.ThrowingConsumer;\n+\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Random;\n+import java.util.function.BiFunction;\n+import java.util.function.Function;\n+\n+import static java.util.function.Function.identity;\n+import static java.util.stream.Collectors.toList;\n+import static java.util.stream.Collectors.toMap;\n+import static java.util.stream.IntStream.range;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * {@link SequentialChannelStateReaderImpl} Test.\n+ */\n+@RunWith(Parameterized.class)\n+public class SequentialChannelStateReaderImplTest {\n+\n+\t@Parameterized.Parameters(name = \"{0}: stateParLevel={1}, statePartsPerChannel={1}, stateBytesPerPart={2},  parLevel={4}, bufferSize={5}\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 73}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5MzkwMDcyOnYy", "diffSide": "RIGHT", "path": "flink-runtime/src/test/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImplTest.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxMzo1NToxNlrOHXcF7g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQxMjowMzozNlrOHZpwtA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDMzOTU2Ng==", "bodyText": "nit: Can we use ThreadLocalRandom here? It's just a test, but it would be nice to use it everywhere it's possible.", "url": "https://github.com/apache/flink/pull/13351#discussion_r494339566", "createdAt": "2020-09-24T13:55:16Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/test/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImplTest.java", "diffHunk": "@@ -0,0 +1,321 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.memory.MemorySegmentFactory;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.runtime.checkpoint.StateObjectCollection;\n+import org.apache.flink.runtime.checkpoint.TaskStateSnapshot;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.FreeingBufferRecycler;\n+import org.apache.flink.runtime.io.network.buffer.NetworkBuffer;\n+import org.apache.flink.runtime.io.network.buffer.NetworkBufferPool;\n+import org.apache.flink.runtime.io.network.partition.ResultPartition;\n+import org.apache.flink.runtime.io.network.partition.ResultPartitionBuilder;\n+import org.apache.flink.runtime.io.network.partition.ResultSubpartition.BufferAndBacklog;\n+import org.apache.flink.runtime.io.network.partition.ResultSubpartitionView;\n+import org.apache.flink.runtime.io.network.partition.consumer.BufferOrEvent;\n+import org.apache.flink.runtime.io.network.partition.consumer.InputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateBuilder;\n+import org.apache.flink.runtime.jobgraph.OperatorID;\n+import org.apache.flink.runtime.state.InputChannelStateHandle;\n+import org.apache.flink.runtime.state.ResultSubpartitionStateHandle;\n+import org.apache.flink.runtime.state.memory.ByteStreamStateHandle;\n+import org.apache.flink.util.function.ThrowingConsumer;\n+\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Random;\n+import java.util.function.BiFunction;\n+import java.util.function.Function;\n+\n+import static java.util.function.Function.identity;\n+import static java.util.stream.Collectors.toList;\n+import static java.util.stream.Collectors.toMap;\n+import static java.util.stream.IntStream.range;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * {@link SequentialChannelStateReaderImpl} Test.\n+ */\n+@RunWith(Parameterized.class)\n+public class SequentialChannelStateReaderImplTest {\n+\n+\t@Parameterized.Parameters(name = \"{0}: stateParLevel={1}, statePartsPerChannel={1}, stateBytesPerPart={2},  parLevel={4}, bufferSize={5}\")\n+\tpublic static Object[][] parameters() {\n+\t\treturn new Object[][]{\n+\t\t\t{\"NoStateAndNoChannels\", 0, 0, 0, 0, 0},\n+\t\t\t{\"NoState\", 0, 10, 10, 10, 10},\n+\t\t\t{\"ReadPermutedStateWithEqualBuffer\", 10, 10, 10, 10, 10},\n+\t\t\t{\"ReadPermutedStateWithReducedBuffer\", 10, 10, 10, 20, 10},\n+\t\t\t{\"ReadPermutedStateWithIncreasedBuffer\", 10, 10, 10, 10, 20},\n+\t\t};\n+\t}\n+\n+\tprivate final ChannelStateSerializer serializer;\n+\tprivate final Random random;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 85}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTEwNzQwNA==", "bodyText": "I think this wouldn't give performance benefit as tests are currently run sequentially.\nAnd even if run in parallel, a new class instance would be created for each test, so threads wouldn't share Random instance.", "url": "https://github.com/apache/flink/pull/13351#discussion_r495107404", "createdAt": "2020-09-25T16:41:05Z", "author": {"login": "rkhachatryan"}, "path": "flink-runtime/src/test/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImplTest.java", "diffHunk": "@@ -0,0 +1,321 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.memory.MemorySegmentFactory;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.runtime.checkpoint.StateObjectCollection;\n+import org.apache.flink.runtime.checkpoint.TaskStateSnapshot;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.FreeingBufferRecycler;\n+import org.apache.flink.runtime.io.network.buffer.NetworkBuffer;\n+import org.apache.flink.runtime.io.network.buffer.NetworkBufferPool;\n+import org.apache.flink.runtime.io.network.partition.ResultPartition;\n+import org.apache.flink.runtime.io.network.partition.ResultPartitionBuilder;\n+import org.apache.flink.runtime.io.network.partition.ResultSubpartition.BufferAndBacklog;\n+import org.apache.flink.runtime.io.network.partition.ResultSubpartitionView;\n+import org.apache.flink.runtime.io.network.partition.consumer.BufferOrEvent;\n+import org.apache.flink.runtime.io.network.partition.consumer.InputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateBuilder;\n+import org.apache.flink.runtime.jobgraph.OperatorID;\n+import org.apache.flink.runtime.state.InputChannelStateHandle;\n+import org.apache.flink.runtime.state.ResultSubpartitionStateHandle;\n+import org.apache.flink.runtime.state.memory.ByteStreamStateHandle;\n+import org.apache.flink.util.function.ThrowingConsumer;\n+\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Random;\n+import java.util.function.BiFunction;\n+import java.util.function.Function;\n+\n+import static java.util.function.Function.identity;\n+import static java.util.stream.Collectors.toList;\n+import static java.util.stream.Collectors.toMap;\n+import static java.util.stream.IntStream.range;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * {@link SequentialChannelStateReaderImpl} Test.\n+ */\n+@RunWith(Parameterized.class)\n+public class SequentialChannelStateReaderImplTest {\n+\n+\t@Parameterized.Parameters(name = \"{0}: stateParLevel={1}, statePartsPerChannel={1}, stateBytesPerPart={2},  parLevel={4}, bufferSize={5}\")\n+\tpublic static Object[][] parameters() {\n+\t\treturn new Object[][]{\n+\t\t\t{\"NoStateAndNoChannels\", 0, 0, 0, 0, 0},\n+\t\t\t{\"NoState\", 0, 10, 10, 10, 10},\n+\t\t\t{\"ReadPermutedStateWithEqualBuffer\", 10, 10, 10, 10, 10},\n+\t\t\t{\"ReadPermutedStateWithReducedBuffer\", 10, 10, 10, 20, 10},\n+\t\t\t{\"ReadPermutedStateWithIncreasedBuffer\", 10, 10, 10, 10, 20},\n+\t\t};\n+\t}\n+\n+\tprivate final ChannelStateSerializer serializer;\n+\tprivate final Random random;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDMzOTU2Ng=="}, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 85}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjU3ODE4OA==", "bodyText": "The issue is rather that Random is always strictly slower than ThreadLocalRandom because of lock acquisitions. However, you are right, as long as we not using multiple threads, we do not have contention, so the difference is marginal.", "url": "https://github.com/apache/flink/pull/13351#discussion_r496578188", "createdAt": "2020-09-29T09:35:16Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/test/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImplTest.java", "diffHunk": "@@ -0,0 +1,321 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.memory.MemorySegmentFactory;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.runtime.checkpoint.StateObjectCollection;\n+import org.apache.flink.runtime.checkpoint.TaskStateSnapshot;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.FreeingBufferRecycler;\n+import org.apache.flink.runtime.io.network.buffer.NetworkBuffer;\n+import org.apache.flink.runtime.io.network.buffer.NetworkBufferPool;\n+import org.apache.flink.runtime.io.network.partition.ResultPartition;\n+import org.apache.flink.runtime.io.network.partition.ResultPartitionBuilder;\n+import org.apache.flink.runtime.io.network.partition.ResultSubpartition.BufferAndBacklog;\n+import org.apache.flink.runtime.io.network.partition.ResultSubpartitionView;\n+import org.apache.flink.runtime.io.network.partition.consumer.BufferOrEvent;\n+import org.apache.flink.runtime.io.network.partition.consumer.InputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateBuilder;\n+import org.apache.flink.runtime.jobgraph.OperatorID;\n+import org.apache.flink.runtime.state.InputChannelStateHandle;\n+import org.apache.flink.runtime.state.ResultSubpartitionStateHandle;\n+import org.apache.flink.runtime.state.memory.ByteStreamStateHandle;\n+import org.apache.flink.util.function.ThrowingConsumer;\n+\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Random;\n+import java.util.function.BiFunction;\n+import java.util.function.Function;\n+\n+import static java.util.function.Function.identity;\n+import static java.util.stream.Collectors.toList;\n+import static java.util.stream.Collectors.toMap;\n+import static java.util.stream.IntStream.range;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * {@link SequentialChannelStateReaderImpl} Test.\n+ */\n+@RunWith(Parameterized.class)\n+public class SequentialChannelStateReaderImplTest {\n+\n+\t@Parameterized.Parameters(name = \"{0}: stateParLevel={1}, statePartsPerChannel={1}, stateBytesPerPart={2},  parLevel={4}, bufferSize={5}\")\n+\tpublic static Object[][] parameters() {\n+\t\treturn new Object[][]{\n+\t\t\t{\"NoStateAndNoChannels\", 0, 0, 0, 0, 0},\n+\t\t\t{\"NoState\", 0, 10, 10, 10, 10},\n+\t\t\t{\"ReadPermutedStateWithEqualBuffer\", 10, 10, 10, 10, 10},\n+\t\t\t{\"ReadPermutedStateWithReducedBuffer\", 10, 10, 10, 20, 10},\n+\t\t\t{\"ReadPermutedStateWithIncreasedBuffer\", 10, 10, 10, 10, 20},\n+\t\t};\n+\t}\n+\n+\tprivate final ChannelStateSerializer serializer;\n+\tprivate final Random random;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDMzOTU2Ng=="}, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 85}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjY2MDY2MA==", "bodyText": "Random is always strictly slower than ThreadLocalRandom\n\nnextBytes  used in this test is not overridden in ThreadLocalRandom so it's the same.\nAnd the creation of Random seems cheaper than ThreadLocalRandom.\nSo it would be slower in the end.", "url": "https://github.com/apache/flink/pull/13351#discussion_r496660660", "createdAt": "2020-09-29T12:03:36Z", "author": {"login": "rkhachatryan"}, "path": "flink-runtime/src/test/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImplTest.java", "diffHunk": "@@ -0,0 +1,321 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.memory.MemorySegmentFactory;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.runtime.checkpoint.StateObjectCollection;\n+import org.apache.flink.runtime.checkpoint.TaskStateSnapshot;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.FreeingBufferRecycler;\n+import org.apache.flink.runtime.io.network.buffer.NetworkBuffer;\n+import org.apache.flink.runtime.io.network.buffer.NetworkBufferPool;\n+import org.apache.flink.runtime.io.network.partition.ResultPartition;\n+import org.apache.flink.runtime.io.network.partition.ResultPartitionBuilder;\n+import org.apache.flink.runtime.io.network.partition.ResultSubpartition.BufferAndBacklog;\n+import org.apache.flink.runtime.io.network.partition.ResultSubpartitionView;\n+import org.apache.flink.runtime.io.network.partition.consumer.BufferOrEvent;\n+import org.apache.flink.runtime.io.network.partition.consumer.InputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateBuilder;\n+import org.apache.flink.runtime.jobgraph.OperatorID;\n+import org.apache.flink.runtime.state.InputChannelStateHandle;\n+import org.apache.flink.runtime.state.ResultSubpartitionStateHandle;\n+import org.apache.flink.runtime.state.memory.ByteStreamStateHandle;\n+import org.apache.flink.util.function.ThrowingConsumer;\n+\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Random;\n+import java.util.function.BiFunction;\n+import java.util.function.Function;\n+\n+import static java.util.function.Function.identity;\n+import static java.util.stream.Collectors.toList;\n+import static java.util.stream.Collectors.toMap;\n+import static java.util.stream.IntStream.range;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * {@link SequentialChannelStateReaderImpl} Test.\n+ */\n+@RunWith(Parameterized.class)\n+public class SequentialChannelStateReaderImplTest {\n+\n+\t@Parameterized.Parameters(name = \"{0}: stateParLevel={1}, statePartsPerChannel={1}, stateBytesPerPart={2},  parLevel={4}, bufferSize={5}\")\n+\tpublic static Object[][] parameters() {\n+\t\treturn new Object[][]{\n+\t\t\t{\"NoStateAndNoChannels\", 0, 0, 0, 0, 0},\n+\t\t\t{\"NoState\", 0, 10, 10, 10, 10},\n+\t\t\t{\"ReadPermutedStateWithEqualBuffer\", 10, 10, 10, 10, 10},\n+\t\t\t{\"ReadPermutedStateWithReducedBuffer\", 10, 10, 10, 20, 10},\n+\t\t\t{\"ReadPermutedStateWithIncreasedBuffer\", 10, 10, 10, 10, 20},\n+\t\t};\n+\t}\n+\n+\tprivate final ChannelStateSerializer serializer;\n+\tprivate final Random random;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDMzOTU2Ng=="}, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 85}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5MzkwODg1OnYy", "diffSide": "RIGHT", "path": "flink-runtime/src/test/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImplTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxMzo1Njo1NlrOHXcLOQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQxNjo1MToxNVrOHYLR6Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDM0MDkyMQ==", "bodyText": "Could you please extract a function with meaningful name of the lowest level?", "url": "https://github.com/apache/flink/pull/13351#discussion_r494340921", "createdAt": "2020-09-24T13:56:56Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/test/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImplTest.java", "diffHunk": "@@ -0,0 +1,321 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.memory.MemorySegmentFactory;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.runtime.checkpoint.StateObjectCollection;\n+import org.apache.flink.runtime.checkpoint.TaskStateSnapshot;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.FreeingBufferRecycler;\n+import org.apache.flink.runtime.io.network.buffer.NetworkBuffer;\n+import org.apache.flink.runtime.io.network.buffer.NetworkBufferPool;\n+import org.apache.flink.runtime.io.network.partition.ResultPartition;\n+import org.apache.flink.runtime.io.network.partition.ResultPartitionBuilder;\n+import org.apache.flink.runtime.io.network.partition.ResultSubpartition.BufferAndBacklog;\n+import org.apache.flink.runtime.io.network.partition.ResultSubpartitionView;\n+import org.apache.flink.runtime.io.network.partition.consumer.BufferOrEvent;\n+import org.apache.flink.runtime.io.network.partition.consumer.InputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateBuilder;\n+import org.apache.flink.runtime.jobgraph.OperatorID;\n+import org.apache.flink.runtime.state.InputChannelStateHandle;\n+import org.apache.flink.runtime.state.ResultSubpartitionStateHandle;\n+import org.apache.flink.runtime.state.memory.ByteStreamStateHandle;\n+import org.apache.flink.util.function.ThrowingConsumer;\n+\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Random;\n+import java.util.function.BiFunction;\n+import java.util.function.Function;\n+\n+import static java.util.function.Function.identity;\n+import static java.util.stream.Collectors.toList;\n+import static java.util.stream.Collectors.toMap;\n+import static java.util.stream.IntStream.range;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * {@link SequentialChannelStateReaderImpl} Test.\n+ */\n+@RunWith(Parameterized.class)\n+public class SequentialChannelStateReaderImplTest {\n+\n+\t@Parameterized.Parameters(name = \"{0}: stateParLevel={1}, statePartsPerChannel={1}, stateBytesPerPart={2},  parLevel={4}, bufferSize={5}\")\n+\tpublic static Object[][] parameters() {\n+\t\treturn new Object[][]{\n+\t\t\t{\"NoStateAndNoChannels\", 0, 0, 0, 0, 0},\n+\t\t\t{\"NoState\", 0, 10, 10, 10, 10},\n+\t\t\t{\"ReadPermutedStateWithEqualBuffer\", 10, 10, 10, 10, 10},\n+\t\t\t{\"ReadPermutedStateWithReducedBuffer\", 10, 10, 10, 20, 10},\n+\t\t\t{\"ReadPermutedStateWithIncreasedBuffer\", 10, 10, 10, 10, 20},\n+\t\t};\n+\t}\n+\n+\tprivate final ChannelStateSerializer serializer;\n+\tprivate final Random random;\n+\tprivate final int parLevel;\n+\tprivate final int statePartsPerChannel;\n+\tprivate final int stateBytesPerPart;\n+\tprivate final int bufferSize;\n+\tprivate final int stateParLevel;\n+\tprivate final boolean expectToHaveState;\n+\tprivate final int buffersPerChannel;\n+\n+\tpublic SequentialChannelStateReaderImplTest(String desc, int stateParLevel, int statePartsPerChannel, int stateBytesPerPart, int parLevel, int bufferSize) {\n+\t\tthis.serializer = new ChannelStateSerializerImpl();\n+\t\tthis.random = new Random();\n+\t\tthis.parLevel = parLevel;\n+\t\tthis.statePartsPerChannel = statePartsPerChannel;\n+\t\tthis.stateBytesPerPart = stateBytesPerPart;\n+\t\tthis.bufferSize = bufferSize;\n+\t\tthis.stateParLevel = stateParLevel;\n+\t\tthis.expectToHaveState = stateParLevel * statePartsPerChannel * stateBytesPerPart > 0;\n+\t\t// will read without waiting for consumption\n+\t\tthis.buffersPerChannel = Math.max(1, statePartsPerChannel * (bufferSize >= stateBytesPerPart ? 1 : stateBytesPerPart / bufferSize));\n+\t}\n+\n+\t@Test\n+\tpublic void testReadPermutedState() throws Exception {\n+\t\tMap<InputChannelInfo, List<byte[]>> inputChannelsData = generateState(InputChannelInfo::new);\n+\t\tMap<ResultSubpartitionInfo, List<byte[]>> resultPartitionsData = generateState(ResultSubpartitionInfo::new);\n+\n+\t\tSequentialChannelStateReader reader = new SequentialChannelStateReaderImpl(buildSnapshot(writePermuted(inputChannelsData, resultPartitionsData)));\n+\t\tassertEquals(expectToHaveState, reader.hasChannelStates());\n+\n+\t\twithResultPartitions(resultPartitions -> {\n+\t\t\treader.readOutputData(resultPartitions);\n+\t\t\tassertBuffersEquals(resultPartitionsData, collectBuffers(resultPartitions));\n+\t\t});\n+\n+\t\twithInputGates(gates -> {\n+\t\t\treader.readInputData(gates);\n+\t\t\tassertBuffersEquals(inputChannelsData, collectBuffers(gates));\n+\t\t\tassertConsumed(gates);\n+\t\t});\n+\t}\n+\n+\tprivate Map<ResultSubpartitionInfo, List<Buffer>> collectBuffers(ResultPartition[] resultPartitions) throws IOException {\n+\t\tMap<ResultSubpartitionInfo, List<Buffer>> actual = new HashMap<>();\n+\t\tfor (ResultPartition resultPartition : resultPartitions) {\n+\t\t\tfor (int i = 0; i < resultPartition.getNumberOfSubpartitions(); i++) {\n+\t\t\t\tResultSubpartitionInfo info = resultPartition.getSubpartitionInfo(i);\n+\t\t\t\tResultSubpartitionView view = resultPartition.createSubpartitionView(info.getSubPartitionIdx(), () -> { /**/ });\n+\t\t\t\tfor (BufferAndBacklog buffer = view.getNextBuffer(); buffer != null; buffer = view.getNextBuffer()) {\n+\t\t\t\t\tactual.computeIfAbsent(info, unused -> new ArrayList<>()).add(buffer.buffer());\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t\treturn actual;\n+\t}\n+\n+\tprivate Map<InputChannelInfo, List<Buffer>> collectBuffers(InputGate[] gates) throws Exception {\n+\t\tMap<InputChannelInfo, List<Buffer>> actual = new HashMap<>();\n+\t\tfor (InputGate gate : gates) {\n+\t\t\tfor (Optional<BufferOrEvent> next = gate.pollNext(); next.isPresent(); next = gate.pollNext()) {\n+\t\t\t\tactual.computeIfAbsent(\n+\t\t\t\t\tnext.get().getChannelInfo(),\n+\t\t\t\t\tunused -> new ArrayList<>()).add(next.get().getBuffer());\n+\t\t\t}\n+\t\t}\n+\t\treturn actual;\n+\t}\n+\n+\tprivate void assertConsumed(InputGate[] gates) throws InterruptedException, java.util.concurrent.ExecutionException {\n+\t\tfor (InputGate gate: gates) {\n+\t\t\tassertTrue(gate.getStateConsumedFuture().isDone());\n+\t\t\tgate.getStateConsumedFuture().get();\n+\t\t}\n+\t}\n+\n+\tprivate void withInputGates(ThrowingConsumer<InputGate[], Exception> action) throws Exception {\n+\t\tSingleInputGate[] gates = new SingleInputGate[parLevel];\n+\t\tfinal int segmentsToAllocate = parLevel * parLevel * buffersPerChannel;\n+\t\tNetworkBufferPool networkBufferPool = new NetworkBufferPool(segmentsToAllocate, bufferSize);\n+\t\ttry {\n+\t\t\tfor (int i = 0; i < parLevel; i++) {\n+\t\t\t\tgates[i] = new SingleInputGateBuilder()\n+\t\t\t\t\t.setNumberOfChannels(parLevel)\n+\t\t\t\t\t.setSingleInputGateIndex(i)\n+\t\t\t\t\t.setBufferPoolFactory(networkBufferPool.createBufferPool(0, buffersPerChannel))\n+\t\t\t\t\t.setSegmentProvider(networkBufferPool)\n+\t\t\t\t\t.setChannelFactory((builder, gate) -> builder\n+\t\t\t\t\t.setNetworkBuffersPerChannel(buffersPerChannel)\n+\t\t\t\t\t.buildRemoteRecoveredChannel(gate))\n+\t\t\t\t\t.build();\n+\t\t\t\tgates[i].setup();\n+\t\t\t}\n+\t\t\taction.accept(gates);\n+\t\t} finally {\n+\t\t\tfor (InputGate inputGate: gates) {\n+\t\t\t\tinputGate.close();\n+\t\t\t}\n+\t\t\ttry {\n+\t\t\t\tassertEquals(segmentsToAllocate, networkBufferPool.getNumberOfAvailableMemorySegments());\n+\t\t\t} finally {\n+\t\t\t\tnetworkBufferPool.destroyAllBufferPools();\n+\t\t\t\tnetworkBufferPool.destroy();\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate void withResultPartitions(ThrowingConsumer<ResultPartition[], Exception> action) throws Exception {\n+\t\tint segmentsToAllocate = parLevel * parLevel * buffersPerChannel;\n+\t\tNetworkBufferPool networkBufferPool = new NetworkBufferPool(segmentsToAllocate, bufferSize);\n+\t\tResultPartition[] resultPartitions = range(0, parLevel)\n+\t\t\t.mapToObj(i -> new ResultPartitionBuilder().setResultPartitionIndex(i).setNumberOfSubpartitions(parLevel).setNetworkBufferPool(networkBufferPool).build())\n+\t\t\t.toArray(ResultPartition[]::new);\n+\t\ttry {\n+\t\t\tfor (ResultPartition resultPartition: resultPartitions) {\n+\t\t\t\tresultPartition.setup();\n+\t\t\t}\n+\t\t\taction.accept(resultPartitions);\n+\t\t} finally {\n+\t\t\tfor (ResultPartition resultPartition: resultPartitions) {\n+\t\t\t\tresultPartition.close();\n+\t\t\t}\n+\t\t\ttry {\n+\t\t\t\tassertEquals(segmentsToAllocate, networkBufferPool.getNumberOfAvailableMemorySegments());\n+\t\t\t} finally {\n+\t\t\t\tnetworkBufferPool.destroyAllBufferPools();\n+\t\t\t\tnetworkBufferPool.destroy();\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate TaskStateSnapshot buildSnapshot(Tuple2<List<InputChannelStateHandle>, List<ResultSubpartitionStateHandle>> handles) {\n+\t\treturn new TaskStateSnapshot(\n+\t\t\tCollections.singletonMap(new OperatorID(), new OperatorSubtaskState(\n+\t\t\t\tStateObjectCollection.empty(),\n+\t\t\t\tStateObjectCollection.empty(),\n+\t\t\t\tStateObjectCollection.empty(),\n+\t\t\t\tStateObjectCollection.empty(),\n+\t\t\t\tnew StateObjectCollection<>(handles.f0),\n+\t\t\t\tnew StateObjectCollection<>(handles.f1)\n+\t\t\t))\n+\t\t);\n+\t}\n+\n+\tprivate <T> Map<T, List<byte[]>> generateState(BiFunction<Integer, Integer, T> descriptorCreator) {\n+\t\treturn range(0, stateParLevel).boxed().flatMap(\n+\t\t\tgateId -> range(0, stateParLevel).mapToObj(\n+\t\t\t\tchannelId ->\n+\t\t\t\t\tdescriptorCreator.apply(gateId, channelId))).collect(toMap(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 232}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTExMjY4MQ==", "bodyText": "Good idea.\nI'll extract generateSingleChannelState method.", "url": "https://github.com/apache/flink/pull/13351#discussion_r495112681", "createdAt": "2020-09-25T16:51:15Z", "author": {"login": "rkhachatryan"}, "path": "flink-runtime/src/test/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImplTest.java", "diffHunk": "@@ -0,0 +1,321 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.memory.MemorySegmentFactory;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.runtime.checkpoint.StateObjectCollection;\n+import org.apache.flink.runtime.checkpoint.TaskStateSnapshot;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.FreeingBufferRecycler;\n+import org.apache.flink.runtime.io.network.buffer.NetworkBuffer;\n+import org.apache.flink.runtime.io.network.buffer.NetworkBufferPool;\n+import org.apache.flink.runtime.io.network.partition.ResultPartition;\n+import org.apache.flink.runtime.io.network.partition.ResultPartitionBuilder;\n+import org.apache.flink.runtime.io.network.partition.ResultSubpartition.BufferAndBacklog;\n+import org.apache.flink.runtime.io.network.partition.ResultSubpartitionView;\n+import org.apache.flink.runtime.io.network.partition.consumer.BufferOrEvent;\n+import org.apache.flink.runtime.io.network.partition.consumer.InputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateBuilder;\n+import org.apache.flink.runtime.jobgraph.OperatorID;\n+import org.apache.flink.runtime.state.InputChannelStateHandle;\n+import org.apache.flink.runtime.state.ResultSubpartitionStateHandle;\n+import org.apache.flink.runtime.state.memory.ByteStreamStateHandle;\n+import org.apache.flink.util.function.ThrowingConsumer;\n+\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Random;\n+import java.util.function.BiFunction;\n+import java.util.function.Function;\n+\n+import static java.util.function.Function.identity;\n+import static java.util.stream.Collectors.toList;\n+import static java.util.stream.Collectors.toMap;\n+import static java.util.stream.IntStream.range;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * {@link SequentialChannelStateReaderImpl} Test.\n+ */\n+@RunWith(Parameterized.class)\n+public class SequentialChannelStateReaderImplTest {\n+\n+\t@Parameterized.Parameters(name = \"{0}: stateParLevel={1}, statePartsPerChannel={1}, stateBytesPerPart={2},  parLevel={4}, bufferSize={5}\")\n+\tpublic static Object[][] parameters() {\n+\t\treturn new Object[][]{\n+\t\t\t{\"NoStateAndNoChannels\", 0, 0, 0, 0, 0},\n+\t\t\t{\"NoState\", 0, 10, 10, 10, 10},\n+\t\t\t{\"ReadPermutedStateWithEqualBuffer\", 10, 10, 10, 10, 10},\n+\t\t\t{\"ReadPermutedStateWithReducedBuffer\", 10, 10, 10, 20, 10},\n+\t\t\t{\"ReadPermutedStateWithIncreasedBuffer\", 10, 10, 10, 10, 20},\n+\t\t};\n+\t}\n+\n+\tprivate final ChannelStateSerializer serializer;\n+\tprivate final Random random;\n+\tprivate final int parLevel;\n+\tprivate final int statePartsPerChannel;\n+\tprivate final int stateBytesPerPart;\n+\tprivate final int bufferSize;\n+\tprivate final int stateParLevel;\n+\tprivate final boolean expectToHaveState;\n+\tprivate final int buffersPerChannel;\n+\n+\tpublic SequentialChannelStateReaderImplTest(String desc, int stateParLevel, int statePartsPerChannel, int stateBytesPerPart, int parLevel, int bufferSize) {\n+\t\tthis.serializer = new ChannelStateSerializerImpl();\n+\t\tthis.random = new Random();\n+\t\tthis.parLevel = parLevel;\n+\t\tthis.statePartsPerChannel = statePartsPerChannel;\n+\t\tthis.stateBytesPerPart = stateBytesPerPart;\n+\t\tthis.bufferSize = bufferSize;\n+\t\tthis.stateParLevel = stateParLevel;\n+\t\tthis.expectToHaveState = stateParLevel * statePartsPerChannel * stateBytesPerPart > 0;\n+\t\t// will read without waiting for consumption\n+\t\tthis.buffersPerChannel = Math.max(1, statePartsPerChannel * (bufferSize >= stateBytesPerPart ? 1 : stateBytesPerPart / bufferSize));\n+\t}\n+\n+\t@Test\n+\tpublic void testReadPermutedState() throws Exception {\n+\t\tMap<InputChannelInfo, List<byte[]>> inputChannelsData = generateState(InputChannelInfo::new);\n+\t\tMap<ResultSubpartitionInfo, List<byte[]>> resultPartitionsData = generateState(ResultSubpartitionInfo::new);\n+\n+\t\tSequentialChannelStateReader reader = new SequentialChannelStateReaderImpl(buildSnapshot(writePermuted(inputChannelsData, resultPartitionsData)));\n+\t\tassertEquals(expectToHaveState, reader.hasChannelStates());\n+\n+\t\twithResultPartitions(resultPartitions -> {\n+\t\t\treader.readOutputData(resultPartitions);\n+\t\t\tassertBuffersEquals(resultPartitionsData, collectBuffers(resultPartitions));\n+\t\t});\n+\n+\t\twithInputGates(gates -> {\n+\t\t\treader.readInputData(gates);\n+\t\t\tassertBuffersEquals(inputChannelsData, collectBuffers(gates));\n+\t\t\tassertConsumed(gates);\n+\t\t});\n+\t}\n+\n+\tprivate Map<ResultSubpartitionInfo, List<Buffer>> collectBuffers(ResultPartition[] resultPartitions) throws IOException {\n+\t\tMap<ResultSubpartitionInfo, List<Buffer>> actual = new HashMap<>();\n+\t\tfor (ResultPartition resultPartition : resultPartitions) {\n+\t\t\tfor (int i = 0; i < resultPartition.getNumberOfSubpartitions(); i++) {\n+\t\t\t\tResultSubpartitionInfo info = resultPartition.getSubpartitionInfo(i);\n+\t\t\t\tResultSubpartitionView view = resultPartition.createSubpartitionView(info.getSubPartitionIdx(), () -> { /**/ });\n+\t\t\t\tfor (BufferAndBacklog buffer = view.getNextBuffer(); buffer != null; buffer = view.getNextBuffer()) {\n+\t\t\t\t\tactual.computeIfAbsent(info, unused -> new ArrayList<>()).add(buffer.buffer());\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t\treturn actual;\n+\t}\n+\n+\tprivate Map<InputChannelInfo, List<Buffer>> collectBuffers(InputGate[] gates) throws Exception {\n+\t\tMap<InputChannelInfo, List<Buffer>> actual = new HashMap<>();\n+\t\tfor (InputGate gate : gates) {\n+\t\t\tfor (Optional<BufferOrEvent> next = gate.pollNext(); next.isPresent(); next = gate.pollNext()) {\n+\t\t\t\tactual.computeIfAbsent(\n+\t\t\t\t\tnext.get().getChannelInfo(),\n+\t\t\t\t\tunused -> new ArrayList<>()).add(next.get().getBuffer());\n+\t\t\t}\n+\t\t}\n+\t\treturn actual;\n+\t}\n+\n+\tprivate void assertConsumed(InputGate[] gates) throws InterruptedException, java.util.concurrent.ExecutionException {\n+\t\tfor (InputGate gate: gates) {\n+\t\t\tassertTrue(gate.getStateConsumedFuture().isDone());\n+\t\t\tgate.getStateConsumedFuture().get();\n+\t\t}\n+\t}\n+\n+\tprivate void withInputGates(ThrowingConsumer<InputGate[], Exception> action) throws Exception {\n+\t\tSingleInputGate[] gates = new SingleInputGate[parLevel];\n+\t\tfinal int segmentsToAllocate = parLevel * parLevel * buffersPerChannel;\n+\t\tNetworkBufferPool networkBufferPool = new NetworkBufferPool(segmentsToAllocate, bufferSize);\n+\t\ttry {\n+\t\t\tfor (int i = 0; i < parLevel; i++) {\n+\t\t\t\tgates[i] = new SingleInputGateBuilder()\n+\t\t\t\t\t.setNumberOfChannels(parLevel)\n+\t\t\t\t\t.setSingleInputGateIndex(i)\n+\t\t\t\t\t.setBufferPoolFactory(networkBufferPool.createBufferPool(0, buffersPerChannel))\n+\t\t\t\t\t.setSegmentProvider(networkBufferPool)\n+\t\t\t\t\t.setChannelFactory((builder, gate) -> builder\n+\t\t\t\t\t.setNetworkBuffersPerChannel(buffersPerChannel)\n+\t\t\t\t\t.buildRemoteRecoveredChannel(gate))\n+\t\t\t\t\t.build();\n+\t\t\t\tgates[i].setup();\n+\t\t\t}\n+\t\t\taction.accept(gates);\n+\t\t} finally {\n+\t\t\tfor (InputGate inputGate: gates) {\n+\t\t\t\tinputGate.close();\n+\t\t\t}\n+\t\t\ttry {\n+\t\t\t\tassertEquals(segmentsToAllocate, networkBufferPool.getNumberOfAvailableMemorySegments());\n+\t\t\t} finally {\n+\t\t\t\tnetworkBufferPool.destroyAllBufferPools();\n+\t\t\t\tnetworkBufferPool.destroy();\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate void withResultPartitions(ThrowingConsumer<ResultPartition[], Exception> action) throws Exception {\n+\t\tint segmentsToAllocate = parLevel * parLevel * buffersPerChannel;\n+\t\tNetworkBufferPool networkBufferPool = new NetworkBufferPool(segmentsToAllocate, bufferSize);\n+\t\tResultPartition[] resultPartitions = range(0, parLevel)\n+\t\t\t.mapToObj(i -> new ResultPartitionBuilder().setResultPartitionIndex(i).setNumberOfSubpartitions(parLevel).setNetworkBufferPool(networkBufferPool).build())\n+\t\t\t.toArray(ResultPartition[]::new);\n+\t\ttry {\n+\t\t\tfor (ResultPartition resultPartition: resultPartitions) {\n+\t\t\t\tresultPartition.setup();\n+\t\t\t}\n+\t\t\taction.accept(resultPartitions);\n+\t\t} finally {\n+\t\t\tfor (ResultPartition resultPartition: resultPartitions) {\n+\t\t\t\tresultPartition.close();\n+\t\t\t}\n+\t\t\ttry {\n+\t\t\t\tassertEquals(segmentsToAllocate, networkBufferPool.getNumberOfAvailableMemorySegments());\n+\t\t\t} finally {\n+\t\t\t\tnetworkBufferPool.destroyAllBufferPools();\n+\t\t\t\tnetworkBufferPool.destroy();\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate TaskStateSnapshot buildSnapshot(Tuple2<List<InputChannelStateHandle>, List<ResultSubpartitionStateHandle>> handles) {\n+\t\treturn new TaskStateSnapshot(\n+\t\t\tCollections.singletonMap(new OperatorID(), new OperatorSubtaskState(\n+\t\t\t\tStateObjectCollection.empty(),\n+\t\t\t\tStateObjectCollection.empty(),\n+\t\t\t\tStateObjectCollection.empty(),\n+\t\t\t\tStateObjectCollection.empty(),\n+\t\t\t\tnew StateObjectCollection<>(handles.f0),\n+\t\t\t\tnew StateObjectCollection<>(handles.f1)\n+\t\t\t))\n+\t\t);\n+\t}\n+\n+\tprivate <T> Map<T, List<byte[]>> generateState(BiFunction<Integer, Integer, T> descriptorCreator) {\n+\t\treturn range(0, stateParLevel).boxed().flatMap(\n+\t\t\tgateId -> range(0, stateParLevel).mapToObj(\n+\t\t\t\tchannelId ->\n+\t\t\t\t\tdescriptorCreator.apply(gateId, channelId))).collect(toMap(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDM0MDkyMQ=="}, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 232}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5MzkyNTI5OnYy", "diffSide": "RIGHT", "path": "flink-runtime/src/test/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImplTest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxMzo1OTo0OFrOHXcWAQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxMzo1OTo0OFrOHXcWAQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDM0MzY4MQ==", "bodyText": "NoOpBufferAvailablityListener if we ever add a new mandatory method.", "url": "https://github.com/apache/flink/pull/13351#discussion_r494343681", "createdAt": "2020-09-24T13:59:48Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/test/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImplTest.java", "diffHunk": "@@ -0,0 +1,321 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.checkpoint.channel;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.memory.MemorySegmentFactory;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.runtime.checkpoint.StateObjectCollection;\n+import org.apache.flink.runtime.checkpoint.TaskStateSnapshot;\n+import org.apache.flink.runtime.io.network.buffer.Buffer;\n+import org.apache.flink.runtime.io.network.buffer.FreeingBufferRecycler;\n+import org.apache.flink.runtime.io.network.buffer.NetworkBuffer;\n+import org.apache.flink.runtime.io.network.buffer.NetworkBufferPool;\n+import org.apache.flink.runtime.io.network.partition.ResultPartition;\n+import org.apache.flink.runtime.io.network.partition.ResultPartitionBuilder;\n+import org.apache.flink.runtime.io.network.partition.ResultSubpartition.BufferAndBacklog;\n+import org.apache.flink.runtime.io.network.partition.ResultSubpartitionView;\n+import org.apache.flink.runtime.io.network.partition.consumer.BufferOrEvent;\n+import org.apache.flink.runtime.io.network.partition.consumer.InputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate;\n+import org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateBuilder;\n+import org.apache.flink.runtime.jobgraph.OperatorID;\n+import org.apache.flink.runtime.state.InputChannelStateHandle;\n+import org.apache.flink.runtime.state.ResultSubpartitionStateHandle;\n+import org.apache.flink.runtime.state.memory.ByteStreamStateHandle;\n+import org.apache.flink.util.function.ThrowingConsumer;\n+\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Random;\n+import java.util.function.BiFunction;\n+import java.util.function.Function;\n+\n+import static java.util.function.Function.identity;\n+import static java.util.stream.Collectors.toList;\n+import static java.util.stream.Collectors.toMap;\n+import static java.util.stream.IntStream.range;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * {@link SequentialChannelStateReaderImpl} Test.\n+ */\n+@RunWith(Parameterized.class)\n+public class SequentialChannelStateReaderImplTest {\n+\n+\t@Parameterized.Parameters(name = \"{0}: stateParLevel={1}, statePartsPerChannel={1}, stateBytesPerPart={2},  parLevel={4}, bufferSize={5}\")\n+\tpublic static Object[][] parameters() {\n+\t\treturn new Object[][]{\n+\t\t\t{\"NoStateAndNoChannels\", 0, 0, 0, 0, 0},\n+\t\t\t{\"NoState\", 0, 10, 10, 10, 10},\n+\t\t\t{\"ReadPermutedStateWithEqualBuffer\", 10, 10, 10, 10, 10},\n+\t\t\t{\"ReadPermutedStateWithReducedBuffer\", 10, 10, 10, 20, 10},\n+\t\t\t{\"ReadPermutedStateWithIncreasedBuffer\", 10, 10, 10, 10, 20},\n+\t\t};\n+\t}\n+\n+\tprivate final ChannelStateSerializer serializer;\n+\tprivate final Random random;\n+\tprivate final int parLevel;\n+\tprivate final int statePartsPerChannel;\n+\tprivate final int stateBytesPerPart;\n+\tprivate final int bufferSize;\n+\tprivate final int stateParLevel;\n+\tprivate final boolean expectToHaveState;\n+\tprivate final int buffersPerChannel;\n+\n+\tpublic SequentialChannelStateReaderImplTest(String desc, int stateParLevel, int statePartsPerChannel, int stateBytesPerPart, int parLevel, int bufferSize) {\n+\t\tthis.serializer = new ChannelStateSerializerImpl();\n+\t\tthis.random = new Random();\n+\t\tthis.parLevel = parLevel;\n+\t\tthis.statePartsPerChannel = statePartsPerChannel;\n+\t\tthis.stateBytesPerPart = stateBytesPerPart;\n+\t\tthis.bufferSize = bufferSize;\n+\t\tthis.stateParLevel = stateParLevel;\n+\t\tthis.expectToHaveState = stateParLevel * statePartsPerChannel * stateBytesPerPart > 0;\n+\t\t// will read without waiting for consumption\n+\t\tthis.buffersPerChannel = Math.max(1, statePartsPerChannel * (bufferSize >= stateBytesPerPart ? 1 : stateBytesPerPart / bufferSize));\n+\t}\n+\n+\t@Test\n+\tpublic void testReadPermutedState() throws Exception {\n+\t\tMap<InputChannelInfo, List<byte[]>> inputChannelsData = generateState(InputChannelInfo::new);\n+\t\tMap<ResultSubpartitionInfo, List<byte[]>> resultPartitionsData = generateState(ResultSubpartitionInfo::new);\n+\n+\t\tSequentialChannelStateReader reader = new SequentialChannelStateReaderImpl(buildSnapshot(writePermuted(inputChannelsData, resultPartitionsData)));\n+\t\tassertEquals(expectToHaveState, reader.hasChannelStates());\n+\n+\t\twithResultPartitions(resultPartitions -> {\n+\t\t\treader.readOutputData(resultPartitions);\n+\t\t\tassertBuffersEquals(resultPartitionsData, collectBuffers(resultPartitions));\n+\t\t});\n+\n+\t\twithInputGates(gates -> {\n+\t\t\treader.readInputData(gates);\n+\t\t\tassertBuffersEquals(inputChannelsData, collectBuffers(gates));\n+\t\t\tassertConsumed(gates);\n+\t\t});\n+\t}\n+\n+\tprivate Map<ResultSubpartitionInfo, List<Buffer>> collectBuffers(ResultPartition[] resultPartitions) throws IOException {\n+\t\tMap<ResultSubpartitionInfo, List<Buffer>> actual = new HashMap<>();\n+\t\tfor (ResultPartition resultPartition : resultPartitions) {\n+\t\t\tfor (int i = 0; i < resultPartition.getNumberOfSubpartitions(); i++) {\n+\t\t\t\tResultSubpartitionInfo info = resultPartition.getSubpartitionInfo(i);\n+\t\t\t\tResultSubpartitionView view = resultPartition.createSubpartitionView(info.getSubPartitionIdx(), () -> { /**/ });", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b018aaeb6eeeccfd5f1d0a0c3fa142f8a25f3e49"}, "originalPosition": 132}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5Mzk0NjUwOnYy", "diffSide": "RIGHT", "path": "flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/StreamTask.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxNDowNDoxMFrOHXcjPw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxNDowNDoxMFrOHXcjPw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDM0NzA3MQ==", "bodyText": "Okay you just did what I wrote on the last commit. Probably squashing makes sense.", "url": "https://github.com/apache/flink/pull/13351#discussion_r494347071", "createdAt": "2020-09-24T14:04:10Z", "author": {"login": "AHeise"}, "path": "flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/StreamTask.java", "diffHunk": "@@ -504,23 +502,18 @@ protected void beforeInvoke() throws Exception {\n \n \tprivate void readRecoveredChannelState() throws IOException, InterruptedException {\n \t\tSequentialChannelStateReader reader = getEnvironment().getTaskStateManager().getSequentialChannelStateReader();\n-\t\tif (reader.hasChannelStates()) {\n-\t\t\treader.readOutputData(getEnvironment().getAllWriters());\n-\t\t\tchannelIOExecutor.execute(() -> {\n-\t\t\t\ttry {\n-\t\t\t\t\treader.readInputData(getEnvironment().getAllInputGates());\n-\t\t\t\t} catch (Exception e) {\n-\t\t\t\t\tasyncExceptionHandler.handleAsyncException(\"Unable to read channel state\", e);\n-\t\t\t\t}\n-\t\t\t});\n-\n-\t\t\tfor (InputGate inputGate : getEnvironment().getAllInputGates()) {\n-\t\t\t\tinputGate\n-\t\t\t\t\t.getStateConsumedFuture()\n-\t\t\t\t\t.thenRun(() -> mainMailboxExecutor.execute(inputGate::requestPartitions, \"Input gate request partitions\"));\n+\t\treader.readOutputData(getEnvironment().getAllWriters());\n+\t\tchannelIOExecutor.execute(() -> {\n+\t\t\ttry {\n+\t\t\t\treader.readInputData(getEnvironment().getAllInputGates());\n+\t\t\t} catch (Exception e) {\n+\t\t\t\tasyncExceptionHandler.handleAsyncException(\"Unable to read channel state\", e);\n \t\t\t}\n-\t\t} else {\n-\t\t\trequestPartitions();\n+\t\t});\n+\t\tfor (InputGate inputGate : getEnvironment().getAllInputGates()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4913b2f902918470bd20236df46d60c84d97293d"}, "originalPosition": 44}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5Mzk0ODc1OnYy", "diffSide": "RIGHT", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/api/reader/AbstractRecordReader.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxNDowNDozN1rOHXckkQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxNDowNDozN1rOHXckkQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDM0NzQwOQ==", "bodyText": "nit: typo in commit message.", "url": "https://github.com/apache/flink/pull/13351#discussion_r494347409", "createdAt": "2020-09-24T14:04:37Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/api/reader/AbstractRecordReader.java", "diffHunk": "@@ -45,6 +45,8 @@\n \n \tprivate RecordDeserializer<T> currentRecordDeserializer;\n \n+\tprivate boolean finishedStateReading;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6ef9ee7deb0111ea4d1e113f9258d89a1c5b336e"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5Mzk1NjU1OnYy", "diffSide": "RIGHT", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/consumer/RecoveredInputChannel.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxNDowNjoxN1rOHXcprw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQxNjo1NDo1NVrOHYLZjA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDM0ODcxOQ==", "bodyText": "protected?", "url": "https://github.com/apache/flink/pull/13351#discussion_r494348719", "createdAt": "2020-09-24T14:06:17Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/consumer/RecoveredInputChannel.java", "diffHunk": "@@ -77,13 +78,18 @@\n \t\tbufferManager = new BufferManager(inputGate.getMemorySegmentProvider(), this, 0);\n \t}\n \n+\tpublic final InputChannel toInputChannel() throws IOException {\n+\t\tPreconditions.checkState(stateConsumedFuture.isDone(), \"recovered state is not fully consumed\");\n+\t\treturn toInputChannelInternal();\n+\t}\n+\n \t@Override\n \tpublic void setChannelStateWriter(ChannelStateWriter channelStateWriter) {\n \t\tcheckState(this.channelStateWriter == null, \"Already initialized\");\n \t\tthis.channelStateWriter = checkNotNull(channelStateWriter);\n \t}\n \n-\tpublic abstract InputChannel toInputChannel() throws IOException;\n+\tpublic abstract InputChannel toInputChannelInternal() throws IOException;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6ef9ee7deb0111ea4d1e113f9258d89a1c5b336e"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTExNDYzNg==", "bodyText": "Yes :)", "url": "https://github.com/apache/flink/pull/13351#discussion_r495114636", "createdAt": "2020-09-25T16:54:55Z", "author": {"login": "rkhachatryan"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/consumer/RecoveredInputChannel.java", "diffHunk": "@@ -77,13 +78,18 @@\n \t\tbufferManager = new BufferManager(inputGate.getMemorySegmentProvider(), this, 0);\n \t}\n \n+\tpublic final InputChannel toInputChannel() throws IOException {\n+\t\tPreconditions.checkState(stateConsumedFuture.isDone(), \"recovered state is not fully consumed\");\n+\t\treturn toInputChannelInternal();\n+\t}\n+\n \t@Override\n \tpublic void setChannelStateWriter(ChannelStateWriter channelStateWriter) {\n \t\tcheckState(this.channelStateWriter == null, \"Already initialized\");\n \t\tthis.channelStateWriter = checkNotNull(channelStateWriter);\n \t}\n \n-\tpublic abstract InputChannel toInputChannel() throws IOException;\n+\tpublic abstract InputChannel toInputChannelInternal() throws IOException;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDM0ODcxOQ=="}, "originalCommit": {"oid": "6ef9ee7deb0111ea4d1e113f9258d89a1c5b336e"}, "originalPosition": 24}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5Mzk3MDU0OnYy", "diffSide": "RIGHT", "path": "flink-runtime/src/test/java/org/apache/flink/runtime/checkpoint/channel/ChannelStateSerializerImplTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxNDowOTozNFrOHXcyig==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQxNjo1NjowMlrOHYLb5w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDM1MDk4Ng==", "bodyText": "Okay, now I see that you removed the original ChannelStateReader. I'd probably state that in the commit that adds SeqReader.", "url": "https://github.com/apache/flink/pull/13351#discussion_r494350986", "createdAt": "2020-09-24T14:09:34Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/test/java/org/apache/flink/runtime/checkpoint/channel/ChannelStateSerializerImplTest.java", "diffHunk": "@@ -36,7 +36,6 @@\n import java.util.Random;\n \n import static org.apache.flink.runtime.checkpoint.channel.ChannelStateByteBuffer.wrap;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4a598b96aa0250255f8758ce4ce57df23072dac7"}, "originalPosition": 3}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTExNTIzOQ==", "bodyText": "Done.", "url": "https://github.com/apache/flink/pull/13351#discussion_r495115239", "createdAt": "2020-09-25T16:56:02Z", "author": {"login": "rkhachatryan"}, "path": "flink-runtime/src/test/java/org/apache/flink/runtime/checkpoint/channel/ChannelStateSerializerImplTest.java", "diffHunk": "@@ -36,7 +36,6 @@\n import java.util.Random;\n \n import static org.apache.flink.runtime.checkpoint.channel.ChannelStateByteBuffer.wrap;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDM1MDk4Ng=="}, "originalCommit": {"oid": "4a598b96aa0250255f8758ce4ce57df23072dac7"}, "originalPosition": 3}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5Mzk3MTkzOnYy", "diffSide": "RIGHT", "path": "flink-runtime/src/test/java/org/apache/flink/runtime/checkpoint/channel/ChannelStateSerializerImplTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxNDowOTo1MFrOHXczYg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQwNzozOTo1MVrOHX4gWA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDM1MTIwMg==", "bodyText": "What's that change about?", "url": "https://github.com/apache/flink/pull/13351#discussion_r494351202", "createdAt": "2020-09-24T14:09:50Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/test/java/org/apache/flink/runtime/checkpoint/channel/ChannelStateSerializerImplTest.java", "diffHunk": "@@ -146,4 +145,10 @@ private void readAndCheck(byte[] data, ChannelStateSerializerImpl serializer, By\n \t\t}\n \t}\n \n+\tstatic byte[] generateData(int len) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4a598b96aa0250255f8758ce4ce57df23072dac7"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDgwNTA4MA==", "bodyText": "Previously, this method was imported from ChannelStateReaderImplTest.\nIn this commit ChannelStateReaderImplTest was removed and generateData moved to this class.", "url": "https://github.com/apache/flink/pull/13351#discussion_r494805080", "createdAt": "2020-09-25T07:39:51Z", "author": {"login": "rkhachatryan"}, "path": "flink-runtime/src/test/java/org/apache/flink/runtime/checkpoint/channel/ChannelStateSerializerImplTest.java", "diffHunk": "@@ -146,4 +145,10 @@ private void readAndCheck(byte[] data, ChannelStateSerializerImpl serializer, By\n \t\t}\n \t}\n \n+\tstatic byte[] generateData(int len) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDM1MTIwMg=="}, "originalCommit": {"oid": "4a598b96aa0250255f8758ce4ce57df23072dac7"}, "originalPosition": 12}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5Mzk3NDA0OnYy", "diffSide": "LEFT", "path": "flink-runtime/src/test/java/org/apache/flink/runtime/state/ChannelPersistenceITCase.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxNDoxMDoxNFrOHXc0ow==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQxNjo1NTo0M1rOHYLbLQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDM1MTUyMw==", "bodyText": "It looks like some cleanup is needed here.", "url": "https://github.com/apache/flink/pull/13351#discussion_r494351523", "createdAt": "2020-09-24T14:10:14Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/test/java/org/apache/flink/runtime/state/ChannelPersistenceITCase.java", "diffHunk": "@@ -76,17 +67,17 @@ public void testReadWritten() throws Exception {\n \t\t\tsingletonMap(resultSubpartitionInfo, resultSubpartitionInfoData)\n \t\t);\n \n-\t\tassertArrayEquals(inputChannelInfoData, read(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4a598b96aa0250255f8758ce4ce57df23072dac7"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTExNTA1Mw==", "bodyText": "Right, forgot about this test :)", "url": "https://github.com/apache/flink/pull/13351#discussion_r495115053", "createdAt": "2020-09-25T16:55:43Z", "author": {"login": "rkhachatryan"}, "path": "flink-runtime/src/test/java/org/apache/flink/runtime/state/ChannelPersistenceITCase.java", "diffHunk": "@@ -76,17 +67,17 @@ public void testReadWritten() throws Exception {\n \t\t\tsingletonMap(resultSubpartitionInfo, resultSubpartitionInfoData)\n \t\t);\n \n-\t\tassertArrayEquals(inputChannelInfoData, read(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDM1MTUyMw=="}, "originalCommit": {"oid": "4a598b96aa0250255f8758ce4ce57df23072dac7"}, "originalPosition": 42}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEwNzk1MTYyOnYy", "diffSide": "RIGHT", "path": "flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/tasks/StreamTaskMultipleInputSelectiveReadingTest.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwNzowMjowNFrOHZdngg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxNzozMTo0MVrOHcm9bg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjQ2MTY5OA==", "bodyText": "@pnowojski,\nthis test started to fail after rebasing of Read channel state unconditionally.\nSo I replaced processSingleStep with processAll.\nPlease take a look.", "url": "https://github.com/apache/flink/pull/13351#discussion_r496461698", "createdAt": "2020-09-29T07:02:04Z", "author": {"login": "rkhachatryan"}, "path": "flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/tasks/StreamTaskMultipleInputSelectiveReadingTest.java", "diffHunk": "@@ -197,22 +198,21 @@ public void testInputStarvation() throws Exception {\n \t\t\ttestHarness.processElement(new StreamRecord<>(\"3\"), 1);\n \t\t\ttestHarness.processElement(new StreamRecord<>(\"4\"), 1);\n \n-\t\t\ttestHarness.processSingleStep();\n \t\t\texpectedOutput.add(new StreamRecord<>(\"[2]: 1\"));\n-\t\t\ttestHarness.processSingleStep();\n \t\t\texpectedOutput.add(new StreamRecord<>(\"[2]: 2\"));\n-\t\t\tassertThat(testHarness.getOutput(), contains(expectedOutput.toArray()));\n+\t\t\ttestHarness.processAll();\n+\t\t\tassertEquals(expectedOutput, new ArrayList<>(testHarness.getOutput()).subList(0, expectedOutput.size()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ce1ddf0120e722f172916745adac6c4829cbae29"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTUyMTg4MQ==", "bodyText": "This test is now not doing what it was intended.\nNow you are processing all elements from the input gate 1 before testHarness.processElement(new StreamRecord<>(\"1\"), 2); (L207/206) is being enqueued to input gate 2.\nI would guess that\n// to avoid starvation, if the input selection is ALL and availableInputsMask is not ALL,\n// always try to check and set the availability of another input\nif (inputSelectionHandler.shouldSetAvailableForAnotherInput()) {\n\tfullCheckAndSetAvailable();\n}\n\ncheck from StreamMultipleInputProcessor#selectNextReadingInputIndex is currently not tested.\nThe intention behind this test is:\n\nto have a long (just as well could be infinite) backlog of records to process on one of the inputs\nintroduce the availability change on the second input, and make sure it's checked/respected (instead of hot looping on the first input)\nalso throw in a third not selected input just to spice things a little bit\n\nWhy did you have to change this test?", "url": "https://github.com/apache/flink/pull/13351#discussion_r499521881", "createdAt": "2020-10-05T11:15:38Z", "author": {"login": "pnowojski"}, "path": "flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/tasks/StreamTaskMultipleInputSelectiveReadingTest.java", "diffHunk": "@@ -197,22 +198,21 @@ public void testInputStarvation() throws Exception {\n \t\t\ttestHarness.processElement(new StreamRecord<>(\"3\"), 1);\n \t\t\ttestHarness.processElement(new StreamRecord<>(\"4\"), 1);\n \n-\t\t\ttestHarness.processSingleStep();\n \t\t\texpectedOutput.add(new StreamRecord<>(\"[2]: 1\"));\n-\t\t\ttestHarness.processSingleStep();\n \t\t\texpectedOutput.add(new StreamRecord<>(\"[2]: 2\"));\n-\t\t\tassertThat(testHarness.getOutput(), contains(expectedOutput.toArray()));\n+\t\t\ttestHarness.processAll();\n+\t\t\tassertEquals(expectedOutput, new ArrayList<>(testHarness.getOutput()).subList(0, expectedOutput.size()));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjQ2MTY5OA=="}, "originalCommit": {"oid": "ce1ddf0120e722f172916745adac6c4829cbae29"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTY5NDU1MQ==", "bodyText": "Thanks for the explanation.\nThe problem was that the output didn't match the expected one. The reason was that requesting partitions now also requires a mailbox step.\nAfter your comment I replaced the fix with:\nfor (int i = 0; i < testHarness.inputGates.length; i++) {\n\ttestHarness.processSingleStep();\n}\n\nDoes it look good to you?", "url": "https://github.com/apache/flink/pull/13351#discussion_r499694551", "createdAt": "2020-10-05T15:41:49Z", "author": {"login": "rkhachatryan"}, "path": "flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/tasks/StreamTaskMultipleInputSelectiveReadingTest.java", "diffHunk": "@@ -197,22 +198,21 @@ public void testInputStarvation() throws Exception {\n \t\t\ttestHarness.processElement(new StreamRecord<>(\"3\"), 1);\n \t\t\ttestHarness.processElement(new StreamRecord<>(\"4\"), 1);\n \n-\t\t\ttestHarness.processSingleStep();\n \t\t\texpectedOutput.add(new StreamRecord<>(\"[2]: 1\"));\n-\t\t\ttestHarness.processSingleStep();\n \t\t\texpectedOutput.add(new StreamRecord<>(\"[2]: 2\"));\n-\t\t\tassertThat(testHarness.getOutput(), contains(expectedOutput.toArray()));\n+\t\t\ttestHarness.processAll();\n+\t\t\tassertEquals(expectedOutput, new ArrayList<>(testHarness.getOutput()).subList(0, expectedOutput.size()));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjQ2MTY5OA=="}, "originalCommit": {"oid": "ce1ddf0120e722f172916745adac6c4829cbae29"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTc2MDQ5NA==", "bodyText": "Hmmm, maybe replace it with more future proof condition? sth like:\nboolean noElementFromInputGate3 = true;\nsteps = 0;\nwhile (noElementFromInputGate3 && steps++ < 100 && testHarness.procesSingleStep()) {\n  noElementFromInputGate3 = ... // check for presence of `new StreamRecord<>(\"[3]: 1\"))`\n}\n\n?\nOr maybe #13351 (comment) ?", "url": "https://github.com/apache/flink/pull/13351#discussion_r499760494", "createdAt": "2020-10-05T17:31:41Z", "author": {"login": "pnowojski"}, "path": "flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/tasks/StreamTaskMultipleInputSelectiveReadingTest.java", "diffHunk": "@@ -197,22 +198,21 @@ public void testInputStarvation() throws Exception {\n \t\t\ttestHarness.processElement(new StreamRecord<>(\"3\"), 1);\n \t\t\ttestHarness.processElement(new StreamRecord<>(\"4\"), 1);\n \n-\t\t\ttestHarness.processSingleStep();\n \t\t\texpectedOutput.add(new StreamRecord<>(\"[2]: 1\"));\n-\t\t\ttestHarness.processSingleStep();\n \t\t\texpectedOutput.add(new StreamRecord<>(\"[2]: 2\"));\n-\t\t\tassertThat(testHarness.getOutput(), contains(expectedOutput.toArray()));\n+\t\t\ttestHarness.processAll();\n+\t\t\tassertEquals(expectedOutput, new ArrayList<>(testHarness.getOutput()).subList(0, expectedOutput.size()));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjQ2MTY5OA=="}, "originalCommit": {"oid": "ce1ddf0120e722f172916745adac6c4829cbae29"}, "originalPosition": 39}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEwODYzMjI0OnYy", "diffSide": "RIGHT", "path": "flink-streaming-java/src/test/java/org/apache/flink/streaming/util/StreamTaskUtil.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwOToyNjowNlrOHZkV9A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQxMjowNTo1MFrOHZp2GQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjU3MTg5Mg==", "bodyText": "An alternative way to propagate the exception would be:\n\t\t\t\ttaskInvocation.exceptionally(e -> {\n\t\t\t\t\tthrow new AssertionError(\"Task has stopped\", e);\n\t\t\t\t});\n\nIt depends if you want the test to fail through assertions or through unexpected exception.", "url": "https://github.com/apache/flink/pull/13351#discussion_r496571892", "createdAt": "2020-09-29T09:26:06Z", "author": {"login": "AHeise"}, "path": "flink-streaming-java/src/test/java/org/apache/flink/streaming/util/StreamTaskUtil.java", "diffHunk": "@@ -29,9 +30,10 @@\n  */\n public class StreamTaskUtil {\n \n-\tpublic static void waitTaskIsRunning(StreamTask<?, ?> task, CompletableFuture<Void> taskInvocation) throws InterruptedException {\n+\tpublic static void waitTaskIsRunning(StreamTask<?, ?> task, CompletableFuture<Void> taskInvocation) throws InterruptedException, ExecutionException {\n \t\twhile (!task.isRunning()) {\n \t\t\tif (taskInvocation.isDone()) {\n+\t\t\t\ttaskInvocation.get();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0f5f4780b86654a878548a7f69b9474e7e3f3618"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjY2MjA0MQ==", "bodyText": "I'd prefer not to wrap it as it's more straightforward to me both in code and in the logs.", "url": "https://github.com/apache/flink/pull/13351#discussion_r496662041", "createdAt": "2020-09-29T12:05:50Z", "author": {"login": "rkhachatryan"}, "path": "flink-streaming-java/src/test/java/org/apache/flink/streaming/util/StreamTaskUtil.java", "diffHunk": "@@ -29,9 +30,10 @@\n  */\n public class StreamTaskUtil {\n \n-\tpublic static void waitTaskIsRunning(StreamTask<?, ?> task, CompletableFuture<Void> taskInvocation) throws InterruptedException {\n+\tpublic static void waitTaskIsRunning(StreamTask<?, ?> task, CompletableFuture<Void> taskInvocation) throws InterruptedException, ExecutionException {\n \t\twhile (!task.isRunning()) {\n \t\t\tif (taskInvocation.isDone()) {\n+\t\t\t\ttaskInvocation.get();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjU3MTg5Mg=="}, "originalCommit": {"oid": "0f5f4780b86654a878548a7f69b9474e7e3f3618"}, "originalPosition": 16}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyOTEyMDIwOnYy", "diffSide": "RIGHT", "path": "flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/tasks/StreamTaskMultipleInputSelectiveReadingTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxNzozNDoyN1rOHcnDXg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxOTozMDowOVrOHcqyeA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTc2MjAxNA==", "bodyText": "Would it work if replaced with testHarness.processAll ?", "url": "https://github.com/apache/flink/pull/13351#discussion_r499762014", "createdAt": "2020-10-05T17:34:27Z", "author": {"login": "pnowojski"}, "path": "flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/tasks/StreamTaskMultipleInputSelectiveReadingTest.java", "diffHunk": "@@ -183,6 +183,9 @@ public void testInputStarvation() throws Exception {\n \t\t\t\t\t.setupOutputForSingletonOperatorChain(new TestInputStarvationMultipleInputOperatorFactory())\n \t\t\t\t\t.build()) {\n \n+\t\t\tfor (int i = 0; i < testHarness.inputGates.length; i++) {\n+\t\t\t\ttestHarness.processSingleStep();\n+\t\t\t}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e8c03361de3a583a8da862bcf3839de214c5cb83"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTgyMzIyNA==", "bodyText": "Yes! \ud83d\ude04", "url": "https://github.com/apache/flink/pull/13351#discussion_r499823224", "createdAt": "2020-10-05T19:30:09Z", "author": {"login": "rkhachatryan"}, "path": "flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/tasks/StreamTaskMultipleInputSelectiveReadingTest.java", "diffHunk": "@@ -183,6 +183,9 @@ public void testInputStarvation() throws Exception {\n \t\t\t\t\t.setupOutputForSingletonOperatorChain(new TestInputStarvationMultipleInputOperatorFactory())\n \t\t\t\t\t.build()) {\n \n+\t\t\tfor (int i = 0; i < testHarness.inputGates.length; i++) {\n+\t\t\t\ttestHarness.processSingleStep();\n+\t\t\t}", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTc2MjAxNA=="}, "originalCommit": {"oid": "e8c03361de3a583a8da862bcf3839de214c5cb83"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE0NTQ5MTM4OnYy", "diffSide": "RIGHT", "path": "flink-quickstart/flink-quickstart-java/src/main/resources/archetype-resources/${project.build.directory}/classes/log4j2.properties", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOVQwOTo0MDozOVrOHfCp7A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOVQwOTo1NzoxMlrOHfDL0w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjMxMTQwNA==", "bodyText": "Something is wrong in this commit. I'm suspecting that you hit some weird maven/intellij interaction. Could you please delete all log4js properties?", "url": "https://github.com/apache/flink/pull/13351#discussion_r502311404", "createdAt": "2020-10-09T09:40:39Z", "author": {"login": "AHeise"}, "path": "flink-quickstart/flink-quickstart-java/src/main/resources/archetype-resources/${project.build.directory}/classes/log4j2.properties", "diffHunk": "@@ -0,0 +1,25 @@\n+################################################################################\n+#  Licensed to the Apache Software Foundation (ASF) under one\n+#  or more contributor license agreements.  See the NOTICE file\n+#  distributed with this work for additional information\n+#  regarding copyright ownership.  The ASF licenses this file\n+#  to you under the Apache License, Version 2.0 (the\n+#  \"License\"); you may not use this file except in compliance\n+#  with the License.  You may obtain a copy of the License at\n+#\n+#      http://www.apache.org/licenses/LICENSE-2.0\n+#\n+#  Unless required by applicable law or agreed to in writing, software\n+#  distributed under the License is distributed on an \"AS IS\" BASIS,\n+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+#  See the License for the specific language governing permissions and\n+# limitations under the License.\n+################################################################################\n+\n+rootLogger.level = INFO", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "99123f77a898f80033fc4b70e6a588c9ef61425f"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjMyMDA4Mw==", "bodyText": "Yes, Intellij added these files and I accidentally committed.\nFixed.", "url": "https://github.com/apache/flink/pull/13351#discussion_r502320083", "createdAt": "2020-10-09T09:57:12Z", "author": {"login": "rkhachatryan"}, "path": "flink-quickstart/flink-quickstart-java/src/main/resources/archetype-resources/${project.build.directory}/classes/log4j2.properties", "diffHunk": "@@ -0,0 +1,25 @@\n+################################################################################\n+#  Licensed to the Apache Software Foundation (ASF) under one\n+#  or more contributor license agreements.  See the NOTICE file\n+#  distributed with this work for additional information\n+#  regarding copyright ownership.  The ASF licenses this file\n+#  to you under the Apache License, Version 2.0 (the\n+#  \"License\"); you may not use this file except in compliance\n+#  with the License.  You may obtain a copy of the License at\n+#\n+#      http://www.apache.org/licenses/LICENSE-2.0\n+#\n+#  Unless required by applicable law or agreed to in writing, software\n+#  distributed under the License is distributed on an \"AS IS\" BASIS,\n+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+#  See the License for the specific language governing permissions and\n+# limitations under the License.\n+################################################################################\n+\n+rootLogger.level = INFO", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjMxMTQwNA=="}, "originalCommit": {"oid": "99123f77a898f80033fc4b70e6a588c9ef61425f"}, "originalPosition": 19}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 445, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}