{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTA3NTc0NTU3", "number": 13729, "title": "[FLINK-19644][hive] Support read latest partition of Hive table in temporal join", "bodyText": "What is the purpose of the change\n\nCurrently, the hive table only support load all partitions in temporal join, this pull request support read latest partition in user configured order.\n\nBrief change log\n\nintroduce PartitionFetcher to fetch partitions\nintroduce PartitionReader that's responsible to read records from partitions\nrefactor HiveLookupFunction.java\ncode refactor, move static function to utils calss\n\nVerifying this change\nAdd ITCase and unit test to cover this change.\nDoes this pull request potentially affect one of the following parts:\n\nDependencies (does it add or upgrade a dependency): (no)\nThe public API, i.e., is any changed class annotated with @Public(Evolving): (no)\nThe serializers: (no)\nThe runtime per-record code paths (performance sensitive): ( no)\nAnything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)\nThe S3 file system connector: (no)\n\nDocumentation\n\nDoes this pull request introduce a new feature? (yes)\nIf yes, how is the feature documented? (docs)", "createdAt": "2020-10-21T14:01:24Z", "url": "https://github.com/apache/flink/pull/13729", "merged": true, "mergeCommit": {"oid": "09386f27b66c86e8148ae8d25d72e3c2ac552362"}, "closed": true, "closedAt": "2020-11-07T06:23:57Z", "author": {"login": "leonardBang"}, "timelineItems": {"totalCount": 31, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdU5OfMABqjM5MDY4NDY3MjY=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdaFf3kAFqTUyNTYzMTUyMw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "659b57b42486d3b79197653fec933ce42766388e", "author": {"user": {"login": "leonardBang", "name": "Leonard Xu"}}, "url": "https://github.com/apache/flink/commit/659b57b42486d3b79197653fec933ce42766388e", "committedDate": "2020-10-21T14:00:31Z", "message": "[FLINK-19644][hive] Support read specific partition of Hive table in temporal join"}, "afterCommit": {"oid": "f028c8d52466fbd6fd5a9cbf3ed85dcbc65fd2ac", "author": {"user": {"login": "leonardBang", "name": "Leonard Xu"}}, "url": "https://github.com/apache/flink/commit/f028c8d52466fbd6fd5a9cbf3ed85dcbc65fd2ac", "committedDate": "2020-10-22T03:15:52Z", "message": "[FLINK-19644][hive] Support read specific partition of Hive table in temporal join"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE0MzM1NzM5", "url": "https://github.com/apache/flink/pull/13729#pullrequestreview-514335739", "createdAt": "2020-10-22T03:32:30Z", "commit": {"oid": "f028c8d52466fbd6fd5a9cbf3ed85dcbc65fd2ac"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQwMzozMjozMVrOHmPVCg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQwMzozMjozMVrOHmPVCg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg1OTA4Mg==", "bodyText": "Why do this? Why not Filesystem also have this lookup capability?", "url": "https://github.com/apache/flink/pull/13729#discussion_r509859082", "createdAt": "2020-10-22T03:32:31Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveLookupFunction.java", "diffHunk": "@@ -47,29 +59,46 @@\n import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n+import java.util.Properties;\n import java.util.stream.Collectors;\n import java.util.stream.IntStream;\n \n+import static org.apache.flink.connectors.hive.HiveTableFactory.LOOKUP_JOIN_CACHE_TTL;\n+import static org.apache.flink.connectors.hive.HiveTableFactory.LOOKUP_JOIN_PARTITION;\n+import static org.apache.flink.table.catalog.hive.util.HivePartitionUtils.getPartitionByPartitionSpecs;\n+import static org.apache.flink.table.catalog.hive.util.HivePartitionUtils.getTableProps;\n+import static org.apache.flink.table.catalog.hive.util.HivePartitionUtils.toHiveTablePartition;\n+import static org.apache.flink.table.catalog.hive.util.HivePartitionUtils.validateAndParsePartitionSpecs;\n+import static org.apache.flink.util.Preconditions.checkNotNull;\n+\n /**\n- * Lookup table function for filesystem connector tables.\n+ * Lookup table function for Hive connector tables.\n  */\n-public class FileSystemLookupFunction<T extends InputSplit> extends TableFunction<RowData> {\n+public class HiveLookupFunction<T extends InputSplit> extends TableFunction<RowData> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f028c8d52466fbd6fd5a9cbf3ed85dcbc65fd2ac"}, "originalPosition": 58}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE0NTQ5MDky", "url": "https://github.com/apache/flink/pull/13729#pullrequestreview-514549092", "createdAt": "2020-10-22T09:48:13Z", "commit": {"oid": "f028c8d52466fbd6fd5a9cbf3ed85dcbc65fd2ac"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQwOTo0ODoxM1rOHmZr1w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQwOTo0ODoxM1rOHmZr1w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDAyODc1OQ==", "bodyText": "Can we add a copy(Partitions) method to HiveTableInputFormat? Then we don't need move so many logicals.", "url": "https://github.com/apache/flink/pull/13729#discussion_r510028759", "createdAt": "2020-10-22T09:48:13Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveLookupFunction.java", "diffHunk": "@@ -134,9 +188,48 @@ public void eval(Object... values) {\n \t\t}\n \t}\n \n-\t@VisibleForTesting\n-\tpublic Duration getCacheTTL() {\n-\t\treturn cacheTTL;\n+\tprivate HiveTableInputFormat getHiveTableInputFormat() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f028c8d52466fbd6fd5a9cbf3ed85dcbc65fd2ac"}, "originalPosition": 165}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE0NTUxNDI1", "url": "https://github.com/apache/flink/pull/13729#pullrequestreview-514551425", "createdAt": "2020-10-22T09:51:05Z", "commit": {"oid": "f028c8d52466fbd6fd5a9cbf3ed85dcbc65fd2ac"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "f028c8d52466fbd6fd5a9cbf3ed85dcbc65fd2ac", "author": {"user": {"login": "leonardBang", "name": "Leonard Xu"}}, "url": "https://github.com/apache/flink/commit/f028c8d52466fbd6fd5a9cbf3ed85dcbc65fd2ac", "committedDate": "2020-10-22T03:15:52Z", "message": "[FLINK-19644][hive] Support read specific partition of Hive table in temporal join"}, "afterCommit": {"oid": "bc53ef3b7595acbc03bdaf1e01360cb802ff5d52", "author": {"user": {"login": "leonardBang", "name": "Leonard Xu"}}, "url": "https://github.com/apache/flink/commit/bc53ef3b7595acbc03bdaf1e01360cb802ff5d52", "committedDate": "2020-11-03T16:58:19Z", "message": "[FLINK-19644][hive] Support read latest partition of Hive table in temporal join"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "bc53ef3b7595acbc03bdaf1e01360cb802ff5d52", "author": {"user": {"login": "leonardBang", "name": "Leonard Xu"}}, "url": "https://github.com/apache/flink/commit/bc53ef3b7595acbc03bdaf1e01360cb802ff5d52", "committedDate": "2020-11-03T16:58:19Z", "message": "[FLINK-19644][hive] Support read latest partition of Hive table in temporal join"}, "afterCommit": {"oid": "9124af4f501b2afeaefa94621a82612846d98783", "author": {"user": {"login": "leonardBang", "name": "Leonard Xu"}}, "url": "https://github.com/apache/flink/commit/9124af4f501b2afeaefa94621a82612846d98783", "committedDate": "2020-11-04T06:38:06Z", "message": "[FLINK-19644][hive] Support read latest partition of Hive table in temporal join"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIzMTEyMTE4", "url": "https://github.com/apache/flink/pull/13729#pullrequestreview-523112118", "createdAt": "2020-11-04T07:56:00Z", "commit": {"oid": "9124af4f501b2afeaefa94621a82612846d98783"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQwNzo1NjowMFrOHtMiUg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQwNzo1NjoxNVrOHtMitA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE1MzM2Mg==", "bodyText": "Revert name changing.", "url": "https://github.com/apache/flink/pull/13729#discussion_r517153362", "createdAt": "2020-11-04T07:56:00Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FilesystemLookupFunction.java", "diffHunk": "@@ -51,125 +45,113 @@\n import java.util.stream.IntStream;\n \n /**\n- * Lookup table function for filesystem connector tables.\n+ * Lookup function for filesystem connector tables.\n+ *\n+ * <p>The hive connector and filesystem connector share read/write files code.\n+ * Currently, only this function only used in hive connector.\n  */\n-public class FileSystemLookupFunction<T extends InputSplit> extends TableFunction<RowData> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9124af4f501b2afeaefa94621a82612846d98783"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE1MzQ2MA==", "bodyText": "lookupCols is never be used", "url": "https://github.com/apache/flink/pull/13729#discussion_r517153460", "createdAt": "2020-11-04T07:56:15Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FilesystemLookupFunction.java", "diffHunk": "@@ -51,125 +45,113 @@\n import java.util.stream.IntStream;\n \n /**\n- * Lookup table function for filesystem connector tables.\n+ * Lookup function for filesystem connector tables.\n+ *\n+ * <p>The hive connector and filesystem connector share read/write files code.\n+ * Currently, only this function only used in hive connector.\n  */\n-public class FileSystemLookupFunction<T extends InputSplit> extends TableFunction<RowData> {\n-\n-\tprivate static final long serialVersionUID = 1L;\n+public class FilesystemLookupFunction<P> extends TableFunction<RowData> {\n \n-\tprivate static final Logger LOG = LoggerFactory.getLogger(FileSystemLookupFunction.class);\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(FilesystemLookupFunction.class);\n \n \t// the max number of retries before throwing exception, in case of failure to load the table into cache\n \tprivate static final int MAX_RETRIES = 3;\n \t// interval between retries\n \tprivate static final Duration RETRY_INTERVAL = Duration.ofSeconds(10);\n \n-\tprivate final InputFormat<RowData, T> inputFormat;\n-\t// names and types of the records returned by the input format\n-\tprivate final String[] producedNames;\n-\tprivate final DataType[] producedTypes;\n+\tprivate final PartitionFetcher<P> partitionFetcher;\n+\tprivate final PartitionReader<P, RowData> partitionReader;\n+\tprivate final int[] lookupCols;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9124af4f501b2afeaefa94621a82612846d98783"}, "originalPosition": 57}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIzMTE1NDU5", "url": "https://github.com/apache/flink/pull/13729#pullrequestreview-523115459", "createdAt": "2020-11-04T08:01:27Z", "commit": {"oid": "9124af4f501b2afeaefa94621a82612846d98783"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQwODowMToyN1rOHtMsgA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQwODowMToyN1rOHtMsgA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE1NTk2OA==", "bodyText": "Can you use int[] lookupKeys?", "url": "https://github.com/apache/flink/pull/13729#discussion_r517155968", "createdAt": "2020-11-04T08:01:27Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FilesystemLookupFunction.java", "diffHunk": "@@ -51,125 +45,113 @@\n import java.util.stream.IntStream;\n \n /**\n- * Lookup table function for filesystem connector tables.\n+ * Lookup function for filesystem connector tables.\n+ *\n+ * <p>The hive connector and filesystem connector share read/write files code.\n+ * Currently, only this function only used in hive connector.\n  */\n-public class FileSystemLookupFunction<T extends InputSplit> extends TableFunction<RowData> {\n-\n-\tprivate static final long serialVersionUID = 1L;\n+public class FilesystemLookupFunction<P> extends TableFunction<RowData> {\n \n-\tprivate static final Logger LOG = LoggerFactory.getLogger(FileSystemLookupFunction.class);\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(FilesystemLookupFunction.class);\n \n \t// the max number of retries before throwing exception, in case of failure to load the table into cache\n \tprivate static final int MAX_RETRIES = 3;\n \t// interval between retries\n \tprivate static final Duration RETRY_INTERVAL = Duration.ofSeconds(10);\n \n-\tprivate final InputFormat<RowData, T> inputFormat;\n-\t// names and types of the records returned by the input format\n-\tprivate final String[] producedNames;\n-\tprivate final DataType[] producedTypes;\n+\tprivate final PartitionFetcher<P> partitionFetcher;\n+\tprivate final PartitionReader<P, RowData> partitionReader;\n+\tprivate final int[] lookupCols;\n+\tprivate final RowData.FieldGetter[] lookupFieldGetters;\n \tprivate final Duration cacheTTL;\n+\tprivate final TypeSerializer<RowData> serializer;\n+\tprivate final DataType[] fieldTypes;\n+\tprivate final String[] fieldNames;\n \n-\t// indices of lookup columns in the record returned by input format\n-\tprivate final int[] lookupCols;\n-\t// use Row as key for the cache\n-\tprivate transient Map<Row, List<RowData>> cache;\n+\t// cache for lookup data\n+\tprivate transient Map<RowData, List<RowData>> cache;\n \t// timestamp when cache expires\n \tprivate transient long nextLoadTime;\n-\t// serializer to copy RowData\n-\tprivate transient TypeSerializer<RowData> serializer;\n-\t// converters to convert data from internal to external in order to generate keys for the cache\n-\tprivate final DataFormatConverter[] converters;\n \n-\tpublic FileSystemLookupFunction(\n-\t\t\tInputFormat<RowData, T> inputFormat,\n+\tpublic FilesystemLookupFunction(\n+\t\t\tPartitionFetcher<P> partitionFetcher,\n+\t\t\tPartitionReader<P, RowData> partitionReader,\n+\t\t\tDataType[] fieldTypes,\n+\t\t\tString[] fieldNames,\n \t\t\tString[] lookupKeys,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9124af4f501b2afeaefa94621a82612846d98783"}, "originalPosition": 84}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIzMTE3MzI4", "url": "https://github.com/apache/flink/pull/13729#pullrequestreview-523117328", "createdAt": "2020-11-04T08:04:28Z", "commit": {"oid": "9124af4f501b2afeaefa94621a82612846d98783"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQwODowNDoyOFrOHtMyJg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQwODowNDoyOFrOHtMyJg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE1NzQxNA==", "bodyText": "I think it is better to change to reloadInterval?", "url": "https://github.com/apache/flink/pull/13729#discussion_r517157414", "createdAt": "2020-11-04T08:04:28Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FilesystemLookupFunction.java", "diffHunk": "@@ -51,125 +45,113 @@\n import java.util.stream.IntStream;\n \n /**\n- * Lookup table function for filesystem connector tables.\n+ * Lookup function for filesystem connector tables.\n+ *\n+ * <p>The hive connector and filesystem connector share read/write files code.\n+ * Currently, only this function only used in hive connector.\n  */\n-public class FileSystemLookupFunction<T extends InputSplit> extends TableFunction<RowData> {\n-\n-\tprivate static final long serialVersionUID = 1L;\n+public class FilesystemLookupFunction<P> extends TableFunction<RowData> {\n \n-\tprivate static final Logger LOG = LoggerFactory.getLogger(FileSystemLookupFunction.class);\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(FilesystemLookupFunction.class);\n \n \t// the max number of retries before throwing exception, in case of failure to load the table into cache\n \tprivate static final int MAX_RETRIES = 3;\n \t// interval between retries\n \tprivate static final Duration RETRY_INTERVAL = Duration.ofSeconds(10);\n \n-\tprivate final InputFormat<RowData, T> inputFormat;\n-\t// names and types of the records returned by the input format\n-\tprivate final String[] producedNames;\n-\tprivate final DataType[] producedTypes;\n+\tprivate final PartitionFetcher<P> partitionFetcher;\n+\tprivate final PartitionReader<P, RowData> partitionReader;\n+\tprivate final int[] lookupCols;\n+\tprivate final RowData.FieldGetter[] lookupFieldGetters;\n \tprivate final Duration cacheTTL;\n+\tprivate final TypeSerializer<RowData> serializer;\n+\tprivate final DataType[] fieldTypes;\n+\tprivate final String[] fieldNames;\n \n-\t// indices of lookup columns in the record returned by input format\n-\tprivate final int[] lookupCols;\n-\t// use Row as key for the cache\n-\tprivate transient Map<Row, List<RowData>> cache;\n+\t// cache for lookup data\n+\tprivate transient Map<RowData, List<RowData>> cache;\n \t// timestamp when cache expires\n \tprivate transient long nextLoadTime;\n-\t// serializer to copy RowData\n-\tprivate transient TypeSerializer<RowData> serializer;\n-\t// converters to convert data from internal to external in order to generate keys for the cache\n-\tprivate final DataFormatConverter[] converters;\n \n-\tpublic FileSystemLookupFunction(\n-\t\t\tInputFormat<RowData, T> inputFormat,\n+\tpublic FilesystemLookupFunction(\n+\t\t\tPartitionFetcher<P> partitionFetcher,\n+\t\t\tPartitionReader<P, RowData> partitionReader,\n+\t\t\tDataType[] fieldTypes,\n+\t\t\tString[] fieldNames,\n \t\t\tString[] lookupKeys,\n-\t\t\tString[] producedNames,\n-\t\t\tDataType[] producedTypes,\n \t\t\tDuration cacheTTL) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9124af4f501b2afeaefa94621a82612846d98783"}, "originalPosition": 87}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIzMTE3NzEw", "url": "https://github.com/apache/flink/pull/13729#pullrequestreview-523117710", "createdAt": "2020-11-04T08:05:03Z", "commit": {"oid": "9124af4f501b2afeaefa94621a82612846d98783"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQwODowNTowM1rOHtMzQw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQwODowNTowM1rOHtMzQw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE1NzY5OQ==", "bodyText": "I think it is better to remove this default value", "url": "https://github.com/apache/flink/pull/13729#discussion_r517157699", "createdAt": "2020-11-04T08:05:03Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemOptions.java", "diffHunk": "@@ -74,24 +74,37 @@\n \t\t\t\t\t\t\t\" NOTES: Please make sure that each partition/file should be written\" +\n \t\t\t\t\t\t\t\" atomically, otherwise the reader may get incomplete data.\");\n \n+\tpublic static final ConfigOption<String> STREAMING_SOURCE_PARTITION_INCLUDE =\n+\t\t\tkey(\"streaming-source.partition.include\")\n+\t\t\t\t\t.stringType()\n+\t\t\t\t\t.defaultValue(\"all\")\n+\t\t\t\t\t.withDescription(\"Option to set the partitions to read, the supported values \" +\n+\t\t\t\t\t\t\t\"are \\\"all\\\" and \\\"latest\\\",\" +\n+\t\t\t\t\t\t\t\" the \\\"all\\\" means read all partitions; the \\\"latest\\\" means read latest \" +\n+\t\t\t\t\t\t\t\"partition in order of streaming-source.partition.order, the \\\"latest\\\" only works\" +\n+\t\t\t\t\t\t\t\" when the streaming hive source table used as temporal table. \" +\n+\t\t\t\t\t\t\t\"By default the option is \\\"all\\\".\\n.\");\n+\n \tpublic static final ConfigOption<Duration> STREAMING_SOURCE_MONITOR_INTERVAL =\n \t\t\tkey(\"streaming-source.monitor-interval\")\n \t\t\t\t\t.durationType()\n \t\t\t\t\t.defaultValue(Duration.ofMinutes(1))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9124af4f501b2afeaefa94621a82612846d98783"}, "originalPosition": 18}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIzMTE4NjYy", "url": "https://github.com/apache/flink/pull/13729#pullrequestreview-523118662", "createdAt": "2020-11-04T08:06:34Z", "commit": {"oid": "9124af4f501b2afeaefa94621a82612846d98783"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQwODowNjozNFrOHtM2IA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQwODowNjozNFrOHtM2IA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE1ODQzMg==", "bodyText": "You can just pass a RowType here", "url": "https://github.com/apache/flink/pull/13729#discussion_r517158432", "createdAt": "2020-11-04T08:06:34Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FilesystemLookupFunction.java", "diffHunk": "@@ -51,125 +45,113 @@\n import java.util.stream.IntStream;\n \n /**\n- * Lookup table function for filesystem connector tables.\n+ * Lookup function for filesystem connector tables.\n+ *\n+ * <p>The hive connector and filesystem connector share read/write files code.\n+ * Currently, only this function only used in hive connector.\n  */\n-public class FileSystemLookupFunction<T extends InputSplit> extends TableFunction<RowData> {\n-\n-\tprivate static final long serialVersionUID = 1L;\n+public class FilesystemLookupFunction<P> extends TableFunction<RowData> {\n \n-\tprivate static final Logger LOG = LoggerFactory.getLogger(FileSystemLookupFunction.class);\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(FilesystemLookupFunction.class);\n \n \t// the max number of retries before throwing exception, in case of failure to load the table into cache\n \tprivate static final int MAX_RETRIES = 3;\n \t// interval between retries\n \tprivate static final Duration RETRY_INTERVAL = Duration.ofSeconds(10);\n \n-\tprivate final InputFormat<RowData, T> inputFormat;\n-\t// names and types of the records returned by the input format\n-\tprivate final String[] producedNames;\n-\tprivate final DataType[] producedTypes;\n+\tprivate final PartitionFetcher<P> partitionFetcher;\n+\tprivate final PartitionReader<P, RowData> partitionReader;\n+\tprivate final int[] lookupCols;\n+\tprivate final RowData.FieldGetter[] lookupFieldGetters;\n \tprivate final Duration cacheTTL;\n+\tprivate final TypeSerializer<RowData> serializer;\n+\tprivate final DataType[] fieldTypes;\n+\tprivate final String[] fieldNames;\n \n-\t// indices of lookup columns in the record returned by input format\n-\tprivate final int[] lookupCols;\n-\t// use Row as key for the cache\n-\tprivate transient Map<Row, List<RowData>> cache;\n+\t// cache for lookup data\n+\tprivate transient Map<RowData, List<RowData>> cache;\n \t// timestamp when cache expires\n \tprivate transient long nextLoadTime;\n-\t// serializer to copy RowData\n-\tprivate transient TypeSerializer<RowData> serializer;\n-\t// converters to convert data from internal to external in order to generate keys for the cache\n-\tprivate final DataFormatConverter[] converters;\n \n-\tpublic FileSystemLookupFunction(\n-\t\t\tInputFormat<RowData, T> inputFormat,\n+\tpublic FilesystemLookupFunction(\n+\t\t\tPartitionFetcher<P> partitionFetcher,\n+\t\t\tPartitionReader<P, RowData> partitionReader,\n+\t\t\tDataType[] fieldTypes,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9124af4f501b2afeaefa94621a82612846d98783"}, "originalPosition": 82}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIzMTE4ODY4", "url": "https://github.com/apache/flink/pull/13729#pullrequestreview-523118868", "createdAt": "2020-11-04T08:06:54Z", "commit": {"oid": "9124af4f501b2afeaefa94621a82612846d98783"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQwODowNjo1NFrOHtM2xA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQwODowNjo1NFrOHtM2xA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE1ODU5Ng==", "bodyText": "Use InternalSerializers.create", "url": "https://github.com/apache/flink/pull/13729#discussion_r517158596", "createdAt": "2020-11-04T08:06:54Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FilesystemLookupFunction.java", "diffHunk": "@@ -51,125 +45,113 @@\n import java.util.stream.IntStream;\n \n /**\n- * Lookup table function for filesystem connector tables.\n+ * Lookup function for filesystem connector tables.\n+ *\n+ * <p>The hive connector and filesystem connector share read/write files code.\n+ * Currently, only this function only used in hive connector.\n  */\n-public class FileSystemLookupFunction<T extends InputSplit> extends TableFunction<RowData> {\n-\n-\tprivate static final long serialVersionUID = 1L;\n+public class FilesystemLookupFunction<P> extends TableFunction<RowData> {\n \n-\tprivate static final Logger LOG = LoggerFactory.getLogger(FileSystemLookupFunction.class);\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(FilesystemLookupFunction.class);\n \n \t// the max number of retries before throwing exception, in case of failure to load the table into cache\n \tprivate static final int MAX_RETRIES = 3;\n \t// interval between retries\n \tprivate static final Duration RETRY_INTERVAL = Duration.ofSeconds(10);\n \n-\tprivate final InputFormat<RowData, T> inputFormat;\n-\t// names and types of the records returned by the input format\n-\tprivate final String[] producedNames;\n-\tprivate final DataType[] producedTypes;\n+\tprivate final PartitionFetcher<P> partitionFetcher;\n+\tprivate final PartitionReader<P, RowData> partitionReader;\n+\tprivate final int[] lookupCols;\n+\tprivate final RowData.FieldGetter[] lookupFieldGetters;\n \tprivate final Duration cacheTTL;\n+\tprivate final TypeSerializer<RowData> serializer;\n+\tprivate final DataType[] fieldTypes;\n+\tprivate final String[] fieldNames;\n \n-\t// indices of lookup columns in the record returned by input format\n-\tprivate final int[] lookupCols;\n-\t// use Row as key for the cache\n-\tprivate transient Map<Row, List<RowData>> cache;\n+\t// cache for lookup data\n+\tprivate transient Map<RowData, List<RowData>> cache;\n \t// timestamp when cache expires\n \tprivate transient long nextLoadTime;\n-\t// serializer to copy RowData\n-\tprivate transient TypeSerializer<RowData> serializer;\n-\t// converters to convert data from internal to external in order to generate keys for the cache\n-\tprivate final DataFormatConverter[] converters;\n \n-\tpublic FileSystemLookupFunction(\n-\t\t\tInputFormat<RowData, T> inputFormat,\n+\tpublic FilesystemLookupFunction(\n+\t\t\tPartitionFetcher<P> partitionFetcher,\n+\t\t\tPartitionReader<P, RowData> partitionReader,\n+\t\t\tDataType[] fieldTypes,\n+\t\t\tString[] fieldNames,\n \t\t\tString[] lookupKeys,\n-\t\t\tString[] producedNames,\n-\t\t\tDataType[] producedTypes,\n \t\t\tDuration cacheTTL) {\n-\t\tlookupCols = new int[lookupKeys.length];\n-\t\tconverters = new DataFormatConverter[lookupKeys.length];\n-\t\tMap<String, Integer> nameToIndex = IntStream.range(0, producedNames.length).boxed().collect(\n-\t\t\t\tCollectors.toMap(i -> producedNames[i], i -> i));\n+\t\tthis.cacheTTL = cacheTTL;\n+\t\tthis.partitionFetcher = partitionFetcher;\n+\t\tthis.partitionReader = partitionReader;\n+\t\tthis.fieldTypes = fieldTypes;\n+\t\tthis.fieldNames = fieldNames;\n+\t\tthis.lookupCols = new int[lookupKeys.length];\n+\t\tthis.lookupFieldGetters = new RowData.FieldGetter[lookupKeys.length];\n+\t\tMap<String, Integer> nameToIndex = IntStream.range(0, fieldNames.length).boxed().collect(\n+\t\t\t\tCollectors.toMap(i -> fieldNames[i], i -> i));\n \t\tfor (int i = 0; i < lookupKeys.length; i++) {\n \t\t\tInteger index = nameToIndex.get(lookupKeys[i]);\n \t\t\tPreconditions.checkArgument(index != null, \"Lookup keys %s not selected\", Arrays.toString(lookupKeys));\n-\t\t\tconverters[i] = DataFormatConverters.getConverterForDataType(producedTypes[index]);\n+\t\t\tlookupFieldGetters[i] = RowData.createFieldGetter(fieldTypes[index].getLogicalType(), index);\n \t\t\tlookupCols[i] = index;\n \t\t}\n-\t\tthis.inputFormat = inputFormat;\n-\t\tthis.producedNames = producedNames;\n-\t\tthis.producedTypes = producedTypes;\n-\t\tthis.cacheTTL = cacheTTL;\n-\t}\n-\n-\t@Override\n-\tpublic TypeInformation<RowData> getResultType() {\n-\t\treturn InternalTypeInfo.ofFields(\n-\t\t\t\tArrays.stream(producedTypes).map(DataType::getLogicalType).toArray(LogicalType[]::new),\n-\t\t\t\tproducedNames);\n+\t\tthis.serializer = getResultType().createSerializer(new ExecutionConfig());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9124af4f501b2afeaefa94621a82612846d98783"}, "originalPosition": 119}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIzMTIwMTgx", "url": "https://github.com/apache/flink/pull/13729#pullrequestreview-523120181", "createdAt": "2020-11-04T08:09:02Z", "commit": {"oid": "9124af4f501b2afeaefa94621a82612846d98783"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQwODowOTowM1rOHtM6oA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQwODowOTowM1rOHtM6oA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE1OTU4NA==", "bodyText": "I think it is better to keep Context here, and we don't need open and close.\n(All open and close are the same)", "url": "https://github.com/apache/flink/pull/13729#discussion_r517159584", "createdAt": "2020-11-04T08:09:03Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/PartitionFetcher.java", "diffHunk": "@@ -0,0 +1,90 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+\n+import java.io.Serializable;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.function.Supplier;\n+\n+/**\n+ * Fetcher to fetch the suitable partitions of a filesystem table.\n+ *\n+ * @param <P> The type to describe a partition.\n+ */\n+@Internal\n+public interface PartitionFetcher<P> extends Serializable {\n+\n+\t/**\n+\t * Open the resources of the fetcher.\n+\t */\n+\tvoid open() throws Exception;\n+\n+\t/**\n+\t * Fetch the suitable partitions, call this method should guarantee the fetcher has opened.\n+\t */\n+\tList<P> fetch() throws Exception;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9124af4f501b2afeaefa94621a82612846d98783"}, "originalPosition": 45}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIzMTM1ODQ3", "url": "https://github.com/apache/flink/pull/13729#pullrequestreview-523135847", "createdAt": "2020-11-04T08:31:26Z", "commit": {"oid": "9124af4f501b2afeaefa94621a82612846d98783"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQwODozMToyNlrOHtNpeA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQwODozMToyNlrOHtNpeA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE3MTU3Ng==", "bodyText": "Just @Nullable OUT read() is OK", "url": "https://github.com/apache/flink/pull/13729#discussion_r517171576", "createdAt": "2020-11-04T08:31:26Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/PartitionReader.java", "diffHunk": "@@ -0,0 +1,75 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.annotation.Internal;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.List;\n+\n+/**\n+ * Reader that reads all records from given partitions.\n+ *\n+ *<P>This reader should only use in non-parallel instance, e.g. : used by lookup function.\n+ *\n+ * @param <P> The type of partition.\n+ * @param <OUT> The type of returned record.\n+ */\n+@Internal\n+public interface PartitionReader<P, OUT> extends Closeable, Serializable {\n+\n+\t/**\n+\t * Opens the reader with given partitions.\n+\t * @throws IOException\n+\t */\n+\tvoid open(List<P> partitions) throws IOException;\n+\n+\t/**\n+\t * Method used to check the partitions have read finished or not.\n+\t *\n+\t *<p>When this method is called, the reader it guaranteed to be opened.\n+\t *\n+\t * @return True if the partitions has read finished.\n+\t * @throws IOException\n+\t */\n+\tboolean hasNext() throws IOException;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9124af4f501b2afeaefa94621a82612846d98783"}, "originalPosition": 53}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIzMTM3MTI5", "url": "https://github.com/apache/flink/pull/13729#pullrequestreview-523137129", "createdAt": "2020-11-04T08:33:20Z", "commit": {"oid": "9124af4f501b2afeaefa94621a82612846d98783"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQwODozMzoyMFrOHtNtNg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQwODozMzoyMFrOHtNtNg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE3MjUzNA==", "bodyText": "Remove this comment, ide will warn this.", "url": "https://github.com/apache/flink/pull/13729#discussion_r517172534", "createdAt": "2020-11-04T08:33:20Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/PartitionReader.java", "diffHunk": "@@ -0,0 +1,75 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.annotation.Internal;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.List;\n+\n+/**\n+ * Reader that reads all records from given partitions.\n+ *\n+ *<P>This reader should only use in non-parallel instance, e.g. : used by lookup function.\n+ *\n+ * @param <P> The type of partition.\n+ * @param <OUT> The type of returned record.\n+ */\n+@Internal\n+public interface PartitionReader<P, OUT> extends Closeable, Serializable {\n+\n+\t/**\n+\t * Opens the reader with given partitions.\n+\t * @throws IOException\n+\t */\n+\tvoid open(List<P> partitions) throws IOException;\n+\n+\t/**\n+\t * Method used to check the partitions have read finished or not.\n+\t *\n+\t *<p>When this method is called, the reader it guaranteed to be opened.\n+\t *\n+\t * @return True if the partitions has read finished.\n+\t * @throws IOException", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9124af4f501b2afeaefa94621a82612846d98783"}, "originalPosition": 51}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIzMTM3Nzc4", "url": "https://github.com/apache/flink/pull/13729#pullrequestreview-523137778", "createdAt": "2020-11-04T08:34:18Z", "commit": {"oid": "9124af4f501b2afeaefa94621a82612846d98783"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQwODozNDoxOFrOHtNvOg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQwODozNDoxOFrOHtNvOg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE3MzA1MA==", "bodyText": "partitionFetcher never null", "url": "https://github.com/apache/flink/pull/13729#discussion_r517173050", "createdAt": "2020-11-04T08:34:18Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FilesystemLookupFunction.java", "diffHunk": "@@ -187,11 +169,34 @@ private void checkCacheReload() {\n \t\t}\n \t}\n \n-\tprivate Row extractKey(RowData row) {\n-\t\tRow key = new Row(lookupCols.length);\n+\tprivate RowData extractLookupKey(RowData row) {\n+\t\tGenericRowData key = new GenericRowData(lookupCols.length);\n \t\tfor (int i = 0; i < lookupCols.length; i++) {\n-\t\t\tkey.setField(i, converters[i].toExternal(row, lookupCols[i]));\n+\t\t\tkey.setField(i, lookupFieldGetters[i].getFieldOrNull(row));\n \t\t}\n \t\treturn key;\n \t}\n+\n+\t@Override\n+\tpublic void close() throws Exception {\n+\t\tif (this.partitionFetcher != null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9124af4f501b2afeaefa94621a82612846d98783"}, "originalPosition": 225}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIzMTU0NTQ2", "url": "https://github.com/apache/flink/pull/13729#pullrequestreview-523154546", "createdAt": "2020-11-04T08:57:14Z", "commit": {"oid": "9124af4f501b2afeaefa94621a82612846d98783"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQwODo1NzoxNFrOHtOjIA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQwODo1NzoxNFrOHtOjIA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE4NjMzNg==", "bodyText": "I think we should support name comparator.", "url": "https://github.com/apache/flink/pull/13729#discussion_r517186336", "createdAt": "2020-11-04T08:57:14Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveLookupTableSource.java", "diffHunk": "@@ -0,0 +1,346 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.configuration.ReadableConfig;\n+import org.apache.flink.connectors.hive.read.HiveInputFormatPartitionReader;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.catalog.hive.client.HiveShim;\n+import org.apache.flink.table.catalog.hive.util.HivePartitionUtils;\n+import org.apache.flink.table.catalog.hive.util.HiveReflectionUtils;\n+import org.apache.flink.table.connector.source.LookupTableSource;\n+import org.apache.flink.table.connector.source.TableFunctionProvider;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.filesystem.FileSystemAllPartitionFetcher;\n+import org.apache.flink.table.filesystem.FileSystemLatestPartitionFetcher;\n+import org.apache.flink.table.filesystem.FileSystemNonPartitionedTableFetcher;\n+import org.apache.flink.table.filesystem.FilesystemLookupFunction;\n+import org.apache.flink.table.filesystem.PartitionFetcher;\n+import org.apache.flink.table.filesystem.PartitionReader;\n+import org.apache.flink.table.filesystem.PartitionTimeExtractor;\n+import org.apache.flink.table.functions.TableFunction;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.metastore.IMetaStoreClient;\n+import org.apache.hadoop.hive.metastore.api.NoSuchObjectException;\n+import org.apache.hadoop.hive.metastore.api.Partition;\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.hive.metastore.api.Table;\n+import org.apache.hadoop.mapred.JobConf;\n+\n+import java.sql.Timestamp;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Properties;\n+import java.util.function.Supplier;\n+\n+import static org.apache.flink.table.filesystem.DefaultPartTimeExtractor.toMills;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.LOOKUP_JOIN_CACHE_TTL;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_CLASS;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_KIND;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_TIMESTAMP_PATTERN;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_CONSUME_START_OFFSET;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_MONITOR_INTERVAL;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_PARTITION_INCLUDE;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_PARTITION_ORDER;\n+import static org.apache.flink.table.utils.PartitionPathUtils.extractPartitionValues;\n+\n+/**\n+ * Hive Table Source that has lookup ability.\n+ *\n+ * <p>Hive Table source has both lookup and continuous read ability, when it acts as continuous read source\n+ * it does not have the lookup ability but can be a temporal table just like other stream sources.\n+ * When it acts as bounded table, it has the lookup ability.\n+ *\n+ * <p>A common user case is use hive dimension table and always lookup the latest partition data, in this case\n+ * hive table source is a continuous read source but currently we implements it by LookupFunction. Because currently\n+ * TableSource can not tell the downstream when the latest partition has been read finished. This is a temporarily\n+ * workaround and will re-implement in the future.\n+ */\n+public class HiveLookupTableSource extends HiveTableSource implements LookupTableSource {\n+\n+\tprivate static final Duration DEFAULT_LOOKUP_MONITOR_INTERVAL = Duration.ofHours(1L);\n+\tprivate final Configuration configuration;\n+\tprivate Duration hiveTableCacheTTL;\n+\n+\tpublic HiveLookupTableSource(\n+\t\t\tJobConf jobConf,\n+\t\t\tReadableConfig flinkConf,\n+\t\t\tObjectPath tablePath,\n+\t\t\tCatalogTable catalogTable) {\n+\t\tsuper(jobConf, flinkConf, tablePath, catalogTable);\n+\t\tthis.configuration = new Configuration();\n+\t\tcatalogTable.getOptions().forEach(configuration::setString);\n+\t\tvalidateLookupConfigurations();\n+\t}\n+\n+\t@Override\n+\tpublic LookupRuntimeProvider getLookupRuntimeProvider(LookupContext context) {\n+\t\treturn TableFunctionProvider.of(getLookupFunction(context.getKeys()));\n+\t}\n+\n+\t@VisibleForTesting\n+\tTableFunction<RowData> getLookupFunction(int[][] keys) {\n+\t\tList<String> keyNames = new ArrayList<>();\n+\t\tTableSchema schema = getTableSchema();\n+\t\tfor (int[] key : keys) {\n+\t\t\tif (key.length > 1) {\n+\t\t\t\tthrow new UnsupportedOperationException(\"Hive lookup can not support nested key now.\");\n+\t\t\t}\n+\t\t\tkeyNames.add(schema.getFieldName(key[0]).get());\n+\t\t}\n+\t\treturn getLookupFunction(keyNames.toArray(new String[0]));\n+\t}\n+\n+\tprivate void validateLookupConfigurations() {\n+\t\tString partitionInclude = configuration.get(STREAMING_SOURCE_PARTITION_INCLUDE);\n+\t\tif (isStreamingSource()) {\n+\t\t\tPreconditions.checkArgument(\n+\t\t\t\t\t!configuration.contains(STREAMING_SOURCE_CONSUME_START_OFFSET),\n+\t\t\t\t\tString.format(\n+\t\t\t\t\t\t\t\"The '%s' is not supported when set '%s' to 'latest'\",\n+\t\t\t\t\t\t\tSTREAMING_SOURCE_CONSUME_START_OFFSET.key(),\n+\t\t\t\t\t\t\tSTREAMING_SOURCE_PARTITION_INCLUDE.key()));\n+\n+\t\t\tDuration monitorInterval = configuration.get(STREAMING_SOURCE_MONITOR_INTERVAL);\n+\t\t\tif (monitorInterval.equals(STREAMING_SOURCE_MONITOR_INTERVAL.defaultValue())) {\n+\t\t\t\tmonitorInterval = DEFAULT_LOOKUP_MONITOR_INTERVAL;\n+\t\t\t}\n+\t\t\tPreconditions.checkArgument(\n+\t\t\t\t\tmonitorInterval.toMillis() >= DEFAULT_LOOKUP_MONITOR_INTERVAL.toMillis(),\n+\t\t\t\t\tString.format(\n+\t\t\t\t\t\t\t\"Currently the value of '%s' is required bigger or equal to default value '%s' \" +\n+\t\t\t\t\t\t\t\t\t\"when set '%s' to 'latest', but actual is '%s'\",\n+\t\t\t\t\t\t\tSTREAMING_SOURCE_MONITOR_INTERVAL.key(),\n+\t\t\t\t\t\t\tDEFAULT_LOOKUP_MONITOR_INTERVAL.toMillis(),\n+\t\t\t\t\t\t\tSTREAMING_SOURCE_PARTITION_INCLUDE.key(),\n+\t\t\t\t\t\t\tmonitorInterval.toMillis())\n+\t\t\t);\n+\n+\t\t\thiveTableCacheTTL = monitorInterval;\n+\t\t} else {\n+\t\t\tPreconditions.checkArgument(\n+\t\t\t\t\t\"all\".equals(partitionInclude),\n+\t\t\t\t\tString.format(\"The only supported %s for lookup is '%s' in batch source,\" +\n+\t\t\t\t\t\t\t\" but actual is '%s'\", STREAMING_SOURCE_PARTITION_INCLUDE.key(), \"all\", partitionInclude));\n+\n+\t\t\thiveTableCacheTTL = configuration.get(LOOKUP_JOIN_CACHE_TTL);\n+\t\t}\n+\t}\n+\n+\tprivate TableFunction<RowData> getLookupFunction(String[] keys) {\n+\n+\t\tfinal String defaultPartitionName = jobConf.get(HiveConf.ConfVars.DEFAULTPARTITIONNAME.varname,\n+\t\t\t\tHiveConf.ConfVars.DEFAULTPARTITIONNAME.defaultStrVal);\n+\n+\t\tPartitionFetcher.Context context = new HiveTablePartitionFetcherContext(\n+\t\t\t\ttablePath,\n+\t\t\t\thiveShim,\n+\t\t\t\tnew JobConfWrapper(jobConf),\n+\t\t\t\tcatalogTable.getPartitionKeys(),\n+\t\t\t\tgetProducedTableSchema().getFieldDataTypes(),\n+\t\t\t\tgetProducedTableSchema().getFieldNames(),\n+\t\t\t\tconfiguration,\n+\t\t\t\tdefaultPartitionName);\n+\n+\t\tPartitionFetcher<HiveTablePartition> partitionFetcher;\n+\t\tif (catalogTable.getPartitionKeys().isEmpty()) {\n+\t\t\t// non-partitioned table\n+\t\t\tpartitionFetcher = new FileSystemNonPartitionedTableFetcher(context);\n+\n+\t\t} else if (isStreamingSource()) {\n+\t\t\t// streaming-read partitioned table\n+\t\t\tpartitionFetcher = new FileSystemLatestPartitionFetcher(context);\n+\n+\t\t} else {\n+\t\t\t// bounded-read partitioned table\n+\t\t\tpartitionFetcher = new FileSystemAllPartitionFetcher(context);\n+\t\t}\n+\n+\t\tPartitionReader<HiveTablePartition, RowData> partitionReader = new HiveInputFormatPartitionReader(\n+\t\t\t\tjobConf,\n+\t\t\t\thiveVersion,\n+\t\t\t\ttablePath,\n+\t\t\t\tgetProducedTableSchema().getFieldDataTypes(),\n+\t\t\t\tgetProducedTableSchema().getFieldNames(),\n+\t\t\t\tcatalogTable.getPartitionKeys(),\n+\t\t\t\tprojectedFields,\n+\t\t\t\tflinkConf.get(HiveOptions.TABLE_EXEC_HIVE_FALLBACK_MAPRED_READER));\n+\n+\t\treturn new FilesystemLookupFunction<>(\n+\t\t\t\tpartitionFetcher,\n+\t\t\t\tpartitionReader,\n+\t\t\t\tgetProducedTableSchema().getFieldDataTypes(),\n+\t\t\t\tgetProducedTableSchema().getFieldNames(),\n+\t\t\t\tkeys,\n+\t\t\t\thiveTableCacheTTL);\n+\t}\n+\n+\t/**\n+\t * PartitionFetcher.Context for {@link HiveTablePartition}.\n+\t */\n+\tstatic class HiveTablePartitionFetcherContext implements PartitionFetcher.Context<HiveTablePartition> {\n+\n+\t\tprivate static final long serialVersionUID = 1L;\n+\t\tprivate final ObjectPath tablePath;\n+\t\tprivate final HiveShim hiveShim;\n+\t\tprivate final JobConfWrapper confWrapper;\n+\t\tprivate final List<String> partitionKeys;\n+\t\tprivate final DataType[] fieldTypes;\n+\t\tprivate final String[] fieldNames;\n+\t\tprivate final Configuration configuration;\n+\t\tprivate final String defaultPartitionName;\n+\n+\t\tprivate transient IMetaStoreClient metaStoreClient;\n+\t\tprivate transient StorageDescriptor tableSd;\n+\t\tprivate transient Properties tableProps;\n+\t\tprivate transient PartitionTimeExtractor extractor;\n+\t\tprivate transient ConsumeOrder consumeOrder;\n+\t\tprivate transient Path tableLocation;\n+\t\tprivate transient Table table;\n+\t\tprivate transient FileSystem fs;\n+\n+\t\tpublic HiveTablePartitionFetcherContext(\n+\t\t\t\tObjectPath tablePath,\n+\t\t\t\tHiveShim hiveShim,\n+\t\t\t\tJobConfWrapper confWrapper,\n+\t\t\t\tList<String> partitionKeys,\n+\t\t\t\tDataType[] fieldTypes,\n+\t\t\t\tString[] fieldNames,\n+\t\t\t\tConfiguration configuration,\n+\t\t\t\tString defaultPartitionName) {\n+\t\t\tthis.tablePath = tablePath;\n+\t\t\tthis.hiveShim = hiveShim;\n+\t\t\tthis.confWrapper = confWrapper;\n+\t\t\tthis.partitionKeys = partitionKeys;\n+\t\t\tthis.fieldTypes = fieldTypes;\n+\t\t\tthis.fieldNames = fieldNames;\n+\t\t\tthis.configuration = configuration;\n+\t\t\tthis.defaultPartitionName = defaultPartitionName;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void initialize() throws Exception {\n+\t\t\tmetaStoreClient = hiveShim.getHiveMetastoreClient(new HiveConf(confWrapper.conf(), HiveConf.class));\n+\t\t\ttable = metaStoreClient.getTable(tablePath.getDatabaseName(), tablePath.getObjectName());\n+\t\t\ttableSd = table.getSd();\n+\t\t\ttableProps = HiveReflectionUtils.getTableMetadata(hiveShim, table);\n+\n+\t\t\tString consumeOrderStr = configuration.get(STREAMING_SOURCE_PARTITION_ORDER);\n+\t\t\tconsumeOrder = ConsumeOrder.getConsumeOrder(consumeOrderStr);\n+\t\t\tString extractorKind = configuration.get(PARTITION_TIME_EXTRACTOR_KIND);\n+\t\t\tString extractorClass = configuration.get(PARTITION_TIME_EXTRACTOR_CLASS);\n+\t\t\tString extractorPattern = configuration.get(PARTITION_TIME_EXTRACTOR_TIMESTAMP_PATTERN);\n+\n+\t\t\textractor = PartitionTimeExtractor.create(\n+\t\t\t\t\tThread.currentThread().getContextClassLoader(),\n+\t\t\t\t\textractorKind,\n+\t\t\t\t\textractorClass,\n+\t\t\t\t\textractorPattern);\n+\t\t\ttableLocation = new Path(table.getSd().getLocation());\n+\t\t\tfs = tableLocation.getFileSystem(confWrapper.conf());\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic Optional<HiveTablePartition> getPartition(List<String> partValues) throws Exception {\n+\t\t\tif (partitionKeys.isEmpty()) {\n+\t\t\t\treturn Optional.empty();\n+\t\t\t}\n+\t\t\ttry {\n+\t\t\t\tPartition partition = metaStoreClient.getPartition(\n+\t\t\t\t\t\ttablePath.getDatabaseName(),\n+\t\t\t\t\t\ttablePath.getObjectName(),\n+\t\t\t\t\t\tpartValues);\n+\t\t\t\tHiveTablePartition hiveTablePartition = HivePartitionUtils.toHiveTablePartition(\n+\t\t\t\t\t\tpartitionKeys,\n+\t\t\t\t\t\tfieldNames,\n+\t\t\t\t\t\tfieldTypes,\n+\t\t\t\t\t\thiveShim,\n+\t\t\t\t\t\ttableProps,\n+\t\t\t\t\t\tdefaultPartitionName, partition);\n+\t\t\t\treturn Optional.of(hiveTablePartition);\n+\t\t\t} catch (NoSuchObjectException e) {\n+\t\t\t\treturn Optional.empty();\n+\t\t\t}\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic Optional<HiveTablePartition> getNonPartitionedTablePartition() {\n+\t\t\tif (partitionKeys.isEmpty()) {\n+\t\t\t\treturn Optional.of(new HiveTablePartition(tableSd, tableProps));\n+\t\t\t}\n+\t\t\treturn Optional.empty();\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic List<Tuple2<List<String>, Long>> getAllPartValueToTimeList() {\n+\t\t\tFileStatus[] statuses = HivePartitionUtils.getFileStatusRecurse(tableLocation, partitionKeys.size(), fs);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9124af4f501b2afeaefa94621a82612846d98783"}, "originalPosition": 306}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIzMTU0NzAw", "url": "https://github.com/apache/flink/pull/13729#pullrequestreview-523154700", "createdAt": "2020-11-04T08:57:25Z", "commit": {"oid": "9124af4f501b2afeaefa94621a82612846d98783"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQwODo1NzoyNVrOHtOjkw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQwODo1NzoyNVrOHtOjkw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE4NjQ1MQ==", "bodyText": "extractTimestamp is a inner method", "url": "https://github.com/apache/flink/pull/13729#discussion_r517186451", "createdAt": "2020-11-04T08:57:25Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveLookupTableSource.java", "diffHunk": "@@ -0,0 +1,346 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.configuration.ReadableConfig;\n+import org.apache.flink.connectors.hive.read.HiveInputFormatPartitionReader;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.catalog.hive.client.HiveShim;\n+import org.apache.flink.table.catalog.hive.util.HivePartitionUtils;\n+import org.apache.flink.table.catalog.hive.util.HiveReflectionUtils;\n+import org.apache.flink.table.connector.source.LookupTableSource;\n+import org.apache.flink.table.connector.source.TableFunctionProvider;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.filesystem.FileSystemAllPartitionFetcher;\n+import org.apache.flink.table.filesystem.FileSystemLatestPartitionFetcher;\n+import org.apache.flink.table.filesystem.FileSystemNonPartitionedTableFetcher;\n+import org.apache.flink.table.filesystem.FilesystemLookupFunction;\n+import org.apache.flink.table.filesystem.PartitionFetcher;\n+import org.apache.flink.table.filesystem.PartitionReader;\n+import org.apache.flink.table.filesystem.PartitionTimeExtractor;\n+import org.apache.flink.table.functions.TableFunction;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.metastore.IMetaStoreClient;\n+import org.apache.hadoop.hive.metastore.api.NoSuchObjectException;\n+import org.apache.hadoop.hive.metastore.api.Partition;\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.hive.metastore.api.Table;\n+import org.apache.hadoop.mapred.JobConf;\n+\n+import java.sql.Timestamp;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Properties;\n+import java.util.function.Supplier;\n+\n+import static org.apache.flink.table.filesystem.DefaultPartTimeExtractor.toMills;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.LOOKUP_JOIN_CACHE_TTL;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_CLASS;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_KIND;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_TIMESTAMP_PATTERN;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_CONSUME_START_OFFSET;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_MONITOR_INTERVAL;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_PARTITION_INCLUDE;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_PARTITION_ORDER;\n+import static org.apache.flink.table.utils.PartitionPathUtils.extractPartitionValues;\n+\n+/**\n+ * Hive Table Source that has lookup ability.\n+ *\n+ * <p>Hive Table source has both lookup and continuous read ability, when it acts as continuous read source\n+ * it does not have the lookup ability but can be a temporal table just like other stream sources.\n+ * When it acts as bounded table, it has the lookup ability.\n+ *\n+ * <p>A common user case is use hive dimension table and always lookup the latest partition data, in this case\n+ * hive table source is a continuous read source but currently we implements it by LookupFunction. Because currently\n+ * TableSource can not tell the downstream when the latest partition has been read finished. This is a temporarily\n+ * workaround and will re-implement in the future.\n+ */\n+public class HiveLookupTableSource extends HiveTableSource implements LookupTableSource {\n+\n+\tprivate static final Duration DEFAULT_LOOKUP_MONITOR_INTERVAL = Duration.ofHours(1L);\n+\tprivate final Configuration configuration;\n+\tprivate Duration hiveTableCacheTTL;\n+\n+\tpublic HiveLookupTableSource(\n+\t\t\tJobConf jobConf,\n+\t\t\tReadableConfig flinkConf,\n+\t\t\tObjectPath tablePath,\n+\t\t\tCatalogTable catalogTable) {\n+\t\tsuper(jobConf, flinkConf, tablePath, catalogTable);\n+\t\tthis.configuration = new Configuration();\n+\t\tcatalogTable.getOptions().forEach(configuration::setString);\n+\t\tvalidateLookupConfigurations();\n+\t}\n+\n+\t@Override\n+\tpublic LookupRuntimeProvider getLookupRuntimeProvider(LookupContext context) {\n+\t\treturn TableFunctionProvider.of(getLookupFunction(context.getKeys()));\n+\t}\n+\n+\t@VisibleForTesting\n+\tTableFunction<RowData> getLookupFunction(int[][] keys) {\n+\t\tList<String> keyNames = new ArrayList<>();\n+\t\tTableSchema schema = getTableSchema();\n+\t\tfor (int[] key : keys) {\n+\t\t\tif (key.length > 1) {\n+\t\t\t\tthrow new UnsupportedOperationException(\"Hive lookup can not support nested key now.\");\n+\t\t\t}\n+\t\t\tkeyNames.add(schema.getFieldName(key[0]).get());\n+\t\t}\n+\t\treturn getLookupFunction(keyNames.toArray(new String[0]));\n+\t}\n+\n+\tprivate void validateLookupConfigurations() {\n+\t\tString partitionInclude = configuration.get(STREAMING_SOURCE_PARTITION_INCLUDE);\n+\t\tif (isStreamingSource()) {\n+\t\t\tPreconditions.checkArgument(\n+\t\t\t\t\t!configuration.contains(STREAMING_SOURCE_CONSUME_START_OFFSET),\n+\t\t\t\t\tString.format(\n+\t\t\t\t\t\t\t\"The '%s' is not supported when set '%s' to 'latest'\",\n+\t\t\t\t\t\t\tSTREAMING_SOURCE_CONSUME_START_OFFSET.key(),\n+\t\t\t\t\t\t\tSTREAMING_SOURCE_PARTITION_INCLUDE.key()));\n+\n+\t\t\tDuration monitorInterval = configuration.get(STREAMING_SOURCE_MONITOR_INTERVAL);\n+\t\t\tif (monitorInterval.equals(STREAMING_SOURCE_MONITOR_INTERVAL.defaultValue())) {\n+\t\t\t\tmonitorInterval = DEFAULT_LOOKUP_MONITOR_INTERVAL;\n+\t\t\t}\n+\t\t\tPreconditions.checkArgument(\n+\t\t\t\t\tmonitorInterval.toMillis() >= DEFAULT_LOOKUP_MONITOR_INTERVAL.toMillis(),\n+\t\t\t\t\tString.format(\n+\t\t\t\t\t\t\t\"Currently the value of '%s' is required bigger or equal to default value '%s' \" +\n+\t\t\t\t\t\t\t\t\t\"when set '%s' to 'latest', but actual is '%s'\",\n+\t\t\t\t\t\t\tSTREAMING_SOURCE_MONITOR_INTERVAL.key(),\n+\t\t\t\t\t\t\tDEFAULT_LOOKUP_MONITOR_INTERVAL.toMillis(),\n+\t\t\t\t\t\t\tSTREAMING_SOURCE_PARTITION_INCLUDE.key(),\n+\t\t\t\t\t\t\tmonitorInterval.toMillis())\n+\t\t\t);\n+\n+\t\t\thiveTableCacheTTL = monitorInterval;\n+\t\t} else {\n+\t\t\tPreconditions.checkArgument(\n+\t\t\t\t\t\"all\".equals(partitionInclude),\n+\t\t\t\t\tString.format(\"The only supported %s for lookup is '%s' in batch source,\" +\n+\t\t\t\t\t\t\t\" but actual is '%s'\", STREAMING_SOURCE_PARTITION_INCLUDE.key(), \"all\", partitionInclude));\n+\n+\t\t\thiveTableCacheTTL = configuration.get(LOOKUP_JOIN_CACHE_TTL);\n+\t\t}\n+\t}\n+\n+\tprivate TableFunction<RowData> getLookupFunction(String[] keys) {\n+\n+\t\tfinal String defaultPartitionName = jobConf.get(HiveConf.ConfVars.DEFAULTPARTITIONNAME.varname,\n+\t\t\t\tHiveConf.ConfVars.DEFAULTPARTITIONNAME.defaultStrVal);\n+\n+\t\tPartitionFetcher.Context context = new HiveTablePartitionFetcherContext(\n+\t\t\t\ttablePath,\n+\t\t\t\thiveShim,\n+\t\t\t\tnew JobConfWrapper(jobConf),\n+\t\t\t\tcatalogTable.getPartitionKeys(),\n+\t\t\t\tgetProducedTableSchema().getFieldDataTypes(),\n+\t\t\t\tgetProducedTableSchema().getFieldNames(),\n+\t\t\t\tconfiguration,\n+\t\t\t\tdefaultPartitionName);\n+\n+\t\tPartitionFetcher<HiveTablePartition> partitionFetcher;\n+\t\tif (catalogTable.getPartitionKeys().isEmpty()) {\n+\t\t\t// non-partitioned table\n+\t\t\tpartitionFetcher = new FileSystemNonPartitionedTableFetcher(context);\n+\n+\t\t} else if (isStreamingSource()) {\n+\t\t\t// streaming-read partitioned table\n+\t\t\tpartitionFetcher = new FileSystemLatestPartitionFetcher(context);\n+\n+\t\t} else {\n+\t\t\t// bounded-read partitioned table\n+\t\t\tpartitionFetcher = new FileSystemAllPartitionFetcher(context);\n+\t\t}\n+\n+\t\tPartitionReader<HiveTablePartition, RowData> partitionReader = new HiveInputFormatPartitionReader(\n+\t\t\t\tjobConf,\n+\t\t\t\thiveVersion,\n+\t\t\t\ttablePath,\n+\t\t\t\tgetProducedTableSchema().getFieldDataTypes(),\n+\t\t\t\tgetProducedTableSchema().getFieldNames(),\n+\t\t\t\tcatalogTable.getPartitionKeys(),\n+\t\t\t\tprojectedFields,\n+\t\t\t\tflinkConf.get(HiveOptions.TABLE_EXEC_HIVE_FALLBACK_MAPRED_READER));\n+\n+\t\treturn new FilesystemLookupFunction<>(\n+\t\t\t\tpartitionFetcher,\n+\t\t\t\tpartitionReader,\n+\t\t\t\tgetProducedTableSchema().getFieldDataTypes(),\n+\t\t\t\tgetProducedTableSchema().getFieldNames(),\n+\t\t\t\tkeys,\n+\t\t\t\thiveTableCacheTTL);\n+\t}\n+\n+\t/**\n+\t * PartitionFetcher.Context for {@link HiveTablePartition}.\n+\t */\n+\tstatic class HiveTablePartitionFetcherContext implements PartitionFetcher.Context<HiveTablePartition> {\n+\n+\t\tprivate static final long serialVersionUID = 1L;\n+\t\tprivate final ObjectPath tablePath;\n+\t\tprivate final HiveShim hiveShim;\n+\t\tprivate final JobConfWrapper confWrapper;\n+\t\tprivate final List<String> partitionKeys;\n+\t\tprivate final DataType[] fieldTypes;\n+\t\tprivate final String[] fieldNames;\n+\t\tprivate final Configuration configuration;\n+\t\tprivate final String defaultPartitionName;\n+\n+\t\tprivate transient IMetaStoreClient metaStoreClient;\n+\t\tprivate transient StorageDescriptor tableSd;\n+\t\tprivate transient Properties tableProps;\n+\t\tprivate transient PartitionTimeExtractor extractor;\n+\t\tprivate transient ConsumeOrder consumeOrder;\n+\t\tprivate transient Path tableLocation;\n+\t\tprivate transient Table table;\n+\t\tprivate transient FileSystem fs;\n+\n+\t\tpublic HiveTablePartitionFetcherContext(\n+\t\t\t\tObjectPath tablePath,\n+\t\t\t\tHiveShim hiveShim,\n+\t\t\t\tJobConfWrapper confWrapper,\n+\t\t\t\tList<String> partitionKeys,\n+\t\t\t\tDataType[] fieldTypes,\n+\t\t\t\tString[] fieldNames,\n+\t\t\t\tConfiguration configuration,\n+\t\t\t\tString defaultPartitionName) {\n+\t\t\tthis.tablePath = tablePath;\n+\t\t\tthis.hiveShim = hiveShim;\n+\t\t\tthis.confWrapper = confWrapper;\n+\t\t\tthis.partitionKeys = partitionKeys;\n+\t\t\tthis.fieldTypes = fieldTypes;\n+\t\t\tthis.fieldNames = fieldNames;\n+\t\t\tthis.configuration = configuration;\n+\t\t\tthis.defaultPartitionName = defaultPartitionName;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void initialize() throws Exception {\n+\t\t\tmetaStoreClient = hiveShim.getHiveMetastoreClient(new HiveConf(confWrapper.conf(), HiveConf.class));\n+\t\t\ttable = metaStoreClient.getTable(tablePath.getDatabaseName(), tablePath.getObjectName());\n+\t\t\ttableSd = table.getSd();\n+\t\t\ttableProps = HiveReflectionUtils.getTableMetadata(hiveShim, table);\n+\n+\t\t\tString consumeOrderStr = configuration.get(STREAMING_SOURCE_PARTITION_ORDER);\n+\t\t\tconsumeOrder = ConsumeOrder.getConsumeOrder(consumeOrderStr);\n+\t\t\tString extractorKind = configuration.get(PARTITION_TIME_EXTRACTOR_KIND);\n+\t\t\tString extractorClass = configuration.get(PARTITION_TIME_EXTRACTOR_CLASS);\n+\t\t\tString extractorPattern = configuration.get(PARTITION_TIME_EXTRACTOR_TIMESTAMP_PATTERN);\n+\n+\t\t\textractor = PartitionTimeExtractor.create(\n+\t\t\t\t\tThread.currentThread().getContextClassLoader(),\n+\t\t\t\t\textractorKind,\n+\t\t\t\t\textractorClass,\n+\t\t\t\t\textractorPattern);\n+\t\t\ttableLocation = new Path(table.getSd().getLocation());\n+\t\t\tfs = tableLocation.getFileSystem(confWrapper.conf());\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic Optional<HiveTablePartition> getPartition(List<String> partValues) throws Exception {\n+\t\t\tif (partitionKeys.isEmpty()) {\n+\t\t\t\treturn Optional.empty();\n+\t\t\t}\n+\t\t\ttry {\n+\t\t\t\tPartition partition = metaStoreClient.getPartition(\n+\t\t\t\t\t\ttablePath.getDatabaseName(),\n+\t\t\t\t\t\ttablePath.getObjectName(),\n+\t\t\t\t\t\tpartValues);\n+\t\t\t\tHiveTablePartition hiveTablePartition = HivePartitionUtils.toHiveTablePartition(\n+\t\t\t\t\t\tpartitionKeys,\n+\t\t\t\t\t\tfieldNames,\n+\t\t\t\t\t\tfieldTypes,\n+\t\t\t\t\t\thiveShim,\n+\t\t\t\t\t\ttableProps,\n+\t\t\t\t\t\tdefaultPartitionName, partition);\n+\t\t\t\treturn Optional.of(hiveTablePartition);\n+\t\t\t} catch (NoSuchObjectException e) {\n+\t\t\t\treturn Optional.empty();\n+\t\t\t}\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic Optional<HiveTablePartition> getNonPartitionedTablePartition() {\n+\t\t\tif (partitionKeys.isEmpty()) {\n+\t\t\t\treturn Optional.of(new HiveTablePartition(tableSd, tableProps));\n+\t\t\t}\n+\t\t\treturn Optional.empty();\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic List<Tuple2<List<String>, Long>> getAllPartValueToTimeList() {\n+\t\t\tFileStatus[] statuses = HivePartitionUtils.getFileStatusRecurse(tableLocation, partitionKeys.size(), fs);\n+\t\t\tList<Tuple2<List<String>, Long>> partValueList = new ArrayList<>();\n+\t\t\tfor (FileStatus status : statuses) {\n+\t\t\t\tList<String> partValues = extractPartitionValues(\n+\t\t\t\t\t\tnew org.apache.flink.core.fs.Path(status.getPath().toString()));\n+\t\t\t\tlong timestamp = extractTimestamp(\n+\t\t\t\t\t\tpartitionKeys,\n+\t\t\t\t\t\tpartValues,\n+\t\t\t\t\t\t// to UTC millisecond.\n+\t\t\t\t\t\t() -> TimestampData.fromTimestamp(\n+\t\t\t\t\t\t\t\tnew Timestamp(status.getModificationTime())).getMillisecond());\n+\t\t\t\tpartValueList.add(new Tuple2<>(partValues, timestamp));\n+\t\t\t}\n+\n+\t\t\treturn partValueList;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic long extractTimestamp(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9124af4f501b2afeaefa94621a82612846d98783"}, "originalPosition": 324}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIzODgwNTY5", "url": "https://github.com/apache/flink/pull/13729#pullrequestreview-523880569", "createdAt": "2020-11-05T02:23:06Z", "commit": {"oid": "13202dd2477bfb3730e2cb039311b1a0784368dd"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQwMjoyMzowNlrOHtw5Jw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQwMjoyMzowNlrOHtw5Jw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzc0OTAzMQ==", "bodyText": "Please implement Comparable", "url": "https://github.com/apache/flink/pull/13729#discussion_r517749031", "createdAt": "2020-11-05T02:23:06Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveLookupTableSource.java", "diffHunk": "@@ -0,0 +1,377 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.configuration.ReadableConfig;\n+import org.apache.flink.connectors.hive.read.HiveInputFormatPartitionReader;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.catalog.hive.client.HiveShim;\n+import org.apache.flink.table.catalog.hive.util.HivePartitionUtils;\n+import org.apache.flink.table.catalog.hive.util.HiveReflectionUtils;\n+import org.apache.flink.table.connector.source.LookupTableSource;\n+import org.apache.flink.table.connector.source.TableFunctionProvider;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.filesystem.FileSystemLookupFunction;\n+import org.apache.flink.table.filesystem.PartitionFetcher;\n+import org.apache.flink.table.filesystem.PartitionReader;\n+import org.apache.flink.table.filesystem.PartitionTimeExtractor;\n+import org.apache.flink.table.functions.TableFunction;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.metastore.IMetaStoreClient;\n+import org.apache.hadoop.hive.metastore.api.NoSuchObjectException;\n+import org.apache.hadoop.hive.metastore.api.Partition;\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.hive.metastore.api.Table;\n+import org.apache.hadoop.mapred.JobConf;\n+\n+import java.sql.Timestamp;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Properties;\n+\n+import static org.apache.flink.table.filesystem.DefaultPartTimeExtractor.toMills;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.LOOKUP_JOIN_CACHE_TTL;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_CLASS;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_KIND;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_TIMESTAMP_PATTERN;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_CONSUME_START_OFFSET;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_MONITOR_INTERVAL;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_PARTITION_INCLUDE;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_PARTITION_ORDER;\n+import static org.apache.flink.table.utils.PartitionPathUtils.extractPartitionValues;\n+\n+/**\n+ * Hive Table Source that has lookup ability.\n+ *\n+ * <p>Hive Table source has both lookup and continuous read ability, when it acts as continuous read source\n+ * it does not have the lookup ability but can be a temporal table just like other stream sources.\n+ * When it acts as bounded table, it has the lookup ability.\n+ *\n+ * <p>A common user case is use hive dimension table and always lookup the latest partition data, in this case\n+ * hive table source is a continuous read source but currently we implements it by LookupFunction. Because currently\n+ * TableSource can not tell the downstream when the latest partition has been read finished. This is a temporarily\n+ * workaround and will re-implement in the future.\n+ */\n+public class HiveLookupTableSource extends HiveTableSource implements LookupTableSource {\n+\n+\tprivate static final Duration DEFAULT_LOOKUP_MONITOR_INTERVAL = Duration.ofHours(1L);\n+\tprivate final Configuration configuration;\n+\tprivate Duration hiveTableReloadInterval;\n+\n+\tpublic HiveLookupTableSource(\n+\t\t\tJobConf jobConf,\n+\t\t\tReadableConfig flinkConf,\n+\t\t\tObjectPath tablePath,\n+\t\t\tCatalogTable catalogTable) {\n+\t\tsuper(jobConf, flinkConf, tablePath, catalogTable);\n+\t\tthis.configuration = new Configuration();\n+\t\tcatalogTable.getOptions().forEach(configuration::setString);\n+\t\tvalidateLookupConfigurations();\n+\t}\n+\n+\t@Override\n+\tpublic LookupRuntimeProvider getLookupRuntimeProvider(LookupContext context) {\n+\t\treturn TableFunctionProvider.of(getLookupFunction(context.getKeys()));\n+\t}\n+\n+\t@VisibleForTesting\n+\tTableFunction<RowData> getLookupFunction(int[][] keys) {\n+\t\tint[] keyIndices = new int[keys.length];\n+\t\tint i = 0;\n+\t\tfor (int[] key : keys) {\n+\t\t\tif (key.length > 1) {\n+\t\t\t\tthrow new UnsupportedOperationException(\"Hive lookup can not support nested key now.\");\n+\t\t\t}\n+\t\t\tkeyIndices[i] = key[0];\n+\t\t\ti++;\n+\t\t}\n+\t\treturn getLookupFunction(keyIndices);\n+\t}\n+\n+\tprivate void validateLookupConfigurations() {\n+\t\tString partitionInclude = configuration.get(STREAMING_SOURCE_PARTITION_INCLUDE);\n+\t\tif (isStreamingSource()) {\n+\t\t\tPreconditions.checkArgument(\n+\t\t\t\t\t!configuration.contains(STREAMING_SOURCE_CONSUME_START_OFFSET),\n+\t\t\t\t\tString.format(\n+\t\t\t\t\t\t\t\"The '%s' is not supported when set '%s' to 'latest'\",\n+\t\t\t\t\t\t\tSTREAMING_SOURCE_CONSUME_START_OFFSET.key(),\n+\t\t\t\t\t\t\tSTREAMING_SOURCE_PARTITION_INCLUDE.key()));\n+\n+\t\t\tDuration monitorInterval = configuration.get(STREAMING_SOURCE_MONITOR_INTERVAL) == null\n+\t\t\t\t\t? DEFAULT_LOOKUP_MONITOR_INTERVAL\n+\t\t\t\t\t: configuration.get(STREAMING_SOURCE_MONITOR_INTERVAL);\n+\t\t\tPreconditions.checkArgument(\n+\t\t\t\t\tmonitorInterval.toMillis() >= DEFAULT_LOOKUP_MONITOR_INTERVAL.toMillis(),\n+\t\t\t\t\tString.format(\n+\t\t\t\t\t\t\t\"Currently the value of '%s' is required bigger or equal to default value '%s' \" +\n+\t\t\t\t\t\t\t\t\t\"when set '%s' to 'latest', but actual is '%s'\",\n+\t\t\t\t\t\t\tSTREAMING_SOURCE_MONITOR_INTERVAL.key(),\n+\t\t\t\t\t\t\tDEFAULT_LOOKUP_MONITOR_INTERVAL.toMillis(),\n+\t\t\t\t\t\t\tSTREAMING_SOURCE_PARTITION_INCLUDE.key(),\n+\t\t\t\t\t\t\tmonitorInterval.toMillis())\n+\t\t\t);\n+\n+\t\t\thiveTableReloadInterval = monitorInterval;\n+\t\t} else {\n+\t\t\tPreconditions.checkArgument(\n+\t\t\t\t\t\"all\".equals(partitionInclude),\n+\t\t\t\t\tString.format(\"The only supported %s for lookup is '%s' in batch source,\" +\n+\t\t\t\t\t\t\t\" but actual is '%s'\", STREAMING_SOURCE_PARTITION_INCLUDE.key(), \"all\", partitionInclude));\n+\n+\t\t\thiveTableReloadInterval = configuration.get(LOOKUP_JOIN_CACHE_TTL);\n+\t\t}\n+\t}\n+\n+\tprivate TableFunction<RowData> getLookupFunction(int[] keys) {\n+\n+\t\tfinal String defaultPartitionName = jobConf.get(HiveConf.ConfVars.DEFAULTPARTITIONNAME.varname,\n+\t\t\t\tHiveConf.ConfVars.DEFAULTPARTITIONNAME.defaultStrVal);\n+\n+\t\tPartitionFetcher.Context<HiveTablePartition> fetcherContext = new HiveTablePartitionFetcherContext(\n+\t\t\t\ttablePath,\n+\t\t\t\thiveShim,\n+\t\t\t\tnew JobConfWrapper(jobConf),\n+\t\t\t\tcatalogTable.getPartitionKeys(),\n+\t\t\t\tgetProducedTableSchema().getFieldDataTypes(),\n+\t\t\t\tgetProducedTableSchema().getFieldNames(),\n+\t\t\t\tconfiguration,\n+\t\t\t\tdefaultPartitionName);\n+\n+\t\tPartitionFetcher<HiveTablePartition> partitionFetcher;\n+\t\tif (catalogTable.getPartitionKeys().isEmpty()) {\n+\t\t\t// non-partitioned table, the fetcher fetches the partition which represents the given table.\n+\t\t\tpartitionFetcher = context -> {\n+\t\t\t\tList<HiveTablePartition> partValueList = new ArrayList<>();\n+\t\t\t\tcontext.getNonPartitionedTablePartition().ifPresent(partValueList::add);\n+\t\t\t\treturn partValueList;\n+\t\t\t};\n+\t\t} else if (isStreamingSource()) {\n+\t\t\t// streaming-read partitioned table, the fetcher fetches the latest partition of the given table.\n+\t\t\tpartitionFetcher = context -> {\n+\t\t\t\tList<HiveTablePartition> partValueList = new ArrayList<>();\n+\t\t\t\tList<Tuple2<List<String>, Comparable>> allPartValueToTime = context.getPartValueWithComparableObjList();\n+\t\t\t\t// fetch latest partitions for partitioned table\n+\t\t\t\tif (allPartValueToTime.size() > 0) {\n+\t\t\t\t\t//sort in desc order\n+\t\t\t\t\tallPartValueToTime.sort((o1, o2) -> o2.f1.compareTo(o1.f1));\n+\t\t\t\t\tTuple2<List<String>, Comparable> maxPartition = allPartValueToTime.get(0);\n+\t\t\t\t\tcontext.getPartition(maxPartition.f0).ifPresent(partValueList::add);\n+\t\t\t\t} else {\n+\t\t\t\t\tthrow new IllegalArgumentException(\n+\t\t\t\t\t\t\tString.format(\"At least one partition is required when set '%s' to 'latest' in temporal join,\" +\n+\t\t\t\t\t\t\t\t\t\t\t\" but actual partition number is '%s'\",\n+\t\t\t\t\t\t\t\t\tSTREAMING_SOURCE_PARTITION_INCLUDE.key(), allPartValueToTime.size()));\n+\t\t\t\t}\n+\t\t\t\treturn partValueList;\n+\t\t\t};\n+\t\t} else {\n+\t\t\t// bounded-read partitioned table, the fetcher fetches all partitions of the given filesystem table.\n+\t\t\tpartitionFetcher = context -> {\n+\t\t\t\tList<HiveTablePartition> partValueList = new ArrayList<>();\n+\t\t\t\tList<Tuple2<List<String>, Comparable>> allPartValueToTime = context.getPartValueWithComparableObjList();\n+\t\t\t\tfor (Tuple2<List<String>, Comparable> partValueToTime : allPartValueToTime) {\n+\t\t\t\t\tcontext.getPartition(partValueToTime.f0).ifPresent(partValueList::add);\n+\t\t\t\t}\n+\t\t\t\treturn partValueList;\n+\t\t\t};\n+\t\t}\n+\n+\t\tPartitionReader<HiveTablePartition, RowData> partitionReader = new HiveInputFormatPartitionReader(\n+\t\t\t\tjobConf,\n+\t\t\t\thiveVersion,\n+\t\t\t\ttablePath,\n+\t\t\t\tgetProducedTableSchema().getFieldDataTypes(),\n+\t\t\t\tgetProducedTableSchema().getFieldNames(),\n+\t\t\t\tcatalogTable.getPartitionKeys(),\n+\t\t\t\tprojectedFields,\n+\t\t\t\tflinkConf.get(HiveOptions.TABLE_EXEC_HIVE_FALLBACK_MAPRED_READER));\n+\n+\t\treturn new FileSystemLookupFunction<>(\n+\t\t\t\tpartitionFetcher,\n+\t\t\t\tfetcherContext,\n+\t\t\t\tpartitionReader,\n+\t\t\t\t(RowType) getProducedTableSchema().toRowDataType().getLogicalType(),\n+\t\t\t\tkeys,\n+\t\t\t\thiveTableReloadInterval);\n+\t}\n+\n+\t/**\n+\t * PartitionFetcher.Context for {@link HiveTablePartition}.\n+\t */\n+\tstatic class HiveTablePartitionFetcherContext implements PartitionFetcher.Context<HiveTablePartition> {\n+\n+\t\tprivate static final long serialVersionUID = 1L;\n+\t\tprivate final ObjectPath tablePath;\n+\t\tprivate final HiveShim hiveShim;\n+\t\tprivate final JobConfWrapper confWrapper;\n+\t\tprivate final List<String> partitionKeys;\n+\t\tprivate final DataType[] fieldTypes;\n+\t\tprivate final String[] fieldNames;\n+\t\tprivate final Configuration configuration;\n+\t\tprivate final String defaultPartitionName;\n+\n+\t\tprivate transient IMetaStoreClient metaStoreClient;\n+\t\tprivate transient StorageDescriptor tableSd;\n+\t\tprivate transient Properties tableProps;\n+\t\tprivate transient PartitionTimeExtractor extractor;\n+\t\tprivate transient ConsumeOrder consumeOrder;\n+\t\tprivate transient Path tableLocation;\n+\t\tprivate transient Table table;\n+\t\tprivate transient FileSystem fs;\n+\n+\t\tpublic HiveTablePartitionFetcherContext(\n+\t\t\t\tObjectPath tablePath,\n+\t\t\t\tHiveShim hiveShim,\n+\t\t\t\tJobConfWrapper confWrapper,\n+\t\t\t\tList<String> partitionKeys,\n+\t\t\t\tDataType[] fieldTypes,\n+\t\t\t\tString[] fieldNames,\n+\t\t\t\tConfiguration configuration,\n+\t\t\t\tString defaultPartitionName) {\n+\t\t\tthis.tablePath = tablePath;\n+\t\t\tthis.hiveShim = hiveShim;\n+\t\t\tthis.confWrapper = confWrapper;\n+\t\t\tthis.partitionKeys = partitionKeys;\n+\t\t\tthis.fieldTypes = fieldTypes;\n+\t\t\tthis.fieldNames = fieldNames;\n+\t\t\tthis.configuration = configuration;\n+\t\t\tthis.defaultPartitionName = defaultPartitionName;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void open() throws Exception {\n+\t\t\tmetaStoreClient = hiveShim.getHiveMetastoreClient(new HiveConf(confWrapper.conf(), HiveConf.class));\n+\t\t\ttable = metaStoreClient.getTable(tablePath.getDatabaseName(), tablePath.getObjectName());\n+\t\t\ttableSd = table.getSd();\n+\t\t\ttableProps = HiveReflectionUtils.getTableMetadata(hiveShim, table);\n+\n+\t\t\tString consumeOrderStr = configuration.get(STREAMING_SOURCE_PARTITION_ORDER);\n+\t\t\tconsumeOrder = ConsumeOrder.getConsumeOrder(consumeOrderStr);\n+\t\t\tString extractorKind = configuration.get(PARTITION_TIME_EXTRACTOR_KIND);\n+\t\t\tString extractorClass = configuration.get(PARTITION_TIME_EXTRACTOR_CLASS);\n+\t\t\tString extractorPattern = configuration.get(PARTITION_TIME_EXTRACTOR_TIMESTAMP_PATTERN);\n+\n+\t\t\textractor = PartitionTimeExtractor.create(\n+\t\t\t\t\tThread.currentThread().getContextClassLoader(),\n+\t\t\t\t\textractorKind,\n+\t\t\t\t\textractorClass,\n+\t\t\t\t\textractorPattern);\n+\t\t\ttableLocation = new Path(table.getSd().getLocation());\n+\t\t\tfs = tableLocation.getFileSystem(confWrapper.conf());\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic Optional<HiveTablePartition> getPartition(List<String> partValues) throws Exception {\n+\t\t\tif (partitionKeys.isEmpty()) {\n+\t\t\t\treturn Optional.empty();\n+\t\t\t}\n+\t\t\ttry {\n+\t\t\t\tPartition partition = metaStoreClient.getPartition(\n+\t\t\t\t\t\ttablePath.getDatabaseName(),\n+\t\t\t\t\t\ttablePath.getObjectName(),\n+\t\t\t\t\t\tpartValues);\n+\t\t\t\tHiveTablePartition hiveTablePartition = HivePartitionUtils.toHiveTablePartition(\n+\t\t\t\t\t\tpartitionKeys,\n+\t\t\t\t\t\tfieldNames,\n+\t\t\t\t\t\tfieldTypes,\n+\t\t\t\t\t\thiveShim,\n+\t\t\t\t\t\ttableProps,\n+\t\t\t\t\t\tdefaultPartitionName,\n+\t\t\t\t\t\tpartition);\n+\t\t\t\treturn Optional.of(hiveTablePartition);\n+\t\t\t} catch (NoSuchObjectException e) {\n+\t\t\t\treturn Optional.empty();\n+\t\t\t}\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic Optional<HiveTablePartition> getNonPartitionedTablePartition() {\n+\t\t\tif (partitionKeys.isEmpty()) {\n+\t\t\t\treturn Optional.of(new HiveTablePartition(tableSd, tableProps));\n+\t\t\t}\n+\t\t\treturn Optional.empty();\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic List<Tuple2<List<String>, Comparable>> getPartValueWithComparableObjList() throws Exception {\n+\t\t\tList<Tuple2<List<String>, Comparable>> partValueList = new ArrayList<>();\n+\t\t\tswitch (consumeOrder) {\n+\t\t\t\tcase PARTITION_NAME_ORDER:\n+\t\t\t\t\tList<String> partitionNames = metaStoreClient.listPartitionNames(\n+\t\t\t\t\t\t\ttablePath.getDatabaseName(),\n+\t\t\t\t\t\t\ttablePath.getObjectName(),\n+\t\t\t\t\t\t\tShort.MAX_VALUE);\n+\t\t\t\t\tfor (String partitionName : partitionNames) {\n+\t\t\t\t\t\tList<String> partValues = extractPartitionValues(new org.apache.flink.core.fs.Path(partitionName));\n+\t\t\t\t\t\tComparable comparable = partValues.toString();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "13202dd2477bfb3730e2cb039311b1a0784368dd"}, "originalPosition": 337}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIzODgxMjYz", "url": "https://github.com/apache/flink/pull/13729#pullrequestreview-523881263", "createdAt": "2020-11-05T02:25:15Z", "commit": {"oid": "13202dd2477bfb3730e2cb039311b1a0784368dd"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQwMjoyNToxNVrOHtw_aQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQwMjoyNToxNVrOHtw_aQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzc1MDYzMw==", "bodyText": "Why not listPartitionNames?", "url": "https://github.com/apache/flink/pull/13729#discussion_r517750633", "createdAt": "2020-11-05T02:25:15Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveLookupTableSource.java", "diffHunk": "@@ -0,0 +1,377 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.configuration.ReadableConfig;\n+import org.apache.flink.connectors.hive.read.HiveInputFormatPartitionReader;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.catalog.hive.client.HiveShim;\n+import org.apache.flink.table.catalog.hive.util.HivePartitionUtils;\n+import org.apache.flink.table.catalog.hive.util.HiveReflectionUtils;\n+import org.apache.flink.table.connector.source.LookupTableSource;\n+import org.apache.flink.table.connector.source.TableFunctionProvider;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.filesystem.FileSystemLookupFunction;\n+import org.apache.flink.table.filesystem.PartitionFetcher;\n+import org.apache.flink.table.filesystem.PartitionReader;\n+import org.apache.flink.table.filesystem.PartitionTimeExtractor;\n+import org.apache.flink.table.functions.TableFunction;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.metastore.IMetaStoreClient;\n+import org.apache.hadoop.hive.metastore.api.NoSuchObjectException;\n+import org.apache.hadoop.hive.metastore.api.Partition;\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.hive.metastore.api.Table;\n+import org.apache.hadoop.mapred.JobConf;\n+\n+import java.sql.Timestamp;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Properties;\n+\n+import static org.apache.flink.table.filesystem.DefaultPartTimeExtractor.toMills;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.LOOKUP_JOIN_CACHE_TTL;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_CLASS;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_KIND;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_TIMESTAMP_PATTERN;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_CONSUME_START_OFFSET;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_MONITOR_INTERVAL;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_PARTITION_INCLUDE;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_PARTITION_ORDER;\n+import static org.apache.flink.table.utils.PartitionPathUtils.extractPartitionValues;\n+\n+/**\n+ * Hive Table Source that has lookup ability.\n+ *\n+ * <p>Hive Table source has both lookup and continuous read ability, when it acts as continuous read source\n+ * it does not have the lookup ability but can be a temporal table just like other stream sources.\n+ * When it acts as bounded table, it has the lookup ability.\n+ *\n+ * <p>A common user case is use hive dimension table and always lookup the latest partition data, in this case\n+ * hive table source is a continuous read source but currently we implements it by LookupFunction. Because currently\n+ * TableSource can not tell the downstream when the latest partition has been read finished. This is a temporarily\n+ * workaround and will re-implement in the future.\n+ */\n+public class HiveLookupTableSource extends HiveTableSource implements LookupTableSource {\n+\n+\tprivate static final Duration DEFAULT_LOOKUP_MONITOR_INTERVAL = Duration.ofHours(1L);\n+\tprivate final Configuration configuration;\n+\tprivate Duration hiveTableReloadInterval;\n+\n+\tpublic HiveLookupTableSource(\n+\t\t\tJobConf jobConf,\n+\t\t\tReadableConfig flinkConf,\n+\t\t\tObjectPath tablePath,\n+\t\t\tCatalogTable catalogTable) {\n+\t\tsuper(jobConf, flinkConf, tablePath, catalogTable);\n+\t\tthis.configuration = new Configuration();\n+\t\tcatalogTable.getOptions().forEach(configuration::setString);\n+\t\tvalidateLookupConfigurations();\n+\t}\n+\n+\t@Override\n+\tpublic LookupRuntimeProvider getLookupRuntimeProvider(LookupContext context) {\n+\t\treturn TableFunctionProvider.of(getLookupFunction(context.getKeys()));\n+\t}\n+\n+\t@VisibleForTesting\n+\tTableFunction<RowData> getLookupFunction(int[][] keys) {\n+\t\tint[] keyIndices = new int[keys.length];\n+\t\tint i = 0;\n+\t\tfor (int[] key : keys) {\n+\t\t\tif (key.length > 1) {\n+\t\t\t\tthrow new UnsupportedOperationException(\"Hive lookup can not support nested key now.\");\n+\t\t\t}\n+\t\t\tkeyIndices[i] = key[0];\n+\t\t\ti++;\n+\t\t}\n+\t\treturn getLookupFunction(keyIndices);\n+\t}\n+\n+\tprivate void validateLookupConfigurations() {\n+\t\tString partitionInclude = configuration.get(STREAMING_SOURCE_PARTITION_INCLUDE);\n+\t\tif (isStreamingSource()) {\n+\t\t\tPreconditions.checkArgument(\n+\t\t\t\t\t!configuration.contains(STREAMING_SOURCE_CONSUME_START_OFFSET),\n+\t\t\t\t\tString.format(\n+\t\t\t\t\t\t\t\"The '%s' is not supported when set '%s' to 'latest'\",\n+\t\t\t\t\t\t\tSTREAMING_SOURCE_CONSUME_START_OFFSET.key(),\n+\t\t\t\t\t\t\tSTREAMING_SOURCE_PARTITION_INCLUDE.key()));\n+\n+\t\t\tDuration monitorInterval = configuration.get(STREAMING_SOURCE_MONITOR_INTERVAL) == null\n+\t\t\t\t\t? DEFAULT_LOOKUP_MONITOR_INTERVAL\n+\t\t\t\t\t: configuration.get(STREAMING_SOURCE_MONITOR_INTERVAL);\n+\t\t\tPreconditions.checkArgument(\n+\t\t\t\t\tmonitorInterval.toMillis() >= DEFAULT_LOOKUP_MONITOR_INTERVAL.toMillis(),\n+\t\t\t\t\tString.format(\n+\t\t\t\t\t\t\t\"Currently the value of '%s' is required bigger or equal to default value '%s' \" +\n+\t\t\t\t\t\t\t\t\t\"when set '%s' to 'latest', but actual is '%s'\",\n+\t\t\t\t\t\t\tSTREAMING_SOURCE_MONITOR_INTERVAL.key(),\n+\t\t\t\t\t\t\tDEFAULT_LOOKUP_MONITOR_INTERVAL.toMillis(),\n+\t\t\t\t\t\t\tSTREAMING_SOURCE_PARTITION_INCLUDE.key(),\n+\t\t\t\t\t\t\tmonitorInterval.toMillis())\n+\t\t\t);\n+\n+\t\t\thiveTableReloadInterval = monitorInterval;\n+\t\t} else {\n+\t\t\tPreconditions.checkArgument(\n+\t\t\t\t\t\"all\".equals(partitionInclude),\n+\t\t\t\t\tString.format(\"The only supported %s for lookup is '%s' in batch source,\" +\n+\t\t\t\t\t\t\t\" but actual is '%s'\", STREAMING_SOURCE_PARTITION_INCLUDE.key(), \"all\", partitionInclude));\n+\n+\t\t\thiveTableReloadInterval = configuration.get(LOOKUP_JOIN_CACHE_TTL);\n+\t\t}\n+\t}\n+\n+\tprivate TableFunction<RowData> getLookupFunction(int[] keys) {\n+\n+\t\tfinal String defaultPartitionName = jobConf.get(HiveConf.ConfVars.DEFAULTPARTITIONNAME.varname,\n+\t\t\t\tHiveConf.ConfVars.DEFAULTPARTITIONNAME.defaultStrVal);\n+\n+\t\tPartitionFetcher.Context<HiveTablePartition> fetcherContext = new HiveTablePartitionFetcherContext(\n+\t\t\t\ttablePath,\n+\t\t\t\thiveShim,\n+\t\t\t\tnew JobConfWrapper(jobConf),\n+\t\t\t\tcatalogTable.getPartitionKeys(),\n+\t\t\t\tgetProducedTableSchema().getFieldDataTypes(),\n+\t\t\t\tgetProducedTableSchema().getFieldNames(),\n+\t\t\t\tconfiguration,\n+\t\t\t\tdefaultPartitionName);\n+\n+\t\tPartitionFetcher<HiveTablePartition> partitionFetcher;\n+\t\tif (catalogTable.getPartitionKeys().isEmpty()) {\n+\t\t\t// non-partitioned table, the fetcher fetches the partition which represents the given table.\n+\t\t\tpartitionFetcher = context -> {\n+\t\t\t\tList<HiveTablePartition> partValueList = new ArrayList<>();\n+\t\t\t\tcontext.getNonPartitionedTablePartition().ifPresent(partValueList::add);\n+\t\t\t\treturn partValueList;\n+\t\t\t};\n+\t\t} else if (isStreamingSource()) {\n+\t\t\t// streaming-read partitioned table, the fetcher fetches the latest partition of the given table.\n+\t\t\tpartitionFetcher = context -> {\n+\t\t\t\tList<HiveTablePartition> partValueList = new ArrayList<>();\n+\t\t\t\tList<Tuple2<List<String>, Comparable>> allPartValueToTime = context.getPartValueWithComparableObjList();\n+\t\t\t\t// fetch latest partitions for partitioned table\n+\t\t\t\tif (allPartValueToTime.size() > 0) {\n+\t\t\t\t\t//sort in desc order\n+\t\t\t\t\tallPartValueToTime.sort((o1, o2) -> o2.f1.compareTo(o1.f1));\n+\t\t\t\t\tTuple2<List<String>, Comparable> maxPartition = allPartValueToTime.get(0);\n+\t\t\t\t\tcontext.getPartition(maxPartition.f0).ifPresent(partValueList::add);\n+\t\t\t\t} else {\n+\t\t\t\t\tthrow new IllegalArgumentException(\n+\t\t\t\t\t\t\tString.format(\"At least one partition is required when set '%s' to 'latest' in temporal join,\" +\n+\t\t\t\t\t\t\t\t\t\t\t\" but actual partition number is '%s'\",\n+\t\t\t\t\t\t\t\t\tSTREAMING_SOURCE_PARTITION_INCLUDE.key(), allPartValueToTime.size()));\n+\t\t\t\t}\n+\t\t\t\treturn partValueList;\n+\t\t\t};\n+\t\t} else {\n+\t\t\t// bounded-read partitioned table, the fetcher fetches all partitions of the given filesystem table.\n+\t\t\tpartitionFetcher = context -> {\n+\t\t\t\tList<HiveTablePartition> partValueList = new ArrayList<>();\n+\t\t\t\tList<Tuple2<List<String>, Comparable>> allPartValueToTime = context.getPartValueWithComparableObjList();\n+\t\t\t\tfor (Tuple2<List<String>, Comparable> partValueToTime : allPartValueToTime) {\n+\t\t\t\t\tcontext.getPartition(partValueToTime.f0).ifPresent(partValueList::add);\n+\t\t\t\t}\n+\t\t\t\treturn partValueList;\n+\t\t\t};\n+\t\t}\n+\n+\t\tPartitionReader<HiveTablePartition, RowData> partitionReader = new HiveInputFormatPartitionReader(\n+\t\t\t\tjobConf,\n+\t\t\t\thiveVersion,\n+\t\t\t\ttablePath,\n+\t\t\t\tgetProducedTableSchema().getFieldDataTypes(),\n+\t\t\t\tgetProducedTableSchema().getFieldNames(),\n+\t\t\t\tcatalogTable.getPartitionKeys(),\n+\t\t\t\tprojectedFields,\n+\t\t\t\tflinkConf.get(HiveOptions.TABLE_EXEC_HIVE_FALLBACK_MAPRED_READER));\n+\n+\t\treturn new FileSystemLookupFunction<>(\n+\t\t\t\tpartitionFetcher,\n+\t\t\t\tfetcherContext,\n+\t\t\t\tpartitionReader,\n+\t\t\t\t(RowType) getProducedTableSchema().toRowDataType().getLogicalType(),\n+\t\t\t\tkeys,\n+\t\t\t\thiveTableReloadInterval);\n+\t}\n+\n+\t/**\n+\t * PartitionFetcher.Context for {@link HiveTablePartition}.\n+\t */\n+\tstatic class HiveTablePartitionFetcherContext implements PartitionFetcher.Context<HiveTablePartition> {\n+\n+\t\tprivate static final long serialVersionUID = 1L;\n+\t\tprivate final ObjectPath tablePath;\n+\t\tprivate final HiveShim hiveShim;\n+\t\tprivate final JobConfWrapper confWrapper;\n+\t\tprivate final List<String> partitionKeys;\n+\t\tprivate final DataType[] fieldTypes;\n+\t\tprivate final String[] fieldNames;\n+\t\tprivate final Configuration configuration;\n+\t\tprivate final String defaultPartitionName;\n+\n+\t\tprivate transient IMetaStoreClient metaStoreClient;\n+\t\tprivate transient StorageDescriptor tableSd;\n+\t\tprivate transient Properties tableProps;\n+\t\tprivate transient PartitionTimeExtractor extractor;\n+\t\tprivate transient ConsumeOrder consumeOrder;\n+\t\tprivate transient Path tableLocation;\n+\t\tprivate transient Table table;\n+\t\tprivate transient FileSystem fs;\n+\n+\t\tpublic HiveTablePartitionFetcherContext(\n+\t\t\t\tObjectPath tablePath,\n+\t\t\t\tHiveShim hiveShim,\n+\t\t\t\tJobConfWrapper confWrapper,\n+\t\t\t\tList<String> partitionKeys,\n+\t\t\t\tDataType[] fieldTypes,\n+\t\t\t\tString[] fieldNames,\n+\t\t\t\tConfiguration configuration,\n+\t\t\t\tString defaultPartitionName) {\n+\t\t\tthis.tablePath = tablePath;\n+\t\t\tthis.hiveShim = hiveShim;\n+\t\t\tthis.confWrapper = confWrapper;\n+\t\t\tthis.partitionKeys = partitionKeys;\n+\t\t\tthis.fieldTypes = fieldTypes;\n+\t\t\tthis.fieldNames = fieldNames;\n+\t\t\tthis.configuration = configuration;\n+\t\t\tthis.defaultPartitionName = defaultPartitionName;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void open() throws Exception {\n+\t\t\tmetaStoreClient = hiveShim.getHiveMetastoreClient(new HiveConf(confWrapper.conf(), HiveConf.class));\n+\t\t\ttable = metaStoreClient.getTable(tablePath.getDatabaseName(), tablePath.getObjectName());\n+\t\t\ttableSd = table.getSd();\n+\t\t\ttableProps = HiveReflectionUtils.getTableMetadata(hiveShim, table);\n+\n+\t\t\tString consumeOrderStr = configuration.get(STREAMING_SOURCE_PARTITION_ORDER);\n+\t\t\tconsumeOrder = ConsumeOrder.getConsumeOrder(consumeOrderStr);\n+\t\t\tString extractorKind = configuration.get(PARTITION_TIME_EXTRACTOR_KIND);\n+\t\t\tString extractorClass = configuration.get(PARTITION_TIME_EXTRACTOR_CLASS);\n+\t\t\tString extractorPattern = configuration.get(PARTITION_TIME_EXTRACTOR_TIMESTAMP_PATTERN);\n+\n+\t\t\textractor = PartitionTimeExtractor.create(\n+\t\t\t\t\tThread.currentThread().getContextClassLoader(),\n+\t\t\t\t\textractorKind,\n+\t\t\t\t\textractorClass,\n+\t\t\t\t\textractorPattern);\n+\t\t\ttableLocation = new Path(table.getSd().getLocation());\n+\t\t\tfs = tableLocation.getFileSystem(confWrapper.conf());\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic Optional<HiveTablePartition> getPartition(List<String> partValues) throws Exception {\n+\t\t\tif (partitionKeys.isEmpty()) {\n+\t\t\t\treturn Optional.empty();\n+\t\t\t}\n+\t\t\ttry {\n+\t\t\t\tPartition partition = metaStoreClient.getPartition(\n+\t\t\t\t\t\ttablePath.getDatabaseName(),\n+\t\t\t\t\t\ttablePath.getObjectName(),\n+\t\t\t\t\t\tpartValues);\n+\t\t\t\tHiveTablePartition hiveTablePartition = HivePartitionUtils.toHiveTablePartition(\n+\t\t\t\t\t\tpartitionKeys,\n+\t\t\t\t\t\tfieldNames,\n+\t\t\t\t\t\tfieldTypes,\n+\t\t\t\t\t\thiveShim,\n+\t\t\t\t\t\ttableProps,\n+\t\t\t\t\t\tdefaultPartitionName,\n+\t\t\t\t\t\tpartition);\n+\t\t\t\treturn Optional.of(hiveTablePartition);\n+\t\t\t} catch (NoSuchObjectException e) {\n+\t\t\t\treturn Optional.empty();\n+\t\t\t}\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic Optional<HiveTablePartition> getNonPartitionedTablePartition() {\n+\t\t\tif (partitionKeys.isEmpty()) {\n+\t\t\t\treturn Optional.of(new HiveTablePartition(tableSd, tableProps));\n+\t\t\t}\n+\t\t\treturn Optional.empty();\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic List<Tuple2<List<String>, Comparable>> getPartValueWithComparableObjList() throws Exception {\n+\t\t\tList<Tuple2<List<String>, Comparable>> partValueList = new ArrayList<>();\n+\t\t\tswitch (consumeOrder) {\n+\t\t\t\tcase PARTITION_NAME_ORDER:\n+\t\t\t\t\tList<String> partitionNames = metaStoreClient.listPartitionNames(\n+\t\t\t\t\t\t\ttablePath.getDatabaseName(),\n+\t\t\t\t\t\t\ttablePath.getObjectName(),\n+\t\t\t\t\t\t\tShort.MAX_VALUE);\n+\t\t\t\t\tfor (String partitionName : partitionNames) {\n+\t\t\t\t\t\tList<String> partValues = extractPartitionValues(new org.apache.flink.core.fs.Path(partitionName));\n+\t\t\t\t\t\tComparable comparable = partValues.toString();\n+\t\t\t\t\t\tpartValueList.add(new Tuple2<>(partValues, comparable));\n+\t\t\t\t\t}\n+\t\t\t\t\tbreak;\n+\t\t\t\tcase CREATE_TIME_ORDER:\n+\t\t\t\t\tFileStatus[] statuses = HivePartitionUtils.getFileStatusRecurse(tableLocation, partitionKeys.size(), fs);\n+\t\t\t\t\tfor (FileStatus status : statuses) {\n+\t\t\t\t\t\tList<String> partValues = extractPartitionValues(\n+\t\t\t\t\t\t\t\tnew org.apache.flink.core.fs.Path(status.getPath().toString()));\n+\t\t\t\t\t\tComparable comparable = TimestampData.fromTimestamp(new Timestamp(status.getModificationTime()))\n+\t\t\t\t\t\t\t\t.getMillisecond();\n+\t\t\t\t\t\tpartValueList.add(new Tuple2<>(partValues, comparable));\n+\t\t\t\t\t}\n+\t\t\t\t\tbreak;\n+\t\t\t\tcase PARTITION_TIME_ORDER:\n+\t\t\t\t\tList<Partition> partitions = metaStoreClient.listPartitions(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "13202dd2477bfb3730e2cb039311b1a0784368dd"}, "originalPosition": 352}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIzODgyMTkz", "url": "https://github.com/apache/flink/pull/13729#pullrequestreview-523882193", "createdAt": "2020-11-05T02:27:54Z", "commit": {"oid": "13202dd2477bfb3730e2cb039311b1a0784368dd"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQwMjoyNzo1NFrOHtxGvw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQwMjoyNzo1NFrOHtxGvw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzc1MjUxMQ==", "bodyText": "You can return a ComparablePartition interface in Context.", "url": "https://github.com/apache/flink/pull/13729#discussion_r517752511", "createdAt": "2020-11-05T02:27:54Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveLookupTableSource.java", "diffHunk": "@@ -0,0 +1,377 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.configuration.ReadableConfig;\n+import org.apache.flink.connectors.hive.read.HiveInputFormatPartitionReader;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.catalog.hive.client.HiveShim;\n+import org.apache.flink.table.catalog.hive.util.HivePartitionUtils;\n+import org.apache.flink.table.catalog.hive.util.HiveReflectionUtils;\n+import org.apache.flink.table.connector.source.LookupTableSource;\n+import org.apache.flink.table.connector.source.TableFunctionProvider;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.filesystem.FileSystemLookupFunction;\n+import org.apache.flink.table.filesystem.PartitionFetcher;\n+import org.apache.flink.table.filesystem.PartitionReader;\n+import org.apache.flink.table.filesystem.PartitionTimeExtractor;\n+import org.apache.flink.table.functions.TableFunction;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.metastore.IMetaStoreClient;\n+import org.apache.hadoop.hive.metastore.api.NoSuchObjectException;\n+import org.apache.hadoop.hive.metastore.api.Partition;\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.hive.metastore.api.Table;\n+import org.apache.hadoop.mapred.JobConf;\n+\n+import java.sql.Timestamp;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Properties;\n+\n+import static org.apache.flink.table.filesystem.DefaultPartTimeExtractor.toMills;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.LOOKUP_JOIN_CACHE_TTL;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_CLASS;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_KIND;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_TIMESTAMP_PATTERN;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_CONSUME_START_OFFSET;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_MONITOR_INTERVAL;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_PARTITION_INCLUDE;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_PARTITION_ORDER;\n+import static org.apache.flink.table.utils.PartitionPathUtils.extractPartitionValues;\n+\n+/**\n+ * Hive Table Source that has lookup ability.\n+ *\n+ * <p>Hive Table source has both lookup and continuous read ability, when it acts as continuous read source\n+ * it does not have the lookup ability but can be a temporal table just like other stream sources.\n+ * When it acts as bounded table, it has the lookup ability.\n+ *\n+ * <p>A common user case is use hive dimension table and always lookup the latest partition data, in this case\n+ * hive table source is a continuous read source but currently we implements it by LookupFunction. Because currently\n+ * TableSource can not tell the downstream when the latest partition has been read finished. This is a temporarily\n+ * workaround and will re-implement in the future.\n+ */\n+public class HiveLookupTableSource extends HiveTableSource implements LookupTableSource {\n+\n+\tprivate static final Duration DEFAULT_LOOKUP_MONITOR_INTERVAL = Duration.ofHours(1L);\n+\tprivate final Configuration configuration;\n+\tprivate Duration hiveTableReloadInterval;\n+\n+\tpublic HiveLookupTableSource(\n+\t\t\tJobConf jobConf,\n+\t\t\tReadableConfig flinkConf,\n+\t\t\tObjectPath tablePath,\n+\t\t\tCatalogTable catalogTable) {\n+\t\tsuper(jobConf, flinkConf, tablePath, catalogTable);\n+\t\tthis.configuration = new Configuration();\n+\t\tcatalogTable.getOptions().forEach(configuration::setString);\n+\t\tvalidateLookupConfigurations();\n+\t}\n+\n+\t@Override\n+\tpublic LookupRuntimeProvider getLookupRuntimeProvider(LookupContext context) {\n+\t\treturn TableFunctionProvider.of(getLookupFunction(context.getKeys()));\n+\t}\n+\n+\t@VisibleForTesting\n+\tTableFunction<RowData> getLookupFunction(int[][] keys) {\n+\t\tint[] keyIndices = new int[keys.length];\n+\t\tint i = 0;\n+\t\tfor (int[] key : keys) {\n+\t\t\tif (key.length > 1) {\n+\t\t\t\tthrow new UnsupportedOperationException(\"Hive lookup can not support nested key now.\");\n+\t\t\t}\n+\t\t\tkeyIndices[i] = key[0];\n+\t\t\ti++;\n+\t\t}\n+\t\treturn getLookupFunction(keyIndices);\n+\t}\n+\n+\tprivate void validateLookupConfigurations() {\n+\t\tString partitionInclude = configuration.get(STREAMING_SOURCE_PARTITION_INCLUDE);\n+\t\tif (isStreamingSource()) {\n+\t\t\tPreconditions.checkArgument(\n+\t\t\t\t\t!configuration.contains(STREAMING_SOURCE_CONSUME_START_OFFSET),\n+\t\t\t\t\tString.format(\n+\t\t\t\t\t\t\t\"The '%s' is not supported when set '%s' to 'latest'\",\n+\t\t\t\t\t\t\tSTREAMING_SOURCE_CONSUME_START_OFFSET.key(),\n+\t\t\t\t\t\t\tSTREAMING_SOURCE_PARTITION_INCLUDE.key()));\n+\n+\t\t\tDuration monitorInterval = configuration.get(STREAMING_SOURCE_MONITOR_INTERVAL) == null\n+\t\t\t\t\t? DEFAULT_LOOKUP_MONITOR_INTERVAL\n+\t\t\t\t\t: configuration.get(STREAMING_SOURCE_MONITOR_INTERVAL);\n+\t\t\tPreconditions.checkArgument(\n+\t\t\t\t\tmonitorInterval.toMillis() >= DEFAULT_LOOKUP_MONITOR_INTERVAL.toMillis(),\n+\t\t\t\t\tString.format(\n+\t\t\t\t\t\t\t\"Currently the value of '%s' is required bigger or equal to default value '%s' \" +\n+\t\t\t\t\t\t\t\t\t\"when set '%s' to 'latest', but actual is '%s'\",\n+\t\t\t\t\t\t\tSTREAMING_SOURCE_MONITOR_INTERVAL.key(),\n+\t\t\t\t\t\t\tDEFAULT_LOOKUP_MONITOR_INTERVAL.toMillis(),\n+\t\t\t\t\t\t\tSTREAMING_SOURCE_PARTITION_INCLUDE.key(),\n+\t\t\t\t\t\t\tmonitorInterval.toMillis())\n+\t\t\t);\n+\n+\t\t\thiveTableReloadInterval = monitorInterval;\n+\t\t} else {\n+\t\t\tPreconditions.checkArgument(\n+\t\t\t\t\t\"all\".equals(partitionInclude),\n+\t\t\t\t\tString.format(\"The only supported %s for lookup is '%s' in batch source,\" +\n+\t\t\t\t\t\t\t\" but actual is '%s'\", STREAMING_SOURCE_PARTITION_INCLUDE.key(), \"all\", partitionInclude));\n+\n+\t\t\thiveTableReloadInterval = configuration.get(LOOKUP_JOIN_CACHE_TTL);\n+\t\t}\n+\t}\n+\n+\tprivate TableFunction<RowData> getLookupFunction(int[] keys) {\n+\n+\t\tfinal String defaultPartitionName = jobConf.get(HiveConf.ConfVars.DEFAULTPARTITIONNAME.varname,\n+\t\t\t\tHiveConf.ConfVars.DEFAULTPARTITIONNAME.defaultStrVal);\n+\n+\t\tPartitionFetcher.Context<HiveTablePartition> fetcherContext = new HiveTablePartitionFetcherContext(\n+\t\t\t\ttablePath,\n+\t\t\t\thiveShim,\n+\t\t\t\tnew JobConfWrapper(jobConf),\n+\t\t\t\tcatalogTable.getPartitionKeys(),\n+\t\t\t\tgetProducedTableSchema().getFieldDataTypes(),\n+\t\t\t\tgetProducedTableSchema().getFieldNames(),\n+\t\t\t\tconfiguration,\n+\t\t\t\tdefaultPartitionName);\n+\n+\t\tPartitionFetcher<HiveTablePartition> partitionFetcher;\n+\t\tif (catalogTable.getPartitionKeys().isEmpty()) {\n+\t\t\t// non-partitioned table, the fetcher fetches the partition which represents the given table.\n+\t\t\tpartitionFetcher = context -> {\n+\t\t\t\tList<HiveTablePartition> partValueList = new ArrayList<>();\n+\t\t\t\tcontext.getNonPartitionedTablePartition().ifPresent(partValueList::add);\n+\t\t\t\treturn partValueList;\n+\t\t\t};\n+\t\t} else if (isStreamingSource()) {\n+\t\t\t// streaming-read partitioned table, the fetcher fetches the latest partition of the given table.\n+\t\t\tpartitionFetcher = context -> {\n+\t\t\t\tList<HiveTablePartition> partValueList = new ArrayList<>();\n+\t\t\t\tList<Tuple2<List<String>, Comparable>> allPartValueToTime = context.getPartValueWithComparableObjList();\n+\t\t\t\t// fetch latest partitions for partitioned table\n+\t\t\t\tif (allPartValueToTime.size() > 0) {\n+\t\t\t\t\t//sort in desc order\n+\t\t\t\t\tallPartValueToTime.sort((o1, o2) -> o2.f1.compareTo(o1.f1));\n+\t\t\t\t\tTuple2<List<String>, Comparable> maxPartition = allPartValueToTime.get(0);\n+\t\t\t\t\tcontext.getPartition(maxPartition.f0).ifPresent(partValueList::add);\n+\t\t\t\t} else {\n+\t\t\t\t\tthrow new IllegalArgumentException(\n+\t\t\t\t\t\t\tString.format(\"At least one partition is required when set '%s' to 'latest' in temporal join,\" +\n+\t\t\t\t\t\t\t\t\t\t\t\" but actual partition number is '%s'\",\n+\t\t\t\t\t\t\t\t\tSTREAMING_SOURCE_PARTITION_INCLUDE.key(), allPartValueToTime.size()));\n+\t\t\t\t}\n+\t\t\t\treturn partValueList;\n+\t\t\t};\n+\t\t} else {\n+\t\t\t// bounded-read partitioned table, the fetcher fetches all partitions of the given filesystem table.\n+\t\t\tpartitionFetcher = context -> {\n+\t\t\t\tList<HiveTablePartition> partValueList = new ArrayList<>();\n+\t\t\t\tList<Tuple2<List<String>, Comparable>> allPartValueToTime = context.getPartValueWithComparableObjList();\n+\t\t\t\tfor (Tuple2<List<String>, Comparable> partValueToTime : allPartValueToTime) {\n+\t\t\t\t\tcontext.getPartition(partValueToTime.f0).ifPresent(partValueList::add);\n+\t\t\t\t}\n+\t\t\t\treturn partValueList;\n+\t\t\t};\n+\t\t}\n+\n+\t\tPartitionReader<HiveTablePartition, RowData> partitionReader = new HiveInputFormatPartitionReader(\n+\t\t\t\tjobConf,\n+\t\t\t\thiveVersion,\n+\t\t\t\ttablePath,\n+\t\t\t\tgetProducedTableSchema().getFieldDataTypes(),\n+\t\t\t\tgetProducedTableSchema().getFieldNames(),\n+\t\t\t\tcatalogTable.getPartitionKeys(),\n+\t\t\t\tprojectedFields,\n+\t\t\t\tflinkConf.get(HiveOptions.TABLE_EXEC_HIVE_FALLBACK_MAPRED_READER));\n+\n+\t\treturn new FileSystemLookupFunction<>(\n+\t\t\t\tpartitionFetcher,\n+\t\t\t\tfetcherContext,\n+\t\t\t\tpartitionReader,\n+\t\t\t\t(RowType) getProducedTableSchema().toRowDataType().getLogicalType(),\n+\t\t\t\tkeys,\n+\t\t\t\thiveTableReloadInterval);\n+\t}\n+\n+\t/**\n+\t * PartitionFetcher.Context for {@link HiveTablePartition}.\n+\t */\n+\tstatic class HiveTablePartitionFetcherContext implements PartitionFetcher.Context<HiveTablePartition> {\n+\n+\t\tprivate static final long serialVersionUID = 1L;\n+\t\tprivate final ObjectPath tablePath;\n+\t\tprivate final HiveShim hiveShim;\n+\t\tprivate final JobConfWrapper confWrapper;\n+\t\tprivate final List<String> partitionKeys;\n+\t\tprivate final DataType[] fieldTypes;\n+\t\tprivate final String[] fieldNames;\n+\t\tprivate final Configuration configuration;\n+\t\tprivate final String defaultPartitionName;\n+\n+\t\tprivate transient IMetaStoreClient metaStoreClient;\n+\t\tprivate transient StorageDescriptor tableSd;\n+\t\tprivate transient Properties tableProps;\n+\t\tprivate transient PartitionTimeExtractor extractor;\n+\t\tprivate transient ConsumeOrder consumeOrder;\n+\t\tprivate transient Path tableLocation;\n+\t\tprivate transient Table table;\n+\t\tprivate transient FileSystem fs;\n+\n+\t\tpublic HiveTablePartitionFetcherContext(\n+\t\t\t\tObjectPath tablePath,\n+\t\t\t\tHiveShim hiveShim,\n+\t\t\t\tJobConfWrapper confWrapper,\n+\t\t\t\tList<String> partitionKeys,\n+\t\t\t\tDataType[] fieldTypes,\n+\t\t\t\tString[] fieldNames,\n+\t\t\t\tConfiguration configuration,\n+\t\t\t\tString defaultPartitionName) {\n+\t\t\tthis.tablePath = tablePath;\n+\t\t\tthis.hiveShim = hiveShim;\n+\t\t\tthis.confWrapper = confWrapper;\n+\t\t\tthis.partitionKeys = partitionKeys;\n+\t\t\tthis.fieldTypes = fieldTypes;\n+\t\t\tthis.fieldNames = fieldNames;\n+\t\t\tthis.configuration = configuration;\n+\t\t\tthis.defaultPartitionName = defaultPartitionName;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void open() throws Exception {\n+\t\t\tmetaStoreClient = hiveShim.getHiveMetastoreClient(new HiveConf(confWrapper.conf(), HiveConf.class));\n+\t\t\ttable = metaStoreClient.getTable(tablePath.getDatabaseName(), tablePath.getObjectName());\n+\t\t\ttableSd = table.getSd();\n+\t\t\ttableProps = HiveReflectionUtils.getTableMetadata(hiveShim, table);\n+\n+\t\t\tString consumeOrderStr = configuration.get(STREAMING_SOURCE_PARTITION_ORDER);\n+\t\t\tconsumeOrder = ConsumeOrder.getConsumeOrder(consumeOrderStr);\n+\t\t\tString extractorKind = configuration.get(PARTITION_TIME_EXTRACTOR_KIND);\n+\t\t\tString extractorClass = configuration.get(PARTITION_TIME_EXTRACTOR_CLASS);\n+\t\t\tString extractorPattern = configuration.get(PARTITION_TIME_EXTRACTOR_TIMESTAMP_PATTERN);\n+\n+\t\t\textractor = PartitionTimeExtractor.create(\n+\t\t\t\t\tThread.currentThread().getContextClassLoader(),\n+\t\t\t\t\textractorKind,\n+\t\t\t\t\textractorClass,\n+\t\t\t\t\textractorPattern);\n+\t\t\ttableLocation = new Path(table.getSd().getLocation());\n+\t\t\tfs = tableLocation.getFileSystem(confWrapper.conf());\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic Optional<HiveTablePartition> getPartition(List<String> partValues) throws Exception {\n+\t\t\tif (partitionKeys.isEmpty()) {\n+\t\t\t\treturn Optional.empty();\n+\t\t\t}\n+\t\t\ttry {\n+\t\t\t\tPartition partition = metaStoreClient.getPartition(\n+\t\t\t\t\t\ttablePath.getDatabaseName(),\n+\t\t\t\t\t\ttablePath.getObjectName(),\n+\t\t\t\t\t\tpartValues);\n+\t\t\t\tHiveTablePartition hiveTablePartition = HivePartitionUtils.toHiveTablePartition(\n+\t\t\t\t\t\tpartitionKeys,\n+\t\t\t\t\t\tfieldNames,\n+\t\t\t\t\t\tfieldTypes,\n+\t\t\t\t\t\thiveShim,\n+\t\t\t\t\t\ttableProps,\n+\t\t\t\t\t\tdefaultPartitionName,\n+\t\t\t\t\t\tpartition);\n+\t\t\t\treturn Optional.of(hiveTablePartition);\n+\t\t\t} catch (NoSuchObjectException e) {\n+\t\t\t\treturn Optional.empty();\n+\t\t\t}\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic Optional<HiveTablePartition> getNonPartitionedTablePartition() {\n+\t\t\tif (partitionKeys.isEmpty()) {\n+\t\t\t\treturn Optional.of(new HiveTablePartition(tableSd, tableProps));\n+\t\t\t}\n+\t\t\treturn Optional.empty();\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic List<Tuple2<List<String>, Comparable>> getPartValueWithComparableObjList() throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "13202dd2477bfb3730e2cb039311b1a0784368dd"}, "originalPosition": 327}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIzODgyODk5", "url": "https://github.com/apache/flink/pull/13729#pullrequestreview-523882899", "createdAt": "2020-11-05T02:30:00Z", "commit": {"oid": "13202dd2477bfb3730e2cb039311b1a0784368dd"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQwMjozMDowMFrOHtxMjQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQwMjozMDowMFrOHtxMjQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzc1Mzk5Nw==", "bodyText": "Remove this useless method, you can merge this to getPartition", "url": "https://github.com/apache/flink/pull/13729#discussion_r517753997", "createdAt": "2020-11-05T02:30:00Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveLookupTableSource.java", "diffHunk": "@@ -0,0 +1,377 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.configuration.ReadableConfig;\n+import org.apache.flink.connectors.hive.read.HiveInputFormatPartitionReader;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.catalog.hive.client.HiveShim;\n+import org.apache.flink.table.catalog.hive.util.HivePartitionUtils;\n+import org.apache.flink.table.catalog.hive.util.HiveReflectionUtils;\n+import org.apache.flink.table.connector.source.LookupTableSource;\n+import org.apache.flink.table.connector.source.TableFunctionProvider;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.filesystem.FileSystemLookupFunction;\n+import org.apache.flink.table.filesystem.PartitionFetcher;\n+import org.apache.flink.table.filesystem.PartitionReader;\n+import org.apache.flink.table.filesystem.PartitionTimeExtractor;\n+import org.apache.flink.table.functions.TableFunction;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.metastore.IMetaStoreClient;\n+import org.apache.hadoop.hive.metastore.api.NoSuchObjectException;\n+import org.apache.hadoop.hive.metastore.api.Partition;\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.hive.metastore.api.Table;\n+import org.apache.hadoop.mapred.JobConf;\n+\n+import java.sql.Timestamp;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Properties;\n+\n+import static org.apache.flink.table.filesystem.DefaultPartTimeExtractor.toMills;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.LOOKUP_JOIN_CACHE_TTL;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_CLASS;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_KIND;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_TIMESTAMP_PATTERN;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_CONSUME_START_OFFSET;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_MONITOR_INTERVAL;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_PARTITION_INCLUDE;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_PARTITION_ORDER;\n+import static org.apache.flink.table.utils.PartitionPathUtils.extractPartitionValues;\n+\n+/**\n+ * Hive Table Source that has lookup ability.\n+ *\n+ * <p>Hive Table source has both lookup and continuous read ability, when it acts as continuous read source\n+ * it does not have the lookup ability but can be a temporal table just like other stream sources.\n+ * When it acts as bounded table, it has the lookup ability.\n+ *\n+ * <p>A common user case is use hive dimension table and always lookup the latest partition data, in this case\n+ * hive table source is a continuous read source but currently we implements it by LookupFunction. Because currently\n+ * TableSource can not tell the downstream when the latest partition has been read finished. This is a temporarily\n+ * workaround and will re-implement in the future.\n+ */\n+public class HiveLookupTableSource extends HiveTableSource implements LookupTableSource {\n+\n+\tprivate static final Duration DEFAULT_LOOKUP_MONITOR_INTERVAL = Duration.ofHours(1L);\n+\tprivate final Configuration configuration;\n+\tprivate Duration hiveTableReloadInterval;\n+\n+\tpublic HiveLookupTableSource(\n+\t\t\tJobConf jobConf,\n+\t\t\tReadableConfig flinkConf,\n+\t\t\tObjectPath tablePath,\n+\t\t\tCatalogTable catalogTable) {\n+\t\tsuper(jobConf, flinkConf, tablePath, catalogTable);\n+\t\tthis.configuration = new Configuration();\n+\t\tcatalogTable.getOptions().forEach(configuration::setString);\n+\t\tvalidateLookupConfigurations();\n+\t}\n+\n+\t@Override\n+\tpublic LookupRuntimeProvider getLookupRuntimeProvider(LookupContext context) {\n+\t\treturn TableFunctionProvider.of(getLookupFunction(context.getKeys()));\n+\t}\n+\n+\t@VisibleForTesting\n+\tTableFunction<RowData> getLookupFunction(int[][] keys) {\n+\t\tint[] keyIndices = new int[keys.length];\n+\t\tint i = 0;\n+\t\tfor (int[] key : keys) {\n+\t\t\tif (key.length > 1) {\n+\t\t\t\tthrow new UnsupportedOperationException(\"Hive lookup can not support nested key now.\");\n+\t\t\t}\n+\t\t\tkeyIndices[i] = key[0];\n+\t\t\ti++;\n+\t\t}\n+\t\treturn getLookupFunction(keyIndices);\n+\t}\n+\n+\tprivate void validateLookupConfigurations() {\n+\t\tString partitionInclude = configuration.get(STREAMING_SOURCE_PARTITION_INCLUDE);\n+\t\tif (isStreamingSource()) {\n+\t\t\tPreconditions.checkArgument(\n+\t\t\t\t\t!configuration.contains(STREAMING_SOURCE_CONSUME_START_OFFSET),\n+\t\t\t\t\tString.format(\n+\t\t\t\t\t\t\t\"The '%s' is not supported when set '%s' to 'latest'\",\n+\t\t\t\t\t\t\tSTREAMING_SOURCE_CONSUME_START_OFFSET.key(),\n+\t\t\t\t\t\t\tSTREAMING_SOURCE_PARTITION_INCLUDE.key()));\n+\n+\t\t\tDuration monitorInterval = configuration.get(STREAMING_SOURCE_MONITOR_INTERVAL) == null\n+\t\t\t\t\t? DEFAULT_LOOKUP_MONITOR_INTERVAL\n+\t\t\t\t\t: configuration.get(STREAMING_SOURCE_MONITOR_INTERVAL);\n+\t\t\tPreconditions.checkArgument(\n+\t\t\t\t\tmonitorInterval.toMillis() >= DEFAULT_LOOKUP_MONITOR_INTERVAL.toMillis(),\n+\t\t\t\t\tString.format(\n+\t\t\t\t\t\t\t\"Currently the value of '%s' is required bigger or equal to default value '%s' \" +\n+\t\t\t\t\t\t\t\t\t\"when set '%s' to 'latest', but actual is '%s'\",\n+\t\t\t\t\t\t\tSTREAMING_SOURCE_MONITOR_INTERVAL.key(),\n+\t\t\t\t\t\t\tDEFAULT_LOOKUP_MONITOR_INTERVAL.toMillis(),\n+\t\t\t\t\t\t\tSTREAMING_SOURCE_PARTITION_INCLUDE.key(),\n+\t\t\t\t\t\t\tmonitorInterval.toMillis())\n+\t\t\t);\n+\n+\t\t\thiveTableReloadInterval = monitorInterval;\n+\t\t} else {\n+\t\t\tPreconditions.checkArgument(\n+\t\t\t\t\t\"all\".equals(partitionInclude),\n+\t\t\t\t\tString.format(\"The only supported %s for lookup is '%s' in batch source,\" +\n+\t\t\t\t\t\t\t\" but actual is '%s'\", STREAMING_SOURCE_PARTITION_INCLUDE.key(), \"all\", partitionInclude));\n+\n+\t\t\thiveTableReloadInterval = configuration.get(LOOKUP_JOIN_CACHE_TTL);\n+\t\t}\n+\t}\n+\n+\tprivate TableFunction<RowData> getLookupFunction(int[] keys) {\n+\n+\t\tfinal String defaultPartitionName = jobConf.get(HiveConf.ConfVars.DEFAULTPARTITIONNAME.varname,\n+\t\t\t\tHiveConf.ConfVars.DEFAULTPARTITIONNAME.defaultStrVal);\n+\n+\t\tPartitionFetcher.Context<HiveTablePartition> fetcherContext = new HiveTablePartitionFetcherContext(\n+\t\t\t\ttablePath,\n+\t\t\t\thiveShim,\n+\t\t\t\tnew JobConfWrapper(jobConf),\n+\t\t\t\tcatalogTable.getPartitionKeys(),\n+\t\t\t\tgetProducedTableSchema().getFieldDataTypes(),\n+\t\t\t\tgetProducedTableSchema().getFieldNames(),\n+\t\t\t\tconfiguration,\n+\t\t\t\tdefaultPartitionName);\n+\n+\t\tPartitionFetcher<HiveTablePartition> partitionFetcher;\n+\t\tif (catalogTable.getPartitionKeys().isEmpty()) {\n+\t\t\t// non-partitioned table, the fetcher fetches the partition which represents the given table.\n+\t\t\tpartitionFetcher = context -> {\n+\t\t\t\tList<HiveTablePartition> partValueList = new ArrayList<>();\n+\t\t\t\tcontext.getNonPartitionedTablePartition().ifPresent(partValueList::add);\n+\t\t\t\treturn partValueList;\n+\t\t\t};\n+\t\t} else if (isStreamingSource()) {\n+\t\t\t// streaming-read partitioned table, the fetcher fetches the latest partition of the given table.\n+\t\t\tpartitionFetcher = context -> {\n+\t\t\t\tList<HiveTablePartition> partValueList = new ArrayList<>();\n+\t\t\t\tList<Tuple2<List<String>, Comparable>> allPartValueToTime = context.getPartValueWithComparableObjList();\n+\t\t\t\t// fetch latest partitions for partitioned table\n+\t\t\t\tif (allPartValueToTime.size() > 0) {\n+\t\t\t\t\t//sort in desc order\n+\t\t\t\t\tallPartValueToTime.sort((o1, o2) -> o2.f1.compareTo(o1.f1));\n+\t\t\t\t\tTuple2<List<String>, Comparable> maxPartition = allPartValueToTime.get(0);\n+\t\t\t\t\tcontext.getPartition(maxPartition.f0).ifPresent(partValueList::add);\n+\t\t\t\t} else {\n+\t\t\t\t\tthrow new IllegalArgumentException(\n+\t\t\t\t\t\t\tString.format(\"At least one partition is required when set '%s' to 'latest' in temporal join,\" +\n+\t\t\t\t\t\t\t\t\t\t\t\" but actual partition number is '%s'\",\n+\t\t\t\t\t\t\t\t\tSTREAMING_SOURCE_PARTITION_INCLUDE.key(), allPartValueToTime.size()));\n+\t\t\t\t}\n+\t\t\t\treturn partValueList;\n+\t\t\t};\n+\t\t} else {\n+\t\t\t// bounded-read partitioned table, the fetcher fetches all partitions of the given filesystem table.\n+\t\t\tpartitionFetcher = context -> {\n+\t\t\t\tList<HiveTablePartition> partValueList = new ArrayList<>();\n+\t\t\t\tList<Tuple2<List<String>, Comparable>> allPartValueToTime = context.getPartValueWithComparableObjList();\n+\t\t\t\tfor (Tuple2<List<String>, Comparable> partValueToTime : allPartValueToTime) {\n+\t\t\t\t\tcontext.getPartition(partValueToTime.f0).ifPresent(partValueList::add);\n+\t\t\t\t}\n+\t\t\t\treturn partValueList;\n+\t\t\t};\n+\t\t}\n+\n+\t\tPartitionReader<HiveTablePartition, RowData> partitionReader = new HiveInputFormatPartitionReader(\n+\t\t\t\tjobConf,\n+\t\t\t\thiveVersion,\n+\t\t\t\ttablePath,\n+\t\t\t\tgetProducedTableSchema().getFieldDataTypes(),\n+\t\t\t\tgetProducedTableSchema().getFieldNames(),\n+\t\t\t\tcatalogTable.getPartitionKeys(),\n+\t\t\t\tprojectedFields,\n+\t\t\t\tflinkConf.get(HiveOptions.TABLE_EXEC_HIVE_FALLBACK_MAPRED_READER));\n+\n+\t\treturn new FileSystemLookupFunction<>(\n+\t\t\t\tpartitionFetcher,\n+\t\t\t\tfetcherContext,\n+\t\t\t\tpartitionReader,\n+\t\t\t\t(RowType) getProducedTableSchema().toRowDataType().getLogicalType(),\n+\t\t\t\tkeys,\n+\t\t\t\thiveTableReloadInterval);\n+\t}\n+\n+\t/**\n+\t * PartitionFetcher.Context for {@link HiveTablePartition}.\n+\t */\n+\tstatic class HiveTablePartitionFetcherContext implements PartitionFetcher.Context<HiveTablePartition> {\n+\n+\t\tprivate static final long serialVersionUID = 1L;\n+\t\tprivate final ObjectPath tablePath;\n+\t\tprivate final HiveShim hiveShim;\n+\t\tprivate final JobConfWrapper confWrapper;\n+\t\tprivate final List<String> partitionKeys;\n+\t\tprivate final DataType[] fieldTypes;\n+\t\tprivate final String[] fieldNames;\n+\t\tprivate final Configuration configuration;\n+\t\tprivate final String defaultPartitionName;\n+\n+\t\tprivate transient IMetaStoreClient metaStoreClient;\n+\t\tprivate transient StorageDescriptor tableSd;\n+\t\tprivate transient Properties tableProps;\n+\t\tprivate transient PartitionTimeExtractor extractor;\n+\t\tprivate transient ConsumeOrder consumeOrder;\n+\t\tprivate transient Path tableLocation;\n+\t\tprivate transient Table table;\n+\t\tprivate transient FileSystem fs;\n+\n+\t\tpublic HiveTablePartitionFetcherContext(\n+\t\t\t\tObjectPath tablePath,\n+\t\t\t\tHiveShim hiveShim,\n+\t\t\t\tJobConfWrapper confWrapper,\n+\t\t\t\tList<String> partitionKeys,\n+\t\t\t\tDataType[] fieldTypes,\n+\t\t\t\tString[] fieldNames,\n+\t\t\t\tConfiguration configuration,\n+\t\t\t\tString defaultPartitionName) {\n+\t\t\tthis.tablePath = tablePath;\n+\t\t\tthis.hiveShim = hiveShim;\n+\t\t\tthis.confWrapper = confWrapper;\n+\t\t\tthis.partitionKeys = partitionKeys;\n+\t\t\tthis.fieldTypes = fieldTypes;\n+\t\t\tthis.fieldNames = fieldNames;\n+\t\t\tthis.configuration = configuration;\n+\t\t\tthis.defaultPartitionName = defaultPartitionName;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void open() throws Exception {\n+\t\t\tmetaStoreClient = hiveShim.getHiveMetastoreClient(new HiveConf(confWrapper.conf(), HiveConf.class));\n+\t\t\ttable = metaStoreClient.getTable(tablePath.getDatabaseName(), tablePath.getObjectName());\n+\t\t\ttableSd = table.getSd();\n+\t\t\ttableProps = HiveReflectionUtils.getTableMetadata(hiveShim, table);\n+\n+\t\t\tString consumeOrderStr = configuration.get(STREAMING_SOURCE_PARTITION_ORDER);\n+\t\t\tconsumeOrder = ConsumeOrder.getConsumeOrder(consumeOrderStr);\n+\t\t\tString extractorKind = configuration.get(PARTITION_TIME_EXTRACTOR_KIND);\n+\t\t\tString extractorClass = configuration.get(PARTITION_TIME_EXTRACTOR_CLASS);\n+\t\t\tString extractorPattern = configuration.get(PARTITION_TIME_EXTRACTOR_TIMESTAMP_PATTERN);\n+\n+\t\t\textractor = PartitionTimeExtractor.create(\n+\t\t\t\t\tThread.currentThread().getContextClassLoader(),\n+\t\t\t\t\textractorKind,\n+\t\t\t\t\textractorClass,\n+\t\t\t\t\textractorPattern);\n+\t\t\ttableLocation = new Path(table.getSd().getLocation());\n+\t\t\tfs = tableLocation.getFileSystem(confWrapper.conf());\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic Optional<HiveTablePartition> getPartition(List<String> partValues) throws Exception {\n+\t\t\tif (partitionKeys.isEmpty()) {\n+\t\t\t\treturn Optional.empty();\n+\t\t\t}\n+\t\t\ttry {\n+\t\t\t\tPartition partition = metaStoreClient.getPartition(\n+\t\t\t\t\t\ttablePath.getDatabaseName(),\n+\t\t\t\t\t\ttablePath.getObjectName(),\n+\t\t\t\t\t\tpartValues);\n+\t\t\t\tHiveTablePartition hiveTablePartition = HivePartitionUtils.toHiveTablePartition(\n+\t\t\t\t\t\tpartitionKeys,\n+\t\t\t\t\t\tfieldNames,\n+\t\t\t\t\t\tfieldTypes,\n+\t\t\t\t\t\thiveShim,\n+\t\t\t\t\t\ttableProps,\n+\t\t\t\t\t\tdefaultPartitionName,\n+\t\t\t\t\t\tpartition);\n+\t\t\t\treturn Optional.of(hiveTablePartition);\n+\t\t\t} catch (NoSuchObjectException e) {\n+\t\t\t\treturn Optional.empty();\n+\t\t\t}\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic Optional<HiveTablePartition> getNonPartitionedTablePartition() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "13202dd2477bfb3730e2cb039311b1a0784368dd"}, "originalPosition": 319}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIzODg0Nzg1", "url": "https://github.com/apache/flink/pull/13729#pullrequestreview-523884785", "createdAt": "2020-11-05T02:35:58Z", "commit": {"oid": "13202dd2477bfb3730e2cb039311b1a0784368dd"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQwMjozNTo1OFrOHtxb7w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQwMjozNTo1OFrOHtxb7w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzc1NzkzNQ==", "bodyText": "Do we need to change this default value to partition-name?", "url": "https://github.com/apache/flink/pull/13729#discussion_r517757935", "createdAt": "2020-11-05T02:35:58Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemOptions.java", "diffHunk": "@@ -74,24 +74,39 @@\n \t\t\t\t\t\t\t\" NOTES: Please make sure that each partition/file should be written\" +\n \t\t\t\t\t\t\t\" atomically, otherwise the reader may get incomplete data.\");\n \n+\tpublic static final ConfigOption<String> STREAMING_SOURCE_PARTITION_INCLUDE =\n+\t\t\tkey(\"streaming-source.partition.include\")\n+\t\t\t\t\t.stringType()\n+\t\t\t\t\t.defaultValue(\"all\")\n+\t\t\t\t\t.withDescription(\"Option to set the partitions to read, the supported values \" +\n+\t\t\t\t\t\t\t\"are \\\"all\\\" and \\\"latest\\\",\" +\n+\t\t\t\t\t\t\t\" the \\\"all\\\" means read all partitions; the \\\"latest\\\" means read latest \" +\n+\t\t\t\t\t\t\t\"partition in order of streaming-source.partition.order, the \\\"latest\\\" only works\" +\n+\t\t\t\t\t\t\t\" when the streaming hive source table used as temporal table. \" +\n+\t\t\t\t\t\t\t\"By default the option is \\\"all\\\".\\n.\");\n+\n \tpublic static final ConfigOption<Duration> STREAMING_SOURCE_MONITOR_INTERVAL =\n \t\t\tkey(\"streaming-source.monitor-interval\")\n \t\t\t\t\t.durationType()\n-\t\t\t\t\t.defaultValue(Duration.ofMinutes(1))\n+\t\t\t\t\t.noDefaultValue()\n \t\t\t\t\t.withDescription(\"Time interval for consecutively monitoring partition/file.\");\n \n-\tpublic static final ConfigOption<String> STREAMING_SOURCE_CONSUME_ORDER =\n-\t\t\tkey(\"streaming-source.consume-order\")\n+\tpublic static final ConfigOption<String> STREAMING_SOURCE_PARTITION_ORDER =\n+\t\t\tkey(\"streaming-source.partition-order\")\n \t\t\t\t\t.stringType()\n \t\t\t\t\t.defaultValue(\"create-time\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "13202dd2477bfb3730e2cb039311b1a0784368dd"}, "originalPosition": 27}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIzODkwMTIx", "url": "https://github.com/apache/flink/pull/13729#pullrequestreview-523890121", "createdAt": "2020-11-05T02:53:03Z", "commit": {"oid": "13202dd2477bfb3730e2cb039311b1a0784368dd"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQwMjo1MzowM1rOHtxuIg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQwMjo1MzowM1rOHtxuIg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzc2MjU5NA==", "bodyText": "Absent should throw exception", "url": "https://github.com/apache/flink/pull/13729#discussion_r517762594", "createdAt": "2020-11-05T02:53:03Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveLookupTableSource.java", "diffHunk": "@@ -0,0 +1,377 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.configuration.ReadableConfig;\n+import org.apache.flink.connectors.hive.read.HiveInputFormatPartitionReader;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.catalog.hive.client.HiveShim;\n+import org.apache.flink.table.catalog.hive.util.HivePartitionUtils;\n+import org.apache.flink.table.catalog.hive.util.HiveReflectionUtils;\n+import org.apache.flink.table.connector.source.LookupTableSource;\n+import org.apache.flink.table.connector.source.TableFunctionProvider;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.filesystem.FileSystemLookupFunction;\n+import org.apache.flink.table.filesystem.PartitionFetcher;\n+import org.apache.flink.table.filesystem.PartitionReader;\n+import org.apache.flink.table.filesystem.PartitionTimeExtractor;\n+import org.apache.flink.table.functions.TableFunction;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.metastore.IMetaStoreClient;\n+import org.apache.hadoop.hive.metastore.api.NoSuchObjectException;\n+import org.apache.hadoop.hive.metastore.api.Partition;\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.hive.metastore.api.Table;\n+import org.apache.hadoop.mapred.JobConf;\n+\n+import java.sql.Timestamp;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Properties;\n+\n+import static org.apache.flink.table.filesystem.DefaultPartTimeExtractor.toMills;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.LOOKUP_JOIN_CACHE_TTL;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_CLASS;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_KIND;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_TIMESTAMP_PATTERN;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_CONSUME_START_OFFSET;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_MONITOR_INTERVAL;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_PARTITION_INCLUDE;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_PARTITION_ORDER;\n+import static org.apache.flink.table.utils.PartitionPathUtils.extractPartitionValues;\n+\n+/**\n+ * Hive Table Source that has lookup ability.\n+ *\n+ * <p>Hive Table source has both lookup and continuous read ability, when it acts as continuous read source\n+ * it does not have the lookup ability but can be a temporal table just like other stream sources.\n+ * When it acts as bounded table, it has the lookup ability.\n+ *\n+ * <p>A common user case is use hive dimension table and always lookup the latest partition data, in this case\n+ * hive table source is a continuous read source but currently we implements it by LookupFunction. Because currently\n+ * TableSource can not tell the downstream when the latest partition has been read finished. This is a temporarily\n+ * workaround and will re-implement in the future.\n+ */\n+public class HiveLookupTableSource extends HiveTableSource implements LookupTableSource {\n+\n+\tprivate static final Duration DEFAULT_LOOKUP_MONITOR_INTERVAL = Duration.ofHours(1L);\n+\tprivate final Configuration configuration;\n+\tprivate Duration hiveTableReloadInterval;\n+\n+\tpublic HiveLookupTableSource(\n+\t\t\tJobConf jobConf,\n+\t\t\tReadableConfig flinkConf,\n+\t\t\tObjectPath tablePath,\n+\t\t\tCatalogTable catalogTable) {\n+\t\tsuper(jobConf, flinkConf, tablePath, catalogTable);\n+\t\tthis.configuration = new Configuration();\n+\t\tcatalogTable.getOptions().forEach(configuration::setString);\n+\t\tvalidateLookupConfigurations();\n+\t}\n+\n+\t@Override\n+\tpublic LookupRuntimeProvider getLookupRuntimeProvider(LookupContext context) {\n+\t\treturn TableFunctionProvider.of(getLookupFunction(context.getKeys()));\n+\t}\n+\n+\t@VisibleForTesting\n+\tTableFunction<RowData> getLookupFunction(int[][] keys) {\n+\t\tint[] keyIndices = new int[keys.length];\n+\t\tint i = 0;\n+\t\tfor (int[] key : keys) {\n+\t\t\tif (key.length > 1) {\n+\t\t\t\tthrow new UnsupportedOperationException(\"Hive lookup can not support nested key now.\");\n+\t\t\t}\n+\t\t\tkeyIndices[i] = key[0];\n+\t\t\ti++;\n+\t\t}\n+\t\treturn getLookupFunction(keyIndices);\n+\t}\n+\n+\tprivate void validateLookupConfigurations() {\n+\t\tString partitionInclude = configuration.get(STREAMING_SOURCE_PARTITION_INCLUDE);\n+\t\tif (isStreamingSource()) {\n+\t\t\tPreconditions.checkArgument(\n+\t\t\t\t\t!configuration.contains(STREAMING_SOURCE_CONSUME_START_OFFSET),\n+\t\t\t\t\tString.format(\n+\t\t\t\t\t\t\t\"The '%s' is not supported when set '%s' to 'latest'\",\n+\t\t\t\t\t\t\tSTREAMING_SOURCE_CONSUME_START_OFFSET.key(),\n+\t\t\t\t\t\t\tSTREAMING_SOURCE_PARTITION_INCLUDE.key()));\n+\n+\t\t\tDuration monitorInterval = configuration.get(STREAMING_SOURCE_MONITOR_INTERVAL) == null\n+\t\t\t\t\t? DEFAULT_LOOKUP_MONITOR_INTERVAL\n+\t\t\t\t\t: configuration.get(STREAMING_SOURCE_MONITOR_INTERVAL);\n+\t\t\tPreconditions.checkArgument(\n+\t\t\t\t\tmonitorInterval.toMillis() >= DEFAULT_LOOKUP_MONITOR_INTERVAL.toMillis(),\n+\t\t\t\t\tString.format(\n+\t\t\t\t\t\t\t\"Currently the value of '%s' is required bigger or equal to default value '%s' \" +\n+\t\t\t\t\t\t\t\t\t\"when set '%s' to 'latest', but actual is '%s'\",\n+\t\t\t\t\t\t\tSTREAMING_SOURCE_MONITOR_INTERVAL.key(),\n+\t\t\t\t\t\t\tDEFAULT_LOOKUP_MONITOR_INTERVAL.toMillis(),\n+\t\t\t\t\t\t\tSTREAMING_SOURCE_PARTITION_INCLUDE.key(),\n+\t\t\t\t\t\t\tmonitorInterval.toMillis())\n+\t\t\t);\n+\n+\t\t\thiveTableReloadInterval = monitorInterval;\n+\t\t} else {\n+\t\t\tPreconditions.checkArgument(\n+\t\t\t\t\t\"all\".equals(partitionInclude),\n+\t\t\t\t\tString.format(\"The only supported %s for lookup is '%s' in batch source,\" +\n+\t\t\t\t\t\t\t\" but actual is '%s'\", STREAMING_SOURCE_PARTITION_INCLUDE.key(), \"all\", partitionInclude));\n+\n+\t\t\thiveTableReloadInterval = configuration.get(LOOKUP_JOIN_CACHE_TTL);\n+\t\t}\n+\t}\n+\n+\tprivate TableFunction<RowData> getLookupFunction(int[] keys) {\n+\n+\t\tfinal String defaultPartitionName = jobConf.get(HiveConf.ConfVars.DEFAULTPARTITIONNAME.varname,\n+\t\t\t\tHiveConf.ConfVars.DEFAULTPARTITIONNAME.defaultStrVal);\n+\n+\t\tPartitionFetcher.Context<HiveTablePartition> fetcherContext = new HiveTablePartitionFetcherContext(\n+\t\t\t\ttablePath,\n+\t\t\t\thiveShim,\n+\t\t\t\tnew JobConfWrapper(jobConf),\n+\t\t\t\tcatalogTable.getPartitionKeys(),\n+\t\t\t\tgetProducedTableSchema().getFieldDataTypes(),\n+\t\t\t\tgetProducedTableSchema().getFieldNames(),\n+\t\t\t\tconfiguration,\n+\t\t\t\tdefaultPartitionName);\n+\n+\t\tPartitionFetcher<HiveTablePartition> partitionFetcher;\n+\t\tif (catalogTable.getPartitionKeys().isEmpty()) {\n+\t\t\t// non-partitioned table, the fetcher fetches the partition which represents the given table.\n+\t\t\tpartitionFetcher = context -> {\n+\t\t\t\tList<HiveTablePartition> partValueList = new ArrayList<>();\n+\t\t\t\tcontext.getNonPartitionedTablePartition().ifPresent(partValueList::add);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "13202dd2477bfb3730e2cb039311b1a0784368dd"}, "originalPosition": 176}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIzOTgyNzg0", "url": "https://github.com/apache/flink/pull/13729#pullrequestreview-523982784", "createdAt": "2020-11-05T07:26:35Z", "commit": {"oid": "8768964c023e7d241203485203ad5216958365a0"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQwNzoyNjozNVrOHt2Yqw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQwNzoyNjozNVrOHt2Yqw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzgzOTAxOQ==", "bodyText": "Better to org.apache.flink.connectors.hive.util, because this is for read and write instead of for HiveCatalog.", "url": "https://github.com/apache/flink/pull/13729#discussion_r517839019", "createdAt": "2020-11-05T07:26:35Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/util/HivePartitionUtils.java", "diffHunk": "@@ -0,0 +1,210 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.catalog.hive.util;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8768964c023e7d241203485203ad5216958365a0"}, "originalPosition": 19}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "8768964c023e7d241203485203ad5216958365a0", "author": {"user": {"login": "leonardBang", "name": "Leonard Xu"}}, "url": "https://github.com/apache/flink/commit/8768964c023e7d241203485203ad5216958365a0", "committedDate": "2020-11-05T05:44:02Z", "message": "minor"}, "afterCommit": {"oid": "aefdb5f32451d2a47b8cd57b46bc559ccde5b797", "author": {"user": {"login": "leonardBang", "name": "Leonard Xu"}}, "url": "https://github.com/apache/flink/commit/aefdb5f32451d2a47b8cd57b46bc559ccde5b797", "committedDate": "2020-11-06T08:33:41Z", "message": "rebase"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "aefdb5f32451d2a47b8cd57b46bc559ccde5b797", "author": {"user": {"login": "leonardBang", "name": "Leonard Xu"}}, "url": "https://github.com/apache/flink/commit/aefdb5f32451d2a47b8cd57b46bc559ccde5b797", "committedDate": "2020-11-06T08:33:41Z", "message": "rebase"}, "afterCommit": {"oid": "4a41ac978fceda679c3cd3f09a26011ed8d16029", "author": {"user": {"login": "leonardBang", "name": "Leonard Xu"}}, "url": "https://github.com/apache/flink/commit/4a41ac978fceda679c3cd3f09a26011ed8d16029", "committedDate": "2020-11-06T10:19:57Z", "message": "[FLINK-19644][hive] Support read latest partition of Hive table in temporal join"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI1MTQ5Njc0", "url": "https://github.com/apache/flink/pull/13729#pullrequestreview-525149674", "createdAt": "2020-11-06T13:30:51Z", "commit": {"oid": "4a41ac978fceda679c3cd3f09a26011ed8d16029"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQxMzozMDo1MlrOHuuGLA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQxMzozMDo1MlrOHuuGLA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODc1MTc4OA==", "bodyText": "Does this mean we require STREAMING_SOURCE_ENABLE to be set in order to load latest partition in temporal join? IMHO this limitation is not very friendly because many users still want to be able to use their hive tables in batch analysis. I think we should only require STREAMING_SOURCE_ENABLE if users want to consume data in a streaming fashion, i.e. periodically monitor files/partitions and fetch data incrementally.", "url": "https://github.com/apache/flink/pull/13729#discussion_r518751788", "createdAt": "2020-11-06T13:30:52Z", "author": {"login": "lirui-apache"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveLookupTableSource.java", "diffHunk": "@@ -0,0 +1,285 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.configuration.ReadableConfig;\n+import org.apache.flink.connectors.hive.read.HiveInputFormatPartitionReader;\n+import org.apache.flink.connectors.hive.read.HivePartitionFetcherContextBase;\n+import org.apache.flink.connectors.hive.util.HivePartitionUtils;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.catalog.hive.client.HiveShim;\n+import org.apache.flink.table.connector.source.LookupTableSource;\n+import org.apache.flink.table.connector.source.TableFunctionProvider;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.filesystem.FileSystemLookupFunction;\n+import org.apache.flink.table.filesystem.PartitionFetcher;\n+import org.apache.flink.table.filesystem.PartitionReader;\n+import org.apache.flink.table.functions.TableFunction;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.metastore.api.NoSuchObjectException;\n+import org.apache.hadoop.hive.metastore.api.Partition;\n+import org.apache.hadoop.mapred.JobConf;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static org.apache.flink.table.filesystem.FileSystemOptions.LOOKUP_JOIN_CACHE_TTL;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_CONSUME_START_OFFSET;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_MONITOR_INTERVAL;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_PARTITION_INCLUDE;\n+\n+/**\n+ * Hive Table Source that has lookup ability.\n+ *\n+ * <p>Hive Table source has both lookup and continuous read ability, when it acts as continuous read source\n+ * it does not have the lookup ability but can be a temporal table just like other stream sources.\n+ * When it acts as bounded table, it has the lookup ability.\n+ *\n+ * <p>A common user case is use hive table as dimension table and always lookup the latest partition data, in this\n+ * case, hive table source is a continuous read source but currently we implements it by LookupFunction. Because\n+ * currently TableSource can not tell the downstream when the latest partition has been read finished. This is a\n+ * temporarily workaround and will re-implement in the future.\n+ */\n+public class HiveLookupTableSource extends HiveTableSource implements LookupTableSource {\n+\n+\tprivate static final Duration DEFAULT_LOOKUP_MONITOR_INTERVAL = Duration.ofHours(1L);\n+\tprivate final Configuration configuration;\n+\tprivate Duration hiveTableReloadInterval;\n+\n+\tpublic HiveLookupTableSource(\n+\t\t\tJobConf jobConf,\n+\t\t\tReadableConfig flinkConf,\n+\t\t\tObjectPath tablePath,\n+\t\t\tCatalogTable catalogTable) {\n+\t\tsuper(jobConf, flinkConf, tablePath, catalogTable);\n+\t\tthis.configuration = new Configuration();\n+\t\tcatalogTable.getOptions().forEach(configuration::setString);\n+\t\tvalidateLookupConfigurations();\n+\t}\n+\n+\t@Override\n+\tpublic LookupRuntimeProvider getLookupRuntimeProvider(LookupContext context) {\n+\t\treturn TableFunctionProvider.of(getLookupFunction(context.getKeys()));\n+\t}\n+\n+\t@VisibleForTesting\n+\tTableFunction<RowData> getLookupFunction(int[][] keys) {\n+\t\tint[] keyIndices = new int[keys.length];\n+\t\tint i = 0;\n+\t\tfor (int[] key : keys) {\n+\t\t\tif (key.length > 1) {\n+\t\t\t\tthrow new UnsupportedOperationException(\"Hive lookup can not support nested key now.\");\n+\t\t\t}\n+\t\t\tkeyIndices[i] = key[0];\n+\t\t\ti++;\n+\t\t}\n+\t\treturn getLookupFunction(keyIndices);\n+\t}\n+\n+\tprivate void validateLookupConfigurations() {\n+\t\tString partitionInclude = configuration.get(STREAMING_SOURCE_PARTITION_INCLUDE);\n+\t\tif (isStreamingSource()) {\n+\t\t\tPreconditions.checkArgument(\n+\t\t\t\t\t!configuration.contains(STREAMING_SOURCE_CONSUME_START_OFFSET),\n+\t\t\t\t\tString.format(\n+\t\t\t\t\t\t\t\"The '%s' is not supported when set '%s' to 'latest'\",\n+\t\t\t\t\t\t\tSTREAMING_SOURCE_CONSUME_START_OFFSET.key(),\n+\t\t\t\t\t\t\tSTREAMING_SOURCE_PARTITION_INCLUDE.key()));\n+\n+\t\t\tDuration monitorInterval = configuration.get(STREAMING_SOURCE_MONITOR_INTERVAL) == null\n+\t\t\t\t\t? DEFAULT_LOOKUP_MONITOR_INTERVAL\n+\t\t\t\t\t: configuration.get(STREAMING_SOURCE_MONITOR_INTERVAL);\n+\t\t\tPreconditions.checkArgument(\n+\t\t\t\t\tmonitorInterval.toMillis() >= DEFAULT_LOOKUP_MONITOR_INTERVAL.toMillis(),\n+\t\t\t\t\tString.format(\n+\t\t\t\t\t\t\t\"Currently the value of '%s' is required bigger or equal to default value '%s' \" +\n+\t\t\t\t\t\t\t\t\t\"when set '%s' to 'latest', but actual is '%s'\",\n+\t\t\t\t\t\t\tSTREAMING_SOURCE_MONITOR_INTERVAL.key(),\n+\t\t\t\t\t\t\tDEFAULT_LOOKUP_MONITOR_INTERVAL.toMillis(),\n+\t\t\t\t\t\t\tSTREAMING_SOURCE_PARTITION_INCLUDE.key(),\n+\t\t\t\t\t\t\tmonitorInterval.toMillis())\n+\t\t\t);\n+\n+\t\t\thiveTableReloadInterval = monitorInterval;\n+\t\t} else {\n+\t\t\tPreconditions.checkArgument(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4a41ac978fceda679c3cd3f09a26011ed8d16029"}, "originalPosition": 130}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "924b755349e6ef14f84d26a83a9addb4dd9dc6c6", "author": {"user": {"login": "leonardBang", "name": "Leonard Xu"}}, "url": "https://github.com/apache/flink/commit/924b755349e6ef14f84d26a83a9addb4dd9dc6c6", "committedDate": "2020-11-07T04:13:20Z", "message": "[FLINK-19644][hive] Support read latest partition of Hive table in temporal join"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "4a41ac978fceda679c3cd3f09a26011ed8d16029", "author": {"user": {"login": "leonardBang", "name": "Leonard Xu"}}, "url": "https://github.com/apache/flink/commit/4a41ac978fceda679c3cd3f09a26011ed8d16029", "committedDate": "2020-11-06T10:19:57Z", "message": "[FLINK-19644][hive] Support read latest partition of Hive table in temporal join"}, "afterCommit": {"oid": "924b755349e6ef14f84d26a83a9addb4dd9dc6c6", "author": {"user": {"login": "leonardBang", "name": "Leonard Xu"}}, "url": "https://github.com/apache/flink/commit/924b755349e6ef14f84d26a83a9addb4dd9dc6c6", "committedDate": "2020-11-07T04:13:20Z", "message": "[FLINK-19644][hive] Support read latest partition of Hive table in temporal join"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI1NjMxNTIz", "url": "https://github.com/apache/flink/pull/13729#pullrequestreview-525631523", "createdAt": "2020-11-07T06:23:36Z", "commit": {"oid": "924b755349e6ef14f84d26a83a9addb4dd9dc6c6"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2989, "cost": 1, "resetAt": "2021-10-28T16:48:13Z"}}}