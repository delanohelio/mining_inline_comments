{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDM0NTIxNDYy", "number": 12657, "title": "[FLINK-18086][tests][e2e][kafka] Migrate SQLClientKafkaITCase to use DDL and new options to create tables", "bodyText": "What is the purpose of the change\nThe existing SQLClientKafkaITCase uses YAML to register old connectors which can't cover new connector and format implementations. However, we got many reported bugs related to classloading, class shading about the new connectors.\nThis PR migrate this e2e test to execute a SQL file by passing the lines to SQL CLI terminal by using {{java.lang.Process#getOutputStream}}. Thus we slightly update AutoClosableProcess to support setting the standard inputs.\nThis also disovers 2 bug/improvement (FLINK-18302, FLINK-18303) when we migrate to DDL and fixed together in this PR.\nBrief change log\n\nSupport to set standard inputs for AutoClosableProcess\nMigrate SQLClientKafkaITCase to use DDL and new options to create tables\nFix SQL client uses wrong class loader when execute INSERT statements\nFix Filesystem connector doesn't flush part files after rolling interval\n\nVerifying this change\nThis change is covered by existing e2e tests.\nDoes this pull request potentially affect one of the following parts:\n\nDependencies (does it add or upgrade a dependency): (yes / no)\nThe public API, i.e., is any changed class annotated with @Public(Evolving): (yes / no)\nThe serializers: (yes / no / don't know)\nThe runtime per-record code paths (performance sensitive): (yes / no / don't know)\nAnything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / no / don't know)\nThe S3 file system connector: (yes / no / don't know)\n\nDocumentation\n\nDoes this pull request introduce a new feature? (yes / no)\nIf yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)", "createdAt": "2020-06-15T12:54:30Z", "url": "https://github.com/apache/flink/pull/12657", "merged": true, "mergeCommit": {"oid": "e2db1dcc6a5a60210a4452849048b915cf794aaf"}, "closed": true, "closedAt": "2020-06-17T02:05:58Z", "author": {"login": "wuchong"}, "timelineItems": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcrrWKWgFqTQzMTEwNjQ2Nw==", "endCursor": "Y3Vyc29yOnYyOpPPAAABcr4AzNgBqjM0NDk5MDQyMTk=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMxMTA2NDY3", "url": "https://github.com/apache/flink/pull/12657#pullrequestreview-431106467", "createdAt": "2020-06-16T01:54:57Z", "commit": {"oid": "14cdb5d641588f9dfa7f8536fe92e3e57abd8e14"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMxMTM5MzU5", "url": "https://github.com/apache/flink/pull/12657#pullrequestreview-431139359", "createdAt": "2020-06-16T03:45:13Z", "commit": {"oid": "14cdb5d641588f9dfa7f8536fe92e3e57abd8e14"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQwMzo0NToxM1rOGkKOAA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQwMzo0NToxM1rOGkKOAA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDU2OTM0NA==", "bodyText": "most operations in executeQueryInternal  method and executeUpdateInternal method are already wrapped in user classloader, only deployer.deploy() is  needed. Otherwise, it's better we should remove those wrappers.  btw, add some tests in sql client to verify the fix ?", "url": "https://github.com/apache/flink/pull/12657#discussion_r440569344", "createdAt": "2020-06-16T03:45:13Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-sql-client/src/main/java/org/apache/flink/table/client/gateway/local/LocalExecutor.java", "diffHunk": "@@ -462,7 +462,7 @@ public ResolvedExpression parseSqlExpression(String sqlExpression, TableSchema i\n \t@Override\n \tpublic ResultDescriptor executeQuery(String sessionId, String query) throws SqlExecutionException {\n \t\tfinal ExecutionContext<?> context = getExecutionContext(sessionId);\n-\t\treturn executeQueryInternal(sessionId, context, query);\n+\t\treturn context.wrapClassLoader(() -> executeQueryInternal(sessionId, context, query));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "14cdb5d641588f9dfa7f8536fe92e3e57abd8e14"}, "originalPosition": 5}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "14cdb5d641588f9dfa7f8536fe92e3e57abd8e14", "author": {"user": {"login": "wuchong", "name": "Jark Wu"}}, "url": "https://github.com/apache/flink/commit/14cdb5d641588f9dfa7f8536fe92e3e57abd8e14", "committedDate": "2020-06-15T12:48:51Z", "message": "[FLINK-18303][filesystem][hive] Fix Filesystem connector doesn't flush part files after rolling interval"}, "afterCommit": {"oid": "50108ca39ff114f9c85ae5f2b5cc72eb3dd4357a", "author": {"user": {"login": "wuchong", "name": "Jark Wu"}}, "url": "https://github.com/apache/flink/commit/50108ca39ff114f9c85ae5f2b5cc72eb3dd4357a", "committedDate": "2020-06-16T03:51:25Z", "message": "[FLINK-18303][filesystem][hive] Fix Filesystem connector doesn't flush part files after rolling interval\n\nThis commit introduces option 'sink.rolling-policy.check-interval' (default 1min) to control the frequency to check part file rollover."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMxMjA3NzY5", "url": "https://github.com/apache/flink/pull/12657#pullrequestreview-431207769", "createdAt": "2020-06-16T06:54:35Z", "commit": {"oid": "50108ca39ff114f9c85ae5f2b5cc72eb3dd4357a"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQwNjo1NDozNVrOGkNl_w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQwNzowNjowN1rOGkN7Gw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDYyNDYzOQ==", "bodyText": "I don't think we need start a thread.\nThe inputs are something like commands, should print them blocking.", "url": "https://github.com/apache/flink/pull/12657#discussion_r440624639", "createdAt": "2020-06-16T06:54:35Z", "author": {"login": "JingsongLi"}, "path": "flink-end-to-end-tests/flink-end-to-end-tests-common/src/main/java/org/apache/flink/tests/util/AutoClosableProcess.java", "diffHunk": "@@ -167,6 +186,18 @@ private static void processStream(final InputStream stream, final Consumer<Strin\n \t\t).start();\n \t}\n \n+\tprivate static void processOutputStream(final OutputStream stream, final List<String> inputs) {\n+\t\tnew Thread(() -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "50108ca39ff114f9c85ae5f2b5cc72eb3dd4357a"}, "originalPosition": 97}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDYyNTMzOA==", "bodyText": "processOutputStream(inputs) looks confuse to me. Can we just name it printLinesToProcess?", "url": "https://github.com/apache/flink/pull/12657#discussion_r440625338", "createdAt": "2020-06-16T06:56:00Z", "author": {"login": "JingsongLi"}, "path": "flink-end-to-end-tests/flink-end-to-end-tests-common/src/main/java/org/apache/flink/tests/util/AutoClosableProcess.java", "diffHunk": "@@ -167,6 +186,18 @@ private static void processStream(final InputStream stream, final Consumer<Strin\n \t\t).start();\n \t}\n \n+\tprivate static void processOutputStream(final OutputStream stream, final List<String> inputs) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "50108ca39ff114f9c85ae5f2b5cc72eb3dd4357a"}, "originalPosition": 96}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDYyNjQ5OQ==", "bodyText": "inputs -> inputLines?", "url": "https://github.com/apache/flink/pull/12657#discussion_r440626499", "createdAt": "2020-06-16T06:58:23Z", "author": {"login": "JingsongLi"}, "path": "flink-end-to-end-tests/flink-end-to-end-tests-common/src/main/java/org/apache/flink/tests/util/AutoClosableProcess.java", "diffHunk": "@@ -167,6 +186,18 @@ private static void processStream(final InputStream stream, final Consumer<Strin\n \t\t).start();\n \t}\n \n+\tprivate static void processOutputStream(final OutputStream stream, final List<String> inputs) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "50108ca39ff114f9c85ae5f2b5cc72eb3dd4357a"}, "originalPosition": 96}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDYyNjYwMQ==", "bodyText": "inputLines can be String...", "url": "https://github.com/apache/flink/pull/12657#discussion_r440626601", "createdAt": "2020-06-16T06:58:36Z", "author": {"login": "JingsongLi"}, "path": "flink-end-to-end-tests/flink-end-to-end-tests-common/src/main/java/org/apache/flink/tests/util/AutoClosableProcess.java", "diffHunk": "@@ -89,6 +95,11 @@ public AutoClosableProcessBuilder setStderrProcessor(final Consumer<String> stde\n \t\t\treturn this;\n \t\t}\n \n+\t\tpublic AutoClosableProcessBuilder setStdInputs(final List<String> inputLines) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "50108ca39ff114f9c85ae5f2b5cc72eb3dd4357a"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDYzMDA0Mw==", "bodyText": "Add disable-quote-character to remove \"?", "url": "https://github.com/apache/flink/pull/12657#discussion_r440630043", "createdAt": "2020-06-16T07:06:07Z", "author": {"login": "JingsongLi"}, "path": "flink-end-to-end-tests/flink-end-to-end-tests-common-kafka/src/test/java/org/apache/flink/tests/util/kafka/SQLClientKafkaITCase.java", "diffHunk": "@@ -179,80 +174,55 @@ public void testKafka() throws Exception {\n \t\t}\n \t}\n \n-\tprivate void insertIntoAvroTable(ClusterController clusterController) throws IOException {\n-\t\tLOG.info(\"Executing SQL: Kafka {} JSON -> Kafka {} Avro\", kafkaSQLVersion, kafkaSQLVersion);\n-\t\tString sqlStatement1 = \"INSERT INTO AvroBothTable\\n\" +\n-\t\t\t\t\"  SELECT\\n\" +\n-\t\t\t\t\"    CAST(TUMBLE_START(rowtime, INTERVAL '1' HOUR) AS VARCHAR) AS event_timestamp,\\n\" +\n-\t\t\t\t\"    user,\\n\" +\n-\t\t\t\t\"    RegReplace(event.message, ' is ', ' was ') AS message,\\n\" +\n-\t\t\t\t\"    COUNT(*) AS duplicate_count\\n\" +\n-\t\t\t\t\"  FROM JsonSourceTable\\n\" +\n-\t\t\t\t\"  WHERE user IS NOT NULL\\n\" +\n-\t\t\t\t\"  GROUP BY\\n\" +\n-\t\t\t\t\"    user,\\n\" +\n-\t\t\t\t\"    event.message,\\n\" +\n-\t\t\t\t\"    TUMBLE(rowtime, INTERVAL '1' HOUR)\";\n-\n-\t\tclusterController.submitSQLJob(new SQLJobSubmission.SQLJobSubmissionBuilder(sqlStatement1)\n-\t\t\t\t.addJar(sqlAvroJar)\n-\t\t\t\t.addJars(apacheAvroJars)\n-\t\t\t\t.addJar(sqlJsonJar)\n-\t\t\t\t.addJar(sqlConnectorKafkaJar)\n-\t\t\t\t.addJar(sqlToolBoxJar)\n-\t\t\t\t.setSessionEnvFile(this.sqlClientSessionConf.toAbsolutePath().toString())\n-\t\t\t\t.build());\n-\t}\n-\n-\tprivate void insertIntoCsvSinkTable(ClusterController clusterController) throws IOException {\n-\t\tLOG.info(\"Executing SQL: Kafka {} Avro -> Csv sink\", kafkaSQLVersion);\n-\t\tString sqlStatement2 = \"INSERT INTO CsvSinkTable\\n\" +\n-\t\t\t\t\"   SELECT AvroBothTable.*, RegReplace('Test constant folding.', 'Test', 'Success') AS constant\\n\" +\n-\t\t\t\t\"   FROM AvroBothTable\";\n-\n-\t\tclusterController.submitSQLJob(new SQLJobSubmission.SQLJobSubmissionBuilder(sqlStatement2)\n-\t\t\t\t.addJar(sqlAvroJar)\n-\t\t\t\t.addJars(apacheAvroJars)\n-\t\t\t\t.addJar(sqlJsonJar)\n-\t\t\t\t.addJar(sqlConnectorKafkaJar)\n-\t\t\t\t.addJar(sqlToolBoxJar)\n-\t\t\t\t.setSessionEnvFile(this.sqlClientSessionConf.toAbsolutePath().toString())\n-\t\t\t\t.build()\n-\t\t);\n+\tprivate void executeSqlStatements(ClusterController clusterController, List<String> sqlLines) throws IOException {\n+\t\tLOG.info(\"Executing Kafka {} end-to-end SQL statements.\", kafkaSQLVersion);\n+\t\tclusterController.submitSQLJob(new SQLJobSubmission.SQLJobSubmissionBuilder(sqlLines)\n+\t\t\t.addJar(sqlAvroJar)\n+\t\t\t.addJars(apacheAvroJars)\n+\t\t\t.addJar(sqlConnectorKafkaJar)\n+\t\t\t.addJar(sqlToolBoxJar)\n+\t\t\t.build());\n \t}\n \n-\tprivate String initializeSessionYaml(Map<String, String> vars) throws IOException {\n-\t\tURL url = SQLClientKafkaITCase.class.getClassLoader().getResource(KAFKA_JSON_SOURCE_SCHEMA_YAML);\n+\tprivate List<String> initializeSqlLines(Map<String, String> vars) throws IOException {\n+\t\tURL url = SQLClientKafkaITCase.class.getClassLoader().getResource(KAFKA_E2E_SQL);\n \t\tif (url == null) {\n-\t\t\tthrow new FileNotFoundException(KAFKA_JSON_SOURCE_SCHEMA_YAML);\n+\t\t\tthrow new FileNotFoundException(KAFKA_E2E_SQL);\n \t\t}\n \n-\t\tString schema = FileUtils.readFileUtf8(new File(url.getFile()));\n-\t\tfor (Map.Entry<String, String> var : vars.entrySet()) {\n-\t\t\tschema = schema.replace(var.getKey(), var.getValue());\n+\t\tList<String> lines = Files.readAllLines(new File(url.getFile()).toPath());\n+\t\tList<String> result = new ArrayList<>();\n+\t\tfor (String line : lines) {\n+\t\t\tfor (Map.Entry<String, String> var : vars.entrySet()) {\n+\t\t\t\tline = line.replace(var.getKey(), var.getValue());\n+\t\t\t}\n+\t\t\tresult.add(line);\n \t\t}\n-\t\treturn schema;\n+\n+\t\treturn result;\n \t}\n \n \tprivate void checkCsvResultFile() throws Exception {\n \t\tboolean success = false;\n \t\tfinal Deadline deadline = Deadline.fromNow(Duration.ofSeconds(120));\n-\t\twhile (!success && deadline.hasTimeLeft()) {\n+\t\twhile (deadline.hasTimeLeft()) {\n \t\t\tif (Files.exists(result)) {\n-\t\t\t\tbyte[] bytes = Files.readAllBytes(result);\n-\t\t\t\tString[] lines = new String(bytes, Charsets.UTF_8).split(\"\\n\");\n-\t\t\t\tif (lines.length == 4) {\n+\t\t\t\tList<String> lines = readCsvResultFiles(result);\n+\t\t\t\tif (lines.size() == 4) {\n \t\t\t\t\tsuccess = true;\n \t\t\t\t\tassertThat(\n-\t\t\t\t\t\tlines,\n+\t\t\t\t\t\tlines.toArray(new String[0]),\n \t\t\t\t\t\tarrayContainingInAnyOrder(\n-\t\t\t\t\t\t\t\"2018-03-12 08:00:00.000,Alice,This was a warning.,2,Success constant folding.\",\n-\t\t\t\t\t\t\t\"2018-03-12 09:00:00.000,Bob,This was another warning.,1,Success constant folding.\",\n-\t\t\t\t\t\t\t\"2018-03-12 09:00:00.000,Steve,This was another info.,2,Success constant folding.\",\n-\t\t\t\t\t\t\t\"2018-03-12 09:00:00.000,Alice,This was a info.,1,Success constant folding.\"\n+\t\t\t\t\t\t\t\"\\\"2018-03-12 08:00:00.000\\\",Alice,\\\"This was a warning.\\\",2,\\\"Success constant folding.\\\"\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "50108ca39ff114f9c85ae5f2b5cc72eb3dd4357a"}, "originalPosition": 248}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMxMzI5ODY2", "url": "https://github.com/apache/flink/pull/12657#pullrequestreview-431329866", "createdAt": "2020-06-16T09:35:46Z", "commit": {"oid": "50108ca39ff114f9c85ae5f2b5cc72eb3dd4357a"}, "state": "APPROVED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQwOTozNTo0N1rOGkTV5A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQwOTozNTo0N1rOGkTV5A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDcxODgyMA==", "bodyText": "Actually, try with resource will close this output put stream, and in this way, will close process.\nI think you should create a better method name for this, and add comments to explain.", "url": "https://github.com/apache/flink/pull/12657#discussion_r440718820", "createdAt": "2020-06-16T09:35:47Z", "author": {"login": "JingsongLi"}, "path": "flink-end-to-end-tests/flink-end-to-end-tests-common/src/main/java/org/apache/flink/tests/util/AutoClosableProcess.java", "diffHunk": "@@ -167,6 +186,18 @@ private static void processStream(final InputStream stream, final Consumer<Strin\n \t\t).start();\n \t}\n \n+\tprivate static void processOutputStream(final OutputStream stream, final List<String> inputs) {\n+\t\tnew Thread(() -> {\n+\t\t\ttry (PrintStream printStream = new PrintStream(stream, true, StandardCharsets.UTF_8.name())) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "50108ca39ff114f9c85ae5f2b5cc72eb3dd4357a"}, "originalPosition": 98}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "475eb448054654a9d4cfd0e3e8b32f9f12cdec83", "author": {"user": {"login": "wuchong", "name": "Jark Wu"}}, "url": "https://github.com/apache/flink/commit/475eb448054654a9d4cfd0e3e8b32f9f12cdec83", "committedDate": "2020-06-16T08:33:52Z", "message": "address Jingsong's review comment"}, "afterCommit": {"oid": "957ed3e9d9c1d0b48d11f6573dc3a0d85de01fd9", "author": {"user": {"login": "wuchong", "name": "Jark Wu"}}, "url": "https://github.com/apache/flink/commit/957ed3e9d9c1d0b48d11f6573dc3a0d85de01fd9", "committedDate": "2020-06-16T15:16:21Z", "message": "[FLINK-18303][filesystem][hive] Fix Filesystem connector doesn't flush part files after rolling interval\n\nThis commit introduces option 'sink.rolling-policy.check-interval' (default 1min) to control the frequency to check part file rollover."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7ce45b5c846fa276c21e2c7aa4b9ff1c29ddb1f7", "author": {"user": {"login": "wuchong", "name": "Jark Wu"}}, "url": "https://github.com/apache/flink/commit/7ce45b5c846fa276c21e2c7aa4b9ff1c29ddb1f7", "committedDate": "2020-06-16T16:39:51Z", "message": "[FLINK-18086][tests] Support to set standard inputs for AutoClosableProcess"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "06d96b5f671575b0334b9ee21829e8c8e178f3c6", "author": {"user": {"login": "wuchong", "name": "Jark Wu"}}, "url": "https://github.com/apache/flink/commit/06d96b5f671575b0334b9ee21829e8c8e178f3c6", "committedDate": "2020-06-16T16:39:51Z", "message": "[FLINK-18086][e2e] Migrate SQLClientKafkaITCase to use DDL and new options to create tables\n\nThis closes #12657"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ca1f7642c7120800f1ac93575dcdfa0223c097a4", "author": {"user": {"login": "wuchong", "name": "Jark Wu"}}, "url": "https://github.com/apache/flink/commit/ca1f7642c7120800f1ac93575dcdfa0223c097a4", "committedDate": "2020-06-16T16:39:51Z", "message": "[FLINK-18302][sql-cli] Fix SQL client uses wrong class loader when execute INSERT statements"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e2db1dcc6a5a60210a4452849048b915cf794aaf", "author": {"user": {"login": "wuchong", "name": "Jark Wu"}}, "url": "https://github.com/apache/flink/commit/e2db1dcc6a5a60210a4452849048b915cf794aaf", "committedDate": "2020-06-16T16:39:51Z", "message": "[FLINK-18303][filesystem][hive] Fix Filesystem connector doesn't flush part files after rolling interval\n\nThis commit introduces option 'sink.rolling-policy.check-interval' (default 1min) to control the frequency to check part file rollover."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "957ed3e9d9c1d0b48d11f6573dc3a0d85de01fd9", "author": {"user": {"login": "wuchong", "name": "Jark Wu"}}, "url": "https://github.com/apache/flink/commit/957ed3e9d9c1d0b48d11f6573dc3a0d85de01fd9", "committedDate": "2020-06-16T15:16:21Z", "message": "[FLINK-18303][filesystem][hive] Fix Filesystem connector doesn't flush part files after rolling interval\n\nThis commit introduces option 'sink.rolling-policy.check-interval' (default 1min) to control the frequency to check part file rollover."}, "afterCommit": {"oid": "e2db1dcc6a5a60210a4452849048b915cf794aaf", "author": {"user": {"login": "wuchong", "name": "Jark Wu"}}, "url": "https://github.com/apache/flink/commit/e2db1dcc6a5a60210a4452849048b915cf794aaf", "committedDate": "2020-06-16T16:39:51Z", "message": "[FLINK-18303][filesystem][hive] Fix Filesystem connector doesn't flush part files after rolling interval\n\nThis commit introduces option 'sink.rolling-policy.check-interval' (default 1min) to control the frequency to check part file rollover."}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3486, "cost": 1, "resetAt": "2021-10-28T16:48:13Z"}}}