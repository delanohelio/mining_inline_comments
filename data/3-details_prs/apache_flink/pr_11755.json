{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDAzNzM4NDU4", "number": 11755, "title": "[FLINK-14257][Connectors / FileSystem]Integrate csv to FileSystemTableFactory", "bodyText": "What is the purpose of the change\nThis pull request  Integrates  csv format to FileSystemTableFactory.\nBrief change log\n\nimport BaseRowCsvInputformat\nimport BaseRowCsvFileSystemFormatFactory\n\nVerifying this change\nAdd BaseRowCsvFilesystemITCase and unit tests in formats\nDoes this pull request potentially affect one of the following parts:\n\nDependencies (does it add or upgrade a dependency): (no)\nThe public API, i.e., is any changed class annotated with @Public(Evolving): (no)\nThe serializers: (no)\nThe runtime per-record code paths (performance sensitive): ( no)\nAnything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)\nThe S3 file system connector: (no)\n\nDocumentation\n\nDoes this pull request introduce a new feature? (yes)\nIf yes, how is the feature documented? (avaDocs)", "createdAt": "2020-04-15T12:47:24Z", "url": "https://github.com/apache/flink/pull/11755", "merged": true, "mergeCommit": {"oid": "66308f8c9f899b85d38d893eded0a335cadf3c9b"}, "closed": true, "closedAt": "2020-05-12T07:50:22Z", "author": {"login": "leonardBang"}, "timelineItems": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcaA6YigFqTM5NzgzOTUzMQ==", "endCursor": "Y3Vyc29yOnYyOpPPAAABcgfdLPgFqTQwOTc4NDM2Nw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk3ODM5NTMx", "url": "https://github.com/apache/flink/pull/11755#pullrequestreview-397839531", "createdAt": "2020-04-22T04:43:55Z", "commit": {"oid": "00fde88b6f1a151b95301a739699cbb41964d4bd"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwNDo0Mzo1NVrOGJjG5A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwNDo0ODo1MFrOGJjM3Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY2NTU3Mg==", "bodyText": "Can we abstract RowCsvInputFormatTest to reuse code?", "url": "https://github.com/apache/flink/pull/11755#discussion_r412665572", "createdAt": "2020-04-22T04:43:55Z", "author": {"login": "JingsongLi"}, "path": "flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/BaseRowCsvInputformatTest.java", "diffHunk": "@@ -0,0 +1,798 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.csv;\n+\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.api.DataTypes;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.dataformat.BaseRow;\n+import org.apache.flink.table.dataformat.GenericRow;\n+import org.apache.flink.table.dataformat.SqlTimestamp;\n+import org.apache.flink.table.dataformat.TypeGetterSetters;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+\n+import org.junit.Test;\n+\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStreamWriter;\n+import java.nio.charset.StandardCharsets;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+import static junit.framework.TestCase.assertEquals;\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.Assert.assertNull;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n+\n+/**\n+ * Test suites for {@link BaseRowCsvInputformat}.\n+ */\n+public class BaseRowCsvInputformatTest {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "00fde88b6f1a151b95301a739699cbb41964d4bd"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY2NTc0NA==", "bodyText": "After #11796 , please add streaming test too.", "url": "https://github.com/apache/flink/pull/11755#discussion_r412665744", "createdAt": "2020-04-22T04:44:28Z", "author": {"login": "JingsongLi"}, "path": "flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/BaseRowCsvFilesystemITCase.java", "diffHunk": "@@ -0,0 +1,41 @@\n+package org.apache.flink.formats.csv;\n+\n+import org.apache.flink.table.planner.runtime.batch.sql.BatchFileSystemITCaseBase;\n+\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.List;\n+\n+/**\n+ * Test fot {@link BaseRowCsvFileSystemFormatFactory}.\n+ */\n+@RunWith(Parameterized.class)\n+public class BaseRowCsvFilesystemITCase extends BatchFileSystemITCaseBase {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "00fde88b6f1a151b95301a739699cbb41964d4bd"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY2NjYzMA==", "bodyText": "This class is for single row test? I think we don't need test single row, should be include in CsvRowDeSerializationSchemaTest.\nWe should add multi-rows test, or just add tests in RowCsvFilesystemITCase.java", "url": "https://github.com/apache/flink/pull/11755#discussion_r412666630", "createdAt": "2020-04-22T04:47:18Z", "author": {"login": "JingsongLi"}, "path": "flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/BaseRowCsvDeSerializationTest.java", "diffHunk": "@@ -0,0 +1,306 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.csv;\n+\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.api.DataTypes;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.dataformat.BaseRow;\n+import org.apache.flink.table.dataformat.DataFormatConverters;\n+import org.apache.flink.table.dataformat.GenericRow;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.util.FileUtils;\n+import org.apache.flink.util.TestLogger;\n+\n+import org.junit.Assert;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStreamWriter;\n+import java.math.BigDecimal;\n+import java.nio.charset.StandardCharsets;\n+import java.sql.Date;\n+import java.sql.Time;\n+import java.sql.Timestamp;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.util.ArrayList;\n+import java.util.function.Consumer;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNull;\n+import static org.junit.Assert.fail;\n+\n+/**\n+ * Test for {@link BaseRowCsvInputformat} and {@link BaseRowCsvEncoder}.\n+ */\n+public class BaseRowCsvDeSerializationTest extends TestLogger {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "00fde88b6f1a151b95301a739699cbb41964d4bd"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY2NzEwMQ==", "bodyText": "use RowPartitionComputer.restorePartValueFromType instead", "url": "https://github.com/apache/flink/pull/11755#discussion_r412667101", "createdAt": "2020-04-22T04:48:50Z", "author": {"login": "JingsongLi"}, "path": "flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/BaseRowCsvInputformat.java", "diffHunk": "@@ -0,0 +1,310 @@\n+package org.apache.flink.formats.csv;\n+\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeinfo.Types;\n+import org.apache.flink.api.java.typeutils.RowTypeInfo;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.api.ValidationException;\n+import org.apache.flink.table.dataformat.BaseRow;\n+import org.apache.flink.table.dataformat.BinaryString;\n+import org.apache.flink.table.dataformat.DataFormatConverters;\n+import org.apache.flink.table.dataformat.GenericRow;\n+import org.apache.flink.table.filesystem.PartitionPathUtils;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.utils.TypeConversions;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.JsonNode;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.MappingIterator;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvMapper;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvSchema;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.NoSuchElementException;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.formats.csv.CsvRowDeserializationSchema.createFieldRuntimeConverters;\n+import static org.apache.flink.formats.csv.CsvRowDeserializationSchema.validateArity;\n+import static org.apache.flink.util.Preconditions.checkNotNull;\n+\n+/**\n+ * Input format that reads csv file into {@link BaseRow}.\n+ */\n+public class BaseRowCsvInputformat extends AbstractCsvInputFormat<BaseRow> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate final List<TypeInformation> fieldTypes;\n+\tprivate final List<String> fieldNames;\n+\tprivate final int[] selectFields;\n+\tprivate final List<String> partitionKeys;\n+\tprivate final String defaultPartValue;\n+\tprivate final boolean ignoreParseErrors;\n+\tprivate final long limit;\n+\tprivate final List<String> csvFieldNames;\n+\tprivate final List<TypeInformation> csvFieldTypes;\n+\tprivate final List<String> csvSelectFieldNames;\n+\tprivate final List<TypeInformation> csvSelectTypes;\n+\tprivate final int[] csvFieldMapping;\n+\n+\tprivate transient CsvRowDeserializationSchema.RuntimeConverter runtimeConverter;\n+\tprivate transient List<DataFormatConverters.DataFormatConverter> csvSelectConverters;\n+\tprivate transient MappingIterator<JsonNode> iterator;\n+\tprivate transient boolean end;\n+\tprivate transient long emitted;\n+\t// reuse object for per record\n+\tprivate transient GenericRow rowData;\n+\n+\tprivate BaseRowCsvInputformat(\n+\t\tPath[] filePaths,\n+\t\tCsvSchema csvSchema,\n+\t\tRowTypeInfo rowType,\n+\t\tint[] selectFields,\n+\t\tList<String> partitionKeys,\n+\t\tString defaultPartValue,\n+\t\tboolean ignoreParseErrors,\n+\t\tlong limit) {\n+\t\t\tsuper(filePaths, csvSchema);\n+\t\t\tthis.partitionKeys = partitionKeys;\n+\t\t\tthis.defaultPartValue = defaultPartValue;\n+\t\t\tthis.selectFields = selectFields;\n+\t\t\tthis.ignoreParseErrors = ignoreParseErrors;\n+\t\t\tthis.limit = limit;\n+\t\t\tthis.fieldTypes = Arrays.asList(rowType.getFieldTypes());\n+\t\t\tthis.fieldNames = Arrays.asList(rowType.getFieldNames());\n+\t\t\tthis.emitted = 0;\n+\t\t\t// partition field\n+\t\t\tthis.csvFieldNames = fieldNames.stream()\n+\t\t\t\t.filter(name -> !partitionKeys.contains(name)).collect(Collectors.toList());\n+\t\t\tthis.csvFieldTypes = csvFieldNames.stream()\n+\t\t\t.map(name -> fieldTypes.get(fieldNames.indexOf(name))).collect(Collectors.toList());\n+\t\t\t// project field\n+\t\t\tList<String> selectFieldNames = Arrays.stream(selectFields)\n+\t\t\t\t.mapToObj(fieldNames::get)\n+\t\t\t\t.collect(Collectors.toList());\n+\t\t\tthis.csvSelectFieldNames = selectFieldNames.stream()\n+\t\t\t\t.filter(name -> !partitionKeys.contains(name)).collect(Collectors.toList());\n+\t\t\tthis.csvSelectTypes = csvSelectFieldNames.stream()\n+\t\t\t\t.map(name -> fieldTypes.get(fieldNames.indexOf(name))).collect(Collectors.toList());\n+\t\t\tthis.csvFieldMapping = csvSelectFieldNames.stream().mapToInt(selectFieldNames::indexOf).toArray();\n+\t}\n+\n+\t@Override\n+\tpublic void open(FileInputSplit split) throws IOException {\n+\t\tsuper.open(split);\n+\t\tthis.end = false;\n+\t\tthis.iterator = new CsvMapper()\n+\t\t\t.readerFor(JsonNode.class)\n+\t\t\t.with(csvSchema)\n+\t\t\t.readValues(csvInputStream);\n+\t\tprepareRuntimeConverter();\n+\t\tfillPartitionValueForRecord();\n+\t}\n+\n+\tprivate void fillPartitionValueForRecord() {\n+\t\trowData = new GenericRow(selectFields.length);\n+\t\tPath path = currentSplit.getPath();\n+\t\tLinkedHashMap<String, String> partSpec = PartitionPathUtils.extractPartitionSpecFromPath(path);\n+\t\tfor (int i = 0; i < selectFields.length; i++) {\n+\t\t\tint selectField = selectFields[i];\n+\t\t\tString name = fieldNames.get(selectField);\n+\t\t\tif (partitionKeys.contains(name)) {\n+\t\t\t\tString value = partSpec.get(name);\n+\t\t\t\tvalue = defaultPartValue.equals(value) ? null : value;\n+\t\t\t\trowData.setField(i, convertStringToInternal(value, fieldTypes.get(selectField)));\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate Object convertStringToInternal(String value, TypeInformation type) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "00fde88b6f1a151b95301a739699cbb41964d4bd"}, "originalPosition": 125}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "00fde88b6f1a151b95301a739699cbb41964d4bd", "author": {"user": {"login": "leonardBang", "name": "Leonard Xu"}}, "url": "https://github.com/apache/flink/commit/00fde88b6f1a151b95301a739699cbb41964d4bd", "committedDate": "2020-04-15T12:49:07Z", "message": "[FLINK-14257][Connectors / FileSystem]Integrate csv to FileSystemTableFactory"}, "afterCommit": {"oid": "dff5cdb51db746c02151afd5b4624555def81fac", "author": {"user": {"login": "leonardBang", "name": "Leonard Xu"}}, "url": "https://github.com/apache/flink/commit/dff5cdb51db746c02151afd5b4624555def81fac", "committedDate": "2020-05-07T08:28:41Z", "message": "[FLINK-14257][Connectors / FileSystem]Integrate csv to FileSystemTableFactory."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "dff5cdb51db746c02151afd5b4624555def81fac", "author": {"user": {"login": "leonardBang", "name": "Leonard Xu"}}, "url": "https://github.com/apache/flink/commit/dff5cdb51db746c02151afd5b4624555def81fac", "committedDate": "2020-05-07T08:28:41Z", "message": "[FLINK-14257][Connectors / FileSystem]Integrate csv to FileSystemTableFactory."}, "afterCommit": {"oid": "4be08fc2530e1ed4eafe14e3c0f16780e01aa7d0", "author": {"user": {"login": "leonardBang", "name": "Leonard Xu"}}, "url": "https://github.com/apache/flink/commit/4be08fc2530e1ed4eafe14e3c0f16780e01aa7d0", "committedDate": "2020-05-07T15:06:16Z", "message": "[FLINK-14267] Introduce Row Csv Encoder && [FLINK-14257] Integrate csv to file system connector"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDA4NjAxODI1", "url": "https://github.com/apache/flink/pull/11755#pullrequestreview-408601825", "createdAt": "2020-05-09T02:08:43Z", "commit": {"oid": "64d407d3530d308b4504282ae8fadb1503955f99"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQwMjowODo0M1rOGS3w8Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMVQwNDowMzo1NVrOGTLsRg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ0MTIwMQ==", "bodyText": "Add empty line.", "url": "https://github.com/apache/flink/pull/11755#discussion_r422441201", "createdAt": "2020-05-09T02:08:43Z", "author": {"login": "JingsongLi"}, "path": "flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvRowDataFileSystemFormatFactory.java", "diffHunk": "@@ -0,0 +1,354 @@\n+package org.apache.flink.formats.csv;\n+\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.GenericTypeInfo;\n+import org.apache.flink.api.java.typeutils.RowTypeInfo;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.api.ValidationException;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.FileSystemFormatFactory;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.utils.PartitionPathUtils;\n+\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvSchema;\n+\n+import java.io.BufferedReader;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.io.OutputStream;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ALLOW_COMMENTS;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ARRAY_ELEMENT_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_DISABLE_QUOTE_CHARACTER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ESCAPE_CHARACTER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_FIELD_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_IGNORE_PARSE_ERRORS;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_LINE_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_NULL_LITERAL;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_QUOTE_CHARACTER;\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+import static org.apache.flink.util.Preconditions.checkNotNull;\n+\n+/**\n+ * CSV format factory for file system.\n+ */\n+public class CsvRowDataFileSystemFormatFactory implements FileSystemFormatFactory {\n+\t@Override", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "64d407d3530d308b4504282ae8fadb1503955f99"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ0MTIxMA==", "bodyText": "Remove empty line", "url": "https://github.com/apache/flink/pull/11755#discussion_r422441210", "createdAt": "2020-05-09T02:08:52Z", "author": {"login": "JingsongLi"}, "path": "flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvRowDataFileSystemFormatFactory.java", "diffHunk": "@@ -0,0 +1,354 @@\n+package org.apache.flink.formats.csv;\n+\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.GenericTypeInfo;\n+import org.apache.flink.api.java.typeutils.RowTypeInfo;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.api.ValidationException;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.FileSystemFormatFactory;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.utils.PartitionPathUtils;\n+\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvSchema;\n+\n+import java.io.BufferedReader;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.io.OutputStream;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ALLOW_COMMENTS;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ARRAY_ELEMENT_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_DISABLE_QUOTE_CHARACTER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ESCAPE_CHARACTER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_FIELD_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_IGNORE_PARSE_ERRORS;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_LINE_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_NULL_LITERAL;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_QUOTE_CHARACTER;\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+import static org.apache.flink.util.Preconditions.checkNotNull;\n+\n+/**\n+ * CSV format factory for file system.\n+ */\n+public class CsvRowDataFileSystemFormatFactory implements FileSystemFormatFactory {\n+\t@Override\n+\tpublic InputFormat<RowData, ?> createReader(ReaderContext context) {\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "64d407d3530d308b4504282ae8fadb1503955f99"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjc2NzQ5OQ==", "bodyText": "return null after end = true. And remove !reachedEnd() in while.\nThis can make codes clear and correct emitted number.", "url": "https://github.com/apache/flink/pull/11755#discussion_r422767499", "createdAt": "2020-05-11T04:03:15Z", "author": {"login": "JingsongLi"}, "path": "flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvRowDataFileSystemFormatFactory.java", "diffHunk": "@@ -0,0 +1,352 @@\n+package org.apache.flink.formats.csv;\n+\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.api.java.typeutils.GenericTypeInfo;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.formats.csv.CsvRowDataDeserializationSchema.DeserializationRuntimeConverter;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.FileSystemFormatFactory;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.utils.PartitionPathUtils;\n+\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.JsonNode;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.MappingIterator;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvMapper;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvSchema;\n+\n+import java.io.BufferedReader;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.io.OutputStream;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NoSuchElementException;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ALLOW_COMMENTS;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ARRAY_ELEMENT_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_DISABLE_QUOTE_CHARACTER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ESCAPE_CHARACTER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_FIELD_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_IGNORE_PARSE_ERRORS;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_LINE_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_NULL_LITERAL;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_QUOTE_CHARACTER;\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+\n+/**\n+ * CSV format factory for file system.\n+ */\n+public class CsvRowDataFileSystemFormatFactory implements FileSystemFormatFactory {\n+\t@Override\n+\tpublic InputFormat<RowData, ?> createReader(ReaderContext context) {\n+\n+\t\tDescriptorProperties properties = getValidatedProperties(context.getFormatProperties());\n+\n+\t\tRowType formatRowType = context.getFormatRowType();\n+\n+\t\tString[] fieldNames = context.getSchema().getFieldNames();\n+\t\tList<String> projectFields = Arrays.stream(context.getProjectFields())\n+\t\t\t.mapToObj(idx -> fieldNames[idx])\n+\t\t\t.collect(Collectors.toList());\n+\t\tList<String> csvFields = Arrays.stream(fieldNames)\n+\t\t\t.filter(field -> !context.getPartitionKeys().contains(field))\n+\t\t\t.collect(Collectors.toList());\n+\n+\t\tint[] csvSelectFieldToProjectFieldMapping = context.getFormatProjectFields().stream()\n+\t\t\t.mapToInt(projectFields::indexOf)\n+\t\t\t.toArray();\n+\t\tint[] csvSelectFieldToCsvFieldMapping = context.getFormatProjectFields().stream()\n+\t\t\t.mapToInt(csvFields::indexOf)\n+\t\t\t.toArray();\n+\n+\t\tCsvSchema csvSchema = buildCsvSchema(formatRowType, properties);\n+\n+\t\tboolean ignoreParseErrors = properties.getOptionalBoolean(FORMAT_IGNORE_PARSE_ERRORS).orElse(false);\n+\n+\t\treturn new CsvInputFormat(\n+\t\t\tcontext.getPaths(),\n+\t\t\tcontext.getSchema().getFieldDataTypes(),\n+\t\t\tcontext.getSchema().getFieldNames(),\n+\t\t\tcsvSchema,\n+\t\t\tformatRowType,\n+\t\t\tcontext.getProjectFields(),\n+\t\t\tcontext.getPartitionKeys(),\n+\t\t\tcontext.getDefaultPartName(),\n+\t\t\tcontext.getPushedDownLimit(),\n+\t\t\tcsvSelectFieldToProjectFieldMapping,\n+\t\t\tcsvSelectFieldToCsvFieldMapping,\n+\t\t\tignoreParseErrors);\n+\t}\n+\n+\tprivate CsvSchema buildCsvSchema(RowType rowType, DescriptorProperties properties) {\n+\t\tCsvSchema csvSchema = CsvRowSchemaConverter.convert(rowType);\n+\t\t//format properties\n+\t\tif (properties.containsKey(FORMAT_FIELD_DELIMITER)) {\n+\t\t\tcsvSchema = csvSchema.rebuild()\n+\t\t\t\t.setColumnSeparator(properties.getCharacter(FORMAT_FIELD_DELIMITER))\n+\t\t\t\t.build();\n+\t\t}\n+\t\tif (properties.containsKey(FORMAT_QUOTE_CHARACTER)) {\n+\t\t\tcsvSchema = csvSchema.rebuild()\n+\t\t\t\t.setQuoteChar(properties.getCharacter(FORMAT_QUOTE_CHARACTER))\n+\t\t\t\t.build();\n+\t\t}\n+\t\tif (properties.containsKey(FORMAT_ALLOW_COMMENTS)) {\n+\t\t\tcsvSchema = csvSchema.rebuild()\n+\t\t\t\t.setAllowComments(properties.getBoolean(FORMAT_ALLOW_COMMENTS))\n+\t\t\t\t.build();\n+\t\t}\n+\t\tif (properties.containsKey(FORMAT_ARRAY_ELEMENT_DELIMITER)) {\n+\t\t\tcsvSchema = csvSchema.rebuild()\n+\t\t\t\t.setArrayElementSeparator(properties.getString(FORMAT_ARRAY_ELEMENT_DELIMITER))\n+\t\t\t\t.build();\n+\t\t}\n+\t\tif (properties.containsKey(FORMAT_ESCAPE_CHARACTER)) {\n+\t\t\tcsvSchema = csvSchema.rebuild()\n+\t\t\t\t.setEscapeChar(properties.getCharacter(FORMAT_ESCAPE_CHARACTER))\n+\t\t\t\t.build();\n+\t\t}\n+\t\tif (properties.containsKey(FORMAT_NULL_LITERAL)) {\n+\t\t\tcsvSchema = csvSchema.rebuild()\n+\t\t\t\t.setNullValue(properties.getString(FORMAT_NULL_LITERAL))\n+\t\t\t\t.build();\n+\t\t}\n+\t\tif (properties.containsKey(FORMAT_LINE_DELIMITER)) {\n+\t\t\tcsvSchema = csvSchema.rebuild()\n+\t\t\t\t.setLineSeparator(properties.getString(FORMAT_LINE_DELIMITER))\n+\t\t\t\t.build();\n+\t\t}\n+\t\treturn csvSchema;\n+\t}\n+\n+\t@Override\n+\tpublic Optional<Encoder<RowData>> createEncoder(WriterContext context) {\n+\n+\t\tCsvRowDataSerializationSchema.Builder builder = new CsvRowDataSerializationSchema.Builder(\n+\t\t\tcontext.getFormatRowType());\n+\n+\t\tDescriptorProperties properties = getValidatedProperties(context.getFormatProperties());\n+\n+\t\tproperties.getOptionalCharacter(FORMAT_FIELD_DELIMITER)\n+\t\t\t.ifPresent(builder::setFieldDelimiter);\n+\n+\t\tproperties.getOptionalString(FORMAT_LINE_DELIMITER)\n+\t\t\t.ifPresent(builder::setLineDelimiter);\n+\n+\t\tif (properties.getOptionalBoolean(FORMAT_DISABLE_QUOTE_CHARACTER).orElse(false)) {\n+\t\t\tbuilder.disableQuoteCharacter();\n+\t\t} else {\n+\t\t\tproperties.getOptionalCharacter(FORMAT_QUOTE_CHARACTER).ifPresent(builder::setQuoteCharacter);\n+\t\t}\n+\n+\t\tproperties.getOptionalString(FORMAT_ARRAY_ELEMENT_DELIMITER)\n+\t\t\t.ifPresent(builder::setArrayElementDelimiter);\n+\n+\t\tproperties.getOptionalCharacter(FORMAT_ESCAPE_CHARACTER)\n+\t\t\t.ifPresent(builder::setEscapeCharacter);\n+\n+\t\tproperties.getOptionalString(FORMAT_NULL_LITERAL)\n+\t\t\t.ifPresent(builder::setNullLiteral);\n+\n+\t\treturn Optional.of(new CsvRowDataEncoder(builder.build()));\n+\t}\n+\n+\t@Override\n+\tpublic Optional<BulkWriter.Factory<RowData>> createBulkWriterFactory(WriterContext context) {\n+\t\treturn Optional.empty();\n+\t}\n+\n+\t@Override\n+\tpublic boolean supportsSchemaDerivation() {\n+\t\treturn true;\n+\t}\n+\n+\t@Override\n+\tpublic List<String> supportedProperties() {\n+\t\tfinal List<String> properties = new ArrayList<>();\n+\t\tproperties.add(FORMAT_FIELD_DELIMITER);\n+\t\tproperties.add(FORMAT_LINE_DELIMITER);\n+\t\tproperties.add(FORMAT_DISABLE_QUOTE_CHARACTER);\n+\t\tproperties.add(FORMAT_QUOTE_CHARACTER);\n+\t\tproperties.add(FORMAT_ALLOW_COMMENTS);\n+\t\tproperties.add(FORMAT_IGNORE_PARSE_ERRORS);\n+\t\tproperties.add(FORMAT_ARRAY_ELEMENT_DELIMITER);\n+\t\tproperties.add(FORMAT_ESCAPE_CHARACTER);\n+\t\tproperties.add(FORMAT_NULL_LITERAL);\n+\t\treturn properties;\n+\t}\n+\n+\t@Override\n+\tpublic Map<String, String> requiredContext() {\n+\t\tMap<String, String> context = new HashMap<>();\n+\t\tcontext.put(FORMAT, \"csv\");\n+\t\treturn context;\n+\t}\n+\n+\tprivate static DescriptorProperties getValidatedProperties(Map<String, String> propertiesMap) {\n+\t\tfinal DescriptorProperties properties = new DescriptorProperties(true);\n+\t\tproperties.putProperties(propertiesMap);\n+\n+\t\tproperties.validateString(FORMAT_FIELD_DELIMITER, true, 1, 1);\n+\t\tproperties.validateEnumValues(FORMAT_LINE_DELIMITER, true, Arrays.asList(\"\\r\", \"\\n\", \"\\r\\n\", \"\"));\n+\t\tproperties.validateBoolean(FORMAT_DISABLE_QUOTE_CHARACTER, true);\n+\t\tproperties.validateString(FORMAT_QUOTE_CHARACTER, true, 1, 1);\n+\t\tproperties.validateBoolean(FORMAT_ALLOW_COMMENTS, true);\n+\t\tproperties.validateBoolean(FORMAT_IGNORE_PARSE_ERRORS, true);\n+\t\tproperties.validateString(FORMAT_ARRAY_ELEMENT_DELIMITER, true, 1);\n+\t\tproperties.validateString(FORMAT_ESCAPE_CHARACTER, true, 1, 1);\n+\n+\t\treturn properties;\n+\t}\n+\n+\t/**\n+\t * InputFormat that reads csv record into {@link RowData}.\n+\t */\n+\tpublic static class CsvInputFormat extends AbstractCsvInputFormat<RowData> {\n+\t\tprivate static final long serialVersionUID = 1L;\n+\n+\t\tprivate final RowType formatRowType;\n+\t\tprivate final DataType[] fieldTypes;\n+\t\tprivate final String[] fieldNames;\n+\t\tprivate final int[] selectFields;\n+\t\tprivate final List<String> partitionKeys;\n+\t\tprivate final String defaultPartValue;\n+\t\tprivate final long limit;\n+\t\tprivate final int[] csvSelectFieldToProjectFieldMapping;\n+\t\tprivate final int[] csvSelectFieldToCsvFieldMapping;\n+\t\tprivate final boolean ignoreParseErrors;\n+\n+\t\tprivate transient InputStreamReader inputStreamReader;\n+\t\tprivate transient BufferedReader reader;\n+\t\tprivate transient boolean end;\n+\t\tprivate transient long emitted;\n+\t\t// reuse object for per record\n+\t\tprivate transient GenericRowData rowData;\n+\t\tprivate transient DeserializationRuntimeConverter runtimeConverter;\n+\t\tprivate transient MappingIterator<JsonNode> iterator;\n+\n+\t\tpublic CsvInputFormat(\n+\t\t\t\tPath[] filePaths,\n+\t\t\t\tDataType[] fieldTypes,\n+\t\t\t\tString[] fieldNames,\n+\t\t\t\tCsvSchema csvSchema,\n+\t\t\t\tRowType formatRowType,\n+\t\t\t\tint[] selectFields,\n+\t\t\t\tList<String> partitionKeys,\n+\t\t\t\tString defaultPartValue,\n+\t\t\t\tlong limit,\n+\t\t\t\tint[] csvSelectFieldToProjectFieldMapping,\n+\t\t\t\tint[] csvSelectFieldToCsvFieldMapping,\n+\t\t\t\tboolean ignoreParseErrors) {\n+\t\t\tsuper(filePaths, csvSchema);\n+\t\t\tthis.fieldTypes = fieldTypes;\n+\t\t\tthis.fieldNames = fieldNames;\n+\t\t\tthis.formatRowType = formatRowType;\n+\t\t\tthis.partitionKeys = partitionKeys;\n+\t\t\tthis.defaultPartValue = defaultPartValue;\n+\t\t\tthis.selectFields = selectFields;\n+\t\t\tthis.limit = limit;\n+\t\t\tthis.emitted = 0;\n+\t\t\tthis.csvSelectFieldToProjectFieldMapping = csvSelectFieldToProjectFieldMapping;\n+\t\t\tthis.csvSelectFieldToCsvFieldMapping = csvSelectFieldToCsvFieldMapping;\n+\t\t\tthis.ignoreParseErrors = ignoreParseErrors;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void open(FileInputSplit split) throws IOException {\n+\t\t\tsuper.open(split);\n+\t\t\tthis.end = false;\n+\t\t\tthis.inputStreamReader = new InputStreamReader(csvInputStream);\n+\t\t\tthis.reader = new BufferedReader(inputStreamReader);\n+\t\t\tthis.rowData = PartitionPathUtils.fillPartitionValueForRecord(fieldNames, fieldTypes, selectFields,\n+\t\t\t\tpartitionKeys, currentSplit.getPath(), defaultPartValue);\n+\t\t\tthis.iterator = new CsvMapper()\n+\t\t\t\t.readerFor(JsonNode.class)\n+\t\t\t\t.with(csvSchema)\n+\t\t\t\t.readValues(csvInputStream);\n+\t\t\tprepareRuntimeConverter();\n+\t\t}\n+\n+\t\tprivate void prepareRuntimeConverter(){\n+\t\t\tCsvRowDataDeserializationSchema.Builder builder = new CsvRowDataDeserializationSchema.Builder(\n+\t\t\t\tformatRowType, new GenericTypeInfo<>(RowData.class))\n+\t\t\t\t.setIgnoreParseErrors(ignoreParseErrors);\n+\t\t\tthis.runtimeConverter = builder.build().createRowConverter(formatRowType, true);\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic boolean reachedEnd() throws IOException {\n+\t\t\treturn emitted >= limit || end;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic RowData nextRecord(RowData reuse) throws IOException {\n+\t\t\tGenericRowData csvRow = null;\n+\t\t\tdo {\n+\t\t\t\ttry {\n+\t\t\t\t\tJsonNode root = iterator.nextValue();\n+\t\t\t\t\tcsvRow = (GenericRowData) runtimeConverter.convert(root);\n+\t\t\t\t} catch (NoSuchElementException e) {\n+\t\t\t\t\tend = true;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0fd33d570a8591ef7e5a523c29366a34aa6b986e"}, "originalPosition": 301}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjc2NzY4Ng==", "bodyText": "Minor: this class can be a lambda.", "url": "https://github.com/apache/flink/pull/11755#discussion_r422767686", "createdAt": "2020-05-11T04:03:55Z", "author": {"login": "JingsongLi"}, "path": "flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvRowDataFileSystemFormatFactory.java", "diffHunk": "@@ -0,0 +1,352 @@\n+package org.apache.flink.formats.csv;\n+\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.api.java.typeutils.GenericTypeInfo;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.formats.csv.CsvRowDataDeserializationSchema.DeserializationRuntimeConverter;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.FileSystemFormatFactory;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.utils.PartitionPathUtils;\n+\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.JsonNode;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.MappingIterator;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvMapper;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvSchema;\n+\n+import java.io.BufferedReader;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.io.OutputStream;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NoSuchElementException;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ALLOW_COMMENTS;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ARRAY_ELEMENT_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_DISABLE_QUOTE_CHARACTER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ESCAPE_CHARACTER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_FIELD_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_IGNORE_PARSE_ERRORS;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_LINE_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_NULL_LITERAL;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_QUOTE_CHARACTER;\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+\n+/**\n+ * CSV format factory for file system.\n+ */\n+public class CsvRowDataFileSystemFormatFactory implements FileSystemFormatFactory {\n+\t@Override\n+\tpublic InputFormat<RowData, ?> createReader(ReaderContext context) {\n+\n+\t\tDescriptorProperties properties = getValidatedProperties(context.getFormatProperties());\n+\n+\t\tRowType formatRowType = context.getFormatRowType();\n+\n+\t\tString[] fieldNames = context.getSchema().getFieldNames();\n+\t\tList<String> projectFields = Arrays.stream(context.getProjectFields())\n+\t\t\t.mapToObj(idx -> fieldNames[idx])\n+\t\t\t.collect(Collectors.toList());\n+\t\tList<String> csvFields = Arrays.stream(fieldNames)\n+\t\t\t.filter(field -> !context.getPartitionKeys().contains(field))\n+\t\t\t.collect(Collectors.toList());\n+\n+\t\tint[] csvSelectFieldToProjectFieldMapping = context.getFormatProjectFields().stream()\n+\t\t\t.mapToInt(projectFields::indexOf)\n+\t\t\t.toArray();\n+\t\tint[] csvSelectFieldToCsvFieldMapping = context.getFormatProjectFields().stream()\n+\t\t\t.mapToInt(csvFields::indexOf)\n+\t\t\t.toArray();\n+\n+\t\tCsvSchema csvSchema = buildCsvSchema(formatRowType, properties);\n+\n+\t\tboolean ignoreParseErrors = properties.getOptionalBoolean(FORMAT_IGNORE_PARSE_ERRORS).orElse(false);\n+\n+\t\treturn new CsvInputFormat(\n+\t\t\tcontext.getPaths(),\n+\t\t\tcontext.getSchema().getFieldDataTypes(),\n+\t\t\tcontext.getSchema().getFieldNames(),\n+\t\t\tcsvSchema,\n+\t\t\tformatRowType,\n+\t\t\tcontext.getProjectFields(),\n+\t\t\tcontext.getPartitionKeys(),\n+\t\t\tcontext.getDefaultPartName(),\n+\t\t\tcontext.getPushedDownLimit(),\n+\t\t\tcsvSelectFieldToProjectFieldMapping,\n+\t\t\tcsvSelectFieldToCsvFieldMapping,\n+\t\t\tignoreParseErrors);\n+\t}\n+\n+\tprivate CsvSchema buildCsvSchema(RowType rowType, DescriptorProperties properties) {\n+\t\tCsvSchema csvSchema = CsvRowSchemaConverter.convert(rowType);\n+\t\t//format properties\n+\t\tif (properties.containsKey(FORMAT_FIELD_DELIMITER)) {\n+\t\t\tcsvSchema = csvSchema.rebuild()\n+\t\t\t\t.setColumnSeparator(properties.getCharacter(FORMAT_FIELD_DELIMITER))\n+\t\t\t\t.build();\n+\t\t}\n+\t\tif (properties.containsKey(FORMAT_QUOTE_CHARACTER)) {\n+\t\t\tcsvSchema = csvSchema.rebuild()\n+\t\t\t\t.setQuoteChar(properties.getCharacter(FORMAT_QUOTE_CHARACTER))\n+\t\t\t\t.build();\n+\t\t}\n+\t\tif (properties.containsKey(FORMAT_ALLOW_COMMENTS)) {\n+\t\t\tcsvSchema = csvSchema.rebuild()\n+\t\t\t\t.setAllowComments(properties.getBoolean(FORMAT_ALLOW_COMMENTS))\n+\t\t\t\t.build();\n+\t\t}\n+\t\tif (properties.containsKey(FORMAT_ARRAY_ELEMENT_DELIMITER)) {\n+\t\t\tcsvSchema = csvSchema.rebuild()\n+\t\t\t\t.setArrayElementSeparator(properties.getString(FORMAT_ARRAY_ELEMENT_DELIMITER))\n+\t\t\t\t.build();\n+\t\t}\n+\t\tif (properties.containsKey(FORMAT_ESCAPE_CHARACTER)) {\n+\t\t\tcsvSchema = csvSchema.rebuild()\n+\t\t\t\t.setEscapeChar(properties.getCharacter(FORMAT_ESCAPE_CHARACTER))\n+\t\t\t\t.build();\n+\t\t}\n+\t\tif (properties.containsKey(FORMAT_NULL_LITERAL)) {\n+\t\t\tcsvSchema = csvSchema.rebuild()\n+\t\t\t\t.setNullValue(properties.getString(FORMAT_NULL_LITERAL))\n+\t\t\t\t.build();\n+\t\t}\n+\t\tif (properties.containsKey(FORMAT_LINE_DELIMITER)) {\n+\t\t\tcsvSchema = csvSchema.rebuild()\n+\t\t\t\t.setLineSeparator(properties.getString(FORMAT_LINE_DELIMITER))\n+\t\t\t\t.build();\n+\t\t}\n+\t\treturn csvSchema;\n+\t}\n+\n+\t@Override\n+\tpublic Optional<Encoder<RowData>> createEncoder(WriterContext context) {\n+\n+\t\tCsvRowDataSerializationSchema.Builder builder = new CsvRowDataSerializationSchema.Builder(\n+\t\t\tcontext.getFormatRowType());\n+\n+\t\tDescriptorProperties properties = getValidatedProperties(context.getFormatProperties());\n+\n+\t\tproperties.getOptionalCharacter(FORMAT_FIELD_DELIMITER)\n+\t\t\t.ifPresent(builder::setFieldDelimiter);\n+\n+\t\tproperties.getOptionalString(FORMAT_LINE_DELIMITER)\n+\t\t\t.ifPresent(builder::setLineDelimiter);\n+\n+\t\tif (properties.getOptionalBoolean(FORMAT_DISABLE_QUOTE_CHARACTER).orElse(false)) {\n+\t\t\tbuilder.disableQuoteCharacter();\n+\t\t} else {\n+\t\t\tproperties.getOptionalCharacter(FORMAT_QUOTE_CHARACTER).ifPresent(builder::setQuoteCharacter);\n+\t\t}\n+\n+\t\tproperties.getOptionalString(FORMAT_ARRAY_ELEMENT_DELIMITER)\n+\t\t\t.ifPresent(builder::setArrayElementDelimiter);\n+\n+\t\tproperties.getOptionalCharacter(FORMAT_ESCAPE_CHARACTER)\n+\t\t\t.ifPresent(builder::setEscapeCharacter);\n+\n+\t\tproperties.getOptionalString(FORMAT_NULL_LITERAL)\n+\t\t\t.ifPresent(builder::setNullLiteral);\n+\n+\t\treturn Optional.of(new CsvRowDataEncoder(builder.build()));\n+\t}\n+\n+\t@Override\n+\tpublic Optional<BulkWriter.Factory<RowData>> createBulkWriterFactory(WriterContext context) {\n+\t\treturn Optional.empty();\n+\t}\n+\n+\t@Override\n+\tpublic boolean supportsSchemaDerivation() {\n+\t\treturn true;\n+\t}\n+\n+\t@Override\n+\tpublic List<String> supportedProperties() {\n+\t\tfinal List<String> properties = new ArrayList<>();\n+\t\tproperties.add(FORMAT_FIELD_DELIMITER);\n+\t\tproperties.add(FORMAT_LINE_DELIMITER);\n+\t\tproperties.add(FORMAT_DISABLE_QUOTE_CHARACTER);\n+\t\tproperties.add(FORMAT_QUOTE_CHARACTER);\n+\t\tproperties.add(FORMAT_ALLOW_COMMENTS);\n+\t\tproperties.add(FORMAT_IGNORE_PARSE_ERRORS);\n+\t\tproperties.add(FORMAT_ARRAY_ELEMENT_DELIMITER);\n+\t\tproperties.add(FORMAT_ESCAPE_CHARACTER);\n+\t\tproperties.add(FORMAT_NULL_LITERAL);\n+\t\treturn properties;\n+\t}\n+\n+\t@Override\n+\tpublic Map<String, String> requiredContext() {\n+\t\tMap<String, String> context = new HashMap<>();\n+\t\tcontext.put(FORMAT, \"csv\");\n+\t\treturn context;\n+\t}\n+\n+\tprivate static DescriptorProperties getValidatedProperties(Map<String, String> propertiesMap) {\n+\t\tfinal DescriptorProperties properties = new DescriptorProperties(true);\n+\t\tproperties.putProperties(propertiesMap);\n+\n+\t\tproperties.validateString(FORMAT_FIELD_DELIMITER, true, 1, 1);\n+\t\tproperties.validateEnumValues(FORMAT_LINE_DELIMITER, true, Arrays.asList(\"\\r\", \"\\n\", \"\\r\\n\", \"\"));\n+\t\tproperties.validateBoolean(FORMAT_DISABLE_QUOTE_CHARACTER, true);\n+\t\tproperties.validateString(FORMAT_QUOTE_CHARACTER, true, 1, 1);\n+\t\tproperties.validateBoolean(FORMAT_ALLOW_COMMENTS, true);\n+\t\tproperties.validateBoolean(FORMAT_IGNORE_PARSE_ERRORS, true);\n+\t\tproperties.validateString(FORMAT_ARRAY_ELEMENT_DELIMITER, true, 1);\n+\t\tproperties.validateString(FORMAT_ESCAPE_CHARACTER, true, 1, 1);\n+\n+\t\treturn properties;\n+\t}\n+\n+\t/**\n+\t * InputFormat that reads csv record into {@link RowData}.\n+\t */\n+\tpublic static class CsvInputFormat extends AbstractCsvInputFormat<RowData> {\n+\t\tprivate static final long serialVersionUID = 1L;\n+\n+\t\tprivate final RowType formatRowType;\n+\t\tprivate final DataType[] fieldTypes;\n+\t\tprivate final String[] fieldNames;\n+\t\tprivate final int[] selectFields;\n+\t\tprivate final List<String> partitionKeys;\n+\t\tprivate final String defaultPartValue;\n+\t\tprivate final long limit;\n+\t\tprivate final int[] csvSelectFieldToProjectFieldMapping;\n+\t\tprivate final int[] csvSelectFieldToCsvFieldMapping;\n+\t\tprivate final boolean ignoreParseErrors;\n+\n+\t\tprivate transient InputStreamReader inputStreamReader;\n+\t\tprivate transient BufferedReader reader;\n+\t\tprivate transient boolean end;\n+\t\tprivate transient long emitted;\n+\t\t// reuse object for per record\n+\t\tprivate transient GenericRowData rowData;\n+\t\tprivate transient DeserializationRuntimeConverter runtimeConverter;\n+\t\tprivate transient MappingIterator<JsonNode> iterator;\n+\n+\t\tpublic CsvInputFormat(\n+\t\t\t\tPath[] filePaths,\n+\t\t\t\tDataType[] fieldTypes,\n+\t\t\t\tString[] fieldNames,\n+\t\t\t\tCsvSchema csvSchema,\n+\t\t\t\tRowType formatRowType,\n+\t\t\t\tint[] selectFields,\n+\t\t\t\tList<String> partitionKeys,\n+\t\t\t\tString defaultPartValue,\n+\t\t\t\tlong limit,\n+\t\t\t\tint[] csvSelectFieldToProjectFieldMapping,\n+\t\t\t\tint[] csvSelectFieldToCsvFieldMapping,\n+\t\t\t\tboolean ignoreParseErrors) {\n+\t\t\tsuper(filePaths, csvSchema);\n+\t\t\tthis.fieldTypes = fieldTypes;\n+\t\t\tthis.fieldNames = fieldNames;\n+\t\t\tthis.formatRowType = formatRowType;\n+\t\t\tthis.partitionKeys = partitionKeys;\n+\t\t\tthis.defaultPartValue = defaultPartValue;\n+\t\t\tthis.selectFields = selectFields;\n+\t\t\tthis.limit = limit;\n+\t\t\tthis.emitted = 0;\n+\t\t\tthis.csvSelectFieldToProjectFieldMapping = csvSelectFieldToProjectFieldMapping;\n+\t\t\tthis.csvSelectFieldToCsvFieldMapping = csvSelectFieldToCsvFieldMapping;\n+\t\t\tthis.ignoreParseErrors = ignoreParseErrors;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void open(FileInputSplit split) throws IOException {\n+\t\t\tsuper.open(split);\n+\t\t\tthis.end = false;\n+\t\t\tthis.inputStreamReader = new InputStreamReader(csvInputStream);\n+\t\t\tthis.reader = new BufferedReader(inputStreamReader);\n+\t\t\tthis.rowData = PartitionPathUtils.fillPartitionValueForRecord(fieldNames, fieldTypes, selectFields,\n+\t\t\t\tpartitionKeys, currentSplit.getPath(), defaultPartValue);\n+\t\t\tthis.iterator = new CsvMapper()\n+\t\t\t\t.readerFor(JsonNode.class)\n+\t\t\t\t.with(csvSchema)\n+\t\t\t\t.readValues(csvInputStream);\n+\t\t\tprepareRuntimeConverter();\n+\t\t}\n+\n+\t\tprivate void prepareRuntimeConverter(){\n+\t\t\tCsvRowDataDeserializationSchema.Builder builder = new CsvRowDataDeserializationSchema.Builder(\n+\t\t\t\tformatRowType, new GenericTypeInfo<>(RowData.class))\n+\t\t\t\t.setIgnoreParseErrors(ignoreParseErrors);\n+\t\t\tthis.runtimeConverter = builder.build().createRowConverter(formatRowType, true);\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic boolean reachedEnd() throws IOException {\n+\t\t\treturn emitted >= limit || end;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic RowData nextRecord(RowData reuse) throws IOException {\n+\t\t\tGenericRowData csvRow = null;\n+\t\t\tdo {\n+\t\t\t\ttry {\n+\t\t\t\t\tJsonNode root = iterator.nextValue();\n+\t\t\t\t\tcsvRow = (GenericRowData) runtimeConverter.convert(root);\n+\t\t\t\t} catch (NoSuchElementException e) {\n+\t\t\t\t\tend = true;\n+\t\t\t\t} catch (Throwable t) {\n+\t\t\t\t\tif (!ignoreParseErrors) {\n+\t\t\t\t\t\tthrow new IOException(\"Failed to deserialize CSV row.\", t);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t} while (csvRow == null && !reachedEnd());\n+\n+\t\t\tGenericRowData returnRecord = null;\n+\t\t\tif (csvRow != null) {\n+\t\t\t\treturnRecord = rowData;\n+\t\t\t\tfor (int i = 0; i < csvSelectFieldToCsvFieldMapping.length; i++) {\n+\t\t\t\t\treturnRecord.setField(csvSelectFieldToProjectFieldMapping[i],\n+\t\t\t\t\t\tcsvRow.getField(csvSelectFieldToCsvFieldMapping[i]));\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\temitted++;\n+\t\t\treturn returnRecord;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void close() throws IOException {\n+\t\t\tsuper.close();\n+\t\t\tif (reader != null) {\n+\t\t\t\treader.close();\n+\t\t\t\treader = null;\n+\t\t\t}\n+\t\t\tif (inputStreamReader != null) {\n+\t\t\t\tinputStreamReader.close();\n+\t\t\t\tinputStreamReader = null;\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * A {@link Encoder} writes {@link RowData} record into {@link java.io.OutputStream} with csv format.\n+\t */\n+\tpublic static class CsvRowDataEncoder implements Encoder<RowData> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0fd33d570a8591ef7e5a523c29366a34aa6b986e"}, "originalPosition": 338}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b72a27e8fa3a6b21e5505435be096eea97d7da07", "author": {"user": {"login": "leonardBang", "name": "Leonard Xu"}}, "url": "https://github.com/apache/flink/commit/b72a27e8fa3a6b21e5505435be096eea97d7da07", "committedDate": "2020-05-11T10:12:44Z", "message": "[FLINK-14257][Connectors / FileSystem]Integrate csv to FileSystemTableFactory"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "73ca85b532ef3efcef5e7795db77c79d7bf82131", "author": {"user": {"login": "leonardBang", "name": "Leonard Xu"}}, "url": "https://github.com/apache/flink/commit/73ca85b532ef3efcef5e7795db77c79d7bf82131", "committedDate": "2020-05-11T11:52:11Z", "message": "rebase and address comment"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "0fd33d570a8591ef7e5a523c29366a34aa6b986e", "author": {"user": {"login": "leonardBang", "name": "Leonard Xu"}}, "url": "https://github.com/apache/flink/commit/0fd33d570a8591ef7e5a523c29366a34aa6b986e", "committedDate": "2020-05-10T15:29:22Z", "message": "update"}, "afterCommit": {"oid": "73ca85b532ef3efcef5e7795db77c79d7bf82131", "author": {"user": {"login": "leonardBang", "name": "Leonard Xu"}}, "url": "https://github.com/apache/flink/commit/73ca85b532ef3efcef5e7795db77c79d7bf82131", "committedDate": "2020-05-11T11:52:11Z", "message": "rebase and address comment"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDA5NjUzNjE5", "url": "https://github.com/apache/flink/pull/11755#pullrequestreview-409653619", "createdAt": "2020-05-12T02:18:04Z", "commit": {"oid": "73ca85b532ef3efcef5e7795db77c79d7bf82131"}, "state": "COMMENTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQwMjoxODowNFrOGTzusA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQwMjoxOTozNlrOGTzwMw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQyMzY2NA==", "bodyText": "CsvFileSystemFormatFactory", "url": "https://github.com/apache/flink/pull/11755#discussion_r423423664", "createdAt": "2020-05-12T02:18:04Z", "author": {"login": "JingsongLi"}, "path": "flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvRowDataFileSystemFormatFactory.java", "diffHunk": "@@ -0,0 +1,333 @@\n+package org.apache.flink.formats.csv;\n+\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.api.java.typeutils.GenericTypeInfo;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.formats.csv.CsvRowDataDeserializationSchema.DeserializationRuntimeConverter;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.FileSystemFormatFactory;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.utils.PartitionPathUtils;\n+\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.JsonNode;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.MappingIterator;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvMapper;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvSchema;\n+\n+import java.io.BufferedReader;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NoSuchElementException;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ALLOW_COMMENTS;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ARRAY_ELEMENT_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_DISABLE_QUOTE_CHARACTER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ESCAPE_CHARACTER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_FIELD_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_IGNORE_PARSE_ERRORS;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_LINE_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_NULL_LITERAL;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_QUOTE_CHARACTER;\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+\n+/**\n+ * CSV format factory for file system.\n+ */\n+public class CsvRowDataFileSystemFormatFactory implements FileSystemFormatFactory {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "73ca85b532ef3efcef5e7795db77c79d7bf82131"}, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQyMzgyNQ==", "bodyText": "Use getOptionalChar.ifPresent getOptionalString.ifPresent", "url": "https://github.com/apache/flink/pull/11755#discussion_r423423825", "createdAt": "2020-05-12T02:18:39Z", "author": {"login": "JingsongLi"}, "path": "flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvRowDataFileSystemFormatFactory.java", "diffHunk": "@@ -0,0 +1,333 @@\n+package org.apache.flink.formats.csv;\n+\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.api.java.typeutils.GenericTypeInfo;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.formats.csv.CsvRowDataDeserializationSchema.DeserializationRuntimeConverter;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.FileSystemFormatFactory;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.utils.PartitionPathUtils;\n+\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.JsonNode;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.MappingIterator;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvMapper;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvSchema;\n+\n+import java.io.BufferedReader;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NoSuchElementException;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ALLOW_COMMENTS;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ARRAY_ELEMENT_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_DISABLE_QUOTE_CHARACTER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ESCAPE_CHARACTER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_FIELD_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_IGNORE_PARSE_ERRORS;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_LINE_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_NULL_LITERAL;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_QUOTE_CHARACTER;\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+\n+/**\n+ * CSV format factory for file system.\n+ */\n+public class CsvRowDataFileSystemFormatFactory implements FileSystemFormatFactory {\n+\n+\t@Override\n+\tpublic InputFormat<RowData, ?> createReader(ReaderContext context) {\n+\t\tDescriptorProperties properties = getValidatedProperties(context.getFormatProperties());\n+\n+\t\tRowType formatRowType = context.getFormatRowType();\n+\n+\t\tString[] fieldNames = context.getSchema().getFieldNames();\n+\t\tList<String> projectFields = Arrays.stream(context.getProjectFields())\n+\t\t\t.mapToObj(idx -> fieldNames[idx])\n+\t\t\t.collect(Collectors.toList());\n+\t\tList<String> csvFields = Arrays.stream(fieldNames)\n+\t\t\t.filter(field -> !context.getPartitionKeys().contains(field))\n+\t\t\t.collect(Collectors.toList());\n+\n+\t\tint[] csvSelectFieldToProjectFieldMapping = context.getFormatProjectFields().stream()\n+\t\t\t.mapToInt(projectFields::indexOf)\n+\t\t\t.toArray();\n+\t\tint[] csvSelectFieldToCsvFieldMapping = context.getFormatProjectFields().stream()\n+\t\t\t.mapToInt(csvFields::indexOf)\n+\t\t\t.toArray();\n+\n+\t\tCsvSchema csvSchema = buildCsvSchema(formatRowType, properties);\n+\n+\t\tboolean ignoreParseErrors = properties.getOptionalBoolean(FORMAT_IGNORE_PARSE_ERRORS).orElse(false);\n+\n+\t\treturn new CsvInputFormat(\n+\t\t\tcontext.getPaths(),\n+\t\t\tcontext.getSchema().getFieldDataTypes(),\n+\t\t\tcontext.getSchema().getFieldNames(),\n+\t\t\tcsvSchema,\n+\t\t\tformatRowType,\n+\t\t\tcontext.getProjectFields(),\n+\t\t\tcontext.getPartitionKeys(),\n+\t\t\tcontext.getDefaultPartName(),\n+\t\t\tcontext.getPushedDownLimit(),\n+\t\t\tcsvSelectFieldToProjectFieldMapping,\n+\t\t\tcsvSelectFieldToCsvFieldMapping,\n+\t\t\tignoreParseErrors);\n+\t}\n+\n+\tprivate CsvSchema buildCsvSchema(RowType rowType, DescriptorProperties properties) {\n+\t\tCsvSchema csvSchema = CsvRowSchemaConverter.convert(rowType);\n+\t\t//format properties\n+\t\tif (properties.containsKey(FORMAT_FIELD_DELIMITER)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "73ca85b532ef3efcef5e7795db77c79d7bf82131"}, "originalPosition": 94}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQyMzk0MQ==", "bodyText": "CsvFilesystemBatchITCase\nOthers too.", "url": "https://github.com/apache/flink/pull/11755#discussion_r423423941", "createdAt": "2020-05-12T02:19:07Z", "author": {"login": "JingsongLi"}, "path": "flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/CsvRowDataFilesystemBatchITCase.java", "diffHunk": "@@ -0,0 +1,85 @@\n+package org.apache.flink.formats.csv;\n+\n+import org.apache.flink.table.planner.runtime.batch.sql.BatchFileSystemITCaseBase;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.util.FileUtils;\n+\n+import org.junit.Test;\n+import org.junit.experimental.runners.Enclosed;\n+import org.junit.runner.RunWith;\n+\n+import java.io.File;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+/**\n+ * ITCase to test csv format for {@link CsvRowDataFileSystemFormatFactory} in batch mode.\n+ */\n+@RunWith(Enclosed.class)\n+public class CsvRowDataFilesystemBatchITCase {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "73ca85b532ef3efcef5e7795db77c79d7bf82131"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQyNDAxOA==", "bodyText": "Ditto.", "url": "https://github.com/apache/flink/pull/11755#discussion_r423424018", "createdAt": "2020-05-12T02:19:27Z", "author": {"login": "JingsongLi"}, "path": "flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/CsvRowDataFilesystemBatchITCase.java", "diffHunk": "@@ -0,0 +1,85 @@\n+package org.apache.flink.formats.csv;\n+\n+import org.apache.flink.table.planner.runtime.batch.sql.BatchFileSystemITCaseBase;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.util.FileUtils;\n+\n+import org.junit.Test;\n+import org.junit.experimental.runners.Enclosed;\n+import org.junit.runner.RunWith;\n+\n+import java.io.File;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+/**\n+ * ITCase to test csv format for {@link CsvRowDataFileSystemFormatFactory} in batch mode.\n+ */\n+@RunWith(Enclosed.class)\n+public class CsvRowDataFilesystemBatchITCase {\n+\n+\t/**\n+\t * General IT cases for CsvRowDataFilesystem in batch mode.\n+\t */\n+\tpublic static class GeneralCsvRowDataFilesystemBatchITCase extends BatchFileSystemITCaseBase {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "73ca85b532ef3efcef5e7795db77c79d7bf82131"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQyNDAzMQ==", "bodyText": "Ditto.", "url": "https://github.com/apache/flink/pull/11755#discussion_r423424031", "createdAt": "2020-05-12T02:19:31Z", "author": {"login": "JingsongLi"}, "path": "flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/CsvRowDataFilesystemBatchITCase.java", "diffHunk": "@@ -0,0 +1,85 @@\n+package org.apache.flink.formats.csv;\n+\n+import org.apache.flink.table.planner.runtime.batch.sql.BatchFileSystemITCaseBase;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.util.FileUtils;\n+\n+import org.junit.Test;\n+import org.junit.experimental.runners.Enclosed;\n+import org.junit.runner.RunWith;\n+\n+import java.io.File;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+/**\n+ * ITCase to test csv format for {@link CsvRowDataFileSystemFormatFactory} in batch mode.\n+ */\n+@RunWith(Enclosed.class)\n+public class CsvRowDataFilesystemBatchITCase {\n+\n+\t/**\n+\t * General IT cases for CsvRowDataFilesystem in batch mode.\n+\t */\n+\tpublic static class GeneralCsvRowDataFilesystemBatchITCase extends BatchFileSystemITCaseBase {\n+\n+\t\t@Override\n+\t\tpublic String[] formatProperties() {\n+\t\t\tList<String> ret = new ArrayList<>();\n+\t\t\tret.add(\"'format'='csv'\");\n+\t\t\tret.add(\"'format.field-delimiter'=';'\");\n+\t\t\tret.add(\"'format.quote-character'='#'\");\n+\t\t\treturn ret.toArray(new String[0]);\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * Enriched IT cases that including testParseError and testEscapeChar for CsvRowDataFilesystem in batch mode.\n+\t */\n+\tpublic static class EnrichedCsvRowDataFilesystemBatchITCase extends BatchFileSystemITCaseBase {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "73ca85b532ef3efcef5e7795db77c79d7bf82131"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQyNDA1MQ==", "bodyText": "Ditto.", "url": "https://github.com/apache/flink/pull/11755#discussion_r423424051", "createdAt": "2020-05-12T02:19:36Z", "author": {"login": "JingsongLi"}, "path": "flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/CsvRowDataFilesystemStreamITCase.java", "diffHunk": "@@ -0,0 +1,38 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.csv;\n+\n+import org.apache.flink.table.planner.runtime.stream.FsStreamingSinkITCaseBase;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * ITCase to test csv format for {@link CsvRowDataFileSystemFormatFactory} in stream mode.\n+ */\n+public class CsvRowDataFilesystemStreamITCase extends FsStreamingSinkITCaseBase {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "73ca85b532ef3efcef5e7795db77c79d7bf82131"}, "originalPosition": 29}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "44142403791d63981b591fc062018b5d41dee7a8", "author": {"user": {"login": "leonardBang", "name": "Leonard Xu"}}, "url": "https://github.com/apache/flink/commit/44142403791d63981b591fc062018b5d41dee7a8", "committedDate": "2020-05-12T04:06:03Z", "message": "address comments"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "e37e2a56d5aebe347db41704db929783393a28ff", "author": {"user": {"login": "leonardBang", "name": "Leonard Xu"}}, "url": "https://github.com/apache/flink/commit/e37e2a56d5aebe347db41704db929783393a28ff", "committedDate": "2020-05-12T03:58:22Z", "message": "address comments"}, "afterCommit": {"oid": "44142403791d63981b591fc062018b5d41dee7a8", "author": {"user": {"login": "leonardBang", "name": "Leonard Xu"}}, "url": "https://github.com/apache/flink/commit/44142403791d63981b591fc062018b5d41dee7a8", "committedDate": "2020-05-12T04:06:03Z", "message": "address comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDA5Nzg0MzY3", "url": "https://github.com/apache/flink/pull/11755#pullrequestreview-409784367", "createdAt": "2020-05-12T07:50:35Z", "commit": {"oid": "44142403791d63981b591fc062018b5d41dee7a8"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1796, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}