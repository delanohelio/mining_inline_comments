{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzU5MDA4NDE2", "number": 10762, "title": "[FLINK-15115][kafka] Drop Kafka 0.8/0.9", "bodyText": "This PR removes the Kafka 0.8/0.9 connectors.\nFor 0.8 this is a fairly straight-forward removal, while for 0.9 several existing tests and classes have been merged into 0.10.\nI recommended reviewing commits individually.\nI first refactored the KafkaConsumerThreadTest to use an actual Consumer implementation instead of a mock, since the latter lead to the usual mocking issues when migrating the test to 0.10; namely API changes that went unnoticed by the mockito setup, causing test failures which took way too long to figure out.\nNext I removed the 0.8 connector, which was straight-forward since it is largely independent from other versions.\nThe 0.9 removal is split into 2 commits: the first removes the 0.9 SQL jar, the second one the actual 0.9 connector.\nSince the 0.10 connector extended the 0.9 connector I had to merge the 0.9 connector into 0.10. This also presented some opportunities for simplifications (like the removal of call bridges).", "createdAt": "2020-01-03T14:33:04Z", "url": "https://github.com/apache/flink/pull/10762", "merged": true, "mergeCommit": {"oid": "0cf5a8d0488945cd528ce1bed41d559b2be7e62f"}, "closed": true, "closedAt": "2020-01-13T13:45:56Z", "author": {"login": "zentol"}, "timelineItems": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABb2vTXmABqjI5MjAzNDE3ODI=", "endCursor": "Y3Vyc29yOnYyOpPPAAABb56yryABqjI5NDI4MzYzNTk=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "1db0755ac2b1b88eac234c6213aa76317a6d5307", "author": {"user": {"login": "zentol", "name": "Chesnay Schepler"}}, "url": "https://github.com/apache/flink/commit/1db0755ac2b1b88eac234c6213aa76317a6d5307", "committedDate": "2020-01-03T13:29:56Z", "message": "+9"}, "afterCommit": {"oid": "6bfb6d18af5fdfbddadf6da53adab78567d64aa9", "author": {"user": {"login": "zentol", "name": "Chesnay Schepler"}}, "url": "https://github.com/apache/flink/commit/6bfb6d18af5fdfbddadf6da53adab78567d64aa9", "committedDate": "2020-01-03T14:33:26Z", "message": "[FLINK-15115][kafka] Drop Kafka 0.9"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQxMjc2ODYy", "url": "https://github.com/apache/flink/pull/10762#pullrequestreview-341276862", "createdAt": "2020-01-10T16:28:11Z", "commit": {"oid": "18dd7d344258743705da311508a1fc7d0873da45"}, "state": "APPROVED", "comments": {"totalCount": 12, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xMFQxNjoyODoxMlrOFcZTMA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xMFQxNjo1MzozN1rOFcaCxw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTMxODk2MA==", "bodyText": "Should we throw UnsupportedOperationException here in order to avoid NPE somewhere else in the code?", "url": "https://github.com/apache/flink/pull/10762#discussion_r365318960", "createdAt": "2020-01-10T16:28:12Z", "author": {"login": "tillrohrmann"}, "path": "flink-connectors/flink-connector-kafka-0.9/src/test/java/org/apache/flink/streaming/connectors/kafka/internal/KafkaConsumerThreadTest.java", "diffHunk": "@@ -875,109 +878,194 @@ void reassignPartitions(List<KafkaTopicPartitionState<TopicPartition>> newPartit\n \t}\n \n \t@SuppressWarnings(\"unchecked\")\n-\tprivate static KafkaConsumer<byte[], byte[]> createMockConsumer(\n+\tprivate static TestConsumer createMockConsumer(\n \t\t\tfinal Map<TopicPartition, Long> mockConsumerAssignmentAndPosition,\n \t\t\tfinal Map<TopicPartition, Long> mockRetrievedPositions,\n \t\t\tfinal boolean earlyWakeup,\n \t\t\tfinal OneShotLatch midAssignmentLatch,\n \t\t\tfinal OneShotLatch continueAssignmentLatch) {\n \n-\t\tfinal KafkaConsumer<byte[], byte[]> mockConsumer = mock(KafkaConsumer.class);\n+\t\treturn new TestConsumer(mockConsumerAssignmentAndPosition, mockRetrievedPositions, earlyWakeup, midAssignmentLatch, continueAssignmentLatch);\n+\t}\n \n-\t\twhen(mockConsumer.assignment()).thenAnswer(new Answer<Object>() {\n-\t\t\t@Override\n-\t\t\tpublic Object answer(InvocationOnMock invocationOnMock) throws Throwable {\n-\t\t\t\tif (midAssignmentLatch != null) {\n-\t\t\t\t\tmidAssignmentLatch.trigger();\n-\t\t\t\t}\n+\tprivate static class TestConsumer implements Consumer<byte[], byte[]> {\n+\t\tprivate final Map<TopicPartition, Long> mockConsumerAssignmentAndPosition;\n+\t\tprivate final Map<TopicPartition, Long> mockRetrievedPositions;\n+\t\tprivate final boolean earlyWakeup;\n+\t\tprivate final OneShotLatch midAssignmentLatch;\n+\t\tprivate final OneShotLatch continueAssignmentLatch;\n+\n+\t\tprivate int numWakeupCalls = 0;\n+\n+\t\tprivate TestConsumer(Map<TopicPartition, Long> mockConsumerAssignmentAndPosition, Map<TopicPartition, Long> mockRetrievedPositions, boolean earlyWakeup, OneShotLatch midAssignmentLatch, OneShotLatch continueAssignmentLatch) {\n+\t\t\tthis.mockConsumerAssignmentAndPosition = mockConsumerAssignmentAndPosition;\n+\t\t\tthis.mockRetrievedPositions = mockRetrievedPositions;\n+\t\t\tthis.earlyWakeup = earlyWakeup;\n+\t\t\tthis.midAssignmentLatch = midAssignmentLatch;\n+\t\t\tthis.continueAssignmentLatch = continueAssignmentLatch;\n+\t\t}\n \n-\t\t\t\tif (continueAssignmentLatch != null) {\n+\t\t@Override\n+\t\tpublic Set<TopicPartition> assignment() {\n+\t\t\tif (midAssignmentLatch != null) {\n+\t\t\t\tmidAssignmentLatch.trigger();\n+\t\t\t}\n+\n+\t\t\tif (continueAssignmentLatch != null) {\n+\t\t\t\ttry {\n \t\t\t\t\tcontinueAssignmentLatch.await();\n+\t\t\t\t} catch (InterruptedException e) {\n+\t\t\t\t\tThread.currentThread().interrupt();\n \t\t\t\t}\n-\t\t\t\treturn mockConsumerAssignmentAndPosition.keySet();\n \t\t\t}\n-\t\t});\n+\t\t\treturn mockConsumerAssignmentAndPosition.keySet();\n+\t\t}\n \n-\t\twhen(mockConsumer.poll(anyLong())).thenReturn(mock(ConsumerRecords.class));\n+\t\t@Override\n+\t\tpublic Set<String> subscription() {\n+\t\t\treturn null;\n+\t\t}\n \n-\t\tif (!earlyWakeup) {\n-\t\t\twhen(mockConsumer.position(any(TopicPartition.class))).thenAnswer(new Answer<Object>() {\n-\t\t\t\t@Override\n-\t\t\t\tpublic Object answer(InvocationOnMock invocationOnMock) throws Throwable {\n-\t\t\t\t\treturn mockConsumerAssignmentAndPosition.get(invocationOnMock.getArgument(0));\n-\t\t\t\t}\n-\t\t\t});\n-\t\t} else {\n-\t\t\twhen(mockConsumer.position(any(TopicPartition.class))).thenThrow(new WakeupException());\n+\t\t@Override\n+\t\tpublic void subscribe(List<String> list) {\n \t\t}\n \n-\t\tdoAnswer(new Answer() {\n-\t\t\t@Override\n-\t\t\tpublic Object answer(InvocationOnMock invocationOnMock) throws Throwable {\n-\t\t\t\tmockConsumerAssignmentAndPosition.clear();\n+\t\t@Override\n+\t\tpublic void subscribe(List<String> list, ConsumerRebalanceListener consumerRebalanceListener) {\n+\t\t}\n \n-\t\t\t\tList<TopicPartition> assignedPartitions = invocationOnMock.getArgument(0);\n-\t\t\t\tfor (TopicPartition assigned : assignedPartitions) {\n-\t\t\t\t\tmockConsumerAssignmentAndPosition.put(assigned, null);\n-\t\t\t\t}\n-\t\t\t\treturn null;\n+\t\t@Override\n+\t\tpublic void assign(List<TopicPartition> assignedPartitions) {\n+\t\t\tmockConsumerAssignmentAndPosition.clear();\n+\n+\t\t\tfor (TopicPartition assigned : assignedPartitions) {\n+\t\t\t\tmockConsumerAssignmentAndPosition.put(assigned, null);\n \t\t\t}\n-\t\t}).when(mockConsumer).assign(anyListOf(TopicPartition.class));\n+\t\t}\n \n-\t\tdoAnswer(new Answer() {\n-\t\t\t@Override\n-\t\t\tpublic Object answer(InvocationOnMock invocationOnMock) throws Throwable {\n-\t\t\t\tTopicPartition partition = invocationOnMock.getArgument(0);\n-\t\t\t\tlong position = invocationOnMock.getArgument(1);\n+\t\t@Override\n+\t\tpublic void subscribe(Pattern pattern, ConsumerRebalanceListener consumerRebalanceListener) {\n+\t\t}\n \n-\t\t\t\tif (!mockConsumerAssignmentAndPosition.containsKey(partition)) {\n-\t\t\t\t\tthrow new Exception(\"the current mock assignment does not contain partition \" + partition);\n-\t\t\t\t} else {\n-\t\t\t\t\tmockConsumerAssignmentAndPosition.put(partition, position);\n-\t\t\t\t}\n-\t\t\t\treturn null;\n-\t\t\t}\n-\t\t}).when(mockConsumer).seek(any(TopicPartition.class), anyLong());\n+\t\t@Override\n+\t\tpublic void unsubscribe() {\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic ConsumerRecords<byte[], byte[]> poll(long l) {\n+\t\t\treturn mock(ConsumerRecords.class);\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void commitSync() {\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void commitSync(Map<TopicPartition, OffsetAndMetadata> map) {\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void commitAsync() {\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void commitAsync(OffsetCommitCallback offsetCommitCallback) {\n+\t\t}\n \n-\t\tdoAnswer(new Answer() {\n-\t\t\t@Override\n-\t\t\tpublic Object answer(InvocationOnMock invocationOnMock) throws Throwable {\n-\t\t\t\tTopicPartition partition = invocationOnMock.getArgument(0);\n+\t\t@Override\n+\t\tpublic void commitAsync(Map<TopicPartition, OffsetAndMetadata> map, OffsetCommitCallback offsetCommitCallback) {\n+\t\t}\n \n+\t\t@Override\n+\t\tpublic void seek(TopicPartition partition, long position) {\n+\t\t\tif (!mockConsumerAssignmentAndPosition.containsKey(partition)) {\n+\t\t\t\tthrow new RuntimeException(\"the current mock assignment does not contain partition \" + partition);\n+\t\t\t} else {\n+\t\t\t\tmockConsumerAssignmentAndPosition.put(partition, position);\n+\t\t\t}\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void seekToBeginning(TopicPartition... partitions) {\n+\t\t\tfor (TopicPartition partition : partitions) {\n \t\t\t\tif (!mockConsumerAssignmentAndPosition.containsKey(partition)) {\n-\t\t\t\t\tthrow new Exception(\"the current mock assignment does not contain partition \" + partition);\n+\t\t\t\t\tthrow new RuntimeException(\"the current mock assignment does not contain partition \" + partition);\n \t\t\t\t} else {\n \t\t\t\t\tLong mockRetrievedPosition = mockRetrievedPositions.get(partition);\n \t\t\t\t\tif (mockRetrievedPosition == null) {\n-\t\t\t\t\t\tthrow new Exception(\"mock consumer needed to retrieve a position, but no value was provided in the mock values for retrieval\");\n+\t\t\t\t\t\tthrow new RuntimeException(\"mock consumer needed to retrieve a position, but no value was provided in the mock values for retrieval\");\n \t\t\t\t\t} else {\n \t\t\t\t\t\tmockConsumerAssignmentAndPosition.put(partition, mockRetrievedPositions.get(partition));\n \t\t\t\t\t}\n \t\t\t\t}\n-\t\t\t\treturn null;\n \t\t\t}\n-\t\t}).when(mockConsumer).seekToBeginning(any(TopicPartition.class));\n-\n-\t\tdoAnswer(new Answer() {\n-\t\t\t@Override\n-\t\t\tpublic Object answer(InvocationOnMock invocationOnMock) throws Throwable {\n-\t\t\t\tTopicPartition partition = invocationOnMock.getArgument(0);\n+\t\t}\n \n+\t\t@Override\n+\t\tpublic void seekToEnd(TopicPartition... partitions) {\n+\t\t\tfor (TopicPartition partition : partitions) {\n \t\t\t\tif (!mockConsumerAssignmentAndPosition.containsKey(partition)) {\n-\t\t\t\t\tthrow new Exception(\"the current mock assignment does not contain partition \" + partition);\n+\t\t\t\t\tthrow new RuntimeException(\"the current mock assignment does not contain partition \" + partition);\n \t\t\t\t} else {\n \t\t\t\t\tLong mockRetrievedPosition = mockRetrievedPositions.get(partition);\n \t\t\t\t\tif (mockRetrievedPosition == null) {\n-\t\t\t\t\t\tthrow new Exception(\"mock consumer needed to retrieve a position, but no value was provided in the mock values for retrieval\");\n+\t\t\t\t\t\tthrow new RuntimeException(\"mock consumer needed to retrieve a position, but no value was provided in the mock values for retrieval\");\n \t\t\t\t\t} else {\n \t\t\t\t\t\tmockConsumerAssignmentAndPosition.put(partition, mockRetrievedPositions.get(partition));\n \t\t\t\t\t}\n \t\t\t\t}\n-\t\t\t\treturn null;\n \t\t\t}\n-\t\t}).when(mockConsumer).seekToEnd(any(TopicPartition.class));\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic long position(TopicPartition topicPartition) {\n+\t\t\tif (!earlyWakeup) {\n+\t\t\t\treturn mockConsumerAssignmentAndPosition.get(topicPartition);\n+\t\t\t} else {\n+\t\t\t\tthrow new WakeupException();\n+\t\t\t}\n+\t\t}\n \n-\t\treturn mockConsumer;\n+\t\t@Override\n+\t\tpublic OffsetAndMetadata committed(TopicPartition topicPartition) {\n+\t\t\treturn null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "18dd7d344258743705da311508a1fc7d0873da45"}, "originalPosition": 392}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTMxODk2OQ==", "bodyText": "Same here with the UnsupportedOperationException.", "url": "https://github.com/apache/flink/pull/10762#discussion_r365318969", "createdAt": "2020-01-10T16:28:13Z", "author": {"login": "tillrohrmann"}, "path": "flink-connectors/flink-connector-kafka-0.9/src/test/java/org/apache/flink/streaming/connectors/kafka/internal/KafkaConsumerThreadTest.java", "diffHunk": "@@ -875,109 +878,194 @@ void reassignPartitions(List<KafkaTopicPartitionState<TopicPartition>> newPartit\n \t}\n \n \t@SuppressWarnings(\"unchecked\")\n-\tprivate static KafkaConsumer<byte[], byte[]> createMockConsumer(\n+\tprivate static TestConsumer createMockConsumer(\n \t\t\tfinal Map<TopicPartition, Long> mockConsumerAssignmentAndPosition,\n \t\t\tfinal Map<TopicPartition, Long> mockRetrievedPositions,\n \t\t\tfinal boolean earlyWakeup,\n \t\t\tfinal OneShotLatch midAssignmentLatch,\n \t\t\tfinal OneShotLatch continueAssignmentLatch) {\n \n-\t\tfinal KafkaConsumer<byte[], byte[]> mockConsumer = mock(KafkaConsumer.class);\n+\t\treturn new TestConsumer(mockConsumerAssignmentAndPosition, mockRetrievedPositions, earlyWakeup, midAssignmentLatch, continueAssignmentLatch);\n+\t}\n \n-\t\twhen(mockConsumer.assignment()).thenAnswer(new Answer<Object>() {\n-\t\t\t@Override\n-\t\t\tpublic Object answer(InvocationOnMock invocationOnMock) throws Throwable {\n-\t\t\t\tif (midAssignmentLatch != null) {\n-\t\t\t\t\tmidAssignmentLatch.trigger();\n-\t\t\t\t}\n+\tprivate static class TestConsumer implements Consumer<byte[], byte[]> {\n+\t\tprivate final Map<TopicPartition, Long> mockConsumerAssignmentAndPosition;\n+\t\tprivate final Map<TopicPartition, Long> mockRetrievedPositions;\n+\t\tprivate final boolean earlyWakeup;\n+\t\tprivate final OneShotLatch midAssignmentLatch;\n+\t\tprivate final OneShotLatch continueAssignmentLatch;\n+\n+\t\tprivate int numWakeupCalls = 0;\n+\n+\t\tprivate TestConsumer(Map<TopicPartition, Long> mockConsumerAssignmentAndPosition, Map<TopicPartition, Long> mockRetrievedPositions, boolean earlyWakeup, OneShotLatch midAssignmentLatch, OneShotLatch continueAssignmentLatch) {\n+\t\t\tthis.mockConsumerAssignmentAndPosition = mockConsumerAssignmentAndPosition;\n+\t\t\tthis.mockRetrievedPositions = mockRetrievedPositions;\n+\t\t\tthis.earlyWakeup = earlyWakeup;\n+\t\t\tthis.midAssignmentLatch = midAssignmentLatch;\n+\t\t\tthis.continueAssignmentLatch = continueAssignmentLatch;\n+\t\t}\n \n-\t\t\t\tif (continueAssignmentLatch != null) {\n+\t\t@Override\n+\t\tpublic Set<TopicPartition> assignment() {\n+\t\t\tif (midAssignmentLatch != null) {\n+\t\t\t\tmidAssignmentLatch.trigger();\n+\t\t\t}\n+\n+\t\t\tif (continueAssignmentLatch != null) {\n+\t\t\t\ttry {\n \t\t\t\t\tcontinueAssignmentLatch.await();\n+\t\t\t\t} catch (InterruptedException e) {\n+\t\t\t\t\tThread.currentThread().interrupt();\n \t\t\t\t}\n-\t\t\t\treturn mockConsumerAssignmentAndPosition.keySet();\n \t\t\t}\n-\t\t});\n+\t\t\treturn mockConsumerAssignmentAndPosition.keySet();\n+\t\t}\n \n-\t\twhen(mockConsumer.poll(anyLong())).thenReturn(mock(ConsumerRecords.class));\n+\t\t@Override\n+\t\tpublic Set<String> subscription() {\n+\t\t\treturn null;\n+\t\t}\n \n-\t\tif (!earlyWakeup) {\n-\t\t\twhen(mockConsumer.position(any(TopicPartition.class))).thenAnswer(new Answer<Object>() {\n-\t\t\t\t@Override\n-\t\t\t\tpublic Object answer(InvocationOnMock invocationOnMock) throws Throwable {\n-\t\t\t\t\treturn mockConsumerAssignmentAndPosition.get(invocationOnMock.getArgument(0));\n-\t\t\t\t}\n-\t\t\t});\n-\t\t} else {\n-\t\t\twhen(mockConsumer.position(any(TopicPartition.class))).thenThrow(new WakeupException());\n+\t\t@Override\n+\t\tpublic void subscribe(List<String> list) {\n \t\t}\n \n-\t\tdoAnswer(new Answer() {\n-\t\t\t@Override\n-\t\t\tpublic Object answer(InvocationOnMock invocationOnMock) throws Throwable {\n-\t\t\t\tmockConsumerAssignmentAndPosition.clear();\n+\t\t@Override\n+\t\tpublic void subscribe(List<String> list, ConsumerRebalanceListener consumerRebalanceListener) {\n+\t\t}\n \n-\t\t\t\tList<TopicPartition> assignedPartitions = invocationOnMock.getArgument(0);\n-\t\t\t\tfor (TopicPartition assigned : assignedPartitions) {\n-\t\t\t\t\tmockConsumerAssignmentAndPosition.put(assigned, null);\n-\t\t\t\t}\n-\t\t\t\treturn null;\n+\t\t@Override\n+\t\tpublic void assign(List<TopicPartition> assignedPartitions) {\n+\t\t\tmockConsumerAssignmentAndPosition.clear();\n+\n+\t\t\tfor (TopicPartition assigned : assignedPartitions) {\n+\t\t\t\tmockConsumerAssignmentAndPosition.put(assigned, null);\n \t\t\t}\n-\t\t}).when(mockConsumer).assign(anyListOf(TopicPartition.class));\n+\t\t}\n \n-\t\tdoAnswer(new Answer() {\n-\t\t\t@Override\n-\t\t\tpublic Object answer(InvocationOnMock invocationOnMock) throws Throwable {\n-\t\t\t\tTopicPartition partition = invocationOnMock.getArgument(0);\n-\t\t\t\tlong position = invocationOnMock.getArgument(1);\n+\t\t@Override\n+\t\tpublic void subscribe(Pattern pattern, ConsumerRebalanceListener consumerRebalanceListener) {\n+\t\t}\n \n-\t\t\t\tif (!mockConsumerAssignmentAndPosition.containsKey(partition)) {\n-\t\t\t\t\tthrow new Exception(\"the current mock assignment does not contain partition \" + partition);\n-\t\t\t\t} else {\n-\t\t\t\t\tmockConsumerAssignmentAndPosition.put(partition, position);\n-\t\t\t\t}\n-\t\t\t\treturn null;\n-\t\t\t}\n-\t\t}).when(mockConsumer).seek(any(TopicPartition.class), anyLong());\n+\t\t@Override\n+\t\tpublic void unsubscribe() {\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic ConsumerRecords<byte[], byte[]> poll(long l) {\n+\t\t\treturn mock(ConsumerRecords.class);\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void commitSync() {\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void commitSync(Map<TopicPartition, OffsetAndMetadata> map) {\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void commitAsync() {\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void commitAsync(OffsetCommitCallback offsetCommitCallback) {\n+\t\t}\n \n-\t\tdoAnswer(new Answer() {\n-\t\t\t@Override\n-\t\t\tpublic Object answer(InvocationOnMock invocationOnMock) throws Throwable {\n-\t\t\t\tTopicPartition partition = invocationOnMock.getArgument(0);\n+\t\t@Override\n+\t\tpublic void commitAsync(Map<TopicPartition, OffsetAndMetadata> map, OffsetCommitCallback offsetCommitCallback) {\n+\t\t}\n \n+\t\t@Override\n+\t\tpublic void seek(TopicPartition partition, long position) {\n+\t\t\tif (!mockConsumerAssignmentAndPosition.containsKey(partition)) {\n+\t\t\t\tthrow new RuntimeException(\"the current mock assignment does not contain partition \" + partition);\n+\t\t\t} else {\n+\t\t\t\tmockConsumerAssignmentAndPosition.put(partition, position);\n+\t\t\t}\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void seekToBeginning(TopicPartition... partitions) {\n+\t\t\tfor (TopicPartition partition : partitions) {\n \t\t\t\tif (!mockConsumerAssignmentAndPosition.containsKey(partition)) {\n-\t\t\t\t\tthrow new Exception(\"the current mock assignment does not contain partition \" + partition);\n+\t\t\t\t\tthrow new RuntimeException(\"the current mock assignment does not contain partition \" + partition);\n \t\t\t\t} else {\n \t\t\t\t\tLong mockRetrievedPosition = mockRetrievedPositions.get(partition);\n \t\t\t\t\tif (mockRetrievedPosition == null) {\n-\t\t\t\t\t\tthrow new Exception(\"mock consumer needed to retrieve a position, but no value was provided in the mock values for retrieval\");\n+\t\t\t\t\t\tthrow new RuntimeException(\"mock consumer needed to retrieve a position, but no value was provided in the mock values for retrieval\");\n \t\t\t\t\t} else {\n \t\t\t\t\t\tmockConsumerAssignmentAndPosition.put(partition, mockRetrievedPositions.get(partition));\n \t\t\t\t\t}\n \t\t\t\t}\n-\t\t\t\treturn null;\n \t\t\t}\n-\t\t}).when(mockConsumer).seekToBeginning(any(TopicPartition.class));\n-\n-\t\tdoAnswer(new Answer() {\n-\t\t\t@Override\n-\t\t\tpublic Object answer(InvocationOnMock invocationOnMock) throws Throwable {\n-\t\t\t\tTopicPartition partition = invocationOnMock.getArgument(0);\n+\t\t}\n \n+\t\t@Override\n+\t\tpublic void seekToEnd(TopicPartition... partitions) {\n+\t\t\tfor (TopicPartition partition : partitions) {\n \t\t\t\tif (!mockConsumerAssignmentAndPosition.containsKey(partition)) {\n-\t\t\t\t\tthrow new Exception(\"the current mock assignment does not contain partition \" + partition);\n+\t\t\t\t\tthrow new RuntimeException(\"the current mock assignment does not contain partition \" + partition);\n \t\t\t\t} else {\n \t\t\t\t\tLong mockRetrievedPosition = mockRetrievedPositions.get(partition);\n \t\t\t\t\tif (mockRetrievedPosition == null) {\n-\t\t\t\t\t\tthrow new Exception(\"mock consumer needed to retrieve a position, but no value was provided in the mock values for retrieval\");\n+\t\t\t\t\t\tthrow new RuntimeException(\"mock consumer needed to retrieve a position, but no value was provided in the mock values for retrieval\");\n \t\t\t\t\t} else {\n \t\t\t\t\t\tmockConsumerAssignmentAndPosition.put(partition, mockRetrievedPositions.get(partition));\n \t\t\t\t\t}\n \t\t\t\t}\n-\t\t\t\treturn null;\n \t\t\t}\n-\t\t}).when(mockConsumer).seekToEnd(any(TopicPartition.class));\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic long position(TopicPartition topicPartition) {\n+\t\t\tif (!earlyWakeup) {\n+\t\t\t\treturn mockConsumerAssignmentAndPosition.get(topicPartition);\n+\t\t\t} else {\n+\t\t\t\tthrow new WakeupException();\n+\t\t\t}\n+\t\t}\n \n-\t\treturn mockConsumer;\n+\t\t@Override\n+\t\tpublic OffsetAndMetadata committed(TopicPartition topicPartition) {\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic Map<MetricName, ? extends Metric> metrics() {\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic List<PartitionInfo> partitionsFor(String s) {\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic Map<String, List<PartitionInfo>> listTopics() {\n+\t\t\treturn null;\n+\t\t}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "18dd7d344258743705da311508a1fc7d0873da45"}, "originalPosition": 408}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTMyMTEwMA==", "bodyText": "docs/dev/datastream_api.md and docs/dev/datastream_api.zh.md still contain a reference to FlinkKafkaConsumer08", "url": "https://github.com/apache/flink/pull/10762#discussion_r365321100", "createdAt": "2020-01-10T16:32:35Z", "author": {"login": "tillrohrmann"}, "path": "docs/dev/connectors/kafka.zh.md", "diffHunk": "@@ -161,22 +151,18 @@ Flink \u7684 Kafka consumer \u79f0\u4e3a `FlinkKafkaConsumer08`\uff08\u6216\u9002\u7528\u4e8e Kafka 0.9.\n {% highlight java %}\n Properties properties = new Properties();\n properties.setProperty(\"bootstrap.servers\", \"localhost:9092\");\n-// \u4ec5 Kafka 0.8 \u9700\u8981\n-properties.setProperty(\"zookeeper.connect\", \"localhost:2181\");\n properties.setProperty(\"group.id\", \"test\");\n DataStream<String> stream = env\n-  .addSource(new FlinkKafkaConsumer08<>(\"topic\", new SimpleStringSchema(), properties));\n+  .addSource(new FlinkKafkaConsumer09<>(\"topic\", new SimpleStringSchema(), properties));\n {% endhighlight %}\n </div>\n <div data-lang=\"scala\" markdown=\"1\">\n {% highlight scala %}\n val properties = new Properties()\n properties.setProperty(\"bootstrap.servers\", \"localhost:9092\")\n-// \u4ec5 Kafka 0.8 \u9700\u8981\n-properties.setProperty(\"zookeeper.connect\", \"localhost:2181\")\n properties.setProperty(\"group.id\", \"test\")\n stream = env\n-    .addSource(new FlinkKafkaConsumer08[String](\"topic\", new SimpleStringSchema(), properties))\n+    .addSource(new FlinkKafkaConsumer09[String](\"topic\", new SimpleStringSchema(), properties))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3e0a89f84b48d5daa31fac2a2f6e916f4ae596e2"}, "originalPosition": 72}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTMyMjAwMA==", "bodyText": "flink-runtime/pom.xml contains reference to Kafka 0.8 in line 562.", "url": "https://github.com/apache/flink/pull/10762#discussion_r365322000", "createdAt": "2020-01-10T16:34:20Z", "author": {"login": "tillrohrmann"}, "path": "tools/travis/stage.sh", "diffHunk": "@@ -128,11 +128,6 @@ flink-connectors/flink-sql-connector-kafka,\"\n MODULES_TESTS=\"\\\n flink-tests\"\n \n-# we can only build the Kafka 0.8 connector when building for Scala 2.11\n-if [[ $PROFILE == *\"scala-2.11\"* ]]; then\n-    MODULES_CONNECTORS=\"$MODULES_CONNECTORS,flink-connectors/flink-connector-kafka-0.8\"\n-fi\n-", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3e0a89f84b48d5daa31fac2a2f6e916f4ae596e2"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTMyMjY1Mg==", "bodyText": "ConnectorDescriptorValidator.java contains a reference to Kafka 0.8 in line 47.", "url": "https://github.com/apache/flink/pull/10762#discussion_r365322652", "createdAt": "2020-01-10T16:35:36Z", "author": {"login": "tillrohrmann"}, "path": "flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/table/descriptors/KafkaValidator.java", "diffHunk": "@@ -38,7 +38,6 @@\n public class KafkaValidator extends ConnectorDescriptorValidator {\n \n \tpublic static final String CONNECTOR_TYPE_VALUE_KAFKA = \"kafka\";\n-\tpublic static final String CONNECTOR_VERSION_VALUE_08 = \"0.8\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3e0a89f84b48d5daa31fac2a2f6e916f4ae596e2"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTMyMzAwNQ==", "bodyText": "KafkaShortRetentionTestBase.java contains in line 253 Kafka 0.8 specific code. In line 254 the same class contains Kafka 0.9 specific code.", "url": "https://github.com/apache/flink/pull/10762#discussion_r365323005", "createdAt": "2020-01-10T16:36:24Z", "author": {"login": "tillrohrmann"}, "path": "flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/table/descriptors/KafkaValidator.java", "diffHunk": "@@ -38,7 +38,6 @@\n public class KafkaValidator extends ConnectorDescriptorValidator {\n \n \tpublic static final String CONNECTOR_TYPE_VALUE_KAFKA = \"kafka\";\n-\tpublic static final String CONNECTOR_VERSION_VALUE_08 = \"0.8\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTMyMjY1Mg=="}, "originalCommit": {"oid": "3e0a89f84b48d5daa31fac2a2f6e916f4ae596e2"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTMyNjEwOA==", "bodyText": "docs/dev/event_time.md, docs/dev/event_time.zh.md, docs/dev/event_timestamps_watermarks.md and docs/dev/event_timestamps_watermarks.zh.md contains a reference to FlinkKafkaConsumer09", "url": "https://github.com/apache/flink/pull/10762#discussion_r365326108", "createdAt": "2020-01-10T16:42:54Z", "author": {"login": "tillrohrmann"}, "path": "docs/dev/connectors/kafka.md", "diffHunk": "@@ -263,7 +255,7 @@ Example:\n {% highlight java %}\n final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n \n-FlinkKafkaConsumer09<String> myConsumer = new FlinkKafkaConsumer09<>(...);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6bfb6d18af5fdfbddadf6da53adab78567d64aa9"}, "originalPosition": 64}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTMyNzc1MA==", "bodyText": "In line 117, there is still a dependency to flink-connector-kafka-0.9.", "url": "https://github.com/apache/flink/pull/10762#discussion_r365327750", "createdAt": "2020-01-10T16:46:15Z", "author": {"login": "tillrohrmann"}, "path": "flink-connectors/flink-connector-kafka-0.10/pom.xml", "diffHunk": "@@ -46,7 +46,7 @@ under the License.\n \n \t\t<dependency>\n \t\t\t<groupId>org.apache.flink</groupId>\n-\t\t\t<artifactId>flink-connector-kafka-0.9_${scala.binary.version}</artifactId>\n+\t\t\t<artifactId>flink-connector-kafka-base_${scala.binary.version}</artifactId>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6bfb6d18af5fdfbddadf6da53adab78567d64aa9"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTMyODQ4MQ==", "bodyText": "Can we avoid null and instead pass in an empty collection? If not, then let's add @Nullable annotation.", "url": "https://github.com/apache/flink/pull/10762#discussion_r365328481", "createdAt": "2020-01-10T16:47:51Z", "author": {"login": "tillrohrmann"}, "path": "flink-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaConsumer010.java", "diffHunk": "@@ -173,7 +205,38 @@ public FlinkKafkaConsumer010(Pattern subscriptionPattern, DeserializationSchema<\n \t */\n \t@PublicEvolving\n \tpublic FlinkKafkaConsumer010(Pattern subscriptionPattern, KafkaDeserializationSchema<T> deserializer, Properties props) {\n-\t\tsuper(subscriptionPattern, deserializer, props);\n+\t\tthis(null, subscriptionPattern, deserializer, props);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6bfb6d18af5fdfbddadf6da53adab78567d64aa9"}, "originalPosition": 77}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTMyODY4OQ==", "bodyText": "Same here with the null value for Pattern. I think it would be good to avoid it if possible.", "url": "https://github.com/apache/flink/pull/10762#discussion_r365328689", "createdAt": "2020-01-10T16:48:20Z", "author": {"login": "tillrohrmann"}, "path": "flink-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaConsumer010.java", "diffHunk": "@@ -130,7 +162,7 @@ public FlinkKafkaConsumer010(List<String> topics, DeserializationSchema<T> deser\n \t *           The properties that are used to configure both the fetcher and the offset handler.\n \t */\n \tpublic FlinkKafkaConsumer010(List<String> topics, KafkaDeserializationSchema<T> deserializer, Properties props) {\n-\t\tsuper(topics, deserializer, props);\n+\t\tthis(topics, null, deserializer, props);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6bfb6d18af5fdfbddadf6da53adab78567d64aa9"}, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTMzMTA3MQ==", "bodyText": "I would suggest to either fail or to return an empty map but not null.", "url": "https://github.com/apache/flink/pull/10762#discussion_r365331071", "createdAt": "2020-01-10T16:53:28Z", "author": {"login": "tillrohrmann"}, "path": "flink-connectors/flink-connector-kafka-0.10/src/test/java/org/apache/flink/streaming/connectors/kafka/internal/KafkaConsumerThreadTest.java", "diffHunk": "@@ -1047,17 +1047,41 @@ public OffsetAndMetadata committed(TopicPartition topicPartition) {\n \t\t}\n \n \t\t@Override\n-\t\tpublic void pause(TopicPartition... topicPartitions) {\n+\t\tpublic Set<TopicPartition> paused() {\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void pause(Collection<TopicPartition> collection) {\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void resume(Collection<TopicPartition> collection) {\n \t\t}\n \n \t\t@Override\n-\t\tpublic void resume(TopicPartition... topicPartitions) {\n+\t\tpublic Map<TopicPartition, OffsetAndTimestamp> offsetsForTimes(Map<TopicPartition, Long> map) {\n+\t\t\treturn null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6bfb6d18af5fdfbddadf6da53adab78567d64aa9"}, "originalPosition": 105}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTMzMTE0Mw==", "bodyText": "Same here with null.", "url": "https://github.com/apache/flink/pull/10762#discussion_r365331143", "createdAt": "2020-01-10T16:53:37Z", "author": {"login": "tillrohrmann"}, "path": "flink-connectors/flink-connector-kafka-0.10/src/test/java/org/apache/flink/streaming/connectors/kafka/internal/KafkaConsumerThreadTest.java", "diffHunk": "@@ -1047,17 +1047,41 @@ public OffsetAndMetadata committed(TopicPartition topicPartition) {\n \t\t}\n \n \t\t@Override\n-\t\tpublic void pause(TopicPartition... topicPartitions) {\n+\t\tpublic Set<TopicPartition> paused() {\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void pause(Collection<TopicPartition> collection) {\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void resume(Collection<TopicPartition> collection) {\n \t\t}\n \n \t\t@Override\n-\t\tpublic void resume(TopicPartition... topicPartitions) {\n+\t\tpublic Map<TopicPartition, OffsetAndTimestamp> offsetsForTimes(Map<TopicPartition, Long> map) {\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic Map<TopicPartition, Long> beginningOffsets(Collection<TopicPartition> collection) {\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic Map<TopicPartition, Long> endOffsets(Collection<TopicPartition> collection) {\n+\t\t\treturn null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6bfb6d18af5fdfbddadf6da53adab78567d64aa9"}, "originalPosition": 115}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9db1c796618532ffa1ca3542714d56957aa1bffc", "author": {"user": {"login": "zentol", "name": "Chesnay Schepler"}}, "url": "https://github.com/apache/flink/commit/9db1c796618532ffa1ca3542714d56957aa1bffc", "committedDate": "2020-01-13T11:38:18Z", "message": "[FLINK-15115][kafka] Reduce mocking\n\nReplace the mocked KafkaConsumer with a custom Consumer implementation. Migrate all users toward the Consumer interface instead of the concrete KafkaConsumer implementation.\nUsing an actual implementation highlights API changes when migrating to later custom versions and prevents issues due to subtle mocking gotchas."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9bebf0bc9b4d1a726d53504f48d3ecfcc2441b37", "author": {"user": {"login": "zentol", "name": "Chesnay Schepler"}}, "url": "https://github.com/apache/flink/commit/9bebf0bc9b4d1a726d53504f48d3ecfcc2441b37", "committedDate": "2020-01-13T11:38:18Z", "message": "[FLINK-15115][kafka] Drop Kafka 0.8"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2b78e6f3398952c5d5c79822375cc56ee7b96d8a", "author": {"user": {"login": "zentol", "name": "Chesnay Schepler"}}, "url": "https://github.com/apache/flink/commit/2b78e6f3398952c5d5c79822375cc56ee7b96d8a", "committedDate": "2020-01-13T11:38:18Z", "message": "[FLINK-15115][kafka] Drop Kafka 0.9 SQL jar"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "458b80ac0f12dd117ee23fa8ff4f18d74c1a46e5", "author": {"user": {"login": "zentol", "name": "Chesnay Schepler"}}, "url": "https://github.com/apache/flink/commit/458b80ac0f12dd117ee23fa8ff4f18d74c1a46e5", "committedDate": "2020-01-13T11:38:18Z", "message": "[hotfix][kafka][legal] Correct version in NOTICE"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0604a77cf764e1c36abb91352c9ce9410d7e883a", "author": {"user": {"login": "zentol", "name": "Chesnay Schepler"}}, "url": "https://github.com/apache/flink/commit/0604a77cf764e1c36abb91352c9ce9410d7e883a", "committedDate": "2020-01-13T11:38:18Z", "message": "[FLINK-15115][kafka] Drop Kafka 0.9"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "62027cd3e2e3237eebb24fb2ded311feb0ded645", "author": {"user": {"login": "zentol", "name": "Chesnay Schepler"}}, "url": "https://github.com/apache/flink/commit/62027cd3e2e3237eebb24fb2ded311feb0ded645", "committedDate": "2020-01-12T21:53:45Z", "message": "throw all the exceptions"}, "afterCommit": {"oid": "0604a77cf764e1c36abb91352c9ce9410d7e883a", "author": {"user": {"login": "zentol", "name": "Chesnay Schepler"}}, "url": "https://github.com/apache/flink/commit/0604a77cf764e1c36abb91352c9ce9410d7e883a", "committedDate": "2020-01-13T11:38:18Z", "message": "[FLINK-15115][kafka] Drop Kafka 0.9"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4920, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}