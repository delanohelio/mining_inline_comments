{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDE5MjE5NTc2", "number": 12206, "reviewThreads": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQwMjozNDoxNlrOD9OTpQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQwMzowMDozNFrOD9OgcA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1NTI0MTMzOnYy", "diffSide": "RIGHT", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveRowDataPartitionComputer.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQwMjozNDoxNlrOGWl4Xw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQwMjozNDoxNlrOGWl4Xw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjM0MjQ5NQ==", "bodyText": "Perhaps we should rename the old HivePartitionComputer to HiveRowPartitionComputer?", "url": "https://github.com/apache/flink/pull/12206#discussion_r426342495", "createdAt": "2020-05-18T02:34:16Z", "author": {"login": "lirui-apache"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveRowDataPartitionComputer.java", "diffHunk": "@@ -0,0 +1,77 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+import org.apache.flink.table.catalog.hive.client.HiveShim;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.util.DataFormatConverters;\n+import org.apache.flink.table.filesystem.RowDataPartitionComputer;\n+import org.apache.flink.table.functions.hive.conversion.HiveInspectors;\n+import org.apache.flink.table.functions.hive.conversion.HiveObjectConversion;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.utils.TypeConversions;\n+\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+\n+import java.util.Arrays;\n+import java.util.LinkedHashMap;\n+\n+/**\n+ * A {@link RowDataPartitionComputer} that converts Flink objects to Hive objects before computing the partition value strings.\n+ */\n+public class HiveRowDataPartitionComputer extends RowDataPartitionComputer {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "521334c523a5004883f94e1fb996cdd18081500c"}, "originalPosition": 39}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1NTI1OTgzOnYy", "diffSide": "RIGHT", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSink.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQwMjo0OToxOFrOGWmDQg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQwMzoxNjo1OVrOGWmW2g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjM0NTI4Mg==", "bodyText": "We have already get the output format class. I think it's more reliable to check the class to decide whether the table is stored as orc or parquet.", "url": "https://github.com/apache/flink/pull/12206#discussion_r426345282", "createdAt": "2020-05-18T02:49:18Z", "author": {"login": "lirui-apache"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSink.java", "diffHunk": "@@ -143,21 +206,51 @@ public HiveTableSink(JobConf jobConf, ObjectPath tablePath, CatalogTable table)\n \t\t}\n \t}\n \n-\t@Override\n-\tpublic TableSink<Row> configure(String[] fieldNames, TypeInformation<?>[] fieldTypes) {\n-\t\treturn new HiveTableSink(jobConf, tablePath, catalogTable);\n+\tprivate BulkWriter.Factory<RowData> createBulkWriterFactory(String[] partitionColumns,\n+\t\t\tStorageDescriptor sd) {\n+\t\tString serLib = sd.getSerdeInfo().getSerializationLib().toLowerCase();\n+\t\tint formatFieldCount = tableSchema.getFieldCount() - partitionColumns.length;\n+\t\tString[] formatNames = new String[formatFieldCount];\n+\t\tLogicalType[] formatTypes = new LogicalType[formatFieldCount];\n+\t\tfor (int i = 0; i < formatFieldCount; i++) {\n+\t\t\tformatNames[i] = tableSchema.getFieldName(i).get();\n+\t\t\tformatTypes[i] = tableSchema.getFieldDataType(i).get().getLogicalType();\n+\t\t}\n+\t\tRowType formatType = RowType.of(formatTypes, formatNames);\n+\t\tConfiguration formatConf = new Configuration(jobConf);\n+\t\tsd.getSerdeInfo().getParameters().forEach(formatConf::set);\n+\t\tBulkWriter.Factory<RowData> bulkFactory;\n+\t\tif (serLib.contains(\"parquet\")) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ea5f52f890c9cb5728fd7772f59bea4b38f07ca"}, "originalPosition": 235}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjM1MDI5OA==", "bodyText": "I created a JIRA for this refactor: https://issues.apache.org/jira/browse/FLINK-17784", "url": "https://github.com/apache/flink/pull/12206#discussion_r426350298", "createdAt": "2020-05-18T03:16:59Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSink.java", "diffHunk": "@@ -143,21 +206,51 @@ public HiveTableSink(JobConf jobConf, ObjectPath tablePath, CatalogTable table)\n \t\t}\n \t}\n \n-\t@Override\n-\tpublic TableSink<Row> configure(String[] fieldNames, TypeInformation<?>[] fieldTypes) {\n-\t\treturn new HiveTableSink(jobConf, tablePath, catalogTable);\n+\tprivate BulkWriter.Factory<RowData> createBulkWriterFactory(String[] partitionColumns,\n+\t\t\tStorageDescriptor sd) {\n+\t\tString serLib = sd.getSerdeInfo().getSerializationLib().toLowerCase();\n+\t\tint formatFieldCount = tableSchema.getFieldCount() - partitionColumns.length;\n+\t\tString[] formatNames = new String[formatFieldCount];\n+\t\tLogicalType[] formatTypes = new LogicalType[formatFieldCount];\n+\t\tfor (int i = 0; i < formatFieldCount; i++) {\n+\t\t\tformatNames[i] = tableSchema.getFieldName(i).get();\n+\t\t\tformatTypes[i] = tableSchema.getFieldDataType(i).get().getLogicalType();\n+\t\t}\n+\t\tRowType formatType = RowType.of(formatTypes, formatNames);\n+\t\tConfiguration formatConf = new Configuration(jobConf);\n+\t\tsd.getSerdeInfo().getParameters().forEach(formatConf::set);\n+\t\tBulkWriter.Factory<RowData> bulkFactory;\n+\t\tif (serLib.contains(\"parquet\")) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjM0NTI4Mg=="}, "originalCommit": {"oid": "2ea5f52f890c9cb5728fd7772f59bea4b38f07ca"}, "originalPosition": 235}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1NTI2MzgwOnYy", "diffSide": "RIGHT", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/write/HiveOutputFormatFactory.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQwMjo1MjowOFrOGWmFlA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQwMzowNzoyNVrOGWmQTw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjM0NTg3Ng==", "bodyText": "I think we already have a HiveOutputFormatFactory, what's the difference between them?", "url": "https://github.com/apache/flink/pull/12206#discussion_r426345876", "createdAt": "2020-05-18T02:52:08Z", "author": {"login": "lirui-apache"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/write/HiveOutputFormatFactory.java", "diffHunk": "@@ -0,0 +1,81 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive.write;\n+\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.runtime.fs.hdfs.HadoopFileSystem;\n+import org.apache.flink.table.filesystem.OutputFormatFactory;\n+import org.apache.flink.types.Row;\n+\n+import org.apache.hadoop.hive.ql.exec.FileSinkOperator.RecordWriter;\n+import org.apache.hadoop.io.Writable;\n+\n+import java.io.IOException;\n+import java.util.function.Function;\n+\n+/**\n+ * Hive {@link OutputFormatFactory}, use {@link RecordWriter} to write record.\n+ */\n+public class HiveOutputFormatFactory implements OutputFormatFactory<Row> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ea5f52f890c9cb5728fd7772f59bea4b38f07ca"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjM0ODYyMw==", "bodyText": "I will delete old one to extract RecordWriterFactory.", "url": "https://github.com/apache/flink/pull/12206#discussion_r426348623", "createdAt": "2020-05-18T03:07:25Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/write/HiveOutputFormatFactory.java", "diffHunk": "@@ -0,0 +1,81 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive.write;\n+\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.runtime.fs.hdfs.HadoopFileSystem;\n+import org.apache.flink.table.filesystem.OutputFormatFactory;\n+import org.apache.flink.types.Row;\n+\n+import org.apache.hadoop.hive.ql.exec.FileSinkOperator.RecordWriter;\n+import org.apache.hadoop.io.Writable;\n+\n+import java.io.IOException;\n+import java.util.function.Function;\n+\n+/**\n+ * Hive {@link OutputFormatFactory}, use {@link RecordWriter} to write record.\n+ */\n+public class HiveOutputFormatFactory implements OutputFormatFactory<Row> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjM0NTg3Ng=="}, "originalCommit": {"oid": "2ea5f52f890c9cb5728fd7772f59bea4b38f07ca"}, "originalPosition": 36}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1NTI3NDA4OnYy", "diffSide": "RIGHT", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/client/HiveShim.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQwMzowMDozNFrOGWmL4Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQwMzoxNzo1OVrOGWmXnA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjM0NzQ4OQ==", "bodyText": "Shouldn't this method be placed in OrcShim? It's strange that HiveShim needs to understand how flink-orc works.", "url": "https://github.com/apache/flink/pull/12206#discussion_r426347489", "createdAt": "2020-05-18T03:00:34Z", "author": {"login": "lirui-apache"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/client/HiveShim.java", "diffHunk": "@@ -205,4 +208,10 @@ SimpleGenericUDAFParameterInfo createUDAFParameterInfo(ObjectInspector[] params,\n \t */\n \tvoid createTableWithConstraints(IMetaStoreClient client, Table table, Configuration conf,\n \t\t\tUniqueConstraint pk, List<Byte> pkTraits, List<String> notNullCols, List<Byte> nnTraits);\n+\n+\t/**\n+\t * Create orc {@link BulkWriter.Factory} for different hive versions.\n+\t */\n+\tBulkWriter.Factory<RowData> createOrcBulkWriterFactory(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ea5f52f890c9cb5728fd7772f59bea4b38f07ca"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjM0ODc4NQ==", "bodyText": "I think this is hive shim, because orc factory is related to hive version.\nIf not this, will be a ugly if else.", "url": "https://github.com/apache/flink/pull/12206#discussion_r426348785", "createdAt": "2020-05-18T03:08:23Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/client/HiveShim.java", "diffHunk": "@@ -205,4 +208,10 @@ SimpleGenericUDAFParameterInfo createUDAFParameterInfo(ObjectInspector[] params,\n \t */\n \tvoid createTableWithConstraints(IMetaStoreClient client, Table table, Configuration conf,\n \t\t\tUniqueConstraint pk, List<Byte> pkTraits, List<String> notNullCols, List<Byte> nnTraits);\n+\n+\t/**\n+\t * Create orc {@link BulkWriter.Factory} for different hive versions.\n+\t */\n+\tBulkWriter.Factory<RowData> createOrcBulkWriterFactory(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjM0NzQ4OQ=="}, "originalCommit": {"oid": "2ea5f52f890c9cb5728fd7772f59bea4b38f07ca"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjM1MDQ5Mg==", "bodyText": "I created a orc shim improvement: https://issues.apache.org/jira/browse/FLINK-17785", "url": "https://github.com/apache/flink/pull/12206#discussion_r426350492", "createdAt": "2020-05-18T03:17:59Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/client/HiveShim.java", "diffHunk": "@@ -205,4 +208,10 @@ SimpleGenericUDAFParameterInfo createUDAFParameterInfo(ObjectInspector[] params,\n \t */\n \tvoid createTableWithConstraints(IMetaStoreClient client, Table table, Configuration conf,\n \t\t\tUniqueConstraint pk, List<Byte> pkTraits, List<String> notNullCols, List<Byte> nnTraits);\n+\n+\t/**\n+\t * Create orc {@link BulkWriter.Factory} for different hive versions.\n+\t */\n+\tBulkWriter.Factory<RowData> createOrcBulkWriterFactory(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjM0NzQ4OQ=="}, "originalCommit": {"oid": "2ea5f52f890c9cb5728fd7772f59bea4b38f07ca"}, "originalPosition": 20}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1356, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}