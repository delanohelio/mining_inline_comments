{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTAxOTk2NTc3", "number": 13604, "reviewThreads": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQxNTowODo1NlrOEtiDeA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQxNTozNTozNlrOEti1nQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE2MTc5MzIwOnYy", "diffSide": "RIGHT", "path": "flink-tests/src/test/java/org/apache/flink/test/checkpointing/UnalignedCheckpointCompatibilityITCase.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQxNTowODo1NlrOHhX5Pw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQxOToxOTo0OVrOHhhlYw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDc1NjU0Mw==", "bodyText": "It's generally an anti-pattern in tests to \"just sleep for a while\". Can't you use the backpressure monitoring to wait until there is backpressure? (or would that take longer?)", "url": "https://github.com/apache/flink/pull/13604#discussion_r504756543", "createdAt": "2020-10-14T15:08:56Z", "author": {"login": "rmetzger"}, "path": "flink-tests/src/test/java/org/apache/flink/test/checkpointing/UnalignedCheckpointCompatibilityITCase.java", "diffHunk": "@@ -107,28 +119,55 @@ public void test() throws Exception {\n \t}\n \n \tprivate Tuple2<String, Map<String, Object>> runAndTakeSavepoint() throws Exception {\n-\t\tJobClient jobClient = submitJobInitially(env(startAligned, 0, emptyMap()));\n-\t\tThread.sleep(FIRST_RUN_EL_COUNT * FIRST_RUN_BACKPRESSURE_MS); // wait for all tasks to run and some backpressure from sink\n-\t\tFuture<Map<String, Object>> accFuture = jobClient.getAccumulators();\n-\t\tFuture<String> savepointFuture = jobClient.stopWithSavepoint(false, tempFolder().toURI().toString());\n-\t\treturn new Tuple2<>(savepointFuture.get(), accFuture.get());\n+\t\treturn withCluster(new Configuration(), miniCluster -> {\n+\t\t\tJobClient jobClient = submitJobInitially(env(startAligned, 0));\n+\t\t\twaitForAllTaskRunning(miniCluster, jobClient.getJobID(), Deadline.fromNow(Duration.of(30, ChronoUnit.SECONDS)));\n+\t\t\tThread.sleep(FIRST_RUN_BACKPRESSURE_MS); // wait for some backpressure from sink", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "755d9eb83e7fa1f497fe083850ce369d8d1d5940"}, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDkxMzA3NA==", "bodyText": "The original test used sleeping for waiting until all jobs are running (hence this bug). However, whether backpressure build up or not is quite irrelevant for the semantics of the test. It's just testing that some outputs are present. I could probably wait on the output metric.\nHowever, it feels like LOTS of such building blocks are missing and should probably be at a more general place. (I tried to do that with waitForAllTaskRunning but it's easily overlooked. Ideally it should be in MiniCluster).", "url": "https://github.com/apache/flink/pull/13604#discussion_r504913074", "createdAt": "2020-10-14T19:15:30Z", "author": {"login": "AHeise"}, "path": "flink-tests/src/test/java/org/apache/flink/test/checkpointing/UnalignedCheckpointCompatibilityITCase.java", "diffHunk": "@@ -107,28 +119,55 @@ public void test() throws Exception {\n \t}\n \n \tprivate Tuple2<String, Map<String, Object>> runAndTakeSavepoint() throws Exception {\n-\t\tJobClient jobClient = submitJobInitially(env(startAligned, 0, emptyMap()));\n-\t\tThread.sleep(FIRST_RUN_EL_COUNT * FIRST_RUN_BACKPRESSURE_MS); // wait for all tasks to run and some backpressure from sink\n-\t\tFuture<Map<String, Object>> accFuture = jobClient.getAccumulators();\n-\t\tFuture<String> savepointFuture = jobClient.stopWithSavepoint(false, tempFolder().toURI().toString());\n-\t\treturn new Tuple2<>(savepointFuture.get(), accFuture.get());\n+\t\treturn withCluster(new Configuration(), miniCluster -> {\n+\t\t\tJobClient jobClient = submitJobInitially(env(startAligned, 0));\n+\t\t\twaitForAllTaskRunning(miniCluster, jobClient.getJobID(), Deadline.fromNow(Duration.of(30, ChronoUnit.SECONDS)));\n+\t\t\tThread.sleep(FIRST_RUN_BACKPRESSURE_MS); // wait for some backpressure from sink", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDc1NjU0Mw=="}, "originalCommit": {"oid": "755d9eb83e7fa1f497fe083850ce369d8d1d5940"}, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDkxNTI5OQ==", "bodyText": "Maybe such utilities should also be made available to the user, for writing their own tests.\nThere are a lot of very useful utilities everywhere in our tests, but it's difficult to discover them. Ideally we have them categorized and standardized somewhere.", "url": "https://github.com/apache/flink/pull/13604#discussion_r504915299", "createdAt": "2020-10-14T19:19:49Z", "author": {"login": "rmetzger"}, "path": "flink-tests/src/test/java/org/apache/flink/test/checkpointing/UnalignedCheckpointCompatibilityITCase.java", "diffHunk": "@@ -107,28 +119,55 @@ public void test() throws Exception {\n \t}\n \n \tprivate Tuple2<String, Map<String, Object>> runAndTakeSavepoint() throws Exception {\n-\t\tJobClient jobClient = submitJobInitially(env(startAligned, 0, emptyMap()));\n-\t\tThread.sleep(FIRST_RUN_EL_COUNT * FIRST_RUN_BACKPRESSURE_MS); // wait for all tasks to run and some backpressure from sink\n-\t\tFuture<Map<String, Object>> accFuture = jobClient.getAccumulators();\n-\t\tFuture<String> savepointFuture = jobClient.stopWithSavepoint(false, tempFolder().toURI().toString());\n-\t\treturn new Tuple2<>(savepointFuture.get(), accFuture.get());\n+\t\treturn withCluster(new Configuration(), miniCluster -> {\n+\t\t\tJobClient jobClient = submitJobInitially(env(startAligned, 0));\n+\t\t\twaitForAllTaskRunning(miniCluster, jobClient.getJobID(), Deadline.fromNow(Duration.of(30, ChronoUnit.SECONDS)));\n+\t\t\tThread.sleep(FIRST_RUN_BACKPRESSURE_MS); // wait for some backpressure from sink", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDc1NjU0Mw=="}, "originalCommit": {"oid": "755d9eb83e7fa1f497fe083850ce369d8d1d5940"}, "originalPosition": 69}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE2MTgzMTQ0OnYy", "diffSide": "RIGHT", "path": "flink-tests/src/test/java/org/apache/flink/test/checkpointing/UnalignedCheckpointCompatibilityITCase.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQxNToxNjozN1rOHhYQdA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQxOToxMzoxMFrOHhhXMg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDc2MjQ4NA==", "bodyText": "Why are you not reusing the cluster for the different tests / jobs?", "url": "https://github.com/apache/flink/pull/13604#discussion_r504762484", "createdAt": "2020-10-14T15:16:37Z", "author": {"login": "rmetzger"}, "path": "flink-tests/src/test/java/org/apache/flink/test/checkpointing/UnalignedCheckpointCompatibilityITCase.java", "diffHunk": "@@ -107,28 +119,55 @@ public void test() throws Exception {\n \t}\n \n \tprivate Tuple2<String, Map<String, Object>> runAndTakeSavepoint() throws Exception {\n-\t\tJobClient jobClient = submitJobInitially(env(startAligned, 0, emptyMap()));\n-\t\tThread.sleep(FIRST_RUN_EL_COUNT * FIRST_RUN_BACKPRESSURE_MS); // wait for all tasks to run and some backpressure from sink\n-\t\tFuture<Map<String, Object>> accFuture = jobClient.getAccumulators();\n-\t\tFuture<String> savepointFuture = jobClient.stopWithSavepoint(false, tempFolder().toURI().toString());\n-\t\treturn new Tuple2<>(savepointFuture.get(), accFuture.get());\n+\t\treturn withCluster(new Configuration(), miniCluster -> {\n+\t\t\tJobClient jobClient = submitJobInitially(env(startAligned, 0));\n+\t\t\twaitForAllTaskRunning(miniCluster, jobClient.getJobID(), Deadline.fromNow(Duration.of(30, ChronoUnit.SECONDS)));\n+\t\t\tThread.sleep(FIRST_RUN_BACKPRESSURE_MS); // wait for some backpressure from sink\n+\n+\t\t\tFuture<Map<String, Object>> accFuture = jobClient.getAccumulators();\n+\t\t\tFuture<String> savepointFuture = jobClient.stopWithSavepoint(false, tempFolder().toURI().toString());\n+\t\t\treturn new Tuple2<>(savepointFuture.get(), accFuture.get());\n+\t\t});\n \t}\n \n \tprivate Tuple2<String, Map<String, Object>> runAndTakeExternalCheckpoint() throws Exception {\n \t\tFile folder = tempFolder();\n-\t\tJobClient jobClient = submitJobInitially(externalCheckpointEnv(startAligned, folder, 100));\n-\t\tFile metadata = waitForChild(waitForChild(waitForChild(folder))); // structure: root/attempt/checkpoint/_metadata\n-\t\tcancelJob(jobClient);\n-\t\treturn new Tuple2<>(metadata.getParentFile().toString(), emptyMap());\n+\t\tfinal Configuration conf = new Configuration();\n+\t\tconf.set(CHECKPOINTS_DIRECTORY, folder.toURI().toString());\n+\t\t// prevent deletion of checkpoint files while it's being checked and used\n+\t\tconf.set(MAX_RETAINED_CHECKPOINTS, Integer.MAX_VALUE);\n+\t\treturn withCluster(conf,\n+\t\t\tminiCluster -> {\n+\t\t\t\tJobClient jobClient = submitJobInitially(externalCheckpointEnv(startAligned, 100));\n+\t\t\t\tFile metadata = waitForChild(waitForChild(waitForChild(folder))); // structure: root/attempt/checkpoint/_metadata\n+\t\t\t\tcancelJob(jobClient);\n+\t\t\t\treturn new Tuple2<>(metadata.getParentFile().toString(), emptyMap());\n+\t\t\t});\n \t}\n \n \tprivate static JobClient submitJobInitially(StreamExecutionEnvironment env) throws Exception {\n \t\treturn env.executeAsync(dag(FIRST_RUN_EL_COUNT, true, FIRST_RUN_BACKPRESSURE_MS, env));\n \t}\n \n \tprivate Map<String, Object> runFromSavepoint(String path, boolean isAligned, int totalCount) throws Exception {\n-\t\tStreamExecutionEnvironment env = env(isAligned, 50, Collections.singletonMap(SAVEPOINT_PATH, path));\n-\t\treturn env.execute(dag(totalCount, false, 0, env)).getJobExecutionResult().getAllAccumulatorResults();\n+\t\treturn withCluster(new Configuration(), miniCluster -> {\n+\t\t\tStreamExecutionEnvironment env = env(isAligned, 50);\n+\t\t\tfinal StreamGraph streamGraph = dag(totalCount, false, 0, env);\n+\t\t\tstreamGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(path));\n+\t\t\treturn env.execute(streamGraph).getJobExecutionResult().getAllAccumulatorResults();\n+\t\t});\n+\t}\n+\n+\tprivate <T> T withCluster(Configuration configuration,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "755d9eb83e7fa1f497fe083850ce369d8d1d5940"}, "originalPosition": 111}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDkxMTU2NQ==", "bodyText": "Good point. Originally, I needed to inject the configuration to restore from savepoints, but that didn't work. So I can probably leave a cluster per test. A global cluster would probably also work, I just need to make the TempFolder a class rule.", "url": "https://github.com/apache/flink/pull/13604#discussion_r504911565", "createdAt": "2020-10-14T19:12:59Z", "author": {"login": "AHeise"}, "path": "flink-tests/src/test/java/org/apache/flink/test/checkpointing/UnalignedCheckpointCompatibilityITCase.java", "diffHunk": "@@ -107,28 +119,55 @@ public void test() throws Exception {\n \t}\n \n \tprivate Tuple2<String, Map<String, Object>> runAndTakeSavepoint() throws Exception {\n-\t\tJobClient jobClient = submitJobInitially(env(startAligned, 0, emptyMap()));\n-\t\tThread.sleep(FIRST_RUN_EL_COUNT * FIRST_RUN_BACKPRESSURE_MS); // wait for all tasks to run and some backpressure from sink\n-\t\tFuture<Map<String, Object>> accFuture = jobClient.getAccumulators();\n-\t\tFuture<String> savepointFuture = jobClient.stopWithSavepoint(false, tempFolder().toURI().toString());\n-\t\treturn new Tuple2<>(savepointFuture.get(), accFuture.get());\n+\t\treturn withCluster(new Configuration(), miniCluster -> {\n+\t\t\tJobClient jobClient = submitJobInitially(env(startAligned, 0));\n+\t\t\twaitForAllTaskRunning(miniCluster, jobClient.getJobID(), Deadline.fromNow(Duration.of(30, ChronoUnit.SECONDS)));\n+\t\t\tThread.sleep(FIRST_RUN_BACKPRESSURE_MS); // wait for some backpressure from sink\n+\n+\t\t\tFuture<Map<String, Object>> accFuture = jobClient.getAccumulators();\n+\t\t\tFuture<String> savepointFuture = jobClient.stopWithSavepoint(false, tempFolder().toURI().toString());\n+\t\t\treturn new Tuple2<>(savepointFuture.get(), accFuture.get());\n+\t\t});\n \t}\n \n \tprivate Tuple2<String, Map<String, Object>> runAndTakeExternalCheckpoint() throws Exception {\n \t\tFile folder = tempFolder();\n-\t\tJobClient jobClient = submitJobInitially(externalCheckpointEnv(startAligned, folder, 100));\n-\t\tFile metadata = waitForChild(waitForChild(waitForChild(folder))); // structure: root/attempt/checkpoint/_metadata\n-\t\tcancelJob(jobClient);\n-\t\treturn new Tuple2<>(metadata.getParentFile().toString(), emptyMap());\n+\t\tfinal Configuration conf = new Configuration();\n+\t\tconf.set(CHECKPOINTS_DIRECTORY, folder.toURI().toString());\n+\t\t// prevent deletion of checkpoint files while it's being checked and used\n+\t\tconf.set(MAX_RETAINED_CHECKPOINTS, Integer.MAX_VALUE);\n+\t\treturn withCluster(conf,\n+\t\t\tminiCluster -> {\n+\t\t\t\tJobClient jobClient = submitJobInitially(externalCheckpointEnv(startAligned, 100));\n+\t\t\t\tFile metadata = waitForChild(waitForChild(waitForChild(folder))); // structure: root/attempt/checkpoint/_metadata\n+\t\t\t\tcancelJob(jobClient);\n+\t\t\t\treturn new Tuple2<>(metadata.getParentFile().toString(), emptyMap());\n+\t\t\t});\n \t}\n \n \tprivate static JobClient submitJobInitially(StreamExecutionEnvironment env) throws Exception {\n \t\treturn env.executeAsync(dag(FIRST_RUN_EL_COUNT, true, FIRST_RUN_BACKPRESSURE_MS, env));\n \t}\n \n \tprivate Map<String, Object> runFromSavepoint(String path, boolean isAligned, int totalCount) throws Exception {\n-\t\tStreamExecutionEnvironment env = env(isAligned, 50, Collections.singletonMap(SAVEPOINT_PATH, path));\n-\t\treturn env.execute(dag(totalCount, false, 0, env)).getJobExecutionResult().getAllAccumulatorResults();\n+\t\treturn withCluster(new Configuration(), miniCluster -> {\n+\t\t\tStreamExecutionEnvironment env = env(isAligned, 50);\n+\t\t\tfinal StreamGraph streamGraph = dag(totalCount, false, 0, env);\n+\t\t\tstreamGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(path));\n+\t\t\treturn env.execute(streamGraph).getJobExecutionResult().getAllAccumulatorResults();\n+\t\t});\n+\t}\n+\n+\tprivate <T> T withCluster(Configuration configuration,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDc2MjQ4NA=="}, "originalCommit": {"oid": "755d9eb83e7fa1f497fe083850ce369d8d1d5940"}, "originalPosition": 111}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDkxMTY2Ng==", "bodyText": "Good point. Originally, I needed to inject the configuration to restore from savepoints, but that didn't work. So I can probably leave a cluster per test. A global cluster would probably also work, I just need to make the TempFolder a class rule.", "url": "https://github.com/apache/flink/pull/13604#discussion_r504911666", "createdAt": "2020-10-14T19:13:10Z", "author": {"login": "AHeise"}, "path": "flink-tests/src/test/java/org/apache/flink/test/checkpointing/UnalignedCheckpointCompatibilityITCase.java", "diffHunk": "@@ -107,28 +119,55 @@ public void test() throws Exception {\n \t}\n \n \tprivate Tuple2<String, Map<String, Object>> runAndTakeSavepoint() throws Exception {\n-\t\tJobClient jobClient = submitJobInitially(env(startAligned, 0, emptyMap()));\n-\t\tThread.sleep(FIRST_RUN_EL_COUNT * FIRST_RUN_BACKPRESSURE_MS); // wait for all tasks to run and some backpressure from sink\n-\t\tFuture<Map<String, Object>> accFuture = jobClient.getAccumulators();\n-\t\tFuture<String> savepointFuture = jobClient.stopWithSavepoint(false, tempFolder().toURI().toString());\n-\t\treturn new Tuple2<>(savepointFuture.get(), accFuture.get());\n+\t\treturn withCluster(new Configuration(), miniCluster -> {\n+\t\t\tJobClient jobClient = submitJobInitially(env(startAligned, 0));\n+\t\t\twaitForAllTaskRunning(miniCluster, jobClient.getJobID(), Deadline.fromNow(Duration.of(30, ChronoUnit.SECONDS)));\n+\t\t\tThread.sleep(FIRST_RUN_BACKPRESSURE_MS); // wait for some backpressure from sink\n+\n+\t\t\tFuture<Map<String, Object>> accFuture = jobClient.getAccumulators();\n+\t\t\tFuture<String> savepointFuture = jobClient.stopWithSavepoint(false, tempFolder().toURI().toString());\n+\t\t\treturn new Tuple2<>(savepointFuture.get(), accFuture.get());\n+\t\t});\n \t}\n \n \tprivate Tuple2<String, Map<String, Object>> runAndTakeExternalCheckpoint() throws Exception {\n \t\tFile folder = tempFolder();\n-\t\tJobClient jobClient = submitJobInitially(externalCheckpointEnv(startAligned, folder, 100));\n-\t\tFile metadata = waitForChild(waitForChild(waitForChild(folder))); // structure: root/attempt/checkpoint/_metadata\n-\t\tcancelJob(jobClient);\n-\t\treturn new Tuple2<>(metadata.getParentFile().toString(), emptyMap());\n+\t\tfinal Configuration conf = new Configuration();\n+\t\tconf.set(CHECKPOINTS_DIRECTORY, folder.toURI().toString());\n+\t\t// prevent deletion of checkpoint files while it's being checked and used\n+\t\tconf.set(MAX_RETAINED_CHECKPOINTS, Integer.MAX_VALUE);\n+\t\treturn withCluster(conf,\n+\t\t\tminiCluster -> {\n+\t\t\t\tJobClient jobClient = submitJobInitially(externalCheckpointEnv(startAligned, 100));\n+\t\t\t\tFile metadata = waitForChild(waitForChild(waitForChild(folder))); // structure: root/attempt/checkpoint/_metadata\n+\t\t\t\tcancelJob(jobClient);\n+\t\t\t\treturn new Tuple2<>(metadata.getParentFile().toString(), emptyMap());\n+\t\t\t});\n \t}\n \n \tprivate static JobClient submitJobInitially(StreamExecutionEnvironment env) throws Exception {\n \t\treturn env.executeAsync(dag(FIRST_RUN_EL_COUNT, true, FIRST_RUN_BACKPRESSURE_MS, env));\n \t}\n \n \tprivate Map<String, Object> runFromSavepoint(String path, boolean isAligned, int totalCount) throws Exception {\n-\t\tStreamExecutionEnvironment env = env(isAligned, 50, Collections.singletonMap(SAVEPOINT_PATH, path));\n-\t\treturn env.execute(dag(totalCount, false, 0, env)).getJobExecutionResult().getAllAccumulatorResults();\n+\t\treturn withCluster(new Configuration(), miniCluster -> {\n+\t\t\tStreamExecutionEnvironment env = env(isAligned, 50);\n+\t\t\tfinal StreamGraph streamGraph = dag(totalCount, false, 0, env);\n+\t\t\tstreamGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(path));\n+\t\t\treturn env.execute(streamGraph).getJobExecutionResult().getAllAccumulatorResults();\n+\t\t});\n+\t}\n+\n+\tprivate <T> T withCluster(Configuration configuration,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDc2MjQ4NA=="}, "originalCommit": {"oid": "755d9eb83e7fa1f497fe083850ce369d8d1d5940"}, "originalPosition": 111}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE2MTkyMTU3OnYy", "diffSide": "RIGHT", "path": "flink-test-utils-parent/flink-test-utils/src/main/java/org/apache/flink/test/util/TestBaseUtils.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQxNTozNTozNlrOHhZJbA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQxOTo0ODo1NVrOHhikvw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDc3NzA2OA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \tpublic static void waitForAllTaskRunning(MiniClusterResource miniCluster, JobID jobID, Deadline deadline) throws Exception {\n          \n          \n            \n            \t\ttry (final RestClusterClient<?> clusterClient = new RestClusterClient<Object>(\n          \n          \n            \n            \t\t\t\tminiCluster.getClientConfiguration(),\n          \n          \n            \n            \t\t\t\tStandaloneClusterId.getInstance())) {\n          \n          \n            \n            \t\t\tJobMessageParameters params = new JobMessageParameters();\n          \n          \n            \n            \t\t\tparams.jobPathParameter.resolve(jobID);\n          \n          \n            \n            \t\t\torg.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(() -> {\n          \n          \n            \n            \t\t\t\tfinal JobDetailsInfo jobDetailsInfo = clusterClient.sendRequest(\n          \n          \n            \n            \t\t\t\t\tJobDetailsHeaders.getInstance(),\n          \n          \n            \n            \t\t\t\t\tparams,\n          \n          \n            \n            \t\t\t\t\tEmptyRequestBody.getInstance()).get();\n          \n          \n            \n            \t\t\t\treturn jobDetailsInfo.getJobStatus() == JobStatus.RUNNING &&\n          \n          \n            \n            \t\t\t\t\tjobDetailsInfo.getJobVerticesPerState().get(ExecutionState.RUNNING) ==\n          \n          \n            \n            \t\t\t\t\t\tjobDetailsInfo.getJobVertexInfos().size();\n          \n          \n            \n            \t\t\t}, deadline, 500);\n          \n          \n            \n            \t\t}\n          \n          \n            \n            \t}\n          \n          \n            \n            \tpublic static void waitForAllTaskRunning(MiniCluster miniCluster, JobID jobID, Deadline deadline) throws Exception {\n          \n          \n            \n            \t\torg.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(() -> {\n          \n          \n            \n            \t\t\tAccessExecutionGraph ec = miniCluster.getExecutionGraph(jobID).get();\n          \n          \n            \n            \t\t\treturn ec.getState() == JobStatus.RUNNING && ec.getAllVertices()\n          \n          \n            \n            \t\t\t\t.values()\n          \n          \n            \n            \t\t\t\t.stream()\n          \n          \n            \n            \t\t\t\t.allMatch(jobVertex ->\n          \n          \n            \n            \t\t\t\t\tArrays.stream(jobVertex.getTaskVertices()).allMatch(task -> task.getExecutionState() == ExecutionState.RUNNING)\n          \n          \n            \n            \t\t\t\t);\n          \n          \n            \n            \t\t}, deadline);\n          \n          \n            \n            \t}", "url": "https://github.com/apache/flink/pull/13604#discussion_r504777068", "createdAt": "2020-10-14T15:35:36Z", "author": {"login": "rmetzger"}, "path": "flink-test-utils-parent/flink-test-utils/src/main/java/org/apache/flink/test/util/TestBaseUtils.java", "diffHunk": "@@ -102,6 +116,24 @@ private static void verifyJvmOptions() {\n \t\t\t\t+ \"m\", heap > MINIMUM_HEAP_SIZE_MB - 50);\n \t}\n \n+\tpublic static void waitForAllTaskRunning(MiniClusterResource miniCluster, JobID jobID, Deadline deadline) throws Exception {\n+\t\ttry (final RestClusterClient<?> clusterClient = new RestClusterClient<Object>(\n+\t\t\t\tminiCluster.getClientConfiguration(),\n+\t\t\t\tStandaloneClusterId.getInstance())) {\n+\t\t\tJobMessageParameters params = new JobMessageParameters();\n+\t\t\tparams.jobPathParameter.resolve(jobID);\n+\t\t\torg.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(() -> {\n+\t\t\t\tfinal JobDetailsInfo jobDetailsInfo = clusterClient.sendRequest(\n+\t\t\t\t\tJobDetailsHeaders.getInstance(),\n+\t\t\t\t\tparams,\n+\t\t\t\t\tEmptyRequestBody.getInstance()).get();\n+\t\t\t\treturn jobDetailsInfo.getJobStatus() == JobStatus.RUNNING &&\n+\t\t\t\t\tjobDetailsInfo.getJobVerticesPerState().get(ExecutionState.RUNNING) ==\n+\t\t\t\t\t\tjobDetailsInfo.getJobVertexInfos().size();\n+\t\t\t}, deadline, 500);\n+\t\t}\n+\t}\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "755d9eb83e7fa1f497fe083850ce369d8d1d5940"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDc3NzM1OA==", "bodyText": "Not sure about the code formatting, and I haven't tested it much. But I guess you get the idea.", "url": "https://github.com/apache/flink/pull/13604#discussion_r504777358", "createdAt": "2020-10-14T15:36:00Z", "author": {"login": "rmetzger"}, "path": "flink-test-utils-parent/flink-test-utils/src/main/java/org/apache/flink/test/util/TestBaseUtils.java", "diffHunk": "@@ -102,6 +116,24 @@ private static void verifyJvmOptions() {\n \t\t\t\t+ \"m\", heap > MINIMUM_HEAP_SIZE_MB - 50);\n \t}\n \n+\tpublic static void waitForAllTaskRunning(MiniClusterResource miniCluster, JobID jobID, Deadline deadline) throws Exception {\n+\t\ttry (final RestClusterClient<?> clusterClient = new RestClusterClient<Object>(\n+\t\t\t\tminiCluster.getClientConfiguration(),\n+\t\t\t\tStandaloneClusterId.getInstance())) {\n+\t\t\tJobMessageParameters params = new JobMessageParameters();\n+\t\t\tparams.jobPathParameter.resolve(jobID);\n+\t\t\torg.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(() -> {\n+\t\t\t\tfinal JobDetailsInfo jobDetailsInfo = clusterClient.sendRequest(\n+\t\t\t\t\tJobDetailsHeaders.getInstance(),\n+\t\t\t\t\tparams,\n+\t\t\t\t\tEmptyRequestBody.getInstance()).get();\n+\t\t\t\treturn jobDetailsInfo.getJobStatus() == JobStatus.RUNNING &&\n+\t\t\t\t\tjobDetailsInfo.getJobVerticesPerState().get(ExecutionState.RUNNING) ==\n+\t\t\t\t\t\tjobDetailsInfo.getJobVertexInfos().size();\n+\t\t\t}, deadline, 500);\n+\t\t}\n+\t}\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDc3NzA2OA=="}, "originalCommit": {"oid": "755d9eb83e7fa1f497fe083850ce369d8d1d5940"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDkxNDEwNg==", "bodyText": "Hm Till actually proposed to use rest client, but I can't really see why your version wouldn't work.", "url": "https://github.com/apache/flink/pull/13604#discussion_r504914106", "createdAt": "2020-10-14T19:17:31Z", "author": {"login": "AHeise"}, "path": "flink-test-utils-parent/flink-test-utils/src/main/java/org/apache/flink/test/util/TestBaseUtils.java", "diffHunk": "@@ -102,6 +116,24 @@ private static void verifyJvmOptions() {\n \t\t\t\t+ \"m\", heap > MINIMUM_HEAP_SIZE_MB - 50);\n \t}\n \n+\tpublic static void waitForAllTaskRunning(MiniClusterResource miniCluster, JobID jobID, Deadline deadline) throws Exception {\n+\t\ttry (final RestClusterClient<?> clusterClient = new RestClusterClient<Object>(\n+\t\t\t\tminiCluster.getClientConfiguration(),\n+\t\t\t\tStandaloneClusterId.getInstance())) {\n+\t\t\tJobMessageParameters params = new JobMessageParameters();\n+\t\t\tparams.jobPathParameter.resolve(jobID);\n+\t\t\torg.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(() -> {\n+\t\t\t\tfinal JobDetailsInfo jobDetailsInfo = clusterClient.sendRequest(\n+\t\t\t\t\tJobDetailsHeaders.getInstance(),\n+\t\t\t\t\tparams,\n+\t\t\t\t\tEmptyRequestBody.getInstance()).get();\n+\t\t\t\treturn jobDetailsInfo.getJobStatus() == JobStatus.RUNNING &&\n+\t\t\t\t\tjobDetailsInfo.getJobVerticesPerState().get(ExecutionState.RUNNING) ==\n+\t\t\t\t\t\tjobDetailsInfo.getJobVertexInfos().size();\n+\t\t\t}, deadline, 500);\n+\t\t}\n+\t}\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDc3NzA2OA=="}, "originalCommit": {"oid": "755d9eb83e7fa1f497fe083850ce369d8d1d5940"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDkzMTUxOQ==", "bodyText": "With your version, I can actually move it into flink-runtime CommonTestUtils.", "url": "https://github.com/apache/flink/pull/13604#discussion_r504931519", "createdAt": "2020-10-14T19:48:55Z", "author": {"login": "AHeise"}, "path": "flink-test-utils-parent/flink-test-utils/src/main/java/org/apache/flink/test/util/TestBaseUtils.java", "diffHunk": "@@ -102,6 +116,24 @@ private static void verifyJvmOptions() {\n \t\t\t\t+ \"m\", heap > MINIMUM_HEAP_SIZE_MB - 50);\n \t}\n \n+\tpublic static void waitForAllTaskRunning(MiniClusterResource miniCluster, JobID jobID, Deadline deadline) throws Exception {\n+\t\ttry (final RestClusterClient<?> clusterClient = new RestClusterClient<Object>(\n+\t\t\t\tminiCluster.getClientConfiguration(),\n+\t\t\t\tStandaloneClusterId.getInstance())) {\n+\t\t\tJobMessageParameters params = new JobMessageParameters();\n+\t\t\tparams.jobPathParameter.resolve(jobID);\n+\t\t\torg.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(() -> {\n+\t\t\t\tfinal JobDetailsInfo jobDetailsInfo = clusterClient.sendRequest(\n+\t\t\t\t\tJobDetailsHeaders.getInstance(),\n+\t\t\t\t\tparams,\n+\t\t\t\t\tEmptyRequestBody.getInstance()).get();\n+\t\t\t\treturn jobDetailsInfo.getJobStatus() == JobStatus.RUNNING &&\n+\t\t\t\t\tjobDetailsInfo.getJobVerticesPerState().get(ExecutionState.RUNNING) ==\n+\t\t\t\t\t\tjobDetailsInfo.getJobVertexInfos().size();\n+\t\t\t}, deadline, 500);\n+\t\t}\n+\t}\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDc3NzA2OA=="}, "originalCommit": {"oid": "755d9eb83e7fa1f497fe083850ce369d8d1d5940"}, "originalPosition": 54}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 125, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}