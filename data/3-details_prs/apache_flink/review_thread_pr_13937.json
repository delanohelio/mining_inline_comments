{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTE1ODkxMzcw", "number": 13937, "reviewThreads": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQxMDo1OTowMlrOE2Clyg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQxMTo0Mzo1MFrOE2DfOQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI1MTAxMDAyOnYy", "diffSide": "RIGHT", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveCompactReader.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQxMDo1OTowMlrOHupgwA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQxMDo1OTowMlrOHupgwA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODY3NjY3Mg==", "bodyText": "Why do we need a CompactBulkReader? It seems all we need is just a CompactReader.Factory?", "url": "https://github.com/apache/flink/pull/13937#discussion_r518676672", "createdAt": "2020-11-06T10:59:02Z", "author": {"login": "lirui-apache"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveCompactReader.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive.read;\n+\n+import org.apache.flink.connector.file.src.reader.BulkFormat;\n+import org.apache.flink.connectors.hive.HiveTablePartition;\n+import org.apache.flink.connectors.hive.JobConfWrapper;\n+import org.apache.flink.core.fs.FileSystem;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.catalog.hive.client.HiveShim;\n+import org.apache.flink.table.catalog.hive.client.HiveShimLoader;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.filesystem.stream.compact.CompactBulkReader;\n+import org.apache.flink.table.filesystem.stream.compact.CompactContext;\n+import org.apache.flink.table.filesystem.stream.compact.CompactReader;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.mapred.JobConf;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+import static org.apache.flink.connectors.hive.util.HivePartitionUtils.restorePartitionValueFromType;\n+import static org.apache.flink.table.utils.PartitionPathUtils.extractPartitionSpecFromPath;\n+\n+/**\n+ * The {@link CompactReader} to delegate hive bulk format.\n+ */\n+public class HiveCompactReader extends CompactBulkReader<RowData> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0d6b4e7fbd9948ee6c4db339bc49b93a823c3b88"}, "originalPosition": 52}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI1MTA0MjUxOnYy", "diffSide": "RIGHT", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveCompactReader.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQxMTowNzowMlrOHup0iA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wN1QwNDoxOTo0NlrOHvDMKA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODY4MTczNg==", "bodyText": "Is it possible that partition spec cannot be extracted from path? E.g. when we write to a static partition and the partition location is different from the default one?", "url": "https://github.com/apache/flink/pull/13937#discussion_r518681736", "createdAt": "2020-11-06T11:07:02Z", "author": {"login": "lirui-apache"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveCompactReader.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive.read;\n+\n+import org.apache.flink.connector.file.src.reader.BulkFormat;\n+import org.apache.flink.connectors.hive.HiveTablePartition;\n+import org.apache.flink.connectors.hive.JobConfWrapper;\n+import org.apache.flink.core.fs.FileSystem;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.catalog.hive.client.HiveShim;\n+import org.apache.flink.table.catalog.hive.client.HiveShimLoader;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.filesystem.stream.compact.CompactBulkReader;\n+import org.apache.flink.table.filesystem.stream.compact.CompactContext;\n+import org.apache.flink.table.filesystem.stream.compact.CompactReader;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.mapred.JobConf;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+import static org.apache.flink.connectors.hive.util.HivePartitionUtils.restorePartitionValueFromType;\n+import static org.apache.flink.table.utils.PartitionPathUtils.extractPartitionSpecFromPath;\n+\n+/**\n+ * The {@link CompactReader} to delegate hive bulk format.\n+ */\n+public class HiveCompactReader extends CompactBulkReader<RowData> {\n+\n+\tprivate HiveCompactReader(BulkFormat.Reader<RowData> reader) throws IOException {\n+\t\tsuper(reader);\n+\t}\n+\n+\tpublic static CompactReader.Factory<RowData> factory(\n+\t\t\tStorageDescriptor sd,\n+\t\t\tProperties properties,\n+\t\t\tJobConf jobConf,\n+\t\t\tCatalogTable catalogTable,\n+\t\t\tString hiveVersion,\n+\t\t\tRowType producedRowType,\n+\t\t\tboolean useMapRedReader) {\n+\t\treturn new Factory(\n+\t\t\t\tsd,\n+\t\t\t\tproperties,\n+\t\t\t\tnew JobConfWrapper(jobConf),\n+\t\t\t\tcatalogTable.getPartitionKeys(),\n+\t\t\t\tcatalogTable.getSchema().getFieldNames(),\n+\t\t\t\tcatalogTable.getSchema().getFieldDataTypes(),\n+\t\t\t\thiveVersion,\n+\t\t\t\tproducedRowType,\n+\t\t\t\tuseMapRedReader);\n+\t}\n+\n+\t/**\n+\t * Factory to create {@link HiveCompactReader}.\n+\t */\n+\tprivate static class Factory implements CompactReader.Factory<RowData> {\n+\n+\t\tprivate static final long serialVersionUID = 1L;\n+\n+\t\tprivate final StorageDescriptor sd;\n+\t\tprivate final Properties properties;\n+\t\tprivate final JobConfWrapper jobConfWrapper;\n+\t\tprivate final List<String> partitionKeys;\n+\t\tprivate final String[] fieldNames;\n+\t\tprivate final DataType[] fieldTypes;\n+\t\tprivate final String hiveVersion;\n+\t\tprivate final HiveShim shim;\n+\t\tprivate final RowType producedRowType;\n+\t\tprivate final boolean useMapRedReader;\n+\n+\t\tprivate Factory(\n+\t\t\t\tStorageDescriptor sd,\n+\t\t\t\tProperties properties,\n+\t\t\t\tJobConfWrapper jobConfWrapper,\n+\t\t\t\tList<String> partitionKeys,\n+\t\t\t\tString[] fieldNames,\n+\t\t\t\tDataType[] fieldTypes,\n+\t\t\t\tString hiveVersion,\n+\t\t\t\tRowType producedRowType,\n+\t\t\t\tboolean useMapRedReader) {\n+\t\t\tthis.sd = sd;\n+\t\t\tthis.properties = properties;\n+\t\t\tthis.jobConfWrapper = jobConfWrapper;\n+\t\t\tthis.partitionKeys = partitionKeys;\n+\t\t\tthis.fieldNames = fieldNames;\n+\t\t\tthis.fieldTypes = fieldTypes;\n+\t\t\tthis.hiveVersion = hiveVersion;\n+\t\t\tthis.shim = HiveShimLoader.loadHiveShim(hiveVersion);\n+\t\t\tthis.producedRowType = producedRowType;\n+\t\t\tthis.useMapRedReader = useMapRedReader;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic CompactReader<RowData> create(CompactContext context) throws IOException {\n+\t\t\tHiveSourceSplit split = createSplit(context.getPath(), context.getFileSystem());\n+\t\t\tHiveBulkFormatAdapter format = new HiveBulkFormatAdapter(\n+\t\t\t\t\tjobConfWrapper, partitionKeys, fieldNames, fieldTypes, hiveVersion, producedRowType, useMapRedReader);\n+\t\t\tBulkFormat.Reader<RowData> reader = format.createReader(context.getConfig(), split);\n+\t\t\treturn new HiveCompactReader(reader);\n+\t\t}\n+\n+\t\tprivate HiveSourceSplit createSplit(Path path, FileSystem fs) throws IOException {\n+\t\t\tlong len = fs.getFileStatus(path).getLen();\n+\t\t\treturn new HiveSourceSplit(\"id\", path, 0, len, new String[0], null, createPartition(path));\n+\t\t}\n+\n+\t\tprivate HiveTablePartition createPartition(Path path) {\n+\t\t\tMap<String, Object> partitionSpec = new LinkedHashMap<>();\n+\t\t\tMap<String, DataType> nameToTypes = new HashMap<>();\n+\t\t\tfor (int i = 0; i < fieldNames.length; i++) {\n+\t\t\t\tnameToTypes.put(fieldNames[i], fieldTypes[i]);\n+\t\t\t}\n+\t\t\tfor (Map.Entry<String, String> entry : extractPartitionSpecFromPath(path).entrySet()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0d6b4e7fbd9948ee6c4db339bc49b93a823c3b88"}, "originalPosition": 138}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODc5NDMzNA==", "bodyText": "We don't support this, now the path must be come from table.getSd(), and the rule must be partitioned path.", "url": "https://github.com/apache/flink/pull/13937#discussion_r518794334", "createdAt": "2020-11-06T14:42:03Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveCompactReader.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive.read;\n+\n+import org.apache.flink.connector.file.src.reader.BulkFormat;\n+import org.apache.flink.connectors.hive.HiveTablePartition;\n+import org.apache.flink.connectors.hive.JobConfWrapper;\n+import org.apache.flink.core.fs.FileSystem;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.catalog.hive.client.HiveShim;\n+import org.apache.flink.table.catalog.hive.client.HiveShimLoader;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.filesystem.stream.compact.CompactBulkReader;\n+import org.apache.flink.table.filesystem.stream.compact.CompactContext;\n+import org.apache.flink.table.filesystem.stream.compact.CompactReader;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.mapred.JobConf;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+import static org.apache.flink.connectors.hive.util.HivePartitionUtils.restorePartitionValueFromType;\n+import static org.apache.flink.table.utils.PartitionPathUtils.extractPartitionSpecFromPath;\n+\n+/**\n+ * The {@link CompactReader} to delegate hive bulk format.\n+ */\n+public class HiveCompactReader extends CompactBulkReader<RowData> {\n+\n+\tprivate HiveCompactReader(BulkFormat.Reader<RowData> reader) throws IOException {\n+\t\tsuper(reader);\n+\t}\n+\n+\tpublic static CompactReader.Factory<RowData> factory(\n+\t\t\tStorageDescriptor sd,\n+\t\t\tProperties properties,\n+\t\t\tJobConf jobConf,\n+\t\t\tCatalogTable catalogTable,\n+\t\t\tString hiveVersion,\n+\t\t\tRowType producedRowType,\n+\t\t\tboolean useMapRedReader) {\n+\t\treturn new Factory(\n+\t\t\t\tsd,\n+\t\t\t\tproperties,\n+\t\t\t\tnew JobConfWrapper(jobConf),\n+\t\t\t\tcatalogTable.getPartitionKeys(),\n+\t\t\t\tcatalogTable.getSchema().getFieldNames(),\n+\t\t\t\tcatalogTable.getSchema().getFieldDataTypes(),\n+\t\t\t\thiveVersion,\n+\t\t\t\tproducedRowType,\n+\t\t\t\tuseMapRedReader);\n+\t}\n+\n+\t/**\n+\t * Factory to create {@link HiveCompactReader}.\n+\t */\n+\tprivate static class Factory implements CompactReader.Factory<RowData> {\n+\n+\t\tprivate static final long serialVersionUID = 1L;\n+\n+\t\tprivate final StorageDescriptor sd;\n+\t\tprivate final Properties properties;\n+\t\tprivate final JobConfWrapper jobConfWrapper;\n+\t\tprivate final List<String> partitionKeys;\n+\t\tprivate final String[] fieldNames;\n+\t\tprivate final DataType[] fieldTypes;\n+\t\tprivate final String hiveVersion;\n+\t\tprivate final HiveShim shim;\n+\t\tprivate final RowType producedRowType;\n+\t\tprivate final boolean useMapRedReader;\n+\n+\t\tprivate Factory(\n+\t\t\t\tStorageDescriptor sd,\n+\t\t\t\tProperties properties,\n+\t\t\t\tJobConfWrapper jobConfWrapper,\n+\t\t\t\tList<String> partitionKeys,\n+\t\t\t\tString[] fieldNames,\n+\t\t\t\tDataType[] fieldTypes,\n+\t\t\t\tString hiveVersion,\n+\t\t\t\tRowType producedRowType,\n+\t\t\t\tboolean useMapRedReader) {\n+\t\t\tthis.sd = sd;\n+\t\t\tthis.properties = properties;\n+\t\t\tthis.jobConfWrapper = jobConfWrapper;\n+\t\t\tthis.partitionKeys = partitionKeys;\n+\t\t\tthis.fieldNames = fieldNames;\n+\t\t\tthis.fieldTypes = fieldTypes;\n+\t\t\tthis.hiveVersion = hiveVersion;\n+\t\t\tthis.shim = HiveShimLoader.loadHiveShim(hiveVersion);\n+\t\t\tthis.producedRowType = producedRowType;\n+\t\t\tthis.useMapRedReader = useMapRedReader;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic CompactReader<RowData> create(CompactContext context) throws IOException {\n+\t\t\tHiveSourceSplit split = createSplit(context.getPath(), context.getFileSystem());\n+\t\t\tHiveBulkFormatAdapter format = new HiveBulkFormatAdapter(\n+\t\t\t\t\tjobConfWrapper, partitionKeys, fieldNames, fieldTypes, hiveVersion, producedRowType, useMapRedReader);\n+\t\t\tBulkFormat.Reader<RowData> reader = format.createReader(context.getConfig(), split);\n+\t\t\treturn new HiveCompactReader(reader);\n+\t\t}\n+\n+\t\tprivate HiveSourceSplit createSplit(Path path, FileSystem fs) throws IOException {\n+\t\t\tlong len = fs.getFileStatus(path).getLen();\n+\t\t\treturn new HiveSourceSplit(\"id\", path, 0, len, new String[0], null, createPartition(path));\n+\t\t}\n+\n+\t\tprivate HiveTablePartition createPartition(Path path) {\n+\t\t\tMap<String, Object> partitionSpec = new LinkedHashMap<>();\n+\t\t\tMap<String, DataType> nameToTypes = new HashMap<>();\n+\t\t\tfor (int i = 0; i < fieldNames.length; i++) {\n+\t\t\t\tnameToTypes.put(fieldNames[i], fieldTypes[i]);\n+\t\t\t}\n+\t\t\tfor (Map.Entry<String, String> entry : extractPartitionSpecFromPath(path).entrySet()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODY4MTczNg=="}, "originalCommit": {"oid": "0d6b4e7fbd9948ee6c4db339bc49b93a823c3b88"}, "originalPosition": 138}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTA4Njc3NQ==", "bodyText": "Let's then log a jira to track this. Although not popular, it's still a valid use case in hive.", "url": "https://github.com/apache/flink/pull/13937#discussion_r519086775", "createdAt": "2020-11-07T03:00:10Z", "author": {"login": "lirui-apache"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveCompactReader.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive.read;\n+\n+import org.apache.flink.connector.file.src.reader.BulkFormat;\n+import org.apache.flink.connectors.hive.HiveTablePartition;\n+import org.apache.flink.connectors.hive.JobConfWrapper;\n+import org.apache.flink.core.fs.FileSystem;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.catalog.hive.client.HiveShim;\n+import org.apache.flink.table.catalog.hive.client.HiveShimLoader;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.filesystem.stream.compact.CompactBulkReader;\n+import org.apache.flink.table.filesystem.stream.compact.CompactContext;\n+import org.apache.flink.table.filesystem.stream.compact.CompactReader;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.mapred.JobConf;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+import static org.apache.flink.connectors.hive.util.HivePartitionUtils.restorePartitionValueFromType;\n+import static org.apache.flink.table.utils.PartitionPathUtils.extractPartitionSpecFromPath;\n+\n+/**\n+ * The {@link CompactReader} to delegate hive bulk format.\n+ */\n+public class HiveCompactReader extends CompactBulkReader<RowData> {\n+\n+\tprivate HiveCompactReader(BulkFormat.Reader<RowData> reader) throws IOException {\n+\t\tsuper(reader);\n+\t}\n+\n+\tpublic static CompactReader.Factory<RowData> factory(\n+\t\t\tStorageDescriptor sd,\n+\t\t\tProperties properties,\n+\t\t\tJobConf jobConf,\n+\t\t\tCatalogTable catalogTable,\n+\t\t\tString hiveVersion,\n+\t\t\tRowType producedRowType,\n+\t\t\tboolean useMapRedReader) {\n+\t\treturn new Factory(\n+\t\t\t\tsd,\n+\t\t\t\tproperties,\n+\t\t\t\tnew JobConfWrapper(jobConf),\n+\t\t\t\tcatalogTable.getPartitionKeys(),\n+\t\t\t\tcatalogTable.getSchema().getFieldNames(),\n+\t\t\t\tcatalogTable.getSchema().getFieldDataTypes(),\n+\t\t\t\thiveVersion,\n+\t\t\t\tproducedRowType,\n+\t\t\t\tuseMapRedReader);\n+\t}\n+\n+\t/**\n+\t * Factory to create {@link HiveCompactReader}.\n+\t */\n+\tprivate static class Factory implements CompactReader.Factory<RowData> {\n+\n+\t\tprivate static final long serialVersionUID = 1L;\n+\n+\t\tprivate final StorageDescriptor sd;\n+\t\tprivate final Properties properties;\n+\t\tprivate final JobConfWrapper jobConfWrapper;\n+\t\tprivate final List<String> partitionKeys;\n+\t\tprivate final String[] fieldNames;\n+\t\tprivate final DataType[] fieldTypes;\n+\t\tprivate final String hiveVersion;\n+\t\tprivate final HiveShim shim;\n+\t\tprivate final RowType producedRowType;\n+\t\tprivate final boolean useMapRedReader;\n+\n+\t\tprivate Factory(\n+\t\t\t\tStorageDescriptor sd,\n+\t\t\t\tProperties properties,\n+\t\t\t\tJobConfWrapper jobConfWrapper,\n+\t\t\t\tList<String> partitionKeys,\n+\t\t\t\tString[] fieldNames,\n+\t\t\t\tDataType[] fieldTypes,\n+\t\t\t\tString hiveVersion,\n+\t\t\t\tRowType producedRowType,\n+\t\t\t\tboolean useMapRedReader) {\n+\t\t\tthis.sd = sd;\n+\t\t\tthis.properties = properties;\n+\t\t\tthis.jobConfWrapper = jobConfWrapper;\n+\t\t\tthis.partitionKeys = partitionKeys;\n+\t\t\tthis.fieldNames = fieldNames;\n+\t\t\tthis.fieldTypes = fieldTypes;\n+\t\t\tthis.hiveVersion = hiveVersion;\n+\t\t\tthis.shim = HiveShimLoader.loadHiveShim(hiveVersion);\n+\t\t\tthis.producedRowType = producedRowType;\n+\t\t\tthis.useMapRedReader = useMapRedReader;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic CompactReader<RowData> create(CompactContext context) throws IOException {\n+\t\t\tHiveSourceSplit split = createSplit(context.getPath(), context.getFileSystem());\n+\t\t\tHiveBulkFormatAdapter format = new HiveBulkFormatAdapter(\n+\t\t\t\t\tjobConfWrapper, partitionKeys, fieldNames, fieldTypes, hiveVersion, producedRowType, useMapRedReader);\n+\t\t\tBulkFormat.Reader<RowData> reader = format.createReader(context.getConfig(), split);\n+\t\t\treturn new HiveCompactReader(reader);\n+\t\t}\n+\n+\t\tprivate HiveSourceSplit createSplit(Path path, FileSystem fs) throws IOException {\n+\t\t\tlong len = fs.getFileStatus(path).getLen();\n+\t\t\treturn new HiveSourceSplit(\"id\", path, 0, len, new String[0], null, createPartition(path));\n+\t\t}\n+\n+\t\tprivate HiveTablePartition createPartition(Path path) {\n+\t\t\tMap<String, Object> partitionSpec = new LinkedHashMap<>();\n+\t\t\tMap<String, DataType> nameToTypes = new HashMap<>();\n+\t\t\tfor (int i = 0; i < fieldNames.length; i++) {\n+\t\t\t\tnameToTypes.put(fieldNames[i], fieldTypes[i]);\n+\t\t\t}\n+\t\t\tfor (Map.Entry<String, String> entry : extractPartitionSpecFromPath(path).entrySet()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODY4MTczNg=="}, "originalCommit": {"oid": "0d6b4e7fbd9948ee6c4db339bc49b93a823c3b88"}, "originalPosition": 138}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTA5NzM4NA==", "bodyText": "https://issues.apache.org/jira/browse/FLINK-20040", "url": "https://github.com/apache/flink/pull/13937#discussion_r519097384", "createdAt": "2020-11-07T04:19:46Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveCompactReader.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive.read;\n+\n+import org.apache.flink.connector.file.src.reader.BulkFormat;\n+import org.apache.flink.connectors.hive.HiveTablePartition;\n+import org.apache.flink.connectors.hive.JobConfWrapper;\n+import org.apache.flink.core.fs.FileSystem;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.catalog.hive.client.HiveShim;\n+import org.apache.flink.table.catalog.hive.client.HiveShimLoader;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.filesystem.stream.compact.CompactBulkReader;\n+import org.apache.flink.table.filesystem.stream.compact.CompactContext;\n+import org.apache.flink.table.filesystem.stream.compact.CompactReader;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.mapred.JobConf;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+import static org.apache.flink.connectors.hive.util.HivePartitionUtils.restorePartitionValueFromType;\n+import static org.apache.flink.table.utils.PartitionPathUtils.extractPartitionSpecFromPath;\n+\n+/**\n+ * The {@link CompactReader} to delegate hive bulk format.\n+ */\n+public class HiveCompactReader extends CompactBulkReader<RowData> {\n+\n+\tprivate HiveCompactReader(BulkFormat.Reader<RowData> reader) throws IOException {\n+\t\tsuper(reader);\n+\t}\n+\n+\tpublic static CompactReader.Factory<RowData> factory(\n+\t\t\tStorageDescriptor sd,\n+\t\t\tProperties properties,\n+\t\t\tJobConf jobConf,\n+\t\t\tCatalogTable catalogTable,\n+\t\t\tString hiveVersion,\n+\t\t\tRowType producedRowType,\n+\t\t\tboolean useMapRedReader) {\n+\t\treturn new Factory(\n+\t\t\t\tsd,\n+\t\t\t\tproperties,\n+\t\t\t\tnew JobConfWrapper(jobConf),\n+\t\t\t\tcatalogTable.getPartitionKeys(),\n+\t\t\t\tcatalogTable.getSchema().getFieldNames(),\n+\t\t\t\tcatalogTable.getSchema().getFieldDataTypes(),\n+\t\t\t\thiveVersion,\n+\t\t\t\tproducedRowType,\n+\t\t\t\tuseMapRedReader);\n+\t}\n+\n+\t/**\n+\t * Factory to create {@link HiveCompactReader}.\n+\t */\n+\tprivate static class Factory implements CompactReader.Factory<RowData> {\n+\n+\t\tprivate static final long serialVersionUID = 1L;\n+\n+\t\tprivate final StorageDescriptor sd;\n+\t\tprivate final Properties properties;\n+\t\tprivate final JobConfWrapper jobConfWrapper;\n+\t\tprivate final List<String> partitionKeys;\n+\t\tprivate final String[] fieldNames;\n+\t\tprivate final DataType[] fieldTypes;\n+\t\tprivate final String hiveVersion;\n+\t\tprivate final HiveShim shim;\n+\t\tprivate final RowType producedRowType;\n+\t\tprivate final boolean useMapRedReader;\n+\n+\t\tprivate Factory(\n+\t\t\t\tStorageDescriptor sd,\n+\t\t\t\tProperties properties,\n+\t\t\t\tJobConfWrapper jobConfWrapper,\n+\t\t\t\tList<String> partitionKeys,\n+\t\t\t\tString[] fieldNames,\n+\t\t\t\tDataType[] fieldTypes,\n+\t\t\t\tString hiveVersion,\n+\t\t\t\tRowType producedRowType,\n+\t\t\t\tboolean useMapRedReader) {\n+\t\t\tthis.sd = sd;\n+\t\t\tthis.properties = properties;\n+\t\t\tthis.jobConfWrapper = jobConfWrapper;\n+\t\t\tthis.partitionKeys = partitionKeys;\n+\t\t\tthis.fieldNames = fieldNames;\n+\t\t\tthis.fieldTypes = fieldTypes;\n+\t\t\tthis.hiveVersion = hiveVersion;\n+\t\t\tthis.shim = HiveShimLoader.loadHiveShim(hiveVersion);\n+\t\t\tthis.producedRowType = producedRowType;\n+\t\t\tthis.useMapRedReader = useMapRedReader;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic CompactReader<RowData> create(CompactContext context) throws IOException {\n+\t\t\tHiveSourceSplit split = createSplit(context.getPath(), context.getFileSystem());\n+\t\t\tHiveBulkFormatAdapter format = new HiveBulkFormatAdapter(\n+\t\t\t\t\tjobConfWrapper, partitionKeys, fieldNames, fieldTypes, hiveVersion, producedRowType, useMapRedReader);\n+\t\t\tBulkFormat.Reader<RowData> reader = format.createReader(context.getConfig(), split);\n+\t\t\treturn new HiveCompactReader(reader);\n+\t\t}\n+\n+\t\tprivate HiveSourceSplit createSplit(Path path, FileSystem fs) throws IOException {\n+\t\t\tlong len = fs.getFileStatus(path).getLen();\n+\t\t\treturn new HiveSourceSplit(\"id\", path, 0, len, new String[0], null, createPartition(path));\n+\t\t}\n+\n+\t\tprivate HiveTablePartition createPartition(Path path) {\n+\t\t\tMap<String, Object> partitionSpec = new LinkedHashMap<>();\n+\t\t\tMap<String, DataType> nameToTypes = new HashMap<>();\n+\t\t\tfor (int i = 0; i < fieldNames.length; i++) {\n+\t\t\t\tnameToTypes.put(fieldNames[i], fieldTypes[i]);\n+\t\t\t}\n+\t\t\tfor (Map.Entry<String, String> entry : extractPartitionSpecFromPath(path).entrySet()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODY4MTczNg=="}, "originalCommit": {"oid": "0d6b4e7fbd9948ee6c4db339bc49b93a823c3b88"}, "originalPosition": 138}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI1MTE1NzA1OnYy", "diffSide": "RIGHT", "path": "flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveSinkCompactionITCase.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQxMTo0Mzo1MFrOHuq5kA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQxNDo0MzowNlrOHuwu_w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODY5OTQwOA==", "bodyText": "I think only HiveTableSource performs parallelism inference. But this test won't use HiveTableSource, right?", "url": "https://github.com/apache/flink/pull/13937#discussion_r518699408", "createdAt": "2020-11-06T11:43:50Z", "author": {"login": "lirui-apache"}, "path": "flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveSinkCompactionITCase.java", "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+import org.apache.flink.table.api.SqlDialect;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.catalog.exceptions.TableNotExistException;\n+import org.apache.flink.table.catalog.hive.HiveCatalog;\n+import org.apache.flink.table.catalog.hive.HiveTestUtils;\n+import org.apache.flink.table.planner.runtime.stream.sql.CompactionITCaseBase;\n+\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Collection;\n+\n+/**\n+ * Test sink file compaction of hive tables.\n+ */\n+@RunWith(Parameterized.class)\n+public class HiveSinkCompactionITCase extends CompactionITCaseBase {\n+\n+\t@Parameterized.Parameters(name = \"format = {0}\")\n+\tpublic static Collection<String> parameters() {\n+\t\treturn Arrays.asList(\"sequencefile\", \"parquet\");\n+\t}\n+\n+\t@Parameterized.Parameter\n+\tpublic String format;\n+\n+\tprivate HiveCatalog hiveCatalog;\n+\n+\t@Override\n+\t@Before\n+\tpublic void init() throws IOException {\n+\t\thiveCatalog = HiveTestUtils.createHiveCatalog();\n+\t\ttEnv().registerCatalog(hiveCatalog.getName(), hiveCatalog);\n+\t\ttEnv().useCatalog(hiveCatalog.getName());\n+\n+\t\t// avoid too large parallelism lead to scheduler dead lock in streaming mode\n+\t\ttEnv().getConfig().getConfiguration().set(\n+\t\t\t\tHiveOptions.TABLE_EXEC_HIVE_INFER_SOURCE_PARALLELISM, false);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0d6b4e7fbd9948ee6c4db339bc49b93a823c3b88"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODc5NTAwNw==", "bodyText": "We are using. There is select * from sink_table", "url": "https://github.com/apache/flink/pull/13937#discussion_r518795007", "createdAt": "2020-11-06T14:43:06Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveSinkCompactionITCase.java", "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+import org.apache.flink.table.api.SqlDialect;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.catalog.exceptions.TableNotExistException;\n+import org.apache.flink.table.catalog.hive.HiveCatalog;\n+import org.apache.flink.table.catalog.hive.HiveTestUtils;\n+import org.apache.flink.table.planner.runtime.stream.sql.CompactionITCaseBase;\n+\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Collection;\n+\n+/**\n+ * Test sink file compaction of hive tables.\n+ */\n+@RunWith(Parameterized.class)\n+public class HiveSinkCompactionITCase extends CompactionITCaseBase {\n+\n+\t@Parameterized.Parameters(name = \"format = {0}\")\n+\tpublic static Collection<String> parameters() {\n+\t\treturn Arrays.asList(\"sequencefile\", \"parquet\");\n+\t}\n+\n+\t@Parameterized.Parameter\n+\tpublic String format;\n+\n+\tprivate HiveCatalog hiveCatalog;\n+\n+\t@Override\n+\t@Before\n+\tpublic void init() throws IOException {\n+\t\thiveCatalog = HiveTestUtils.createHiveCatalog();\n+\t\ttEnv().registerCatalog(hiveCatalog.getName(), hiveCatalog);\n+\t\ttEnv().useCatalog(hiveCatalog.getName());\n+\n+\t\t// avoid too large parallelism lead to scheduler dead lock in streaming mode\n+\t\ttEnv().getConfig().getConfiguration().set(\n+\t\t\t\tHiveOptions.TABLE_EXEC_HIVE_INFER_SOURCE_PARALLELISM, false);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODY5OTQwOA=="}, "originalCommit": {"oid": "0d6b4e7fbd9948ee6c4db339bc49b93a823c3b88"}, "originalPosition": 62}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4941, "cost": 1, "resetAt": "2021-11-12T11:57:46Z"}}}