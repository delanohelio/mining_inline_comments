{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTAyMjM5MzIy", "number": 13614, "reviewThreads": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQxMTozNzozM1rOEtcPpg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNlQxMDo1MDowN1rOEubwdQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE2MDg0MTM0OnYy", "diffSide": "RIGHT", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/buffer/BufferConsumer.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQxMTozNzozM1rOHhOwZg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQxMzoyMDoyOFrOHhSqOQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDYwNjgyMg==", "bodyText": "How could it be that bytesToSkip > buffer.getMaxCapacity() if we already asserted bytesToSkip <= bytesReadable? If that's impossible, we could replace this if with just\nslice = buffer.readOnlySlice(currentReaderPosition + bytesToSkip, bytesReadable - bytesToSkip);\n\nas that would also return an empty buffer if bytesToSkip == bytesReadable?", "url": "https://github.com/apache/flink/pull/13614#discussion_r504606822", "createdAt": "2020-10-14T11:37:33Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/buffer/BufferConsumer.java", "diffHunk": "@@ -108,6 +109,30 @@ public Buffer build() {\n \t\treturn slice.retainBuffer();\n \t}\n \n+\t/**\n+\t * @param bytesToSkip number of bytes to skip from currentReaderPosition\n+\t * @return sliced {@link Buffer} containing the not yet consumed data, Returned {@link Buffer} shares the reference\n+\t * counter with the parent {@link BufferConsumer} - in order to recycle memory both of them must be recycled/closed.\n+\t */\n+\tBuffer skipBuild(int bytesToSkip) {\n+\t\twriterPosition.update();\n+\t\tint cachedWriterPosition = writerPosition.getCached();\n+\t\tBuffer slice;\n+\n+\t\tint bytesReadable = cachedWriterPosition - currentReaderPosition;\n+\t\tcheckState(bytesToSkip <= bytesReadable, \"bytes to skip beyond readable range\");\n+\n+\t\tif (bytesToSkip < buffer.getMaxCapacity()) {\n+\t\t\tslice = buffer.readOnlySlice(currentReaderPosition + bytesToSkip, bytesReadable - bytesToSkip);\n+\t\t} else {\n+\t\t\t// return an empty buffer if beyond buffer max capacity\n+\t\t\tslice = buffer.readOnlySlice(currentReaderPosition, 0);\n+\t\t}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "47d3d60c3dc74ab402ce657c31c27568b190de40"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDY3MDc3Nw==", "bodyText": "You are right, bytesToSkip > buffer.getMaxCapacity() is not possible.\nHowever, the problem is if (bytesToSkip == buffer.getMaxCapacity()), which is the case when a long record occupy the entire buffer, currentReaderPosition + bytesToSkip euqals to buffer.getMaxCapacity(), which means I would read starting from buffer.getMaxCapacity(). I guess I would get an out-of-range exception if I do that LOL", "url": "https://github.com/apache/flink/pull/13614#discussion_r504670777", "createdAt": "2020-10-14T13:20:28Z", "author": {"login": "curcur"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/buffer/BufferConsumer.java", "diffHunk": "@@ -108,6 +109,30 @@ public Buffer build() {\n \t\treturn slice.retainBuffer();\n \t}\n \n+\t/**\n+\t * @param bytesToSkip number of bytes to skip from currentReaderPosition\n+\t * @return sliced {@link Buffer} containing the not yet consumed data, Returned {@link Buffer} shares the reference\n+\t * counter with the parent {@link BufferConsumer} - in order to recycle memory both of them must be recycled/closed.\n+\t */\n+\tBuffer skipBuild(int bytesToSkip) {\n+\t\twriterPosition.update();\n+\t\tint cachedWriterPosition = writerPosition.getCached();\n+\t\tBuffer slice;\n+\n+\t\tint bytesReadable = cachedWriterPosition - currentReaderPosition;\n+\t\tcheckState(bytesToSkip <= bytesReadable, \"bytes to skip beyond readable range\");\n+\n+\t\tif (bytesToSkip < buffer.getMaxCapacity()) {\n+\t\t\tslice = buffer.readOnlySlice(currentReaderPosition + bytesToSkip, bytesReadable - bytesToSkip);\n+\t\t} else {\n+\t\t\t// return an empty buffer if beyond buffer max capacity\n+\t\t\tslice = buffer.readOnlySlice(currentReaderPosition, 0);\n+\t\t}", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDYwNjgyMg=="}, "originalCommit": {"oid": "47d3d60c3dc74ab402ce657c31c27568b190de40"}, "originalPosition": 30}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE2MDk0Mjg5OnYy", "diffSide": "RIGHT", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/buffer/BufferConsumer.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQxMjowNjowOFrOHhPtWQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQxMjowNjowOFrOHhPtWQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDYyMjQyNQ==", "bodyText": "nit: isStartOfDataBuffer()", "url": "https://github.com/apache/flink/pull/13614#discussion_r504622425", "createdAt": "2020-10-14T12:06:08Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/buffer/BufferConsumer.java", "diffHunk": "@@ -159,6 +184,14 @@ int getCurrentReaderPosition() {\n \t\treturn currentReaderPosition;\n \t}\n \n+\tboolean startOfDataBuffer() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "47d3d60c3dc74ab402ce657c31c27568b190de40"}, "originalPosition": 43}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE2MDk3OTk4OnYy", "diffSide": "RIGHT", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/buffer/BufferConsumerWithPartialRecordLength.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQxMjoxNjoxOVrOHhQDmA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNVQxMToyNDoyMVrOHiDG2g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDYyODEyMA==", "bodyText": "Do you really need to return the buffer here? I have a feeling that the could would be simpler if:\n\nthis method was returning just true/false if cleanup has finished or not.\nskipBuild(...) would be replaced with skip(...), that would just move the offset, without returning the buffer\n.build() call would be required afterwards, to get the remaining data", "url": "https://github.com/apache/flink/pull/13614#discussion_r504628120", "createdAt": "2020-10-14T12:16:19Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/buffer/BufferConsumerWithPartialRecordLength.java", "diffHunk": "@@ -0,0 +1,120 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.io.network.buffer;\n+\n+import javax.annotation.concurrent.NotThreadSafe;\n+\n+import static org.apache.flink.util.Preconditions.checkNotNull;\n+import static org.apache.flink.util.Preconditions.checkState;\n+\n+/**\n+ * BufferConsumer with partial record length if a record is spanning over buffers\n+ *\n+ * <p>`partialRecordLength` is the length of bytes to skip in order to start with a complete record,\n+ * from position index 0 of the underlying MemorySegment. `partialRecordLength` is used in approximate\n+ * local recovery to find the start position of a complete record on a BufferConsumer, so called\n+ * `partial record clean-up`.\n+ *\n+ * <p>Partial records happen if a record can not fit into one buffer, then the remaining part of the same record\n+ * is put into the next buffer. Hence partial records only exist at the beginning of a buffer.\n+ * Partial record clean-up is needed in the mode of approximate local recovery.\n+ * If a record is spanning over multiple buffers, and the first (several) buffers have got lost due to the failure\n+ * of the receiver task, the remaining data belonging to the same record in transition should be cleaned up.\n+ *\n+ * <p> If partialRecordLength == 0, the buffer starts with a complete record</p>\n+ * <p> If partialRecordLength > 0, the buffer starts with a partial record, its length = partialRecordLength</p>\n+ * <p> If partialRecordLength < 0, partialRecordLength is undefined. It is currently used in\n+ * \t\t\t\t\t\t\t\t\t{@cite ResultSubpartitionRecoveredStateHandler#recover}</p>\n+ */\n+@NotThreadSafe\n+public class BufferConsumerWithPartialRecordLength {\n+\tprivate final BufferConsumer bufferConsumer;\n+\tprivate final int partialRecordLength;\n+\n+\tpublic BufferConsumerWithPartialRecordLength(BufferConsumer bufferConsumer, int partialRecordLength) {\n+\t\tthis.bufferConsumer = checkNotNull(bufferConsumer);\n+\t\tthis.partialRecordLength = partialRecordLength;\n+\t}\n+\n+\tpublic BufferConsumer getBufferConsumer() {\n+\t\treturn bufferConsumer;\n+\t}\n+\n+\tpublic int getPartialRecordLength() {\n+\t\treturn partialRecordLength;\n+\t}\n+\n+\tpublic Buffer build() {\n+\t\treturn bufferConsumer.build();\n+\t}\n+\n+\tpublic PartialRecordCleanupResult cleanupPartialRecord() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "47d3d60c3dc74ab402ce657c31c27568b190de40"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDY3NTkxNg==", "bodyText": "skipBuild(...) would be replaced with skip(...), that would just move the offset, without returning the buffer\n.build() call would be required afterwards, to get the remaining data\n\n\nI was also thinking of that, and it would be perfect if we do not have the case bytesToSkip == buffer.getMaxCapacity().\nIn the current build(), currentReaderPosition can never be buffer.getMaxCapacity(), because the consumer has already been marked as finished currentReaderPosition = cachedWriterPosition if the full buffer is read. Remember in case of a record spanning over buffers, we finish the builder first and then request a new builder.\nBut I can still handle this special case in build() if you are fine to change the build() method. I was trying to not touch the build() method which is used in the normal PipelinedSubPartition read. It would be easier if later we really want to change to a different way to handle partial records.\nI am fine with either way.\n\nDo you really need to return the buffer here? I have a feeling that the could be simpler if:\n\nthis method was returning just true/false if cleanup has finished or not.\n\n\nYes, I need both whether the clean-up is successful + returned buffer, which would be called later in PipelinedApproximateSubpartition within pollBuffer method to\n\ndecide whether the clean up is successful\nHandle sliced Buffer if successful or not.\n\nThe reason I am returning an empty buffer instead of a null buffer in case of bytesToSkip == buffer.getMaxCapacity() is also to be easier to follow the current logic in pollBuffer what if we do not read data.\nOverall I want to wrap the clean-up logic within BufferConsumerWithPartialRecordLength, which I guess is one of the main reasons why we introduce this class?", "url": "https://github.com/apache/flink/pull/13614#discussion_r504675916", "createdAt": "2020-10-14T13:27:36Z", "author": {"login": "curcur"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/buffer/BufferConsumerWithPartialRecordLength.java", "diffHunk": "@@ -0,0 +1,120 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.io.network.buffer;\n+\n+import javax.annotation.concurrent.NotThreadSafe;\n+\n+import static org.apache.flink.util.Preconditions.checkNotNull;\n+import static org.apache.flink.util.Preconditions.checkState;\n+\n+/**\n+ * BufferConsumer with partial record length if a record is spanning over buffers\n+ *\n+ * <p>`partialRecordLength` is the length of bytes to skip in order to start with a complete record,\n+ * from position index 0 of the underlying MemorySegment. `partialRecordLength` is used in approximate\n+ * local recovery to find the start position of a complete record on a BufferConsumer, so called\n+ * `partial record clean-up`.\n+ *\n+ * <p>Partial records happen if a record can not fit into one buffer, then the remaining part of the same record\n+ * is put into the next buffer. Hence partial records only exist at the beginning of a buffer.\n+ * Partial record clean-up is needed in the mode of approximate local recovery.\n+ * If a record is spanning over multiple buffers, and the first (several) buffers have got lost due to the failure\n+ * of the receiver task, the remaining data belonging to the same record in transition should be cleaned up.\n+ *\n+ * <p> If partialRecordLength == 0, the buffer starts with a complete record</p>\n+ * <p> If partialRecordLength > 0, the buffer starts with a partial record, its length = partialRecordLength</p>\n+ * <p> If partialRecordLength < 0, partialRecordLength is undefined. It is currently used in\n+ * \t\t\t\t\t\t\t\t\t{@cite ResultSubpartitionRecoveredStateHandler#recover}</p>\n+ */\n+@NotThreadSafe\n+public class BufferConsumerWithPartialRecordLength {\n+\tprivate final BufferConsumer bufferConsumer;\n+\tprivate final int partialRecordLength;\n+\n+\tpublic BufferConsumerWithPartialRecordLength(BufferConsumer bufferConsumer, int partialRecordLength) {\n+\t\tthis.bufferConsumer = checkNotNull(bufferConsumer);\n+\t\tthis.partialRecordLength = partialRecordLength;\n+\t}\n+\n+\tpublic BufferConsumer getBufferConsumer() {\n+\t\treturn bufferConsumer;\n+\t}\n+\n+\tpublic int getPartialRecordLength() {\n+\t\treturn partialRecordLength;\n+\t}\n+\n+\tpublic Buffer build() {\n+\t\treturn bufferConsumer.build();\n+\t}\n+\n+\tpublic PartialRecordCleanupResult cleanupPartialRecord() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDYyODEyMA=="}, "originalCommit": {"oid": "47d3d60c3dc74ab402ce657c31c27568b190de40"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTQ2NDUzOA==", "bodyText": "In the current build(), currentReaderPosition can never be buffer.getMaxCapacity()\n\nis that really the case? I think the BufferConsumer should be able to handle building empty buffers, after writer has appended and committed the data, but hasn't yet called BufferBuilder#finish().  After all those are two separate calls happening in a different thread.", "url": "https://github.com/apache/flink/pull/13614#discussion_r505464538", "createdAt": "2020-10-15T11:24:21Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/buffer/BufferConsumerWithPartialRecordLength.java", "diffHunk": "@@ -0,0 +1,120 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.io.network.buffer;\n+\n+import javax.annotation.concurrent.NotThreadSafe;\n+\n+import static org.apache.flink.util.Preconditions.checkNotNull;\n+import static org.apache.flink.util.Preconditions.checkState;\n+\n+/**\n+ * BufferConsumer with partial record length if a record is spanning over buffers\n+ *\n+ * <p>`partialRecordLength` is the length of bytes to skip in order to start with a complete record,\n+ * from position index 0 of the underlying MemorySegment. `partialRecordLength` is used in approximate\n+ * local recovery to find the start position of a complete record on a BufferConsumer, so called\n+ * `partial record clean-up`.\n+ *\n+ * <p>Partial records happen if a record can not fit into one buffer, then the remaining part of the same record\n+ * is put into the next buffer. Hence partial records only exist at the beginning of a buffer.\n+ * Partial record clean-up is needed in the mode of approximate local recovery.\n+ * If a record is spanning over multiple buffers, and the first (several) buffers have got lost due to the failure\n+ * of the receiver task, the remaining data belonging to the same record in transition should be cleaned up.\n+ *\n+ * <p> If partialRecordLength == 0, the buffer starts with a complete record</p>\n+ * <p> If partialRecordLength > 0, the buffer starts with a partial record, its length = partialRecordLength</p>\n+ * <p> If partialRecordLength < 0, partialRecordLength is undefined. It is currently used in\n+ * \t\t\t\t\t\t\t\t\t{@cite ResultSubpartitionRecoveredStateHandler#recover}</p>\n+ */\n+@NotThreadSafe\n+public class BufferConsumerWithPartialRecordLength {\n+\tprivate final BufferConsumer bufferConsumer;\n+\tprivate final int partialRecordLength;\n+\n+\tpublic BufferConsumerWithPartialRecordLength(BufferConsumer bufferConsumer, int partialRecordLength) {\n+\t\tthis.bufferConsumer = checkNotNull(bufferConsumer);\n+\t\tthis.partialRecordLength = partialRecordLength;\n+\t}\n+\n+\tpublic BufferConsumer getBufferConsumer() {\n+\t\treturn bufferConsumer;\n+\t}\n+\n+\tpublic int getPartialRecordLength() {\n+\t\treturn partialRecordLength;\n+\t}\n+\n+\tpublic Buffer build() {\n+\t\treturn bufferConsumer.build();\n+\t}\n+\n+\tpublic PartialRecordCleanupResult cleanupPartialRecord() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDYyODEyMA=="}, "originalCommit": {"oid": "47d3d60c3dc74ab402ce657c31c27568b190de40"}, "originalPosition": 67}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE2MTA5MzI0OnYy", "diffSide": "RIGHT", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/BufferWritingResultPartition.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQxMjo0NDo0MlrOHhRIOw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNVQwMzo0OToxN1rOHhvx6Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDY0NTY5MQ==", "bodyText": "nit: those names are a bit lengthy, also it's a bit confusing that getNewEmptySubpartitionBufferBuilderForNewRecord is not appending the data, while getNewEmptySubpartitionBufferBuilderForRecordContinuation is.\nMaybe can you extract those lines above into a method:\nBufferBuilder buffer = appendDataForSubpartitionNewRecord(record, targetSubpartition);\n\nand for the broadcast version\nBufferBuilder buffer = appendDataForBroadcastNewRecord(record, targetSubpartition);\n\n...", "url": "https://github.com/apache/flink/pull/13614#discussion_r504645691", "createdAt": "2020-10-14T12:44:42Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/BufferWritingResultPartition.java", "diffHunk": "@@ -128,26 +128,40 @@ protected void flushAllSubpartitions(boolean finishProducers) {\n \n \t@Override\n \tpublic void emitRecord(ByteBuffer record, int targetSubpartition) throws IOException {\n-\t\tdo {\n-\t\t\tfinal BufferBuilder bufferBuilder = getSubpartitionBufferBuilder(targetSubpartition);\n-\t\t\tbufferBuilder.appendAndCommit(record);\n+\t\tBufferBuilder buffer = subpartitionBufferBuilders[targetSubpartition];\n \n-\t\t\tif (bufferBuilder.isFull()) {\n-\t\t\t\tfinishSubpartitionBufferBuilder(targetSubpartition);\n-\t\t\t}\n-\t\t} while (record.hasRemaining());\n+\t\tif (buffer == null) {\n+\t\t\tbuffer = getNewEmptySubpartitionBufferBuilderForNewRecord(targetSubpartition);\n+\t\t}\n+\t\tbuffer.appendAndCommit(record);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "47d3d60c3dc74ab402ce657c31c27568b190de40"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTE0Nzg4MQ==", "bodyText": "That's a good idea!", "url": "https://github.com/apache/flink/pull/13614#discussion_r505147881", "createdAt": "2020-10-15T03:49:17Z", "author": {"login": "curcur"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/BufferWritingResultPartition.java", "diffHunk": "@@ -128,26 +128,40 @@ protected void flushAllSubpartitions(boolean finishProducers) {\n \n \t@Override\n \tpublic void emitRecord(ByteBuffer record, int targetSubpartition) throws IOException {\n-\t\tdo {\n-\t\t\tfinal BufferBuilder bufferBuilder = getSubpartitionBufferBuilder(targetSubpartition);\n-\t\t\tbufferBuilder.appendAndCommit(record);\n+\t\tBufferBuilder buffer = subpartitionBufferBuilders[targetSubpartition];\n \n-\t\t\tif (bufferBuilder.isFull()) {\n-\t\t\t\tfinishSubpartitionBufferBuilder(targetSubpartition);\n-\t\t\t}\n-\t\t} while (record.hasRemaining());\n+\t\tif (buffer == null) {\n+\t\t\tbuffer = getNewEmptySubpartitionBufferBuilderForNewRecord(targetSubpartition);\n+\t\t}\n+\t\tbuffer.appendAndCommit(record);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDY0NTY5MQ=="}, "originalCommit": {"oid": "47d3d60c3dc74ab402ce657c31c27568b190de40"}, "originalPosition": 16}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE2MTEwMDg5OnYy", "diffSide": "RIGHT", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/BufferWritingResultPartition.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQxMjo0NjoyMVrOHhRMyQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNVQwNzozOToyOVrOHh3rQw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDY0Njg1Nw==", "bodyText": "...\nbuffer = appendDataForSubpartitionRecordContinuation(record, targetSubpartition);\n\nand for the broadcast version\nbuffer = appendDataForBroadcastRecordContinuation(record, targetSubpartition);\n\n?", "url": "https://github.com/apache/flink/pull/13614#discussion_r504646857", "createdAt": "2020-10-14T12:46:21Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/BufferWritingResultPartition.java", "diffHunk": "@@ -128,26 +128,40 @@ protected void flushAllSubpartitions(boolean finishProducers) {\n \n \t@Override\n \tpublic void emitRecord(ByteBuffer record, int targetSubpartition) throws IOException {\n-\t\tdo {\n-\t\t\tfinal BufferBuilder bufferBuilder = getSubpartitionBufferBuilder(targetSubpartition);\n-\t\t\tbufferBuilder.appendAndCommit(record);\n+\t\tBufferBuilder buffer = subpartitionBufferBuilders[targetSubpartition];\n \n-\t\t\tif (bufferBuilder.isFull()) {\n-\t\t\t\tfinishSubpartitionBufferBuilder(targetSubpartition);\n-\t\t\t}\n-\t\t} while (record.hasRemaining());\n+\t\tif (buffer == null) {\n+\t\t\tbuffer = getNewEmptySubpartitionBufferBuilderForNewRecord(targetSubpartition);\n+\t\t}\n+\t\tbuffer.appendAndCommit(record);\n+\n+\t\twhile (record.hasRemaining()) {\n+\t\t\tfinishSubpartitionBufferBuilder(targetSubpartition);\n+\t\t\tbuffer = getNewEmptySubpartitionBufferBuilderForRecordContinuation(record, targetSubpartition);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "47d3d60c3dc74ab402ce657c31c27568b190de40"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTI3NzI1MQ==", "bodyText": "I've rewritten the first commit.", "url": "https://github.com/apache/flink/pull/13614#discussion_r505277251", "createdAt": "2020-10-15T07:39:29Z", "author": {"login": "curcur"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/BufferWritingResultPartition.java", "diffHunk": "@@ -128,26 +128,40 @@ protected void flushAllSubpartitions(boolean finishProducers) {\n \n \t@Override\n \tpublic void emitRecord(ByteBuffer record, int targetSubpartition) throws IOException {\n-\t\tdo {\n-\t\t\tfinal BufferBuilder bufferBuilder = getSubpartitionBufferBuilder(targetSubpartition);\n-\t\t\tbufferBuilder.appendAndCommit(record);\n+\t\tBufferBuilder buffer = subpartitionBufferBuilders[targetSubpartition];\n \n-\t\t\tif (bufferBuilder.isFull()) {\n-\t\t\t\tfinishSubpartitionBufferBuilder(targetSubpartition);\n-\t\t\t}\n-\t\t} while (record.hasRemaining());\n+\t\tif (buffer == null) {\n+\t\t\tbuffer = getNewEmptySubpartitionBufferBuilderForNewRecord(targetSubpartition);\n+\t\t}\n+\t\tbuffer.appendAndCommit(record);\n+\n+\t\twhile (record.hasRemaining()) {\n+\t\t\tfinishSubpartitionBufferBuilder(targetSubpartition);\n+\t\t\tbuffer = getNewEmptySubpartitionBufferBuilderForRecordContinuation(record, targetSubpartition);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDY0Njg1Nw=="}, "originalCommit": {"oid": "47d3d60c3dc74ab402ce657c31c27568b190de40"}, "originalPosition": 20}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE2MTEyMzQwOnYy", "diffSide": "RIGHT", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/BufferWritingResultPartition.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQxMjo1MToyN1rOHhRaWQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNVQwNzozOTo0NlrOHh3s0Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDY1MDMyOQ==", "bodyText": "if you moved appending data into this method as I suggested above, you could deduplicate this code with:\nreturn getNewEmptyBroadcastBufferBuilderForRecordContinuation(record, 0);", "url": "https://github.com/apache/flink/pull/13614#discussion_r504650329", "createdAt": "2020-10-14T12:51:27Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/BufferWritingResultPartition.java", "diffHunk": "@@ -211,46 +225,62 @@ protected void releaseInternal() {\n \t\t}\n \t}\n \n-\tprivate BufferBuilder getSubpartitionBufferBuilder(int targetSubpartition) throws IOException {\n-\t\tfinal BufferBuilder bufferBuilder = subpartitionBufferBuilders[targetSubpartition];\n-\t\tif (bufferBuilder != null) {\n-\t\t\treturn bufferBuilder;\n-\t\t}\n+\tprivate BufferBuilder getNewEmptySubpartitionBufferBuilderForNewRecord(int targetSubpartition) throws IOException {\n+\t\tfinal BufferBuilder bufferBuilder = requestNewSubpartitionBufferBuilder(targetSubpartition);\n+\t\tsubpartitions[targetSubpartition].add(bufferBuilder.createBufferConsumerFromBeginning(), 0);\n \n-\t\treturn getNewSubpartitionBufferBuilder(targetSubpartition);\n+\t\treturn bufferBuilder;\n \t}\n \n-\tprivate BufferBuilder getNewSubpartitionBufferBuilder(int targetSubpartition) throws IOException {\n+\tprivate BufferBuilder getNewEmptySubpartitionBufferBuilderForRecordContinuation(\n+\t\t\tfinal ByteBuffer remainingRecordBytes,\n+\t\t\tfinal int targetSubpartition) throws IOException {\n+\t\tfinal BufferBuilder bufferBuilder = requestNewSubpartitionBufferBuilder(targetSubpartition);\n+\t\tfinal int partialRecordBytes = bufferBuilder.appendAndCommit(remainingRecordBytes);\n+\t\tsubpartitions[targetSubpartition].add(bufferBuilder.createBufferConsumerFromBeginning(), partialRecordBytes);\n+\n+\t\treturn bufferBuilder;\n+\t}\n+\n+\tprivate BufferBuilder requestNewSubpartitionBufferBuilder(int targetSubpartition) throws IOException {\n \t\tcheckInProduceState();\n \t\tensureUnicastMode();\n-\n \t\tfinal BufferBuilder bufferBuilder = requestNewBufferBuilderFromPool(targetSubpartition);\n-\t\tsubpartitions[targetSubpartition].add(bufferBuilder.createBufferConsumer());\n \t\tsubpartitionBufferBuilders[targetSubpartition] = bufferBuilder;\n+\n \t\treturn bufferBuilder;\n \t}\n \n-\tprivate BufferBuilder getBroadcastBufferBuilder() throws IOException {\n-\t\tif (broadcastBufferBuilder != null) {\n-\t\t\treturn broadcastBufferBuilder;\n+\tprivate BufferBuilder getNewEmptyBroadcastBufferBuilderForNewRecord() throws IOException {\n+\t\tfinal BufferBuilder bufferBuilder = requestNewBroadcastBufferBuilder();\n+\t\ttry (final BufferConsumer consumer = bufferBuilder.createBufferConsumerFromBeginning()) {\n+\t\t\tfor (ResultSubpartition subpartition : subpartitions) {\n+\t\t\t\tsubpartition.add(consumer.copy(), 0);\n+\t\t\t}\n \t\t}\n \n-\t\treturn getNewBroadcastBufferBuilder();\n+\t\treturn bufferBuilder;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "47d3d60c3dc74ab402ce657c31c27568b190de40"}, "originalPosition": 115}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTI3NzY0OQ==", "bodyText": "I've rewritten the first commit.", "url": "https://github.com/apache/flink/pull/13614#discussion_r505277649", "createdAt": "2020-10-15T07:39:46Z", "author": {"login": "curcur"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/BufferWritingResultPartition.java", "diffHunk": "@@ -211,46 +225,62 @@ protected void releaseInternal() {\n \t\t}\n \t}\n \n-\tprivate BufferBuilder getSubpartitionBufferBuilder(int targetSubpartition) throws IOException {\n-\t\tfinal BufferBuilder bufferBuilder = subpartitionBufferBuilders[targetSubpartition];\n-\t\tif (bufferBuilder != null) {\n-\t\t\treturn bufferBuilder;\n-\t\t}\n+\tprivate BufferBuilder getNewEmptySubpartitionBufferBuilderForNewRecord(int targetSubpartition) throws IOException {\n+\t\tfinal BufferBuilder bufferBuilder = requestNewSubpartitionBufferBuilder(targetSubpartition);\n+\t\tsubpartitions[targetSubpartition].add(bufferBuilder.createBufferConsumerFromBeginning(), 0);\n \n-\t\treturn getNewSubpartitionBufferBuilder(targetSubpartition);\n+\t\treturn bufferBuilder;\n \t}\n \n-\tprivate BufferBuilder getNewSubpartitionBufferBuilder(int targetSubpartition) throws IOException {\n+\tprivate BufferBuilder getNewEmptySubpartitionBufferBuilderForRecordContinuation(\n+\t\t\tfinal ByteBuffer remainingRecordBytes,\n+\t\t\tfinal int targetSubpartition) throws IOException {\n+\t\tfinal BufferBuilder bufferBuilder = requestNewSubpartitionBufferBuilder(targetSubpartition);\n+\t\tfinal int partialRecordBytes = bufferBuilder.appendAndCommit(remainingRecordBytes);\n+\t\tsubpartitions[targetSubpartition].add(bufferBuilder.createBufferConsumerFromBeginning(), partialRecordBytes);\n+\n+\t\treturn bufferBuilder;\n+\t}\n+\n+\tprivate BufferBuilder requestNewSubpartitionBufferBuilder(int targetSubpartition) throws IOException {\n \t\tcheckInProduceState();\n \t\tensureUnicastMode();\n-\n \t\tfinal BufferBuilder bufferBuilder = requestNewBufferBuilderFromPool(targetSubpartition);\n-\t\tsubpartitions[targetSubpartition].add(bufferBuilder.createBufferConsumer());\n \t\tsubpartitionBufferBuilders[targetSubpartition] = bufferBuilder;\n+\n \t\treturn bufferBuilder;\n \t}\n \n-\tprivate BufferBuilder getBroadcastBufferBuilder() throws IOException {\n-\t\tif (broadcastBufferBuilder != null) {\n-\t\t\treturn broadcastBufferBuilder;\n+\tprivate BufferBuilder getNewEmptyBroadcastBufferBuilderForNewRecord() throws IOException {\n+\t\tfinal BufferBuilder bufferBuilder = requestNewBroadcastBufferBuilder();\n+\t\ttry (final BufferConsumer consumer = bufferBuilder.createBufferConsumerFromBeginning()) {\n+\t\t\tfor (ResultSubpartition subpartition : subpartitions) {\n+\t\t\t\tsubpartition.add(consumer.copy(), 0);\n+\t\t\t}\n \t\t}\n \n-\t\treturn getNewBroadcastBufferBuilder();\n+\t\treturn bufferBuilder;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDY1MDMyOQ=="}, "originalCommit": {"oid": "47d3d60c3dc74ab402ce657c31c27568b190de40"}, "originalPosition": 115}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE2MTE0NDMwOnYy", "diffSide": "RIGHT", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/BufferWritingResultPartition.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQxMjo1NjoxMVrOHhRm8A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNVQwNjowMTowNlrOHhx7fg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDY1MzU1Mg==", "bodyText": "hmmm, maybe it will be easier to deduplicate/generalise some of the code, if you replaced partialRecordBytes (partialRecordLength) with remainingRecordBytes.remaining() (remainingRecordLength - remainingRecordLength could be more than the size of one single Buffer, and would basically mean what's the remaining record length. if remainingRecordLength > buffer.size() it means we can not complete record clean up in one call)\nThis would slightly change the implementation of skipBuild(...), but maybe it would make the code overally a bit simpler?", "url": "https://github.com/apache/flink/pull/13614#discussion_r504653552", "createdAt": "2020-10-14T12:56:11Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/BufferWritingResultPartition.java", "diffHunk": "@@ -211,46 +225,62 @@ protected void releaseInternal() {\n \t\t}\n \t}\n \n-\tprivate BufferBuilder getSubpartitionBufferBuilder(int targetSubpartition) throws IOException {\n-\t\tfinal BufferBuilder bufferBuilder = subpartitionBufferBuilders[targetSubpartition];\n-\t\tif (bufferBuilder != null) {\n-\t\t\treturn bufferBuilder;\n-\t\t}\n+\tprivate BufferBuilder getNewEmptySubpartitionBufferBuilderForNewRecord(int targetSubpartition) throws IOException {\n+\t\tfinal BufferBuilder bufferBuilder = requestNewSubpartitionBufferBuilder(targetSubpartition);\n+\t\tsubpartitions[targetSubpartition].add(bufferBuilder.createBufferConsumerFromBeginning(), 0);\n \n-\t\treturn getNewSubpartitionBufferBuilder(targetSubpartition);\n+\t\treturn bufferBuilder;\n \t}\n \n-\tprivate BufferBuilder getNewSubpartitionBufferBuilder(int targetSubpartition) throws IOException {\n+\tprivate BufferBuilder getNewEmptySubpartitionBufferBuilderForRecordContinuation(\n+\t\t\tfinal ByteBuffer remainingRecordBytes,\n+\t\t\tfinal int targetSubpartition) throws IOException {\n+\t\tfinal BufferBuilder bufferBuilder = requestNewSubpartitionBufferBuilder(targetSubpartition);\n+\t\tfinal int partialRecordBytes = bufferBuilder.appendAndCommit(remainingRecordBytes);\n+\t\tsubpartitions[targetSubpartition].add(bufferBuilder.createBufferConsumerFromBeginning(), partialRecordBytes);\n+\n+\t\treturn bufferBuilder;\n+\t}\n+\n+\tprivate BufferBuilder requestNewSubpartitionBufferBuilder(int targetSubpartition) throws IOException {\n \t\tcheckInProduceState();\n \t\tensureUnicastMode();\n-\n \t\tfinal BufferBuilder bufferBuilder = requestNewBufferBuilderFromPool(targetSubpartition);\n-\t\tsubpartitions[targetSubpartition].add(bufferBuilder.createBufferConsumer());\n \t\tsubpartitionBufferBuilders[targetSubpartition] = bufferBuilder;\n+\n \t\treturn bufferBuilder;\n \t}\n \n-\tprivate BufferBuilder getBroadcastBufferBuilder() throws IOException {\n-\t\tif (broadcastBufferBuilder != null) {\n-\t\t\treturn broadcastBufferBuilder;\n+\tprivate BufferBuilder getNewEmptyBroadcastBufferBuilderForNewRecord() throws IOException {\n+\t\tfinal BufferBuilder bufferBuilder = requestNewBroadcastBufferBuilder();\n+\t\ttry (final BufferConsumer consumer = bufferBuilder.createBufferConsumerFromBeginning()) {\n+\t\t\tfor (ResultSubpartition subpartition : subpartitions) {\n+\t\t\t\tsubpartition.add(consumer.copy(), 0);\n+\t\t\t}\n \t\t}\n \n-\t\treturn getNewBroadcastBufferBuilder();\n+\t\treturn bufferBuilder;\n \t}\n \n-\tprivate BufferBuilder getNewBroadcastBufferBuilder() throws IOException {\n+\tprivate BufferBuilder getNewEmptyBroadcastBufferBuilderForRecordContinuation(\n+\t\t\tfinal ByteBuffer remainingRecordBytes) throws IOException {\n+\t\t\tfinal BufferBuilder bufferBuilder = requestNewBroadcastBufferBuilder();\n+\t\tfinal int partialRecordBytes = bufferBuilder.appendAndCommit(remainingRecordBytes);\n+\t\ttry (final BufferConsumer consumer = bufferBuilder.createBufferConsumerFromBeginning()) {\n+\t\t\tfor (ResultSubpartition subpartition : subpartitions) {\n+\t\t\t\tsubpartition.add(consumer.copy(), partialRecordBytes);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "47d3d60c3dc74ab402ce657c31c27568b190de40"}, "originalPosition": 125}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTE4MzEwMg==", "bodyText": "I think that's roughly the same (from the complexity point of view).\nThe only difference is easier to differentiate when remainingRecordLength == buffer.size(), but I think that's not that much big difference.\nI agree the original code is a bit duplicated. If that's the concern, I've rewritten the code (literally just rewrite), to see whether it looks much better now.", "url": "https://github.com/apache/flink/pull/13614#discussion_r505183102", "createdAt": "2020-10-15T06:01:06Z", "author": {"login": "curcur"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/BufferWritingResultPartition.java", "diffHunk": "@@ -211,46 +225,62 @@ protected void releaseInternal() {\n \t\t}\n \t}\n \n-\tprivate BufferBuilder getSubpartitionBufferBuilder(int targetSubpartition) throws IOException {\n-\t\tfinal BufferBuilder bufferBuilder = subpartitionBufferBuilders[targetSubpartition];\n-\t\tif (bufferBuilder != null) {\n-\t\t\treturn bufferBuilder;\n-\t\t}\n+\tprivate BufferBuilder getNewEmptySubpartitionBufferBuilderForNewRecord(int targetSubpartition) throws IOException {\n+\t\tfinal BufferBuilder bufferBuilder = requestNewSubpartitionBufferBuilder(targetSubpartition);\n+\t\tsubpartitions[targetSubpartition].add(bufferBuilder.createBufferConsumerFromBeginning(), 0);\n \n-\t\treturn getNewSubpartitionBufferBuilder(targetSubpartition);\n+\t\treturn bufferBuilder;\n \t}\n \n-\tprivate BufferBuilder getNewSubpartitionBufferBuilder(int targetSubpartition) throws IOException {\n+\tprivate BufferBuilder getNewEmptySubpartitionBufferBuilderForRecordContinuation(\n+\t\t\tfinal ByteBuffer remainingRecordBytes,\n+\t\t\tfinal int targetSubpartition) throws IOException {\n+\t\tfinal BufferBuilder bufferBuilder = requestNewSubpartitionBufferBuilder(targetSubpartition);\n+\t\tfinal int partialRecordBytes = bufferBuilder.appendAndCommit(remainingRecordBytes);\n+\t\tsubpartitions[targetSubpartition].add(bufferBuilder.createBufferConsumerFromBeginning(), partialRecordBytes);\n+\n+\t\treturn bufferBuilder;\n+\t}\n+\n+\tprivate BufferBuilder requestNewSubpartitionBufferBuilder(int targetSubpartition) throws IOException {\n \t\tcheckInProduceState();\n \t\tensureUnicastMode();\n-\n \t\tfinal BufferBuilder bufferBuilder = requestNewBufferBuilderFromPool(targetSubpartition);\n-\t\tsubpartitions[targetSubpartition].add(bufferBuilder.createBufferConsumer());\n \t\tsubpartitionBufferBuilders[targetSubpartition] = bufferBuilder;\n+\n \t\treturn bufferBuilder;\n \t}\n \n-\tprivate BufferBuilder getBroadcastBufferBuilder() throws IOException {\n-\t\tif (broadcastBufferBuilder != null) {\n-\t\t\treturn broadcastBufferBuilder;\n+\tprivate BufferBuilder getNewEmptyBroadcastBufferBuilderForNewRecord() throws IOException {\n+\t\tfinal BufferBuilder bufferBuilder = requestNewBroadcastBufferBuilder();\n+\t\ttry (final BufferConsumer consumer = bufferBuilder.createBufferConsumerFromBeginning()) {\n+\t\t\tfor (ResultSubpartition subpartition : subpartitions) {\n+\t\t\t\tsubpartition.add(consumer.copy(), 0);\n+\t\t\t}\n \t\t}\n \n-\t\treturn getNewBroadcastBufferBuilder();\n+\t\treturn bufferBuilder;\n \t}\n \n-\tprivate BufferBuilder getNewBroadcastBufferBuilder() throws IOException {\n+\tprivate BufferBuilder getNewEmptyBroadcastBufferBuilderForRecordContinuation(\n+\t\t\tfinal ByteBuffer remainingRecordBytes) throws IOException {\n+\t\t\tfinal BufferBuilder bufferBuilder = requestNewBroadcastBufferBuilder();\n+\t\tfinal int partialRecordBytes = bufferBuilder.appendAndCommit(remainingRecordBytes);\n+\t\ttry (final BufferConsumer consumer = bufferBuilder.createBufferConsumerFromBeginning()) {\n+\t\t\tfor (ResultSubpartition subpartition : subpartitions) {\n+\t\t\t\tsubpartition.add(consumer.copy(), partialRecordBytes);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDY1MzU1Mg=="}, "originalCommit": {"oid": "47d3d60c3dc74ab402ce657c31c27568b190de40"}, "originalPosition": 125}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE2MTE1NDA0OnYy", "diffSide": "RIGHT", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/PipelinedSubpartition.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQxMjo1ODoyNFrOHhRs3w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNVQwNjowMzoyNlrOHhx-hQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDY1NTA3MQ==", "bodyText": "inline?", "url": "https://github.com/apache/flink/pull/13614#discussion_r504655071", "createdAt": "2020-10-14T12:58:24Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/PipelinedSubpartition.java", "diffHunk": "@@ -305,6 +308,10 @@ BufferAndBacklog pollBuffer() {\n \t\t}\n \t}\n \n+\tBuffer buildSliceBuffer(BufferConsumerWithPartialRecordLength buffer) {\n+\t\treturn buffer.build();\n+\t}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "47d3d60c3dc74ab402ce657c31c27568b190de40"}, "originalPosition": 125}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTE4Mzg3Nw==", "bodyText": "This method is going to be @OverRide by PipelinedApproximateSubPartition later.\nBut I think it might be confused if changing it here. I will inline the change for now.", "url": "https://github.com/apache/flink/pull/13614#discussion_r505183877", "createdAt": "2020-10-15T06:03:26Z", "author": {"login": "curcur"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/PipelinedSubpartition.java", "diffHunk": "@@ -305,6 +308,10 @@ BufferAndBacklog pollBuffer() {\n \t\t}\n \t}\n \n+\tBuffer buildSliceBuffer(BufferConsumerWithPartialRecordLength buffer) {\n+\t\treturn buffer.build();\n+\t}", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDY1NTA3MQ=="}, "originalCommit": {"oid": "47d3d60c3dc74ab402ce657c31c27568b190de40"}, "originalPosition": 125}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE3MDUxNDgxOnYy", "diffSide": "RIGHT", "path": "flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/ResultPartitionTest.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNlQwODoyNDo0N1rOHit0kA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQwNzozMTo1NFrOHkub2w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjE2NDM2OA==", "bodyText": "Why do you need this special used only in the tests method? Can not you use the same thing as the other test are doing (for example above):\nResultSubpartitionView readView = pipelinedSubpartition.createSubpartitionView(0, new NoOpBufferAvailablityListener());\nreadView. getNextBuffer()\n\n?", "url": "https://github.com/apache/flink/pull/13614#discussion_r506164368", "createdAt": "2020-10-16T08:24:47Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/ResultPartitionTest.java", "diffHunk": "@@ -526,6 +526,44 @@ public void testFlushBoundedBlockingResultPartition() throws IOException {\n \t\tassertNull(readView2.getNextBuffer());\n \t}\n \n+\t@Test\n+\tpublic void testEmitRecordWithRecordSpanningMultipleBuffers() throws Exception {\n+\t\tBufferWritingResultPartition bufferWritingResultPartition = createResultPartition(ResultPartitionType.PIPELINED);\n+\t\tPipelinedSubpartition pipelinedSubpartition = (PipelinedSubpartition) bufferWritingResultPartition.subpartitions[0];\n+\t\tint partialLength = bufferSize / 3;\n+\n+\t\ttry {\n+\t\t\t// emit the first record, record length = partialLength\n+\t\t\tbufferWritingResultPartition.emitRecord(ByteBuffer.allocate(partialLength), 0);\n+\t\t\t// emit the second record, record length = bufferSize\n+\t\t\tbufferWritingResultPartition.emitRecord(ByteBuffer.allocate(bufferSize), 0);\n+\t\t} finally {\n+\t\t\tassertEquals(2, pipelinedSubpartition.getCurrentNumberOfBuffers());\n+\t\t\tassertEquals(0, pipelinedSubpartition.getNextBuffer().getPartialRecordLength());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fd401b11ff98b70455ee395f8b4078851ab1ec9f"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzE0ODU3OA==", "bodyText": "Hmm, it is a bit different, the view can only read BufferAndBacklog, that's the sliced buffer + some other information.\nWhat I need is PartialRecordLength in the buffer queue cell in PrioritizedDeque<BufferConsumerWithPartialRecordLength> buffers.\nHmm, I am actually having a similar concern as you, so I can definitely remove this assertion. This information is tested in later PRs in different ways as well.", "url": "https://github.com/apache/flink/pull/13614#discussion_r507148578", "createdAt": "2020-10-18T13:24:32Z", "author": {"login": "curcur"}, "path": "flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/ResultPartitionTest.java", "diffHunk": "@@ -526,6 +526,44 @@ public void testFlushBoundedBlockingResultPartition() throws IOException {\n \t\tassertNull(readView2.getNextBuffer());\n \t}\n \n+\t@Test\n+\tpublic void testEmitRecordWithRecordSpanningMultipleBuffers() throws Exception {\n+\t\tBufferWritingResultPartition bufferWritingResultPartition = createResultPartition(ResultPartitionType.PIPELINED);\n+\t\tPipelinedSubpartition pipelinedSubpartition = (PipelinedSubpartition) bufferWritingResultPartition.subpartitions[0];\n+\t\tint partialLength = bufferSize / 3;\n+\n+\t\ttry {\n+\t\t\t// emit the first record, record length = partialLength\n+\t\t\tbufferWritingResultPartition.emitRecord(ByteBuffer.allocate(partialLength), 0);\n+\t\t\t// emit the second record, record length = bufferSize\n+\t\t\tbufferWritingResultPartition.emitRecord(ByteBuffer.allocate(bufferSize), 0);\n+\t\t} finally {\n+\t\t\tassertEquals(2, pipelinedSubpartition.getCurrentNumberOfBuffers());\n+\t\t\tassertEquals(0, pipelinedSubpartition.getNextBuffer().getPartialRecordLength());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjE2NDM2OA=="}, "originalCommit": {"oid": "fd401b11ff98b70455ee395f8b4078851ab1ec9f"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODI3MTU3OQ==", "bodyText": "Ok I get it now, thanks for the explanation. I missed that view.pollBuffer() is doing much more. If you think it's valuable to have this unit tested in ResultPartitionTest, then such @VisibleForTesting method is ok \ud83d\udc4d", "url": "https://github.com/apache/flink/pull/13614#discussion_r508271579", "createdAt": "2020-10-20T07:31:54Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/ResultPartitionTest.java", "diffHunk": "@@ -526,6 +526,44 @@ public void testFlushBoundedBlockingResultPartition() throws IOException {\n \t\tassertNull(readView2.getNextBuffer());\n \t}\n \n+\t@Test\n+\tpublic void testEmitRecordWithRecordSpanningMultipleBuffers() throws Exception {\n+\t\tBufferWritingResultPartition bufferWritingResultPartition = createResultPartition(ResultPartitionType.PIPELINED);\n+\t\tPipelinedSubpartition pipelinedSubpartition = (PipelinedSubpartition) bufferWritingResultPartition.subpartitions[0];\n+\t\tint partialLength = bufferSize / 3;\n+\n+\t\ttry {\n+\t\t\t// emit the first record, record length = partialLength\n+\t\t\tbufferWritingResultPartition.emitRecord(ByteBuffer.allocate(partialLength), 0);\n+\t\t\t// emit the second record, record length = bufferSize\n+\t\t\tbufferWritingResultPartition.emitRecord(ByteBuffer.allocate(bufferSize), 0);\n+\t\t} finally {\n+\t\t\tassertEquals(2, pipelinedSubpartition.getCurrentNumberOfBuffers());\n+\t\t\tassertEquals(0, pipelinedSubpartition.getNextBuffer().getPartialRecordLength());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjE2NDM2OA=="}, "originalCommit": {"oid": "fd401b11ff98b70455ee395f8b4078851ab1ec9f"}, "originalPosition": 17}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE3MTIyMjAwOnYy", "diffSide": "RIGHT", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/BufferWritingResultPartition.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNlQxMDo0NDo1NVrOHi1B6w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQwNzoxOToyOVrOHkt_BQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjI4MjQ3NQ==", "bodyText": "could it be createBufferConsumer()?", "url": "https://github.com/apache/flink/pull/13614#discussion_r506282475", "createdAt": "2020-10-16T10:44:55Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/BufferWritingResultPartition.java", "diffHunk": "@@ -211,46 +223,83 @@ protected void releaseInternal() {\n \t\t}\n \t}\n \n-\tprivate BufferBuilder getSubpartitionBufferBuilder(int targetSubpartition) throws IOException {\n-\t\tfinal BufferBuilder bufferBuilder = subpartitionBufferBuilders[targetSubpartition];\n-\t\tif (bufferBuilder != null) {\n-\t\t\treturn bufferBuilder;\n+\tprivate BufferBuilder appendDataToSubpartitionForNewRecord(\n+\t\t\tByteBuffer record, int targetSubpartition) throws IOException {\n+\t\tBufferBuilder buffer = subpartitionBufferBuilders[targetSubpartition];\n+\n+\t\tif (buffer == null) {\n+\t\t\tbuffer = requestNewSubpartitionBufferBuilder(targetSubpartition);\n+\t\t\tsubpartitions[targetSubpartition].add(buffer.createBufferConsumerFromBeginning(), 0);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a70ae897b37437579d8aab2ba3a744553a08feea"}, "originalPosition": 76}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzA1OTA2Ng==", "bodyText": "Yes, it could be. In fact, in all existing cases, the consumer is created from the beginning. I was trying to remove createBufferConsumer() to use createBufferConsumerFromBeginning() in all places.\nBut at a second thought, I am not happy with createBufferConsumerFromBeginning(), and that's why I am still keeping createBufferConsumer().\nI think I probably have a way to get rid of createBufferConsumerFromBeginning(), with a bit more complicated clean-up logic.", "url": "https://github.com/apache/flink/pull/13614#discussion_r507059066", "createdAt": "2020-10-18T09:25:10Z", "author": {"login": "curcur"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/BufferWritingResultPartition.java", "diffHunk": "@@ -211,46 +223,83 @@ protected void releaseInternal() {\n \t\t}\n \t}\n \n-\tprivate BufferBuilder getSubpartitionBufferBuilder(int targetSubpartition) throws IOException {\n-\t\tfinal BufferBuilder bufferBuilder = subpartitionBufferBuilders[targetSubpartition];\n-\t\tif (bufferBuilder != null) {\n-\t\t\treturn bufferBuilder;\n+\tprivate BufferBuilder appendDataToSubpartitionForNewRecord(\n+\t\t\tByteBuffer record, int targetSubpartition) throws IOException {\n+\t\tBufferBuilder buffer = subpartitionBufferBuilders[targetSubpartition];\n+\n+\t\tif (buffer == null) {\n+\t\t\tbuffer = requestNewSubpartitionBufferBuilder(targetSubpartition);\n+\t\t\tsubpartitions[targetSubpartition].add(buffer.createBufferConsumerFromBeginning(), 0);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjI4MjQ3NQ=="}, "originalCommit": {"oid": "a70ae897b37437579d8aab2ba3a744553a08feea"}, "originalPosition": 76}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzE3MTA1Mw==", "bodyText": "I've actually tried a bit to remove createBufferConsumerFromBeginning, which can work, but the clean-up logic is not as simple and beautful as it is now: skip() + build(). (Reason: skip can be unsuccessful because a partial record may not been written or visible yet before skip(), but can be visible after skip()).\nSo I will keep the way we are going right now.\nTo your question:\ncould it be createBufferConsumer()?\nThe answer is yes to create a new record, but I think the meaning is more clear to use createBufferConsumerFromBeginning. What do you think?\nI can definitely change it to createBufferConsumer()", "url": "https://github.com/apache/flink/pull/13614#discussion_r507171053", "createdAt": "2020-10-18T14:39:48Z", "author": {"login": "curcur"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/BufferWritingResultPartition.java", "diffHunk": "@@ -211,46 +223,83 @@ protected void releaseInternal() {\n \t\t}\n \t}\n \n-\tprivate BufferBuilder getSubpartitionBufferBuilder(int targetSubpartition) throws IOException {\n-\t\tfinal BufferBuilder bufferBuilder = subpartitionBufferBuilders[targetSubpartition];\n-\t\tif (bufferBuilder != null) {\n-\t\t\treturn bufferBuilder;\n+\tprivate BufferBuilder appendDataToSubpartitionForNewRecord(\n+\t\t\tByteBuffer record, int targetSubpartition) throws IOException {\n+\t\tBufferBuilder buffer = subpartitionBufferBuilders[targetSubpartition];\n+\n+\t\tif (buffer == null) {\n+\t\t\tbuffer = requestNewSubpartitionBufferBuilder(targetSubpartition);\n+\t\t\tsubpartitions[targetSubpartition].add(buffer.createBufferConsumerFromBeginning(), 0);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjI4MjQ3NQ=="}, "originalCommit": {"oid": "a70ae897b37437579d8aab2ba3a744553a08feea"}, "originalPosition": 76}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODI2NDE5Nw==", "bodyText": "The answer is yes to create a new record, but I think the meaning is more clear to use createBufferConsumerFromBeginning. What do you think?\n\n\ud83d\udc4d Ok, let's keep it as it is.", "url": "https://github.com/apache/flink/pull/13614#discussion_r508264197", "createdAt": "2020-10-20T07:19:29Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/BufferWritingResultPartition.java", "diffHunk": "@@ -211,46 +223,83 @@ protected void releaseInternal() {\n \t\t}\n \t}\n \n-\tprivate BufferBuilder getSubpartitionBufferBuilder(int targetSubpartition) throws IOException {\n-\t\tfinal BufferBuilder bufferBuilder = subpartitionBufferBuilders[targetSubpartition];\n-\t\tif (bufferBuilder != null) {\n-\t\t\treturn bufferBuilder;\n+\tprivate BufferBuilder appendDataToSubpartitionForNewRecord(\n+\t\t\tByteBuffer record, int targetSubpartition) throws IOException {\n+\t\tBufferBuilder buffer = subpartitionBufferBuilders[targetSubpartition];\n+\n+\t\tif (buffer == null) {\n+\t\t\tbuffer = requestNewSubpartitionBufferBuilder(targetSubpartition);\n+\t\t\tsubpartitions[targetSubpartition].add(buffer.createBufferConsumerFromBeginning(), 0);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjI4MjQ3NQ=="}, "originalCommit": {"oid": "a70ae897b37437579d8aab2ba3a744553a08feea"}, "originalPosition": 76}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE3MTI0NzI1OnYy", "diffSide": "RIGHT", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/BufferWritingResultPartition.java", "isResolved": true, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNlQxMDo1MDowN1rOHi1Syw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQwNzozMDo0OVrOHkuZeQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjI4Njc5NQ==", "bodyText": "unicast. That's I think better and more consistent naming scheme compared to subpartition.\nSo maybe rename the  methods with subpartition in the name:\nrequestNewSubpartitionBufferBuilder  -> requetnNewUnicastBufferBuilder\nappendDataToSubpartitionForNewRecord -> appendUnicastDataForNewRecord\netc...\n?", "url": "https://github.com/apache/flink/pull/13614#discussion_r506286795", "createdAt": "2020-10-16T10:50:07Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/BufferWritingResultPartition.java", "diffHunk": "@@ -211,46 +223,83 @@ protected void releaseInternal() {\n \t\t}\n \t}\n \n-\tprivate BufferBuilder getSubpartitionBufferBuilder(int targetSubpartition) throws IOException {\n-\t\tfinal BufferBuilder bufferBuilder = subpartitionBufferBuilders[targetSubpartition];\n-\t\tif (bufferBuilder != null) {\n-\t\t\treturn bufferBuilder;\n+\tprivate BufferBuilder appendDataToSubpartitionForNewRecord(\n+\t\t\tByteBuffer record, int targetSubpartition) throws IOException {\n+\t\tBufferBuilder buffer = subpartitionBufferBuilders[targetSubpartition];\n+\n+\t\tif (buffer == null) {\n+\t\t\tbuffer = requestNewSubpartitionBufferBuilder(targetSubpartition);\n+\t\t\tsubpartitions[targetSubpartition].add(buffer.createBufferConsumerFromBeginning(), 0);\n \t\t}\n \n-\t\treturn getNewSubpartitionBufferBuilder(targetSubpartition);\n+\t\tbuffer.appendAndCommit(record);\n+\n+\t\treturn buffer;\n \t}\n \n-\tprivate BufferBuilder getNewSubpartitionBufferBuilder(int targetSubpartition) throws IOException {\n-\t\tcheckInProduceState();\n-\t\tensureUnicastMode();\n+\tprivate BufferBuilder appendDataToSubpartitionForRecordContinuation(\n+\t\t\tfinal ByteBuffer remainingRecordBytes,\n+\t\t\tfinal int targetSubpartition) throws IOException {\n+\t\tfinal BufferBuilder buffer = requestNewSubpartitionBufferBuilder(targetSubpartition);\n+\t\t// !! Be aware, in case of partialRecordBytes != 0, partial length and data has to `appendAndCommit` first\n+\t\t// before consumer is created. Otherwise it would be confused with the case the buffer starting\n+\t\t// with a complete record.\n+\t\t// !! The next two lines can not change order.\n+\t\tfinal int partialRecordBytes = buffer.appendAndCommit(remainingRecordBytes);\n+\t\tsubpartitions[targetSubpartition].add(buffer.createBufferConsumerFromBeginning(), partialRecordBytes);\n+\n+\t\treturn buffer;\n+\t}\n \n-\t\tfinal BufferBuilder bufferBuilder = requestNewBufferBuilderFromPool(targetSubpartition);\n-\t\tsubpartitions[targetSubpartition].add(bufferBuilder.createBufferConsumer());\n-\t\tsubpartitionBufferBuilders[targetSubpartition] = bufferBuilder;\n-\t\treturn bufferBuilder;\n+\tprivate BufferBuilder appendDataToBroadcastForNewRecord(ByteBuffer record) throws IOException {\n+\t\tBufferBuilder buffer = broadcastBufferBuilder;\n+\n+\t\tif (buffer == null) {\n+\t\t\tbuffer = requestNewBroadcastBufferBuilder();\n+\t\t\tcreateBroadcastBufferConsumers(buffer, 0);\n+\t\t}\n+\n+\t\tbuffer.appendAndCommit(record);\n+\n+\t\treturn buffer;\n \t}\n \n-\tprivate BufferBuilder getBroadcastBufferBuilder() throws IOException {\n-\t\tif (broadcastBufferBuilder != null) {\n-\t\t\treturn broadcastBufferBuilder;\n+\tprivate BufferBuilder appendDataToBroadcastForRecordContinuation(\n+\t\t\tfinal ByteBuffer remainingRecordBytes) throws IOException {\n+\t\tfinal BufferBuilder buffer = requestNewBroadcastBufferBuilder();\n+\t\t// !! Be aware, in case of partialRecordBytes != 0, partial length and data has to `appendAndCommit` first\n+\t\t// before consumer is created. Otherwise it would be confused with the case the buffer starting\n+\t\t// with a complete record.\n+\t\t// !! The next two lines can not change order.\n+\t\tfinal int partialRecordBytes = buffer.appendAndCommit(remainingRecordBytes);\n+\t\tcreateBroadcastBufferConsumers(buffer, partialRecordBytes);\n+\n+\t\treturn buffer;\n+\t}\n+\n+\tprivate void createBroadcastBufferConsumers(BufferBuilder buffer, int partialRecordBytes) throws IOException {\n+\t\ttry (final BufferConsumer consumer = buffer.createBufferConsumerFromBeginning()) {\n+\t\t\tfor (ResultSubpartition subpartition : subpartitions) {\n+\t\t\t\tsubpartition.add(consumer.copy(), partialRecordBytes);\n+\t\t\t}\n \t\t}\n+\t}\n \n-\t\treturn getNewBroadcastBufferBuilder();\n+\tprivate BufferBuilder requestNewSubpartitionBufferBuilder(int targetSubpartition) throws IOException {\n+\t\tcheckInProduceState();\n+\t\tensureUnicastMode();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a70ae897b37437579d8aab2ba3a744553a08feea"}, "originalPosition": 146}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzE1Mzc3MA==", "bodyText": "I am not sure, I think these are consistent with this\n/** For non-broadcast mode, each subpartition maintains a separate BufferBuilder which might be null. */\n\tprivate final BufferBuilder[] subpartitionBufferBuilders;\n\nDo you want to change these to unicastBufferBuilders?\nIt is a bit confusing as well?", "url": "https://github.com/apache/flink/pull/13614#discussion_r507153770", "createdAt": "2020-10-18T13:40:49Z", "author": {"login": "curcur"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/BufferWritingResultPartition.java", "diffHunk": "@@ -211,46 +223,83 @@ protected void releaseInternal() {\n \t\t}\n \t}\n \n-\tprivate BufferBuilder getSubpartitionBufferBuilder(int targetSubpartition) throws IOException {\n-\t\tfinal BufferBuilder bufferBuilder = subpartitionBufferBuilders[targetSubpartition];\n-\t\tif (bufferBuilder != null) {\n-\t\t\treturn bufferBuilder;\n+\tprivate BufferBuilder appendDataToSubpartitionForNewRecord(\n+\t\t\tByteBuffer record, int targetSubpartition) throws IOException {\n+\t\tBufferBuilder buffer = subpartitionBufferBuilders[targetSubpartition];\n+\n+\t\tif (buffer == null) {\n+\t\t\tbuffer = requestNewSubpartitionBufferBuilder(targetSubpartition);\n+\t\t\tsubpartitions[targetSubpartition].add(buffer.createBufferConsumerFromBeginning(), 0);\n \t\t}\n \n-\t\treturn getNewSubpartitionBufferBuilder(targetSubpartition);\n+\t\tbuffer.appendAndCommit(record);\n+\n+\t\treturn buffer;\n \t}\n \n-\tprivate BufferBuilder getNewSubpartitionBufferBuilder(int targetSubpartition) throws IOException {\n-\t\tcheckInProduceState();\n-\t\tensureUnicastMode();\n+\tprivate BufferBuilder appendDataToSubpartitionForRecordContinuation(\n+\t\t\tfinal ByteBuffer remainingRecordBytes,\n+\t\t\tfinal int targetSubpartition) throws IOException {\n+\t\tfinal BufferBuilder buffer = requestNewSubpartitionBufferBuilder(targetSubpartition);\n+\t\t// !! Be aware, in case of partialRecordBytes != 0, partial length and data has to `appendAndCommit` first\n+\t\t// before consumer is created. Otherwise it would be confused with the case the buffer starting\n+\t\t// with a complete record.\n+\t\t// !! The next two lines can not change order.\n+\t\tfinal int partialRecordBytes = buffer.appendAndCommit(remainingRecordBytes);\n+\t\tsubpartitions[targetSubpartition].add(buffer.createBufferConsumerFromBeginning(), partialRecordBytes);\n+\n+\t\treturn buffer;\n+\t}\n \n-\t\tfinal BufferBuilder bufferBuilder = requestNewBufferBuilderFromPool(targetSubpartition);\n-\t\tsubpartitions[targetSubpartition].add(bufferBuilder.createBufferConsumer());\n-\t\tsubpartitionBufferBuilders[targetSubpartition] = bufferBuilder;\n-\t\treturn bufferBuilder;\n+\tprivate BufferBuilder appendDataToBroadcastForNewRecord(ByteBuffer record) throws IOException {\n+\t\tBufferBuilder buffer = broadcastBufferBuilder;\n+\n+\t\tif (buffer == null) {\n+\t\t\tbuffer = requestNewBroadcastBufferBuilder();\n+\t\t\tcreateBroadcastBufferConsumers(buffer, 0);\n+\t\t}\n+\n+\t\tbuffer.appendAndCommit(record);\n+\n+\t\treturn buffer;\n \t}\n \n-\tprivate BufferBuilder getBroadcastBufferBuilder() throws IOException {\n-\t\tif (broadcastBufferBuilder != null) {\n-\t\t\treturn broadcastBufferBuilder;\n+\tprivate BufferBuilder appendDataToBroadcastForRecordContinuation(\n+\t\t\tfinal ByteBuffer remainingRecordBytes) throws IOException {\n+\t\tfinal BufferBuilder buffer = requestNewBroadcastBufferBuilder();\n+\t\t// !! Be aware, in case of partialRecordBytes != 0, partial length and data has to `appendAndCommit` first\n+\t\t// before consumer is created. Otherwise it would be confused with the case the buffer starting\n+\t\t// with a complete record.\n+\t\t// !! The next two lines can not change order.\n+\t\tfinal int partialRecordBytes = buffer.appendAndCommit(remainingRecordBytes);\n+\t\tcreateBroadcastBufferConsumers(buffer, partialRecordBytes);\n+\n+\t\treturn buffer;\n+\t}\n+\n+\tprivate void createBroadcastBufferConsumers(BufferBuilder buffer, int partialRecordBytes) throws IOException {\n+\t\ttry (final BufferConsumer consumer = buffer.createBufferConsumerFromBeginning()) {\n+\t\t\tfor (ResultSubpartition subpartition : subpartitions) {\n+\t\t\t\tsubpartition.add(consumer.copy(), partialRecordBytes);\n+\t\t\t}\n \t\t}\n+\t}\n \n-\t\treturn getNewBroadcastBufferBuilder();\n+\tprivate BufferBuilder requestNewSubpartitionBufferBuilder(int targetSubpartition) throws IOException {\n+\t\tcheckInProduceState();\n+\t\tensureUnicastMode();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjI4Njc5NQ=="}, "originalCommit": {"oid": "a70ae897b37437579d8aab2ba3a744553a08feea"}, "originalPosition": 146}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODI2MjE0Nw==", "bodyText": "I think the unicast is more accurate and better prefix/infix compared to subpartition. unicast is after all exactly describing this:\n\nunicast is a one-to-one transmission from one point in the network to another poin\n\nWhile subpartition is hmmm, what does it actually mean? Without a context you would have no idea what this name represents. And unicast is from the same domain as already used broadcast name, so I would vote to rename all subpartition usages in this context to unicast.\nMaybe in your commit, when you are adding some methods use unicast, and add a follow up hotfix commit to rename the remaining things like the one you mentioned:\nprivate final BufferBuilder[] subpartitionBufferBuilders;", "url": "https://github.com/apache/flink/pull/13614#discussion_r508262147", "createdAt": "2020-10-20T07:15:52Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/BufferWritingResultPartition.java", "diffHunk": "@@ -211,46 +223,83 @@ protected void releaseInternal() {\n \t\t}\n \t}\n \n-\tprivate BufferBuilder getSubpartitionBufferBuilder(int targetSubpartition) throws IOException {\n-\t\tfinal BufferBuilder bufferBuilder = subpartitionBufferBuilders[targetSubpartition];\n-\t\tif (bufferBuilder != null) {\n-\t\t\treturn bufferBuilder;\n+\tprivate BufferBuilder appendDataToSubpartitionForNewRecord(\n+\t\t\tByteBuffer record, int targetSubpartition) throws IOException {\n+\t\tBufferBuilder buffer = subpartitionBufferBuilders[targetSubpartition];\n+\n+\t\tif (buffer == null) {\n+\t\t\tbuffer = requestNewSubpartitionBufferBuilder(targetSubpartition);\n+\t\t\tsubpartitions[targetSubpartition].add(buffer.createBufferConsumerFromBeginning(), 0);\n \t\t}\n \n-\t\treturn getNewSubpartitionBufferBuilder(targetSubpartition);\n+\t\tbuffer.appendAndCommit(record);\n+\n+\t\treturn buffer;\n \t}\n \n-\tprivate BufferBuilder getNewSubpartitionBufferBuilder(int targetSubpartition) throws IOException {\n-\t\tcheckInProduceState();\n-\t\tensureUnicastMode();\n+\tprivate BufferBuilder appendDataToSubpartitionForRecordContinuation(\n+\t\t\tfinal ByteBuffer remainingRecordBytes,\n+\t\t\tfinal int targetSubpartition) throws IOException {\n+\t\tfinal BufferBuilder buffer = requestNewSubpartitionBufferBuilder(targetSubpartition);\n+\t\t// !! Be aware, in case of partialRecordBytes != 0, partial length and data has to `appendAndCommit` first\n+\t\t// before consumer is created. Otherwise it would be confused with the case the buffer starting\n+\t\t// with a complete record.\n+\t\t// !! The next two lines can not change order.\n+\t\tfinal int partialRecordBytes = buffer.appendAndCommit(remainingRecordBytes);\n+\t\tsubpartitions[targetSubpartition].add(buffer.createBufferConsumerFromBeginning(), partialRecordBytes);\n+\n+\t\treturn buffer;\n+\t}\n \n-\t\tfinal BufferBuilder bufferBuilder = requestNewBufferBuilderFromPool(targetSubpartition);\n-\t\tsubpartitions[targetSubpartition].add(bufferBuilder.createBufferConsumer());\n-\t\tsubpartitionBufferBuilders[targetSubpartition] = bufferBuilder;\n-\t\treturn bufferBuilder;\n+\tprivate BufferBuilder appendDataToBroadcastForNewRecord(ByteBuffer record) throws IOException {\n+\t\tBufferBuilder buffer = broadcastBufferBuilder;\n+\n+\t\tif (buffer == null) {\n+\t\t\tbuffer = requestNewBroadcastBufferBuilder();\n+\t\t\tcreateBroadcastBufferConsumers(buffer, 0);\n+\t\t}\n+\n+\t\tbuffer.appendAndCommit(record);\n+\n+\t\treturn buffer;\n \t}\n \n-\tprivate BufferBuilder getBroadcastBufferBuilder() throws IOException {\n-\t\tif (broadcastBufferBuilder != null) {\n-\t\t\treturn broadcastBufferBuilder;\n+\tprivate BufferBuilder appendDataToBroadcastForRecordContinuation(\n+\t\t\tfinal ByteBuffer remainingRecordBytes) throws IOException {\n+\t\tfinal BufferBuilder buffer = requestNewBroadcastBufferBuilder();\n+\t\t// !! Be aware, in case of partialRecordBytes != 0, partial length and data has to `appendAndCommit` first\n+\t\t// before consumer is created. Otherwise it would be confused with the case the buffer starting\n+\t\t// with a complete record.\n+\t\t// !! The next two lines can not change order.\n+\t\tfinal int partialRecordBytes = buffer.appendAndCommit(remainingRecordBytes);\n+\t\tcreateBroadcastBufferConsumers(buffer, partialRecordBytes);\n+\n+\t\treturn buffer;\n+\t}\n+\n+\tprivate void createBroadcastBufferConsumers(BufferBuilder buffer, int partialRecordBytes) throws IOException {\n+\t\ttry (final BufferConsumer consumer = buffer.createBufferConsumerFromBeginning()) {\n+\t\t\tfor (ResultSubpartition subpartition : subpartitions) {\n+\t\t\t\tsubpartition.add(consumer.copy(), partialRecordBytes);\n+\t\t\t}\n \t\t}\n+\t}\n \n-\t\treturn getNewBroadcastBufferBuilder();\n+\tprivate BufferBuilder requestNewSubpartitionBufferBuilder(int targetSubpartition) throws IOException {\n+\t\tcheckInProduceState();\n+\t\tensureUnicastMode();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjI4Njc5NQ=="}, "originalCommit": {"oid": "a70ae897b37437579d8aab2ba3a744553a08feea"}, "originalPosition": 146}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODI2ODQzMg==", "bodyText": "Sure, do you mind my changing the name within the same PR?", "url": "https://github.com/apache/flink/pull/13614#discussion_r508268432", "createdAt": "2020-10-20T07:26:39Z", "author": {"login": "curcur"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/BufferWritingResultPartition.java", "diffHunk": "@@ -211,46 +223,83 @@ protected void releaseInternal() {\n \t\t}\n \t}\n \n-\tprivate BufferBuilder getSubpartitionBufferBuilder(int targetSubpartition) throws IOException {\n-\t\tfinal BufferBuilder bufferBuilder = subpartitionBufferBuilders[targetSubpartition];\n-\t\tif (bufferBuilder != null) {\n-\t\t\treturn bufferBuilder;\n+\tprivate BufferBuilder appendDataToSubpartitionForNewRecord(\n+\t\t\tByteBuffer record, int targetSubpartition) throws IOException {\n+\t\tBufferBuilder buffer = subpartitionBufferBuilders[targetSubpartition];\n+\n+\t\tif (buffer == null) {\n+\t\t\tbuffer = requestNewSubpartitionBufferBuilder(targetSubpartition);\n+\t\t\tsubpartitions[targetSubpartition].add(buffer.createBufferConsumerFromBeginning(), 0);\n \t\t}\n \n-\t\treturn getNewSubpartitionBufferBuilder(targetSubpartition);\n+\t\tbuffer.appendAndCommit(record);\n+\n+\t\treturn buffer;\n \t}\n \n-\tprivate BufferBuilder getNewSubpartitionBufferBuilder(int targetSubpartition) throws IOException {\n-\t\tcheckInProduceState();\n-\t\tensureUnicastMode();\n+\tprivate BufferBuilder appendDataToSubpartitionForRecordContinuation(\n+\t\t\tfinal ByteBuffer remainingRecordBytes,\n+\t\t\tfinal int targetSubpartition) throws IOException {\n+\t\tfinal BufferBuilder buffer = requestNewSubpartitionBufferBuilder(targetSubpartition);\n+\t\t// !! Be aware, in case of partialRecordBytes != 0, partial length and data has to `appendAndCommit` first\n+\t\t// before consumer is created. Otherwise it would be confused with the case the buffer starting\n+\t\t// with a complete record.\n+\t\t// !! The next two lines can not change order.\n+\t\tfinal int partialRecordBytes = buffer.appendAndCommit(remainingRecordBytes);\n+\t\tsubpartitions[targetSubpartition].add(buffer.createBufferConsumerFromBeginning(), partialRecordBytes);\n+\n+\t\treturn buffer;\n+\t}\n \n-\t\tfinal BufferBuilder bufferBuilder = requestNewBufferBuilderFromPool(targetSubpartition);\n-\t\tsubpartitions[targetSubpartition].add(bufferBuilder.createBufferConsumer());\n-\t\tsubpartitionBufferBuilders[targetSubpartition] = bufferBuilder;\n-\t\treturn bufferBuilder;\n+\tprivate BufferBuilder appendDataToBroadcastForNewRecord(ByteBuffer record) throws IOException {\n+\t\tBufferBuilder buffer = broadcastBufferBuilder;\n+\n+\t\tif (buffer == null) {\n+\t\t\tbuffer = requestNewBroadcastBufferBuilder();\n+\t\t\tcreateBroadcastBufferConsumers(buffer, 0);\n+\t\t}\n+\n+\t\tbuffer.appendAndCommit(record);\n+\n+\t\treturn buffer;\n \t}\n \n-\tprivate BufferBuilder getBroadcastBufferBuilder() throws IOException {\n-\t\tif (broadcastBufferBuilder != null) {\n-\t\t\treturn broadcastBufferBuilder;\n+\tprivate BufferBuilder appendDataToBroadcastForRecordContinuation(\n+\t\t\tfinal ByteBuffer remainingRecordBytes) throws IOException {\n+\t\tfinal BufferBuilder buffer = requestNewBroadcastBufferBuilder();\n+\t\t// !! Be aware, in case of partialRecordBytes != 0, partial length and data has to `appendAndCommit` first\n+\t\t// before consumer is created. Otherwise it would be confused with the case the buffer starting\n+\t\t// with a complete record.\n+\t\t// !! The next two lines can not change order.\n+\t\tfinal int partialRecordBytes = buffer.appendAndCommit(remainingRecordBytes);\n+\t\tcreateBroadcastBufferConsumers(buffer, partialRecordBytes);\n+\n+\t\treturn buffer;\n+\t}\n+\n+\tprivate void createBroadcastBufferConsumers(BufferBuilder buffer, int partialRecordBytes) throws IOException {\n+\t\ttry (final BufferConsumer consumer = buffer.createBufferConsumerFromBeginning()) {\n+\t\t\tfor (ResultSubpartition subpartition : subpartitions) {\n+\t\t\t\tsubpartition.add(consumer.copy(), partialRecordBytes);\n+\t\t\t}\n \t\t}\n+\t}\n \n-\t\treturn getNewBroadcastBufferBuilder();\n+\tprivate BufferBuilder requestNewSubpartitionBufferBuilder(int targetSubpartition) throws IOException {\n+\t\tcheckInProduceState();\n+\t\tensureUnicastMode();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjI4Njc5NQ=="}, "originalCommit": {"oid": "a70ae897b37437579d8aab2ba3a744553a08feea"}, "originalPosition": 146}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODI3MDk2OQ==", "bodyText": "Please do so in the same PR :) as I described. New code that you are adding please add with already corrected name. Pre existing code that you are going to rename, please rename in a separate hotfix commit.", "url": "https://github.com/apache/flink/pull/13614#discussion_r508270969", "createdAt": "2020-10-20T07:30:49Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/BufferWritingResultPartition.java", "diffHunk": "@@ -211,46 +223,83 @@ protected void releaseInternal() {\n \t\t}\n \t}\n \n-\tprivate BufferBuilder getSubpartitionBufferBuilder(int targetSubpartition) throws IOException {\n-\t\tfinal BufferBuilder bufferBuilder = subpartitionBufferBuilders[targetSubpartition];\n-\t\tif (bufferBuilder != null) {\n-\t\t\treturn bufferBuilder;\n+\tprivate BufferBuilder appendDataToSubpartitionForNewRecord(\n+\t\t\tByteBuffer record, int targetSubpartition) throws IOException {\n+\t\tBufferBuilder buffer = subpartitionBufferBuilders[targetSubpartition];\n+\n+\t\tif (buffer == null) {\n+\t\t\tbuffer = requestNewSubpartitionBufferBuilder(targetSubpartition);\n+\t\t\tsubpartitions[targetSubpartition].add(buffer.createBufferConsumerFromBeginning(), 0);\n \t\t}\n \n-\t\treturn getNewSubpartitionBufferBuilder(targetSubpartition);\n+\t\tbuffer.appendAndCommit(record);\n+\n+\t\treturn buffer;\n \t}\n \n-\tprivate BufferBuilder getNewSubpartitionBufferBuilder(int targetSubpartition) throws IOException {\n-\t\tcheckInProduceState();\n-\t\tensureUnicastMode();\n+\tprivate BufferBuilder appendDataToSubpartitionForRecordContinuation(\n+\t\t\tfinal ByteBuffer remainingRecordBytes,\n+\t\t\tfinal int targetSubpartition) throws IOException {\n+\t\tfinal BufferBuilder buffer = requestNewSubpartitionBufferBuilder(targetSubpartition);\n+\t\t// !! Be aware, in case of partialRecordBytes != 0, partial length and data has to `appendAndCommit` first\n+\t\t// before consumer is created. Otherwise it would be confused with the case the buffer starting\n+\t\t// with a complete record.\n+\t\t// !! The next two lines can not change order.\n+\t\tfinal int partialRecordBytes = buffer.appendAndCommit(remainingRecordBytes);\n+\t\tsubpartitions[targetSubpartition].add(buffer.createBufferConsumerFromBeginning(), partialRecordBytes);\n+\n+\t\treturn buffer;\n+\t}\n \n-\t\tfinal BufferBuilder bufferBuilder = requestNewBufferBuilderFromPool(targetSubpartition);\n-\t\tsubpartitions[targetSubpartition].add(bufferBuilder.createBufferConsumer());\n-\t\tsubpartitionBufferBuilders[targetSubpartition] = bufferBuilder;\n-\t\treturn bufferBuilder;\n+\tprivate BufferBuilder appendDataToBroadcastForNewRecord(ByteBuffer record) throws IOException {\n+\t\tBufferBuilder buffer = broadcastBufferBuilder;\n+\n+\t\tif (buffer == null) {\n+\t\t\tbuffer = requestNewBroadcastBufferBuilder();\n+\t\t\tcreateBroadcastBufferConsumers(buffer, 0);\n+\t\t}\n+\n+\t\tbuffer.appendAndCommit(record);\n+\n+\t\treturn buffer;\n \t}\n \n-\tprivate BufferBuilder getBroadcastBufferBuilder() throws IOException {\n-\t\tif (broadcastBufferBuilder != null) {\n-\t\t\treturn broadcastBufferBuilder;\n+\tprivate BufferBuilder appendDataToBroadcastForRecordContinuation(\n+\t\t\tfinal ByteBuffer remainingRecordBytes) throws IOException {\n+\t\tfinal BufferBuilder buffer = requestNewBroadcastBufferBuilder();\n+\t\t// !! Be aware, in case of partialRecordBytes != 0, partial length and data has to `appendAndCommit` first\n+\t\t// before consumer is created. Otherwise it would be confused with the case the buffer starting\n+\t\t// with a complete record.\n+\t\t// !! The next two lines can not change order.\n+\t\tfinal int partialRecordBytes = buffer.appendAndCommit(remainingRecordBytes);\n+\t\tcreateBroadcastBufferConsumers(buffer, partialRecordBytes);\n+\n+\t\treturn buffer;\n+\t}\n+\n+\tprivate void createBroadcastBufferConsumers(BufferBuilder buffer, int partialRecordBytes) throws IOException {\n+\t\ttry (final BufferConsumer consumer = buffer.createBufferConsumerFromBeginning()) {\n+\t\t\tfor (ResultSubpartition subpartition : subpartitions) {\n+\t\t\t\tsubpartition.add(consumer.copy(), partialRecordBytes);\n+\t\t\t}\n \t\t}\n+\t}\n \n-\t\treturn getNewBroadcastBufferBuilder();\n+\tprivate BufferBuilder requestNewSubpartitionBufferBuilder(int targetSubpartition) throws IOException {\n+\t\tcheckInProduceState();\n+\t\tensureUnicastMode();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjI4Njc5NQ=="}, "originalCommit": {"oid": "a70ae897b37437579d8aab2ba3a744553a08feea"}, "originalPosition": 146}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 133, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}