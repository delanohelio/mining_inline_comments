{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTAyMDEzOTQ5", "number": 13605, "title": "[FLINK-19599][table] Introduce Filesystem format factories to integrate new FileSource to table", "bodyText": "What is the purpose of the change & Brief change log\nIntroduce BulkFormatFactory: BulkFormat createBulkFormat(context).\nFilesystem connector use this factory to create BulkFormat, and use new FileSource to read files.\nFileSystemSource takes precedence over BulkFormatFactory to read files.\nIntroduce BulkWriterFactory: BulkWriter.Factory createBulkWriter(context).\nFilesystem connector use this factory to create BulkWriter, and use streaming file sink to write files.\nFileSystemSink takes precedence over BulkWriterFactory to write files.\nIntroduce ParquetBulkFormatFactory to implement new interfaces.\nVerifying this change\nThis change is already covered by existing tests, such as:\n\nParquetFsStreamingSinkITCase\nParquetFileSystemITCase\n\nDoes this pull request potentially affect one of the following parts:\n\nDependencies (does it add or upgrade a dependency): no\nThe public API, i.e., is any changed class annotated with @Public(Evolving): no\nThe serializers: no\nThe runtime per-record code paths (performance sensitive): no\nAnything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: no\nThe S3 file system connector: no\n\nDocumentation\n\nDoes this pull request introduce a new feature? (yes)\nIf yes, how is the feature documented? (JavaDocs)", "createdAt": "2020-10-13T06:19:37Z", "url": "https://github.com/apache/flink/pull/13605", "merged": true, "mergeCommit": {"oid": "68361845c25f85e13526d3634d5437cb37050cfc"}, "closed": true, "closedAt": "2020-10-29T07:42:09Z", "author": {"login": "JingsongLi"}, "timelineItems": {"totalCount": 12, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdSUsZaABqjM4NzQ1MzAyMTQ=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdXI4lEAH2gAyNTAyMDEzOTQ5OjVlZjg4YzhmNDAwYjZjMmVmNWY1Y2E2YmJkNzUwYzE3ZjI3MTM3Yzc=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "8beabe2e1b04afe449610c44f4c376909b463e10", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/flink/commit/8beabe2e1b04afe449610c44f4c376909b463e10", "committedDate": "2020-10-13T06:23:01Z", "message": "Minors"}, "afterCommit": {"oid": "76ff23e27070dc54e6af85dd91c2742457621aa9", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/flink/commit/76ff23e27070dc54e6af85dd91c2742457621aa9", "committedDate": "2020-10-14T03:32:19Z", "message": "[FLINK-19599][table] Introduce BulkFormatFactory to integrate new FileSource to table"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "76ff23e27070dc54e6af85dd91c2742457621aa9", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/flink/commit/76ff23e27070dc54e6af85dd91c2742457621aa9", "committedDate": "2020-10-14T03:32:19Z", "message": "[FLINK-19599][table] Introduce BulkFormatFactory to integrate new FileSource to table"}, "afterCommit": {"oid": "315b69fafef254ff273bc36c713d28166260e4c6", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/flink/commit/315b69fafef254ff273bc36c713d28166260e4c6", "committedDate": "2020-10-27T07:29:28Z", "message": "[FLINK-19599][table] Introduce Filesystem format factories to integrate new FileSource to table"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE4MzAzOTcw", "url": "https://github.com/apache/flink/pull/13605#pullrequestreview-518303970", "createdAt": "2020-10-28T03:45:19Z", "commit": {"oid": "05116d4768052821a37adfa725e60e40f4f71176"}, "state": "COMMENTED", "comments": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOFQwMzo0NToxOVrOHpY-Vg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOFQwNDozMzowNFrOHpZs_w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzE2MjgzOA==", "bodyText": "Why UTC_TIMEZONE option is not in the set?", "url": "https://github.com/apache/flink/pull/13605#discussion_r513162838", "createdAt": "2020-10-28T03:45:19Z", "author": {"login": "wuchong"}, "path": "flink-formats/flink-parquet/src/main/java/org/apache/flink/formats/parquet/ParquetFileSystemFormatFactory.java", "diffHunk": "@@ -93,124 +117,17 @@ private static Configuration getParquetConfiguration(ReadableConfig options) {\n \t}\n \n \t@Override\n-\tpublic InputFormat<RowData, ?> createReader(ReaderContext context) {\n-\t\treturn new ParquetInputFormat(\n-\t\t\t\tcontext.getPaths(),\n-\t\t\t\tcontext.getSchema().getFieldNames(),\n-\t\t\t\tcontext.getSchema().getFieldDataTypes(),\n-\t\t\t\tcontext.getProjectFields(),\n-\t\t\t\tcontext.getDefaultPartName(),\n-\t\t\t\tcontext.getPushedDownLimit(),\n-\t\t\t\tgetParquetConfiguration(context.getFormatOptions()),\n-\t\t\t\tcontext.getFormatOptions().get(UTC_TIMEZONE));\n+\tpublic String factoryIdentifier() {\n+\t\treturn \"parquet\";\n \t}\n \n \t@Override\n-\tpublic Optional<BulkWriter.Factory<RowData>> createBulkWriterFactory(WriterContext context) {\n-\t\treturn Optional.of(ParquetRowDataBuilder.createWriterFactory(\n-\t\t\t\tRowType.of(Arrays.stream(context.getFormatFieldTypes())\n-\t\t\t\t\t\t\t\t.map(DataType::getLogicalType)\n-\t\t\t\t\t\t\t\t.toArray(LogicalType[]::new),\n-\t\t\t\t\t\tcontext.getFormatFieldNames()),\n-\t\t\t\tgetParquetConfiguration(context.getFormatOptions()),\n-\t\t\t\tcontext.getFormatOptions().get(UTC_TIMEZONE)));\n+\tpublic Set<ConfigOption<?>> requiredOptions() {\n+\t\treturn new HashSet<>();\n \t}\n \n \t@Override\n-\tpublic Optional<Encoder<RowData>> createEncoder(WriterContext context) {\n-\t\treturn Optional.empty();\n-\t}\n-\n-\t/**\n-\t * An implementation of {@link ParquetInputFormat} to read {@link RowData} records\n-\t * from Parquet files.\n-\t */\n-\tpublic static class ParquetInputFormat extends FileInputFormat<RowData> {\n-\n-\t\tprivate static final long serialVersionUID = 1L;\n-\n-\t\tprivate final String[] fullFieldNames;\n-\t\tprivate final DataType[] fullFieldTypes;\n-\t\tprivate final int[] selectedFields;\n-\t\tprivate final String partDefaultName;\n-\t\tprivate final boolean utcTimestamp;\n-\t\tprivate final SerializableConfiguration conf;\n-\t\tprivate final long limit;\n-\n-\t\tprivate transient ParquetColumnarRowSplitReader reader;\n-\t\tprivate transient long currentReadCount;\n-\n-\t\tpublic ParquetInputFormat(\n-\t\t\t\tPath[] paths,\n-\t\t\t\tString[] fullFieldNames,\n-\t\t\t\tDataType[] fullFieldTypes,\n-\t\t\t\tint[] selectedFields,\n-\t\t\t\tString partDefaultName,\n-\t\t\t\tlong limit,\n-\t\t\t\tConfiguration conf,\n-\t\t\t\tboolean utcTimestamp) {\n-\t\t\tsuper.setFilePaths(paths);\n-\t\t\tthis.limit = limit;\n-\t\t\tthis.partDefaultName = partDefaultName;\n-\t\t\tthis.fullFieldNames = fullFieldNames;\n-\t\t\tthis.fullFieldTypes = fullFieldTypes;\n-\t\t\tthis.selectedFields = selectedFields;\n-\t\t\tthis.conf = new SerializableConfiguration(conf);\n-\t\t\tthis.utcTimestamp = utcTimestamp;\n-\t\t}\n-\n-\t\t@Override\n-\t\tpublic void open(FileInputSplit fileSplit) throws IOException {\n-\t\t\t// generate partition specs.\n-\t\t\tList<String> fieldNameList = Arrays.asList(fullFieldNames);\n-\t\t\tLinkedHashMap<String, String> partSpec = PartitionPathUtils.extractPartitionSpecFromPath(\n-\t\t\t\t\tfileSplit.getPath());\n-\t\t\tLinkedHashMap<String, Object> partObjects = new LinkedHashMap<>();\n-\t\t\tpartSpec.forEach((k, v) -> partObjects.put(k, restorePartValueFromType(\n-\t\t\t\t\tpartDefaultName.equals(v) ? null : v,\n-\t\t\t\t\tfullFieldTypes[fieldNameList.indexOf(k)])));\n-\n-\t\t\tthis.reader = ParquetSplitReaderUtil.genPartColumnarRowReader(\n-\t\t\t\t\tutcTimestamp,\n-\t\t\t\t\ttrue,\n-\t\t\t\t\tconf.conf(),\n-\t\t\t\t\tfullFieldNames,\n-\t\t\t\t\tfullFieldTypes,\n-\t\t\t\t\tpartObjects,\n-\t\t\t\t\tselectedFields,\n-\t\t\t\t\tDEFAULT_SIZE,\n-\t\t\t\t\tnew Path(fileSplit.getPath().toString()),\n-\t\t\t\t\tfileSplit.getStart(),\n-\t\t\t\t\tfileSplit.getLength());\n-\t\t\tthis.currentReadCount = 0L;\n-\t\t}\n-\n-\t\t@Override\n-\t\tpublic boolean supportsMultiPaths() {\n-\t\t\treturn true;\n-\t\t}\n-\n-\t\t@Override\n-\t\tpublic boolean reachedEnd() throws IOException {\n-\t\t\tif (currentReadCount >= limit) {\n-\t\t\t\treturn true;\n-\t\t\t} else {\n-\t\t\t\treturn reader.reachedEnd();\n-\t\t\t}\n-\t\t}\n-\n-\t\t@Override\n-\t\tpublic RowData nextRecord(RowData reuse) {\n-\t\t\tcurrentReadCount++;\n-\t\t\treturn reader.nextRecord();\n-\t\t}\n-\n-\t\t@Override\n-\t\tpublic void close() throws IOException {\n-\t\t\tif (reader != null) {\n-\t\t\t\tthis.reader.close();\n-\t\t\t}\n-\t\t\tthis.reader = null;\n-\t\t}\n+\tpublic Set<ConfigOption<?>> optionalOptions() {\n+\t\treturn new HashSet<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "05116d4768052821a37adfa725e60e40f4f71176"}, "originalPosition": 250}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzE2NDkzOA==", "bodyText": "Remove this comment?", "url": "https://github.com/apache/flink/pull/13605#discussion_r513164938", "createdAt": "2020-10-28T03:53:08Z", "author": {"login": "wuchong"}, "path": "flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/BulkFormatFactory.java", "diffHunk": "@@ -0,0 +1,42 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.factories;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.configuration.ReadableConfig;\n+import org.apache.flink.connector.file.src.reader.BulkFormat;\n+import org.apache.flink.table.connector.format.BulkDecodingFormat;\n+import org.apache.flink.table.data.RowData;\n+\n+/**\n+ * Base interface for configuring a {@link BulkFormat} for file system connector.\n+ *\n+ * @see FactoryUtil#createTableFactoryHelper(DynamicTableFactory, DynamicTableFactory.Context)\n+ */\n+@Internal\n+public interface BulkFormatFactory extends DecodingFormatFactory<BulkFormat<RowData>> {\n+\t// interface is used for discovery but is already fully specified by the generics", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "05116d4768052821a37adfa725e60e40f4f71176"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzE2NTM2OA==", "bodyText": "How about naming this BulkReaderFormatFactory which is more align with BulkWriterFormatFactory.", "url": "https://github.com/apache/flink/pull/13605#discussion_r513165368", "createdAt": "2020-10-28T03:54:47Z", "author": {"login": "wuchong"}, "path": "flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/BulkFormatFactory.java", "diffHunk": "@@ -0,0 +1,42 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.factories;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.configuration.ReadableConfig;\n+import org.apache.flink.connector.file.src.reader.BulkFormat;\n+import org.apache.flink.table.connector.format.BulkDecodingFormat;\n+import org.apache.flink.table.data.RowData;\n+\n+/**\n+ * Base interface for configuring a {@link BulkFormat} for file system connector.\n+ *\n+ * @see FactoryUtil#createTableFactoryHelper(DynamicTableFactory, DynamicTableFactory.Context)\n+ */\n+@Internal\n+public interface BulkFormatFactory extends DecodingFormatFactory<BulkFormat<RowData>> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "05116d4768052821a37adfa725e60e40f4f71176"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzE2NTQ2OA==", "bodyText": "How about naming this BulkWriterFormatFactory which is more align with BulkReaderFormatFactory.", "url": "https://github.com/apache/flink/pull/13605#discussion_r513165468", "createdAt": "2020-10-28T03:55:10Z", "author": {"login": "wuchong"}, "path": "flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/BulkWriterFactory.java", "diffHunk": "@@ -0,0 +1,33 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.factories;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.table.data.RowData;\n+\n+/**\n+ * Base interface for configuring a {@link BulkWriter.Factory} for file system connector.\n+ *\n+ * @see FactoryUtil#createTableFactoryHelper(DynamicTableFactory, DynamicTableFactory.Context)\n+ */\n+@Internal\n+public interface BulkWriterFactory extends EncodingFormatFactory<BulkWriter.Factory<RowData>> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "05116d4768052821a37adfa725e60e40f4f71176"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzE2NjIzMA==", "bodyText": "Do we really need this factory? It seems duplicate with the SerializationSchema. I'm afraid we will introduce a lot of duplicate codes if we can't reuse existing SerializationFormatFactorys.", "url": "https://github.com/apache/flink/pull/13605#discussion_r513166230", "createdAt": "2020-10-28T03:58:34Z", "author": {"login": "wuchong"}, "path": "flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/EncoderFactory.java", "diffHunk": "@@ -0,0 +1,33 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.factories;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.table.data.RowData;\n+\n+/**\n+ * Base interface for configuring a {@link Encoder} for file system connector.\n+ *\n+ * @see FactoryUtil#createTableFactoryHelper(DynamicTableFactory, DynamicTableFactory.Context)\n+ */\n+@Internal\n+public interface EncoderFactory extends EncodingFormatFactory<Encoder<RowData>> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "05116d4768052821a37adfa725e60e40f4f71176"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzE2NzY1NA==", "bodyText": "Why not reuse FactoryUtil.TableFactoryHelper#discoverOptionalEncodingFormat?", "url": "https://github.com/apache/flink/pull/13605#discussion_r513167654", "createdAt": "2020-10-28T04:04:30Z", "author": {"login": "wuchong"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/AbstractFileSystemTable.java", "diffHunk": "@@ -53,15 +62,75 @@\n \t\tcontext.getCatalogTable().getOptions().forEach(tableOptions::setString);\n \t\tthis.schema = TableSchemaUtils.getPhysicalSchema(context.getCatalogTable().getSchema());\n \t\tthis.partitionKeys = context.getCatalogTable().getPartitionKeys();\n-\t\tthis.path = new Path(context.getCatalogTable().getOptions().getOrDefault(PATH.key(), PATH.defaultValue()));\n-\t\tthis.defaultPartName = context.getCatalogTable().getOptions().getOrDefault(\n-\t\t\t\tPARTITION_DEFAULT_NAME.key(), PARTITION_DEFAULT_NAME.defaultValue());\n+\t\tthis.path = new Path(tableOptions.get(PATH));\n+\t\tthis.defaultPartName = tableOptions.get(PARTITION_DEFAULT_NAME);\n \t}\n \n-\tstatic FileSystemFormatFactory createFormatFactory(ReadableConfig tableOptions) {\n+\tReadableConfig formatOptions(String identifier) {\n+\t\treturn new DelegatingConfiguration(tableOptions, identifier + \".\");\n+\t}\n+\n+\tFileSystemFormatFactory createFormatFactory() {\n \t\treturn FactoryUtil.discoverFactory(\n \t\t\t\tThread.currentThread().getContextClassLoader(),\n \t\t\t\tFileSystemFormatFactory.class,\n \t\t\t\ttableOptions.get(FactoryUtil.FORMAT));\n \t}\n+\n+\t@SuppressWarnings(\"rawtypes\")\n+\t<F extends EncodingFormatFactory<?>> Optional<EncodingFormat> discoverOptionalEncodingFormat(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "05116d4768052821a37adfa725e60e40f4f71176"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzE3MTE2NQ==", "bodyText": "Why this is still needed? Do we need to migrate all the formats to use EncodingFormatFactory and DecodingFormatFactory before we can remove these code?  If yes, could you create an issue for that?", "url": "https://github.com/apache/flink/pull/13605#discussion_r513171165", "createdAt": "2020-10-28T04:18:55Z", "author": {"login": "wuchong"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSink.java", "diffHunk": "@@ -199,15 +202,32 @@ private Path toStagingPath() {\n \t}\n \n \t@SuppressWarnings(\"unchecked\")\n-\tprivate OutputFormatFactory<RowData> createOutputFormatFactory() {\n-\t\tObject writer = createWriter();\n+\tprivate OutputFormatFactory<RowData> createOutputFormatFactory(Context sinkContext) {\n+\t\tObject writer = createWriter(sinkContext);\n \t\treturn writer instanceof Encoder ?\n \t\t\t\tpath -> createEncoderOutputFormat((Encoder<RowData>) writer, path) :\n \t\t\t\tpath -> createBulkWriterOutputFormat((BulkWriter.Factory<RowData>) writer, path);\n \t}\n \n-\tprivate Object createWriter() {\n-\t\tFileSystemFormatFactory formatFactory = createFormatFactory(tableOptions);\n+\tprivate DataType getFormatDataType() {\n+\t\tTableSchema.Builder builder = TableSchema.builder();\n+\t\tschema.getTableColumns().forEach(column -> {\n+\t\t\tif (!partitionKeys.contains(column.getName())) {\n+\t\t\t\tbuilder.add(column);\n+\t\t\t}\n+\t\t});\n+\t\treturn builder.build().toRowDataType();\n+\t}\n+\n+\tprivate Object createWriter(Context sinkContext) {\n+\t\t@SuppressWarnings(\"rawtypes\")\n+\t\tOptional<EncodingFormat> encodingFormat = discoverOptionalEncodingFormat(BulkWriterFactory.class)\n+\t\t\t\t.map(Optional::of).orElseGet(() -> discoverOptionalEncodingFormat(EncoderFactory.class));\n+\t\tif (encodingFormat.isPresent()) {\n+\t\t\treturn encodingFormat.get().createRuntimeEncoder(sinkContext, getFormatDataType());\n+\t\t}\n+\n+\t\tFileSystemFormatFactory formatFactory = createFormatFactory();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "05116d4768052821a37adfa725e60e40f4f71176"}, "originalPosition": 100}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzE3Mzk2OQ==", "bodyText": "Why we should avoid using ContinuousFileMonitoringFunction here? and why not return SourceFunctionProvider of InputFormatSourceFunction directly?", "url": "https://github.com/apache/flink/pull/13605#discussion_r513173969", "createdAt": "2020-10-28T04:30:04Z", "author": {"login": "wuchong"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSource.java", "diffHunk": "@@ -88,7 +92,16 @@ private FileSystemTableSource(\n \t}\n \n \t@Override\n-\tpublic ScanRuntimeProvider getScanRuntimeProvider(ScanContext runtimeProviderContext) {\n+\tpublic ScanRuntimeProvider getScanRuntimeProvider(ScanContext scanContext) {\n+\t\tOptional<BulkDecodingFormat<RowData>> bulkDecodingFormat = discoverBulkDecodingFormat();\n+\n+\t\tif (!partitionKeys.isEmpty() && getOrFetchPartitions().isEmpty()) {\n+\t\t\t// When this table has no partition, just return a empty source.\n+\t\t\treturn InputFormatProvider.of(new CollectionInputFormat<>(new ArrayList<>(), null));\n+\t\t} else if (bulkDecodingFormat.isPresent()) {\n+\t\t\treturn SourceProvider.of(createBulkFormatSource(bulkDecodingFormat.get(), scanContext));\n+\t\t}\n+\n \t\treturn new DataStreamScanProvider() {\n \t\t\t@Override\n \t\t\tpublic DataStream<RowData> produceDataStream(StreamExecutionEnvironment execEnv) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "05116d4768052821a37adfa725e60e40f4f71176"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzE3NDc4Mw==", "bodyText": "I think Long.MAX_VALUE can't represent no limit, right?", "url": "https://github.com/apache/flink/pull/13605#discussion_r513174783", "createdAt": "2020-10-28T04:33:04Z", "author": {"login": "wuchong"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSource.java", "diffHunk": "@@ -108,13 +121,39 @@ public boolean isBounded() {\n \t\t};\n \t}\n \n-\tprivate InputFormat<RowData, ?> getInputFormat() {\n-\t\t// When this table has no partition, just return a empty source.\n-\t\tif (!partitionKeys.isEmpty() && getOrFetchPartitions().isEmpty()) {\n-\t\t\treturn new CollectionInputFormat<>(new ArrayList<>(), null);\n+\tprivate FileSource<RowData> createBulkFormatSource(\n+\t\t\tBulkDecodingFormat<RowData> decodingFormat, ScanContext scanContext) {\n+\t\tdecodingFormat.applyLimit(pushedDownLimit());\n+\t\tdecodingFormat.applyFilters(pushedDownFilters());\n+\t\tBulkFormat<RowData> bulkFormat = decodingFormat.createRuntimeDecoder(\n+\t\t\t\tscanContext, getProducedDataType());\n+\t\tFileSource.FileSourceBuilder<RowData> builder = FileSource\n+\t\t\t\t.forBulkFileFormat(bulkFormat, paths());\n+\t\treturn builder.build();\n+\t}\n+\n+\tprivate Path[] paths() {\n+\t\tif (partitionKeys.isEmpty()) {\n+\t\t\treturn new Path[] {path};\n+\t\t} else {\n+\t\t\treturn getOrFetchPartitions().stream()\n+\t\t\t\t\t.map(FileSystemTableSource.this::toFullLinkedPartSpec)\n+\t\t\t\t\t.map(PartitionPathUtils::generatePartitionPath)\n+\t\t\t\t\t.map(n -> new Path(path, n))\n+\t\t\t\t\t.toArray(Path[]::new);\n \t\t}\n+\t}\n+\n+\tprivate long pushedDownLimit() {\n+\t\treturn limit == null ? Long.MAX_VALUE : limit;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "05116d4768052821a37adfa725e60e40f4f71176"}, "originalPosition": 73}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b56cf8831300d22fa4777f3f65dd6236ca39f8b5", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/flink/commit/b56cf8831300d22fa4777f3f65dd6236ca39f8b5", "committedDate": "2020-10-28T07:14:55Z", "message": "[FLINK-19599][table] Introduce Filesystem format factories to integrate new FileSource to table"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6fdf4fe5293ba62a0dd946bfd050b19e575799f6", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/flink/commit/6fdf4fe5293ba62a0dd946bfd050b19e575799f6", "committedDate": "2020-10-28T07:14:55Z", "message": "Refactor create format factory"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "61fed9445fbd820423b73ef520d5f9ffc9e0606d", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/flink/commit/61fed9445fbd820423b73ef520d5f9ffc9e0606d", "committedDate": "2020-10-28T07:14:56Z", "message": "Address comments"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "05116d4768052821a37adfa725e60e40f4f71176", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/flink/commit/05116d4768052821a37adfa725e60e40f4f71176", "committedDate": "2020-10-27T12:01:12Z", "message": "Refactor create format factory"}, "afterCommit": {"oid": "61fed9445fbd820423b73ef520d5f9ffc9e0606d", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/flink/commit/61fed9445fbd820423b73ef520d5f9ffc9e0606d", "committedDate": "2020-10-28T07:14:56Z", "message": "Address comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "47e39b41aa88aa9887046123bcc561dad3340374", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/flink/commit/47e39b41aa88aa9887046123bcc561dad3340374", "committedDate": "2020-10-28T08:02:46Z", "message": "Pass SerializationSchemaFormat into source/sink"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fa0dc913f56aa2567c30073c044f688f9ed74fee", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/flink/commit/fa0dc913f56aa2567c30073c044f688f9ed74fee", "committedDate": "2020-10-28T08:21:48Z", "message": "Fix exceptions"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "96468e31f7614ef314c655f97a63fcabe83505a8", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/flink/commit/96468e31f7614ef314c655f97a63fcabe83505a8", "committedDate": "2020-10-28T11:14:39Z", "message": "checkstyle"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE4NTg0NzA3", "url": "https://github.com/apache/flink/pull/13605#pullrequestreview-518584707", "createdAt": "2020-10-28T11:58:41Z", "commit": {"oid": "96468e31f7614ef314c655f97a63fcabe83505a8"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5ef88c8f400b6c2ef5f5ca6bbd750c17f27137c7", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/flink/commit/5ef88c8f400b6c2ef5f5ca6bbd750c17f27137c7", "committedDate": "2020-10-29T02:38:32Z", "message": "Move TestCsvFileSystemFormatFactory to table runtime"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3195, "cost": 1, "resetAt": "2021-10-28T16:48:13Z"}}}