{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzk2MTI5NTM1", "number": 11574, "reviewThreads": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMlQwMzo0MDowM1rODtyL0w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QwNTo0NjozOVrODuNyQw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ5MzM0NzM5OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemFormatFactory.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMlQwMzo0MDowM1rOF_aPHA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMlQwNTo0Mzo1M1rOF_cB9g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjAzNDQ2MA==", "bodyText": "using two optional interface looks strange\uff0ccould we make them to one like createWriter(WriterContext) ?", "url": "https://github.com/apache/flink/pull/11574#discussion_r402034460", "createdAt": "2020-04-02T03:40:03Z", "author": {"login": "leonardBang"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemFormatFactory.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.dataformat.BaseRow;\n+import org.apache.flink.table.expressions.Expression;\n+import org.apache.flink.table.factories.TableFormatFactory;\n+import org.apache.flink.table.types.DataType;\n+\n+import java.util.List;\n+import java.util.Optional;\n+\n+/**\n+ * File system format factory for creating configured instances of reader and writer.\n+ */\n+@Internal\n+public interface FileSystemFormatFactory extends TableFormatFactory<BaseRow> {\n+\n+\t/**\n+\t * Create {@link InputFormat} reader.\n+\t */\n+\tInputFormat<BaseRow, ?> createReader(ReaderContext context);\n+\n+\t/**\n+\t * Create {@link Encoder} writer.\n+\t */\n+\tOptional<Encoder<BaseRow>> createEncoder(WriterContext context);\n+\n+\t/**\n+\t * Create {@link BulkWriter.Factory} writer.\n+\t */\n+\tOptional<BulkWriter.Factory<BaseRow>> createBulkWriterFactory(WriterContext context);\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd8a10137fa50eb11035082c95878fb6aa6a66c6"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjA2Mzg2Mg==", "bodyText": "I have tried to unify them to a method, but return value is hard to define, if I just define a generic T, this should be every object. So I define two methods to formats.", "url": "https://github.com/apache/flink/pull/11574#discussion_r402063862", "createdAt": "2020-04-02T05:43:53Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemFormatFactory.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.dataformat.BaseRow;\n+import org.apache.flink.table.expressions.Expression;\n+import org.apache.flink.table.factories.TableFormatFactory;\n+import org.apache.flink.table.types.DataType;\n+\n+import java.util.List;\n+import java.util.Optional;\n+\n+/**\n+ * File system format factory for creating configured instances of reader and writer.\n+ */\n+@Internal\n+public interface FileSystemFormatFactory extends TableFormatFactory<BaseRow> {\n+\n+\t/**\n+\t * Create {@link InputFormat} reader.\n+\t */\n+\tInputFormat<BaseRow, ?> createReader(ReaderContext context);\n+\n+\t/**\n+\t * Create {@link Encoder} writer.\n+\t */\n+\tOptional<Encoder<BaseRow>> createEncoder(WriterContext context);\n+\n+\t/**\n+\t * Create {@link BulkWriter.Factory} writer.\n+\t */\n+\tOptional<BulkWriter.Factory<BaseRow>> createBulkWriterFactory(WriterContext context);\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjAzNDQ2MA=="}, "originalCommit": {"oid": "cd8a10137fa50eb11035082c95878fb6aa6a66c6"}, "originalPosition": 55}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ5MzM0OTQ1OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableFactory.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMlQwMzo0MToxOFrOF_aQSA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMlQwMzo0MToxOFrOF_aQSA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjAzNDc2MA==", "bodyText": "use lowercase <p>", "url": "https://github.com/apache/flink/pull/11574#discussion_r402034760", "createdAt": "2020-04-02T03:41:18Z", "author": {"login": "leonardBang"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableFactory.java", "diffHunk": "@@ -0,0 +1,129 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.configuration.ConfigOption;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.dataformat.BaseRow;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.TableFactory;\n+import org.apache.flink.table.factories.TableFactoryService;\n+import org.apache.flink.table.factories.TableSinkFactory;\n+import org.apache.flink.table.factories.TableSourceFactory;\n+import org.apache.flink.table.sinks.TableSink;\n+import org.apache.flink.table.sources.TableSource;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.apache.flink.configuration.ConfigOptions.key;\n+import static org.apache.flink.table.descriptors.ConnectorDescriptorValidator.CONNECTOR;\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+import static org.apache.flink.table.descriptors.Schema.SCHEMA;\n+\n+/**\n+ * File system {@link TableFactory}.\n+ *\n+ * <P>File system support:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd8a10137fa50eb11035082c95878fb6aa6a66c6"}, "originalPosition": 45}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ5MzM1NDYwOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableFactory.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMlQwMzo0NDozN1rOF_aTPg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMlQwNTo0NjozMlrOF_cEyg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjAzNTUxOA==", "bodyText": "statement * 1. The partition ... is not a support but an agreement, we can place in new paragraph.", "url": "https://github.com/apache/flink/pull/11574#discussion_r402035518", "createdAt": "2020-04-02T03:44:37Z", "author": {"login": "leonardBang"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableFactory.java", "diffHunk": "@@ -0,0 +1,129 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.configuration.ConfigOption;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.dataformat.BaseRow;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.TableFactory;\n+import org.apache.flink.table.factories.TableFactoryService;\n+import org.apache.flink.table.factories.TableSinkFactory;\n+import org.apache.flink.table.factories.TableSourceFactory;\n+import org.apache.flink.table.sinks.TableSink;\n+import org.apache.flink.table.sources.TableSource;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.apache.flink.configuration.ConfigOptions.key;\n+import static org.apache.flink.table.descriptors.ConnectorDescriptorValidator.CONNECTOR;\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+import static org.apache.flink.table.descriptors.Schema.SCHEMA;\n+\n+/**\n+ * File system {@link TableFactory}.\n+ *\n+ * <P>File system support:\n+ * 1.The partition information should be in the file system path, whether it's a temporary", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd8a10137fa50eb11035082c95878fb6aa6a66c6"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjA2NDU4Ng==", "bodyText": "I remove \"File system support\"", "url": "https://github.com/apache/flink/pull/11574#discussion_r402064586", "createdAt": "2020-04-02T05:46:32Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableFactory.java", "diffHunk": "@@ -0,0 +1,129 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.configuration.ConfigOption;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.dataformat.BaseRow;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.TableFactory;\n+import org.apache.flink.table.factories.TableFactoryService;\n+import org.apache.flink.table.factories.TableSinkFactory;\n+import org.apache.flink.table.factories.TableSourceFactory;\n+import org.apache.flink.table.sinks.TableSink;\n+import org.apache.flink.table.sources.TableSource;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.apache.flink.configuration.ConfigOptions.key;\n+import static org.apache.flink.table.descriptors.ConnectorDescriptorValidator.CONNECTOR;\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+import static org.apache.flink.table.descriptors.Schema.SCHEMA;\n+\n+/**\n+ * File system {@link TableFactory}.\n+ *\n+ * <P>File system support:\n+ * 1.The partition information should be in the file system path, whether it's a temporary", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjAzNTUxOA=="}, "originalCommit": {"oid": "cd8a10137fa50eb11035082c95878fb6aa6a66c6"}, "originalPosition": 46}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ5MzM3NTEyOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableFactory.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMlQwMzo1ODoyMVrOF_afHg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMlQwMzo1ODoyMVrOF_afHg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjAzODU1OA==", "bodyText": "Here we change the CONNECTOR_VALUE\u3001PATH different with FileSystemValidator,\n public static final String CONNECTOR_TYPE_VALUE = \"filesystem\"; public static final String CONNECTOR_PATH = \"connector.path\";\nIIUC, you want to avoid conflict with current CsvTableSinkFactoryBase/CsvTableSourceFactoryBase, so if user use  path can route to this factory, use connector.path will route to oldfactory(i.e.CsvTableSinkFactoryBase/CsvTableSourceFactoryBase )\nThis is a new feature and face to user, I think we'd better add docs.", "url": "https://github.com/apache/flink/pull/11574#discussion_r402038558", "createdAt": "2020-04-02T03:58:21Z", "author": {"login": "leonardBang"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableFactory.java", "diffHunk": "@@ -0,0 +1,129 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.configuration.ConfigOption;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.dataformat.BaseRow;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.TableFactory;\n+import org.apache.flink.table.factories.TableFactoryService;\n+import org.apache.flink.table.factories.TableSinkFactory;\n+import org.apache.flink.table.factories.TableSourceFactory;\n+import org.apache.flink.table.sinks.TableSink;\n+import org.apache.flink.table.sources.TableSource;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.apache.flink.configuration.ConfigOptions.key;\n+import static org.apache.flink.table.descriptors.ConnectorDescriptorValidator.CONNECTOR;\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+import static org.apache.flink.table.descriptors.Schema.SCHEMA;\n+\n+/**\n+ * File system {@link TableFactory}.\n+ *\n+ * <P>File system support:\n+ * 1.The partition information should be in the file system path, whether it's a temporary\n+ * table or a catalog table.\n+ * 2.Support insert into (append) and insert overwrite.\n+ * 3.Support static and dynamic partition inserting.\n+ */\n+public class FileSystemTableFactory implements\n+\t\tTableSourceFactory<BaseRow>,\n+\t\tTableSinkFactory<BaseRow> {\n+\n+\tpublic static final String CONNECTOR_VALUE = \"filesystem\";\n+\tpublic static final String PATH = \"path\";\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd8a10137fa50eb11035082c95878fb6aa6a66c6"}, "originalPosition": 57}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ5MzM3NzI1OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableFactory.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMlQwMzo1OTozOFrOF_agUw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMlQwNTo1Nzo0N1rOF_cSGw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjAzODg2Nw==", "bodyText": "Do some basic validate properties here to align with other tableSource/Sink Factory?\nI think we can do validate in FileSystemValidator\uff0c and also move fields CONNECTOR_VALUE\u3001PATH\u3001PARTITION_DEFAULT_NAME to FileSystemValidator`\uff0c maybe this will make the class more clear?", "url": "https://github.com/apache/flink/pull/11574#discussion_r402038867", "createdAt": "2020-04-02T03:59:38Z", "author": {"login": "leonardBang"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableFactory.java", "diffHunk": "@@ -0,0 +1,129 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.configuration.ConfigOption;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.dataformat.BaseRow;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.TableFactory;\n+import org.apache.flink.table.factories.TableFactoryService;\n+import org.apache.flink.table.factories.TableSinkFactory;\n+import org.apache.flink.table.factories.TableSourceFactory;\n+import org.apache.flink.table.sinks.TableSink;\n+import org.apache.flink.table.sources.TableSource;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.apache.flink.configuration.ConfigOptions.key;\n+import static org.apache.flink.table.descriptors.ConnectorDescriptorValidator.CONNECTOR;\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+import static org.apache.flink.table.descriptors.Schema.SCHEMA;\n+\n+/**\n+ * File system {@link TableFactory}.\n+ *\n+ * <P>File system support:\n+ * 1.The partition information should be in the file system path, whether it's a temporary\n+ * table or a catalog table.\n+ * 2.Support insert into (append) and insert overwrite.\n+ * 3.Support static and dynamic partition inserting.\n+ */\n+public class FileSystemTableFactory implements\n+\t\tTableSourceFactory<BaseRow>,\n+\t\tTableSinkFactory<BaseRow> {\n+\n+\tpublic static final String CONNECTOR_VALUE = \"filesystem\";\n+\tpublic static final String PATH = \"path\";\n+\n+\tpublic static final ConfigOption<String> PARTITION_DEFAULT_NAME = key(\"partition.default-name\")\n+\t\t\t.stringType()\n+\t\t\t.defaultValue(\"__DEFAULT_PARTITION__\")\n+\t\t\t.withDescription(\"The default partition name in case the dynamic partition\" +\n+\t\t\t\t\t\" column value is null/empty string\");\n+\n+\t@Override\n+\tpublic Map<String, String> requiredContext() {\n+\t\tMap<String, String> context = new HashMap<>();\n+\t\tcontext.put(CONNECTOR, CONNECTOR_VALUE);\n+\t\treturn context;\n+\t}\n+\n+\t@Override\n+\tpublic List<String> supportedProperties() {\n+\t\tList<String> properties = new ArrayList<>();\n+\n+\t\t// path\n+\t\tproperties.add(PATH);\n+\n+\t\t// schema\n+\t\tproperties.add(SCHEMA + \".#.\" + DescriptorProperties.TABLE_SCHEMA_DATA_TYPE);\n+\t\tproperties.add(SCHEMA + \".#.\" + DescriptorProperties.TABLE_SCHEMA_NAME);\n+\n+\t\tproperties.add(PARTITION_DEFAULT_NAME.key());\n+\n+\t\t// format\n+\t\tproperties.add(FORMAT);\n+\t\tproperties.add(FORMAT + \".*\");\n+\n+\t\treturn properties;\n+\t}\n+\n+\t@Override\n+\tpublic TableSource<BaseRow> createTableSource(TableSourceFactory.Context context) {\n+\t\tDescriptorProperties properties = new DescriptorProperties();\n+\t\tproperties.putProperties(context.getTable().getProperties());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd8a10137fa50eb11035082c95878fb6aa6a66c6"}, "originalPosition": 94}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjA2Nzk5NQ==", "bodyText": "We can, but not now, after https://issues.apache.org/jira/browse/FLINK-16904 , we can move them to validator.", "url": "https://github.com/apache/flink/pull/11574#discussion_r402067995", "createdAt": "2020-04-02T05:57:47Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableFactory.java", "diffHunk": "@@ -0,0 +1,129 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.configuration.ConfigOption;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.dataformat.BaseRow;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.TableFactory;\n+import org.apache.flink.table.factories.TableFactoryService;\n+import org.apache.flink.table.factories.TableSinkFactory;\n+import org.apache.flink.table.factories.TableSourceFactory;\n+import org.apache.flink.table.sinks.TableSink;\n+import org.apache.flink.table.sources.TableSource;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.apache.flink.configuration.ConfigOptions.key;\n+import static org.apache.flink.table.descriptors.ConnectorDescriptorValidator.CONNECTOR;\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+import static org.apache.flink.table.descriptors.Schema.SCHEMA;\n+\n+/**\n+ * File system {@link TableFactory}.\n+ *\n+ * <P>File system support:\n+ * 1.The partition information should be in the file system path, whether it's a temporary\n+ * table or a catalog table.\n+ * 2.Support insert into (append) and insert overwrite.\n+ * 3.Support static and dynamic partition inserting.\n+ */\n+public class FileSystemTableFactory implements\n+\t\tTableSourceFactory<BaseRow>,\n+\t\tTableSinkFactory<BaseRow> {\n+\n+\tpublic static final String CONNECTOR_VALUE = \"filesystem\";\n+\tpublic static final String PATH = \"path\";\n+\n+\tpublic static final ConfigOption<String> PARTITION_DEFAULT_NAME = key(\"partition.default-name\")\n+\t\t\t.stringType()\n+\t\t\t.defaultValue(\"__DEFAULT_PARTITION__\")\n+\t\t\t.withDescription(\"The default partition name in case the dynamic partition\" +\n+\t\t\t\t\t\" column value is null/empty string\");\n+\n+\t@Override\n+\tpublic Map<String, String> requiredContext() {\n+\t\tMap<String, String> context = new HashMap<>();\n+\t\tcontext.put(CONNECTOR, CONNECTOR_VALUE);\n+\t\treturn context;\n+\t}\n+\n+\t@Override\n+\tpublic List<String> supportedProperties() {\n+\t\tList<String> properties = new ArrayList<>();\n+\n+\t\t// path\n+\t\tproperties.add(PATH);\n+\n+\t\t// schema\n+\t\tproperties.add(SCHEMA + \".#.\" + DescriptorProperties.TABLE_SCHEMA_DATA_TYPE);\n+\t\tproperties.add(SCHEMA + \".#.\" + DescriptorProperties.TABLE_SCHEMA_NAME);\n+\n+\t\tproperties.add(PARTITION_DEFAULT_NAME.key());\n+\n+\t\t// format\n+\t\tproperties.add(FORMAT);\n+\t\tproperties.add(FORMAT + \".*\");\n+\n+\t\treturn properties;\n+\t}\n+\n+\t@Override\n+\tpublic TableSource<BaseRow> createTableSource(TableSourceFactory.Context context) {\n+\t\tDescriptorProperties properties = new DescriptorProperties();\n+\t\tproperties.putProperties(context.getTable().getProperties());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjAzODg2Nw=="}, "originalCommit": {"oid": "cd8a10137fa50eb11035082c95878fb6aa6a66c6"}, "originalPosition": 94}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ5MzQwMTc2OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/RowDataPartitionComputer.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMlQwNDoxNjo1NFrOF_auvA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMlQwNTo1OToyMFrOF_cUGw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjA0MjU1Ng==", "bodyText": "could we rename to BaseRowPartitionComputer before FLIP-95? otherwise the note will not match the name.", "url": "https://github.com/apache/flink/pull/11574#discussion_r402042556", "createdAt": "2020-04-02T04:16:54Z", "author": {"login": "leonardBang"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/RowDataPartitionComputer.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.table.dataformat.BaseRow;\n+import org.apache.flink.table.dataformat.GenericRow;\n+import org.apache.flink.table.dataformat.TypeGetterSetters;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+\n+import java.util.Arrays;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+/**\n+ * {@link PartitionComputer} for {@link BaseRow}.\n+ */\n+@Internal\n+public class RowDataPartitionComputer implements PartitionComputer<BaseRow> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd8a10137fa50eb11035082c95878fb6aa6a66c6"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjA2ODUwNw==", "bodyText": "I don't want to another rename... I think even RowData for now, it can explain the behavior.", "url": "https://github.com/apache/flink/pull/11574#discussion_r402068507", "createdAt": "2020-04-02T05:59:20Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/RowDataPartitionComputer.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.table.dataformat.BaseRow;\n+import org.apache.flink.table.dataformat.GenericRow;\n+import org.apache.flink.table.dataformat.TypeGetterSetters;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+\n+import java.util.Arrays;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+/**\n+ * {@link PartitionComputer} for {@link BaseRow}.\n+ */\n+@Internal\n+public class RowDataPartitionComputer implements PartitionComputer<BaseRow> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjA0MjU1Ng=="}, "originalCommit": {"oid": "cd8a10137fa50eb11035082c95878fb6aa6a66c6"}, "originalPosition": 38}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ5MzQwNzA4OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/utils/TestRowDataCsvInputFormat.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMlQwNDoyMDo1NVrOF_axwg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMlQwNDoyMDo1NVrOF_axwg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjA0MzMzMA==", "bodyText": "The value only from  partition column, use Unsupported partition type to make it clear?", "url": "https://github.com/apache/flink/pull/11574#discussion_r402043330", "createdAt": "2020-04-02T04:20:55Z", "author": {"login": "leonardBang"}, "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/utils/TestRowDataCsvInputFormat.java", "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.utils;\n+\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.io.RichInputFormat;\n+import org.apache.flink.api.common.io.statistics.BaseStatistics;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeinfo.Types;\n+import org.apache.flink.api.java.io.RowCsvInputFormat;\n+import org.apache.flink.api.java.typeutils.RowTypeInfo;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.core.io.InputSplitAssigner;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.dataformat.BaseRow;\n+import org.apache.flink.table.dataformat.BinaryString;\n+import org.apache.flink.table.dataformat.DataFormatConverters;\n+import org.apache.flink.table.dataformat.GenericRow;\n+import org.apache.flink.table.filesystem.PartitionPathUtils;\n+import org.apache.flink.table.types.utils.TypeConversions;\n+import org.apache.flink.types.Row;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * The {@link InputFormat} that output {@link BaseRow}.\n+ */\n+public class TestRowDataCsvInputFormat extends RichInputFormat<BaseRow, FileInputSplit> {\n+\n+\tprivate final List<String> partitionKeys;\n+\tprivate final String defaultPartValue;\n+\tprivate final int[] selectFields;\n+\tprivate final long limit;\n+\tprivate final RowCsvInputFormat inputFormat;\n+\tprivate final List<TypeInformation> fieldTypes;\n+\tprivate final List<String> fieldNames;\n+\tprivate final List<DataFormatConverters.DataFormatConverter> csvSelectConverters;\n+\tprivate final int[] csvFieldMapping;\n+\n+\tprivate transient Row csvRow;\n+\tprivate transient GenericRow row;\n+\tprivate transient long emitted;\n+\n+\tpublic TestRowDataCsvInputFormat(\n+\t\t\tPath[] paths,\n+\t\t\tTableSchema schema,\n+\t\t\tList<String> partitionKeys,\n+\t\t\tString defaultPartValue,\n+\t\t\tint[] selectFields,\n+\t\t\tlong limit) {\n+\t\tthis.partitionKeys = partitionKeys;\n+\t\tthis.defaultPartValue = defaultPartValue;\n+\t\tthis.selectFields = selectFields;\n+\t\tthis.limit = limit;\n+\t\tRowTypeInfo rowType = (RowTypeInfo) schema.toRowType();\n+\t\tthis.fieldTypes = Arrays.asList(rowType.getFieldTypes());\n+\t\tthis.fieldNames = Arrays.asList(rowType.getFieldNames());\n+\n+\t\tList<String> csvFieldNames = fieldNames.stream()\n+\t\t\t\t.filter(name -> !partitionKeys.contains(name)).collect(Collectors.toList());\n+\n+\t\tList<String> selectFieldNames = Arrays.stream(selectFields)\n+\t\t\t\t.mapToObj(fieldNames::get)\n+\t\t\t\t.collect(Collectors.toList());\n+\t\tList<String> csvSelectFieldNames = selectFieldNames.stream()\n+\t\t\t\t.filter(name -> !partitionKeys.contains(name)).collect(Collectors.toList());\n+\t\tList<TypeInformation> csvSelectTypes = csvSelectFieldNames.stream()\n+\t\t\t\t.map(name -> fieldTypes.get(fieldNames.indexOf(name))).collect(Collectors.toList());\n+\t\tthis.csvSelectConverters = csvSelectTypes.stream()\n+\t\t\t\t.map(TypeConversions::fromLegacyInfoToDataType)\n+\t\t\t\t.map(DataFormatConverters::getConverterForDataType)\n+\t\t\t\t.collect(Collectors.toList());\n+\t\tint[] csvSelectFields = csvSelectFieldNames.stream().mapToInt(csvFieldNames::indexOf).toArray();\n+\t\tthis.inputFormat = new RowCsvInputFormat(\n+\t\t\t\tnull, csvSelectTypes.toArray(new TypeInformation[0]), csvSelectFields);\n+\t\tthis.inputFormat.setFilePaths(paths);\n+\n+\t\tthis.csvFieldMapping = csvSelectFieldNames.stream().mapToInt(selectFieldNames::indexOf).toArray();\n+\t\tthis.emitted = 0;\n+\t}\n+\n+\t@Override\n+\tpublic void configure(Configuration parameters) {\n+\t\tinputFormat.configure(parameters);\n+\t}\n+\n+\t@Override\n+\tpublic BaseStatistics getStatistics(BaseStatistics cachedStatistics) throws IOException {\n+\t\treturn inputFormat.getStatistics(cachedStatistics);\n+\t}\n+\n+\t@Override\n+\tpublic FileInputSplit[] createInputSplits(int minNumSplits) throws IOException {\n+\t\treturn inputFormat.createInputSplits(minNumSplits);\n+\t}\n+\n+\t@Override\n+\tpublic InputSplitAssigner getInputSplitAssigner(FileInputSplit[] inputSplits) {\n+\t\treturn inputFormat.getInputSplitAssigner(inputSplits);\n+\t}\n+\n+\t@Override\n+\tpublic void open(FileInputSplit split) throws IOException {\n+\t\tinputFormat.open(split);\n+\t\tPath path = split.getPath();\n+\t\tLinkedHashMap<String, String> partSpec = PartitionPathUtils.extractPartitionSpecFromPath(path);\n+\t\tthis.row = new GenericRow(selectFields.length);\n+\t\tfor (int i = 0; i < selectFields.length; i++) {\n+\t\t\tint selectField = selectFields[i];\n+\t\t\tString name = fieldNames.get(selectField);\n+\t\t\tif (partitionKeys.contains(name)) {\n+\t\t\t\tString value = partSpec.get(name);\n+\t\t\t\tvalue = defaultPartValue.equals(value) ? null : value;\n+\t\t\t\tthis.row.setField(\n+\t\t\t\t\t\ti, convertStringToInternal(value, fieldTypes.get(selectField)));\n+\t\t\t}\n+\t\t}\n+\t\tthis.csvRow = new Row(csvSelectConverters.size());\n+\t}\n+\n+\tprivate Object convertStringToInternal(String value, TypeInformation type) {\n+\t\tif (type.equals(Types.INT)) {\n+\t\t\treturn Integer.parseInt(value);\n+\t\t} else if (type.equals(Types.LONG)) {\n+\t\t\treturn Long.parseLong(value);\n+\t\t} else if (type.equals(Types.STRING)) {\n+\t\t\treturn BinaryString.fromString(value);\n+\t\t} else {\n+\t\t\tthrow new UnsupportedOperationException(\"Unsupported type: \" + type);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd8a10137fa50eb11035082c95878fb6aa6a66c6"}, "originalPosition": 151}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ5MzQxMjQ5OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSink.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMlQwNDoyNDo0MFrOF_a05g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMlQwNDoyNDo0MFrOF_a05g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjA0NDEzNA==", "bodyText": "Add a serialVersionUID?", "url": "https://github.com/apache/flink/pull/11574#discussion_r402044134", "createdAt": "2020-04-02T04:24:40Z", "author": {"login": "leonardBang"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSink.java", "diffHunk": "@@ -0,0 +1,271 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.api.common.io.OutputFormat;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.fs.FSDataOutputStream;\n+import org.apache.flink.core.fs.FileSystem;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.DataStreamSink;\n+import org.apache.flink.table.api.TableException;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.dataformat.BaseRow;\n+import org.apache.flink.table.sinks.AppendStreamTableSink;\n+import org.apache.flink.table.sinks.OverwritableTableSink;\n+import org.apache.flink.table.sinks.PartitionableTableSink;\n+import org.apache.flink.table.sinks.TableSink;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+\n+/**\n+ * File system {@link TableSink}.\n+ */\n+public class FileSystemTableSink implements\n+\t\tAppendStreamTableSink<BaseRow>,\n+\t\tPartitionableTableSink,\n+\t\tOverwritableTableSink {\n+\n+\tprivate final TableSchema schema;\n+\tprivate final List<String> partitionKeys;\n+\tprivate final Path path;\n+\tprivate final String defaultPartName;\n+\tprivate final FileSystemFormatFactory formatFactory;\n+\n+\tprivate boolean overwrite = false;\n+\tprivate boolean dynamicGrouping = false;\n+\tprivate LinkedHashMap<String, String> staticPartitions = new LinkedHashMap<>();\n+\n+\t/**\n+\t * Construct a file system table sink.\n+\t *\n+\t * @param schema schema of the table.\n+\t * @param path directory path of the file system table.\n+\t * @param partitionKeys partition keys of the table.\n+\t * @param defaultPartName The default partition name in case the dynamic partition column value\n+\t *                        is null/empty string.\n+\t * @param formatFactory format factory to create reader.\n+\t */\n+\tpublic FileSystemTableSink(\n+\t\t\tTableSchema schema,\n+\t\t\tPath path,\n+\t\t\tList<String> partitionKeys,\n+\t\t\tString defaultPartName,\n+\t\t\tFileSystemFormatFactory formatFactory) {\n+\t\tthis.schema = schema;\n+\t\tthis.path = path;\n+\t\tthis.defaultPartName = defaultPartName;\n+\t\tthis.formatFactory = formatFactory;\n+\t\tthis.partitionKeys = partitionKeys;\n+\t}\n+\n+\t@Override\n+\tpublic final DataStreamSink<BaseRow> consumeDataStream(DataStream<BaseRow> dataStream) {\n+\t\tRowDataPartitionComputer computer = new RowDataPartitionComputer(\n+\t\t\t\tdefaultPartName,\n+\t\t\t\tschema.getFieldNames(),\n+\t\t\t\tschema.getFieldDataTypes(),\n+\t\t\t\tpartitionKeys.toArray(new String[0]));\n+\n+\t\tFileSystemOutputFormat.Builder<BaseRow> builder = new FileSystemOutputFormat.Builder<>();\n+\t\tbuilder.setPartitionComputer(computer);\n+\t\tbuilder.setDynamicGrouped(dynamicGrouping);\n+\t\tbuilder.setPartitionColumns(partitionKeys.toArray(new String[0]));\n+\t\tbuilder.setFormatFactory(createOutputFormatFactory(\n+\t\t\t\tformatFactory, this::getNonPartitionTypes));\n+\t\tbuilder.setMetaStoreFactory(createTableMetaStoreFactory(path));\n+\t\tbuilder.setOverwrite(overwrite);\n+\t\tbuilder.setStaticPartitions(staticPartitions);\n+\t\tbuilder.setTempPath(toStagingPath());\n+\t\treturn dataStream.writeUsingOutputFormat(builder.build())\n+\t\t\t\t.setParallelism(dataStream.getParallelism());\n+\t}\n+\n+\tprivate DataType[] getNonPartitionTypes() {\n+\t\treturn Arrays.stream(schema.getFieldNames())\n+\t\t\t\t.filter(name -> !partitionKeys.contains(name))\n+\t\t\t\t.map(name -> schema.getFieldDataType(name).get())\n+\t\t\t\t.toArray(DataType[]::new);\n+\t}\n+\n+\tprivate Path toStagingPath() {\n+\t\tPath stagingDir = new Path(path, \".staging_\" + System.currentTimeMillis());\n+\t\ttry {\n+\t\t\tFileSystem fs = stagingDir.getFileSystem();\n+\t\t\tPreconditions.checkState(\n+\t\t\t\t\tfs.exists(stagingDir) || fs.mkdirs(stagingDir),\n+\t\t\t\t\t\"Failed to create staging dir \" + stagingDir);\n+\t\t\treturn stagingDir;\n+\t\t} catch (IOException e) {\n+\t\t\tthrow new RuntimeException(e);\n+\t\t}\n+\t}\n+\n+\tprivate static OutputFormatFactory<BaseRow> createOutputFormatFactory(\n+\t\t\tFileSystemFormatFactory formatFactory,\n+\t\t\tFileSystemFormatFactory.WriterContext context) {\n+\t\tOptional<Encoder<BaseRow>> encoder = formatFactory.createEncoder(context);\n+\t\tOptional<BulkWriter.Factory<BaseRow>> bulk = formatFactory.createBulkWriterFactory(context);\n+\n+\t\tif (!encoder.isPresent() && !bulk.isPresent()) {\n+\t\t\tthrow new TableException(\n+\t\t\t\t\tformatFactory + \" format should implement at least one Encoder or BulkWriter\");\n+\t\t}\n+\t\treturn encoder\n+\t\t\t\t.<OutputFormatFactory<BaseRow>>map(en -> path -> createEncoderOutputFormat(en, path))\n+\t\t\t\t.orElseGet(() -> path -> createBulkWriterOutputFormat(bulk.get(), path));\n+\t}\n+\n+\tprivate static OutputFormat<BaseRow> createBulkWriterOutputFormat(\n+\t\t\tBulkWriter.Factory<BaseRow> factory,\n+\t\t\tPath path) {\n+\t\treturn new OutputFormat<BaseRow>() {\n+\n+\t\t\tprivate BulkWriter<BaseRow> writer;\n+\n+\t\t\t@Override\n+\t\t\tpublic void configure(Configuration parameters) {\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic void open(int taskNumber, int numTasks) throws IOException {\n+\t\t\t\tthis.writer = factory.create(path.getFileSystem()\n+\t\t\t\t\t\t.create(path, FileSystem.WriteMode.OVERWRITE));\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic void writeRecord(BaseRow record) throws IOException {\n+\t\t\t\twriter.addElement(record);\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic void close() throws IOException {\n+\t\t\t\twriter.flush();\n+\t\t\t\twriter.finish();\n+\t\t\t}\n+\t\t};\n+\t}\n+\n+\tprivate static OutputFormat<BaseRow> createEncoderOutputFormat(\n+\t\t\tEncoder<BaseRow> encoder,\n+\t\t\tPath path) {\n+\t\treturn new OutputFormat<BaseRow>() {\n+\n+\t\t\tprivate FSDataOutputStream output;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd8a10137fa50eb11035082c95878fb6aa6a66c6"}, "originalPosition": 181}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ5MzQxMjgzOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSink.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMlQwNDoyNDo1MFrOF_a1FA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMlQwNDoyNDo1MFrOF_a1FA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjA0NDE4MA==", "bodyText": "Add a serialVersionUID?", "url": "https://github.com/apache/flink/pull/11574#discussion_r402044180", "createdAt": "2020-04-02T04:24:50Z", "author": {"login": "leonardBang"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSink.java", "diffHunk": "@@ -0,0 +1,271 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.api.common.io.OutputFormat;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.fs.FSDataOutputStream;\n+import org.apache.flink.core.fs.FileSystem;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.DataStreamSink;\n+import org.apache.flink.table.api.TableException;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.dataformat.BaseRow;\n+import org.apache.flink.table.sinks.AppendStreamTableSink;\n+import org.apache.flink.table.sinks.OverwritableTableSink;\n+import org.apache.flink.table.sinks.PartitionableTableSink;\n+import org.apache.flink.table.sinks.TableSink;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+\n+/**\n+ * File system {@link TableSink}.\n+ */\n+public class FileSystemTableSink implements\n+\t\tAppendStreamTableSink<BaseRow>,\n+\t\tPartitionableTableSink,\n+\t\tOverwritableTableSink {\n+\n+\tprivate final TableSchema schema;\n+\tprivate final List<String> partitionKeys;\n+\tprivate final Path path;\n+\tprivate final String defaultPartName;\n+\tprivate final FileSystemFormatFactory formatFactory;\n+\n+\tprivate boolean overwrite = false;\n+\tprivate boolean dynamicGrouping = false;\n+\tprivate LinkedHashMap<String, String> staticPartitions = new LinkedHashMap<>();\n+\n+\t/**\n+\t * Construct a file system table sink.\n+\t *\n+\t * @param schema schema of the table.\n+\t * @param path directory path of the file system table.\n+\t * @param partitionKeys partition keys of the table.\n+\t * @param defaultPartName The default partition name in case the dynamic partition column value\n+\t *                        is null/empty string.\n+\t * @param formatFactory format factory to create reader.\n+\t */\n+\tpublic FileSystemTableSink(\n+\t\t\tTableSchema schema,\n+\t\t\tPath path,\n+\t\t\tList<String> partitionKeys,\n+\t\t\tString defaultPartName,\n+\t\t\tFileSystemFormatFactory formatFactory) {\n+\t\tthis.schema = schema;\n+\t\tthis.path = path;\n+\t\tthis.defaultPartName = defaultPartName;\n+\t\tthis.formatFactory = formatFactory;\n+\t\tthis.partitionKeys = partitionKeys;\n+\t}\n+\n+\t@Override\n+\tpublic final DataStreamSink<BaseRow> consumeDataStream(DataStream<BaseRow> dataStream) {\n+\t\tRowDataPartitionComputer computer = new RowDataPartitionComputer(\n+\t\t\t\tdefaultPartName,\n+\t\t\t\tschema.getFieldNames(),\n+\t\t\t\tschema.getFieldDataTypes(),\n+\t\t\t\tpartitionKeys.toArray(new String[0]));\n+\n+\t\tFileSystemOutputFormat.Builder<BaseRow> builder = new FileSystemOutputFormat.Builder<>();\n+\t\tbuilder.setPartitionComputer(computer);\n+\t\tbuilder.setDynamicGrouped(dynamicGrouping);\n+\t\tbuilder.setPartitionColumns(partitionKeys.toArray(new String[0]));\n+\t\tbuilder.setFormatFactory(createOutputFormatFactory(\n+\t\t\t\tformatFactory, this::getNonPartitionTypes));\n+\t\tbuilder.setMetaStoreFactory(createTableMetaStoreFactory(path));\n+\t\tbuilder.setOverwrite(overwrite);\n+\t\tbuilder.setStaticPartitions(staticPartitions);\n+\t\tbuilder.setTempPath(toStagingPath());\n+\t\treturn dataStream.writeUsingOutputFormat(builder.build())\n+\t\t\t\t.setParallelism(dataStream.getParallelism());\n+\t}\n+\n+\tprivate DataType[] getNonPartitionTypes() {\n+\t\treturn Arrays.stream(schema.getFieldNames())\n+\t\t\t\t.filter(name -> !partitionKeys.contains(name))\n+\t\t\t\t.map(name -> schema.getFieldDataType(name).get())\n+\t\t\t\t.toArray(DataType[]::new);\n+\t}\n+\n+\tprivate Path toStagingPath() {\n+\t\tPath stagingDir = new Path(path, \".staging_\" + System.currentTimeMillis());\n+\t\ttry {\n+\t\t\tFileSystem fs = stagingDir.getFileSystem();\n+\t\t\tPreconditions.checkState(\n+\t\t\t\t\tfs.exists(stagingDir) || fs.mkdirs(stagingDir),\n+\t\t\t\t\t\"Failed to create staging dir \" + stagingDir);\n+\t\t\treturn stagingDir;\n+\t\t} catch (IOException e) {\n+\t\t\tthrow new RuntimeException(e);\n+\t\t}\n+\t}\n+\n+\tprivate static OutputFormatFactory<BaseRow> createOutputFormatFactory(\n+\t\t\tFileSystemFormatFactory formatFactory,\n+\t\t\tFileSystemFormatFactory.WriterContext context) {\n+\t\tOptional<Encoder<BaseRow>> encoder = formatFactory.createEncoder(context);\n+\t\tOptional<BulkWriter.Factory<BaseRow>> bulk = formatFactory.createBulkWriterFactory(context);\n+\n+\t\tif (!encoder.isPresent() && !bulk.isPresent()) {\n+\t\t\tthrow new TableException(\n+\t\t\t\t\tformatFactory + \" format should implement at least one Encoder or BulkWriter\");\n+\t\t}\n+\t\treturn encoder\n+\t\t\t\t.<OutputFormatFactory<BaseRow>>map(en -> path -> createEncoderOutputFormat(en, path))\n+\t\t\t\t.orElseGet(() -> path -> createBulkWriterOutputFormat(bulk.get(), path));\n+\t}\n+\n+\tprivate static OutputFormat<BaseRow> createBulkWriterOutputFormat(\n+\t\t\tBulkWriter.Factory<BaseRow> factory,\n+\t\t\tPath path) {\n+\t\treturn new OutputFormat<BaseRow>() {\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd8a10137fa50eb11035082c95878fb6aa6a66c6"}, "originalPosition": 150}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ5Nzg2NzM1OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableFactory.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QwNTo0NToyM1rOGAF0xA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QwNjoyNzozOFrOGAGnXg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjc0ODYxMg==", "bodyText": "Should we filter FORMAT.* properties here? otherwise we get all table connector properties.", "url": "https://github.com/apache/flink/pull/11574#discussion_r402748612", "createdAt": "2020-04-03T05:45:23Z", "author": {"login": "leonardBang"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableFactory.java", "diffHunk": "@@ -0,0 +1,143 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.configuration.ConfigOption;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.dataformat.BaseRow;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.TableFactory;\n+import org.apache.flink.table.factories.TableFactoryService;\n+import org.apache.flink.table.factories.TableSinkFactory;\n+import org.apache.flink.table.factories.TableSourceFactory;\n+import org.apache.flink.table.sinks.TableSink;\n+import org.apache.flink.table.sources.TableSource;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.apache.flink.configuration.ConfigOptions.key;\n+import static org.apache.flink.table.descriptors.ConnectorDescriptorValidator.CONNECTOR;\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+import static org.apache.flink.table.descriptors.Schema.SCHEMA;\n+\n+/**\n+ * File system {@link TableFactory}.\n+ *\n+ * <p>1.The partition information should be in the file system path, whether it's a temporary\n+ * table or a catalog table.\n+ * 2.Support insert into (append) and insert overwrite.\n+ * 3.Support static and dynamic partition inserting.\n+ *\n+ * <p>Migrate to new source/sink interface after FLIP-95 is ready.\n+ */\n+public class FileSystemTableFactory implements\n+\t\tTableSourceFactory<BaseRow>,\n+\t\tTableSinkFactory<BaseRow> {\n+\n+\tpublic static final String CONNECTOR_VALUE = \"filesystem\";\n+\n+\t/**\n+\t * Not use \"connector.path\" because:\n+\t * 1.Using \"connector.path\" will conflict with current batch csv source and batch csv sink.\n+\t * 2.This is compatible with FLIP-122.\n+\t */\n+\tpublic static final String PATH = \"path\";\n+\n+\t/**\n+\t * Move these properties to validator after FLINK-16904.\n+\t */\n+\tpublic static final ConfigOption<String> PARTITION_DEFAULT_NAME = key(\"partition.default-name\")\n+\t\t\t.stringType()\n+\t\t\t.defaultValue(\"__DEFAULT_PARTITION__\")\n+\t\t\t.withDescription(\"The default partition name in case the dynamic partition\" +\n+\t\t\t\t\t\" column value is null/empty string\");\n+\n+\t@Override\n+\tpublic Map<String, String> requiredContext() {\n+\t\tMap<String, String> context = new HashMap<>();\n+\t\tcontext.put(CONNECTOR, CONNECTOR_VALUE);\n+\t\treturn context;\n+\t}\n+\n+\t@Override\n+\tpublic List<String> supportedProperties() {\n+\t\tList<String> properties = new ArrayList<>();\n+\n+\t\t// path\n+\t\tproperties.add(PATH);\n+\n+\t\t// schema\n+\t\tproperties.add(SCHEMA + \".#.\" + DescriptorProperties.TABLE_SCHEMA_DATA_TYPE);\n+\t\tproperties.add(SCHEMA + \".#.\" + DescriptorProperties.TABLE_SCHEMA_NAME);\n+\n+\t\tproperties.add(PARTITION_DEFAULT_NAME.key());\n+\n+\t\t// format\n+\t\tproperties.add(FORMAT);\n+\t\tproperties.add(FORMAT + \".*\");\n+\n+\t\treturn properties;\n+\t}\n+\n+\t@Override\n+\tpublic TableSource<BaseRow> createTableSource(TableSourceFactory.Context context) {\n+\t\tDescriptorProperties properties = new DescriptorProperties();\n+\t\tproperties.putProperties(context.getTable().getProperties());\n+\n+\t\treturn new FileSystemTableSource(\n+\t\t\t\tcontext.getTable().getSchema(),\n+\t\t\t\tnew Path(properties.getString(PATH)),\n+\t\t\t\tcontext.getTable().getPartitionKeys(),\n+\t\t\t\tgetPartitionDefaultName(properties),\n+\t\t\t\tgetFormatProperties(context.getTable().getProperties()));\n+\t}\n+\n+\t@Override\n+\tpublic TableSink<BaseRow> createTableSink(TableSinkFactory.Context context) {\n+\t\tDescriptorProperties properties = new DescriptorProperties();\n+\t\tproperties.putProperties(context.getTable().getProperties());\n+\n+\t\treturn new FileSystemTableSink(\n+\t\t\t\tcontext.getTable().getSchema(),\n+\t\t\t\tnew Path(properties.getString(PATH)),\n+\t\t\t\tcontext.getTable().getPartitionKeys(),\n+\t\t\t\tgetPartitionDefaultName(properties),\n+\t\t\t\tgetFormatProperties(context.getTable().getProperties()));\n+\t}\n+\n+\tprivate static Map<String, String> getFormatProperties(Map<String, String> tableProperties) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ad9d0b168f308a5205430b2dcf85e680f20449b3"}, "originalPosition": 127}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjc1MzY5OA==", "bodyText": "Don't need do this now, you can see there is a magic in TableFactoryService.filterSupportedPropertiesFactorySpecific.", "url": "https://github.com/apache/flink/pull/11574#discussion_r402753698", "createdAt": "2020-04-03T06:03:11Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableFactory.java", "diffHunk": "@@ -0,0 +1,143 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.configuration.ConfigOption;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.dataformat.BaseRow;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.TableFactory;\n+import org.apache.flink.table.factories.TableFactoryService;\n+import org.apache.flink.table.factories.TableSinkFactory;\n+import org.apache.flink.table.factories.TableSourceFactory;\n+import org.apache.flink.table.sinks.TableSink;\n+import org.apache.flink.table.sources.TableSource;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.apache.flink.configuration.ConfigOptions.key;\n+import static org.apache.flink.table.descriptors.ConnectorDescriptorValidator.CONNECTOR;\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+import static org.apache.flink.table.descriptors.Schema.SCHEMA;\n+\n+/**\n+ * File system {@link TableFactory}.\n+ *\n+ * <p>1.The partition information should be in the file system path, whether it's a temporary\n+ * table or a catalog table.\n+ * 2.Support insert into (append) and insert overwrite.\n+ * 3.Support static and dynamic partition inserting.\n+ *\n+ * <p>Migrate to new source/sink interface after FLIP-95 is ready.\n+ */\n+public class FileSystemTableFactory implements\n+\t\tTableSourceFactory<BaseRow>,\n+\t\tTableSinkFactory<BaseRow> {\n+\n+\tpublic static final String CONNECTOR_VALUE = \"filesystem\";\n+\n+\t/**\n+\t * Not use \"connector.path\" because:\n+\t * 1.Using \"connector.path\" will conflict with current batch csv source and batch csv sink.\n+\t * 2.This is compatible with FLIP-122.\n+\t */\n+\tpublic static final String PATH = \"path\";\n+\n+\t/**\n+\t * Move these properties to validator after FLINK-16904.\n+\t */\n+\tpublic static final ConfigOption<String> PARTITION_DEFAULT_NAME = key(\"partition.default-name\")\n+\t\t\t.stringType()\n+\t\t\t.defaultValue(\"__DEFAULT_PARTITION__\")\n+\t\t\t.withDescription(\"The default partition name in case the dynamic partition\" +\n+\t\t\t\t\t\" column value is null/empty string\");\n+\n+\t@Override\n+\tpublic Map<String, String> requiredContext() {\n+\t\tMap<String, String> context = new HashMap<>();\n+\t\tcontext.put(CONNECTOR, CONNECTOR_VALUE);\n+\t\treturn context;\n+\t}\n+\n+\t@Override\n+\tpublic List<String> supportedProperties() {\n+\t\tList<String> properties = new ArrayList<>();\n+\n+\t\t// path\n+\t\tproperties.add(PATH);\n+\n+\t\t// schema\n+\t\tproperties.add(SCHEMA + \".#.\" + DescriptorProperties.TABLE_SCHEMA_DATA_TYPE);\n+\t\tproperties.add(SCHEMA + \".#.\" + DescriptorProperties.TABLE_SCHEMA_NAME);\n+\n+\t\tproperties.add(PARTITION_DEFAULT_NAME.key());\n+\n+\t\t// format\n+\t\tproperties.add(FORMAT);\n+\t\tproperties.add(FORMAT + \".*\");\n+\n+\t\treturn properties;\n+\t}\n+\n+\t@Override\n+\tpublic TableSource<BaseRow> createTableSource(TableSourceFactory.Context context) {\n+\t\tDescriptorProperties properties = new DescriptorProperties();\n+\t\tproperties.putProperties(context.getTable().getProperties());\n+\n+\t\treturn new FileSystemTableSource(\n+\t\t\t\tcontext.getTable().getSchema(),\n+\t\t\t\tnew Path(properties.getString(PATH)),\n+\t\t\t\tcontext.getTable().getPartitionKeys(),\n+\t\t\t\tgetPartitionDefaultName(properties),\n+\t\t\t\tgetFormatProperties(context.getTable().getProperties()));\n+\t}\n+\n+\t@Override\n+\tpublic TableSink<BaseRow> createTableSink(TableSinkFactory.Context context) {\n+\t\tDescriptorProperties properties = new DescriptorProperties();\n+\t\tproperties.putProperties(context.getTable().getProperties());\n+\n+\t\treturn new FileSystemTableSink(\n+\t\t\t\tcontext.getTable().getSchema(),\n+\t\t\t\tnew Path(properties.getString(PATH)),\n+\t\t\t\tcontext.getTable().getPartitionKeys(),\n+\t\t\t\tgetPartitionDefaultName(properties),\n+\t\t\t\tgetFormatProperties(context.getTable().getProperties()));\n+\t}\n+\n+\tprivate static Map<String, String> getFormatProperties(Map<String, String> tableProperties) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjc0ODYxMg=="}, "originalCommit": {"oid": "ad9d0b168f308a5205430b2dcf85e680f20449b3"}, "originalPosition": 127}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjc2MTU2Ng==", "bodyText": "Oh, you're wright, I missed this logic.", "url": "https://github.com/apache/flink/pull/11574#discussion_r402761566", "createdAt": "2020-04-03T06:27:38Z", "author": {"login": "leonardBang"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableFactory.java", "diffHunk": "@@ -0,0 +1,143 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.configuration.ConfigOption;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.dataformat.BaseRow;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.TableFactory;\n+import org.apache.flink.table.factories.TableFactoryService;\n+import org.apache.flink.table.factories.TableSinkFactory;\n+import org.apache.flink.table.factories.TableSourceFactory;\n+import org.apache.flink.table.sinks.TableSink;\n+import org.apache.flink.table.sources.TableSource;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.apache.flink.configuration.ConfigOptions.key;\n+import static org.apache.flink.table.descriptors.ConnectorDescriptorValidator.CONNECTOR;\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+import static org.apache.flink.table.descriptors.Schema.SCHEMA;\n+\n+/**\n+ * File system {@link TableFactory}.\n+ *\n+ * <p>1.The partition information should be in the file system path, whether it's a temporary\n+ * table or a catalog table.\n+ * 2.Support insert into (append) and insert overwrite.\n+ * 3.Support static and dynamic partition inserting.\n+ *\n+ * <p>Migrate to new source/sink interface after FLIP-95 is ready.\n+ */\n+public class FileSystemTableFactory implements\n+\t\tTableSourceFactory<BaseRow>,\n+\t\tTableSinkFactory<BaseRow> {\n+\n+\tpublic static final String CONNECTOR_VALUE = \"filesystem\";\n+\n+\t/**\n+\t * Not use \"connector.path\" because:\n+\t * 1.Using \"connector.path\" will conflict with current batch csv source and batch csv sink.\n+\t * 2.This is compatible with FLIP-122.\n+\t */\n+\tpublic static final String PATH = \"path\";\n+\n+\t/**\n+\t * Move these properties to validator after FLINK-16904.\n+\t */\n+\tpublic static final ConfigOption<String> PARTITION_DEFAULT_NAME = key(\"partition.default-name\")\n+\t\t\t.stringType()\n+\t\t\t.defaultValue(\"__DEFAULT_PARTITION__\")\n+\t\t\t.withDescription(\"The default partition name in case the dynamic partition\" +\n+\t\t\t\t\t\" column value is null/empty string\");\n+\n+\t@Override\n+\tpublic Map<String, String> requiredContext() {\n+\t\tMap<String, String> context = new HashMap<>();\n+\t\tcontext.put(CONNECTOR, CONNECTOR_VALUE);\n+\t\treturn context;\n+\t}\n+\n+\t@Override\n+\tpublic List<String> supportedProperties() {\n+\t\tList<String> properties = new ArrayList<>();\n+\n+\t\t// path\n+\t\tproperties.add(PATH);\n+\n+\t\t// schema\n+\t\tproperties.add(SCHEMA + \".#.\" + DescriptorProperties.TABLE_SCHEMA_DATA_TYPE);\n+\t\tproperties.add(SCHEMA + \".#.\" + DescriptorProperties.TABLE_SCHEMA_NAME);\n+\n+\t\tproperties.add(PARTITION_DEFAULT_NAME.key());\n+\n+\t\t// format\n+\t\tproperties.add(FORMAT);\n+\t\tproperties.add(FORMAT + \".*\");\n+\n+\t\treturn properties;\n+\t}\n+\n+\t@Override\n+\tpublic TableSource<BaseRow> createTableSource(TableSourceFactory.Context context) {\n+\t\tDescriptorProperties properties = new DescriptorProperties();\n+\t\tproperties.putProperties(context.getTable().getProperties());\n+\n+\t\treturn new FileSystemTableSource(\n+\t\t\t\tcontext.getTable().getSchema(),\n+\t\t\t\tnew Path(properties.getString(PATH)),\n+\t\t\t\tcontext.getTable().getPartitionKeys(),\n+\t\t\t\tgetPartitionDefaultName(properties),\n+\t\t\t\tgetFormatProperties(context.getTable().getProperties()));\n+\t}\n+\n+\t@Override\n+\tpublic TableSink<BaseRow> createTableSink(TableSinkFactory.Context context) {\n+\t\tDescriptorProperties properties = new DescriptorProperties();\n+\t\tproperties.putProperties(context.getTable().getProperties());\n+\n+\t\treturn new FileSystemTableSink(\n+\t\t\t\tcontext.getTable().getSchema(),\n+\t\t\t\tnew Path(properties.getString(PATH)),\n+\t\t\t\tcontext.getTable().getPartitionKeys(),\n+\t\t\t\tgetPartitionDefaultName(properties),\n+\t\t\t\tgetFormatProperties(context.getTable().getProperties()));\n+\t}\n+\n+\tprivate static Map<String, String> getFormatProperties(Map<String, String> tableProperties) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjc0ODYxMg=="}, "originalCommit": {"oid": "ad9d0b168f308a5205430b2dcf85e680f20449b3"}, "originalPosition": 127}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ5Nzg2OTQ3OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSink.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QwNTo0NjozOVrOGAF1_w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QwNTo0NjozOVrOGAF1_w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjc0ODkyNw==", "bodyText": "how about name to bulkWriterFactory?", "url": "https://github.com/apache/flink/pull/11574#discussion_r402748927", "createdAt": "2020-04-03T05:46:39Z", "author": {"login": "leonardBang"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSink.java", "diffHunk": "@@ -0,0 +1,289 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.api.common.io.OutputFormat;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.fs.FSDataOutputStream;\n+import org.apache.flink.core.fs.FileSystem;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.DataStreamSink;\n+import org.apache.flink.table.api.TableException;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.dataformat.BaseRow;\n+import org.apache.flink.table.sinks.AppendStreamTableSink;\n+import org.apache.flink.table.sinks.OverwritableTableSink;\n+import org.apache.flink.table.sinks.PartitionableTableSink;\n+import org.apache.flink.table.sinks.TableSink;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+\n+import static org.apache.flink.table.filesystem.FileSystemTableFactory.createFormatFactory;\n+\n+/**\n+ * File system {@link TableSink}.\n+ */\n+public class FileSystemTableSink implements\n+\t\tAppendStreamTableSink<BaseRow>,\n+\t\tPartitionableTableSink,\n+\t\tOverwritableTableSink {\n+\n+\tprivate final TableSchema schema;\n+\tprivate final List<String> partitionKeys;\n+\tprivate final Path path;\n+\tprivate final String defaultPartName;\n+\tprivate final Map<String, String> formatProperties;\n+\n+\tprivate boolean overwrite = false;\n+\tprivate boolean dynamicGrouping = false;\n+\tprivate LinkedHashMap<String, String> staticPartitions = new LinkedHashMap<>();\n+\n+\t/**\n+\t * Construct a file system table sink.\n+\t *\n+\t * @param schema schema of the table.\n+\t * @param path directory path of the file system table.\n+\t * @param partitionKeys partition keys of the table.\n+\t * @param defaultPartName The default partition name in case the dynamic partition column value\n+\t *                        is null/empty string.\n+\t * @param formatProperties format properties.\n+\t */\n+\tpublic FileSystemTableSink(\n+\t\t\tTableSchema schema,\n+\t\t\tPath path,\n+\t\t\tList<String> partitionKeys,\n+\t\t\tString defaultPartName,\n+\t\t\tMap<String, String> formatProperties) {\n+\t\tthis.schema = schema;\n+\t\tthis.path = path;\n+\t\tthis.defaultPartName = defaultPartName;\n+\t\tthis.formatProperties = formatProperties;\n+\t\tthis.partitionKeys = partitionKeys;\n+\t}\n+\n+\t@Override\n+\tpublic final DataStreamSink<BaseRow> consumeDataStream(DataStream<BaseRow> dataStream) {\n+\t\tRowDataPartitionComputer computer = new RowDataPartitionComputer(\n+\t\t\t\tdefaultPartName,\n+\t\t\t\tschema.getFieldNames(),\n+\t\t\t\tschema.getFieldDataTypes(),\n+\t\t\t\tpartitionKeys.toArray(new String[0]));\n+\n+\t\tFileSystemOutputFormat.Builder<BaseRow> builder = new FileSystemOutputFormat.Builder<>();\n+\t\tbuilder.setPartitionComputer(computer);\n+\t\tbuilder.setDynamicGrouped(dynamicGrouping);\n+\t\tbuilder.setPartitionColumns(partitionKeys.toArray(new String[0]));\n+\t\tbuilder.setFormatFactory(createOutputFormatFactory());\n+\t\tbuilder.setMetaStoreFactory(createTableMetaStoreFactory(path));\n+\t\tbuilder.setOverwrite(overwrite);\n+\t\tbuilder.setStaticPartitions(staticPartitions);\n+\t\tbuilder.setTempPath(toStagingPath());\n+\t\treturn dataStream.writeUsingOutputFormat(builder.build())\n+\t\t\t\t.setParallelism(dataStream.getParallelism());\n+\t}\n+\n+\tprivate Path toStagingPath() {\n+\t\tPath stagingDir = new Path(path, \".staging_\" + System.currentTimeMillis());\n+\t\ttry {\n+\t\t\tFileSystem fs = stagingDir.getFileSystem();\n+\t\t\tPreconditions.checkState(\n+\t\t\t\t\tfs.exists(stagingDir) || fs.mkdirs(stagingDir),\n+\t\t\t\t\t\"Failed to create staging dir \" + stagingDir);\n+\t\t\treturn stagingDir;\n+\t\t} catch (IOException e) {\n+\t\t\tthrow new RuntimeException(e);\n+\t\t}\n+\t}\n+\n+\tprivate OutputFormatFactory<BaseRow> createOutputFormatFactory() {\n+\t\tFileSystemFormatFactory formatFactory = createFormatFactory(formatProperties);\n+\t\tFileSystemFormatFactory.WriterContext context = new FileSystemFormatFactory.WriterContext() {\n+\n+\t\t\t@Override\n+\t\t\tpublic TableSchema getSchema() {\n+\t\t\t\treturn schema;\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic Map<String, String> getFormatProperties() {\n+\t\t\t\treturn formatProperties;\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic List<String> getPartitionKeys() {\n+\t\t\t\treturn partitionKeys;\n+\t\t\t}\n+\t\t};\n+\n+\t\tOptional<Encoder<BaseRow>> encoder = formatFactory.createEncoder(context);\n+\t\tOptional<BulkWriter.Factory<BaseRow>> bulk = formatFactory.createBulkWriterFactory(context);\n+\n+\t\tif (!encoder.isPresent() && !bulk.isPresent()) {\n+\t\t\tthrow new TableException(\n+\t\t\t\t\tformatFactory + \" format should implement at least one Encoder or BulkWriter\");\n+\t\t}\n+\t\treturn encoder\n+\t\t\t\t.<OutputFormatFactory<BaseRow>>map(en -> path -> createEncoderOutputFormat(en, path))\n+\t\t\t\t.orElseGet(() -> {\n+\t\t\t\t\t// Optional is not serializable.\n+\t\t\t\t\tBulkWriter.Factory<BaseRow> bulkWriter = bulk.get();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ad9d0b168f308a5205430b2dcf85e680f20449b3"}, "originalPosition": 155}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 581, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}