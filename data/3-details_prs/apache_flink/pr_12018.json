{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDE0NTYwMDcz", "number": 12018, "title": "[FLINK-17307] Add collector to deserialize in KafkaDeserializationSchema", "bodyText": "What is the purpose of the change\nThis PR adds a way to emit multiple records from\nKafkaDeserializationSchema. This is possible through a collector, which\nwill buffer deserialized records in a queue and then emit all records\natomically. The queue is reused for all incoming Kafka records to\nminimize creating new objects on the hot path.\nVerifying this change\nAll existing kafka tests should pass.\nAdded:\n\norg.apache.flink.streaming.connectors.kafka.KafkaConsumerTestBase#runCollectingSchemaTest\n\nDoes this pull request potentially affect one of the following parts:\n\nDependencies (does it add or upgrade a dependency): (yes / no)\nThe public API, i.e., is any changed class annotated with @Public(Evolving): (yes / no)\nThe serializers: (yes / no / don't know)\nThe runtime per-record code paths (performance sensitive): (yes / no / don't know)\nAnything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (yes / no / don't know)\nThe S3 file system connector: (yes / no / don't know)\n\nDocumentation\n\nDoes this pull request introduce a new feature? (yes / no)\nIf yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)", "createdAt": "2020-05-07T09:14:16Z", "url": "https://github.com/apache/flink/pull/12018", "merged": true, "mergeCommit": {"oid": "fb9d864e21489d6fc389d6ebb9f661f4fb4aafb2"}, "closed": true, "closedAt": "2020-05-12T11:50:33Z", "author": {"login": "dawidwys"}, "timelineItems": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcgMeRVgFqTQwOTAxNTI0Mg==", "endCursor": "Y3Vyc29yOnYyOpPPAAABcgiJS3gFqTQwOTkyNzc2Mg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDA5MDE1MjQy", "url": "https://github.com/apache/flink/pull/12018#pullrequestreview-409015242", "createdAt": "2020-05-11T09:38:24Z", "commit": {"oid": "9cf30a56133e39d0e895f99796201d99a8447249"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMVQwOTozODoyNFrOGTUmDg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMVQwOTo0MDoxOFrOGTUqKQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjkxMzU1MA==", "bodyText": "The changes in this file are purely an orthogonal refactoring, right? Could you put these in a separate commit, along with the removal of the emitRecord() method on the fetcher that is only used in tests?", "url": "https://github.com/apache/flink/pull/12018#discussion_r422913550", "createdAt": "2020-05-11T09:38:24Z", "author": {"login": "aljoscha"}, "path": "flink-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/internals/AbstractFetcherTest.java", "diffHunk": "@@ -108,13 +112,13 @@ public void testSkipCorruptedRecord() throws Exception {\n \n \t\tfinal KafkaTopicPartitionState<Object> partitionStateHolder = fetcher.subscribedPartitionStates().get(0);\n \n-\t\tfetcher.emitRecord(1L, partitionStateHolder, 1L);\n-\t\tfetcher.emitRecord(2L, partitionStateHolder, 2L);\n+\t\temitRecord(fetcher, 1L, partitionStateHolder, 1L);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9cf30a56133e39d0e895f99796201d99a8447249"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjkxNDYwMQ==", "bodyText": "So we're not emitting all the records of the batch after one of them signals \"end of stream\"? I recall that an initial version still emitted all records.", "url": "https://github.com/apache/flink/pull/12018#discussion_r422914601", "createdAt": "2020-05-11T09:40:18Z", "author": {"login": "aljoscha"}, "path": "flink-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka010Fetcher.java", "diffHunk": "@@ -228,4 +226,33 @@ protected void doCommitInternalOffsetsToKafka(\n \t\t// record the work to be committed by the main consumer thread and make sure the consumer notices that\n \t\tconsumerThread.setOffsetsToCommit(offsetsToCommit, commitCallback);\n \t}\n+\n+\tprivate class KafkaCollector implements Collector<T> {\n+\t\tprivate final Queue<T> records = new ArrayDeque<>();\n+\n+\t\tprivate boolean endOfStreamSignalled = false;\n+\n+\t\t@Override\n+\t\tpublic void collect(T record) {\n+\t\t\t// do not emit subsequent elements if the end of the stream reached", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9cf30a56133e39d0e895f99796201d99a8447249"}, "originalPosition": 87}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "53e37282adc57dff328367a84f15132c80ccd54d", "author": {"user": {"login": "dawidwys", "name": "Dawid Wysakowicz"}}, "url": "https://github.com/apache/flink/commit/53e37282adc57dff328367a84f15132c80ccd54d", "committedDate": "2020-05-11T11:48:41Z", "message": "[hotfix][kafka] Remove unused method AbstractFetcher#emitRecord"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e7e6890ac5b32caacc424dd8cf7417d4569873e9", "author": {"user": {"login": "dawidwys", "name": "Dawid Wysakowicz"}}, "url": "https://github.com/apache/flink/commit/e7e6890ac5b32caacc424dd8cf7417d4569873e9", "committedDate": "2020-05-11T11:50:47Z", "message": "[FLINK-17307] Add collector to deserialize in KafkaDeserializationSchema\n\nThis PR adds a way to emit multiple records from\nKafkaDeserializationSchema. This is possible through a collector, which\nwill buffer deserialized records in a queue and then emit all records\natomically. The queue is reused for all incoming Kafka records to\nminimize creating new objects on the hot path."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "9cf30a56133e39d0e895f99796201d99a8447249", "author": {"user": {"login": "dawidwys", "name": "Dawid Wysakowicz"}}, "url": "https://github.com/apache/flink/commit/9cf30a56133e39d0e895f99796201d99a8447249", "committedDate": "2020-05-07T09:06:18Z", "message": "[FLINK-17307] Add collector to deserialize in KafkaDeserializationSchema\n\nThis PR adds a way to emit multiple records from\nKafkaDeserializationSchema. This is possible through a collector, which\nwill buffer deserialized records in a queue and then emit all records\natomically. The queue is reused for all incoming Kafka records to\nminimize creating new objects on the hot path."}, "afterCommit": {"oid": "e7e6890ac5b32caacc424dd8cf7417d4569873e9", "author": {"user": {"login": "dawidwys", "name": "Dawid Wysakowicz"}}, "url": "https://github.com/apache/flink/commit/e7e6890ac5b32caacc424dd8cf7417d4569873e9", "committedDate": "2020-05-11T11:50:47Z", "message": "[FLINK-17307] Add collector to deserialize in KafkaDeserializationSchema\n\nThis PR adds a way to emit multiple records from\nKafkaDeserializationSchema. This is possible through a collector, which\nwill buffer deserialized records in a queue and then emit all records\natomically. The queue is reused for all incoming Kafka records to\nminimize creating new objects on the hot path."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDA5OTI3NzYy", "url": "https://github.com/apache/flink/pull/12018#pullrequestreview-409927762", "createdAt": "2020-05-12T10:58:35Z", "commit": {"oid": "e7e6890ac5b32caacc424dd8cf7417d4569873e9"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4640, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}