{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDEzMzUwNDA2", "number": 11990, "title": "[FLINK-17387][hive] Implement LookupableTableSource for Hive connector", "bodyText": "What is the purpose of the change\nImplement LookupableTableSource for Hive connector.\nBrief change log\n\nMake HiveTableSource implement LookupableTableSource\nChang CommonLookupJoin to call TypeInfoDataTypeConverter for the type conversion\nAdd test case\n\nVerifying this change\nExisting and added test cases.\nDoes this pull request potentially affect one of the following parts:\n\nDependencies (does it add or upgrade a dependency): no\nThe public API, i.e., is any changed class annotated with @Public(Evolving): no\nThe serializers: no\nThe runtime per-record code paths (performance sensitive): no\nAnything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: no\nThe S3 file system connector: no\n\nDocumentation\n\nDoes this pull request introduce a new feature? yes\nIf yes, how is the feature documented? docs", "createdAt": "2020-05-05T07:29:14Z", "url": "https://github.com/apache/flink/pull/11990", "merged": true, "mergeCommit": {"oid": "2e6acb6bba04576cb4f967868d10fbbf0c576be1"}, "closed": true, "closedAt": "2020-05-15T01:45:02Z", "author": {"login": "lirui-apache"}, "timelineItems": {"totalCount": 15, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcfWAGGgFqTQwODQyNzE1MA==", "endCursor": "Y3Vyc29yOnYyOpPPAAABchYBRxAFqTQxMjI5MzgxNQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDA4NDI3MTUw", "url": "https://github.com/apache/flink/pull/11990#pullrequestreview-408427150", "createdAt": "2020-05-08T18:15:40Z", "commit": {"oid": "9a940870922bfb7e92ea199e3affd893cbadd745"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOFQxODoxNTo0MVrOGSuyUA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOFQxODoxNTo0MVrOGSuyUA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjI5NDA5Ng==", "bodyText": "does it mean it caches all rows of a hive file?", "url": "https://github.com/apache/flink/pull/11990#discussion_r422294096", "createdAt": "2020-05-08T18:15:41Z", "author": {"login": "bowenli86"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveTableLookupFunction.java", "diffHunk": "@@ -0,0 +1,142 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive.read;\n+\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.connectors.hive.FlinkHiveException;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.functions.FunctionContext;\n+import org.apache.flink.table.functions.TableFunction;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+/**\n+ * Lookup table function for Hive tables.\n+ */\n+public class HiveTableLookupFunction extends TableFunction<RowData> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate final HiveTableInputFormat inputFormat;\n+\t// indices of lookup columns in the record returned by input format\n+\tprivate final int[] lookupCols;\n+\tprivate transient Map<RowData, List<RowData>> cache;\n+\t// timestamp when cache expires\n+\tprivate transient long cacheExpire;\n+\tprivate final Duration cacheTTL = Duration.ofHours(1);\n+\n+\tpublic HiveTableLookupFunction(HiveTableInputFormat inputFormat, String[] lookupKeys) {\n+\t\tlookupCols = new int[lookupKeys.length];\n+\t\tString[] allFields = inputFormat.getFieldNames();\n+\t\tMap<String, Integer> nameToIndex = IntStream.range(0, allFields.length).boxed().collect(\n+\t\t\t\tCollectors.toMap(i -> allFields[i], i -> i));\n+\t\tList<Integer> selectedIndices = Arrays.stream(inputFormat.getSelectedFields()).boxed().collect(Collectors.toList());\n+\t\tfor (int i = 0; i < lookupKeys.length; i++) {\n+\t\t\tInteger index = nameToIndex.get(lookupKeys[i]);\n+\t\t\tPreconditions.checkArgument(index != null, \"Lookup keys %s not found in table schema\", Arrays.toString(lookupKeys));\n+\t\t\tindex = selectedIndices.indexOf(index);\n+\t\t\tPreconditions.checkArgument(index >= 0, \"Lookup keys %s not selected\", Arrays.toString(lookupKeys));\n+\t\t\tlookupCols[i] = index;\n+\t\t}\n+\t\tthis.inputFormat = inputFormat;\n+\t}\n+\n+\t@Override\n+\tpublic TypeInformation<RowData> getResultType() {\n+\t\tString[] allNames = inputFormat.getFieldNames();\n+\t\tDataType[] allTypes = inputFormat.getFieldTypes();\n+\t\tint[] selected = inputFormat.getSelectedFields();\n+\t\treturn new RowDataTypeInfo(\n+\t\t\t\tArrays.stream(selected).mapToObj(i -> allTypes[i].getLogicalType()).toArray(LogicalType[]::new),\n+\t\t\t\tArrays.stream(selected).mapToObj(i -> allNames[i]).toArray(String[]::new));\n+\t}\n+\n+\t@Override\n+\tpublic void open(FunctionContext context) throws Exception {\n+\t\tsuper.open(context);\n+\t\tcache = new HashMap<>();\n+\t\tcacheExpire = -1;\n+\t}\n+\n+\tpublic void eval(Object... values) {\n+\t\tPreconditions.checkArgument(values.length == lookupCols.length, \"Number of values and lookup keys mismatch\");\n+\t\treloadCache();\n+\t\tRowData probeKey = GenericRowData.of(values);\n+\t\tList<RowData> matchedRows = cache.get(probeKey);\n+\t\tif (matchedRows != null) {\n+\t\t\tfor (RowData matchedRow : matchedRows) {\n+\t\t\t\tcollect(matchedRow);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate void reloadCache() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9a940870922bfb7e92ea199e3affd893cbadd745"}, "originalPosition": 102}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "9a940870922bfb7e92ea199e3affd893cbadd745", "author": {"user": {"login": "lirui-apache", "name": "Rui Li"}}, "url": "https://github.com/apache/flink/commit/9a940870922bfb7e92ea199e3affd893cbadd745", "committedDate": "2020-05-05T07:25:06Z", "message": "[FLINK-17387][hive] Implement LookupableTableSource for Hive connector"}, "afterCommit": {"oid": "c9a2b1d53715e766991efb85ec98ba96c14f0f7b", "author": {"user": {"login": "lirui-apache", "name": "Rui Li"}}, "url": "https://github.com/apache/flink/commit/c9a2b1d53715e766991efb85ec98ba96c14f0f7b", "committedDate": "2020-05-09T03:07:23Z", "message": "make cache ttl configurable"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDA5NzE4OTcx", "url": "https://github.com/apache/flink/pull/11990#pullrequestreview-409718971", "createdAt": "2020-05-12T05:58:08Z", "commit": {"oid": "c9a2b1d53715e766991efb85ec98ba96c14f0f7b"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQwNTo1ODowOFrOGT3HTA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQwNjowNzo1MFrOGT3UTg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQ3OTExNg==", "bodyText": "throw new UnsupportedOperationException", "url": "https://github.com/apache/flink/pull/11990#discussion_r423479116", "createdAt": "2020-05-12T05:58:08Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSource.java", "diffHunk": "@@ -382,4 +390,25 @@ public String explainSource() {\n \t\t}\n \t\treturn TableConnectorUtils.generateRuntimeName(getClass(), getTableSchema().getFieldNames()) + explain;\n \t}\n+\n+\t@Override\n+\tpublic TableFunction<RowData> getLookupFunction(String[] lookupKeys) {\n+\t\t// always use MR reader for the lookup function\n+\t\tList<HiveTablePartition> allPartitions = initAllPartitions();\n+\t\tDuration cacheTTL = Duration.ofMinutes(\n+\t\t\t\thiveTableCacheTTL != null ?\n+\t\t\t\t\t\tLong.parseLong(hiveTableCacheTTL) :\n+\t\t\t\t\t\tHiveOptions.LOOKUP_JOIN_CACHE_TTL.defaultValue());\n+\t\treturn new HiveTableLookupFunction(getInputFormat(allPartitions, true), lookupKeys, cacheTTL);\n+\t}\n+\n+\t@Override\n+\tpublic AsyncTableFunction<RowData> getAsyncLookupFunction(String[] lookupKeys) {\n+\t\treturn null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c9a2b1d53715e766991efb85ec98ba96c14f0f7b"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQ3OTYzNQ==", "bodyText": "move to FileSystemOptions", "url": "https://github.com/apache/flink/pull/11990#discussion_r423479635", "createdAt": "2020-05-12T05:59:41Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveOptions.java", "diffHunk": "@@ -45,4 +45,10 @@\n \t\t\tkey(\"table.exec.hive.infer-source-parallelism.max\")\n \t\t\t\t\t.defaultValue(1000)\n \t\t\t\t\t.withDescription(\"Sets max infer parallelism for source operator.\");\n+\n+\tpublic static final ConfigOption<Integer> LOOKUP_JOIN_CACHE_TTL =\n+\t\t\tkey(\"lookup.join.cache.ttl\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c9a2b1d53715e766991efb85ec98ba96c14f0f7b"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQ3OTc1OA==", "bodyText": "Duration", "url": "https://github.com/apache/flink/pull/11990#discussion_r423479758", "createdAt": "2020-05-12T06:00:03Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveOptions.java", "diffHunk": "@@ -45,4 +45,10 @@\n \t\t\tkey(\"table.exec.hive.infer-source-parallelism.max\")\n \t\t\t\t\t.defaultValue(1000)\n \t\t\t\t\t.withDescription(\"Sets max infer parallelism for source operator.\");\n+\n+\tpublic static final ConfigOption<Integer> LOOKUP_JOIN_CACHE_TTL =\n+\t\t\tkey(\"lookup.join.cache.ttl\")\n+\t\t\t\t\t.intType()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c9a2b1d53715e766991efb85ec98ba96c14f0f7b"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQ3OTk2Mg==", "bodyText": "defaultValue should init here.\nCan construct a flink Configuration here.", "url": "https://github.com/apache/flink/pull/11990#discussion_r423479962", "createdAt": "2020-05-12T06:00:35Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSource.java", "diffHunk": "@@ -286,6 +293,7 @@ public boolean isLimitPushedDown() {\n \t\t\tList<String> partitionColNames = catalogTable.getPartitionKeys();\n \t\t\tTable hiveTable = client.getTable(dbName, tableName);\n \t\t\tProperties tableProps = HiveReflectionUtils.getTableMetadata(hiveShim, hiveTable);\n+\t\t\thiveTableCacheTTL = tableProps.getProperty(HiveOptions.LOOKUP_JOIN_CACHE_TTL.key());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c9a2b1d53715e766991efb85ec98ba96c14f0f7b"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQ4MDQxNw==", "bodyText": "Can you move this class to table, and rename to FileSystemLookupFunction?", "url": "https://github.com/apache/flink/pull/11990#discussion_r423480417", "createdAt": "2020-05-12T06:02:05Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveTableLookupFunction.java", "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive.read;\n+\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.connectors.hive.FlinkHiveException;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.functions.FunctionContext;\n+import org.apache.flink.table.functions.TableFunction;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+/**\n+ * Lookup table function for Hive tables.\n+ */\n+public class HiveTableLookupFunction extends TableFunction<RowData> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c9a2b1d53715e766991efb85ec98ba96c14f0f7b"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQ4MTAyMA==", "bodyText": "nextLoadTime?", "url": "https://github.com/apache/flink/pull/11990#discussion_r423481020", "createdAt": "2020-05-12T06:03:46Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveTableLookupFunction.java", "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive.read;\n+\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.connectors.hive.FlinkHiveException;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.functions.FunctionContext;\n+import org.apache.flink.table.functions.TableFunction;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+/**\n+ * Lookup table function for Hive tables.\n+ */\n+public class HiveTableLookupFunction extends TableFunction<RowData> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate final HiveTableInputFormat inputFormat;\n+\t// indices of lookup columns in the record returned by input format\n+\tprivate final int[] lookupCols;\n+\tprivate transient Map<RowData, List<RowData>> cache;\n+\t// timestamp when cache expires\n+\tprivate transient long cacheExpire;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c9a2b1d53715e766991efb85ec98ba96c14f0f7b"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQ4MTIwMQ==", "bodyText": "checkCacheReload?", "url": "https://github.com/apache/flink/pull/11990#discussion_r423481201", "createdAt": "2020-05-12T06:04:19Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveTableLookupFunction.java", "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive.read;\n+\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.connectors.hive.FlinkHiveException;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.functions.FunctionContext;\n+import org.apache.flink.table.functions.TableFunction;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+/**\n+ * Lookup table function for Hive tables.\n+ */\n+public class HiveTableLookupFunction extends TableFunction<RowData> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate final HiveTableInputFormat inputFormat;\n+\t// indices of lookup columns in the record returned by input format\n+\tprivate final int[] lookupCols;\n+\tprivate transient Map<RowData, List<RowData>> cache;\n+\t// timestamp when cache expires\n+\tprivate transient long cacheExpire;\n+\tprivate final Duration cacheTTL;\n+\n+\tpublic HiveTableLookupFunction(HiveTableInputFormat inputFormat, String[] lookupKeys, Duration cacheTTL) {\n+\t\tlookupCols = new int[lookupKeys.length];\n+\t\tString[] allFields = inputFormat.getFieldNames();\n+\t\tMap<String, Integer> nameToIndex = IntStream.range(0, allFields.length).boxed().collect(\n+\t\t\t\tCollectors.toMap(i -> allFields[i], i -> i));\n+\t\tList<Integer> selectedIndices = Arrays.stream(inputFormat.getSelectedFields()).boxed().collect(Collectors.toList());\n+\t\tfor (int i = 0; i < lookupKeys.length; i++) {\n+\t\t\tInteger index = nameToIndex.get(lookupKeys[i]);\n+\t\t\tPreconditions.checkArgument(index != null, \"Lookup keys %s not found in table schema\", Arrays.toString(lookupKeys));\n+\t\t\tindex = selectedIndices.indexOf(index);\n+\t\t\tPreconditions.checkArgument(index >= 0, \"Lookup keys %s not selected\", Arrays.toString(lookupKeys));\n+\t\t\tlookupCols[i] = index;\n+\t\t}\n+\t\tthis.inputFormat = inputFormat;\n+\t\tthis.cacheTTL = cacheTTL;\n+\t}\n+\n+\t@Override\n+\tpublic TypeInformation<RowData> getResultType() {\n+\t\tString[] allNames = inputFormat.getFieldNames();\n+\t\tDataType[] allTypes = inputFormat.getFieldTypes();\n+\t\tint[] selected = inputFormat.getSelectedFields();\n+\t\treturn new RowDataTypeInfo(\n+\t\t\t\tArrays.stream(selected).mapToObj(i -> allTypes[i].getLogicalType()).toArray(LogicalType[]::new),\n+\t\t\t\tArrays.stream(selected).mapToObj(i -> allNames[i]).toArray(String[]::new));\n+\t}\n+\n+\t@Override\n+\tpublic void open(FunctionContext context) throws Exception {\n+\t\tsuper.open(context);\n+\t\tcache = new HashMap<>();\n+\t\tcacheExpire = -1;\n+\t}\n+\n+\tpublic void eval(Object... values) {\n+\t\tPreconditions.checkArgument(values.length == lookupCols.length, \"Number of values and lookup keys mismatch\");\n+\t\treloadCache();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c9a2b1d53715e766991efb85ec98ba96c14f0f7b"}, "originalPosition": 94}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQ4MjAzNg==", "bodyText": "Maybe not GenericRowData.", "url": "https://github.com/apache/flink/pull/11990#discussion_r423482036", "createdAt": "2020-05-12T06:06:41Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveTableLookupFunction.java", "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive.read;\n+\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.connectors.hive.FlinkHiveException;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.functions.FunctionContext;\n+import org.apache.flink.table.functions.TableFunction;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+/**\n+ * Lookup table function for Hive tables.\n+ */\n+public class HiveTableLookupFunction extends TableFunction<RowData> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate final HiveTableInputFormat inputFormat;\n+\t// indices of lookup columns in the record returned by input format\n+\tprivate final int[] lookupCols;\n+\tprivate transient Map<RowData, List<RowData>> cache;\n+\t// timestamp when cache expires\n+\tprivate transient long cacheExpire;\n+\tprivate final Duration cacheTTL;\n+\n+\tpublic HiveTableLookupFunction(HiveTableInputFormat inputFormat, String[] lookupKeys, Duration cacheTTL) {\n+\t\tlookupCols = new int[lookupKeys.length];\n+\t\tString[] allFields = inputFormat.getFieldNames();\n+\t\tMap<String, Integer> nameToIndex = IntStream.range(0, allFields.length).boxed().collect(\n+\t\t\t\tCollectors.toMap(i -> allFields[i], i -> i));\n+\t\tList<Integer> selectedIndices = Arrays.stream(inputFormat.getSelectedFields()).boxed().collect(Collectors.toList());\n+\t\tfor (int i = 0; i < lookupKeys.length; i++) {\n+\t\t\tInteger index = nameToIndex.get(lookupKeys[i]);\n+\t\t\tPreconditions.checkArgument(index != null, \"Lookup keys %s not found in table schema\", Arrays.toString(lookupKeys));\n+\t\t\tindex = selectedIndices.indexOf(index);\n+\t\t\tPreconditions.checkArgument(index >= 0, \"Lookup keys %s not selected\", Arrays.toString(lookupKeys));\n+\t\t\tlookupCols[i] = index;\n+\t\t}\n+\t\tthis.inputFormat = inputFormat;\n+\t\tthis.cacheTTL = cacheTTL;\n+\t}\n+\n+\t@Override\n+\tpublic TypeInformation<RowData> getResultType() {\n+\t\tString[] allNames = inputFormat.getFieldNames();\n+\t\tDataType[] allTypes = inputFormat.getFieldTypes();\n+\t\tint[] selected = inputFormat.getSelectedFields();\n+\t\treturn new RowDataTypeInfo(\n+\t\t\t\tArrays.stream(selected).mapToObj(i -> allTypes[i].getLogicalType()).toArray(LogicalType[]::new),\n+\t\t\t\tArrays.stream(selected).mapToObj(i -> allNames[i]).toArray(String[]::new));\n+\t}\n+\n+\t@Override\n+\tpublic void open(FunctionContext context) throws Exception {\n+\t\tsuper.open(context);\n+\t\tcache = new HashMap<>();\n+\t\tcacheExpire = -1;\n+\t}\n+\n+\tpublic void eval(Object... values) {\n+\t\tPreconditions.checkArgument(values.length == lookupCols.length, \"Number of values and lookup keys mismatch\");\n+\t\treloadCache();\n+\t\tRowData probeKey = GenericRowData.of(values);\n+\t\tList<RowData> matchedRows = cache.get(probeKey);\n+\t\tif (matchedRows != null) {\n+\t\t\tfor (RowData matchedRow : matchedRows) {\n+\t\t\t\tcollect(matchedRow);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t@VisibleForTesting\n+\tpublic Duration getCacheTTL() {\n+\t\treturn cacheTTL;\n+\t}\n+\n+\tprivate void reloadCache() {\n+\t\tif (cacheExpire > System.currentTimeMillis()) {\n+\t\t\treturn;\n+\t\t}\n+\t\tcache.clear();\n+\t\ttry {\n+\t\t\tHiveTableInputSplit[] inputSplits = inputFormat.createInputSplits(1);\n+\t\t\tGenericRowData reuse = new GenericRowData(inputFormat.getSelectedFields().length);\n+\t\t\tfor (HiveTableInputSplit split : inputSplits) {\n+\t\t\t\tinputFormat.open(split);\n+\t\t\t\twhile (!inputFormat.reachedEnd()) {\n+\t\t\t\t\tGenericRowData row = (GenericRowData) inputFormat.nextRecord(reuse);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c9a2b1d53715e766991efb85ec98ba96c14f0f7b"}, "originalPosition": 120}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQ4MjE4Mw==", "bodyText": "You should use type serializer to copy.", "url": "https://github.com/apache/flink/pull/11990#discussion_r423482183", "createdAt": "2020-05-12T06:07:09Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveTableLookupFunction.java", "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive.read;\n+\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.connectors.hive.FlinkHiveException;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.functions.FunctionContext;\n+import org.apache.flink.table.functions.TableFunction;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+/**\n+ * Lookup table function for Hive tables.\n+ */\n+public class HiveTableLookupFunction extends TableFunction<RowData> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate final HiveTableInputFormat inputFormat;\n+\t// indices of lookup columns in the record returned by input format\n+\tprivate final int[] lookupCols;\n+\tprivate transient Map<RowData, List<RowData>> cache;\n+\t// timestamp when cache expires\n+\tprivate transient long cacheExpire;\n+\tprivate final Duration cacheTTL;\n+\n+\tpublic HiveTableLookupFunction(HiveTableInputFormat inputFormat, String[] lookupKeys, Duration cacheTTL) {\n+\t\tlookupCols = new int[lookupKeys.length];\n+\t\tString[] allFields = inputFormat.getFieldNames();\n+\t\tMap<String, Integer> nameToIndex = IntStream.range(0, allFields.length).boxed().collect(\n+\t\t\t\tCollectors.toMap(i -> allFields[i], i -> i));\n+\t\tList<Integer> selectedIndices = Arrays.stream(inputFormat.getSelectedFields()).boxed().collect(Collectors.toList());\n+\t\tfor (int i = 0; i < lookupKeys.length; i++) {\n+\t\t\tInteger index = nameToIndex.get(lookupKeys[i]);\n+\t\t\tPreconditions.checkArgument(index != null, \"Lookup keys %s not found in table schema\", Arrays.toString(lookupKeys));\n+\t\t\tindex = selectedIndices.indexOf(index);\n+\t\t\tPreconditions.checkArgument(index >= 0, \"Lookup keys %s not selected\", Arrays.toString(lookupKeys));\n+\t\t\tlookupCols[i] = index;\n+\t\t}\n+\t\tthis.inputFormat = inputFormat;\n+\t\tthis.cacheTTL = cacheTTL;\n+\t}\n+\n+\t@Override\n+\tpublic TypeInformation<RowData> getResultType() {\n+\t\tString[] allNames = inputFormat.getFieldNames();\n+\t\tDataType[] allTypes = inputFormat.getFieldTypes();\n+\t\tint[] selected = inputFormat.getSelectedFields();\n+\t\treturn new RowDataTypeInfo(\n+\t\t\t\tArrays.stream(selected).mapToObj(i -> allTypes[i].getLogicalType()).toArray(LogicalType[]::new),\n+\t\t\t\tArrays.stream(selected).mapToObj(i -> allNames[i]).toArray(String[]::new));\n+\t}\n+\n+\t@Override\n+\tpublic void open(FunctionContext context) throws Exception {\n+\t\tsuper.open(context);\n+\t\tcache = new HashMap<>();\n+\t\tcacheExpire = -1;\n+\t}\n+\n+\tpublic void eval(Object... values) {\n+\t\tPreconditions.checkArgument(values.length == lookupCols.length, \"Number of values and lookup keys mismatch\");\n+\t\treloadCache();\n+\t\tRowData probeKey = GenericRowData.of(values);\n+\t\tList<RowData> matchedRows = cache.get(probeKey);\n+\t\tif (matchedRows != null) {\n+\t\t\tfor (RowData matchedRow : matchedRows) {\n+\t\t\t\tcollect(matchedRow);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t@VisibleForTesting\n+\tpublic Duration getCacheTTL() {\n+\t\treturn cacheTTL;\n+\t}\n+\n+\tprivate void reloadCache() {\n+\t\tif (cacheExpire > System.currentTimeMillis()) {\n+\t\t\treturn;\n+\t\t}\n+\t\tcache.clear();\n+\t\ttry {\n+\t\t\tHiveTableInputSplit[] inputSplits = inputFormat.createInputSplits(1);\n+\t\t\tGenericRowData reuse = new GenericRowData(inputFormat.getSelectedFields().length);\n+\t\t\tfor (HiveTableInputSplit split : inputSplits) {\n+\t\t\t\tinputFormat.open(split);\n+\t\t\t\twhile (!inputFormat.reachedEnd()) {\n+\t\t\t\t\tGenericRowData row = (GenericRowData) inputFormat.nextRecord(reuse);\n+\t\t\t\t\tGenericRowData key = extractKey(row);\n+\t\t\t\t\tList<RowData> rows = cache.computeIfAbsent(key, k -> new ArrayList<>());\n+\t\t\t\t\trows.add(copyReference(row));\n+\t\t\t\t}\n+\t\t\t\tinputFormat.close();\n+\t\t\t}\n+\t\t\tcacheExpire = System.currentTimeMillis() + cacheTTL.toMillis();\n+\t\t} catch (IOException e) {\n+\t\t\tthrow new FlinkHiveException(\"Failed to load hive table into cache\", e);\n+\t\t}\n+\t}\n+\n+\tprivate GenericRowData extractKey(GenericRowData row) {\n+\t\tGenericRowData key = new GenericRowData(lookupCols.length);\n+\t\tfor (int i = 0; i < lookupCols.length; i++) {\n+\t\t\tkey.setField(i, row.getField(lookupCols[i]));\n+\t\t}\n+\t\treturn key;\n+\t}\n+\n+\tprivate static GenericRowData copyReference(GenericRowData rowData) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c9a2b1d53715e766991efb85ec98ba96c14f0f7b"}, "originalPosition": 141}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQ4MjMxNg==", "bodyText": "It is external objects, instead of internal.", "url": "https://github.com/apache/flink/pull/11990#discussion_r423482316", "createdAt": "2020-05-12T06:07:31Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveTableLookupFunction.java", "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive.read;\n+\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.connectors.hive.FlinkHiveException;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.functions.FunctionContext;\n+import org.apache.flink.table.functions.TableFunction;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+/**\n+ * Lookup table function for Hive tables.\n+ */\n+public class HiveTableLookupFunction extends TableFunction<RowData> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate final HiveTableInputFormat inputFormat;\n+\t// indices of lookup columns in the record returned by input format\n+\tprivate final int[] lookupCols;\n+\tprivate transient Map<RowData, List<RowData>> cache;\n+\t// timestamp when cache expires\n+\tprivate transient long cacheExpire;\n+\tprivate final Duration cacheTTL;\n+\n+\tpublic HiveTableLookupFunction(HiveTableInputFormat inputFormat, String[] lookupKeys, Duration cacheTTL) {\n+\t\tlookupCols = new int[lookupKeys.length];\n+\t\tString[] allFields = inputFormat.getFieldNames();\n+\t\tMap<String, Integer> nameToIndex = IntStream.range(0, allFields.length).boxed().collect(\n+\t\t\t\tCollectors.toMap(i -> allFields[i], i -> i));\n+\t\tList<Integer> selectedIndices = Arrays.stream(inputFormat.getSelectedFields()).boxed().collect(Collectors.toList());\n+\t\tfor (int i = 0; i < lookupKeys.length; i++) {\n+\t\t\tInteger index = nameToIndex.get(lookupKeys[i]);\n+\t\t\tPreconditions.checkArgument(index != null, \"Lookup keys %s not found in table schema\", Arrays.toString(lookupKeys));\n+\t\t\tindex = selectedIndices.indexOf(index);\n+\t\t\tPreconditions.checkArgument(index >= 0, \"Lookup keys %s not selected\", Arrays.toString(lookupKeys));\n+\t\t\tlookupCols[i] = index;\n+\t\t}\n+\t\tthis.inputFormat = inputFormat;\n+\t\tthis.cacheTTL = cacheTTL;\n+\t}\n+\n+\t@Override\n+\tpublic TypeInformation<RowData> getResultType() {\n+\t\tString[] allNames = inputFormat.getFieldNames();\n+\t\tDataType[] allTypes = inputFormat.getFieldTypes();\n+\t\tint[] selected = inputFormat.getSelectedFields();\n+\t\treturn new RowDataTypeInfo(\n+\t\t\t\tArrays.stream(selected).mapToObj(i -> allTypes[i].getLogicalType()).toArray(LogicalType[]::new),\n+\t\t\t\tArrays.stream(selected).mapToObj(i -> allNames[i]).toArray(String[]::new));\n+\t}\n+\n+\t@Override\n+\tpublic void open(FunctionContext context) throws Exception {\n+\t\tsuper.open(context);\n+\t\tcache = new HashMap<>();\n+\t\tcacheExpire = -1;\n+\t}\n+\n+\tpublic void eval(Object... values) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c9a2b1d53715e766991efb85ec98ba96c14f0f7b"}, "originalPosition": 92}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQ4MjQ0Ng==", "bodyText": "Please add test for string key.", "url": "https://github.com/apache/flink/pull/11990#discussion_r423482446", "createdAt": "2020-05-12T06:07:50Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveLookupJoinTest.java", "diffHunk": "@@ -0,0 +1,90 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+import org.apache.flink.connectors.hive.read.HiveTableLookupFunction;\n+import org.apache.flink.table.api.EnvironmentSettings;\n+import org.apache.flink.table.api.TableEnvironment;\n+import org.apache.flink.table.api.TableUtils;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.catalog.ObjectIdentifier;\n+import org.apache.flink.table.catalog.hive.HiveCatalog;\n+import org.apache.flink.table.catalog.hive.HiveTestUtils;\n+import org.apache.flink.table.factories.TableSourceFactoryContextImpl;\n+import org.apache.flink.table.planner.factories.utils.TestCollectionTableFactory;\n+import org.apache.flink.types.Row;\n+\n+import com.klarna.hiverunner.HiveShell;\n+import com.klarna.hiverunner.annotations.HiveSQL;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+\n+import java.time.Duration;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+import static org.junit.Assert.assertEquals;\n+\n+/**\n+ * Test lookup join of hive tables.\n+ */\n+@RunWith(FlinkStandaloneHiveRunner.class)\n+public class HiveLookupJoinTest {\n+\n+\t@HiveSQL(files = {})\n+\tprivate static HiveShell hiveShell;\n+\n+\t@Test\n+\tpublic void test() throws Exception {\n+\t\tEnvironmentSettings settings = EnvironmentSettings.newInstance().inStreamingMode().useBlinkPlanner().build();\n+\t\tTableEnvironment tableEnv = TableEnvironment.create(settings);\n+\t\tHiveCatalog hiveCatalog = HiveTestUtils.createHiveCatalog(hiveShell.getHiveConf());\n+\t\ttableEnv.registerCatalog(hiveCatalog.getName(), hiveCatalog);\n+\t\ttableEnv.useCatalog(hiveCatalog.getName());\n+\n+\t\thiveShell.execute(String.format(\"create table build (x int,y string,z int) tblproperties ('%s'='5')\",\n+\t\t\t\tHiveOptions.LOOKUP_JOIN_CACHE_TTL.key()));\n+\n+\t\t// verify we properly configured the cache TTL\n+\t\tObjectIdentifier tableIdentifier = ObjectIdentifier.of(hiveCatalog.getName(), \"default\", \"build\");\n+\t\tCatalogTable catalogTable = (CatalogTable) hiveCatalog.getTable(tableIdentifier.toObjectPath());\n+\t\tHiveTableSource hiveTableSource = (HiveTableSource) ((HiveTableFactory) hiveCatalog.getTableFactory().get()).createTableSource(\n+\t\t\t\tnew TableSourceFactoryContextImpl(tableIdentifier, catalogTable, tableEnv.getConfig().getConfiguration()));\n+\t\tHiveTableLookupFunction lookupFunction = (HiveTableLookupFunction) hiveTableSource.getLookupFunction(new String[]{\"x\"});\n+\t\tassertEquals(Duration.ofMinutes(5), lookupFunction.getCacheTTL());\n+\n+\t\ttry {\n+\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"default\", \"build\")\n+\t\t\t\t\t.addRow(new Object[]{1, \"a\", 10})\n+\t\t\t\t\t.addRow(new Object[]{2, \"a\", 21})\n+\t\t\t\t\t.addRow(new Object[]{2, \"b\", 22})\n+\t\t\t\t\t.addRow(new Object[]{3, \"c\", 33})\n+\t\t\t\t\t.commit();\n+\n+\t\t\tTestCollectionTableFactory.initData(Arrays.asList(Row.of(1, 1), Row.of(1, 0), Row.of(2, 1), Row.of(2, 3), Row.of(3, 1), Row.of(4, 4)));\n+\t\t\ttableEnv.sqlUpdate(\"create table default_catalog.default_database.probe (x int,y int,p as proctime()) with ('connector'='COLLECTION','is-bounded' = 'false')\");\n+\n+\t\t\tList<Row> results = TableUtils.collectToList(tableEnv.sqlQuery(\"select p.x,p.y from default_catalog.default_database.probe as p join \" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c9a2b1d53715e766991efb85ec98ba96c14f0f7b"}, "originalPosition": 83}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "c9a2b1d53715e766991efb85ec98ba96c14f0f7b", "author": {"user": {"login": "lirui-apache", "name": "Rui Li"}}, "url": "https://github.com/apache/flink/commit/c9a2b1d53715e766991efb85ec98ba96c14f0f7b", "committedDate": "2020-05-09T03:07:23Z", "message": "make cache ttl configurable"}, "afterCommit": {"oid": "3591a7e419816e0dc6a13110e180df331726fa44", "author": {"user": {"login": "lirui-apache", "name": "Rui Li"}}, "url": "https://github.com/apache/flink/commit/3591a7e419816e0dc6a13110e180df331726fa44", "committedDate": "2020-05-12T15:41:57Z", "message": "address comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "56807c3ce9342de2d968ebc87dc08526d41a34b3", "author": {"user": {"login": "lirui-apache", "name": "Rui Li"}}, "url": "https://github.com/apache/flink/commit/56807c3ce9342de2d968ebc87dc08526d41a34b3", "committedDate": "2020-05-14T09:09:26Z", "message": "[FLINK-17387][hive] Implement LookupableTableSource for Hive connector"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "68710aef41a6071c54efa41b27688d48d58aceae", "author": {"user": {"login": "lirui-apache", "name": "Rui Li"}}, "url": "https://github.com/apache/flink/commit/68710aef41a6071c54efa41b27688d48d58aceae", "committedDate": "2020-05-14T09:09:26Z", "message": "make cache ttl configurable"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "050f12452d8acee1c8d1caea478a036fcc32722d", "author": {"user": {"login": "lirui-apache", "name": "Rui Li"}}, "url": "https://github.com/apache/flink/commit/050f12452d8acee1c8d1caea478a036fcc32722d", "committedDate": "2020-05-14T09:09:26Z", "message": "address comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "660e6d3b46147c9f3d4aec7d095c38d16766aa73", "author": {"user": {"login": "lirui-apache", "name": "Rui Li"}}, "url": "https://github.com/apache/flink/commit/660e6d3b46147c9f3d4aec7d095c38d16766aa73", "committedDate": "2020-05-14T09:09:26Z", "message": "address comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "362c7705fb7e930c5de9a4e384cf6dc368b4e9d3", "author": {"user": {"login": "lirui-apache", "name": "Rui Li"}}, "url": "https://github.com/apache/flink/commit/362c7705fb7e930c5de9a4e384cf6dc368b4e9d3", "committedDate": "2020-05-14T09:09:26Z", "message": "address comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "897828752903134fb9bf2173b787c453774c4580", "author": {"user": {"login": "lirui-apache", "name": "Rui Li"}}, "url": "https://github.com/apache/flink/commit/897828752903134fb9bf2173b787c453774c4580", "committedDate": "2020-05-14T09:09:26Z", "message": "address comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDExNjAxMzUz", "url": "https://github.com/apache/flink/pull/11990#pullrequestreview-411601353", "createdAt": "2020-05-14T08:58:31Z", "commit": {"oid": "3591a7e419816e0dc6a13110e180df331726fa44"}, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQwODo1ODozMVrOGVSsBw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQwOToxNTowOFrOGVTVMQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDk3OTQ2Mw==", "bodyText": "Don't need this class?", "url": "https://github.com/apache/flink/pull/11990#discussion_r424979463", "createdAt": "2020-05-14T08:58:31Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemLookupFunction.java", "diffHunk": "@@ -0,0 +1,169 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.api.common.ExecutionConfig;\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.core.io.InputSplit;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.util.DataFormatConverters;\n+import org.apache.flink.table.data.util.DataFormatConverters.DataFormatConverter;\n+import org.apache.flink.table.functions.FunctionContext;\n+import org.apache.flink.table.functions.TableFunction;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.util.FlinkRuntimeException;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+/**\n+ * Lookup table function for filesystem connector tables.\n+ */\n+public class FileSystemLookupFunction<T extends InputSplit> extends TableFunction<RowData> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate final InputFormat<RowData, T> inputFormat;\n+\tprivate final LookupContext context;\n+\t// indices of lookup columns in the record returned by input format\n+\tprivate final int[] lookupCols;\n+\t// use Row as key because we'll get external data in eval\n+\tprivate transient Map<Row, List<RowData>> cache;\n+\t// timestamp when cache expires\n+\tprivate transient long nextLoadTime;\n+\t// serializer to copy RowData\n+\tprivate transient TypeSerializer<RowData> serializer;\n+\t// converters to convert data from internal to external in order to generate keys for the cache\n+\tprivate final DataFormatConverter[] converters;\n+\n+\tpublic FileSystemLookupFunction(InputFormat<RowData, T> inputFormat, String[] lookupKeys, LookupContext context) {\n+\t\tlookupCols = new int[lookupKeys.length];\n+\t\tconverters = new DataFormatConverter[lookupKeys.length];\n+\t\tMap<String, Integer> nameToIndex = IntStream.range(0, context.selectedNames.length).boxed().collect(\n+\t\t\t\tCollectors.toMap(i -> context.selectedNames[i], i -> i));\n+\t\tfor (int i = 0; i < lookupKeys.length; i++) {\n+\t\t\tInteger index = nameToIndex.get(lookupKeys[i]);\n+\t\t\tPreconditions.checkArgument(index != null, \"Lookup keys %s not selected\", Arrays.toString(lookupKeys));\n+\t\t\tconverters[i] = DataFormatConverters.getConverterForDataType(context.selectedTypes[index]);\n+\t\t\tlookupCols[i] = index;\n+\t\t}\n+\t\tthis.inputFormat = inputFormat;\n+\t\tthis.context = context;\n+\t}\n+\n+\t@Override\n+\tpublic TypeInformation<RowData> getResultType() {\n+\t\treturn new RowDataTypeInfo(\n+\t\t\t\tArrays.stream(context.selectedTypes).map(DataType::getLogicalType).toArray(LogicalType[]::new),\n+\t\t\t\tcontext.selectedNames);\n+\t}\n+\n+\t@Override\n+\tpublic void open(FunctionContext context) throws Exception {\n+\t\tsuper.open(context);\n+\t\tcache = new HashMap<>();\n+\t\tnextLoadTime = -1;\n+\t\t// TODO: get ExecutionConfig from context?\n+\t\tserializer = getResultType().createSerializer(new ExecutionConfig());\n+\t}\n+\n+\tpublic void eval(Object... values) {\n+\t\tPreconditions.checkArgument(values.length == lookupCols.length, \"Number of values and lookup keys mismatch\");\n+\t\tcheckCacheReload();\n+\t\tRow probeKey = Row.of(values);\n+\t\tList<RowData> matchedRows = cache.get(probeKey);\n+\t\tif (matchedRows != null) {\n+\t\t\tfor (RowData matchedRow : matchedRows) {\n+\t\t\t\tcollect(matchedRow);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t@VisibleForTesting\n+\tpublic Duration getCacheTTL() {\n+\t\treturn context.cacheTTL;\n+\t}\n+\n+\tprivate void checkCacheReload() {\n+\t\tif (nextLoadTime > System.currentTimeMillis()) {\n+\t\t\treturn;\n+\t\t}\n+\t\tcache.clear();\n+\t\ttry {\n+\t\t\tT[] inputSplits = inputFormat.createInputSplits(1);\n+\t\t\tGenericRowData reuse = new GenericRowData(context.selectedNames.length);\n+\t\t\tfor (T split : inputSplits) {\n+\t\t\t\tinputFormat.open(split);\n+\t\t\t\twhile (!inputFormat.reachedEnd()) {\n+\t\t\t\t\tRowData row = inputFormat.nextRecord(reuse);\n+\t\t\t\t\tRow key = extractKey(row);\n+\t\t\t\t\tList<RowData> rows = cache.computeIfAbsent(key, k -> new ArrayList<>());\n+\t\t\t\t\trows.add(serializer.copy(row));\n+\t\t\t\t}\n+\t\t\t\tinputFormat.close();\n+\t\t\t}\n+\t\t\tnextLoadTime = System.currentTimeMillis() + getCacheTTL().toMillis();\n+\t\t} catch (IOException e) {\n+\t\t\tthrow new FlinkRuntimeException(\"Failed to load table into cache\", e);\n+\t\t}\n+\t}\n+\n+\tprivate Row extractKey(RowData row) {\n+\t\tRow key = new Row(lookupCols.length);\n+\t\tfor (int i = 0; i < lookupCols.length; i++) {\n+\t\t\tkey.setField(i, converters[i].toExternal(row, lookupCols[i]));\n+\t\t}\n+\t\treturn key;\n+\t}\n+\n+\t/**\n+\t * A class to store context information for the lookup function.\n+\t */\n+\tpublic static class LookupContext implements Serializable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3591a7e419816e0dc6a13110e180df331726fa44"}, "originalPosition": 154}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDk4MjQ2OA==", "bodyText": "You can just use TimeUtils.parseDuration", "url": "https://github.com/apache/flink/pull/11990#discussion_r424982468", "createdAt": "2020-05-14T09:03:09Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSource.java", "diffHunk": "@@ -366,6 +375,12 @@ public boolean isLimitPushedDown() {\n \t\t\tList<String> partitionColNames = catalogTable.getPartitionKeys();\n \t\t\tTable hiveTable = client.getTable(dbName, tableName);\n \t\t\tProperties tableProps = HiveReflectionUtils.getTableMetadata(hiveShim, hiveTable);\n+\t\t\tString ttlStr = tableProps.getProperty(FileSystemOptions.LOOKUP_JOIN_CACHE_TTL.key());\n+\t\t\tConfiguration configuration = new Configuration();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3591a7e419816e0dc6a13110e180df331726fa44"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDk4NTI0NQ==", "bodyText": "Using hive DDL in flink.", "url": "https://github.com/apache/flink/pull/11990#discussion_r424985245", "createdAt": "2020-05-14T09:07:34Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveLookupJoinTest.java", "diffHunk": "@@ -0,0 +1,96 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+import org.apache.flink.table.api.EnvironmentSettings;\n+import org.apache.flink.table.api.TableEnvironment;\n+import org.apache.flink.table.api.internal.TableImpl;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.catalog.ObjectIdentifier;\n+import org.apache.flink.table.catalog.hive.HiveCatalog;\n+import org.apache.flink.table.catalog.hive.HiveTestUtils;\n+import org.apache.flink.table.factories.TableSourceFactoryContextImpl;\n+import org.apache.flink.table.filesystem.FileSystemLookupFunction;\n+import org.apache.flink.table.filesystem.FileSystemOptions;\n+import org.apache.flink.table.planner.factories.utils.TestCollectionTableFactory;\n+import org.apache.flink.types.Row;\n+\n+import org.apache.flink.shaded.guava18.com.google.common.collect.Lists;\n+\n+import com.klarna.hiverunner.HiveShell;\n+import com.klarna.hiverunner.annotations.HiveSQL;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+\n+import java.time.Duration;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+import static org.junit.Assert.assertEquals;\n+\n+/**\n+ * Test lookup join of hive tables.\n+ */\n+@RunWith(FlinkStandaloneHiveRunner.class)\n+public class HiveLookupJoinTest {\n+\n+\t@HiveSQL(files = {})\n+\tprivate static HiveShell hiveShell;\n+\n+\t@Test\n+\tpublic void test() throws Exception {\n+\t\tEnvironmentSettings settings = EnvironmentSettings.newInstance().inStreamingMode().useBlinkPlanner().build();\n+\t\tTableEnvironment tableEnv = TableEnvironment.create(settings);\n+\t\tHiveCatalog hiveCatalog = HiveTestUtils.createHiveCatalog(hiveShell.getHiveConf());\n+\t\ttableEnv.registerCatalog(hiveCatalog.getName(), hiveCatalog);\n+\t\ttableEnv.useCatalog(hiveCatalog.getName());\n+\n+\t\thiveShell.execute(String.format(\"create table build (x int,y string,z int) tblproperties ('%s'='5min')\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3591a7e419816e0dc6a13110e180df331726fa44"}, "originalPosition": 64}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDk4NTQ4Mw==", "bodyText": "Put codes to before.", "url": "https://github.com/apache/flink/pull/11990#discussion_r424985483", "createdAt": "2020-05-14T09:07:59Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveLookupJoinTest.java", "diffHunk": "@@ -0,0 +1,96 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+import org.apache.flink.table.api.EnvironmentSettings;\n+import org.apache.flink.table.api.TableEnvironment;\n+import org.apache.flink.table.api.internal.TableImpl;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.catalog.ObjectIdentifier;\n+import org.apache.flink.table.catalog.hive.HiveCatalog;\n+import org.apache.flink.table.catalog.hive.HiveTestUtils;\n+import org.apache.flink.table.factories.TableSourceFactoryContextImpl;\n+import org.apache.flink.table.filesystem.FileSystemLookupFunction;\n+import org.apache.flink.table.filesystem.FileSystemOptions;\n+import org.apache.flink.table.planner.factories.utils.TestCollectionTableFactory;\n+import org.apache.flink.types.Row;\n+\n+import org.apache.flink.shaded.guava18.com.google.common.collect.Lists;\n+\n+import com.klarna.hiverunner.HiveShell;\n+import com.klarna.hiverunner.annotations.HiveSQL;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+\n+import java.time.Duration;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+import static org.junit.Assert.assertEquals;\n+\n+/**\n+ * Test lookup join of hive tables.\n+ */\n+@RunWith(FlinkStandaloneHiveRunner.class)\n+public class HiveLookupJoinTest {\n+\n+\t@HiveSQL(files = {})\n+\tprivate static HiveShell hiveShell;\n+\n+\t@Test\n+\tpublic void test() throws Exception {\n+\t\tEnvironmentSettings settings = EnvironmentSettings.newInstance().inStreamingMode().useBlinkPlanner().build();\n+\t\tTableEnvironment tableEnv = TableEnvironment.create(settings);\n+\t\tHiveCatalog hiveCatalog = HiveTestUtils.createHiveCatalog(hiveShell.getHiveConf());\n+\t\ttableEnv.registerCatalog(hiveCatalog.getName(), hiveCatalog);\n+\t\ttableEnv.useCatalog(hiveCatalog.getName());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3591a7e419816e0dc6a13110e180df331726fa44"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDk5MDAwMQ==", "bodyText": "values are internal structures, should be converted to external.", "url": "https://github.com/apache/flink/pull/11990#discussion_r424990001", "createdAt": "2020-05-14T09:15:08Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemLookupFunction.java", "diffHunk": "@@ -0,0 +1,169 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.api.common.ExecutionConfig;\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.core.io.InputSplit;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.util.DataFormatConverters;\n+import org.apache.flink.table.data.util.DataFormatConverters.DataFormatConverter;\n+import org.apache.flink.table.functions.FunctionContext;\n+import org.apache.flink.table.functions.TableFunction;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.util.FlinkRuntimeException;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+/**\n+ * Lookup table function for filesystem connector tables.\n+ */\n+public class FileSystemLookupFunction<T extends InputSplit> extends TableFunction<RowData> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate final InputFormat<RowData, T> inputFormat;\n+\tprivate final LookupContext context;\n+\t// indices of lookup columns in the record returned by input format\n+\tprivate final int[] lookupCols;\n+\t// use Row as key because we'll get external data in eval\n+\tprivate transient Map<Row, List<RowData>> cache;\n+\t// timestamp when cache expires\n+\tprivate transient long nextLoadTime;\n+\t// serializer to copy RowData\n+\tprivate transient TypeSerializer<RowData> serializer;\n+\t// converters to convert data from internal to external in order to generate keys for the cache\n+\tprivate final DataFormatConverter[] converters;\n+\n+\tpublic FileSystemLookupFunction(InputFormat<RowData, T> inputFormat, String[] lookupKeys, LookupContext context) {\n+\t\tlookupCols = new int[lookupKeys.length];\n+\t\tconverters = new DataFormatConverter[lookupKeys.length];\n+\t\tMap<String, Integer> nameToIndex = IntStream.range(0, context.selectedNames.length).boxed().collect(\n+\t\t\t\tCollectors.toMap(i -> context.selectedNames[i], i -> i));\n+\t\tfor (int i = 0; i < lookupKeys.length; i++) {\n+\t\t\tInteger index = nameToIndex.get(lookupKeys[i]);\n+\t\t\tPreconditions.checkArgument(index != null, \"Lookup keys %s not selected\", Arrays.toString(lookupKeys));\n+\t\t\tconverters[i] = DataFormatConverters.getConverterForDataType(context.selectedTypes[index]);\n+\t\t\tlookupCols[i] = index;\n+\t\t}\n+\t\tthis.inputFormat = inputFormat;\n+\t\tthis.context = context;\n+\t}\n+\n+\t@Override\n+\tpublic TypeInformation<RowData> getResultType() {\n+\t\treturn new RowDataTypeInfo(\n+\t\t\t\tArrays.stream(context.selectedTypes).map(DataType::getLogicalType).toArray(LogicalType[]::new),\n+\t\t\t\tcontext.selectedNames);\n+\t}\n+\n+\t@Override\n+\tpublic void open(FunctionContext context) throws Exception {\n+\t\tsuper.open(context);\n+\t\tcache = new HashMap<>();\n+\t\tnextLoadTime = -1;\n+\t\t// TODO: get ExecutionConfig from context?\n+\t\tserializer = getResultType().createSerializer(new ExecutionConfig());\n+\t}\n+\n+\tpublic void eval(Object... values) {\n+\t\tPreconditions.checkArgument(values.length == lookupCols.length, \"Number of values and lookup keys mismatch\");\n+\t\tcheckCacheReload();\n+\t\tRow probeKey = Row.of(values);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3591a7e419816e0dc6a13110e180df331726fa44"}, "originalPosition": 105}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "013cea649d481b713737fd32f94dd8b46d0f3426", "author": {"user": {"login": "lirui-apache", "name": "Rui Li"}}, "url": "https://github.com/apache/flink/commit/013cea649d481b713737fd32f94dd8b46d0f3426", "committedDate": "2020-05-14T09:56:20Z", "message": "address comments"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "3591a7e419816e0dc6a13110e180df331726fa44", "author": {"user": {"login": "lirui-apache", "name": "Rui Li"}}, "url": "https://github.com/apache/flink/commit/3591a7e419816e0dc6a13110e180df331726fa44", "committedDate": "2020-05-12T15:41:57Z", "message": "address comments"}, "afterCommit": {"oid": "013cea649d481b713737fd32f94dd8b46d0f3426", "author": {"user": {"login": "lirui-apache", "name": "Rui Li"}}, "url": "https://github.com/apache/flink/commit/013cea649d481b713737fd32f94dd8b46d0f3426", "committedDate": "2020-05-14T09:56:20Z", "message": "address comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "99f3bca89ea39ba4b354fee78dcc233394b6477e", "author": {"user": {"login": "lirui-apache", "name": "Rui Li"}}, "url": "https://github.com/apache/flink/commit/99f3bca89ea39ba4b354fee78dcc233394b6477e", "committedDate": "2020-05-14T10:00:55Z", "message": "update doc"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDEyMjkzODE1", "url": "https://github.com/apache/flink/pull/11990#pullrequestreview-412293815", "createdAt": "2020-05-15T01:44:42Z", "commit": {"oid": "99f3bca89ea39ba4b354fee78dcc233394b6477e"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4557, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}