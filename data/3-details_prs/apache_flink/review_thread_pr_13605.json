{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTAyMDEzOTQ5", "number": 13605, "reviewThreads": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOFQwMzo0NToxOVrOEyo1hg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOFQwNDozMzowNFrOEypWIA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIxNTMzMzE4OnYy", "diffSide": "RIGHT", "path": "flink-formats/flink-parquet/src/main/java/org/apache/flink/formats/parquet/ParquetFileSystemFormatFactory.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOFQwMzo0NToxOVrOHpY-Vg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOFQwNjoxMToxM1rOHpbU5A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzE2MjgzOA==", "bodyText": "Why UTC_TIMEZONE option is not in the set?", "url": "https://github.com/apache/flink/pull/13605#discussion_r513162838", "createdAt": "2020-10-28T03:45:19Z", "author": {"login": "wuchong"}, "path": "flink-formats/flink-parquet/src/main/java/org/apache/flink/formats/parquet/ParquetFileSystemFormatFactory.java", "diffHunk": "@@ -93,124 +117,17 @@ private static Configuration getParquetConfiguration(ReadableConfig options) {\n \t}\n \n \t@Override\n-\tpublic InputFormat<RowData, ?> createReader(ReaderContext context) {\n-\t\treturn new ParquetInputFormat(\n-\t\t\t\tcontext.getPaths(),\n-\t\t\t\tcontext.getSchema().getFieldNames(),\n-\t\t\t\tcontext.getSchema().getFieldDataTypes(),\n-\t\t\t\tcontext.getProjectFields(),\n-\t\t\t\tcontext.getDefaultPartName(),\n-\t\t\t\tcontext.getPushedDownLimit(),\n-\t\t\t\tgetParquetConfiguration(context.getFormatOptions()),\n-\t\t\t\tcontext.getFormatOptions().get(UTC_TIMEZONE));\n+\tpublic String factoryIdentifier() {\n+\t\treturn \"parquet\";\n \t}\n \n \t@Override\n-\tpublic Optional<BulkWriter.Factory<RowData>> createBulkWriterFactory(WriterContext context) {\n-\t\treturn Optional.of(ParquetRowDataBuilder.createWriterFactory(\n-\t\t\t\tRowType.of(Arrays.stream(context.getFormatFieldTypes())\n-\t\t\t\t\t\t\t\t.map(DataType::getLogicalType)\n-\t\t\t\t\t\t\t\t.toArray(LogicalType[]::new),\n-\t\t\t\t\t\tcontext.getFormatFieldNames()),\n-\t\t\t\tgetParquetConfiguration(context.getFormatOptions()),\n-\t\t\t\tcontext.getFormatOptions().get(UTC_TIMEZONE)));\n+\tpublic Set<ConfigOption<?>> requiredOptions() {\n+\t\treturn new HashSet<>();\n \t}\n \n \t@Override\n-\tpublic Optional<Encoder<RowData>> createEncoder(WriterContext context) {\n-\t\treturn Optional.empty();\n-\t}\n-\n-\t/**\n-\t * An implementation of {@link ParquetInputFormat} to read {@link RowData} records\n-\t * from Parquet files.\n-\t */\n-\tpublic static class ParquetInputFormat extends FileInputFormat<RowData> {\n-\n-\t\tprivate static final long serialVersionUID = 1L;\n-\n-\t\tprivate final String[] fullFieldNames;\n-\t\tprivate final DataType[] fullFieldTypes;\n-\t\tprivate final int[] selectedFields;\n-\t\tprivate final String partDefaultName;\n-\t\tprivate final boolean utcTimestamp;\n-\t\tprivate final SerializableConfiguration conf;\n-\t\tprivate final long limit;\n-\n-\t\tprivate transient ParquetColumnarRowSplitReader reader;\n-\t\tprivate transient long currentReadCount;\n-\n-\t\tpublic ParquetInputFormat(\n-\t\t\t\tPath[] paths,\n-\t\t\t\tString[] fullFieldNames,\n-\t\t\t\tDataType[] fullFieldTypes,\n-\t\t\t\tint[] selectedFields,\n-\t\t\t\tString partDefaultName,\n-\t\t\t\tlong limit,\n-\t\t\t\tConfiguration conf,\n-\t\t\t\tboolean utcTimestamp) {\n-\t\t\tsuper.setFilePaths(paths);\n-\t\t\tthis.limit = limit;\n-\t\t\tthis.partDefaultName = partDefaultName;\n-\t\t\tthis.fullFieldNames = fullFieldNames;\n-\t\t\tthis.fullFieldTypes = fullFieldTypes;\n-\t\t\tthis.selectedFields = selectedFields;\n-\t\t\tthis.conf = new SerializableConfiguration(conf);\n-\t\t\tthis.utcTimestamp = utcTimestamp;\n-\t\t}\n-\n-\t\t@Override\n-\t\tpublic void open(FileInputSplit fileSplit) throws IOException {\n-\t\t\t// generate partition specs.\n-\t\t\tList<String> fieldNameList = Arrays.asList(fullFieldNames);\n-\t\t\tLinkedHashMap<String, String> partSpec = PartitionPathUtils.extractPartitionSpecFromPath(\n-\t\t\t\t\tfileSplit.getPath());\n-\t\t\tLinkedHashMap<String, Object> partObjects = new LinkedHashMap<>();\n-\t\t\tpartSpec.forEach((k, v) -> partObjects.put(k, restorePartValueFromType(\n-\t\t\t\t\tpartDefaultName.equals(v) ? null : v,\n-\t\t\t\t\tfullFieldTypes[fieldNameList.indexOf(k)])));\n-\n-\t\t\tthis.reader = ParquetSplitReaderUtil.genPartColumnarRowReader(\n-\t\t\t\t\tutcTimestamp,\n-\t\t\t\t\ttrue,\n-\t\t\t\t\tconf.conf(),\n-\t\t\t\t\tfullFieldNames,\n-\t\t\t\t\tfullFieldTypes,\n-\t\t\t\t\tpartObjects,\n-\t\t\t\t\tselectedFields,\n-\t\t\t\t\tDEFAULT_SIZE,\n-\t\t\t\t\tnew Path(fileSplit.getPath().toString()),\n-\t\t\t\t\tfileSplit.getStart(),\n-\t\t\t\t\tfileSplit.getLength());\n-\t\t\tthis.currentReadCount = 0L;\n-\t\t}\n-\n-\t\t@Override\n-\t\tpublic boolean supportsMultiPaths() {\n-\t\t\treturn true;\n-\t\t}\n-\n-\t\t@Override\n-\t\tpublic boolean reachedEnd() throws IOException {\n-\t\t\tif (currentReadCount >= limit) {\n-\t\t\t\treturn true;\n-\t\t\t} else {\n-\t\t\t\treturn reader.reachedEnd();\n-\t\t\t}\n-\t\t}\n-\n-\t\t@Override\n-\t\tpublic RowData nextRecord(RowData reuse) {\n-\t\t\tcurrentReadCount++;\n-\t\t\treturn reader.nextRecord();\n-\t\t}\n-\n-\t\t@Override\n-\t\tpublic void close() throws IOException {\n-\t\t\tif (reader != null) {\n-\t\t\t\tthis.reader.close();\n-\t\t\t}\n-\t\t\tthis.reader = null;\n-\t\t}\n+\tpublic Set<ConfigOption<?>> optionalOptions() {\n+\t\treturn new HashSet<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "05116d4768052821a37adfa725e60e40f4f71176"}, "originalPosition": 250}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzIwMTM4MA==", "bodyText": "We can't enumerate all of its options, so we can't validate them, so we simply don't add any", "url": "https://github.com/apache/flink/pull/13605#discussion_r513201380", "createdAt": "2020-10-28T06:11:13Z", "author": {"login": "JingsongLi"}, "path": "flink-formats/flink-parquet/src/main/java/org/apache/flink/formats/parquet/ParquetFileSystemFormatFactory.java", "diffHunk": "@@ -93,124 +117,17 @@ private static Configuration getParquetConfiguration(ReadableConfig options) {\n \t}\n \n \t@Override\n-\tpublic InputFormat<RowData, ?> createReader(ReaderContext context) {\n-\t\treturn new ParquetInputFormat(\n-\t\t\t\tcontext.getPaths(),\n-\t\t\t\tcontext.getSchema().getFieldNames(),\n-\t\t\t\tcontext.getSchema().getFieldDataTypes(),\n-\t\t\t\tcontext.getProjectFields(),\n-\t\t\t\tcontext.getDefaultPartName(),\n-\t\t\t\tcontext.getPushedDownLimit(),\n-\t\t\t\tgetParquetConfiguration(context.getFormatOptions()),\n-\t\t\t\tcontext.getFormatOptions().get(UTC_TIMEZONE));\n+\tpublic String factoryIdentifier() {\n+\t\treturn \"parquet\";\n \t}\n \n \t@Override\n-\tpublic Optional<BulkWriter.Factory<RowData>> createBulkWriterFactory(WriterContext context) {\n-\t\treturn Optional.of(ParquetRowDataBuilder.createWriterFactory(\n-\t\t\t\tRowType.of(Arrays.stream(context.getFormatFieldTypes())\n-\t\t\t\t\t\t\t\t.map(DataType::getLogicalType)\n-\t\t\t\t\t\t\t\t.toArray(LogicalType[]::new),\n-\t\t\t\t\t\tcontext.getFormatFieldNames()),\n-\t\t\t\tgetParquetConfiguration(context.getFormatOptions()),\n-\t\t\t\tcontext.getFormatOptions().get(UTC_TIMEZONE)));\n+\tpublic Set<ConfigOption<?>> requiredOptions() {\n+\t\treturn new HashSet<>();\n \t}\n \n \t@Override\n-\tpublic Optional<Encoder<RowData>> createEncoder(WriterContext context) {\n-\t\treturn Optional.empty();\n-\t}\n-\n-\t/**\n-\t * An implementation of {@link ParquetInputFormat} to read {@link RowData} records\n-\t * from Parquet files.\n-\t */\n-\tpublic static class ParquetInputFormat extends FileInputFormat<RowData> {\n-\n-\t\tprivate static final long serialVersionUID = 1L;\n-\n-\t\tprivate final String[] fullFieldNames;\n-\t\tprivate final DataType[] fullFieldTypes;\n-\t\tprivate final int[] selectedFields;\n-\t\tprivate final String partDefaultName;\n-\t\tprivate final boolean utcTimestamp;\n-\t\tprivate final SerializableConfiguration conf;\n-\t\tprivate final long limit;\n-\n-\t\tprivate transient ParquetColumnarRowSplitReader reader;\n-\t\tprivate transient long currentReadCount;\n-\n-\t\tpublic ParquetInputFormat(\n-\t\t\t\tPath[] paths,\n-\t\t\t\tString[] fullFieldNames,\n-\t\t\t\tDataType[] fullFieldTypes,\n-\t\t\t\tint[] selectedFields,\n-\t\t\t\tString partDefaultName,\n-\t\t\t\tlong limit,\n-\t\t\t\tConfiguration conf,\n-\t\t\t\tboolean utcTimestamp) {\n-\t\t\tsuper.setFilePaths(paths);\n-\t\t\tthis.limit = limit;\n-\t\t\tthis.partDefaultName = partDefaultName;\n-\t\t\tthis.fullFieldNames = fullFieldNames;\n-\t\t\tthis.fullFieldTypes = fullFieldTypes;\n-\t\t\tthis.selectedFields = selectedFields;\n-\t\t\tthis.conf = new SerializableConfiguration(conf);\n-\t\t\tthis.utcTimestamp = utcTimestamp;\n-\t\t}\n-\n-\t\t@Override\n-\t\tpublic void open(FileInputSplit fileSplit) throws IOException {\n-\t\t\t// generate partition specs.\n-\t\t\tList<String> fieldNameList = Arrays.asList(fullFieldNames);\n-\t\t\tLinkedHashMap<String, String> partSpec = PartitionPathUtils.extractPartitionSpecFromPath(\n-\t\t\t\t\tfileSplit.getPath());\n-\t\t\tLinkedHashMap<String, Object> partObjects = new LinkedHashMap<>();\n-\t\t\tpartSpec.forEach((k, v) -> partObjects.put(k, restorePartValueFromType(\n-\t\t\t\t\tpartDefaultName.equals(v) ? null : v,\n-\t\t\t\t\tfullFieldTypes[fieldNameList.indexOf(k)])));\n-\n-\t\t\tthis.reader = ParquetSplitReaderUtil.genPartColumnarRowReader(\n-\t\t\t\t\tutcTimestamp,\n-\t\t\t\t\ttrue,\n-\t\t\t\t\tconf.conf(),\n-\t\t\t\t\tfullFieldNames,\n-\t\t\t\t\tfullFieldTypes,\n-\t\t\t\t\tpartObjects,\n-\t\t\t\t\tselectedFields,\n-\t\t\t\t\tDEFAULT_SIZE,\n-\t\t\t\t\tnew Path(fileSplit.getPath().toString()),\n-\t\t\t\t\tfileSplit.getStart(),\n-\t\t\t\t\tfileSplit.getLength());\n-\t\t\tthis.currentReadCount = 0L;\n-\t\t}\n-\n-\t\t@Override\n-\t\tpublic boolean supportsMultiPaths() {\n-\t\t\treturn true;\n-\t\t}\n-\n-\t\t@Override\n-\t\tpublic boolean reachedEnd() throws IOException {\n-\t\t\tif (currentReadCount >= limit) {\n-\t\t\t\treturn true;\n-\t\t\t} else {\n-\t\t\t\treturn reader.reachedEnd();\n-\t\t\t}\n-\t\t}\n-\n-\t\t@Override\n-\t\tpublic RowData nextRecord(RowData reuse) {\n-\t\t\tcurrentReadCount++;\n-\t\t\treturn reader.nextRecord();\n-\t\t}\n-\n-\t\t@Override\n-\t\tpublic void close() throws IOException {\n-\t\t\tif (reader != null) {\n-\t\t\t\tthis.reader.close();\n-\t\t\t}\n-\t\t\tthis.reader = null;\n-\t\t}\n+\tpublic Set<ConfigOption<?>> optionalOptions() {\n+\t\treturn new HashSet<>();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzE2MjgzOA=="}, "originalCommit": {"oid": "05116d4768052821a37adfa725e60e40f4f71176"}, "originalPosition": 250}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIxNTM0ODAzOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/BulkFormatFactory.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOFQwMzo1MzowOFrOHpZGig==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOFQwMzo1MzowOFrOHpZGig==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzE2NDkzOA==", "bodyText": "Remove this comment?", "url": "https://github.com/apache/flink/pull/13605#discussion_r513164938", "createdAt": "2020-10-28T03:53:08Z", "author": {"login": "wuchong"}, "path": "flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/BulkFormatFactory.java", "diffHunk": "@@ -0,0 +1,42 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.factories;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.configuration.ReadableConfig;\n+import org.apache.flink.connector.file.src.reader.BulkFormat;\n+import org.apache.flink.table.connector.format.BulkDecodingFormat;\n+import org.apache.flink.table.data.RowData;\n+\n+/**\n+ * Base interface for configuring a {@link BulkFormat} for file system connector.\n+ *\n+ * @see FactoryUtil#createTableFactoryHelper(DynamicTableFactory, DynamicTableFactory.Context)\n+ */\n+@Internal\n+public interface BulkFormatFactory extends DecodingFormatFactory<BulkFormat<RowData>> {\n+\t// interface is used for discovery but is already fully specified by the generics", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "05116d4768052821a37adfa725e60e40f4f71176"}, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIxNTM1MTE4OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/BulkFormatFactory.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOFQwMzo1NDo0N1rOHpZIOA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOFQwMzo1NDo0N1rOHpZIOA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzE2NTM2OA==", "bodyText": "How about naming this BulkReaderFormatFactory which is more align with BulkWriterFormatFactory.", "url": "https://github.com/apache/flink/pull/13605#discussion_r513165368", "createdAt": "2020-10-28T03:54:47Z", "author": {"login": "wuchong"}, "path": "flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/BulkFormatFactory.java", "diffHunk": "@@ -0,0 +1,42 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.factories;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.configuration.ReadableConfig;\n+import org.apache.flink.connector.file.src.reader.BulkFormat;\n+import org.apache.flink.table.connector.format.BulkDecodingFormat;\n+import org.apache.flink.table.data.RowData;\n+\n+/**\n+ * Base interface for configuring a {@link BulkFormat} for file system connector.\n+ *\n+ * @see FactoryUtil#createTableFactoryHelper(DynamicTableFactory, DynamicTableFactory.Context)\n+ */\n+@Internal\n+public interface BulkFormatFactory extends DecodingFormatFactory<BulkFormat<RowData>> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "05116d4768052821a37adfa725e60e40f4f71176"}, "originalPosition": 33}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIxNTM1MTk5OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/BulkWriterFactory.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOFQwMzo1NToxMFrOHpZInA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOFQwMzo1NToxMFrOHpZInA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzE2NTQ2OA==", "bodyText": "How about naming this BulkWriterFormatFactory which is more align with BulkReaderFormatFactory.", "url": "https://github.com/apache/flink/pull/13605#discussion_r513165468", "createdAt": "2020-10-28T03:55:10Z", "author": {"login": "wuchong"}, "path": "flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/BulkWriterFactory.java", "diffHunk": "@@ -0,0 +1,33 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.factories;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.table.data.RowData;\n+\n+/**\n+ * Base interface for configuring a {@link BulkWriter.Factory} for file system connector.\n+ *\n+ * @see FactoryUtil#createTableFactoryHelper(DynamicTableFactory, DynamicTableFactory.Context)\n+ */\n+@Internal\n+public interface BulkWriterFactory extends EncodingFormatFactory<BulkWriter.Factory<RowData>> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "05116d4768052821a37adfa725e60e40f4f71176"}, "originalPosition": 31}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIxNTM1NzEwOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/EncoderFactory.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOFQwMzo1ODozNFrOHpZLlg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOFQwNjowNDo1OFrOHpbOEg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzE2NjIzMA==", "bodyText": "Do we really need this factory? It seems duplicate with the SerializationSchema. I'm afraid we will introduce a lot of duplicate codes if we can't reuse existing SerializationFormatFactorys.", "url": "https://github.com/apache/flink/pull/13605#discussion_r513166230", "createdAt": "2020-10-28T03:58:34Z", "author": {"login": "wuchong"}, "path": "flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/EncoderFactory.java", "diffHunk": "@@ -0,0 +1,33 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.factories;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.table.data.RowData;\n+\n+/**\n+ * Base interface for configuring a {@link Encoder} for file system connector.\n+ *\n+ * @see FactoryUtil#createTableFactoryHelper(DynamicTableFactory, DynamicTableFactory.Context)\n+ */\n+@Internal\n+public interface EncoderFactory extends EncodingFormatFactory<Encoder<RowData>> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "05116d4768052821a37adfa725e60e40f4f71176"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzE5OTYzNA==", "bodyText": "I think you are right, we can try to wrap SerializationSchema and DeserializationSchema", "url": "https://github.com/apache/flink/pull/13605#discussion_r513199634", "createdAt": "2020-10-28T06:04:58Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/EncoderFactory.java", "diffHunk": "@@ -0,0 +1,33 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.factories;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.table.data.RowData;\n+\n+/**\n+ * Base interface for configuring a {@link Encoder} for file system connector.\n+ *\n+ * @see FactoryUtil#createTableFactoryHelper(DynamicTableFactory, DynamicTableFactory.Context)\n+ */\n+@Internal\n+public interface EncoderFactory extends EncodingFormatFactory<Encoder<RowData>> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzE2NjIzMA=="}, "originalCommit": {"oid": "05116d4768052821a37adfa725e60e40f4f71176"}, "originalPosition": 31}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIxNTM2NzEzOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/AbstractFileSystemTable.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOFQwNDowNDozMFrOHpZRJg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOFQwNzoxMTo0NlrOHpcj7w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzE2NzY1NA==", "bodyText": "Why not reuse FactoryUtil.TableFactoryHelper#discoverOptionalEncodingFormat?", "url": "https://github.com/apache/flink/pull/13605#discussion_r513167654", "createdAt": "2020-10-28T04:04:30Z", "author": {"login": "wuchong"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/AbstractFileSystemTable.java", "diffHunk": "@@ -53,15 +62,75 @@\n \t\tcontext.getCatalogTable().getOptions().forEach(tableOptions::setString);\n \t\tthis.schema = TableSchemaUtils.getPhysicalSchema(context.getCatalogTable().getSchema());\n \t\tthis.partitionKeys = context.getCatalogTable().getPartitionKeys();\n-\t\tthis.path = new Path(context.getCatalogTable().getOptions().getOrDefault(PATH.key(), PATH.defaultValue()));\n-\t\tthis.defaultPartName = context.getCatalogTable().getOptions().getOrDefault(\n-\t\t\t\tPARTITION_DEFAULT_NAME.key(), PARTITION_DEFAULT_NAME.defaultValue());\n+\t\tthis.path = new Path(tableOptions.get(PATH));\n+\t\tthis.defaultPartName = tableOptions.get(PARTITION_DEFAULT_NAME);\n \t}\n \n-\tstatic FileSystemFormatFactory createFormatFactory(ReadableConfig tableOptions) {\n+\tReadableConfig formatOptions(String identifier) {\n+\t\treturn new DelegatingConfiguration(tableOptions, identifier + \".\");\n+\t}\n+\n+\tFileSystemFormatFactory createFormatFactory() {\n \t\treturn FactoryUtil.discoverFactory(\n \t\t\t\tThread.currentThread().getContextClassLoader(),\n \t\t\t\tFileSystemFormatFactory.class,\n \t\t\t\ttableOptions.get(FactoryUtil.FORMAT));\n \t}\n+\n+\t@SuppressWarnings(\"rawtypes\")\n+\t<F extends EncodingFormatFactory<?>> Optional<EncodingFormat> discoverOptionalEncodingFormat(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "05116d4768052821a37adfa725e60e40f4f71176"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzIwMDQ4Ng==", "bodyText": "See below comments:\n\t/**\n\t * Unlike {@link FactoryUtil#discoverFactory}, it will not throw an exception if it cannot\n\t * find the factory.\n\t */\n\nDo you think we should modify FactoryUtil.TableFactoryHelper#discoverOptionalEncodingFormat?", "url": "https://github.com/apache/flink/pull/13605#discussion_r513200486", "createdAt": "2020-10-28T06:07:56Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/AbstractFileSystemTable.java", "diffHunk": "@@ -53,15 +62,75 @@\n \t\tcontext.getCatalogTable().getOptions().forEach(tableOptions::setString);\n \t\tthis.schema = TableSchemaUtils.getPhysicalSchema(context.getCatalogTable().getSchema());\n \t\tthis.partitionKeys = context.getCatalogTable().getPartitionKeys();\n-\t\tthis.path = new Path(context.getCatalogTable().getOptions().getOrDefault(PATH.key(), PATH.defaultValue()));\n-\t\tthis.defaultPartName = context.getCatalogTable().getOptions().getOrDefault(\n-\t\t\t\tPARTITION_DEFAULT_NAME.key(), PARTITION_DEFAULT_NAME.defaultValue());\n+\t\tthis.path = new Path(tableOptions.get(PATH));\n+\t\tthis.defaultPartName = tableOptions.get(PARTITION_DEFAULT_NAME);\n \t}\n \n-\tstatic FileSystemFormatFactory createFormatFactory(ReadableConfig tableOptions) {\n+\tReadableConfig formatOptions(String identifier) {\n+\t\treturn new DelegatingConfiguration(tableOptions, identifier + \".\");\n+\t}\n+\n+\tFileSystemFormatFactory createFormatFactory() {\n \t\treturn FactoryUtil.discoverFactory(\n \t\t\t\tThread.currentThread().getContextClassLoader(),\n \t\t\t\tFileSystemFormatFactory.class,\n \t\t\t\ttableOptions.get(FactoryUtil.FORMAT));\n \t}\n+\n+\t@SuppressWarnings(\"rawtypes\")\n+\t<F extends EncodingFormatFactory<?>> Optional<EncodingFormat> discoverOptionalEncodingFormat(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzE2NzY1NA=="}, "originalCommit": {"oid": "05116d4768052821a37adfa725e60e40f4f71176"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzIyMTYxNQ==", "bodyText": "I will move these logic to FileSystemTableFactory and use FactoryUtil to create formats.", "url": "https://github.com/apache/flink/pull/13605#discussion_r513221615", "createdAt": "2020-10-28T07:11:46Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/AbstractFileSystemTable.java", "diffHunk": "@@ -53,15 +62,75 @@\n \t\tcontext.getCatalogTable().getOptions().forEach(tableOptions::setString);\n \t\tthis.schema = TableSchemaUtils.getPhysicalSchema(context.getCatalogTable().getSchema());\n \t\tthis.partitionKeys = context.getCatalogTable().getPartitionKeys();\n-\t\tthis.path = new Path(context.getCatalogTable().getOptions().getOrDefault(PATH.key(), PATH.defaultValue()));\n-\t\tthis.defaultPartName = context.getCatalogTable().getOptions().getOrDefault(\n-\t\t\t\tPARTITION_DEFAULT_NAME.key(), PARTITION_DEFAULT_NAME.defaultValue());\n+\t\tthis.path = new Path(tableOptions.get(PATH));\n+\t\tthis.defaultPartName = tableOptions.get(PARTITION_DEFAULT_NAME);\n \t}\n \n-\tstatic FileSystemFormatFactory createFormatFactory(ReadableConfig tableOptions) {\n+\tReadableConfig formatOptions(String identifier) {\n+\t\treturn new DelegatingConfiguration(tableOptions, identifier + \".\");\n+\t}\n+\n+\tFileSystemFormatFactory createFormatFactory() {\n \t\treturn FactoryUtil.discoverFactory(\n \t\t\t\tThread.currentThread().getContextClassLoader(),\n \t\t\t\tFileSystemFormatFactory.class,\n \t\t\t\ttableOptions.get(FactoryUtil.FORMAT));\n \t}\n+\n+\t@SuppressWarnings(\"rawtypes\")\n+\t<F extends EncodingFormatFactory<?>> Optional<EncodingFormat> discoverOptionalEncodingFormat(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzE2NzY1NA=="}, "originalCommit": {"oid": "05116d4768052821a37adfa725e60e40f4f71176"}, "originalPosition": 50}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIxNTM5MTQwOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSink.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOFQwNDoxODo1NVrOHpZe3Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOFQwNjowNzowNFrOHpbQUg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzE3MTE2NQ==", "bodyText": "Why this is still needed? Do we need to migrate all the formats to use EncodingFormatFactory and DecodingFormatFactory before we can remove these code?  If yes, could you create an issue for that?", "url": "https://github.com/apache/flink/pull/13605#discussion_r513171165", "createdAt": "2020-10-28T04:18:55Z", "author": {"login": "wuchong"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSink.java", "diffHunk": "@@ -199,15 +202,32 @@ private Path toStagingPath() {\n \t}\n \n \t@SuppressWarnings(\"unchecked\")\n-\tprivate OutputFormatFactory<RowData> createOutputFormatFactory() {\n-\t\tObject writer = createWriter();\n+\tprivate OutputFormatFactory<RowData> createOutputFormatFactory(Context sinkContext) {\n+\t\tObject writer = createWriter(sinkContext);\n \t\treturn writer instanceof Encoder ?\n \t\t\t\tpath -> createEncoderOutputFormat((Encoder<RowData>) writer, path) :\n \t\t\t\tpath -> createBulkWriterOutputFormat((BulkWriter.Factory<RowData>) writer, path);\n \t}\n \n-\tprivate Object createWriter() {\n-\t\tFileSystemFormatFactory formatFactory = createFormatFactory(tableOptions);\n+\tprivate DataType getFormatDataType() {\n+\t\tTableSchema.Builder builder = TableSchema.builder();\n+\t\tschema.getTableColumns().forEach(column -> {\n+\t\t\tif (!partitionKeys.contains(column.getName())) {\n+\t\t\t\tbuilder.add(column);\n+\t\t\t}\n+\t\t});\n+\t\treturn builder.build().toRowDataType();\n+\t}\n+\n+\tprivate Object createWriter(Context sinkContext) {\n+\t\t@SuppressWarnings(\"rawtypes\")\n+\t\tOptional<EncodingFormat> encodingFormat = discoverOptionalEncodingFormat(BulkWriterFactory.class)\n+\t\t\t\t.map(Optional::of).orElseGet(() -> discoverOptionalEncodingFormat(EncoderFactory.class));\n+\t\tif (encodingFormat.isPresent()) {\n+\t\t\treturn encodingFormat.get().createRuntimeEncoder(sinkContext, getFormatDataType());\n+\t\t}\n+\n+\t\tFileSystemFormatFactory formatFactory = createFormatFactory();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "05116d4768052821a37adfa725e60e40f4f71176"}, "originalPosition": 100}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzIwMDIxMA==", "bodyText": "https://issues.apache.org/jira/browse/FLINK-19845", "url": "https://github.com/apache/flink/pull/13605#discussion_r513200210", "createdAt": "2020-10-28T06:07:04Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSink.java", "diffHunk": "@@ -199,15 +202,32 @@ private Path toStagingPath() {\n \t}\n \n \t@SuppressWarnings(\"unchecked\")\n-\tprivate OutputFormatFactory<RowData> createOutputFormatFactory() {\n-\t\tObject writer = createWriter();\n+\tprivate OutputFormatFactory<RowData> createOutputFormatFactory(Context sinkContext) {\n+\t\tObject writer = createWriter(sinkContext);\n \t\treturn writer instanceof Encoder ?\n \t\t\t\tpath -> createEncoderOutputFormat((Encoder<RowData>) writer, path) :\n \t\t\t\tpath -> createBulkWriterOutputFormat((BulkWriter.Factory<RowData>) writer, path);\n \t}\n \n-\tprivate Object createWriter() {\n-\t\tFileSystemFormatFactory formatFactory = createFormatFactory(tableOptions);\n+\tprivate DataType getFormatDataType() {\n+\t\tTableSchema.Builder builder = TableSchema.builder();\n+\t\tschema.getTableColumns().forEach(column -> {\n+\t\t\tif (!partitionKeys.contains(column.getName())) {\n+\t\t\t\tbuilder.add(column);\n+\t\t\t}\n+\t\t});\n+\t\treturn builder.build().toRowDataType();\n+\t}\n+\n+\tprivate Object createWriter(Context sinkContext) {\n+\t\t@SuppressWarnings(\"rawtypes\")\n+\t\tOptional<EncodingFormat> encodingFormat = discoverOptionalEncodingFormat(BulkWriterFactory.class)\n+\t\t\t\t.map(Optional::of).orElseGet(() -> discoverOptionalEncodingFormat(EncoderFactory.class));\n+\t\tif (encodingFormat.isPresent()) {\n+\t\t\treturn encodingFormat.get().createRuntimeEncoder(sinkContext, getFormatDataType());\n+\t\t}\n+\n+\t\tFileSystemFormatFactory formatFactory = createFormatFactory();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzE3MTE2NQ=="}, "originalCommit": {"oid": "05116d4768052821a37adfa725e60e40f4f71176"}, "originalPosition": 100}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIxNTQxMTAyOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSource.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOFQwNDozMDowNFrOHpZp0Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOFQwNjozMDo1N1rOHpbsYg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzE3Mzk2OQ==", "bodyText": "Why we should avoid using ContinuousFileMonitoringFunction here? and why not return SourceFunctionProvider of InputFormatSourceFunction directly?", "url": "https://github.com/apache/flink/pull/13605#discussion_r513173969", "createdAt": "2020-10-28T04:30:04Z", "author": {"login": "wuchong"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSource.java", "diffHunk": "@@ -88,7 +92,16 @@ private FileSystemTableSource(\n \t}\n \n \t@Override\n-\tpublic ScanRuntimeProvider getScanRuntimeProvider(ScanContext runtimeProviderContext) {\n+\tpublic ScanRuntimeProvider getScanRuntimeProvider(ScanContext scanContext) {\n+\t\tOptional<BulkDecodingFormat<RowData>> bulkDecodingFormat = discoverBulkDecodingFormat();\n+\n+\t\tif (!partitionKeys.isEmpty() && getOrFetchPartitions().isEmpty()) {\n+\t\t\t// When this table has no partition, just return a empty source.\n+\t\t\treturn InputFormatProvider.of(new CollectionInputFormat<>(new ArrayList<>(), null));\n+\t\t} else if (bulkDecodingFormat.isPresent()) {\n+\t\t\treturn SourceProvider.of(createBulkFormatSource(bulkDecodingFormat.get(), scanContext));\n+\t\t}\n+\n \t\treturn new DataStreamScanProvider() {\n \t\t\t@Override\n \t\t\tpublic DataStream<RowData> produceDataStream(StreamExecutionEnvironment execEnv) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "05116d4768052821a37adfa725e60e40f4f71176"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzIwMDg5Nw==", "bodyText": "The ContinuousFileMonitoringFunction can not accept multiple paths. Default StreamEnv.createInput will create continuous function.", "url": "https://github.com/apache/flink/pull/13605#discussion_r513200897", "createdAt": "2020-10-28T06:09:26Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSource.java", "diffHunk": "@@ -88,7 +92,16 @@ private FileSystemTableSource(\n \t}\n \n \t@Override\n-\tpublic ScanRuntimeProvider getScanRuntimeProvider(ScanContext runtimeProviderContext) {\n+\tpublic ScanRuntimeProvider getScanRuntimeProvider(ScanContext scanContext) {\n+\t\tOptional<BulkDecodingFormat<RowData>> bulkDecodingFormat = discoverBulkDecodingFormat();\n+\n+\t\tif (!partitionKeys.isEmpty() && getOrFetchPartitions().isEmpty()) {\n+\t\t\t// When this table has no partition, just return a empty source.\n+\t\t\treturn InputFormatProvider.of(new CollectionInputFormat<>(new ArrayList<>(), null));\n+\t\t} else if (bulkDecodingFormat.isPresent()) {\n+\t\t\treturn SourceProvider.of(createBulkFormatSource(bulkDecodingFormat.get(), scanContext));\n+\t\t}\n+\n \t\treturn new DataStreamScanProvider() {\n \t\t\t@Override\n \t\t\tpublic DataStream<RowData> produceDataStream(StreamExecutionEnvironment execEnv) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzE3Mzk2OQ=="}, "originalCommit": {"oid": "05116d4768052821a37adfa725e60e40f4f71176"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzIwNTU2Mw==", "bodyText": "Then can we return SourceFunctionProvider.of(InputFormatSourceFunction) instead of DataStreamScanProvider here?", "url": "https://github.com/apache/flink/pull/13605#discussion_r513205563", "createdAt": "2020-10-28T06:25:01Z", "author": {"login": "wuchong"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSource.java", "diffHunk": "@@ -88,7 +92,16 @@ private FileSystemTableSource(\n \t}\n \n \t@Override\n-\tpublic ScanRuntimeProvider getScanRuntimeProvider(ScanContext runtimeProviderContext) {\n+\tpublic ScanRuntimeProvider getScanRuntimeProvider(ScanContext scanContext) {\n+\t\tOptional<BulkDecodingFormat<RowData>> bulkDecodingFormat = discoverBulkDecodingFormat();\n+\n+\t\tif (!partitionKeys.isEmpty() && getOrFetchPartitions().isEmpty()) {\n+\t\t\t// When this table has no partition, just return a empty source.\n+\t\t\treturn InputFormatProvider.of(new CollectionInputFormat<>(new ArrayList<>(), null));\n+\t\t} else if (bulkDecodingFormat.isPresent()) {\n+\t\t\treturn SourceProvider.of(createBulkFormatSource(bulkDecodingFormat.get(), scanContext));\n+\t\t}\n+\n \t\treturn new DataStreamScanProvider() {\n \t\t\t@Override\n \t\t\tpublic DataStream<RowData> produceDataStream(StreamExecutionEnvironment execEnv) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzE3Mzk2OQ=="}, "originalCommit": {"oid": "05116d4768052821a37adfa725e60e40f4f71176"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzIwNzM5NA==", "bodyText": "Indeed, we can!", "url": "https://github.com/apache/flink/pull/13605#discussion_r513207394", "createdAt": "2020-10-28T06:30:57Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSource.java", "diffHunk": "@@ -88,7 +92,16 @@ private FileSystemTableSource(\n \t}\n \n \t@Override\n-\tpublic ScanRuntimeProvider getScanRuntimeProvider(ScanContext runtimeProviderContext) {\n+\tpublic ScanRuntimeProvider getScanRuntimeProvider(ScanContext scanContext) {\n+\t\tOptional<BulkDecodingFormat<RowData>> bulkDecodingFormat = discoverBulkDecodingFormat();\n+\n+\t\tif (!partitionKeys.isEmpty() && getOrFetchPartitions().isEmpty()) {\n+\t\t\t// When this table has no partition, just return a empty source.\n+\t\t\treturn InputFormatProvider.of(new CollectionInputFormat<>(new ArrayList<>(), null));\n+\t\t} else if (bulkDecodingFormat.isPresent()) {\n+\t\t\treturn SourceProvider.of(createBulkFormatSource(bulkDecodingFormat.get(), scanContext));\n+\t\t}\n+\n \t\treturn new DataStreamScanProvider() {\n \t\t\t@Override\n \t\t\tpublic DataStream<RowData> produceDataStream(StreamExecutionEnvironment execEnv) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzE3Mzk2OQ=="}, "originalCommit": {"oid": "05116d4768052821a37adfa725e60e40f4f71176"}, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIxNTQxNjY0OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSource.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOFQwNDozMzowNFrOHpZs_w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOFQwNjozNTowMVrOHpbxKw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzE3NDc4Mw==", "bodyText": "I think Long.MAX_VALUE can't represent no limit, right?", "url": "https://github.com/apache/flink/pull/13605#discussion_r513174783", "createdAt": "2020-10-28T04:33:04Z", "author": {"login": "wuchong"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSource.java", "diffHunk": "@@ -108,13 +121,39 @@ public boolean isBounded() {\n \t\t};\n \t}\n \n-\tprivate InputFormat<RowData, ?> getInputFormat() {\n-\t\t// When this table has no partition, just return a empty source.\n-\t\tif (!partitionKeys.isEmpty() && getOrFetchPartitions().isEmpty()) {\n-\t\t\treturn new CollectionInputFormat<>(new ArrayList<>(), null);\n+\tprivate FileSource<RowData> createBulkFormatSource(\n+\t\t\tBulkDecodingFormat<RowData> decodingFormat, ScanContext scanContext) {\n+\t\tdecodingFormat.applyLimit(pushedDownLimit());\n+\t\tdecodingFormat.applyFilters(pushedDownFilters());\n+\t\tBulkFormat<RowData> bulkFormat = decodingFormat.createRuntimeDecoder(\n+\t\t\t\tscanContext, getProducedDataType());\n+\t\tFileSource.FileSourceBuilder<RowData> builder = FileSource\n+\t\t\t\t.forBulkFileFormat(bulkFormat, paths());\n+\t\treturn builder.build();\n+\t}\n+\n+\tprivate Path[] paths() {\n+\t\tif (partitionKeys.isEmpty()) {\n+\t\t\treturn new Path[] {path};\n+\t\t} else {\n+\t\t\treturn getOrFetchPartitions().stream()\n+\t\t\t\t\t.map(FileSystemTableSource.this::toFullLinkedPartSpec)\n+\t\t\t\t\t.map(PartitionPathUtils::generatePartitionPath)\n+\t\t\t\t\t.map(n -> new Path(path, n))\n+\t\t\t\t\t.toArray(Path[]::new);\n \t\t}\n+\t}\n+\n+\tprivate long pushedDownLimit() {\n+\t\treturn limit == null ? Long.MAX_VALUE : limit;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "05116d4768052821a37adfa725e60e40f4f71176"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzIwMTA5Mg==", "bodyText": "We can keep null value", "url": "https://github.com/apache/flink/pull/13605#discussion_r513201092", "createdAt": "2020-10-28T06:10:05Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSource.java", "diffHunk": "@@ -108,13 +121,39 @@ public boolean isBounded() {\n \t\t};\n \t}\n \n-\tprivate InputFormat<RowData, ?> getInputFormat() {\n-\t\t// When this table has no partition, just return a empty source.\n-\t\tif (!partitionKeys.isEmpty() && getOrFetchPartitions().isEmpty()) {\n-\t\t\treturn new CollectionInputFormat<>(new ArrayList<>(), null);\n+\tprivate FileSource<RowData> createBulkFormatSource(\n+\t\t\tBulkDecodingFormat<RowData> decodingFormat, ScanContext scanContext) {\n+\t\tdecodingFormat.applyLimit(pushedDownLimit());\n+\t\tdecodingFormat.applyFilters(pushedDownFilters());\n+\t\tBulkFormat<RowData> bulkFormat = decodingFormat.createRuntimeDecoder(\n+\t\t\t\tscanContext, getProducedDataType());\n+\t\tFileSource.FileSourceBuilder<RowData> builder = FileSource\n+\t\t\t\t.forBulkFileFormat(bulkFormat, paths());\n+\t\treturn builder.build();\n+\t}\n+\n+\tprivate Path[] paths() {\n+\t\tif (partitionKeys.isEmpty()) {\n+\t\t\treturn new Path[] {path};\n+\t\t} else {\n+\t\t\treturn getOrFetchPartitions().stream()\n+\t\t\t\t\t.map(FileSystemTableSource.this::toFullLinkedPartSpec)\n+\t\t\t\t\t.map(PartitionPathUtils::generatePartitionPath)\n+\t\t\t\t\t.map(n -> new Path(path, n))\n+\t\t\t\t\t.toArray(Path[]::new);\n \t\t}\n+\t}\n+\n+\tprivate long pushedDownLimit() {\n+\t\treturn limit == null ? Long.MAX_VALUE : limit;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzE3NDc4Mw=="}, "originalCommit": {"oid": "05116d4768052821a37adfa725e60e40f4f71176"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzIwODYxOQ==", "bodyText": "We can do something like this:\n\t\tif (limit != null) {\n\t\t\tdecodingFormat.applyLimit(limit);\n\t\t}\n\t\tif (filters != null && filters.size() > 0) {\n\t\t\tdecodingFormat.applyFilters(filters);\n\t\t}", "url": "https://github.com/apache/flink/pull/13605#discussion_r513208619", "createdAt": "2020-10-28T06:35:01Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSource.java", "diffHunk": "@@ -108,13 +121,39 @@ public boolean isBounded() {\n \t\t};\n \t}\n \n-\tprivate InputFormat<RowData, ?> getInputFormat() {\n-\t\t// When this table has no partition, just return a empty source.\n-\t\tif (!partitionKeys.isEmpty() && getOrFetchPartitions().isEmpty()) {\n-\t\t\treturn new CollectionInputFormat<>(new ArrayList<>(), null);\n+\tprivate FileSource<RowData> createBulkFormatSource(\n+\t\t\tBulkDecodingFormat<RowData> decodingFormat, ScanContext scanContext) {\n+\t\tdecodingFormat.applyLimit(pushedDownLimit());\n+\t\tdecodingFormat.applyFilters(pushedDownFilters());\n+\t\tBulkFormat<RowData> bulkFormat = decodingFormat.createRuntimeDecoder(\n+\t\t\t\tscanContext, getProducedDataType());\n+\t\tFileSource.FileSourceBuilder<RowData> builder = FileSource\n+\t\t\t\t.forBulkFileFormat(bulkFormat, paths());\n+\t\treturn builder.build();\n+\t}\n+\n+\tprivate Path[] paths() {\n+\t\tif (partitionKeys.isEmpty()) {\n+\t\t\treturn new Path[] {path};\n+\t\t} else {\n+\t\t\treturn getOrFetchPartitions().stream()\n+\t\t\t\t\t.map(FileSystemTableSource.this::toFullLinkedPartSpec)\n+\t\t\t\t\t.map(PartitionPathUtils::generatePartitionPath)\n+\t\t\t\t\t.map(n -> new Path(path, n))\n+\t\t\t\t\t.toArray(Path[]::new);\n \t\t}\n+\t}\n+\n+\tprivate long pushedDownLimit() {\n+\t\treturn limit == null ? Long.MAX_VALUE : limit;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzE3NDc4Mw=="}, "originalCommit": {"oid": "05116d4768052821a37adfa725e60e40f4f71176"}, "originalPosition": 73}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 129, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}