{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTE2NTg5MzQx", "number": 13957, "title": "[FLINK-19823][table][fs-connector] Filesystem connector supports de/serialization schema", "bodyText": "What is the purpose of the change\nIntegrate de/serialization schema to Filesystem connector.\nBrief change log\n\nIntroduce DeserializationSchemaAdapter\nIntroduce SerializationSchemaAdapter\nClean FileSystemFormatFactorys\nIntegrate to FileSystemTableSource and FileSystemTableSink\nCorrect getChangelogMode\n\nVerifying this change\nThis change is already covered by existing tests*.\nDoes this pull request potentially affect one of the following parts:\n\nDependencies (does it add or upgrade a dependency): no\nThe public API, i.e., is any changed class annotated with @Public(Evolving): no\nThe serializers: no\nThe runtime per-record code paths (performance sensitive): no\nAnything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: no\nThe S3 file system connector: no\n\nDocumentation\n\nDoes this pull request introduce a new feature? yes\nIf yes, how is the feature documented? JavaDocs", "createdAt": "2020-11-06T08:30:19Z", "url": "https://github.com/apache/flink/pull/13957", "merged": true, "mergeCommit": {"oid": "f850f556f7f26f99636058481e57a251a9b654fb"}, "closed": true, "closedAt": "2020-11-07T11:18:20Z", "author": {"login": "JingsongLi"}, "timelineItems": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdaCHkygH2gAyNTE2NTg5MzQxOjFiM2U2MzY0YzBiOThlYjRiODJkMzk2YWI5MWVkOWI5MjhmNTRhODU=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdaGoaJAFqTUyNTYzNTM5Nw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "1b3e6364c0b98eb4b82d396ab91ed9b928f54a85", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/flink/commit/1b3e6364c0b98eb4b82d396ab91ed9b928f54a85", "committedDate": "2020-11-07T02:27:21Z", "message": "[FLINK-19823][table][fs-connector] Filesystem connector supports de/serialization schema"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0c4e62b055b5cd62d95be004423262ed9ce5eb86", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/flink/commit/0c4e62b055b5cd62d95be004423262ed9ce5eb86", "committedDate": "2020-11-07T03:37:48Z", "message": "Add DebeziumJsonFileSystemITCase"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "eded6a80f900965a008691d061057dae9c0d7cfe", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/flink/commit/eded6a80f900965a008691d061057dae9c0d7cfe", "committedDate": "2020-11-06T08:27:09Z", "message": "[FLINK-19823][table][fs-connector] Filesystem connector supports de/serialization schema"}, "afterCommit": {"oid": "0c4e62b055b5cd62d95be004423262ed9ce5eb86", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/flink/commit/0c4e62b055b5cd62d95be004423262ed9ce5eb86", "committedDate": "2020-11-07T03:37:48Z", "message": "Add DebeziumJsonFileSystemITCase"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f9c06b641c424a4e3f4e91ecc668b599dd45b6e1", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/flink/commit/f9c06b641c424a4e3f4e91ecc668b599dd45b6e1", "committedDate": "2020-11-07T05:17:53Z", "message": "Fix case"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI1NjIwODE1", "url": "https://github.com/apache/flink/pull/13957#pullrequestreview-525620815", "createdAt": "2020-11-07T04:59:05Z", "commit": {"oid": "0c4e62b055b5cd62d95be004423262ed9ce5eb86"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wN1QwNTowMzozNVrOHvDYvQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wN1QwNTozODowOFrOHvECOQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTEwMDYwNQ==", "bodyText": "return reader?", "url": "https://github.com/apache/flink/pull/13957#discussion_r519100605", "createdAt": "2020-11-07T05:03:35Z", "author": {"login": "wuchong"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/DeserializationSchemaAdapter.java", "diffHunk": "@@ -0,0 +1,316 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.api.common.io.DelimitedInputFormat;\n+import org.apache.flink.api.common.serialization.DeserializationSchema;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.connector.file.src.FileSourceSplit;\n+import org.apache.flink.connector.file.src.reader.BulkFormat;\n+import org.apache.flink.connector.file.src.util.ArrayResultIterator;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.metrics.MetricGroup;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.utils.PartitionPathUtils;\n+import org.apache.flink.util.Collector;\n+import org.apache.flink.util.InstantiationUtil;\n+import org.apache.flink.util.UserCodeClassLoader;\n+\n+import javax.annotation.Nullable;\n+\n+import java.io.IOException;\n+import java.util.ArrayDeque;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Queue;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.connector.file.src.util.CheckpointedPosition.NO_OFFSET;\n+import static org.apache.flink.table.data.vector.VectorizedColumnBatch.DEFAULT_SIZE;\n+\n+/**\n+ * Adapter to turn a {@link DeserializationSchema} into a {@link BulkFormat}.\n+ */\n+public class DeserializationSchemaAdapter implements BulkFormat<RowData, FileSourceSplit> {\n+\n+\tprivate static final int BATCH_SIZE = 100;\n+\n+\t// NOTE, deserializationSchema produce full format fields with original order\n+\tprivate final DeserializationSchema<RowData> deserializationSchema;\n+\n+\tprivate final String[] fieldNames;\n+\tprivate final DataType[] fieldTypes;\n+\tprivate final int[] projectFields;\n+\tprivate final RowType projectedRowType;\n+\n+\tprivate final List<String> partitionKeys;\n+\tprivate final String defaultPartValue;\n+\n+\tprivate final int[] toProjectedField;\n+\tprivate final RowData.FieldGetter[] formatFieldGetters;\n+\n+\tpublic DeserializationSchemaAdapter(\n+\t\t\tDeserializationSchema<RowData> deserializationSchema,\n+\t\t\tTableSchema schema,\n+\t\t\tint[] projectFields,\n+\t\t\tList<String> partitionKeys,\n+\t\t\tString defaultPartValue) {\n+\t\tthis.deserializationSchema = deserializationSchema;\n+\t\tthis.fieldNames = schema.getFieldNames();\n+\t\tthis.fieldTypes = schema.getFieldDataTypes();\n+\t\tthis.projectFields = projectFields;\n+\t\tthis.partitionKeys = partitionKeys;\n+\t\tthis.defaultPartValue = defaultPartValue;\n+\n+\t\tList<String> projectedNames = Arrays.stream(projectFields)\n+\t\t\t\t.mapToObj(idx -> schema.getFieldNames()[idx])\n+\t\t\t\t.collect(Collectors.toList());\n+\n+\t\tthis.projectedRowType = RowType.of(\n+\t\t\t\tArrays.stream(projectFields).mapToObj(idx ->\n+\t\t\t\t\t\tschema.getFieldDataTypes()[idx].getLogicalType()).toArray(LogicalType[]::new),\n+\t\t\t\tprojectedNames.toArray(new String[0]));\n+\n+\t\tList<String> formatFields = Arrays.stream(schema.getFieldNames())\n+\t\t\t\t.filter(field -> !partitionKeys.contains(field))\n+\t\t\t\t.collect(Collectors.toList());\n+\n+\t\tList<String> formatProjectedFields = projectedNames.stream()\n+\t\t\t\t.filter(field -> !partitionKeys.contains(field))\n+\t\t\t\t.collect(Collectors.toList());\n+\n+\t\tthis.toProjectedField = formatProjectedFields.stream()\n+\t\t\t\t.mapToInt(projectedNames::indexOf)\n+\t\t\t\t.toArray();\n+\n+\t\tthis.formatFieldGetters = new RowData.FieldGetter[formatProjectedFields.size()];\n+\t\tfor (int i = 0; i < formatProjectedFields.size(); i++) {\n+\t\t\tString name = formatProjectedFields.get(i);\n+\t\t\tthis.formatFieldGetters[i] = RowData.createFieldGetter(\n+\t\t\t\t\tschema.getFieldDataType(name).get().getLogicalType(),\n+\t\t\t\t\tformatFields.indexOf(name));\n+\t\t}\n+\t}\n+\n+\tprivate DeserializationSchema<RowData> createDeserialization() throws IOException {\n+\t\ttry {\n+\t\t\tDeserializationSchema<RowData> deserialization = InstantiationUtil.clone(deserializationSchema);\n+\t\t\tdeserialization.open(new DeserializationSchema.InitializationContext() {\n+\t\t\t\t@Override\n+\t\t\t\tpublic MetricGroup getMetricGroup() {\n+\t\t\t\t\tthrow new UnsupportedOperationException(\"MetricGroup is unsupported in BulkFormat.\");\n+\t\t\t\t}\n+\n+\t\t\t\t@Override\n+\t\t\t\tpublic UserCodeClassLoader getUserCodeClassLoader() {\n+\t\t\t\t\treturn (UserCodeClassLoader) Thread.currentThread().getContextClassLoader();\n+\t\t\t\t}\n+\t\t\t});\n+\t\t\treturn deserialization;\n+\t\t} catch (Exception e) {\n+\t\t\tthrow new IOException(e);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic Reader createReader(Configuration config, FileSourceSplit split) throws IOException {\n+\t\treturn new Reader(config, split);\n+\t}\n+\n+\t@Override\n+\tpublic Reader restoreReader(Configuration config, FileSourceSplit split) throws IOException {\n+\t\tReader reader = new Reader(config, split);\n+\t\treader.seek(split.getReaderPosition().get().getRecordsAfterOffset());\n+\t\treturn null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c4e62b055b5cd62d95be004423262ed9ce5eb86"}, "originalPosition": 148}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTEwODI5NA==", "bodyText": "IntStream.range?", "url": "https://github.com/apache/flink/pull/13957#discussion_r519108294", "createdAt": "2020-11-07T05:29:26Z", "author": {"login": "wuchong"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSink.java", "diffHunk": "@@ -251,6 +258,14 @@ private RowDataPartitionComputer partitionComputer() {\n \t\t\t\t//noinspection unchecked\n \t\t\t\treturn Optional.of(FileInputFormatCompactReader.factory((FileInputFormat<RowData>) format));\n \t\t\t}\n+\t\t} else if (deserializationFormat != null) {\n+\t\t\t// NOTE, we need pass full format types to deserializationFormat\n+\t\t\tDeserializationSchema<RowData> decoder = deserializationFormat.createRuntimeDecoder(\n+\t\t\t\t\tcreateSourceContext(context), getFormatDataType());\n+\t\t\tint[] projectedFields = IntStream.of(0, schema.getFieldCount()).toArray();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c4e62b055b5cd62d95be004423262ed9ce5eb86"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTEwOTcyMA==", "bodyText": "Select some fields to trigger the projection push down? We can write one more column into the sink, e.g UPPER(name), and then select out the upper_name instead of the `name.", "url": "https://github.com/apache/flink/pull/13957#discussion_r519109720", "createdAt": "2020-11-07T05:33:50Z", "author": {"login": "wuchong"}, "path": "flink-formats/flink-json/src/test/java/org/apache/flink/formats/json/debezium/DebeziumJsonFileSystemITCase.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.json.debezium;\n+\n+import org.apache.flink.table.planner.runtime.utils.StreamingTestBase;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.util.CloseableIterator;\n+import org.apache.flink.util.CollectionUtil;\n+\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URL;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import static java.lang.String.format;\n+\n+/**\n+ * Test Filesystem connector with DebeziumJson.\n+ */\n+public class DebeziumJsonFileSystemITCase extends StreamingTestBase {\n+\n+\tprivate static final List<String> EXPECTED = Arrays.asList(\n+\t\t\t\"+I(101,scooter,Small 2-wheel scooter,3.14)\",\n+\t\t\t\"+I(102,car battery,12V car battery,8.1)\",\n+\t\t\t\"+I(103,12-pack drill bits,12-pack of drill bits with sizes ranging from #40 to #3,0.8)\",\n+\t\t\t\"+I(104,hammer,12oz carpenter's hammer,0.75)\",\n+\t\t\t\"+I(105,hammer,14oz carpenter's hammer,0.875)\",\n+\t\t\t\"+I(106,hammer,16oz carpenter's hammer,1.0)\",\n+\t\t\t\"+I(107,rocks,box of assorted rocks,5.3)\",\n+\t\t\t\"+I(108,jacket,water resistent black wind breaker,0.1)\",\n+\t\t\t\"+I(109,spare tire,24 inch spare tire,22.2)\",\n+\t\t\t\"-D(106,hammer,16oz carpenter's hammer,1.0)\", // -U\n+\t\t\t\"+I(106,hammer,18oz carpenter hammer,1.0)\", // +U\n+\t\t\t\"-D(107,rocks,box of assorted rocks,5.3)\", // -U\n+\t\t\t\"+I(107,rocks,box of assorted rocks,5.1)\", // +U\n+\t\t\t\"+I(110,jacket,water resistent white wind breaker,0.2)\",\n+\t\t\t\"+I(111,scooter,Big 2-wheel scooter ,5.18)\",\n+\t\t\t\"-D(110,jacket,water resistent white wind breaker,0.2)\", // -U\n+\t\t\t\"+I(110,jacket,new water resistent white wind breaker,0.5)\", // +U\n+\t\t\t\"-D(111,scooter,Big 2-wheel scooter ,5.18)\", // -U\n+\t\t\t\"+I(111,scooter,Big 2-wheel scooter ,5.17)\", // +U\n+\t\t\t\"-D(111,scooter,Big 2-wheel scooter ,5.17)\"\n+\t);\n+\n+\tprivate File source;\n+\tprivate File sink;\n+\n+\tprivate void prepareTables(boolean isPartition) throws IOException {\n+\t\tbyte[] bytes = readBytes(\"debezium-data-schema-exclude.txt\");\n+\t\tsource = TEMPORARY_FOLDER.newFolder();\n+\t\tFile file;\n+\t\tif (isPartition) {\n+\t\t\tFile partition = new File(source, \"p=1\");\n+\t\t\tpartition.mkdirs();\n+\t\t\tfile = new File(partition, \"my_file\");\n+\t\t} else {\n+\t\t\tfile = new File(source, \"my_file\");\n+\t\t}\n+\t\tfile.createNewFile();\n+\t\tFiles.write(file.toPath(), bytes);\n+\n+\t\tsink = TEMPORARY_FOLDER.newFolder();\n+\n+\t\tenv().setParallelism(1);\n+\t}\n+\n+\tprivate void createTable(String name, String path, boolean isPartition) {\n+\t\ttEnv().executeSql(format(\"create table %s (\", name) +\n+\t\t\t\t\"id int, name string, description string, weight float\" +\n+\t\t\t\t(isPartition ? \", p int) partitioned by (p) \" : \")\") +\n+\t\t\t\t\" with (\" +\n+\t\t\t\t\"'connector'='filesystem',\" +\n+\t\t\t\t\"'format'='debezium-json',\" +\n+\t\t\t\tformat(\"'path'='%s'\", path) +\n+\t\t\t\t\")\");\n+\t}\n+\n+\t@Test\n+\tpublic void testNonPartition() throws Exception {\n+\t\tprepareTables(false);\n+\t\tcreateTable(\"source\", source.toURI().toString(), false);\n+\t\tcreateTable(\"sink\", sink.toURI().toString(), false);\n+\n+\t\ttEnv().executeSql(\"insert into sink select * from source\").await();\n+\t\tCloseableIterator<Row> iter = tEnv().executeSql(\"select * from sink\").collect();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f9c06b641c424a4e3f4e91ecc668b599dd45b6e1"}, "originalPosition": 108}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTExMTIyNQ==", "bodyText": "remove.", "url": "https://github.com/apache/flink/pull/13957#discussion_r519111225", "createdAt": "2020-11-07T05:38:08Z", "author": {"login": "wuchong"}, "path": "flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/data/BinaryRowDataTest.java", "diffHunk": "@@ -94,6 +94,10 @@\n  */\n public class BinaryRowDataTest {\n \n+\tpublic static void main(String[] args) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f9c06b641c424a4e3f4e91ecc668b599dd45b6e1"}, "originalPosition": 4}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "41b8b8583a8b8e08979515d6bcb1475fdd231664", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/flink/commit/41b8b8583a8b8e08979515d6bcb1475fdd231664", "committedDate": "2020-11-07T06:09:28Z", "message": "Address comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e71978fc6c6da487d3b75da6a3fa8b86d6bd2f2e", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/flink/commit/e71978fc6c6da487d3b75da6a3fa8b86d6bd2f2e", "committedDate": "2020-11-07T06:13:32Z", "message": "Add JsonFileCompactionITCase"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI1NjM1Mzk3", "url": "https://github.com/apache/flink/pull/13957#pullrequestreview-525635397", "createdAt": "2020-11-07T07:42:50Z", "commit": {"oid": "e71978fc6c6da487d3b75da6a3fa8b86d6bd2f2e"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4425, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}