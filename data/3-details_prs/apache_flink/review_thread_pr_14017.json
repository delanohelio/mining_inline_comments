{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTE4NDM3ODg1", "number": 14017, "reviewThreads": {"totalCount": 19, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQwNDowODo0MVrOE3_RLw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQwODo1MDoyMVrOE5Uoyw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI3MTQzNzI3OnYy", "diffSide": "RIGHT", "path": "docs/dev/table/connectors/upsert-kafka.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQwNDowODo0MVrOHxpwyQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQwNDowODo0MVrOHxpwyQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTgyNjUwNQ==", "bodyText": "\"Upsert Kafka SQL Connector\".\nThere is no \"Upsert Kafka\" in Apache projects...", "url": "https://github.com/apache/flink/pull/14017#discussion_r521826505", "createdAt": "2020-11-12T04:08:41Z", "author": {"login": "wuchong"}, "path": "docs/dev/table/connectors/upsert-kafka.md", "diffHunk": "@@ -0,0 +1,183 @@\n+---\n+title: \"Apache Upsert Kafka SQL Connector\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "88a1f9ed960f7cf504645f3a430542aa9d8e5b77"}, "originalPosition": 2}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI3MTk5MDYyOnYy", "diffSide": "RIGHT", "path": "docs/dev/table/connectors/upsert-kafka.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQwODoxMDowOFrOHxuyHg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQwODoxMDowOFrOHxuyHg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTkwODc2Ng==", "bodyText": "The Upsert Kafka connector allows for reading data from and writing data into Kafka topics in the upsert fashion.\nAs a source, the upsert-kafka connector produces a changelog stream, where each data record represents an update or delete event. More precisely, the value in a data record is interpreted as an UPDATE of the last value for the same key, if any (if a corresponding key doesn\u2019t exist yet, the update will be considered an INSERT). Using the table analogy, a data record in a changelog stream is interpreted as an UPSERT aka INSERT/UPDATE because any existing row with the same key is overwritten. Also, null values are interpreted in a special way: a record with a null value represents a \u201cDELETE\u201d.\nAs a sink, the upsert-kafka connector can consume a changelog stream. It will write INSERT/UPDATE_AFTER data as normal Kafka messages value, and write DELETE data as Kafka messages with null values (indicate tombstone for the key). Flink will guarantee the message ordering on the primary key by partition data on the values of the primary key columns, so the update/deletion messages on the same key will fall into the same partition.", "url": "https://github.com/apache/flink/pull/14017#discussion_r521908766", "createdAt": "2020-11-12T08:10:08Z", "author": {"login": "wuchong"}, "path": "docs/dev/table/connectors/upsert-kafka.md", "diffHunk": "@@ -0,0 +1,183 @@\n+---\n+title: \"Apache Upsert Kafka SQL Connector\"\n+nav-title: Upsert Kafka\n+nav-parent_id: sql-connectors\n+nav-pos: 2\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+<span class=\"label label-primary\">Scan Source: Unbounded</span>\n+<span class=\"label label-primary\">Sink: Streaming Upsert Mode</span>\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+The Upsert Kafka connector allows for reading data from and writing data into Kafka topics in the upsert fashion.\n+\n+More precisely, the value part of the record is interpreted as an \"UPDATE\" of the last value for the same key.\n+If the record doesn't exist yet, it will treat the record as an INSERT message. With the tombstone semantic in kafka,\n+it will interpret the record whose value part is null as a DELETE message.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "88a1f9ed960f7cf504645f3a430542aa9d8e5b77"}, "originalPosition": 36}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI3MTk5MjA0OnYy", "diffSide": "RIGHT", "path": "docs/dev/table/connectors/upsert-kafka.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQwODoxMDoyOVrOHxuy5Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQwODoxMDoyOVrOHxuy5Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTkwODk2NQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              <artifactId>flink-connector-kafka_2.11</artifactId>\n          \n          \n            \n              <artifactId>flink-connector-kafka_{{site.scala_version_suffix}}</artifactId>", "url": "https://github.com/apache/flink/pull/14017#discussion_r521908965", "createdAt": "2020-11-12T08:10:29Z", "author": {"login": "wuchong"}, "path": "docs/dev/table/connectors/upsert-kafka.md", "diffHunk": "@@ -0,0 +1,183 @@\n+---\n+title: \"Apache Upsert Kafka SQL Connector\"\n+nav-title: Upsert Kafka\n+nav-parent_id: sql-connectors\n+nav-pos: 2\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+<span class=\"label label-primary\">Scan Source: Unbounded</span>\n+<span class=\"label label-primary\">Sink: Streaming Upsert Mode</span>\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+The Upsert Kafka connector allows for reading data from and writing data into Kafka topics in the upsert fashion.\n+\n+More precisely, the value part of the record is interpreted as an \"UPDATE\" of the last value for the same key.\n+If the record doesn't exist yet, it will treat the record as an INSERT message. With the tombstone semantic in kafka,\n+it will interpret the record whose value part is null as a DELETE message.\n+\n+\n+Dependencies\n+------------\n+\n+To use the connector, add the following Maven dependency to your project:\n+{% highlight xml %}\n+<dependency>\n+  <groupId>org.apache.flink</groupId>\n+  <artifactId>flink-connector-kafka_2.11</artifactId>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "88a1f9ed960f7cf504645f3a430542aa9d8e5b77"}, "originalPosition": 46}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI3MTk5NzI1OnYy", "diffSide": "RIGHT", "path": "docs/dev/table/connectors/upsert-kafka.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQwODoxMTo1OVrOHxu2Gg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQwODoxMTo1OVrOHxu2Gg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTkwOTc4Ng==", "bodyText": "In order to setup the upsert-kafka connector, the following table provide dependency information for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.\n\n| Kafka Version       | Maven dependency                                          | SQL Client JAR         |\n| :------------------ | :-------------------------------------------------------- | :----------------------|\n| universal               | `flink-connector-kafka{{site.scala_version_suffix}}`      | {% if site.is_stable %} [Download](https://repo.maven.apache.org/maven2/org/apache/flink/flink-sql-connector-kafka{{site.scala_version_suffix}}/{{site.version}}/flink-sql-connector-kafka{{site.scala_version_suffix}}-{{site.version}}.jar) {% else %} Only available for [stable releases]({{ site.stable_baseurl }}/dev/table/connectors/upsert-kafka.html) {% endif %}|\n\n\nadd SQL CLI JAR\nuse variables instead of hard code the version.", "url": "https://github.com/apache/flink/pull/14017#discussion_r521909786", "createdAt": "2020-11-12T08:11:59Z", "author": {"login": "wuchong"}, "path": "docs/dev/table/connectors/upsert-kafka.md", "diffHunk": "@@ -0,0 +1,183 @@\n+---\n+title: \"Apache Upsert Kafka SQL Connector\"\n+nav-title: Upsert Kafka\n+nav-parent_id: sql-connectors\n+nav-pos: 2\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+<span class=\"label label-primary\">Scan Source: Unbounded</span>\n+<span class=\"label label-primary\">Sink: Streaming Upsert Mode</span>\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+The Upsert Kafka connector allows for reading data from and writing data into Kafka topics in the upsert fashion.\n+\n+More precisely, the value part of the record is interpreted as an \"UPDATE\" of the last value for the same key.\n+If the record doesn't exist yet, it will treat the record as an INSERT message. With the tombstone semantic in kafka,\n+it will interpret the record whose value part is null as a DELETE message.\n+\n+\n+Dependencies\n+------------\n+\n+To use the connector, add the following Maven dependency to your project:\n+{% highlight xml %}\n+<dependency>\n+  <groupId>org.apache.flink</groupId>\n+  <artifactId>flink-connector-kafka_2.11</artifactId>\n+  <version>1.12-SNAPSHOT</version>\n+</dependency>\n+{% endhighlight %}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "88a1f9ed960f7cf504645f3a430542aa9d8e5b77"}, "originalPosition": 49}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI3MjAyNzc3OnYy", "diffSide": "RIGHT", "path": "docs/dev/table/connectors/upsert-kafka.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQwODoxOToyM1rOHxvHpw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQwODoxOToyM1rOHxvHpw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTkxNDI3OQ==", "bodyText": "We don't need this as this is the default value.", "url": "https://github.com/apache/flink/pull/14017#discussion_r521914279", "createdAt": "2020-11-12T08:19:23Z", "author": {"login": "wuchong"}, "path": "docs/dev/table/connectors/upsert-kafka.md", "diffHunk": "@@ -0,0 +1,183 @@\n+---\n+title: \"Apache Upsert Kafka SQL Connector\"\n+nav-title: Upsert Kafka\n+nav-parent_id: sql-connectors\n+nav-pos: 2\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+<span class=\"label label-primary\">Scan Source: Unbounded</span>\n+<span class=\"label label-primary\">Sink: Streaming Upsert Mode</span>\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+The Upsert Kafka connector allows for reading data from and writing data into Kafka topics in the upsert fashion.\n+\n+More precisely, the value part of the record is interpreted as an \"UPDATE\" of the last value for the same key.\n+If the record doesn't exist yet, it will treat the record as an INSERT message. With the tombstone semantic in kafka,\n+it will interpret the record whose value part is null as a DELETE message.\n+\n+\n+Dependencies\n+------------\n+\n+To use the connector, add the following Maven dependency to your project:\n+{% highlight xml %}\n+<dependency>\n+  <groupId>org.apache.flink</groupId>\n+  <artifactId>flink-connector-kafka_2.11</artifactId>\n+  <version>1.12-SNAPSHOT</version>\n+</dependency>\n+{% endhighlight %}\n+\n+How to create an Upsert Kafka table\n+----------------\n+\n+The example below shows how to create an Upsert Kafka table:\n+\n+<div class=\"codetabs\" markdown=\"1\">\n+<div data-lang=\"SQL\" markdown=\"1\">\n+{% highlight sql %}\n+CREATE TABLE upsertKafkaTable (\n+  user_id BIGINT,\n+  item_id BIGINT,\n+  category_id BIGINT,\n+  behaviour STRING,\n+  ts TIMESTAMP(3),\n+  PRIMARY KEY (user_id) NOT ENFORCED\n+) WITH (\n+  'connector' = 'upsert-kafka',\n+  'topic' = 'user_behaviour',\n+  'properties.bootstrap.servers' = 'localhost:9092',\n+  'key.format' = 'json',\n+  'value.format' = 'json',\n+  'value.fields-include' = 'ALL'", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "88a1f9ed960f7cf504645f3a430542aa9d8e5b77"}, "originalPosition": 72}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI3MjAzMTc4OnYy", "diffSide": "RIGHT", "path": "docs/dev/table/connectors/upsert-kafka.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQwODoyMDowM1rOHxvKRQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQwODoyMDowM1rOHxvKRQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTkxNDk0OQ==", "bodyText": "Please add a more concreate example for how to insert an aggregation result into this upsert kafka table, and how to read from the upsert kafka table.\nYou can learn examples in FLIP-149 and this doc.", "url": "https://github.com/apache/flink/pull/14017#discussion_r521914949", "createdAt": "2020-11-12T08:20:03Z", "author": {"login": "wuchong"}, "path": "docs/dev/table/connectors/upsert-kafka.md", "diffHunk": "@@ -0,0 +1,183 @@\n+---\n+title: \"Apache Upsert Kafka SQL Connector\"\n+nav-title: Upsert Kafka\n+nav-parent_id: sql-connectors\n+nav-pos: 2\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+<span class=\"label label-primary\">Scan Source: Unbounded</span>\n+<span class=\"label label-primary\">Sink: Streaming Upsert Mode</span>\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+The Upsert Kafka connector allows for reading data from and writing data into Kafka topics in the upsert fashion.\n+\n+More precisely, the value part of the record is interpreted as an \"UPDATE\" of the last value for the same key.\n+If the record doesn't exist yet, it will treat the record as an INSERT message. With the tombstone semantic in kafka,\n+it will interpret the record whose value part is null as a DELETE message.\n+\n+\n+Dependencies\n+------------\n+\n+To use the connector, add the following Maven dependency to your project:\n+{% highlight xml %}\n+<dependency>\n+  <groupId>org.apache.flink</groupId>\n+  <artifactId>flink-connector-kafka_2.11</artifactId>\n+  <version>1.12-SNAPSHOT</version>\n+</dependency>\n+{% endhighlight %}\n+\n+How to create an Upsert Kafka table\n+----------------\n+\n+The example below shows how to create an Upsert Kafka table:\n+\n+<div class=\"codetabs\" markdown=\"1\">\n+<div data-lang=\"SQL\" markdown=\"1\">\n+{% highlight sql %}\n+CREATE TABLE upsertKafkaTable (\n+  user_id BIGINT,\n+  item_id BIGINT,\n+  category_id BIGINT,\n+  behaviour STRING,\n+  ts TIMESTAMP(3),\n+  PRIMARY KEY (user_id) NOT ENFORCED\n+) WITH (\n+  'connector' = 'upsert-kafka',\n+  'topic' = 'user_behaviour',\n+  'properties.bootstrap.servers' = 'localhost:9092',\n+  'key.format' = 'json',\n+  'value.format' = 'json',\n+  'value.fields-include' = 'ALL'\n+)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "88a1f9ed960f7cf504645f3a430542aa9d8e5b77"}, "originalPosition": 73}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI3MjA1ODA5OnYy", "diffSide": "RIGHT", "path": "docs/dev/table/connectors/upsert-kafka.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQwODoyNDowM1rOHxvbZA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQwODoyNDowM1rOHxvbZA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTkxOTMzMg==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                  <td>Topic name to read from or write to when the table is used as source or sink.</td>\n          \n          \n            \n                  <td>The Kafka topic name to read from and write to.</td>", "url": "https://github.com/apache/flink/pull/14017#discussion_r521919332", "createdAt": "2020-11-12T08:24:03Z", "author": {"login": "wuchong"}, "path": "docs/dev/table/connectors/upsert-kafka.md", "diffHunk": "@@ -0,0 +1,183 @@\n+---\n+title: \"Apache Upsert Kafka SQL Connector\"\n+nav-title: Upsert Kafka\n+nav-parent_id: sql-connectors\n+nav-pos: 2\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+<span class=\"label label-primary\">Scan Source: Unbounded</span>\n+<span class=\"label label-primary\">Sink: Streaming Upsert Mode</span>\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+The Upsert Kafka connector allows for reading data from and writing data into Kafka topics in the upsert fashion.\n+\n+More precisely, the value part of the record is interpreted as an \"UPDATE\" of the last value for the same key.\n+If the record doesn't exist yet, it will treat the record as an INSERT message. With the tombstone semantic in kafka,\n+it will interpret the record whose value part is null as a DELETE message.\n+\n+\n+Dependencies\n+------------\n+\n+To use the connector, add the following Maven dependency to your project:\n+{% highlight xml %}\n+<dependency>\n+  <groupId>org.apache.flink</groupId>\n+  <artifactId>flink-connector-kafka_2.11</artifactId>\n+  <version>1.12-SNAPSHOT</version>\n+</dependency>\n+{% endhighlight %}\n+\n+How to create an Upsert Kafka table\n+----------------\n+\n+The example below shows how to create an Upsert Kafka table:\n+\n+<div class=\"codetabs\" markdown=\"1\">\n+<div data-lang=\"SQL\" markdown=\"1\">\n+{% highlight sql %}\n+CREATE TABLE upsertKafkaTable (\n+  user_id BIGINT,\n+  item_id BIGINT,\n+  category_id BIGINT,\n+  behaviour STRING,\n+  ts TIMESTAMP(3),\n+  PRIMARY KEY (user_id) NOT ENFORCED\n+) WITH (\n+  'connector' = 'upsert-kafka',\n+  'topic' = 'user_behaviour',\n+  'properties.bootstrap.servers' = 'localhost:9092',\n+  'key.format' = 'json',\n+  'value.format' = 'json',\n+  'value.fields-include' = 'ALL'\n+)\n+{% endhighlight %}\n+</div>\n+</div>\n+\n+<span class=\"label label-danger\">Attention</span> Make sure to define the primary key in the ddl.\n+\n+Connector Options\n+----------------\n+\n+<table class=\"table table-bordered\">\n+    <thead>\n+      <tr>\n+      <th class=\"text-left\" style=\"width: 25%\">Option</th>\n+      <th class=\"text-center\" style=\"width: 8%\">Required</th>\n+      <th class=\"text-center\" style=\"width: 7%\">Default</th>\n+      <th class=\"text-center\" style=\"width: 10%\">Type</th>\n+      <th class=\"text-center\" style=\"width: 50%\">Description</th>\n+    </tr>\n+    </thead>\n+    <tbody>\n+    <tr>\n+      <td><h5>connector</h5></td>\n+      <td>required</td>\n+      <td style=\"word-wrap: break-word;\">(none)</td>\n+      <td>String</td>\n+      <td>Specify which connector to use, for the Upsert Kafka use: <code>'upsert-kafka'</code>.</td>\n+    </tr>\n+    <tr>\n+      <td><h5>topic</h5></td>\n+      <td>required</td>\n+      <td style=\"word-wrap: break-word;\">(none)</td>\n+      <td>String</td>\n+      <td>Topic name to read from or write to when the table is used as source or sink.</td>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "88a1f9ed960f7cf504645f3a430542aa9d8e5b77"}, "originalPosition": 106}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI3MjA2NDE0OnYy", "diffSide": "RIGHT", "path": "docs/dev/table/connectors/upsert-kafka.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQwODoyNDo1OVrOHxvfPQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQwODoyNDo1OVrOHxvfPQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTkyMDMxNw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                  The supported formats are <code>'csv'</code>, <code>'json'</code>, <code>'avro'</code>.\n          \n          \n            \n                  The supported formats include <code>'csv'</code>, <code>'json'</code>, <code>'avro'</code>.", "url": "https://github.com/apache/flink/pull/14017#discussion_r521920317", "createdAt": "2020-11-12T08:24:59Z", "author": {"login": "wuchong"}, "path": "docs/dev/table/connectors/upsert-kafka.md", "diffHunk": "@@ -0,0 +1,183 @@\n+---\n+title: \"Apache Upsert Kafka SQL Connector\"\n+nav-title: Upsert Kafka\n+nav-parent_id: sql-connectors\n+nav-pos: 2\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+<span class=\"label label-primary\">Scan Source: Unbounded</span>\n+<span class=\"label label-primary\">Sink: Streaming Upsert Mode</span>\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+The Upsert Kafka connector allows for reading data from and writing data into Kafka topics in the upsert fashion.\n+\n+More precisely, the value part of the record is interpreted as an \"UPDATE\" of the last value for the same key.\n+If the record doesn't exist yet, it will treat the record as an INSERT message. With the tombstone semantic in kafka,\n+it will interpret the record whose value part is null as a DELETE message.\n+\n+\n+Dependencies\n+------------\n+\n+To use the connector, add the following Maven dependency to your project:\n+{% highlight xml %}\n+<dependency>\n+  <groupId>org.apache.flink</groupId>\n+  <artifactId>flink-connector-kafka_2.11</artifactId>\n+  <version>1.12-SNAPSHOT</version>\n+</dependency>\n+{% endhighlight %}\n+\n+How to create an Upsert Kafka table\n+----------------\n+\n+The example below shows how to create an Upsert Kafka table:\n+\n+<div class=\"codetabs\" markdown=\"1\">\n+<div data-lang=\"SQL\" markdown=\"1\">\n+{% highlight sql %}\n+CREATE TABLE upsertKafkaTable (\n+  user_id BIGINT,\n+  item_id BIGINT,\n+  category_id BIGINT,\n+  behaviour STRING,\n+  ts TIMESTAMP(3),\n+  PRIMARY KEY (user_id) NOT ENFORCED\n+) WITH (\n+  'connector' = 'upsert-kafka',\n+  'topic' = 'user_behaviour',\n+  'properties.bootstrap.servers' = 'localhost:9092',\n+  'key.format' = 'json',\n+  'value.format' = 'json',\n+  'value.fields-include' = 'ALL'\n+)\n+{% endhighlight %}\n+</div>\n+</div>\n+\n+<span class=\"label label-danger\">Attention</span> Make sure to define the primary key in the ddl.\n+\n+Connector Options\n+----------------\n+\n+<table class=\"table table-bordered\">\n+    <thead>\n+      <tr>\n+      <th class=\"text-left\" style=\"width: 25%\">Option</th>\n+      <th class=\"text-center\" style=\"width: 8%\">Required</th>\n+      <th class=\"text-center\" style=\"width: 7%\">Default</th>\n+      <th class=\"text-center\" style=\"width: 10%\">Type</th>\n+      <th class=\"text-center\" style=\"width: 50%\">Description</th>\n+    </tr>\n+    </thead>\n+    <tbody>\n+    <tr>\n+      <td><h5>connector</h5></td>\n+      <td>required</td>\n+      <td style=\"word-wrap: break-word;\">(none)</td>\n+      <td>String</td>\n+      <td>Specify which connector to use, for the Upsert Kafka use: <code>'upsert-kafka'</code>.</td>\n+    </tr>\n+    <tr>\n+      <td><h5>topic</h5></td>\n+      <td>required</td>\n+      <td style=\"word-wrap: break-word;\">(none)</td>\n+      <td>String</td>\n+      <td>Topic name to read from or write to when the table is used as source or sink.</td>\n+    </tr>\n+    <tr>\n+      <td><h5>properties.bootstrap.servers</h5></td>\n+      <td>required</td>\n+      <td style=\"word-wrap: break-word;\">(none)</td>\n+      <td>String</td>\n+      <td>Comma separated list of Kafka brokers.</td>\n+    </tr>\n+    <tr>\n+      <td><h5>key.format</h5></td>\n+      <td>required</td>\n+      <td style=\"word-wrap: break-word;\">(none)</td>\n+      <td>String</td>\n+      <td>The format used to deserialize and serialize the key part of the Kafka messages that is derived from the primary key.\n+      The supported formats are <code>'csv'</code>, <code>'json'</code>, <code>'avro'</code>.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "88a1f9ed960f7cf504645f3a430542aa9d8e5b77"}, "originalPosition": 121}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI3MjA2NjM1OnYy", "diffSide": "RIGHT", "path": "docs/dev/table/connectors/upsert-kafka.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQwODoyNToyNlrOHxvgeQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQwODoyNToyNlrOHxvgeQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTkyMDYzMw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                  The supported formats are <code>'csv'</code>, <code>'json'</code>, <code>'avro'</code>.\n          \n          \n            \n                  The supported formats include <code>'csv'</code>, <code>'json'</code>, <code>'avro'</code>.", "url": "https://github.com/apache/flink/pull/14017#discussion_r521920633", "createdAt": "2020-11-12T08:25:26Z", "author": {"login": "wuchong"}, "path": "docs/dev/table/connectors/upsert-kafka.md", "diffHunk": "@@ -0,0 +1,183 @@\n+---\n+title: \"Apache Upsert Kafka SQL Connector\"\n+nav-title: Upsert Kafka\n+nav-parent_id: sql-connectors\n+nav-pos: 2\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+<span class=\"label label-primary\">Scan Source: Unbounded</span>\n+<span class=\"label label-primary\">Sink: Streaming Upsert Mode</span>\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+The Upsert Kafka connector allows for reading data from and writing data into Kafka topics in the upsert fashion.\n+\n+More precisely, the value part of the record is interpreted as an \"UPDATE\" of the last value for the same key.\n+If the record doesn't exist yet, it will treat the record as an INSERT message. With the tombstone semantic in kafka,\n+it will interpret the record whose value part is null as a DELETE message.\n+\n+\n+Dependencies\n+------------\n+\n+To use the connector, add the following Maven dependency to your project:\n+{% highlight xml %}\n+<dependency>\n+  <groupId>org.apache.flink</groupId>\n+  <artifactId>flink-connector-kafka_2.11</artifactId>\n+  <version>1.12-SNAPSHOT</version>\n+</dependency>\n+{% endhighlight %}\n+\n+How to create an Upsert Kafka table\n+----------------\n+\n+The example below shows how to create an Upsert Kafka table:\n+\n+<div class=\"codetabs\" markdown=\"1\">\n+<div data-lang=\"SQL\" markdown=\"1\">\n+{% highlight sql %}\n+CREATE TABLE upsertKafkaTable (\n+  user_id BIGINT,\n+  item_id BIGINT,\n+  category_id BIGINT,\n+  behaviour STRING,\n+  ts TIMESTAMP(3),\n+  PRIMARY KEY (user_id) NOT ENFORCED\n+) WITH (\n+  'connector' = 'upsert-kafka',\n+  'topic' = 'user_behaviour',\n+  'properties.bootstrap.servers' = 'localhost:9092',\n+  'key.format' = 'json',\n+  'value.format' = 'json',\n+  'value.fields-include' = 'ALL'\n+)\n+{% endhighlight %}\n+</div>\n+</div>\n+\n+<span class=\"label label-danger\">Attention</span> Make sure to define the primary key in the ddl.\n+\n+Connector Options\n+----------------\n+\n+<table class=\"table table-bordered\">\n+    <thead>\n+      <tr>\n+      <th class=\"text-left\" style=\"width: 25%\">Option</th>\n+      <th class=\"text-center\" style=\"width: 8%\">Required</th>\n+      <th class=\"text-center\" style=\"width: 7%\">Default</th>\n+      <th class=\"text-center\" style=\"width: 10%\">Type</th>\n+      <th class=\"text-center\" style=\"width: 50%\">Description</th>\n+    </tr>\n+    </thead>\n+    <tbody>\n+    <tr>\n+      <td><h5>connector</h5></td>\n+      <td>required</td>\n+      <td style=\"word-wrap: break-word;\">(none)</td>\n+      <td>String</td>\n+      <td>Specify which connector to use, for the Upsert Kafka use: <code>'upsert-kafka'</code>.</td>\n+    </tr>\n+    <tr>\n+      <td><h5>topic</h5></td>\n+      <td>required</td>\n+      <td style=\"word-wrap: break-word;\">(none)</td>\n+      <td>String</td>\n+      <td>Topic name to read from or write to when the table is used as source or sink.</td>\n+    </tr>\n+    <tr>\n+      <td><h5>properties.bootstrap.servers</h5></td>\n+      <td>required</td>\n+      <td style=\"word-wrap: break-word;\">(none)</td>\n+      <td>String</td>\n+      <td>Comma separated list of Kafka brokers.</td>\n+    </tr>\n+    <tr>\n+      <td><h5>key.format</h5></td>\n+      <td>required</td>\n+      <td style=\"word-wrap: break-word;\">(none)</td>\n+      <td>String</td>\n+      <td>The format used to deserialize and serialize the key part of the Kafka messages that is derived from the primary key.\n+      The supported formats are <code>'csv'</code>, <code>'json'</code>, <code>'avro'</code>.\n+      Please refer to <a href=\"{% link dev/table/connectors/formats/index.md %}\">Formats</a> page for more details and more format options.\n+      </td>\n+    </tr>\n+    <tr>\n+      <td><h5>value.format</h5></td>\n+      <td>required</td>\n+      <td style=\"word-wrap: break-word;\">(none)</td>\n+      <td>String</td>\n+      <td>The format used to deserialize and serialize the value part of the Kafka messages.\n+      The supported formats are <code>'csv'</code>, <code>'json'</code>, <code>'avro'</code>.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "88a1f9ed960f7cf504645f3a430542aa9d8e5b77"}, "originalPosition": 131}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI3MjA2OTE3OnYy", "diffSide": "RIGHT", "path": "docs/dev/table/connectors/upsert-kafka.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQwODoyNjoxNlrOHxviQw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQwODoyNjoxNlrOHxviQw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTkyMTA5MQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                  <td>The format used to deserialize and serialize the key part of the Kafka messages that is derived from the primary key.\n          \n          \n            \n                  <td>The format used to deserialize and serialize the key part of the Kafka messages. The key part fields are specified by the PRIMARY KEY syntax.", "url": "https://github.com/apache/flink/pull/14017#discussion_r521921091", "createdAt": "2020-11-12T08:26:16Z", "author": {"login": "wuchong"}, "path": "docs/dev/table/connectors/upsert-kafka.md", "diffHunk": "@@ -0,0 +1,183 @@\n+---\n+title: \"Apache Upsert Kafka SQL Connector\"\n+nav-title: Upsert Kafka\n+nav-parent_id: sql-connectors\n+nav-pos: 2\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+<span class=\"label label-primary\">Scan Source: Unbounded</span>\n+<span class=\"label label-primary\">Sink: Streaming Upsert Mode</span>\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+The Upsert Kafka connector allows for reading data from and writing data into Kafka topics in the upsert fashion.\n+\n+More precisely, the value part of the record is interpreted as an \"UPDATE\" of the last value for the same key.\n+If the record doesn't exist yet, it will treat the record as an INSERT message. With the tombstone semantic in kafka,\n+it will interpret the record whose value part is null as a DELETE message.\n+\n+\n+Dependencies\n+------------\n+\n+To use the connector, add the following Maven dependency to your project:\n+{% highlight xml %}\n+<dependency>\n+  <groupId>org.apache.flink</groupId>\n+  <artifactId>flink-connector-kafka_2.11</artifactId>\n+  <version>1.12-SNAPSHOT</version>\n+</dependency>\n+{% endhighlight %}\n+\n+How to create an Upsert Kafka table\n+----------------\n+\n+The example below shows how to create an Upsert Kafka table:\n+\n+<div class=\"codetabs\" markdown=\"1\">\n+<div data-lang=\"SQL\" markdown=\"1\">\n+{% highlight sql %}\n+CREATE TABLE upsertKafkaTable (\n+  user_id BIGINT,\n+  item_id BIGINT,\n+  category_id BIGINT,\n+  behaviour STRING,\n+  ts TIMESTAMP(3),\n+  PRIMARY KEY (user_id) NOT ENFORCED\n+) WITH (\n+  'connector' = 'upsert-kafka',\n+  'topic' = 'user_behaviour',\n+  'properties.bootstrap.servers' = 'localhost:9092',\n+  'key.format' = 'json',\n+  'value.format' = 'json',\n+  'value.fields-include' = 'ALL'\n+)\n+{% endhighlight %}\n+</div>\n+</div>\n+\n+<span class=\"label label-danger\">Attention</span> Make sure to define the primary key in the ddl.\n+\n+Connector Options\n+----------------\n+\n+<table class=\"table table-bordered\">\n+    <thead>\n+      <tr>\n+      <th class=\"text-left\" style=\"width: 25%\">Option</th>\n+      <th class=\"text-center\" style=\"width: 8%\">Required</th>\n+      <th class=\"text-center\" style=\"width: 7%\">Default</th>\n+      <th class=\"text-center\" style=\"width: 10%\">Type</th>\n+      <th class=\"text-center\" style=\"width: 50%\">Description</th>\n+    </tr>\n+    </thead>\n+    <tbody>\n+    <tr>\n+      <td><h5>connector</h5></td>\n+      <td>required</td>\n+      <td style=\"word-wrap: break-word;\">(none)</td>\n+      <td>String</td>\n+      <td>Specify which connector to use, for the Upsert Kafka use: <code>'upsert-kafka'</code>.</td>\n+    </tr>\n+    <tr>\n+      <td><h5>topic</h5></td>\n+      <td>required</td>\n+      <td style=\"word-wrap: break-word;\">(none)</td>\n+      <td>String</td>\n+      <td>Topic name to read from or write to when the table is used as source or sink.</td>\n+    </tr>\n+    <tr>\n+      <td><h5>properties.bootstrap.servers</h5></td>\n+      <td>required</td>\n+      <td style=\"word-wrap: break-word;\">(none)</td>\n+      <td>String</td>\n+      <td>Comma separated list of Kafka brokers.</td>\n+    </tr>\n+    <tr>\n+      <td><h5>key.format</h5></td>\n+      <td>required</td>\n+      <td style=\"word-wrap: break-word;\">(none)</td>\n+      <td>String</td>\n+      <td>The format used to deserialize and serialize the key part of the Kafka messages that is derived from the primary key.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "88a1f9ed960f7cf504645f3a430542aa9d8e5b77"}, "originalPosition": 120}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI3MjA3Nzk1OnYy", "diffSide": "RIGHT", "path": "docs/dev/table/connectors/upsert-kafka.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQwODoyODoyMlrOHxvnYQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQwODoyODoyMlrOHxvnYQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTkyMjQwMQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                   <td>Controls which field should end up in the value as well. Available value:\n          \n          \n            \n                   <td>Controls which fields should end up in the value as well. Available values:", "url": "https://github.com/apache/flink/pull/14017#discussion_r521922401", "createdAt": "2020-11-12T08:28:22Z", "author": {"login": "wuchong"}, "path": "docs/dev/table/connectors/upsert-kafka.md", "diffHunk": "@@ -0,0 +1,183 @@\n+---\n+title: \"Apache Upsert Kafka SQL Connector\"\n+nav-title: Upsert Kafka\n+nav-parent_id: sql-connectors\n+nav-pos: 2\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+<span class=\"label label-primary\">Scan Source: Unbounded</span>\n+<span class=\"label label-primary\">Sink: Streaming Upsert Mode</span>\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+The Upsert Kafka connector allows for reading data from and writing data into Kafka topics in the upsert fashion.\n+\n+More precisely, the value part of the record is interpreted as an \"UPDATE\" of the last value for the same key.\n+If the record doesn't exist yet, it will treat the record as an INSERT message. With the tombstone semantic in kafka,\n+it will interpret the record whose value part is null as a DELETE message.\n+\n+\n+Dependencies\n+------------\n+\n+To use the connector, add the following Maven dependency to your project:\n+{% highlight xml %}\n+<dependency>\n+  <groupId>org.apache.flink</groupId>\n+  <artifactId>flink-connector-kafka_2.11</artifactId>\n+  <version>1.12-SNAPSHOT</version>\n+</dependency>\n+{% endhighlight %}\n+\n+How to create an Upsert Kafka table\n+----------------\n+\n+The example below shows how to create an Upsert Kafka table:\n+\n+<div class=\"codetabs\" markdown=\"1\">\n+<div data-lang=\"SQL\" markdown=\"1\">\n+{% highlight sql %}\n+CREATE TABLE upsertKafkaTable (\n+  user_id BIGINT,\n+  item_id BIGINT,\n+  category_id BIGINT,\n+  behaviour STRING,\n+  ts TIMESTAMP(3),\n+  PRIMARY KEY (user_id) NOT ENFORCED\n+) WITH (\n+  'connector' = 'upsert-kafka',\n+  'topic' = 'user_behaviour',\n+  'properties.bootstrap.servers' = 'localhost:9092',\n+  'key.format' = 'json',\n+  'value.format' = 'json',\n+  'value.fields-include' = 'ALL'\n+)\n+{% endhighlight %}\n+</div>\n+</div>\n+\n+<span class=\"label label-danger\">Attention</span> Make sure to define the primary key in the ddl.\n+\n+Connector Options\n+----------------\n+\n+<table class=\"table table-bordered\">\n+    <thead>\n+      <tr>\n+      <th class=\"text-left\" style=\"width: 25%\">Option</th>\n+      <th class=\"text-center\" style=\"width: 8%\">Required</th>\n+      <th class=\"text-center\" style=\"width: 7%\">Default</th>\n+      <th class=\"text-center\" style=\"width: 10%\">Type</th>\n+      <th class=\"text-center\" style=\"width: 50%\">Description</th>\n+    </tr>\n+    </thead>\n+    <tbody>\n+    <tr>\n+      <td><h5>connector</h5></td>\n+      <td>required</td>\n+      <td style=\"word-wrap: break-word;\">(none)</td>\n+      <td>String</td>\n+      <td>Specify which connector to use, for the Upsert Kafka use: <code>'upsert-kafka'</code>.</td>\n+    </tr>\n+    <tr>\n+      <td><h5>topic</h5></td>\n+      <td>required</td>\n+      <td style=\"word-wrap: break-word;\">(none)</td>\n+      <td>String</td>\n+      <td>Topic name to read from or write to when the table is used as source or sink.</td>\n+    </tr>\n+    <tr>\n+      <td><h5>properties.bootstrap.servers</h5></td>\n+      <td>required</td>\n+      <td style=\"word-wrap: break-word;\">(none)</td>\n+      <td>String</td>\n+      <td>Comma separated list of Kafka brokers.</td>\n+    </tr>\n+    <tr>\n+      <td><h5>key.format</h5></td>\n+      <td>required</td>\n+      <td style=\"word-wrap: break-word;\">(none)</td>\n+      <td>String</td>\n+      <td>The format used to deserialize and serialize the key part of the Kafka messages that is derived from the primary key.\n+      The supported formats are <code>'csv'</code>, <code>'json'</code>, <code>'avro'</code>.\n+      Please refer to <a href=\"{% link dev/table/connectors/formats/index.md %}\">Formats</a> page for more details and more format options.\n+      </td>\n+    </tr>\n+    <tr>\n+      <td><h5>value.format</h5></td>\n+      <td>required</td>\n+      <td style=\"word-wrap: break-word;\">(none)</td>\n+      <td>String</td>\n+      <td>The format used to deserialize and serialize the value part of the Kafka messages.\n+      The supported formats are <code>'csv'</code>, <code>'json'</code>, <code>'avro'</code>.\n+      Please refer to <a href=\"{% link dev/table/connectors/formats/index.md %}\">Formats</a> page for more details and more format options.\n+      </td>\n+    </tr>\n+    <tr>\n+       <td><h5>value.fields-include</h5></td>\n+       <td>required</td>\n+       <td style=\"word-wrap: break-word;\"><code>'ALL'</code></td>\n+       <td>String</td>\n+       <td>Controls which field should end up in the value as well. Available value:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "88a1f9ed960f7cf504645f3a430542aa9d8e5b77"}, "originalPosition": 140}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI3MjA4MTc5OnYy", "diffSide": "RIGHT", "path": "docs/dev/table/connectors/upsert-kafka.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQwODoyOToyN1rOHxvpxg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQwODoyOToyN1rOHxvpxg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTkyMzAxNA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                     <li><code>ALL</code>: the value part of the record contains all fields of the schema.</li>\n          \n          \n            \n                     <li><code>ALL</code>: the value part of the record contains all fields of the schema, even if they are part of the key.</li>", "url": "https://github.com/apache/flink/pull/14017#discussion_r521923014", "createdAt": "2020-11-12T08:29:27Z", "author": {"login": "wuchong"}, "path": "docs/dev/table/connectors/upsert-kafka.md", "diffHunk": "@@ -0,0 +1,183 @@\n+---\n+title: \"Apache Upsert Kafka SQL Connector\"\n+nav-title: Upsert Kafka\n+nav-parent_id: sql-connectors\n+nav-pos: 2\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+<span class=\"label label-primary\">Scan Source: Unbounded</span>\n+<span class=\"label label-primary\">Sink: Streaming Upsert Mode</span>\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+The Upsert Kafka connector allows for reading data from and writing data into Kafka topics in the upsert fashion.\n+\n+More precisely, the value part of the record is interpreted as an \"UPDATE\" of the last value for the same key.\n+If the record doesn't exist yet, it will treat the record as an INSERT message. With the tombstone semantic in kafka,\n+it will interpret the record whose value part is null as a DELETE message.\n+\n+\n+Dependencies\n+------------\n+\n+To use the connector, add the following Maven dependency to your project:\n+{% highlight xml %}\n+<dependency>\n+  <groupId>org.apache.flink</groupId>\n+  <artifactId>flink-connector-kafka_2.11</artifactId>\n+  <version>1.12-SNAPSHOT</version>\n+</dependency>\n+{% endhighlight %}\n+\n+How to create an Upsert Kafka table\n+----------------\n+\n+The example below shows how to create an Upsert Kafka table:\n+\n+<div class=\"codetabs\" markdown=\"1\">\n+<div data-lang=\"SQL\" markdown=\"1\">\n+{% highlight sql %}\n+CREATE TABLE upsertKafkaTable (\n+  user_id BIGINT,\n+  item_id BIGINT,\n+  category_id BIGINT,\n+  behaviour STRING,\n+  ts TIMESTAMP(3),\n+  PRIMARY KEY (user_id) NOT ENFORCED\n+) WITH (\n+  'connector' = 'upsert-kafka',\n+  'topic' = 'user_behaviour',\n+  'properties.bootstrap.servers' = 'localhost:9092',\n+  'key.format' = 'json',\n+  'value.format' = 'json',\n+  'value.fields-include' = 'ALL'\n+)\n+{% endhighlight %}\n+</div>\n+</div>\n+\n+<span class=\"label label-danger\">Attention</span> Make sure to define the primary key in the ddl.\n+\n+Connector Options\n+----------------\n+\n+<table class=\"table table-bordered\">\n+    <thead>\n+      <tr>\n+      <th class=\"text-left\" style=\"width: 25%\">Option</th>\n+      <th class=\"text-center\" style=\"width: 8%\">Required</th>\n+      <th class=\"text-center\" style=\"width: 7%\">Default</th>\n+      <th class=\"text-center\" style=\"width: 10%\">Type</th>\n+      <th class=\"text-center\" style=\"width: 50%\">Description</th>\n+    </tr>\n+    </thead>\n+    <tbody>\n+    <tr>\n+      <td><h5>connector</h5></td>\n+      <td>required</td>\n+      <td style=\"word-wrap: break-word;\">(none)</td>\n+      <td>String</td>\n+      <td>Specify which connector to use, for the Upsert Kafka use: <code>'upsert-kafka'</code>.</td>\n+    </tr>\n+    <tr>\n+      <td><h5>topic</h5></td>\n+      <td>required</td>\n+      <td style=\"word-wrap: break-word;\">(none)</td>\n+      <td>String</td>\n+      <td>Topic name to read from or write to when the table is used as source or sink.</td>\n+    </tr>\n+    <tr>\n+      <td><h5>properties.bootstrap.servers</h5></td>\n+      <td>required</td>\n+      <td style=\"word-wrap: break-word;\">(none)</td>\n+      <td>String</td>\n+      <td>Comma separated list of Kafka brokers.</td>\n+    </tr>\n+    <tr>\n+      <td><h5>key.format</h5></td>\n+      <td>required</td>\n+      <td style=\"word-wrap: break-word;\">(none)</td>\n+      <td>String</td>\n+      <td>The format used to deserialize and serialize the key part of the Kafka messages that is derived from the primary key.\n+      The supported formats are <code>'csv'</code>, <code>'json'</code>, <code>'avro'</code>.\n+      Please refer to <a href=\"{% link dev/table/connectors/formats/index.md %}\">Formats</a> page for more details and more format options.\n+      </td>\n+    </tr>\n+    <tr>\n+      <td><h5>value.format</h5></td>\n+      <td>required</td>\n+      <td style=\"word-wrap: break-word;\">(none)</td>\n+      <td>String</td>\n+      <td>The format used to deserialize and serialize the value part of the Kafka messages.\n+      The supported formats are <code>'csv'</code>, <code>'json'</code>, <code>'avro'</code>.\n+      Please refer to <a href=\"{% link dev/table/connectors/formats/index.md %}\">Formats</a> page for more details and more format options.\n+      </td>\n+    </tr>\n+    <tr>\n+       <td><h5>value.fields-include</h5></td>\n+       <td>required</td>\n+       <td style=\"word-wrap: break-word;\"><code>'ALL'</code></td>\n+       <td>String</td>\n+       <td>Controls which field should end up in the value as well. Available value:\n+       <ul>\n+         <li><code>ALL</code>: the value part of the record contains all fields of the schema.</li>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "88a1f9ed960f7cf504645f3a430542aa9d8e5b77"}, "originalPosition": 142}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI3MjA4NzI2OnYy", "diffSide": "RIGHT", "path": "docs/dev/table/connectors/upsert-kafka.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQwODozMDozM1rOHxvs2w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQwODozMDozM1rOHxvs2w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTkyMzgwMw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                     <li><code>EXCEPT_KEY</code>: the value part of the record doesn't include primary key values.</li>\n          \n          \n            \n                     <li><code>EXCEPT_KEY</code>: the value part of the record contains all fields of the schema except the key fields.</li>", "url": "https://github.com/apache/flink/pull/14017#discussion_r521923803", "createdAt": "2020-11-12T08:30:33Z", "author": {"login": "wuchong"}, "path": "docs/dev/table/connectors/upsert-kafka.md", "diffHunk": "@@ -0,0 +1,183 @@\n+---\n+title: \"Apache Upsert Kafka SQL Connector\"\n+nav-title: Upsert Kafka\n+nav-parent_id: sql-connectors\n+nav-pos: 2\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+<span class=\"label label-primary\">Scan Source: Unbounded</span>\n+<span class=\"label label-primary\">Sink: Streaming Upsert Mode</span>\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+The Upsert Kafka connector allows for reading data from and writing data into Kafka topics in the upsert fashion.\n+\n+More precisely, the value part of the record is interpreted as an \"UPDATE\" of the last value for the same key.\n+If the record doesn't exist yet, it will treat the record as an INSERT message. With the tombstone semantic in kafka,\n+it will interpret the record whose value part is null as a DELETE message.\n+\n+\n+Dependencies\n+------------\n+\n+To use the connector, add the following Maven dependency to your project:\n+{% highlight xml %}\n+<dependency>\n+  <groupId>org.apache.flink</groupId>\n+  <artifactId>flink-connector-kafka_2.11</artifactId>\n+  <version>1.12-SNAPSHOT</version>\n+</dependency>\n+{% endhighlight %}\n+\n+How to create an Upsert Kafka table\n+----------------\n+\n+The example below shows how to create an Upsert Kafka table:\n+\n+<div class=\"codetabs\" markdown=\"1\">\n+<div data-lang=\"SQL\" markdown=\"1\">\n+{% highlight sql %}\n+CREATE TABLE upsertKafkaTable (\n+  user_id BIGINT,\n+  item_id BIGINT,\n+  category_id BIGINT,\n+  behaviour STRING,\n+  ts TIMESTAMP(3),\n+  PRIMARY KEY (user_id) NOT ENFORCED\n+) WITH (\n+  'connector' = 'upsert-kafka',\n+  'topic' = 'user_behaviour',\n+  'properties.bootstrap.servers' = 'localhost:9092',\n+  'key.format' = 'json',\n+  'value.format' = 'json',\n+  'value.fields-include' = 'ALL'\n+)\n+{% endhighlight %}\n+</div>\n+</div>\n+\n+<span class=\"label label-danger\">Attention</span> Make sure to define the primary key in the ddl.\n+\n+Connector Options\n+----------------\n+\n+<table class=\"table table-bordered\">\n+    <thead>\n+      <tr>\n+      <th class=\"text-left\" style=\"width: 25%\">Option</th>\n+      <th class=\"text-center\" style=\"width: 8%\">Required</th>\n+      <th class=\"text-center\" style=\"width: 7%\">Default</th>\n+      <th class=\"text-center\" style=\"width: 10%\">Type</th>\n+      <th class=\"text-center\" style=\"width: 50%\">Description</th>\n+    </tr>\n+    </thead>\n+    <tbody>\n+    <tr>\n+      <td><h5>connector</h5></td>\n+      <td>required</td>\n+      <td style=\"word-wrap: break-word;\">(none)</td>\n+      <td>String</td>\n+      <td>Specify which connector to use, for the Upsert Kafka use: <code>'upsert-kafka'</code>.</td>\n+    </tr>\n+    <tr>\n+      <td><h5>topic</h5></td>\n+      <td>required</td>\n+      <td style=\"word-wrap: break-word;\">(none)</td>\n+      <td>String</td>\n+      <td>Topic name to read from or write to when the table is used as source or sink.</td>\n+    </tr>\n+    <tr>\n+      <td><h5>properties.bootstrap.servers</h5></td>\n+      <td>required</td>\n+      <td style=\"word-wrap: break-word;\">(none)</td>\n+      <td>String</td>\n+      <td>Comma separated list of Kafka brokers.</td>\n+    </tr>\n+    <tr>\n+      <td><h5>key.format</h5></td>\n+      <td>required</td>\n+      <td style=\"word-wrap: break-word;\">(none)</td>\n+      <td>String</td>\n+      <td>The format used to deserialize and serialize the key part of the Kafka messages that is derived from the primary key.\n+      The supported formats are <code>'csv'</code>, <code>'json'</code>, <code>'avro'</code>.\n+      Please refer to <a href=\"{% link dev/table/connectors/formats/index.md %}\">Formats</a> page for more details and more format options.\n+      </td>\n+    </tr>\n+    <tr>\n+      <td><h5>value.format</h5></td>\n+      <td>required</td>\n+      <td style=\"word-wrap: break-word;\">(none)</td>\n+      <td>String</td>\n+      <td>The format used to deserialize and serialize the value part of the Kafka messages.\n+      The supported formats are <code>'csv'</code>, <code>'json'</code>, <code>'avro'</code>.\n+      Please refer to <a href=\"{% link dev/table/connectors/formats/index.md %}\">Formats</a> page for more details and more format options.\n+      </td>\n+    </tr>\n+    <tr>\n+       <td><h5>value.fields-include</h5></td>\n+       <td>required</td>\n+       <td style=\"word-wrap: break-word;\"><code>'ALL'</code></td>\n+       <td>String</td>\n+       <td>Controls which field should end up in the value as well. Available value:\n+       <ul>\n+         <li><code>ALL</code>: the value part of the record contains all fields of the schema.</li>\n+         <li><code>EXCEPT_KEY</code>: the value part of the record doesn't include primary key values.</li>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "88a1f9ed960f7cf504645f3a430542aa9d8e5b77"}, "originalPosition": 143}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI3MjA4ODMxOnYy", "diffSide": "RIGHT", "path": "docs/dev/table/connectors/upsert-kafka.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQwODozMDo1MFrOHxvthw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQwODozMDo1MFrOHxvthw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTkyMzk3NQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                  <td>Defines the parallelism of the Upsert kafka sink operator. By default, the parallelism is determined by the framework using the same parallelism of the upstream chained operator.</td>\n          \n          \n            \n                  <td>Defines the parallelism of the upsert-kafka sink operator. By default, the parallelism is determined by the framework using the same parallelism of the upstream chained operator.</td>", "url": "https://github.com/apache/flink/pull/14017#discussion_r521923975", "createdAt": "2020-11-12T08:30:50Z", "author": {"login": "wuchong"}, "path": "docs/dev/table/connectors/upsert-kafka.md", "diffHunk": "@@ -0,0 +1,183 @@\n+---\n+title: \"Apache Upsert Kafka SQL Connector\"\n+nav-title: Upsert Kafka\n+nav-parent_id: sql-connectors\n+nav-pos: 2\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+<span class=\"label label-primary\">Scan Source: Unbounded</span>\n+<span class=\"label label-primary\">Sink: Streaming Upsert Mode</span>\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+The Upsert Kafka connector allows for reading data from and writing data into Kafka topics in the upsert fashion.\n+\n+More precisely, the value part of the record is interpreted as an \"UPDATE\" of the last value for the same key.\n+If the record doesn't exist yet, it will treat the record as an INSERT message. With the tombstone semantic in kafka,\n+it will interpret the record whose value part is null as a DELETE message.\n+\n+\n+Dependencies\n+------------\n+\n+To use the connector, add the following Maven dependency to your project:\n+{% highlight xml %}\n+<dependency>\n+  <groupId>org.apache.flink</groupId>\n+  <artifactId>flink-connector-kafka_2.11</artifactId>\n+  <version>1.12-SNAPSHOT</version>\n+</dependency>\n+{% endhighlight %}\n+\n+How to create an Upsert Kafka table\n+----------------\n+\n+The example below shows how to create an Upsert Kafka table:\n+\n+<div class=\"codetabs\" markdown=\"1\">\n+<div data-lang=\"SQL\" markdown=\"1\">\n+{% highlight sql %}\n+CREATE TABLE upsertKafkaTable (\n+  user_id BIGINT,\n+  item_id BIGINT,\n+  category_id BIGINT,\n+  behaviour STRING,\n+  ts TIMESTAMP(3),\n+  PRIMARY KEY (user_id) NOT ENFORCED\n+) WITH (\n+  'connector' = 'upsert-kafka',\n+  'topic' = 'user_behaviour',\n+  'properties.bootstrap.servers' = 'localhost:9092',\n+  'key.format' = 'json',\n+  'value.format' = 'json',\n+  'value.fields-include' = 'ALL'\n+)\n+{% endhighlight %}\n+</div>\n+</div>\n+\n+<span class=\"label label-danger\">Attention</span> Make sure to define the primary key in the ddl.\n+\n+Connector Options\n+----------------\n+\n+<table class=\"table table-bordered\">\n+    <thead>\n+      <tr>\n+      <th class=\"text-left\" style=\"width: 25%\">Option</th>\n+      <th class=\"text-center\" style=\"width: 8%\">Required</th>\n+      <th class=\"text-center\" style=\"width: 7%\">Default</th>\n+      <th class=\"text-center\" style=\"width: 10%\">Type</th>\n+      <th class=\"text-center\" style=\"width: 50%\">Description</th>\n+    </tr>\n+    </thead>\n+    <tbody>\n+    <tr>\n+      <td><h5>connector</h5></td>\n+      <td>required</td>\n+      <td style=\"word-wrap: break-word;\">(none)</td>\n+      <td>String</td>\n+      <td>Specify which connector to use, for the Upsert Kafka use: <code>'upsert-kafka'</code>.</td>\n+    </tr>\n+    <tr>\n+      <td><h5>topic</h5></td>\n+      <td>required</td>\n+      <td style=\"word-wrap: break-word;\">(none)</td>\n+      <td>String</td>\n+      <td>Topic name to read from or write to when the table is used as source or sink.</td>\n+    </tr>\n+    <tr>\n+      <td><h5>properties.bootstrap.servers</h5></td>\n+      <td>required</td>\n+      <td style=\"word-wrap: break-word;\">(none)</td>\n+      <td>String</td>\n+      <td>Comma separated list of Kafka brokers.</td>\n+    </tr>\n+    <tr>\n+      <td><h5>key.format</h5></td>\n+      <td>required</td>\n+      <td style=\"word-wrap: break-word;\">(none)</td>\n+      <td>String</td>\n+      <td>The format used to deserialize and serialize the key part of the Kafka messages that is derived from the primary key.\n+      The supported formats are <code>'csv'</code>, <code>'json'</code>, <code>'avro'</code>.\n+      Please refer to <a href=\"{% link dev/table/connectors/formats/index.md %}\">Formats</a> page for more details and more format options.\n+      </td>\n+    </tr>\n+    <tr>\n+      <td><h5>value.format</h5></td>\n+      <td>required</td>\n+      <td style=\"word-wrap: break-word;\">(none)</td>\n+      <td>String</td>\n+      <td>The format used to deserialize and serialize the value part of the Kafka messages.\n+      The supported formats are <code>'csv'</code>, <code>'json'</code>, <code>'avro'</code>.\n+      Please refer to <a href=\"{% link dev/table/connectors/formats/index.md %}\">Formats</a> page for more details and more format options.\n+      </td>\n+    </tr>\n+    <tr>\n+       <td><h5>value.fields-include</h5></td>\n+       <td>required</td>\n+       <td style=\"word-wrap: break-word;\"><code>'ALL'</code></td>\n+       <td>String</td>\n+       <td>Controls which field should end up in the value as well. Available value:\n+       <ul>\n+         <li><code>ALL</code>: the value part of the record contains all fields of the schema.</li>\n+         <li><code>EXCEPT_KEY</code>: the value part of the record doesn't include primary key values.</li>\n+       </ul>\n+       </td>\n+    </tr>\n+    <tr>\n+      <td><h5>sink.parallelism</h5></td>\n+      <td>optional</td>\n+      <td style=\"word-wrap: break-word;\">(none)</td>\n+      <td>Integer</td>\n+      <td>Defines the parallelism of the Upsert kafka sink operator. By default, the parallelism is determined by the framework using the same parallelism of the upstream chained operator.</td>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "88a1f9ed960f7cf504645f3a430542aa9d8e5b77"}, "originalPosition": 152}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI3MjA4OTYxOnYy", "diffSide": "RIGHT", "path": "docs/dev/table/connectors/upsert-kafka.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQwODozMTowOFrOHxvuUA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQwODozMTowOFrOHxvuUA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTkyNDE3Ng==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The Upsert Kafka always works in the upsert fashion and requires to define the primary key in the ddl. With the assumption that records with the same key are ordered,\n          \n          \n            \n            The Upsert Kafka always works in the upsert fashion and requires to define the primary key in the DDL. With the assumption that records with the same key should be ordered in the same partition,", "url": "https://github.com/apache/flink/pull/14017#discussion_r521924176", "createdAt": "2020-11-12T08:31:08Z", "author": {"login": "wuchong"}, "path": "docs/dev/table/connectors/upsert-kafka.md", "diffHunk": "@@ -0,0 +1,183 @@\n+---\n+title: \"Apache Upsert Kafka SQL Connector\"\n+nav-title: Upsert Kafka\n+nav-parent_id: sql-connectors\n+nav-pos: 2\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+<span class=\"label label-primary\">Scan Source: Unbounded</span>\n+<span class=\"label label-primary\">Sink: Streaming Upsert Mode</span>\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+The Upsert Kafka connector allows for reading data from and writing data into Kafka topics in the upsert fashion.\n+\n+More precisely, the value part of the record is interpreted as an \"UPDATE\" of the last value for the same key.\n+If the record doesn't exist yet, it will treat the record as an INSERT message. With the tombstone semantic in kafka,\n+it will interpret the record whose value part is null as a DELETE message.\n+\n+\n+Dependencies\n+------------\n+\n+To use the connector, add the following Maven dependency to your project:\n+{% highlight xml %}\n+<dependency>\n+  <groupId>org.apache.flink</groupId>\n+  <artifactId>flink-connector-kafka_2.11</artifactId>\n+  <version>1.12-SNAPSHOT</version>\n+</dependency>\n+{% endhighlight %}\n+\n+How to create an Upsert Kafka table\n+----------------\n+\n+The example below shows how to create an Upsert Kafka table:\n+\n+<div class=\"codetabs\" markdown=\"1\">\n+<div data-lang=\"SQL\" markdown=\"1\">\n+{% highlight sql %}\n+CREATE TABLE upsertKafkaTable (\n+  user_id BIGINT,\n+  item_id BIGINT,\n+  category_id BIGINT,\n+  behaviour STRING,\n+  ts TIMESTAMP(3),\n+  PRIMARY KEY (user_id) NOT ENFORCED\n+) WITH (\n+  'connector' = 'upsert-kafka',\n+  'topic' = 'user_behaviour',\n+  'properties.bootstrap.servers' = 'localhost:9092',\n+  'key.format' = 'json',\n+  'value.format' = 'json',\n+  'value.fields-include' = 'ALL'\n+)\n+{% endhighlight %}\n+</div>\n+</div>\n+\n+<span class=\"label label-danger\">Attention</span> Make sure to define the primary key in the ddl.\n+\n+Connector Options\n+----------------\n+\n+<table class=\"table table-bordered\">\n+    <thead>\n+      <tr>\n+      <th class=\"text-left\" style=\"width: 25%\">Option</th>\n+      <th class=\"text-center\" style=\"width: 8%\">Required</th>\n+      <th class=\"text-center\" style=\"width: 7%\">Default</th>\n+      <th class=\"text-center\" style=\"width: 10%\">Type</th>\n+      <th class=\"text-center\" style=\"width: 50%\">Description</th>\n+    </tr>\n+    </thead>\n+    <tbody>\n+    <tr>\n+      <td><h5>connector</h5></td>\n+      <td>required</td>\n+      <td style=\"word-wrap: break-word;\">(none)</td>\n+      <td>String</td>\n+      <td>Specify which connector to use, for the Upsert Kafka use: <code>'upsert-kafka'</code>.</td>\n+    </tr>\n+    <tr>\n+      <td><h5>topic</h5></td>\n+      <td>required</td>\n+      <td style=\"word-wrap: break-word;\">(none)</td>\n+      <td>String</td>\n+      <td>Topic name to read from or write to when the table is used as source or sink.</td>\n+    </tr>\n+    <tr>\n+      <td><h5>properties.bootstrap.servers</h5></td>\n+      <td>required</td>\n+      <td style=\"word-wrap: break-word;\">(none)</td>\n+      <td>String</td>\n+      <td>Comma separated list of Kafka brokers.</td>\n+    </tr>\n+    <tr>\n+      <td><h5>key.format</h5></td>\n+      <td>required</td>\n+      <td style=\"word-wrap: break-word;\">(none)</td>\n+      <td>String</td>\n+      <td>The format used to deserialize and serialize the key part of the Kafka messages that is derived from the primary key.\n+      The supported formats are <code>'csv'</code>, <code>'json'</code>, <code>'avro'</code>.\n+      Please refer to <a href=\"{% link dev/table/connectors/formats/index.md %}\">Formats</a> page for more details and more format options.\n+      </td>\n+    </tr>\n+    <tr>\n+      <td><h5>value.format</h5></td>\n+      <td>required</td>\n+      <td style=\"word-wrap: break-word;\">(none)</td>\n+      <td>String</td>\n+      <td>The format used to deserialize and serialize the value part of the Kafka messages.\n+      The supported formats are <code>'csv'</code>, <code>'json'</code>, <code>'avro'</code>.\n+      Please refer to <a href=\"{% link dev/table/connectors/formats/index.md %}\">Formats</a> page for more details and more format options.\n+      </td>\n+    </tr>\n+    <tr>\n+       <td><h5>value.fields-include</h5></td>\n+       <td>required</td>\n+       <td style=\"word-wrap: break-word;\"><code>'ALL'</code></td>\n+       <td>String</td>\n+       <td>Controls which field should end up in the value as well. Available value:\n+       <ul>\n+         <li><code>ALL</code>: the value part of the record contains all fields of the schema.</li>\n+         <li><code>EXCEPT_KEY</code>: the value part of the record doesn't include primary key values.</li>\n+       </ul>\n+       </td>\n+    </tr>\n+    <tr>\n+      <td><h5>sink.parallelism</h5></td>\n+      <td>optional</td>\n+      <td style=\"word-wrap: break-word;\">(none)</td>\n+      <td>Integer</td>\n+      <td>Defines the parallelism of the Upsert kafka sink operator. By default, the parallelism is determined by the framework using the same parallelism of the upstream chained operator.</td>\n+    </tr>\n+    </tbody>\n+</table>\n+\n+Features\n+----------------\n+\n+### Primary Key Constraints\n+\n+The Upsert Kafka always works in the upsert fashion and requires to define the primary key in the ddl. With the assumption that records with the same key are ordered,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "88a1f9ed960f7cf504645f3a430542aa9d8e5b77"}, "originalPosition": 162}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI3MjExODI0OnYy", "diffSide": "RIGHT", "path": "docs/dev/table/connectors/upsert-kafka.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQwODozOToxNFrOHxv__w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQwODozOToxNFrOHxv__w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTkyODcwMw==", "bodyText": "We have moved these part to the top.", "url": "https://github.com/apache/flink/pull/14017#discussion_r521928703", "createdAt": "2020-11-12T08:39:14Z", "author": {"login": "wuchong"}, "path": "docs/dev/table/connectors/upsert-kafka.md", "diffHunk": "@@ -0,0 +1,183 @@\n+---\n+title: \"Apache Upsert Kafka SQL Connector\"\n+nav-title: Upsert Kafka\n+nav-parent_id: sql-connectors\n+nav-pos: 2\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+<span class=\"label label-primary\">Scan Source: Unbounded</span>\n+<span class=\"label label-primary\">Sink: Streaming Upsert Mode</span>\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+The Upsert Kafka connector allows for reading data from and writing data into Kafka topics in the upsert fashion.\n+\n+More precisely, the value part of the record is interpreted as an \"UPDATE\" of the last value for the same key.\n+If the record doesn't exist yet, it will treat the record as an INSERT message. With the tombstone semantic in kafka,\n+it will interpret the record whose value part is null as a DELETE message.\n+\n+\n+Dependencies\n+------------\n+\n+To use the connector, add the following Maven dependency to your project:\n+{% highlight xml %}\n+<dependency>\n+  <groupId>org.apache.flink</groupId>\n+  <artifactId>flink-connector-kafka_2.11</artifactId>\n+  <version>1.12-SNAPSHOT</version>\n+</dependency>\n+{% endhighlight %}\n+\n+How to create an Upsert Kafka table\n+----------------\n+\n+The example below shows how to create an Upsert Kafka table:\n+\n+<div class=\"codetabs\" markdown=\"1\">\n+<div data-lang=\"SQL\" markdown=\"1\">\n+{% highlight sql %}\n+CREATE TABLE upsertKafkaTable (\n+  user_id BIGINT,\n+  item_id BIGINT,\n+  category_id BIGINT,\n+  behaviour STRING,\n+  ts TIMESTAMP(3),\n+  PRIMARY KEY (user_id) NOT ENFORCED\n+) WITH (\n+  'connector' = 'upsert-kafka',\n+  'topic' = 'user_behaviour',\n+  'properties.bootstrap.servers' = 'localhost:9092',\n+  'key.format' = 'json',\n+  'value.format' = 'json',\n+  'value.fields-include' = 'ALL'\n+)\n+{% endhighlight %}\n+</div>\n+</div>\n+\n+<span class=\"label label-danger\">Attention</span> Make sure to define the primary key in the ddl.\n+\n+Connector Options\n+----------------\n+\n+<table class=\"table table-bordered\">\n+    <thead>\n+      <tr>\n+      <th class=\"text-left\" style=\"width: 25%\">Option</th>\n+      <th class=\"text-center\" style=\"width: 8%\">Required</th>\n+      <th class=\"text-center\" style=\"width: 7%\">Default</th>\n+      <th class=\"text-center\" style=\"width: 10%\">Type</th>\n+      <th class=\"text-center\" style=\"width: 50%\">Description</th>\n+    </tr>\n+    </thead>\n+    <tbody>\n+    <tr>\n+      <td><h5>connector</h5></td>\n+      <td>required</td>\n+      <td style=\"word-wrap: break-word;\">(none)</td>\n+      <td>String</td>\n+      <td>Specify which connector to use, for the Upsert Kafka use: <code>'upsert-kafka'</code>.</td>\n+    </tr>\n+    <tr>\n+      <td><h5>topic</h5></td>\n+      <td>required</td>\n+      <td style=\"word-wrap: break-word;\">(none)</td>\n+      <td>String</td>\n+      <td>Topic name to read from or write to when the table is used as source or sink.</td>\n+    </tr>\n+    <tr>\n+      <td><h5>properties.bootstrap.servers</h5></td>\n+      <td>required</td>\n+      <td style=\"word-wrap: break-word;\">(none)</td>\n+      <td>String</td>\n+      <td>Comma separated list of Kafka brokers.</td>\n+    </tr>\n+    <tr>\n+      <td><h5>key.format</h5></td>\n+      <td>required</td>\n+      <td style=\"word-wrap: break-word;\">(none)</td>\n+      <td>String</td>\n+      <td>The format used to deserialize and serialize the key part of the Kafka messages that is derived from the primary key.\n+      The supported formats are <code>'csv'</code>, <code>'json'</code>, <code>'avro'</code>.\n+      Please refer to <a href=\"{% link dev/table/connectors/formats/index.md %}\">Formats</a> page for more details and more format options.\n+      </td>\n+    </tr>\n+    <tr>\n+      <td><h5>value.format</h5></td>\n+      <td>required</td>\n+      <td style=\"word-wrap: break-word;\">(none)</td>\n+      <td>String</td>\n+      <td>The format used to deserialize and serialize the value part of the Kafka messages.\n+      The supported formats are <code>'csv'</code>, <code>'json'</code>, <code>'avro'</code>.\n+      Please refer to <a href=\"{% link dev/table/connectors/formats/index.md %}\">Formats</a> page for more details and more format options.\n+      </td>\n+    </tr>\n+    <tr>\n+       <td><h5>value.fields-include</h5></td>\n+       <td>required</td>\n+       <td style=\"word-wrap: break-word;\"><code>'ALL'</code></td>\n+       <td>String</td>\n+       <td>Controls which field should end up in the value as well. Available value:\n+       <ul>\n+         <li><code>ALL</code>: the value part of the record contains all fields of the schema.</li>\n+         <li><code>EXCEPT_KEY</code>: the value part of the record doesn't include primary key values.</li>\n+       </ul>\n+       </td>\n+    </tr>\n+    <tr>\n+      <td><h5>sink.parallelism</h5></td>\n+      <td>optional</td>\n+      <td style=\"word-wrap: break-word;\">(none)</td>\n+      <td>Integer</td>\n+      <td>Defines the parallelism of the Upsert kafka sink operator. By default, the parallelism is determined by the framework using the same parallelism of the upstream chained operator.</td>\n+    </tr>\n+    </tbody>\n+</table>\n+\n+Features\n+----------------\n+\n+### Primary Key Constraints\n+\n+The Upsert Kafka always works in the upsert fashion and requires to define the primary key in the ddl. With the assumption that records with the same key are ordered,\n+the primary key semantic on the changelog source means the materialized changelog is unique on the primary keys. The primary key definition will also control\n+which fields should end up in Kafka\u2019s key.\n+\n+### Changelog Source\n+\n+When the Upsert Kafka works as the source, it produces the changelog stream from the topic. Generally speaking, the Upsert Kafka requires the topic is <a href=\"https://kafka.apache.org/documentation/#compaction\"><code>'compacted'</code></a>\n+and all data with the same key are in the same partition. To keep the data integrity, the Upsert Kafka disallows to specify the reading position, which means Upsert Kafka always uses the `earliest-offset`.\n+\n+### Changelog Sink\n+\n+The Upsert Kafka has the ability to keep the records with the same key into the same partition by <code>'HASH'</code> partitioner. At the same time, the upsert-kafka will\n+always try their best to not send the UPDATE-BEFORE message to the storage comparing to the kafka with CDC format.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "88a1f9ed960f7cf504645f3a430542aa9d8e5b77"}, "originalPosition": 174}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI3MjE0MzI1OnYy", "diffSide": "RIGHT", "path": "docs/dev/table/connectors/upsert-kafka.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQwODo0NTozM1rOHxwPMg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQwODo0NTozM1rOHxwPMg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTkzMjU5NA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            With the at-least-once guarantees, it may get duplicate records but keep the order of the records. When the Upsert Kafka materializes the changelog, the duplicate records will\n          \n          \n            \n            not destroy of the final results. Therefore, at-least-once guarantees is enough.\n          \n          \n            \n            This means, Flink may write duplicate records with the same key into the Kafka topic. But as the connector is working in the upsert mode, the last record on the same key will take effect when reading back as a source. Therefore, the upsert-kafka connector achieves idempotent writes just like the [HBase sink]({{ site.baseurl }}/dev/table/connectors/hbase.html).", "url": "https://github.com/apache/flink/pull/14017#discussion_r521932594", "createdAt": "2020-11-12T08:45:33Z", "author": {"login": "wuchong"}, "path": "docs/dev/table/connectors/upsert-kafka.md", "diffHunk": "@@ -0,0 +1,183 @@\n+---\n+title: \"Apache Upsert Kafka SQL Connector\"\n+nav-title: Upsert Kafka\n+nav-parent_id: sql-connectors\n+nav-pos: 2\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+<span class=\"label label-primary\">Scan Source: Unbounded</span>\n+<span class=\"label label-primary\">Sink: Streaming Upsert Mode</span>\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+The Upsert Kafka connector allows for reading data from and writing data into Kafka topics in the upsert fashion.\n+\n+More precisely, the value part of the record is interpreted as an \"UPDATE\" of the last value for the same key.\n+If the record doesn't exist yet, it will treat the record as an INSERT message. With the tombstone semantic in kafka,\n+it will interpret the record whose value part is null as a DELETE message.\n+\n+\n+Dependencies\n+------------\n+\n+To use the connector, add the following Maven dependency to your project:\n+{% highlight xml %}\n+<dependency>\n+  <groupId>org.apache.flink</groupId>\n+  <artifactId>flink-connector-kafka_2.11</artifactId>\n+  <version>1.12-SNAPSHOT</version>\n+</dependency>\n+{% endhighlight %}\n+\n+How to create an Upsert Kafka table\n+----------------\n+\n+The example below shows how to create an Upsert Kafka table:\n+\n+<div class=\"codetabs\" markdown=\"1\">\n+<div data-lang=\"SQL\" markdown=\"1\">\n+{% highlight sql %}\n+CREATE TABLE upsertKafkaTable (\n+  user_id BIGINT,\n+  item_id BIGINT,\n+  category_id BIGINT,\n+  behaviour STRING,\n+  ts TIMESTAMP(3),\n+  PRIMARY KEY (user_id) NOT ENFORCED\n+) WITH (\n+  'connector' = 'upsert-kafka',\n+  'topic' = 'user_behaviour',\n+  'properties.bootstrap.servers' = 'localhost:9092',\n+  'key.format' = 'json',\n+  'value.format' = 'json',\n+  'value.fields-include' = 'ALL'\n+)\n+{% endhighlight %}\n+</div>\n+</div>\n+\n+<span class=\"label label-danger\">Attention</span> Make sure to define the primary key in the ddl.\n+\n+Connector Options\n+----------------\n+\n+<table class=\"table table-bordered\">\n+    <thead>\n+      <tr>\n+      <th class=\"text-left\" style=\"width: 25%\">Option</th>\n+      <th class=\"text-center\" style=\"width: 8%\">Required</th>\n+      <th class=\"text-center\" style=\"width: 7%\">Default</th>\n+      <th class=\"text-center\" style=\"width: 10%\">Type</th>\n+      <th class=\"text-center\" style=\"width: 50%\">Description</th>\n+    </tr>\n+    </thead>\n+    <tbody>\n+    <tr>\n+      <td><h5>connector</h5></td>\n+      <td>required</td>\n+      <td style=\"word-wrap: break-word;\">(none)</td>\n+      <td>String</td>\n+      <td>Specify which connector to use, for the Upsert Kafka use: <code>'upsert-kafka'</code>.</td>\n+    </tr>\n+    <tr>\n+      <td><h5>topic</h5></td>\n+      <td>required</td>\n+      <td style=\"word-wrap: break-word;\">(none)</td>\n+      <td>String</td>\n+      <td>Topic name to read from or write to when the table is used as source or sink.</td>\n+    </tr>\n+    <tr>\n+      <td><h5>properties.bootstrap.servers</h5></td>\n+      <td>required</td>\n+      <td style=\"word-wrap: break-word;\">(none)</td>\n+      <td>String</td>\n+      <td>Comma separated list of Kafka brokers.</td>\n+    </tr>\n+    <tr>\n+      <td><h5>key.format</h5></td>\n+      <td>required</td>\n+      <td style=\"word-wrap: break-word;\">(none)</td>\n+      <td>String</td>\n+      <td>The format used to deserialize and serialize the key part of the Kafka messages that is derived from the primary key.\n+      The supported formats are <code>'csv'</code>, <code>'json'</code>, <code>'avro'</code>.\n+      Please refer to <a href=\"{% link dev/table/connectors/formats/index.md %}\">Formats</a> page for more details and more format options.\n+      </td>\n+    </tr>\n+    <tr>\n+      <td><h5>value.format</h5></td>\n+      <td>required</td>\n+      <td style=\"word-wrap: break-word;\">(none)</td>\n+      <td>String</td>\n+      <td>The format used to deserialize and serialize the value part of the Kafka messages.\n+      The supported formats are <code>'csv'</code>, <code>'json'</code>, <code>'avro'</code>.\n+      Please refer to <a href=\"{% link dev/table/connectors/formats/index.md %}\">Formats</a> page for more details and more format options.\n+      </td>\n+    </tr>\n+    <tr>\n+       <td><h5>value.fields-include</h5></td>\n+       <td>required</td>\n+       <td style=\"word-wrap: break-word;\"><code>'ALL'</code></td>\n+       <td>String</td>\n+       <td>Controls which field should end up in the value as well. Available value:\n+       <ul>\n+         <li><code>ALL</code>: the value part of the record contains all fields of the schema.</li>\n+         <li><code>EXCEPT_KEY</code>: the value part of the record doesn't include primary key values.</li>\n+       </ul>\n+       </td>\n+    </tr>\n+    <tr>\n+      <td><h5>sink.parallelism</h5></td>\n+      <td>optional</td>\n+      <td style=\"word-wrap: break-word;\">(none)</td>\n+      <td>Integer</td>\n+      <td>Defines the parallelism of the Upsert kafka sink operator. By default, the parallelism is determined by the framework using the same parallelism of the upstream chained operator.</td>\n+    </tr>\n+    </tbody>\n+</table>\n+\n+Features\n+----------------\n+\n+### Primary Key Constraints\n+\n+The Upsert Kafka always works in the upsert fashion and requires to define the primary key in the ddl. With the assumption that records with the same key are ordered,\n+the primary key semantic on the changelog source means the materialized changelog is unique on the primary keys. The primary key definition will also control\n+which fields should end up in Kafka\u2019s key.\n+\n+### Changelog Source\n+\n+When the Upsert Kafka works as the source, it produces the changelog stream from the topic. Generally speaking, the Upsert Kafka requires the topic is <a href=\"https://kafka.apache.org/documentation/#compaction\"><code>'compacted'</code></a>\n+and all data with the same key are in the same partition. To keep the data integrity, the Upsert Kafka disallows to specify the reading position, which means Upsert Kafka always uses the `earliest-offset`.\n+\n+### Changelog Sink\n+\n+The Upsert Kafka has the ability to keep the records with the same key into the same partition by <code>'HASH'</code> partitioner. At the same time, the upsert-kafka will\n+always try their best to not send the UPDATE-BEFORE message to the storage comparing to the kafka with CDC format.\n+\n+### Consistency Guarantees\n+\n+By default, an Upsert Kafka sink ingests data with at-least-once guarantees into a Kafka topic if the query is executed with [checkpointing enabled]({% link dev/stream/state/checkpointing.md %}#enabling-and-configuring-checkpointing).\n+\n+With the at-least-once guarantees, it may get duplicate records but keep the order of the records. When the Upsert Kafka materializes the changelog, the duplicate records will\n+not destroy of the final results. Therefore, at-least-once guarantees is enough.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "88a1f9ed960f7cf504645f3a430542aa9d8e5b77"}, "originalPosition": 181}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI4NTQyMTcxOnYy", "diffSide": "RIGHT", "path": "docs/dev/table/connectors/upsert-kafka.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQwODo0OTo0NlrOHztJWA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQwODo0OTo0NlrOHztJWA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzk3OTA5Ng==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              PRIMARY KEY region NOT ENFORCED\n          \n          \n            \n              PRIMARY KEY (region) NOT ENFORCED", "url": "https://github.com/apache/flink/pull/14017#discussion_r523979096", "createdAt": "2020-11-16T08:49:46Z", "author": {"login": "wuchong"}, "path": "docs/dev/table/connectors/upsert-kafka.md", "diffHunk": "@@ -0,0 +1,204 @@\n+---\n+title: \"Upsert Kafka SQL Connector\"\n+nav-title: Upsert Kafka\n+nav-parent_id: sql-connectors\n+nav-pos: 3\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+<span class=\"label label-primary\">Scan Source: Unbounded</span>\n+<span class=\"label label-primary\">Sink: Streaming Upsert Mode</span>\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+The Upsert Kafka connector allows for reading data from and writing data into Kafka topics in the upsert fashion.\n+\n+As a source, the upsert-kafka connector produces a changelog stream, where each data record represents\n+an update or delete event. More precisely, the value in a data record is interpreted as an UPDATE of\n+the last value for the same key, if any (if a corresponding key doesn\u2019t exist yet, the update will\n+be considered an INSERT). Using the table analogy, a data record in a changelog stream is interpreted\n+as an UPSERT aka INSERT/UPDATE because any existing row with the same key is overwritten. Also, null\n+values are interpreted in a special way: a record with a null value represents a \u201cDELETE\u201d.\n+\n+As a sink, the upsert-kafka connector can consume a changelog stream. It will write INSERT/UPDATE_AFTER\n+data as normal Kafka messages value, and write DELETE data as Kafka messages with null values\n+(indicate tombstone for the key). Flink will guarantee the message ordering on the primary key by\n+partition data on the values of the primary key columns, so the update/deletion messages on the same\n+key will fall into the same partition.\n+\n+Dependencies\n+------------\n+\n+In order to set up the upsert-kafka connector, the following table provide dependency information for\n+both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.\n+\n+{% assign connector = site.data.sql-connectors['upsert-kafka'] %}\n+{% include sql-connector-download-table.html\n+    connector=connector\n+%}\n+\n+Full Example\n+----------------\n+\n+The example below shows how to create and use an Upsert Kafka table:\n+\n+<div class=\"codetabs\" markdown=\"1\">\n+<div data-lang=\"SQL\" markdown=\"1\">\n+{% highlight sql %}\n+CREATE TABLE pageviews_per_region (\n+  region STRING,\n+  pv BIGINT,\n+  uv BIGINT,\n+  PRIMARY KEY region NOT ENFORCED", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7dff1e172e3b99d819cfc1cbc1eedf71bea94626"}, "originalPosition": 70}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI4NTQyNDExOnYy", "diffSide": "RIGHT", "path": "docs/dev/table/connectors/upsert-kafka.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQwODo1MDoyMVrOHztKrQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQxMDowMzowMlrOHzyHig==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzk3OTQzNw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              'key.format' = 'csv',\n          \n          \n            \n              'key.format' = 'avro',\n          \n      \n    \n    \n  \n\n\u6700\u597d\u4e0d\u8981\u7528 csv\uff0c\u6211\u8bb0\u5f97 csv \u5728\u5904\u7406\u5355\u5b57\u6bb5\u7684\u65f6\u5019\u662f\u6709\u95ee\u9898\u7684\u3002\n\u53ef\u4ee5\u672c\u5730\u6d4b\u8bd5\u8bd5\u8bd5\u8fd9\u4e2a example\u3002", "url": "https://github.com/apache/flink/pull/14017#discussion_r523979437", "createdAt": "2020-11-16T08:50:21Z", "author": {"login": "wuchong"}, "path": "docs/dev/table/connectors/upsert-kafka.md", "diffHunk": "@@ -0,0 +1,204 @@\n+---\n+title: \"Upsert Kafka SQL Connector\"\n+nav-title: Upsert Kafka\n+nav-parent_id: sql-connectors\n+nav-pos: 3\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+<span class=\"label label-primary\">Scan Source: Unbounded</span>\n+<span class=\"label label-primary\">Sink: Streaming Upsert Mode</span>\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+The Upsert Kafka connector allows for reading data from and writing data into Kafka topics in the upsert fashion.\n+\n+As a source, the upsert-kafka connector produces a changelog stream, where each data record represents\n+an update or delete event. More precisely, the value in a data record is interpreted as an UPDATE of\n+the last value for the same key, if any (if a corresponding key doesn\u2019t exist yet, the update will\n+be considered an INSERT). Using the table analogy, a data record in a changelog stream is interpreted\n+as an UPSERT aka INSERT/UPDATE because any existing row with the same key is overwritten. Also, null\n+values are interpreted in a special way: a record with a null value represents a \u201cDELETE\u201d.\n+\n+As a sink, the upsert-kafka connector can consume a changelog stream. It will write INSERT/UPDATE_AFTER\n+data as normal Kafka messages value, and write DELETE data as Kafka messages with null values\n+(indicate tombstone for the key). Flink will guarantee the message ordering on the primary key by\n+partition data on the values of the primary key columns, so the update/deletion messages on the same\n+key will fall into the same partition.\n+\n+Dependencies\n+------------\n+\n+In order to set up the upsert-kafka connector, the following table provide dependency information for\n+both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.\n+\n+{% assign connector = site.data.sql-connectors['upsert-kafka'] %}\n+{% include sql-connector-download-table.html\n+    connector=connector\n+%}\n+\n+Full Example\n+----------------\n+\n+The example below shows how to create and use an Upsert Kafka table:\n+\n+<div class=\"codetabs\" markdown=\"1\">\n+<div data-lang=\"SQL\" markdown=\"1\">\n+{% highlight sql %}\n+CREATE TABLE pageviews_per_region (\n+  region STRING,\n+  pv BIGINT,\n+  uv BIGINT,\n+  PRIMARY KEY region NOT ENFORCED\n+) WITH (\n+  'connector' = 'upsert-kafka',\n+  'topic' = 'pageviews_per_region',\n+  'properties.bootstrap.servers' = '...',\n+  'key.format' = 'csv',", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7dff1e172e3b99d819cfc1cbc1eedf71bea94626"}, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDA2MDU1NA==", "bodyText": "\u6211\u4f1a\u68c0\u67e5\u4e0b\u7684", "url": "https://github.com/apache/flink/pull/14017#discussion_r524060554", "createdAt": "2020-11-16T10:03:02Z", "author": {"login": "fsk119"}, "path": "docs/dev/table/connectors/upsert-kafka.md", "diffHunk": "@@ -0,0 +1,204 @@\n+---\n+title: \"Upsert Kafka SQL Connector\"\n+nav-title: Upsert Kafka\n+nav-parent_id: sql-connectors\n+nav-pos: 3\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+<span class=\"label label-primary\">Scan Source: Unbounded</span>\n+<span class=\"label label-primary\">Sink: Streaming Upsert Mode</span>\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+The Upsert Kafka connector allows for reading data from and writing data into Kafka topics in the upsert fashion.\n+\n+As a source, the upsert-kafka connector produces a changelog stream, where each data record represents\n+an update or delete event. More precisely, the value in a data record is interpreted as an UPDATE of\n+the last value for the same key, if any (if a corresponding key doesn\u2019t exist yet, the update will\n+be considered an INSERT). Using the table analogy, a data record in a changelog stream is interpreted\n+as an UPSERT aka INSERT/UPDATE because any existing row with the same key is overwritten. Also, null\n+values are interpreted in a special way: a record with a null value represents a \u201cDELETE\u201d.\n+\n+As a sink, the upsert-kafka connector can consume a changelog stream. It will write INSERT/UPDATE_AFTER\n+data as normal Kafka messages value, and write DELETE data as Kafka messages with null values\n+(indicate tombstone for the key). Flink will guarantee the message ordering on the primary key by\n+partition data on the values of the primary key columns, so the update/deletion messages on the same\n+key will fall into the same partition.\n+\n+Dependencies\n+------------\n+\n+In order to set up the upsert-kafka connector, the following table provide dependency information for\n+both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.\n+\n+{% assign connector = site.data.sql-connectors['upsert-kafka'] %}\n+{% include sql-connector-download-table.html\n+    connector=connector\n+%}\n+\n+Full Example\n+----------------\n+\n+The example below shows how to create and use an Upsert Kafka table:\n+\n+<div class=\"codetabs\" markdown=\"1\">\n+<div data-lang=\"SQL\" markdown=\"1\">\n+{% highlight sql %}\n+CREATE TABLE pageviews_per_region (\n+  region STRING,\n+  pv BIGINT,\n+  uv BIGINT,\n+  PRIMARY KEY region NOT ENFORCED\n+) WITH (\n+  'connector' = 'upsert-kafka',\n+  'topic' = 'pageviews_per_region',\n+  'properties.bootstrap.servers' = '...',\n+  'key.format' = 'csv',", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzk3OTQzNw=="}, "originalCommit": {"oid": "7dff1e172e3b99d819cfc1cbc1eedf71bea94626"}, "originalPosition": 75}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4869, "cost": 1, "resetAt": "2021-11-12T11:57:46Z"}}}