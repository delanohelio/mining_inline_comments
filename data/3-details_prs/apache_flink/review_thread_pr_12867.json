{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDQ3MzI3NDM3", "number": 12867, "reviewThreads": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQwOToxMzozNlrOEOmTHA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQwODoxMzo1OVrOEQCL5A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgzNzQzMDA0OnYy", "diffSide": "RIGHT", "path": "flink-streaming-java/src/test/java/org/apache/flink/streaming/api/operators/collect/utils/CollectSinkFunctionTestWrapper.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQwOToxMzozNlrOGx1chA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQwOToxMzozNlrOGx1chA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDkwOTA2MA==", "bodyText": "typo: Funtion", "url": "https://github.com/apache/flink/pull/12867#discussion_r454909060", "createdAt": "2020-07-15T09:13:36Z", "author": {"login": "godfreyhe"}, "path": "flink-streaming-java/src/test/java/org/apache/flink/streaming/api/operators/collect/utils/CollectSinkFunctionTestWrapper.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.api.operators.collect.utils;\n+\n+import org.apache.flink.api.common.accumulators.Accumulator;\n+import org.apache.flink.api.common.accumulators.SerializedListAccumulator;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.runtime.io.disk.iomanager.IOManager;\n+import org.apache.flink.runtime.io.disk.iomanager.IOManagerAsync;\n+import org.apache.flink.runtime.memory.MemoryManager;\n+import org.apache.flink.runtime.operators.testutils.MockEnvironment;\n+import org.apache.flink.runtime.operators.testutils.MockEnvironmentBuilder;\n+import org.apache.flink.streaming.api.operators.StreamingRuntimeContext;\n+import org.apache.flink.streaming.api.operators.collect.CollectCoordinationRequest;\n+import org.apache.flink.streaming.api.operators.collect.CollectCoordinationResponse;\n+import org.apache.flink.streaming.api.operators.collect.CollectSinkFunction;\n+import org.apache.flink.streaming.api.operators.collect.CollectSinkOperatorCoordinator;\n+import org.apache.flink.streaming.util.MockStreamingRuntimeContext;\n+\n+import org.junit.Assert;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.concurrent.TimeUnit;\n+\n+/**\n+ * A wrapper class for creating, checkpointing and closing\n+ * {@link org.apache.flink.streaming.api.operators.collect.CollectSinkFunction} for tests.\n+ */\n+public class CollectSinkFunctionTestWrapper<IN> {\n+\n+\tpublic static final String ACCUMULATOR_NAME = \"tableCollectAccumulator\";\n+\n+\tprivate static final int SOCKET_TIMEOUT_MILLIS = 1000;\n+\tprivate static final int FUTURE_TIMEOUT_MILLIS = 10000;\n+\tprivate static final int MAX_RETIRES = 100;\n+\n+\tprivate final TypeSerializer<IN> serializer;\n+\tprivate final int maxBytesPerBatch;\n+\n+\tprivate final IOManager ioManager;\n+\tprivate final StreamingRuntimeContext runtimeContext;\n+\tprivate final MockOperatorEventGateway gateway;\n+\tprivate final CollectSinkOperatorCoordinator coordinator;\n+\tprivate final MockFunctionInitializationContext functionInitializationContext;\n+\n+\tprivate CollectSinkFunction<IN> function;\n+\n+\tpublic CollectSinkFunctionTestWrapper(TypeSerializer<IN> serializer, int maxBytesPerBatch) throws Exception {\n+\t\tthis.serializer = serializer;\n+\t\tthis.maxBytesPerBatch = maxBytesPerBatch;\n+\n+\t\tthis.ioManager = new IOManagerAsync();\n+\t\tMockEnvironment environment = new MockEnvironmentBuilder()\n+\t\t\t.setTaskName(\"mockTask\")\n+\t\t\t.setManagedMemorySize(4 * MemoryManager.DEFAULT_PAGE_SIZE)\n+\t\t\t.setIOManager(ioManager)\n+\t\t\t.build();\n+\t\tthis.runtimeContext = new MockStreamingRuntimeContext(false, 1, 0, environment);\n+\t\tthis.gateway = new MockOperatorEventGateway();\n+\n+\t\tthis.coordinator = new CollectSinkOperatorCoordinator(SOCKET_TIMEOUT_MILLIS);\n+\t\tthis.coordinator.start();\n+\n+\t\tthis.functionInitializationContext = new MockFunctionInitializationContext();\n+\t}\n+\n+\tpublic void closeWrapper() throws Exception {\n+\t\tcoordinator.close();\n+\t\tioManager.close();\n+\t}\n+\n+\tpublic CollectSinkOperatorCoordinator getCoordinator() {\n+\t\treturn coordinator;\n+\t}\n+\n+\tpublic void openFunction() throws Exception {\n+\t\tfunction = new CollectSinkFunction<>(serializer, maxBytesPerBatch, ACCUMULATOR_NAME);\n+\t\tfunction.setRuntimeContext(runtimeContext);\n+\t\tfunction.setOperatorEventGateway(gateway);\n+\t\tfunction.open(new Configuration());\n+\t\tcoordinator.handleEventFromOperator(0, gateway.getNextEvent());\n+\t}\n+\n+\tpublic void openFunctionWithState() throws Exception {\n+\t\tfunctionInitializationContext.getOperatorStateStore().revertToLastSuccessCheckpoint();\n+\t\tfunction = new CollectSinkFunction<>(serializer, maxBytesPerBatch, ACCUMULATOR_NAME);\n+\t\tfunction.setRuntimeContext(runtimeContext);\n+\t\tfunction.setOperatorEventGateway(gateway);\n+\t\tfunction.initializeState(functionInitializationContext);\n+\t\tfunction.open(new Configuration());\n+\t\tcoordinator.handleEventFromOperator(0, gateway.getNextEvent());\n+\t}\n+\n+\tpublic void invoke(IN record) throws Exception {\n+\t\tfunction.invoke(record, null);\n+\t}\n+\n+\tpublic void checkpointFunction(long checkpointId) throws Exception {\n+\t\tfunction.snapshotState(new MockFunctionSnapshotContext(checkpointId));\n+\t\tfunctionInitializationContext.getOperatorStateStore().checkpointBegin(checkpointId);\n+\t}\n+\n+\tpublic void checkpointComplete(long checkpointId) {\n+\t\tfunction.notifyCheckpointComplete(checkpointId);\n+\t\tfunctionInitializationContext.getOperatorStateStore().checkpointSuccess(checkpointId);\n+\t}\n+\n+\tpublic void closeFunctionNormally() throws Exception {\n+\t\t// this is a normal shutdown\n+\t\tfunction.accumulateFinalResults();\n+\t\tfunction.close();\n+\t}\n+\n+\tpublic void closeFuntionAbnormally() throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c3ac42923fc06b5d0d86c01c814918916ab86790"}, "originalPosition": 134}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgzNzQ0ODM3OnYy", "diffSide": "RIGHT", "path": "flink-streaming-java/src/test/java/org/apache/flink/streaming/api/operators/collect/utils/CollectSinkFunctionTestWrapper.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQwOToxODozMVrOGx1n3A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQwOToxODozMVrOGx1n3A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDkxMTk2NA==", "bodyText": "typo: Accumualtor", "url": "https://github.com/apache/flink/pull/12867#discussion_r454911964", "createdAt": "2020-07-15T09:18:31Z", "author": {"login": "godfreyhe"}, "path": "flink-streaming-java/src/test/java/org/apache/flink/streaming/api/operators/collect/utils/CollectSinkFunctionTestWrapper.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.api.operators.collect.utils;\n+\n+import org.apache.flink.api.common.accumulators.Accumulator;\n+import org.apache.flink.api.common.accumulators.SerializedListAccumulator;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.runtime.io.disk.iomanager.IOManager;\n+import org.apache.flink.runtime.io.disk.iomanager.IOManagerAsync;\n+import org.apache.flink.runtime.memory.MemoryManager;\n+import org.apache.flink.runtime.operators.testutils.MockEnvironment;\n+import org.apache.flink.runtime.operators.testutils.MockEnvironmentBuilder;\n+import org.apache.flink.streaming.api.operators.StreamingRuntimeContext;\n+import org.apache.flink.streaming.api.operators.collect.CollectCoordinationRequest;\n+import org.apache.flink.streaming.api.operators.collect.CollectCoordinationResponse;\n+import org.apache.flink.streaming.api.operators.collect.CollectSinkFunction;\n+import org.apache.flink.streaming.api.operators.collect.CollectSinkOperatorCoordinator;\n+import org.apache.flink.streaming.util.MockStreamingRuntimeContext;\n+\n+import org.junit.Assert;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.concurrent.TimeUnit;\n+\n+/**\n+ * A wrapper class for creating, checkpointing and closing\n+ * {@link org.apache.flink.streaming.api.operators.collect.CollectSinkFunction} for tests.\n+ */\n+public class CollectSinkFunctionTestWrapper<IN> {\n+\n+\tpublic static final String ACCUMULATOR_NAME = \"tableCollectAccumulator\";\n+\n+\tprivate static final int SOCKET_TIMEOUT_MILLIS = 1000;\n+\tprivate static final int FUTURE_TIMEOUT_MILLIS = 10000;\n+\tprivate static final int MAX_RETIRES = 100;\n+\n+\tprivate final TypeSerializer<IN> serializer;\n+\tprivate final int maxBytesPerBatch;\n+\n+\tprivate final IOManager ioManager;\n+\tprivate final StreamingRuntimeContext runtimeContext;\n+\tprivate final MockOperatorEventGateway gateway;\n+\tprivate final CollectSinkOperatorCoordinator coordinator;\n+\tprivate final MockFunctionInitializationContext functionInitializationContext;\n+\n+\tprivate CollectSinkFunction<IN> function;\n+\n+\tpublic CollectSinkFunctionTestWrapper(TypeSerializer<IN> serializer, int maxBytesPerBatch) throws Exception {\n+\t\tthis.serializer = serializer;\n+\t\tthis.maxBytesPerBatch = maxBytesPerBatch;\n+\n+\t\tthis.ioManager = new IOManagerAsync();\n+\t\tMockEnvironment environment = new MockEnvironmentBuilder()\n+\t\t\t.setTaskName(\"mockTask\")\n+\t\t\t.setManagedMemorySize(4 * MemoryManager.DEFAULT_PAGE_SIZE)\n+\t\t\t.setIOManager(ioManager)\n+\t\t\t.build();\n+\t\tthis.runtimeContext = new MockStreamingRuntimeContext(false, 1, 0, environment);\n+\t\tthis.gateway = new MockOperatorEventGateway();\n+\n+\t\tthis.coordinator = new CollectSinkOperatorCoordinator(SOCKET_TIMEOUT_MILLIS);\n+\t\tthis.coordinator.start();\n+\n+\t\tthis.functionInitializationContext = new MockFunctionInitializationContext();\n+\t}\n+\n+\tpublic void closeWrapper() throws Exception {\n+\t\tcoordinator.close();\n+\t\tioManager.close();\n+\t}\n+\n+\tpublic CollectSinkOperatorCoordinator getCoordinator() {\n+\t\treturn coordinator;\n+\t}\n+\n+\tpublic void openFunction() throws Exception {\n+\t\tfunction = new CollectSinkFunction<>(serializer, maxBytesPerBatch, ACCUMULATOR_NAME);\n+\t\tfunction.setRuntimeContext(runtimeContext);\n+\t\tfunction.setOperatorEventGateway(gateway);\n+\t\tfunction.open(new Configuration());\n+\t\tcoordinator.handleEventFromOperator(0, gateway.getNextEvent());\n+\t}\n+\n+\tpublic void openFunctionWithState() throws Exception {\n+\t\tfunctionInitializationContext.getOperatorStateStore().revertToLastSuccessCheckpoint();\n+\t\tfunction = new CollectSinkFunction<>(serializer, maxBytesPerBatch, ACCUMULATOR_NAME);\n+\t\tfunction.setRuntimeContext(runtimeContext);\n+\t\tfunction.setOperatorEventGateway(gateway);\n+\t\tfunction.initializeState(functionInitializationContext);\n+\t\tfunction.open(new Configuration());\n+\t\tcoordinator.handleEventFromOperator(0, gateway.getNextEvent());\n+\t}\n+\n+\tpublic void invoke(IN record) throws Exception {\n+\t\tfunction.invoke(record, null);\n+\t}\n+\n+\tpublic void checkpointFunction(long checkpointId) throws Exception {\n+\t\tfunction.snapshotState(new MockFunctionSnapshotContext(checkpointId));\n+\t\tfunctionInitializationContext.getOperatorStateStore().checkpointBegin(checkpointId);\n+\t}\n+\n+\tpublic void checkpointComplete(long checkpointId) {\n+\t\tfunction.notifyCheckpointComplete(checkpointId);\n+\t\tfunctionInitializationContext.getOperatorStateStore().checkpointSuccess(checkpointId);\n+\t}\n+\n+\tpublic void closeFunctionNormally() throws Exception {\n+\t\t// this is a normal shutdown\n+\t\tfunction.accumulateFinalResults();\n+\t\tfunction.close();\n+\t}\n+\n+\tpublic void closeFuntionAbnormally() throws Exception {\n+\t\t// this is an exceptional shutdown\n+\t\tfunction.close();\n+\t\tcoordinator.subtaskFailed(0, null);\n+\t}\n+\n+\tpublic CollectCoordinationResponse sendRequestAndGetResponse(String version, long offset) throws Exception {\n+\t\tCollectCoordinationResponse response;\n+\t\tfor (int i = 0; i < MAX_RETIRES; i++) {\n+\t\t\tresponse = sendRequest(version, offset);\n+\t\t\tif (response.getLastCheckpointedOffset() >= 0) {\n+\t\t\t\treturn response;\n+\t\t\t}\n+\t\t}\n+\t\tthrow new RuntimeException(\"Too many retries in sendRequestAndGetValidResponse\");\n+\t}\n+\n+\tprivate CollectCoordinationResponse sendRequest(String version, long offset) throws Exception {\n+\t\tCollectCoordinationRequest request = new CollectCoordinationRequest(version, offset);\n+\t\t// we add a timeout to not block the tests if it fails\n+\t\treturn ((CollectCoordinationResponse) coordinator\n+\t\t\t.handleCoordinationRequest(request).get(FUTURE_TIMEOUT_MILLIS, TimeUnit.MILLISECONDS));\n+\t}\n+\n+\tpublic Tuple2<Long, CollectCoordinationResponse> getAccumualtorResults() throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c3ac42923fc06b5d0d86c01c814918916ab86790"}, "originalPosition": 158}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgzNzU2NjkyOnYy", "diffSide": "RIGHT", "path": "flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/collect/CollectResultIterator.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQwOTo1MDoyN1rOGx2xDQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNlQwOToxMToxM1rOGyiIsQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDkzMDcwMQ==", "bodyText": "I think this branch is unnecessary, because it's illegal that checkpoint interval is less than MINIMAL_CHECKPOINT_TIME, many places have such validation, e.g. CheckpointConfig.setCheckpointInterval", "url": "https://github.com/apache/flink/pull/12867#discussion_r454930701", "createdAt": "2020-07-15T09:50:27Z", "author": {"login": "godfreyhe"}, "path": "flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/collect/CollectResultIterator.java", "diffHunk": "@@ -40,18 +43,37 @@\n \tpublic CollectResultIterator(\n \t\t\tCompletableFuture<OperatorID> operatorIdFuture,\n \t\t\tTypeSerializer<T> serializer,\n-\t\t\tString accumulatorName) {\n-\t\tthis.fetcher = new CollectResultFetcher<>(operatorIdFuture, serializer, accumulatorName);\n+\t\t\tString accumulatorName,\n+\t\t\tCheckpointConfig checkpointConfig) {\n+\t\tif (checkpointConfig.getCheckpointingMode() == CheckpointingMode.EXACTLY_ONCE) {\n+\t\t\tif (checkpointConfig.getCheckpointInterval() >= CheckpointCoordinatorConfiguration.MINIMAL_CHECKPOINT_TIME) {\n+\t\t\t\tthis.fetcher = new CollectResultFetcher<>(\n+\t\t\t\t\tnew CheckpointedCollectResultBuffer<>(serializer),\n+\t\t\t\t\toperatorIdFuture,\n+\t\t\t\t\taccumulatorName);\n+\t\t\t} else {\n+\t\t\t\tthis.fetcher = new CollectResultFetcher<>(\n+\t\t\t\t\tnew UncheckpointedCollectResultBuffer<>(serializer, false),\n+\t\t\t\t\toperatorIdFuture,\n+\t\t\t\t\taccumulatorName);\n+\t\t\t}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c3ac42923fc06b5d0d86c01c814918916ab86790"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTYzMzMyNA==", "bodyText": "If checkpointConfig.getCheckpointInterval() >= CheckpointCoordinatorConfiguration.MINIMAL_CHECKPOINT_TIME we are sure that the user explicitly enables a checkpoint. Otherwise we have to sync with the default value of checkpoint interval in CheckpointCoordinatorConfiguration.", "url": "https://github.com/apache/flink/pull/12867#discussion_r455633324", "createdAt": "2020-07-16T08:58:27Z", "author": {"login": "tsreaper"}, "path": "flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/collect/CollectResultIterator.java", "diffHunk": "@@ -40,18 +43,37 @@\n \tpublic CollectResultIterator(\n \t\t\tCompletableFuture<OperatorID> operatorIdFuture,\n \t\t\tTypeSerializer<T> serializer,\n-\t\t\tString accumulatorName) {\n-\t\tthis.fetcher = new CollectResultFetcher<>(operatorIdFuture, serializer, accumulatorName);\n+\t\t\tString accumulatorName,\n+\t\t\tCheckpointConfig checkpointConfig) {\n+\t\tif (checkpointConfig.getCheckpointingMode() == CheckpointingMode.EXACTLY_ONCE) {\n+\t\t\tif (checkpointConfig.getCheckpointInterval() >= CheckpointCoordinatorConfiguration.MINIMAL_CHECKPOINT_TIME) {\n+\t\t\t\tthis.fetcher = new CollectResultFetcher<>(\n+\t\t\t\t\tnew CheckpointedCollectResultBuffer<>(serializer),\n+\t\t\t\t\toperatorIdFuture,\n+\t\t\t\t\taccumulatorName);\n+\t\t\t} else {\n+\t\t\t\tthis.fetcher = new CollectResultFetcher<>(\n+\t\t\t\t\tnew UncheckpointedCollectResultBuffer<>(serializer, false),\n+\t\t\t\t\toperatorIdFuture,\n+\t\t\t\t\taccumulatorName);\n+\t\t\t}", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDkzMDcwMQ=="}, "originalCommit": {"oid": "c3ac42923fc06b5d0d86c01c814918916ab86790"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTY0MTI2NQ==", "bodyText": "OK... It seems that checkpointConfig.isCheckpointingEnabled() is a better solution.", "url": "https://github.com/apache/flink/pull/12867#discussion_r455641265", "createdAt": "2020-07-16T09:11:13Z", "author": {"login": "tsreaper"}, "path": "flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/collect/CollectResultIterator.java", "diffHunk": "@@ -40,18 +43,37 @@\n \tpublic CollectResultIterator(\n \t\t\tCompletableFuture<OperatorID> operatorIdFuture,\n \t\t\tTypeSerializer<T> serializer,\n-\t\t\tString accumulatorName) {\n-\t\tthis.fetcher = new CollectResultFetcher<>(operatorIdFuture, serializer, accumulatorName);\n+\t\t\tString accumulatorName,\n+\t\t\tCheckpointConfig checkpointConfig) {\n+\t\tif (checkpointConfig.getCheckpointingMode() == CheckpointingMode.EXACTLY_ONCE) {\n+\t\t\tif (checkpointConfig.getCheckpointInterval() >= CheckpointCoordinatorConfiguration.MINIMAL_CHECKPOINT_TIME) {\n+\t\t\t\tthis.fetcher = new CollectResultFetcher<>(\n+\t\t\t\t\tnew CheckpointedCollectResultBuffer<>(serializer),\n+\t\t\t\t\toperatorIdFuture,\n+\t\t\t\t\taccumulatorName);\n+\t\t\t} else {\n+\t\t\t\tthis.fetcher = new CollectResultFetcher<>(\n+\t\t\t\t\tnew UncheckpointedCollectResultBuffer<>(serializer, false),\n+\t\t\t\t\toperatorIdFuture,\n+\t\t\t\t\taccumulatorName);\n+\t\t\t}", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDkzMDcwMQ=="}, "originalCommit": {"oid": "c3ac42923fc06b5d0d86c01c814918916ab86790"}, "originalPosition": 29}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgzNzkzMjk3OnYy", "diffSide": "RIGHT", "path": "flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/collect/UncheckpointedCollectResultBuffer.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQxMTo0MToxMVrOGx6Qlw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQxMTo0MToxMVrOGx6Qlw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDk4NzkyNw==", "bodyText": "change addResults  as a utility method ? so that we can handle the variables (e.g. offset) in one method, and make this method more readable.", "url": "https://github.com/apache/flink/pull/12867#discussion_r454987927", "createdAt": "2020-07-15T11:41:11Z", "author": {"login": "godfreyhe"}, "path": "flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/collect/UncheckpointedCollectResultBuffer.java", "diffHunk": "@@ -0,0 +1,58 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.api.operators.collect;\n+\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+\n+import java.io.IOException;\n+\n+/**\n+ * A buffer which encapsulates the logic of dealing with the response from the {@link CollectSinkFunction}.\n+ * It ignores the checkpoint related fields in the response.\n+ * See Java doc of {@link CollectSinkFunction} for explanation of this communication protocol.\n+ */\n+public class UncheckpointedCollectResultBuffer<T> extends AbstractCollectResultBuffer<T> {\n+\n+\tprivate final boolean failureTolerance;\n+\n+\tpublic UncheckpointedCollectResultBuffer(TypeSerializer<T> serializer, boolean failureTolerance) {\n+\t\tsuper(serializer);\n+\t\tthis.failureTolerance = failureTolerance;\n+\t}\n+\n+\t@Override\n+\tpublic void dealWithResponse(CollectCoordinationResponse response, long responseOffset) throws IOException {\n+\t\tString responseVersion = response.getVersion();\n+\n+\t\tif (!version.equals(responseVersion)) {\n+\t\t\tif (!INIT_VERSION.equals(version) && !failureTolerance) {\n+\t\t\t\t// sink restarted but we do not tolerate failure\n+\t\t\t\tthrow new RuntimeException(\"Job restarted\");\n+\t\t\t}\n+\n+\t\t\treset();\n+\t\t\tversion = responseVersion;\n+\t\t}\n+\n+\t\taddResults(response, responseOffset);\n+\t\t// the results are instantly visible by users\n+\t\tuserVisibleTail = offset;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c3ac42923fc06b5d0d86c01c814918916ab86790"}, "originalPosition": 55}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgzODAxMDI0OnYy", "diffSide": "RIGHT", "path": "flink-streaming-java/src/test/java/org/apache/flink/streaming/api/operators/collect/CollectSinkFunctionRandomITCase.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQxMjowNDozNlrOGx6_Qw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNlQwODo1OTowNFrOGyhrMw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDk5OTg3NQ==", "bodyText": "checkpointedData for  UncheckpointedDataFeeder ?", "url": "https://github.com/apache/flink/pull/12867#discussion_r454999875", "createdAt": "2020-07-15T12:04:36Z", "author": {"login": "godfreyhe"}, "path": "flink-streaming-java/src/test/java/org/apache/flink/streaming/api/operators/collect/CollectSinkFunctionRandomITCase.java", "diffHunk": "@@ -0,0 +1,366 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.api.operators.collect;\n+\n+import org.apache.flink.api.common.JobID;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.common.typeutils.base.IntSerializer;\n+import org.apache.flink.runtime.jobgraph.OperatorID;\n+import org.apache.flink.streaming.api.operators.collect.utils.CollectSinkFunctionTestWrapper;\n+import org.apache.flink.streaming.api.operators.collect.utils.TestJobClient;\n+import org.apache.flink.util.OptionalFailure;\n+import org.apache.flink.util.TestLogger;\n+import org.apache.flink.util.function.RunnableWithException;\n+\n+import org.hamcrest.CoreMatchers;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.ListIterator;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.CompletableFuture;\n+\n+import static org.apache.flink.streaming.api.operators.collect.utils.CollectSinkFunctionTestWrapper.ACCUMULATOR_NAME;\n+\n+/**\n+ * Random IT cases for {@link CollectSinkFunction}.\n+ * It will perform random insert, random checkpoint and random restart.\n+ */\n+public class CollectSinkFunctionRandomITCase extends TestLogger {\n+\n+\tprivate static final int MAX_RESULTS_PER_BATCH = 3;\n+\tprivate static final JobID TEST_JOB_ID = new JobID();\n+\tprivate static final OperatorID TEST_OPERATOR_ID = new OperatorID();\n+\n+\tprivate static final TypeSerializer<Integer> serializer = IntSerializer.INSTANCE;\n+\n+\tprivate CollectSinkFunctionTestWrapper<Integer> functionWrapper;\n+\tprivate boolean jobFinished;\n+\n+\t@Test\n+\tpublic void testUncheckpointedFunction() throws Exception {\n+\t\t// run multiple times for this random test\n+\t\tfor (int testCount = 30; testCount > 0; testCount--) {\n+\t\t\tfunctionWrapper = new CollectSinkFunctionTestWrapper<>(serializer, MAX_RESULTS_PER_BATCH * 4);\n+\t\t\tjobFinished = false;\n+\n+\t\t\tList<Integer> expected = new ArrayList<>();\n+\t\t\tfor (int i = 0; i < 50; i++) {\n+\t\t\t\texpected.add(i);\n+\t\t\t}\n+\t\t\tThread feeder = new ThreadWithException(new UncheckpointedDataFeeder(expected));\n+\n+\t\t\tList<Integer> actual = runFunctionRandomTest(feeder);\n+\t\t\tassertResultsEqualAfterSort(expected, actual);\n+\n+\t\t\tfunctionWrapper.closeWrapper();\n+\t\t}\n+\t}\n+\n+\t@Test\n+\tpublic void testCheckpointedFunction() throws Exception {\n+\t\t// run multiple times for this random test\n+\t\tfor (int testCount = 30; testCount > 0; testCount--) {\n+\t\t\tfunctionWrapper = new CollectSinkFunctionTestWrapper<>(serializer, MAX_RESULTS_PER_BATCH * 4);\n+\t\t\tjobFinished = false;\n+\n+\t\t\tList<Integer> expected = new ArrayList<>();\n+\t\t\tfor (int i = 0; i < 50; i++) {\n+\t\t\t\texpected.add(i);\n+\t\t\t}\n+\t\t\tThread feeder = new ThreadWithException(new CheckpointedDataFeeder(expected));\n+\n+\t\t\tList<Integer> actual = runFunctionRandomTest(feeder);\n+\t\t\tassertResultsEqualAfterSort(expected, actual);\n+\n+\t\t\tfunctionWrapper.closeWrapper();\n+\t\t}\n+\t}\n+\n+\tprivate List<Integer> runFunctionRandomTest(Thread feeder) throws Exception {\n+\t\tCollectClient collectClient = new CollectClient();\n+\t\tThread client = new ThreadWithException(collectClient);\n+\n+\t\tThread.UncaughtExceptionHandler exceptionHandler = (t, e) -> {\n+\t\t\tfeeder.interrupt();\n+\t\t\tclient.interrupt();\n+\t\t\te.printStackTrace();\n+\t\t};\n+\t\tfeeder.setUncaughtExceptionHandler(exceptionHandler);\n+\t\tclient.setUncaughtExceptionHandler(exceptionHandler);\n+\n+\t\tfeeder.start();\n+\t\tclient.start();\n+\t\tfeeder.join();\n+\t\tclient.join();\n+\n+\t\treturn collectClient.results;\n+\t}\n+\n+\tprivate void assertResultsEqualAfterSort(List<Integer> expected, List<Integer> actual) {\n+\t\tCollections.sort(expected);\n+\t\tCollections.sort(actual);\n+\t\tAssert.assertThat(actual, CoreMatchers.is(expected));\n+\t}\n+\n+\t/**\n+\t * A {@link RunnableWithException} feeding data to the function. It will fail when half of the data is fed.\n+\t */\n+\tprivate class UncheckpointedDataFeeder implements RunnableWithException {\n+\n+\t\tprivate LinkedList<Integer> data;\n+\t\tprivate final List<Integer> checkpointedData;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c3ac42923fc06b5d0d86c01c814918916ab86790"}, "originalPosition": 133}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTYzMzcxNQ==", "bodyText": "originalData might be the proper name.", "url": "https://github.com/apache/flink/pull/12867#discussion_r455633715", "createdAt": "2020-07-16T08:59:04Z", "author": {"login": "tsreaper"}, "path": "flink-streaming-java/src/test/java/org/apache/flink/streaming/api/operators/collect/CollectSinkFunctionRandomITCase.java", "diffHunk": "@@ -0,0 +1,366 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.api.operators.collect;\n+\n+import org.apache.flink.api.common.JobID;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.common.typeutils.base.IntSerializer;\n+import org.apache.flink.runtime.jobgraph.OperatorID;\n+import org.apache.flink.streaming.api.operators.collect.utils.CollectSinkFunctionTestWrapper;\n+import org.apache.flink.streaming.api.operators.collect.utils.TestJobClient;\n+import org.apache.flink.util.OptionalFailure;\n+import org.apache.flink.util.TestLogger;\n+import org.apache.flink.util.function.RunnableWithException;\n+\n+import org.hamcrest.CoreMatchers;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.ListIterator;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.CompletableFuture;\n+\n+import static org.apache.flink.streaming.api.operators.collect.utils.CollectSinkFunctionTestWrapper.ACCUMULATOR_NAME;\n+\n+/**\n+ * Random IT cases for {@link CollectSinkFunction}.\n+ * It will perform random insert, random checkpoint and random restart.\n+ */\n+public class CollectSinkFunctionRandomITCase extends TestLogger {\n+\n+\tprivate static final int MAX_RESULTS_PER_BATCH = 3;\n+\tprivate static final JobID TEST_JOB_ID = new JobID();\n+\tprivate static final OperatorID TEST_OPERATOR_ID = new OperatorID();\n+\n+\tprivate static final TypeSerializer<Integer> serializer = IntSerializer.INSTANCE;\n+\n+\tprivate CollectSinkFunctionTestWrapper<Integer> functionWrapper;\n+\tprivate boolean jobFinished;\n+\n+\t@Test\n+\tpublic void testUncheckpointedFunction() throws Exception {\n+\t\t// run multiple times for this random test\n+\t\tfor (int testCount = 30; testCount > 0; testCount--) {\n+\t\t\tfunctionWrapper = new CollectSinkFunctionTestWrapper<>(serializer, MAX_RESULTS_PER_BATCH * 4);\n+\t\t\tjobFinished = false;\n+\n+\t\t\tList<Integer> expected = new ArrayList<>();\n+\t\t\tfor (int i = 0; i < 50; i++) {\n+\t\t\t\texpected.add(i);\n+\t\t\t}\n+\t\t\tThread feeder = new ThreadWithException(new UncheckpointedDataFeeder(expected));\n+\n+\t\t\tList<Integer> actual = runFunctionRandomTest(feeder);\n+\t\t\tassertResultsEqualAfterSort(expected, actual);\n+\n+\t\t\tfunctionWrapper.closeWrapper();\n+\t\t}\n+\t}\n+\n+\t@Test\n+\tpublic void testCheckpointedFunction() throws Exception {\n+\t\t// run multiple times for this random test\n+\t\tfor (int testCount = 30; testCount > 0; testCount--) {\n+\t\t\tfunctionWrapper = new CollectSinkFunctionTestWrapper<>(serializer, MAX_RESULTS_PER_BATCH * 4);\n+\t\t\tjobFinished = false;\n+\n+\t\t\tList<Integer> expected = new ArrayList<>();\n+\t\t\tfor (int i = 0; i < 50; i++) {\n+\t\t\t\texpected.add(i);\n+\t\t\t}\n+\t\t\tThread feeder = new ThreadWithException(new CheckpointedDataFeeder(expected));\n+\n+\t\t\tList<Integer> actual = runFunctionRandomTest(feeder);\n+\t\t\tassertResultsEqualAfterSort(expected, actual);\n+\n+\t\t\tfunctionWrapper.closeWrapper();\n+\t\t}\n+\t}\n+\n+\tprivate List<Integer> runFunctionRandomTest(Thread feeder) throws Exception {\n+\t\tCollectClient collectClient = new CollectClient();\n+\t\tThread client = new ThreadWithException(collectClient);\n+\n+\t\tThread.UncaughtExceptionHandler exceptionHandler = (t, e) -> {\n+\t\t\tfeeder.interrupt();\n+\t\t\tclient.interrupt();\n+\t\t\te.printStackTrace();\n+\t\t};\n+\t\tfeeder.setUncaughtExceptionHandler(exceptionHandler);\n+\t\tclient.setUncaughtExceptionHandler(exceptionHandler);\n+\n+\t\tfeeder.start();\n+\t\tclient.start();\n+\t\tfeeder.join();\n+\t\tclient.join();\n+\n+\t\treturn collectClient.results;\n+\t}\n+\n+\tprivate void assertResultsEqualAfterSort(List<Integer> expected, List<Integer> actual) {\n+\t\tCollections.sort(expected);\n+\t\tCollections.sort(actual);\n+\t\tAssert.assertThat(actual, CoreMatchers.is(expected));\n+\t}\n+\n+\t/**\n+\t * A {@link RunnableWithException} feeding data to the function. It will fail when half of the data is fed.\n+\t */\n+\tprivate class UncheckpointedDataFeeder implements RunnableWithException {\n+\n+\t\tprivate LinkedList<Integer> data;\n+\t\tprivate final List<Integer> checkpointedData;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDk5OTg3NQ=="}, "originalCommit": {"oid": "c3ac42923fc06b5d0d86c01c814918916ab86790"}, "originalPosition": 133}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0NTQ4NDkwOnYy", "diffSide": "RIGHT", "path": "docs/dev/table/sql/queries.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QwMTo1MDowMFrOGzCnig==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QwMTo1MDowMFrOGzCnig==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjE3MzQ1MA==", "bodyText": "users do not know what the the function calls means", "url": "https://github.com/apache/flink/pull/12867#discussion_r456173450", "createdAt": "2020-07-17T01:50:00Z", "author": {"login": "godfreyhe"}, "path": "docs/dev/table/sql/queries.md", "diffHunk": "@@ -145,21 +145,16 @@ A SELECT statement or a VALUES statement can be executed to collect the content\n `TableResult.collect()` method returns a closeable row iterator. The select job will not be finished unless all result data has been collected. We should actively close the job to avoid resource leak through the `CloseableIterator#close()` method. \n We can also print the select result to client console through the `TableResult.print()` method. The result data in `TableResult` can be accessed only once. Thus, `collect()` and `print()` must not be called after each other.\n \n-For streaming job, `TableResult.collect()` method or `TableResult.print` method guarantee end-to-end exactly-once record delivery. This requires the checkpointing mechanism to be enabled. By default, checkpointing is disabled. To enable checkpointing, we can set checkpointing properties (see the <a href=\"{{ site.baseurl }}/ops/config.html#checkpointing\">checkpointing config</a> for details) through `TableConfig`.\n-So a result record can be accessed by client only after its corresponding checkpoint completes.\n-\n-**Notes:** For streaming mode, only append-only query is supported now. \n+`TableResult.collect()` and `TableResult.print()` have slightly different behaviors under different checkpointing settings (to enable checkpointing for a streaming job, see <a href=\"{{ site.baseurl }}/ops/config.html#checkpointing\">checkpointing config</a>).\n+* If the user is running a batch job, or does not enable checkpointing for a streaming job, `TableResult.collect()` and `TableResult.print()` have neither exactly-once nor at-least-once guarantee. Query results are immediately accessible by the clients once they're produced, but the function calls will throw an exception when the job fails and restarts;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "577a355b33c5bdd7176f207201dc1b33d98c665a"}, "originalPosition": 9}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0NTgzNzg4OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/TableResult.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QwNToyNjowMVrOGzFyyw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QwNToyNjowMVrOGzFyyw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjIyNTQ4Mw==", "bodyText": "ditto", "url": "https://github.com/apache/flink/pull/12867#discussion_r456225483", "createdAt": "2020-07-17T05:26:01Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/TableResult.java", "diffHunk": "@@ -132,10 +132,22 @@\n \t *  }\n \t * }</pre>\n \t *\n-\t * <p>For streaming mode, this method guarantees end-to-end exactly-once record delivery\n-\t * which requires the checkpointing mechanism to be enabled.\n-\t * By default, checkpointing is disabled. To enable checkpointing, set checkpointing properties\n-\t * (see ExecutionCheckpointingOptions) through {@link TableConfig#getConfiguration()}.\n+\t * <p>This method has slightly different behaviors under different checkpointing settings\n+\t * (to enable checkpointing for a streaming job,\n+\t * set checkpointing properties through {@link TableConfig#getConfiguration()}).\n+\t * <ul>\n+\t *     <li>If the user is running a batch job, or does not enable checkpointing for a streaming job,\n+\t *     this method has neither exactly-once nor at-least-once guarantee.\n+\t *     Query results are immediately accessible by the clients once they're produced,\n+\t *     but the function calls will throw an exception when the job fails and restarts.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a64ca67c2e50380c4b6aca1748fb71a3c620377a"}, "originalPosition": 15}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0NTgzOTgyOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/TableResult.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QwNToyNzoxN1rOGzFz8A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QwNToyNzoxN1rOGzFz8A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjIyNTc3Ng==", "bodyText": "ditto", "url": "https://github.com/apache/flink/pull/12867#discussion_r456225776", "createdAt": "2020-07-17T05:27:17Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/TableResult.java", "diffHunk": "@@ -146,10 +158,22 @@\n \t/**\n \t * Print the result contents as tableau form to client console.\n \t *\n-\t * <p>For streaming mode, this method guarantees end-to-end exactly-once record delivery\n-\t * which requires the checkpointing mechanism to be enabled.\n-\t * By default, checkpointing is disabled. To enable checkpointing, set checkpointing properties\n-\t * (see ExecutionCheckpointingOptions) through {@link TableConfig#getConfiguration()}.\n+\t * <p>This method has slightly different behaviors under different checkpointing settings\n+\t * (to enable checkpointing for a streaming job,\n+\t * set checkpointing properties through {@link TableConfig#getConfiguration()}).\n+\t * <ul>\n+\t *     <li>If the user is running a batch job, or does not enable checkpointing for a streaming job,\n+\t *     this method has neither exactly-once nor at-least-once guarantee.\n+\t *     Query results are immediately accessible by the clients once they're produced,\n+\t *     but the function calls will throw an exception when the job fails and restarts.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a64ca67c2e50380c4b6aca1748fb71a3c620377a"}, "originalPosition": 42}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0NTg0MDQ3OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/TableResult.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QwNToyNzo0NVrOGzF0ZA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QwNToyNzo0NVrOGzF0ZA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjIyNTg5Mg==", "bodyText": "If exactly-once checkpointing is enabled for a streaming job, ?", "url": "https://github.com/apache/flink/pull/12867#discussion_r456225892", "createdAt": "2020-07-17T05:27:45Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/TableResult.java", "diffHunk": "@@ -132,10 +132,22 @@\n \t *  }\n \t * }</pre>\n \t *\n-\t * <p>For streaming mode, this method guarantees end-to-end exactly-once record delivery\n-\t * which requires the checkpointing mechanism to be enabled.\n-\t * By default, checkpointing is disabled. To enable checkpointing, set checkpointing properties\n-\t * (see ExecutionCheckpointingOptions) through {@link TableConfig#getConfiguration()}.\n+\t * <p>This method has slightly different behaviors under different checkpointing settings\n+\t * (to enable checkpointing for a streaming job,\n+\t * set checkpointing properties through {@link TableConfig#getConfiguration()}).\n+\t * <ul>\n+\t *     <li>If the user is running a batch job, or does not enable checkpointing for a streaming job,\n+\t *     this method has neither exactly-once nor at-least-once guarantee.\n+\t *     Query results are immediately accessible by the clients once they're produced,\n+\t *     but the function calls will throw an exception when the job fails and restarts.\n+\t *     <li>If the user enables exactly-once checkpointing for a streaming job,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a64ca67c2e50380c4b6aca1748fb71a3c620377a"}, "originalPosition": 16}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0NTg0MTE2OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/TableResult.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QwNToyODoxOFrOGzF03w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QwNToyODoxOFrOGzF03w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjIyNjAxNQ==", "bodyText": "please update the doc of TableResult in flink-python", "url": "https://github.com/apache/flink/pull/12867#discussion_r456226015", "createdAt": "2020-07-17T05:28:18Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/TableResult.java", "diffHunk": "@@ -146,10 +158,22 @@\n \t/**\n \t * Print the result contents as tableau form to client console.\n \t *\n-\t * <p>For streaming mode, this method guarantees end-to-end exactly-once record delivery\n-\t * which requires the checkpointing mechanism to be enabled.\n-\t * By default, checkpointing is disabled. To enable checkpointing, set checkpointing properties\n-\t * (see ExecutionCheckpointingOptions) through {@link TableConfig#getConfiguration()}.\n+\t * <p>This method has slightly different behaviors under different checkpointing settings", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a64ca67c2e50380c4b6aca1748fb71a3c620377a"}, "originalPosition": 35}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1MjQ4NDg0OnYy", "diffSide": "RIGHT", "path": "flink-python/pyflink/table/table_result.py", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQwODoxNDowMFrOGz-6eA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQwODoxNDowMFrOGz-6eA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzE2MTMzNg==", "bodyText": "Reduce indentation", "url": "https://github.com/apache/flink/pull/12867#discussion_r457161336", "createdAt": "2020-07-20T08:14:00Z", "author": {"login": "godfreyhe"}, "path": "flink-python/pyflink/table/table_result.py", "diffHunk": "@@ -134,10 +134,19 @@ def print(self):\n         \"\"\"\n         Print the result contents as tableau form to client console.\n \n-        For streaming mode, this method guarantees end-to-end exactly-once record delivery\n-        which requires the checkpointing mechanism to be enabled.\n-        By default, checkpointing is disabled. To enable checkpointing, set checkpointing properties\n-        (see ExecutionCheckpointingOptions) through `TableConfig#getConfiguration()`.\n+        This method has slightly different behaviors under different checkpointing settings.\n+\n+            - For batch jobs or streaming jobs without checkpointing,\n+              this method has neither exactly-once nor at-least-once guarantee.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bf49c2297508e5dde1a9e0d8d4dfbafefc5f46f2"}, "originalPosition": 11}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4184, "cost": 1, "resetAt": "2021-11-12T11:57:46Z"}}}