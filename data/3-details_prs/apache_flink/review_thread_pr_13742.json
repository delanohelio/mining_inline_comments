{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTA4MDc1NjI4", "number": 13742, "reviewThreads": {"totalCount": 28, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QwODoyNDozMVrOExGsQg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QxMjo0MDowMVrOEyVO-w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE5OTI1MzE0OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/delegation/BatchPlanner.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QwODoyNDozMVrOHnDxaQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QwODoyNDozMVrOHnDxaQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDcxODMxMw==", "bodyText": "reorder the import", "url": "https://github.com/apache/flink/pull/13742#discussion_r510718313", "createdAt": "2020-10-23T08:24:31Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/delegation/BatchPlanner.scala", "diffHunk": "@@ -26,20 +26,20 @@ import org.apache.flink.table.operations.{CatalogSinkModifyOperation, ModifyOper\n import org.apache.flink.table.planner.operations.PlannerQueryOperation\n import org.apache.flink.table.planner.plan.`trait`.FlinkRelDistributionTraitDef\n import org.apache.flink.table.planner.plan.nodes.exec.{BatchExecNode, ExecNode}\n-import org.apache.flink.table.planner.plan.nodes.process.DAGProcessContext\n+import org.apache.flink.table.planner.plan.nodes.process.{DAGProcessContext, DAGProcessor}\n import org.apache.flink.table.planner.plan.optimize.{BatchCommonSubGraphBasedOptimizer, Optimizer}\n-import org.apache.flink.table.planner.plan.reuse.DeadlockBreakupProcessor\n+import org.apache.flink.table.planner.plan.reuse.{DeadlockBreakupProcessor, MultipleInputNodeCreationProcessor}\n import org.apache.flink.table.planner.plan.utils.{ExecNodePlanDumper, FlinkRelOptUtil}\n import org.apache.flink.table.planner.sinks.{BatchSelectTableSink, SelectTableSinkBase}\n import org.apache.flink.table.planner.utils.{DummyStreamExecutionEnvironment, ExecutorUtils, PlanUtil}\n-\n import org.apache.calcite.plan.{ConventionTraitDef, RelTrait, RelTraitDef}\n import org.apache.calcite.rel.logical.LogicalTableModify\n import org.apache.calcite.rel.{RelCollationTraitDef, RelNode}\n import org.apache.calcite.sql.SqlExplainLevel\n-\n import java.util\n \n+import org.apache.flink.table.api.config.OptimizerConfigOptions", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3c50597400946ab21e7dba24673a0b5df30eaf2b"}, "originalPosition": 20}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNDU5MzA2OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/config/OptimizerConfigOptions.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNVQwMjozOToxNlrOHn1zjQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNVQwMjozOToxNlrOHn1zjQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTUzODA2MQ==", "bodyText": "nodes -> operators", "url": "https://github.com/apache/flink/pull/13742#discussion_r511538061", "createdAt": "2020-10-25T02:39:16Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/config/OptimizerConfigOptions.java", "diffHunk": "@@ -99,4 +99,12 @@\n \t\tkey(\"table.optimizer.join-reorder-enabled\")\n \t\t\t.defaultValue(false)\n \t\t\t.withDescription(\"Enables join reorder in optimizer. Default is disabled.\");\n+\n+\t@Documentation.TableOption(execMode = Documentation.ExecMode.BATCH_STREAMING)\n+\tpublic static final ConfigOption<Boolean> TABLE_OPTIMIZER_MULTIPLE_INPUT_ENABLED =\n+\t\tkey(\"table.optimizer.multiple-input-enabled\")\n+\t\t\t.defaultValue(false)\n+\t\t\t.withDescription(\"Enables creating multiple input nodes to reduce shuffling. \" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3c50597400946ab21e7dba24673a0b5df30eaf2b"}, "originalPosition": 9}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNDYxMjI4OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/InputPriorityConflictResolver.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNVQwMzowOToyMFrOHn173A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNVQwMzowOToyMFrOHn173A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTU0MDE4OA==", "bodyText": "add some comments to explain the fields", "url": "https://github.com/apache/flink/pull/13742#discussion_r511540188", "createdAt": "2020-10-25T03:09:20Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/InputPriorityConflictResolver.java", "diffHunk": "@@ -97,31 +98,79 @@\n public class InputPriorityConflictResolver {\n \n \tprivate final List<ExecNode<?, ?>> roots;\n+\tprivate final Set<ExecNode<?, ?>> boundaries;\n+\tprivate final ExecEdge.DamBehavior safeDamBehavior;\n+\tprivate final ShuffleMode shuffleMode;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3c50597400946ab21e7dba24673a0b5df30eaf2b"}, "originalPosition": 14}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNDYxNTM2OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/InputPriorityConflictResolver.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNVQwMzoxNDo0MlrOHn19Kw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNVQwMzoxNDo0MlrOHn19Kw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTU0MDUyMw==", "bodyText": "We should introduce another class to calculate the input orders, named: InputOrderDerivation ? maybe we also need a Base class of InputOrderDerivation and InputPriorityConflictResolver", "url": "https://github.com/apache/flink/pull/13742#discussion_r511540523", "createdAt": "2020-10-25T03:14:42Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/InputPriorityConflictResolver.java", "diffHunk": "@@ -97,31 +98,79 @@\n public class InputPriorityConflictResolver {\n \n \tprivate final List<ExecNode<?, ?>> roots;\n+\tprivate final Set<ExecNode<?, ?>> boundaries;\n+\tprivate final ExecEdge.DamBehavior safeDamBehavior;\n+\tprivate final ShuffleMode shuffleMode;\n \n \tprivate TopologyGraph graph;\n \n-\tpublic InputPriorityConflictResolver(List<ExecNode<?, ?>> roots) {\n+\tpublic InputPriorityConflictResolver(\n+\t\t\tList<ExecNode<?, ?>> roots,\n+\t\t\tSet<ExecNode<?, ?>> boundaries,\n+\t\t\tExecEdge.DamBehavior safeDamBehavior,\n+\t\t\tShuffleMode shuffleMode) {\n \t\tPreconditions.checkArgument(\n \t\t\troots.stream().allMatch(root -> root instanceof BatchExecNode),\n \t\t\t\"InputPriorityConflictResolver can only be used for batch jobs.\");\n \t\tthis.roots = roots;\n+\t\tthis.boundaries = boundaries;\n+\t\tthis.safeDamBehavior = safeDamBehavior;\n+\t\tthis.shuffleMode = shuffleMode;\n \t}\n \n \tpublic void detectAndResolve() {\n \t\t// build an initial topology graph\n-\t\tgraph = new TopologyGraph(roots);\n+\t\tgraph = new TopologyGraph(roots, boundaries);\n \n \t\t// check and resolve conflicts about input priorities\n \t\tAbstractExecNodeExactlyOnceVisitor inputPriorityVisitor = new AbstractExecNodeExactlyOnceVisitor() {\n \t\t\t@Override\n \t\t\tprotected void visitNode(ExecNode<?, ?> node) {\n-\t\t\t\tvisitInputs(node);\n+\t\t\t\tif (!boundaries.contains(node)) {\n+\t\t\t\t\tvisitInputs(node);\n+\t\t\t\t}\n \t\t\t\tcheckInputPriorities(node);\n \t\t\t}\n \t\t};\n \t\troots.forEach(n -> n.accept(inputPriorityVisitor));\n \t}\n \n+\tpublic Map<ExecNode<?, ?>, Integer> calculateInputOrder() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3c50597400946ab21e7dba24673a0b5df30eaf2b"}, "originalPosition": 52}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNDYyMjQxOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/InputPriorityConflictResolver.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNVQwMzoyNToxNVrOHn2AQA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNVQwMzoyNToxNVrOHn2AQA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTU0MTMxMg==", "bodyText": "please check the number of roots should always be 1. If we separate this part of logic into another class, we can give ExecNode<?, ?> root instead of List<ExecNode<?, ?>> roots as a part of the constructor parameters", "url": "https://github.com/apache/flink/pull/13742#discussion_r511541312", "createdAt": "2020-10-25T03:25:15Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/InputPriorityConflictResolver.java", "diffHunk": "@@ -97,31 +98,79 @@\n public class InputPriorityConflictResolver {\n \n \tprivate final List<ExecNode<?, ?>> roots;\n+\tprivate final Set<ExecNode<?, ?>> boundaries;\n+\tprivate final ExecEdge.DamBehavior safeDamBehavior;\n+\tprivate final ShuffleMode shuffleMode;\n \n \tprivate TopologyGraph graph;\n \n-\tpublic InputPriorityConflictResolver(List<ExecNode<?, ?>> roots) {\n+\tpublic InputPriorityConflictResolver(\n+\t\t\tList<ExecNode<?, ?>> roots,\n+\t\t\tSet<ExecNode<?, ?>> boundaries,\n+\t\t\tExecEdge.DamBehavior safeDamBehavior,\n+\t\t\tShuffleMode shuffleMode) {\n \t\tPreconditions.checkArgument(\n \t\t\troots.stream().allMatch(root -> root instanceof BatchExecNode),\n \t\t\t\"InputPriorityConflictResolver can only be used for batch jobs.\");\n \t\tthis.roots = roots;\n+\t\tthis.boundaries = boundaries;\n+\t\tthis.safeDamBehavior = safeDamBehavior;\n+\t\tthis.shuffleMode = shuffleMode;\n \t}\n \n \tpublic void detectAndResolve() {\n \t\t// build an initial topology graph\n-\t\tgraph = new TopologyGraph(roots);\n+\t\tgraph = new TopologyGraph(roots, boundaries);\n \n \t\t// check and resolve conflicts about input priorities\n \t\tAbstractExecNodeExactlyOnceVisitor inputPriorityVisitor = new AbstractExecNodeExactlyOnceVisitor() {\n \t\t\t@Override\n \t\t\tprotected void visitNode(ExecNode<?, ?> node) {\n-\t\t\t\tvisitInputs(node);\n+\t\t\t\tif (!boundaries.contains(node)) {\n+\t\t\t\t\tvisitInputs(node);\n+\t\t\t\t}\n \t\t\t\tcheckInputPriorities(node);\n \t\t\t}\n \t\t};\n \t\troots.forEach(n -> n.accept(inputPriorityVisitor));\n \t}\n \n+\tpublic Map<ExecNode<?, ?>, Integer> calculateInputOrder() {\n+\t\t// we first calculate the topological order of all nodes in the graph\n+\t\tdetectAndResolve();\n+\t\t// check that no exchange is contained in the multiple input node\n+\t\tAbstractExecNodeExactlyOnceVisitor inputPriorityVisitor = new AbstractExecNodeExactlyOnceVisitor() {\n+\t\t\t@Override\n+\t\t\tprotected void visitNode(ExecNode<?, ?> node) {\n+\t\t\t\tif (boundaries.contains(node)) {\n+\t\t\t\t\treturn;\n+\t\t\t\t}\n+\t\t\t\tvisitInputs(node);\n+\t\t\t\tPreconditions.checkState(\n+\t\t\t\t\t!(node instanceof BatchExecExchange),\n+\t\t\t\t\t\"There is exchange in a multiple input node. This is a bug.\");\n+\t\t\t}\n+\t\t};\n+\t\troots.forEach(n -> n.accept(inputPriorityVisitor));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3c50597400946ab21e7dba24673a0b5df30eaf2b"}, "originalPosition": 68}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNTA2MzE0OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/MultipleInputNodeCreationProcessor.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNVQxMzowNjowMlrOHn5RVA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNVQxMzowNjowMlrOHn5RVA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTU5NDgzNg==", "bodyText": "wrapper.execNode instanceof Exchange", "url": "https://github.com/apache/flink/pull/13742#discussion_r511594836", "createdAt": "2020-10-25T13:06:02Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/MultipleInputNodeCreationProcessor.java", "diffHunk": "@@ -0,0 +1,481 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.reuse;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.streaming.api.transformations.ShuffleMode;\n+import org.apache.flink.streaming.api.transformations.SourceTransformation;\n+import org.apache.flink.table.planner.plan.nodes.exec.AbstractExecNodeExactlyOnceVisitor;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecEdge;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecBoundedStreamScan;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecExchange;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecMultipleInputNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecUnion;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecDataStreamScan;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecExchange;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecMultipleInputNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecUnion;\n+import org.apache.flink.table.planner.plan.nodes.process.DAGProcessContext;\n+import org.apache.flink.table.planner.plan.nodes.process.DAGProcessor;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.calcite.rel.RelDistribution;\n+import org.apache.calcite.rel.RelNode;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.Set;\n+\n+/**\n+ * A {@link DAGProcessor} which organize {@link ExecNode}s into multiple input nodes.\n+ *\n+ * <p>For a detailed explanation of the algorithm, see appendix of the\n+ * <a href=\"https://docs.google.com/document/d/1qKVohV12qn-bM51cBZ8Hcgp31ntwClxjoiNBUOqVHsI\">design doc</a>.\n+ */\n+public class MultipleInputNodeCreationProcessor implements DAGProcessor {\n+\n+\tprivate final boolean isStreaming;\n+\n+\tpublic MultipleInputNodeCreationProcessor(boolean isStreaming) {\n+\t\tthis.isStreaming = isStreaming;\n+\t}\n+\n+\t@Override\n+\tpublic List<ExecNode<?, ?>> process(List<ExecNode<?, ?>> sinkNodes, DAGProcessContext context) {\n+\t\tif (!isStreaming) {\n+\t\t\t// As multiple input nodes use function call to deliver records between sub-operators,\n+\t\t\t// we cannot rely on network buffers to buffer records not yet ready to be read,\n+\t\t\t// so only BLOCKING dam behavior is safe here.\n+\t\t\t// If conflict is detected under this stricter constraint,\n+\t\t\t// we add a PIPELINED exchange to mark that its input and output node cannot be merged\n+\t\t\t// into the same multiple input node\n+\t\t\tInputPriorityConflictResolver resolver = new InputPriorityConflictResolver(\n+\t\t\t\tsinkNodes,\n+\t\t\t\tCollections.emptySet(),\n+\t\t\t\tExecEdge.DamBehavior.BLOCKING,\n+\t\t\t\tShuffleMode.PIPELINED);\n+\t\t\tresolver.detectAndResolve();\n+\t\t}\n+\n+\t\tList<ExecNodeWrapper> sinkWrappers = wrapExecNodes(sinkNodes);\n+\t\t// sort all nodes in topological order, sinks come first and sources come last\n+\t\tList<ExecNodeWrapper> orderedWrappers = topologicalSort(sinkWrappers);\n+\t\t// group nodes into multiple input groups\n+\t\tcreateMultipleInputGroups(orderedWrappers);\n+\t\t// apply optimizations to remove unnecessary nodes out of multiple input groups\n+\t\toptimizeMultipleInputGroups(orderedWrappers);\n+\n+\t\t// create the real multiple input nodes\n+\t\treturn createMultipleInputNodes(sinkWrappers);\n+\t}\n+\n+\t// --------------------------------------------------------------------------------\n+\t// Wrapping and Sorting\n+\t// --------------------------------------------------------------------------------\n+\n+\tprivate List<ExecNodeWrapper> wrapExecNodes(List<ExecNode<?, ?>> sinkNodes) {\n+\t\tMap<ExecNode<?, ?>, ExecNodeWrapper> wrapperMap = new HashMap<>();\n+\t\tAbstractExecNodeExactlyOnceVisitor visitor = new AbstractExecNodeExactlyOnceVisitor() {\n+\t\t\t@Override\n+\t\t\tprotected void visitNode(ExecNode<?, ?> node) {\n+\t\t\t\tExecNodeWrapper wrapper = wrapperMap.computeIfAbsent(node, k -> new ExecNodeWrapper(node));\n+\t\t\t\tfor (ExecNode<?, ?> input : node.getInputNodes()) {\n+\t\t\t\t\tExecNodeWrapper inputWrapper = wrapperMap.computeIfAbsent(input, k -> new ExecNodeWrapper(input));\n+\t\t\t\t\twrapper.inputs.add(inputWrapper);\n+\t\t\t\t\tinputWrapper.outputs.add(wrapper);\n+\t\t\t\t}\n+\t\t\t\tvisitInputs(node);\n+\t\t\t}\n+\t\t};\n+\t\tsinkNodes.forEach(s -> s.accept(visitor));\n+\n+\t\tList<ExecNodeWrapper> sinkWrappers = new ArrayList<>();\n+\t\tfor (ExecNode<?, ?> sink : sinkNodes) {\n+\t\t\tExecNodeWrapper sinkWrapper = wrapperMap.get(sink);\n+\t\t\tPreconditions.checkNotNull(sinkWrapper, \"Sink node is not wrapped. This is a bug.\");\n+\t\t\tsinkWrappers.add(sinkWrapper);\n+\t\t}\n+\t\treturn sinkWrappers;\n+\t}\n+\n+\tprivate List<ExecNodeWrapper> topologicalSort(List<ExecNodeWrapper> sinkWrappers) {\n+\t\tList<ExecNodeWrapper> result = new ArrayList<>();\n+\t\tQueue<ExecNodeWrapper> queue = new LinkedList<>(sinkWrappers);\n+\t\tMap<ExecNodeWrapper, Integer> visitCountMap = new HashMap<>();\n+\n+\t\twhile (!queue.isEmpty()) {\n+\t\t\tExecNodeWrapper wrapper = queue.poll();\n+\t\t\tresult.add(wrapper);\n+\t\t\tfor (ExecNodeWrapper inputWrapper : wrapper.inputs) {\n+\t\t\t\tint visitCount = visitCountMap.compute(inputWrapper, (k, v) -> v == null ? 1 : v + 1);\n+\t\t\t\tif (visitCount == inputWrapper.outputs.size()) {\n+\t\t\t\t\tqueue.offer(inputWrapper);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\n+\t\treturn result;\n+\t}\n+\n+\t// --------------------------------------------------------------------------------\n+\t// Multiple Input Groups Creating\n+\t// --------------------------------------------------------------------------------\n+\n+\tprivate void createMultipleInputGroups(List<ExecNodeWrapper> orderedWrappers) {\n+\t\t// wrappers are checked in topological order from sinks to sources\n+\t\tfor (ExecNodeWrapper wrapper : orderedWrappers) {\n+\t\t\t// we skip nodes which cannot be a member of a multiple input node\n+\t\t\tif (!canBeMultipleInputNodeMember(wrapper)) {\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\t// we first try to assign this wrapper into the same group with its outputs\n+\t\t\tMultipleInputGroup outputGroup = canBeInSameGroupWithOutputs(wrapper);\n+\t\t\tif (outputGroup != null) {\n+\t\t\t\twrapper.addToGroup(outputGroup);\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\t// we then try to create a new multiple input group with this node as the root\n+\t\t\tif (canBeRootOfMultipleInputGroup(wrapper)) {\n+\t\t\t\twrapper.createGroup();\n+\t\t\t}\n+\n+\t\t\t// all our attempts failed, this node will not be in a multiple input node\n+\t\t}\n+\t}\n+\n+\tprivate boolean canBeMultipleInputNodeMember(ExecNodeWrapper wrapper) {\n+\t\tif (wrapper.inputs.isEmpty()) {\n+\t\t\t// sources cannot be a member of multiple input node\n+\t\t\treturn false;\n+\t\t}\n+\t\tif (wrapper.execNode instanceof BatchExecExchange) {\n+\t\t\t// exchange cannot be a member of multiple input node\n+\t\t\treturn false;\n+\t\t}\n+\t\tif (wrapper.execNode instanceof StreamExecExchange) {\n+\t\t\t// exchange cannot be a member of multiple input node\n+\t\t\treturn false;\n+\t\t}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3c50597400946ab21e7dba24673a0b5df30eaf2b"}, "originalPosition": 183}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNTA2NjQwOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/MultipleInputNodeCreationProcessor.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNVQxMzowOTo0MlrOHn5TAw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQwMzowOTowNVrOHn_orw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTU5NTI2Nw==", "bodyText": "rename to replaceGroup ?", "url": "https://github.com/apache/flink/pull/13742#discussion_r511595267", "createdAt": "2020-10-25T13:09:42Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/MultipleInputNodeCreationProcessor.java", "diffHunk": "@@ -0,0 +1,481 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.reuse;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.streaming.api.transformations.ShuffleMode;\n+import org.apache.flink.streaming.api.transformations.SourceTransformation;\n+import org.apache.flink.table.planner.plan.nodes.exec.AbstractExecNodeExactlyOnceVisitor;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecEdge;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecBoundedStreamScan;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecExchange;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecMultipleInputNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecUnion;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecDataStreamScan;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecExchange;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecMultipleInputNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecUnion;\n+import org.apache.flink.table.planner.plan.nodes.process.DAGProcessContext;\n+import org.apache.flink.table.planner.plan.nodes.process.DAGProcessor;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.calcite.rel.RelDistribution;\n+import org.apache.calcite.rel.RelNode;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.Set;\n+\n+/**\n+ * A {@link DAGProcessor} which organize {@link ExecNode}s into multiple input nodes.\n+ *\n+ * <p>For a detailed explanation of the algorithm, see appendix of the\n+ * <a href=\"https://docs.google.com/document/d/1qKVohV12qn-bM51cBZ8Hcgp31ntwClxjoiNBUOqVHsI\">design doc</a>.\n+ */\n+public class MultipleInputNodeCreationProcessor implements DAGProcessor {\n+\n+\tprivate final boolean isStreaming;\n+\n+\tpublic MultipleInputNodeCreationProcessor(boolean isStreaming) {\n+\t\tthis.isStreaming = isStreaming;\n+\t}\n+\n+\t@Override\n+\tpublic List<ExecNode<?, ?>> process(List<ExecNode<?, ?>> sinkNodes, DAGProcessContext context) {\n+\t\tif (!isStreaming) {\n+\t\t\t// As multiple input nodes use function call to deliver records between sub-operators,\n+\t\t\t// we cannot rely on network buffers to buffer records not yet ready to be read,\n+\t\t\t// so only BLOCKING dam behavior is safe here.\n+\t\t\t// If conflict is detected under this stricter constraint,\n+\t\t\t// we add a PIPELINED exchange to mark that its input and output node cannot be merged\n+\t\t\t// into the same multiple input node\n+\t\t\tInputPriorityConflictResolver resolver = new InputPriorityConflictResolver(\n+\t\t\t\tsinkNodes,\n+\t\t\t\tCollections.emptySet(),\n+\t\t\t\tExecEdge.DamBehavior.BLOCKING,\n+\t\t\t\tShuffleMode.PIPELINED);\n+\t\t\tresolver.detectAndResolve();\n+\t\t}\n+\n+\t\tList<ExecNodeWrapper> sinkWrappers = wrapExecNodes(sinkNodes);\n+\t\t// sort all nodes in topological order, sinks come first and sources come last\n+\t\tList<ExecNodeWrapper> orderedWrappers = topologicalSort(sinkWrappers);\n+\t\t// group nodes into multiple input groups\n+\t\tcreateMultipleInputGroups(orderedWrappers);\n+\t\t// apply optimizations to remove unnecessary nodes out of multiple input groups\n+\t\toptimizeMultipleInputGroups(orderedWrappers);\n+\n+\t\t// create the real multiple input nodes\n+\t\treturn createMultipleInputNodes(sinkWrappers);\n+\t}\n+\n+\t// --------------------------------------------------------------------------------\n+\t// Wrapping and Sorting\n+\t// --------------------------------------------------------------------------------\n+\n+\tprivate List<ExecNodeWrapper> wrapExecNodes(List<ExecNode<?, ?>> sinkNodes) {\n+\t\tMap<ExecNode<?, ?>, ExecNodeWrapper> wrapperMap = new HashMap<>();\n+\t\tAbstractExecNodeExactlyOnceVisitor visitor = new AbstractExecNodeExactlyOnceVisitor() {\n+\t\t\t@Override\n+\t\t\tprotected void visitNode(ExecNode<?, ?> node) {\n+\t\t\t\tExecNodeWrapper wrapper = wrapperMap.computeIfAbsent(node, k -> new ExecNodeWrapper(node));\n+\t\t\t\tfor (ExecNode<?, ?> input : node.getInputNodes()) {\n+\t\t\t\t\tExecNodeWrapper inputWrapper = wrapperMap.computeIfAbsent(input, k -> new ExecNodeWrapper(input));\n+\t\t\t\t\twrapper.inputs.add(inputWrapper);\n+\t\t\t\t\tinputWrapper.outputs.add(wrapper);\n+\t\t\t\t}\n+\t\t\t\tvisitInputs(node);\n+\t\t\t}\n+\t\t};\n+\t\tsinkNodes.forEach(s -> s.accept(visitor));\n+\n+\t\tList<ExecNodeWrapper> sinkWrappers = new ArrayList<>();\n+\t\tfor (ExecNode<?, ?> sink : sinkNodes) {\n+\t\t\tExecNodeWrapper sinkWrapper = wrapperMap.get(sink);\n+\t\t\tPreconditions.checkNotNull(sinkWrapper, \"Sink node is not wrapped. This is a bug.\");\n+\t\t\tsinkWrappers.add(sinkWrapper);\n+\t\t}\n+\t\treturn sinkWrappers;\n+\t}\n+\n+\tprivate List<ExecNodeWrapper> topologicalSort(List<ExecNodeWrapper> sinkWrappers) {\n+\t\tList<ExecNodeWrapper> result = new ArrayList<>();\n+\t\tQueue<ExecNodeWrapper> queue = new LinkedList<>(sinkWrappers);\n+\t\tMap<ExecNodeWrapper, Integer> visitCountMap = new HashMap<>();\n+\n+\t\twhile (!queue.isEmpty()) {\n+\t\t\tExecNodeWrapper wrapper = queue.poll();\n+\t\t\tresult.add(wrapper);\n+\t\t\tfor (ExecNodeWrapper inputWrapper : wrapper.inputs) {\n+\t\t\t\tint visitCount = visitCountMap.compute(inputWrapper, (k, v) -> v == null ? 1 : v + 1);\n+\t\t\t\tif (visitCount == inputWrapper.outputs.size()) {\n+\t\t\t\t\tqueue.offer(inputWrapper);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\n+\t\treturn result;\n+\t}\n+\n+\t// --------------------------------------------------------------------------------\n+\t// Multiple Input Groups Creating\n+\t// --------------------------------------------------------------------------------\n+\n+\tprivate void createMultipleInputGroups(List<ExecNodeWrapper> orderedWrappers) {\n+\t\t// wrappers are checked in topological order from sinks to sources\n+\t\tfor (ExecNodeWrapper wrapper : orderedWrappers) {\n+\t\t\t// we skip nodes which cannot be a member of a multiple input node\n+\t\t\tif (!canBeMultipleInputNodeMember(wrapper)) {\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\t// we first try to assign this wrapper into the same group with its outputs\n+\t\t\tMultipleInputGroup outputGroup = canBeInSameGroupWithOutputs(wrapper);\n+\t\t\tif (outputGroup != null) {\n+\t\t\t\twrapper.addToGroup(outputGroup);\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\t// we then try to create a new multiple input group with this node as the root\n+\t\t\tif (canBeRootOfMultipleInputGroup(wrapper)) {\n+\t\t\t\twrapper.createGroup();\n+\t\t\t}\n+\n+\t\t\t// all our attempts failed, this node will not be in a multiple input node\n+\t\t}\n+\t}\n+\n+\tprivate boolean canBeMultipleInputNodeMember(ExecNodeWrapper wrapper) {\n+\t\tif (wrapper.inputs.isEmpty()) {\n+\t\t\t// sources cannot be a member of multiple input node\n+\t\t\treturn false;\n+\t\t}\n+\t\tif (wrapper.execNode instanceof BatchExecExchange) {\n+\t\t\t// exchange cannot be a member of multiple input node\n+\t\t\treturn false;\n+\t\t}\n+\t\tif (wrapper.execNode instanceof StreamExecExchange) {\n+\t\t\t// exchange cannot be a member of multiple input node\n+\t\t\treturn false;\n+\t\t}\n+\n+\t\treturn true;\n+\t}\n+\n+\t/**\n+\t * A node can only be assigned into the same multiple input group of its outputs\n+\t * if all outputs have a group and are the same.\n+\t *\n+\t * @return the {@link MultipleInputGroup} of the outputs if all outputs have a\n+\t *         group and are the same, null otherwise\n+\t */\n+\tprivate MultipleInputGroup canBeInSameGroupWithOutputs(ExecNodeWrapper wrapper) {\n+\t\tif (wrapper.outputs.isEmpty()) {\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\tMultipleInputGroup outputGroup = wrapper.outputs.get(0).group;\n+\t\tif (outputGroup == null) {\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\tfor (ExecNodeWrapper outputWrapper : wrapper.outputs) {\n+\t\t\tif (outputWrapper.group != outputGroup) {\n+\t\t\t\treturn null;\n+\t\t\t}\n+\t\t}\n+\n+\t\treturn outputGroup;\n+\t}\n+\n+\tprivate boolean canBeRootOfMultipleInputGroup(ExecNodeWrapper wrapper) {\n+\t\t// only a node with more than one input can be the root,\n+\t\t// as one-input operator chaining are handled by operator chains\n+\t\treturn wrapper.inputs.size() >= 2;\n+\t}\n+\n+\t// --------------------------------------------------------------------------------\n+\t// Multiple Input Groups Optimizing\n+\t// --------------------------------------------------------------------------------\n+\n+\tprivate void optimizeMultipleInputGroups(List<ExecNodeWrapper> orderedWrappers) {\n+\t\t// wrappers are checked in topological order from sources to sinks\n+\t\tfor (int i = orderedWrappers.size() - 1; i >= 0; i--) {\n+\t\t\tExecNodeWrapper wrapper = orderedWrappers.get(i);\n+\t\t\tMultipleInputGroup group = wrapper.group;\n+\t\t\tif (group == null) {\n+\t\t\t\t// we only consider nodes currently in a multiple input group\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\tboolean isUnion =\n+\t\t\t\twrapper.execNode instanceof BatchExecUnion || wrapper.execNode instanceof StreamExecUnion;\n+\n+\t\t\tif (group.members.size() == 1) {\n+\t\t\t\tPreconditions.checkState(\n+\t\t\t\t\twrapper == group.root,\n+\t\t\t\t\t\"The only member of a multiple input group is not its root. This is a bug.\");\n+\t\t\t\t// optimization 1. we clean up multiple input groups with only 1 member,\n+\t\t\t\t// unless one of its input is a FLIP-27 source (for maximizing source chaining),\n+\t\t\t\t// however unions do not apply to this optimization because they're not real operators\n+\t\t\t\tif (isUnion || wrapper.inputs.stream().noneMatch(inputWrapper -> isNewSource(inputWrapper.execNode))) {\n+\t\t\t\t\twrapper.removeFromGroup();\n+\t\t\t\t}\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\tif (!isTailOfMultipleInputGroup(wrapper)) {\n+\t\t\t\t// we're not removing a node from the middle of a multiple input group\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\tboolean shouldRemove = false;\n+\t\t\tif (isUnion) {\n+\t\t\t\t// optimization 2. we do not allow union to be the tail of a multiple input\n+\t\t\t\t// as we're paying extra function calls for this, unless one of the united\n+\t\t\t\t// input is a FLIP-27 source\n+\t\t\t\tshouldRemove = wrapper.inputs.stream().noneMatch(inputWrapper -> isNewSource(inputWrapper.execNode));\n+\t\t\t} else if (wrapper.inputs.size() == 1) {\n+\t\t\t\t// optimization 3. for one-input operators we'll remove it unless its input\n+\t\t\t\t// is an exchange or a FLIP-27 source, this is mainly to avoid the following\n+\t\t\t\t// pattern:\n+\t\t\t\t// non-chainable source -> calc --\\\n+\t\t\t\t//                                 join ->\n+\t\t\t\t// non-chainable source -> calc --/\n+\t\t\t\t// if we move two calcs into the multiple input group rooted at the join, we're\n+\t\t\t\t// directly shuffling large amount of records from the source without filtering\n+\t\t\t\t// by the calc\n+\t\t\t\tExecNode<?, ?> input = wrapper.inputs.get(0).execNode;\n+\t\t\t\tshouldRemove = !(input instanceof BatchExecExchange) &&\n+\t\t\t\t\t!(input instanceof StreamExecExchange) &&\n+\t\t\t\t\t!isNewSource(input);\n+\t\t\t}\n+\n+\t\t\t// optimization 4. for singleton operations (for example singleton global agg)\n+\t\t\t// we're not including it into the multiple input node as we have to ensure that\n+\t\t\t// the whole multiple input can only have 1 parallelism.\n+\t\t\t// continuous singleton operations connected by forwarding shuffle will be dealt\n+\t\t\t// together with optimization 3\n+\t\t\tshouldRemove |= wrapper.inputs.stream().anyMatch(inputWrapper ->\n+\t\t\t\tinputWrapper.execNode instanceof BatchExecExchange &&\n+\t\t\t\t\t((BatchExecExchange) inputWrapper.execNode)\n+\t\t\t\t\t\t.distribution.getType() == RelDistribution.Type.SINGLETON);\n+\n+\t\t\tif (shouldRemove) {\n+\t\t\t\twrapper.removeFromGroup();\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate boolean isTailOfMultipleInputGroup(ExecNodeWrapper wrapper) {\n+\t\tPreconditions.checkNotNull(\n+\t\t\twrapper.group,\n+\t\t\t\"Exec node wrapper does not have a multiple input group. This is a bug.\");\n+\t\tfor (ExecNodeWrapper inputWrapper : wrapper.inputs) {\n+\t\t\tif (inputWrapper.group == wrapper.group) {\n+\t\t\t\t// one of the input is in the same group, so this node is not the tail of the group\n+\t\t\t\treturn false;\n+\t\t\t}\n+\t\t}\n+\t\treturn true;\n+\t}\n+\n+\tprivate boolean isNewSource(ExecNode<?, ?> node) {\n+\t\tif (node instanceof BatchExecBoundedStreamScan) {\n+\t\t\tBatchExecBoundedStreamScan scan = (BatchExecBoundedStreamScan) node;\n+\t\t\treturn scan.boundedStreamTable().dataStream().getTransformation() instanceof SourceTransformation;\n+\t\t} else if (node instanceof StreamExecDataStreamScan) {\n+\t\t\tStreamExecDataStreamScan scan = (StreamExecDataStreamScan) node;\n+\t\t\treturn scan.dataStreamTable().dataStream().getTransformation() instanceof SourceTransformation;\n+\t\t}\n+\t\treturn false;\n+\t}\n+\n+\t// --------------------------------------------------------------------------------\n+\t// Multiple Input Nodes Creating\n+\t// --------------------------------------------------------------------------------\n+\n+\tprivate List<ExecNode<?, ?>> createMultipleInputNodes(List<ExecNodeWrapper> sinkWrappers) {\n+\t\tList<ExecNode<?, ?>> result = new ArrayList<>();\n+\t\tMap<ExecNodeWrapper, ExecNode<?, ?>> visitMap = new HashMap<>();\n+\t\tfor (ExecNodeWrapper sinkWrapper : sinkWrappers) {\n+\t\t\tresult.add(getMultipleInputNode(sinkWrapper, visitMap));\n+\t\t}\n+\t\treturn result;\n+\t}\n+\n+\tprivate ExecNode<?, ?> getMultipleInputNode(\n+\t\t\tExecNodeWrapper wrapper,\n+\t\t\tMap<ExecNodeWrapper, ExecNode<?, ?>> visitMap) {\n+\t\tif (visitMap.containsKey(wrapper)) {\n+\t\t\treturn visitMap.get(wrapper);\n+\t\t}\n+\n+\t\tfor (int i = 0; i < wrapper.inputs.size(); i++) {\n+\t\t\twrapper.execNode.replaceInputNode(i, (ExecNode) getMultipleInputNode(wrapper.inputs.get(i), visitMap));\n+\t\t}\n+\n+\t\tExecNode<?, ?> ret;\n+\t\tif (wrapper.group != null && wrapper == wrapper.group.root) {\n+\t\t\tret = createMultipleInputNode(wrapper.group, visitMap);\n+\t\t} else {\n+\t\t\tret = wrapper.execNode;\n+\t\t}\n+\t\tvisitMap.put(wrapper, ret);\n+\t\treturn ret;\n+\t}\n+\n+\tprivate ExecNode<?, ?> createMultipleInputNode(\n+\t\t\tMultipleInputGroup group,\n+\t\t\tMap<ExecNodeWrapper, ExecNode<?, ?>> visitMap) {\n+\t\t// calculate the inputs of the multiple input node\n+\t\tList<Tuple2<ExecNode<?, ?>, ExecEdge>> inputs = new ArrayList<>();\n+\t\tfor (ExecNodeWrapper member : group.members) {\n+\t\t\tfor (int i = 0; i < member.inputs.size(); i++) {\n+\t\t\t\tExecNodeWrapper memberInput = member.inputs.get(i);\n+\t\t\t\tif (group.members.contains(memberInput)) {\n+\t\t\t\t\tcontinue;\n+\t\t\t\t}\n+\t\t\t\tPreconditions.checkState(\n+\t\t\t\t\tvisitMap.containsKey(memberInput),\n+\t\t\t\t\t\"Input of a multiple input member is not visited. This is a bug.\");\n+\n+\t\t\t\tExecNode<?, ?> inputNode = visitMap.get(memberInput);\n+\t\t\t\tExecEdge inputEdge = member.execNode.getInputEdges().get(i);\n+\t\t\t\tinputs.add(Tuple2.of(inputNode, inputEdge));\n+\t\t\t}\n+\t\t}\n+\n+\t\tif (isStreaming) {\n+\t\t\treturn createStreamMultipleInputNode(group, inputs);\n+\t\t} else {\n+\t\t\treturn createBatchMultipleInputNode(group, inputs);\n+\t\t}\n+\t}\n+\n+\tprivate StreamExecMultipleInputNode createStreamMultipleInputNode(\n+\t\t\tMultipleInputGroup group,\n+\t\t\tList<Tuple2<ExecNode<?, ?>, ExecEdge>> inputs) {\n+\t\tRelNode outputRel = (RelNode) group.root.execNode;\n+\t\tRelNode[] inputRels = new RelNode[inputs.size()];\n+\t\tfor (int i = 0; i < inputs.size(); i++) {\n+\t\t\tinputRels[i] = (RelNode) inputs.get(i).f0;\n+\t\t}\n+\n+\t\treturn new StreamExecMultipleInputNode(\n+\t\t\toutputRel.getCluster(),\n+\t\t\toutputRel.getTraitSet(),\n+\t\t\tinputRels,\n+\t\t\toutputRel);\n+\t}\n+\n+\tprivate BatchExecMultipleInputNode createBatchMultipleInputNode(\n+\t\t\tMultipleInputGroup group,\n+\t\t\tList<Tuple2<ExecNode<?, ?>, ExecEdge>> inputs) {\n+\t\t// first calculate the input orders using InputPriorityConflictResolver\n+\t\tSet<ExecNode<?, ?>> inputSet = new HashSet<>();\n+\t\tfor (Tuple2<ExecNode<?, ?>, ExecEdge> t : inputs) {\n+\t\t\tinputSet.add(t.f0);\n+\t\t}\n+\t\tInputPriorityConflictResolver resolver = new InputPriorityConflictResolver(\n+\t\t\tCollections.singletonList(group.root.execNode),\n+\t\t\tinputSet,\n+\t\t\tExecEdge.DamBehavior.BLOCKING,\n+\t\t\tShuffleMode.PIPELINED);\n+\t\tMap<ExecNode<?, ?>, Integer> inputOrderMap = resolver.calculateInputOrder();\n+\n+\t\t// then create input rels and edges with the input orders\n+\t\tRelNode outputRel = (RelNode) group.root.execNode;\n+\t\tRelNode[] inputRels = new RelNode[inputs.size()];\n+\t\tExecEdge[] inputEdges = new ExecEdge[inputs.size()];\n+\t\tfor (int i = 0; i < inputs.size(); i++) {\n+\t\t\tExecNode<?, ?> inputNode = inputs.get(i).f0;\n+\t\t\tExecEdge originalInputEdge = inputs.get(i).f1;\n+\t\t\tinputRels[i] = (RelNode) inputNode;\n+\t\t\tinputEdges[i] = ExecEdge.builder()\n+\t\t\t\t.requiredShuffle(originalInputEdge.getRequiredShuffle())\n+\t\t\t\t.damBehavior(originalInputEdge.getDamBehavior())\n+\t\t\t\t.priority(inputOrderMap.get(inputNode))\n+\t\t\t\t.build();\n+\t\t}\n+\n+\t\treturn new BatchExecMultipleInputNode(\n+\t\t\toutputRel.getCluster(),\n+\t\t\toutputRel.getTraitSet(),\n+\t\t\tinputRels,\n+\t\t\toutputRel,\n+\t\t\tinputEdges);\n+\t}\n+\n+\t// --------------------------------------------------------------------------------\n+\t// Helper Classes\n+\t// --------------------------------------------------------------------------------\n+\n+\tprivate static class ExecNodeWrapper {\n+\t\tprivate final ExecNode<?, ?> execNode;\n+\t\tprivate final List<ExecNodeWrapper> inputs;\n+\t\tprivate final List<ExecNodeWrapper> outputs;\n+\t\tprivate MultipleInputGroup group;\n+\n+\t\tprivate ExecNodeWrapper(ExecNode<?, ?> execNode) {\n+\t\t\tthis.execNode = execNode;\n+\t\t\tthis.inputs = new ArrayList<>();\n+\t\t\tthis.outputs = new ArrayList<>();\n+\t\t\tthis.group = null;\n+\t\t}\n+\n+\t\tprivate void createGroup() {\n+\t\t\tthis.group = new MultipleInputGroup(this);\n+\t\t}\n+\n+\t\tprivate void addToGroup(MultipleInputGroup group) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3c50597400946ab21e7dba24673a0b5df30eaf2b"}, "originalPosition": 454}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTY5OTExOQ==", "bodyText": "We expect this wrapper not to be in any group.", "url": "https://github.com/apache/flink/pull/13742#discussion_r511699119", "createdAt": "2020-10-26T03:09:05Z", "author": {"login": "tsreaper"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/MultipleInputNodeCreationProcessor.java", "diffHunk": "@@ -0,0 +1,481 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.reuse;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.streaming.api.transformations.ShuffleMode;\n+import org.apache.flink.streaming.api.transformations.SourceTransformation;\n+import org.apache.flink.table.planner.plan.nodes.exec.AbstractExecNodeExactlyOnceVisitor;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecEdge;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecBoundedStreamScan;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecExchange;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecMultipleInputNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecUnion;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecDataStreamScan;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecExchange;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecMultipleInputNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecUnion;\n+import org.apache.flink.table.planner.plan.nodes.process.DAGProcessContext;\n+import org.apache.flink.table.planner.plan.nodes.process.DAGProcessor;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.calcite.rel.RelDistribution;\n+import org.apache.calcite.rel.RelNode;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.Set;\n+\n+/**\n+ * A {@link DAGProcessor} which organize {@link ExecNode}s into multiple input nodes.\n+ *\n+ * <p>For a detailed explanation of the algorithm, see appendix of the\n+ * <a href=\"https://docs.google.com/document/d/1qKVohV12qn-bM51cBZ8Hcgp31ntwClxjoiNBUOqVHsI\">design doc</a>.\n+ */\n+public class MultipleInputNodeCreationProcessor implements DAGProcessor {\n+\n+\tprivate final boolean isStreaming;\n+\n+\tpublic MultipleInputNodeCreationProcessor(boolean isStreaming) {\n+\t\tthis.isStreaming = isStreaming;\n+\t}\n+\n+\t@Override\n+\tpublic List<ExecNode<?, ?>> process(List<ExecNode<?, ?>> sinkNodes, DAGProcessContext context) {\n+\t\tif (!isStreaming) {\n+\t\t\t// As multiple input nodes use function call to deliver records between sub-operators,\n+\t\t\t// we cannot rely on network buffers to buffer records not yet ready to be read,\n+\t\t\t// so only BLOCKING dam behavior is safe here.\n+\t\t\t// If conflict is detected under this stricter constraint,\n+\t\t\t// we add a PIPELINED exchange to mark that its input and output node cannot be merged\n+\t\t\t// into the same multiple input node\n+\t\t\tInputPriorityConflictResolver resolver = new InputPriorityConflictResolver(\n+\t\t\t\tsinkNodes,\n+\t\t\t\tCollections.emptySet(),\n+\t\t\t\tExecEdge.DamBehavior.BLOCKING,\n+\t\t\t\tShuffleMode.PIPELINED);\n+\t\t\tresolver.detectAndResolve();\n+\t\t}\n+\n+\t\tList<ExecNodeWrapper> sinkWrappers = wrapExecNodes(sinkNodes);\n+\t\t// sort all nodes in topological order, sinks come first and sources come last\n+\t\tList<ExecNodeWrapper> orderedWrappers = topologicalSort(sinkWrappers);\n+\t\t// group nodes into multiple input groups\n+\t\tcreateMultipleInputGroups(orderedWrappers);\n+\t\t// apply optimizations to remove unnecessary nodes out of multiple input groups\n+\t\toptimizeMultipleInputGroups(orderedWrappers);\n+\n+\t\t// create the real multiple input nodes\n+\t\treturn createMultipleInputNodes(sinkWrappers);\n+\t}\n+\n+\t// --------------------------------------------------------------------------------\n+\t// Wrapping and Sorting\n+\t// --------------------------------------------------------------------------------\n+\n+\tprivate List<ExecNodeWrapper> wrapExecNodes(List<ExecNode<?, ?>> sinkNodes) {\n+\t\tMap<ExecNode<?, ?>, ExecNodeWrapper> wrapperMap = new HashMap<>();\n+\t\tAbstractExecNodeExactlyOnceVisitor visitor = new AbstractExecNodeExactlyOnceVisitor() {\n+\t\t\t@Override\n+\t\t\tprotected void visitNode(ExecNode<?, ?> node) {\n+\t\t\t\tExecNodeWrapper wrapper = wrapperMap.computeIfAbsent(node, k -> new ExecNodeWrapper(node));\n+\t\t\t\tfor (ExecNode<?, ?> input : node.getInputNodes()) {\n+\t\t\t\t\tExecNodeWrapper inputWrapper = wrapperMap.computeIfAbsent(input, k -> new ExecNodeWrapper(input));\n+\t\t\t\t\twrapper.inputs.add(inputWrapper);\n+\t\t\t\t\tinputWrapper.outputs.add(wrapper);\n+\t\t\t\t}\n+\t\t\t\tvisitInputs(node);\n+\t\t\t}\n+\t\t};\n+\t\tsinkNodes.forEach(s -> s.accept(visitor));\n+\n+\t\tList<ExecNodeWrapper> sinkWrappers = new ArrayList<>();\n+\t\tfor (ExecNode<?, ?> sink : sinkNodes) {\n+\t\t\tExecNodeWrapper sinkWrapper = wrapperMap.get(sink);\n+\t\t\tPreconditions.checkNotNull(sinkWrapper, \"Sink node is not wrapped. This is a bug.\");\n+\t\t\tsinkWrappers.add(sinkWrapper);\n+\t\t}\n+\t\treturn sinkWrappers;\n+\t}\n+\n+\tprivate List<ExecNodeWrapper> topologicalSort(List<ExecNodeWrapper> sinkWrappers) {\n+\t\tList<ExecNodeWrapper> result = new ArrayList<>();\n+\t\tQueue<ExecNodeWrapper> queue = new LinkedList<>(sinkWrappers);\n+\t\tMap<ExecNodeWrapper, Integer> visitCountMap = new HashMap<>();\n+\n+\t\twhile (!queue.isEmpty()) {\n+\t\t\tExecNodeWrapper wrapper = queue.poll();\n+\t\t\tresult.add(wrapper);\n+\t\t\tfor (ExecNodeWrapper inputWrapper : wrapper.inputs) {\n+\t\t\t\tint visitCount = visitCountMap.compute(inputWrapper, (k, v) -> v == null ? 1 : v + 1);\n+\t\t\t\tif (visitCount == inputWrapper.outputs.size()) {\n+\t\t\t\t\tqueue.offer(inputWrapper);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\n+\t\treturn result;\n+\t}\n+\n+\t// --------------------------------------------------------------------------------\n+\t// Multiple Input Groups Creating\n+\t// --------------------------------------------------------------------------------\n+\n+\tprivate void createMultipleInputGroups(List<ExecNodeWrapper> orderedWrappers) {\n+\t\t// wrappers are checked in topological order from sinks to sources\n+\t\tfor (ExecNodeWrapper wrapper : orderedWrappers) {\n+\t\t\t// we skip nodes which cannot be a member of a multiple input node\n+\t\t\tif (!canBeMultipleInputNodeMember(wrapper)) {\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\t// we first try to assign this wrapper into the same group with its outputs\n+\t\t\tMultipleInputGroup outputGroup = canBeInSameGroupWithOutputs(wrapper);\n+\t\t\tif (outputGroup != null) {\n+\t\t\t\twrapper.addToGroup(outputGroup);\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\t// we then try to create a new multiple input group with this node as the root\n+\t\t\tif (canBeRootOfMultipleInputGroup(wrapper)) {\n+\t\t\t\twrapper.createGroup();\n+\t\t\t}\n+\n+\t\t\t// all our attempts failed, this node will not be in a multiple input node\n+\t\t}\n+\t}\n+\n+\tprivate boolean canBeMultipleInputNodeMember(ExecNodeWrapper wrapper) {\n+\t\tif (wrapper.inputs.isEmpty()) {\n+\t\t\t// sources cannot be a member of multiple input node\n+\t\t\treturn false;\n+\t\t}\n+\t\tif (wrapper.execNode instanceof BatchExecExchange) {\n+\t\t\t// exchange cannot be a member of multiple input node\n+\t\t\treturn false;\n+\t\t}\n+\t\tif (wrapper.execNode instanceof StreamExecExchange) {\n+\t\t\t// exchange cannot be a member of multiple input node\n+\t\t\treturn false;\n+\t\t}\n+\n+\t\treturn true;\n+\t}\n+\n+\t/**\n+\t * A node can only be assigned into the same multiple input group of its outputs\n+\t * if all outputs have a group and are the same.\n+\t *\n+\t * @return the {@link MultipleInputGroup} of the outputs if all outputs have a\n+\t *         group and are the same, null otherwise\n+\t */\n+\tprivate MultipleInputGroup canBeInSameGroupWithOutputs(ExecNodeWrapper wrapper) {\n+\t\tif (wrapper.outputs.isEmpty()) {\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\tMultipleInputGroup outputGroup = wrapper.outputs.get(0).group;\n+\t\tif (outputGroup == null) {\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\tfor (ExecNodeWrapper outputWrapper : wrapper.outputs) {\n+\t\t\tif (outputWrapper.group != outputGroup) {\n+\t\t\t\treturn null;\n+\t\t\t}\n+\t\t}\n+\n+\t\treturn outputGroup;\n+\t}\n+\n+\tprivate boolean canBeRootOfMultipleInputGroup(ExecNodeWrapper wrapper) {\n+\t\t// only a node with more than one input can be the root,\n+\t\t// as one-input operator chaining are handled by operator chains\n+\t\treturn wrapper.inputs.size() >= 2;\n+\t}\n+\n+\t// --------------------------------------------------------------------------------\n+\t// Multiple Input Groups Optimizing\n+\t// --------------------------------------------------------------------------------\n+\n+\tprivate void optimizeMultipleInputGroups(List<ExecNodeWrapper> orderedWrappers) {\n+\t\t// wrappers are checked in topological order from sources to sinks\n+\t\tfor (int i = orderedWrappers.size() - 1; i >= 0; i--) {\n+\t\t\tExecNodeWrapper wrapper = orderedWrappers.get(i);\n+\t\t\tMultipleInputGroup group = wrapper.group;\n+\t\t\tif (group == null) {\n+\t\t\t\t// we only consider nodes currently in a multiple input group\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\tboolean isUnion =\n+\t\t\t\twrapper.execNode instanceof BatchExecUnion || wrapper.execNode instanceof StreamExecUnion;\n+\n+\t\t\tif (group.members.size() == 1) {\n+\t\t\t\tPreconditions.checkState(\n+\t\t\t\t\twrapper == group.root,\n+\t\t\t\t\t\"The only member of a multiple input group is not its root. This is a bug.\");\n+\t\t\t\t// optimization 1. we clean up multiple input groups with only 1 member,\n+\t\t\t\t// unless one of its input is a FLIP-27 source (for maximizing source chaining),\n+\t\t\t\t// however unions do not apply to this optimization because they're not real operators\n+\t\t\t\tif (isUnion || wrapper.inputs.stream().noneMatch(inputWrapper -> isNewSource(inputWrapper.execNode))) {\n+\t\t\t\t\twrapper.removeFromGroup();\n+\t\t\t\t}\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\tif (!isTailOfMultipleInputGroup(wrapper)) {\n+\t\t\t\t// we're not removing a node from the middle of a multiple input group\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\tboolean shouldRemove = false;\n+\t\t\tif (isUnion) {\n+\t\t\t\t// optimization 2. we do not allow union to be the tail of a multiple input\n+\t\t\t\t// as we're paying extra function calls for this, unless one of the united\n+\t\t\t\t// input is a FLIP-27 source\n+\t\t\t\tshouldRemove = wrapper.inputs.stream().noneMatch(inputWrapper -> isNewSource(inputWrapper.execNode));\n+\t\t\t} else if (wrapper.inputs.size() == 1) {\n+\t\t\t\t// optimization 3. for one-input operators we'll remove it unless its input\n+\t\t\t\t// is an exchange or a FLIP-27 source, this is mainly to avoid the following\n+\t\t\t\t// pattern:\n+\t\t\t\t// non-chainable source -> calc --\\\n+\t\t\t\t//                                 join ->\n+\t\t\t\t// non-chainable source -> calc --/\n+\t\t\t\t// if we move two calcs into the multiple input group rooted at the join, we're\n+\t\t\t\t// directly shuffling large amount of records from the source without filtering\n+\t\t\t\t// by the calc\n+\t\t\t\tExecNode<?, ?> input = wrapper.inputs.get(0).execNode;\n+\t\t\t\tshouldRemove = !(input instanceof BatchExecExchange) &&\n+\t\t\t\t\t!(input instanceof StreamExecExchange) &&\n+\t\t\t\t\t!isNewSource(input);\n+\t\t\t}\n+\n+\t\t\t// optimization 4. for singleton operations (for example singleton global agg)\n+\t\t\t// we're not including it into the multiple input node as we have to ensure that\n+\t\t\t// the whole multiple input can only have 1 parallelism.\n+\t\t\t// continuous singleton operations connected by forwarding shuffle will be dealt\n+\t\t\t// together with optimization 3\n+\t\t\tshouldRemove |= wrapper.inputs.stream().anyMatch(inputWrapper ->\n+\t\t\t\tinputWrapper.execNode instanceof BatchExecExchange &&\n+\t\t\t\t\t((BatchExecExchange) inputWrapper.execNode)\n+\t\t\t\t\t\t.distribution.getType() == RelDistribution.Type.SINGLETON);\n+\n+\t\t\tif (shouldRemove) {\n+\t\t\t\twrapper.removeFromGroup();\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate boolean isTailOfMultipleInputGroup(ExecNodeWrapper wrapper) {\n+\t\tPreconditions.checkNotNull(\n+\t\t\twrapper.group,\n+\t\t\t\"Exec node wrapper does not have a multiple input group. This is a bug.\");\n+\t\tfor (ExecNodeWrapper inputWrapper : wrapper.inputs) {\n+\t\t\tif (inputWrapper.group == wrapper.group) {\n+\t\t\t\t// one of the input is in the same group, so this node is not the tail of the group\n+\t\t\t\treturn false;\n+\t\t\t}\n+\t\t}\n+\t\treturn true;\n+\t}\n+\n+\tprivate boolean isNewSource(ExecNode<?, ?> node) {\n+\t\tif (node instanceof BatchExecBoundedStreamScan) {\n+\t\t\tBatchExecBoundedStreamScan scan = (BatchExecBoundedStreamScan) node;\n+\t\t\treturn scan.boundedStreamTable().dataStream().getTransformation() instanceof SourceTransformation;\n+\t\t} else if (node instanceof StreamExecDataStreamScan) {\n+\t\t\tStreamExecDataStreamScan scan = (StreamExecDataStreamScan) node;\n+\t\t\treturn scan.dataStreamTable().dataStream().getTransformation() instanceof SourceTransformation;\n+\t\t}\n+\t\treturn false;\n+\t}\n+\n+\t// --------------------------------------------------------------------------------\n+\t// Multiple Input Nodes Creating\n+\t// --------------------------------------------------------------------------------\n+\n+\tprivate List<ExecNode<?, ?>> createMultipleInputNodes(List<ExecNodeWrapper> sinkWrappers) {\n+\t\tList<ExecNode<?, ?>> result = new ArrayList<>();\n+\t\tMap<ExecNodeWrapper, ExecNode<?, ?>> visitMap = new HashMap<>();\n+\t\tfor (ExecNodeWrapper sinkWrapper : sinkWrappers) {\n+\t\t\tresult.add(getMultipleInputNode(sinkWrapper, visitMap));\n+\t\t}\n+\t\treturn result;\n+\t}\n+\n+\tprivate ExecNode<?, ?> getMultipleInputNode(\n+\t\t\tExecNodeWrapper wrapper,\n+\t\t\tMap<ExecNodeWrapper, ExecNode<?, ?>> visitMap) {\n+\t\tif (visitMap.containsKey(wrapper)) {\n+\t\t\treturn visitMap.get(wrapper);\n+\t\t}\n+\n+\t\tfor (int i = 0; i < wrapper.inputs.size(); i++) {\n+\t\t\twrapper.execNode.replaceInputNode(i, (ExecNode) getMultipleInputNode(wrapper.inputs.get(i), visitMap));\n+\t\t}\n+\n+\t\tExecNode<?, ?> ret;\n+\t\tif (wrapper.group != null && wrapper == wrapper.group.root) {\n+\t\t\tret = createMultipleInputNode(wrapper.group, visitMap);\n+\t\t} else {\n+\t\t\tret = wrapper.execNode;\n+\t\t}\n+\t\tvisitMap.put(wrapper, ret);\n+\t\treturn ret;\n+\t}\n+\n+\tprivate ExecNode<?, ?> createMultipleInputNode(\n+\t\t\tMultipleInputGroup group,\n+\t\t\tMap<ExecNodeWrapper, ExecNode<?, ?>> visitMap) {\n+\t\t// calculate the inputs of the multiple input node\n+\t\tList<Tuple2<ExecNode<?, ?>, ExecEdge>> inputs = new ArrayList<>();\n+\t\tfor (ExecNodeWrapper member : group.members) {\n+\t\t\tfor (int i = 0; i < member.inputs.size(); i++) {\n+\t\t\t\tExecNodeWrapper memberInput = member.inputs.get(i);\n+\t\t\t\tif (group.members.contains(memberInput)) {\n+\t\t\t\t\tcontinue;\n+\t\t\t\t}\n+\t\t\t\tPreconditions.checkState(\n+\t\t\t\t\tvisitMap.containsKey(memberInput),\n+\t\t\t\t\t\"Input of a multiple input member is not visited. This is a bug.\");\n+\n+\t\t\t\tExecNode<?, ?> inputNode = visitMap.get(memberInput);\n+\t\t\t\tExecEdge inputEdge = member.execNode.getInputEdges().get(i);\n+\t\t\t\tinputs.add(Tuple2.of(inputNode, inputEdge));\n+\t\t\t}\n+\t\t}\n+\n+\t\tif (isStreaming) {\n+\t\t\treturn createStreamMultipleInputNode(group, inputs);\n+\t\t} else {\n+\t\t\treturn createBatchMultipleInputNode(group, inputs);\n+\t\t}\n+\t}\n+\n+\tprivate StreamExecMultipleInputNode createStreamMultipleInputNode(\n+\t\t\tMultipleInputGroup group,\n+\t\t\tList<Tuple2<ExecNode<?, ?>, ExecEdge>> inputs) {\n+\t\tRelNode outputRel = (RelNode) group.root.execNode;\n+\t\tRelNode[] inputRels = new RelNode[inputs.size()];\n+\t\tfor (int i = 0; i < inputs.size(); i++) {\n+\t\t\tinputRels[i] = (RelNode) inputs.get(i).f0;\n+\t\t}\n+\n+\t\treturn new StreamExecMultipleInputNode(\n+\t\t\toutputRel.getCluster(),\n+\t\t\toutputRel.getTraitSet(),\n+\t\t\tinputRels,\n+\t\t\toutputRel);\n+\t}\n+\n+\tprivate BatchExecMultipleInputNode createBatchMultipleInputNode(\n+\t\t\tMultipleInputGroup group,\n+\t\t\tList<Tuple2<ExecNode<?, ?>, ExecEdge>> inputs) {\n+\t\t// first calculate the input orders using InputPriorityConflictResolver\n+\t\tSet<ExecNode<?, ?>> inputSet = new HashSet<>();\n+\t\tfor (Tuple2<ExecNode<?, ?>, ExecEdge> t : inputs) {\n+\t\t\tinputSet.add(t.f0);\n+\t\t}\n+\t\tInputPriorityConflictResolver resolver = new InputPriorityConflictResolver(\n+\t\t\tCollections.singletonList(group.root.execNode),\n+\t\t\tinputSet,\n+\t\t\tExecEdge.DamBehavior.BLOCKING,\n+\t\t\tShuffleMode.PIPELINED);\n+\t\tMap<ExecNode<?, ?>, Integer> inputOrderMap = resolver.calculateInputOrder();\n+\n+\t\t// then create input rels and edges with the input orders\n+\t\tRelNode outputRel = (RelNode) group.root.execNode;\n+\t\tRelNode[] inputRels = new RelNode[inputs.size()];\n+\t\tExecEdge[] inputEdges = new ExecEdge[inputs.size()];\n+\t\tfor (int i = 0; i < inputs.size(); i++) {\n+\t\t\tExecNode<?, ?> inputNode = inputs.get(i).f0;\n+\t\t\tExecEdge originalInputEdge = inputs.get(i).f1;\n+\t\t\tinputRels[i] = (RelNode) inputNode;\n+\t\t\tinputEdges[i] = ExecEdge.builder()\n+\t\t\t\t.requiredShuffle(originalInputEdge.getRequiredShuffle())\n+\t\t\t\t.damBehavior(originalInputEdge.getDamBehavior())\n+\t\t\t\t.priority(inputOrderMap.get(inputNode))\n+\t\t\t\t.build();\n+\t\t}\n+\n+\t\treturn new BatchExecMultipleInputNode(\n+\t\t\toutputRel.getCluster(),\n+\t\t\toutputRel.getTraitSet(),\n+\t\t\tinputRels,\n+\t\t\toutputRel,\n+\t\t\tinputEdges);\n+\t}\n+\n+\t// --------------------------------------------------------------------------------\n+\t// Helper Classes\n+\t// --------------------------------------------------------------------------------\n+\n+\tprivate static class ExecNodeWrapper {\n+\t\tprivate final ExecNode<?, ?> execNode;\n+\t\tprivate final List<ExecNodeWrapper> inputs;\n+\t\tprivate final List<ExecNodeWrapper> outputs;\n+\t\tprivate MultipleInputGroup group;\n+\n+\t\tprivate ExecNodeWrapper(ExecNode<?, ?> execNode) {\n+\t\t\tthis.execNode = execNode;\n+\t\t\tthis.inputs = new ArrayList<>();\n+\t\t\tthis.outputs = new ArrayList<>();\n+\t\t\tthis.group = null;\n+\t\t}\n+\n+\t\tprivate void createGroup() {\n+\t\t\tthis.group = new MultipleInputGroup(this);\n+\t\t}\n+\n+\t\tprivate void addToGroup(MultipleInputGroup group) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTU5NTI2Nw=="}, "originalCommit": {"oid": "3c50597400946ab21e7dba24673a0b5df30eaf2b"}, "originalPosition": 454}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNTA2OTE4OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/MultipleInputNodeCreationProcessor.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNVQxMzoxMzowM1rOHn5UeA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNVQxMzoxMzowM1rOHn5UeA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTU5NTY0MA==", "bodyText": "wrapper.execNode instanceof Union", "url": "https://github.com/apache/flink/pull/13742#discussion_r511595640", "createdAt": "2020-10-25T13:13:03Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/MultipleInputNodeCreationProcessor.java", "diffHunk": "@@ -0,0 +1,481 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.reuse;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.streaming.api.transformations.ShuffleMode;\n+import org.apache.flink.streaming.api.transformations.SourceTransformation;\n+import org.apache.flink.table.planner.plan.nodes.exec.AbstractExecNodeExactlyOnceVisitor;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecEdge;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecBoundedStreamScan;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecExchange;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecMultipleInputNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecUnion;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecDataStreamScan;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecExchange;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecMultipleInputNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecUnion;\n+import org.apache.flink.table.planner.plan.nodes.process.DAGProcessContext;\n+import org.apache.flink.table.planner.plan.nodes.process.DAGProcessor;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.calcite.rel.RelDistribution;\n+import org.apache.calcite.rel.RelNode;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.Set;\n+\n+/**\n+ * A {@link DAGProcessor} which organize {@link ExecNode}s into multiple input nodes.\n+ *\n+ * <p>For a detailed explanation of the algorithm, see appendix of the\n+ * <a href=\"https://docs.google.com/document/d/1qKVohV12qn-bM51cBZ8Hcgp31ntwClxjoiNBUOqVHsI\">design doc</a>.\n+ */\n+public class MultipleInputNodeCreationProcessor implements DAGProcessor {\n+\n+\tprivate final boolean isStreaming;\n+\n+\tpublic MultipleInputNodeCreationProcessor(boolean isStreaming) {\n+\t\tthis.isStreaming = isStreaming;\n+\t}\n+\n+\t@Override\n+\tpublic List<ExecNode<?, ?>> process(List<ExecNode<?, ?>> sinkNodes, DAGProcessContext context) {\n+\t\tif (!isStreaming) {\n+\t\t\t// As multiple input nodes use function call to deliver records between sub-operators,\n+\t\t\t// we cannot rely on network buffers to buffer records not yet ready to be read,\n+\t\t\t// so only BLOCKING dam behavior is safe here.\n+\t\t\t// If conflict is detected under this stricter constraint,\n+\t\t\t// we add a PIPELINED exchange to mark that its input and output node cannot be merged\n+\t\t\t// into the same multiple input node\n+\t\t\tInputPriorityConflictResolver resolver = new InputPriorityConflictResolver(\n+\t\t\t\tsinkNodes,\n+\t\t\t\tCollections.emptySet(),\n+\t\t\t\tExecEdge.DamBehavior.BLOCKING,\n+\t\t\t\tShuffleMode.PIPELINED);\n+\t\t\tresolver.detectAndResolve();\n+\t\t}\n+\n+\t\tList<ExecNodeWrapper> sinkWrappers = wrapExecNodes(sinkNodes);\n+\t\t// sort all nodes in topological order, sinks come first and sources come last\n+\t\tList<ExecNodeWrapper> orderedWrappers = topologicalSort(sinkWrappers);\n+\t\t// group nodes into multiple input groups\n+\t\tcreateMultipleInputGroups(orderedWrappers);\n+\t\t// apply optimizations to remove unnecessary nodes out of multiple input groups\n+\t\toptimizeMultipleInputGroups(orderedWrappers);\n+\n+\t\t// create the real multiple input nodes\n+\t\treturn createMultipleInputNodes(sinkWrappers);\n+\t}\n+\n+\t// --------------------------------------------------------------------------------\n+\t// Wrapping and Sorting\n+\t// --------------------------------------------------------------------------------\n+\n+\tprivate List<ExecNodeWrapper> wrapExecNodes(List<ExecNode<?, ?>> sinkNodes) {\n+\t\tMap<ExecNode<?, ?>, ExecNodeWrapper> wrapperMap = new HashMap<>();\n+\t\tAbstractExecNodeExactlyOnceVisitor visitor = new AbstractExecNodeExactlyOnceVisitor() {\n+\t\t\t@Override\n+\t\t\tprotected void visitNode(ExecNode<?, ?> node) {\n+\t\t\t\tExecNodeWrapper wrapper = wrapperMap.computeIfAbsent(node, k -> new ExecNodeWrapper(node));\n+\t\t\t\tfor (ExecNode<?, ?> input : node.getInputNodes()) {\n+\t\t\t\t\tExecNodeWrapper inputWrapper = wrapperMap.computeIfAbsent(input, k -> new ExecNodeWrapper(input));\n+\t\t\t\t\twrapper.inputs.add(inputWrapper);\n+\t\t\t\t\tinputWrapper.outputs.add(wrapper);\n+\t\t\t\t}\n+\t\t\t\tvisitInputs(node);\n+\t\t\t}\n+\t\t};\n+\t\tsinkNodes.forEach(s -> s.accept(visitor));\n+\n+\t\tList<ExecNodeWrapper> sinkWrappers = new ArrayList<>();\n+\t\tfor (ExecNode<?, ?> sink : sinkNodes) {\n+\t\t\tExecNodeWrapper sinkWrapper = wrapperMap.get(sink);\n+\t\t\tPreconditions.checkNotNull(sinkWrapper, \"Sink node is not wrapped. This is a bug.\");\n+\t\t\tsinkWrappers.add(sinkWrapper);\n+\t\t}\n+\t\treturn sinkWrappers;\n+\t}\n+\n+\tprivate List<ExecNodeWrapper> topologicalSort(List<ExecNodeWrapper> sinkWrappers) {\n+\t\tList<ExecNodeWrapper> result = new ArrayList<>();\n+\t\tQueue<ExecNodeWrapper> queue = new LinkedList<>(sinkWrappers);\n+\t\tMap<ExecNodeWrapper, Integer> visitCountMap = new HashMap<>();\n+\n+\t\twhile (!queue.isEmpty()) {\n+\t\t\tExecNodeWrapper wrapper = queue.poll();\n+\t\t\tresult.add(wrapper);\n+\t\t\tfor (ExecNodeWrapper inputWrapper : wrapper.inputs) {\n+\t\t\t\tint visitCount = visitCountMap.compute(inputWrapper, (k, v) -> v == null ? 1 : v + 1);\n+\t\t\t\tif (visitCount == inputWrapper.outputs.size()) {\n+\t\t\t\t\tqueue.offer(inputWrapper);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\n+\t\treturn result;\n+\t}\n+\n+\t// --------------------------------------------------------------------------------\n+\t// Multiple Input Groups Creating\n+\t// --------------------------------------------------------------------------------\n+\n+\tprivate void createMultipleInputGroups(List<ExecNodeWrapper> orderedWrappers) {\n+\t\t// wrappers are checked in topological order from sinks to sources\n+\t\tfor (ExecNodeWrapper wrapper : orderedWrappers) {\n+\t\t\t// we skip nodes which cannot be a member of a multiple input node\n+\t\t\tif (!canBeMultipleInputNodeMember(wrapper)) {\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\t// we first try to assign this wrapper into the same group with its outputs\n+\t\t\tMultipleInputGroup outputGroup = canBeInSameGroupWithOutputs(wrapper);\n+\t\t\tif (outputGroup != null) {\n+\t\t\t\twrapper.addToGroup(outputGroup);\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\t// we then try to create a new multiple input group with this node as the root\n+\t\t\tif (canBeRootOfMultipleInputGroup(wrapper)) {\n+\t\t\t\twrapper.createGroup();\n+\t\t\t}\n+\n+\t\t\t// all our attempts failed, this node will not be in a multiple input node\n+\t\t}\n+\t}\n+\n+\tprivate boolean canBeMultipleInputNodeMember(ExecNodeWrapper wrapper) {\n+\t\tif (wrapper.inputs.isEmpty()) {\n+\t\t\t// sources cannot be a member of multiple input node\n+\t\t\treturn false;\n+\t\t}\n+\t\tif (wrapper.execNode instanceof BatchExecExchange) {\n+\t\t\t// exchange cannot be a member of multiple input node\n+\t\t\treturn false;\n+\t\t}\n+\t\tif (wrapper.execNode instanceof StreamExecExchange) {\n+\t\t\t// exchange cannot be a member of multiple input node\n+\t\t\treturn false;\n+\t\t}\n+\n+\t\treturn true;\n+\t}\n+\n+\t/**\n+\t * A node can only be assigned into the same multiple input group of its outputs\n+\t * if all outputs have a group and are the same.\n+\t *\n+\t * @return the {@link MultipleInputGroup} of the outputs if all outputs have a\n+\t *         group and are the same, null otherwise\n+\t */\n+\tprivate MultipleInputGroup canBeInSameGroupWithOutputs(ExecNodeWrapper wrapper) {\n+\t\tif (wrapper.outputs.isEmpty()) {\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\tMultipleInputGroup outputGroup = wrapper.outputs.get(0).group;\n+\t\tif (outputGroup == null) {\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\tfor (ExecNodeWrapper outputWrapper : wrapper.outputs) {\n+\t\t\tif (outputWrapper.group != outputGroup) {\n+\t\t\t\treturn null;\n+\t\t\t}\n+\t\t}\n+\n+\t\treturn outputGroup;\n+\t}\n+\n+\tprivate boolean canBeRootOfMultipleInputGroup(ExecNodeWrapper wrapper) {\n+\t\t// only a node with more than one input can be the root,\n+\t\t// as one-input operator chaining are handled by operator chains\n+\t\treturn wrapper.inputs.size() >= 2;\n+\t}\n+\n+\t// --------------------------------------------------------------------------------\n+\t// Multiple Input Groups Optimizing\n+\t// --------------------------------------------------------------------------------\n+\n+\tprivate void optimizeMultipleInputGroups(List<ExecNodeWrapper> orderedWrappers) {\n+\t\t// wrappers are checked in topological order from sources to sinks\n+\t\tfor (int i = orderedWrappers.size() - 1; i >= 0; i--) {\n+\t\t\tExecNodeWrapper wrapper = orderedWrappers.get(i);\n+\t\t\tMultipleInputGroup group = wrapper.group;\n+\t\t\tif (group == null) {\n+\t\t\t\t// we only consider nodes currently in a multiple input group\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\tboolean isUnion =\n+\t\t\t\twrapper.execNode instanceof BatchExecUnion || wrapper.execNode instanceof StreamExecUnion;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3c50597400946ab21e7dba24673a0b5df30eaf2b"}, "originalPosition": 235}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNTA4MTA0OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/MultipleInputNodeCreationProcessor.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNVQxMzoyNjoyMVrOHn5aUQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQwMzowOTo0NVrOHn_pUQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTU5NzEzNw==", "bodyText": "Do these optimizations necessary, or can we delete any optimization, but the result is correct", "url": "https://github.com/apache/flink/pull/13742#discussion_r511597137", "createdAt": "2020-10-25T13:26:21Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/MultipleInputNodeCreationProcessor.java", "diffHunk": "@@ -0,0 +1,481 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.reuse;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.streaming.api.transformations.ShuffleMode;\n+import org.apache.flink.streaming.api.transformations.SourceTransformation;\n+import org.apache.flink.table.planner.plan.nodes.exec.AbstractExecNodeExactlyOnceVisitor;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecEdge;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecBoundedStreamScan;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecExchange;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecMultipleInputNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecUnion;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecDataStreamScan;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecExchange;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecMultipleInputNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecUnion;\n+import org.apache.flink.table.planner.plan.nodes.process.DAGProcessContext;\n+import org.apache.flink.table.planner.plan.nodes.process.DAGProcessor;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.calcite.rel.RelDistribution;\n+import org.apache.calcite.rel.RelNode;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.Set;\n+\n+/**\n+ * A {@link DAGProcessor} which organize {@link ExecNode}s into multiple input nodes.\n+ *\n+ * <p>For a detailed explanation of the algorithm, see appendix of the\n+ * <a href=\"https://docs.google.com/document/d/1qKVohV12qn-bM51cBZ8Hcgp31ntwClxjoiNBUOqVHsI\">design doc</a>.\n+ */\n+public class MultipleInputNodeCreationProcessor implements DAGProcessor {\n+\n+\tprivate final boolean isStreaming;\n+\n+\tpublic MultipleInputNodeCreationProcessor(boolean isStreaming) {\n+\t\tthis.isStreaming = isStreaming;\n+\t}\n+\n+\t@Override\n+\tpublic List<ExecNode<?, ?>> process(List<ExecNode<?, ?>> sinkNodes, DAGProcessContext context) {\n+\t\tif (!isStreaming) {\n+\t\t\t// As multiple input nodes use function call to deliver records between sub-operators,\n+\t\t\t// we cannot rely on network buffers to buffer records not yet ready to be read,\n+\t\t\t// so only BLOCKING dam behavior is safe here.\n+\t\t\t// If conflict is detected under this stricter constraint,\n+\t\t\t// we add a PIPELINED exchange to mark that its input and output node cannot be merged\n+\t\t\t// into the same multiple input node\n+\t\t\tInputPriorityConflictResolver resolver = new InputPriorityConflictResolver(\n+\t\t\t\tsinkNodes,\n+\t\t\t\tCollections.emptySet(),\n+\t\t\t\tExecEdge.DamBehavior.BLOCKING,\n+\t\t\t\tShuffleMode.PIPELINED);\n+\t\t\tresolver.detectAndResolve();\n+\t\t}\n+\n+\t\tList<ExecNodeWrapper> sinkWrappers = wrapExecNodes(sinkNodes);\n+\t\t// sort all nodes in topological order, sinks come first and sources come last\n+\t\tList<ExecNodeWrapper> orderedWrappers = topologicalSort(sinkWrappers);\n+\t\t// group nodes into multiple input groups\n+\t\tcreateMultipleInputGroups(orderedWrappers);\n+\t\t// apply optimizations to remove unnecessary nodes out of multiple input groups", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3c50597400946ab21e7dba24673a0b5df30eaf2b"}, "originalPosition": 88}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTY5OTI4MQ==", "bodyText": "They're best-to-have, not a must.", "url": "https://github.com/apache/flink/pull/13742#discussion_r511699281", "createdAt": "2020-10-26T03:09:45Z", "author": {"login": "tsreaper"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/MultipleInputNodeCreationProcessor.java", "diffHunk": "@@ -0,0 +1,481 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.reuse;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.streaming.api.transformations.ShuffleMode;\n+import org.apache.flink.streaming.api.transformations.SourceTransformation;\n+import org.apache.flink.table.planner.plan.nodes.exec.AbstractExecNodeExactlyOnceVisitor;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecEdge;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecBoundedStreamScan;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecExchange;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecMultipleInputNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecUnion;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecDataStreamScan;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecExchange;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecMultipleInputNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecUnion;\n+import org.apache.flink.table.planner.plan.nodes.process.DAGProcessContext;\n+import org.apache.flink.table.planner.plan.nodes.process.DAGProcessor;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.calcite.rel.RelDistribution;\n+import org.apache.calcite.rel.RelNode;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.Set;\n+\n+/**\n+ * A {@link DAGProcessor} which organize {@link ExecNode}s into multiple input nodes.\n+ *\n+ * <p>For a detailed explanation of the algorithm, see appendix of the\n+ * <a href=\"https://docs.google.com/document/d/1qKVohV12qn-bM51cBZ8Hcgp31ntwClxjoiNBUOqVHsI\">design doc</a>.\n+ */\n+public class MultipleInputNodeCreationProcessor implements DAGProcessor {\n+\n+\tprivate final boolean isStreaming;\n+\n+\tpublic MultipleInputNodeCreationProcessor(boolean isStreaming) {\n+\t\tthis.isStreaming = isStreaming;\n+\t}\n+\n+\t@Override\n+\tpublic List<ExecNode<?, ?>> process(List<ExecNode<?, ?>> sinkNodes, DAGProcessContext context) {\n+\t\tif (!isStreaming) {\n+\t\t\t// As multiple input nodes use function call to deliver records between sub-operators,\n+\t\t\t// we cannot rely on network buffers to buffer records not yet ready to be read,\n+\t\t\t// so only BLOCKING dam behavior is safe here.\n+\t\t\t// If conflict is detected under this stricter constraint,\n+\t\t\t// we add a PIPELINED exchange to mark that its input and output node cannot be merged\n+\t\t\t// into the same multiple input node\n+\t\t\tInputPriorityConflictResolver resolver = new InputPriorityConflictResolver(\n+\t\t\t\tsinkNodes,\n+\t\t\t\tCollections.emptySet(),\n+\t\t\t\tExecEdge.DamBehavior.BLOCKING,\n+\t\t\t\tShuffleMode.PIPELINED);\n+\t\t\tresolver.detectAndResolve();\n+\t\t}\n+\n+\t\tList<ExecNodeWrapper> sinkWrappers = wrapExecNodes(sinkNodes);\n+\t\t// sort all nodes in topological order, sinks come first and sources come last\n+\t\tList<ExecNodeWrapper> orderedWrappers = topologicalSort(sinkWrappers);\n+\t\t// group nodes into multiple input groups\n+\t\tcreateMultipleInputGroups(orderedWrappers);\n+\t\t// apply optimizations to remove unnecessary nodes out of multiple input groups", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTU5NzEzNw=="}, "originalCommit": {"oid": "3c50597400946ab21e7dba24673a0b5df30eaf2b"}, "originalPosition": 88}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNTA4MTY3OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/MultipleInputNodeCreationProcessor.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNVQxMzoyNzowN1rOHn5ang==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNVQxMzoyNzowN1rOHn5ang==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTU5NzIxNA==", "bodyText": "sink -> root", "url": "https://github.com/apache/flink/pull/13742#discussion_r511597214", "createdAt": "2020-10-25T13:27:07Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/MultipleInputNodeCreationProcessor.java", "diffHunk": "@@ -0,0 +1,481 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.reuse;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.streaming.api.transformations.ShuffleMode;\n+import org.apache.flink.streaming.api.transformations.SourceTransformation;\n+import org.apache.flink.table.planner.plan.nodes.exec.AbstractExecNodeExactlyOnceVisitor;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecEdge;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecBoundedStreamScan;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecExchange;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecMultipleInputNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecUnion;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecDataStreamScan;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecExchange;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecMultipleInputNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecUnion;\n+import org.apache.flink.table.planner.plan.nodes.process.DAGProcessContext;\n+import org.apache.flink.table.planner.plan.nodes.process.DAGProcessor;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.calcite.rel.RelDistribution;\n+import org.apache.calcite.rel.RelNode;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.Set;\n+\n+/**\n+ * A {@link DAGProcessor} which organize {@link ExecNode}s into multiple input nodes.\n+ *\n+ * <p>For a detailed explanation of the algorithm, see appendix of the\n+ * <a href=\"https://docs.google.com/document/d/1qKVohV12qn-bM51cBZ8Hcgp31ntwClxjoiNBUOqVHsI\">design doc</a>.\n+ */\n+public class MultipleInputNodeCreationProcessor implements DAGProcessor {\n+\n+\tprivate final boolean isStreaming;\n+\n+\tpublic MultipleInputNodeCreationProcessor(boolean isStreaming) {\n+\t\tthis.isStreaming = isStreaming;\n+\t}\n+\n+\t@Override\n+\tpublic List<ExecNode<?, ?>> process(List<ExecNode<?, ?>> sinkNodes, DAGProcessContext context) {\n+\t\tif (!isStreaming) {\n+\t\t\t// As multiple input nodes use function call to deliver records between sub-operators,\n+\t\t\t// we cannot rely on network buffers to buffer records not yet ready to be read,\n+\t\t\t// so only BLOCKING dam behavior is safe here.\n+\t\t\t// If conflict is detected under this stricter constraint,\n+\t\t\t// we add a PIPELINED exchange to mark that its input and output node cannot be merged\n+\t\t\t// into the same multiple input node\n+\t\t\tInputPriorityConflictResolver resolver = new InputPriorityConflictResolver(\n+\t\t\t\tsinkNodes,\n+\t\t\t\tCollections.emptySet(),\n+\t\t\t\tExecEdge.DamBehavior.BLOCKING,\n+\t\t\t\tShuffleMode.PIPELINED);\n+\t\t\tresolver.detectAndResolve();\n+\t\t}\n+\n+\t\tList<ExecNodeWrapper> sinkWrappers = wrapExecNodes(sinkNodes);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3c50597400946ab21e7dba24673a0b5df30eaf2b"}, "originalPosition": 83}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNTA4MjY0OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/MultipleInputNodeCreationProcessor.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNVQxMzoyODoxNlrOHn5bEQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNVQxMzoyODoxNlrOHn5bEQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTU5NzMyOQ==", "bodyText": "visitMap -> visitedMap", "url": "https://github.com/apache/flink/pull/13742#discussion_r511597329", "createdAt": "2020-10-25T13:28:16Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/MultipleInputNodeCreationProcessor.java", "diffHunk": "@@ -0,0 +1,481 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.reuse;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.streaming.api.transformations.ShuffleMode;\n+import org.apache.flink.streaming.api.transformations.SourceTransformation;\n+import org.apache.flink.table.planner.plan.nodes.exec.AbstractExecNodeExactlyOnceVisitor;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecEdge;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecBoundedStreamScan;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecExchange;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecMultipleInputNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecUnion;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecDataStreamScan;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecExchange;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecMultipleInputNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecUnion;\n+import org.apache.flink.table.planner.plan.nodes.process.DAGProcessContext;\n+import org.apache.flink.table.planner.plan.nodes.process.DAGProcessor;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.calcite.rel.RelDistribution;\n+import org.apache.calcite.rel.RelNode;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.Set;\n+\n+/**\n+ * A {@link DAGProcessor} which organize {@link ExecNode}s into multiple input nodes.\n+ *\n+ * <p>For a detailed explanation of the algorithm, see appendix of the\n+ * <a href=\"https://docs.google.com/document/d/1qKVohV12qn-bM51cBZ8Hcgp31ntwClxjoiNBUOqVHsI\">design doc</a>.\n+ */\n+public class MultipleInputNodeCreationProcessor implements DAGProcessor {\n+\n+\tprivate final boolean isStreaming;\n+\n+\tpublic MultipleInputNodeCreationProcessor(boolean isStreaming) {\n+\t\tthis.isStreaming = isStreaming;\n+\t}\n+\n+\t@Override\n+\tpublic List<ExecNode<?, ?>> process(List<ExecNode<?, ?>> sinkNodes, DAGProcessContext context) {\n+\t\tif (!isStreaming) {\n+\t\t\t// As multiple input nodes use function call to deliver records between sub-operators,\n+\t\t\t// we cannot rely on network buffers to buffer records not yet ready to be read,\n+\t\t\t// so only BLOCKING dam behavior is safe here.\n+\t\t\t// If conflict is detected under this stricter constraint,\n+\t\t\t// we add a PIPELINED exchange to mark that its input and output node cannot be merged\n+\t\t\t// into the same multiple input node\n+\t\t\tInputPriorityConflictResolver resolver = new InputPriorityConflictResolver(\n+\t\t\t\tsinkNodes,\n+\t\t\t\tCollections.emptySet(),\n+\t\t\t\tExecEdge.DamBehavior.BLOCKING,\n+\t\t\t\tShuffleMode.PIPELINED);\n+\t\t\tresolver.detectAndResolve();\n+\t\t}\n+\n+\t\tList<ExecNodeWrapper> sinkWrappers = wrapExecNodes(sinkNodes);\n+\t\t// sort all nodes in topological order, sinks come first and sources come last\n+\t\tList<ExecNodeWrapper> orderedWrappers = topologicalSort(sinkWrappers);\n+\t\t// group nodes into multiple input groups\n+\t\tcreateMultipleInputGroups(orderedWrappers);\n+\t\t// apply optimizations to remove unnecessary nodes out of multiple input groups\n+\t\toptimizeMultipleInputGroups(orderedWrappers);\n+\n+\t\t// create the real multiple input nodes\n+\t\treturn createMultipleInputNodes(sinkWrappers);\n+\t}\n+\n+\t// --------------------------------------------------------------------------------\n+\t// Wrapping and Sorting\n+\t// --------------------------------------------------------------------------------\n+\n+\tprivate List<ExecNodeWrapper> wrapExecNodes(List<ExecNode<?, ?>> sinkNodes) {\n+\t\tMap<ExecNode<?, ?>, ExecNodeWrapper> wrapperMap = new HashMap<>();\n+\t\tAbstractExecNodeExactlyOnceVisitor visitor = new AbstractExecNodeExactlyOnceVisitor() {\n+\t\t\t@Override\n+\t\t\tprotected void visitNode(ExecNode<?, ?> node) {\n+\t\t\t\tExecNodeWrapper wrapper = wrapperMap.computeIfAbsent(node, k -> new ExecNodeWrapper(node));\n+\t\t\t\tfor (ExecNode<?, ?> input : node.getInputNodes()) {\n+\t\t\t\t\tExecNodeWrapper inputWrapper = wrapperMap.computeIfAbsent(input, k -> new ExecNodeWrapper(input));\n+\t\t\t\t\twrapper.inputs.add(inputWrapper);\n+\t\t\t\t\tinputWrapper.outputs.add(wrapper);\n+\t\t\t\t}\n+\t\t\t\tvisitInputs(node);\n+\t\t\t}\n+\t\t};\n+\t\tsinkNodes.forEach(s -> s.accept(visitor));\n+\n+\t\tList<ExecNodeWrapper> sinkWrappers = new ArrayList<>();\n+\t\tfor (ExecNode<?, ?> sink : sinkNodes) {\n+\t\t\tExecNodeWrapper sinkWrapper = wrapperMap.get(sink);\n+\t\t\tPreconditions.checkNotNull(sinkWrapper, \"Sink node is not wrapped. This is a bug.\");\n+\t\t\tsinkWrappers.add(sinkWrapper);\n+\t\t}\n+\t\treturn sinkWrappers;\n+\t}\n+\n+\tprivate List<ExecNodeWrapper> topologicalSort(List<ExecNodeWrapper> sinkWrappers) {\n+\t\tList<ExecNodeWrapper> result = new ArrayList<>();\n+\t\tQueue<ExecNodeWrapper> queue = new LinkedList<>(sinkWrappers);\n+\t\tMap<ExecNodeWrapper, Integer> visitCountMap = new HashMap<>();\n+\n+\t\twhile (!queue.isEmpty()) {\n+\t\t\tExecNodeWrapper wrapper = queue.poll();\n+\t\t\tresult.add(wrapper);\n+\t\t\tfor (ExecNodeWrapper inputWrapper : wrapper.inputs) {\n+\t\t\t\tint visitCount = visitCountMap.compute(inputWrapper, (k, v) -> v == null ? 1 : v + 1);\n+\t\t\t\tif (visitCount == inputWrapper.outputs.size()) {\n+\t\t\t\t\tqueue.offer(inputWrapper);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\n+\t\treturn result;\n+\t}\n+\n+\t// --------------------------------------------------------------------------------\n+\t// Multiple Input Groups Creating\n+\t// --------------------------------------------------------------------------------\n+\n+\tprivate void createMultipleInputGroups(List<ExecNodeWrapper> orderedWrappers) {\n+\t\t// wrappers are checked in topological order from sinks to sources\n+\t\tfor (ExecNodeWrapper wrapper : orderedWrappers) {\n+\t\t\t// we skip nodes which cannot be a member of a multiple input node\n+\t\t\tif (!canBeMultipleInputNodeMember(wrapper)) {\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\t// we first try to assign this wrapper into the same group with its outputs\n+\t\t\tMultipleInputGroup outputGroup = canBeInSameGroupWithOutputs(wrapper);\n+\t\t\tif (outputGroup != null) {\n+\t\t\t\twrapper.addToGroup(outputGroup);\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\t// we then try to create a new multiple input group with this node as the root\n+\t\t\tif (canBeRootOfMultipleInputGroup(wrapper)) {\n+\t\t\t\twrapper.createGroup();\n+\t\t\t}\n+\n+\t\t\t// all our attempts failed, this node will not be in a multiple input node\n+\t\t}\n+\t}\n+\n+\tprivate boolean canBeMultipleInputNodeMember(ExecNodeWrapper wrapper) {\n+\t\tif (wrapper.inputs.isEmpty()) {\n+\t\t\t// sources cannot be a member of multiple input node\n+\t\t\treturn false;\n+\t\t}\n+\t\tif (wrapper.execNode instanceof BatchExecExchange) {\n+\t\t\t// exchange cannot be a member of multiple input node\n+\t\t\treturn false;\n+\t\t}\n+\t\tif (wrapper.execNode instanceof StreamExecExchange) {\n+\t\t\t// exchange cannot be a member of multiple input node\n+\t\t\treturn false;\n+\t\t}\n+\n+\t\treturn true;\n+\t}\n+\n+\t/**\n+\t * A node can only be assigned into the same multiple input group of its outputs\n+\t * if all outputs have a group and are the same.\n+\t *\n+\t * @return the {@link MultipleInputGroup} of the outputs if all outputs have a\n+\t *         group and are the same, null otherwise\n+\t */\n+\tprivate MultipleInputGroup canBeInSameGroupWithOutputs(ExecNodeWrapper wrapper) {\n+\t\tif (wrapper.outputs.isEmpty()) {\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\tMultipleInputGroup outputGroup = wrapper.outputs.get(0).group;\n+\t\tif (outputGroup == null) {\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\tfor (ExecNodeWrapper outputWrapper : wrapper.outputs) {\n+\t\t\tif (outputWrapper.group != outputGroup) {\n+\t\t\t\treturn null;\n+\t\t\t}\n+\t\t}\n+\n+\t\treturn outputGroup;\n+\t}\n+\n+\tprivate boolean canBeRootOfMultipleInputGroup(ExecNodeWrapper wrapper) {\n+\t\t// only a node with more than one input can be the root,\n+\t\t// as one-input operator chaining are handled by operator chains\n+\t\treturn wrapper.inputs.size() >= 2;\n+\t}\n+\n+\t// --------------------------------------------------------------------------------\n+\t// Multiple Input Groups Optimizing\n+\t// --------------------------------------------------------------------------------\n+\n+\tprivate void optimizeMultipleInputGroups(List<ExecNodeWrapper> orderedWrappers) {\n+\t\t// wrappers are checked in topological order from sources to sinks\n+\t\tfor (int i = orderedWrappers.size() - 1; i >= 0; i--) {\n+\t\t\tExecNodeWrapper wrapper = orderedWrappers.get(i);\n+\t\t\tMultipleInputGroup group = wrapper.group;\n+\t\t\tif (group == null) {\n+\t\t\t\t// we only consider nodes currently in a multiple input group\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\tboolean isUnion =\n+\t\t\t\twrapper.execNode instanceof BatchExecUnion || wrapper.execNode instanceof StreamExecUnion;\n+\n+\t\t\tif (group.members.size() == 1) {\n+\t\t\t\tPreconditions.checkState(\n+\t\t\t\t\twrapper == group.root,\n+\t\t\t\t\t\"The only member of a multiple input group is not its root. This is a bug.\");\n+\t\t\t\t// optimization 1. we clean up multiple input groups with only 1 member,\n+\t\t\t\t// unless one of its input is a FLIP-27 source (for maximizing source chaining),\n+\t\t\t\t// however unions do not apply to this optimization because they're not real operators\n+\t\t\t\tif (isUnion || wrapper.inputs.stream().noneMatch(inputWrapper -> isNewSource(inputWrapper.execNode))) {\n+\t\t\t\t\twrapper.removeFromGroup();\n+\t\t\t\t}\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\tif (!isTailOfMultipleInputGroup(wrapper)) {\n+\t\t\t\t// we're not removing a node from the middle of a multiple input group\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\tboolean shouldRemove = false;\n+\t\t\tif (isUnion) {\n+\t\t\t\t// optimization 2. we do not allow union to be the tail of a multiple input\n+\t\t\t\t// as we're paying extra function calls for this, unless one of the united\n+\t\t\t\t// input is a FLIP-27 source\n+\t\t\t\tshouldRemove = wrapper.inputs.stream().noneMatch(inputWrapper -> isNewSource(inputWrapper.execNode));\n+\t\t\t} else if (wrapper.inputs.size() == 1) {\n+\t\t\t\t// optimization 3. for one-input operators we'll remove it unless its input\n+\t\t\t\t// is an exchange or a FLIP-27 source, this is mainly to avoid the following\n+\t\t\t\t// pattern:\n+\t\t\t\t// non-chainable source -> calc --\\\n+\t\t\t\t//                                 join ->\n+\t\t\t\t// non-chainable source -> calc --/\n+\t\t\t\t// if we move two calcs into the multiple input group rooted at the join, we're\n+\t\t\t\t// directly shuffling large amount of records from the source without filtering\n+\t\t\t\t// by the calc\n+\t\t\t\tExecNode<?, ?> input = wrapper.inputs.get(0).execNode;\n+\t\t\t\tshouldRemove = !(input instanceof BatchExecExchange) &&\n+\t\t\t\t\t!(input instanceof StreamExecExchange) &&\n+\t\t\t\t\t!isNewSource(input);\n+\t\t\t}\n+\n+\t\t\t// optimization 4. for singleton operations (for example singleton global agg)\n+\t\t\t// we're not including it into the multiple input node as we have to ensure that\n+\t\t\t// the whole multiple input can only have 1 parallelism.\n+\t\t\t// continuous singleton operations connected by forwarding shuffle will be dealt\n+\t\t\t// together with optimization 3\n+\t\t\tshouldRemove |= wrapper.inputs.stream().anyMatch(inputWrapper ->\n+\t\t\t\tinputWrapper.execNode instanceof BatchExecExchange &&\n+\t\t\t\t\t((BatchExecExchange) inputWrapper.execNode)\n+\t\t\t\t\t\t.distribution.getType() == RelDistribution.Type.SINGLETON);\n+\n+\t\t\tif (shouldRemove) {\n+\t\t\t\twrapper.removeFromGroup();\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate boolean isTailOfMultipleInputGroup(ExecNodeWrapper wrapper) {\n+\t\tPreconditions.checkNotNull(\n+\t\t\twrapper.group,\n+\t\t\t\"Exec node wrapper does not have a multiple input group. This is a bug.\");\n+\t\tfor (ExecNodeWrapper inputWrapper : wrapper.inputs) {\n+\t\t\tif (inputWrapper.group == wrapper.group) {\n+\t\t\t\t// one of the input is in the same group, so this node is not the tail of the group\n+\t\t\t\treturn false;\n+\t\t\t}\n+\t\t}\n+\t\treturn true;\n+\t}\n+\n+\tprivate boolean isNewSource(ExecNode<?, ?> node) {\n+\t\tif (node instanceof BatchExecBoundedStreamScan) {\n+\t\t\tBatchExecBoundedStreamScan scan = (BatchExecBoundedStreamScan) node;\n+\t\t\treturn scan.boundedStreamTable().dataStream().getTransformation() instanceof SourceTransformation;\n+\t\t} else if (node instanceof StreamExecDataStreamScan) {\n+\t\t\tStreamExecDataStreamScan scan = (StreamExecDataStreamScan) node;\n+\t\t\treturn scan.dataStreamTable().dataStream().getTransformation() instanceof SourceTransformation;\n+\t\t}\n+\t\treturn false;\n+\t}\n+\n+\t// --------------------------------------------------------------------------------\n+\t// Multiple Input Nodes Creating\n+\t// --------------------------------------------------------------------------------\n+\n+\tprivate List<ExecNode<?, ?>> createMultipleInputNodes(List<ExecNodeWrapper> sinkWrappers) {\n+\t\tList<ExecNode<?, ?>> result = new ArrayList<>();\n+\t\tMap<ExecNodeWrapper, ExecNode<?, ?>> visitMap = new HashMap<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3c50597400946ab21e7dba24673a0b5df30eaf2b"}, "originalPosition": 323}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNTEyNjcyOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/MultipleInputNodeCreationProcessor.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNVQxNDoxODo1MlrOHn5xBg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQwMzoxNDoxN1rOHn_s4A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTYwMjk1MA==", "bodyText": "SourceProvider could also provide new Source", "url": "https://github.com/apache/flink/pull/13742#discussion_r511602950", "createdAt": "2020-10-25T14:18:52Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/MultipleInputNodeCreationProcessor.java", "diffHunk": "@@ -0,0 +1,481 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.reuse;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.streaming.api.transformations.ShuffleMode;\n+import org.apache.flink.streaming.api.transformations.SourceTransformation;\n+import org.apache.flink.table.planner.plan.nodes.exec.AbstractExecNodeExactlyOnceVisitor;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecEdge;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecBoundedStreamScan;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecExchange;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecMultipleInputNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecUnion;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecDataStreamScan;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecExchange;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecMultipleInputNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecUnion;\n+import org.apache.flink.table.planner.plan.nodes.process.DAGProcessContext;\n+import org.apache.flink.table.planner.plan.nodes.process.DAGProcessor;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.calcite.rel.RelDistribution;\n+import org.apache.calcite.rel.RelNode;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.Set;\n+\n+/**\n+ * A {@link DAGProcessor} which organize {@link ExecNode}s into multiple input nodes.\n+ *\n+ * <p>For a detailed explanation of the algorithm, see appendix of the\n+ * <a href=\"https://docs.google.com/document/d/1qKVohV12qn-bM51cBZ8Hcgp31ntwClxjoiNBUOqVHsI\">design doc</a>.\n+ */\n+public class MultipleInputNodeCreationProcessor implements DAGProcessor {\n+\n+\tprivate final boolean isStreaming;\n+\n+\tpublic MultipleInputNodeCreationProcessor(boolean isStreaming) {\n+\t\tthis.isStreaming = isStreaming;\n+\t}\n+\n+\t@Override\n+\tpublic List<ExecNode<?, ?>> process(List<ExecNode<?, ?>> sinkNodes, DAGProcessContext context) {\n+\t\tif (!isStreaming) {\n+\t\t\t// As multiple input nodes use function call to deliver records between sub-operators,\n+\t\t\t// we cannot rely on network buffers to buffer records not yet ready to be read,\n+\t\t\t// so only BLOCKING dam behavior is safe here.\n+\t\t\t// If conflict is detected under this stricter constraint,\n+\t\t\t// we add a PIPELINED exchange to mark that its input and output node cannot be merged\n+\t\t\t// into the same multiple input node\n+\t\t\tInputPriorityConflictResolver resolver = new InputPriorityConflictResolver(\n+\t\t\t\tsinkNodes,\n+\t\t\t\tCollections.emptySet(),\n+\t\t\t\tExecEdge.DamBehavior.BLOCKING,\n+\t\t\t\tShuffleMode.PIPELINED);\n+\t\t\tresolver.detectAndResolve();\n+\t\t}\n+\n+\t\tList<ExecNodeWrapper> sinkWrappers = wrapExecNodes(sinkNodes);\n+\t\t// sort all nodes in topological order, sinks come first and sources come last\n+\t\tList<ExecNodeWrapper> orderedWrappers = topologicalSort(sinkWrappers);\n+\t\t// group nodes into multiple input groups\n+\t\tcreateMultipleInputGroups(orderedWrappers);\n+\t\t// apply optimizations to remove unnecessary nodes out of multiple input groups\n+\t\toptimizeMultipleInputGroups(orderedWrappers);\n+\n+\t\t// create the real multiple input nodes\n+\t\treturn createMultipleInputNodes(sinkWrappers);\n+\t}\n+\n+\t// --------------------------------------------------------------------------------\n+\t// Wrapping and Sorting\n+\t// --------------------------------------------------------------------------------\n+\n+\tprivate List<ExecNodeWrapper> wrapExecNodes(List<ExecNode<?, ?>> sinkNodes) {\n+\t\tMap<ExecNode<?, ?>, ExecNodeWrapper> wrapperMap = new HashMap<>();\n+\t\tAbstractExecNodeExactlyOnceVisitor visitor = new AbstractExecNodeExactlyOnceVisitor() {\n+\t\t\t@Override\n+\t\t\tprotected void visitNode(ExecNode<?, ?> node) {\n+\t\t\t\tExecNodeWrapper wrapper = wrapperMap.computeIfAbsent(node, k -> new ExecNodeWrapper(node));\n+\t\t\t\tfor (ExecNode<?, ?> input : node.getInputNodes()) {\n+\t\t\t\t\tExecNodeWrapper inputWrapper = wrapperMap.computeIfAbsent(input, k -> new ExecNodeWrapper(input));\n+\t\t\t\t\twrapper.inputs.add(inputWrapper);\n+\t\t\t\t\tinputWrapper.outputs.add(wrapper);\n+\t\t\t\t}\n+\t\t\t\tvisitInputs(node);\n+\t\t\t}\n+\t\t};\n+\t\tsinkNodes.forEach(s -> s.accept(visitor));\n+\n+\t\tList<ExecNodeWrapper> sinkWrappers = new ArrayList<>();\n+\t\tfor (ExecNode<?, ?> sink : sinkNodes) {\n+\t\t\tExecNodeWrapper sinkWrapper = wrapperMap.get(sink);\n+\t\t\tPreconditions.checkNotNull(sinkWrapper, \"Sink node is not wrapped. This is a bug.\");\n+\t\t\tsinkWrappers.add(sinkWrapper);\n+\t\t}\n+\t\treturn sinkWrappers;\n+\t}\n+\n+\tprivate List<ExecNodeWrapper> topologicalSort(List<ExecNodeWrapper> sinkWrappers) {\n+\t\tList<ExecNodeWrapper> result = new ArrayList<>();\n+\t\tQueue<ExecNodeWrapper> queue = new LinkedList<>(sinkWrappers);\n+\t\tMap<ExecNodeWrapper, Integer> visitCountMap = new HashMap<>();\n+\n+\t\twhile (!queue.isEmpty()) {\n+\t\t\tExecNodeWrapper wrapper = queue.poll();\n+\t\t\tresult.add(wrapper);\n+\t\t\tfor (ExecNodeWrapper inputWrapper : wrapper.inputs) {\n+\t\t\t\tint visitCount = visitCountMap.compute(inputWrapper, (k, v) -> v == null ? 1 : v + 1);\n+\t\t\t\tif (visitCount == inputWrapper.outputs.size()) {\n+\t\t\t\t\tqueue.offer(inputWrapper);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\n+\t\treturn result;\n+\t}\n+\n+\t// --------------------------------------------------------------------------------\n+\t// Multiple Input Groups Creating\n+\t// --------------------------------------------------------------------------------\n+\n+\tprivate void createMultipleInputGroups(List<ExecNodeWrapper> orderedWrappers) {\n+\t\t// wrappers are checked in topological order from sinks to sources\n+\t\tfor (ExecNodeWrapper wrapper : orderedWrappers) {\n+\t\t\t// we skip nodes which cannot be a member of a multiple input node\n+\t\t\tif (!canBeMultipleInputNodeMember(wrapper)) {\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\t// we first try to assign this wrapper into the same group with its outputs\n+\t\t\tMultipleInputGroup outputGroup = canBeInSameGroupWithOutputs(wrapper);\n+\t\t\tif (outputGroup != null) {\n+\t\t\t\twrapper.addToGroup(outputGroup);\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\t// we then try to create a new multiple input group with this node as the root\n+\t\t\tif (canBeRootOfMultipleInputGroup(wrapper)) {\n+\t\t\t\twrapper.createGroup();\n+\t\t\t}\n+\n+\t\t\t// all our attempts failed, this node will not be in a multiple input node\n+\t\t}\n+\t}\n+\n+\tprivate boolean canBeMultipleInputNodeMember(ExecNodeWrapper wrapper) {\n+\t\tif (wrapper.inputs.isEmpty()) {\n+\t\t\t// sources cannot be a member of multiple input node\n+\t\t\treturn false;\n+\t\t}\n+\t\tif (wrapper.execNode instanceof BatchExecExchange) {\n+\t\t\t// exchange cannot be a member of multiple input node\n+\t\t\treturn false;\n+\t\t}\n+\t\tif (wrapper.execNode instanceof StreamExecExchange) {\n+\t\t\t// exchange cannot be a member of multiple input node\n+\t\t\treturn false;\n+\t\t}\n+\n+\t\treturn true;\n+\t}\n+\n+\t/**\n+\t * A node can only be assigned into the same multiple input group of its outputs\n+\t * if all outputs have a group and are the same.\n+\t *\n+\t * @return the {@link MultipleInputGroup} of the outputs if all outputs have a\n+\t *         group and are the same, null otherwise\n+\t */\n+\tprivate MultipleInputGroup canBeInSameGroupWithOutputs(ExecNodeWrapper wrapper) {\n+\t\tif (wrapper.outputs.isEmpty()) {\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\tMultipleInputGroup outputGroup = wrapper.outputs.get(0).group;\n+\t\tif (outputGroup == null) {\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\tfor (ExecNodeWrapper outputWrapper : wrapper.outputs) {\n+\t\t\tif (outputWrapper.group != outputGroup) {\n+\t\t\t\treturn null;\n+\t\t\t}\n+\t\t}\n+\n+\t\treturn outputGroup;\n+\t}\n+\n+\tprivate boolean canBeRootOfMultipleInputGroup(ExecNodeWrapper wrapper) {\n+\t\t// only a node with more than one input can be the root,\n+\t\t// as one-input operator chaining are handled by operator chains\n+\t\treturn wrapper.inputs.size() >= 2;\n+\t}\n+\n+\t// --------------------------------------------------------------------------------\n+\t// Multiple Input Groups Optimizing\n+\t// --------------------------------------------------------------------------------\n+\n+\tprivate void optimizeMultipleInputGroups(List<ExecNodeWrapper> orderedWrappers) {\n+\t\t// wrappers are checked in topological order from sources to sinks\n+\t\tfor (int i = orderedWrappers.size() - 1; i >= 0; i--) {\n+\t\t\tExecNodeWrapper wrapper = orderedWrappers.get(i);\n+\t\t\tMultipleInputGroup group = wrapper.group;\n+\t\t\tif (group == null) {\n+\t\t\t\t// we only consider nodes currently in a multiple input group\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\tboolean isUnion =\n+\t\t\t\twrapper.execNode instanceof BatchExecUnion || wrapper.execNode instanceof StreamExecUnion;\n+\n+\t\t\tif (group.members.size() == 1) {\n+\t\t\t\tPreconditions.checkState(\n+\t\t\t\t\twrapper == group.root,\n+\t\t\t\t\t\"The only member of a multiple input group is not its root. This is a bug.\");\n+\t\t\t\t// optimization 1. we clean up multiple input groups with only 1 member,\n+\t\t\t\t// unless one of its input is a FLIP-27 source (for maximizing source chaining),\n+\t\t\t\t// however unions do not apply to this optimization because they're not real operators\n+\t\t\t\tif (isUnion || wrapper.inputs.stream().noneMatch(inputWrapper -> isNewSource(inputWrapper.execNode))) {\n+\t\t\t\t\twrapper.removeFromGroup();\n+\t\t\t\t}\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\tif (!isTailOfMultipleInputGroup(wrapper)) {\n+\t\t\t\t// we're not removing a node from the middle of a multiple input group\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\tboolean shouldRemove = false;\n+\t\t\tif (isUnion) {\n+\t\t\t\t// optimization 2. we do not allow union to be the tail of a multiple input\n+\t\t\t\t// as we're paying extra function calls for this, unless one of the united\n+\t\t\t\t// input is a FLIP-27 source\n+\t\t\t\tshouldRemove = wrapper.inputs.stream().noneMatch(inputWrapper -> isNewSource(inputWrapper.execNode));\n+\t\t\t} else if (wrapper.inputs.size() == 1) {\n+\t\t\t\t// optimization 3. for one-input operators we'll remove it unless its input\n+\t\t\t\t// is an exchange or a FLIP-27 source, this is mainly to avoid the following\n+\t\t\t\t// pattern:\n+\t\t\t\t// non-chainable source -> calc --\\\n+\t\t\t\t//                                 join ->\n+\t\t\t\t// non-chainable source -> calc --/\n+\t\t\t\t// if we move two calcs into the multiple input group rooted at the join, we're\n+\t\t\t\t// directly shuffling large amount of records from the source without filtering\n+\t\t\t\t// by the calc\n+\t\t\t\tExecNode<?, ?> input = wrapper.inputs.get(0).execNode;\n+\t\t\t\tshouldRemove = !(input instanceof BatchExecExchange) &&\n+\t\t\t\t\t!(input instanceof StreamExecExchange) &&\n+\t\t\t\t\t!isNewSource(input);\n+\t\t\t}\n+\n+\t\t\t// optimization 4. for singleton operations (for example singleton global agg)\n+\t\t\t// we're not including it into the multiple input node as we have to ensure that\n+\t\t\t// the whole multiple input can only have 1 parallelism.\n+\t\t\t// continuous singleton operations connected by forwarding shuffle will be dealt\n+\t\t\t// together with optimization 3\n+\t\t\tshouldRemove |= wrapper.inputs.stream().anyMatch(inputWrapper ->\n+\t\t\t\tinputWrapper.execNode instanceof BatchExecExchange &&\n+\t\t\t\t\t((BatchExecExchange) inputWrapper.execNode)\n+\t\t\t\t\t\t.distribution.getType() == RelDistribution.Type.SINGLETON);\n+\n+\t\t\tif (shouldRemove) {\n+\t\t\t\twrapper.removeFromGroup();\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate boolean isTailOfMultipleInputGroup(ExecNodeWrapper wrapper) {\n+\t\tPreconditions.checkNotNull(\n+\t\t\twrapper.group,\n+\t\t\t\"Exec node wrapper does not have a multiple input group. This is a bug.\");\n+\t\tfor (ExecNodeWrapper inputWrapper : wrapper.inputs) {\n+\t\t\tif (inputWrapper.group == wrapper.group) {\n+\t\t\t\t// one of the input is in the same group, so this node is not the tail of the group\n+\t\t\t\treturn false;\n+\t\t\t}\n+\t\t}\n+\t\treturn true;\n+\t}\n+\n+\tprivate boolean isNewSource(ExecNode<?, ?> node) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3c50597400946ab21e7dba24673a0b5df30eaf2b"}, "originalPosition": 306}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTcwMDE5Mg==", "bodyText": "FLIP-146 is not ready when this PR is submitted. I'll rebase the master branch.", "url": "https://github.com/apache/flink/pull/13742#discussion_r511700192", "createdAt": "2020-10-26T03:14:17Z", "author": {"login": "tsreaper"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/MultipleInputNodeCreationProcessor.java", "diffHunk": "@@ -0,0 +1,481 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.reuse;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.streaming.api.transformations.ShuffleMode;\n+import org.apache.flink.streaming.api.transformations.SourceTransformation;\n+import org.apache.flink.table.planner.plan.nodes.exec.AbstractExecNodeExactlyOnceVisitor;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecEdge;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecBoundedStreamScan;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecExchange;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecMultipleInputNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecUnion;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecDataStreamScan;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecExchange;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecMultipleInputNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecUnion;\n+import org.apache.flink.table.planner.plan.nodes.process.DAGProcessContext;\n+import org.apache.flink.table.planner.plan.nodes.process.DAGProcessor;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.calcite.rel.RelDistribution;\n+import org.apache.calcite.rel.RelNode;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.Set;\n+\n+/**\n+ * A {@link DAGProcessor} which organize {@link ExecNode}s into multiple input nodes.\n+ *\n+ * <p>For a detailed explanation of the algorithm, see appendix of the\n+ * <a href=\"https://docs.google.com/document/d/1qKVohV12qn-bM51cBZ8Hcgp31ntwClxjoiNBUOqVHsI\">design doc</a>.\n+ */\n+public class MultipleInputNodeCreationProcessor implements DAGProcessor {\n+\n+\tprivate final boolean isStreaming;\n+\n+\tpublic MultipleInputNodeCreationProcessor(boolean isStreaming) {\n+\t\tthis.isStreaming = isStreaming;\n+\t}\n+\n+\t@Override\n+\tpublic List<ExecNode<?, ?>> process(List<ExecNode<?, ?>> sinkNodes, DAGProcessContext context) {\n+\t\tif (!isStreaming) {\n+\t\t\t// As multiple input nodes use function call to deliver records between sub-operators,\n+\t\t\t// we cannot rely on network buffers to buffer records not yet ready to be read,\n+\t\t\t// so only BLOCKING dam behavior is safe here.\n+\t\t\t// If conflict is detected under this stricter constraint,\n+\t\t\t// we add a PIPELINED exchange to mark that its input and output node cannot be merged\n+\t\t\t// into the same multiple input node\n+\t\t\tInputPriorityConflictResolver resolver = new InputPriorityConflictResolver(\n+\t\t\t\tsinkNodes,\n+\t\t\t\tCollections.emptySet(),\n+\t\t\t\tExecEdge.DamBehavior.BLOCKING,\n+\t\t\t\tShuffleMode.PIPELINED);\n+\t\t\tresolver.detectAndResolve();\n+\t\t}\n+\n+\t\tList<ExecNodeWrapper> sinkWrappers = wrapExecNodes(sinkNodes);\n+\t\t// sort all nodes in topological order, sinks come first and sources come last\n+\t\tList<ExecNodeWrapper> orderedWrappers = topologicalSort(sinkWrappers);\n+\t\t// group nodes into multiple input groups\n+\t\tcreateMultipleInputGroups(orderedWrappers);\n+\t\t// apply optimizations to remove unnecessary nodes out of multiple input groups\n+\t\toptimizeMultipleInputGroups(orderedWrappers);\n+\n+\t\t// create the real multiple input nodes\n+\t\treturn createMultipleInputNodes(sinkWrappers);\n+\t}\n+\n+\t// --------------------------------------------------------------------------------\n+\t// Wrapping and Sorting\n+\t// --------------------------------------------------------------------------------\n+\n+\tprivate List<ExecNodeWrapper> wrapExecNodes(List<ExecNode<?, ?>> sinkNodes) {\n+\t\tMap<ExecNode<?, ?>, ExecNodeWrapper> wrapperMap = new HashMap<>();\n+\t\tAbstractExecNodeExactlyOnceVisitor visitor = new AbstractExecNodeExactlyOnceVisitor() {\n+\t\t\t@Override\n+\t\t\tprotected void visitNode(ExecNode<?, ?> node) {\n+\t\t\t\tExecNodeWrapper wrapper = wrapperMap.computeIfAbsent(node, k -> new ExecNodeWrapper(node));\n+\t\t\t\tfor (ExecNode<?, ?> input : node.getInputNodes()) {\n+\t\t\t\t\tExecNodeWrapper inputWrapper = wrapperMap.computeIfAbsent(input, k -> new ExecNodeWrapper(input));\n+\t\t\t\t\twrapper.inputs.add(inputWrapper);\n+\t\t\t\t\tinputWrapper.outputs.add(wrapper);\n+\t\t\t\t}\n+\t\t\t\tvisitInputs(node);\n+\t\t\t}\n+\t\t};\n+\t\tsinkNodes.forEach(s -> s.accept(visitor));\n+\n+\t\tList<ExecNodeWrapper> sinkWrappers = new ArrayList<>();\n+\t\tfor (ExecNode<?, ?> sink : sinkNodes) {\n+\t\t\tExecNodeWrapper sinkWrapper = wrapperMap.get(sink);\n+\t\t\tPreconditions.checkNotNull(sinkWrapper, \"Sink node is not wrapped. This is a bug.\");\n+\t\t\tsinkWrappers.add(sinkWrapper);\n+\t\t}\n+\t\treturn sinkWrappers;\n+\t}\n+\n+\tprivate List<ExecNodeWrapper> topologicalSort(List<ExecNodeWrapper> sinkWrappers) {\n+\t\tList<ExecNodeWrapper> result = new ArrayList<>();\n+\t\tQueue<ExecNodeWrapper> queue = new LinkedList<>(sinkWrappers);\n+\t\tMap<ExecNodeWrapper, Integer> visitCountMap = new HashMap<>();\n+\n+\t\twhile (!queue.isEmpty()) {\n+\t\t\tExecNodeWrapper wrapper = queue.poll();\n+\t\t\tresult.add(wrapper);\n+\t\t\tfor (ExecNodeWrapper inputWrapper : wrapper.inputs) {\n+\t\t\t\tint visitCount = visitCountMap.compute(inputWrapper, (k, v) -> v == null ? 1 : v + 1);\n+\t\t\t\tif (visitCount == inputWrapper.outputs.size()) {\n+\t\t\t\t\tqueue.offer(inputWrapper);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\n+\t\treturn result;\n+\t}\n+\n+\t// --------------------------------------------------------------------------------\n+\t// Multiple Input Groups Creating\n+\t// --------------------------------------------------------------------------------\n+\n+\tprivate void createMultipleInputGroups(List<ExecNodeWrapper> orderedWrappers) {\n+\t\t// wrappers are checked in topological order from sinks to sources\n+\t\tfor (ExecNodeWrapper wrapper : orderedWrappers) {\n+\t\t\t// we skip nodes which cannot be a member of a multiple input node\n+\t\t\tif (!canBeMultipleInputNodeMember(wrapper)) {\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\t// we first try to assign this wrapper into the same group with its outputs\n+\t\t\tMultipleInputGroup outputGroup = canBeInSameGroupWithOutputs(wrapper);\n+\t\t\tif (outputGroup != null) {\n+\t\t\t\twrapper.addToGroup(outputGroup);\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\t// we then try to create a new multiple input group with this node as the root\n+\t\t\tif (canBeRootOfMultipleInputGroup(wrapper)) {\n+\t\t\t\twrapper.createGroup();\n+\t\t\t}\n+\n+\t\t\t// all our attempts failed, this node will not be in a multiple input node\n+\t\t}\n+\t}\n+\n+\tprivate boolean canBeMultipleInputNodeMember(ExecNodeWrapper wrapper) {\n+\t\tif (wrapper.inputs.isEmpty()) {\n+\t\t\t// sources cannot be a member of multiple input node\n+\t\t\treturn false;\n+\t\t}\n+\t\tif (wrapper.execNode instanceof BatchExecExchange) {\n+\t\t\t// exchange cannot be a member of multiple input node\n+\t\t\treturn false;\n+\t\t}\n+\t\tif (wrapper.execNode instanceof StreamExecExchange) {\n+\t\t\t// exchange cannot be a member of multiple input node\n+\t\t\treturn false;\n+\t\t}\n+\n+\t\treturn true;\n+\t}\n+\n+\t/**\n+\t * A node can only be assigned into the same multiple input group of its outputs\n+\t * if all outputs have a group and are the same.\n+\t *\n+\t * @return the {@link MultipleInputGroup} of the outputs if all outputs have a\n+\t *         group and are the same, null otherwise\n+\t */\n+\tprivate MultipleInputGroup canBeInSameGroupWithOutputs(ExecNodeWrapper wrapper) {\n+\t\tif (wrapper.outputs.isEmpty()) {\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\tMultipleInputGroup outputGroup = wrapper.outputs.get(0).group;\n+\t\tif (outputGroup == null) {\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\tfor (ExecNodeWrapper outputWrapper : wrapper.outputs) {\n+\t\t\tif (outputWrapper.group != outputGroup) {\n+\t\t\t\treturn null;\n+\t\t\t}\n+\t\t}\n+\n+\t\treturn outputGroup;\n+\t}\n+\n+\tprivate boolean canBeRootOfMultipleInputGroup(ExecNodeWrapper wrapper) {\n+\t\t// only a node with more than one input can be the root,\n+\t\t// as one-input operator chaining are handled by operator chains\n+\t\treturn wrapper.inputs.size() >= 2;\n+\t}\n+\n+\t// --------------------------------------------------------------------------------\n+\t// Multiple Input Groups Optimizing\n+\t// --------------------------------------------------------------------------------\n+\n+\tprivate void optimizeMultipleInputGroups(List<ExecNodeWrapper> orderedWrappers) {\n+\t\t// wrappers are checked in topological order from sources to sinks\n+\t\tfor (int i = orderedWrappers.size() - 1; i >= 0; i--) {\n+\t\t\tExecNodeWrapper wrapper = orderedWrappers.get(i);\n+\t\t\tMultipleInputGroup group = wrapper.group;\n+\t\t\tif (group == null) {\n+\t\t\t\t// we only consider nodes currently in a multiple input group\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\tboolean isUnion =\n+\t\t\t\twrapper.execNode instanceof BatchExecUnion || wrapper.execNode instanceof StreamExecUnion;\n+\n+\t\t\tif (group.members.size() == 1) {\n+\t\t\t\tPreconditions.checkState(\n+\t\t\t\t\twrapper == group.root,\n+\t\t\t\t\t\"The only member of a multiple input group is not its root. This is a bug.\");\n+\t\t\t\t// optimization 1. we clean up multiple input groups with only 1 member,\n+\t\t\t\t// unless one of its input is a FLIP-27 source (for maximizing source chaining),\n+\t\t\t\t// however unions do not apply to this optimization because they're not real operators\n+\t\t\t\tif (isUnion || wrapper.inputs.stream().noneMatch(inputWrapper -> isNewSource(inputWrapper.execNode))) {\n+\t\t\t\t\twrapper.removeFromGroup();\n+\t\t\t\t}\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\tif (!isTailOfMultipleInputGroup(wrapper)) {\n+\t\t\t\t// we're not removing a node from the middle of a multiple input group\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\tboolean shouldRemove = false;\n+\t\t\tif (isUnion) {\n+\t\t\t\t// optimization 2. we do not allow union to be the tail of a multiple input\n+\t\t\t\t// as we're paying extra function calls for this, unless one of the united\n+\t\t\t\t// input is a FLIP-27 source\n+\t\t\t\tshouldRemove = wrapper.inputs.stream().noneMatch(inputWrapper -> isNewSource(inputWrapper.execNode));\n+\t\t\t} else if (wrapper.inputs.size() == 1) {\n+\t\t\t\t// optimization 3. for one-input operators we'll remove it unless its input\n+\t\t\t\t// is an exchange or a FLIP-27 source, this is mainly to avoid the following\n+\t\t\t\t// pattern:\n+\t\t\t\t// non-chainable source -> calc --\\\n+\t\t\t\t//                                 join ->\n+\t\t\t\t// non-chainable source -> calc --/\n+\t\t\t\t// if we move two calcs into the multiple input group rooted at the join, we're\n+\t\t\t\t// directly shuffling large amount of records from the source without filtering\n+\t\t\t\t// by the calc\n+\t\t\t\tExecNode<?, ?> input = wrapper.inputs.get(0).execNode;\n+\t\t\t\tshouldRemove = !(input instanceof BatchExecExchange) &&\n+\t\t\t\t\t!(input instanceof StreamExecExchange) &&\n+\t\t\t\t\t!isNewSource(input);\n+\t\t\t}\n+\n+\t\t\t// optimization 4. for singleton operations (for example singleton global agg)\n+\t\t\t// we're not including it into the multiple input node as we have to ensure that\n+\t\t\t// the whole multiple input can only have 1 parallelism.\n+\t\t\t// continuous singleton operations connected by forwarding shuffle will be dealt\n+\t\t\t// together with optimization 3\n+\t\t\tshouldRemove |= wrapper.inputs.stream().anyMatch(inputWrapper ->\n+\t\t\t\tinputWrapper.execNode instanceof BatchExecExchange &&\n+\t\t\t\t\t((BatchExecExchange) inputWrapper.execNode)\n+\t\t\t\t\t\t.distribution.getType() == RelDistribution.Type.SINGLETON);\n+\n+\t\t\tif (shouldRemove) {\n+\t\t\t\twrapper.removeFromGroup();\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate boolean isTailOfMultipleInputGroup(ExecNodeWrapper wrapper) {\n+\t\tPreconditions.checkNotNull(\n+\t\t\twrapper.group,\n+\t\t\t\"Exec node wrapper does not have a multiple input group. This is a bug.\");\n+\t\tfor (ExecNodeWrapper inputWrapper : wrapper.inputs) {\n+\t\t\tif (inputWrapper.group == wrapper.group) {\n+\t\t\t\t// one of the input is in the same group, so this node is not the tail of the group\n+\t\t\t\treturn false;\n+\t\t\t}\n+\t\t}\n+\t\treturn true;\n+\t}\n+\n+\tprivate boolean isNewSource(ExecNode<?, ?> node) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTYwMjk1MA=="}, "originalCommit": {"oid": "3c50597400946ab21e7dba24673a0b5df30eaf2b"}, "originalPosition": 306}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNTEzMTA1OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/MultipleInputNodeCreationProcessor.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNVQxNDoyMzoyM1rOHn5zMw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQwMzoxNTo1MFrOHn_uFA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTYwMzUwNw==", "bodyText": "Tail => Header ?", "url": "https://github.com/apache/flink/pull/13742#discussion_r511603507", "createdAt": "2020-10-25T14:23:23Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/MultipleInputNodeCreationProcessor.java", "diffHunk": "@@ -0,0 +1,481 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.reuse;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.streaming.api.transformations.ShuffleMode;\n+import org.apache.flink.streaming.api.transformations.SourceTransformation;\n+import org.apache.flink.table.planner.plan.nodes.exec.AbstractExecNodeExactlyOnceVisitor;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecEdge;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecBoundedStreamScan;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecExchange;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecMultipleInputNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecUnion;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecDataStreamScan;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecExchange;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecMultipleInputNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecUnion;\n+import org.apache.flink.table.planner.plan.nodes.process.DAGProcessContext;\n+import org.apache.flink.table.planner.plan.nodes.process.DAGProcessor;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.calcite.rel.RelDistribution;\n+import org.apache.calcite.rel.RelNode;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.Set;\n+\n+/**\n+ * A {@link DAGProcessor} which organize {@link ExecNode}s into multiple input nodes.\n+ *\n+ * <p>For a detailed explanation of the algorithm, see appendix of the\n+ * <a href=\"https://docs.google.com/document/d/1qKVohV12qn-bM51cBZ8Hcgp31ntwClxjoiNBUOqVHsI\">design doc</a>.\n+ */\n+public class MultipleInputNodeCreationProcessor implements DAGProcessor {\n+\n+\tprivate final boolean isStreaming;\n+\n+\tpublic MultipleInputNodeCreationProcessor(boolean isStreaming) {\n+\t\tthis.isStreaming = isStreaming;\n+\t}\n+\n+\t@Override\n+\tpublic List<ExecNode<?, ?>> process(List<ExecNode<?, ?>> sinkNodes, DAGProcessContext context) {\n+\t\tif (!isStreaming) {\n+\t\t\t// As multiple input nodes use function call to deliver records between sub-operators,\n+\t\t\t// we cannot rely on network buffers to buffer records not yet ready to be read,\n+\t\t\t// so only BLOCKING dam behavior is safe here.\n+\t\t\t// If conflict is detected under this stricter constraint,\n+\t\t\t// we add a PIPELINED exchange to mark that its input and output node cannot be merged\n+\t\t\t// into the same multiple input node\n+\t\t\tInputPriorityConflictResolver resolver = new InputPriorityConflictResolver(\n+\t\t\t\tsinkNodes,\n+\t\t\t\tCollections.emptySet(),\n+\t\t\t\tExecEdge.DamBehavior.BLOCKING,\n+\t\t\t\tShuffleMode.PIPELINED);\n+\t\t\tresolver.detectAndResolve();\n+\t\t}\n+\n+\t\tList<ExecNodeWrapper> sinkWrappers = wrapExecNodes(sinkNodes);\n+\t\t// sort all nodes in topological order, sinks come first and sources come last\n+\t\tList<ExecNodeWrapper> orderedWrappers = topologicalSort(sinkWrappers);\n+\t\t// group nodes into multiple input groups\n+\t\tcreateMultipleInputGroups(orderedWrappers);\n+\t\t// apply optimizations to remove unnecessary nodes out of multiple input groups\n+\t\toptimizeMultipleInputGroups(orderedWrappers);\n+\n+\t\t// create the real multiple input nodes\n+\t\treturn createMultipleInputNodes(sinkWrappers);\n+\t}\n+\n+\t// --------------------------------------------------------------------------------\n+\t// Wrapping and Sorting\n+\t// --------------------------------------------------------------------------------\n+\n+\tprivate List<ExecNodeWrapper> wrapExecNodes(List<ExecNode<?, ?>> sinkNodes) {\n+\t\tMap<ExecNode<?, ?>, ExecNodeWrapper> wrapperMap = new HashMap<>();\n+\t\tAbstractExecNodeExactlyOnceVisitor visitor = new AbstractExecNodeExactlyOnceVisitor() {\n+\t\t\t@Override\n+\t\t\tprotected void visitNode(ExecNode<?, ?> node) {\n+\t\t\t\tExecNodeWrapper wrapper = wrapperMap.computeIfAbsent(node, k -> new ExecNodeWrapper(node));\n+\t\t\t\tfor (ExecNode<?, ?> input : node.getInputNodes()) {\n+\t\t\t\t\tExecNodeWrapper inputWrapper = wrapperMap.computeIfAbsent(input, k -> new ExecNodeWrapper(input));\n+\t\t\t\t\twrapper.inputs.add(inputWrapper);\n+\t\t\t\t\tinputWrapper.outputs.add(wrapper);\n+\t\t\t\t}\n+\t\t\t\tvisitInputs(node);\n+\t\t\t}\n+\t\t};\n+\t\tsinkNodes.forEach(s -> s.accept(visitor));\n+\n+\t\tList<ExecNodeWrapper> sinkWrappers = new ArrayList<>();\n+\t\tfor (ExecNode<?, ?> sink : sinkNodes) {\n+\t\t\tExecNodeWrapper sinkWrapper = wrapperMap.get(sink);\n+\t\t\tPreconditions.checkNotNull(sinkWrapper, \"Sink node is not wrapped. This is a bug.\");\n+\t\t\tsinkWrappers.add(sinkWrapper);\n+\t\t}\n+\t\treturn sinkWrappers;\n+\t}\n+\n+\tprivate List<ExecNodeWrapper> topologicalSort(List<ExecNodeWrapper> sinkWrappers) {\n+\t\tList<ExecNodeWrapper> result = new ArrayList<>();\n+\t\tQueue<ExecNodeWrapper> queue = new LinkedList<>(sinkWrappers);\n+\t\tMap<ExecNodeWrapper, Integer> visitCountMap = new HashMap<>();\n+\n+\t\twhile (!queue.isEmpty()) {\n+\t\t\tExecNodeWrapper wrapper = queue.poll();\n+\t\t\tresult.add(wrapper);\n+\t\t\tfor (ExecNodeWrapper inputWrapper : wrapper.inputs) {\n+\t\t\t\tint visitCount = visitCountMap.compute(inputWrapper, (k, v) -> v == null ? 1 : v + 1);\n+\t\t\t\tif (visitCount == inputWrapper.outputs.size()) {\n+\t\t\t\t\tqueue.offer(inputWrapper);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\n+\t\treturn result;\n+\t}\n+\n+\t// --------------------------------------------------------------------------------\n+\t// Multiple Input Groups Creating\n+\t// --------------------------------------------------------------------------------\n+\n+\tprivate void createMultipleInputGroups(List<ExecNodeWrapper> orderedWrappers) {\n+\t\t// wrappers are checked in topological order from sinks to sources\n+\t\tfor (ExecNodeWrapper wrapper : orderedWrappers) {\n+\t\t\t// we skip nodes which cannot be a member of a multiple input node\n+\t\t\tif (!canBeMultipleInputNodeMember(wrapper)) {\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\t// we first try to assign this wrapper into the same group with its outputs\n+\t\t\tMultipleInputGroup outputGroup = canBeInSameGroupWithOutputs(wrapper);\n+\t\t\tif (outputGroup != null) {\n+\t\t\t\twrapper.addToGroup(outputGroup);\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\t// we then try to create a new multiple input group with this node as the root\n+\t\t\tif (canBeRootOfMultipleInputGroup(wrapper)) {\n+\t\t\t\twrapper.createGroup();\n+\t\t\t}\n+\n+\t\t\t// all our attempts failed, this node will not be in a multiple input node\n+\t\t}\n+\t}\n+\n+\tprivate boolean canBeMultipleInputNodeMember(ExecNodeWrapper wrapper) {\n+\t\tif (wrapper.inputs.isEmpty()) {\n+\t\t\t// sources cannot be a member of multiple input node\n+\t\t\treturn false;\n+\t\t}\n+\t\tif (wrapper.execNode instanceof BatchExecExchange) {\n+\t\t\t// exchange cannot be a member of multiple input node\n+\t\t\treturn false;\n+\t\t}\n+\t\tif (wrapper.execNode instanceof StreamExecExchange) {\n+\t\t\t// exchange cannot be a member of multiple input node\n+\t\t\treturn false;\n+\t\t}\n+\n+\t\treturn true;\n+\t}\n+\n+\t/**\n+\t * A node can only be assigned into the same multiple input group of its outputs\n+\t * if all outputs have a group and are the same.\n+\t *\n+\t * @return the {@link MultipleInputGroup} of the outputs if all outputs have a\n+\t *         group and are the same, null otherwise\n+\t */\n+\tprivate MultipleInputGroup canBeInSameGroupWithOutputs(ExecNodeWrapper wrapper) {\n+\t\tif (wrapper.outputs.isEmpty()) {\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\tMultipleInputGroup outputGroup = wrapper.outputs.get(0).group;\n+\t\tif (outputGroup == null) {\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\tfor (ExecNodeWrapper outputWrapper : wrapper.outputs) {\n+\t\t\tif (outputWrapper.group != outputGroup) {\n+\t\t\t\treturn null;\n+\t\t\t}\n+\t\t}\n+\n+\t\treturn outputGroup;\n+\t}\n+\n+\tprivate boolean canBeRootOfMultipleInputGroup(ExecNodeWrapper wrapper) {\n+\t\t// only a node with more than one input can be the root,\n+\t\t// as one-input operator chaining are handled by operator chains\n+\t\treturn wrapper.inputs.size() >= 2;\n+\t}\n+\n+\t// --------------------------------------------------------------------------------\n+\t// Multiple Input Groups Optimizing\n+\t// --------------------------------------------------------------------------------\n+\n+\tprivate void optimizeMultipleInputGroups(List<ExecNodeWrapper> orderedWrappers) {\n+\t\t// wrappers are checked in topological order from sources to sinks\n+\t\tfor (int i = orderedWrappers.size() - 1; i >= 0; i--) {\n+\t\t\tExecNodeWrapper wrapper = orderedWrappers.get(i);\n+\t\t\tMultipleInputGroup group = wrapper.group;\n+\t\t\tif (group == null) {\n+\t\t\t\t// we only consider nodes currently in a multiple input group\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\tboolean isUnion =\n+\t\t\t\twrapper.execNode instanceof BatchExecUnion || wrapper.execNode instanceof StreamExecUnion;\n+\n+\t\t\tif (group.members.size() == 1) {\n+\t\t\t\tPreconditions.checkState(\n+\t\t\t\t\twrapper == group.root,\n+\t\t\t\t\t\"The only member of a multiple input group is not its root. This is a bug.\");\n+\t\t\t\t// optimization 1. we clean up multiple input groups with only 1 member,\n+\t\t\t\t// unless one of its input is a FLIP-27 source (for maximizing source chaining),\n+\t\t\t\t// however unions do not apply to this optimization because they're not real operators\n+\t\t\t\tif (isUnion || wrapper.inputs.stream().noneMatch(inputWrapper -> isNewSource(inputWrapper.execNode))) {\n+\t\t\t\t\twrapper.removeFromGroup();\n+\t\t\t\t}\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\tif (!isTailOfMultipleInputGroup(wrapper)) {\n+\t\t\t\t// we're not removing a node from the middle of a multiple input group\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\tboolean shouldRemove = false;\n+\t\t\tif (isUnion) {\n+\t\t\t\t// optimization 2. we do not allow union to be the tail of a multiple input\n+\t\t\t\t// as we're paying extra function calls for this, unless one of the united\n+\t\t\t\t// input is a FLIP-27 source\n+\t\t\t\tshouldRemove = wrapper.inputs.stream().noneMatch(inputWrapper -> isNewSource(inputWrapper.execNode));\n+\t\t\t} else if (wrapper.inputs.size() == 1) {\n+\t\t\t\t// optimization 3. for one-input operators we'll remove it unless its input\n+\t\t\t\t// is an exchange or a FLIP-27 source, this is mainly to avoid the following\n+\t\t\t\t// pattern:\n+\t\t\t\t// non-chainable source -> calc --\\\n+\t\t\t\t//                                 join ->\n+\t\t\t\t// non-chainable source -> calc --/\n+\t\t\t\t// if we move two calcs into the multiple input group rooted at the join, we're\n+\t\t\t\t// directly shuffling large amount of records from the source without filtering\n+\t\t\t\t// by the calc\n+\t\t\t\tExecNode<?, ?> input = wrapper.inputs.get(0).execNode;\n+\t\t\t\tshouldRemove = !(input instanceof BatchExecExchange) &&\n+\t\t\t\t\t!(input instanceof StreamExecExchange) &&\n+\t\t\t\t\t!isNewSource(input);\n+\t\t\t}\n+\n+\t\t\t// optimization 4. for singleton operations (for example singleton global agg)\n+\t\t\t// we're not including it into the multiple input node as we have to ensure that\n+\t\t\t// the whole multiple input can only have 1 parallelism.\n+\t\t\t// continuous singleton operations connected by forwarding shuffle will be dealt\n+\t\t\t// together with optimization 3\n+\t\t\tshouldRemove |= wrapper.inputs.stream().anyMatch(inputWrapper ->\n+\t\t\t\tinputWrapper.execNode instanceof BatchExecExchange &&\n+\t\t\t\t\t((BatchExecExchange) inputWrapper.execNode)\n+\t\t\t\t\t\t.distribution.getType() == RelDistribution.Type.SINGLETON);\n+\n+\t\t\tif (shouldRemove) {\n+\t\t\t\twrapper.removeFromGroup();\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate boolean isTailOfMultipleInputGroup(ExecNodeWrapper wrapper) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3c50597400946ab21e7dba24673a0b5df30eaf2b"}, "originalPosition": 293}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTcwMDUwMA==", "bodyText": "I actually don't like the idea of head and tail in the multiple input operator as they're ambiguous. Why don't we use phrases like input, entrance and root?", "url": "https://github.com/apache/flink/pull/13742#discussion_r511700500", "createdAt": "2020-10-26T03:15:50Z", "author": {"login": "tsreaper"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/MultipleInputNodeCreationProcessor.java", "diffHunk": "@@ -0,0 +1,481 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.reuse;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.streaming.api.transformations.ShuffleMode;\n+import org.apache.flink.streaming.api.transformations.SourceTransformation;\n+import org.apache.flink.table.planner.plan.nodes.exec.AbstractExecNodeExactlyOnceVisitor;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecEdge;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecBoundedStreamScan;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecExchange;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecMultipleInputNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecUnion;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecDataStreamScan;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecExchange;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecMultipleInputNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecUnion;\n+import org.apache.flink.table.planner.plan.nodes.process.DAGProcessContext;\n+import org.apache.flink.table.planner.plan.nodes.process.DAGProcessor;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.calcite.rel.RelDistribution;\n+import org.apache.calcite.rel.RelNode;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.Set;\n+\n+/**\n+ * A {@link DAGProcessor} which organize {@link ExecNode}s into multiple input nodes.\n+ *\n+ * <p>For a detailed explanation of the algorithm, see appendix of the\n+ * <a href=\"https://docs.google.com/document/d/1qKVohV12qn-bM51cBZ8Hcgp31ntwClxjoiNBUOqVHsI\">design doc</a>.\n+ */\n+public class MultipleInputNodeCreationProcessor implements DAGProcessor {\n+\n+\tprivate final boolean isStreaming;\n+\n+\tpublic MultipleInputNodeCreationProcessor(boolean isStreaming) {\n+\t\tthis.isStreaming = isStreaming;\n+\t}\n+\n+\t@Override\n+\tpublic List<ExecNode<?, ?>> process(List<ExecNode<?, ?>> sinkNodes, DAGProcessContext context) {\n+\t\tif (!isStreaming) {\n+\t\t\t// As multiple input nodes use function call to deliver records between sub-operators,\n+\t\t\t// we cannot rely on network buffers to buffer records not yet ready to be read,\n+\t\t\t// so only BLOCKING dam behavior is safe here.\n+\t\t\t// If conflict is detected under this stricter constraint,\n+\t\t\t// we add a PIPELINED exchange to mark that its input and output node cannot be merged\n+\t\t\t// into the same multiple input node\n+\t\t\tInputPriorityConflictResolver resolver = new InputPriorityConflictResolver(\n+\t\t\t\tsinkNodes,\n+\t\t\t\tCollections.emptySet(),\n+\t\t\t\tExecEdge.DamBehavior.BLOCKING,\n+\t\t\t\tShuffleMode.PIPELINED);\n+\t\t\tresolver.detectAndResolve();\n+\t\t}\n+\n+\t\tList<ExecNodeWrapper> sinkWrappers = wrapExecNodes(sinkNodes);\n+\t\t// sort all nodes in topological order, sinks come first and sources come last\n+\t\tList<ExecNodeWrapper> orderedWrappers = topologicalSort(sinkWrappers);\n+\t\t// group nodes into multiple input groups\n+\t\tcreateMultipleInputGroups(orderedWrappers);\n+\t\t// apply optimizations to remove unnecessary nodes out of multiple input groups\n+\t\toptimizeMultipleInputGroups(orderedWrappers);\n+\n+\t\t// create the real multiple input nodes\n+\t\treturn createMultipleInputNodes(sinkWrappers);\n+\t}\n+\n+\t// --------------------------------------------------------------------------------\n+\t// Wrapping and Sorting\n+\t// --------------------------------------------------------------------------------\n+\n+\tprivate List<ExecNodeWrapper> wrapExecNodes(List<ExecNode<?, ?>> sinkNodes) {\n+\t\tMap<ExecNode<?, ?>, ExecNodeWrapper> wrapperMap = new HashMap<>();\n+\t\tAbstractExecNodeExactlyOnceVisitor visitor = new AbstractExecNodeExactlyOnceVisitor() {\n+\t\t\t@Override\n+\t\t\tprotected void visitNode(ExecNode<?, ?> node) {\n+\t\t\t\tExecNodeWrapper wrapper = wrapperMap.computeIfAbsent(node, k -> new ExecNodeWrapper(node));\n+\t\t\t\tfor (ExecNode<?, ?> input : node.getInputNodes()) {\n+\t\t\t\t\tExecNodeWrapper inputWrapper = wrapperMap.computeIfAbsent(input, k -> new ExecNodeWrapper(input));\n+\t\t\t\t\twrapper.inputs.add(inputWrapper);\n+\t\t\t\t\tinputWrapper.outputs.add(wrapper);\n+\t\t\t\t}\n+\t\t\t\tvisitInputs(node);\n+\t\t\t}\n+\t\t};\n+\t\tsinkNodes.forEach(s -> s.accept(visitor));\n+\n+\t\tList<ExecNodeWrapper> sinkWrappers = new ArrayList<>();\n+\t\tfor (ExecNode<?, ?> sink : sinkNodes) {\n+\t\t\tExecNodeWrapper sinkWrapper = wrapperMap.get(sink);\n+\t\t\tPreconditions.checkNotNull(sinkWrapper, \"Sink node is not wrapped. This is a bug.\");\n+\t\t\tsinkWrappers.add(sinkWrapper);\n+\t\t}\n+\t\treturn sinkWrappers;\n+\t}\n+\n+\tprivate List<ExecNodeWrapper> topologicalSort(List<ExecNodeWrapper> sinkWrappers) {\n+\t\tList<ExecNodeWrapper> result = new ArrayList<>();\n+\t\tQueue<ExecNodeWrapper> queue = new LinkedList<>(sinkWrappers);\n+\t\tMap<ExecNodeWrapper, Integer> visitCountMap = new HashMap<>();\n+\n+\t\twhile (!queue.isEmpty()) {\n+\t\t\tExecNodeWrapper wrapper = queue.poll();\n+\t\t\tresult.add(wrapper);\n+\t\t\tfor (ExecNodeWrapper inputWrapper : wrapper.inputs) {\n+\t\t\t\tint visitCount = visitCountMap.compute(inputWrapper, (k, v) -> v == null ? 1 : v + 1);\n+\t\t\t\tif (visitCount == inputWrapper.outputs.size()) {\n+\t\t\t\t\tqueue.offer(inputWrapper);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\n+\t\treturn result;\n+\t}\n+\n+\t// --------------------------------------------------------------------------------\n+\t// Multiple Input Groups Creating\n+\t// --------------------------------------------------------------------------------\n+\n+\tprivate void createMultipleInputGroups(List<ExecNodeWrapper> orderedWrappers) {\n+\t\t// wrappers are checked in topological order from sinks to sources\n+\t\tfor (ExecNodeWrapper wrapper : orderedWrappers) {\n+\t\t\t// we skip nodes which cannot be a member of a multiple input node\n+\t\t\tif (!canBeMultipleInputNodeMember(wrapper)) {\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\t// we first try to assign this wrapper into the same group with its outputs\n+\t\t\tMultipleInputGroup outputGroup = canBeInSameGroupWithOutputs(wrapper);\n+\t\t\tif (outputGroup != null) {\n+\t\t\t\twrapper.addToGroup(outputGroup);\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\t// we then try to create a new multiple input group with this node as the root\n+\t\t\tif (canBeRootOfMultipleInputGroup(wrapper)) {\n+\t\t\t\twrapper.createGroup();\n+\t\t\t}\n+\n+\t\t\t// all our attempts failed, this node will not be in a multiple input node\n+\t\t}\n+\t}\n+\n+\tprivate boolean canBeMultipleInputNodeMember(ExecNodeWrapper wrapper) {\n+\t\tif (wrapper.inputs.isEmpty()) {\n+\t\t\t// sources cannot be a member of multiple input node\n+\t\t\treturn false;\n+\t\t}\n+\t\tif (wrapper.execNode instanceof BatchExecExchange) {\n+\t\t\t// exchange cannot be a member of multiple input node\n+\t\t\treturn false;\n+\t\t}\n+\t\tif (wrapper.execNode instanceof StreamExecExchange) {\n+\t\t\t// exchange cannot be a member of multiple input node\n+\t\t\treturn false;\n+\t\t}\n+\n+\t\treturn true;\n+\t}\n+\n+\t/**\n+\t * A node can only be assigned into the same multiple input group of its outputs\n+\t * if all outputs have a group and are the same.\n+\t *\n+\t * @return the {@link MultipleInputGroup} of the outputs if all outputs have a\n+\t *         group and are the same, null otherwise\n+\t */\n+\tprivate MultipleInputGroup canBeInSameGroupWithOutputs(ExecNodeWrapper wrapper) {\n+\t\tif (wrapper.outputs.isEmpty()) {\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\tMultipleInputGroup outputGroup = wrapper.outputs.get(0).group;\n+\t\tif (outputGroup == null) {\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\tfor (ExecNodeWrapper outputWrapper : wrapper.outputs) {\n+\t\t\tif (outputWrapper.group != outputGroup) {\n+\t\t\t\treturn null;\n+\t\t\t}\n+\t\t}\n+\n+\t\treturn outputGroup;\n+\t}\n+\n+\tprivate boolean canBeRootOfMultipleInputGroup(ExecNodeWrapper wrapper) {\n+\t\t// only a node with more than one input can be the root,\n+\t\t// as one-input operator chaining are handled by operator chains\n+\t\treturn wrapper.inputs.size() >= 2;\n+\t}\n+\n+\t// --------------------------------------------------------------------------------\n+\t// Multiple Input Groups Optimizing\n+\t// --------------------------------------------------------------------------------\n+\n+\tprivate void optimizeMultipleInputGroups(List<ExecNodeWrapper> orderedWrappers) {\n+\t\t// wrappers are checked in topological order from sources to sinks\n+\t\tfor (int i = orderedWrappers.size() - 1; i >= 0; i--) {\n+\t\t\tExecNodeWrapper wrapper = orderedWrappers.get(i);\n+\t\t\tMultipleInputGroup group = wrapper.group;\n+\t\t\tif (group == null) {\n+\t\t\t\t// we only consider nodes currently in a multiple input group\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\tboolean isUnion =\n+\t\t\t\twrapper.execNode instanceof BatchExecUnion || wrapper.execNode instanceof StreamExecUnion;\n+\n+\t\t\tif (group.members.size() == 1) {\n+\t\t\t\tPreconditions.checkState(\n+\t\t\t\t\twrapper == group.root,\n+\t\t\t\t\t\"The only member of a multiple input group is not its root. This is a bug.\");\n+\t\t\t\t// optimization 1. we clean up multiple input groups with only 1 member,\n+\t\t\t\t// unless one of its input is a FLIP-27 source (for maximizing source chaining),\n+\t\t\t\t// however unions do not apply to this optimization because they're not real operators\n+\t\t\t\tif (isUnion || wrapper.inputs.stream().noneMatch(inputWrapper -> isNewSource(inputWrapper.execNode))) {\n+\t\t\t\t\twrapper.removeFromGroup();\n+\t\t\t\t}\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\tif (!isTailOfMultipleInputGroup(wrapper)) {\n+\t\t\t\t// we're not removing a node from the middle of a multiple input group\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\tboolean shouldRemove = false;\n+\t\t\tif (isUnion) {\n+\t\t\t\t// optimization 2. we do not allow union to be the tail of a multiple input\n+\t\t\t\t// as we're paying extra function calls for this, unless one of the united\n+\t\t\t\t// input is a FLIP-27 source\n+\t\t\t\tshouldRemove = wrapper.inputs.stream().noneMatch(inputWrapper -> isNewSource(inputWrapper.execNode));\n+\t\t\t} else if (wrapper.inputs.size() == 1) {\n+\t\t\t\t// optimization 3. for one-input operators we'll remove it unless its input\n+\t\t\t\t// is an exchange or a FLIP-27 source, this is mainly to avoid the following\n+\t\t\t\t// pattern:\n+\t\t\t\t// non-chainable source -> calc --\\\n+\t\t\t\t//                                 join ->\n+\t\t\t\t// non-chainable source -> calc --/\n+\t\t\t\t// if we move two calcs into the multiple input group rooted at the join, we're\n+\t\t\t\t// directly shuffling large amount of records from the source without filtering\n+\t\t\t\t// by the calc\n+\t\t\t\tExecNode<?, ?> input = wrapper.inputs.get(0).execNode;\n+\t\t\t\tshouldRemove = !(input instanceof BatchExecExchange) &&\n+\t\t\t\t\t!(input instanceof StreamExecExchange) &&\n+\t\t\t\t\t!isNewSource(input);\n+\t\t\t}\n+\n+\t\t\t// optimization 4. for singleton operations (for example singleton global agg)\n+\t\t\t// we're not including it into the multiple input node as we have to ensure that\n+\t\t\t// the whole multiple input can only have 1 parallelism.\n+\t\t\t// continuous singleton operations connected by forwarding shuffle will be dealt\n+\t\t\t// together with optimization 3\n+\t\t\tshouldRemove |= wrapper.inputs.stream().anyMatch(inputWrapper ->\n+\t\t\t\tinputWrapper.execNode instanceof BatchExecExchange &&\n+\t\t\t\t\t((BatchExecExchange) inputWrapper.execNode)\n+\t\t\t\t\t\t.distribution.getType() == RelDistribution.Type.SINGLETON);\n+\n+\t\t\tif (shouldRemove) {\n+\t\t\t\twrapper.removeFromGroup();\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate boolean isTailOfMultipleInputGroup(ExecNodeWrapper wrapper) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTYwMzUwNw=="}, "originalCommit": {"oid": "3c50597400946ab21e7dba24673a0b5df30eaf2b"}, "originalPosition": 293}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNzI5Mzk4OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/TopologyGraph.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxMjowODowNVrOHoMmqQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxMjowODowNVrOHoMmqQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTkxMTU5Mw==", "bodyText": "order -> distance", "url": "https://github.com/apache/flink/pull/13742#discussion_r511911593", "createdAt": "2020-10-26T12:08:05Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/TopologyGraph.java", "diffHunk": "@@ -0,0 +1,206 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.reuse;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.table.planner.plan.nodes.exec.AbstractExecNodeExactlyOnceVisitor;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecBoundedStreamScan;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.Set;\n+\n+/**\n+ * A data structure storing the topological and input priority information of an {@link ExecNode} graph.\n+ */\n+@Internal\n+class TopologyGraph {\n+\n+\tprivate final Map<ExecNode<?, ?>, TopologyNode> nodes;\n+\n+\tTopologyGraph(List<ExecNode<?, ?>> roots) {\n+\t\tthis(roots, Collections.emptySet());\n+\t}\n+\n+\tTopologyGraph(List<ExecNode<?, ?>> roots, Set<ExecNode<?, ?>> boundaries) {\n+\t\tthis.nodes = new HashMap<>();\n+\n+\t\t// we first link all edges in the original exec node graph\n+\t\tAbstractExecNodeExactlyOnceVisitor visitor = new AbstractExecNodeExactlyOnceVisitor() {\n+\t\t\t@Override\n+\t\t\tprotected void visitNode(ExecNode<?, ?> node) {\n+\t\t\t\tif (boundaries.contains(node)) {\n+\t\t\t\t\treturn;\n+\t\t\t\t}\n+\t\t\t\tfor (ExecNode<?, ?> input : node.getInputNodes()) {\n+\t\t\t\t\tlink(input, node);\n+\t\t\t\t}\n+\t\t\t\tvisitInputs(node);\n+\t\t\t}\n+\t\t};\n+\t\troots.forEach(n -> n.accept(visitor));\n+\t}\n+\n+\t/**\n+\t * Link an edge from `from` node to `to` node if no loop will occur after adding this edge.\n+\t * Returns if this edge is successfully added.\n+\t */\n+\tboolean link(ExecNode<?, ?> from, ExecNode<?, ?> to) {\n+\t\tTopologyNode fromNode = getTopologyNode(from);\n+\t\tTopologyNode toNode = getTopologyNode(to);\n+\n+\t\tif (canReach(toNode, fromNode)) {\n+\t\t\t// invalid edge, as `to` is the predecessor of `from`\n+\t\t\treturn false;\n+\t\t} else {\n+\t\t\t// link `from` and `to`\n+\t\t\tfromNode.outputs.add(toNode);\n+\t\t\ttoNode.inputs.add(fromNode);\n+\t\t\treturn true;\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * Remove the edge from `from` node to `to` node. If there is no edge between them then do nothing.\n+\t */\n+\tvoid unlink(ExecNode<?, ?> from, ExecNode<?, ?> to) {\n+\t\tTopologyNode fromNode = getTopologyNode(from);\n+\t\tTopologyNode toNode = getTopologyNode(to);\n+\n+\t\tfromNode.outputs.remove(toNode);\n+\t\ttoNode.inputs.remove(fromNode);\n+\t}\n+\n+\t/**\n+\t * Calculate the maximum distance of the currently added nodes from the nodes without inputs.\n+\t * The smallest order is 0 (which are exactly the nodes without inputs) and the distances of", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1121f5b1525294821397a62519e33f21c01a097a"}, "originalPosition": 101}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwODIzNzA2OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/AbstractInputPriorityConflictResolver.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxNTozOTozNFrOHoVntA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxNTozOTozNFrOHoVntA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjA1OTMxNg==", "bodyText": "InputPriorityBasedTopologyGraphGenerator ?  even we can simplified it as TopologyGraphGenerator", "url": "https://github.com/apache/flink/pull/13742#discussion_r512059316", "createdAt": "2020-10-26T15:39:34Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/AbstractInputPriorityConflictResolver.java", "diffHunk": "@@ -88,41 +76,58 @@\n  *\n  * <p>This class maintains a topological graph in which an edge pointing from vertex A to vertex B indicates\n  * that the results from vertex A need to be read before those from vertex B. A loop in the graph indicates\n- * a deadlock, and we resolve such deadlock by inserting a {@link BatchExecExchange} with batch shuffle mode.\n+ * a deadlock, and different subclasses of this class resolve the conflict in different ways.\n  *\n  * <p>For a detailed explanation of the algorithm, see appendix of the\n  * <a href=\"https://docs.google.com/document/d/1qKVohV12qn-bM51cBZ8Hcgp31ntwClxjoiNBUOqVHsI\">design doc</a>.\n  */\n @Internal\n-public class InputPriorityConflictResolver {\n+public abstract class AbstractInputPriorityConflictResolver {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1121f5b1525294821397a62519e33f21c01a097a"}, "originalPosition": 39}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwODI1NzIwOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/AbstractInputPriorityConflictResolver.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxNTo0Mzo0NFrOHoV0TQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxNTo0Mzo0NFrOHoV0TQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjA2MjU0MQ==", "bodyText": "createTopologyGraph?", "url": "https://github.com/apache/flink/pull/13742#discussion_r512062541", "createdAt": "2020-10-26T15:43:44Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/AbstractInputPriorityConflictResolver.java", "diffHunk": "@@ -88,41 +76,58 @@\n  *\n  * <p>This class maintains a topological graph in which an edge pointing from vertex A to vertex B indicates\n  * that the results from vertex A need to be read before those from vertex B. A loop in the graph indicates\n- * a deadlock, and we resolve such deadlock by inserting a {@link BatchExecExchange} with batch shuffle mode.\n+ * a deadlock, and different subclasses of this class resolve the conflict in different ways.\n  *\n  * <p>For a detailed explanation of the algorithm, see appendix of the\n  * <a href=\"https://docs.google.com/document/d/1qKVohV12qn-bM51cBZ8Hcgp31ntwClxjoiNBUOqVHsI\">design doc</a>.\n  */\n @Internal\n-public class InputPriorityConflictResolver {\n+public abstract class AbstractInputPriorityConflictResolver {\n \n \tprivate final List<ExecNode<?, ?>> roots;\n+\tprivate final Set<ExecNode<?, ?>> boundaries;\n+\tprivate final ExecEdge.DamBehavior safeDamBehavior;\n \n-\tprivate TopologyGraph graph;\n+\tprotected TopologyGraph graph;\n \n-\tpublic InputPriorityConflictResolver(List<ExecNode<?, ?>> roots) {\n+\t/**\n+\t * Create an {@link AbstractInputPriorityConflictResolver} for the given {@link ExecNode} sub-graph.\n+\t *\n+\t * @param roots the first layer of nodes on the output side of the sub-graph\n+\t * @param boundaries the first layer of nodes on the input side of the sub-graph\n+\t * @param safeDamBehavior when checking for conflicts we'll ignore the edges with\n+\t *                        {@link ExecEdge.DamBehavior} stricter or equal than this\n+\t */\n+\tpublic AbstractInputPriorityConflictResolver(\n+\t\t\tList<ExecNode<?, ?>> roots,\n+\t\t\tSet<ExecNode<?, ?>> boundaries,\n+\t\t\tExecEdge.DamBehavior safeDamBehavior) {\n \t\tPreconditions.checkArgument(\n \t\t\troots.stream().allMatch(root -> root instanceof BatchExecNode),\n \t\t\t\"InputPriorityConflictResolver can only be used for batch jobs.\");\n \t\tthis.roots = roots;\n+\t\tthis.boundaries = boundaries;\n+\t\tthis.safeDamBehavior = safeDamBehavior;\n \t}\n \n-\tpublic void detectAndResolve() {\n+\tprotected void createTopologyGraphAndResolveConflict() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1121f5b1525294821397a62519e33f21c01a097a"}, "originalPosition": 70}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwODI2NDAxOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/AbstractInputPriorityConflictResolver.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxNTo0NTowNlrOHoV4kw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxNTo0NTowNlrOHoV4kw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjA2MzYzNQ==", "bodyText": "resolveInputPriorityConflict", "url": "https://github.com/apache/flink/pull/13742#discussion_r512063635", "createdAt": "2020-10-26T15:45:06Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/AbstractInputPriorityConflictResolver.java", "diffHunk": "@@ -205,146 +205,5 @@ protected void visitNode(ExecNode<?, ?> node) {\n \t\treturn ret;\n \t}\n \n-\tprivate BatchExecExchange createExchange(ExecNode<?, ?> node, int idx) {\n-\t\tRelNode inputRel = (RelNode) node.getInputNodes().get(idx);\n-\n-\t\tFlinkRelDistribution distribution;\n-\t\tExecEdge.RequiredShuffle requiredShuffle = node.getInputEdges().get(idx).getRequiredShuffle();\n-\t\tif (requiredShuffle.getType() == ExecEdge.ShuffleType.HASH) {\n-\t\t\tdistribution = FlinkRelDistribution.hash(requiredShuffle.getKeys(), true);\n-\t\t} else if (requiredShuffle.getType() == ExecEdge.ShuffleType.BROADCAST) {\n-\t\t\t// should not occur\n-\t\t\tthrow new IllegalStateException(\n-\t\t\t\t\"Trying to resolve input priority conflict on broadcast side. This is not expected.\");\n-\t\t} else if (requiredShuffle.getType() == ExecEdge.ShuffleType.SINGLETON) {\n-\t\t\tdistribution = FlinkRelDistribution.SINGLETON();\n-\t\t} else {\n-\t\t\tdistribution = FlinkRelDistribution.ANY();\n-\t\t}\n-\n-\t\tBatchExecExchange exchange = new BatchExecExchange(\n-\t\t\tinputRel.getCluster(),\n-\t\t\tinputRel.getTraitSet().replace(distribution),\n-\t\t\tinputRel,\n-\t\t\tdistribution);\n-\t\texchange.setRequiredShuffleMode(ShuffleMode.BATCH);\n-\t\treturn exchange;\n-\t}\n-\n-\t/**\n-\t * A data structure storing the topological information of an {@link ExecNode} graph.\n-\t */\n-\t@VisibleForTesting\n-\tstatic class TopologyGraph {\n-\t\tprivate final Map<ExecNode<?, ?>, TopologyNode> nodes;\n-\n-\t\tTopologyGraph(List<ExecNode<?, ?>> roots) {\n-\t\t\tthis.nodes = new HashMap<>();\n-\n-\t\t\t// we first link all edges in the original exec node graph\n-\t\t\tAbstractExecNodeExactlyOnceVisitor visitor = new AbstractExecNodeExactlyOnceVisitor() {\n-\t\t\t\t@Override\n-\t\t\t\tprotected void visitNode(ExecNode<?, ?> node) {\n-\t\t\t\t\tfor (ExecNode<?, ?> input : node.getInputNodes()) {\n-\t\t\t\t\t\tlink(input, node);\n-\t\t\t\t\t}\n-\t\t\t\t\tvisitInputs(node);\n-\t\t\t\t}\n-\t\t\t};\n-\t\t\troots.forEach(n -> n.accept(visitor));\n-\t\t}\n-\n-\t\t/**\n-\t\t * Link an edge from `from` node to `to` node if no loop will occur after adding this edge.\n-\t\t * Returns if this edge is successfully added.\n-\t\t */\n-\t\tboolean link(ExecNode<?, ?> from, ExecNode<?, ?> to) {\n-\t\t\tTopologyNode fromNode = getTopologyNode(from);\n-\t\t\tTopologyNode toNode = getTopologyNode(to);\n-\n-\t\t\tif (canReach(toNode, fromNode)) {\n-\t\t\t\t// invalid edge, as `to` is the predecessor of `from`\n-\t\t\t\treturn false;\n-\t\t\t} else {\n-\t\t\t\t// link `from` and `to`\n-\t\t\t\tfromNode.outputs.add(toNode);\n-\t\t\t\ttoNode.inputs.add(fromNode);\n-\t\t\t\treturn true;\n-\t\t\t}\n-\t\t}\n-\n-\t\t/**\n-\t\t * Remove the edge from `from` node to `to` node. If there is no edge between them then do nothing.\n-\t\t */\n-\t\tvoid unlink(ExecNode<?, ?> from, ExecNode<?, ?> to) {\n-\t\t\tTopologyNode fromNode = getTopologyNode(from);\n-\t\t\tTopologyNode toNode = getTopologyNode(to);\n-\n-\t\t\tfromNode.outputs.remove(toNode);\n-\t\t\ttoNode.inputs.remove(fromNode);\n-\t\t}\n-\n-\t\t@VisibleForTesting\n-\t\tboolean canReach(ExecNode<?, ?> from, ExecNode<?, ?> to) {\n-\t\t\tTopologyNode fromNode = getTopologyNode(from);\n-\t\t\tTopologyNode toNode = getTopologyNode(to);\n-\t\t\treturn canReach(fromNode, toNode);\n-\t\t}\n-\n-\t\tprivate boolean canReach(TopologyNode from, TopologyNode to) {\n-\t\t\tSet<TopologyNode> visited = new HashSet<>();\n-\t\t\tvisited.add(from);\n-\t\t\tQueue<TopologyNode> queue = new LinkedList<>();\n-\t\t\tqueue.offer(from);\n-\n-\t\t\twhile (!queue.isEmpty()) {\n-\t\t\t\tTopologyNode node = queue.poll();\n-\t\t\t\tif (to.equals(node)) {\n-\t\t\t\t\treturn true;\n-\t\t\t\t}\n-\n-\t\t\t\tfor (TopologyNode next : node.outputs) {\n-\t\t\t\t\tif (visited.contains(next)) {\n-\t\t\t\t\t\tcontinue;\n-\t\t\t\t\t}\n-\t\t\t\t\tvisited.add(next);\n-\t\t\t\t\tqueue.offer(next);\n-\t\t\t\t}\n-\t\t\t}\n-\n-\t\t\treturn false;\n-\t\t}\n-\n-\t\tprivate TopologyNode getTopologyNode(ExecNode<?, ?> execNode) {\n-\t\t\t// NOTE: We treat different `BatchExecBoundedStreamScan`s with same `DataStream` object as the same\n-\t\t\tif (execNode instanceof BatchExecBoundedStreamScan) {\n-\t\t\t\tDataStream<?> currentStream =\n-\t\t\t\t\t((BatchExecBoundedStreamScan) execNode).boundedStreamTable().dataStream();\n-\t\t\t\tfor (Map.Entry<ExecNode<?, ?>, TopologyNode> entry : nodes.entrySet()) {\n-\t\t\t\t\tExecNode<?, ?> key = entry.getKey();\n-\t\t\t\t\tif (key instanceof BatchExecBoundedStreamScan) {\n-\t\t\t\t\t\tDataStream<?> existingStream =\n-\t\t\t\t\t\t\t((BatchExecBoundedStreamScan) key).boundedStreamTable().dataStream();\n-\t\t\t\t\t\tif (existingStream.equals(currentStream)) {\n-\t\t\t\t\t\t\treturn entry.getValue();\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\n-\t\t\t\tTopologyNode result = new TopologyNode();\n-\t\t\t\tnodes.put(execNode, result);\n-\t\t\t\treturn result;\n-\t\t\t} else {\n-\t\t\t\treturn nodes.computeIfAbsent(execNode, k -> new TopologyNode());\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\t/**\n-\t * A node in the {@link TopologyGraph}.\n-\t */\n-\tprivate static class TopologyNode {\n-\t\tprivate final Set<TopologyNode> inputs = new HashSet<>();\n-\t\tprivate final Set<TopologyNode> outputs = new HashSet<>();\n-\t}\n+\tprotected abstract void resolveConflict(ExecNode<?, ?> node, int conflictInput);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1121f5b1525294821397a62519e33f21c01a097a"}, "originalPosition": 285}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwODI2NzY3OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/AbstractInputPriorityConflictResolver.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxNTo0NTo1MVrOHoV64g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxNTo0NTo1MVrOHoV64g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjA2NDIyNg==", "bodyText": "updateTopologyGraph", "url": "https://github.com/apache/flink/pull/13742#discussion_r512064226", "createdAt": "2020-10-26T15:45:51Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/AbstractInputPriorityConflictResolver.java", "diffHunk": "@@ -88,41 +76,58 @@\n  *\n  * <p>This class maintains a topological graph in which an edge pointing from vertex A to vertex B indicates\n  * that the results from vertex A need to be read before those from vertex B. A loop in the graph indicates\n- * a deadlock, and we resolve such deadlock by inserting a {@link BatchExecExchange} with batch shuffle mode.\n+ * a deadlock, and different subclasses of this class resolve the conflict in different ways.\n  *\n  * <p>For a detailed explanation of the algorithm, see appendix of the\n  * <a href=\"https://docs.google.com/document/d/1qKVohV12qn-bM51cBZ8Hcgp31ntwClxjoiNBUOqVHsI\">design doc</a>.\n  */\n @Internal\n-public class InputPriorityConflictResolver {\n+public abstract class AbstractInputPriorityConflictResolver {\n \n \tprivate final List<ExecNode<?, ?>> roots;\n+\tprivate final Set<ExecNode<?, ?>> boundaries;\n+\tprivate final ExecEdge.DamBehavior safeDamBehavior;\n \n-\tprivate TopologyGraph graph;\n+\tprotected TopologyGraph graph;\n \n-\tpublic InputPriorityConflictResolver(List<ExecNode<?, ?>> roots) {\n+\t/**\n+\t * Create an {@link AbstractInputPriorityConflictResolver} for the given {@link ExecNode} sub-graph.\n+\t *\n+\t * @param roots the first layer of nodes on the output side of the sub-graph\n+\t * @param boundaries the first layer of nodes on the input side of the sub-graph\n+\t * @param safeDamBehavior when checking for conflicts we'll ignore the edges with\n+\t *                        {@link ExecEdge.DamBehavior} stricter or equal than this\n+\t */\n+\tpublic AbstractInputPriorityConflictResolver(\n+\t\t\tList<ExecNode<?, ?>> roots,\n+\t\t\tSet<ExecNode<?, ?>> boundaries,\n+\t\t\tExecEdge.DamBehavior safeDamBehavior) {\n \t\tPreconditions.checkArgument(\n \t\t\troots.stream().allMatch(root -> root instanceof BatchExecNode),\n \t\t\t\"InputPriorityConflictResolver can only be used for batch jobs.\");\n \t\tthis.roots = roots;\n+\t\tthis.boundaries = boundaries;\n+\t\tthis.safeDamBehavior = safeDamBehavior;\n \t}\n \n-\tpublic void detectAndResolve() {\n+\tprotected void createTopologyGraphAndResolveConflict() {\n \t\t// build an initial topology graph\n-\t\tgraph = new TopologyGraph(roots);\n+\t\tgraph = new TopologyGraph(roots, boundaries);\n \n \t\t// check and resolve conflicts about input priorities\n \t\tAbstractExecNodeExactlyOnceVisitor inputPriorityVisitor = new AbstractExecNodeExactlyOnceVisitor() {\n \t\t\t@Override\n \t\t\tprotected void visitNode(ExecNode<?, ?> node) {\n-\t\t\t\tvisitInputs(node);\n-\t\t\t\tcheckInputPriorities(node);\n+\t\t\t\tif (!boundaries.contains(node)) {\n+\t\t\t\t\tvisitInputs(node);\n+\t\t\t\t}\n+\t\t\t\tupdateTopologyGraphAndResolveConflict(node);\n \t\t\t}\n \t\t};\n \t\troots.forEach(n -> n.accept(inputPriorityVisitor));\n \t}\n \n-\tprivate void checkInputPriorities(ExecNode<?, ?> node) {\n+\tprivate void updateTopologyGraphAndResolveConflict(ExecNode<?, ?> node) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1121f5b1525294821397a62519e33f21c01a097a"}, "originalPosition": 91}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwODI2OTIzOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/InputPriorityConflictResolverWithExchange.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxNTo0NjowOVrOHoV70g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxNTo0NjowOVrOHoV70g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjA2NDQ2Ng==", "bodyText": "InputPriorityConflictResolver", "url": "https://github.com/apache/flink/pull/13742#discussion_r512064466", "createdAt": "2020-10-26T15:46:09Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/InputPriorityConflictResolverWithExchange.java", "diffHunk": "@@ -0,0 +1,100 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.reuse;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.streaming.api.transformations.ShuffleMode;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecEdge;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecExchange;\n+import org.apache.flink.table.planner.plan.trait.FlinkRelDistribution;\n+\n+import org.apache.calcite.rel.RelNode;\n+\n+import java.util.Collections;\n+import java.util.List;\n+\n+/**\n+ * Subclass of the {@link AbstractInputPriorityConflictResolver}.\n+ *\n+ * <p>This class resolve conflicts by inserting a {@link BatchExecExchange} into the conflicting input.\n+ */\n+@Internal\n+public class InputPriorityConflictResolverWithExchange extends AbstractInputPriorityConflictResolver {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1121f5b1525294821397a62519e33f21c01a097a"}, "originalPosition": 39}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwODI4NzQ5OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/InputOrderCalculator.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxNTo0OTo0NlrOHoWHIw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxNTo0OTo0NlrOHoWHIw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjA2NzM2Mw==", "bodyText": "the package name is not correct", "url": "https://github.com/apache/flink/pull/13742#discussion_r512067363", "createdAt": "2020-10-26T15:49:46Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/InputOrderCalculator.java", "diffHunk": "@@ -0,0 +1,85 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.reuse;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1121f5b1525294821397a62519e33f21c01a097a"}, "originalPosition": 19}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwODMwNzA5OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecMultipleInputNode.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxNTo1MzozOFrOHoWTGQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QwMzoxMzowOFrOHop4rA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjA3MDQyNQ==", "bodyText": "what if we always set the ChainingStrategy as HEAD_WITH_SOURCES  ?", "url": "https://github.com/apache/flink/pull/13742#discussion_r512070425", "createdAt": "2020-10-26T15:53:38Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecMultipleInputNode.scala", "diffHunk": "@@ -99,6 +100,14 @@ class BatchExecMultipleInputNode(\n     val memoryKB = generator.getManagedMemoryWeight\n     ExecNode.setManagedMemoryWeight(multipleInputTransform, memoryKB * 1024)\n \n+    if (withSourceChaining) {\n+      // set chaining strategy for source chaining\n+      multipleInputTransform.setChainingStrategy(ChainingStrategy.HEAD_WITH_SOURCES)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1121f5b1525294821397a62519e33f21c01a097a"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjM5MTM0MA==", "bodyText": "It's OK. I just want to be more precise.", "url": "https://github.com/apache/flink/pull/13742#discussion_r512391340", "createdAt": "2020-10-27T03:13:08Z", "author": {"login": "tsreaper"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecMultipleInputNode.scala", "diffHunk": "@@ -99,6 +100,14 @@ class BatchExecMultipleInputNode(\n     val memoryKB = generator.getManagedMemoryWeight\n     ExecNode.setManagedMemoryWeight(multipleInputTransform, memoryKB * 1024)\n \n+    if (withSourceChaining) {\n+      // set chaining strategy for source chaining\n+      multipleInputTransform.setChainingStrategy(ChainingStrategy.HEAD_WITH_SOURCES)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjA3MDQyNQ=="}, "originalCommit": {"oid": "1121f5b1525294821397a62519e33f21c01a097a"}, "originalPosition": 31}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIxMDQ5MjExOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/processor/DeadlockBreakupProcessor.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QwNDoxNzoyNlrOHoq3CQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QwNDoxNzoyNlrOHoq3CQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjQwNzMwNQ==", "bodyText": "move it to org.apache.flink.table.planner.plan.nodes.process", "url": "https://github.com/apache/flink/pull/13742#discussion_r512407305", "createdAt": "2020-10-27T04:17:26Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/processor/DeadlockBreakupProcessor.java", "diffHunk": "@@ -0,0 +1,52 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.processor;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "006756c81fa5c333b6fdc4bcbfc8820492a18e9b"}, "originalPosition": 19}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIxMDQ5NDA4OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecMultipleInputNode.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QwNDoxODo0NVrOHoq4KA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QwNDoxODo0NVrOHoq4KA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjQwNzU5Mg==", "bodyText": "reorder the import", "url": "https://github.com/apache/flink/pull/13742#discussion_r512407592", "createdAt": "2020-10-27T04:18:45Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecMultipleInputNode.scala", "diffHunk": "@@ -27,12 +27,12 @@ import org.apache.flink.table.planner.plan.nodes.exec.{BatchExecNode, ExecEdge,\n import org.apache.flink.table.planner.plan.nodes.physical.MultipleInputRel\n import org.apache.flink.table.runtime.operators.multipleinput.{BatchMultipleInputStreamOperatorFactory, TableOperatorWrapperGenerator}\n import org.apache.flink.table.runtime.typeutils.InternalTypeInfo\n-\n import org.apache.calcite.plan.{RelOptCluster, RelTraitSet}\n import org.apache.calcite.rel.RelNode\n-\n import java.util\n \n+import org.apache.flink.streaming.api.operators.ChainingStrategy", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "006756c81fa5c333b6fdc4bcbfc8820492a18e9b"}, "originalPosition": 10}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIxMDUwNjA0OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/plan/processor/utils/InputPriorityConflictResolverTest.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QwNDoyNjoxMVrOHoq-4g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QwNDoyNjoxMVrOHoq-4g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjQwOTMxNA==", "bodyText": "also check the shuffle mode ?", "url": "https://github.com/apache/flink/pull/13742#discussion_r512409314", "createdAt": "2020-10-27T04:26:11Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/plan/processor/utils/InputPriorityConflictResolverTest.java", "diffHunk": "@@ -0,0 +1,82 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.processor.utils;\n+\n+import org.apache.flink.streaming.api.transformations.ShuffleMode;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecEdge;\n+import org.apache.flink.table.planner.plan.nodes.exec.TestingBatchExecNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecExchange;\n+\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.util.Collections;\n+\n+/**\n+ * Tests for {@link InputPriorityConflictResolver}.\n+ */\n+public class InputPriorityConflictResolverTest {\n+\n+\t@Test\n+\tpublic void testDetectAndResolve() {\n+\t\t// P = ExecEdge.DamBehavior.PIPELINED, E = ExecEdge.DamBehavior.END_INPUT\n+\t\t// P100 = PIPELINED + priority 100\n+\t\t//\n+\t\t// 0 --------(P0)----> 1 --(P0)-----------> 7\n+\t\t//  \\                    \\-(P0)-> 2 -(P0)--/\n+\t\t//   \\-------(P0)----> 3 --(P1)-----------/\n+\t\t//    \\------(P0)----> 4 --(P10)---------/\n+\t\t//     \\              /                 /\n+\t\t//      \\    8 -(P0)-<                 /\n+\t\t//       \\            \\               /\n+\t\t//        \\--(E0)----> 5 --(P10)-----/\n+\t\t// 6 ---------(P100)----------------/\n+\t\tTestingBatchExecNode[] nodes = new TestingBatchExecNode[9];\n+\t\tfor (int i = 0; i < nodes.length; i++) {\n+\t\t\tnodes[i] = new TestingBatchExecNode();\n+\t\t}\n+\t\tnodes[1].addInput(nodes[0], ExecEdge.builder().priority(0).build());\n+\t\tnodes[2].addInput(nodes[1], ExecEdge.builder().priority(0).build());\n+\t\tnodes[3].addInput(nodes[0], ExecEdge.builder().priority(0).build());\n+\t\tnodes[4].addInput(nodes[8], ExecEdge.builder().priority(0).build());\n+\t\tnodes[4].addInput(nodes[0], ExecEdge.builder().priority(0).build());\n+\t\tnodes[5].addInput(nodes[8], ExecEdge.builder().priority(0).build());\n+\t\tnodes[5].addInput(nodes[0], ExecEdge.builder().damBehavior(ExecEdge.DamBehavior.END_INPUT).priority(0).build());\n+\t\tnodes[7].addInput(nodes[1], ExecEdge.builder().priority(0).build());\n+\t\tnodes[7].addInput(nodes[2], ExecEdge.builder().priority(0).build());\n+\t\tnodes[7].addInput(nodes[3], ExecEdge.builder().priority(1).build());\n+\t\tnodes[7].addInput(nodes[4], ExecEdge.builder().priority(10).build());\n+\t\tnodes[7].addInput(nodes[5], ExecEdge.builder().priority(10).build());\n+\t\tnodes[7].addInput(nodes[6], ExecEdge.builder().priority(100).build());\n+\n+\t\tInputPriorityConflictResolver resolver = new InputPriorityConflictResolver(\n+\t\t\tCollections.singletonList(nodes[7]),\n+\t\t\tExecEdge.DamBehavior.END_INPUT,\n+\t\t\tShuffleMode.BATCH);\n+\t\tresolver.detectAndResolve();\n+\t\tAssert.assertEquals(nodes[1], nodes[7].getInputNodes().get(0));\n+\t\tAssert.assertEquals(nodes[2], nodes[7].getInputNodes().get(1));\n+\t\tAssert.assertTrue(nodes[7].getInputNodes().get(2) instanceof BatchExecExchange);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "006756c81fa5c333b6fdc4bcbfc8820492a18e9b"}, "originalPosition": 75}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIxMDUyNjAwOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/processor/MultipleInputNodeCreationProcessor.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QwNDozOTowMFrOHorKVA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QwNDozOTowMFrOHorKVA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjQxMjI0NA==", "bodyText": "should also consider StreamExecExchange here? BatchExecExchange -> Exchange", "url": "https://github.com/apache/flink/pull/13742#discussion_r512412244", "createdAt": "2020-10-27T04:39:00Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/processor/MultipleInputNodeCreationProcessor.java", "diffHunk": "@@ -0,0 +1,484 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.processor;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.streaming.api.transformations.ShuffleMode;\n+import org.apache.flink.streaming.api.transformations.SourceTransformation;\n+import org.apache.flink.table.connector.source.SourceProvider;\n+import org.apache.flink.table.planner.plan.nodes.common.CommonPhysicalTableSourceScan;\n+import org.apache.flink.table.planner.plan.nodes.exec.AbstractExecNodeExactlyOnceVisitor;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecEdge;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecBoundedStreamScan;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecExchange;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecMultipleInputNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecDataStreamScan;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecMultipleInputNode;\n+import org.apache.flink.table.planner.plan.nodes.process.DAGProcessContext;\n+import org.apache.flink.table.planner.plan.nodes.process.DAGProcessor;\n+import org.apache.flink.table.planner.plan.processor.utils.InputOrderCalculator;\n+import org.apache.flink.table.planner.plan.processor.utils.InputPriorityConflictResolver;\n+import org.apache.flink.table.runtime.connector.source.ScanRuntimeProviderContext;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.calcite.rel.RelDistribution;\n+import org.apache.calcite.rel.RelNode;\n+import org.apache.calcite.rel.core.Exchange;\n+import org.apache.calcite.rel.core.Union;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.Set;\n+\n+/**\n+ * A {@link DAGProcessor} which organize {@link ExecNode}s into multiple input nodes.\n+ *\n+ * <p>For a detailed explanation of the algorithm, see appendix of the\n+ * <a href=\"https://docs.google.com/document/d/1qKVohV12qn-bM51cBZ8Hcgp31ntwClxjoiNBUOqVHsI\">design doc</a>.\n+ */\n+public class MultipleInputNodeCreationProcessor implements DAGProcessor {\n+\n+\tprivate final boolean isStreaming;\n+\n+\tpublic MultipleInputNodeCreationProcessor(boolean isStreaming) {\n+\t\tthis.isStreaming = isStreaming;\n+\t}\n+\n+\t@Override\n+\tpublic List<ExecNode<?, ?>> process(List<ExecNode<?, ?>> roots, DAGProcessContext context) {\n+\t\tif (!isStreaming) {\n+\t\t\t// As multiple input nodes use function call to deliver records between sub-operators,\n+\t\t\t// we cannot rely on network buffers to buffer records not yet ready to be read,\n+\t\t\t// so only BLOCKING dam behavior is safe here.\n+\t\t\t// If conflict is detected under this stricter constraint,\n+\t\t\t// we add a PIPELINED exchange to mark that its input and output node cannot be merged\n+\t\t\t// into the same multiple input node\n+\t\t\tInputPriorityConflictResolver resolver = new InputPriorityConflictResolver(\n+\t\t\t\troots,\n+\t\t\t\tExecEdge.DamBehavior.BLOCKING,\n+\t\t\t\tShuffleMode.PIPELINED);\n+\t\t\tresolver.detectAndResolve();\n+\t\t}\n+\n+\t\tList<ExecNodeWrapper> rootWrappers = wrapExecNodes(roots);\n+\t\t// sort all nodes in topological order, sinks come first and sources come last\n+\t\tList<ExecNodeWrapper> orderedWrappers = topologicalSort(rootWrappers);\n+\t\t// group nodes into multiple input groups\n+\t\tcreateMultipleInputGroups(orderedWrappers);\n+\t\t// apply optimizations to remove unnecessary nodes out of multiple input groups\n+\t\toptimizeMultipleInputGroups(orderedWrappers);\n+\n+\t\t// create the real multiple input nodes\n+\t\treturn createMultipleInputNodes(rootWrappers);\n+\t}\n+\n+\t// --------------------------------------------------------------------------------\n+\t// Wrapping and Sorting\n+\t// --------------------------------------------------------------------------------\n+\n+\tprivate List<ExecNodeWrapper> wrapExecNodes(List<ExecNode<?, ?>> rootNodes) {\n+\t\tMap<ExecNode<?, ?>, ExecNodeWrapper> wrapperMap = new HashMap<>();\n+\t\tAbstractExecNodeExactlyOnceVisitor visitor = new AbstractExecNodeExactlyOnceVisitor() {\n+\t\t\t@Override\n+\t\t\tprotected void visitNode(ExecNode<?, ?> node) {\n+\t\t\t\tExecNodeWrapper wrapper = wrapperMap.computeIfAbsent(node, k -> new ExecNodeWrapper(node));\n+\t\t\t\tfor (ExecNode<?, ?> input : node.getInputNodes()) {\n+\t\t\t\t\tExecNodeWrapper inputWrapper = wrapperMap.computeIfAbsent(input, k -> new ExecNodeWrapper(input));\n+\t\t\t\t\twrapper.inputs.add(inputWrapper);\n+\t\t\t\t\tinputWrapper.outputs.add(wrapper);\n+\t\t\t\t}\n+\t\t\t\tvisitInputs(node);\n+\t\t\t}\n+\t\t};\n+\t\trootNodes.forEach(s -> s.accept(visitor));\n+\n+\t\tList<ExecNodeWrapper> rootWrappers = new ArrayList<>();\n+\t\tfor (ExecNode<?, ?> root : rootNodes) {\n+\t\t\tExecNodeWrapper rootWrapper = wrapperMap.get(root);\n+\t\t\tPreconditions.checkNotNull(rootWrapper, \"Root node is not wrapped. This is a bug.\");\n+\t\t\trootWrappers.add(rootWrapper);\n+\t\t}\n+\t\treturn rootWrappers;\n+\t}\n+\n+\tprivate List<ExecNodeWrapper> topologicalSort(List<ExecNodeWrapper> rootWrappers) {\n+\t\tList<ExecNodeWrapper> result = new ArrayList<>();\n+\t\tQueue<ExecNodeWrapper> queue = new LinkedList<>(rootWrappers);\n+\t\tMap<ExecNodeWrapper, Integer> visitCountMap = new HashMap<>();\n+\n+\t\twhile (!queue.isEmpty()) {\n+\t\t\tExecNodeWrapper wrapper = queue.poll();\n+\t\t\tresult.add(wrapper);\n+\t\t\tfor (ExecNodeWrapper inputWrapper : wrapper.inputs) {\n+\t\t\t\tint visitCount = visitCountMap.compute(inputWrapper, (k, v) -> v == null ? 1 : v + 1);\n+\t\t\t\tif (visitCount == inputWrapper.outputs.size()) {\n+\t\t\t\t\tqueue.offer(inputWrapper);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\n+\t\treturn result;\n+\t}\n+\n+\t// --------------------------------------------------------------------------------\n+\t// Multiple Input Groups Creating\n+\t// --------------------------------------------------------------------------------\n+\n+\tprivate void createMultipleInputGroups(List<ExecNodeWrapper> orderedWrappers) {\n+\t\t// wrappers are checked in topological order from sinks to sources\n+\t\tfor (ExecNodeWrapper wrapper : orderedWrappers) {\n+\t\t\t// we skip nodes which cannot be a member of a multiple input node\n+\t\t\tif (!canBeMultipleInputNodeMember(wrapper)) {\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\t// we first try to assign this wrapper into the same group with its outputs\n+\t\t\tMultipleInputGroup outputGroup = canBeInSameGroupWithOutputs(wrapper);\n+\t\t\tif (outputGroup != null) {\n+\t\t\t\twrapper.addToGroup(outputGroup);\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\t// we then try to create a new multiple input group with this node as the root\n+\t\t\tif (canBeRootOfMultipleInputGroup(wrapper)) {\n+\t\t\t\twrapper.createGroup();\n+\t\t\t}\n+\n+\t\t\t// all our attempts failed, this node will not be in a multiple input node\n+\t\t}\n+\t}\n+\n+\tprivate boolean canBeMultipleInputNodeMember(ExecNodeWrapper wrapper) {\n+\t\tif (wrapper.inputs.isEmpty()) {\n+\t\t\t// sources cannot be a member of multiple input node\n+\t\t\treturn false;\n+\t\t}\n+\t\tif (wrapper.execNode instanceof Exchange) {\n+\t\t\t// exchange cannot be a member of multiple input node\n+\t\t\treturn false;\n+\t\t}\n+\n+\t\treturn true;\n+\t}\n+\n+\t/**\n+\t * A node can only be assigned into the same multiple input group of its outputs\n+\t * if all outputs have a group and are the same.\n+\t *\n+\t * @return the {@link MultipleInputGroup} of the outputs if all outputs have a\n+\t *         group and are the same, null otherwise\n+\t */\n+\tprivate MultipleInputGroup canBeInSameGroupWithOutputs(ExecNodeWrapper wrapper) {\n+\t\tif (wrapper.outputs.isEmpty()) {\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\tMultipleInputGroup outputGroup = wrapper.outputs.get(0).group;\n+\t\tif (outputGroup == null) {\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\tfor (ExecNodeWrapper outputWrapper : wrapper.outputs) {\n+\t\t\tif (outputWrapper.group != outputGroup) {\n+\t\t\t\treturn null;\n+\t\t\t}\n+\t\t}\n+\n+\t\treturn outputGroup;\n+\t}\n+\n+\tprivate boolean canBeRootOfMultipleInputGroup(ExecNodeWrapper wrapper) {\n+\t\t// only a node with more than one input can be the root,\n+\t\t// as one-input operator chaining are handled by operator chains\n+\t\treturn wrapper.inputs.size() >= 2;\n+\t}\n+\n+\t// --------------------------------------------------------------------------------\n+\t// Multiple Input Groups Optimizing\n+\t// --------------------------------------------------------------------------------\n+\n+\tprivate void optimizeMultipleInputGroups(List<ExecNodeWrapper> orderedWrappers) {\n+\t\t// wrappers are checked in topological order from sources to sinks\n+\t\tfor (int i = orderedWrappers.size() - 1; i >= 0; i--) {\n+\t\t\tExecNodeWrapper wrapper = orderedWrappers.get(i);\n+\t\t\tMultipleInputGroup group = wrapper.group;\n+\t\t\tif (group == null) {\n+\t\t\t\t// we only consider nodes currently in a multiple input group\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\tboolean isUnion = wrapper.execNode instanceof Union;\n+\n+\t\t\tif (group.members.size() == 1) {\n+\t\t\t\tPreconditions.checkState(\n+\t\t\t\t\twrapper == group.root,\n+\t\t\t\t\t\"The only member of a multiple input group is not its root. This is a bug.\");\n+\t\t\t\t// optimization 1. we clean up multiple input groups with only 1 member,\n+\t\t\t\t// unless one of its input is a FLIP-27 source (for maximizing source chaining),\n+\t\t\t\t// however unions do not apply to this optimization because they're not real operators\n+\t\t\t\tif (isUnion || wrapper.inputs.stream().noneMatch(\n+\t\t\t\t\t\tinputWrapper -> isChainableSource(inputWrapper.execNode))) {\n+\t\t\t\t\twrapper.removeFromGroup();\n+\t\t\t\t}\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\tif (!isEntranceOfMultipleInputGroup(wrapper)) {\n+\t\t\t\t// we're not removing a node from the middle of a multiple input group\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\tboolean shouldRemove = false;\n+\t\t\tif (isUnion) {\n+\t\t\t\t// optimization 2. we do not allow union to be the tail of a multiple input\n+\t\t\t\t// as we're paying extra function calls for this, unless one of the united\n+\t\t\t\t// input is a FLIP-27 source\n+\t\t\t\tshouldRemove = wrapper.inputs.stream().noneMatch(\n+\t\t\t\t\tinputWrapper -> isChainableSource(inputWrapper.execNode));\n+\t\t\t} else if (wrapper.inputs.size() == 1) {\n+\t\t\t\t// optimization 3. for one-input operators we'll remove it unless its input\n+\t\t\t\t// is an exchange or a FLIP-27 source, this is mainly to avoid the following\n+\t\t\t\t// pattern:\n+\t\t\t\t// non-chainable source -> calc --\\\n+\t\t\t\t//                                 join ->\n+\t\t\t\t// non-chainable source -> calc --/\n+\t\t\t\t// if we move two calcs into the multiple input group rooted at the join, we're\n+\t\t\t\t// directly shuffling large amount of records from the source without filtering\n+\t\t\t\t// by the calc\n+\t\t\t\tExecNode<?, ?> input = wrapper.inputs.get(0).execNode;\n+\t\t\t\tshouldRemove = !(input instanceof Exchange) && !isChainableSource(input);\n+\t\t\t}\n+\n+\t\t\t// optimization 4. for singleton operations (for example singleton global agg)\n+\t\t\t// we're not including it into the multiple input node as we have to ensure that\n+\t\t\t// the whole multiple input can only have 1 parallelism.\n+\t\t\t// continuous singleton operations connected by forwarding shuffle will be dealt\n+\t\t\t// together with optimization 3\n+\t\t\tshouldRemove |= wrapper.inputs.stream().anyMatch(inputWrapper ->\n+\t\t\t\tinputWrapper.execNode instanceof BatchExecExchange &&", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "006756c81fa5c333b6fdc4bcbfc8820492a18e9b"}, "originalPosition": 280}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIxMDY1ODYxOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/processor/utils/TopologyGraph.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QwNTo1Mjo0NlrOHosXPw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QwNTo1Mjo0NlrOHosXPw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjQzMTkzNQ==", "bodyText": "give distance a definition ?", "url": "https://github.com/apache/flink/pull/13742#discussion_r512431935", "createdAt": "2020-10-27T05:52:46Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/processor/utils/TopologyGraph.java", "diffHunk": "@@ -0,0 +1,206 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.processor.utils;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.table.planner.plan.nodes.exec.AbstractExecNodeExactlyOnceVisitor;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecBoundedStreamScan;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.Set;\n+\n+/**\n+ * A data structure storing the topological and input priority information of an {@link ExecNode} graph.\n+ */\n+@Internal\n+class TopologyGraph {\n+\n+\tprivate final Map<ExecNode<?, ?>, TopologyNode> nodes;\n+\n+\tTopologyGraph(List<ExecNode<?, ?>> roots) {\n+\t\tthis(roots, Collections.emptySet());\n+\t}\n+\n+\tTopologyGraph(List<ExecNode<?, ?>> roots, Set<ExecNode<?, ?>> boundaries) {\n+\t\tthis.nodes = new HashMap<>();\n+\n+\t\t// we first link all edges in the original exec node graph\n+\t\tAbstractExecNodeExactlyOnceVisitor visitor = new AbstractExecNodeExactlyOnceVisitor() {\n+\t\t\t@Override\n+\t\t\tprotected void visitNode(ExecNode<?, ?> node) {\n+\t\t\t\tif (boundaries.contains(node)) {\n+\t\t\t\t\treturn;\n+\t\t\t\t}\n+\t\t\t\tfor (ExecNode<?, ?> input : node.getInputNodes()) {\n+\t\t\t\t\tlink(input, node);\n+\t\t\t\t}\n+\t\t\t\tvisitInputs(node);\n+\t\t\t}\n+\t\t};\n+\t\troots.forEach(n -> n.accept(visitor));\n+\t}\n+\n+\t/**\n+\t * Link an edge from `from` node to `to` node if no loop will occur after adding this edge.\n+\t * Returns if this edge is successfully added.\n+\t */\n+\tboolean link(ExecNode<?, ?> from, ExecNode<?, ?> to) {\n+\t\tTopologyNode fromNode = getTopologyNode(from);\n+\t\tTopologyNode toNode = getTopologyNode(to);\n+\n+\t\tif (canReach(toNode, fromNode)) {\n+\t\t\t// invalid edge, as `to` is the predecessor of `from`\n+\t\t\treturn false;\n+\t\t} else {\n+\t\t\t// link `from` and `to`\n+\t\t\tfromNode.outputs.add(toNode);\n+\t\t\ttoNode.inputs.add(fromNode);\n+\t\t\treturn true;\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * Remove the edge from `from` node to `to` node. If there is no edge between them then do nothing.\n+\t */\n+\tvoid unlink(ExecNode<?, ?> from, ExecNode<?, ?> to) {\n+\t\tTopologyNode fromNode = getTopologyNode(from);\n+\t\tTopologyNode toNode = getTopologyNode(to);\n+\n+\t\tfromNode.outputs.remove(toNode);\n+\t\ttoNode.inputs.remove(fromNode);\n+\t}\n+\n+\t/**\n+\t * Calculate the maximum distance of the currently added nodes from the nodes without inputs.\n+\t * The smallest distance is 0 (which are exactly the nodes without inputs) and the distances of\n+\t * other nodes are the largest distances in their inputs plus 1.\n+\t */\n+\tMap<ExecNode<?, ?>, Integer> calculateDistance() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "006756c81fa5c333b6fdc4bcbfc8820492a18e9b"}, "originalPosition": 104}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIxMDY2MTk2OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/processor/utils/TopologyGraph.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QwNTo1NDozNlrOHosZYQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QwNTo1NDozNlrOHosZYQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjQzMjQ4MQ==", "bodyText": "getOrCreateTopologyNode ?", "url": "https://github.com/apache/flink/pull/13742#discussion_r512432481", "createdAt": "2020-10-27T05:54:36Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/processor/utils/TopologyGraph.java", "diffHunk": "@@ -0,0 +1,206 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.processor.utils;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.table.planner.plan.nodes.exec.AbstractExecNodeExactlyOnceVisitor;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecBoundedStreamScan;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.Set;\n+\n+/**\n+ * A data structure storing the topological and input priority information of an {@link ExecNode} graph.\n+ */\n+@Internal\n+class TopologyGraph {\n+\n+\tprivate final Map<ExecNode<?, ?>, TopologyNode> nodes;\n+\n+\tTopologyGraph(List<ExecNode<?, ?>> roots) {\n+\t\tthis(roots, Collections.emptySet());\n+\t}\n+\n+\tTopologyGraph(List<ExecNode<?, ?>> roots, Set<ExecNode<?, ?>> boundaries) {\n+\t\tthis.nodes = new HashMap<>();\n+\n+\t\t// we first link all edges in the original exec node graph\n+\t\tAbstractExecNodeExactlyOnceVisitor visitor = new AbstractExecNodeExactlyOnceVisitor() {\n+\t\t\t@Override\n+\t\t\tprotected void visitNode(ExecNode<?, ?> node) {\n+\t\t\t\tif (boundaries.contains(node)) {\n+\t\t\t\t\treturn;\n+\t\t\t\t}\n+\t\t\t\tfor (ExecNode<?, ?> input : node.getInputNodes()) {\n+\t\t\t\t\tlink(input, node);\n+\t\t\t\t}\n+\t\t\t\tvisitInputs(node);\n+\t\t\t}\n+\t\t};\n+\t\troots.forEach(n -> n.accept(visitor));\n+\t}\n+\n+\t/**\n+\t * Link an edge from `from` node to `to` node if no loop will occur after adding this edge.\n+\t * Returns if this edge is successfully added.\n+\t */\n+\tboolean link(ExecNode<?, ?> from, ExecNode<?, ?> to) {\n+\t\tTopologyNode fromNode = getTopologyNode(from);\n+\t\tTopologyNode toNode = getTopologyNode(to);\n+\n+\t\tif (canReach(toNode, fromNode)) {\n+\t\t\t// invalid edge, as `to` is the predecessor of `from`\n+\t\t\treturn false;\n+\t\t} else {\n+\t\t\t// link `from` and `to`\n+\t\t\tfromNode.outputs.add(toNode);\n+\t\t\ttoNode.inputs.add(fromNode);\n+\t\t\treturn true;\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * Remove the edge from `from` node to `to` node. If there is no edge between them then do nothing.\n+\t */\n+\tvoid unlink(ExecNode<?, ?> from, ExecNode<?, ?> to) {\n+\t\tTopologyNode fromNode = getTopologyNode(from);\n+\t\tTopologyNode toNode = getTopologyNode(to);\n+\n+\t\tfromNode.outputs.remove(toNode);\n+\t\ttoNode.inputs.remove(fromNode);\n+\t}\n+\n+\t/**\n+\t * Calculate the maximum distance of the currently added nodes from the nodes without inputs.\n+\t * The smallest distance is 0 (which are exactly the nodes without inputs) and the distances of\n+\t * other nodes are the largest distances in their inputs plus 1.\n+\t */\n+\tMap<ExecNode<?, ?>, Integer> calculateDistance() {\n+\t\tMap<ExecNode<?, ?>, Integer> result = new HashMap<>();\n+\t\tMap<TopologyNode, Integer> inputsVisitedMap = new HashMap<>();\n+\n+\t\tQueue<TopologyNode> queue = new LinkedList<>();\n+\t\tfor (TopologyNode node : nodes.values()) {\n+\t\t\tif (node.inputs.size() == 0) {\n+\t\t\t\tqueue.offer(node);\n+\t\t\t}\n+\t\t}\n+\n+\t\twhile (!queue.isEmpty()) {\n+\t\t\tTopologyNode node = queue.poll();\n+\t\t\tint dist = -1;\n+\t\t\tfor (TopologyNode input : node.inputs) {\n+\t\t\t\tdist = Math.max(\n+\t\t\t\t\t\tdist,\n+\t\t\t\t\tPreconditions.checkNotNull(\n+\t\t\t\t\t\tresult.get(input.execNode),\n+\t\t\t\t\t\t\"The distance of an input node is not calculated. This is a bug.\"));\n+\t\t\t}\n+\t\t\tdist++;\n+\t\t\tresult.put(node.execNode, dist);\n+\n+\t\t\tfor (TopologyNode output : node.outputs) {\n+\t\t\t\tint inputsVisited = inputsVisitedMap.compute(output, (k, v) -> v == null ? 1 : v + 1);\n+\t\t\t\tif (inputsVisited == output.inputs.size()) {\n+\t\t\t\t\tqueue.offer(output);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\n+\t\treturn result;\n+\t}\n+\n+\t@VisibleForTesting\n+\tboolean canReach(ExecNode<?, ?> from, ExecNode<?, ?> to) {\n+\t\tTopologyNode fromNode = getTopologyNode(from);\n+\t\tTopologyNode toNode = getTopologyNode(to);\n+\t\treturn canReach(fromNode, toNode);\n+\t}\n+\n+\tprivate boolean canReach(TopologyNode from, TopologyNode to) {\n+\t\tSet<TopologyNode> visited = new HashSet<>();\n+\t\tvisited.add(from);\n+\t\tQueue<TopologyNode> queue = new LinkedList<>();\n+\t\tqueue.offer(from);\n+\n+\t\twhile (!queue.isEmpty()) {\n+\t\t\tTopologyNode node = queue.poll();\n+\t\t\tif (to.equals(node)) {\n+\t\t\t\treturn true;\n+\t\t\t}\n+\n+\t\t\tfor (TopologyNode next : node.outputs) {\n+\t\t\t\tif (visited.contains(next)) {\n+\t\t\t\t\tcontinue;\n+\t\t\t\t}\n+\t\t\t\tvisited.add(next);\n+\t\t\t\tqueue.offer(next);\n+\t\t\t}\n+\t\t}\n+\n+\t\treturn false;\n+\t}\n+\n+\tprivate TopologyNode getTopologyNode(ExecNode<?, ?> execNode) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "006756c81fa5c333b6fdc4bcbfc8820492a18e9b"}, "originalPosition": 170}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIxMjEyMTU1OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/processors/utils/TopologyGraph.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QxMjo0MDowMVrOHo6Jkg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QxMjo0MDowMVrOHo6Jkg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjY1NzgxMA==", "bodyText": "rename to calculateMaxDistance", "url": "https://github.com/apache/flink/pull/13742#discussion_r512657810", "createdAt": "2020-10-27T12:40:01Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/processors/utils/TopologyGraph.java", "diffHunk": "@@ -100,6 +100,9 @@ void unlink(ExecNode<?, ?> from, ExecNode<?, ?> to) {\n \t * Calculate the maximum distance of the currently added nodes from the nodes without inputs.\n \t * The smallest distance is 0 (which are exactly the nodes without inputs) and the distances of\n \t * other nodes are the largest distances in their inputs plus 1.\n+\t *\n+\t * <p>Distance of a node is defined as the number of edges one needs to go through from the\n+\t * nodes without inputs to this node.\n \t */\n \tMap<ExecNode<?, ?>, Integer> calculateDistance() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d56e9dc09351c5465a26c4c3a047ea11bbe0515d"}, "originalPosition": 39}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 76, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}