{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDE0NzExNjM2", "number": 12025, "title": "[FLINK-17435] [hive] Hive non-partitioned source supports streaming read", "bodyText": "What is the purpose of the change\nfor non-partitioned hive source, we can use existing continuous file processing mechanism to read new data. This pr aim let non-partitioned hive source support streaming read.\nBrief change log\n\nA hive FileInputFormat that wraps HiveTableInputFormat\ncreate streaming source with PROCESS_CONTINUOUSLY mode\n\nVerifying this change\nThis change added tests and can be verified as follows:\n\nExtended HiveTableSourceTest for streaming read on non-partitioned source\n\nDoes this pull request potentially affect one of the following parts:\n\nDependencies (does it add or upgrade a dependency): (yes / no)\nThe public API, i.e., is any changed class annotated with @Public(Evolving): (yes / no)\nThe serializers: (yes / no / don't know)\nThe runtime per-record code paths (performance sensitive): (yes / no / don't know)\nAnything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (yes / no / don't know)\nThe S3 file system connector: (yes / no / don't know)\n\nDocumentation\n\nDoes this pull request introduce a new feature? (yes / no)\nIf yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)", "createdAt": "2020-05-07T14:09:32Z", "url": "https://github.com/apache/flink/pull/12025", "merged": true, "mergeCommit": {"oid": "32bd0944d0519093c0a4d5d809c6636eb3a7fc31"}, "closed": true, "closedAt": "2020-05-15T12:47:15Z", "author": {"login": "godfreyhe"}, "timelineItems": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABchNoDjAH2gAyNDE0NzExNjM2OjA0ZGIwNGQ4ZDFiNDFjYTA1NjE4YzViMDliODNiMzAzMWJmN2VlMTE=", "endCursor": "Y3Vyc29yOnYyOpPPAAABchheEzAFqTQxMjYxNDY0NA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "04db04d8d1b41ca05618c5b09b83b3031bf7ee11", "author": {"user": {"login": "godfreyhe", "name": "godfrey he"}}, "url": "https://github.com/apache/flink/commit/04db04d8d1b41ca05618c5b09b83b3031bf7ee11", "committedDate": "2020-05-14T13:38:06Z", "message": "[FLINK-17435] [hive] Hive non-partitioned source supports streaming read"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "536fe9b624ee00e02522c57cb904a4c795d425c7", "author": {"user": {"login": "godfreyhe", "name": "godfrey he"}}, "url": "https://github.com/apache/flink/commit/536fe9b624ee00e02522c57cb904a4c795d425c7", "committedDate": "2020-05-08T01:58:33Z", "message": "fix unstable tests"}, "afterCommit": {"oid": "04db04d8d1b41ca05618c5b09b83b3031bf7ee11", "author": {"user": {"login": "godfreyhe", "name": "godfrey he"}}, "url": "https://github.com/apache/flink/commit/04db04d8d1b41ca05618c5b09b83b3031bf7ee11", "committedDate": "2020-05-14T13:38:06Z", "message": "[FLINK-17435] [hive] Hive non-partitioned source supports streaming read"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d4b70dbd62d22edbaf0c3ff54ca61a35ac93c04e", "author": {"user": {"login": "godfreyhe", "name": "godfrey he"}}, "url": "https://github.com/apache/flink/commit/d4b70dbd62d22edbaf0c3ff54ca61a35ac93c04e", "committedDate": "2020-05-15T02:08:00Z", "message": "fix checkstyle error"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDEyMzExMTY5", "url": "https://github.com/apache/flink/pull/12025#pullrequestreview-412311169", "createdAt": "2020-05-15T02:44:56Z", "commit": {"oid": "d4b70dbd62d22edbaf0c3ff54ca61a35ac93c04e"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQwMjo0NDo1N1rOGV0ouA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQwMjo1NzoxNVrOGV00ag==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUzNTY3Mg==", "bodyText": "Above should have a blank line.", "url": "https://github.com/apache/flink/pull/12025#discussion_r425535672", "createdAt": "2020-05-15T02:44:57Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/ConsumeOrder.java", "diffHunk": "@@ -0,0 +1,59 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+/**\n+ * {@link ConsumeOrder} defines the orders to continuously consume stream source.\n+ */\n+public enum ConsumeOrder {\n+\n+\t/**\n+\t * create-time compare partition/file creation time,\n+\t * this is not the partition create time in Hive metaStore,\n+\t * but the folder/file create time in filesystem.\n+\t */\n+\tCREATE_TIME_ORDER(\"create-time\"),\n+\n+\t/**\n+\t * partition-time compare time represented by partition name.\n+\t */\n+\tPARTITION_TIME_ORDER(\"partition-time\");\n+\n+\tprivate final String order;\n+\tConsumeOrder(String order) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d4b70dbd62d22edbaf0c3ff54ca61a35ac93c04e"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUzNjA0MA==", "bodyText": "ConsumeOrder.values() -> values()", "url": "https://github.com/apache/flink/pull/12025#discussion_r425536040", "createdAt": "2020-05-15T02:46:27Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/ConsumeOrder.java", "diffHunk": "@@ -0,0 +1,59 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+/**\n+ * {@link ConsumeOrder} defines the orders to continuously consume stream source.\n+ */\n+public enum ConsumeOrder {\n+\n+\t/**\n+\t * create-time compare partition/file creation time,\n+\t * this is not the partition create time in Hive metaStore,\n+\t * but the folder/file create time in filesystem.\n+\t */\n+\tCREATE_TIME_ORDER(\"create-time\"),\n+\n+\t/**\n+\t * partition-time compare time represented by partition name.\n+\t */\n+\tPARTITION_TIME_ORDER(\"partition-time\");\n+\n+\tprivate final String order;\n+\tConsumeOrder(String order) {\n+\t\tthis.order = order;\n+\t}\n+\n+\t@Override\n+\tpublic String toString() {\n+\t\treturn order;\n+\t}\n+\n+\t/**\n+\t * Get {@link ConsumeOrder} from consume order string.\n+\t */\n+\tpublic static ConsumeOrder getConsumeOrder(String consumeOrderStr) {\n+\t\tfor (ConsumeOrder consumeOrder : ConsumeOrder.values()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d4b70dbd62d22edbaf0c3ff54ca61a35ac93c04e"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUzNjExMg==", "bodyText": "equalsIgnoreCase", "url": "https://github.com/apache/flink/pull/12025#discussion_r425536112", "createdAt": "2020-05-15T02:46:47Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/ConsumeOrder.java", "diffHunk": "@@ -0,0 +1,59 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+/**\n+ * {@link ConsumeOrder} defines the orders to continuously consume stream source.\n+ */\n+public enum ConsumeOrder {\n+\n+\t/**\n+\t * create-time compare partition/file creation time,\n+\t * this is not the partition create time in Hive metaStore,\n+\t * but the folder/file create time in filesystem.\n+\t */\n+\tCREATE_TIME_ORDER(\"create-time\"),\n+\n+\t/**\n+\t * partition-time compare time represented by partition name.\n+\t */\n+\tPARTITION_TIME_ORDER(\"partition-time\");\n+\n+\tprivate final String order;\n+\tConsumeOrder(String order) {\n+\t\tthis.order = order;\n+\t}\n+\n+\t@Override\n+\tpublic String toString() {\n+\t\treturn order;\n+\t}\n+\n+\t/**\n+\t * Get {@link ConsumeOrder} from consume order string.\n+\t */\n+\tpublic static ConsumeOrder getConsumeOrder(String consumeOrderStr) {\n+\t\tfor (ConsumeOrder consumeOrder : ConsumeOrder.values()) {\n+\t\t\tif (consumeOrder.order.equals(consumeOrderStr)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d4b70dbd62d22edbaf0c3ff54ca61a35ac93c04e"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUzNjI1OQ==", "bodyText": "throw exception, we should not pass a null to operators.", "url": "https://github.com/apache/flink/pull/12025#discussion_r425536259", "createdAt": "2020-05-15T02:47:23Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/ConsumeOrder.java", "diffHunk": "@@ -0,0 +1,59 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+/**\n+ * {@link ConsumeOrder} defines the orders to continuously consume stream source.\n+ */\n+public enum ConsumeOrder {\n+\n+\t/**\n+\t * create-time compare partition/file creation time,\n+\t * this is not the partition create time in Hive metaStore,\n+\t * but the folder/file create time in filesystem.\n+\t */\n+\tCREATE_TIME_ORDER(\"create-time\"),\n+\n+\t/**\n+\t * partition-time compare time represented by partition name.\n+\t */\n+\tPARTITION_TIME_ORDER(\"partition-time\");\n+\n+\tprivate final String order;\n+\tConsumeOrder(String order) {\n+\t\tthis.order = order;\n+\t}\n+\n+\t@Override\n+\tpublic String toString() {\n+\t\treturn order;\n+\t}\n+\n+\t/**\n+\t * Get {@link ConsumeOrder} from consume order string.\n+\t */\n+\tpublic static ConsumeOrder getConsumeOrder(String consumeOrderStr) {\n+\t\tfor (ConsumeOrder consumeOrder : ConsumeOrder.values()) {\n+\t\t\tif (consumeOrder.order.equals(consumeOrderStr)) {\n+\t\t\t\treturn consumeOrder;\n+\t\t\t}\n+\t\t}\n+\t\treturn null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d4b70dbd62d22edbaf0c3ff54ca61a35ac93c04e"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUzNjcwMA==", "bodyText": "Configuration configuration = new Configuration();\noptions.forEach(configuration::setString)\n\nWe can use Configuration to get option.", "url": "https://github.com/apache/flink/pull/12025#discussion_r425536700", "createdAt": "2020-05-15T02:49:19Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSource.java", "diffHunk": "@@ -261,6 +267,57 @@ private boolean isStreamingSource() {\n \t\treturn new DataStreamSource<>(source);\n \t}\n \n+\tprivate DataStream<RowData> createStreamSourceForNonPartitionTable(\n+\t\t\tStreamExecutionEnvironment execEnv,\n+\t\t\tTypeInformation<RowData> typeInfo,\n+\t\t\tHiveTableInputFormat inputFormat,\n+\t\t\tHiveTablePartition hiveTable) {\n+\t\tHiveTableFileInputFormat fileInputFormat = new HiveTableFileInputFormat(\n+\t\t\t\tinputFormat,\n+\t\t\t\thiveTable);\n+\t\tfileInputFormat.setFilePath(getFilePath());\n+\n+\t\tfinal Map<String, String> properties = catalogTable.getOptions();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d4b70dbd62d22edbaf0c3ff54ca61a35ac93c04e"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUzNzExNA==", "bodyText": "never null.", "url": "https://github.com/apache/flink/pull/12025#discussion_r425537114", "createdAt": "2020-05-15T02:50:56Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSource.java", "diffHunk": "@@ -261,6 +267,57 @@ private boolean isStreamingSource() {\n \t\treturn new DataStreamSource<>(source);\n \t}\n \n+\tprivate DataStream<RowData> createStreamSourceForNonPartitionTable(\n+\t\t\tStreamExecutionEnvironment execEnv,\n+\t\t\tTypeInformation<RowData> typeInfo,\n+\t\t\tHiveTableInputFormat inputFormat,\n+\t\t\tHiveTablePartition hiveTable) {\n+\t\tHiveTableFileInputFormat fileInputFormat = new HiveTableFileInputFormat(\n+\t\t\t\tinputFormat,\n+\t\t\t\thiveTable);\n+\t\tfileInputFormat.setFilePath(getFilePath());\n+\n+\t\tfinal Map<String, String> properties = catalogTable.getOptions();\n+\n+\t\tString consumeOrderStr = properties.getOrDefault(\n+\t\t\t\tSTREAMING_SOURCE_CONSUME_ORDER.key(),\n+\t\t\t\tSTREAMING_SOURCE_CONSUME_ORDER.defaultValue());\n+\t\tConsumeOrder consumeOrder = ConsumeOrder.getConsumeOrder(consumeOrderStr);\n+\t\tif (consumeOrder != ConsumeOrder.CREATE_TIME_ORDER) {\n+\t\t\tthrow new UnsupportedOperationException(\"Unsupported consumer order: \" + consumeOrder);\n+\t\t}\n+\n+\t\tString consumeOffset = properties.getOrDefault(\n+\t\t\t\tSTREAMING_SOURCE_CONSUME_START_OFFSET.key(),\n+\t\t\t\tSTREAMING_SOURCE_CONSUME_START_OFFSET.defaultValue());\n+\t\tlong currentReadTime = Long.MIN_VALUE;\n+\t\tif (consumeOffset != null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d4b70dbd62d22edbaf0c3ff54ca61a35ac93c04e"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUzNzY0MQ==", "bodyText": "Remove getFilePath, we can get path from allHivePartitions(should be a single object when no partitions.).", "url": "https://github.com/apache/flink/pull/12025#discussion_r425537641", "createdAt": "2020-05-15T02:53:10Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSource.java", "diffHunk": "@@ -261,6 +267,57 @@ private boolean isStreamingSource() {\n \t\treturn new DataStreamSource<>(source);\n \t}\n \n+\tprivate DataStream<RowData> createStreamSourceForNonPartitionTable(\n+\t\t\tStreamExecutionEnvironment execEnv,\n+\t\t\tTypeInformation<RowData> typeInfo,\n+\t\t\tHiveTableInputFormat inputFormat,\n+\t\t\tHiveTablePartition hiveTable) {\n+\t\tHiveTableFileInputFormat fileInputFormat = new HiveTableFileInputFormat(\n+\t\t\t\tinputFormat,\n+\t\t\t\thiveTable);\n+\t\tfileInputFormat.setFilePath(getFilePath());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d4b70dbd62d22edbaf0c3ff54ca61a35ac93c04e"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUzODEwOQ==", "bodyText": "Please add comments:\n\nOnly support FileInputSplit\nOnly support renaming inserting\ngetSplit use flinks instead format.", "url": "https://github.com/apache/flink/pull/12025#discussion_r425538109", "createdAt": "2020-05-15T02:55:05Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveTableFileInputFormat.java", "diffHunk": "@@ -0,0 +1,100 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive.read;\n+\n+import org.apache.flink.api.common.functions.RuntimeContext;\n+import org.apache.flink.api.common.io.FileInputFormat;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.connectors.hive.HiveTablePartition;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.table.data.RowData;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.mapred.FileSplit;\n+\n+import java.io.IOException;\n+import java.net.URI;\n+\n+/**\n+ * A {@link FileInputFormat} that wraps a {@link HiveTableInputFormat}.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d4b70dbd62d22edbaf0c3ff54ca61a35ac93c04e"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUzODI5MQ==", "bodyText": "A better way is first super., second do own works.\nsame below.", "url": "https://github.com/apache/flink/pull/12025#discussion_r425538291", "createdAt": "2020-05-15T02:55:47Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveTableFileInputFormat.java", "diffHunk": "@@ -0,0 +1,100 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive.read;\n+\n+import org.apache.flink.api.common.functions.RuntimeContext;\n+import org.apache.flink.api.common.io.FileInputFormat;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.connectors.hive.HiveTablePartition;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.table.data.RowData;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.mapred.FileSplit;\n+\n+import java.io.IOException;\n+import java.net.URI;\n+\n+/**\n+ * A {@link FileInputFormat} that wraps a {@link HiveTableInputFormat}.\n+ */\n+public class HiveTableFileInputFormat extends FileInputFormat<RowData> {\n+\n+\tprivate HiveTableInputFormat inputFormat;\n+\tprivate HiveTablePartition hiveTablePartition;\n+\n+\tpublic HiveTableFileInputFormat(\n+\t\t\tHiveTableInputFormat inputFormat,\n+\t\t\tHiveTablePartition hiveTablePartition) {\n+\t\tthis.inputFormat = inputFormat;\n+\t\tthis.hiveTablePartition = hiveTablePartition;\n+\t}\n+\n+\t@Override\n+\tpublic void open(FileInputSplit fileSplit) throws IOException {\n+\t\tURI uri = fileSplit.getPath().toUri();\n+\t\tHiveTableInputSplit split = new HiveTableInputSplit(\n+\t\t\t\tfileSplit.getSplitNumber(),\n+\t\t\t\tnew FileSplit(new Path(uri), fileSplit.getStart(), fileSplit.getLength(), (String[]) null),\n+\t\t\t\tinputFormat.getJobConf(),\n+\t\t\t\thiveTablePartition\n+\t\t);\n+\t\tinputFormat.open(split);\n+\t}\n+\n+\t@Override\n+\tpublic boolean reachedEnd() throws IOException {\n+\t\treturn inputFormat.reachedEnd();\n+\t}\n+\n+\t@Override\n+\tpublic RowData nextRecord(RowData reuse) throws IOException {\n+\t\treturn inputFormat.nextRecord(reuse);\n+\t}\n+\n+\t@Override\n+\tpublic void configure(Configuration parameters) {\n+\t\tinputFormat.configure(parameters);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d4b70dbd62d22edbaf0c3ff54ca61a35ac93c04e"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUzODY2Ng==", "bodyText": "useMapredReader only works in parquet and orc, you should use them.", "url": "https://github.com/apache/flink/pull/12025#discussion_r425538666", "createdAt": "2020-05-15T02:57:15Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveTableSourceTest.java", "diffHunk": "@@ -531,6 +534,76 @@ public void testStreamPartitionRead() throws Exception {\n \t\tjob.cancel();\n \t}\n \n+\t@Test(timeout = 30000)\n+\tpublic void testNonPartitionStreamingSourceWithMapredReader() throws Exception {\n+\t\ttestNonPartitionStreamingSource(true, \"test_mapred_reader\");\n+\t}\n+\n+\t@Test(timeout = 30000)\n+\tpublic void testNonPartitionStreamingSourceWithVectorizedReader() throws Exception {\n+\t\ttestNonPartitionStreamingSource(false, \"test_vectorized_reader\");\n+\t}\n+\n+\tprivate void testNonPartitionStreamingSource(Boolean useMapredReader, String tblName) throws Exception {\n+\t\tfinal String catalogName = \"hive\";\n+\t\tfinal String dbName = \"source_db\";\n+\t\thiveShell.execute(\"CREATE TABLE source_db.\" + tblName + \" (\" +\n+\t\t\t\t\"  a INT,\" +\n+\t\t\t\t\"  b CHAR(1) \" +\n+\t\t\t\t\") TBLPROPERTIES (\" +\n+\t\t\t\t\"  'streaming-source.enable'='true',\" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d4b70dbd62d22edbaf0c3ff54ca61a35ac93c04e"}, "originalPosition": 53}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c38c037f31687b4d20ef0e183d42cc5f64d3bb45", "author": {"user": {"login": "godfreyhe", "name": "godfrey he"}}, "url": "https://github.com/apache/flink/commit/c38c037f31687b4d20ef0e183d42cc5f64d3bb45", "committedDate": "2020-05-15T03:45:34Z", "message": "address comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDEyNjE0NjQ0", "url": "https://github.com/apache/flink/pull/12025#pullrequestreview-412614644", "createdAt": "2020-05-15T12:45:18Z", "commit": {"oid": "c38c037f31687b4d20ef0e183d42cc5f64d3bb45"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4658, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}