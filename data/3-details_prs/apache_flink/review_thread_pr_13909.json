{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTE0ODYyMTc4", "number": 13909, "reviewThreads": {"totalCount": 16, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQxMjoxNjozM1rOE1oW0Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQxNTowNDoxMlrOE2HxEA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0NjcxMTg1OnYy", "diffSide": "RIGHT", "path": "docs/dev/table/connectors/formats/index.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQxMjoxNjozM1rOHuAqQQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQxMjoxNjozM1rOHuAqQQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODAwNzM2MQ==", "bodyText": "According to the offline discussion, call the format raw instead.", "url": "https://github.com/apache/flink/pull/13909#discussion_r518007361", "createdAt": "2020-11-05T12:16:33Z", "author": {"login": "twalthr"}, "path": "docs/dev/table/connectors/formats/index.md", "diffHunk": "@@ -68,5 +68,10 @@ Flink supports the following formats:\n          <td><a href=\"{% link dev/table/connectors/formats/orc.md %}\">Apache ORC</a></td>\n          <td><a href=\"{% link dev/table/connectors/filesystem.md %}\">Filesystem</a></td>\n         </tr>\n+        <tr>\n+        <td><a href=\"{% link dev/table/connectors/formats/singleValue.md %}\">Single Value</a></td>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8de5115aa0bbc4bc486dced3d2bc67d34823659f"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0NjcxOTk4OnYy", "diffSide": "RIGHT", "path": "docs/dev/table/connectors/formats/single-field.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQxMjoxODo0OFrOHuAu7w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQxNDo0NDo1MlrOHuGnBg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODAwODU1OQ==", "bodyText": "I don't understand this comment:\nencodes `null` values as `null` `byte[]`\n\nDo you mean:\nencodes `null` values as `null` of `byte[]` type", "url": "https://github.com/apache/flink/pull/13909#discussion_r518008559", "createdAt": "2020-11-05T12:18:48Z", "author": {"login": "twalthr"}, "path": "docs/dev/table/connectors/formats/single-field.md", "diffHunk": "@@ -0,0 +1,151 @@\n+---\n+title: \"Single Field Format\"\n+nav-title: SingleField\n+nav-parent_id: sql-formats\n+nav-pos: 7\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+<span class=\"label label-info\">Format: Serialization Schema</span>\n+<span class=\"label label-info\">Format: Deserialization Schema</span>\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+The SingleField format allows to read and write data where the data contains only a single field, and that field is not wrapped within a JSON object, or an Avro record.\n+\n+Currently, the SingleField format supports `String`, `byte[]` and primitive type.\n+\n+Note: this format encodes `null` values as `null` `byte[]`. This may have limitation when used in `upsert-kafka`, because `upsert-kafka` treats `null` values as a tombstone message (DELETE on the key). Therefore, we recommend avoiding using `upsert-kafka` connector and `single-field` format if the field can have a `null` value.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8de5115aa0bbc4bc486dced3d2bc67d34823659f"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODEwNDgzOA==", "bodyText": "Yes.", "url": "https://github.com/apache/flink/pull/13909#discussion_r518104838", "createdAt": "2020-11-05T14:44:52Z", "author": {"login": "wuchong"}, "path": "docs/dev/table/connectors/formats/single-field.md", "diffHunk": "@@ -0,0 +1,151 @@\n+---\n+title: \"Single Field Format\"\n+nav-title: SingleField\n+nav-parent_id: sql-formats\n+nav-pos: 7\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+<span class=\"label label-info\">Format: Serialization Schema</span>\n+<span class=\"label label-info\">Format: Deserialization Schema</span>\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+The SingleField format allows to read and write data where the data contains only a single field, and that field is not wrapped within a JSON object, or an Avro record.\n+\n+Currently, the SingleField format supports `String`, `byte[]` and primitive type.\n+\n+Note: this format encodes `null` values as `null` `byte[]`. This may have limitation when used in `upsert-kafka`, because `upsert-kafka` treats `null` values as a tombstone message (DELETE on the key). Therefore, we recommend avoiding using `upsert-kafka` connector and `single-field` format if the field can have a `null` value.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODAwODU1OQ=="}, "originalCommit": {"oid": "8de5115aa0bbc4bc486dced3d2bc67d34823659f"}, "originalPosition": 36}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0NjcyNzk4OnYy", "diffSide": "RIGHT", "path": "docs/dev/table/connectors/formats/single-field.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQxMjoyMTowNVrOHuAzrA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQxMjoyMTowNVrOHuAzrA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODAwOTc3Mg==", "bodyText": "we recommend avoiding using `upsert-kafka` connector and the `raw` format as a `value.format`\n\nWe should emphasize that a key format is fine.", "url": "https://github.com/apache/flink/pull/13909#discussion_r518009772", "createdAt": "2020-11-05T12:21:05Z", "author": {"login": "twalthr"}, "path": "docs/dev/table/connectors/formats/single-field.md", "diffHunk": "@@ -0,0 +1,151 @@\n+---\n+title: \"Single Field Format\"\n+nav-title: SingleField\n+nav-parent_id: sql-formats\n+nav-pos: 7\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+<span class=\"label label-info\">Format: Serialization Schema</span>\n+<span class=\"label label-info\">Format: Deserialization Schema</span>\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+The SingleField format allows to read and write data where the data contains only a single field, and that field is not wrapped within a JSON object, or an Avro record.\n+\n+Currently, the SingleField format supports `String`, `byte[]` and primitive type.\n+\n+Note: this format encodes `null` values as `null` `byte[]`. This may have limitation when used in `upsert-kafka`, because `upsert-kafka` treats `null` values as a tombstone message (DELETE on the key). Therefore, we recommend avoiding using `upsert-kafka` connector and `single-field` format if the field can have a `null` value.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8de5115aa0bbc4bc486dced3d2bc67d34823659f"}, "originalPosition": 36}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0Njc0NTE3OnYy", "diffSide": "RIGHT", "path": "docs/dev/table/connectors/formats/single-field.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQxMjoyNjowNFrOHuA-Ag==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQxMjoyNjowNFrOHuA-Ag==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODAxMjQxOA==", "bodyText": "We should also allow to configure the endianness. Every format that encodes to bytes requires this information.", "url": "https://github.com/apache/flink/pull/13909#discussion_r518012418", "createdAt": "2020-11-05T12:26:04Z", "author": {"login": "twalthr"}, "path": "docs/dev/table/connectors/formats/single-field.md", "diffHunk": "@@ -0,0 +1,151 @@\n+---\n+title: \"Single Field Format\"\n+nav-title: SingleField\n+nav-parent_id: sql-formats\n+nav-pos: 7\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+<span class=\"label label-info\">Format: Serialization Schema</span>\n+<span class=\"label label-info\">Format: Deserialization Schema</span>\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+The SingleField format allows to read and write data where the data contains only a single field, and that field is not wrapped within a JSON object, or an Avro record.\n+\n+Currently, the SingleField format supports `String`, `byte[]` and primitive type.\n+\n+Note: this format encodes `null` values as `null` `byte[]`. This may have limitation when used in `upsert-kafka`, because `upsert-kafka` treats `null` values as a tombstone message (DELETE on the key). Therefore, we recommend avoiding using `upsert-kafka` connector and `single-field` format if the field can have a `null` value.\n+\n+Example\n+----------------\n+\n+For example, you may have following raw log data in Kafka and want to read and analyse such data using Flink SQL.\n+\n+```\n+47.29.201.179 - - [28/Feb/2019:13:17:10 +0000] \"GET /?p=1 HTTP/2.0\" 200 5316 \"https://domain.com/?p=1\" \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36\" \"2.75\"\n+```\n+\n+The following creates a table where it reads from (and writes to) the underlying Kafka topic as an anonymous string value by using `single-field` format:\n+\n+<div class=\"codetabs\" markdown=\"1\">\n+<div data-lang=\"SQL\" markdown=\"1\">\n+{% highlight sql %}\n+CREATE TABLE nginx_log (\n+  log STRING\n+) WITH (\n+  'connector' = 'kafka',\n+  'topic' = 'nginx_log',\n+  'properties.bootstrap.servers' = 'localhost:9092',\n+  'properties.group.id' = 'testGroup',\n+  'format' = 'single-field'\n+)\n+{% endhighlight %}\n+</div>\n+</div>\n+\n+Then you can read out the raw data as a pure string, and split it into multiple fields using user-defined-function for further analysing, e.g. `my_split` in the example.\n+\n+<div class=\"codetabs\" markdown=\"1\">\n+<div data-lang=\"SQL\" markdown=\"1\">\n+{% highlight sql %}\n+SELECT t.hostname, t.datetime, t.url, t.browser, ...\n+FROM(\n+  SELECT my_split(log) as t FROM nginx_log\n+);\n+{% endhighlight %}\n+</div>\n+</div>\n+\n+In contrast, you can also write a single field of STRING type into Kafka topic as an anonymous string value.\n+\n+Format Options\n+----------------\n+\n+<table class=\"table table-bordered\">\n+    <thead>\n+      <tr>\n+        <th class=\"text-left\" style=\"width: 25%\">Option</th>\n+        <th class=\"text-center\" style=\"width: 8%\">Required</th>\n+        <th class=\"text-center\" style=\"width: 7%\">Default</th>\n+        <th class=\"text-center\" style=\"width: 10%\">Type</th>\n+        <th class=\"text-center\" style=\"width: 50%\">Description</th>\n+      </tr>\n+    </thead>\n+    <tbody>\n+    <tr>\n+      <td><h5>format</h5></td>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8de5115aa0bbc4bc486dced3d2bc67d34823659f"}, "originalPosition": 95}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0Njc0OTAxOnYy", "diffSide": "RIGHT", "path": "docs/dev/table/connectors/formats/single-field.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQxMjoyNzoxMVrOHuBAYQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQxMjoyNzoxMVrOHuBAYQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODAxMzAyNQ==", "bodyText": "We should also allow to specify a different charset. This should be configurable.", "url": "https://github.com/apache/flink/pull/13909#discussion_r518013025", "createdAt": "2020-11-05T12:27:11Z", "author": {"login": "twalthr"}, "path": "docs/dev/table/connectors/formats/single-field.md", "diffHunk": "@@ -0,0 +1,151 @@\n+---\n+title: \"Single Field Format\"\n+nav-title: SingleField\n+nav-parent_id: sql-formats\n+nav-pos: 7\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+<span class=\"label label-info\">Format: Serialization Schema</span>\n+<span class=\"label label-info\">Format: Deserialization Schema</span>\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+The SingleField format allows to read and write data where the data contains only a single field, and that field is not wrapped within a JSON object, or an Avro record.\n+\n+Currently, the SingleField format supports `String`, `byte[]` and primitive type.\n+\n+Note: this format encodes `null` values as `null` `byte[]`. This may have limitation when used in `upsert-kafka`, because `upsert-kafka` treats `null` values as a tombstone message (DELETE on the key). Therefore, we recommend avoiding using `upsert-kafka` connector and `single-field` format if the field can have a `null` value.\n+\n+Example\n+----------------\n+\n+For example, you may have following raw log data in Kafka and want to read and analyse such data using Flink SQL.\n+\n+```\n+47.29.201.179 - - [28/Feb/2019:13:17:10 +0000] \"GET /?p=1 HTTP/2.0\" 200 5316 \"https://domain.com/?p=1\" \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36\" \"2.75\"\n+```\n+\n+The following creates a table where it reads from (and writes to) the underlying Kafka topic as an anonymous string value by using `single-field` format:\n+\n+<div class=\"codetabs\" markdown=\"1\">\n+<div data-lang=\"SQL\" markdown=\"1\">\n+{% highlight sql %}\n+CREATE TABLE nginx_log (\n+  log STRING\n+) WITH (\n+  'connector' = 'kafka',\n+  'topic' = 'nginx_log',\n+  'properties.bootstrap.servers' = 'localhost:9092',\n+  'properties.group.id' = 'testGroup',\n+  'format' = 'single-field'\n+)\n+{% endhighlight %}\n+</div>\n+</div>\n+\n+Then you can read out the raw data as a pure string, and split it into multiple fields using user-defined-function for further analysing, e.g. `my_split` in the example.\n+\n+<div class=\"codetabs\" markdown=\"1\">\n+<div data-lang=\"SQL\" markdown=\"1\">\n+{% highlight sql %}\n+SELECT t.hostname, t.datetime, t.url, t.browser, ...\n+FROM(\n+  SELECT my_split(log) as t FROM nginx_log\n+);\n+{% endhighlight %}\n+</div>\n+</div>\n+\n+In contrast, you can also write a single field of STRING type into Kafka topic as an anonymous string value.\n+\n+Format Options\n+----------------\n+\n+<table class=\"table table-bordered\">\n+    <thead>\n+      <tr>\n+        <th class=\"text-left\" style=\"width: 25%\">Option</th>\n+        <th class=\"text-center\" style=\"width: 8%\">Required</th>\n+        <th class=\"text-center\" style=\"width: 7%\">Default</th>\n+        <th class=\"text-center\" style=\"width: 10%\">Type</th>\n+        <th class=\"text-center\" style=\"width: 50%\">Description</th>\n+      </tr>\n+    </thead>\n+    <tbody>\n+    <tr>\n+      <td><h5>format</h5></td>\n+      <td>required</td>\n+      <td style=\"word-wrap: break-word;\">(none)</td>\n+      <td>String</td>\n+      <td>Specify what format to use, here should be 'single-field'.</td>\n+    </tr>\n+    </tbody>\n+</table>\n+\n+Data Type Mapping\n+----------------\n+\n+The table below details the SQL types the format supports, including details of the serializer and deserializer class for encoding and decoding.\n+\n+<table class=\"table table-bordered\">\n+    <thead>\n+      <tr>\n+        <th class=\"text-left\">Flink SQL type</th>\n+        <th class=\"text-left\">Value</th>\n+      </tr>\n+    </thead>\n+    <tbody>\n+    <tr>\n+      <td><code>CHAR / VARCHAR / STRING</code></td>\n+      <td>A UTF-8 encoded text string</td>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8de5115aa0bbc4bc486dced3d2bc67d34823659f"}, "originalPosition": 119}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0Njc2MDk0OnYy", "diffSide": "RIGHT", "path": "docs/dev/table/connectors/formats/single-field.md", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQxMjozMDoyOFrOHuBHtg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQxNToyOTo0NlrOHuIsDA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODAxNDkwMg==", "bodyText": "let's also support BINARY/VARBINARY/BYTES and the RAW type itself to finalize this story", "url": "https://github.com/apache/flink/pull/13909#discussion_r518014902", "createdAt": "2020-11-05T12:30:28Z", "author": {"login": "twalthr"}, "path": "docs/dev/table/connectors/formats/single-field.md", "diffHunk": "@@ -0,0 +1,151 @@\n+---\n+title: \"Single Field Format\"\n+nav-title: SingleField\n+nav-parent_id: sql-formats\n+nav-pos: 7\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+<span class=\"label label-info\">Format: Serialization Schema</span>\n+<span class=\"label label-info\">Format: Deserialization Schema</span>\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+The SingleField format allows to read and write data where the data contains only a single field, and that field is not wrapped within a JSON object, or an Avro record.\n+\n+Currently, the SingleField format supports `String`, `byte[]` and primitive type.\n+\n+Note: this format encodes `null` values as `null` `byte[]`. This may have limitation when used in `upsert-kafka`, because `upsert-kafka` treats `null` values as a tombstone message (DELETE on the key). Therefore, we recommend avoiding using `upsert-kafka` connector and `single-field` format if the field can have a `null` value.\n+\n+Example\n+----------------\n+\n+For example, you may have following raw log data in Kafka and want to read and analyse such data using Flink SQL.\n+\n+```\n+47.29.201.179 - - [28/Feb/2019:13:17:10 +0000] \"GET /?p=1 HTTP/2.0\" 200 5316 \"https://domain.com/?p=1\" \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36\" \"2.75\"\n+```\n+\n+The following creates a table where it reads from (and writes to) the underlying Kafka topic as an anonymous string value by using `single-field` format:\n+\n+<div class=\"codetabs\" markdown=\"1\">\n+<div data-lang=\"SQL\" markdown=\"1\">\n+{% highlight sql %}\n+CREATE TABLE nginx_log (\n+  log STRING\n+) WITH (\n+  'connector' = 'kafka',\n+  'topic' = 'nginx_log',\n+  'properties.bootstrap.servers' = 'localhost:9092',\n+  'properties.group.id' = 'testGroup',\n+  'format' = 'single-field'\n+)\n+{% endhighlight %}\n+</div>\n+</div>\n+\n+Then you can read out the raw data as a pure string, and split it into multiple fields using user-defined-function for further analysing, e.g. `my_split` in the example.\n+\n+<div class=\"codetabs\" markdown=\"1\">\n+<div data-lang=\"SQL\" markdown=\"1\">\n+{% highlight sql %}\n+SELECT t.hostname, t.datetime, t.url, t.browser, ...\n+FROM(\n+  SELECT my_split(log) as t FROM nginx_log\n+);\n+{% endhighlight %}\n+</div>\n+</div>\n+\n+In contrast, you can also write a single field of STRING type into Kafka topic as an anonymous string value.\n+\n+Format Options\n+----------------\n+\n+<table class=\"table table-bordered\">\n+    <thead>\n+      <tr>\n+        <th class=\"text-left\" style=\"width: 25%\">Option</th>\n+        <th class=\"text-center\" style=\"width: 8%\">Required</th>\n+        <th class=\"text-center\" style=\"width: 7%\">Default</th>\n+        <th class=\"text-center\" style=\"width: 10%\">Type</th>\n+        <th class=\"text-center\" style=\"width: 50%\">Description</th>\n+      </tr>\n+    </thead>\n+    <tbody>\n+    <tr>\n+      <td><h5>format</h5></td>\n+      <td>required</td>\n+      <td style=\"word-wrap: break-word;\">(none)</td>\n+      <td>String</td>\n+      <td>Specify what format to use, here should be 'single-field'.</td>\n+    </tr>\n+    </tbody>\n+</table>\n+\n+Data Type Mapping\n+----------------\n+\n+The table below details the SQL types the format supports, including details of the serializer and deserializer class for encoding and decoding.\n+\n+<table class=\"table table-bordered\">\n+    <thead>\n+      <tr>\n+        <th class=\"text-left\">Flink SQL type</th>\n+        <th class=\"text-left\">Value</th>\n+      </tr>\n+    </thead>\n+    <tbody>\n+    <tr>\n+      <td><code>CHAR / VARCHAR / STRING</code></td>\n+      <td>A UTF-8 encoded text string</td>\n+    </tr>\n+    <tr>\n+      <td><code>BOOLEAN</code></td>\n+      <td>A single byte to indicate boolean value, 0 means false, 1 means true.</td>\n+    </tr>\n+    <tr>\n+      <td><code>TINYINT</code></td>\n+      <td>A 8-bit signed number</td>\n+    </tr>\n+    <tr>\n+      <td><code>SMALLINT</code></td>\n+      <td>A 16-bit signed number</td>\n+    </tr>\n+    <tr>\n+      <td><code>INT</code></td>\n+      <td>A 32-bit signed integer</td>\n+    </tr>\n+    <tr>\n+      <td><code>BIGINT</code></td>\n+      <td>A 64-bit signed integer</td>\n+    </tr>\n+    <tr>\n+      <td><code>FLOAT</code></td>\n+      <td>A 32-bit floating point number</td>\n+    </tr>\n+    <tr>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8de5115aa0bbc4bc486dced3d2bc67d34823659f"}, "originalPosition": 145}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODEyMDI5OQ==", "bodyText": "I'm not sure about RAW type, because currently it's hard to declare RAW type in DDL.\nAs an alternative, users can declare it as BYTES and use UDF to deserialize the bytes.", "url": "https://github.com/apache/flink/pull/13909#discussion_r518120299", "createdAt": "2020-11-05T15:05:01Z", "author": {"login": "wuchong"}, "path": "docs/dev/table/connectors/formats/single-field.md", "diffHunk": "@@ -0,0 +1,151 @@\n+---\n+title: \"Single Field Format\"\n+nav-title: SingleField\n+nav-parent_id: sql-formats\n+nav-pos: 7\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+<span class=\"label label-info\">Format: Serialization Schema</span>\n+<span class=\"label label-info\">Format: Deserialization Schema</span>\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+The SingleField format allows to read and write data where the data contains only a single field, and that field is not wrapped within a JSON object, or an Avro record.\n+\n+Currently, the SingleField format supports `String`, `byte[]` and primitive type.\n+\n+Note: this format encodes `null` values as `null` `byte[]`. This may have limitation when used in `upsert-kafka`, because `upsert-kafka` treats `null` values as a tombstone message (DELETE on the key). Therefore, we recommend avoiding using `upsert-kafka` connector and `single-field` format if the field can have a `null` value.\n+\n+Example\n+----------------\n+\n+For example, you may have following raw log data in Kafka and want to read and analyse such data using Flink SQL.\n+\n+```\n+47.29.201.179 - - [28/Feb/2019:13:17:10 +0000] \"GET /?p=1 HTTP/2.0\" 200 5316 \"https://domain.com/?p=1\" \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36\" \"2.75\"\n+```\n+\n+The following creates a table where it reads from (and writes to) the underlying Kafka topic as an anonymous string value by using `single-field` format:\n+\n+<div class=\"codetabs\" markdown=\"1\">\n+<div data-lang=\"SQL\" markdown=\"1\">\n+{% highlight sql %}\n+CREATE TABLE nginx_log (\n+  log STRING\n+) WITH (\n+  'connector' = 'kafka',\n+  'topic' = 'nginx_log',\n+  'properties.bootstrap.servers' = 'localhost:9092',\n+  'properties.group.id' = 'testGroup',\n+  'format' = 'single-field'\n+)\n+{% endhighlight %}\n+</div>\n+</div>\n+\n+Then you can read out the raw data as a pure string, and split it into multiple fields using user-defined-function for further analysing, e.g. `my_split` in the example.\n+\n+<div class=\"codetabs\" markdown=\"1\">\n+<div data-lang=\"SQL\" markdown=\"1\">\n+{% highlight sql %}\n+SELECT t.hostname, t.datetime, t.url, t.browser, ...\n+FROM(\n+  SELECT my_split(log) as t FROM nginx_log\n+);\n+{% endhighlight %}\n+</div>\n+</div>\n+\n+In contrast, you can also write a single field of STRING type into Kafka topic as an anonymous string value.\n+\n+Format Options\n+----------------\n+\n+<table class=\"table table-bordered\">\n+    <thead>\n+      <tr>\n+        <th class=\"text-left\" style=\"width: 25%\">Option</th>\n+        <th class=\"text-center\" style=\"width: 8%\">Required</th>\n+        <th class=\"text-center\" style=\"width: 7%\">Default</th>\n+        <th class=\"text-center\" style=\"width: 10%\">Type</th>\n+        <th class=\"text-center\" style=\"width: 50%\">Description</th>\n+      </tr>\n+    </thead>\n+    <tbody>\n+    <tr>\n+      <td><h5>format</h5></td>\n+      <td>required</td>\n+      <td style=\"word-wrap: break-word;\">(none)</td>\n+      <td>String</td>\n+      <td>Specify what format to use, here should be 'single-field'.</td>\n+    </tr>\n+    </tbody>\n+</table>\n+\n+Data Type Mapping\n+----------------\n+\n+The table below details the SQL types the format supports, including details of the serializer and deserializer class for encoding and decoding.\n+\n+<table class=\"table table-bordered\">\n+    <thead>\n+      <tr>\n+        <th class=\"text-left\">Flink SQL type</th>\n+        <th class=\"text-left\">Value</th>\n+      </tr>\n+    </thead>\n+    <tbody>\n+    <tr>\n+      <td><code>CHAR / VARCHAR / STRING</code></td>\n+      <td>A UTF-8 encoded text string</td>\n+    </tr>\n+    <tr>\n+      <td><code>BOOLEAN</code></td>\n+      <td>A single byte to indicate boolean value, 0 means false, 1 means true.</td>\n+    </tr>\n+    <tr>\n+      <td><code>TINYINT</code></td>\n+      <td>A 8-bit signed number</td>\n+    </tr>\n+    <tr>\n+      <td><code>SMALLINT</code></td>\n+      <td>A 16-bit signed number</td>\n+    </tr>\n+    <tr>\n+      <td><code>INT</code></td>\n+      <td>A 32-bit signed integer</td>\n+    </tr>\n+    <tr>\n+      <td><code>BIGINT</code></td>\n+      <td>A 64-bit signed integer</td>\n+    </tr>\n+    <tr>\n+      <td><code>FLOAT</code></td>\n+      <td>A 32-bit floating point number</td>\n+    </tr>\n+    <tr>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODAxNDkwMg=="}, "originalCommit": {"oid": "8de5115aa0bbc4bc486dced3d2bc67d34823659f"}, "originalPosition": 145}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODEzODg5Mg==", "bodyText": "True, it is difficult to declare the RAW type in DDL. But this is an orthogonal topic that might be solved soon. We should support all data types for our implementations otherwise we need to multiple iterations to get the connectors/formats into a usable state. IMHO adding the RAW here is not a big overhead.", "url": "https://github.com/apache/flink/pull/13909#discussion_r518138892", "createdAt": "2020-11-05T15:29:46Z", "author": {"login": "twalthr"}, "path": "docs/dev/table/connectors/formats/single-field.md", "diffHunk": "@@ -0,0 +1,151 @@\n+---\n+title: \"Single Field Format\"\n+nav-title: SingleField\n+nav-parent_id: sql-formats\n+nav-pos: 7\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+<span class=\"label label-info\">Format: Serialization Schema</span>\n+<span class=\"label label-info\">Format: Deserialization Schema</span>\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+The SingleField format allows to read and write data where the data contains only a single field, and that field is not wrapped within a JSON object, or an Avro record.\n+\n+Currently, the SingleField format supports `String`, `byte[]` and primitive type.\n+\n+Note: this format encodes `null` values as `null` `byte[]`. This may have limitation when used in `upsert-kafka`, because `upsert-kafka` treats `null` values as a tombstone message (DELETE on the key). Therefore, we recommend avoiding using `upsert-kafka` connector and `single-field` format if the field can have a `null` value.\n+\n+Example\n+----------------\n+\n+For example, you may have following raw log data in Kafka and want to read and analyse such data using Flink SQL.\n+\n+```\n+47.29.201.179 - - [28/Feb/2019:13:17:10 +0000] \"GET /?p=1 HTTP/2.0\" 200 5316 \"https://domain.com/?p=1\" \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36\" \"2.75\"\n+```\n+\n+The following creates a table where it reads from (and writes to) the underlying Kafka topic as an anonymous string value by using `single-field` format:\n+\n+<div class=\"codetabs\" markdown=\"1\">\n+<div data-lang=\"SQL\" markdown=\"1\">\n+{% highlight sql %}\n+CREATE TABLE nginx_log (\n+  log STRING\n+) WITH (\n+  'connector' = 'kafka',\n+  'topic' = 'nginx_log',\n+  'properties.bootstrap.servers' = 'localhost:9092',\n+  'properties.group.id' = 'testGroup',\n+  'format' = 'single-field'\n+)\n+{% endhighlight %}\n+</div>\n+</div>\n+\n+Then you can read out the raw data as a pure string, and split it into multiple fields using user-defined-function for further analysing, e.g. `my_split` in the example.\n+\n+<div class=\"codetabs\" markdown=\"1\">\n+<div data-lang=\"SQL\" markdown=\"1\">\n+{% highlight sql %}\n+SELECT t.hostname, t.datetime, t.url, t.browser, ...\n+FROM(\n+  SELECT my_split(log) as t FROM nginx_log\n+);\n+{% endhighlight %}\n+</div>\n+</div>\n+\n+In contrast, you can also write a single field of STRING type into Kafka topic as an anonymous string value.\n+\n+Format Options\n+----------------\n+\n+<table class=\"table table-bordered\">\n+    <thead>\n+      <tr>\n+        <th class=\"text-left\" style=\"width: 25%\">Option</th>\n+        <th class=\"text-center\" style=\"width: 8%\">Required</th>\n+        <th class=\"text-center\" style=\"width: 7%\">Default</th>\n+        <th class=\"text-center\" style=\"width: 10%\">Type</th>\n+        <th class=\"text-center\" style=\"width: 50%\">Description</th>\n+      </tr>\n+    </thead>\n+    <tbody>\n+    <tr>\n+      <td><h5>format</h5></td>\n+      <td>required</td>\n+      <td style=\"word-wrap: break-word;\">(none)</td>\n+      <td>String</td>\n+      <td>Specify what format to use, here should be 'single-field'.</td>\n+    </tr>\n+    </tbody>\n+</table>\n+\n+Data Type Mapping\n+----------------\n+\n+The table below details the SQL types the format supports, including details of the serializer and deserializer class for encoding and decoding.\n+\n+<table class=\"table table-bordered\">\n+    <thead>\n+      <tr>\n+        <th class=\"text-left\">Flink SQL type</th>\n+        <th class=\"text-left\">Value</th>\n+      </tr>\n+    </thead>\n+    <tbody>\n+    <tr>\n+      <td><code>CHAR / VARCHAR / STRING</code></td>\n+      <td>A UTF-8 encoded text string</td>\n+    </tr>\n+    <tr>\n+      <td><code>BOOLEAN</code></td>\n+      <td>A single byte to indicate boolean value, 0 means false, 1 means true.</td>\n+    </tr>\n+    <tr>\n+      <td><code>TINYINT</code></td>\n+      <td>A 8-bit signed number</td>\n+    </tr>\n+    <tr>\n+      <td><code>SMALLINT</code></td>\n+      <td>A 16-bit signed number</td>\n+    </tr>\n+    <tr>\n+      <td><code>INT</code></td>\n+      <td>A 32-bit signed integer</td>\n+    </tr>\n+    <tr>\n+      <td><code>BIGINT</code></td>\n+      <td>A 64-bit signed integer</td>\n+    </tr>\n+    <tr>\n+      <td><code>FLOAT</code></td>\n+      <td>A 32-bit floating point number</td>\n+    </tr>\n+    <tr>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODAxNDkwMg=="}, "originalCommit": {"oid": "8de5115aa0bbc4bc486dced3d2bc67d34823659f"}, "originalPosition": 145}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0Njc2ODI0OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/formats/singlefield/SingleFieldDeserializationSchema.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQxMjozMjozNlrOHuBMFg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQxMjozMjozNlrOHuBMFg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODAxNjAyMg==", "bodyText": "nit: call thisproducedTypeInfo for consistency\nfinal?", "url": "https://github.com/apache/flink/pull/13909#discussion_r518016022", "createdAt": "2020-11-05T12:32:36Z", "author": {"login": "twalthr"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/formats/singlefield/SingleFieldDeserializationSchema.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.formats.singlefield;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.common.serialization.DeserializationSchema;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.common.typeutils.base.BooleanSerializer;\n+import org.apache.flink.api.common.typeutils.base.ByteSerializer;\n+import org.apache.flink.api.common.typeutils.base.DoubleSerializer;\n+import org.apache.flink.api.common.typeutils.base.FloatSerializer;\n+import org.apache.flink.api.common.typeutils.base.IntSerializer;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.api.common.typeutils.base.ShortSerializer;\n+import org.apache.flink.core.memory.DataInputDeserializer;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.types.logical.LogicalType;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.Objects;\n+\n+import static org.apache.flink.util.Preconditions.checkNotNull;\n+\n+/**\n+ * Deserialization schema from single field data to Flink Table/SQL internal data structure {@link RowData}.\n+ */\n+@Internal\n+public class SingleFieldDeserializationSchema implements DeserializationSchema<RowData> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate final LogicalType fieldType;\n+\tprivate final DeserializationRuntimeConverter converter;\n+\tprivate TypeInformation<RowData> typeInfo;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8de5115aa0bbc4bc486dced3d2bc67d34823659f"}, "originalPosition": 54}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0Njc2OTY3OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/formats/singlefield/SingleFieldDeserializationSchema.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQxMjozMzowM1rOHuBNBA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQxMjozMzowM1rOHuBNBA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODAxNjI2MA==", "bodyText": "nit: call this deserializedType?", "url": "https://github.com/apache/flink/pull/13909#discussion_r518016260", "createdAt": "2020-11-05T12:33:03Z", "author": {"login": "twalthr"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/formats/singlefield/SingleFieldDeserializationSchema.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.formats.singlefield;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.common.serialization.DeserializationSchema;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.common.typeutils.base.BooleanSerializer;\n+import org.apache.flink.api.common.typeutils.base.ByteSerializer;\n+import org.apache.flink.api.common.typeutils.base.DoubleSerializer;\n+import org.apache.flink.api.common.typeutils.base.FloatSerializer;\n+import org.apache.flink.api.common.typeutils.base.IntSerializer;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.api.common.typeutils.base.ShortSerializer;\n+import org.apache.flink.core.memory.DataInputDeserializer;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.types.logical.LogicalType;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.Objects;\n+\n+import static org.apache.flink.util.Preconditions.checkNotNull;\n+\n+/**\n+ * Deserialization schema from single field data to Flink Table/SQL internal data structure {@link RowData}.\n+ */\n+@Internal\n+public class SingleFieldDeserializationSchema implements DeserializationSchema<RowData> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate final LogicalType fieldType;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8de5115aa0bbc4bc486dced3d2bc67d34823659f"}, "originalPosition": 52}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0NjgxNTI1OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/formats/singlefield/SingleFieldDeserializationSchema.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQxMjo0NTozMFrOHuBo0A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQxMjo0NTozMFrOHuBo0A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODAyMzM3Ng==", "bodyText": "Would we have a performance benefit of implementing the deserialization logic ourselves? instead of setting fields and delegating to other classes.", "url": "https://github.com/apache/flink/pull/13909#discussion_r518023376", "createdAt": "2020-11-05T12:45:30Z", "author": {"login": "twalthr"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/formats/singlefield/SingleFieldDeserializationSchema.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.formats.singlefield;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.common.serialization.DeserializationSchema;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.common.typeutils.base.BooleanSerializer;\n+import org.apache.flink.api.common.typeutils.base.ByteSerializer;\n+import org.apache.flink.api.common.typeutils.base.DoubleSerializer;\n+import org.apache.flink.api.common.typeutils.base.FloatSerializer;\n+import org.apache.flink.api.common.typeutils.base.IntSerializer;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.api.common.typeutils.base.ShortSerializer;\n+import org.apache.flink.core.memory.DataInputDeserializer;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.types.logical.LogicalType;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.Objects;\n+\n+import static org.apache.flink.util.Preconditions.checkNotNull;\n+\n+/**\n+ * Deserialization schema from single field data to Flink Table/SQL internal data structure {@link RowData}.\n+ */\n+@Internal\n+public class SingleFieldDeserializationSchema implements DeserializationSchema<RowData> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate final LogicalType fieldType;\n+\tprivate final DeserializationRuntimeConverter converter;\n+\tprivate TypeInformation<RowData> typeInfo;\n+\n+\tprivate transient GenericRowData reuse;\n+\n+\tpublic SingleFieldDeserializationSchema(\n+\t\t\tLogicalType fieldType,\n+\t\t\tTypeInformation<RowData> resultTypeInfo) {\n+\t\tthis.fieldType = checkNotNull(fieldType);\n+\t\tthis.typeInfo = checkNotNull(resultTypeInfo);\n+\t\tthis.converter = createConverter(fieldType);\n+\t}\n+\n+\t@Override\n+\tpublic void open(InitializationContext context) throws Exception {\n+\t\treuse = new GenericRowData(1);\n+\t}\n+\n+\t@Override\n+\tpublic RowData deserialize(byte[] message) throws IOException {\n+\t\treuse.setField(0, converter.convert(message));\n+\t\treturn reuse;\n+\t}\n+\n+\t@Override\n+\tpublic boolean isEndOfStream(RowData nextElement) {\n+\t\treturn false;\n+\t}\n+\n+\t@Override\n+\tpublic TypeInformation<RowData> getProducedType() {\n+\t\treturn typeInfo;\n+\t}\n+\n+\t@Override\n+\tpublic boolean equals(Object o) {\n+\t\tif (this == o) {\n+\t\t\treturn true;\n+\t\t}\n+\t\tif (o == null || getClass() != o.getClass()) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tSingleFieldDeserializationSchema that = (SingleFieldDeserializationSchema) o;\n+\t\treturn typeInfo.equals(that.typeInfo) && fieldType.equals(that.fieldType);\n+\t}\n+\n+\t@Override\n+\tpublic int hashCode() {\n+\t\treturn Objects.hash(typeInfo, fieldType);\n+\t}\n+\n+\t// ------------------------------------------------------------------------\n+\n+\t/**\n+\t * Runtime converter that convert byte[] to internal data structure object.\n+\t */\n+\t@FunctionalInterface\n+\tprivate interface DeserializationRuntimeConverter extends Serializable {\n+\t\tObject convert(byte[] data) throws IOException;\n+\t}\n+\n+\t/**\n+\t * Creates a runtime converter.\n+\t */\n+\tprivate static DeserializationRuntimeConverter createConverter(LogicalType type) {\n+\t\tswitch (type.getTypeRoot()) {\n+\t\t\tcase CHAR:\n+\t\t\tcase VARCHAR:\n+\t\t\t\treturn data -> {\n+\t\t\t\t\t// put null check logic in the lambda instead of wrapping outside\n+\t\t\t\t\t// to avoid virtual method invoking.\n+\t\t\t\t\tif (data == null) {\n+\t\t\t\t\t\treturn null;\n+\t\t\t\t\t}\n+\t\t\t\t\treturn StringData.fromBytes(data);\n+\t\t\t\t};\n+\t\t\tcase VARBINARY:\n+\t\t\tcase BINARY:\n+\t\t\t\treturn data -> data;\n+\t\t\tcase TINYINT:\n+\t\t\t\treturn createConverterUsingSerializer(ByteSerializer.INSTANCE);\n+\t\t\tcase SMALLINT:\n+\t\t\t\treturn createConverterUsingSerializer(ShortSerializer.INSTANCE);\n+\t\t\tcase INTEGER:\n+\t\t\t\treturn createConverterUsingSerializer(IntSerializer.INSTANCE);\n+\t\t\tcase BIGINT:\n+\t\t\t\treturn createConverterUsingSerializer(LongSerializer.INSTANCE);\n+\t\t\tcase FLOAT:\n+\t\t\t\treturn createConverterUsingSerializer(FloatSerializer.INSTANCE);\n+\t\t\tcase DOUBLE:\n+\t\t\t\treturn createConverterUsingSerializer(DoubleSerializer.INSTANCE);\n+\t\t\tcase BOOLEAN:\n+\t\t\t\treturn createConverterUsingSerializer(BooleanSerializer.INSTANCE);\n+\t\t\tdefault:\n+\t\t\t\tthrow new UnsupportedOperationException(\"'single-format' currently doesn't support type: \" + type);\n+\t\t}\n+\t}\n+\n+\tprivate static DeserializationRuntimeConverter createConverterUsingSerializer(\n+\t\t\tfinal TypeSerializer<?> serializer) {\n+\t\treturn new DelegatingDeserializationConverter(serializer);\n+\t}\n+\n+\tprivate static final class DelegatingDeserializationConverter\n+\t\timplements DeserializationRuntimeConverter {\n+\t\tprivate static final long serialVersionUID = 1L;\n+\t\tprivate final DataInputDeserializer source = new DataInputDeserializer();\n+\t\tprivate final TypeSerializer<?> serializer;\n+\n+\t\tprotected DelegatingDeserializationConverter(TypeSerializer<?> serializer) {\n+\t\t\tthis.serializer = serializer;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic Object convert(byte[] data) throws IOException {\n+\t\t\tif (data == null) {\n+\t\t\t\treturn null;\n+\t\t\t}\n+\t\t\tsource.setBuffer(data);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8de5115aa0bbc4bc486dced3d2bc67d34823659f"}, "originalPosition": 171}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0NjgyMzU3OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/formats/singlefield/SingleFieldSerializationSchema.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQxMjo0Nzo0NFrOHuBtxA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQxMjo0Nzo0NFrOHuBtxA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODAyNDY0NA==", "bodyText": "Same comment as before, this looks overly complicated to just convert a a couple of data types to bytes. We should think about doing it manually.", "url": "https://github.com/apache/flink/pull/13909#discussion_r518024644", "createdAt": "2020-11-05T12:47:44Z", "author": {"login": "twalthr"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/formats/singlefield/SingleFieldSerializationSchema.java", "diffHunk": "@@ -0,0 +1,166 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.formats.singlefield;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.common.serialization.SerializationSchema;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.common.typeutils.base.BooleanSerializer;\n+import org.apache.flink.api.common.typeutils.base.ByteSerializer;\n+import org.apache.flink.api.common.typeutils.base.DoubleSerializer;\n+import org.apache.flink.api.common.typeutils.base.FloatSerializer;\n+import org.apache.flink.api.common.typeutils.base.IntSerializer;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.api.common.typeutils.base.ShortSerializer;\n+import org.apache.flink.core.memory.DataOutputSerializer;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.RowData.FieldGetter;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.types.logical.LogicalType;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.Objects;\n+\n+/**\n+ * Serialization schema that serializes an {@link RowData} object into a single field bytes.\n+ */\n+@Internal\n+public class SingleFieldSerializationSchema implements SerializationSchema<RowData> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate final LogicalType fieldType;\n+\n+\tprivate final SerializationRuntimeConverter converter;\n+\n+\tprivate final FieldGetter fieldGetter;\n+\n+\tpublic SingleFieldSerializationSchema(LogicalType fieldType) {\n+\t\tthis.fieldType = fieldType;\n+\t\tthis.fieldGetter = RowData.createFieldGetter(fieldType, 0);\n+\t\tthis.converter = createConverter(fieldType);\n+\t}\n+\n+\t@Override\n+\tpublic byte[] serialize(RowData row) {\n+\t\ttry {\n+\t\t\treturn converter.convert(fieldGetter.getFieldOrNull(row));\n+\t\t} catch (IOException e) {\n+\t\t\tthrow new RuntimeException(\"Could not serialize row '\" + row + \"'. \", e);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic boolean equals(Object o) {\n+\t\tif (this == o) {\n+\t\t\treturn true;\n+\t\t}\n+\t\tif (o == null || getClass() != o.getClass()) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tSingleFieldSerializationSchema that = (SingleFieldSerializationSchema) o;\n+\t\treturn fieldType.equals(that.fieldType);\n+\t}\n+\n+\t@Override\n+\tpublic int hashCode() {\n+\t\treturn Objects.hash(fieldType);\n+\t}\n+\n+\t// ------------------------------------------------------------------------\n+\n+\t/**\n+\t * Runtime converter that convert an object of internal data structure to byte[].\n+\t */\n+\t@FunctionalInterface\n+\tprivate interface SerializationRuntimeConverter extends Serializable {\n+\t\tbyte[] convert(Object value) throws IOException;\n+\t}\n+\n+\t/**\n+\t * Creates a runtime converter.\n+\t */\n+\tprivate SerializationRuntimeConverter createConverter(LogicalType type) {\n+\t\tswitch (type.getTypeRoot()) {\n+\t\t\tcase CHAR:\n+\t\t\tcase VARCHAR:\n+\t\t\t\treturn value -> {\n+\t\t\t\t\t// put null check logic in the lambda instead of wrapping outside\n+\t\t\t\t\t// to avoid virtual method invoking.\n+\t\t\t\t\tif (value == null) {\n+\t\t\t\t\t\treturn null;\n+\t\t\t\t\t}\n+\t\t\t\t\treturn ((StringData) value).toBytes();\n+\t\t\t\t};\n+\t\t\tcase VARBINARY:\n+\t\t\tcase BINARY:\n+\t\t\t\treturn value -> {\n+\t\t\t\t\tif (value == null) {\n+\t\t\t\t\t\treturn null;\n+\t\t\t\t\t}\n+\t\t\t\t\treturn (byte[]) value;\n+\t\t\t\t};\n+\t\t\tcase TINYINT:\n+\t\t\t\treturn createConverterUsingSerializer(ByteSerializer.INSTANCE);\n+\t\t\tcase SMALLINT:\n+\t\t\t\treturn createConverterUsingSerializer(ShortSerializer.INSTANCE);\n+\t\t\tcase INTEGER:\n+\t\t\t\treturn createConverterUsingSerializer(IntSerializer.INSTANCE);\n+\t\t\tcase BIGINT:\n+\t\t\t\treturn createConverterUsingSerializer(LongSerializer.INSTANCE);\n+\t\t\tcase FLOAT:\n+\t\t\t\treturn createConverterUsingSerializer(FloatSerializer.INSTANCE);\n+\t\t\tcase DOUBLE:\n+\t\t\t\treturn createConverterUsingSerializer(DoubleSerializer.INSTANCE);\n+\t\t\tcase BOOLEAN:\n+\t\t\t\treturn createConverterUsingSerializer(BooleanSerializer.INSTANCE);\n+\t\t\tdefault:\n+\t\t\t\tthrow new UnsupportedOperationException(\"'single-format' currently doesn't support type: \" + type);\n+\t\t}\n+\t}\n+\n+\t@SuppressWarnings(\"unchecked\")\n+\tprivate static SerializationRuntimeConverter createConverterUsingSerializer(\n+\t\t\tTypeSerializer<?> serializer) {\n+\t\treturn new DelegatingSerializationConverter((TypeSerializer<Object>) serializer);\n+\t}\n+\n+\tprivate static final class DelegatingSerializationConverter\n+\t\timplements SerializationRuntimeConverter {\n+\t\tprivate static final long serialVersionUID = 1L;\n+\t\tprivate final DataOutputSerializer dos = new DataOutputSerializer(16);\n+\t\tprivate final TypeSerializer<Object> delegatingSerializer;\n+\n+\t\tprotected DelegatingSerializationConverter(TypeSerializer<Object> delegatingSerializer) {\n+\t\t\tthis.delegatingSerializer = delegatingSerializer;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic byte[] convert(Object value) throws IOException {\n+\t\t\tif (value == null) {\n+\t\t\t\treturn null;\n+\t\t\t}\n+\t\t\tdelegatingSerializer.serialize(value, dos);\n+\t\t\tbyte[] ret = dos.getCopyOfBuffer();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8de5115aa0bbc4bc486dced3d2bc67d34823659f"}, "originalPosition": 161}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0Njg0MTY3OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/formats/singlefield/SingleFieldSerializationSchema.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQxMjo1Mjo0OFrOHuB45A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQxNToyNjoyM1rOHuIiDw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODAyNzQ5Mg==", "bodyText": "Is this Flink's representation of strings in bytes? I could imagine that most people assume \"string\".getBytes() semantics. Is the string length included in the bytes are as well?", "url": "https://github.com/apache/flink/pull/13909#discussion_r518027492", "createdAt": "2020-11-05T12:52:48Z", "author": {"login": "twalthr"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/formats/singlefield/SingleFieldSerializationSchema.java", "diffHunk": "@@ -0,0 +1,166 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.formats.singlefield;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.common.serialization.SerializationSchema;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.common.typeutils.base.BooleanSerializer;\n+import org.apache.flink.api.common.typeutils.base.ByteSerializer;\n+import org.apache.flink.api.common.typeutils.base.DoubleSerializer;\n+import org.apache.flink.api.common.typeutils.base.FloatSerializer;\n+import org.apache.flink.api.common.typeutils.base.IntSerializer;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.api.common.typeutils.base.ShortSerializer;\n+import org.apache.flink.core.memory.DataOutputSerializer;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.RowData.FieldGetter;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.types.logical.LogicalType;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.Objects;\n+\n+/**\n+ * Serialization schema that serializes an {@link RowData} object into a single field bytes.\n+ */\n+@Internal\n+public class SingleFieldSerializationSchema implements SerializationSchema<RowData> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate final LogicalType fieldType;\n+\n+\tprivate final SerializationRuntimeConverter converter;\n+\n+\tprivate final FieldGetter fieldGetter;\n+\n+\tpublic SingleFieldSerializationSchema(LogicalType fieldType) {\n+\t\tthis.fieldType = fieldType;\n+\t\tthis.fieldGetter = RowData.createFieldGetter(fieldType, 0);\n+\t\tthis.converter = createConverter(fieldType);\n+\t}\n+\n+\t@Override\n+\tpublic byte[] serialize(RowData row) {\n+\t\ttry {\n+\t\t\treturn converter.convert(fieldGetter.getFieldOrNull(row));\n+\t\t} catch (IOException e) {\n+\t\t\tthrow new RuntimeException(\"Could not serialize row '\" + row + \"'. \", e);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic boolean equals(Object o) {\n+\t\tif (this == o) {\n+\t\t\treturn true;\n+\t\t}\n+\t\tif (o == null || getClass() != o.getClass()) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tSingleFieldSerializationSchema that = (SingleFieldSerializationSchema) o;\n+\t\treturn fieldType.equals(that.fieldType);\n+\t}\n+\n+\t@Override\n+\tpublic int hashCode() {\n+\t\treturn Objects.hash(fieldType);\n+\t}\n+\n+\t// ------------------------------------------------------------------------\n+\n+\t/**\n+\t * Runtime converter that convert an object of internal data structure to byte[].\n+\t */\n+\t@FunctionalInterface\n+\tprivate interface SerializationRuntimeConverter extends Serializable {\n+\t\tbyte[] convert(Object value) throws IOException;\n+\t}\n+\n+\t/**\n+\t * Creates a runtime converter.\n+\t */\n+\tprivate SerializationRuntimeConverter createConverter(LogicalType type) {\n+\t\tswitch (type.getTypeRoot()) {\n+\t\t\tcase CHAR:\n+\t\t\tcase VARCHAR:\n+\t\t\t\treturn value -> {\n+\t\t\t\t\t// put null check logic in the lambda instead of wrapping outside\n+\t\t\t\t\t// to avoid virtual method invoking.\n+\t\t\t\t\tif (value == null) {\n+\t\t\t\t\t\treturn null;\n+\t\t\t\t\t}\n+\t\t\t\t\treturn ((StringData) value).toBytes();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8de5115aa0bbc4bc486dced3d2bc67d34823659f"}, "originalPosition": 110}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODEzNjMzNQ==", "bodyText": "I'm sure this is the same semantic of \"string\".getBytes(), becuase we are using StringData.fromBytes(stringBytes) to save decoding overhead (stringBytes to String) in the sources.\nThe string length is not included in the bytes. The length is appended by the StringDataSerializer.", "url": "https://github.com/apache/flink/pull/13909#discussion_r518136335", "createdAt": "2020-11-05T15:26:23Z", "author": {"login": "wuchong"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/formats/singlefield/SingleFieldSerializationSchema.java", "diffHunk": "@@ -0,0 +1,166 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.formats.singlefield;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.common.serialization.SerializationSchema;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.common.typeutils.base.BooleanSerializer;\n+import org.apache.flink.api.common.typeutils.base.ByteSerializer;\n+import org.apache.flink.api.common.typeutils.base.DoubleSerializer;\n+import org.apache.flink.api.common.typeutils.base.FloatSerializer;\n+import org.apache.flink.api.common.typeutils.base.IntSerializer;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.api.common.typeutils.base.ShortSerializer;\n+import org.apache.flink.core.memory.DataOutputSerializer;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.RowData.FieldGetter;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.types.logical.LogicalType;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.Objects;\n+\n+/**\n+ * Serialization schema that serializes an {@link RowData} object into a single field bytes.\n+ */\n+@Internal\n+public class SingleFieldSerializationSchema implements SerializationSchema<RowData> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate final LogicalType fieldType;\n+\n+\tprivate final SerializationRuntimeConverter converter;\n+\n+\tprivate final FieldGetter fieldGetter;\n+\n+\tpublic SingleFieldSerializationSchema(LogicalType fieldType) {\n+\t\tthis.fieldType = fieldType;\n+\t\tthis.fieldGetter = RowData.createFieldGetter(fieldType, 0);\n+\t\tthis.converter = createConverter(fieldType);\n+\t}\n+\n+\t@Override\n+\tpublic byte[] serialize(RowData row) {\n+\t\ttry {\n+\t\t\treturn converter.convert(fieldGetter.getFieldOrNull(row));\n+\t\t} catch (IOException e) {\n+\t\t\tthrow new RuntimeException(\"Could not serialize row '\" + row + \"'. \", e);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic boolean equals(Object o) {\n+\t\tif (this == o) {\n+\t\t\treturn true;\n+\t\t}\n+\t\tif (o == null || getClass() != o.getClass()) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tSingleFieldSerializationSchema that = (SingleFieldSerializationSchema) o;\n+\t\treturn fieldType.equals(that.fieldType);\n+\t}\n+\n+\t@Override\n+\tpublic int hashCode() {\n+\t\treturn Objects.hash(fieldType);\n+\t}\n+\n+\t// ------------------------------------------------------------------------\n+\n+\t/**\n+\t * Runtime converter that convert an object of internal data structure to byte[].\n+\t */\n+\t@FunctionalInterface\n+\tprivate interface SerializationRuntimeConverter extends Serializable {\n+\t\tbyte[] convert(Object value) throws IOException;\n+\t}\n+\n+\t/**\n+\t * Creates a runtime converter.\n+\t */\n+\tprivate SerializationRuntimeConverter createConverter(LogicalType type) {\n+\t\tswitch (type.getTypeRoot()) {\n+\t\t\tcase CHAR:\n+\t\t\tcase VARCHAR:\n+\t\t\t\treturn value -> {\n+\t\t\t\t\t// put null check logic in the lambda instead of wrapping outside\n+\t\t\t\t\t// to avoid virtual method invoking.\n+\t\t\t\t\tif (value == null) {\n+\t\t\t\t\t\treturn null;\n+\t\t\t\t\t}\n+\t\t\t\t\treturn ((StringData) value).toBytes();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODAyNzQ5Mg=="}, "originalCommit": {"oid": "8de5115aa0bbc4bc486dced3d2bc67d34823659f"}, "originalPosition": 110}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0Njg3NTU2OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/formats/singlefield/SingleFieldSerDeSchemaTest.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQxMzowMTozOFrOHuCODg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQxMzowMTozOFrOHuCODg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODAzMjkxMA==", "bodyText": "Use a JUnit @Parameterized instead?", "url": "https://github.com/apache/flink/pull/13909#discussion_r518032910", "createdAt": "2020-11-05T13:01:38Z", "author": {"login": "twalthr"}, "path": "flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/formats/singlefield/SingleFieldSerDeSchemaTest.java", "diffHunk": "@@ -0,0 +1,170 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.formats.singlefield;\n+\n+import org.apache.flink.api.common.serialization.DeserializationSchema;\n+import org.apache.flink.api.common.serialization.SerializationSchema;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.conversion.DataStructureConverter;\n+import org.apache.flink.table.data.conversion.DataStructureConverters;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.types.Row;\n+\n+import org.junit.Test;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+\n+import static org.apache.flink.table.api.DataTypes.BIGINT;\n+import static org.apache.flink.table.api.DataTypes.BINARY;\n+import static org.apache.flink.table.api.DataTypes.BOOLEAN;\n+import static org.apache.flink.table.api.DataTypes.BYTES;\n+import static org.apache.flink.table.api.DataTypes.DOUBLE;\n+import static org.apache.flink.table.api.DataTypes.FIELD;\n+import static org.apache.flink.table.api.DataTypes.FLOAT;\n+import static org.apache.flink.table.api.DataTypes.INT;\n+import static org.apache.flink.table.api.DataTypes.ROW;\n+import static org.apache.flink.table.api.DataTypes.SMALLINT;\n+import static org.apache.flink.table.api.DataTypes.STRING;\n+import static org.apache.flink.table.api.DataTypes.TINYINT;\n+import static org.apache.flink.table.api.DataTypes.VARCHAR;\n+import static org.junit.Assert.assertEquals;\n+import static org.mockito.Mockito.mock;\n+\n+/**\n+ * Tests for {@link SingleFieldDeserializationSchema} {@link SingleFieldSerializationSchema}.\n+ */\n+public class SingleFieldSerDeSchemaTest {\n+\n+\t@Test\n+\tpublic void testSerializationAndDeserialization() throws Exception {\n+\t\tfor (TestSpec testSpec : testData) {\n+\t\t\trunTest(testSpec);\n+\t\t}\n+\t}\n+\n+\tprivate void runTest(TestSpec testSpec) throws Exception {\n+\t\tSingleFieldDeserializationSchema deserializationSchema = new SingleFieldDeserializationSchema(\n+\t\t\ttestSpec.type.getLogicalType(), TypeInformation.of(RowData.class));\n+\t\tSingleFieldSerializationSchema serializationSchema = new SingleFieldSerializationSchema(\n+\t\t\ttestSpec.type.getLogicalType());\n+\t\tdeserializationSchema.open(mock(DeserializationSchema.InitializationContext.class));\n+\t\tserializationSchema.open(mock(SerializationSchema.InitializationContext.class));\n+\n+\t\tRow row = Row.of(testSpec.value);\n+\t\tDataStructureConverter<Object, Object> converter = DataStructureConverters.getConverter(\n+\t\t\tROW(FIELD(\"single\", testSpec.type)));\n+\t\tRowData originalRowData = (RowData) converter.toInternal(row);\n+\n+\t\tbyte[] serializedBytes = serializationSchema.serialize(originalRowData);\n+\t\tRowData deserializeRowData = deserializationSchema.deserialize(serializedBytes);\n+\n+\t\tRow actual = (Row) converter.toExternal(deserializeRowData);\n+\t\tassertEquals(row, actual);\n+\t}\n+\n+\tprivate static List<TestSpec> testData = Arrays.asList(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8de5115aa0bbc4bc486dced3d2bc67d34823659f"}, "originalPosition": 83}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI1MTgwNzE4OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/formats/raw/RawFormatDeserializationSchema.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQxNDo1MTo1NlrOHuxFbg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQxNTo0MTozMVrOHuzGvw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODgwMDc1MA==", "bodyText": "can't we use RawValueData.fromBytes here and do lazy deserialization?", "url": "https://github.com/apache/flink/pull/13909#discussion_r518800750", "createdAt": "2020-11-06T14:51:56Z", "author": {"login": "twalthr"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/formats/raw/RawFormatDeserializationSchema.java", "diffHunk": "@@ -0,0 +1,343 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.formats.raw;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.common.serialization.DeserializationSchema;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.core.memory.DataInputDeserializer;\n+import org.apache.flink.core.memory.HeapMemorySegment;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RawValueData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RawType;\n+import org.apache.flink.types.DeserializationException;\n+import org.apache.flink.util.function.FunctionUtils;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.nio.charset.Charset;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Objects;\n+import java.util.function.Consumer;\n+\n+import static org.apache.flink.util.Preconditions.checkNotNull;\n+\n+/**\n+ * Deserialization schema from raw (byte based) value to Flink Table/SQL internal data structure {@link RowData}.\n+ */\n+@Internal\n+public class RawFormatDeserializationSchema implements DeserializationSchema<RowData> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate final LogicalType deserializedType;\n+\n+\tprivate final DeserializationRuntimeConverter converter;\n+\n+\tprivate final TypeInformation<RowData> producedTypeInfo;\n+\n+\tprivate final String charsetName;\n+\n+\tprivate final boolean isBigEndian;\n+\n+\tprivate transient GenericRowData reuse;\n+\n+\tpublic RawFormatDeserializationSchema(\n+\t\t\tLogicalType deserializedType,\n+\t\t\tTypeInformation<RowData> producedTypeInfo,\n+\t\t\tString charsetName,\n+\t\t\tboolean isBigEndian) {\n+\t\tthis.deserializedType = checkNotNull(deserializedType);\n+\t\tthis.producedTypeInfo = checkNotNull(producedTypeInfo);\n+\t\tthis.converter = createConverter(deserializedType, charsetName, isBigEndian);\n+\t\tthis.charsetName = charsetName;\n+\t\tthis.isBigEndian = isBigEndian;\n+\t}\n+\n+\t@Override\n+\tpublic void open(InitializationContext context) throws Exception {\n+\t\treuse = new GenericRowData(1);\n+\t\tconverter.open();\n+\t}\n+\n+\t@Override\n+\tpublic RowData deserialize(byte[] message) throws IOException {\n+\t\treuse.setField(0, converter.convert(message));\n+\t\treturn reuse;\n+\t}\n+\n+\t@Override\n+\tpublic boolean isEndOfStream(RowData nextElement) {\n+\t\treturn false;\n+\t}\n+\n+\t@Override\n+\tpublic TypeInformation<RowData> getProducedType() {\n+\t\treturn producedTypeInfo;\n+\t}\n+\n+\t@Override\n+\tpublic boolean equals(Object o) {\n+\t\tif (this == o) {\n+\t\t\treturn true;\n+\t\t}\n+\t\tif (o == null || getClass() != o.getClass()) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tRawFormatDeserializationSchema that = (RawFormatDeserializationSchema) o;\n+\t\treturn producedTypeInfo.equals(that.producedTypeInfo) &&\n+\t\t\tdeserializedType.equals(that.deserializedType) &&\n+\t\t\tcharsetName.equals(that.charsetName) &&\n+\t\t\tisBigEndian == that.isBigEndian;\n+\t}\n+\n+\t@Override\n+\tpublic int hashCode() {\n+\t\treturn Objects.hash(producedTypeInfo, deserializedType, charsetName, isBigEndian);\n+\t}\n+\n+\t// ------------------------------------------------------------------------\n+\n+\t/**\n+\t * Runtime converter that convert byte[] to internal data structure object.\n+\t */\n+\t@FunctionalInterface\n+\tprivate interface DeserializationRuntimeConverter extends Serializable {\n+\n+\t\tdefault void open() {}\n+\n+\t\tObject convert(byte[] data) throws IOException;\n+\t}\n+\n+\t/**\n+\t * Creates a runtime converter.\n+\t */\n+\tprivate static DeserializationRuntimeConverter createConverter(\n+\t\t\tLogicalType type, String charsetName, boolean isBigEndian) {\n+\t\tfinal DeserializationRuntimeConverter converter = createNotNullConverter(type, charsetName, isBigEndian);\n+\t\tfinal Consumer<byte[]> validator = createDataLengthValidator(type);\n+\n+\t\treturn new DeserializationRuntimeConverter() {\n+\t\t\tprivate static final long serialVersionUID = 1L;\n+\n+\t\t\t@Override\n+\t\t\tpublic void open() {\n+\t\t\t\tconverter.open();\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic Object convert(byte[] data) throws IOException {\n+\t\t\t\tif (data == null) {\n+\t\t\t\t\treturn null;\n+\t\t\t\t}\n+\t\t\t\tvalidator.accept(data);\n+\t\t\t\treturn converter.convert(data);\n+\t\t\t}\n+\t\t};\n+\t}\n+\n+\tprivate static DeserializationRuntimeConverter createNotNullConverter(\n+\t\t\tLogicalType type, String charsetName, boolean isBigEndian) {\n+\t\tswitch (type.getTypeRoot()) {\n+\t\t\tcase CHAR:\n+\t\t\tcase VARCHAR:\n+\t\t\t\treturn createStringConverter(charsetName);\n+\n+\t\t\tcase VARBINARY:\n+\t\t\tcase BINARY:\n+\t\t\t\treturn data -> data;\n+\n+\t\t\tcase RAW:\n+\t\t\t\treturn createRawValueConverter((RawType<?>) type);\n+\n+\t\t\tcase BOOLEAN:\n+\t\t\t\treturn data -> data[0] != 0;\n+\n+\t\t\tcase TINYINT:\n+\t\t\t\treturn data -> data[0];\n+\n+\t\t\tcase SMALLINT:\n+\t\t\t\treturn createEndiannessAwareConverter(\n+\t\t\t\t\tisBigEndian,\n+\t\t\t\t\tsegment -> segment.getShortBigEndian(0),\n+\t\t\t\t\tsegment -> segment.getShortLittleEndian(0)\n+\t\t\t\t);\n+\n+\t\t\tcase INTEGER:\n+\t\t\t\treturn createEndiannessAwareConverter(\n+\t\t\t\t\tisBigEndian,\n+\t\t\t\t\tsegment -> segment.getIntBigEndian(0),\n+\t\t\t\t\tsegment -> segment.getIntLittleEndian(0)\n+\t\t\t\t);\n+\n+\t\t\tcase BIGINT:\n+\t\t\t\treturn createEndiannessAwareConverter(\n+\t\t\t\t\tisBigEndian,\n+\t\t\t\t\tsegment -> segment.getLongBigEndian(0),\n+\t\t\t\t\tsegment -> segment.getLongLittleEndian(0)\n+\t\t\t\t);\n+\n+\t\t\tcase FLOAT:\n+\t\t\t\treturn createEndiannessAwareConverter(\n+\t\t\t\t\tisBigEndian,\n+\t\t\t\t\tsegment -> segment.getFloatBigEndian(0),\n+\t\t\t\t\tsegment -> segment.getFloatLittleEndian(0)\n+\t\t\t\t);\n+\n+\t\t\tcase DOUBLE:\n+\t\t\t\treturn createEndiannessAwareConverter(\n+\t\t\t\t\tisBigEndian,\n+\t\t\t\t\tsegment -> segment.getDoubleBigEndian(0),\n+\t\t\t\t\tsegment -> segment.getDoubleLittleEndian(0)\n+\t\t\t\t);\n+\n+\t\t\tdefault:\n+\t\t\t\tthrow new UnsupportedOperationException(\"'raw' format currently doesn't support type: \" + type);\n+\t\t}\n+\t}\n+\n+\tprivate static DeserializationRuntimeConverter createStringConverter(final String charsetName) {\n+\t\t// this also checks the charsetName is valid\n+\t\tCharset charset = Charset.forName(charsetName);\n+\t\tif (charset == StandardCharsets.UTF_8) {\n+\t\t\t// avoid UTF-8 decoding if the given charset is UTF-8\n+\t\t\t// because the underlying bytes of StringData is in UTF-8 encoding\n+\t\t\treturn StringData::fromBytes;\n+\t\t}\n+\n+\t\treturn new DeserializationRuntimeConverter() {\n+\t\t\tprivate static final long serialVersionUID = 1L;\n+\t\t\tprivate transient Charset charset;\n+\t\t\t@Override\n+\t\t\tpublic void open() {\n+\t\t\t\tcharset = Charset.forName(charsetName);\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic Object convert(byte[] data) {\n+\t\t\t\tString str = new String(data, charset);\n+\t\t\t\treturn StringData.fromString(str);\n+\t\t\t}\n+\t\t};\n+\t}\n+\n+\tprivate static DeserializationRuntimeConverter createRawValueConverter(RawType<?> rawType) {\n+\t\tfinal TypeSerializer<?> serializer = rawType.getTypeSerializer();\n+\n+\t\treturn new DeserializationRuntimeConverter() {\n+\t\t\tprivate static final long serialVersionUID = 1L;\n+\t\t\tprivate final DataInputDeserializer source = new DataInputDeserializer();\n+\t\t\t@Override\n+\t\t\tpublic Object convert(byte[] data) throws IOException {\n+\t\t\t\tsource.setBuffer(data);\n+\t\t\t\treturn RawValueData.fromObject(serializer.deserialize(source));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d60a354989d3e3a3e6cea3fcce6903a35fa72965"}, "originalPosition": 253}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODgzMzg1NQ==", "bodyText": "Yes. I think we can.", "url": "https://github.com/apache/flink/pull/13909#discussion_r518833855", "createdAt": "2020-11-06T15:41:31Z", "author": {"login": "wuchong"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/formats/raw/RawFormatDeserializationSchema.java", "diffHunk": "@@ -0,0 +1,343 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.formats.raw;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.common.serialization.DeserializationSchema;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.core.memory.DataInputDeserializer;\n+import org.apache.flink.core.memory.HeapMemorySegment;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RawValueData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RawType;\n+import org.apache.flink.types.DeserializationException;\n+import org.apache.flink.util.function.FunctionUtils;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.nio.charset.Charset;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Objects;\n+import java.util.function.Consumer;\n+\n+import static org.apache.flink.util.Preconditions.checkNotNull;\n+\n+/**\n+ * Deserialization schema from raw (byte based) value to Flink Table/SQL internal data structure {@link RowData}.\n+ */\n+@Internal\n+public class RawFormatDeserializationSchema implements DeserializationSchema<RowData> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate final LogicalType deserializedType;\n+\n+\tprivate final DeserializationRuntimeConverter converter;\n+\n+\tprivate final TypeInformation<RowData> producedTypeInfo;\n+\n+\tprivate final String charsetName;\n+\n+\tprivate final boolean isBigEndian;\n+\n+\tprivate transient GenericRowData reuse;\n+\n+\tpublic RawFormatDeserializationSchema(\n+\t\t\tLogicalType deserializedType,\n+\t\t\tTypeInformation<RowData> producedTypeInfo,\n+\t\t\tString charsetName,\n+\t\t\tboolean isBigEndian) {\n+\t\tthis.deserializedType = checkNotNull(deserializedType);\n+\t\tthis.producedTypeInfo = checkNotNull(producedTypeInfo);\n+\t\tthis.converter = createConverter(deserializedType, charsetName, isBigEndian);\n+\t\tthis.charsetName = charsetName;\n+\t\tthis.isBigEndian = isBigEndian;\n+\t}\n+\n+\t@Override\n+\tpublic void open(InitializationContext context) throws Exception {\n+\t\treuse = new GenericRowData(1);\n+\t\tconverter.open();\n+\t}\n+\n+\t@Override\n+\tpublic RowData deserialize(byte[] message) throws IOException {\n+\t\treuse.setField(0, converter.convert(message));\n+\t\treturn reuse;\n+\t}\n+\n+\t@Override\n+\tpublic boolean isEndOfStream(RowData nextElement) {\n+\t\treturn false;\n+\t}\n+\n+\t@Override\n+\tpublic TypeInformation<RowData> getProducedType() {\n+\t\treturn producedTypeInfo;\n+\t}\n+\n+\t@Override\n+\tpublic boolean equals(Object o) {\n+\t\tif (this == o) {\n+\t\t\treturn true;\n+\t\t}\n+\t\tif (o == null || getClass() != o.getClass()) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tRawFormatDeserializationSchema that = (RawFormatDeserializationSchema) o;\n+\t\treturn producedTypeInfo.equals(that.producedTypeInfo) &&\n+\t\t\tdeserializedType.equals(that.deserializedType) &&\n+\t\t\tcharsetName.equals(that.charsetName) &&\n+\t\t\tisBigEndian == that.isBigEndian;\n+\t}\n+\n+\t@Override\n+\tpublic int hashCode() {\n+\t\treturn Objects.hash(producedTypeInfo, deserializedType, charsetName, isBigEndian);\n+\t}\n+\n+\t// ------------------------------------------------------------------------\n+\n+\t/**\n+\t * Runtime converter that convert byte[] to internal data structure object.\n+\t */\n+\t@FunctionalInterface\n+\tprivate interface DeserializationRuntimeConverter extends Serializable {\n+\n+\t\tdefault void open() {}\n+\n+\t\tObject convert(byte[] data) throws IOException;\n+\t}\n+\n+\t/**\n+\t * Creates a runtime converter.\n+\t */\n+\tprivate static DeserializationRuntimeConverter createConverter(\n+\t\t\tLogicalType type, String charsetName, boolean isBigEndian) {\n+\t\tfinal DeserializationRuntimeConverter converter = createNotNullConverter(type, charsetName, isBigEndian);\n+\t\tfinal Consumer<byte[]> validator = createDataLengthValidator(type);\n+\n+\t\treturn new DeserializationRuntimeConverter() {\n+\t\t\tprivate static final long serialVersionUID = 1L;\n+\n+\t\t\t@Override\n+\t\t\tpublic void open() {\n+\t\t\t\tconverter.open();\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic Object convert(byte[] data) throws IOException {\n+\t\t\t\tif (data == null) {\n+\t\t\t\t\treturn null;\n+\t\t\t\t}\n+\t\t\t\tvalidator.accept(data);\n+\t\t\t\treturn converter.convert(data);\n+\t\t\t}\n+\t\t};\n+\t}\n+\n+\tprivate static DeserializationRuntimeConverter createNotNullConverter(\n+\t\t\tLogicalType type, String charsetName, boolean isBigEndian) {\n+\t\tswitch (type.getTypeRoot()) {\n+\t\t\tcase CHAR:\n+\t\t\tcase VARCHAR:\n+\t\t\t\treturn createStringConverter(charsetName);\n+\n+\t\t\tcase VARBINARY:\n+\t\t\tcase BINARY:\n+\t\t\t\treturn data -> data;\n+\n+\t\t\tcase RAW:\n+\t\t\t\treturn createRawValueConverter((RawType<?>) type);\n+\n+\t\t\tcase BOOLEAN:\n+\t\t\t\treturn data -> data[0] != 0;\n+\n+\t\t\tcase TINYINT:\n+\t\t\t\treturn data -> data[0];\n+\n+\t\t\tcase SMALLINT:\n+\t\t\t\treturn createEndiannessAwareConverter(\n+\t\t\t\t\tisBigEndian,\n+\t\t\t\t\tsegment -> segment.getShortBigEndian(0),\n+\t\t\t\t\tsegment -> segment.getShortLittleEndian(0)\n+\t\t\t\t);\n+\n+\t\t\tcase INTEGER:\n+\t\t\t\treturn createEndiannessAwareConverter(\n+\t\t\t\t\tisBigEndian,\n+\t\t\t\t\tsegment -> segment.getIntBigEndian(0),\n+\t\t\t\t\tsegment -> segment.getIntLittleEndian(0)\n+\t\t\t\t);\n+\n+\t\t\tcase BIGINT:\n+\t\t\t\treturn createEndiannessAwareConverter(\n+\t\t\t\t\tisBigEndian,\n+\t\t\t\t\tsegment -> segment.getLongBigEndian(0),\n+\t\t\t\t\tsegment -> segment.getLongLittleEndian(0)\n+\t\t\t\t);\n+\n+\t\t\tcase FLOAT:\n+\t\t\t\treturn createEndiannessAwareConverter(\n+\t\t\t\t\tisBigEndian,\n+\t\t\t\t\tsegment -> segment.getFloatBigEndian(0),\n+\t\t\t\t\tsegment -> segment.getFloatLittleEndian(0)\n+\t\t\t\t);\n+\n+\t\t\tcase DOUBLE:\n+\t\t\t\treturn createEndiannessAwareConverter(\n+\t\t\t\t\tisBigEndian,\n+\t\t\t\t\tsegment -> segment.getDoubleBigEndian(0),\n+\t\t\t\t\tsegment -> segment.getDoubleLittleEndian(0)\n+\t\t\t\t);\n+\n+\t\t\tdefault:\n+\t\t\t\tthrow new UnsupportedOperationException(\"'raw' format currently doesn't support type: \" + type);\n+\t\t}\n+\t}\n+\n+\tprivate static DeserializationRuntimeConverter createStringConverter(final String charsetName) {\n+\t\t// this also checks the charsetName is valid\n+\t\tCharset charset = Charset.forName(charsetName);\n+\t\tif (charset == StandardCharsets.UTF_8) {\n+\t\t\t// avoid UTF-8 decoding if the given charset is UTF-8\n+\t\t\t// because the underlying bytes of StringData is in UTF-8 encoding\n+\t\t\treturn StringData::fromBytes;\n+\t\t}\n+\n+\t\treturn new DeserializationRuntimeConverter() {\n+\t\t\tprivate static final long serialVersionUID = 1L;\n+\t\t\tprivate transient Charset charset;\n+\t\t\t@Override\n+\t\t\tpublic void open() {\n+\t\t\t\tcharset = Charset.forName(charsetName);\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic Object convert(byte[] data) {\n+\t\t\t\tString str = new String(data, charset);\n+\t\t\t\treturn StringData.fromString(str);\n+\t\t\t}\n+\t\t};\n+\t}\n+\n+\tprivate static DeserializationRuntimeConverter createRawValueConverter(RawType<?> rawType) {\n+\t\tfinal TypeSerializer<?> serializer = rawType.getTypeSerializer();\n+\n+\t\treturn new DeserializationRuntimeConverter() {\n+\t\t\tprivate static final long serialVersionUID = 1L;\n+\t\t\tprivate final DataInputDeserializer source = new DataInputDeserializer();\n+\t\t\t@Override\n+\t\t\tpublic Object convert(byte[] data) throws IOException {\n+\t\t\t\tsource.setBuffer(data);\n+\t\t\t\treturn RawValueData.fromObject(serializer.deserialize(source));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODgwMDc1MA=="}, "originalCommit": {"oid": "d60a354989d3e3a3e6cea3fcce6903a35fa72965"}, "originalPosition": 253}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI1MTgzNDIwOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/formats/raw/RawFormatFactory.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQxNDo1ODoyM1rOHuxWLQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQxNDo1ODoyM1rOHuxWLQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODgwNTAzNw==", "bodyText": "nit: Defines the endianness for bytes of numeric values.", "url": "https://github.com/apache/flink/pull/13909#discussion_r518805037", "createdAt": "2020-11-06T14:58:23Z", "author": {"login": "twalthr"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/formats/raw/RawFormatFactory.java", "diffHunk": "@@ -0,0 +1,186 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.formats.raw;\n+\n+import org.apache.flink.api.common.serialization.DeserializationSchema;\n+import org.apache.flink.api.common.serialization.SerializationSchema;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.configuration.ConfigOption;\n+import org.apache.flink.configuration.ConfigOptions;\n+import org.apache.flink.configuration.ReadableConfig;\n+import org.apache.flink.table.api.ValidationException;\n+import org.apache.flink.table.connector.ChangelogMode;\n+import org.apache.flink.table.connector.format.DecodingFormat;\n+import org.apache.flink.table.connector.format.EncodingFormat;\n+import org.apache.flink.table.connector.sink.DynamicTableSink;\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.factories.DeserializationFormatFactory;\n+import org.apache.flink.table.factories.DynamicTableFactory.Context;\n+import org.apache.flink.table.factories.FactoryUtil;\n+import org.apache.flink.table.factories.SerializationFormatFactory;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+\n+import java.nio.charset.Charset;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Format factory for raw format which allows to read and write raw (byte based) values as a single column.\n+ */\n+public class RawFormatFactory implements DeserializationFormatFactory, SerializationFormatFactory {\n+\n+\tpublic static final String IDENTIFIER = \"raw\";\n+\tprivate static final String BIG_ENDIAN = \"big-endian\";\n+\tprivate static final String LITTLE_ENDIAN = \"little-endian\";\n+\n+\tpublic static final ConfigOption<String> ENDIANNESS = ConfigOptions\n+\t\t.key(\"endianness\")\n+\t\t.stringType()\n+\t\t.defaultValue(BIG_ENDIAN)\n+\t\t.withDescription(\"Defines the endianness of the bytes of the numeric value.\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d60a354989d3e3a3e6cea3fcce6903a35fa72965"}, "originalPosition": 62}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI1MTg0MjQxOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/formats/raw/RawFormatFactory.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQxNTowMDoxM1rOHuxa-g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQxNTowMDoxM1rOHuxa-g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODgwNjI2Ng==", "bodyText": "shall we perform real type validation here? e.g. TIMESTAMP or NULL would still pass", "url": "https://github.com/apache/flink/pull/13909#discussion_r518806266", "createdAt": "2020-11-06T15:00:13Z", "author": {"login": "twalthr"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/formats/raw/RawFormatFactory.java", "diffHunk": "@@ -0,0 +1,186 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.formats.raw;\n+\n+import org.apache.flink.api.common.serialization.DeserializationSchema;\n+import org.apache.flink.api.common.serialization.SerializationSchema;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.configuration.ConfigOption;\n+import org.apache.flink.configuration.ConfigOptions;\n+import org.apache.flink.configuration.ReadableConfig;\n+import org.apache.flink.table.api.ValidationException;\n+import org.apache.flink.table.connector.ChangelogMode;\n+import org.apache.flink.table.connector.format.DecodingFormat;\n+import org.apache.flink.table.connector.format.EncodingFormat;\n+import org.apache.flink.table.connector.sink.DynamicTableSink;\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.factories.DeserializationFormatFactory;\n+import org.apache.flink.table.factories.DynamicTableFactory.Context;\n+import org.apache.flink.table.factories.FactoryUtil;\n+import org.apache.flink.table.factories.SerializationFormatFactory;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+\n+import java.nio.charset.Charset;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Format factory for raw format which allows to read and write raw (byte based) values as a single column.\n+ */\n+public class RawFormatFactory implements DeserializationFormatFactory, SerializationFormatFactory {\n+\n+\tpublic static final String IDENTIFIER = \"raw\";\n+\tprivate static final String BIG_ENDIAN = \"big-endian\";\n+\tprivate static final String LITTLE_ENDIAN = \"little-endian\";\n+\n+\tpublic static final ConfigOption<String> ENDIANNESS = ConfigOptions\n+\t\t.key(\"endianness\")\n+\t\t.stringType()\n+\t\t.defaultValue(BIG_ENDIAN)\n+\t\t.withDescription(\"Defines the endianness of the bytes of the numeric value.\");\n+\n+\tpublic static final ConfigOption<String> CHARSET = ConfigOptions\n+\t\t.key(\"charset\")\n+\t\t.stringType()\n+\t\t.defaultValue(StandardCharsets.UTF_8.displayName())\n+\t\t.withDescription(\"Defines the string charset.\");\n+\n+\t@Override\n+\tpublic String factoryIdentifier() {\n+\t\treturn IDENTIFIER;\n+\t}\n+\n+\t@Override\n+\tpublic Set<ConfigOption<?>> requiredOptions() {\n+\t\treturn Collections.emptySet();\n+\t}\n+\n+\t@Override\n+\tpublic Set<ConfigOption<?>> optionalOptions() {\n+\t\tSet<ConfigOption<?>> options = new HashSet<>();\n+\t\toptions.add(ENDIANNESS);\n+\t\toptions.add(CHARSET);\n+\t\treturn options;\n+\t}\n+\n+\t@Override\n+\tpublic DecodingFormat<DeserializationSchema<RowData>> createDecodingFormat(\n+\t\t\tContext context,\n+\t\t\tReadableConfig formatOptions) {\n+\t\tFactoryUtil.validateFactoryOptions(this, formatOptions);\n+\t\tfinal String charsetName = validateAndGetCharsetName(formatOptions);\n+\t\tfinal boolean isBigEndian = isBigEndian(formatOptions);\n+\n+\t\treturn new DecodingFormat<DeserializationSchema<RowData>>() {\n+\t\t\t@Override\n+\t\t\tpublic DeserializationSchema<RowData> createRuntimeDecoder(\n+\t\t\t\t\tDynamicTableSource.Context context,\n+\t\t\t\t\tDataType producedDataType) {\n+\t\t\t\tfinal RowType physicalRowType = (RowType) producedDataType.getLogicalType();\n+\t\t\t\tfinal LogicalType fieldType = validateAndExtractSingleField(physicalRowType);\n+\t\t\t\tfinal TypeInformation<RowData> producedTypeInfo = context.createTypeInformation(producedDataType);\n+\t\t\t\treturn new RawFormatDeserializationSchema(fieldType, producedTypeInfo, charsetName, isBigEndian);\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic ChangelogMode getChangelogMode() {\n+\t\t\t\treturn ChangelogMode.insertOnly();\n+\t\t\t}\n+\t\t};\n+\t}\n+\n+\t@Override\n+\tpublic EncodingFormat<SerializationSchema<RowData>> createEncodingFormat(\n+\t\t\tContext context, ReadableConfig formatOptions) {\n+\t\tFactoryUtil.validateFactoryOptions(this, formatOptions);\n+\t\tfinal String charsetName = validateAndGetCharsetName(formatOptions);\n+\t\tfinal boolean isBigEndian = isBigEndian(formatOptions);\n+\n+\t\treturn new EncodingFormat<SerializationSchema<RowData>>() {\n+\t\t\t@Override\n+\t\t\tpublic SerializationSchema<RowData> createRuntimeEncoder(\n+\t\t\t\t\tDynamicTableSink.Context context,\n+\t\t\t\t\tDataType consumedDataType) {\n+\t\t\t\tfinal RowType physicalRowType = (RowType) consumedDataType.getLogicalType();\n+\t\t\t\tfinal LogicalType fieldType = validateAndExtractSingleField(physicalRowType);\n+\t\t\t\treturn new RawFormatSerializationSchema(fieldType, charsetName, isBigEndian);\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic ChangelogMode getChangelogMode() {\n+\t\t\t\treturn ChangelogMode.insertOnly();\n+\t\t\t}\n+\t\t};\n+\t}\n+\n+\t// ------------------------------------------------------------------------------------------\n+\n+\t/**\n+\t * Validates and extract the single field type from the given physical row schema.\n+\t */\n+\tprivate static LogicalType validateAndExtractSingleField(RowType physicalRowType) {\n+\t\tif (physicalRowType.getFieldCount() != 1) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d60a354989d3e3a3e6cea3fcce6903a35fa72965"}, "originalPosition": 144}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI1MTg1ODA4OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/formats/raw/RawFormatDeserializationSchema.java", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQxNTowNDoxMlrOHuxk6g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQxNjoxNzo1OFrOHu0ewg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODgwODgxMA==", "bodyText": "I think data is never null at this location.", "url": "https://github.com/apache/flink/pull/13909#discussion_r518808810", "createdAt": "2020-11-06T15:04:12Z", "author": {"login": "twalthr"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/formats/raw/RawFormatDeserializationSchema.java", "diffHunk": "@@ -0,0 +1,343 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.formats.raw;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.common.serialization.DeserializationSchema;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.core.memory.DataInputDeserializer;\n+import org.apache.flink.core.memory.HeapMemorySegment;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RawValueData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RawType;\n+import org.apache.flink.types.DeserializationException;\n+import org.apache.flink.util.function.FunctionUtils;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.nio.charset.Charset;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Objects;\n+import java.util.function.Consumer;\n+\n+import static org.apache.flink.util.Preconditions.checkNotNull;\n+\n+/**\n+ * Deserialization schema from raw (byte based) value to Flink Table/SQL internal data structure {@link RowData}.\n+ */\n+@Internal\n+public class RawFormatDeserializationSchema implements DeserializationSchema<RowData> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate final LogicalType deserializedType;\n+\n+\tprivate final DeserializationRuntimeConverter converter;\n+\n+\tprivate final TypeInformation<RowData> producedTypeInfo;\n+\n+\tprivate final String charsetName;\n+\n+\tprivate final boolean isBigEndian;\n+\n+\tprivate transient GenericRowData reuse;\n+\n+\tpublic RawFormatDeserializationSchema(\n+\t\t\tLogicalType deserializedType,\n+\t\t\tTypeInformation<RowData> producedTypeInfo,\n+\t\t\tString charsetName,\n+\t\t\tboolean isBigEndian) {\n+\t\tthis.deserializedType = checkNotNull(deserializedType);\n+\t\tthis.producedTypeInfo = checkNotNull(producedTypeInfo);\n+\t\tthis.converter = createConverter(deserializedType, charsetName, isBigEndian);\n+\t\tthis.charsetName = charsetName;\n+\t\tthis.isBigEndian = isBigEndian;\n+\t}\n+\n+\t@Override\n+\tpublic void open(InitializationContext context) throws Exception {\n+\t\treuse = new GenericRowData(1);\n+\t\tconverter.open();\n+\t}\n+\n+\t@Override\n+\tpublic RowData deserialize(byte[] message) throws IOException {\n+\t\treuse.setField(0, converter.convert(message));\n+\t\treturn reuse;\n+\t}\n+\n+\t@Override\n+\tpublic boolean isEndOfStream(RowData nextElement) {\n+\t\treturn false;\n+\t}\n+\n+\t@Override\n+\tpublic TypeInformation<RowData> getProducedType() {\n+\t\treturn producedTypeInfo;\n+\t}\n+\n+\t@Override\n+\tpublic boolean equals(Object o) {\n+\t\tif (this == o) {\n+\t\t\treturn true;\n+\t\t}\n+\t\tif (o == null || getClass() != o.getClass()) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tRawFormatDeserializationSchema that = (RawFormatDeserializationSchema) o;\n+\t\treturn producedTypeInfo.equals(that.producedTypeInfo) &&\n+\t\t\tdeserializedType.equals(that.deserializedType) &&\n+\t\t\tcharsetName.equals(that.charsetName) &&\n+\t\t\tisBigEndian == that.isBigEndian;\n+\t}\n+\n+\t@Override\n+\tpublic int hashCode() {\n+\t\treturn Objects.hash(producedTypeInfo, deserializedType, charsetName, isBigEndian);\n+\t}\n+\n+\t// ------------------------------------------------------------------------\n+\n+\t/**\n+\t * Runtime converter that convert byte[] to internal data structure object.\n+\t */\n+\t@FunctionalInterface\n+\tprivate interface DeserializationRuntimeConverter extends Serializable {\n+\n+\t\tdefault void open() {}\n+\n+\t\tObject convert(byte[] data) throws IOException;\n+\t}\n+\n+\t/**\n+\t * Creates a runtime converter.\n+\t */\n+\tprivate static DeserializationRuntimeConverter createConverter(\n+\t\t\tLogicalType type, String charsetName, boolean isBigEndian) {\n+\t\tfinal DeserializationRuntimeConverter converter = createNotNullConverter(type, charsetName, isBigEndian);\n+\t\tfinal Consumer<byte[]> validator = createDataLengthValidator(type);\n+\n+\t\treturn new DeserializationRuntimeConverter() {\n+\t\t\tprivate static final long serialVersionUID = 1L;\n+\n+\t\t\t@Override\n+\t\t\tpublic void open() {\n+\t\t\t\tconverter.open();\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic Object convert(byte[] data) throws IOException {\n+\t\t\t\tif (data == null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d60a354989d3e3a3e6cea3fcce6903a35fa72965"}, "originalPosition": 150}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODgzMzczMA==", "bodyText": "Why? The Kafka message value might be null.", "url": "https://github.com/apache/flink/pull/13909#discussion_r518833730", "createdAt": "2020-11-06T15:41:17Z", "author": {"login": "wuchong"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/formats/raw/RawFormatDeserializationSchema.java", "diffHunk": "@@ -0,0 +1,343 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.formats.raw;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.common.serialization.DeserializationSchema;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.core.memory.DataInputDeserializer;\n+import org.apache.flink.core.memory.HeapMemorySegment;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RawValueData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RawType;\n+import org.apache.flink.types.DeserializationException;\n+import org.apache.flink.util.function.FunctionUtils;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.nio.charset.Charset;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Objects;\n+import java.util.function.Consumer;\n+\n+import static org.apache.flink.util.Preconditions.checkNotNull;\n+\n+/**\n+ * Deserialization schema from raw (byte based) value to Flink Table/SQL internal data structure {@link RowData}.\n+ */\n+@Internal\n+public class RawFormatDeserializationSchema implements DeserializationSchema<RowData> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate final LogicalType deserializedType;\n+\n+\tprivate final DeserializationRuntimeConverter converter;\n+\n+\tprivate final TypeInformation<RowData> producedTypeInfo;\n+\n+\tprivate final String charsetName;\n+\n+\tprivate final boolean isBigEndian;\n+\n+\tprivate transient GenericRowData reuse;\n+\n+\tpublic RawFormatDeserializationSchema(\n+\t\t\tLogicalType deserializedType,\n+\t\t\tTypeInformation<RowData> producedTypeInfo,\n+\t\t\tString charsetName,\n+\t\t\tboolean isBigEndian) {\n+\t\tthis.deserializedType = checkNotNull(deserializedType);\n+\t\tthis.producedTypeInfo = checkNotNull(producedTypeInfo);\n+\t\tthis.converter = createConverter(deserializedType, charsetName, isBigEndian);\n+\t\tthis.charsetName = charsetName;\n+\t\tthis.isBigEndian = isBigEndian;\n+\t}\n+\n+\t@Override\n+\tpublic void open(InitializationContext context) throws Exception {\n+\t\treuse = new GenericRowData(1);\n+\t\tconverter.open();\n+\t}\n+\n+\t@Override\n+\tpublic RowData deserialize(byte[] message) throws IOException {\n+\t\treuse.setField(0, converter.convert(message));\n+\t\treturn reuse;\n+\t}\n+\n+\t@Override\n+\tpublic boolean isEndOfStream(RowData nextElement) {\n+\t\treturn false;\n+\t}\n+\n+\t@Override\n+\tpublic TypeInformation<RowData> getProducedType() {\n+\t\treturn producedTypeInfo;\n+\t}\n+\n+\t@Override\n+\tpublic boolean equals(Object o) {\n+\t\tif (this == o) {\n+\t\t\treturn true;\n+\t\t}\n+\t\tif (o == null || getClass() != o.getClass()) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tRawFormatDeserializationSchema that = (RawFormatDeserializationSchema) o;\n+\t\treturn producedTypeInfo.equals(that.producedTypeInfo) &&\n+\t\t\tdeserializedType.equals(that.deserializedType) &&\n+\t\t\tcharsetName.equals(that.charsetName) &&\n+\t\t\tisBigEndian == that.isBigEndian;\n+\t}\n+\n+\t@Override\n+\tpublic int hashCode() {\n+\t\treturn Objects.hash(producedTypeInfo, deserializedType, charsetName, isBigEndian);\n+\t}\n+\n+\t// ------------------------------------------------------------------------\n+\n+\t/**\n+\t * Runtime converter that convert byte[] to internal data structure object.\n+\t */\n+\t@FunctionalInterface\n+\tprivate interface DeserializationRuntimeConverter extends Serializable {\n+\n+\t\tdefault void open() {}\n+\n+\t\tObject convert(byte[] data) throws IOException;\n+\t}\n+\n+\t/**\n+\t * Creates a runtime converter.\n+\t */\n+\tprivate static DeserializationRuntimeConverter createConverter(\n+\t\t\tLogicalType type, String charsetName, boolean isBigEndian) {\n+\t\tfinal DeserializationRuntimeConverter converter = createNotNullConverter(type, charsetName, isBigEndian);\n+\t\tfinal Consumer<byte[]> validator = createDataLengthValidator(type);\n+\n+\t\treturn new DeserializationRuntimeConverter() {\n+\t\t\tprivate static final long serialVersionUID = 1L;\n+\n+\t\t\t@Override\n+\t\t\tpublic void open() {\n+\t\t\t\tconverter.open();\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic Object convert(byte[] data) throws IOException {\n+\t\t\t\tif (data == null) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODgwODgxMA=="}, "originalCommit": {"oid": "d60a354989d3e3a3e6cea3fcce6903a35fa72965"}, "originalPosition": 150}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODg1MTg2OQ==", "bodyText": "at other locations we also don't check that:\nJsonRowDataDeserializationSchema#deserialize or CsvRowDataDeserializationSchema#deserialize\nthe JavaDocs of DeserializationSchema don't mention that either", "url": "https://github.com/apache/flink/pull/13909#discussion_r518851869", "createdAt": "2020-11-06T16:10:21Z", "author": {"login": "twalthr"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/formats/raw/RawFormatDeserializationSchema.java", "diffHunk": "@@ -0,0 +1,343 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.formats.raw;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.common.serialization.DeserializationSchema;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.core.memory.DataInputDeserializer;\n+import org.apache.flink.core.memory.HeapMemorySegment;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RawValueData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RawType;\n+import org.apache.flink.types.DeserializationException;\n+import org.apache.flink.util.function.FunctionUtils;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.nio.charset.Charset;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Objects;\n+import java.util.function.Consumer;\n+\n+import static org.apache.flink.util.Preconditions.checkNotNull;\n+\n+/**\n+ * Deserialization schema from raw (byte based) value to Flink Table/SQL internal data structure {@link RowData}.\n+ */\n+@Internal\n+public class RawFormatDeserializationSchema implements DeserializationSchema<RowData> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate final LogicalType deserializedType;\n+\n+\tprivate final DeserializationRuntimeConverter converter;\n+\n+\tprivate final TypeInformation<RowData> producedTypeInfo;\n+\n+\tprivate final String charsetName;\n+\n+\tprivate final boolean isBigEndian;\n+\n+\tprivate transient GenericRowData reuse;\n+\n+\tpublic RawFormatDeserializationSchema(\n+\t\t\tLogicalType deserializedType,\n+\t\t\tTypeInformation<RowData> producedTypeInfo,\n+\t\t\tString charsetName,\n+\t\t\tboolean isBigEndian) {\n+\t\tthis.deserializedType = checkNotNull(deserializedType);\n+\t\tthis.producedTypeInfo = checkNotNull(producedTypeInfo);\n+\t\tthis.converter = createConverter(deserializedType, charsetName, isBigEndian);\n+\t\tthis.charsetName = charsetName;\n+\t\tthis.isBigEndian = isBigEndian;\n+\t}\n+\n+\t@Override\n+\tpublic void open(InitializationContext context) throws Exception {\n+\t\treuse = new GenericRowData(1);\n+\t\tconverter.open();\n+\t}\n+\n+\t@Override\n+\tpublic RowData deserialize(byte[] message) throws IOException {\n+\t\treuse.setField(0, converter.convert(message));\n+\t\treturn reuse;\n+\t}\n+\n+\t@Override\n+\tpublic boolean isEndOfStream(RowData nextElement) {\n+\t\treturn false;\n+\t}\n+\n+\t@Override\n+\tpublic TypeInformation<RowData> getProducedType() {\n+\t\treturn producedTypeInfo;\n+\t}\n+\n+\t@Override\n+\tpublic boolean equals(Object o) {\n+\t\tif (this == o) {\n+\t\t\treturn true;\n+\t\t}\n+\t\tif (o == null || getClass() != o.getClass()) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tRawFormatDeserializationSchema that = (RawFormatDeserializationSchema) o;\n+\t\treturn producedTypeInfo.equals(that.producedTypeInfo) &&\n+\t\t\tdeserializedType.equals(that.deserializedType) &&\n+\t\t\tcharsetName.equals(that.charsetName) &&\n+\t\t\tisBigEndian == that.isBigEndian;\n+\t}\n+\n+\t@Override\n+\tpublic int hashCode() {\n+\t\treturn Objects.hash(producedTypeInfo, deserializedType, charsetName, isBigEndian);\n+\t}\n+\n+\t// ------------------------------------------------------------------------\n+\n+\t/**\n+\t * Runtime converter that convert byte[] to internal data structure object.\n+\t */\n+\t@FunctionalInterface\n+\tprivate interface DeserializationRuntimeConverter extends Serializable {\n+\n+\t\tdefault void open() {}\n+\n+\t\tObject convert(byte[] data) throws IOException;\n+\t}\n+\n+\t/**\n+\t * Creates a runtime converter.\n+\t */\n+\tprivate static DeserializationRuntimeConverter createConverter(\n+\t\t\tLogicalType type, String charsetName, boolean isBigEndian) {\n+\t\tfinal DeserializationRuntimeConverter converter = createNotNullConverter(type, charsetName, isBigEndian);\n+\t\tfinal Consumer<byte[]> validator = createDataLengthValidator(type);\n+\n+\t\treturn new DeserializationRuntimeConverter() {\n+\t\t\tprivate static final long serialVersionUID = 1L;\n+\n+\t\t\t@Override\n+\t\t\tpublic void open() {\n+\t\t\t\tconverter.open();\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic Object convert(byte[] data) throws IOException {\n+\t\t\t\tif (data == null) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODgwODgxMA=="}, "originalCommit": {"oid": "d60a354989d3e3a3e6cea3fcce6903a35fa72965"}, "originalPosition": 150}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODg1MjE4OA==", "bodyText": "but let's keep the null check for now", "url": "https://github.com/apache/flink/pull/13909#discussion_r518852188", "createdAt": "2020-11-06T16:10:52Z", "author": {"login": "twalthr"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/formats/raw/RawFormatDeserializationSchema.java", "diffHunk": "@@ -0,0 +1,343 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.formats.raw;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.common.serialization.DeserializationSchema;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.core.memory.DataInputDeserializer;\n+import org.apache.flink.core.memory.HeapMemorySegment;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RawValueData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RawType;\n+import org.apache.flink.types.DeserializationException;\n+import org.apache.flink.util.function.FunctionUtils;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.nio.charset.Charset;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Objects;\n+import java.util.function.Consumer;\n+\n+import static org.apache.flink.util.Preconditions.checkNotNull;\n+\n+/**\n+ * Deserialization schema from raw (byte based) value to Flink Table/SQL internal data structure {@link RowData}.\n+ */\n+@Internal\n+public class RawFormatDeserializationSchema implements DeserializationSchema<RowData> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate final LogicalType deserializedType;\n+\n+\tprivate final DeserializationRuntimeConverter converter;\n+\n+\tprivate final TypeInformation<RowData> producedTypeInfo;\n+\n+\tprivate final String charsetName;\n+\n+\tprivate final boolean isBigEndian;\n+\n+\tprivate transient GenericRowData reuse;\n+\n+\tpublic RawFormatDeserializationSchema(\n+\t\t\tLogicalType deserializedType,\n+\t\t\tTypeInformation<RowData> producedTypeInfo,\n+\t\t\tString charsetName,\n+\t\t\tboolean isBigEndian) {\n+\t\tthis.deserializedType = checkNotNull(deserializedType);\n+\t\tthis.producedTypeInfo = checkNotNull(producedTypeInfo);\n+\t\tthis.converter = createConverter(deserializedType, charsetName, isBigEndian);\n+\t\tthis.charsetName = charsetName;\n+\t\tthis.isBigEndian = isBigEndian;\n+\t}\n+\n+\t@Override\n+\tpublic void open(InitializationContext context) throws Exception {\n+\t\treuse = new GenericRowData(1);\n+\t\tconverter.open();\n+\t}\n+\n+\t@Override\n+\tpublic RowData deserialize(byte[] message) throws IOException {\n+\t\treuse.setField(0, converter.convert(message));\n+\t\treturn reuse;\n+\t}\n+\n+\t@Override\n+\tpublic boolean isEndOfStream(RowData nextElement) {\n+\t\treturn false;\n+\t}\n+\n+\t@Override\n+\tpublic TypeInformation<RowData> getProducedType() {\n+\t\treturn producedTypeInfo;\n+\t}\n+\n+\t@Override\n+\tpublic boolean equals(Object o) {\n+\t\tif (this == o) {\n+\t\t\treturn true;\n+\t\t}\n+\t\tif (o == null || getClass() != o.getClass()) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tRawFormatDeserializationSchema that = (RawFormatDeserializationSchema) o;\n+\t\treturn producedTypeInfo.equals(that.producedTypeInfo) &&\n+\t\t\tdeserializedType.equals(that.deserializedType) &&\n+\t\t\tcharsetName.equals(that.charsetName) &&\n+\t\t\tisBigEndian == that.isBigEndian;\n+\t}\n+\n+\t@Override\n+\tpublic int hashCode() {\n+\t\treturn Objects.hash(producedTypeInfo, deserializedType, charsetName, isBigEndian);\n+\t}\n+\n+\t// ------------------------------------------------------------------------\n+\n+\t/**\n+\t * Runtime converter that convert byte[] to internal data structure object.\n+\t */\n+\t@FunctionalInterface\n+\tprivate interface DeserializationRuntimeConverter extends Serializable {\n+\n+\t\tdefault void open() {}\n+\n+\t\tObject convert(byte[] data) throws IOException;\n+\t}\n+\n+\t/**\n+\t * Creates a runtime converter.\n+\t */\n+\tprivate static DeserializationRuntimeConverter createConverter(\n+\t\t\tLogicalType type, String charsetName, boolean isBigEndian) {\n+\t\tfinal DeserializationRuntimeConverter converter = createNotNullConverter(type, charsetName, isBigEndian);\n+\t\tfinal Consumer<byte[]> validator = createDataLengthValidator(type);\n+\n+\t\treturn new DeserializationRuntimeConverter() {\n+\t\t\tprivate static final long serialVersionUID = 1L;\n+\n+\t\t\t@Override\n+\t\t\tpublic void open() {\n+\t\t\t\tconverter.open();\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic Object convert(byte[] data) throws IOException {\n+\t\t\t\tif (data == null) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODgwODgxMA=="}, "originalCommit": {"oid": "d60a354989d3e3a3e6cea3fcce6903a35fa72965"}, "originalPosition": 150}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODg1NjM4Ng==", "bodyText": "I think both JSON and CSV format didn't implement correctly. There will be NPE if encountering tombstone messages. That can be skipped when ignoreParseErrors is enabled, but I think the format should handle tombstone messages.", "url": "https://github.com/apache/flink/pull/13909#discussion_r518856386", "createdAt": "2020-11-06T16:17:58Z", "author": {"login": "wuchong"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/formats/raw/RawFormatDeserializationSchema.java", "diffHunk": "@@ -0,0 +1,343 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.formats.raw;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.common.serialization.DeserializationSchema;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.core.memory.DataInputDeserializer;\n+import org.apache.flink.core.memory.HeapMemorySegment;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RawValueData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RawType;\n+import org.apache.flink.types.DeserializationException;\n+import org.apache.flink.util.function.FunctionUtils;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.nio.charset.Charset;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Objects;\n+import java.util.function.Consumer;\n+\n+import static org.apache.flink.util.Preconditions.checkNotNull;\n+\n+/**\n+ * Deserialization schema from raw (byte based) value to Flink Table/SQL internal data structure {@link RowData}.\n+ */\n+@Internal\n+public class RawFormatDeserializationSchema implements DeserializationSchema<RowData> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate final LogicalType deserializedType;\n+\n+\tprivate final DeserializationRuntimeConverter converter;\n+\n+\tprivate final TypeInformation<RowData> producedTypeInfo;\n+\n+\tprivate final String charsetName;\n+\n+\tprivate final boolean isBigEndian;\n+\n+\tprivate transient GenericRowData reuse;\n+\n+\tpublic RawFormatDeserializationSchema(\n+\t\t\tLogicalType deserializedType,\n+\t\t\tTypeInformation<RowData> producedTypeInfo,\n+\t\t\tString charsetName,\n+\t\t\tboolean isBigEndian) {\n+\t\tthis.deserializedType = checkNotNull(deserializedType);\n+\t\tthis.producedTypeInfo = checkNotNull(producedTypeInfo);\n+\t\tthis.converter = createConverter(deserializedType, charsetName, isBigEndian);\n+\t\tthis.charsetName = charsetName;\n+\t\tthis.isBigEndian = isBigEndian;\n+\t}\n+\n+\t@Override\n+\tpublic void open(InitializationContext context) throws Exception {\n+\t\treuse = new GenericRowData(1);\n+\t\tconverter.open();\n+\t}\n+\n+\t@Override\n+\tpublic RowData deserialize(byte[] message) throws IOException {\n+\t\treuse.setField(0, converter.convert(message));\n+\t\treturn reuse;\n+\t}\n+\n+\t@Override\n+\tpublic boolean isEndOfStream(RowData nextElement) {\n+\t\treturn false;\n+\t}\n+\n+\t@Override\n+\tpublic TypeInformation<RowData> getProducedType() {\n+\t\treturn producedTypeInfo;\n+\t}\n+\n+\t@Override\n+\tpublic boolean equals(Object o) {\n+\t\tif (this == o) {\n+\t\t\treturn true;\n+\t\t}\n+\t\tif (o == null || getClass() != o.getClass()) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tRawFormatDeserializationSchema that = (RawFormatDeserializationSchema) o;\n+\t\treturn producedTypeInfo.equals(that.producedTypeInfo) &&\n+\t\t\tdeserializedType.equals(that.deserializedType) &&\n+\t\t\tcharsetName.equals(that.charsetName) &&\n+\t\t\tisBigEndian == that.isBigEndian;\n+\t}\n+\n+\t@Override\n+\tpublic int hashCode() {\n+\t\treturn Objects.hash(producedTypeInfo, deserializedType, charsetName, isBigEndian);\n+\t}\n+\n+\t// ------------------------------------------------------------------------\n+\n+\t/**\n+\t * Runtime converter that convert byte[] to internal data structure object.\n+\t */\n+\t@FunctionalInterface\n+\tprivate interface DeserializationRuntimeConverter extends Serializable {\n+\n+\t\tdefault void open() {}\n+\n+\t\tObject convert(byte[] data) throws IOException;\n+\t}\n+\n+\t/**\n+\t * Creates a runtime converter.\n+\t */\n+\tprivate static DeserializationRuntimeConverter createConverter(\n+\t\t\tLogicalType type, String charsetName, boolean isBigEndian) {\n+\t\tfinal DeserializationRuntimeConverter converter = createNotNullConverter(type, charsetName, isBigEndian);\n+\t\tfinal Consumer<byte[]> validator = createDataLengthValidator(type);\n+\n+\t\treturn new DeserializationRuntimeConverter() {\n+\t\t\tprivate static final long serialVersionUID = 1L;\n+\n+\t\t\t@Override\n+\t\t\tpublic void open() {\n+\t\t\t\tconverter.open();\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic Object convert(byte[] data) throws IOException {\n+\t\t\t\tif (data == null) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODgwODgxMA=="}, "originalCommit": {"oid": "d60a354989d3e3a3e6cea3fcce6903a35fa72965"}, "originalPosition": 150}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4910, "cost": 1, "resetAt": "2021-11-12T11:57:46Z"}}}