{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDY2MTY3ODY3", "number": 13119, "reviewThreads": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQwMjo0MTowOVrOEXcwtA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQwMzowMDozMlrOEXc9zg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzMDIzOTI0OnYy", "diffSide": "RIGHT", "path": "flink-python/pyflink/datastream/data_stream.py", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQwMjo0MTowOVrOG_Px4Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQwNTowMjo0MlrOG_R4dg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk3MjAwMQ==", "bodyText": "Cannot override partitioning for KeyedStream.", "url": "https://github.com/apache/flink/pull/13119#discussion_r468972001", "createdAt": "2020-08-12T02:41:09Z", "author": {"login": "hequn8128"}, "path": "flink-python/pyflink/datastream/data_stream.py", "diffHunk": "@@ -563,6 +654,27 @@ def key_by(self, key_selector: Union[Callable, KeySelector],\n                key_type_info: TypeInformation = None) -> 'KeyedStream':\n         return self._origin_stream.key_by(key_selector, key_type_info)\n \n+    def union(self, *streams) -> 'DataStream':\n+        return self._values().union(*streams)\n+\n+    def shuffle(self) -> 'DataStream':\n+        return self._origin_stream.shuffle()\n+\n+    def project(self, *field_indexes) -> 'DataStream':\n+        return self._origin_stream.project(*field_indexes)\n+\n+    def rescale(self) -> 'DataStream':\n+        return self._origin_stream.rescale()\n+\n+    def rebalance(self) -> 'DataStream':\n+        return self._origin_stream.rebalance()\n+\n+    def forward(self) -> 'DataStream':\n+        return self._origin_stream.forward()\n+\n+    def broadcast(self) -> 'DataStream':\n+        return self._origin_stream.broadcast()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ec6e531e07a441ccb328f735cc40d618087798c4"}, "originalPosition": 121}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTAwNjQ1NA==", "bodyText": "Yes, I have tested them and revised the code.", "url": "https://github.com/apache/flink/pull/13119#discussion_r469006454", "createdAt": "2020-08-12T05:02:42Z", "author": {"login": "shuiqiangchen"}, "path": "flink-python/pyflink/datastream/data_stream.py", "diffHunk": "@@ -563,6 +654,27 @@ def key_by(self, key_selector: Union[Callable, KeySelector],\n                key_type_info: TypeInformation = None) -> 'KeyedStream':\n         return self._origin_stream.key_by(key_selector, key_type_info)\n \n+    def union(self, *streams) -> 'DataStream':\n+        return self._values().union(*streams)\n+\n+    def shuffle(self) -> 'DataStream':\n+        return self._origin_stream.shuffle()\n+\n+    def project(self, *field_indexes) -> 'DataStream':\n+        return self._origin_stream.project(*field_indexes)\n+\n+    def rescale(self) -> 'DataStream':\n+        return self._origin_stream.rescale()\n+\n+    def rebalance(self) -> 'DataStream':\n+        return self._origin_stream.rebalance()\n+\n+    def forward(self) -> 'DataStream':\n+        return self._origin_stream.forward()\n+\n+    def broadcast(self) -> 'DataStream':\n+        return self._origin_stream.broadcast()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk3MjAwMQ=="}, "originalCommit": {"oid": "ec6e531e07a441ccb328f735cc40d618087798c4"}, "originalPosition": 121}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzMDI0MTA5OnYy", "diffSide": "RIGHT", "path": "flink-python/pyflink/datastream/data_stream.py", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQwMjo0MjoxNVrOG_Py8A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQwMjo0MjoxNVrOG_Py8A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk3MjI3Mg==", "bodyText": "project on Keyedstream should preserve the hash partitioning, i.e., we can't simply project on its origin_stream.", "url": "https://github.com/apache/flink/pull/13119#discussion_r468972272", "createdAt": "2020-08-12T02:42:15Z", "author": {"login": "hequn8128"}, "path": "flink-python/pyflink/datastream/data_stream.py", "diffHunk": "@@ -563,6 +654,27 @@ def key_by(self, key_selector: Union[Callable, KeySelector],\n                key_type_info: TypeInformation = None) -> 'KeyedStream':\n         return self._origin_stream.key_by(key_selector, key_type_info)\n \n+    def union(self, *streams) -> 'DataStream':\n+        return self._values().union(*streams)\n+\n+    def shuffle(self) -> 'DataStream':\n+        return self._origin_stream.shuffle()\n+\n+    def project(self, *field_indexes) -> 'DataStream':", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ec6e531e07a441ccb328f735cc40d618087798c4"}, "originalPosition": 108}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzMDI3MTI4OnYy", "diffSide": "RIGHT", "path": "flink-python/pyflink/datastream/data_stream.py", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQwMjo1OTo0MFrOG_QEUQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQwNDo1NTo0OVrOG_Rxow==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk3NjcyMQ==", "bodyText": "I find we have bugs when handling parallelism for the keyby. Suppose we perform x -> key_by-> map, the internal java stream graph would be x -> map1 -> keyBy -> map2 -> map3. The parallelism of map1 should equal to x and map2 should equal to map3.", "url": "https://github.com/apache/flink/pull/13119#discussion_r468976721", "createdAt": "2020-08-12T02:59:40Z", "author": {"login": "hequn8128"}, "path": "flink-python/pyflink/datastream/data_stream.py", "diffHunk": "@@ -563,6 +654,27 @@ def key_by(self, key_selector: Union[Callable, KeySelector],\n                key_type_info: TypeInformation = None) -> 'KeyedStream':\n         return self._origin_stream.key_by(key_selector, key_type_info)\n \n+    def union(self, *streams) -> 'DataStream':\n+        return self._values().union(*streams)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ec6e531e07a441ccb328f735cc40d618087798c4"}, "originalPosition": 103}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTAwNDcwNw==", "bodyText": "Yes, it's a critical issue that would affect the correctness of DAG, I will fix it in this JIRA.", "url": "https://github.com/apache/flink/pull/13119#discussion_r469004707", "createdAt": "2020-08-12T04:55:49Z", "author": {"login": "shuiqiangchen"}, "path": "flink-python/pyflink/datastream/data_stream.py", "diffHunk": "@@ -563,6 +654,27 @@ def key_by(self, key_selector: Union[Callable, KeySelector],\n                key_type_info: TypeInformation = None) -> 'KeyedStream':\n         return self._origin_stream.key_by(key_selector, key_type_info)\n \n+    def union(self, *streams) -> 'DataStream':\n+        return self._values().union(*streams)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk3NjcyMQ=="}, "originalCommit": {"oid": "ec6e531e07a441ccb328f735cc40d618087798c4"}, "originalPosition": 103}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzMDI3Mjc4OnYy", "diffSide": "RIGHT", "path": "flink-python/pyflink/datastream/tests/test_data_stream.py", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQwMzowMDozMlrOG_QFOA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQwNDo1Njo1M1rOG_Rysw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk3Njk1Mg==", "bodyText": "These tests are not correct. Exceptions should be thrown since we cannot override partitioning for KeyedStream.", "url": "https://github.com/apache/flink/pull/13119#discussion_r468976952", "createdAt": "2020-08-12T03:00:32Z", "author": {"login": "hequn8128"}, "path": "flink-python/pyflink/datastream/tests/test_data_stream.py", "diffHunk": "@@ -242,6 +242,68 @@ def test_print_with_align_output(self):\n         self.assertEqual(3, len(plan['nodes']))\n         self.assertEqual(\"Sink: Print to Std. Out\", plan['nodes'][2]['type'])\n \n+    def test_union_stream(self):\n+        ds_1 = self.env.from_collection([1, 2, 3])\n+        ds_2 = self.env.from_collection([4, 5, 6])\n+        ds_3 = self.env.from_collection([7, 8, 9])\n+\n+        united_stream = ds_3.union(ds_1, ds_2)\n+\n+        united_stream.map(lambda x: x + 1).add_sink(self.test_sink)\n+        exec_plan = eval(self.env.get_execution_plan())\n+        source_ids = []\n+        union_node_pre_ids = []\n+        for node in exec_plan['nodes']:\n+            if node['pact'] == 'Data Source':\n+                source_ids.append(node['id'])\n+            if node['pact'] == 'Operator':\n+                for pre in node['predecessors']:\n+                    union_node_pre_ids.append(pre['id'])\n+\n+        source_ids.sort()\n+        union_node_pre_ids.sort()\n+        self.assertEqual(source_ids, union_node_pre_ids)\n+\n+    def test_project(self):\n+        ds = self.env.from_collection([[1, 2, 3, 4], [5, 6, 7, 8]],\n+                                      type_info=Types.TUPLE(\n+                                          [Types.INT(), Types.INT(), Types.INT(), Types.INT()]))\n+        ds.project(1, 3).map(lambda x: (x[0], x[1] + 1)).add_sink(self.test_sink)\n+        exec_plan = eval(self.env.get_execution_plan())\n+        self.assertEqual(exec_plan['nodes'][1]['type'], 'Projection')\n+\n+    def test_broadcast(self):\n+        ds_1 = self.env.from_collection([1, 2, 3])\n+        ds_1.broadcast().map(lambda x: x + 1).set_parallelism(3).add_sink(self.test_sink)\n+        exec_plan = eval(self.env.get_execution_plan())\n+        broadcast_node = exec_plan['nodes'][1]\n+        pre_ship_strategy = broadcast_node['predecessors'][0]['ship_strategy']\n+        self.assertEqual(pre_ship_strategy, 'BROADCAST')\n+\n+    def test_rebalance(self):\n+        ds_1 = self.env.from_collection([1, 2, 3])\n+        ds_1.rebalance().map(lambda x: x + 1).set_parallelism(3).add_sink(self.test_sink)\n+        exec_plan = eval(self.env.get_execution_plan())\n+        rebalance_node = exec_plan['nodes'][1]\n+        pre_ship_strategy = rebalance_node['predecessors'][0]['ship_strategy']\n+        self.assertEqual(pre_ship_strategy, 'REBALANCE')\n+\n+    def test_rescale(self):\n+        ds_1 = self.env.from_collection([1, 2, 3])\n+        ds_1.rescale().map(lambda x: x + 1).set_parallelism(3).add_sink(self.test_sink)\n+        exec_plan = eval(self.env.get_execution_plan())\n+        rescale_node = exec_plan['nodes'][1]\n+        pre_ship_strategy = rescale_node['predecessors'][0]['ship_strategy']\n+        self.assertEqual(pre_ship_strategy, 'RESCALE')\n+\n+    def test_shuffle(self):\n+        ds_1 = self.env.from_collection([1, 2, 3])\n+        ds_1.shuffle().map(lambda x: x + 1).set_parallelism(3).add_sink(self.test_sink)\n+        exec_plan = eval(self.env.get_execution_plan())\n+        shuffle_node = exec_plan['nodes'][1]\n+        pre_ship_strategy = shuffle_node['predecessors'][0]['ship_strategy']\n+        self.assertEqual(pre_ship_strategy, 'SHUFFLE')", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ec6e531e07a441ccb328f735cc40d618087798c4"}, "originalPosition": 64}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTAwNDk3OQ==", "bodyText": "Thank you , I have added a specific test for overriding partitioning of KeyedStream.\n    def test_keyed_stream_partitioning(self):         \n        ds = self.env.from_collection([('ab', 1), ('bdc', 2), ('cfgs', 3), ('deeefg', 4)])\n        keyed_stream = ds.key_by(lambda x: x[1])\n        with self.assertRaises(Exception):\n            keyed_stream.shuffle()\n\n        with self.assertRaises(Exception):\n            keyed_stream.rebalance()\n\n        with self.assertRaises(Exception):\n            keyed_stream.rescale()\n\n        with self.assertRaises(Exception):\n            keyed_stream.broadcast()\n\n        with self.assertRaises(Exception):\n            keyed_stream.forward()", "url": "https://github.com/apache/flink/pull/13119#discussion_r469004979", "createdAt": "2020-08-12T04:56:53Z", "author": {"login": "shuiqiangchen"}, "path": "flink-python/pyflink/datastream/tests/test_data_stream.py", "diffHunk": "@@ -242,6 +242,68 @@ def test_print_with_align_output(self):\n         self.assertEqual(3, len(plan['nodes']))\n         self.assertEqual(\"Sink: Print to Std. Out\", plan['nodes'][2]['type'])\n \n+    def test_union_stream(self):\n+        ds_1 = self.env.from_collection([1, 2, 3])\n+        ds_2 = self.env.from_collection([4, 5, 6])\n+        ds_3 = self.env.from_collection([7, 8, 9])\n+\n+        united_stream = ds_3.union(ds_1, ds_2)\n+\n+        united_stream.map(lambda x: x + 1).add_sink(self.test_sink)\n+        exec_plan = eval(self.env.get_execution_plan())\n+        source_ids = []\n+        union_node_pre_ids = []\n+        for node in exec_plan['nodes']:\n+            if node['pact'] == 'Data Source':\n+                source_ids.append(node['id'])\n+            if node['pact'] == 'Operator':\n+                for pre in node['predecessors']:\n+                    union_node_pre_ids.append(pre['id'])\n+\n+        source_ids.sort()\n+        union_node_pre_ids.sort()\n+        self.assertEqual(source_ids, union_node_pre_ids)\n+\n+    def test_project(self):\n+        ds = self.env.from_collection([[1, 2, 3, 4], [5, 6, 7, 8]],\n+                                      type_info=Types.TUPLE(\n+                                          [Types.INT(), Types.INT(), Types.INT(), Types.INT()]))\n+        ds.project(1, 3).map(lambda x: (x[0], x[1] + 1)).add_sink(self.test_sink)\n+        exec_plan = eval(self.env.get_execution_plan())\n+        self.assertEqual(exec_plan['nodes'][1]['type'], 'Projection')\n+\n+    def test_broadcast(self):\n+        ds_1 = self.env.from_collection([1, 2, 3])\n+        ds_1.broadcast().map(lambda x: x + 1).set_parallelism(3).add_sink(self.test_sink)\n+        exec_plan = eval(self.env.get_execution_plan())\n+        broadcast_node = exec_plan['nodes'][1]\n+        pre_ship_strategy = broadcast_node['predecessors'][0]['ship_strategy']\n+        self.assertEqual(pre_ship_strategy, 'BROADCAST')\n+\n+    def test_rebalance(self):\n+        ds_1 = self.env.from_collection([1, 2, 3])\n+        ds_1.rebalance().map(lambda x: x + 1).set_parallelism(3).add_sink(self.test_sink)\n+        exec_plan = eval(self.env.get_execution_plan())\n+        rebalance_node = exec_plan['nodes'][1]\n+        pre_ship_strategy = rebalance_node['predecessors'][0]['ship_strategy']\n+        self.assertEqual(pre_ship_strategy, 'REBALANCE')\n+\n+    def test_rescale(self):\n+        ds_1 = self.env.from_collection([1, 2, 3])\n+        ds_1.rescale().map(lambda x: x + 1).set_parallelism(3).add_sink(self.test_sink)\n+        exec_plan = eval(self.env.get_execution_plan())\n+        rescale_node = exec_plan['nodes'][1]\n+        pre_ship_strategy = rescale_node['predecessors'][0]['ship_strategy']\n+        self.assertEqual(pre_ship_strategy, 'RESCALE')\n+\n+    def test_shuffle(self):\n+        ds_1 = self.env.from_collection([1, 2, 3])\n+        ds_1.shuffle().map(lambda x: x + 1).set_parallelism(3).add_sink(self.test_sink)\n+        exec_plan = eval(self.env.get_execution_plan())\n+        shuffle_node = exec_plan['nodes'][1]\n+        pre_ship_strategy = shuffle_node['predecessors'][0]['ship_strategy']\n+        self.assertEqual(pre_ship_strategy, 'SHUFFLE')", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk3Njk1Mg=="}, "originalCommit": {"oid": "ec6e531e07a441ccb328f735cc40d618087798c4"}, "originalPosition": 64}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 536, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}