{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzU4NTk2NTE3", "number": 10745, "title": "[FLINK-15445][connectors/jdbc] JDBC Table Source didn't work for Type\u2026", "bodyText": "\u2026s with precision (or/and scale)\nWhat is the purpose of the change\nJDBC table source didn't work for types with precision (and/or scale) in blink planner, such as TIMESTAMP(6), DECIMAL(10, 4). The ValidationException described in FLINK-15445 would be thrown. The root cause is JDBCTableSource didn't override getProducedDataType interface, and returns wrong datatype. This PR fix it and add an itcase to verify.\nBrief change log\n\n6daa934 Override getProducedDataType() for JDBCTableSource and add tests to verify\n2cc4e7b Override 'explainSource()` API to explain the pushdown applied and add test to verify\ned26962 Add validate API to check if this dialect instance support a specific data type\na12d3d5 Refactor the validation of data types for dialects\n\nVerifying this change\nThis change added tests\nDoes this pull request potentially affect one of the following parts:\n\nDependencies (does it add or upgrade a dependency): (yes / no)\nThe public API, i.e., is any changed class annotated with @Public(Evolving): (yes / no)\nThe serializers: (yes / no / don't know)\nThe runtime per-record code paths (performance sensitive): (yes / no / don't know)\nAnything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / no / don't know)\nThe S3 file system connector: (yes / no / don't know)\n\nDocumentation\n\nDoes this pull request introduce a new feature? (yes / no)\nIf yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)", "createdAt": "2020-01-02T09:24:49Z", "url": "https://github.com/apache/flink/pull/10745", "merged": true, "mergeCommit": {"oid": "4ad2c5f88d013054dbc022dbcea34386b7c9587b"}, "closed": true, "closedAt": "2020-02-12T08:24:55Z", "author": {"login": "docete"}, "timelineItems": {"totalCount": 26, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABb2WY9mgFqTMzNzU4MTcxMg==", "endCursor": "Y3Vyc29yOnYyOpPPAAABcDdm8FAH2gAyMzU4NTk2NTE3Ojc3OTA3OTdlNWU5MGFmMDJiYzk0YjQ4ZjUzMjYwNzc1NzM2MDBmMjU=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzM3NTgxNzEy", "url": "https://github.com/apache/flink/pull/10745#pullrequestreview-337581712", "createdAt": "2020-01-02T09:31:49Z", "commit": {"oid": "0c00c793d00c60c7dafc5b4f8209459a0dcc940b"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wMlQwOTozMTo0OVrOFZn2cg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wMlQwOTozMTo0OVrOFZn2cg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MjQxMTYzNA==", "bodyText": "This is wrong fix, you should modify returnType.", "url": "https://github.com/apache/flink/pull/10745#discussion_r362411634", "createdAt": "2020-01-02T09:31:49Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCTableSource.java", "diffHunk": "@@ -112,6 +113,11 @@ public boolean isBounded() {\n \t\treturn returnType;\n \t}\n \n+\t@Override\n+\tpublic DataType getProducedDataType() {\n+\t\treturn schema.toRowDataType();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c00c793d00c60c7dafc5b4f8209459a0dcc940b"}, "originalPosition": 14}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzM3NjQ3MjYy", "url": "https://github.com/apache/flink/pull/10745#pullrequestreview-337647262", "createdAt": "2020-01-02T12:58:50Z", "commit": {"oid": "2cc4e7b6e88e9f655235b8c97707c41f45b6b238"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wMlQxMjo1ODo1MFrOFZrFBw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wMlQxMzowMjo1MlrOFZrIlA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MjQ2NDUxOQ==", "bodyText": "We can mark it as provided and put besides old planner dependency.\n<scope>provided</scope>\n<optional>true</optional>", "url": "https://github.com/apache/flink/pull/10745#discussion_r362464519", "createdAt": "2020-01-02T12:58:50Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-jdbc/pom.xml", "diffHunk": "@@ -82,5 +82,20 @@ under the License.\n \t\t\t<type>test-jar</type>\n \t\t\t<scope>test</scope>\n \t\t</dependency>\n+\n+\t\t<dependency>\n+\t\t\t<groupId>org.apache.flink</groupId>\n+\t\t\t<artifactId>flink-table-planner-blink_${scala.binary.version}</artifactId>\n+\t\t\t<version>${project.version}</version>\n+\t\t\t<scope>test</scope>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2cc4e7b6e88e9f655235b8c97707c41f45b6b238"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MjQ2NTE5Ng==", "bodyText": "Please remove the overrided implementation of getReturnType()", "url": "https://github.com/apache/flink/pull/10745#discussion_r362465196", "createdAt": "2020-01-02T13:01:52Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCTableSource.java", "diffHunk": "@@ -112,6 +120,11 @@ public boolean isBounded() {\n \t\treturn returnType;\n \t}\n \n+\t@Override\n+\tpublic DataType getProducedDataType() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2cc4e7b6e88e9f655235b8c97707c41f45b6b238"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MjQ2NTQyOA==", "bodyText": "Use returnType.getFieldNames instead reconstruct the selected field names again?", "url": "https://github.com/apache/flink/pull/10745#discussion_r362465428", "createdAt": "2020-01-02T13:02:52Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCTableSource.java", "diffHunk": "@@ -180,6 +193,20 @@ public boolean equals(Object o) {\n \t\t}\n \t}\n \n+\t@Override\n+\tpublic String explainSource() {\n+\t\tif (selectFields == null) {\n+\t\t\treturn String.format(\n+\t\t\t\t\"JDBCTableSource(read fields: %s)\", String.join(\", \", schema.getFieldNames()));\n+\t\t} else {\n+\t\t\tString[] fields = new String[selectFields.length];\n+\t\t\tfor (int i = 0; i < selectFields.length; i++) {\n+\t\t\t\tfields[i] = schema.getFieldName(selectFields[i]).get();\n+\t\t\t}\n+\t\t\treturn String.format(\"JDBCTableSource(read fields: %s)\", String.join(\", \", fields));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2cc4e7b6e88e9f655235b8c97707c41f45b6b238"}, "originalPosition": 69}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzM3OTM0Mzgx", "url": "https://github.com/apache/flink/pull/10745#pullrequestreview-337934381", "createdAt": "2020-01-03T02:40:52Z", "commit": {"oid": "2cc4e7b6e88e9f655235b8c97707c41f45b6b238"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wM1QwMjo0MDo1MlrOFZ5EaQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wM1QwMjo0MDo1MlrOFZ5EaQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MjY5MzczNw==", "bodyText": "Could you add full list of types to have a full coverage? For example, add TIMESTAMP, TIMESTAMP(9), DECIMAL(38, 18), DECIMAL, FLOAT (we have a bug for float before), etc...\nI would also suggest to combine source integrate tests and sink integrate tests, e.g.  read from collections and write into jdbc using SQL, and read from JDBC to verify the result.", "url": "https://github.com/apache/flink/pull/10745#discussion_r362693737", "createdAt": "2020-01-03T02:40:52Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceITCase.java", "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.api.java.io.jdbc;\n+\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.table.api.EnvironmentSettings;\n+import org.apache.flink.table.api.java.StreamTableEnvironment;\n+import org.apache.flink.table.runtime.utils.StreamITCase;\n+import org.apache.flink.test.util.AbstractTestBase;\n+import org.apache.flink.types.Row;\n+\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.sql.Connection;\n+import java.sql.DriverManager;\n+import java.sql.SQLException;\n+import java.sql.Statement;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+\n+/**\n+ * ITCase for {@link JDBCTableSource}.\n+ */\n+public class JDBCTableSourceITCase extends AbstractTestBase {\n+\n+\tpublic static final String DRIVER_CLASS = \"org.apache.derby.jdbc.EmbeddedDriver\";\n+\tpublic static final String DB_URL = \"jdbc:derby:memory:test\";\n+\tpublic static final String INPUT_TABLE = \"jdbcSource\";\n+\n+\t@Before\n+\tpublic void before() throws ClassNotFoundException, SQLException {\n+\t\tSystem.setProperty(\"derby.stream.error.field\", JDBCTestBase.class.getCanonicalName() + \".DEV_NULL\");\n+\t\tClass.forName(DRIVER_CLASS);\n+\n+\t\ttry (\n+\t\t\tConnection conn = DriverManager.getConnection(DB_URL + \";create=true\");\n+\t\t\tStatement statement = conn.createStatement()) {\n+\t\t\tstatement.executeUpdate(\"CREATE TABLE \" + INPUT_TABLE + \" (\" +\n+\t\t\t\t\t\"id BIGINT NOT NULL,\" +\n+\t\t\t\t\t\"timestamp6_col TIMESTAMP, \" +\n+\t\t\t\t\t\"time_col TIME, \" +\n+\t\t\t\t\t\"decimal_col DECIMAL(10, 4))\");\n+\t\t\tstatement.executeUpdate(\"INSERT INTO \" + INPUT_TABLE + \" VALUES (\" +\n+\t\t\t\t\t\"1, TIMESTAMP('2020-01-01 15:35:00.123456'), TIME('15:35:00'), 100.1234)\");\n+\t\t\tstatement.executeUpdate(\"INSERT INTO \" + INPUT_TABLE + \" VALUES (\" +\n+\t\t\t\t\t\"2, TIMESTAMP('2020-01-01 15:36:01.123456'), TIME('15:36:01'), 101.1234)\");\n+\t\t}\n+\t}\n+\n+\t@After\n+\tpublic void clearOutputTable() throws Exception {\n+\t\tClass.forName(DRIVER_CLASS);\n+\t\ttry (\n+\t\t\tConnection conn = DriverManager.getConnection(DB_URL);\n+\t\t\tStatement stat = conn.createStatement()) {\n+\t\t\tstat.execute(\"DROP TABLE \" + INPUT_TABLE);\n+\t\t}\n+\t}\n+\n+\t@Test\n+\tpublic void testJDBCSource() throws Exception {\n+\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+\t\tEnvironmentSettings envSettings = EnvironmentSettings.newInstance()\n+\t\t\t.useBlinkPlanner()\n+\t\t\t.inStreamingMode()\n+\t\t\t.build();\n+\t\tStreamTableEnvironment tEnv = StreamTableEnvironment.create(env, envSettings);\n+\n+\t\ttEnv.sqlUpdate(\n+\t\t\t\"CREATE TABLE \" + INPUT_TABLE + \"(\" +\n+\t\t\t\t\"id BIGINT,\" +\n+\t\t\t\t\"timestamp6_col TIMESTAMP(6),\" +\n+\t\t\t\t\"time_col TIME,\" +\n+\t\t\t\t\"decimal_col DECIMAL(10, 4)\" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2cc4e7b6e88e9f655235b8c97707c41f45b6b238"}, "originalPosition": 93}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f0a7af07324182da2121f873e8e857459f425194", "author": {"user": {"login": "docete", "name": "Zhenghua Gao"}}, "url": "https://github.com/apache/flink/commit/f0a7af07324182da2121f873e8e857459f425194", "committedDate": "2020-02-04T07:56:32Z", "message": "[FLINK-15445][connectors/jdbc] JDBC Table Source didn't work for Types with precision (or/and scale)"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4a2e1789331ad3d6b7ed5a34f2a64c7893528a1b", "author": {"user": {"login": "docete", "name": "Zhenghua Gao"}}, "url": "https://github.com/apache/flink/commit/4a2e1789331ad3d6b7ed5a34f2a64c7893528a1b", "committedDate": "2020-02-04T07:58:57Z", "message": "fixup: adjust dependencies"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "24f4f9782b3332d62bfb0933b423ae3a5843318b", "author": {"user": {"login": "docete", "name": "Zhenghua Gao"}}, "url": "https://github.com/apache/flink/commit/24f4f9782b3332d62bfb0933b423ae3a5843318b", "committedDate": "2020-02-04T07:58:57Z", "message": "fixup: fix projection push-down"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a22c3c94d4e6323543ca9b23254bcea343625f5d", "author": {"user": {"login": "docete", "name": "Zhenghua Gao"}}, "url": "https://github.com/apache/flink/commit/a22c3c94d4e6323543ca9b23254bcea343625f5d", "committedDate": "2020-02-04T07:58:57Z", "message": "[FLINK-15445][connectors/jdbc] JDBCTableSource should override and change explainSource() API to explain the pushdown applied"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a3c34fd3dc54726056e63db06136917a566068a4", "author": {"user": {"login": "docete", "name": "Zhenghua Gao"}}, "url": "https://github.com/apache/flink/commit/a3c34fd3dc54726056e63db06136917a566068a4", "committedDate": "2020-02-04T07:58:57Z", "message": "fixup: address Jark's comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5858540ad796f272248b09f6b65ea9f2dd402a6f", "author": {"user": {"login": "docete", "name": "Zhenghua Gao"}}, "url": "https://github.com/apache/flink/commit/5858540ad796f272248b09f6b65ea9f2dd402a6f", "committedDate": "2020-02-04T07:58:57Z", "message": "fixup: test more data types"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5ed269626192adabb37b5481c21c91ad4c892592", "author": {"user": {"login": "docete", "name": "Zhenghua Gao"}}, "url": "https://github.com/apache/flink/commit/5ed269626192adabb37b5481c21c91ad4c892592", "committedDate": "2020-02-04T07:58:57Z", "message": "[FLINK-15445][connectors/jdbc] validate if the dialect instance support a specific data type"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e8d921d91449b3c3c8afea064af7ec99daf8d976", "author": {"user": {"login": "docete", "name": "Zhenghua Gao"}}, "url": "https://github.com/apache/flink/commit/e8d921d91449b3c3c8afea064af7ec99daf8d976", "committedDate": "2020-02-04T07:58:57Z", "message": "fixup: derby only support decimal with precision [1,31]"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f7456ac7e14681e18199941e0962b207eb99bef9", "author": {"user": {"login": "docete", "name": "Zhenghua Gao"}}, "url": "https://github.com/apache/flink/commit/f7456ac7e14681e18199941e0962b207eb99bef9", "committedDate": "2020-02-04T08:59:05Z", "message": "fixup: rebase to resolve conflicts"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "d915cd1e7d4efc5a2fb114be21483e249f2eca0b", "author": {"user": {"login": "docete", "name": "Zhenghua Gao"}}, "url": "https://github.com/apache/flink/commit/d915cd1e7d4efc5a2fb114be21483e249f2eca0b", "committedDate": "2020-01-13T01:51:43Z", "message": "fixup: derby only support decimal with precision [1,31]"}, "afterCommit": {"oid": "f7456ac7e14681e18199941e0962b207eb99bef9", "author": {"user": {"login": "docete", "name": "Zhenghua Gao"}}, "url": "https://github.com/apache/flink/commit/f7456ac7e14681e18199941e0962b207eb99bef9", "committedDate": "2020-02-04T08:59:05Z", "message": "fixup: rebase to resolve conflicts"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU0OTA5MDI0", "url": "https://github.com/apache/flink/pull/10745#pullrequestreview-354909024", "createdAt": "2020-02-07T03:10:06Z", "commit": {"oid": "f7456ac7e14681e18199941e0962b207eb99bef9"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wN1QwMzoxMDowNlrOFmxIrQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wN1QwNDowODo0N1rOFmx0Zg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjE5NTI0NQ==", "bodyText": "Can we remove the returnType member field?\nIt's error-prone to maintain two objects. The returnType is only used in getDataStream and can be derived via TypeConversions.fromDataTypeToLegacyInfo(producedDataType).", "url": "https://github.com/apache/flink/pull/10745#discussion_r376195245", "createdAt": "2020-02-07T03:10:06Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCTableSource.java", "diffHunk": "@@ -73,17 +75,23 @@ private JDBCTableSource(\n \t\tthis.selectFields = selectFields;\n \n \t\tfinal TypeInformation<?>[] schemaTypeInfos = schema.getFieldTypes();\n+\t\tfinal DataType[] schemaDataTypes = schema.getFieldDataTypes();\n \t\tfinal String[] schemaFieldNames = schema.getFieldNames();\n \t\tif (selectFields != null) {\n \t\t\tTypeInformation<?>[] typeInfos = new TypeInformation[selectFields.length];\n-\t\t\tString[] typeNames = new String[selectFields.length];\n+\t\t\tDataType[] dataTypes = new DataType[selectFields.length];\n+\t\t\tString[] fieldNames = new String[selectFields.length];\n \t\t\tfor (int i = 0; i < selectFields.length; i++) {\n \t\t\t\ttypeInfos[i] = schemaTypeInfos[selectFields[i]];\n-\t\t\t\ttypeNames[i] = schemaFieldNames[selectFields[i]];\n+\t\t\t\tdataTypes[i] = schemaDataTypes[selectFields[i]];\n+\t\t\t\tfieldNames[i] = schemaFieldNames[selectFields[i]];\n \t\t\t}\n-\t\t\tthis.returnType = new RowTypeInfo(typeInfos, typeNames);\n+\t\t\tthis.returnType = new RowTypeInfo(typeInfos, fieldNames);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f7456ac7e14681e18199941e0962b207eb99bef9"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjE5NjEzMw==", "bodyText": "I think it would be better to move the validation logic into JDBCValidator.\nhttps://github.com/apache/flink/blob/master/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/table/descriptors/JDBCValidator.java#L73", "url": "https://github.com/apache/flink/pull/10745#discussion_r376196133", "createdAt": "2020-02-07T03:14:13Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceSinkFactory.java", "diffHunk": "@@ -130,8 +130,11 @@\n \t\tTableSchema schema = TableSchemaUtils.getPhysicalSchema(\n \t\t\tdescriptorProperties.getTableSchema(SCHEMA));\n \n+\t\tJDBCOptions options = getJDBCOptions(descriptorProperties);\n+\t\toptions.getDialect().validate(schema);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f7456ac7e14681e18199941e0962b207eb99bef9"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjE5ODM0NQ==", "bodyText": "You can create a unit test JDBCDataTypeTest to verify all the types with different precision with different dialects. This doesn't involve a job submission, and is a lightweight unit test. You can take FlinkDDLDataTypeTest as an example.", "url": "https://github.com/apache/flink/pull/10745#discussion_r376198345", "createdAt": "2020-02-07T03:25:04Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceITCase.java", "diffHunk": "@@ -18,93 +18,167 @@\n \n package org.apache.flink.api.java.io.jdbc;\n \n-import org.apache.flink.streaming.api.datastream.DataStream;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n import org.apache.flink.table.api.EnvironmentSettings;\n-import org.apache.flink.table.api.Table;\n+import org.apache.flink.table.api.TableException;\n import org.apache.flink.table.api.java.StreamTableEnvironment;\n import org.apache.flink.table.runtime.utils.StreamITCase;\n+import org.apache.flink.test.util.AbstractTestBase;\n import org.apache.flink.types.Row;\n \n-import org.junit.BeforeClass;\n+import org.junit.After;\n+import org.junit.Before;\n import org.junit.Test;\n \n-import java.util.ArrayList;\n+import java.sql.Connection;\n+import java.sql.DriverManager;\n+import java.sql.SQLException;\n+import java.sql.Statement;\n+import java.util.Arrays;\n import java.util.List;\n \n+\n /**\n- * IT case for {@link JDBCTableSource}.\n+ * ITCase for {@link JDBCTableSource}.\n  */\n-public class JDBCTableSourceITCase extends JDBCTestBase {\n-\n-\tprivate static final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n-\tprivate static final EnvironmentSettings bsSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();\n-\tprivate static final StreamTableEnvironment tEnv = StreamTableEnvironment.create(env, bsSettings);\n-\n-\tstatic final String TABLE_SOURCE_SQL = \"CREATE TABLE books (\" +\n-\t\t\" id int, \" +\n-\t\t\" title varchar, \" +\n-\t\t\" author varchar, \" +\n-\t\t\" price double, \" +\n-\t\t\" qty int \" +\n-\t\t\") with (\" +\n-\t\t\" 'connector.type' = 'jdbc', \" +\n-\t\t\" 'connector.url' = 'jdbc:derby:memory:ebookshop', \" +\n-\t\t\" 'connector.table' = 'books', \" +\n-\t\t\" 'connector.driver' = 'org.apache.derby.jdbc.EmbeddedDriver' \" +\n-\t\t\")\";\n-\n-\t@BeforeClass\n-\tpublic static void createTable() {\n-\t\ttEnv.sqlUpdate(TABLE_SOURCE_SQL);\n+public class JDBCTableSourceITCase extends AbstractTestBase {\n+\n+\tpublic static final String DRIVER_CLASS = \"org.apache.derby.jdbc.EmbeddedDriver\";\n+\tpublic static final String DB_URL = \"jdbc:derby:memory:test\";\n+\tpublic static final String INPUT_TABLE = \"jdbcSource\";\n+\n+\t@Before\n+\tpublic void before() throws ClassNotFoundException, SQLException {\n+\t\tSystem.setProperty(\"derby.stream.error.field\", JDBCTestBase.class.getCanonicalName() + \".DEV_NULL\");\n+\t\tClass.forName(DRIVER_CLASS);\n+\n+\t\ttry (\n+\t\t\tConnection conn = DriverManager.getConnection(DB_URL + \";create=true\");\n+\t\t\tStatement statement = conn.createStatement()) {\n+\t\t\tstatement.executeUpdate(\"CREATE TABLE \" + INPUT_TABLE + \" (\" +\n+\t\t\t\t\t\"id BIGINT NOT NULL,\" +\n+\t\t\t\t\t\"timestamp6_col TIMESTAMP, \" +\n+\t\t\t\t\t\"timestamp9_col TIMESTAMP, \" +\n+\t\t\t\t\t\"time_col TIME, \" +\n+\t\t\t\t\t\"real_col FLOAT(23), \" +    // A precision of 23 or less makes FLOAT equivalent to REAL.\n+\t\t\t\t\t\"double_col FLOAT(24),\" +   // A precision of 24 or greater makes FLOAT equivalent to DOUBLE PRECISION.\n+\t\t\t\t\t\"decimal_col DECIMAL(10, 4))\");\n+\t\t\tstatement.executeUpdate(\"INSERT INTO \" + INPUT_TABLE + \" VALUES (\" +\n+\t\t\t\t\t\"1, TIMESTAMP('2020-01-01 15:35:00.123456'), TIMESTAMP('2020-01-01 15:35:00.123456789'), \" +\n+\t\t\t\t\t\"TIME('15:35:00'), 1.175E-37, 1.79769E+308, 100.1234)\");\n+\t\t\tstatement.executeUpdate(\"INSERT INTO \" + INPUT_TABLE + \" VALUES (\" +\n+\t\t\t\t\t\"2, TIMESTAMP('2020-01-01 15:36:01.123456'), TIMESTAMP('2020-01-01 15:36:01.123456789'), \" +\n+\t\t\t\t\t\"TIME('15:36:01'), -1.175E-37, -1.79769E+308, 101.1234)\");\n+\t\t}\n+\t}\n+\n+\t@After\n+\tpublic void clearOutputTable() throws Exception {\n+\t\tClass.forName(DRIVER_CLASS);\n+\t\ttry (\n+\t\t\tConnection conn = DriverManager.getConnection(DB_URL);\n+\t\t\tStatement stat = conn.createStatement()) {\n+\t\t\tstat.execute(\"DROP TABLE \" + INPUT_TABLE);\n+\t\t}\n \t}\n \n \t@Test\n-\tpublic void testFieldsProjection() throws Exception {\n-\t\tStreamITCase.clear();\n+\tpublic void testJDBCSource() throws Exception {\n+\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+\t\tEnvironmentSettings envSettings = EnvironmentSettings.newInstance()\n+\t\t\t.useBlinkPlanner()\n+\t\t\t.inStreamingMode()\n+\t\t\t.build();\n+\t\tStreamTableEnvironment tEnv = StreamTableEnvironment.create(env, envSettings);\n+\n+\t\ttEnv.sqlUpdate(\n+\t\t\t\"CREATE TABLE \" + INPUT_TABLE + \"(\" +\n+\t\t\t\t\"id BIGINT,\" +\n+\t\t\t\t\"timestamp6_col TIMESTAMP(6),\" +\n+\t\t\t\t\"timestamp9_col TIMESTAMP(9),\" +\n+\t\t\t\t\"time_col TIME,\" +\n+\t\t\t\t\"real_col FLOAT,\" +\n+\t\t\t\t\"double_col DOUBLE,\" +\n+\t\t\t\t\"decimal_col DECIMAL(10, 4)\" +\n+\t\t\t\t\") WITH (\" +\n+\t\t\t\t\"  'connector.type'='jdbc',\" +\n+\t\t\t\t\"  'connector.url'='\" + DB_URL + \"',\" +\n+\t\t\t\t\"  'connector.table'='\" + INPUT_TABLE + \"'\" +\n+\t\t\t\t\")\"\n+\t\t);\n \n-\t\tTable result = tEnv.sqlQuery(SELECT_ID_BOOKS);\n-\t\tDataStream<Row> resultSet = tEnv.toAppendStream(result, Row.class);\n-\t\tresultSet.addSink(new StreamITCase.StringSink<>());\n+\t\tStreamITCase.clear();\n+\t\ttEnv.toAppendStream(tEnv.sqlQuery(\"SELECT * FROM \" + INPUT_TABLE), Row.class)\n+\t\t\t.addSink(new StreamITCase.StringSink<>());\n \t\tenv.execute();\n \n-\t\tList<String> expected = new ArrayList<>();\n-\t\texpected.add(\"1001\");\n-\t\texpected.add(\"1002\");\n-\t\texpected.add(\"1003\");\n-\t\texpected.add(\"1004\");\n-\t\texpected.add(\"1005\");\n-\t\texpected.add(\"1006\");\n-\t\texpected.add(\"1007\");\n-\t\texpected.add(\"1008\");\n-\t\texpected.add(\"1009\");\n-\t\texpected.add(\"1010\");\n-\n+\t\tList<String> expected =\n+\t\t\tArrays.asList(\n+\t\t\t\t\"1,2020-01-01T15:35:00.123456,2020-01-01T15:35:00.123456789,15:35,1.175E-37,1.79769E308,100.1234\",\n+\t\t\t\t\"2,2020-01-01T15:36:01.123456,2020-01-01T15:36:01.123456789,15:36:01,-1.175E-37,-1.79769E308,101.1234\");\n \t\tStreamITCase.compareWithList(expected);\n \t}\n \n \t@Test\n-\tpublic void testAllFieldsSelection() throws Exception {\n-\t\tStreamITCase.clear();\n+\tpublic void testProjectableJDBCSource() throws Exception {\n+\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+\t\tEnvironmentSettings envSettings = EnvironmentSettings.newInstance()\n+\t\t\t\t.useBlinkPlanner()\n+\t\t\t\t.inStreamingMode()\n+\t\t\t\t.build();\n+\t\tStreamTableEnvironment tEnv = StreamTableEnvironment.create(env, envSettings);\n+\n+\t\ttEnv.sqlUpdate(\n+\t\t\t\"CREATE TABLE \" + INPUT_TABLE + \"(\" +\n+\t\t\t\t\"id BIGINT,\" +\n+\t\t\t\t\"timestamp6_col TIMESTAMP(6),\" +\n+\t\t\t\t\"timestamp9_col TIMESTAMP(9),\" +\n+\t\t\t\t\"time_col TIME,\" +\n+\t\t\t\t\"real_col FLOAT,\" +\n+\t\t\t\t\"decimal_col DECIMAL(10, 4)\" +\n+\t\t\t\t\") WITH (\" +\n+\t\t\t\t\"  'connector.type'='jdbc',\" +\n+\t\t\t\t\"  'connector.url'='\" + DB_URL + \"',\" +\n+\t\t\t\t\"  'connector.table'='\" + INPUT_TABLE + \"'\" +\n+\t\t\t\t\")\"\n+\t\t);\n \n-\t\tTable result = tEnv.sqlQuery(SELECT_ALL_BOOKS);\n-\t\tDataStream<Row> resultSet = tEnv.toAppendStream(result, Row.class);\n-\t\tresultSet.addSink(new StreamITCase.StringSink<>());\n+\t\tStreamITCase.clear();\n+\t\ttEnv.toAppendStream(tEnv.sqlQuery(\"SELECT timestamp6_col, decimal_col FROM \" + INPUT_TABLE), Row.class)\n+\t\t\t\t.addSink(new StreamITCase.StringSink<>());\n \t\tenv.execute();\n \n-\t\tList<String> expected = new ArrayList<>();\n-\t\texpected.add(\"1001,Java public for dummies,Tan Ah Teck,11.11,11\");\n-\t\texpected.add(\"1002,More Java for dummies,Tan Ah Teck,22.22,22\");\n-\t\texpected.add(\"1003,More Java for more dummies,Mohammad Ali,33.33,33\");\n-\t\texpected.add(\"1004,A Cup of Java,Kumar,44.44,44\");\n-\t\texpected.add(\"1005,A Teaspoon of Java,Kevin Jones,55.55,55\");\n-\t\texpected.add(\"1006,A Teaspoon of Java 1.4,Kevin Jones,66.66,66\");\n-\t\texpected.add(\"1007,A Teaspoon of Java 1.5,Kevin Jones,77.77,77\");\n-\t\texpected.add(\"1008,A Teaspoon of Java 1.6,Kevin Jones,88.88,88\");\n-\t\texpected.add(\"1009,A Teaspoon of Java 1.7,Kevin Jones,99.99,99\");\n-\t\texpected.add(\"1010,A Teaspoon of Java 1.8,Kevin Jones,null,1010\");\n-\n+\t\tList<String> expected =\n+\t\t\tArrays.asList(\n+\t\t\t\t\"2020-01-01T15:35:00.123456,100.1234\",\n+\t\t\t\t\"2020-01-01T15:36:01.123456,101.1234\");\n \t\tStreamITCase.compareWithList(expected);\n \t}\n \n+\t@Test(expected = TableException.class)\n+\tpublic void testInvalidPrecisionOfJDBCSource() throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f7456ac7e14681e18199941e0962b207eb99bef9"}, "originalPosition": 203}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjE5OTA2OQ==", "bodyText": "throws ValidationException on the method signature. And please add a description about the exception in the javadoc.", "url": "https://github.com/apache/flink/pull/10745#discussion_r376199069", "createdAt": "2020-02-07T03:28:03Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialect.java", "diffHunk": "@@ -35,6 +37,15 @@\n \t */\n \tboolean canHandle(String url);\n \n+\t/**\n+\t * Check if this dialect instance support a specific data type in table schema.\n+\t *\n+\t * @param schema the table schema\n+\t */\n+\tdefault void validate(TableSchema schema) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f7456ac7e14681e18199941e0962b207eb99bef9"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjE5OTA5OA==", "bodyText": "remove emtpy line.", "url": "https://github.com/apache/flink/pull/10745#discussion_r376199098", "createdAt": "2020-02-07T03:28:10Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialect.java", "diffHunk": "@@ -35,6 +37,15 @@\n \t */\n \tboolean canHandle(String url);\n \n+\t/**\n+\t * Check if this dialect instance support a specific data type in table schema.\n+\t *", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f7456ac7e14681e18199941e0962b207eb99bef9"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjIwMjUwNQ==", "bodyText": "We don't need to return if the return type is void.", "url": "https://github.com/apache/flink/pull/10745#discussion_r376202505", "createdAt": "2020-02-07T03:46:24Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialect.java", "diffHunk": "@@ -35,6 +37,15 @@\n \t */\n \tboolean canHandle(String url);\n \n+\t/**\n+\t * Check if this dialect instance support a specific data type in table schema.\n+\t *\n+\t * @param schema the table schema\n+\t */\n+\tdefault void validate(TableSchema schema) {\n+\t\treturn;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f7456ac7e14681e18199941e0962b207eb99bef9"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjIwMzcwNQ==", "bodyText": "It seems that the validation logic is the same, maybe we can refactor it a bit more to have a AbstractJDBCDialect which implements JDBCDialect.\n\tprivate abstract static class AbstractDialect implements JDBCDialect {\n\n\t\t@Override\n\t\tpublic void validate(TableSchema schema) throws ValidationException {\n\t\t\t// implement the common validation logic here\n\t\t}\n\t\t\n\t\tpublic abstract int maxDecimalPrecision();\n\t\t\n\t\tpublic abstract int minDecimalPrecision();\n\t\t\n\t\tpublic abstract int maxTimestampPrecision();\n\t\t\n\t\tpublic abstract int minTimestampPrecision();\n\t\t\n\t\tpublic abstract List<LogicalTypeRoot> unsupportedTypes();\n\t}", "url": "https://github.com/apache/flink/pull/10745#discussion_r376203705", "createdAt": "2020-02-07T03:52:30Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialects.java", "diffHunk": "@@ -70,11 +105,33 @@ public String quoteIdentifier(String identifier) {\n \n \t\tprivate static final long serialVersionUID = 1L;\n \n+\t\tprivate static final int MAX_MYSQL_TIMESTAMP_PRECISION = 6;\n+\n+\t\tprivate static final int MIN_MYSQL_TIMESTAMP_PRECISION = 0;\n+\n \t\t@Override\n \t\tpublic boolean canHandle(String url) {\n \t\t\treturn url.startsWith(\"jdbc:mysql:\");\n \t\t}\n \n+\t\t@Override\n+\t\tpublic void validate(TableSchema schema) {\n+\t\t\tfor (int i = 0; i < schema.getFieldCount(); i++) {\n+\t\t\t\tDataType dt = schema.getFieldDataType(i).get();\n+\t\t\t\tString fieldName = schema.getFieldName(i).get();\n+\t\t\t\tif (TIMESTAMP_WITHOUT_TIME_ZONE == dt.getLogicalType().getTypeRoot()) {\n+\t\t\t\t\tint precision = ((TimestampType) dt.getLogicalType()).getPrecision();\n+\t\t\t\t\tif (precision > MAX_MYSQL_TIMESTAMP_PRECISION) {\n+\t\t\t\t\t\tthrow new ValidationException(\n+\t\t\t\t\t\t\t\tString.format(\"The precision of %s is out of range [%d, %d].\",\n+\t\t\t\t\t\t\t\t\t\tfieldName,\n+\t\t\t\t\t\t\t\t\t\tMIN_MYSQL_TIMESTAMP_PRECISION,\n+\t\t\t\t\t\t\t\t\t\tMAX_MYSQL_TIMESTAMP_PRECISION));\n+\t\t\t\t\t}\n+\t\t\t\t}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f7456ac7e14681e18199941e0962b207eb99bef9"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjIwNDE1NA==", "bodyText": "Could you add a comment above these constants that includes documentation link describes the precision? So that we can have the single truth.", "url": "https://github.com/apache/flink/pull/10745#discussion_r376204154", "createdAt": "2020-02-07T03:55:16Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialects.java", "diffHunk": "@@ -50,11 +60,36 @@\n \n \t\tprivate static final long serialVersionUID = 1L;\n \n+\t\tprivate static final int MAX_DERBY_DECIMAL_PRECISION = 31;\n+\n+\t\tprivate static final int MIN_DERBY_DECIMAL_PRECISION = 1;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f7456ac7e14681e18199941e0962b207eb99bef9"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjIwNTUwNQ==", "bodyText": "Should we use stat.executeUdpate?", "url": "https://github.com/apache/flink/pull/10745#discussion_r376205505", "createdAt": "2020-02-07T04:03:04Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceITCase.java", "diffHunk": "@@ -18,93 +18,167 @@\n \n package org.apache.flink.api.java.io.jdbc;\n \n-import org.apache.flink.streaming.api.datastream.DataStream;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n import org.apache.flink.table.api.EnvironmentSettings;\n-import org.apache.flink.table.api.Table;\n+import org.apache.flink.table.api.TableException;\n import org.apache.flink.table.api.java.StreamTableEnvironment;\n import org.apache.flink.table.runtime.utils.StreamITCase;\n+import org.apache.flink.test.util.AbstractTestBase;\n import org.apache.flink.types.Row;\n \n-import org.junit.BeforeClass;\n+import org.junit.After;\n+import org.junit.Before;\n import org.junit.Test;\n \n-import java.util.ArrayList;\n+import java.sql.Connection;\n+import java.sql.DriverManager;\n+import java.sql.SQLException;\n+import java.sql.Statement;\n+import java.util.Arrays;\n import java.util.List;\n \n+\n /**\n- * IT case for {@link JDBCTableSource}.\n+ * ITCase for {@link JDBCTableSource}.\n  */\n-public class JDBCTableSourceITCase extends JDBCTestBase {\n-\n-\tprivate static final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n-\tprivate static final EnvironmentSettings bsSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();\n-\tprivate static final StreamTableEnvironment tEnv = StreamTableEnvironment.create(env, bsSettings);\n-\n-\tstatic final String TABLE_SOURCE_SQL = \"CREATE TABLE books (\" +\n-\t\t\" id int, \" +\n-\t\t\" title varchar, \" +\n-\t\t\" author varchar, \" +\n-\t\t\" price double, \" +\n-\t\t\" qty int \" +\n-\t\t\") with (\" +\n-\t\t\" 'connector.type' = 'jdbc', \" +\n-\t\t\" 'connector.url' = 'jdbc:derby:memory:ebookshop', \" +\n-\t\t\" 'connector.table' = 'books', \" +\n-\t\t\" 'connector.driver' = 'org.apache.derby.jdbc.EmbeddedDriver' \" +\n-\t\t\")\";\n-\n-\t@BeforeClass\n-\tpublic static void createTable() {\n-\t\ttEnv.sqlUpdate(TABLE_SOURCE_SQL);\n+public class JDBCTableSourceITCase extends AbstractTestBase {\n+\n+\tpublic static final String DRIVER_CLASS = \"org.apache.derby.jdbc.EmbeddedDriver\";\n+\tpublic static final String DB_URL = \"jdbc:derby:memory:test\";\n+\tpublic static final String INPUT_TABLE = \"jdbcSource\";\n+\n+\t@Before\n+\tpublic void before() throws ClassNotFoundException, SQLException {\n+\t\tSystem.setProperty(\"derby.stream.error.field\", JDBCTestBase.class.getCanonicalName() + \".DEV_NULL\");\n+\t\tClass.forName(DRIVER_CLASS);\n+\n+\t\ttry (\n+\t\t\tConnection conn = DriverManager.getConnection(DB_URL + \";create=true\");\n+\t\t\tStatement statement = conn.createStatement()) {\n+\t\t\tstatement.executeUpdate(\"CREATE TABLE \" + INPUT_TABLE + \" (\" +\n+\t\t\t\t\t\"id BIGINT NOT NULL,\" +\n+\t\t\t\t\t\"timestamp6_col TIMESTAMP, \" +\n+\t\t\t\t\t\"timestamp9_col TIMESTAMP, \" +\n+\t\t\t\t\t\"time_col TIME, \" +\n+\t\t\t\t\t\"real_col FLOAT(23), \" +    // A precision of 23 or less makes FLOAT equivalent to REAL.\n+\t\t\t\t\t\"double_col FLOAT(24),\" +   // A precision of 24 or greater makes FLOAT equivalent to DOUBLE PRECISION.\n+\t\t\t\t\t\"decimal_col DECIMAL(10, 4))\");\n+\t\t\tstatement.executeUpdate(\"INSERT INTO \" + INPUT_TABLE + \" VALUES (\" +\n+\t\t\t\t\t\"1, TIMESTAMP('2020-01-01 15:35:00.123456'), TIMESTAMP('2020-01-01 15:35:00.123456789'), \" +\n+\t\t\t\t\t\"TIME('15:35:00'), 1.175E-37, 1.79769E+308, 100.1234)\");\n+\t\t\tstatement.executeUpdate(\"INSERT INTO \" + INPUT_TABLE + \" VALUES (\" +\n+\t\t\t\t\t\"2, TIMESTAMP('2020-01-01 15:36:01.123456'), TIMESTAMP('2020-01-01 15:36:01.123456789'), \" +\n+\t\t\t\t\t\"TIME('15:36:01'), -1.175E-37, -1.79769E+308, 101.1234)\");\n+\t\t}\n+\t}\n+\n+\t@After\n+\tpublic void clearOutputTable() throws Exception {\n+\t\tClass.forName(DRIVER_CLASS);\n+\t\ttry (\n+\t\t\tConnection conn = DriverManager.getConnection(DB_URL);\n+\t\t\tStatement stat = conn.createStatement()) {\n+\t\t\tstat.execute(\"DROP TABLE \" + INPUT_TABLE);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f7456ac7e14681e18199941e0962b207eb99bef9"}, "originalPosition": 91}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjIwNjQzOA==", "bodyText": "Do we really need this? Is there any error messages thrown when run these tests?", "url": "https://github.com/apache/flink/pull/10745#discussion_r376206438", "createdAt": "2020-02-07T04:08:47Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceITCase.java", "diffHunk": "@@ -18,93 +18,167 @@\n \n package org.apache.flink.api.java.io.jdbc;\n \n-import org.apache.flink.streaming.api.datastream.DataStream;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n import org.apache.flink.table.api.EnvironmentSettings;\n-import org.apache.flink.table.api.Table;\n+import org.apache.flink.table.api.TableException;\n import org.apache.flink.table.api.java.StreamTableEnvironment;\n import org.apache.flink.table.runtime.utils.StreamITCase;\n+import org.apache.flink.test.util.AbstractTestBase;\n import org.apache.flink.types.Row;\n \n-import org.junit.BeforeClass;\n+import org.junit.After;\n+import org.junit.Before;\n import org.junit.Test;\n \n-import java.util.ArrayList;\n+import java.sql.Connection;\n+import java.sql.DriverManager;\n+import java.sql.SQLException;\n+import java.sql.Statement;\n+import java.util.Arrays;\n import java.util.List;\n \n+\n /**\n- * IT case for {@link JDBCTableSource}.\n+ * ITCase for {@link JDBCTableSource}.\n  */\n-public class JDBCTableSourceITCase extends JDBCTestBase {\n-\n-\tprivate static final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n-\tprivate static final EnvironmentSettings bsSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();\n-\tprivate static final StreamTableEnvironment tEnv = StreamTableEnvironment.create(env, bsSettings);\n-\n-\tstatic final String TABLE_SOURCE_SQL = \"CREATE TABLE books (\" +\n-\t\t\" id int, \" +\n-\t\t\" title varchar, \" +\n-\t\t\" author varchar, \" +\n-\t\t\" price double, \" +\n-\t\t\" qty int \" +\n-\t\t\") with (\" +\n-\t\t\" 'connector.type' = 'jdbc', \" +\n-\t\t\" 'connector.url' = 'jdbc:derby:memory:ebookshop', \" +\n-\t\t\" 'connector.table' = 'books', \" +\n-\t\t\" 'connector.driver' = 'org.apache.derby.jdbc.EmbeddedDriver' \" +\n-\t\t\")\";\n-\n-\t@BeforeClass\n-\tpublic static void createTable() {\n-\t\ttEnv.sqlUpdate(TABLE_SOURCE_SQL);\n+public class JDBCTableSourceITCase extends AbstractTestBase {\n+\n+\tpublic static final String DRIVER_CLASS = \"org.apache.derby.jdbc.EmbeddedDriver\";\n+\tpublic static final String DB_URL = \"jdbc:derby:memory:test\";\n+\tpublic static final String INPUT_TABLE = \"jdbcSource\";\n+\n+\t@Before\n+\tpublic void before() throws ClassNotFoundException, SQLException {\n+\t\tSystem.setProperty(\"derby.stream.error.field\", JDBCTestBase.class.getCanonicalName() + \".DEV_NULL\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f7456ac7e14681e18199941e0962b207eb99bef9"}, "originalPosition": 62}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "905972ad292b8657c04b5c4165371f0d8088f824", "author": {"user": {"login": "docete", "name": "Zhenghua Gao"}}, "url": "https://github.com/apache/flink/commit/905972ad292b8657c04b5c4165371f0d8088f824", "committedDate": "2020-02-07T08:21:35Z", "message": "fixup: remove the old style RowTypeInfo"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1aa14851215234acf47801d002c436bfd51588c2", "author": {"user": {"login": "docete", "name": "Zhenghua Gao"}}, "url": "https://github.com/apache/flink/commit/1aa14851215234acf47801d002c436bfd51588c2", "committedDate": "2020-02-07T08:28:48Z", "message": "fixup: remove useless code"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU1NTI0NDk4", "url": "https://github.com/apache/flink/pull/10745#pullrequestreview-355524498", "createdAt": "2020-02-08T02:39:41Z", "commit": {"oid": "f7456ac7e14681e18199941e0962b207eb99bef9"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wOFQwMjozOTo0MVrOFnOx_w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wOFQwMjozOTo0MVrOFnOx_w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjY4MDk1OQ==", "bodyText": "seems indent is not correct.", "url": "https://github.com/apache/flink/pull/10745#discussion_r376680959", "createdAt": "2020-02-08T02:39:41Z", "author": {"login": "libenchao"}, "path": "flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceITCase.java", "diffHunk": "@@ -18,93 +18,167 @@\n \n package org.apache.flink.api.java.io.jdbc;\n \n-import org.apache.flink.streaming.api.datastream.DataStream;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n import org.apache.flink.table.api.EnvironmentSettings;\n-import org.apache.flink.table.api.Table;\n+import org.apache.flink.table.api.TableException;\n import org.apache.flink.table.api.java.StreamTableEnvironment;\n import org.apache.flink.table.runtime.utils.StreamITCase;\n+import org.apache.flink.test.util.AbstractTestBase;\n import org.apache.flink.types.Row;\n \n-import org.junit.BeforeClass;\n+import org.junit.After;\n+import org.junit.Before;\n import org.junit.Test;\n \n-import java.util.ArrayList;\n+import java.sql.Connection;\n+import java.sql.DriverManager;\n+import java.sql.SQLException;\n+import java.sql.Statement;\n+import java.util.Arrays;\n import java.util.List;\n \n+\n /**\n- * IT case for {@link JDBCTableSource}.\n+ * ITCase for {@link JDBCTableSource}.\n  */\n-public class JDBCTableSourceITCase extends JDBCTestBase {\n-\n-\tprivate static final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n-\tprivate static final EnvironmentSettings bsSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();\n-\tprivate static final StreamTableEnvironment tEnv = StreamTableEnvironment.create(env, bsSettings);\n-\n-\tstatic final String TABLE_SOURCE_SQL = \"CREATE TABLE books (\" +\n-\t\t\" id int, \" +\n-\t\t\" title varchar, \" +\n-\t\t\" author varchar, \" +\n-\t\t\" price double, \" +\n-\t\t\" qty int \" +\n-\t\t\") with (\" +\n-\t\t\" 'connector.type' = 'jdbc', \" +\n-\t\t\" 'connector.url' = 'jdbc:derby:memory:ebookshop', \" +\n-\t\t\" 'connector.table' = 'books', \" +\n-\t\t\" 'connector.driver' = 'org.apache.derby.jdbc.EmbeddedDriver' \" +\n-\t\t\")\";\n-\n-\t@BeforeClass\n-\tpublic static void createTable() {\n-\t\ttEnv.sqlUpdate(TABLE_SOURCE_SQL);\n+public class JDBCTableSourceITCase extends AbstractTestBase {\n+\n+\tpublic static final String DRIVER_CLASS = \"org.apache.derby.jdbc.EmbeddedDriver\";\n+\tpublic static final String DB_URL = \"jdbc:derby:memory:test\";\n+\tpublic static final String INPUT_TABLE = \"jdbcSource\";\n+\n+\t@Before\n+\tpublic void before() throws ClassNotFoundException, SQLException {\n+\t\tSystem.setProperty(\"derby.stream.error.field\", JDBCTestBase.class.getCanonicalName() + \".DEV_NULL\");\n+\t\tClass.forName(DRIVER_CLASS);\n+\n+\t\ttry (\n+\t\t\tConnection conn = DriverManager.getConnection(DB_URL + \";create=true\");\n+\t\t\tStatement statement = conn.createStatement()) {\n+\t\t\tstatement.executeUpdate(\"CREATE TABLE \" + INPUT_TABLE + \" (\" +\n+\t\t\t\t\t\"id BIGINT NOT NULL,\" +\n+\t\t\t\t\t\"timestamp6_col TIMESTAMP, \" +\n+\t\t\t\t\t\"timestamp9_col TIMESTAMP, \" +\n+\t\t\t\t\t\"time_col TIME, \" +\n+\t\t\t\t\t\"real_col FLOAT(23), \" +    // A precision of 23 or less makes FLOAT equivalent to REAL.\n+\t\t\t\t\t\"double_col FLOAT(24),\" +   // A precision of 24 or greater makes FLOAT equivalent to DOUBLE PRECISION.\n+\t\t\t\t\t\"decimal_col DECIMAL(10, 4))\");\n+\t\t\tstatement.executeUpdate(\"INSERT INTO \" + INPUT_TABLE + \" VALUES (\" +\n+\t\t\t\t\t\"1, TIMESTAMP('2020-01-01 15:35:00.123456'), TIMESTAMP('2020-01-01 15:35:00.123456789'), \" +\n+\t\t\t\t\t\"TIME('15:35:00'), 1.175E-37, 1.79769E+308, 100.1234)\");\n+\t\t\tstatement.executeUpdate(\"INSERT INTO \" + INPUT_TABLE + \" VALUES (\" +\n+\t\t\t\t\t\"2, TIMESTAMP('2020-01-01 15:36:01.123456'), TIMESTAMP('2020-01-01 15:36:01.123456789'), \" +\n+\t\t\t\t\t\"TIME('15:36:01'), -1.175E-37, -1.79769E+308, 101.1234)\");\n+\t\t}\n+\t}\n+\n+\t@After\n+\tpublic void clearOutputTable() throws Exception {\n+\t\tClass.forName(DRIVER_CLASS);\n+\t\ttry (\n+\t\t\tConnection conn = DriverManager.getConnection(DB_URL);\n+\t\t\tStatement stat = conn.createStatement()) {\n+\t\t\tstat.execute(\"DROP TABLE \" + INPUT_TABLE);\n+\t\t}\n \t}\n \n \t@Test\n-\tpublic void testFieldsProjection() throws Exception {\n-\t\tStreamITCase.clear();\n+\tpublic void testJDBCSource() throws Exception {\n+\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+\t\tEnvironmentSettings envSettings = EnvironmentSettings.newInstance()\n+\t\t\t.useBlinkPlanner()\n+\t\t\t.inStreamingMode()\n+\t\t\t.build();\n+\t\tStreamTableEnvironment tEnv = StreamTableEnvironment.create(env, envSettings);\n+\n+\t\ttEnv.sqlUpdate(\n+\t\t\t\"CREATE TABLE \" + INPUT_TABLE + \"(\" +\n+\t\t\t\t\"id BIGINT,\" +\n+\t\t\t\t\"timestamp6_col TIMESTAMP(6),\" +\n+\t\t\t\t\"timestamp9_col TIMESTAMP(9),\" +\n+\t\t\t\t\"time_col TIME,\" +\n+\t\t\t\t\"real_col FLOAT,\" +\n+\t\t\t\t\"double_col DOUBLE,\" +\n+\t\t\t\t\"decimal_col DECIMAL(10, 4)\" +\n+\t\t\t\t\") WITH (\" +\n+\t\t\t\t\"  'connector.type'='jdbc',\" +\n+\t\t\t\t\"  'connector.url'='\" + DB_URL + \"',\" +\n+\t\t\t\t\"  'connector.table'='\" + INPUT_TABLE + \"'\" +\n+\t\t\t\t\")\"\n+\t\t);\n \n-\t\tTable result = tEnv.sqlQuery(SELECT_ID_BOOKS);\n-\t\tDataStream<Row> resultSet = tEnv.toAppendStream(result, Row.class);\n-\t\tresultSet.addSink(new StreamITCase.StringSink<>());\n+\t\tStreamITCase.clear();\n+\t\ttEnv.toAppendStream(tEnv.sqlQuery(\"SELECT * FROM \" + INPUT_TABLE), Row.class)\n+\t\t\t.addSink(new StreamITCase.StringSink<>());\n \t\tenv.execute();\n \n-\t\tList<String> expected = new ArrayList<>();\n-\t\texpected.add(\"1001\");\n-\t\texpected.add(\"1002\");\n-\t\texpected.add(\"1003\");\n-\t\texpected.add(\"1004\");\n-\t\texpected.add(\"1005\");\n-\t\texpected.add(\"1006\");\n-\t\texpected.add(\"1007\");\n-\t\texpected.add(\"1008\");\n-\t\texpected.add(\"1009\");\n-\t\texpected.add(\"1010\");\n-\n+\t\tList<String> expected =\n+\t\t\tArrays.asList(\n+\t\t\t\t\"1,2020-01-01T15:35:00.123456,2020-01-01T15:35:00.123456789,15:35,1.175E-37,1.79769E308,100.1234\",\n+\t\t\t\t\"2,2020-01-01T15:36:01.123456,2020-01-01T15:36:01.123456789,15:36:01,-1.175E-37,-1.79769E308,101.1234\");\n \t\tStreamITCase.compareWithList(expected);\n \t}\n \n \t@Test\n-\tpublic void testAllFieldsSelection() throws Exception {\n-\t\tStreamITCase.clear();\n+\tpublic void testProjectableJDBCSource() throws Exception {\n+\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+\t\tEnvironmentSettings envSettings = EnvironmentSettings.newInstance()\n+\t\t\t\t.useBlinkPlanner()\n+\t\t\t\t.inStreamingMode()\n+\t\t\t\t.build();\n+\t\tStreamTableEnvironment tEnv = StreamTableEnvironment.create(env, envSettings);\n+\n+\t\ttEnv.sqlUpdate(\n+\t\t\t\"CREATE TABLE \" + INPUT_TABLE + \"(\" +\n+\t\t\t\t\"id BIGINT,\" +\n+\t\t\t\t\"timestamp6_col TIMESTAMP(6),\" +\n+\t\t\t\t\"timestamp9_col TIMESTAMP(9),\" +\n+\t\t\t\t\"time_col TIME,\" +\n+\t\t\t\t\"real_col FLOAT,\" +\n+\t\t\t\t\"decimal_col DECIMAL(10, 4)\" +\n+\t\t\t\t\") WITH (\" +\n+\t\t\t\t\"  'connector.type'='jdbc',\" +\n+\t\t\t\t\"  'connector.url'='\" + DB_URL + \"',\" +\n+\t\t\t\t\"  'connector.table'='\" + INPUT_TABLE + \"'\" +\n+\t\t\t\t\")\"\n+\t\t);\n \n-\t\tTable result = tEnv.sqlQuery(SELECT_ALL_BOOKS);\n-\t\tDataStream<Row> resultSet = tEnv.toAppendStream(result, Row.class);\n-\t\tresultSet.addSink(new StreamITCase.StringSink<>());\n+\t\tStreamITCase.clear();\n+\t\ttEnv.toAppendStream(tEnv.sqlQuery(\"SELECT timestamp6_col, decimal_col FROM \" + INPUT_TABLE), Row.class)\n+\t\t\t\t.addSink(new StreamITCase.StringSink<>());\n \t\tenv.execute();\n \n-\t\tList<String> expected = new ArrayList<>();\n-\t\texpected.add(\"1001,Java public for dummies,Tan Ah Teck,11.11,11\");\n-\t\texpected.add(\"1002,More Java for dummies,Tan Ah Teck,22.22,22\");\n-\t\texpected.add(\"1003,More Java for more dummies,Mohammad Ali,33.33,33\");\n-\t\texpected.add(\"1004,A Cup of Java,Kumar,44.44,44\");\n-\t\texpected.add(\"1005,A Teaspoon of Java,Kevin Jones,55.55,55\");\n-\t\texpected.add(\"1006,A Teaspoon of Java 1.4,Kevin Jones,66.66,66\");\n-\t\texpected.add(\"1007,A Teaspoon of Java 1.5,Kevin Jones,77.77,77\");\n-\t\texpected.add(\"1008,A Teaspoon of Java 1.6,Kevin Jones,88.88,88\");\n-\t\texpected.add(\"1009,A Teaspoon of Java 1.7,Kevin Jones,99.99,99\");\n-\t\texpected.add(\"1010,A Teaspoon of Java 1.8,Kevin Jones,null,1010\");\n-\n+\t\tList<String> expected =\n+\t\t\tArrays.asList(\n+\t\t\t\t\"2020-01-01T15:35:00.123456,100.1234\",\n+\t\t\t\t\"2020-01-01T15:36:01.123456,101.1234\");\n \t\tStreamITCase.compareWithList(expected);\n \t}\n \n+\t@Test(expected = TableException.class)\n+\tpublic void testInvalidPrecisionOfJDBCSource() throws Exception {\n+\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+\t\tEnvironmentSettings envSettings = EnvironmentSettings.newInstance()\n+\t\t\t\t.useBlinkPlanner()\n+\t\t\t\t.inStreamingMode()\n+\t\t\t\t.build();\n+\t\tStreamTableEnvironment tEnv = StreamTableEnvironment.create(env, envSettings);\n+\n+\t\ttEnv.sqlUpdate(\n+\t\t\"CREATE TABLE \" + INPUT_TABLE + \"(\" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f7456ac7e14681e18199941e0962b207eb99bef9"}, "originalPosition": 212}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a12d3d5cee04047c93c01119ad07332a23b7deb0", "author": {"user": {"login": "docete", "name": "Zhenghua Gao"}}, "url": "https://github.com/apache/flink/commit/a12d3d5cee04047c93c01119ad07332a23b7deb0", "committedDate": "2020-02-10T06:48:07Z", "message": "[FLINK-15445][connectors/jdbc] Refactor the validation of data types for dialects"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6336abbade5c71df3bd26fad40209e89642810d8", "author": {"user": {"login": "docete", "name": "Zhenghua Gao"}}, "url": "https://github.com/apache/flink/commit/6336abbade5c71df3bd26fad40209e89642810d8", "committedDate": "2020-02-10T07:23:18Z", "message": "fixup: address Jark&benchao's comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "330adf7ac3582db6b47a093e6f342351a3ba7e43", "author": {"user": {"login": "docete", "name": "Zhenghua Gao"}}, "url": "https://github.com/apache/flink/commit/330adf7ac3582db6b47a093e6f342351a3ba7e43", "committedDate": "2020-02-10T08:50:35Z", "message": "fixup: checkstyle"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU2NDEwNDIx", "url": "https://github.com/apache/flink/pull/10745#pullrequestreview-356410421", "createdAt": "2020-02-11T04:13:55Z", "commit": {"oid": "330adf7ac3582db6b47a093e6f342351a3ba7e43"}, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQwNDoxMzo1NlrOFn9HMw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQwNToyNTo1NVrOFn9wKg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ0MDA1MQ==", "bodyText": "Could we just simply dt.getLogicalType() instanceof VarBinaryType  to match it is a VarBinaryType? I think currently there isn't a LegacyTypeInformationType which is VARBINARY.  The same to the below if branches.", "url": "https://github.com/apache/flink/pull/10745#discussion_r377440051", "createdAt": "2020-02-11T04:13:56Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialects.java", "diffHunk": "@@ -46,10 +59,84 @@\n \t\treturn Optional.empty();\n \t}\n \n-\tprivate static class DerbyDialect implements JDBCDialect {\n+\tprivate abstract static class AbstractDialect implements JDBCDialect {\n+\n+\t\t@Override\n+\t\tpublic void validate(TableSchema schema) throws ValidationException {\n+\t\t\tfor (int i = 0; i < schema.getFieldCount(); i++) {\n+\t\t\t\tDataType dt = schema.getFieldDataType(i).get();\n+\t\t\t\tString fieldName = schema.getFieldName(i).get();\n+\n+\t\t\t\t// TODO: We can't convert VARBINARY(n) data type to\n+\t\t\t\t//  PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO in LegacyTypeInfoDataTypeConverter\n+\t\t\t\t//  when n is smaller than Integer.MAX_VALUE\n+\t\t\t\tif (unsupportedTypes().contains(dt.getLogicalType().getTypeRoot()) ||\n+\t\t\t\t\t\t(!(dt.getLogicalType() instanceof LegacyTypeInformationType) &&\n+\t\t\t\t\t\t(VARBINARY == dt.getLogicalType().getTypeRoot()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "330adf7ac3582db6b47a093e6f342351a3ba7e43"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ0MDYzMg==", "bodyText": "Improve the error message a bit more:\nString.format(\"The precision of filed '%s' is out of the TIMESTAMP precision range [%d, %d] supported by the %s dialect.\",\nfieldName,\nminTimestampPrecision(),\nmaxTimestampPrecision(),\ndialectName);", "url": "https://github.com/apache/flink/pull/10745#discussion_r377440632", "createdAt": "2020-02-11T04:18:24Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialects.java", "diffHunk": "@@ -46,10 +59,84 @@\n \t\treturn Optional.empty();\n \t}\n \n-\tprivate static class DerbyDialect implements JDBCDialect {\n+\tprivate abstract static class AbstractDialect implements JDBCDialect {\n+\n+\t\t@Override\n+\t\tpublic void validate(TableSchema schema) throws ValidationException {\n+\t\t\tfor (int i = 0; i < schema.getFieldCount(); i++) {\n+\t\t\t\tDataType dt = schema.getFieldDataType(i).get();\n+\t\t\t\tString fieldName = schema.getFieldName(i).get();\n+\n+\t\t\t\t// TODO: We can't convert VARBINARY(n) data type to\n+\t\t\t\t//  PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO in LegacyTypeInfoDataTypeConverter\n+\t\t\t\t//  when n is smaller than Integer.MAX_VALUE\n+\t\t\t\tif (unsupportedTypes().contains(dt.getLogicalType().getTypeRoot()) ||\n+\t\t\t\t\t\t(!(dt.getLogicalType() instanceof LegacyTypeInformationType) &&\n+\t\t\t\t\t\t(VARBINARY == dt.getLogicalType().getTypeRoot()\n+\t\t\t\t\t\t\t&& Integer.MAX_VALUE != ((VarBinaryType) dt.getLogicalType()).getLength()))) {\n+\t\t\t\t\tthrow new ValidationException(\n+\t\t\t\t\t\t\tString.format(\"The dialect don't support type: %s.\", dt.toString()));\n+\t\t\t\t}\n+\n+\t\t\t\t// only validate precision of DECIMAL type for blink planner\n+\t\t\t\tif (!(dt.getLogicalType() instanceof LegacyTypeInformationType)\n+\t\t\t\t\t\t&& DECIMAL == dt.getLogicalType().getTypeRoot()) {\n+\t\t\t\t\tint precision = ((DecimalType) dt.getLogicalType()).getPrecision();\n+\t\t\t\t\tif (precision > maxDecimalPrecision()\n+\t\t\t\t\t\t\t|| precision < minDecimalPrecision()) {\n+\t\t\t\t\t\tthrow new ValidationException(\n+\t\t\t\t\t\t\t\tString.format(\"The precision of %s is out of the range [%d, %d].\",\n+\t\t\t\t\t\t\t\t\t\tfieldName,\n+\t\t\t\t\t\t\t\t\t\tminDecimalPrecision(),\n+\t\t\t\t\t\t\t\t\t\tmaxDecimalPrecision()));\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\n+\t\t\t\t// only validate precision of DECIMAL type for blink planner\n+\t\t\t\tif (!(dt.getLogicalType() instanceof LegacyTypeInformationType)\n+\t\t\t\t\t\t&& TIMESTAMP_WITHOUT_TIME_ZONE == dt.getLogicalType().getTypeRoot()) {\n+\t\t\t\t\tint precision = ((TimestampType) dt.getLogicalType()).getPrecision();\n+\t\t\t\t\tif (precision > maxTimestampPrecision()\n+\t\t\t\t\t\t\t|| precision < minTimestampPrecision()) {\n+\t\t\t\t\t\tthrow new ValidationException(\n+\t\t\t\t\t\t\t\tString.format(\"The precision of %s is out of the range [%d, %d].\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "330adf7ac3582db6b47a093e6f342351a3ba7e43"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ0MDcwOA==", "bodyText": "The same to the DECIMAL type.", "url": "https://github.com/apache/flink/pull/10745#discussion_r377440708", "createdAt": "2020-02-11T04:19:01Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialects.java", "diffHunk": "@@ -46,10 +59,84 @@\n \t\treturn Optional.empty();\n \t}\n \n-\tprivate static class DerbyDialect implements JDBCDialect {\n+\tprivate abstract static class AbstractDialect implements JDBCDialect {\n+\n+\t\t@Override\n+\t\tpublic void validate(TableSchema schema) throws ValidationException {\n+\t\t\tfor (int i = 0; i < schema.getFieldCount(); i++) {\n+\t\t\t\tDataType dt = schema.getFieldDataType(i).get();\n+\t\t\t\tString fieldName = schema.getFieldName(i).get();\n+\n+\t\t\t\t// TODO: We can't convert VARBINARY(n) data type to\n+\t\t\t\t//  PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO in LegacyTypeInfoDataTypeConverter\n+\t\t\t\t//  when n is smaller than Integer.MAX_VALUE\n+\t\t\t\tif (unsupportedTypes().contains(dt.getLogicalType().getTypeRoot()) ||\n+\t\t\t\t\t\t(!(dt.getLogicalType() instanceof LegacyTypeInformationType) &&\n+\t\t\t\t\t\t(VARBINARY == dt.getLogicalType().getTypeRoot()\n+\t\t\t\t\t\t\t&& Integer.MAX_VALUE != ((VarBinaryType) dt.getLogicalType()).getLength()))) {\n+\t\t\t\t\tthrow new ValidationException(\n+\t\t\t\t\t\t\tString.format(\"The dialect don't support type: %s.\", dt.toString()));\n+\t\t\t\t}\n+\n+\t\t\t\t// only validate precision of DECIMAL type for blink planner\n+\t\t\t\tif (!(dt.getLogicalType() instanceof LegacyTypeInformationType)\n+\t\t\t\t\t\t&& DECIMAL == dt.getLogicalType().getTypeRoot()) {\n+\t\t\t\t\tint precision = ((DecimalType) dt.getLogicalType()).getPrecision();\n+\t\t\t\t\tif (precision > maxDecimalPrecision()\n+\t\t\t\t\t\t\t|| precision < minDecimalPrecision()) {\n+\t\t\t\t\t\tthrow new ValidationException(\n+\t\t\t\t\t\t\t\tString.format(\"The precision of %s is out of the range [%d, %d].\",\n+\t\t\t\t\t\t\t\t\t\tfieldName,\n+\t\t\t\t\t\t\t\t\t\tminDecimalPrecision(),\n+\t\t\t\t\t\t\t\t\t\tmaxDecimalPrecision()));\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\n+\t\t\t\t// only validate precision of DECIMAL type for blink planner\n+\t\t\t\tif (!(dt.getLogicalType() instanceof LegacyTypeInformationType)\n+\t\t\t\t\t\t&& TIMESTAMP_WITHOUT_TIME_ZONE == dt.getLogicalType().getTypeRoot()) {\n+\t\t\t\t\tint precision = ((TimestampType) dt.getLogicalType()).getPrecision();\n+\t\t\t\t\tif (precision > maxTimestampPrecision()\n+\t\t\t\t\t\t\t|| precision < minTimestampPrecision()) {\n+\t\t\t\t\t\tthrow new ValidationException(\n+\t\t\t\t\t\t\t\tString.format(\"The precision of %s is out of the range [%d, %d].\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ0MDYzMg=="}, "originalCommit": {"oid": "330adf7ac3582db6b47a093e6f342351a3ba7e43"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ1MDQzNw==", "bodyText": "I think we don't need this test any more, because is already covered by JDBCDataTypeTest.", "url": "https://github.com/apache/flink/pull/10745#discussion_r377450437", "createdAt": "2020-02-11T05:25:19Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceITCase.java", "diffHunk": "@@ -18,93 +18,167 @@\n \n package org.apache.flink.api.java.io.jdbc;\n \n-import org.apache.flink.streaming.api.datastream.DataStream;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n import org.apache.flink.table.api.EnvironmentSettings;\n-import org.apache.flink.table.api.Table;\n+import org.apache.flink.table.api.TableException;\n import org.apache.flink.table.api.java.StreamTableEnvironment;\n import org.apache.flink.table.runtime.utils.StreamITCase;\n+import org.apache.flink.test.util.AbstractTestBase;\n import org.apache.flink.types.Row;\n \n-import org.junit.BeforeClass;\n+import org.junit.After;\n+import org.junit.Before;\n import org.junit.Test;\n \n-import java.util.ArrayList;\n+import java.sql.Connection;\n+import java.sql.DriverManager;\n+import java.sql.SQLException;\n+import java.sql.Statement;\n+import java.util.Arrays;\n import java.util.List;\n \n+\n /**\n- * IT case for {@link JDBCTableSource}.\n+ * ITCase for {@link JDBCTableSource}.\n  */\n-public class JDBCTableSourceITCase extends JDBCTestBase {\n-\n-\tprivate static final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n-\tprivate static final EnvironmentSettings bsSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();\n-\tprivate static final StreamTableEnvironment tEnv = StreamTableEnvironment.create(env, bsSettings);\n-\n-\tstatic final String TABLE_SOURCE_SQL = \"CREATE TABLE books (\" +\n-\t\t\" id int, \" +\n-\t\t\" title varchar, \" +\n-\t\t\" author varchar, \" +\n-\t\t\" price double, \" +\n-\t\t\" qty int \" +\n-\t\t\") with (\" +\n-\t\t\" 'connector.type' = 'jdbc', \" +\n-\t\t\" 'connector.url' = 'jdbc:derby:memory:ebookshop', \" +\n-\t\t\" 'connector.table' = 'books', \" +\n-\t\t\" 'connector.driver' = 'org.apache.derby.jdbc.EmbeddedDriver' \" +\n-\t\t\")\";\n-\n-\t@BeforeClass\n-\tpublic static void createTable() {\n-\t\ttEnv.sqlUpdate(TABLE_SOURCE_SQL);\n+public class JDBCTableSourceITCase extends AbstractTestBase {\n+\n+\tpublic static final String DRIVER_CLASS = \"org.apache.derby.jdbc.EmbeddedDriver\";\n+\tpublic static final String DB_URL = \"jdbc:derby:memory:test\";\n+\tpublic static final String INPUT_TABLE = \"jdbcSource\";\n+\n+\t@Before\n+\tpublic void before() throws ClassNotFoundException, SQLException {\n+\t\tSystem.setProperty(\"derby.stream.error.field\", JDBCTestBase.class.getCanonicalName() + \".DEV_NULL\");\n+\t\tClass.forName(DRIVER_CLASS);\n+\n+\t\ttry (\n+\t\t\tConnection conn = DriverManager.getConnection(DB_URL + \";create=true\");\n+\t\t\tStatement statement = conn.createStatement()) {\n+\t\t\tstatement.executeUpdate(\"CREATE TABLE \" + INPUT_TABLE + \" (\" +\n+\t\t\t\t\t\"id BIGINT NOT NULL,\" +\n+\t\t\t\t\t\"timestamp6_col TIMESTAMP, \" +\n+\t\t\t\t\t\"timestamp9_col TIMESTAMP, \" +\n+\t\t\t\t\t\"time_col TIME, \" +\n+\t\t\t\t\t\"real_col FLOAT(23), \" +    // A precision of 23 or less makes FLOAT equivalent to REAL.\n+\t\t\t\t\t\"double_col FLOAT(24),\" +   // A precision of 24 or greater makes FLOAT equivalent to DOUBLE PRECISION.\n+\t\t\t\t\t\"decimal_col DECIMAL(10, 4))\");\n+\t\t\tstatement.executeUpdate(\"INSERT INTO \" + INPUT_TABLE + \" VALUES (\" +\n+\t\t\t\t\t\"1, TIMESTAMP('2020-01-01 15:35:00.123456'), TIMESTAMP('2020-01-01 15:35:00.123456789'), \" +\n+\t\t\t\t\t\"TIME('15:35:00'), 1.175E-37, 1.79769E+308, 100.1234)\");\n+\t\t\tstatement.executeUpdate(\"INSERT INTO \" + INPUT_TABLE + \" VALUES (\" +\n+\t\t\t\t\t\"2, TIMESTAMP('2020-01-01 15:36:01.123456'), TIMESTAMP('2020-01-01 15:36:01.123456789'), \" +\n+\t\t\t\t\t\"TIME('15:36:01'), -1.175E-37, -1.79769E+308, 101.1234)\");\n+\t\t}\n+\t}\n+\n+\t@After\n+\tpublic void clearOutputTable() throws Exception {\n+\t\tClass.forName(DRIVER_CLASS);\n+\t\ttry (\n+\t\t\tConnection conn = DriverManager.getConnection(DB_URL);\n+\t\t\tStatement stat = conn.createStatement()) {\n+\t\t\tstat.execute(\"DROP TABLE \" + INPUT_TABLE);\n+\t\t}\n \t}\n \n \t@Test\n-\tpublic void testFieldsProjection() throws Exception {\n-\t\tStreamITCase.clear();\n+\tpublic void testJDBCSource() throws Exception {\n+\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+\t\tEnvironmentSettings envSettings = EnvironmentSettings.newInstance()\n+\t\t\t.useBlinkPlanner()\n+\t\t\t.inStreamingMode()\n+\t\t\t.build();\n+\t\tStreamTableEnvironment tEnv = StreamTableEnvironment.create(env, envSettings);\n+\n+\t\ttEnv.sqlUpdate(\n+\t\t\t\"CREATE TABLE \" + INPUT_TABLE + \"(\" +\n+\t\t\t\t\"id BIGINT,\" +\n+\t\t\t\t\"timestamp6_col TIMESTAMP(6),\" +\n+\t\t\t\t\"timestamp9_col TIMESTAMP(9),\" +\n+\t\t\t\t\"time_col TIME,\" +\n+\t\t\t\t\"real_col FLOAT,\" +\n+\t\t\t\t\"double_col DOUBLE,\" +\n+\t\t\t\t\"decimal_col DECIMAL(10, 4)\" +\n+\t\t\t\t\") WITH (\" +\n+\t\t\t\t\"  'connector.type'='jdbc',\" +\n+\t\t\t\t\"  'connector.url'='\" + DB_URL + \"',\" +\n+\t\t\t\t\"  'connector.table'='\" + INPUT_TABLE + \"'\" +\n+\t\t\t\t\")\"\n+\t\t);\n \n-\t\tTable result = tEnv.sqlQuery(SELECT_ID_BOOKS);\n-\t\tDataStream<Row> resultSet = tEnv.toAppendStream(result, Row.class);\n-\t\tresultSet.addSink(new StreamITCase.StringSink<>());\n+\t\tStreamITCase.clear();\n+\t\ttEnv.toAppendStream(tEnv.sqlQuery(\"SELECT * FROM \" + INPUT_TABLE), Row.class)\n+\t\t\t.addSink(new StreamITCase.StringSink<>());\n \t\tenv.execute();\n \n-\t\tList<String> expected = new ArrayList<>();\n-\t\texpected.add(\"1001\");\n-\t\texpected.add(\"1002\");\n-\t\texpected.add(\"1003\");\n-\t\texpected.add(\"1004\");\n-\t\texpected.add(\"1005\");\n-\t\texpected.add(\"1006\");\n-\t\texpected.add(\"1007\");\n-\t\texpected.add(\"1008\");\n-\t\texpected.add(\"1009\");\n-\t\texpected.add(\"1010\");\n-\n+\t\tList<String> expected =\n+\t\t\tArrays.asList(\n+\t\t\t\t\"1,2020-01-01T15:35:00.123456,2020-01-01T15:35:00.123456789,15:35,1.175E-37,1.79769E308,100.1234\",\n+\t\t\t\t\"2,2020-01-01T15:36:01.123456,2020-01-01T15:36:01.123456789,15:36:01,-1.175E-37,-1.79769E308,101.1234\");\n \t\tStreamITCase.compareWithList(expected);\n \t}\n \n \t@Test\n-\tpublic void testAllFieldsSelection() throws Exception {\n-\t\tStreamITCase.clear();\n+\tpublic void testProjectableJDBCSource() throws Exception {\n+\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+\t\tEnvironmentSettings envSettings = EnvironmentSettings.newInstance()\n+\t\t\t\t.useBlinkPlanner()\n+\t\t\t\t.inStreamingMode()\n+\t\t\t\t.build();\n+\t\tStreamTableEnvironment tEnv = StreamTableEnvironment.create(env, envSettings);\n+\n+\t\ttEnv.sqlUpdate(\n+\t\t\t\"CREATE TABLE \" + INPUT_TABLE + \"(\" +\n+\t\t\t\t\"id BIGINT,\" +\n+\t\t\t\t\"timestamp6_col TIMESTAMP(6),\" +\n+\t\t\t\t\"timestamp9_col TIMESTAMP(9),\" +\n+\t\t\t\t\"time_col TIME,\" +\n+\t\t\t\t\"real_col FLOAT,\" +\n+\t\t\t\t\"decimal_col DECIMAL(10, 4)\" +\n+\t\t\t\t\") WITH (\" +\n+\t\t\t\t\"  'connector.type'='jdbc',\" +\n+\t\t\t\t\"  'connector.url'='\" + DB_URL + \"',\" +\n+\t\t\t\t\"  'connector.table'='\" + INPUT_TABLE + \"'\" +\n+\t\t\t\t\")\"\n+\t\t);\n \n-\t\tTable result = tEnv.sqlQuery(SELECT_ALL_BOOKS);\n-\t\tDataStream<Row> resultSet = tEnv.toAppendStream(result, Row.class);\n-\t\tresultSet.addSink(new StreamITCase.StringSink<>());\n+\t\tStreamITCase.clear();\n+\t\ttEnv.toAppendStream(tEnv.sqlQuery(\"SELECT timestamp6_col, decimal_col FROM \" + INPUT_TABLE), Row.class)\n+\t\t\t\t.addSink(new StreamITCase.StringSink<>());\n \t\tenv.execute();\n \n-\t\tList<String> expected = new ArrayList<>();\n-\t\texpected.add(\"1001,Java public for dummies,Tan Ah Teck,11.11,11\");\n-\t\texpected.add(\"1002,More Java for dummies,Tan Ah Teck,22.22,22\");\n-\t\texpected.add(\"1003,More Java for more dummies,Mohammad Ali,33.33,33\");\n-\t\texpected.add(\"1004,A Cup of Java,Kumar,44.44,44\");\n-\t\texpected.add(\"1005,A Teaspoon of Java,Kevin Jones,55.55,55\");\n-\t\texpected.add(\"1006,A Teaspoon of Java 1.4,Kevin Jones,66.66,66\");\n-\t\texpected.add(\"1007,A Teaspoon of Java 1.5,Kevin Jones,77.77,77\");\n-\t\texpected.add(\"1008,A Teaspoon of Java 1.6,Kevin Jones,88.88,88\");\n-\t\texpected.add(\"1009,A Teaspoon of Java 1.7,Kevin Jones,99.99,99\");\n-\t\texpected.add(\"1010,A Teaspoon of Java 1.8,Kevin Jones,null,1010\");\n-\n+\t\tList<String> expected =\n+\t\t\tArrays.asList(\n+\t\t\t\t\"2020-01-01T15:35:00.123456,100.1234\",\n+\t\t\t\t\"2020-01-01T15:36:01.123456,101.1234\");\n \t\tStreamITCase.compareWithList(expected);\n \t}\n \n+\t@Test(expected = TableException.class)\n+\tpublic void testInvalidPrecisionOfJDBCSource() throws Exception {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjE5ODM0NQ=="}, "originalCommit": {"oid": "f7456ac7e14681e18199941e0962b207eb99bef9"}, "originalPosition": 203}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ1MDUzOA==", "bodyText": "String.format(\"The %s dialect doesn't support type: %s.\", dialectName, dt.toString())", "url": "https://github.com/apache/flink/pull/10745#discussion_r377450538", "createdAt": "2020-02-11T05:25:55Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialects.java", "diffHunk": "@@ -46,10 +59,84 @@\n \t\treturn Optional.empty();\n \t}\n \n-\tprivate static class DerbyDialect implements JDBCDialect {\n+\tprivate abstract static class AbstractDialect implements JDBCDialect {\n+\n+\t\t@Override\n+\t\tpublic void validate(TableSchema schema) throws ValidationException {\n+\t\t\tfor (int i = 0; i < schema.getFieldCount(); i++) {\n+\t\t\t\tDataType dt = schema.getFieldDataType(i).get();\n+\t\t\t\tString fieldName = schema.getFieldName(i).get();\n+\n+\t\t\t\t// TODO: We can't convert VARBINARY(n) data type to\n+\t\t\t\t//  PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO in LegacyTypeInfoDataTypeConverter\n+\t\t\t\t//  when n is smaller than Integer.MAX_VALUE\n+\t\t\t\tif (unsupportedTypes().contains(dt.getLogicalType().getTypeRoot()) ||\n+\t\t\t\t\t\t(!(dt.getLogicalType() instanceof LegacyTypeInformationType) &&\n+\t\t\t\t\t\t(VARBINARY == dt.getLogicalType().getTypeRoot()\n+\t\t\t\t\t\t\t&& Integer.MAX_VALUE != ((VarBinaryType) dt.getLogicalType()).getLength()))) {\n+\t\t\t\t\tthrow new ValidationException(\n+\t\t\t\t\t\t\tString.format(\"The dialect don't support type: %s.\", dt.toString()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "330adf7ac3582db6b47a093e6f342351a3ba7e43"}, "originalPosition": 46}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5e538c4875e6241d494585c6e2a8f586ac078131", "author": {"user": {"login": "docete", "name": "Zhenghua Gao"}}, "url": "https://github.com/apache/flink/commit/5e538c4875e6241d494585c6e2a8f586ac078131", "committedDate": "2020-02-11T12:30:59Z", "message": "fixup: address jark's comments about improving error msg"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU2NjM2MzQ5", "url": "https://github.com/apache/flink/pull/10745#pullrequestreview-356636349", "createdAt": "2020-02-11T13:01:23Z", "commit": {"oid": "5e538c4875e6241d494585c6e2a8f586ac078131"}, "state": "APPROVED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQxMzowMToyM1rOFoIC0g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQxMzowMToyM1rOFoIC0g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzYxOTE1NA==", "bodyText": "Nit: don't -> doesn't", "url": "https://github.com/apache/flink/pull/10745#discussion_r377619154", "createdAt": "2020-02-11T13:01:23Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialects.java", "diffHunk": "@@ -71,43 +71,48 @@ public void validate(TableSchema schema) throws ValidationException {\n \t\t\t\t//  PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO in LegacyTypeInfoDataTypeConverter\n \t\t\t\t//  when n is smaller than Integer.MAX_VALUE\n \t\t\t\tif (unsupportedTypes().contains(dt.getLogicalType().getTypeRoot()) ||\n-\t\t\t\t\t\t(!(dt.getLogicalType() instanceof LegacyTypeInformationType) &&\n-\t\t\t\t\t\t(VARBINARY == dt.getLogicalType().getTypeRoot()\n-\t\t\t\t\t\t\t&& Integer.MAX_VALUE != ((VarBinaryType) dt.getLogicalType()).getLength()))) {\n+\t\t\t\t\t\t(dt.getLogicalType() instanceof VarBinaryType\n+\t\t\t\t\t\t\t&& Integer.MAX_VALUE != ((VarBinaryType) dt.getLogicalType()).getLength())) {\n \t\t\t\t\tthrow new ValidationException(\n-\t\t\t\t\t\t\tString.format(\"The dialect don't support type: %s.\", dt.toString()));\n+\t\t\t\t\t\t\tString.format(\"The %s dialect don't support type: %s.\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e538c4875e6241d494585c6e2a8f586ac078131"}, "originalPosition": 11}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2f27266467ee029955ac26e1acc0e64628d8f2dd", "author": {"user": {"login": "docete", "name": "Zhenghua Gao"}}, "url": "https://github.com/apache/flink/commit/2f27266467ee029955ac26e1acc0e64628d8f2dd", "committedDate": "2020-02-12T02:54:08Z", "message": "fixup: checkstyle"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5f30317b81fbc0dcf2d518adb8a0ddde1847bfe6", "author": {"user": {"login": "docete", "name": "Zhenghua Gao"}}, "url": "https://github.com/apache/flink/commit/5f30317b81fbc0dcf2d518adb8a0ddde1847bfe6", "committedDate": "2020-02-12T03:14:58Z", "message": "fixup: Nit don't -> doesn't"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7790797e5e90af02bc94b48f5326077573600f25", "author": {"user": {"login": "docete", "name": "Zhenghua Gao"}}, "url": "https://github.com/apache/flink/commit/7790797e5e90af02bc94b48f5326077573600f25", "committedDate": "2020-02-12T03:17:38Z", "message": "fixup: checkstyle"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4892, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}