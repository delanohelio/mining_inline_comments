{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTAyMjM5MzIy", "number": 13614, "title": "[FLINK-19547][runtime] Clean up partial record when reconnecting for Approximate Local Recovery ", "bodyText": "What is the purpose of the change\nPartial records happen if a record can not fit into one buffer, then the remaining part of the same record is put into the next buffer. Hence partial records only exist at the beginning of a buffer. Partial record clean-up is needed in the mode of approximate local recovery. If a record is spanning over multiple buffers, and the first (several) buffers have got lost due to the failure of the receiver task, the remaining data belonging to the same record in transition should be cleaned up.\npartialRecordLength is the length of bytes to skip in order to start with a complete record, from position index 0 of the underlying MemorySegment. partialRecordLength is used in approximate local recovery to find the start position of a complete record on a BufferConsumer, so-called partial record clean-up.\nBrief change log\n\nAPI change to add partialRecordLength when creating a BufferConsumer\nAdd partialRecordLength in BufferWritingResultPartition when emitRecord and broadcastRecord\nUpdate the type of buffers in PipelinedSubpartition frPrioritizedDeque<BufferConsumer> to PrioritizedDeque<BufferConsumerWithPartialRecordLength>\nImplement partial record clean-up logic in BufferConsumerWithPartialRecordLength\n\nVerifying this change\n\nadd unit tests to test partial record length\npass existing tests to make sure the changes do not affect data write/read/transition.\n\nDoes this pull request potentially affect one of the following parts:\n\nDependencies (does it add or upgrade a dependency): (no)\nThe public API, i.e., is any changed class annotated with @Public(Evolving): (no)\nThe serializers: (no)\nThe runtime per-record code paths (performance sensitive): (yes)\nAnything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (no)\nThe S3 file system connector: (no)\n\nDocumentation\n\nDoes this pull request introduce a new feature? (no)", "createdAt": "2020-10-13T12:01:59Z", "url": "https://github.com/apache/flink/pull/13614", "merged": true, "mergeCommit": {"oid": "698f9624d3acc1f32240cc15329b542126a73451"}, "closed": true, "closedAt": "2020-10-27T08:01:18Z", "author": {"login": "curcur"}, "timelineItems": {"totalCount": 16, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdSHlumgBqjM4NzA5NzMxMDI=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdWgD1PgBqjM5MjM2OTgxNzU=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "0f671afd0bdf2d4f6243b44346560446dc170385", "author": {"user": {"login": "curcur", "name": "Yuan Mei"}}, "url": "https://github.com/apache/flink/commit/0f671afd0bdf2d4f6243b44346560446dc170385", "committedDate": "2020-10-13T11:49:28Z", "message": "[FLINK-19547] Update `BufferConsumer` with `BufferConsumerWithPartialRecordLength` in PipelinedSubpartition\n\nBufferConsumerWithPartialRecordLength includes BufferConsumer as well as partialRecordLength\n\nThis commit includes two changes\n1. Update BufferConsumer with BufferConsumerWithPartialRecordLength in PipelinedSubpartition\n2. Implement partial record cleanup logic in BufferConsumer"}, "afterCommit": {"oid": "f51a6098947a0967f0dadc32903bf1288fb48941", "author": {"user": {"login": "curcur", "name": "Yuan Mei"}}, "url": "https://github.com/apache/flink/commit/f51a6098947a0967f0dadc32903bf1288fb48941", "committedDate": "2020-10-13T12:17:31Z", "message": "[FLINK-19547] Update `BufferConsumer` with `BufferConsumerWithPartialRecordLength` in PipelinedSubpartition.java\n\nBufferConsumerWithPartialRecordLength includes BufferConsumer as well as partialRecordLength\n\nThis commit includes two changes\n1. Update BufferConsumer with BufferConsumerWithPartialRecordLength in PipelinedSubpartition\n2. Implement partial record cleanup logic in BufferConsumer"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "f51a6098947a0967f0dadc32903bf1288fb48941", "author": {"user": {"login": "curcur", "name": "Yuan Mei"}}, "url": "https://github.com/apache/flink/commit/f51a6098947a0967f0dadc32903bf1288fb48941", "committedDate": "2020-10-13T12:17:31Z", "message": "[FLINK-19547] Update `BufferConsumer` with `BufferConsumerWithPartialRecordLength` in PipelinedSubpartition.java\n\nBufferConsumerWithPartialRecordLength includes BufferConsumer as well as partialRecordLength\n\nThis commit includes two changes\n1. Update BufferConsumer with BufferConsumerWithPartialRecordLength in PipelinedSubpartition\n2. Implement partial record cleanup logic in BufferConsumer"}, "afterCommit": {"oid": "47d3d60c3dc74ab402ce657c31c27568b190de40", "author": {"user": {"login": "curcur", "name": "Yuan Mei"}}, "url": "https://github.com/apache/flink/commit/47d3d60c3dc74ab402ce657c31c27568b190de40", "committedDate": "2020-10-13T12:31:22Z", "message": "[FLINK-19547] Update `BufferConsumer` with `BufferConsumerWithPartialRecordLength` in PipelinedSubpartition.java\n\nBufferConsumerWithPartialRecordLength includes BufferConsumer as well as partialRecordLength\n\nThis commit includes two changes\n1. Update BufferConsumer with BufferConsumerWithPartialRecordLength in PipelinedSubpartition\n2. Implement partial record cleanup logic in BufferConsumer"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA4MjU0ODYw", "url": "https://github.com/apache/flink/pull/13614#pullrequestreview-508254860", "createdAt": "2020-10-14T11:37:33Z", "commit": {"oid": "47d3d60c3dc74ab402ce657c31c27568b190de40"}, "state": "COMMENTED", "comments": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQxMTozNzozM1rOHhOwZg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQxMjo1ODoyNFrOHhRs3w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDYwNjgyMg==", "bodyText": "How could it be that bytesToSkip > buffer.getMaxCapacity() if we already asserted bytesToSkip <= bytesReadable? If that's impossible, we could replace this if with just\nslice = buffer.readOnlySlice(currentReaderPosition + bytesToSkip, bytesReadable - bytesToSkip);\n\nas that would also return an empty buffer if bytesToSkip == bytesReadable?", "url": "https://github.com/apache/flink/pull/13614#discussion_r504606822", "createdAt": "2020-10-14T11:37:33Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/buffer/BufferConsumer.java", "diffHunk": "@@ -108,6 +109,30 @@ public Buffer build() {\n \t\treturn slice.retainBuffer();\n \t}\n \n+\t/**\n+\t * @param bytesToSkip number of bytes to skip from currentReaderPosition\n+\t * @return sliced {@link Buffer} containing the not yet consumed data, Returned {@link Buffer} shares the reference\n+\t * counter with the parent {@link BufferConsumer} - in order to recycle memory both of them must be recycled/closed.\n+\t */\n+\tBuffer skipBuild(int bytesToSkip) {\n+\t\twriterPosition.update();\n+\t\tint cachedWriterPosition = writerPosition.getCached();\n+\t\tBuffer slice;\n+\n+\t\tint bytesReadable = cachedWriterPosition - currentReaderPosition;\n+\t\tcheckState(bytesToSkip <= bytesReadable, \"bytes to skip beyond readable range\");\n+\n+\t\tif (bytesToSkip < buffer.getMaxCapacity()) {\n+\t\t\tslice = buffer.readOnlySlice(currentReaderPosition + bytesToSkip, bytesReadable - bytesToSkip);\n+\t\t} else {\n+\t\t\t// return an empty buffer if beyond buffer max capacity\n+\t\t\tslice = buffer.readOnlySlice(currentReaderPosition, 0);\n+\t\t}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "47d3d60c3dc74ab402ce657c31c27568b190de40"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDYyMjQyNQ==", "bodyText": "nit: isStartOfDataBuffer()", "url": "https://github.com/apache/flink/pull/13614#discussion_r504622425", "createdAt": "2020-10-14T12:06:08Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/buffer/BufferConsumer.java", "diffHunk": "@@ -159,6 +184,14 @@ int getCurrentReaderPosition() {\n \t\treturn currentReaderPosition;\n \t}\n \n+\tboolean startOfDataBuffer() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "47d3d60c3dc74ab402ce657c31c27568b190de40"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDYyODEyMA==", "bodyText": "Do you really need to return the buffer here? I have a feeling that the could would be simpler if:\n\nthis method was returning just true/false if cleanup has finished or not.\nskipBuild(...) would be replaced with skip(...), that would just move the offset, without returning the buffer\n.build() call would be required afterwards, to get the remaining data", "url": "https://github.com/apache/flink/pull/13614#discussion_r504628120", "createdAt": "2020-10-14T12:16:19Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/buffer/BufferConsumerWithPartialRecordLength.java", "diffHunk": "@@ -0,0 +1,120 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.io.network.buffer;\n+\n+import javax.annotation.concurrent.NotThreadSafe;\n+\n+import static org.apache.flink.util.Preconditions.checkNotNull;\n+import static org.apache.flink.util.Preconditions.checkState;\n+\n+/**\n+ * BufferConsumer with partial record length if a record is spanning over buffers\n+ *\n+ * <p>`partialRecordLength` is the length of bytes to skip in order to start with a complete record,\n+ * from position index 0 of the underlying MemorySegment. `partialRecordLength` is used in approximate\n+ * local recovery to find the start position of a complete record on a BufferConsumer, so called\n+ * `partial record clean-up`.\n+ *\n+ * <p>Partial records happen if a record can not fit into one buffer, then the remaining part of the same record\n+ * is put into the next buffer. Hence partial records only exist at the beginning of a buffer.\n+ * Partial record clean-up is needed in the mode of approximate local recovery.\n+ * If a record is spanning over multiple buffers, and the first (several) buffers have got lost due to the failure\n+ * of the receiver task, the remaining data belonging to the same record in transition should be cleaned up.\n+ *\n+ * <p> If partialRecordLength == 0, the buffer starts with a complete record</p>\n+ * <p> If partialRecordLength > 0, the buffer starts with a partial record, its length = partialRecordLength</p>\n+ * <p> If partialRecordLength < 0, partialRecordLength is undefined. It is currently used in\n+ * \t\t\t\t\t\t\t\t\t{@cite ResultSubpartitionRecoveredStateHandler#recover}</p>\n+ */\n+@NotThreadSafe\n+public class BufferConsumerWithPartialRecordLength {\n+\tprivate final BufferConsumer bufferConsumer;\n+\tprivate final int partialRecordLength;\n+\n+\tpublic BufferConsumerWithPartialRecordLength(BufferConsumer bufferConsumer, int partialRecordLength) {\n+\t\tthis.bufferConsumer = checkNotNull(bufferConsumer);\n+\t\tthis.partialRecordLength = partialRecordLength;\n+\t}\n+\n+\tpublic BufferConsumer getBufferConsumer() {\n+\t\treturn bufferConsumer;\n+\t}\n+\n+\tpublic int getPartialRecordLength() {\n+\t\treturn partialRecordLength;\n+\t}\n+\n+\tpublic Buffer build() {\n+\t\treturn bufferConsumer.build();\n+\t}\n+\n+\tpublic PartialRecordCleanupResult cleanupPartialRecord() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "47d3d60c3dc74ab402ce657c31c27568b190de40"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDY0NTY5MQ==", "bodyText": "nit: those names are a bit lengthy, also it's a bit confusing that getNewEmptySubpartitionBufferBuilderForNewRecord is not appending the data, while getNewEmptySubpartitionBufferBuilderForRecordContinuation is.\nMaybe can you extract those lines above into a method:\nBufferBuilder buffer = appendDataForSubpartitionNewRecord(record, targetSubpartition);\n\nand for the broadcast version\nBufferBuilder buffer = appendDataForBroadcastNewRecord(record, targetSubpartition);\n\n...", "url": "https://github.com/apache/flink/pull/13614#discussion_r504645691", "createdAt": "2020-10-14T12:44:42Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/BufferWritingResultPartition.java", "diffHunk": "@@ -128,26 +128,40 @@ protected void flushAllSubpartitions(boolean finishProducers) {\n \n \t@Override\n \tpublic void emitRecord(ByteBuffer record, int targetSubpartition) throws IOException {\n-\t\tdo {\n-\t\t\tfinal BufferBuilder bufferBuilder = getSubpartitionBufferBuilder(targetSubpartition);\n-\t\t\tbufferBuilder.appendAndCommit(record);\n+\t\tBufferBuilder buffer = subpartitionBufferBuilders[targetSubpartition];\n \n-\t\t\tif (bufferBuilder.isFull()) {\n-\t\t\t\tfinishSubpartitionBufferBuilder(targetSubpartition);\n-\t\t\t}\n-\t\t} while (record.hasRemaining());\n+\t\tif (buffer == null) {\n+\t\t\tbuffer = getNewEmptySubpartitionBufferBuilderForNewRecord(targetSubpartition);\n+\t\t}\n+\t\tbuffer.appendAndCommit(record);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "47d3d60c3dc74ab402ce657c31c27568b190de40"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDY0Njg1Nw==", "bodyText": "...\nbuffer = appendDataForSubpartitionRecordContinuation(record, targetSubpartition);\n\nand for the broadcast version\nbuffer = appendDataForBroadcastRecordContinuation(record, targetSubpartition);\n\n?", "url": "https://github.com/apache/flink/pull/13614#discussion_r504646857", "createdAt": "2020-10-14T12:46:21Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/BufferWritingResultPartition.java", "diffHunk": "@@ -128,26 +128,40 @@ protected void flushAllSubpartitions(boolean finishProducers) {\n \n \t@Override\n \tpublic void emitRecord(ByteBuffer record, int targetSubpartition) throws IOException {\n-\t\tdo {\n-\t\t\tfinal BufferBuilder bufferBuilder = getSubpartitionBufferBuilder(targetSubpartition);\n-\t\t\tbufferBuilder.appendAndCommit(record);\n+\t\tBufferBuilder buffer = subpartitionBufferBuilders[targetSubpartition];\n \n-\t\t\tif (bufferBuilder.isFull()) {\n-\t\t\t\tfinishSubpartitionBufferBuilder(targetSubpartition);\n-\t\t\t}\n-\t\t} while (record.hasRemaining());\n+\t\tif (buffer == null) {\n+\t\t\tbuffer = getNewEmptySubpartitionBufferBuilderForNewRecord(targetSubpartition);\n+\t\t}\n+\t\tbuffer.appendAndCommit(record);\n+\n+\t\twhile (record.hasRemaining()) {\n+\t\t\tfinishSubpartitionBufferBuilder(targetSubpartition);\n+\t\t\tbuffer = getNewEmptySubpartitionBufferBuilderForRecordContinuation(record, targetSubpartition);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "47d3d60c3dc74ab402ce657c31c27568b190de40"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDY1MDMyOQ==", "bodyText": "if you moved appending data into this method as I suggested above, you could deduplicate this code with:\nreturn getNewEmptyBroadcastBufferBuilderForRecordContinuation(record, 0);", "url": "https://github.com/apache/flink/pull/13614#discussion_r504650329", "createdAt": "2020-10-14T12:51:27Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/BufferWritingResultPartition.java", "diffHunk": "@@ -211,46 +225,62 @@ protected void releaseInternal() {\n \t\t}\n \t}\n \n-\tprivate BufferBuilder getSubpartitionBufferBuilder(int targetSubpartition) throws IOException {\n-\t\tfinal BufferBuilder bufferBuilder = subpartitionBufferBuilders[targetSubpartition];\n-\t\tif (bufferBuilder != null) {\n-\t\t\treturn bufferBuilder;\n-\t\t}\n+\tprivate BufferBuilder getNewEmptySubpartitionBufferBuilderForNewRecord(int targetSubpartition) throws IOException {\n+\t\tfinal BufferBuilder bufferBuilder = requestNewSubpartitionBufferBuilder(targetSubpartition);\n+\t\tsubpartitions[targetSubpartition].add(bufferBuilder.createBufferConsumerFromBeginning(), 0);\n \n-\t\treturn getNewSubpartitionBufferBuilder(targetSubpartition);\n+\t\treturn bufferBuilder;\n \t}\n \n-\tprivate BufferBuilder getNewSubpartitionBufferBuilder(int targetSubpartition) throws IOException {\n+\tprivate BufferBuilder getNewEmptySubpartitionBufferBuilderForRecordContinuation(\n+\t\t\tfinal ByteBuffer remainingRecordBytes,\n+\t\t\tfinal int targetSubpartition) throws IOException {\n+\t\tfinal BufferBuilder bufferBuilder = requestNewSubpartitionBufferBuilder(targetSubpartition);\n+\t\tfinal int partialRecordBytes = bufferBuilder.appendAndCommit(remainingRecordBytes);\n+\t\tsubpartitions[targetSubpartition].add(bufferBuilder.createBufferConsumerFromBeginning(), partialRecordBytes);\n+\n+\t\treturn bufferBuilder;\n+\t}\n+\n+\tprivate BufferBuilder requestNewSubpartitionBufferBuilder(int targetSubpartition) throws IOException {\n \t\tcheckInProduceState();\n \t\tensureUnicastMode();\n-\n \t\tfinal BufferBuilder bufferBuilder = requestNewBufferBuilderFromPool(targetSubpartition);\n-\t\tsubpartitions[targetSubpartition].add(bufferBuilder.createBufferConsumer());\n \t\tsubpartitionBufferBuilders[targetSubpartition] = bufferBuilder;\n+\n \t\treturn bufferBuilder;\n \t}\n \n-\tprivate BufferBuilder getBroadcastBufferBuilder() throws IOException {\n-\t\tif (broadcastBufferBuilder != null) {\n-\t\t\treturn broadcastBufferBuilder;\n+\tprivate BufferBuilder getNewEmptyBroadcastBufferBuilderForNewRecord() throws IOException {\n+\t\tfinal BufferBuilder bufferBuilder = requestNewBroadcastBufferBuilder();\n+\t\ttry (final BufferConsumer consumer = bufferBuilder.createBufferConsumerFromBeginning()) {\n+\t\t\tfor (ResultSubpartition subpartition : subpartitions) {\n+\t\t\t\tsubpartition.add(consumer.copy(), 0);\n+\t\t\t}\n \t\t}\n \n-\t\treturn getNewBroadcastBufferBuilder();\n+\t\treturn bufferBuilder;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "47d3d60c3dc74ab402ce657c31c27568b190de40"}, "originalPosition": 115}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDY1MzU1Mg==", "bodyText": "hmmm, maybe it will be easier to deduplicate/generalise some of the code, if you replaced partialRecordBytes (partialRecordLength) with remainingRecordBytes.remaining() (remainingRecordLength - remainingRecordLength could be more than the size of one single Buffer, and would basically mean what's the remaining record length. if remainingRecordLength > buffer.size() it means we can not complete record clean up in one call)\nThis would slightly change the implementation of skipBuild(...), but maybe it would make the code overally a bit simpler?", "url": "https://github.com/apache/flink/pull/13614#discussion_r504653552", "createdAt": "2020-10-14T12:56:11Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/BufferWritingResultPartition.java", "diffHunk": "@@ -211,46 +225,62 @@ protected void releaseInternal() {\n \t\t}\n \t}\n \n-\tprivate BufferBuilder getSubpartitionBufferBuilder(int targetSubpartition) throws IOException {\n-\t\tfinal BufferBuilder bufferBuilder = subpartitionBufferBuilders[targetSubpartition];\n-\t\tif (bufferBuilder != null) {\n-\t\t\treturn bufferBuilder;\n-\t\t}\n+\tprivate BufferBuilder getNewEmptySubpartitionBufferBuilderForNewRecord(int targetSubpartition) throws IOException {\n+\t\tfinal BufferBuilder bufferBuilder = requestNewSubpartitionBufferBuilder(targetSubpartition);\n+\t\tsubpartitions[targetSubpartition].add(bufferBuilder.createBufferConsumerFromBeginning(), 0);\n \n-\t\treturn getNewSubpartitionBufferBuilder(targetSubpartition);\n+\t\treturn bufferBuilder;\n \t}\n \n-\tprivate BufferBuilder getNewSubpartitionBufferBuilder(int targetSubpartition) throws IOException {\n+\tprivate BufferBuilder getNewEmptySubpartitionBufferBuilderForRecordContinuation(\n+\t\t\tfinal ByteBuffer remainingRecordBytes,\n+\t\t\tfinal int targetSubpartition) throws IOException {\n+\t\tfinal BufferBuilder bufferBuilder = requestNewSubpartitionBufferBuilder(targetSubpartition);\n+\t\tfinal int partialRecordBytes = bufferBuilder.appendAndCommit(remainingRecordBytes);\n+\t\tsubpartitions[targetSubpartition].add(bufferBuilder.createBufferConsumerFromBeginning(), partialRecordBytes);\n+\n+\t\treturn bufferBuilder;\n+\t}\n+\n+\tprivate BufferBuilder requestNewSubpartitionBufferBuilder(int targetSubpartition) throws IOException {\n \t\tcheckInProduceState();\n \t\tensureUnicastMode();\n-\n \t\tfinal BufferBuilder bufferBuilder = requestNewBufferBuilderFromPool(targetSubpartition);\n-\t\tsubpartitions[targetSubpartition].add(bufferBuilder.createBufferConsumer());\n \t\tsubpartitionBufferBuilders[targetSubpartition] = bufferBuilder;\n+\n \t\treturn bufferBuilder;\n \t}\n \n-\tprivate BufferBuilder getBroadcastBufferBuilder() throws IOException {\n-\t\tif (broadcastBufferBuilder != null) {\n-\t\t\treturn broadcastBufferBuilder;\n+\tprivate BufferBuilder getNewEmptyBroadcastBufferBuilderForNewRecord() throws IOException {\n+\t\tfinal BufferBuilder bufferBuilder = requestNewBroadcastBufferBuilder();\n+\t\ttry (final BufferConsumer consumer = bufferBuilder.createBufferConsumerFromBeginning()) {\n+\t\t\tfor (ResultSubpartition subpartition : subpartitions) {\n+\t\t\t\tsubpartition.add(consumer.copy(), 0);\n+\t\t\t}\n \t\t}\n \n-\t\treturn getNewBroadcastBufferBuilder();\n+\t\treturn bufferBuilder;\n \t}\n \n-\tprivate BufferBuilder getNewBroadcastBufferBuilder() throws IOException {\n+\tprivate BufferBuilder getNewEmptyBroadcastBufferBuilderForRecordContinuation(\n+\t\t\tfinal ByteBuffer remainingRecordBytes) throws IOException {\n+\t\t\tfinal BufferBuilder bufferBuilder = requestNewBroadcastBufferBuilder();\n+\t\tfinal int partialRecordBytes = bufferBuilder.appendAndCommit(remainingRecordBytes);\n+\t\ttry (final BufferConsumer consumer = bufferBuilder.createBufferConsumerFromBeginning()) {\n+\t\t\tfor (ResultSubpartition subpartition : subpartitions) {\n+\t\t\t\tsubpartition.add(consumer.copy(), partialRecordBytes);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "47d3d60c3dc74ab402ce657c31c27568b190de40"}, "originalPosition": 125}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDY1NTA3MQ==", "bodyText": "inline?", "url": "https://github.com/apache/flink/pull/13614#discussion_r504655071", "createdAt": "2020-10-14T12:58:24Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/PipelinedSubpartition.java", "diffHunk": "@@ -305,6 +308,10 @@ BufferAndBacklog pollBuffer() {\n \t\t}\n \t}\n \n+\tBuffer buildSliceBuffer(BufferConsumerWithPartialRecordLength buffer) {\n+\t\treturn buffer.build();\n+\t}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "47d3d60c3dc74ab402ce657c31c27568b190de40"}, "originalPosition": 125}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "47d3d60c3dc74ab402ce657c31c27568b190de40", "author": {"user": {"login": "curcur", "name": "Yuan Mei"}}, "url": "https://github.com/apache/flink/commit/47d3d60c3dc74ab402ce657c31c27568b190de40", "committedDate": "2020-10-13T12:31:22Z", "message": "[FLINK-19547] Update `BufferConsumer` with `BufferConsumerWithPartialRecordLength` in PipelinedSubpartition.java\n\nBufferConsumerWithPartialRecordLength includes BufferConsumer as well as partialRecordLength\n\nThis commit includes two changes\n1. Update BufferConsumer with BufferConsumerWithPartialRecordLength in PipelinedSubpartition\n2. Implement partial record cleanup logic in BufferConsumer"}, "afterCommit": {"oid": "a001a7083dd0f78db05ef337692b5ddf08b354b0", "author": {"user": {"login": "curcur", "name": "Yuan Mei"}}, "url": "https://github.com/apache/flink/commit/a001a7083dd0f78db05ef337692b5ddf08b354b0", "committedDate": "2020-10-15T06:15:01Z", "message": "[FLINK-19547] Update `BufferConsumer` with `BufferConsumerWithPartialRecordLength` in PipelinedSubpartition.java\n\nBufferConsumerWithPartialRecordLength includes BufferConsumer as well as partialRecordLength\n\nThis commit includes two changes\n1. Update BufferConsumer with BufferConsumerWithPartialRecordLength in PipelinedSubpartition\n2. Implement partial record cleanup logic in BufferConsumer"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "a001a7083dd0f78db05ef337692b5ddf08b354b0", "author": {"user": {"login": "curcur", "name": "Yuan Mei"}}, "url": "https://github.com/apache/flink/commit/a001a7083dd0f78db05ef337692b5ddf08b354b0", "committedDate": "2020-10-15T06:15:01Z", "message": "[FLINK-19547] Update `BufferConsumer` with `BufferConsumerWithPartialRecordLength` in PipelinedSubpartition.java\n\nBufferConsumerWithPartialRecordLength includes BufferConsumer as well as partialRecordLength\n\nThis commit includes two changes\n1. Update BufferConsumer with BufferConsumerWithPartialRecordLength in PipelinedSubpartition\n2. Implement partial record cleanup logic in BufferConsumer"}, "afterCommit": {"oid": "553ff42a3ded621e45167d5a9f426934bf460ea4", "author": {"user": {"login": "curcur", "name": "Yuan Mei"}}, "url": "https://github.com/apache/flink/commit/553ff42a3ded621e45167d5a9f426934bf460ea4", "committedDate": "2020-10-15T16:44:36Z", "message": "[FLINK-19547] Update `BufferConsumer` with `BufferConsumerWithPartialRecordLength` in PipelinedSubpartition.java\n\nBufferConsumerWithPartialRecordLength includes BufferConsumer as well as partialRecordLength\n\nThis commit includes two changes\n1. Update BufferConsumer with BufferConsumerWithPartialRecordLength in PipelinedSubpartition\n2. Implement partial record cleanup logic in BufferConsumer"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "553ff42a3ded621e45167d5a9f426934bf460ea4", "author": {"user": {"login": "curcur", "name": "Yuan Mei"}}, "url": "https://github.com/apache/flink/commit/553ff42a3ded621e45167d5a9f426934bf460ea4", "committedDate": "2020-10-15T16:44:36Z", "message": "[FLINK-19547] Update `BufferConsumer` with `BufferConsumerWithPartialRecordLength` in PipelinedSubpartition.java\n\nBufferConsumerWithPartialRecordLength includes BufferConsumer as well as partialRecordLength\n\nThis commit includes two changes\n1. Update BufferConsumer with BufferConsumerWithPartialRecordLength in PipelinedSubpartition\n2. Implement partial record cleanup logic in BufferConsumer"}, "afterCommit": {"oid": "fd401b11ff98b70455ee395f8b4078851ab1ec9f", "author": {"user": {"login": "curcur", "name": "Yuan Mei"}}, "url": "https://github.com/apache/flink/commit/fd401b11ff98b70455ee395f8b4078851ab1ec9f", "committedDate": "2020-10-16T03:05:15Z", "message": "[FLINK-19547] Update `BufferConsumer` with `BufferConsumerWithPartialRecordLength` in PipelinedSubpartition.java\n\nBufferConsumerWithPartialRecordLength includes BufferConsumer as well as partialRecordLength\n\nThis commit includes two changes\n1. Update BufferConsumer with BufferConsumerWithPartialRecordLength in PipelinedSubpartition\n2. Implement partial record cleanup logic in BufferConsumer"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "fd401b11ff98b70455ee395f8b4078851ab1ec9f", "author": {"user": {"login": "curcur", "name": "Yuan Mei"}}, "url": "https://github.com/apache/flink/commit/fd401b11ff98b70455ee395f8b4078851ab1ec9f", "committedDate": "2020-10-16T03:05:15Z", "message": "[FLINK-19547] Update `BufferConsumer` with `BufferConsumerWithPartialRecordLength` in PipelinedSubpartition.java\n\nBufferConsumerWithPartialRecordLength includes BufferConsumer as well as partialRecordLength\n\nThis commit includes two changes\n1. Update BufferConsumer with BufferConsumerWithPartialRecordLength in PipelinedSubpartition\n2. Implement partial record cleanup logic in BufferConsumer"}, "afterCommit": {"oid": "a70ae897b37437579d8aab2ba3a744553a08feea", "author": {"user": {"login": "curcur", "name": "Yuan Mei"}}, "url": "https://github.com/apache/flink/commit/a70ae897b37437579d8aab2ba3a744553a08feea", "committedDate": "2020-10-16T10:30:18Z", "message": "[FLINK-19547] Update `BufferConsumer` with `BufferConsumerWithPartialRecordLength` in PipelinedSubpartition.java\n\nBufferConsumerWithPartialRecordLength includes BufferConsumer as well as partialRecordLength\n\nThis commit includes two changes\n1. Update BufferConsumer with BufferConsumerWithPartialRecordLength in PipelinedSubpartition\n2. Implement partial record cleanup logic in BufferConsumer"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTEwMjU1NjQx", "url": "https://github.com/apache/flink/pull/13614#pullrequestreview-510255641", "createdAt": "2020-10-16T08:24:47Z", "commit": {"oid": "fd401b11ff98b70455ee395f8b4078851ab1ec9f"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNlQwODoyNDo0N1rOHit0kA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNlQxMDo1MDowN1rOHi1Syw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjE2NDM2OA==", "bodyText": "Why do you need this special used only in the tests method? Can not you use the same thing as the other test are doing (for example above):\nResultSubpartitionView readView = pipelinedSubpartition.createSubpartitionView(0, new NoOpBufferAvailablityListener());\nreadView. getNextBuffer()\n\n?", "url": "https://github.com/apache/flink/pull/13614#discussion_r506164368", "createdAt": "2020-10-16T08:24:47Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/ResultPartitionTest.java", "diffHunk": "@@ -526,6 +526,44 @@ public void testFlushBoundedBlockingResultPartition() throws IOException {\n \t\tassertNull(readView2.getNextBuffer());\n \t}\n \n+\t@Test\n+\tpublic void testEmitRecordWithRecordSpanningMultipleBuffers() throws Exception {\n+\t\tBufferWritingResultPartition bufferWritingResultPartition = createResultPartition(ResultPartitionType.PIPELINED);\n+\t\tPipelinedSubpartition pipelinedSubpartition = (PipelinedSubpartition) bufferWritingResultPartition.subpartitions[0];\n+\t\tint partialLength = bufferSize / 3;\n+\n+\t\ttry {\n+\t\t\t// emit the first record, record length = partialLength\n+\t\t\tbufferWritingResultPartition.emitRecord(ByteBuffer.allocate(partialLength), 0);\n+\t\t\t// emit the second record, record length = bufferSize\n+\t\t\tbufferWritingResultPartition.emitRecord(ByteBuffer.allocate(bufferSize), 0);\n+\t\t} finally {\n+\t\t\tassertEquals(2, pipelinedSubpartition.getCurrentNumberOfBuffers());\n+\t\t\tassertEquals(0, pipelinedSubpartition.getNextBuffer().getPartialRecordLength());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fd401b11ff98b70455ee395f8b4078851ab1ec9f"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjI4MjQ3NQ==", "bodyText": "could it be createBufferConsumer()?", "url": "https://github.com/apache/flink/pull/13614#discussion_r506282475", "createdAt": "2020-10-16T10:44:55Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/BufferWritingResultPartition.java", "diffHunk": "@@ -211,46 +223,83 @@ protected void releaseInternal() {\n \t\t}\n \t}\n \n-\tprivate BufferBuilder getSubpartitionBufferBuilder(int targetSubpartition) throws IOException {\n-\t\tfinal BufferBuilder bufferBuilder = subpartitionBufferBuilders[targetSubpartition];\n-\t\tif (bufferBuilder != null) {\n-\t\t\treturn bufferBuilder;\n+\tprivate BufferBuilder appendDataToSubpartitionForNewRecord(\n+\t\t\tByteBuffer record, int targetSubpartition) throws IOException {\n+\t\tBufferBuilder buffer = subpartitionBufferBuilders[targetSubpartition];\n+\n+\t\tif (buffer == null) {\n+\t\t\tbuffer = requestNewSubpartitionBufferBuilder(targetSubpartition);\n+\t\t\tsubpartitions[targetSubpartition].add(buffer.createBufferConsumerFromBeginning(), 0);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a70ae897b37437579d8aab2ba3a744553a08feea"}, "originalPosition": 76}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjI4Njc5NQ==", "bodyText": "unicast. That's I think better and more consistent naming scheme compared to subpartition.\nSo maybe rename the  methods with subpartition in the name:\nrequestNewSubpartitionBufferBuilder  -> requetnNewUnicastBufferBuilder\nappendDataToSubpartitionForNewRecord -> appendUnicastDataForNewRecord\netc...\n?", "url": "https://github.com/apache/flink/pull/13614#discussion_r506286795", "createdAt": "2020-10-16T10:50:07Z", "author": {"login": "pnowojski"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/BufferWritingResultPartition.java", "diffHunk": "@@ -211,46 +223,83 @@ protected void releaseInternal() {\n \t\t}\n \t}\n \n-\tprivate BufferBuilder getSubpartitionBufferBuilder(int targetSubpartition) throws IOException {\n-\t\tfinal BufferBuilder bufferBuilder = subpartitionBufferBuilders[targetSubpartition];\n-\t\tif (bufferBuilder != null) {\n-\t\t\treturn bufferBuilder;\n+\tprivate BufferBuilder appendDataToSubpartitionForNewRecord(\n+\t\t\tByteBuffer record, int targetSubpartition) throws IOException {\n+\t\tBufferBuilder buffer = subpartitionBufferBuilders[targetSubpartition];\n+\n+\t\tif (buffer == null) {\n+\t\t\tbuffer = requestNewSubpartitionBufferBuilder(targetSubpartition);\n+\t\t\tsubpartitions[targetSubpartition].add(buffer.createBufferConsumerFromBeginning(), 0);\n \t\t}\n \n-\t\treturn getNewSubpartitionBufferBuilder(targetSubpartition);\n+\t\tbuffer.appendAndCommit(record);\n+\n+\t\treturn buffer;\n \t}\n \n-\tprivate BufferBuilder getNewSubpartitionBufferBuilder(int targetSubpartition) throws IOException {\n-\t\tcheckInProduceState();\n-\t\tensureUnicastMode();\n+\tprivate BufferBuilder appendDataToSubpartitionForRecordContinuation(\n+\t\t\tfinal ByteBuffer remainingRecordBytes,\n+\t\t\tfinal int targetSubpartition) throws IOException {\n+\t\tfinal BufferBuilder buffer = requestNewSubpartitionBufferBuilder(targetSubpartition);\n+\t\t// !! Be aware, in case of partialRecordBytes != 0, partial length and data has to `appendAndCommit` first\n+\t\t// before consumer is created. Otherwise it would be confused with the case the buffer starting\n+\t\t// with a complete record.\n+\t\t// !! The next two lines can not change order.\n+\t\tfinal int partialRecordBytes = buffer.appendAndCommit(remainingRecordBytes);\n+\t\tsubpartitions[targetSubpartition].add(buffer.createBufferConsumerFromBeginning(), partialRecordBytes);\n+\n+\t\treturn buffer;\n+\t}\n \n-\t\tfinal BufferBuilder bufferBuilder = requestNewBufferBuilderFromPool(targetSubpartition);\n-\t\tsubpartitions[targetSubpartition].add(bufferBuilder.createBufferConsumer());\n-\t\tsubpartitionBufferBuilders[targetSubpartition] = bufferBuilder;\n-\t\treturn bufferBuilder;\n+\tprivate BufferBuilder appendDataToBroadcastForNewRecord(ByteBuffer record) throws IOException {\n+\t\tBufferBuilder buffer = broadcastBufferBuilder;\n+\n+\t\tif (buffer == null) {\n+\t\t\tbuffer = requestNewBroadcastBufferBuilder();\n+\t\t\tcreateBroadcastBufferConsumers(buffer, 0);\n+\t\t}\n+\n+\t\tbuffer.appendAndCommit(record);\n+\n+\t\treturn buffer;\n \t}\n \n-\tprivate BufferBuilder getBroadcastBufferBuilder() throws IOException {\n-\t\tif (broadcastBufferBuilder != null) {\n-\t\t\treturn broadcastBufferBuilder;\n+\tprivate BufferBuilder appendDataToBroadcastForRecordContinuation(\n+\t\t\tfinal ByteBuffer remainingRecordBytes) throws IOException {\n+\t\tfinal BufferBuilder buffer = requestNewBroadcastBufferBuilder();\n+\t\t// !! Be aware, in case of partialRecordBytes != 0, partial length and data has to `appendAndCommit` first\n+\t\t// before consumer is created. Otherwise it would be confused with the case the buffer starting\n+\t\t// with a complete record.\n+\t\t// !! The next two lines can not change order.\n+\t\tfinal int partialRecordBytes = buffer.appendAndCommit(remainingRecordBytes);\n+\t\tcreateBroadcastBufferConsumers(buffer, partialRecordBytes);\n+\n+\t\treturn buffer;\n+\t}\n+\n+\tprivate void createBroadcastBufferConsumers(BufferBuilder buffer, int partialRecordBytes) throws IOException {\n+\t\ttry (final BufferConsumer consumer = buffer.createBufferConsumerFromBeginning()) {\n+\t\t\tfor (ResultSubpartition subpartition : subpartitions) {\n+\t\t\t\tsubpartition.add(consumer.copy(), partialRecordBytes);\n+\t\t\t}\n \t\t}\n+\t}\n \n-\t\treturn getNewBroadcastBufferBuilder();\n+\tprivate BufferBuilder requestNewSubpartitionBufferBuilder(int targetSubpartition) throws IOException {\n+\t\tcheckInProduceState();\n+\t\tensureUnicastMode();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a70ae897b37437579d8aab2ba3a744553a08feea"}, "originalPosition": 146}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "a70ae897b37437579d8aab2ba3a744553a08feea", "author": {"user": {"login": "curcur", "name": "Yuan Mei"}}, "url": "https://github.com/apache/flink/commit/a70ae897b37437579d8aab2ba3a744553a08feea", "committedDate": "2020-10-16T10:30:18Z", "message": "[FLINK-19547] Update `BufferConsumer` with `BufferConsumerWithPartialRecordLength` in PipelinedSubpartition.java\n\nBufferConsumerWithPartialRecordLength includes BufferConsumer as well as partialRecordLength\n\nThis commit includes two changes\n1. Update BufferConsumer with BufferConsumerWithPartialRecordLength in PipelinedSubpartition\n2. Implement partial record cleanup logic in BufferConsumer"}, "afterCommit": {"oid": "8a7c7987492c18b09b9111b8a341792a7cd508c1", "author": {"user": {"login": "curcur", "name": "Yuan Mei"}}, "url": "https://github.com/apache/flink/commit/8a7c7987492c18b09b9111b8a341792a7cd508c1", "committedDate": "2020-10-21T09:05:36Z", "message": "[FLINK-19547] Update `BufferConsumer` with `BufferConsumerWithPartialRecordLength` in PipelinedSubpartition.java\n\nBufferConsumerWithPartialRecordLength includes BufferConsumer as well as partialRecordLength\n\nThis commit includes two changes\n1. Update BufferConsumer with BufferConsumerWithPartialRecordLength in PipelinedSubpartition\n2. Implement partial record cleanup logic in BufferConsumer"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "8a7c7987492c18b09b9111b8a341792a7cd508c1", "author": {"user": {"login": "curcur", "name": "Yuan Mei"}}, "url": "https://github.com/apache/flink/commit/8a7c7987492c18b09b9111b8a341792a7cd508c1", "committedDate": "2020-10-21T09:05:36Z", "message": "[FLINK-19547] Update `BufferConsumer` with `BufferConsumerWithPartialRecordLength` in PipelinedSubpartition.java\n\nBufferConsumerWithPartialRecordLength includes BufferConsumer as well as partialRecordLength\n\nThis commit includes two changes\n1. Update BufferConsumer with BufferConsumerWithPartialRecordLength in PipelinedSubpartition\n2. Implement partial record cleanup logic in BufferConsumer"}, "afterCommit": {"oid": "6df9fdf311ff193f14dccedc4a4e2ad6ab017444", "author": {"user": {"login": "curcur", "name": "Yuan Mei"}}, "url": "https://github.com/apache/flink/commit/6df9fdf311ff193f14dccedc4a4e2ad6ab017444", "committedDate": "2020-10-21T09:42:24Z", "message": "[FLINK-19547] Update `BufferConsumer` with `BufferConsumerWithPartialRecordLength` in PipelinedSubpartition.java\n\nBufferConsumerWithPartialRecordLength includes BufferConsumer as well as partialRecordLength\n\nThis commit includes two changes\n1. Update BufferConsumer with BufferConsumerWithPartialRecordLength in PipelinedSubpartition\n2. Implement partial record cleanup logic in BufferConsumer"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "6df9fdf311ff193f14dccedc4a4e2ad6ab017444", "author": {"user": {"login": "curcur", "name": "Yuan Mei"}}, "url": "https://github.com/apache/flink/commit/6df9fdf311ff193f14dccedc4a4e2ad6ab017444", "committedDate": "2020-10-21T09:42:24Z", "message": "[FLINK-19547] Update `BufferConsumer` with `BufferConsumerWithPartialRecordLength` in PipelinedSubpartition.java\n\nBufferConsumerWithPartialRecordLength includes BufferConsumer as well as partialRecordLength\n\nThis commit includes two changes\n1. Update BufferConsumer with BufferConsumerWithPartialRecordLength in PipelinedSubpartition\n2. Implement partial record cleanup logic in BufferConsumer"}, "afterCommit": {"oid": "886615ca80abb71f262e5c8e981ae7f0b0247729", "author": {"user": {"login": "curcur", "name": "Yuan Mei"}}, "url": "https://github.com/apache/flink/commit/886615ca80abb71f262e5c8e981ae7f0b0247729", "committedDate": "2020-10-21T09:59:31Z", "message": "[hotfix][runtime] Rename XXXSubPartitionBufferBuilderXXX to XXXUnicastBufferBuilderXXX"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE2ODkxNjcy", "url": "https://github.com/apache/flink/pull/13614#pullrequestreview-516891672", "createdAt": "2020-10-26T15:23:13Z", "commit": {"oid": "886615ca80abb71f262e5c8e981ae7f0b0247729"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "283687ee841a69b4a64df4faad1df5dde9b8980b", "author": {"user": {"login": "curcur", "name": "Yuan Mei"}}, "url": "https://github.com/apache/flink/commit/283687ee841a69b4a64df4faad1df5dde9b8980b", "committedDate": "2020-10-27T03:02:46Z", "message": "[FLINK-19547][Runtime] Add the partialRecordLength when creating a BufferConsumer\n\nPartial records happen if a record can not fit into one buffer, then the remaining part of the same record\nis put into the next buffer. Hence partial records only exist at the beginning of a buffer.\nPartial record clean-up is needed in the mode of approximate local recovery.\nIf a record is spanning over multiple buffers, and the first (several) buffers have got lost due to the failure\nof the receiver task, the remaining data belonging to the same record in transition should be cleaned up.\n\n`partialRecordLength` is the length of bytes to skip in order to start with a complete record,\nfrom position index 0 of the underlying MemorySegment. `partialRecordLength` is used in approximate\nlocal recovery to find the start position of a complete record on a BufferConsumer, so-called\n`partial record clean-up`.\n\nThis commit includes\n1). API change to add `partialRecordLength` when creating a BufferConsumer\n2). Add `partialRecordLength` in `BufferWritingResultPartition` when emitRecord and `broadcastRecord`"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8121ca7fdcf5963717432c192ec2440a92d2d021", "author": {"user": {"login": "curcur", "name": "Yuan Mei"}}, "url": "https://github.com/apache/flink/commit/8121ca7fdcf5963717432c192ec2440a92d2d021", "committedDate": "2020-10-27T03:02:46Z", "message": "[FLINK-19547][Runtime] Update `BufferConsumer` with `BufferConsumerWithPartialRecordLength` in PipelinedSubpartition.java\n\nBufferConsumerWithPartialRecordLength includes BufferConsumer as well as partialRecordLength\n\nThis commit includes two changes\n1. Update BufferConsumer with BufferConsumerWithPartialRecordLength in PipelinedSubpartition\n2. Implement partial record cleanup logic in BufferConsumer"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "534b8f7e737aef2ba2c1fa3198a2a53ed66ca080", "author": {"user": {"login": "curcur", "name": "Yuan Mei"}}, "url": "https://github.com/apache/flink/commit/534b8f7e737aef2ba2c1fa3198a2a53ed66ca080", "committedDate": "2020-10-27T03:02:46Z", "message": "[hotfix][runtime] Rename XXXSubPartitionBufferBuilderXXX to XXXUnicastBufferBuilderXXX"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "886615ca80abb71f262e5c8e981ae7f0b0247729", "author": {"user": {"login": "curcur", "name": "Yuan Mei"}}, "url": "https://github.com/apache/flink/commit/886615ca80abb71f262e5c8e981ae7f0b0247729", "committedDate": "2020-10-21T09:59:31Z", "message": "[hotfix][runtime] Rename XXXSubPartitionBufferBuilderXXX to XXXUnicastBufferBuilderXXX"}, "afterCommit": {"oid": "534b8f7e737aef2ba2c1fa3198a2a53ed66ca080", "author": {"user": {"login": "curcur", "name": "Yuan Mei"}}, "url": "https://github.com/apache/flink/commit/534b8f7e737aef2ba2c1fa3198a2a53ed66ca080", "committedDate": "2020-10-27T03:02:46Z", "message": "[hotfix][runtime] Rename XXXSubPartitionBufferBuilderXXX to XXXUnicastBufferBuilderXXX"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3214, "cost": 1, "resetAt": "2021-10-28T16:48:13Z"}}}