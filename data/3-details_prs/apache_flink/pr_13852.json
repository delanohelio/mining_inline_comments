{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTEyNzc5MDMy", "number": 13852, "title": "[FLINK-19875][table] Integrate file compaction to filesystem connector", "bodyText": "What is the purpose of the change\nIntegrate file compaction to filesystem connector\nBrief change log\nIntroduce Filesystem options:\n\nAUTO_COMPACTION\nCOMPACTION_FILE_SIZE (Default is the rolling file size)\n\nComplete compaction:\n\nIntroduce CompactFileWriter\nIntroduce compaction to FileSystemTableSink\n\nVerifying this change\n\nCsvFileCompactionITCase compaction based on FileInputFormat\nParquetFileCompactionITCase compaction based on BulkFormat\n\nDoes this pull request potentially affect one of the following parts:\n\nDependencies (does it add or upgrade a dependency): no\nThe public API, i.e., is any changed class annotated with @Public(Evolving): no\nThe serializers: no\nThe runtime per-record code paths (performance sensitive): no\nAnything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: no\nThe S3 file system connector: no\n\nDocumentation\n\nDoes this pull request introduce a new feature? yes\nIf yes, how is the feature documented? JavaDocs", "createdAt": "2020-10-30T06:04:18Z", "url": "https://github.com/apache/flink/pull/13852", "merged": true, "mergeCommit": {"oid": "0c7659ed668b0b0d35b08bf01e9d6d04b8833d43"}, "closed": true, "closedAt": "2020-11-03T02:10:25Z", "author": {"login": "JingsongLi"}, "timelineItems": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdYbLxpgBqjM5NDU5NjA4NzM=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdYjRAMAFqTUyMTU2NTQwOQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "e8dc8dcf6c81859a3bab198a560e239de6269fa6", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/flink/commit/e8dc8dcf6c81859a3bab198a560e239de6269fa6", "committedDate": "2020-10-30T08:57:07Z", "message": "checkstyle"}, "afterCommit": {"oid": "64b74e0fea0674cad4b91a1fa1302a5eafa30578", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/flink/commit/64b74e0fea0674cad4b91a1fa1302a5eafa30578", "committedDate": "2020-11-02T02:31:22Z", "message": "[FLINK-19875][table] Integrate file compaction to filesystem connector"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "949d1689d348b587d0ecb7637f519af311ad1d68", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/flink/commit/949d1689d348b587d0ecb7637f519af311ad1d68", "committedDate": "2020-11-02T06:30:40Z", "message": "[FLINK-19875][table] Integrate file compaction to filesystem connector"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "05d33c698b08037649abcf941a88995ca07524f7", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/flink/commit/05d33c698b08037649abcf941a88995ca07524f7", "committedDate": "2020-11-02T03:14:33Z", "message": "Fix cases"}, "afterCommit": {"oid": "949d1689d348b587d0ecb7637f519af311ad1d68", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/flink/commit/949d1689d348b587d0ecb7637f519af311ad1d68", "committedDate": "2020-11-02T06:30:40Z", "message": "[FLINK-19875][table] Integrate file compaction to filesystem connector"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIxMzU5NjA2", "url": "https://github.com/apache/flink/pull/13852#pullrequestreview-521359606", "createdAt": "2020-11-02T06:34:35Z", "commit": {"oid": "949d1689d348b587d0ecb7637f519af311ad1d68"}, "state": "COMMENTED", "comments": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwNjozNDozNVrOHr3tpg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwNzo1NjoyNlrOHr5bVw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTc2MzYyMg==", "bodyText": "Why do we need to call clear here?", "url": "https://github.com/apache/flink/pull/13852#discussion_r515763622", "createdAt": "2020-11-02T06:34:35Z", "author": {"login": "lirui-apache"}, "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/runtime/stream/sql/FileCompactionITCaseBase.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.runtime.stream.sql;\n+\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeinfo.Types;\n+import org.apache.flink.api.java.typeutils.RowTypeInfo;\n+import org.apache.flink.streaming.api.scala.DataStream;\n+import org.apache.flink.table.planner.runtime.utils.ParallelFiniteTestSource;\n+import org.apache.flink.table.planner.runtime.utils.StreamingTestBase;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.util.CloseableIterator;\n+import org.apache.flink.util.CollectionUtil;\n+import org.apache.flink.util.FileUtils;\n+\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.List;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * Streaming sink File Compaction ITCase base, test checkpoint.\n+ */\n+public abstract class FileCompactionITCaseBase extends StreamingTestBase {\n+\n+\t@Rule\n+\tpublic Timeout timeoutPerTest = Timeout.seconds(60);\n+\n+\tprivate String resultPath;\n+\n+\tprivate List<Row> rows;\n+\n+\t@Before\n+\tpublic void init() throws IOException {\n+\t\tresultPath = tempFolder().newFolder().toURI().toString();\n+\t\tclear();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "949d1689d348b587d0ecb7637f519af311ad1d68"}, "originalPosition": 64}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTc2ODgzNw==", "bodyText": "Can we reuse the COMPACTED_PREFIX defined in CompactOperator?", "url": "https://github.com/apache/flink/pull/13852#discussion_r515768837", "createdAt": "2020-11-02T06:53:02Z", "author": {"login": "lirui-apache"}, "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/runtime/stream/sql/FileCompactionITCaseBase.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.runtime.stream.sql;\n+\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeinfo.Types;\n+import org.apache.flink.api.java.typeutils.RowTypeInfo;\n+import org.apache.flink.streaming.api.scala.DataStream;\n+import org.apache.flink.table.planner.runtime.utils.ParallelFiniteTestSource;\n+import org.apache.flink.table.planner.runtime.utils.StreamingTestBase;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.util.CloseableIterator;\n+import org.apache.flink.util.CollectionUtil;\n+import org.apache.flink.util.FileUtils;\n+\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.List;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * Streaming sink File Compaction ITCase base, test checkpoint.\n+ */\n+public abstract class FileCompactionITCaseBase extends StreamingTestBase {\n+\n+\t@Rule\n+\tpublic Timeout timeoutPerTest = Timeout.seconds(60);\n+\n+\tprivate String resultPath;\n+\n+\tprivate List<Row> rows;\n+\n+\t@Before\n+\tpublic void init() throws IOException {\n+\t\tresultPath = tempFolder().newFolder().toURI().toString();\n+\t\tclear();\n+\n+\t\tenv().setParallelism(3);\n+\t\tenv().enableCheckpointing(100);\n+\n+\t\trows = new ArrayList<>();\n+\t\tfor (int i = 0; i < 100; i++) {\n+\t\t\trows.add(Row.of(i, String.valueOf(i % 10), String.valueOf(i)));\n+\t\t}\n+\n+\t\tDataStream<Row> stream = new DataStream<>(env().getJavaEnv().addSource(\n+\t\t\t\tnew ParallelFiniteTestSource<>(rows),\n+\t\t\t\tnew RowTypeInfo(\n+\t\t\t\t\t\tnew TypeInformation[] {Types.INT, Types.STRING, Types.STRING},\n+\t\t\t\t\t\tnew String[] {\"a\", \"b\", \"c\"})));\n+\n+\t\ttEnv().createTemporaryView(\"my_table\",  stream);\n+\t}\n+\n+\t@After\n+\tpublic void clear() throws IOException {\n+\t\tFileUtils.deleteDirectory(new File(URI.create(resultPath)));\n+\t}\n+\n+\tprotected abstract String format();\n+\n+\t@Test\n+\tpublic void testNonPartition() throws Exception {\n+\t\ttEnv().executeSql(\"CREATE TABLE sink_table (a int, b string, c string) with (\" + options() + \")\");\n+\t\ttEnv().executeSql(\"insert into sink_table select * from my_table\").await();\n+\n+\t\tList<Row> results = toListAndClose(tEnv().executeSql(\"select * from sink_table\").collect());\n+\t\tresults.sort(Comparator.comparingInt(o -> (Integer) o.getField(0)));\n+\t\tassertEquals(rows, results);\n+\n+\t\tFile[] files = new File(URI.create(resultPath)).listFiles(\n+\t\t\t\t(dir, name) -> name.startsWith(\"compacted-part-\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "949d1689d348b587d0ecb7637f519af311ad1d68"}, "originalPosition": 100}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTc2OTIyOQ==", "bodyText": "Seems this is redundant because we list files with a filter that only returns files with this prefix", "url": "https://github.com/apache/flink/pull/13852#discussion_r515769229", "createdAt": "2020-11-02T06:54:28Z", "author": {"login": "lirui-apache"}, "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/runtime/stream/sql/FileCompactionITCaseBase.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.runtime.stream.sql;\n+\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeinfo.Types;\n+import org.apache.flink.api.java.typeutils.RowTypeInfo;\n+import org.apache.flink.streaming.api.scala.DataStream;\n+import org.apache.flink.table.planner.runtime.utils.ParallelFiniteTestSource;\n+import org.apache.flink.table.planner.runtime.utils.StreamingTestBase;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.util.CloseableIterator;\n+import org.apache.flink.util.CollectionUtil;\n+import org.apache.flink.util.FileUtils;\n+\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.List;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * Streaming sink File Compaction ITCase base, test checkpoint.\n+ */\n+public abstract class FileCompactionITCaseBase extends StreamingTestBase {\n+\n+\t@Rule\n+\tpublic Timeout timeoutPerTest = Timeout.seconds(60);\n+\n+\tprivate String resultPath;\n+\n+\tprivate List<Row> rows;\n+\n+\t@Before\n+\tpublic void init() throws IOException {\n+\t\tresultPath = tempFolder().newFolder().toURI().toString();\n+\t\tclear();\n+\n+\t\tenv().setParallelism(3);\n+\t\tenv().enableCheckpointing(100);\n+\n+\t\trows = new ArrayList<>();\n+\t\tfor (int i = 0; i < 100; i++) {\n+\t\t\trows.add(Row.of(i, String.valueOf(i % 10), String.valueOf(i)));\n+\t\t}\n+\n+\t\tDataStream<Row> stream = new DataStream<>(env().getJavaEnv().addSource(\n+\t\t\t\tnew ParallelFiniteTestSource<>(rows),\n+\t\t\t\tnew RowTypeInfo(\n+\t\t\t\t\t\tnew TypeInformation[] {Types.INT, Types.STRING, Types.STRING},\n+\t\t\t\t\t\tnew String[] {\"a\", \"b\", \"c\"})));\n+\n+\t\ttEnv().createTemporaryView(\"my_table\",  stream);\n+\t}\n+\n+\t@After\n+\tpublic void clear() throws IOException {\n+\t\tFileUtils.deleteDirectory(new File(URI.create(resultPath)));\n+\t}\n+\n+\tprotected abstract String format();\n+\n+\t@Test\n+\tpublic void testNonPartition() throws Exception {\n+\t\ttEnv().executeSql(\"CREATE TABLE sink_table (a int, b string, c string) with (\" + options() + \")\");\n+\t\ttEnv().executeSql(\"insert into sink_table select * from my_table\").await();\n+\n+\t\tList<Row> results = toListAndClose(tEnv().executeSql(\"select * from sink_table\").collect());\n+\t\tresults.sort(Comparator.comparingInt(o -> (Integer) o.getField(0)));\n+\t\tassertEquals(rows, results);\n+\n+\t\tFile[] files = new File(URI.create(resultPath)).listFiles(\n+\t\t\t\t(dir, name) -> name.startsWith(\"compacted-part-\"));\n+\t\tassertEquals(Arrays.toString(files), 1, files.length);\n+\n+\t\tString fileName = files[0].getName();\n+\t\tassertTrue(fileName, fileName.startsWith(\"compacted-part-\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "949d1689d348b587d0ecb7637f519af311ad1d68"}, "originalPosition": 104}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTc2OTYwNA==", "bodyText": "Can we also verify there's no un-compacted files left?", "url": "https://github.com/apache/flink/pull/13852#discussion_r515769604", "createdAt": "2020-11-02T06:55:44Z", "author": {"login": "lirui-apache"}, "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/runtime/stream/sql/FileCompactionITCaseBase.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.runtime.stream.sql;\n+\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeinfo.Types;\n+import org.apache.flink.api.java.typeutils.RowTypeInfo;\n+import org.apache.flink.streaming.api.scala.DataStream;\n+import org.apache.flink.table.planner.runtime.utils.ParallelFiniteTestSource;\n+import org.apache.flink.table.planner.runtime.utils.StreamingTestBase;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.util.CloseableIterator;\n+import org.apache.flink.util.CollectionUtil;\n+import org.apache.flink.util.FileUtils;\n+\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.List;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * Streaming sink File Compaction ITCase base, test checkpoint.\n+ */\n+public abstract class FileCompactionITCaseBase extends StreamingTestBase {\n+\n+\t@Rule\n+\tpublic Timeout timeoutPerTest = Timeout.seconds(60);\n+\n+\tprivate String resultPath;\n+\n+\tprivate List<Row> rows;\n+\n+\t@Before\n+\tpublic void init() throws IOException {\n+\t\tresultPath = tempFolder().newFolder().toURI().toString();\n+\t\tclear();\n+\n+\t\tenv().setParallelism(3);\n+\t\tenv().enableCheckpointing(100);\n+\n+\t\trows = new ArrayList<>();\n+\t\tfor (int i = 0; i < 100; i++) {\n+\t\t\trows.add(Row.of(i, String.valueOf(i % 10), String.valueOf(i)));\n+\t\t}\n+\n+\t\tDataStream<Row> stream = new DataStream<>(env().getJavaEnv().addSource(\n+\t\t\t\tnew ParallelFiniteTestSource<>(rows),\n+\t\t\t\tnew RowTypeInfo(\n+\t\t\t\t\t\tnew TypeInformation[] {Types.INT, Types.STRING, Types.STRING},\n+\t\t\t\t\t\tnew String[] {\"a\", \"b\", \"c\"})));\n+\n+\t\ttEnv().createTemporaryView(\"my_table\",  stream);\n+\t}\n+\n+\t@After\n+\tpublic void clear() throws IOException {\n+\t\tFileUtils.deleteDirectory(new File(URI.create(resultPath)));\n+\t}\n+\n+\tprotected abstract String format();\n+\n+\t@Test\n+\tpublic void testNonPartition() throws Exception {\n+\t\ttEnv().executeSql(\"CREATE TABLE sink_table (a int, b string, c string) with (\" + options() + \")\");\n+\t\ttEnv().executeSql(\"insert into sink_table select * from my_table\").await();\n+\n+\t\tList<Row> results = toListAndClose(tEnv().executeSql(\"select * from sink_table\").collect());\n+\t\tresults.sort(Comparator.comparingInt(o -> (Integer) o.getField(0)));\n+\t\tassertEquals(rows, results);\n+\n+\t\tFile[] files = new File(URI.create(resultPath)).listFiles(\n+\t\t\t\t(dir, name) -> name.startsWith(\"compacted-part-\"));\n+\t\tassertEquals(Arrays.toString(files), 1, files.length);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "949d1689d348b587d0ecb7637f519af311ad1d68"}, "originalPosition": 101}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTc3MjY3OA==", "bodyText": "Let's have some more high-level explanations here, e.g. what will be compacted, when the compaction happens, whether files are usable before compaction, etc.", "url": "https://github.com/apache/flink/pull/13852#discussion_r515772678", "createdAt": "2020-11-02T07:06:10Z", "author": {"login": "lirui-apache"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemOptions.java", "diffHunk": "@@ -185,4 +185,17 @@\n \t\t\t\t\t.defaultValue(\"_SUCCESS\")\n \t\t\t\t\t.withDescription(\"The file name for success-file partition commit policy,\" +\n \t\t\t\t\t\t\t\" default is '_SUCCESS'.\");\n+\n+\tpublic static final ConfigOption<Boolean> AUTO_COMPACTION =\n+\t\t\tkey(\"auto-compaction\")\n+\t\t\t\t\t.booleanType()\n+\t\t\t\t\t.defaultValue(false)\n+\t\t\t\t\t.withDescription(\"Whether to enable automatic compaction in streaming sink or not.\" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "949d1689d348b587d0ecb7637f519af311ad1d68"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTc3NDY3OQ==", "bodyText": "FileInputFormatCompactReader?", "url": "https://github.com/apache/flink/pull/13852#discussion_r515774679", "createdAt": "2020-11-02T07:12:44Z", "author": {"login": "lirui-apache"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/compact/FileInputFormatReader.java", "diffHunk": "@@ -0,0 +1,72 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem.stream.compact;\n+\n+import org.apache.flink.api.common.io.FileInputFormat;\n+import org.apache.flink.core.fs.FileInputSplit;\n+\n+import java.io.IOException;\n+\n+/**\n+ * The {@link CompactReader} to delegate {@link FileInputFormat}.\n+ */\n+public class FileInputFormatReader<T> implements CompactReader<T> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "949d1689d348b587d0ecb7637f519af311ad1d68"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTc3ODc4MQ==", "bodyText": "Can we have a method to create builder from an OutputFileConfig instance? To make sure we won't lose anything here.", "url": "https://github.com/apache/flink/pull/13852#discussion_r515778781", "createdAt": "2020-11-02T07:24:27Z", "author": {"login": "lirui-apache"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSink.java", "diffHunk": "@@ -138,15 +154,28 @@ public SinkRuntimeProvider getSinkRuntimeProvider(Context sinkContext) {\n \t\t\treturn dataStream.writeUsingOutputFormat(builder.build())\n \t\t\t\t\t.setParallelism(dataStream.getParallelism());\n \t\t} else {\n+\t\t\tif (overwrite) {\n+\t\t\t\tthrow new IllegalStateException(\"Streaming mode not support overwrite.\");\n+\t\t\t}\n+\n+\t\t\tboolean autoCompaction = tableOptions.getBoolean(FileSystemOptions.AUTO_COMPACTION);\n \t\t\tObject writer = createWriter(sinkContext);\n+\t\t\tboolean isEncoder = writer instanceof Encoder;\n \t\t\tTableBucketAssigner assigner = new TableBucketAssigner(computer);\n \t\t\tTableRollingPolicy rollingPolicy = new TableRollingPolicy(\n-\t\t\t\t\t!(writer instanceof Encoder),\n+\t\t\t\t\t!isEncoder || autoCompaction,\n \t\t\t\t\ttableOptions.get(SINK_ROLLING_POLICY_FILE_SIZE).getBytes(),\n \t\t\t\t\ttableOptions.get(SINK_ROLLING_POLICY_ROLLOVER_INTERVAL).toMillis());\n \n+\t\t\tif (autoCompaction) {\n+\t\t\t\toutputFileConfig = OutputFileConfig.builder()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "949d1689d348b587d0ecb7637f519af311ad1d68"}, "originalPosition": 101}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTc5MTcwMw==", "bodyText": "createCompactReaderFactory?", "url": "https://github.com/apache/flink/pull/13852#discussion_r515791703", "createdAt": "2020-11-02T07:56:26Z", "author": {"login": "lirui-apache"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSink.java", "diffHunk": "@@ -161,20 +190,120 @@ public SinkRuntimeProvider getSinkRuntimeProvider(Context sinkContext) {\n \t\t\t\t\t\t.withOutputFileConfig(outputFileConfig)\n \t\t\t\t\t\t.withRollingPolicy(rollingPolicy);\n \t\t\t}\n-\t\t\treturn createStreamingSink(\n-\t\t\t\t\ttableOptions,\n+\n+\t\t\tlong bucketCheckInterval = tableOptions.get(SINK_ROLLING_POLICY_CHECK_INTERVAL).toMillis();\n+\n+\t\t\tDataStream<PartitionCommitInfo> writerStream;\n+\t\t\tif (autoCompaction) {\n+\t\t\t\tlong compactionSize = tableOptions\n+\t\t\t\t\t\t.getOptional(FileSystemOptions.COMPACTION_FILE_SIZE)\n+\t\t\t\t\t\t.orElse(tableOptions.get(SINK_ROLLING_POLICY_FILE_SIZE))\n+\t\t\t\t\t\t.getBytes();\n+\n+\t\t\t\tCompactReader.Factory<RowData> reader = createCompactReader(sinkContext).orElseThrow(\n+\t\t\t\t\t\t() -> new TableException(\"Please implement available reader for compaction:\" +\n+\t\t\t\t\t\t\t\t\" BulkFormat, FileInputFormat.\"));\n+\n+\t\t\t\twriterStream = StreamingSink.compactionWriter(\n+\t\t\t\t\t\tdataStream,\n+\t\t\t\t\t\tbucketCheckInterval,\n+\t\t\t\t\t\tbucketsBuilder,\n+\t\t\t\t\t\tfsFactory,\n+\t\t\t\t\t\tpath,\n+\t\t\t\t\t\treader,\n+\t\t\t\t\t\tcompactionSize);\n+\t\t\t} else {\n+\t\t\t\twriterStream = StreamingSink.writer(\n+\t\t\t\t\t\tdataStream, bucketCheckInterval, bucketsBuilder);\n+\t\t\t}\n+\n+\t\t\treturn StreamingSink.sink(\n+\t\t\t\t\twriterStream,\n \t\t\t\t\tpath,\n-\t\t\t\t\tpartitionKeys,\n \t\t\t\t\ttableIdentifier,\n-\t\t\t\t\toverwrite,\n-\t\t\t\t\tdataStream,\n-\t\t\t\t\tbucketsBuilder,\n+\t\t\t\t\tpartitionKeys,\n \t\t\t\t\tmetaStoreFactory,\n \t\t\t\t\tfsFactory,\n-\t\t\t\t\ttableOptions.get(SINK_ROLLING_POLICY_CHECK_INTERVAL).toMillis());\n+\t\t\t\t\ttableOptions);\n+\t\t}\n+\t}\n+\n+\tprivate Optional<CompactReader.Factory<RowData>> createCompactReader(Context context) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "949d1689d348b587d0ecb7637f519af311ad1d68"}, "originalPosition": 162}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b1bffe4fa9f3adbe03d77dd7bbf8502d39619e13", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/flink/commit/b1bffe4fa9f3adbe03d77dd7bbf8502d39619e13", "committedDate": "2020-11-02T09:44:35Z", "message": "Address comments and fix bug"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3fc1fd9bea063dca0c8f7c0ae5e3ff9aaef4fe17", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/flink/commit/3fc1fd9bea063dca0c8f7c0ae5e3ff9aaef4fe17", "committedDate": "2020-11-02T09:50:09Z", "message": "Add test to end input"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIxNTU2Nzk3", "url": "https://github.com/apache/flink/pull/13852#pullrequestreview-521556797", "createdAt": "2020-11-02T11:43:08Z", "commit": {"oid": "b1bffe4fa9f3adbe03d77dd7bbf8502d39619e13"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQxMTo0MzowOFrOHsBCjA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQxMTo0MzowOFrOHsBCjA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTkxNjQyOA==", "bodyText": "Why do we call withPartPrefix twice?", "url": "https://github.com/apache/flink/pull/13852#discussion_r515916428", "createdAt": "2020-11-02T11:43:08Z", "author": {"login": "lirui-apache"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSink.java", "diffHunk": "@@ -127,108 +127,118 @@ public SinkRuntimeProvider getSinkRuntimeProvider(Context sinkContext) {\n \t}\n \n \tprivate DataStreamSink<?> consume(DataStream<RowData> dataStream, Context sinkContext) {\n-\t\tRowDataPartitionComputer computer = new RowDataPartitionComputer(\n+\t\tif (sinkContext.isBounded()) {\n+\t\t\treturn createBatchSink(dataStream, sinkContext);\n+\t\t} else {\n+\t\t\tif (overwrite) {\n+\t\t\t\tthrow new IllegalStateException(\"Streaming mode not support overwrite.\");\n+\t\t\t}\n+\n+\t\t\treturn createStreamingSink(dataStream, sinkContext);\n+\t\t}\n+\t}\n+\n+\tprivate RowDataPartitionComputer partitionComputer() {\n+\t\treturn new RowDataPartitionComputer(\n \t\t\t\tdefaultPartName,\n \t\t\t\tschema.getFieldNames(),\n \t\t\t\tschema.getFieldDataTypes(),\n \t\t\t\tpartitionKeys.toArray(new String[0]));\n+\t}\n \n-\t\tEmptyMetaStoreFactory metaStoreFactory = new EmptyMetaStoreFactory(path);\n-\t\tOutputFileConfig outputFileConfig = OutputFileConfig.builder()\n+\tprivate DataStreamSink<RowData> createBatchSink(\n+\t\t\tDataStream<RowData> inputStream, Context sinkContext) {\n+\t\tFileSystemOutputFormat.Builder<RowData> builder = new FileSystemOutputFormat.Builder<>();\n+\t\tbuilder.setPartitionComputer(partitionComputer());\n+\t\tbuilder.setDynamicGrouped(dynamicGrouping);\n+\t\tbuilder.setPartitionColumns(partitionKeys.toArray(new String[0]));\n+\t\tbuilder.setFormatFactory(createOutputFormatFactory(sinkContext));\n+\t\tbuilder.setMetaStoreFactory(new EmptyMetaStoreFactory(path));\n+\t\tbuilder.setOverwrite(overwrite);\n+\t\tbuilder.setStaticPartitions(staticPartitions);\n+\t\tbuilder.setTempPath(toStagingPath());\n+\t\tbuilder.setOutputFileConfig(OutputFileConfig.builder()\n \t\t\t\t.withPartPrefix(\"part-\" + UUID.randomUUID().toString())\n-\t\t\t\t.build();\n+\t\t\t\t.build());\n+\t\treturn inputStream.writeUsingOutputFormat(builder.build())\n+\t\t\t\t.setParallelism(inputStream.getParallelism());\n+\t}\n+\n+\tprivate DataStreamSink<?> createStreamingSink(\n+\t\t\tDataStream<RowData> dataStream, Context sinkContext) {\n \t\tFileSystemFactory fsFactory = FileSystem::get;\n+\t\tRowDataPartitionComputer computer = partitionComputer();\n \n-\t\tif (sinkContext.isBounded()) {\n-\t\t\tFileSystemOutputFormat.Builder<RowData> builder = new FileSystemOutputFormat.Builder<>();\n-\t\t\tbuilder.setPartitionComputer(computer);\n-\t\t\tbuilder.setDynamicGrouped(dynamicGrouping);\n-\t\t\tbuilder.setPartitionColumns(partitionKeys.toArray(new String[0]));\n-\t\t\tbuilder.setFormatFactory(createOutputFormatFactory(sinkContext));\n-\t\t\tbuilder.setMetaStoreFactory(metaStoreFactory);\n-\t\t\tbuilder.setFileSystemFactory(fsFactory);\n-\t\t\tbuilder.setOverwrite(overwrite);\n-\t\t\tbuilder.setStaticPartitions(staticPartitions);\n-\t\t\tbuilder.setTempPath(toStagingPath());\n-\t\t\tbuilder.setOutputFileConfig(outputFileConfig);\n-\t\t\treturn dataStream.writeUsingOutputFormat(builder.build())\n-\t\t\t\t\t.setParallelism(dataStream.getParallelism());\n+\t\tboolean autoCompaction = tableOptions.getBoolean(FileSystemOptions.AUTO_COMPACTION);\n+\t\tObject writer = createWriter(sinkContext);\n+\t\tboolean isEncoder = writer instanceof Encoder;\n+\t\tTableBucketAssigner assigner = new TableBucketAssigner(computer);\n+\t\tTableRollingPolicy rollingPolicy = new TableRollingPolicy(\n+\t\t\t\t!isEncoder || autoCompaction,\n+\t\t\t\ttableOptions.get(SINK_ROLLING_POLICY_FILE_SIZE).getBytes(),\n+\t\t\t\ttableOptions.get(SINK_ROLLING_POLICY_ROLLOVER_INTERVAL).toMillis());\n+\n+\t\tString randomPrefix = \"part-\" + UUID.randomUUID().toString();\n+\t\tOutputFileConfig.OutputFileConfigBuilder fileNamingBuilder = OutputFileConfig.builder();\n+\t\tfileNamingBuilder = autoCompaction ?\n+\t\t\t\tfileNamingBuilder.withPartPrefix(convertToUncompacted(randomPrefix)) :\n+\t\t\t\tfileNamingBuilder.withPartPrefix(randomPrefix);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b1bffe4fa9f3adbe03d77dd7bbf8502d39619e13"}, "originalPosition": 86}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIxNTY1NDA5", "url": "https://github.com/apache/flink/pull/13852#pullrequestreview-521565409", "createdAt": "2020-11-02T11:56:40Z", "commit": {"oid": "3fc1fd9bea063dca0c8f7c0ae5e3ff9aaef4fe17"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4992, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}