{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzU4NTk2NTE3", "number": 10745, "reviewThreads": {"totalCount": 20, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wMlQwOTozMTo0OVrODVaIsQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQxMzowMToyM1rODeyzkA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjIzNzc0ODk3OnYy", "diffSide": "RIGHT", "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCTableSource.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wMlQwOTozMTo0OVrOFZn2cg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wMlQxMDowODozMVrOFZod4A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MjQxMTYzNA==", "bodyText": "This is wrong fix, you should modify returnType.", "url": "https://github.com/apache/flink/pull/10745#discussion_r362411634", "createdAt": "2020-01-02T09:31:49Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCTableSource.java", "diffHunk": "@@ -112,6 +113,11 @@ public boolean isBounded() {\n \t\treturn returnType;\n \t}\n \n+\t@Override\n+\tpublic DataType getProducedDataType() {\n+\t\treturn schema.toRowDataType();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c00c793d00c60c7dafc5b4f8209459a0dcc940b"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MjQyMTcyOA==", "bodyText": "updated", "url": "https://github.com/apache/flink/pull/10745#discussion_r362421728", "createdAt": "2020-01-02T10:08:31Z", "author": {"login": "docete"}, "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCTableSource.java", "diffHunk": "@@ -112,6 +113,11 @@ public boolean isBounded() {\n \t\treturn returnType;\n \t}\n \n+\t@Override\n+\tpublic DataType getProducedDataType() {\n+\t\treturn schema.toRowDataType();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MjQxMTYzNA=="}, "originalCommit": {"oid": "0c00c793d00c60c7dafc5b4f8209459a0dcc940b"}, "originalPosition": 14}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjIzODExMjQ0OnYy", "diffSide": "RIGHT", "path": "flink-connectors/flink-jdbc/pom.xml", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wMlQxMjo1ODo1MFrOFZrFBw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wM1QwMjozNToyNVrOFZ5CJw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MjQ2NDUxOQ==", "bodyText": "We can mark it as provided and put besides old planner dependency.\n<scope>provided</scope>\n<optional>true</optional>", "url": "https://github.com/apache/flink/pull/10745#discussion_r362464519", "createdAt": "2020-01-02T12:58:50Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-jdbc/pom.xml", "diffHunk": "@@ -82,5 +82,20 @@ under the License.\n \t\t\t<type>test-jar</type>\n \t\t\t<scope>test</scope>\n \t\t</dependency>\n+\n+\t\t<dependency>\n+\t\t\t<groupId>org.apache.flink</groupId>\n+\t\t\t<artifactId>flink-table-planner-blink_${scala.binary.version}</artifactId>\n+\t\t\t<version>${project.version}</version>\n+\t\t\t<scope>test</scope>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2cc4e7b6e88e9f655235b8c97707c41f45b6b238"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MjY5MTM0NA==", "bodyText": "The connector code should not depend on planner now. Only testing code depend on them. I will open a ticket to remove planner dependencies from connectors by changing the scope to test.", "url": "https://github.com/apache/flink/pull/10745#discussion_r362691344", "createdAt": "2020-01-03T02:17:07Z", "author": {"login": "docete"}, "path": "flink-connectors/flink-jdbc/pom.xml", "diffHunk": "@@ -82,5 +82,20 @@ under the License.\n \t\t\t<type>test-jar</type>\n \t\t\t<scope>test</scope>\n \t\t</dependency>\n+\n+\t\t<dependency>\n+\t\t\t<groupId>org.apache.flink</groupId>\n+\t\t\t<artifactId>flink-table-planner-blink_${scala.binary.version}</artifactId>\n+\t\t\t<version>${project.version}</version>\n+\t\t\t<scope>test</scope>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MjQ2NDUxOQ=="}, "originalCommit": {"oid": "2cc4e7b6e88e9f655235b8c97707c41f45b6b238"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MjY5MTkzMg==", "bodyText": "https://issues.apache.org/jira/browse/FLINK-15460", "url": "https://github.com/apache/flink/pull/10745#discussion_r362691932", "createdAt": "2020-01-03T02:23:44Z", "author": {"login": "docete"}, "path": "flink-connectors/flink-jdbc/pom.xml", "diffHunk": "@@ -82,5 +82,20 @@ under the License.\n \t\t\t<type>test-jar</type>\n \t\t\t<scope>test</scope>\n \t\t</dependency>\n+\n+\t\t<dependency>\n+\t\t\t<groupId>org.apache.flink</groupId>\n+\t\t\t<artifactId>flink-table-planner-blink_${scala.binary.version}</artifactId>\n+\t\t\t<version>${project.version}</version>\n+\t\t\t<scope>test</scope>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MjQ2NDUxOQ=="}, "originalCommit": {"oid": "2cc4e7b6e88e9f655235b8c97707c41f45b6b238"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MjY5MzE1OQ==", "bodyText": "Thanks.", "url": "https://github.com/apache/flink/pull/10745#discussion_r362693159", "createdAt": "2020-01-03T02:35:25Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-jdbc/pom.xml", "diffHunk": "@@ -82,5 +82,20 @@ under the License.\n \t\t\t<type>test-jar</type>\n \t\t\t<scope>test</scope>\n \t\t</dependency>\n+\n+\t\t<dependency>\n+\t\t\t<groupId>org.apache.flink</groupId>\n+\t\t\t<artifactId>flink-table-planner-blink_${scala.binary.version}</artifactId>\n+\t\t\t<version>${project.version}</version>\n+\t\t\t<scope>test</scope>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MjQ2NDUxOQ=="}, "originalCommit": {"oid": "2cc4e7b6e88e9f655235b8c97707c41f45b6b238"}, "originalPosition": 9}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjIzODExNjkyOnYy", "diffSide": "RIGHT", "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCTableSource.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wMlQxMzowMTo1MlrOFZrHrA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wM1QwMjoxNzoyNlrOFZ47Kw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MjQ2NTE5Ng==", "bodyText": "Please remove the overrided implementation of getReturnType()", "url": "https://github.com/apache/flink/pull/10745#discussion_r362465196", "createdAt": "2020-01-02T13:01:52Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCTableSource.java", "diffHunk": "@@ -112,6 +120,11 @@ public boolean isBounded() {\n \t\treturn returnType;\n \t}\n \n+\t@Override\n+\tpublic DataType getProducedDataType() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2cc4e7b6e88e9f655235b8c97707c41f45b6b238"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MjY5MTM3MQ==", "bodyText": "Sure", "url": "https://github.com/apache/flink/pull/10745#discussion_r362691371", "createdAt": "2020-01-03T02:17:26Z", "author": {"login": "docete"}, "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCTableSource.java", "diffHunk": "@@ -112,6 +120,11 @@ public boolean isBounded() {\n \t\treturn returnType;\n \t}\n \n+\t@Override\n+\tpublic DataType getProducedDataType() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MjQ2NTE5Ng=="}, "originalCommit": {"oid": "2cc4e7b6e88e9f655235b8c97707c41f45b6b238"}, "originalPosition": 48}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjIzODExODM1OnYy", "diffSide": "RIGHT", "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCTableSource.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wMlQxMzowMjo1MlrOFZrIlA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wM1QwMjoxNzozNlrOFZ47OQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MjQ2NTQyOA==", "bodyText": "Use returnType.getFieldNames instead reconstruct the selected field names again?", "url": "https://github.com/apache/flink/pull/10745#discussion_r362465428", "createdAt": "2020-01-02T13:02:52Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCTableSource.java", "diffHunk": "@@ -180,6 +193,20 @@ public boolean equals(Object o) {\n \t\t}\n \t}\n \n+\t@Override\n+\tpublic String explainSource() {\n+\t\tif (selectFields == null) {\n+\t\t\treturn String.format(\n+\t\t\t\t\"JDBCTableSource(read fields: %s)\", String.join(\", \", schema.getFieldNames()));\n+\t\t} else {\n+\t\t\tString[] fields = new String[selectFields.length];\n+\t\t\tfor (int i = 0; i < selectFields.length; i++) {\n+\t\t\t\tfields[i] = schema.getFieldName(selectFields[i]).get();\n+\t\t\t}\n+\t\t\treturn String.format(\"JDBCTableSource(read fields: %s)\", String.join(\", \", fields));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2cc4e7b6e88e9f655235b8c97707c41f45b6b238"}, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MjY5MTM4NQ==", "bodyText": "Sure", "url": "https://github.com/apache/flink/pull/10745#discussion_r362691385", "createdAt": "2020-01-03T02:17:36Z", "author": {"login": "docete"}, "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCTableSource.java", "diffHunk": "@@ -180,6 +193,20 @@ public boolean equals(Object o) {\n \t\t}\n \t}\n \n+\t@Override\n+\tpublic String explainSource() {\n+\t\tif (selectFields == null) {\n+\t\t\treturn String.format(\n+\t\t\t\t\"JDBCTableSource(read fields: %s)\", String.join(\", \", schema.getFieldNames()));\n+\t\t} else {\n+\t\t\tString[] fields = new String[selectFields.length];\n+\t\t\tfor (int i = 0; i < selectFields.length; i++) {\n+\t\t\t\tfields[i] = schema.getFieldName(selectFields[i]).get();\n+\t\t\t}\n+\t\t\treturn String.format(\"JDBCTableSource(read fields: %s)\", String.join(\", \", fields));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MjQ2NTQyOA=="}, "originalCommit": {"oid": "2cc4e7b6e88e9f655235b8c97707c41f45b6b238"}, "originalPosition": 69}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjIzOTU5Mzc5OnYy", "diffSide": "RIGHT", "path": "flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceITCase.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wM1QwMjo0MDo1MlrOFZ5EaQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQwOTowODowM1rOFlMmhA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MjY5MzczNw==", "bodyText": "Could you add full list of types to have a full coverage? For example, add TIMESTAMP, TIMESTAMP(9), DECIMAL(38, 18), DECIMAL, FLOAT (we have a bug for float before), etc...\nI would also suggest to combine source integrate tests and sink integrate tests, e.g.  read from collections and write into jdbc using SQL, and read from JDBC to verify the result.", "url": "https://github.com/apache/flink/pull/10745#discussion_r362693737", "createdAt": "2020-01-03T02:40:52Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceITCase.java", "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.api.java.io.jdbc;\n+\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.table.api.EnvironmentSettings;\n+import org.apache.flink.table.api.java.StreamTableEnvironment;\n+import org.apache.flink.table.runtime.utils.StreamITCase;\n+import org.apache.flink.test.util.AbstractTestBase;\n+import org.apache.flink.types.Row;\n+\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.sql.Connection;\n+import java.sql.DriverManager;\n+import java.sql.SQLException;\n+import java.sql.Statement;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+\n+/**\n+ * ITCase for {@link JDBCTableSource}.\n+ */\n+public class JDBCTableSourceITCase extends AbstractTestBase {\n+\n+\tpublic static final String DRIVER_CLASS = \"org.apache.derby.jdbc.EmbeddedDriver\";\n+\tpublic static final String DB_URL = \"jdbc:derby:memory:test\";\n+\tpublic static final String INPUT_TABLE = \"jdbcSource\";\n+\n+\t@Before\n+\tpublic void before() throws ClassNotFoundException, SQLException {\n+\t\tSystem.setProperty(\"derby.stream.error.field\", JDBCTestBase.class.getCanonicalName() + \".DEV_NULL\");\n+\t\tClass.forName(DRIVER_CLASS);\n+\n+\t\ttry (\n+\t\t\tConnection conn = DriverManager.getConnection(DB_URL + \";create=true\");\n+\t\t\tStatement statement = conn.createStatement()) {\n+\t\t\tstatement.executeUpdate(\"CREATE TABLE \" + INPUT_TABLE + \" (\" +\n+\t\t\t\t\t\"id BIGINT NOT NULL,\" +\n+\t\t\t\t\t\"timestamp6_col TIMESTAMP, \" +\n+\t\t\t\t\t\"time_col TIME, \" +\n+\t\t\t\t\t\"decimal_col DECIMAL(10, 4))\");\n+\t\t\tstatement.executeUpdate(\"INSERT INTO \" + INPUT_TABLE + \" VALUES (\" +\n+\t\t\t\t\t\"1, TIMESTAMP('2020-01-01 15:35:00.123456'), TIME('15:35:00'), 100.1234)\");\n+\t\t\tstatement.executeUpdate(\"INSERT INTO \" + INPUT_TABLE + \" VALUES (\" +\n+\t\t\t\t\t\"2, TIMESTAMP('2020-01-01 15:36:01.123456'), TIME('15:36:01'), 101.1234)\");\n+\t\t}\n+\t}\n+\n+\t@After\n+\tpublic void clearOutputTable() throws Exception {\n+\t\tClass.forName(DRIVER_CLASS);\n+\t\ttry (\n+\t\t\tConnection conn = DriverManager.getConnection(DB_URL);\n+\t\t\tStatement stat = conn.createStatement()) {\n+\t\t\tstat.execute(\"DROP TABLE \" + INPUT_TABLE);\n+\t\t}\n+\t}\n+\n+\t@Test\n+\tpublic void testJDBCSource() throws Exception {\n+\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+\t\tEnvironmentSettings envSettings = EnvironmentSettings.newInstance()\n+\t\t\t.useBlinkPlanner()\n+\t\t\t.inStreamingMode()\n+\t\t\t.build();\n+\t\tStreamTableEnvironment tEnv = StreamTableEnvironment.create(env, envSettings);\n+\n+\t\ttEnv.sqlUpdate(\n+\t\t\t\"CREATE TABLE \" + INPUT_TABLE + \"(\" +\n+\t\t\t\t\"id BIGINT,\" +\n+\t\t\t\t\"timestamp6_col TIMESTAMP(6),\" +\n+\t\t\t\t\"time_col TIME,\" +\n+\t\t\t\t\"decimal_col DECIMAL(10, 4)\" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2cc4e7b6e88e9f655235b8c97707c41f45b6b238"}, "originalPosition": 93}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDU0ODEwMA==", "bodyText": "Have add more types for derby. and the combining of source and sink integrate tests will postpone to since the sinks did not support new type system.", "url": "https://github.com/apache/flink/pull/10745#discussion_r374548100", "createdAt": "2020-02-04T09:08:03Z", "author": {"login": "docete"}, "path": "flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceITCase.java", "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.api.java.io.jdbc;\n+\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.table.api.EnvironmentSettings;\n+import org.apache.flink.table.api.java.StreamTableEnvironment;\n+import org.apache.flink.table.runtime.utils.StreamITCase;\n+import org.apache.flink.test.util.AbstractTestBase;\n+import org.apache.flink.types.Row;\n+\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.sql.Connection;\n+import java.sql.DriverManager;\n+import java.sql.SQLException;\n+import java.sql.Statement;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+\n+/**\n+ * ITCase for {@link JDBCTableSource}.\n+ */\n+public class JDBCTableSourceITCase extends AbstractTestBase {\n+\n+\tpublic static final String DRIVER_CLASS = \"org.apache.derby.jdbc.EmbeddedDriver\";\n+\tpublic static final String DB_URL = \"jdbc:derby:memory:test\";\n+\tpublic static final String INPUT_TABLE = \"jdbcSource\";\n+\n+\t@Before\n+\tpublic void before() throws ClassNotFoundException, SQLException {\n+\t\tSystem.setProperty(\"derby.stream.error.field\", JDBCTestBase.class.getCanonicalName() + \".DEV_NULL\");\n+\t\tClass.forName(DRIVER_CLASS);\n+\n+\t\ttry (\n+\t\t\tConnection conn = DriverManager.getConnection(DB_URL + \";create=true\");\n+\t\t\tStatement statement = conn.createStatement()) {\n+\t\t\tstatement.executeUpdate(\"CREATE TABLE \" + INPUT_TABLE + \" (\" +\n+\t\t\t\t\t\"id BIGINT NOT NULL,\" +\n+\t\t\t\t\t\"timestamp6_col TIMESTAMP, \" +\n+\t\t\t\t\t\"time_col TIME, \" +\n+\t\t\t\t\t\"decimal_col DECIMAL(10, 4))\");\n+\t\t\tstatement.executeUpdate(\"INSERT INTO \" + INPUT_TABLE + \" VALUES (\" +\n+\t\t\t\t\t\"1, TIMESTAMP('2020-01-01 15:35:00.123456'), TIME('15:35:00'), 100.1234)\");\n+\t\t\tstatement.executeUpdate(\"INSERT INTO \" + INPUT_TABLE + \" VALUES (\" +\n+\t\t\t\t\t\"2, TIMESTAMP('2020-01-01 15:36:01.123456'), TIME('15:36:01'), 101.1234)\");\n+\t\t}\n+\t}\n+\n+\t@After\n+\tpublic void clearOutputTable() throws Exception {\n+\t\tClass.forName(DRIVER_CLASS);\n+\t\ttry (\n+\t\t\tConnection conn = DriverManager.getConnection(DB_URL);\n+\t\t\tStatement stat = conn.createStatement()) {\n+\t\t\tstat.execute(\"DROP TABLE \" + INPUT_TABLE);\n+\t\t}\n+\t}\n+\n+\t@Test\n+\tpublic void testJDBCSource() throws Exception {\n+\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+\t\tEnvironmentSettings envSettings = EnvironmentSettings.newInstance()\n+\t\t\t.useBlinkPlanner()\n+\t\t\t.inStreamingMode()\n+\t\t\t.build();\n+\t\tStreamTableEnvironment tEnv = StreamTableEnvironment.create(env, envSettings);\n+\n+\t\ttEnv.sqlUpdate(\n+\t\t\t\"CREATE TABLE \" + INPUT_TABLE + \"(\" +\n+\t\t\t\t\"id BIGINT,\" +\n+\t\t\t\t\"timestamp6_col TIMESTAMP(6),\" +\n+\t\t\t\t\"time_col TIME,\" +\n+\t\t\t\t\"decimal_col DECIMAL(10, 4)\" +", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MjY5MzczNw=="}, "originalCommit": {"oid": "2cc4e7b6e88e9f655235b8c97707c41f45b6b238"}, "originalPosition": 93}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMyNjc5MzA0OnYy", "diffSide": "RIGHT", "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCTableSource.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wN1QwMzoxMDowNlrOFmxIrQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wN1QwODozMDoxM1rOFm1gIA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjE5NTI0NQ==", "bodyText": "Can we remove the returnType member field?\nIt's error-prone to maintain two objects. The returnType is only used in getDataStream and can be derived via TypeConversions.fromDataTypeToLegacyInfo(producedDataType).", "url": "https://github.com/apache/flink/pull/10745#discussion_r376195245", "createdAt": "2020-02-07T03:10:06Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCTableSource.java", "diffHunk": "@@ -73,17 +75,23 @@ private JDBCTableSource(\n \t\tthis.selectFields = selectFields;\n \n \t\tfinal TypeInformation<?>[] schemaTypeInfos = schema.getFieldTypes();\n+\t\tfinal DataType[] schemaDataTypes = schema.getFieldDataTypes();\n \t\tfinal String[] schemaFieldNames = schema.getFieldNames();\n \t\tif (selectFields != null) {\n \t\t\tTypeInformation<?>[] typeInfos = new TypeInformation[selectFields.length];\n-\t\t\tString[] typeNames = new String[selectFields.length];\n+\t\t\tDataType[] dataTypes = new DataType[selectFields.length];\n+\t\t\tString[] fieldNames = new String[selectFields.length];\n \t\t\tfor (int i = 0; i < selectFields.length; i++) {\n \t\t\t\ttypeInfos[i] = schemaTypeInfos[selectFields[i]];\n-\t\t\t\ttypeNames[i] = schemaFieldNames[selectFields[i]];\n+\t\t\t\tdataTypes[i] = schemaDataTypes[selectFields[i]];\n+\t\t\t\tfieldNames[i] = schemaFieldNames[selectFields[i]];\n \t\t\t}\n-\t\t\tthis.returnType = new RowTypeInfo(typeInfos, typeNames);\n+\t\t\tthis.returnType = new RowTypeInfo(typeInfos, fieldNames);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f7456ac7e14681e18199941e0962b207eb99bef9"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjI2Njc4NA==", "bodyText": "Make sense.", "url": "https://github.com/apache/flink/pull/10745#discussion_r376266784", "createdAt": "2020-02-07T08:30:13Z", "author": {"login": "docete"}, "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCTableSource.java", "diffHunk": "@@ -73,17 +75,23 @@ private JDBCTableSource(\n \t\tthis.selectFields = selectFields;\n \n \t\tfinal TypeInformation<?>[] schemaTypeInfos = schema.getFieldTypes();\n+\t\tfinal DataType[] schemaDataTypes = schema.getFieldDataTypes();\n \t\tfinal String[] schemaFieldNames = schema.getFieldNames();\n \t\tif (selectFields != null) {\n \t\t\tTypeInformation<?>[] typeInfos = new TypeInformation[selectFields.length];\n-\t\t\tString[] typeNames = new String[selectFields.length];\n+\t\t\tDataType[] dataTypes = new DataType[selectFields.length];\n+\t\t\tString[] fieldNames = new String[selectFields.length];\n \t\t\tfor (int i = 0; i < selectFields.length; i++) {\n \t\t\t\ttypeInfos[i] = schemaTypeInfos[selectFields[i]];\n-\t\t\t\ttypeNames[i] = schemaFieldNames[selectFields[i]];\n+\t\t\t\tdataTypes[i] = schemaDataTypes[selectFields[i]];\n+\t\t\t\tfieldNames[i] = schemaFieldNames[selectFields[i]];\n \t\t\t}\n-\t\t\tthis.returnType = new RowTypeInfo(typeInfos, typeNames);\n+\t\t\tthis.returnType = new RowTypeInfo(typeInfos, fieldNames);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjE5NTI0NQ=="}, "originalCommit": {"oid": "f7456ac7e14681e18199941e0962b207eb99bef9"}, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMyNjc5ODY3OnYy", "diffSide": "RIGHT", "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceSinkFactory.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wN1QwMzoxNDoxM1rOFmxMJQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wN1QwMzoxNDoxM1rOFmxMJQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjE5NjEzMw==", "bodyText": "I think it would be better to move the validation logic into JDBCValidator.\nhttps://github.com/apache/flink/blob/master/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/table/descriptors/JDBCValidator.java#L73", "url": "https://github.com/apache/flink/pull/10745#discussion_r376196133", "createdAt": "2020-02-07T03:14:13Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceSinkFactory.java", "diffHunk": "@@ -130,8 +130,11 @@\n \t\tTableSchema schema = TableSchemaUtils.getPhysicalSchema(\n \t\t\tdescriptorProperties.getTableSchema(SCHEMA));\n \n+\t\tJDBCOptions options = getJDBCOptions(descriptorProperties);\n+\t\toptions.getDialect().validate(schema);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f7456ac7e14681e18199941e0962b207eb99bef9"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMyNjgxMzM0OnYy", "diffSide": "RIGHT", "path": "flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceITCase.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wN1QwMzoyNTowNFrOFmxUyQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQwNToyNToxOVrOFn9vxQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjE5ODM0NQ==", "bodyText": "You can create a unit test JDBCDataTypeTest to verify all the types with different precision with different dialects. This doesn't involve a job submission, and is a lightweight unit test. You can take FlinkDDLDataTypeTest as an example.", "url": "https://github.com/apache/flink/pull/10745#discussion_r376198345", "createdAt": "2020-02-07T03:25:04Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceITCase.java", "diffHunk": "@@ -18,93 +18,167 @@\n \n package org.apache.flink.api.java.io.jdbc;\n \n-import org.apache.flink.streaming.api.datastream.DataStream;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n import org.apache.flink.table.api.EnvironmentSettings;\n-import org.apache.flink.table.api.Table;\n+import org.apache.flink.table.api.TableException;\n import org.apache.flink.table.api.java.StreamTableEnvironment;\n import org.apache.flink.table.runtime.utils.StreamITCase;\n+import org.apache.flink.test.util.AbstractTestBase;\n import org.apache.flink.types.Row;\n \n-import org.junit.BeforeClass;\n+import org.junit.After;\n+import org.junit.Before;\n import org.junit.Test;\n \n-import java.util.ArrayList;\n+import java.sql.Connection;\n+import java.sql.DriverManager;\n+import java.sql.SQLException;\n+import java.sql.Statement;\n+import java.util.Arrays;\n import java.util.List;\n \n+\n /**\n- * IT case for {@link JDBCTableSource}.\n+ * ITCase for {@link JDBCTableSource}.\n  */\n-public class JDBCTableSourceITCase extends JDBCTestBase {\n-\n-\tprivate static final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n-\tprivate static final EnvironmentSettings bsSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();\n-\tprivate static final StreamTableEnvironment tEnv = StreamTableEnvironment.create(env, bsSettings);\n-\n-\tstatic final String TABLE_SOURCE_SQL = \"CREATE TABLE books (\" +\n-\t\t\" id int, \" +\n-\t\t\" title varchar, \" +\n-\t\t\" author varchar, \" +\n-\t\t\" price double, \" +\n-\t\t\" qty int \" +\n-\t\t\") with (\" +\n-\t\t\" 'connector.type' = 'jdbc', \" +\n-\t\t\" 'connector.url' = 'jdbc:derby:memory:ebookshop', \" +\n-\t\t\" 'connector.table' = 'books', \" +\n-\t\t\" 'connector.driver' = 'org.apache.derby.jdbc.EmbeddedDriver' \" +\n-\t\t\")\";\n-\n-\t@BeforeClass\n-\tpublic static void createTable() {\n-\t\ttEnv.sqlUpdate(TABLE_SOURCE_SQL);\n+public class JDBCTableSourceITCase extends AbstractTestBase {\n+\n+\tpublic static final String DRIVER_CLASS = \"org.apache.derby.jdbc.EmbeddedDriver\";\n+\tpublic static final String DB_URL = \"jdbc:derby:memory:test\";\n+\tpublic static final String INPUT_TABLE = \"jdbcSource\";\n+\n+\t@Before\n+\tpublic void before() throws ClassNotFoundException, SQLException {\n+\t\tSystem.setProperty(\"derby.stream.error.field\", JDBCTestBase.class.getCanonicalName() + \".DEV_NULL\");\n+\t\tClass.forName(DRIVER_CLASS);\n+\n+\t\ttry (\n+\t\t\tConnection conn = DriverManager.getConnection(DB_URL + \";create=true\");\n+\t\t\tStatement statement = conn.createStatement()) {\n+\t\t\tstatement.executeUpdate(\"CREATE TABLE \" + INPUT_TABLE + \" (\" +\n+\t\t\t\t\t\"id BIGINT NOT NULL,\" +\n+\t\t\t\t\t\"timestamp6_col TIMESTAMP, \" +\n+\t\t\t\t\t\"timestamp9_col TIMESTAMP, \" +\n+\t\t\t\t\t\"time_col TIME, \" +\n+\t\t\t\t\t\"real_col FLOAT(23), \" +    // A precision of 23 or less makes FLOAT equivalent to REAL.\n+\t\t\t\t\t\"double_col FLOAT(24),\" +   // A precision of 24 or greater makes FLOAT equivalent to DOUBLE PRECISION.\n+\t\t\t\t\t\"decimal_col DECIMAL(10, 4))\");\n+\t\t\tstatement.executeUpdate(\"INSERT INTO \" + INPUT_TABLE + \" VALUES (\" +\n+\t\t\t\t\t\"1, TIMESTAMP('2020-01-01 15:35:00.123456'), TIMESTAMP('2020-01-01 15:35:00.123456789'), \" +\n+\t\t\t\t\t\"TIME('15:35:00'), 1.175E-37, 1.79769E+308, 100.1234)\");\n+\t\t\tstatement.executeUpdate(\"INSERT INTO \" + INPUT_TABLE + \" VALUES (\" +\n+\t\t\t\t\t\"2, TIMESTAMP('2020-01-01 15:36:01.123456'), TIMESTAMP('2020-01-01 15:36:01.123456789'), \" +\n+\t\t\t\t\t\"TIME('15:36:01'), -1.175E-37, -1.79769E+308, 101.1234)\");\n+\t\t}\n+\t}\n+\n+\t@After\n+\tpublic void clearOutputTable() throws Exception {\n+\t\tClass.forName(DRIVER_CLASS);\n+\t\ttry (\n+\t\t\tConnection conn = DriverManager.getConnection(DB_URL);\n+\t\t\tStatement stat = conn.createStatement()) {\n+\t\t\tstat.execute(\"DROP TABLE \" + INPUT_TABLE);\n+\t\t}\n \t}\n \n \t@Test\n-\tpublic void testFieldsProjection() throws Exception {\n-\t\tStreamITCase.clear();\n+\tpublic void testJDBCSource() throws Exception {\n+\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+\t\tEnvironmentSettings envSettings = EnvironmentSettings.newInstance()\n+\t\t\t.useBlinkPlanner()\n+\t\t\t.inStreamingMode()\n+\t\t\t.build();\n+\t\tStreamTableEnvironment tEnv = StreamTableEnvironment.create(env, envSettings);\n+\n+\t\ttEnv.sqlUpdate(\n+\t\t\t\"CREATE TABLE \" + INPUT_TABLE + \"(\" +\n+\t\t\t\t\"id BIGINT,\" +\n+\t\t\t\t\"timestamp6_col TIMESTAMP(6),\" +\n+\t\t\t\t\"timestamp9_col TIMESTAMP(9),\" +\n+\t\t\t\t\"time_col TIME,\" +\n+\t\t\t\t\"real_col FLOAT,\" +\n+\t\t\t\t\"double_col DOUBLE,\" +\n+\t\t\t\t\"decimal_col DECIMAL(10, 4)\" +\n+\t\t\t\t\") WITH (\" +\n+\t\t\t\t\"  'connector.type'='jdbc',\" +\n+\t\t\t\t\"  'connector.url'='\" + DB_URL + \"',\" +\n+\t\t\t\t\"  'connector.table'='\" + INPUT_TABLE + \"'\" +\n+\t\t\t\t\")\"\n+\t\t);\n \n-\t\tTable result = tEnv.sqlQuery(SELECT_ID_BOOKS);\n-\t\tDataStream<Row> resultSet = tEnv.toAppendStream(result, Row.class);\n-\t\tresultSet.addSink(new StreamITCase.StringSink<>());\n+\t\tStreamITCase.clear();\n+\t\ttEnv.toAppendStream(tEnv.sqlQuery(\"SELECT * FROM \" + INPUT_TABLE), Row.class)\n+\t\t\t.addSink(new StreamITCase.StringSink<>());\n \t\tenv.execute();\n \n-\t\tList<String> expected = new ArrayList<>();\n-\t\texpected.add(\"1001\");\n-\t\texpected.add(\"1002\");\n-\t\texpected.add(\"1003\");\n-\t\texpected.add(\"1004\");\n-\t\texpected.add(\"1005\");\n-\t\texpected.add(\"1006\");\n-\t\texpected.add(\"1007\");\n-\t\texpected.add(\"1008\");\n-\t\texpected.add(\"1009\");\n-\t\texpected.add(\"1010\");\n-\n+\t\tList<String> expected =\n+\t\t\tArrays.asList(\n+\t\t\t\t\"1,2020-01-01T15:35:00.123456,2020-01-01T15:35:00.123456789,15:35,1.175E-37,1.79769E308,100.1234\",\n+\t\t\t\t\"2,2020-01-01T15:36:01.123456,2020-01-01T15:36:01.123456789,15:36:01,-1.175E-37,-1.79769E308,101.1234\");\n \t\tStreamITCase.compareWithList(expected);\n \t}\n \n \t@Test\n-\tpublic void testAllFieldsSelection() throws Exception {\n-\t\tStreamITCase.clear();\n+\tpublic void testProjectableJDBCSource() throws Exception {\n+\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+\t\tEnvironmentSettings envSettings = EnvironmentSettings.newInstance()\n+\t\t\t\t.useBlinkPlanner()\n+\t\t\t\t.inStreamingMode()\n+\t\t\t\t.build();\n+\t\tStreamTableEnvironment tEnv = StreamTableEnvironment.create(env, envSettings);\n+\n+\t\ttEnv.sqlUpdate(\n+\t\t\t\"CREATE TABLE \" + INPUT_TABLE + \"(\" +\n+\t\t\t\t\"id BIGINT,\" +\n+\t\t\t\t\"timestamp6_col TIMESTAMP(6),\" +\n+\t\t\t\t\"timestamp9_col TIMESTAMP(9),\" +\n+\t\t\t\t\"time_col TIME,\" +\n+\t\t\t\t\"real_col FLOAT,\" +\n+\t\t\t\t\"decimal_col DECIMAL(10, 4)\" +\n+\t\t\t\t\") WITH (\" +\n+\t\t\t\t\"  'connector.type'='jdbc',\" +\n+\t\t\t\t\"  'connector.url'='\" + DB_URL + \"',\" +\n+\t\t\t\t\"  'connector.table'='\" + INPUT_TABLE + \"'\" +\n+\t\t\t\t\")\"\n+\t\t);\n \n-\t\tTable result = tEnv.sqlQuery(SELECT_ALL_BOOKS);\n-\t\tDataStream<Row> resultSet = tEnv.toAppendStream(result, Row.class);\n-\t\tresultSet.addSink(new StreamITCase.StringSink<>());\n+\t\tStreamITCase.clear();\n+\t\ttEnv.toAppendStream(tEnv.sqlQuery(\"SELECT timestamp6_col, decimal_col FROM \" + INPUT_TABLE), Row.class)\n+\t\t\t\t.addSink(new StreamITCase.StringSink<>());\n \t\tenv.execute();\n \n-\t\tList<String> expected = new ArrayList<>();\n-\t\texpected.add(\"1001,Java public for dummies,Tan Ah Teck,11.11,11\");\n-\t\texpected.add(\"1002,More Java for dummies,Tan Ah Teck,22.22,22\");\n-\t\texpected.add(\"1003,More Java for more dummies,Mohammad Ali,33.33,33\");\n-\t\texpected.add(\"1004,A Cup of Java,Kumar,44.44,44\");\n-\t\texpected.add(\"1005,A Teaspoon of Java,Kevin Jones,55.55,55\");\n-\t\texpected.add(\"1006,A Teaspoon of Java 1.4,Kevin Jones,66.66,66\");\n-\t\texpected.add(\"1007,A Teaspoon of Java 1.5,Kevin Jones,77.77,77\");\n-\t\texpected.add(\"1008,A Teaspoon of Java 1.6,Kevin Jones,88.88,88\");\n-\t\texpected.add(\"1009,A Teaspoon of Java 1.7,Kevin Jones,99.99,99\");\n-\t\texpected.add(\"1010,A Teaspoon of Java 1.8,Kevin Jones,null,1010\");\n-\n+\t\tList<String> expected =\n+\t\t\tArrays.asList(\n+\t\t\t\t\"2020-01-01T15:35:00.123456,100.1234\",\n+\t\t\t\t\"2020-01-01T15:36:01.123456,101.1234\");\n \t\tStreamITCase.compareWithList(expected);\n \t}\n \n+\t@Test(expected = TableException.class)\n+\tpublic void testInvalidPrecisionOfJDBCSource() throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f7456ac7e14681e18199941e0962b207eb99bef9"}, "originalPosition": 203}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ1MDQzNw==", "bodyText": "I think we don't need this test any more, because is already covered by JDBCDataTypeTest.", "url": "https://github.com/apache/flink/pull/10745#discussion_r377450437", "createdAt": "2020-02-11T05:25:19Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceITCase.java", "diffHunk": "@@ -18,93 +18,167 @@\n \n package org.apache.flink.api.java.io.jdbc;\n \n-import org.apache.flink.streaming.api.datastream.DataStream;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n import org.apache.flink.table.api.EnvironmentSettings;\n-import org.apache.flink.table.api.Table;\n+import org.apache.flink.table.api.TableException;\n import org.apache.flink.table.api.java.StreamTableEnvironment;\n import org.apache.flink.table.runtime.utils.StreamITCase;\n+import org.apache.flink.test.util.AbstractTestBase;\n import org.apache.flink.types.Row;\n \n-import org.junit.BeforeClass;\n+import org.junit.After;\n+import org.junit.Before;\n import org.junit.Test;\n \n-import java.util.ArrayList;\n+import java.sql.Connection;\n+import java.sql.DriverManager;\n+import java.sql.SQLException;\n+import java.sql.Statement;\n+import java.util.Arrays;\n import java.util.List;\n \n+\n /**\n- * IT case for {@link JDBCTableSource}.\n+ * ITCase for {@link JDBCTableSource}.\n  */\n-public class JDBCTableSourceITCase extends JDBCTestBase {\n-\n-\tprivate static final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n-\tprivate static final EnvironmentSettings bsSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();\n-\tprivate static final StreamTableEnvironment tEnv = StreamTableEnvironment.create(env, bsSettings);\n-\n-\tstatic final String TABLE_SOURCE_SQL = \"CREATE TABLE books (\" +\n-\t\t\" id int, \" +\n-\t\t\" title varchar, \" +\n-\t\t\" author varchar, \" +\n-\t\t\" price double, \" +\n-\t\t\" qty int \" +\n-\t\t\") with (\" +\n-\t\t\" 'connector.type' = 'jdbc', \" +\n-\t\t\" 'connector.url' = 'jdbc:derby:memory:ebookshop', \" +\n-\t\t\" 'connector.table' = 'books', \" +\n-\t\t\" 'connector.driver' = 'org.apache.derby.jdbc.EmbeddedDriver' \" +\n-\t\t\")\";\n-\n-\t@BeforeClass\n-\tpublic static void createTable() {\n-\t\ttEnv.sqlUpdate(TABLE_SOURCE_SQL);\n+public class JDBCTableSourceITCase extends AbstractTestBase {\n+\n+\tpublic static final String DRIVER_CLASS = \"org.apache.derby.jdbc.EmbeddedDriver\";\n+\tpublic static final String DB_URL = \"jdbc:derby:memory:test\";\n+\tpublic static final String INPUT_TABLE = \"jdbcSource\";\n+\n+\t@Before\n+\tpublic void before() throws ClassNotFoundException, SQLException {\n+\t\tSystem.setProperty(\"derby.stream.error.field\", JDBCTestBase.class.getCanonicalName() + \".DEV_NULL\");\n+\t\tClass.forName(DRIVER_CLASS);\n+\n+\t\ttry (\n+\t\t\tConnection conn = DriverManager.getConnection(DB_URL + \";create=true\");\n+\t\t\tStatement statement = conn.createStatement()) {\n+\t\t\tstatement.executeUpdate(\"CREATE TABLE \" + INPUT_TABLE + \" (\" +\n+\t\t\t\t\t\"id BIGINT NOT NULL,\" +\n+\t\t\t\t\t\"timestamp6_col TIMESTAMP, \" +\n+\t\t\t\t\t\"timestamp9_col TIMESTAMP, \" +\n+\t\t\t\t\t\"time_col TIME, \" +\n+\t\t\t\t\t\"real_col FLOAT(23), \" +    // A precision of 23 or less makes FLOAT equivalent to REAL.\n+\t\t\t\t\t\"double_col FLOAT(24),\" +   // A precision of 24 or greater makes FLOAT equivalent to DOUBLE PRECISION.\n+\t\t\t\t\t\"decimal_col DECIMAL(10, 4))\");\n+\t\t\tstatement.executeUpdate(\"INSERT INTO \" + INPUT_TABLE + \" VALUES (\" +\n+\t\t\t\t\t\"1, TIMESTAMP('2020-01-01 15:35:00.123456'), TIMESTAMP('2020-01-01 15:35:00.123456789'), \" +\n+\t\t\t\t\t\"TIME('15:35:00'), 1.175E-37, 1.79769E+308, 100.1234)\");\n+\t\t\tstatement.executeUpdate(\"INSERT INTO \" + INPUT_TABLE + \" VALUES (\" +\n+\t\t\t\t\t\"2, TIMESTAMP('2020-01-01 15:36:01.123456'), TIMESTAMP('2020-01-01 15:36:01.123456789'), \" +\n+\t\t\t\t\t\"TIME('15:36:01'), -1.175E-37, -1.79769E+308, 101.1234)\");\n+\t\t}\n+\t}\n+\n+\t@After\n+\tpublic void clearOutputTable() throws Exception {\n+\t\tClass.forName(DRIVER_CLASS);\n+\t\ttry (\n+\t\t\tConnection conn = DriverManager.getConnection(DB_URL);\n+\t\t\tStatement stat = conn.createStatement()) {\n+\t\t\tstat.execute(\"DROP TABLE \" + INPUT_TABLE);\n+\t\t}\n \t}\n \n \t@Test\n-\tpublic void testFieldsProjection() throws Exception {\n-\t\tStreamITCase.clear();\n+\tpublic void testJDBCSource() throws Exception {\n+\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+\t\tEnvironmentSettings envSettings = EnvironmentSettings.newInstance()\n+\t\t\t.useBlinkPlanner()\n+\t\t\t.inStreamingMode()\n+\t\t\t.build();\n+\t\tStreamTableEnvironment tEnv = StreamTableEnvironment.create(env, envSettings);\n+\n+\t\ttEnv.sqlUpdate(\n+\t\t\t\"CREATE TABLE \" + INPUT_TABLE + \"(\" +\n+\t\t\t\t\"id BIGINT,\" +\n+\t\t\t\t\"timestamp6_col TIMESTAMP(6),\" +\n+\t\t\t\t\"timestamp9_col TIMESTAMP(9),\" +\n+\t\t\t\t\"time_col TIME,\" +\n+\t\t\t\t\"real_col FLOAT,\" +\n+\t\t\t\t\"double_col DOUBLE,\" +\n+\t\t\t\t\"decimal_col DECIMAL(10, 4)\" +\n+\t\t\t\t\") WITH (\" +\n+\t\t\t\t\"  'connector.type'='jdbc',\" +\n+\t\t\t\t\"  'connector.url'='\" + DB_URL + \"',\" +\n+\t\t\t\t\"  'connector.table'='\" + INPUT_TABLE + \"'\" +\n+\t\t\t\t\")\"\n+\t\t);\n \n-\t\tTable result = tEnv.sqlQuery(SELECT_ID_BOOKS);\n-\t\tDataStream<Row> resultSet = tEnv.toAppendStream(result, Row.class);\n-\t\tresultSet.addSink(new StreamITCase.StringSink<>());\n+\t\tStreamITCase.clear();\n+\t\ttEnv.toAppendStream(tEnv.sqlQuery(\"SELECT * FROM \" + INPUT_TABLE), Row.class)\n+\t\t\t.addSink(new StreamITCase.StringSink<>());\n \t\tenv.execute();\n \n-\t\tList<String> expected = new ArrayList<>();\n-\t\texpected.add(\"1001\");\n-\t\texpected.add(\"1002\");\n-\t\texpected.add(\"1003\");\n-\t\texpected.add(\"1004\");\n-\t\texpected.add(\"1005\");\n-\t\texpected.add(\"1006\");\n-\t\texpected.add(\"1007\");\n-\t\texpected.add(\"1008\");\n-\t\texpected.add(\"1009\");\n-\t\texpected.add(\"1010\");\n-\n+\t\tList<String> expected =\n+\t\t\tArrays.asList(\n+\t\t\t\t\"1,2020-01-01T15:35:00.123456,2020-01-01T15:35:00.123456789,15:35,1.175E-37,1.79769E308,100.1234\",\n+\t\t\t\t\"2,2020-01-01T15:36:01.123456,2020-01-01T15:36:01.123456789,15:36:01,-1.175E-37,-1.79769E308,101.1234\");\n \t\tStreamITCase.compareWithList(expected);\n \t}\n \n \t@Test\n-\tpublic void testAllFieldsSelection() throws Exception {\n-\t\tStreamITCase.clear();\n+\tpublic void testProjectableJDBCSource() throws Exception {\n+\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+\t\tEnvironmentSettings envSettings = EnvironmentSettings.newInstance()\n+\t\t\t\t.useBlinkPlanner()\n+\t\t\t\t.inStreamingMode()\n+\t\t\t\t.build();\n+\t\tStreamTableEnvironment tEnv = StreamTableEnvironment.create(env, envSettings);\n+\n+\t\ttEnv.sqlUpdate(\n+\t\t\t\"CREATE TABLE \" + INPUT_TABLE + \"(\" +\n+\t\t\t\t\"id BIGINT,\" +\n+\t\t\t\t\"timestamp6_col TIMESTAMP(6),\" +\n+\t\t\t\t\"timestamp9_col TIMESTAMP(9),\" +\n+\t\t\t\t\"time_col TIME,\" +\n+\t\t\t\t\"real_col FLOAT,\" +\n+\t\t\t\t\"decimal_col DECIMAL(10, 4)\" +\n+\t\t\t\t\") WITH (\" +\n+\t\t\t\t\"  'connector.type'='jdbc',\" +\n+\t\t\t\t\"  'connector.url'='\" + DB_URL + \"',\" +\n+\t\t\t\t\"  'connector.table'='\" + INPUT_TABLE + \"'\" +\n+\t\t\t\t\")\"\n+\t\t);\n \n-\t\tTable result = tEnv.sqlQuery(SELECT_ALL_BOOKS);\n-\t\tDataStream<Row> resultSet = tEnv.toAppendStream(result, Row.class);\n-\t\tresultSet.addSink(new StreamITCase.StringSink<>());\n+\t\tStreamITCase.clear();\n+\t\ttEnv.toAppendStream(tEnv.sqlQuery(\"SELECT timestamp6_col, decimal_col FROM \" + INPUT_TABLE), Row.class)\n+\t\t\t\t.addSink(new StreamITCase.StringSink<>());\n \t\tenv.execute();\n \n-\t\tList<String> expected = new ArrayList<>();\n-\t\texpected.add(\"1001,Java public for dummies,Tan Ah Teck,11.11,11\");\n-\t\texpected.add(\"1002,More Java for dummies,Tan Ah Teck,22.22,22\");\n-\t\texpected.add(\"1003,More Java for more dummies,Mohammad Ali,33.33,33\");\n-\t\texpected.add(\"1004,A Cup of Java,Kumar,44.44,44\");\n-\t\texpected.add(\"1005,A Teaspoon of Java,Kevin Jones,55.55,55\");\n-\t\texpected.add(\"1006,A Teaspoon of Java 1.4,Kevin Jones,66.66,66\");\n-\t\texpected.add(\"1007,A Teaspoon of Java 1.5,Kevin Jones,77.77,77\");\n-\t\texpected.add(\"1008,A Teaspoon of Java 1.6,Kevin Jones,88.88,88\");\n-\t\texpected.add(\"1009,A Teaspoon of Java 1.7,Kevin Jones,99.99,99\");\n-\t\texpected.add(\"1010,A Teaspoon of Java 1.8,Kevin Jones,null,1010\");\n-\n+\t\tList<String> expected =\n+\t\t\tArrays.asList(\n+\t\t\t\t\"2020-01-01T15:35:00.123456,100.1234\",\n+\t\t\t\t\"2020-01-01T15:36:01.123456,101.1234\");\n \t\tStreamITCase.compareWithList(expected);\n \t}\n \n+\t@Test(expected = TableException.class)\n+\tpublic void testInvalidPrecisionOfJDBCSource() throws Exception {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjE5ODM0NQ=="}, "originalCommit": {"oid": "f7456ac7e14681e18199941e0962b207eb99bef9"}, "originalPosition": 203}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMyNjgxODQwOnYy", "diffSide": "RIGHT", "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialect.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wN1QwMzoyODowM1rOFmxXnQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wOFQwMjozNDowNlrOFnOwnw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjE5OTA2OQ==", "bodyText": "throws ValidationException on the method signature. And please add a description about the exception in the javadoc.", "url": "https://github.com/apache/flink/pull/10745#discussion_r376199069", "createdAt": "2020-02-07T03:28:03Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialect.java", "diffHunk": "@@ -35,6 +37,15 @@\n \t */\n \tboolean canHandle(String url);\n \n+\t/**\n+\t * Check if this dialect instance support a specific data type in table schema.\n+\t *\n+\t * @param schema the table schema\n+\t */\n+\tdefault void validate(TableSchema schema) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f7456ac7e14681e18199941e0962b207eb99bef9"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjY4MDYwNw==", "bodyText": "ValidationException is a RuntimeException, adding it to method signature won' t force caller the check it. Maybe we just need to add it in java doc like LogicalTypeParser.parse and FieldInfoUtils.validateInputTypeInfo ?", "url": "https://github.com/apache/flink/pull/10745#discussion_r376680607", "createdAt": "2020-02-08T02:34:06Z", "author": {"login": "libenchao"}, "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialect.java", "diffHunk": "@@ -35,6 +37,15 @@\n \t */\n \tboolean canHandle(String url);\n \n+\t/**\n+\t * Check if this dialect instance support a specific data type in table schema.\n+\t *\n+\t * @param schema the table schema\n+\t */\n+\tdefault void validate(TableSchema schema) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjE5OTA2OQ=="}, "originalCommit": {"oid": "f7456ac7e14681e18199941e0962b207eb99bef9"}, "originalPosition": 18}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMyNjgxODU5OnYy", "diffSide": "RIGHT", "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialect.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wN1QwMzoyODoxMFrOFmxXug==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wN1QwMzoyODoxMFrOFmxXug==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjE5OTA5OA==", "bodyText": "remove emtpy line.", "url": "https://github.com/apache/flink/pull/10745#discussion_r376199098", "createdAt": "2020-02-07T03:28:10Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialect.java", "diffHunk": "@@ -35,6 +37,15 @@\n \t */\n \tboolean canHandle(String url);\n \n+\t/**\n+\t * Check if this dialect instance support a specific data type in table schema.\n+\t *", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f7456ac7e14681e18199941e0962b207eb99bef9"}, "originalPosition": 15}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMyNjg0MDg0OnYy", "diffSide": "RIGHT", "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialect.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wN1QwMzo0NjoyNFrOFmxlCQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wN1QwMzo0NjoyNFrOFmxlCQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjIwMjUwNQ==", "bodyText": "We don't need to return if the return type is void.", "url": "https://github.com/apache/flink/pull/10745#discussion_r376202505", "createdAt": "2020-02-07T03:46:24Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialect.java", "diffHunk": "@@ -35,6 +37,15 @@\n \t */\n \tboolean canHandle(String url);\n \n+\t/**\n+\t * Check if this dialect instance support a specific data type in table schema.\n+\t *\n+\t * @param schema the table schema\n+\t */\n+\tdefault void validate(TableSchema schema) {\n+\t\treturn;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f7456ac7e14681e18199941e0962b207eb99bef9"}, "originalPosition": 19}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMyNjg0OTA1OnYy", "diffSide": "RIGHT", "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialects.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wN1QwMzo1MjozMFrOFmxpuQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wN1QwMzo1MjozMFrOFmxpuQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjIwMzcwNQ==", "bodyText": "It seems that the validation logic is the same, maybe we can refactor it a bit more to have a AbstractJDBCDialect which implements JDBCDialect.\n\tprivate abstract static class AbstractDialect implements JDBCDialect {\n\n\t\t@Override\n\t\tpublic void validate(TableSchema schema) throws ValidationException {\n\t\t\t// implement the common validation logic here\n\t\t}\n\t\t\n\t\tpublic abstract int maxDecimalPrecision();\n\t\t\n\t\tpublic abstract int minDecimalPrecision();\n\t\t\n\t\tpublic abstract int maxTimestampPrecision();\n\t\t\n\t\tpublic abstract int minTimestampPrecision();\n\t\t\n\t\tpublic abstract List<LogicalTypeRoot> unsupportedTypes();\n\t}", "url": "https://github.com/apache/flink/pull/10745#discussion_r376203705", "createdAt": "2020-02-07T03:52:30Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialects.java", "diffHunk": "@@ -70,11 +105,33 @@ public String quoteIdentifier(String identifier) {\n \n \t\tprivate static final long serialVersionUID = 1L;\n \n+\t\tprivate static final int MAX_MYSQL_TIMESTAMP_PRECISION = 6;\n+\n+\t\tprivate static final int MIN_MYSQL_TIMESTAMP_PRECISION = 0;\n+\n \t\t@Override\n \t\tpublic boolean canHandle(String url) {\n \t\t\treturn url.startsWith(\"jdbc:mysql:\");\n \t\t}\n \n+\t\t@Override\n+\t\tpublic void validate(TableSchema schema) {\n+\t\t\tfor (int i = 0; i < schema.getFieldCount(); i++) {\n+\t\t\t\tDataType dt = schema.getFieldDataType(i).get();\n+\t\t\t\tString fieldName = schema.getFieldName(i).get();\n+\t\t\t\tif (TIMESTAMP_WITHOUT_TIME_ZONE == dt.getLogicalType().getTypeRoot()) {\n+\t\t\t\t\tint precision = ((TimestampType) dt.getLogicalType()).getPrecision();\n+\t\t\t\t\tif (precision > MAX_MYSQL_TIMESTAMP_PRECISION) {\n+\t\t\t\t\t\tthrow new ValidationException(\n+\t\t\t\t\t\t\t\tString.format(\"The precision of %s is out of range [%d, %d].\",\n+\t\t\t\t\t\t\t\t\t\tfieldName,\n+\t\t\t\t\t\t\t\t\t\tMIN_MYSQL_TIMESTAMP_PRECISION,\n+\t\t\t\t\t\t\t\t\t\tMAX_MYSQL_TIMESTAMP_PRECISION));\n+\t\t\t\t\t}\n+\t\t\t\t}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f7456ac7e14681e18199941e0962b207eb99bef9"}, "originalPosition": 86}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMyNjg1MTk5OnYy", "diffSide": "RIGHT", "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialects.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wN1QwMzo1NToxNlrOFmxreg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wN1QwMzo1NToxNlrOFmxreg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjIwNDE1NA==", "bodyText": "Could you add a comment above these constants that includes documentation link describes the precision? So that we can have the single truth.", "url": "https://github.com/apache/flink/pull/10745#discussion_r376204154", "createdAt": "2020-02-07T03:55:16Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialects.java", "diffHunk": "@@ -50,11 +60,36 @@\n \n \t\tprivate static final long serialVersionUID = 1L;\n \n+\t\tprivate static final int MAX_DERBY_DECIMAL_PRECISION = 31;\n+\n+\t\tprivate static final int MIN_DERBY_DECIMAL_PRECISION = 1;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f7456ac7e14681e18199941e0962b207eb99bef9"}, "originalPosition": 28}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMyNjg2MTAzOnYy", "diffSide": "RIGHT", "path": "flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceITCase.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wN1QwNDowMzowNFrOFmxwwQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMFQwNzowMjozNlrOFnbuqg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjIwNTUwNQ==", "bodyText": "Should we use stat.executeUdpate?", "url": "https://github.com/apache/flink/pull/10745#discussion_r376205505", "createdAt": "2020-02-07T04:03:04Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceITCase.java", "diffHunk": "@@ -18,93 +18,167 @@\n \n package org.apache.flink.api.java.io.jdbc;\n \n-import org.apache.flink.streaming.api.datastream.DataStream;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n import org.apache.flink.table.api.EnvironmentSettings;\n-import org.apache.flink.table.api.Table;\n+import org.apache.flink.table.api.TableException;\n import org.apache.flink.table.api.java.StreamTableEnvironment;\n import org.apache.flink.table.runtime.utils.StreamITCase;\n+import org.apache.flink.test.util.AbstractTestBase;\n import org.apache.flink.types.Row;\n \n-import org.junit.BeforeClass;\n+import org.junit.After;\n+import org.junit.Before;\n import org.junit.Test;\n \n-import java.util.ArrayList;\n+import java.sql.Connection;\n+import java.sql.DriverManager;\n+import java.sql.SQLException;\n+import java.sql.Statement;\n+import java.util.Arrays;\n import java.util.List;\n \n+\n /**\n- * IT case for {@link JDBCTableSource}.\n+ * ITCase for {@link JDBCTableSource}.\n  */\n-public class JDBCTableSourceITCase extends JDBCTestBase {\n-\n-\tprivate static final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n-\tprivate static final EnvironmentSettings bsSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();\n-\tprivate static final StreamTableEnvironment tEnv = StreamTableEnvironment.create(env, bsSettings);\n-\n-\tstatic final String TABLE_SOURCE_SQL = \"CREATE TABLE books (\" +\n-\t\t\" id int, \" +\n-\t\t\" title varchar, \" +\n-\t\t\" author varchar, \" +\n-\t\t\" price double, \" +\n-\t\t\" qty int \" +\n-\t\t\") with (\" +\n-\t\t\" 'connector.type' = 'jdbc', \" +\n-\t\t\" 'connector.url' = 'jdbc:derby:memory:ebookshop', \" +\n-\t\t\" 'connector.table' = 'books', \" +\n-\t\t\" 'connector.driver' = 'org.apache.derby.jdbc.EmbeddedDriver' \" +\n-\t\t\")\";\n-\n-\t@BeforeClass\n-\tpublic static void createTable() {\n-\t\ttEnv.sqlUpdate(TABLE_SOURCE_SQL);\n+public class JDBCTableSourceITCase extends AbstractTestBase {\n+\n+\tpublic static final String DRIVER_CLASS = \"org.apache.derby.jdbc.EmbeddedDriver\";\n+\tpublic static final String DB_URL = \"jdbc:derby:memory:test\";\n+\tpublic static final String INPUT_TABLE = \"jdbcSource\";\n+\n+\t@Before\n+\tpublic void before() throws ClassNotFoundException, SQLException {\n+\t\tSystem.setProperty(\"derby.stream.error.field\", JDBCTestBase.class.getCanonicalName() + \".DEV_NULL\");\n+\t\tClass.forName(DRIVER_CLASS);\n+\n+\t\ttry (\n+\t\t\tConnection conn = DriverManager.getConnection(DB_URL + \";create=true\");\n+\t\t\tStatement statement = conn.createStatement()) {\n+\t\t\tstatement.executeUpdate(\"CREATE TABLE \" + INPUT_TABLE + \" (\" +\n+\t\t\t\t\t\"id BIGINT NOT NULL,\" +\n+\t\t\t\t\t\"timestamp6_col TIMESTAMP, \" +\n+\t\t\t\t\t\"timestamp9_col TIMESTAMP, \" +\n+\t\t\t\t\t\"time_col TIME, \" +\n+\t\t\t\t\t\"real_col FLOAT(23), \" +    // A precision of 23 or less makes FLOAT equivalent to REAL.\n+\t\t\t\t\t\"double_col FLOAT(24),\" +   // A precision of 24 or greater makes FLOAT equivalent to DOUBLE PRECISION.\n+\t\t\t\t\t\"decimal_col DECIMAL(10, 4))\");\n+\t\t\tstatement.executeUpdate(\"INSERT INTO \" + INPUT_TABLE + \" VALUES (\" +\n+\t\t\t\t\t\"1, TIMESTAMP('2020-01-01 15:35:00.123456'), TIMESTAMP('2020-01-01 15:35:00.123456789'), \" +\n+\t\t\t\t\t\"TIME('15:35:00'), 1.175E-37, 1.79769E+308, 100.1234)\");\n+\t\t\tstatement.executeUpdate(\"INSERT INTO \" + INPUT_TABLE + \" VALUES (\" +\n+\t\t\t\t\t\"2, TIMESTAMP('2020-01-01 15:36:01.123456'), TIMESTAMP('2020-01-01 15:36:01.123456789'), \" +\n+\t\t\t\t\t\"TIME('15:36:01'), -1.175E-37, -1.79769E+308, 101.1234)\");\n+\t\t}\n+\t}\n+\n+\t@After\n+\tpublic void clearOutputTable() throws Exception {\n+\t\tClass.forName(DRIVER_CLASS);\n+\t\ttry (\n+\t\t\tConnection conn = DriverManager.getConnection(DB_URL);\n+\t\t\tStatement stat = conn.createStatement()) {\n+\t\t\tstat.execute(\"DROP TABLE \" + INPUT_TABLE);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f7456ac7e14681e18199941e0962b207eb99bef9"}, "originalPosition": 91}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg5MzA5OA==", "bodyText": "execute can be used for any SQL statement, but executeUpdate is more clearly here. I will fix this.", "url": "https://github.com/apache/flink/pull/10745#discussion_r376893098", "createdAt": "2020-02-10T07:02:36Z", "author": {"login": "docete"}, "path": "flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceITCase.java", "diffHunk": "@@ -18,93 +18,167 @@\n \n package org.apache.flink.api.java.io.jdbc;\n \n-import org.apache.flink.streaming.api.datastream.DataStream;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n import org.apache.flink.table.api.EnvironmentSettings;\n-import org.apache.flink.table.api.Table;\n+import org.apache.flink.table.api.TableException;\n import org.apache.flink.table.api.java.StreamTableEnvironment;\n import org.apache.flink.table.runtime.utils.StreamITCase;\n+import org.apache.flink.test.util.AbstractTestBase;\n import org.apache.flink.types.Row;\n \n-import org.junit.BeforeClass;\n+import org.junit.After;\n+import org.junit.Before;\n import org.junit.Test;\n \n-import java.util.ArrayList;\n+import java.sql.Connection;\n+import java.sql.DriverManager;\n+import java.sql.SQLException;\n+import java.sql.Statement;\n+import java.util.Arrays;\n import java.util.List;\n \n+\n /**\n- * IT case for {@link JDBCTableSource}.\n+ * ITCase for {@link JDBCTableSource}.\n  */\n-public class JDBCTableSourceITCase extends JDBCTestBase {\n-\n-\tprivate static final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n-\tprivate static final EnvironmentSettings bsSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();\n-\tprivate static final StreamTableEnvironment tEnv = StreamTableEnvironment.create(env, bsSettings);\n-\n-\tstatic final String TABLE_SOURCE_SQL = \"CREATE TABLE books (\" +\n-\t\t\" id int, \" +\n-\t\t\" title varchar, \" +\n-\t\t\" author varchar, \" +\n-\t\t\" price double, \" +\n-\t\t\" qty int \" +\n-\t\t\") with (\" +\n-\t\t\" 'connector.type' = 'jdbc', \" +\n-\t\t\" 'connector.url' = 'jdbc:derby:memory:ebookshop', \" +\n-\t\t\" 'connector.table' = 'books', \" +\n-\t\t\" 'connector.driver' = 'org.apache.derby.jdbc.EmbeddedDriver' \" +\n-\t\t\")\";\n-\n-\t@BeforeClass\n-\tpublic static void createTable() {\n-\t\ttEnv.sqlUpdate(TABLE_SOURCE_SQL);\n+public class JDBCTableSourceITCase extends AbstractTestBase {\n+\n+\tpublic static final String DRIVER_CLASS = \"org.apache.derby.jdbc.EmbeddedDriver\";\n+\tpublic static final String DB_URL = \"jdbc:derby:memory:test\";\n+\tpublic static final String INPUT_TABLE = \"jdbcSource\";\n+\n+\t@Before\n+\tpublic void before() throws ClassNotFoundException, SQLException {\n+\t\tSystem.setProperty(\"derby.stream.error.field\", JDBCTestBase.class.getCanonicalName() + \".DEV_NULL\");\n+\t\tClass.forName(DRIVER_CLASS);\n+\n+\t\ttry (\n+\t\t\tConnection conn = DriverManager.getConnection(DB_URL + \";create=true\");\n+\t\t\tStatement statement = conn.createStatement()) {\n+\t\t\tstatement.executeUpdate(\"CREATE TABLE \" + INPUT_TABLE + \" (\" +\n+\t\t\t\t\t\"id BIGINT NOT NULL,\" +\n+\t\t\t\t\t\"timestamp6_col TIMESTAMP, \" +\n+\t\t\t\t\t\"timestamp9_col TIMESTAMP, \" +\n+\t\t\t\t\t\"time_col TIME, \" +\n+\t\t\t\t\t\"real_col FLOAT(23), \" +    // A precision of 23 or less makes FLOAT equivalent to REAL.\n+\t\t\t\t\t\"double_col FLOAT(24),\" +   // A precision of 24 or greater makes FLOAT equivalent to DOUBLE PRECISION.\n+\t\t\t\t\t\"decimal_col DECIMAL(10, 4))\");\n+\t\t\tstatement.executeUpdate(\"INSERT INTO \" + INPUT_TABLE + \" VALUES (\" +\n+\t\t\t\t\t\"1, TIMESTAMP('2020-01-01 15:35:00.123456'), TIMESTAMP('2020-01-01 15:35:00.123456789'), \" +\n+\t\t\t\t\t\"TIME('15:35:00'), 1.175E-37, 1.79769E+308, 100.1234)\");\n+\t\t\tstatement.executeUpdate(\"INSERT INTO \" + INPUT_TABLE + \" VALUES (\" +\n+\t\t\t\t\t\"2, TIMESTAMP('2020-01-01 15:36:01.123456'), TIMESTAMP('2020-01-01 15:36:01.123456789'), \" +\n+\t\t\t\t\t\"TIME('15:36:01'), -1.175E-37, -1.79769E+308, 101.1234)\");\n+\t\t}\n+\t}\n+\n+\t@After\n+\tpublic void clearOutputTable() throws Exception {\n+\t\tClass.forName(DRIVER_CLASS);\n+\t\ttry (\n+\t\t\tConnection conn = DriverManager.getConnection(DB_URL);\n+\t\t\tStatement stat = conn.createStatement()) {\n+\t\t\tstat.execute(\"DROP TABLE \" + INPUT_TABLE);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjIwNTUwNQ=="}, "originalCommit": {"oid": "f7456ac7e14681e18199941e0962b207eb99bef9"}, "originalPosition": 91}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMyNjg2Njk5OnYy", "diffSide": "RIGHT", "path": "flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceITCase.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wN1QwNDowODo0N1rOFmx0Zg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMFQwNzowNzowNVrOFnby2Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjIwNjQzOA==", "bodyText": "Do we really need this? Is there any error messages thrown when run these tests?", "url": "https://github.com/apache/flink/pull/10745#discussion_r376206438", "createdAt": "2020-02-07T04:08:47Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceITCase.java", "diffHunk": "@@ -18,93 +18,167 @@\n \n package org.apache.flink.api.java.io.jdbc;\n \n-import org.apache.flink.streaming.api.datastream.DataStream;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n import org.apache.flink.table.api.EnvironmentSettings;\n-import org.apache.flink.table.api.Table;\n+import org.apache.flink.table.api.TableException;\n import org.apache.flink.table.api.java.StreamTableEnvironment;\n import org.apache.flink.table.runtime.utils.StreamITCase;\n+import org.apache.flink.test.util.AbstractTestBase;\n import org.apache.flink.types.Row;\n \n-import org.junit.BeforeClass;\n+import org.junit.After;\n+import org.junit.Before;\n import org.junit.Test;\n \n-import java.util.ArrayList;\n+import java.sql.Connection;\n+import java.sql.DriverManager;\n+import java.sql.SQLException;\n+import java.sql.Statement;\n+import java.util.Arrays;\n import java.util.List;\n \n+\n /**\n- * IT case for {@link JDBCTableSource}.\n+ * ITCase for {@link JDBCTableSource}.\n  */\n-public class JDBCTableSourceITCase extends JDBCTestBase {\n-\n-\tprivate static final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n-\tprivate static final EnvironmentSettings bsSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();\n-\tprivate static final StreamTableEnvironment tEnv = StreamTableEnvironment.create(env, bsSettings);\n-\n-\tstatic final String TABLE_SOURCE_SQL = \"CREATE TABLE books (\" +\n-\t\t\" id int, \" +\n-\t\t\" title varchar, \" +\n-\t\t\" author varchar, \" +\n-\t\t\" price double, \" +\n-\t\t\" qty int \" +\n-\t\t\") with (\" +\n-\t\t\" 'connector.type' = 'jdbc', \" +\n-\t\t\" 'connector.url' = 'jdbc:derby:memory:ebookshop', \" +\n-\t\t\" 'connector.table' = 'books', \" +\n-\t\t\" 'connector.driver' = 'org.apache.derby.jdbc.EmbeddedDriver' \" +\n-\t\t\")\";\n-\n-\t@BeforeClass\n-\tpublic static void createTable() {\n-\t\ttEnv.sqlUpdate(TABLE_SOURCE_SQL);\n+public class JDBCTableSourceITCase extends AbstractTestBase {\n+\n+\tpublic static final String DRIVER_CLASS = \"org.apache.derby.jdbc.EmbeddedDriver\";\n+\tpublic static final String DB_URL = \"jdbc:derby:memory:test\";\n+\tpublic static final String INPUT_TABLE = \"jdbcSource\";\n+\n+\t@Before\n+\tpublic void before() throws ClassNotFoundException, SQLException {\n+\t\tSystem.setProperty(\"derby.stream.error.field\", JDBCTestBase.class.getCanonicalName() + \".DEV_NULL\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f7456ac7e14681e18199941e0962b207eb99bef9"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg5NDE2OQ==", "bodyText": "We add this to get rid of derby.log or the UT will end up with the derby.log file in the root of the project (flink-jdbc).", "url": "https://github.com/apache/flink/pull/10745#discussion_r376894169", "createdAt": "2020-02-10T07:07:05Z", "author": {"login": "docete"}, "path": "flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceITCase.java", "diffHunk": "@@ -18,93 +18,167 @@\n \n package org.apache.flink.api.java.io.jdbc;\n \n-import org.apache.flink.streaming.api.datastream.DataStream;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n import org.apache.flink.table.api.EnvironmentSettings;\n-import org.apache.flink.table.api.Table;\n+import org.apache.flink.table.api.TableException;\n import org.apache.flink.table.api.java.StreamTableEnvironment;\n import org.apache.flink.table.runtime.utils.StreamITCase;\n+import org.apache.flink.test.util.AbstractTestBase;\n import org.apache.flink.types.Row;\n \n-import org.junit.BeforeClass;\n+import org.junit.After;\n+import org.junit.Before;\n import org.junit.Test;\n \n-import java.util.ArrayList;\n+import java.sql.Connection;\n+import java.sql.DriverManager;\n+import java.sql.SQLException;\n+import java.sql.Statement;\n+import java.util.Arrays;\n import java.util.List;\n \n+\n /**\n- * IT case for {@link JDBCTableSource}.\n+ * ITCase for {@link JDBCTableSource}.\n  */\n-public class JDBCTableSourceITCase extends JDBCTestBase {\n-\n-\tprivate static final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n-\tprivate static final EnvironmentSettings bsSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();\n-\tprivate static final StreamTableEnvironment tEnv = StreamTableEnvironment.create(env, bsSettings);\n-\n-\tstatic final String TABLE_SOURCE_SQL = \"CREATE TABLE books (\" +\n-\t\t\" id int, \" +\n-\t\t\" title varchar, \" +\n-\t\t\" author varchar, \" +\n-\t\t\" price double, \" +\n-\t\t\" qty int \" +\n-\t\t\") with (\" +\n-\t\t\" 'connector.type' = 'jdbc', \" +\n-\t\t\" 'connector.url' = 'jdbc:derby:memory:ebookshop', \" +\n-\t\t\" 'connector.table' = 'books', \" +\n-\t\t\" 'connector.driver' = 'org.apache.derby.jdbc.EmbeddedDriver' \" +\n-\t\t\")\";\n-\n-\t@BeforeClass\n-\tpublic static void createTable() {\n-\t\ttEnv.sqlUpdate(TABLE_SOURCE_SQL);\n+public class JDBCTableSourceITCase extends AbstractTestBase {\n+\n+\tpublic static final String DRIVER_CLASS = \"org.apache.derby.jdbc.EmbeddedDriver\";\n+\tpublic static final String DB_URL = \"jdbc:derby:memory:test\";\n+\tpublic static final String INPUT_TABLE = \"jdbcSource\";\n+\n+\t@Before\n+\tpublic void before() throws ClassNotFoundException, SQLException {\n+\t\tSystem.setProperty(\"derby.stream.error.field\", JDBCTestBase.class.getCanonicalName() + \".DEV_NULL\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjIwNjQzOA=="}, "originalCommit": {"oid": "f7456ac7e14681e18199941e0962b207eb99bef9"}, "originalPosition": 62}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMyOTkwNTYwOnYy", "diffSide": "RIGHT", "path": "flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceITCase.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wOFQwMjozOTo0MVrOFnOx_w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMFQwNzoyMDo0MlrOFncArA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjY4MDk1OQ==", "bodyText": "seems indent is not correct.", "url": "https://github.com/apache/flink/pull/10745#discussion_r376680959", "createdAt": "2020-02-08T02:39:41Z", "author": {"login": "libenchao"}, "path": "flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceITCase.java", "diffHunk": "@@ -18,93 +18,167 @@\n \n package org.apache.flink.api.java.io.jdbc;\n \n-import org.apache.flink.streaming.api.datastream.DataStream;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n import org.apache.flink.table.api.EnvironmentSettings;\n-import org.apache.flink.table.api.Table;\n+import org.apache.flink.table.api.TableException;\n import org.apache.flink.table.api.java.StreamTableEnvironment;\n import org.apache.flink.table.runtime.utils.StreamITCase;\n+import org.apache.flink.test.util.AbstractTestBase;\n import org.apache.flink.types.Row;\n \n-import org.junit.BeforeClass;\n+import org.junit.After;\n+import org.junit.Before;\n import org.junit.Test;\n \n-import java.util.ArrayList;\n+import java.sql.Connection;\n+import java.sql.DriverManager;\n+import java.sql.SQLException;\n+import java.sql.Statement;\n+import java.util.Arrays;\n import java.util.List;\n \n+\n /**\n- * IT case for {@link JDBCTableSource}.\n+ * ITCase for {@link JDBCTableSource}.\n  */\n-public class JDBCTableSourceITCase extends JDBCTestBase {\n-\n-\tprivate static final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n-\tprivate static final EnvironmentSettings bsSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();\n-\tprivate static final StreamTableEnvironment tEnv = StreamTableEnvironment.create(env, bsSettings);\n-\n-\tstatic final String TABLE_SOURCE_SQL = \"CREATE TABLE books (\" +\n-\t\t\" id int, \" +\n-\t\t\" title varchar, \" +\n-\t\t\" author varchar, \" +\n-\t\t\" price double, \" +\n-\t\t\" qty int \" +\n-\t\t\") with (\" +\n-\t\t\" 'connector.type' = 'jdbc', \" +\n-\t\t\" 'connector.url' = 'jdbc:derby:memory:ebookshop', \" +\n-\t\t\" 'connector.table' = 'books', \" +\n-\t\t\" 'connector.driver' = 'org.apache.derby.jdbc.EmbeddedDriver' \" +\n-\t\t\")\";\n-\n-\t@BeforeClass\n-\tpublic static void createTable() {\n-\t\ttEnv.sqlUpdate(TABLE_SOURCE_SQL);\n+public class JDBCTableSourceITCase extends AbstractTestBase {\n+\n+\tpublic static final String DRIVER_CLASS = \"org.apache.derby.jdbc.EmbeddedDriver\";\n+\tpublic static final String DB_URL = \"jdbc:derby:memory:test\";\n+\tpublic static final String INPUT_TABLE = \"jdbcSource\";\n+\n+\t@Before\n+\tpublic void before() throws ClassNotFoundException, SQLException {\n+\t\tSystem.setProperty(\"derby.stream.error.field\", JDBCTestBase.class.getCanonicalName() + \".DEV_NULL\");\n+\t\tClass.forName(DRIVER_CLASS);\n+\n+\t\ttry (\n+\t\t\tConnection conn = DriverManager.getConnection(DB_URL + \";create=true\");\n+\t\t\tStatement statement = conn.createStatement()) {\n+\t\t\tstatement.executeUpdate(\"CREATE TABLE \" + INPUT_TABLE + \" (\" +\n+\t\t\t\t\t\"id BIGINT NOT NULL,\" +\n+\t\t\t\t\t\"timestamp6_col TIMESTAMP, \" +\n+\t\t\t\t\t\"timestamp9_col TIMESTAMP, \" +\n+\t\t\t\t\t\"time_col TIME, \" +\n+\t\t\t\t\t\"real_col FLOAT(23), \" +    // A precision of 23 or less makes FLOAT equivalent to REAL.\n+\t\t\t\t\t\"double_col FLOAT(24),\" +   // A precision of 24 or greater makes FLOAT equivalent to DOUBLE PRECISION.\n+\t\t\t\t\t\"decimal_col DECIMAL(10, 4))\");\n+\t\t\tstatement.executeUpdate(\"INSERT INTO \" + INPUT_TABLE + \" VALUES (\" +\n+\t\t\t\t\t\"1, TIMESTAMP('2020-01-01 15:35:00.123456'), TIMESTAMP('2020-01-01 15:35:00.123456789'), \" +\n+\t\t\t\t\t\"TIME('15:35:00'), 1.175E-37, 1.79769E+308, 100.1234)\");\n+\t\t\tstatement.executeUpdate(\"INSERT INTO \" + INPUT_TABLE + \" VALUES (\" +\n+\t\t\t\t\t\"2, TIMESTAMP('2020-01-01 15:36:01.123456'), TIMESTAMP('2020-01-01 15:36:01.123456789'), \" +\n+\t\t\t\t\t\"TIME('15:36:01'), -1.175E-37, -1.79769E+308, 101.1234)\");\n+\t\t}\n+\t}\n+\n+\t@After\n+\tpublic void clearOutputTable() throws Exception {\n+\t\tClass.forName(DRIVER_CLASS);\n+\t\ttry (\n+\t\t\tConnection conn = DriverManager.getConnection(DB_URL);\n+\t\t\tStatement stat = conn.createStatement()) {\n+\t\t\tstat.execute(\"DROP TABLE \" + INPUT_TABLE);\n+\t\t}\n \t}\n \n \t@Test\n-\tpublic void testFieldsProjection() throws Exception {\n-\t\tStreamITCase.clear();\n+\tpublic void testJDBCSource() throws Exception {\n+\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+\t\tEnvironmentSettings envSettings = EnvironmentSettings.newInstance()\n+\t\t\t.useBlinkPlanner()\n+\t\t\t.inStreamingMode()\n+\t\t\t.build();\n+\t\tStreamTableEnvironment tEnv = StreamTableEnvironment.create(env, envSettings);\n+\n+\t\ttEnv.sqlUpdate(\n+\t\t\t\"CREATE TABLE \" + INPUT_TABLE + \"(\" +\n+\t\t\t\t\"id BIGINT,\" +\n+\t\t\t\t\"timestamp6_col TIMESTAMP(6),\" +\n+\t\t\t\t\"timestamp9_col TIMESTAMP(9),\" +\n+\t\t\t\t\"time_col TIME,\" +\n+\t\t\t\t\"real_col FLOAT,\" +\n+\t\t\t\t\"double_col DOUBLE,\" +\n+\t\t\t\t\"decimal_col DECIMAL(10, 4)\" +\n+\t\t\t\t\") WITH (\" +\n+\t\t\t\t\"  'connector.type'='jdbc',\" +\n+\t\t\t\t\"  'connector.url'='\" + DB_URL + \"',\" +\n+\t\t\t\t\"  'connector.table'='\" + INPUT_TABLE + \"'\" +\n+\t\t\t\t\")\"\n+\t\t);\n \n-\t\tTable result = tEnv.sqlQuery(SELECT_ID_BOOKS);\n-\t\tDataStream<Row> resultSet = tEnv.toAppendStream(result, Row.class);\n-\t\tresultSet.addSink(new StreamITCase.StringSink<>());\n+\t\tStreamITCase.clear();\n+\t\ttEnv.toAppendStream(tEnv.sqlQuery(\"SELECT * FROM \" + INPUT_TABLE), Row.class)\n+\t\t\t.addSink(new StreamITCase.StringSink<>());\n \t\tenv.execute();\n \n-\t\tList<String> expected = new ArrayList<>();\n-\t\texpected.add(\"1001\");\n-\t\texpected.add(\"1002\");\n-\t\texpected.add(\"1003\");\n-\t\texpected.add(\"1004\");\n-\t\texpected.add(\"1005\");\n-\t\texpected.add(\"1006\");\n-\t\texpected.add(\"1007\");\n-\t\texpected.add(\"1008\");\n-\t\texpected.add(\"1009\");\n-\t\texpected.add(\"1010\");\n-\n+\t\tList<String> expected =\n+\t\t\tArrays.asList(\n+\t\t\t\t\"1,2020-01-01T15:35:00.123456,2020-01-01T15:35:00.123456789,15:35,1.175E-37,1.79769E308,100.1234\",\n+\t\t\t\t\"2,2020-01-01T15:36:01.123456,2020-01-01T15:36:01.123456789,15:36:01,-1.175E-37,-1.79769E308,101.1234\");\n \t\tStreamITCase.compareWithList(expected);\n \t}\n \n \t@Test\n-\tpublic void testAllFieldsSelection() throws Exception {\n-\t\tStreamITCase.clear();\n+\tpublic void testProjectableJDBCSource() throws Exception {\n+\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+\t\tEnvironmentSettings envSettings = EnvironmentSettings.newInstance()\n+\t\t\t\t.useBlinkPlanner()\n+\t\t\t\t.inStreamingMode()\n+\t\t\t\t.build();\n+\t\tStreamTableEnvironment tEnv = StreamTableEnvironment.create(env, envSettings);\n+\n+\t\ttEnv.sqlUpdate(\n+\t\t\t\"CREATE TABLE \" + INPUT_TABLE + \"(\" +\n+\t\t\t\t\"id BIGINT,\" +\n+\t\t\t\t\"timestamp6_col TIMESTAMP(6),\" +\n+\t\t\t\t\"timestamp9_col TIMESTAMP(9),\" +\n+\t\t\t\t\"time_col TIME,\" +\n+\t\t\t\t\"real_col FLOAT,\" +\n+\t\t\t\t\"decimal_col DECIMAL(10, 4)\" +\n+\t\t\t\t\") WITH (\" +\n+\t\t\t\t\"  'connector.type'='jdbc',\" +\n+\t\t\t\t\"  'connector.url'='\" + DB_URL + \"',\" +\n+\t\t\t\t\"  'connector.table'='\" + INPUT_TABLE + \"'\" +\n+\t\t\t\t\")\"\n+\t\t);\n \n-\t\tTable result = tEnv.sqlQuery(SELECT_ALL_BOOKS);\n-\t\tDataStream<Row> resultSet = tEnv.toAppendStream(result, Row.class);\n-\t\tresultSet.addSink(new StreamITCase.StringSink<>());\n+\t\tStreamITCase.clear();\n+\t\ttEnv.toAppendStream(tEnv.sqlQuery(\"SELECT timestamp6_col, decimal_col FROM \" + INPUT_TABLE), Row.class)\n+\t\t\t\t.addSink(new StreamITCase.StringSink<>());\n \t\tenv.execute();\n \n-\t\tList<String> expected = new ArrayList<>();\n-\t\texpected.add(\"1001,Java public for dummies,Tan Ah Teck,11.11,11\");\n-\t\texpected.add(\"1002,More Java for dummies,Tan Ah Teck,22.22,22\");\n-\t\texpected.add(\"1003,More Java for more dummies,Mohammad Ali,33.33,33\");\n-\t\texpected.add(\"1004,A Cup of Java,Kumar,44.44,44\");\n-\t\texpected.add(\"1005,A Teaspoon of Java,Kevin Jones,55.55,55\");\n-\t\texpected.add(\"1006,A Teaspoon of Java 1.4,Kevin Jones,66.66,66\");\n-\t\texpected.add(\"1007,A Teaspoon of Java 1.5,Kevin Jones,77.77,77\");\n-\t\texpected.add(\"1008,A Teaspoon of Java 1.6,Kevin Jones,88.88,88\");\n-\t\texpected.add(\"1009,A Teaspoon of Java 1.7,Kevin Jones,99.99,99\");\n-\t\texpected.add(\"1010,A Teaspoon of Java 1.8,Kevin Jones,null,1010\");\n-\n+\t\tList<String> expected =\n+\t\t\tArrays.asList(\n+\t\t\t\t\"2020-01-01T15:35:00.123456,100.1234\",\n+\t\t\t\t\"2020-01-01T15:36:01.123456,101.1234\");\n \t\tStreamITCase.compareWithList(expected);\n \t}\n \n+\t@Test(expected = TableException.class)\n+\tpublic void testInvalidPrecisionOfJDBCSource() throws Exception {\n+\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+\t\tEnvironmentSettings envSettings = EnvironmentSettings.newInstance()\n+\t\t\t\t.useBlinkPlanner()\n+\t\t\t\t.inStreamingMode()\n+\t\t\t\t.build();\n+\t\tStreamTableEnvironment tEnv = StreamTableEnvironment.create(env, envSettings);\n+\n+\t\ttEnv.sqlUpdate(\n+\t\t\"CREATE TABLE \" + INPUT_TABLE + \"(\" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f7456ac7e14681e18199941e0962b207eb99bef9"}, "originalPosition": 212}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg5NzcwOA==", "bodyText": "nice catch", "url": "https://github.com/apache/flink/pull/10745#discussion_r376897708", "createdAt": "2020-02-10T07:20:42Z", "author": {"login": "docete"}, "path": "flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceITCase.java", "diffHunk": "@@ -18,93 +18,167 @@\n \n package org.apache.flink.api.java.io.jdbc;\n \n-import org.apache.flink.streaming.api.datastream.DataStream;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n import org.apache.flink.table.api.EnvironmentSettings;\n-import org.apache.flink.table.api.Table;\n+import org.apache.flink.table.api.TableException;\n import org.apache.flink.table.api.java.StreamTableEnvironment;\n import org.apache.flink.table.runtime.utils.StreamITCase;\n+import org.apache.flink.test.util.AbstractTestBase;\n import org.apache.flink.types.Row;\n \n-import org.junit.BeforeClass;\n+import org.junit.After;\n+import org.junit.Before;\n import org.junit.Test;\n \n-import java.util.ArrayList;\n+import java.sql.Connection;\n+import java.sql.DriverManager;\n+import java.sql.SQLException;\n+import java.sql.Statement;\n+import java.util.Arrays;\n import java.util.List;\n \n+\n /**\n- * IT case for {@link JDBCTableSource}.\n+ * ITCase for {@link JDBCTableSource}.\n  */\n-public class JDBCTableSourceITCase extends JDBCTestBase {\n-\n-\tprivate static final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n-\tprivate static final EnvironmentSettings bsSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();\n-\tprivate static final StreamTableEnvironment tEnv = StreamTableEnvironment.create(env, bsSettings);\n-\n-\tstatic final String TABLE_SOURCE_SQL = \"CREATE TABLE books (\" +\n-\t\t\" id int, \" +\n-\t\t\" title varchar, \" +\n-\t\t\" author varchar, \" +\n-\t\t\" price double, \" +\n-\t\t\" qty int \" +\n-\t\t\") with (\" +\n-\t\t\" 'connector.type' = 'jdbc', \" +\n-\t\t\" 'connector.url' = 'jdbc:derby:memory:ebookshop', \" +\n-\t\t\" 'connector.table' = 'books', \" +\n-\t\t\" 'connector.driver' = 'org.apache.derby.jdbc.EmbeddedDriver' \" +\n-\t\t\")\";\n-\n-\t@BeforeClass\n-\tpublic static void createTable() {\n-\t\ttEnv.sqlUpdate(TABLE_SOURCE_SQL);\n+public class JDBCTableSourceITCase extends AbstractTestBase {\n+\n+\tpublic static final String DRIVER_CLASS = \"org.apache.derby.jdbc.EmbeddedDriver\";\n+\tpublic static final String DB_URL = \"jdbc:derby:memory:test\";\n+\tpublic static final String INPUT_TABLE = \"jdbcSource\";\n+\n+\t@Before\n+\tpublic void before() throws ClassNotFoundException, SQLException {\n+\t\tSystem.setProperty(\"derby.stream.error.field\", JDBCTestBase.class.getCanonicalName() + \".DEV_NULL\");\n+\t\tClass.forName(DRIVER_CLASS);\n+\n+\t\ttry (\n+\t\t\tConnection conn = DriverManager.getConnection(DB_URL + \";create=true\");\n+\t\t\tStatement statement = conn.createStatement()) {\n+\t\t\tstatement.executeUpdate(\"CREATE TABLE \" + INPUT_TABLE + \" (\" +\n+\t\t\t\t\t\"id BIGINT NOT NULL,\" +\n+\t\t\t\t\t\"timestamp6_col TIMESTAMP, \" +\n+\t\t\t\t\t\"timestamp9_col TIMESTAMP, \" +\n+\t\t\t\t\t\"time_col TIME, \" +\n+\t\t\t\t\t\"real_col FLOAT(23), \" +    // A precision of 23 or less makes FLOAT equivalent to REAL.\n+\t\t\t\t\t\"double_col FLOAT(24),\" +   // A precision of 24 or greater makes FLOAT equivalent to DOUBLE PRECISION.\n+\t\t\t\t\t\"decimal_col DECIMAL(10, 4))\");\n+\t\t\tstatement.executeUpdate(\"INSERT INTO \" + INPUT_TABLE + \" VALUES (\" +\n+\t\t\t\t\t\"1, TIMESTAMP('2020-01-01 15:35:00.123456'), TIMESTAMP('2020-01-01 15:35:00.123456789'), \" +\n+\t\t\t\t\t\"TIME('15:35:00'), 1.175E-37, 1.79769E+308, 100.1234)\");\n+\t\t\tstatement.executeUpdate(\"INSERT INTO \" + INPUT_TABLE + \" VALUES (\" +\n+\t\t\t\t\t\"2, TIMESTAMP('2020-01-01 15:36:01.123456'), TIMESTAMP('2020-01-01 15:36:01.123456789'), \" +\n+\t\t\t\t\t\"TIME('15:36:01'), -1.175E-37, -1.79769E+308, 101.1234)\");\n+\t\t}\n+\t}\n+\n+\t@After\n+\tpublic void clearOutputTable() throws Exception {\n+\t\tClass.forName(DRIVER_CLASS);\n+\t\ttry (\n+\t\t\tConnection conn = DriverManager.getConnection(DB_URL);\n+\t\t\tStatement stat = conn.createStatement()) {\n+\t\t\tstat.execute(\"DROP TABLE \" + INPUT_TABLE);\n+\t\t}\n \t}\n \n \t@Test\n-\tpublic void testFieldsProjection() throws Exception {\n-\t\tStreamITCase.clear();\n+\tpublic void testJDBCSource() throws Exception {\n+\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+\t\tEnvironmentSettings envSettings = EnvironmentSettings.newInstance()\n+\t\t\t.useBlinkPlanner()\n+\t\t\t.inStreamingMode()\n+\t\t\t.build();\n+\t\tStreamTableEnvironment tEnv = StreamTableEnvironment.create(env, envSettings);\n+\n+\t\ttEnv.sqlUpdate(\n+\t\t\t\"CREATE TABLE \" + INPUT_TABLE + \"(\" +\n+\t\t\t\t\"id BIGINT,\" +\n+\t\t\t\t\"timestamp6_col TIMESTAMP(6),\" +\n+\t\t\t\t\"timestamp9_col TIMESTAMP(9),\" +\n+\t\t\t\t\"time_col TIME,\" +\n+\t\t\t\t\"real_col FLOAT,\" +\n+\t\t\t\t\"double_col DOUBLE,\" +\n+\t\t\t\t\"decimal_col DECIMAL(10, 4)\" +\n+\t\t\t\t\") WITH (\" +\n+\t\t\t\t\"  'connector.type'='jdbc',\" +\n+\t\t\t\t\"  'connector.url'='\" + DB_URL + \"',\" +\n+\t\t\t\t\"  'connector.table'='\" + INPUT_TABLE + \"'\" +\n+\t\t\t\t\")\"\n+\t\t);\n \n-\t\tTable result = tEnv.sqlQuery(SELECT_ID_BOOKS);\n-\t\tDataStream<Row> resultSet = tEnv.toAppendStream(result, Row.class);\n-\t\tresultSet.addSink(new StreamITCase.StringSink<>());\n+\t\tStreamITCase.clear();\n+\t\ttEnv.toAppendStream(tEnv.sqlQuery(\"SELECT * FROM \" + INPUT_TABLE), Row.class)\n+\t\t\t.addSink(new StreamITCase.StringSink<>());\n \t\tenv.execute();\n \n-\t\tList<String> expected = new ArrayList<>();\n-\t\texpected.add(\"1001\");\n-\t\texpected.add(\"1002\");\n-\t\texpected.add(\"1003\");\n-\t\texpected.add(\"1004\");\n-\t\texpected.add(\"1005\");\n-\t\texpected.add(\"1006\");\n-\t\texpected.add(\"1007\");\n-\t\texpected.add(\"1008\");\n-\t\texpected.add(\"1009\");\n-\t\texpected.add(\"1010\");\n-\n+\t\tList<String> expected =\n+\t\t\tArrays.asList(\n+\t\t\t\t\"1,2020-01-01T15:35:00.123456,2020-01-01T15:35:00.123456789,15:35,1.175E-37,1.79769E308,100.1234\",\n+\t\t\t\t\"2,2020-01-01T15:36:01.123456,2020-01-01T15:36:01.123456789,15:36:01,-1.175E-37,-1.79769E308,101.1234\");\n \t\tStreamITCase.compareWithList(expected);\n \t}\n \n \t@Test\n-\tpublic void testAllFieldsSelection() throws Exception {\n-\t\tStreamITCase.clear();\n+\tpublic void testProjectableJDBCSource() throws Exception {\n+\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+\t\tEnvironmentSettings envSettings = EnvironmentSettings.newInstance()\n+\t\t\t\t.useBlinkPlanner()\n+\t\t\t\t.inStreamingMode()\n+\t\t\t\t.build();\n+\t\tStreamTableEnvironment tEnv = StreamTableEnvironment.create(env, envSettings);\n+\n+\t\ttEnv.sqlUpdate(\n+\t\t\t\"CREATE TABLE \" + INPUT_TABLE + \"(\" +\n+\t\t\t\t\"id BIGINT,\" +\n+\t\t\t\t\"timestamp6_col TIMESTAMP(6),\" +\n+\t\t\t\t\"timestamp9_col TIMESTAMP(9),\" +\n+\t\t\t\t\"time_col TIME,\" +\n+\t\t\t\t\"real_col FLOAT,\" +\n+\t\t\t\t\"decimal_col DECIMAL(10, 4)\" +\n+\t\t\t\t\") WITH (\" +\n+\t\t\t\t\"  'connector.type'='jdbc',\" +\n+\t\t\t\t\"  'connector.url'='\" + DB_URL + \"',\" +\n+\t\t\t\t\"  'connector.table'='\" + INPUT_TABLE + \"'\" +\n+\t\t\t\t\")\"\n+\t\t);\n \n-\t\tTable result = tEnv.sqlQuery(SELECT_ALL_BOOKS);\n-\t\tDataStream<Row> resultSet = tEnv.toAppendStream(result, Row.class);\n-\t\tresultSet.addSink(new StreamITCase.StringSink<>());\n+\t\tStreamITCase.clear();\n+\t\ttEnv.toAppendStream(tEnv.sqlQuery(\"SELECT timestamp6_col, decimal_col FROM \" + INPUT_TABLE), Row.class)\n+\t\t\t\t.addSink(new StreamITCase.StringSink<>());\n \t\tenv.execute();\n \n-\t\tList<String> expected = new ArrayList<>();\n-\t\texpected.add(\"1001,Java public for dummies,Tan Ah Teck,11.11,11\");\n-\t\texpected.add(\"1002,More Java for dummies,Tan Ah Teck,22.22,22\");\n-\t\texpected.add(\"1003,More Java for more dummies,Mohammad Ali,33.33,33\");\n-\t\texpected.add(\"1004,A Cup of Java,Kumar,44.44,44\");\n-\t\texpected.add(\"1005,A Teaspoon of Java,Kevin Jones,55.55,55\");\n-\t\texpected.add(\"1006,A Teaspoon of Java 1.4,Kevin Jones,66.66,66\");\n-\t\texpected.add(\"1007,A Teaspoon of Java 1.5,Kevin Jones,77.77,77\");\n-\t\texpected.add(\"1008,A Teaspoon of Java 1.6,Kevin Jones,88.88,88\");\n-\t\texpected.add(\"1009,A Teaspoon of Java 1.7,Kevin Jones,99.99,99\");\n-\t\texpected.add(\"1010,A Teaspoon of Java 1.8,Kevin Jones,null,1010\");\n-\n+\t\tList<String> expected =\n+\t\t\tArrays.asList(\n+\t\t\t\t\"2020-01-01T15:35:00.123456,100.1234\",\n+\t\t\t\t\"2020-01-01T15:36:01.123456,101.1234\");\n \t\tStreamITCase.compareWithList(expected);\n \t}\n \n+\t@Test(expected = TableException.class)\n+\tpublic void testInvalidPrecisionOfJDBCSource() throws Exception {\n+\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+\t\tEnvironmentSettings envSettings = EnvironmentSettings.newInstance()\n+\t\t\t\t.useBlinkPlanner()\n+\t\t\t\t.inStreamingMode()\n+\t\t\t\t.build();\n+\t\tStreamTableEnvironment tEnv = StreamTableEnvironment.create(env, envSettings);\n+\n+\t\ttEnv.sqlUpdate(\n+\t\t\"CREATE TABLE \" + INPUT_TABLE + \"(\" +", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjY4MDk1OQ=="}, "originalCommit": {"oid": "f7456ac7e14681e18199941e0962b207eb99bef9"}, "originalPosition": 212}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMzNDk5MjkxOnYy", "diffSide": "RIGHT", "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialects.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQwNDoxMzo1NlrOFn9HMw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQxMTo0NDoxMlrOFoF-VQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ0MDA1MQ==", "bodyText": "Could we just simply dt.getLogicalType() instanceof VarBinaryType  to match it is a VarBinaryType? I think currently there isn't a LegacyTypeInformationType which is VARBINARY.  The same to the below if branches.", "url": "https://github.com/apache/flink/pull/10745#discussion_r377440051", "createdAt": "2020-02-11T04:13:56Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialects.java", "diffHunk": "@@ -46,10 +59,84 @@\n \t\treturn Optional.empty();\n \t}\n \n-\tprivate static class DerbyDialect implements JDBCDialect {\n+\tprivate abstract static class AbstractDialect implements JDBCDialect {\n+\n+\t\t@Override\n+\t\tpublic void validate(TableSchema schema) throws ValidationException {\n+\t\t\tfor (int i = 0; i < schema.getFieldCount(); i++) {\n+\t\t\t\tDataType dt = schema.getFieldDataType(i).get();\n+\t\t\t\tString fieldName = schema.getFieldName(i).get();\n+\n+\t\t\t\t// TODO: We can't convert VARBINARY(n) data type to\n+\t\t\t\t//  PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO in LegacyTypeInfoDataTypeConverter\n+\t\t\t\t//  when n is smaller than Integer.MAX_VALUE\n+\t\t\t\tif (unsupportedTypes().contains(dt.getLogicalType().getTypeRoot()) ||\n+\t\t\t\t\t\t(!(dt.getLogicalType() instanceof LegacyTypeInformationType) &&\n+\t\t\t\t\t\t(VARBINARY == dt.getLogicalType().getTypeRoot()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "330adf7ac3582db6b47a093e6f342351a3ba7e43"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzU4NTIzNw==", "bodyText": "Yes, I think so. Will update soon.", "url": "https://github.com/apache/flink/pull/10745#discussion_r377585237", "createdAt": "2020-02-11T11:44:12Z", "author": {"login": "docete"}, "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialects.java", "diffHunk": "@@ -46,10 +59,84 @@\n \t\treturn Optional.empty();\n \t}\n \n-\tprivate static class DerbyDialect implements JDBCDialect {\n+\tprivate abstract static class AbstractDialect implements JDBCDialect {\n+\n+\t\t@Override\n+\t\tpublic void validate(TableSchema schema) throws ValidationException {\n+\t\t\tfor (int i = 0; i < schema.getFieldCount(); i++) {\n+\t\t\t\tDataType dt = schema.getFieldDataType(i).get();\n+\t\t\t\tString fieldName = schema.getFieldName(i).get();\n+\n+\t\t\t\t// TODO: We can't convert VARBINARY(n) data type to\n+\t\t\t\t//  PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO in LegacyTypeInfoDataTypeConverter\n+\t\t\t\t//  when n is smaller than Integer.MAX_VALUE\n+\t\t\t\tif (unsupportedTypes().contains(dt.getLogicalType().getTypeRoot()) ||\n+\t\t\t\t\t\t(!(dt.getLogicalType() instanceof LegacyTypeInformationType) &&\n+\t\t\t\t\t\t(VARBINARY == dt.getLogicalType().getTypeRoot()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ0MDA1MQ=="}, "originalCommit": {"oid": "330adf7ac3582db6b47a093e6f342351a3ba7e43"}, "originalPosition": 43}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMzNDk5Njg0OnYy", "diffSide": "RIGHT", "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialects.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQwNDoxODoyNFrOFn9JeA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQwNDoxOTowMVrOFn9JxA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ0MDYzMg==", "bodyText": "Improve the error message a bit more:\nString.format(\"The precision of filed '%s' is out of the TIMESTAMP precision range [%d, %d] supported by the %s dialect.\",\nfieldName,\nminTimestampPrecision(),\nmaxTimestampPrecision(),\ndialectName);", "url": "https://github.com/apache/flink/pull/10745#discussion_r377440632", "createdAt": "2020-02-11T04:18:24Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialects.java", "diffHunk": "@@ -46,10 +59,84 @@\n \t\treturn Optional.empty();\n \t}\n \n-\tprivate static class DerbyDialect implements JDBCDialect {\n+\tprivate abstract static class AbstractDialect implements JDBCDialect {\n+\n+\t\t@Override\n+\t\tpublic void validate(TableSchema schema) throws ValidationException {\n+\t\t\tfor (int i = 0; i < schema.getFieldCount(); i++) {\n+\t\t\t\tDataType dt = schema.getFieldDataType(i).get();\n+\t\t\t\tString fieldName = schema.getFieldName(i).get();\n+\n+\t\t\t\t// TODO: We can't convert VARBINARY(n) data type to\n+\t\t\t\t//  PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO in LegacyTypeInfoDataTypeConverter\n+\t\t\t\t//  when n is smaller than Integer.MAX_VALUE\n+\t\t\t\tif (unsupportedTypes().contains(dt.getLogicalType().getTypeRoot()) ||\n+\t\t\t\t\t\t(!(dt.getLogicalType() instanceof LegacyTypeInformationType) &&\n+\t\t\t\t\t\t(VARBINARY == dt.getLogicalType().getTypeRoot()\n+\t\t\t\t\t\t\t&& Integer.MAX_VALUE != ((VarBinaryType) dt.getLogicalType()).getLength()))) {\n+\t\t\t\t\tthrow new ValidationException(\n+\t\t\t\t\t\t\tString.format(\"The dialect don't support type: %s.\", dt.toString()));\n+\t\t\t\t}\n+\n+\t\t\t\t// only validate precision of DECIMAL type for blink planner\n+\t\t\t\tif (!(dt.getLogicalType() instanceof LegacyTypeInformationType)\n+\t\t\t\t\t\t&& DECIMAL == dt.getLogicalType().getTypeRoot()) {\n+\t\t\t\t\tint precision = ((DecimalType) dt.getLogicalType()).getPrecision();\n+\t\t\t\t\tif (precision > maxDecimalPrecision()\n+\t\t\t\t\t\t\t|| precision < minDecimalPrecision()) {\n+\t\t\t\t\t\tthrow new ValidationException(\n+\t\t\t\t\t\t\t\tString.format(\"The precision of %s is out of the range [%d, %d].\",\n+\t\t\t\t\t\t\t\t\t\tfieldName,\n+\t\t\t\t\t\t\t\t\t\tminDecimalPrecision(),\n+\t\t\t\t\t\t\t\t\t\tmaxDecimalPrecision()));\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\n+\t\t\t\t// only validate precision of DECIMAL type for blink planner\n+\t\t\t\tif (!(dt.getLogicalType() instanceof LegacyTypeInformationType)\n+\t\t\t\t\t\t&& TIMESTAMP_WITHOUT_TIME_ZONE == dt.getLogicalType().getTypeRoot()) {\n+\t\t\t\t\tint precision = ((TimestampType) dt.getLogicalType()).getPrecision();\n+\t\t\t\t\tif (precision > maxTimestampPrecision()\n+\t\t\t\t\t\t\t|| precision < minTimestampPrecision()) {\n+\t\t\t\t\t\tthrow new ValidationException(\n+\t\t\t\t\t\t\t\tString.format(\"The precision of %s is out of the range [%d, %d].\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "330adf7ac3582db6b47a093e6f342351a3ba7e43"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ0MDcwOA==", "bodyText": "The same to the DECIMAL type.", "url": "https://github.com/apache/flink/pull/10745#discussion_r377440708", "createdAt": "2020-02-11T04:19:01Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialects.java", "diffHunk": "@@ -46,10 +59,84 @@\n \t\treturn Optional.empty();\n \t}\n \n-\tprivate static class DerbyDialect implements JDBCDialect {\n+\tprivate abstract static class AbstractDialect implements JDBCDialect {\n+\n+\t\t@Override\n+\t\tpublic void validate(TableSchema schema) throws ValidationException {\n+\t\t\tfor (int i = 0; i < schema.getFieldCount(); i++) {\n+\t\t\t\tDataType dt = schema.getFieldDataType(i).get();\n+\t\t\t\tString fieldName = schema.getFieldName(i).get();\n+\n+\t\t\t\t// TODO: We can't convert VARBINARY(n) data type to\n+\t\t\t\t//  PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO in LegacyTypeInfoDataTypeConverter\n+\t\t\t\t//  when n is smaller than Integer.MAX_VALUE\n+\t\t\t\tif (unsupportedTypes().contains(dt.getLogicalType().getTypeRoot()) ||\n+\t\t\t\t\t\t(!(dt.getLogicalType() instanceof LegacyTypeInformationType) &&\n+\t\t\t\t\t\t(VARBINARY == dt.getLogicalType().getTypeRoot()\n+\t\t\t\t\t\t\t&& Integer.MAX_VALUE != ((VarBinaryType) dt.getLogicalType()).getLength()))) {\n+\t\t\t\t\tthrow new ValidationException(\n+\t\t\t\t\t\t\tString.format(\"The dialect don't support type: %s.\", dt.toString()));\n+\t\t\t\t}\n+\n+\t\t\t\t// only validate precision of DECIMAL type for blink planner\n+\t\t\t\tif (!(dt.getLogicalType() instanceof LegacyTypeInformationType)\n+\t\t\t\t\t\t&& DECIMAL == dt.getLogicalType().getTypeRoot()) {\n+\t\t\t\t\tint precision = ((DecimalType) dt.getLogicalType()).getPrecision();\n+\t\t\t\t\tif (precision > maxDecimalPrecision()\n+\t\t\t\t\t\t\t|| precision < minDecimalPrecision()) {\n+\t\t\t\t\t\tthrow new ValidationException(\n+\t\t\t\t\t\t\t\tString.format(\"The precision of %s is out of the range [%d, %d].\",\n+\t\t\t\t\t\t\t\t\t\tfieldName,\n+\t\t\t\t\t\t\t\t\t\tminDecimalPrecision(),\n+\t\t\t\t\t\t\t\t\t\tmaxDecimalPrecision()));\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\n+\t\t\t\t// only validate precision of DECIMAL type for blink planner\n+\t\t\t\tif (!(dt.getLogicalType() instanceof LegacyTypeInformationType)\n+\t\t\t\t\t\t&& TIMESTAMP_WITHOUT_TIME_ZONE == dt.getLogicalType().getTypeRoot()) {\n+\t\t\t\t\tint precision = ((TimestampType) dt.getLogicalType()).getPrecision();\n+\t\t\t\t\tif (precision > maxTimestampPrecision()\n+\t\t\t\t\t\t\t|| precision < minTimestampPrecision()) {\n+\t\t\t\t\t\tthrow new ValidationException(\n+\t\t\t\t\t\t\t\tString.format(\"The precision of %s is out of the range [%d, %d].\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ0MDYzMg=="}, "originalCommit": {"oid": "330adf7ac3582db6b47a093e6f342351a3ba7e43"}, "originalPosition": 70}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMzNTA2MzMxOnYy", "diffSide": "RIGHT", "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialects.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQwNToyNTo1NVrOFn9wKg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQwNToyNTo1NVrOFn9wKg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ1MDUzOA==", "bodyText": "String.format(\"The %s dialect doesn't support type: %s.\", dialectName, dt.toString())", "url": "https://github.com/apache/flink/pull/10745#discussion_r377450538", "createdAt": "2020-02-11T05:25:55Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialects.java", "diffHunk": "@@ -46,10 +59,84 @@\n \t\treturn Optional.empty();\n \t}\n \n-\tprivate static class DerbyDialect implements JDBCDialect {\n+\tprivate abstract static class AbstractDialect implements JDBCDialect {\n+\n+\t\t@Override\n+\t\tpublic void validate(TableSchema schema) throws ValidationException {\n+\t\t\tfor (int i = 0; i < schema.getFieldCount(); i++) {\n+\t\t\t\tDataType dt = schema.getFieldDataType(i).get();\n+\t\t\t\tString fieldName = schema.getFieldName(i).get();\n+\n+\t\t\t\t// TODO: We can't convert VARBINARY(n) data type to\n+\t\t\t\t//  PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO in LegacyTypeInfoDataTypeConverter\n+\t\t\t\t//  when n is smaller than Integer.MAX_VALUE\n+\t\t\t\tif (unsupportedTypes().contains(dt.getLogicalType().getTypeRoot()) ||\n+\t\t\t\t\t\t(!(dt.getLogicalType() instanceof LegacyTypeInformationType) &&\n+\t\t\t\t\t\t(VARBINARY == dt.getLogicalType().getTypeRoot()\n+\t\t\t\t\t\t\t&& Integer.MAX_VALUE != ((VarBinaryType) dt.getLogicalType()).getLength()))) {\n+\t\t\t\t\tthrow new ValidationException(\n+\t\t\t\t\t\t\tString.format(\"The dialect don't support type: %s.\", dt.toString()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "330adf7ac3582db6b47a093e6f342351a3ba7e43"}, "originalPosition": 46}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMzNjE2MjcyOnYy", "diffSide": "RIGHT", "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialects.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQxMzowMToyM1rOFoIC0g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQxMzowMToyM1rOFoIC0g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzYxOTE1NA==", "bodyText": "Nit: don't -> doesn't", "url": "https://github.com/apache/flink/pull/10745#discussion_r377619154", "createdAt": "2020-02-11T13:01:23Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialects.java", "diffHunk": "@@ -71,43 +71,48 @@ public void validate(TableSchema schema) throws ValidationException {\n \t\t\t\t//  PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO in LegacyTypeInfoDataTypeConverter\n \t\t\t\t//  when n is smaller than Integer.MAX_VALUE\n \t\t\t\tif (unsupportedTypes().contains(dt.getLogicalType().getTypeRoot()) ||\n-\t\t\t\t\t\t(!(dt.getLogicalType() instanceof LegacyTypeInformationType) &&\n-\t\t\t\t\t\t(VARBINARY == dt.getLogicalType().getTypeRoot()\n-\t\t\t\t\t\t\t&& Integer.MAX_VALUE != ((VarBinaryType) dt.getLogicalType()).getLength()))) {\n+\t\t\t\t\t\t(dt.getLogicalType() instanceof VarBinaryType\n+\t\t\t\t\t\t\t&& Integer.MAX_VALUE != ((VarBinaryType) dt.getLogicalType()).getLength())) {\n \t\t\t\t\tthrow new ValidationException(\n-\t\t\t\t\t\t\tString.format(\"The dialect don't support type: %s.\", dt.toString()));\n+\t\t\t\t\t\t\tString.format(\"The %s dialect don't support type: %s.\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e538c4875e6241d494585c6e2a8f586ac078131"}, "originalPosition": 11}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1266, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}