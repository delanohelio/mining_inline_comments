{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDE5MjE5NTc2", "number": 12206, "title": "[FLINK-14255][hive] Integrate hive to streaming file sink with parquet and orc", "bodyText": "What is the purpose of the change\nIntegrate hive to streaming file sink.\nBrief change log\n\nIntegrate parquet and orc streaming writer to hive.\nIntegrate to StreamingFileWriter and StreamingFileCommitter.\n\nVerifying this change\n\nHiveTableSinkTest.testPartStreamingWrite\nHiveTableSinkTest.testNonPartStreamingWrite\n\nDoes this pull request potentially affect one of the following parts:\n\nDependencies (does it add or upgrade a dependency): no\nThe public API, i.e., is any changed class annotated with @Public(Evolving): no\nThe serializers: no\nThe runtime per-record code paths (performance sensitive): no\nAnything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: no\nThe S3 file system connector: no\n\nDocumentation\n\nDoes this pull request introduce a new feature? (yes)\nIf yes, how is the feature documented? JavaDocs", "createdAt": "2020-05-18T02:15:51Z", "url": "https://github.com/apache/flink/pull/12206", "merged": true, "mergeCommit": {"oid": "1f668dd3df1a3d9bb8837c05ebdd5e473c55b1ea"}, "closed": true, "closedAt": "2020-05-18T04:10:59Z", "author": {"login": "JingsongLi"}, "timelineItems": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABciWPBzAH2gAyNDE5MjE5NTc2OjUyMTMzNGM1MjNhNTAwNDg4M2Y5NGUxZmI5OTZjZGQxODA4MTUwMGM=", "endCursor": "Y3Vyc29yOnYyOpPPAAABciXK42gFqTQxMzI0ODkyOQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "521334c523a5004883f94e1fb996cdd18081500c", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/flink/commit/521334c523a5004883f94e1fb996cdd18081500c", "committedDate": "2020-05-18T02:13:50Z", "message": "[FLINK-14255][hive] Integrate hive to streaming file sink with parquet and orc"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2ea5f52f890c9cb5728fd7772f59bea4b38f07ca", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/flink/commit/2ea5f52f890c9cb5728fd7772f59bea4b38f07ca", "committedDate": "2020-05-18T02:39:15Z", "message": "Add hive 1.x support"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDEzMjM4ODE3", "url": "https://github.com/apache/flink/pull/12206#pullrequestreview-413238817", "createdAt": "2020-05-18T02:34:16Z", "commit": {"oid": "521334c523a5004883f94e1fb996cdd18081500c"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQwMjozNDoxNlrOGWl4Xw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQwMzowMDozNFrOGWmL4Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjM0MjQ5NQ==", "bodyText": "Perhaps we should rename the old HivePartitionComputer to HiveRowPartitionComputer?", "url": "https://github.com/apache/flink/pull/12206#discussion_r426342495", "createdAt": "2020-05-18T02:34:16Z", "author": {"login": "lirui-apache"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveRowDataPartitionComputer.java", "diffHunk": "@@ -0,0 +1,77 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+import org.apache.flink.table.catalog.hive.client.HiveShim;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.util.DataFormatConverters;\n+import org.apache.flink.table.filesystem.RowDataPartitionComputer;\n+import org.apache.flink.table.functions.hive.conversion.HiveInspectors;\n+import org.apache.flink.table.functions.hive.conversion.HiveObjectConversion;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.utils.TypeConversions;\n+\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+\n+import java.util.Arrays;\n+import java.util.LinkedHashMap;\n+\n+/**\n+ * A {@link RowDataPartitionComputer} that converts Flink objects to Hive objects before computing the partition value strings.\n+ */\n+public class HiveRowDataPartitionComputer extends RowDataPartitionComputer {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "521334c523a5004883f94e1fb996cdd18081500c"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjM0NTI4Mg==", "bodyText": "We have already get the output format class. I think it's more reliable to check the class to decide whether the table is stored as orc or parquet.", "url": "https://github.com/apache/flink/pull/12206#discussion_r426345282", "createdAt": "2020-05-18T02:49:18Z", "author": {"login": "lirui-apache"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSink.java", "diffHunk": "@@ -143,21 +206,51 @@ public HiveTableSink(JobConf jobConf, ObjectPath tablePath, CatalogTable table)\n \t\t}\n \t}\n \n-\t@Override\n-\tpublic TableSink<Row> configure(String[] fieldNames, TypeInformation<?>[] fieldTypes) {\n-\t\treturn new HiveTableSink(jobConf, tablePath, catalogTable);\n+\tprivate BulkWriter.Factory<RowData> createBulkWriterFactory(String[] partitionColumns,\n+\t\t\tStorageDescriptor sd) {\n+\t\tString serLib = sd.getSerdeInfo().getSerializationLib().toLowerCase();\n+\t\tint formatFieldCount = tableSchema.getFieldCount() - partitionColumns.length;\n+\t\tString[] formatNames = new String[formatFieldCount];\n+\t\tLogicalType[] formatTypes = new LogicalType[formatFieldCount];\n+\t\tfor (int i = 0; i < formatFieldCount; i++) {\n+\t\t\tformatNames[i] = tableSchema.getFieldName(i).get();\n+\t\t\tformatTypes[i] = tableSchema.getFieldDataType(i).get().getLogicalType();\n+\t\t}\n+\t\tRowType formatType = RowType.of(formatTypes, formatNames);\n+\t\tConfiguration formatConf = new Configuration(jobConf);\n+\t\tsd.getSerdeInfo().getParameters().forEach(formatConf::set);\n+\t\tBulkWriter.Factory<RowData> bulkFactory;\n+\t\tif (serLib.contains(\"parquet\")) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ea5f52f890c9cb5728fd7772f59bea4b38f07ca"}, "originalPosition": 235}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjM0NTg3Ng==", "bodyText": "I think we already have a HiveOutputFormatFactory, what's the difference between them?", "url": "https://github.com/apache/flink/pull/12206#discussion_r426345876", "createdAt": "2020-05-18T02:52:08Z", "author": {"login": "lirui-apache"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/write/HiveOutputFormatFactory.java", "diffHunk": "@@ -0,0 +1,81 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive.write;\n+\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.runtime.fs.hdfs.HadoopFileSystem;\n+import org.apache.flink.table.filesystem.OutputFormatFactory;\n+import org.apache.flink.types.Row;\n+\n+import org.apache.hadoop.hive.ql.exec.FileSinkOperator.RecordWriter;\n+import org.apache.hadoop.io.Writable;\n+\n+import java.io.IOException;\n+import java.util.function.Function;\n+\n+/**\n+ * Hive {@link OutputFormatFactory}, use {@link RecordWriter} to write record.\n+ */\n+public class HiveOutputFormatFactory implements OutputFormatFactory<Row> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ea5f52f890c9cb5728fd7772f59bea4b38f07ca"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjM0NzQ4OQ==", "bodyText": "Shouldn't this method be placed in OrcShim? It's strange that HiveShim needs to understand how flink-orc works.", "url": "https://github.com/apache/flink/pull/12206#discussion_r426347489", "createdAt": "2020-05-18T03:00:34Z", "author": {"login": "lirui-apache"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/client/HiveShim.java", "diffHunk": "@@ -205,4 +208,10 @@ SimpleGenericUDAFParameterInfo createUDAFParameterInfo(ObjectInspector[] params,\n \t */\n \tvoid createTableWithConstraints(IMetaStoreClient client, Table table, Configuration conf,\n \t\t\tUniqueConstraint pk, List<Byte> pkTraits, List<String> notNullCols, List<Byte> nnTraits);\n+\n+\t/**\n+\t * Create orc {@link BulkWriter.Factory} for different hive versions.\n+\t */\n+\tBulkWriter.Factory<RowData> createOrcBulkWriterFactory(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ea5f52f890c9cb5728fd7772f59bea4b38f07ca"}, "originalPosition": 20}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "299012acbbf33c20a524bfa366bb0a1d07551ec1", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/flink/commit/299012acbbf33c20a524bfa366bb0a1d07551ec1", "committedDate": "2020-05-18T03:09:38Z", "message": "Fix comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDEzMjQ4OTI5", "url": "https://github.com/apache/flink/pull/12206#pullrequestreview-413248929", "createdAt": "2020-05-18T03:19:13Z", "commit": {"oid": "299012acbbf33c20a524bfa366bb0a1d07551ec1"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4147, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}