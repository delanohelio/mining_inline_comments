{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDU1NDQ1NzMx", "number": 12964, "reviewThreads": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNFQwNjoyODo0NFrOERteoA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQwNzo1MDo0OVrOETka2w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg3MDA2MzY4OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushLimitIntoTableSourceScanRule.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNFQwNjoyODo0NFrOG2ka8g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNFQwODoxNjowMlrOG2m4-A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTg3MzAxMA==", "bodyText": "nit: simpler way: use noneMatch instead of ! + anyMatch.\nbtw, it's better we can also update the matches method of PushFilterIntoTableSourceScanRule", "url": "https://github.com/apache/flink/pull/12964#discussion_r459873010", "createdAt": "2020-07-24T06:28:44Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushLimitIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsLimitPushDown;\n+import org.apache.flink.table.plan.stats.TableStats;\n+import org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalSort;\n+import org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalTableSourceScan;\n+import org.apache.flink.table.planner.plan.schema.FlinkPreparingTableBase;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Sort;\n+import org.apache.calcite.rex.RexLiteral;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+\n+/**\n+ * Planner rule that tries to push limit into a {@link FlinkLogicalTableSourceScan},\n+ * which table is a {@link TableSourceTable}. And the table source in the table is a {@link SupportsLimitPushDown}.\n+ * The original limit will still be retained.\n+ * The reasons why the limit still be retained:\n+ * 1.If the source is required to return the exact number of limit number, the implementation\n+ * of the source is highly required. The source is required to accurately control the record\n+ * number of split, and the parallelism setting also need to be adjusted accordingly.\n+ * 2.When remove the limit, maybe filter will be pushed down to the source after limit pushed\n+ * down. The source need know it should do limit first and do the filter later, it is hard to\n+ * implement.\n+ * 3.We can support limit with offset, we can push down offset + fetch to table source.\n+ */\n+public class PushLimitIntoTableSourceScanRule extends RelOptRule {\n+\tpublic static final PushLimitIntoTableSourceScanRule INSTANCE = new PushLimitIntoTableSourceScanRule();\n+\n+\tpublic PushLimitIntoTableSourceScanRule() {\n+\t\tsuper(operand(FlinkLogicalSort.class,\n+\t\t\toperand(FlinkLogicalTableSourceScan.class, none())),\n+\t\t\t\"PushLimitIntoTableSourceScanRule\");\n+\t}\n+\n+\t@Override\n+\tpublic boolean matches(RelOptRuleCall call) {\n+\t\tSort sort = call.rel(0);\n+\t\tTableSourceTable tableSourceTable = call.rel(1).getTable().unwrap(TableSourceTable.class);\n+\n+\t\t// a limit can be pushed down only if it satisfies the two conditions: 1) do not have order by keys, 2) have limit.\n+\t\tboolean onlyLimit = sort.getCollation().getFieldCollations().isEmpty() && sort.fetch != null;\n+\t\treturn onlyLimit\n+\t\t\t&& tableSourceTable != null\n+\t\t\t&& tableSourceTable.tableSource() instanceof SupportsLimitPushDown\n+\t\t\t&& !Arrays.stream(tableSourceTable.extraDigests()).anyMatch(str -> str.startsWith(\"limit=[\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "984b8a182e5a5cc64a9e1957710e9b9fd4768a56"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTkxMzQ2NA==", "bodyText": "ok", "url": "https://github.com/apache/flink/pull/12964#discussion_r459913464", "createdAt": "2020-07-24T08:16:02Z", "author": {"login": "liuyongvs"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushLimitIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsLimitPushDown;\n+import org.apache.flink.table.plan.stats.TableStats;\n+import org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalSort;\n+import org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalTableSourceScan;\n+import org.apache.flink.table.planner.plan.schema.FlinkPreparingTableBase;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Sort;\n+import org.apache.calcite.rex.RexLiteral;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+\n+/**\n+ * Planner rule that tries to push limit into a {@link FlinkLogicalTableSourceScan},\n+ * which table is a {@link TableSourceTable}. And the table source in the table is a {@link SupportsLimitPushDown}.\n+ * The original limit will still be retained.\n+ * The reasons why the limit still be retained:\n+ * 1.If the source is required to return the exact number of limit number, the implementation\n+ * of the source is highly required. The source is required to accurately control the record\n+ * number of split, and the parallelism setting also need to be adjusted accordingly.\n+ * 2.When remove the limit, maybe filter will be pushed down to the source after limit pushed\n+ * down. The source need know it should do limit first and do the filter later, it is hard to\n+ * implement.\n+ * 3.We can support limit with offset, we can push down offset + fetch to table source.\n+ */\n+public class PushLimitIntoTableSourceScanRule extends RelOptRule {\n+\tpublic static final PushLimitIntoTableSourceScanRule INSTANCE = new PushLimitIntoTableSourceScanRule();\n+\n+\tpublic PushLimitIntoTableSourceScanRule() {\n+\t\tsuper(operand(FlinkLogicalSort.class,\n+\t\t\toperand(FlinkLogicalTableSourceScan.class, none())),\n+\t\t\t\"PushLimitIntoTableSourceScanRule\");\n+\t}\n+\n+\t@Override\n+\tpublic boolean matches(RelOptRuleCall call) {\n+\t\tSort sort = call.rel(0);\n+\t\tTableSourceTable tableSourceTable = call.rel(1).getTable().unwrap(TableSourceTable.class);\n+\n+\t\t// a limit can be pushed down only if it satisfies the two conditions: 1) do not have order by keys, 2) have limit.\n+\t\tboolean onlyLimit = sort.getCollation().getFieldCollations().isEmpty() && sort.fetch != null;\n+\t\treturn onlyLimit\n+\t\t\t&& tableSourceTable != null\n+\t\t\t&& tableSourceTable.tableSource() instanceof SupportsLimitPushDown\n+\t\t\t&& !Arrays.stream(tableSourceTable.extraDigests()).anyMatch(str -> str.startsWith(\"limit=[\"));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTg3MzAxMA=="}, "originalCommit": {"oid": "984b8a182e5a5cc64a9e1957710e9b9fd4768a56"}, "originalPosition": 70}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg3MDA3NDY1OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushLimitIntoTableSourceScanRule.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNFQwNjozNDozN1rOG2khdw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNFQwOTo1NDozMFrOG2psHg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTg3NDY3OQ==", "bodyText": "we should update the digest anyway, otherwise the rule will be applied endless loop if limit is 0.", "url": "https://github.com/apache/flink/pull/12964#discussion_r459874679", "createdAt": "2020-07-24T06:34:37Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushLimitIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsLimitPushDown;\n+import org.apache.flink.table.plan.stats.TableStats;\n+import org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalSort;\n+import org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalTableSourceScan;\n+import org.apache.flink.table.planner.plan.schema.FlinkPreparingTableBase;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Sort;\n+import org.apache.calcite.rex.RexLiteral;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+\n+/**\n+ * Planner rule that tries to push limit into a {@link FlinkLogicalTableSourceScan},\n+ * which table is a {@link TableSourceTable}. And the table source in the table is a {@link SupportsLimitPushDown}.\n+ * The original limit will still be retained.\n+ * The reasons why the limit still be retained:\n+ * 1.If the source is required to return the exact number of limit number, the implementation\n+ * of the source is highly required. The source is required to accurately control the record\n+ * number of split, and the parallelism setting also need to be adjusted accordingly.\n+ * 2.When remove the limit, maybe filter will be pushed down to the source after limit pushed\n+ * down. The source need know it should do limit first and do the filter later, it is hard to\n+ * implement.\n+ * 3.We can support limit with offset, we can push down offset + fetch to table source.\n+ */\n+public class PushLimitIntoTableSourceScanRule extends RelOptRule {\n+\tpublic static final PushLimitIntoTableSourceScanRule INSTANCE = new PushLimitIntoTableSourceScanRule();\n+\n+\tpublic PushLimitIntoTableSourceScanRule() {\n+\t\tsuper(operand(FlinkLogicalSort.class,\n+\t\t\toperand(FlinkLogicalTableSourceScan.class, none())),\n+\t\t\t\"PushLimitIntoTableSourceScanRule\");\n+\t}\n+\n+\t@Override\n+\tpublic boolean matches(RelOptRuleCall call) {\n+\t\tSort sort = call.rel(0);\n+\t\tTableSourceTable tableSourceTable = call.rel(1).getTable().unwrap(TableSourceTable.class);\n+\n+\t\t// a limit can be pushed down only if it satisfies the two conditions: 1) do not have order by keys, 2) have limit.\n+\t\tboolean onlyLimit = sort.getCollation().getFieldCollations().isEmpty() && sort.fetch != null;\n+\t\treturn onlyLimit\n+\t\t\t&& tableSourceTable != null\n+\t\t\t&& tableSourceTable.tableSource() instanceof SupportsLimitPushDown\n+\t\t\t&& !Arrays.stream(tableSourceTable.extraDigests()).anyMatch(str -> str.startsWith(\"limit=[\"));\n+\t}\n+\n+\t@Override\n+\tpublic void onMatch(RelOptRuleCall call) {\n+\t\tSort sort = call.rel(0);\n+\t\tFlinkLogicalTableSourceScan scan = call.rel(1);\n+\t\tTableSourceTable tableSourceTable = scan.getTable().unwrap(TableSourceTable.class);\n+\t\tint offset = sort.offset == null ? 0 : RexLiteral.intValue(sort.offset);\n+\t\tint limit = offset + RexLiteral.intValue(sort.fetch);\n+\n+\t\tTableSourceTable newTableSourceTable = applyLimit(limit, tableSourceTable);\n+\n+\t\tFlinkLogicalTableSourceScan newScan = FlinkLogicalTableSourceScan.create(scan.getCluster(), newTableSourceTable);\n+\t\tcall.transformTo(sort.copy(sort.getTraitSet(), Collections.singletonList(newScan)));\n+\t}\n+\n+\tprivate TableSourceTable applyLimit(\n+\t\tlong limit,\n+\t\tFlinkPreparingTableBase relOptTable) {\n+\t\tTableSourceTable oldTableSourceTable = relOptTable.unwrap(TableSourceTable.class);\n+\t\tDynamicTableSource newTableSource = oldTableSourceTable.tableSource().copy();\n+\t\t((SupportsLimitPushDown) newTableSource).applyLimit(limit);\n+\n+\t\tFlinkStatistic statistic = relOptTable.getStatistic();\n+\t\tlong newRowCount = 0;\n+\t\tif (statistic.getRowCount() != null) {\n+\t\t\tnewRowCount = Math.min(limit, statistic.getRowCount().longValue());\n+\t\t} else {\n+\t\t\tnewRowCount = limit;\n+\t\t}\n+\t\t// update TableStats after limit push down\n+\t\tTableStats newTableStats = new TableStats(newRowCount);\n+\t\tFlinkStatistic newStatistic = FlinkStatistic.builder()\n+\t\t\t.statistic(statistic)\n+\t\t\t.tableStats(newTableStats)\n+\t\t\t.build();\n+\n+\t\t// update extraDigests\n+\t\tString[] newExtraDigests = new String[0];\n+\t\tif (limit > 0) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "984b8a182e5a5cc64a9e1957710e9b9fd4768a56"}, "originalPosition": 110}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTkxNjEyMA==", "bodyText": "But it will look strange, like this limit=[0] And this rule FlinkLimit0RemoveRule will remove it.", "url": "https://github.com/apache/flink/pull/12964#discussion_r459916120", "createdAt": "2020-07-24T08:21:42Z", "author": {"login": "liuyongvs"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushLimitIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsLimitPushDown;\n+import org.apache.flink.table.plan.stats.TableStats;\n+import org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalSort;\n+import org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalTableSourceScan;\n+import org.apache.flink.table.planner.plan.schema.FlinkPreparingTableBase;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Sort;\n+import org.apache.calcite.rex.RexLiteral;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+\n+/**\n+ * Planner rule that tries to push limit into a {@link FlinkLogicalTableSourceScan},\n+ * which table is a {@link TableSourceTable}. And the table source in the table is a {@link SupportsLimitPushDown}.\n+ * The original limit will still be retained.\n+ * The reasons why the limit still be retained:\n+ * 1.If the source is required to return the exact number of limit number, the implementation\n+ * of the source is highly required. The source is required to accurately control the record\n+ * number of split, and the parallelism setting also need to be adjusted accordingly.\n+ * 2.When remove the limit, maybe filter will be pushed down to the source after limit pushed\n+ * down. The source need know it should do limit first and do the filter later, it is hard to\n+ * implement.\n+ * 3.We can support limit with offset, we can push down offset + fetch to table source.\n+ */\n+public class PushLimitIntoTableSourceScanRule extends RelOptRule {\n+\tpublic static final PushLimitIntoTableSourceScanRule INSTANCE = new PushLimitIntoTableSourceScanRule();\n+\n+\tpublic PushLimitIntoTableSourceScanRule() {\n+\t\tsuper(operand(FlinkLogicalSort.class,\n+\t\t\toperand(FlinkLogicalTableSourceScan.class, none())),\n+\t\t\t\"PushLimitIntoTableSourceScanRule\");\n+\t}\n+\n+\t@Override\n+\tpublic boolean matches(RelOptRuleCall call) {\n+\t\tSort sort = call.rel(0);\n+\t\tTableSourceTable tableSourceTable = call.rel(1).getTable().unwrap(TableSourceTable.class);\n+\n+\t\t// a limit can be pushed down only if it satisfies the two conditions: 1) do not have order by keys, 2) have limit.\n+\t\tboolean onlyLimit = sort.getCollation().getFieldCollations().isEmpty() && sort.fetch != null;\n+\t\treturn onlyLimit\n+\t\t\t&& tableSourceTable != null\n+\t\t\t&& tableSourceTable.tableSource() instanceof SupportsLimitPushDown\n+\t\t\t&& !Arrays.stream(tableSourceTable.extraDigests()).anyMatch(str -> str.startsWith(\"limit=[\"));\n+\t}\n+\n+\t@Override\n+\tpublic void onMatch(RelOptRuleCall call) {\n+\t\tSort sort = call.rel(0);\n+\t\tFlinkLogicalTableSourceScan scan = call.rel(1);\n+\t\tTableSourceTable tableSourceTable = scan.getTable().unwrap(TableSourceTable.class);\n+\t\tint offset = sort.offset == null ? 0 : RexLiteral.intValue(sort.offset);\n+\t\tint limit = offset + RexLiteral.intValue(sort.fetch);\n+\n+\t\tTableSourceTable newTableSourceTable = applyLimit(limit, tableSourceTable);\n+\n+\t\tFlinkLogicalTableSourceScan newScan = FlinkLogicalTableSourceScan.create(scan.getCluster(), newTableSourceTable);\n+\t\tcall.transformTo(sort.copy(sort.getTraitSet(), Collections.singletonList(newScan)));\n+\t}\n+\n+\tprivate TableSourceTable applyLimit(\n+\t\tlong limit,\n+\t\tFlinkPreparingTableBase relOptTable) {\n+\t\tTableSourceTable oldTableSourceTable = relOptTable.unwrap(TableSourceTable.class);\n+\t\tDynamicTableSource newTableSource = oldTableSourceTable.tableSource().copy();\n+\t\t((SupportsLimitPushDown) newTableSource).applyLimit(limit);\n+\n+\t\tFlinkStatistic statistic = relOptTable.getStatistic();\n+\t\tlong newRowCount = 0;\n+\t\tif (statistic.getRowCount() != null) {\n+\t\t\tnewRowCount = Math.min(limit, statistic.getRowCount().longValue());\n+\t\t} else {\n+\t\t\tnewRowCount = limit;\n+\t\t}\n+\t\t// update TableStats after limit push down\n+\t\tTableStats newTableStats = new TableStats(newRowCount);\n+\t\tFlinkStatistic newStatistic = FlinkStatistic.builder()\n+\t\t\t.statistic(statistic)\n+\t\t\t.tableStats(newTableStats)\n+\t\t\t.build();\n+\n+\t\t// update extraDigests\n+\t\tString[] newExtraDigests = new String[0];\n+\t\tif (limit > 0) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTg3NDY3OQ=="}, "originalCommit": {"oid": "984b8a182e5a5cc64a9e1957710e9b9fd4768a56"}, "originalPosition": 110}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTk1OTMyNg==", "bodyText": "We can't rely on other rules to ensure the correctness of this rule, we must make sure each rule itself is correct.", "url": "https://github.com/apache/flink/pull/12964#discussion_r459959326", "createdAt": "2020-07-24T09:54:30Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushLimitIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsLimitPushDown;\n+import org.apache.flink.table.plan.stats.TableStats;\n+import org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalSort;\n+import org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalTableSourceScan;\n+import org.apache.flink.table.planner.plan.schema.FlinkPreparingTableBase;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Sort;\n+import org.apache.calcite.rex.RexLiteral;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+\n+/**\n+ * Planner rule that tries to push limit into a {@link FlinkLogicalTableSourceScan},\n+ * which table is a {@link TableSourceTable}. And the table source in the table is a {@link SupportsLimitPushDown}.\n+ * The original limit will still be retained.\n+ * The reasons why the limit still be retained:\n+ * 1.If the source is required to return the exact number of limit number, the implementation\n+ * of the source is highly required. The source is required to accurately control the record\n+ * number of split, and the parallelism setting also need to be adjusted accordingly.\n+ * 2.When remove the limit, maybe filter will be pushed down to the source after limit pushed\n+ * down. The source need know it should do limit first and do the filter later, it is hard to\n+ * implement.\n+ * 3.We can support limit with offset, we can push down offset + fetch to table source.\n+ */\n+public class PushLimitIntoTableSourceScanRule extends RelOptRule {\n+\tpublic static final PushLimitIntoTableSourceScanRule INSTANCE = new PushLimitIntoTableSourceScanRule();\n+\n+\tpublic PushLimitIntoTableSourceScanRule() {\n+\t\tsuper(operand(FlinkLogicalSort.class,\n+\t\t\toperand(FlinkLogicalTableSourceScan.class, none())),\n+\t\t\t\"PushLimitIntoTableSourceScanRule\");\n+\t}\n+\n+\t@Override\n+\tpublic boolean matches(RelOptRuleCall call) {\n+\t\tSort sort = call.rel(0);\n+\t\tTableSourceTable tableSourceTable = call.rel(1).getTable().unwrap(TableSourceTable.class);\n+\n+\t\t// a limit can be pushed down only if it satisfies the two conditions: 1) do not have order by keys, 2) have limit.\n+\t\tboolean onlyLimit = sort.getCollation().getFieldCollations().isEmpty() && sort.fetch != null;\n+\t\treturn onlyLimit\n+\t\t\t&& tableSourceTable != null\n+\t\t\t&& tableSourceTable.tableSource() instanceof SupportsLimitPushDown\n+\t\t\t&& !Arrays.stream(tableSourceTable.extraDigests()).anyMatch(str -> str.startsWith(\"limit=[\"));\n+\t}\n+\n+\t@Override\n+\tpublic void onMatch(RelOptRuleCall call) {\n+\t\tSort sort = call.rel(0);\n+\t\tFlinkLogicalTableSourceScan scan = call.rel(1);\n+\t\tTableSourceTable tableSourceTable = scan.getTable().unwrap(TableSourceTable.class);\n+\t\tint offset = sort.offset == null ? 0 : RexLiteral.intValue(sort.offset);\n+\t\tint limit = offset + RexLiteral.intValue(sort.fetch);\n+\n+\t\tTableSourceTable newTableSourceTable = applyLimit(limit, tableSourceTable);\n+\n+\t\tFlinkLogicalTableSourceScan newScan = FlinkLogicalTableSourceScan.create(scan.getCluster(), newTableSourceTable);\n+\t\tcall.transformTo(sort.copy(sort.getTraitSet(), Collections.singletonList(newScan)));\n+\t}\n+\n+\tprivate TableSourceTable applyLimit(\n+\t\tlong limit,\n+\t\tFlinkPreparingTableBase relOptTable) {\n+\t\tTableSourceTable oldTableSourceTable = relOptTable.unwrap(TableSourceTable.class);\n+\t\tDynamicTableSource newTableSource = oldTableSourceTable.tableSource().copy();\n+\t\t((SupportsLimitPushDown) newTableSource).applyLimit(limit);\n+\n+\t\tFlinkStatistic statistic = relOptTable.getStatistic();\n+\t\tlong newRowCount = 0;\n+\t\tif (statistic.getRowCount() != null) {\n+\t\t\tnewRowCount = Math.min(limit, statistic.getRowCount().longValue());\n+\t\t} else {\n+\t\t\tnewRowCount = limit;\n+\t\t}\n+\t\t// update TableStats after limit push down\n+\t\tTableStats newTableStats = new TableStats(newRowCount);\n+\t\tFlinkStatistic newStatistic = FlinkStatistic.builder()\n+\t\t\t.statistic(statistic)\n+\t\t\t.tableStats(newTableStats)\n+\t\t\t.build();\n+\n+\t\t// update extraDigests\n+\t\tString[] newExtraDigests = new String[0];\n+\t\tif (limit > 0) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTg3NDY3OQ=="}, "originalCommit": {"oid": "984b8a182e5a5cc64a9e1957710e9b9fd4768a56"}, "originalPosition": 110}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg3MDA3ODkyOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushLimitIntoTableSourceScanRule.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNFQwNjozNjo0OVrOG2kjxQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNFQwODoxNjoxMlrOG2m5UA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTg3NTI2OQ==", "bodyText": "It's better we can put sort.copy(sort.getTraitSet(), Collections.singletonList(newScan)) in a single line, which could make debugging more easy.", "url": "https://github.com/apache/flink/pull/12964#discussion_r459875269", "createdAt": "2020-07-24T06:36:49Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushLimitIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsLimitPushDown;\n+import org.apache.flink.table.plan.stats.TableStats;\n+import org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalSort;\n+import org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalTableSourceScan;\n+import org.apache.flink.table.planner.plan.schema.FlinkPreparingTableBase;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Sort;\n+import org.apache.calcite.rex.RexLiteral;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+\n+/**\n+ * Planner rule that tries to push limit into a {@link FlinkLogicalTableSourceScan},\n+ * which table is a {@link TableSourceTable}. And the table source in the table is a {@link SupportsLimitPushDown}.\n+ * The original limit will still be retained.\n+ * The reasons why the limit still be retained:\n+ * 1.If the source is required to return the exact number of limit number, the implementation\n+ * of the source is highly required. The source is required to accurately control the record\n+ * number of split, and the parallelism setting also need to be adjusted accordingly.\n+ * 2.When remove the limit, maybe filter will be pushed down to the source after limit pushed\n+ * down. The source need know it should do limit first and do the filter later, it is hard to\n+ * implement.\n+ * 3.We can support limit with offset, we can push down offset + fetch to table source.\n+ */\n+public class PushLimitIntoTableSourceScanRule extends RelOptRule {\n+\tpublic static final PushLimitIntoTableSourceScanRule INSTANCE = new PushLimitIntoTableSourceScanRule();\n+\n+\tpublic PushLimitIntoTableSourceScanRule() {\n+\t\tsuper(operand(FlinkLogicalSort.class,\n+\t\t\toperand(FlinkLogicalTableSourceScan.class, none())),\n+\t\t\t\"PushLimitIntoTableSourceScanRule\");\n+\t}\n+\n+\t@Override\n+\tpublic boolean matches(RelOptRuleCall call) {\n+\t\tSort sort = call.rel(0);\n+\t\tTableSourceTable tableSourceTable = call.rel(1).getTable().unwrap(TableSourceTable.class);\n+\n+\t\t// a limit can be pushed down only if it satisfies the two conditions: 1) do not have order by keys, 2) have limit.\n+\t\tboolean onlyLimit = sort.getCollation().getFieldCollations().isEmpty() && sort.fetch != null;\n+\t\treturn onlyLimit\n+\t\t\t&& tableSourceTable != null\n+\t\t\t&& tableSourceTable.tableSource() instanceof SupportsLimitPushDown\n+\t\t\t&& !Arrays.stream(tableSourceTable.extraDigests()).anyMatch(str -> str.startsWith(\"limit=[\"));\n+\t}\n+\n+\t@Override\n+\tpublic void onMatch(RelOptRuleCall call) {\n+\t\tSort sort = call.rel(0);\n+\t\tFlinkLogicalTableSourceScan scan = call.rel(1);\n+\t\tTableSourceTable tableSourceTable = scan.getTable().unwrap(TableSourceTable.class);\n+\t\tint offset = sort.offset == null ? 0 : RexLiteral.intValue(sort.offset);\n+\t\tint limit = offset + RexLiteral.intValue(sort.fetch);\n+\n+\t\tTableSourceTable newTableSourceTable = applyLimit(limit, tableSourceTable);\n+\n+\t\tFlinkLogicalTableSourceScan newScan = FlinkLogicalTableSourceScan.create(scan.getCluster(), newTableSourceTable);\n+\t\tcall.transformTo(sort.copy(sort.getTraitSet(), Collections.singletonList(newScan)));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "984b8a182e5a5cc64a9e1957710e9b9fd4768a56"}, "originalPosition": 84}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTkxMzU1Mg==", "bodyText": "ok", "url": "https://github.com/apache/flink/pull/12964#discussion_r459913552", "createdAt": "2020-07-24T08:16:12Z", "author": {"login": "liuyongvs"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushLimitIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsLimitPushDown;\n+import org.apache.flink.table.plan.stats.TableStats;\n+import org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalSort;\n+import org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalTableSourceScan;\n+import org.apache.flink.table.planner.plan.schema.FlinkPreparingTableBase;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Sort;\n+import org.apache.calcite.rex.RexLiteral;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+\n+/**\n+ * Planner rule that tries to push limit into a {@link FlinkLogicalTableSourceScan},\n+ * which table is a {@link TableSourceTable}. And the table source in the table is a {@link SupportsLimitPushDown}.\n+ * The original limit will still be retained.\n+ * The reasons why the limit still be retained:\n+ * 1.If the source is required to return the exact number of limit number, the implementation\n+ * of the source is highly required. The source is required to accurately control the record\n+ * number of split, and the parallelism setting also need to be adjusted accordingly.\n+ * 2.When remove the limit, maybe filter will be pushed down to the source after limit pushed\n+ * down. The source need know it should do limit first and do the filter later, it is hard to\n+ * implement.\n+ * 3.We can support limit with offset, we can push down offset + fetch to table source.\n+ */\n+public class PushLimitIntoTableSourceScanRule extends RelOptRule {\n+\tpublic static final PushLimitIntoTableSourceScanRule INSTANCE = new PushLimitIntoTableSourceScanRule();\n+\n+\tpublic PushLimitIntoTableSourceScanRule() {\n+\t\tsuper(operand(FlinkLogicalSort.class,\n+\t\t\toperand(FlinkLogicalTableSourceScan.class, none())),\n+\t\t\t\"PushLimitIntoTableSourceScanRule\");\n+\t}\n+\n+\t@Override\n+\tpublic boolean matches(RelOptRuleCall call) {\n+\t\tSort sort = call.rel(0);\n+\t\tTableSourceTable tableSourceTable = call.rel(1).getTable().unwrap(TableSourceTable.class);\n+\n+\t\t// a limit can be pushed down only if it satisfies the two conditions: 1) do not have order by keys, 2) have limit.\n+\t\tboolean onlyLimit = sort.getCollation().getFieldCollations().isEmpty() && sort.fetch != null;\n+\t\treturn onlyLimit\n+\t\t\t&& tableSourceTable != null\n+\t\t\t&& tableSourceTable.tableSource() instanceof SupportsLimitPushDown\n+\t\t\t&& !Arrays.stream(tableSourceTable.extraDigests()).anyMatch(str -> str.startsWith(\"limit=[\"));\n+\t}\n+\n+\t@Override\n+\tpublic void onMatch(RelOptRuleCall call) {\n+\t\tSort sort = call.rel(0);\n+\t\tFlinkLogicalTableSourceScan scan = call.rel(1);\n+\t\tTableSourceTable tableSourceTable = scan.getTable().unwrap(TableSourceTable.class);\n+\t\tint offset = sort.offset == null ? 0 : RexLiteral.intValue(sort.offset);\n+\t\tint limit = offset + RexLiteral.intValue(sort.fetch);\n+\n+\t\tTableSourceTable newTableSourceTable = applyLimit(limit, tableSourceTable);\n+\n+\t\tFlinkLogicalTableSourceScan newScan = FlinkLogicalTableSourceScan.create(scan.getCluster(), newTableSourceTable);\n+\t\tcall.transformTo(sort.copy(sort.getTraitSet(), Collections.singletonList(newScan)));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTg3NTI2OQ=="}, "originalCommit": {"oid": "984b8a182e5a5cc64a9e1957710e9b9fd4768a56"}, "originalPosition": 84}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg3MDA5MDYzOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/factories/TestValuesTableFactory.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNFQwNjo0MDo0MlrOG2kpnw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNFQwODoxNjoxOFrOG2m5gQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTg3Njc2Nw==", "bodyText": "this line is too long...", "url": "https://github.com/apache/flink/pull/12964#discussion_r459876767", "createdAt": "2020-07-24T06:40:42Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/factories/TestValuesTableFactory.java", "diffHunk": "@@ -359,7 +361,7 @@ private ChangelogMode parseChangelogMode(String string) {\n \t/**\n \t * Values {@link DynamicTableSource} for testing.\n \t */\n-\tprivate static class TestValuesTableSource implements ScanTableSource, LookupTableSource, SupportsProjectionPushDown, SupportsFilterPushDown {\n+\tprivate static class TestValuesTableSource implements ScanTableSource, LookupTableSource, SupportsProjectionPushDown, SupportsFilterPushDown, SupportsLimitPushDown {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "984b8a182e5a5cc64a9e1957710e9b9fd4768a56"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTkxMzYwMQ==", "bodyText": "ok", "url": "https://github.com/apache/flink/pull/12964#discussion_r459913601", "createdAt": "2020-07-24T08:16:18Z", "author": {"login": "liuyongvs"}, "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/factories/TestValuesTableFactory.java", "diffHunk": "@@ -359,7 +361,7 @@ private ChangelogMode parseChangelogMode(String string) {\n \t/**\n \t * Values {@link DynamicTableSource} for testing.\n \t */\n-\tprivate static class TestValuesTableSource implements ScanTableSource, LookupTableSource, SupportsProjectionPushDown, SupportsFilterPushDown {\n+\tprivate static class TestValuesTableSource implements ScanTableSource, LookupTableSource, SupportsProjectionPushDown, SupportsFilterPushDown, SupportsLimitPushDown {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTg3Njc2Nw=="}, "originalCommit": {"oid": "984b8a182e5a5cc64a9e1957710e9b9fd4768a56"}, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg3MDEwNjA2OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushLimitIntoTableSourceScanRule.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNFQwNjo0ODoyNVrOG2kyPA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNFQwOTo1ODoyMFrOG2py6Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTg3ODk3Mg==", "bodyText": "please add a rule test to verify this rule\uff0c just like PushFilterIntoTableSourceScanRuleTest", "url": "https://github.com/apache/flink/pull/12964#discussion_r459878972", "createdAt": "2020-07-24T06:48:25Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushLimitIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsLimitPushDown;\n+import org.apache.flink.table.plan.stats.TableStats;\n+import org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalSort;\n+import org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalTableSourceScan;\n+import org.apache.flink.table.planner.plan.schema.FlinkPreparingTableBase;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Sort;\n+import org.apache.calcite.rex.RexLiteral;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+\n+/**\n+ * Planner rule that tries to push limit into a {@link FlinkLogicalTableSourceScan},\n+ * which table is a {@link TableSourceTable}. And the table source in the table is a {@link SupportsLimitPushDown}.\n+ * The original limit will still be retained.\n+ * The reasons why the limit still be retained:\n+ * 1.If the source is required to return the exact number of limit number, the implementation\n+ * of the source is highly required. The source is required to accurately control the record\n+ * number of split, and the parallelism setting also need to be adjusted accordingly.\n+ * 2.When remove the limit, maybe filter will be pushed down to the source after limit pushed\n+ * down. The source need know it should do limit first and do the filter later, it is hard to\n+ * implement.\n+ * 3.We can support limit with offset, we can push down offset + fetch to table source.\n+ */\n+public class PushLimitIntoTableSourceScanRule extends RelOptRule {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "984b8a182e5a5cc64a9e1957710e9b9fd4768a56"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTkxMzkyOA==", "bodyText": "the unit test is LimitTest.", "url": "https://github.com/apache/flink/pull/12964#discussion_r459913928", "createdAt": "2020-07-24T08:16:53Z", "author": {"login": "liuyongvs"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushLimitIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsLimitPushDown;\n+import org.apache.flink.table.plan.stats.TableStats;\n+import org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalSort;\n+import org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalTableSourceScan;\n+import org.apache.flink.table.planner.plan.schema.FlinkPreparingTableBase;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Sort;\n+import org.apache.calcite.rex.RexLiteral;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+\n+/**\n+ * Planner rule that tries to push limit into a {@link FlinkLogicalTableSourceScan},\n+ * which table is a {@link TableSourceTable}. And the table source in the table is a {@link SupportsLimitPushDown}.\n+ * The original limit will still be retained.\n+ * The reasons why the limit still be retained:\n+ * 1.If the source is required to return the exact number of limit number, the implementation\n+ * of the source is highly required. The source is required to accurately control the record\n+ * number of split, and the parallelism setting also need to be adjusted accordingly.\n+ * 2.When remove the limit, maybe filter will be pushed down to the source after limit pushed\n+ * down. The source need know it should do limit first and do the filter later, it is hard to\n+ * implement.\n+ * 3.We can support limit with offset, we can push down offset + fetch to table source.\n+ */\n+public class PushLimitIntoTableSourceScanRule extends RelOptRule {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTg3ODk3Mg=="}, "originalCommit": {"oid": "984b8a182e5a5cc64a9e1957710e9b9fd4768a56"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTk2MTA2NQ==", "bodyText": "Rule test is different from plan test. Rule test focuses on the new rule you implement and only a few must-involved rules can be added to the rule set to help the test. Plan test will verify the plan based on whole rule sets.", "url": "https://github.com/apache/flink/pull/12964#discussion_r459961065", "createdAt": "2020-07-24T09:58:20Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushLimitIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsLimitPushDown;\n+import org.apache.flink.table.plan.stats.TableStats;\n+import org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalSort;\n+import org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalTableSourceScan;\n+import org.apache.flink.table.planner.plan.schema.FlinkPreparingTableBase;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Sort;\n+import org.apache.calcite.rex.RexLiteral;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+\n+/**\n+ * Planner rule that tries to push limit into a {@link FlinkLogicalTableSourceScan},\n+ * which table is a {@link TableSourceTable}. And the table source in the table is a {@link SupportsLimitPushDown}.\n+ * The original limit will still be retained.\n+ * The reasons why the limit still be retained:\n+ * 1.If the source is required to return the exact number of limit number, the implementation\n+ * of the source is highly required. The source is required to accurately control the record\n+ * number of split, and the parallelism setting also need to be adjusted accordingly.\n+ * 2.When remove the limit, maybe filter will be pushed down to the source after limit pushed\n+ * down. The source need know it should do limit first and do the filter later, it is hard to\n+ * implement.\n+ * 3.We can support limit with offset, we can push down offset + fetch to table source.\n+ */\n+public class PushLimitIntoTableSourceScanRule extends RelOptRule {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTg3ODk3Mg=="}, "originalCommit": {"oid": "984b8a182e5a5cc64a9e1957710e9b9fd4768a56"}, "originalPosition": 51}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4MDcxMzg1OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushLimitIntoTableSourceScanRule.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQwOToxNjozN1rOG4EB9w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQwNzowMDozNlrOG5WFUg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTQzOTQ3OQ==", "bodyText": "how about we remove the if ?", "url": "https://github.com/apache/flink/pull/12964#discussion_r461439479", "createdAt": "2020-07-28T09:16:37Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushLimitIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,122 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsLimitPushDown;\n+import org.apache.flink.table.plan.stats.TableStats;\n+import org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalSort;\n+import org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalTableSourceScan;\n+import org.apache.flink.table.planner.plan.schema.FlinkPreparingTableBase;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Sort;\n+import org.apache.calcite.rex.RexLiteral;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+\n+/**\n+ * Planner rule that tries to push limit into a {@link FlinkLogicalTableSourceScan},\n+ * which table is a {@link TableSourceTable}. And the table source in the table is a {@link SupportsLimitPushDown}.\n+ * The original limit will still be retained.\n+ * The reasons why the limit still be retained:\n+ * 1.If the source is required to return the exact number of limit number, the implementation\n+ * of the source is highly required. The source is required to accurately control the record\n+ * number of split, and the parallelism setting also need to be adjusted accordingly.\n+ * 2.When remove the limit, maybe filter will be pushed down to the source after limit pushed\n+ * down. The source need know it should do limit first and do the filter later, it is hard to\n+ * implement.\n+ * 3.We can support limit with offset, we can push down offset + fetch to table source.\n+ */\n+public class PushLimitIntoTableSourceScanRule extends RelOptRule {\n+\tpublic static final PushLimitIntoTableSourceScanRule INSTANCE = new PushLimitIntoTableSourceScanRule();\n+\n+\tpublic PushLimitIntoTableSourceScanRule() {\n+\t\tsuper(operand(FlinkLogicalSort.class,\n+\t\t\toperand(FlinkLogicalTableSourceScan.class, none())),\n+\t\t\t\"PushLimitIntoTableSourceScanRule\");\n+\t}\n+\n+\t@Override\n+\tpublic boolean matches(RelOptRuleCall call) {\n+\t\tSort sort = call.rel(0);\n+\t\tTableSourceTable tableSourceTable = call.rel(1).getTable().unwrap(TableSourceTable.class);\n+\n+\t\t// a limit can be pushed down only if it satisfies the two conditions: 1) do not have order by keys, 2) have limit.\n+\t\tboolean onlyLimit = sort.getCollation().getFieldCollations().isEmpty() && sort.fetch != null;\n+\t\treturn onlyLimit\n+\t\t\t&& tableSourceTable != null\n+\t\t\t&& tableSourceTable.tableSource() instanceof SupportsLimitPushDown\n+\t\t\t&& Arrays.stream(tableSourceTable.extraDigests()).noneMatch(str -> str.startsWith(\"limit=[\"));\n+\t}\n+\n+\t@Override\n+\tpublic void onMatch(RelOptRuleCall call) {\n+\t\tSort sort = call.rel(0);\n+\t\tFlinkLogicalTableSourceScan scan = call.rel(1);\n+\t\tTableSourceTable tableSourceTable = scan.getTable().unwrap(TableSourceTable.class);\n+\t\tint offset = sort.offset == null ? 0 : RexLiteral.intValue(sort.offset);\n+\t\tint limit = offset + RexLiteral.intValue(sort.fetch);\n+\n+\t\tTableSourceTable newTableSourceTable = applyLimit(limit, tableSourceTable);\n+\n+\t\tFlinkLogicalTableSourceScan newScan = FlinkLogicalTableSourceScan.create(scan.getCluster(), newTableSourceTable);\n+\t\tSort newSort = sort.copy(sort.getTraitSet(), Collections.singletonList(newScan));\n+\t\tcall.transformTo(newSort);\n+\t}\n+\n+\tprivate TableSourceTable applyLimit(\n+\t\tlong limit,\n+\t\tFlinkPreparingTableBase relOptTable) {\n+\t\tTableSourceTable oldTableSourceTable = relOptTable.unwrap(TableSourceTable.class);\n+\t\tDynamicTableSource newTableSource = oldTableSourceTable.tableSource().copy();\n+\t\t((SupportsLimitPushDown) newTableSource).applyLimit(limit);\n+\n+\t\tFlinkStatistic statistic = relOptTable.getStatistic();\n+\t\tlong newRowCount = 0;\n+\t\tif (statistic.getRowCount() != null) {\n+\t\t\tnewRowCount = Math.min(limit, statistic.getRowCount().longValue());\n+\t\t} else {\n+\t\t\tnewRowCount = limit;\n+\t\t}\n+\t\t// update TableStats after limit push down\n+\t\tTableStats newTableStats = new TableStats(newRowCount);\n+\t\tFlinkStatistic newStatistic = FlinkStatistic.builder()\n+\t\t\t.statistic(statistic)\n+\t\t\t.tableStats(newTableStats)\n+\t\t\t.build();\n+\n+\t\t// update extraDigests\n+\t\tString[] newExtraDigests = new String[0];\n+\t\tif (limit >= 0) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0324c22a9b2c85f04481966b2a78f72e92b9c88c"}, "originalPosition": 111}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTQ4ODU1NA==", "bodyText": "I think it can not be removed. Although, it doesn't have problem now. But if the calcite supports this syntax limit x,y, which is the mysql limit offset syntax. y represents limit, which can be negative, such as -1 , you can refer this https://www.cnblogs.com/acm-bingzi/p/msqlLimit.html.\nAnd you can also read the code of calcite FlinkSqlParserImpl.OrderedQueryOrExpr, where has some comments.\nThat is the reason why i add this test testMysqlLimit.", "url": "https://github.com/apache/flink/pull/12964#discussion_r461488554", "createdAt": "2020-07-28T10:45:43Z", "author": {"login": "liuyongvs"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushLimitIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,122 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsLimitPushDown;\n+import org.apache.flink.table.plan.stats.TableStats;\n+import org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalSort;\n+import org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalTableSourceScan;\n+import org.apache.flink.table.planner.plan.schema.FlinkPreparingTableBase;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Sort;\n+import org.apache.calcite.rex.RexLiteral;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+\n+/**\n+ * Planner rule that tries to push limit into a {@link FlinkLogicalTableSourceScan},\n+ * which table is a {@link TableSourceTable}. And the table source in the table is a {@link SupportsLimitPushDown}.\n+ * The original limit will still be retained.\n+ * The reasons why the limit still be retained:\n+ * 1.If the source is required to return the exact number of limit number, the implementation\n+ * of the source is highly required. The source is required to accurately control the record\n+ * number of split, and the parallelism setting also need to be adjusted accordingly.\n+ * 2.When remove the limit, maybe filter will be pushed down to the source after limit pushed\n+ * down. The source need know it should do limit first and do the filter later, it is hard to\n+ * implement.\n+ * 3.We can support limit with offset, we can push down offset + fetch to table source.\n+ */\n+public class PushLimitIntoTableSourceScanRule extends RelOptRule {\n+\tpublic static final PushLimitIntoTableSourceScanRule INSTANCE = new PushLimitIntoTableSourceScanRule();\n+\n+\tpublic PushLimitIntoTableSourceScanRule() {\n+\t\tsuper(operand(FlinkLogicalSort.class,\n+\t\t\toperand(FlinkLogicalTableSourceScan.class, none())),\n+\t\t\t\"PushLimitIntoTableSourceScanRule\");\n+\t}\n+\n+\t@Override\n+\tpublic boolean matches(RelOptRuleCall call) {\n+\t\tSort sort = call.rel(0);\n+\t\tTableSourceTable tableSourceTable = call.rel(1).getTable().unwrap(TableSourceTable.class);\n+\n+\t\t// a limit can be pushed down only if it satisfies the two conditions: 1) do not have order by keys, 2) have limit.\n+\t\tboolean onlyLimit = sort.getCollation().getFieldCollations().isEmpty() && sort.fetch != null;\n+\t\treturn onlyLimit\n+\t\t\t&& tableSourceTable != null\n+\t\t\t&& tableSourceTable.tableSource() instanceof SupportsLimitPushDown\n+\t\t\t&& Arrays.stream(tableSourceTable.extraDigests()).noneMatch(str -> str.startsWith(\"limit=[\"));\n+\t}\n+\n+\t@Override\n+\tpublic void onMatch(RelOptRuleCall call) {\n+\t\tSort sort = call.rel(0);\n+\t\tFlinkLogicalTableSourceScan scan = call.rel(1);\n+\t\tTableSourceTable tableSourceTable = scan.getTable().unwrap(TableSourceTable.class);\n+\t\tint offset = sort.offset == null ? 0 : RexLiteral.intValue(sort.offset);\n+\t\tint limit = offset + RexLiteral.intValue(sort.fetch);\n+\n+\t\tTableSourceTable newTableSourceTable = applyLimit(limit, tableSourceTable);\n+\n+\t\tFlinkLogicalTableSourceScan newScan = FlinkLogicalTableSourceScan.create(scan.getCluster(), newTableSourceTable);\n+\t\tSort newSort = sort.copy(sort.getTraitSet(), Collections.singletonList(newScan));\n+\t\tcall.transformTo(newSort);\n+\t}\n+\n+\tprivate TableSourceTable applyLimit(\n+\t\tlong limit,\n+\t\tFlinkPreparingTableBase relOptTable) {\n+\t\tTableSourceTable oldTableSourceTable = relOptTable.unwrap(TableSourceTable.class);\n+\t\tDynamicTableSource newTableSource = oldTableSourceTable.tableSource().copy();\n+\t\t((SupportsLimitPushDown) newTableSource).applyLimit(limit);\n+\n+\t\tFlinkStatistic statistic = relOptTable.getStatistic();\n+\t\tlong newRowCount = 0;\n+\t\tif (statistic.getRowCount() != null) {\n+\t\t\tnewRowCount = Math.min(limit, statistic.getRowCount().longValue());\n+\t\t} else {\n+\t\t\tnewRowCount = limit;\n+\t\t}\n+\t\t// update TableStats after limit push down\n+\t\tTableStats newTableStats = new TableStats(newRowCount);\n+\t\tFlinkStatistic newStatistic = FlinkStatistic.builder()\n+\t\t\t.statistic(statistic)\n+\t\t\t.tableStats(newTableStats)\n+\t\t\t.build();\n+\n+\t\t// update extraDigests\n+\t\tString[] newExtraDigests = new String[0];\n+\t\tif (limit >= 0) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTQzOTQ3OQ=="}, "originalCommit": {"oid": "0324c22a9b2c85f04481966b2a78f72e92b9c88c"}, "originalPosition": 111}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjEwNzUwNw==", "bodyText": "why not we add limitation limit >= 0 in matches method ?\nbtw, LIMIT x,y can be expressed as LIMIT y OFFSET x in sql standard.", "url": "https://github.com/apache/flink/pull/12964#discussion_r462107507", "createdAt": "2020-07-29T07:52:27Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushLimitIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,122 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsLimitPushDown;\n+import org.apache.flink.table.plan.stats.TableStats;\n+import org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalSort;\n+import org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalTableSourceScan;\n+import org.apache.flink.table.planner.plan.schema.FlinkPreparingTableBase;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Sort;\n+import org.apache.calcite.rex.RexLiteral;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+\n+/**\n+ * Planner rule that tries to push limit into a {@link FlinkLogicalTableSourceScan},\n+ * which table is a {@link TableSourceTable}. And the table source in the table is a {@link SupportsLimitPushDown}.\n+ * The original limit will still be retained.\n+ * The reasons why the limit still be retained:\n+ * 1.If the source is required to return the exact number of limit number, the implementation\n+ * of the source is highly required. The source is required to accurately control the record\n+ * number of split, and the parallelism setting also need to be adjusted accordingly.\n+ * 2.When remove the limit, maybe filter will be pushed down to the source after limit pushed\n+ * down. The source need know it should do limit first and do the filter later, it is hard to\n+ * implement.\n+ * 3.We can support limit with offset, we can push down offset + fetch to table source.\n+ */\n+public class PushLimitIntoTableSourceScanRule extends RelOptRule {\n+\tpublic static final PushLimitIntoTableSourceScanRule INSTANCE = new PushLimitIntoTableSourceScanRule();\n+\n+\tpublic PushLimitIntoTableSourceScanRule() {\n+\t\tsuper(operand(FlinkLogicalSort.class,\n+\t\t\toperand(FlinkLogicalTableSourceScan.class, none())),\n+\t\t\t\"PushLimitIntoTableSourceScanRule\");\n+\t}\n+\n+\t@Override\n+\tpublic boolean matches(RelOptRuleCall call) {\n+\t\tSort sort = call.rel(0);\n+\t\tTableSourceTable tableSourceTable = call.rel(1).getTable().unwrap(TableSourceTable.class);\n+\n+\t\t// a limit can be pushed down only if it satisfies the two conditions: 1) do not have order by keys, 2) have limit.\n+\t\tboolean onlyLimit = sort.getCollation().getFieldCollations().isEmpty() && sort.fetch != null;\n+\t\treturn onlyLimit\n+\t\t\t&& tableSourceTable != null\n+\t\t\t&& tableSourceTable.tableSource() instanceof SupportsLimitPushDown\n+\t\t\t&& Arrays.stream(tableSourceTable.extraDigests()).noneMatch(str -> str.startsWith(\"limit=[\"));\n+\t}\n+\n+\t@Override\n+\tpublic void onMatch(RelOptRuleCall call) {\n+\t\tSort sort = call.rel(0);\n+\t\tFlinkLogicalTableSourceScan scan = call.rel(1);\n+\t\tTableSourceTable tableSourceTable = scan.getTable().unwrap(TableSourceTable.class);\n+\t\tint offset = sort.offset == null ? 0 : RexLiteral.intValue(sort.offset);\n+\t\tint limit = offset + RexLiteral.intValue(sort.fetch);\n+\n+\t\tTableSourceTable newTableSourceTable = applyLimit(limit, tableSourceTable);\n+\n+\t\tFlinkLogicalTableSourceScan newScan = FlinkLogicalTableSourceScan.create(scan.getCluster(), newTableSourceTable);\n+\t\tSort newSort = sort.copy(sort.getTraitSet(), Collections.singletonList(newScan));\n+\t\tcall.transformTo(newSort);\n+\t}\n+\n+\tprivate TableSourceTable applyLimit(\n+\t\tlong limit,\n+\t\tFlinkPreparingTableBase relOptTable) {\n+\t\tTableSourceTable oldTableSourceTable = relOptTable.unwrap(TableSourceTable.class);\n+\t\tDynamicTableSource newTableSource = oldTableSourceTable.tableSource().copy();\n+\t\t((SupportsLimitPushDown) newTableSource).applyLimit(limit);\n+\n+\t\tFlinkStatistic statistic = relOptTable.getStatistic();\n+\t\tlong newRowCount = 0;\n+\t\tif (statistic.getRowCount() != null) {\n+\t\t\tnewRowCount = Math.min(limit, statistic.getRowCount().longValue());\n+\t\t} else {\n+\t\t\tnewRowCount = limit;\n+\t\t}\n+\t\t// update TableStats after limit push down\n+\t\tTableStats newTableStats = new TableStats(newRowCount);\n+\t\tFlinkStatistic newStatistic = FlinkStatistic.builder()\n+\t\t\t.statistic(statistic)\n+\t\t\t.tableStats(newTableStats)\n+\t\t\t.build();\n+\n+\t\t// update extraDigests\n+\t\tString[] newExtraDigests = new String[0];\n+\t\tif (limit >= 0) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTQzOTQ3OQ=="}, "originalCommit": {"oid": "0324c22a9b2c85f04481966b2a78f72e92b9c88c"}, "originalPosition": 111}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mjc4MzgyNg==", "bodyText": "Yeap, calcite fetch member is null when it doesn't have limit currently. And calcite doesn't support mysql  limit x,y syntax.  If it supports, limit 5, -1. The -1 represents the end. That is to say [offset5, end).\nSo the calcite may transform -1 to fetch null when it supports in parser. And we don't need to limit >=0.\nSo i just do as you say, remove the limit >= 0 and  don't add limitation in matches.", "url": "https://github.com/apache/flink/pull/12964#discussion_r462783826", "createdAt": "2020-07-30T07:00:36Z", "author": {"login": "liuyongvs"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushLimitIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,122 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsLimitPushDown;\n+import org.apache.flink.table.plan.stats.TableStats;\n+import org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalSort;\n+import org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalTableSourceScan;\n+import org.apache.flink.table.planner.plan.schema.FlinkPreparingTableBase;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Sort;\n+import org.apache.calcite.rex.RexLiteral;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+\n+/**\n+ * Planner rule that tries to push limit into a {@link FlinkLogicalTableSourceScan},\n+ * which table is a {@link TableSourceTable}. And the table source in the table is a {@link SupportsLimitPushDown}.\n+ * The original limit will still be retained.\n+ * The reasons why the limit still be retained:\n+ * 1.If the source is required to return the exact number of limit number, the implementation\n+ * of the source is highly required. The source is required to accurately control the record\n+ * number of split, and the parallelism setting also need to be adjusted accordingly.\n+ * 2.When remove the limit, maybe filter will be pushed down to the source after limit pushed\n+ * down. The source need know it should do limit first and do the filter later, it is hard to\n+ * implement.\n+ * 3.We can support limit with offset, we can push down offset + fetch to table source.\n+ */\n+public class PushLimitIntoTableSourceScanRule extends RelOptRule {\n+\tpublic static final PushLimitIntoTableSourceScanRule INSTANCE = new PushLimitIntoTableSourceScanRule();\n+\n+\tpublic PushLimitIntoTableSourceScanRule() {\n+\t\tsuper(operand(FlinkLogicalSort.class,\n+\t\t\toperand(FlinkLogicalTableSourceScan.class, none())),\n+\t\t\t\"PushLimitIntoTableSourceScanRule\");\n+\t}\n+\n+\t@Override\n+\tpublic boolean matches(RelOptRuleCall call) {\n+\t\tSort sort = call.rel(0);\n+\t\tTableSourceTable tableSourceTable = call.rel(1).getTable().unwrap(TableSourceTable.class);\n+\n+\t\t// a limit can be pushed down only if it satisfies the two conditions: 1) do not have order by keys, 2) have limit.\n+\t\tboolean onlyLimit = sort.getCollation().getFieldCollations().isEmpty() && sort.fetch != null;\n+\t\treturn onlyLimit\n+\t\t\t&& tableSourceTable != null\n+\t\t\t&& tableSourceTable.tableSource() instanceof SupportsLimitPushDown\n+\t\t\t&& Arrays.stream(tableSourceTable.extraDigests()).noneMatch(str -> str.startsWith(\"limit=[\"));\n+\t}\n+\n+\t@Override\n+\tpublic void onMatch(RelOptRuleCall call) {\n+\t\tSort sort = call.rel(0);\n+\t\tFlinkLogicalTableSourceScan scan = call.rel(1);\n+\t\tTableSourceTable tableSourceTable = scan.getTable().unwrap(TableSourceTable.class);\n+\t\tint offset = sort.offset == null ? 0 : RexLiteral.intValue(sort.offset);\n+\t\tint limit = offset + RexLiteral.intValue(sort.fetch);\n+\n+\t\tTableSourceTable newTableSourceTable = applyLimit(limit, tableSourceTable);\n+\n+\t\tFlinkLogicalTableSourceScan newScan = FlinkLogicalTableSourceScan.create(scan.getCluster(), newTableSourceTable);\n+\t\tSort newSort = sort.copy(sort.getTraitSet(), Collections.singletonList(newScan));\n+\t\tcall.transformTo(newSort);\n+\t}\n+\n+\tprivate TableSourceTable applyLimit(\n+\t\tlong limit,\n+\t\tFlinkPreparingTableBase relOptTable) {\n+\t\tTableSourceTable oldTableSourceTable = relOptTable.unwrap(TableSourceTable.class);\n+\t\tDynamicTableSource newTableSource = oldTableSourceTable.tableSource().copy();\n+\t\t((SupportsLimitPushDown) newTableSource).applyLimit(limit);\n+\n+\t\tFlinkStatistic statistic = relOptTable.getStatistic();\n+\t\tlong newRowCount = 0;\n+\t\tif (statistic.getRowCount() != null) {\n+\t\t\tnewRowCount = Math.min(limit, statistic.getRowCount().longValue());\n+\t\t} else {\n+\t\t\tnewRowCount = limit;\n+\t\t}\n+\t\t// update TableStats after limit push down\n+\t\tTableStats newTableStats = new TableStats(newRowCount);\n+\t\tFlinkStatistic newStatistic = FlinkStatistic.builder()\n+\t\t\t.statistic(statistic)\n+\t\t\t.tableStats(newTableStats)\n+\t\t\t.build();\n+\n+\t\t// update extraDigests\n+\t\tString[] newExtraDigests = new String[0];\n+\t\tif (limit >= 0) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTQzOTQ3OQ=="}, "originalCommit": {"oid": "0324c22a9b2c85f04481966b2a78f72e92b9c88c"}, "originalPosition": 111}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4MDcyNDMwOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/rules/logical/PushLimitIntoLegacyTableSourceScanRuleTest.xml", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQwOToxOTozOFrOG4EImw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQwNzo1ODowN1rOG4tAVw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTQ0MTE3OQ==", "bodyText": "why the digest pattern is not limit=5 ?", "url": "https://github.com/apache/flink/pull/12964#discussion_r461441179", "createdAt": "2020-07-28T09:19:38Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/rules/logical/PushLimitIntoLegacyTableSourceScanRuleTest.xml", "diffHunk": "@@ -0,0 +1,171 @@\n+<?xml version=\"1.0\" ?>\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one or more\n+contributor license agreements.  See the NOTICE file distributed with\n+this work for additional information regarding copyright ownership.\n+The ASF licenses this file to you under the Apache License, Version 2.0\n+(the \"License\"); you may not use this file except in compliance with\n+the License.  You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+<Root>\n+\t<TestCase name=\"testCanPushdownLimitWithoutOffset\">\n+\t\t<Resource name=\"sql\">\n+\t\t\t<![CDATA[SELECT * FROM LimitTable LIMIT 5]]>\n+\t\t</Resource>\n+\t\t<Resource name=\"planBefore\">\n+\t\t\t<![CDATA[\n+LogicalSort(fetch=[5])\n++- LogicalProject(a=[$0], b=[$1], c=[$2])\n+   +- LogicalTableScan(table=[[default_catalog, default_database, LimitTable]])\n+]]>\n+\t\t</Resource>\n+\t\t<Resource name=\"planAfter\">\n+\t\t\t<![CDATA[\n+LogicalProject(a=[$0], b=[$1], c=[$2])\n++- FlinkLogicalSort(fetch=[5])\n+   +- FlinkLogicalLegacyTableSourceScan(table=[[default_catalog, default_database, LimitTable, source: [limit: 5]]], fields=[a, b, c])", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0324c22a9b2c85f04481966b2a78f72e92b9c88c"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTQ3OTAxOA==", "bodyText": "This is PushLimitIntoLegacyTableSourceScanRuleTest, which is legacy Rule.And read the code of TestLegacyLimitableTableSource.explainSource. You can compare the PushProjectIntoLegacyTableSourceScanRuleTest and PushProjectIntoTableSourceScanRuleTest. And there are the same.\nnew: LogicalTableScan(table=[[default_catalog, default_database, MyTable, project=[a, c]]])\nLegacy: LogicalTableScan(table=[[default_catalog, default_database, MyTable, source: [TestSource(physical fields: a, c)]]])", "url": "https://github.com/apache/flink/pull/12964#discussion_r461479018", "createdAt": "2020-07-28T10:26:20Z", "author": {"login": "liuyongvs"}, "path": "flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/rules/logical/PushLimitIntoLegacyTableSourceScanRuleTest.xml", "diffHunk": "@@ -0,0 +1,171 @@\n+<?xml version=\"1.0\" ?>\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one or more\n+contributor license agreements.  See the NOTICE file distributed with\n+this work for additional information regarding copyright ownership.\n+The ASF licenses this file to you under the Apache License, Version 2.0\n+(the \"License\"); you may not use this file except in compliance with\n+the License.  You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+<Root>\n+\t<TestCase name=\"testCanPushdownLimitWithoutOffset\">\n+\t\t<Resource name=\"sql\">\n+\t\t\t<![CDATA[SELECT * FROM LimitTable LIMIT 5]]>\n+\t\t</Resource>\n+\t\t<Resource name=\"planBefore\">\n+\t\t\t<![CDATA[\n+LogicalSort(fetch=[5])\n++- LogicalProject(a=[$0], b=[$1], c=[$2])\n+   +- LogicalTableScan(table=[[default_catalog, default_database, LimitTable]])\n+]]>\n+\t\t</Resource>\n+\t\t<Resource name=\"planAfter\">\n+\t\t\t<![CDATA[\n+LogicalProject(a=[$0], b=[$1], c=[$2])\n++- FlinkLogicalSort(fetch=[5])\n+   +- FlinkLogicalLegacyTableSourceScan(table=[[default_catalog, default_database, LimitTable, source: [limit: 5]]], fields=[a, b, c])", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTQ0MTE3OQ=="}, "originalCommit": {"oid": "0324c22a9b2c85f04481966b2a78f72e92b9c88c"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjExMDgwNw==", "bodyText": "sorry, this comment should be at the line 129 in PushLimitIntoTableSourceScanRuleTest.xml.", "url": "https://github.com/apache/flink/pull/12964#discussion_r462110807", "createdAt": "2020-07-29T07:58:07Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/rules/logical/PushLimitIntoLegacyTableSourceScanRuleTest.xml", "diffHunk": "@@ -0,0 +1,171 @@\n+<?xml version=\"1.0\" ?>\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one or more\n+contributor license agreements.  See the NOTICE file distributed with\n+this work for additional information regarding copyright ownership.\n+The ASF licenses this file to you under the Apache License, Version 2.0\n+(the \"License\"); you may not use this file except in compliance with\n+the License.  You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+<Root>\n+\t<TestCase name=\"testCanPushdownLimitWithoutOffset\">\n+\t\t<Resource name=\"sql\">\n+\t\t\t<![CDATA[SELECT * FROM LimitTable LIMIT 5]]>\n+\t\t</Resource>\n+\t\t<Resource name=\"planBefore\">\n+\t\t\t<![CDATA[\n+LogicalSort(fetch=[5])\n++- LogicalProject(a=[$0], b=[$1], c=[$2])\n+   +- LogicalTableScan(table=[[default_catalog, default_database, LimitTable]])\n+]]>\n+\t\t</Resource>\n+\t\t<Resource name=\"planAfter\">\n+\t\t\t<![CDATA[\n+LogicalProject(a=[$0], b=[$1], c=[$2])\n++- FlinkLogicalSort(fetch=[5])\n+   +- FlinkLogicalLegacyTableSourceScan(table=[[default_catalog, default_database, LimitTable, source: [limit: 5]]], fields=[a, b, c])", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTQ0MTE3OQ=="}, "originalCommit": {"oid": "0324c22a9b2c85f04481966b2a78f72e92b9c88c"}, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4MDc0MDU4OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/rules/logical/PushLimitIntoLegacyTableSourceScanRuleTest.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQwOToyMzo1NFrOG4ESbw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQxMDoyODozM1rOG4Ggww==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTQ0MzY5NQ==", "bodyText": "nit: unify the case of Pushdown \uff1f", "url": "https://github.com/apache/flink/pull/12964#discussion_r461443695", "createdAt": "2020-07-28T09:23:54Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/rules/logical/PushLimitIntoLegacyTableSourceScanRuleTest.scala", "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.flink.table.planner.plan.rules.logical\n+\n+import org.apache.calcite.plan.hep.HepMatchOrder\n+import org.apache.calcite.rel.rules.SortProjectTransposeRule\n+import org.apache.calcite.tools.RuleSets\n+import org.apache.flink.table.api.SqlParserException\n+import org.apache.flink.table.planner.plan.nodes.logical.{FlinkLogicalLegacyTableSourceScan, FlinkLogicalSort}\n+import org.apache.flink.table.planner.plan.optimize.program.{FlinkBatchProgram, FlinkHepRuleSetProgramBuilder, HEP_RULES_EXECUTION_TYPE}\n+import org.apache.flink.table.planner.utils.{TableConfigUtils, TableTestBase}\n+import org.junit.{Before, Test}\n+\n+/**\n+ * Test for [[PushLimitIntoLegacyTableSourceScanRule]].\n+ */\n+class PushLimitIntoLegacyTableSourceScanRuleTest extends TableTestBase {\n+  protected val util = batchTestUtil()\n+\n+  @Before\n+  def setup(): Unit = {\n+    util.buildBatchProgram(FlinkBatchProgram.DEFAULT_REWRITE)\n+    val calciteConfig = TableConfigUtils.getCalciteConfig(util.tableEnv.getConfig)\n+    calciteConfig.getBatchProgram.get.addLast(\n+      \"rules\",\n+      FlinkHepRuleSetProgramBuilder.newBuilder\n+        .setHepRulesExecutionType(HEP_RULES_EXECUTION_TYPE.RULE_COLLECTION)\n+        .setHepMatchOrder(HepMatchOrder.BOTTOM_UP)\n+        .add(RuleSets.ofList(PushLimitIntoLegacyTableSourceScanRule.INSTANCE,\n+          SortProjectTransposeRule.INSTANCE,\n+          // converts calcite rel(RelNode) to flink rel(FlinkRelNode)\n+          FlinkLogicalSort.BATCH_CONVERTER,\n+          FlinkLogicalLegacyTableSourceScan.CONVERTER))\n+        .build()\n+    )\n+\n+    val ddl =\n+      s\"\"\"\n+         |CREATE TABLE LimitTable (\n+         |  a int,\n+         |  b bigint,\n+         |  c string\n+         |) WITH (\n+         |  'connector.type' = 'TestLimitableTableSource',\n+         |  'is-bounded' = 'true'\n+         |)\n+       \"\"\".stripMargin\n+    util.tableEnv.executeSql(ddl)\n+  }\n+\n+  @Test(expected = classOf[SqlParserException])\n+  def testLimitWithNegativeOffset(): Unit = {\n+    util.verifyPlan(\"SELECT a, c FROM LimitTable LIMIT 10 OFFSET -1\")\n+  }\n+\n+  @Test(expected = classOf[SqlParserException])\n+  def testNegativeLimitWithoutOffset(): Unit = {\n+    util.verifyPlan(\"SELECT * FROM LimitTable LIMIT -1\")\n+  }\n+\n+  @Test(expected = classOf[SqlParserException])\n+  def testMysqlLimit(): Unit = {\n+    util.verifyPlan(\"SELECT a, c FROM LimitTable LIMIT 1, 10\")\n+  }\n+\n+  @Test\n+  def testCanPushdownLimitWithoutOffset(): Unit = {\n+    util.verifyPlan(\"SELECT * FROM LimitTable LIMIT 5\")\n+  }\n+\n+  @Test\n+  def testCanPushdownLimitWithOffset(): Unit = {\n+    util.verifyPlan(\"SELECT a, c FROM LimitTable LIMIT 10 OFFSET 1\")\n+  }\n+\n+  @Test\n+  def testCanPushdownFetchWithOffset(): Unit = {\n+    util.verifyPlan(\"SELECT a, c FROM LimitTable OFFSET 10 ROWS FETCH NEXT 10 ROWS ONLY\")\n+  }\n+\n+  @Test\n+  def testCanPushdownFetchWithoutOffset(): Unit = {\n+    util.verifyPlan(\"SELECT a, c FROM LimitTable FETCH FIRST 10 ROWS ONLY\")\n+  }\n+\n+  @Test\n+  def testCannotPushDownWithoutLimit(): Unit = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0324c22a9b2c85f04481966b2a78f72e92b9c88c"}, "originalPosition": 102}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTQ4MDEzMQ==", "bodyText": "ok", "url": "https://github.com/apache/flink/pull/12964#discussion_r461480131", "createdAt": "2020-07-28T10:28:33Z", "author": {"login": "liuyongvs"}, "path": "flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/rules/logical/PushLimitIntoLegacyTableSourceScanRuleTest.scala", "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.flink.table.planner.plan.rules.logical\n+\n+import org.apache.calcite.plan.hep.HepMatchOrder\n+import org.apache.calcite.rel.rules.SortProjectTransposeRule\n+import org.apache.calcite.tools.RuleSets\n+import org.apache.flink.table.api.SqlParserException\n+import org.apache.flink.table.planner.plan.nodes.logical.{FlinkLogicalLegacyTableSourceScan, FlinkLogicalSort}\n+import org.apache.flink.table.planner.plan.optimize.program.{FlinkBatchProgram, FlinkHepRuleSetProgramBuilder, HEP_RULES_EXECUTION_TYPE}\n+import org.apache.flink.table.planner.utils.{TableConfigUtils, TableTestBase}\n+import org.junit.{Before, Test}\n+\n+/**\n+ * Test for [[PushLimitIntoLegacyTableSourceScanRule]].\n+ */\n+class PushLimitIntoLegacyTableSourceScanRuleTest extends TableTestBase {\n+  protected val util = batchTestUtil()\n+\n+  @Before\n+  def setup(): Unit = {\n+    util.buildBatchProgram(FlinkBatchProgram.DEFAULT_REWRITE)\n+    val calciteConfig = TableConfigUtils.getCalciteConfig(util.tableEnv.getConfig)\n+    calciteConfig.getBatchProgram.get.addLast(\n+      \"rules\",\n+      FlinkHepRuleSetProgramBuilder.newBuilder\n+        .setHepRulesExecutionType(HEP_RULES_EXECUTION_TYPE.RULE_COLLECTION)\n+        .setHepMatchOrder(HepMatchOrder.BOTTOM_UP)\n+        .add(RuleSets.ofList(PushLimitIntoLegacyTableSourceScanRule.INSTANCE,\n+          SortProjectTransposeRule.INSTANCE,\n+          // converts calcite rel(RelNode) to flink rel(FlinkRelNode)\n+          FlinkLogicalSort.BATCH_CONVERTER,\n+          FlinkLogicalLegacyTableSourceScan.CONVERTER))\n+        .build()\n+    )\n+\n+    val ddl =\n+      s\"\"\"\n+         |CREATE TABLE LimitTable (\n+         |  a int,\n+         |  b bigint,\n+         |  c string\n+         |) WITH (\n+         |  'connector.type' = 'TestLimitableTableSource',\n+         |  'is-bounded' = 'true'\n+         |)\n+       \"\"\".stripMargin\n+    util.tableEnv.executeSql(ddl)\n+  }\n+\n+  @Test(expected = classOf[SqlParserException])\n+  def testLimitWithNegativeOffset(): Unit = {\n+    util.verifyPlan(\"SELECT a, c FROM LimitTable LIMIT 10 OFFSET -1\")\n+  }\n+\n+  @Test(expected = classOf[SqlParserException])\n+  def testNegativeLimitWithoutOffset(): Unit = {\n+    util.verifyPlan(\"SELECT * FROM LimitTable LIMIT -1\")\n+  }\n+\n+  @Test(expected = classOf[SqlParserException])\n+  def testMysqlLimit(): Unit = {\n+    util.verifyPlan(\"SELECT a, c FROM LimitTable LIMIT 1, 10\")\n+  }\n+\n+  @Test\n+  def testCanPushdownLimitWithoutOffset(): Unit = {\n+    util.verifyPlan(\"SELECT * FROM LimitTable LIMIT 5\")\n+  }\n+\n+  @Test\n+  def testCanPushdownLimitWithOffset(): Unit = {\n+    util.verifyPlan(\"SELECT a, c FROM LimitTable LIMIT 10 OFFSET 1\")\n+  }\n+\n+  @Test\n+  def testCanPushdownFetchWithOffset(): Unit = {\n+    util.verifyPlan(\"SELECT a, c FROM LimitTable OFFSET 10 ROWS FETCH NEXT 10 ROWS ONLY\")\n+  }\n+\n+  @Test\n+  def testCanPushdownFetchWithoutOffset(): Unit = {\n+    util.verifyPlan(\"SELECT a, c FROM LimitTable FETCH FIRST 10 ROWS ONLY\")\n+  }\n+\n+  @Test\n+  def testCannotPushDownWithoutLimit(): Unit = {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTQ0MzY5NQ=="}, "originalCommit": {"oid": "0324c22a9b2c85f04481966b2a78f72e92b9c88c"}, "originalPosition": 102}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NTA0OTE3OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/rules/logical/PushLimitIntoTableSourceScanRuleTest.xml", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQwNzo1OToxMVrOG4tClw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQwNjozODowOVrOG5VDgg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjExMTM4Mw==", "bodyText": "why the digest pattern is not limit=5 ?", "url": "https://github.com/apache/flink/pull/12964#discussion_r462111383", "createdAt": "2020-07-29T07:59:11Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/rules/logical/PushLimitIntoTableSourceScanRuleTest.xml", "diffHunk": "@@ -0,0 +1,171 @@\n+<?xml version=\"1.0\" ?>\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one or more\n+contributor license agreements.  See the NOTICE file distributed with\n+this work for additional information regarding copyright ownership.\n+The ASF licenses this file to you under the Apache License, Version 2.0\n+(the \"License\"); you may not use this file except in compliance with\n+the License.  You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+<Root>\n+\t<TestCase name=\"testCanPushdownLimitWithoutOffset\">\n+\t\t<Resource name=\"sql\">\n+\t\t\t<![CDATA[SELECT a, c FROM LimitTable LIMIT 5]]>\n+\t\t</Resource>\n+\t\t<Resource name=\"planBefore\">\n+\t\t\t<![CDATA[\n+LogicalSort(fetch=[5])\n++- LogicalProject(a=[$0], c=[$2])\n+   +- LogicalTableScan(table=[[default_catalog, default_database, LimitTable]])\n+]]>\n+\t\t</Resource>\n+\t\t<Resource name=\"planAfter\">\n+\t\t\t<![CDATA[\n+LogicalProject(a=[$0], c=[$2])\n++- FlinkLogicalSort(fetch=[5])\n+   +- FlinkLogicalTableSourceScan(table=[[default_catalog, default_database, LimitTable, limit=[5]]], fields=[a, b, c])\n+]]>\n+\t\t</Resource>\n+\t</TestCase>\n+\t<TestCase name=\"testCanPushdownFetchWithOffset\">\n+\t\t<Resource name=\"sql\">\n+\t\t\t<![CDATA[SELECT a, c FROM LimitTable OFFSET 10 ROWS FETCH NEXT 10 ROWS ONLY]]>\n+\t\t</Resource>\n+\t\t<Resource name=\"planBefore\">\n+\t\t\t<![CDATA[\n+LogicalSort(offset=[10], fetch=[10])\n++- LogicalProject(a=[$0], c=[$2])\n+   +- LogicalTableScan(table=[[default_catalog, default_database, LimitTable]])\n+]]>\n+\t\t</Resource>\n+\t\t<Resource name=\"planAfter\">\n+\t\t\t<![CDATA[\n+LogicalProject(a=[$0], c=[$2])\n++- FlinkLogicalSort(offset=[10], fetch=[10])\n+   +- FlinkLogicalTableSourceScan(table=[[default_catalog, default_database, LimitTable, limit=[20]]], fields=[a, b, c])\n+]]>\n+\t\t</Resource>\n+\t</TestCase>\n+\t<TestCase name=\"testCanPushdownFetchWithoutOffset\">\n+\t\t<Resource name=\"sql\">\n+\t\t\t<![CDATA[SELECT a, c FROM LimitTable FETCH FIRST 10 ROWS ONLY]]>\n+\t\t</Resource>\n+\t\t<Resource name=\"planBefore\">\n+\t\t\t<![CDATA[\n+LogicalSort(fetch=[10])\n++- LogicalProject(a=[$0], c=[$2])\n+   +- LogicalTableScan(table=[[default_catalog, default_database, LimitTable]])\n+]]>\n+\t\t</Resource>\n+\t\t<Resource name=\"planAfter\">\n+\t\t\t<![CDATA[\n+LogicalProject(a=[$0], c=[$2])\n++- FlinkLogicalSort(fetch=[10])\n+   +- FlinkLogicalTableSourceScan(table=[[default_catalog, default_database, LimitTable, limit=[10]]], fields=[a, b, c])\n+]]>\n+\t\t</Resource>\n+\t</TestCase>\n+\t<TestCase name=\"testCannotPushDownWithOrderBy\">\n+\t\t<Resource name=\"sql\">\n+\t\t\t<![CDATA[SELECT a, c FROM LimitTable ORDER BY c LIMIT 10]]>\n+\t\t</Resource>\n+\t\t<Resource name=\"planBefore\">\n+\t\t\t<![CDATA[\n+LogicalSort(sort0=[$1], dir0=[ASC-nulls-first], fetch=[10])\n++- LogicalProject(a=[$0], c=[$2])\n+   +- LogicalTableScan(table=[[default_catalog, default_database, LimitTable]])\n+]]>\n+\t\t</Resource>\n+\t\t<Resource name=\"planAfter\">\n+\t\t\t<![CDATA[\n+LogicalProject(a=[$0], c=[$2])\n++- FlinkLogicalSort(sort0=[$2], dir0=[ASC-nulls-first], fetch=[10])\n+   +- FlinkLogicalTableSourceScan(table=[[default_catalog, default_database, LimitTable]], fields=[a, b, c])\n+]]>\n+\t\t</Resource>\n+\t</TestCase>\n+\t<TestCase name=\"testCanPushdownLimitWithOffset\">\n+\t\t<Resource name=\"sql\">\n+\t\t\t<![CDATA[SELECT a, c FROM LimitTable LIMIT 10 OFFSET 1]]>\n+\t\t</Resource>\n+\t\t<Resource name=\"planBefore\">\n+\t\t\t<![CDATA[\n+LogicalSort(offset=[1], fetch=[10])\n++- LogicalProject(a=[$0], c=[$2])\n+   +- LogicalTableScan(table=[[default_catalog, default_database, LimitTable]])\n+]]>\n+\t\t</Resource>\n+\t\t<Resource name=\"planAfter\">\n+\t\t\t<![CDATA[\n+LogicalProject(a=[$0], c=[$2])\n++- FlinkLogicalSort(offset=[1], fetch=[10])\n+   +- FlinkLogicalTableSourceScan(table=[[default_catalog, default_database, LimitTable, limit=[11]]], fields=[a, b, c])\n+]]>\n+\t\t</Resource>\n+\t</TestCase>\n+\t<TestCase name=\"testLimitWithoutOffset\">\n+\t\t<Resource name=\"sql\">\n+\t\t\t<![CDATA[SELECT * FROM LimitTable LIMIT 5]]>\n+\t\t</Resource>\n+\t\t<Resource name=\"planBefore\">\n+\t\t\t<![CDATA[\n+LogicalSort(fetch=[5])\n++- LogicalProject(a=[$0], b=[$1], c=[$2])\n+   +- LogicalTableScan(table=[[default_catalog, default_database, LimitTable]])\n+]]>\n+\t\t</Resource>\n+\t\t<Resource name=\"planAfter\">\n+\t\t\t<![CDATA[\n+LogicalProject(a=[$0], b=[$1], c=[$2])\n++- FlinkLogicalSort(fetch=[5])\n+   +- FlinkLogicalTableSourceScan(table=[[default_catalog, default_database, LimitTable, source: [limit: 5]]], fields=[a, b, c])", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "141bd8e09efe79e55398adbdd0916592231fdfbf"}, "originalPosition": 129}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mjc2Njk3OA==", "bodyText": "sorry, it should be removed. i have replace the testLimitWithoutOffset by  testCanPushdownLimitWithoutOffset. And forgot to remove it.", "url": "https://github.com/apache/flink/pull/12964#discussion_r462766978", "createdAt": "2020-07-30T06:38:09Z", "author": {"login": "liuyongvs"}, "path": "flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/rules/logical/PushLimitIntoTableSourceScanRuleTest.xml", "diffHunk": "@@ -0,0 +1,171 @@\n+<?xml version=\"1.0\" ?>\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one or more\n+contributor license agreements.  See the NOTICE file distributed with\n+this work for additional information regarding copyright ownership.\n+The ASF licenses this file to you under the Apache License, Version 2.0\n+(the \"License\"); you may not use this file except in compliance with\n+the License.  You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+<Root>\n+\t<TestCase name=\"testCanPushdownLimitWithoutOffset\">\n+\t\t<Resource name=\"sql\">\n+\t\t\t<![CDATA[SELECT a, c FROM LimitTable LIMIT 5]]>\n+\t\t</Resource>\n+\t\t<Resource name=\"planBefore\">\n+\t\t\t<![CDATA[\n+LogicalSort(fetch=[5])\n++- LogicalProject(a=[$0], c=[$2])\n+   +- LogicalTableScan(table=[[default_catalog, default_database, LimitTable]])\n+]]>\n+\t\t</Resource>\n+\t\t<Resource name=\"planAfter\">\n+\t\t\t<![CDATA[\n+LogicalProject(a=[$0], c=[$2])\n++- FlinkLogicalSort(fetch=[5])\n+   +- FlinkLogicalTableSourceScan(table=[[default_catalog, default_database, LimitTable, limit=[5]]], fields=[a, b, c])\n+]]>\n+\t\t</Resource>\n+\t</TestCase>\n+\t<TestCase name=\"testCanPushdownFetchWithOffset\">\n+\t\t<Resource name=\"sql\">\n+\t\t\t<![CDATA[SELECT a, c FROM LimitTable OFFSET 10 ROWS FETCH NEXT 10 ROWS ONLY]]>\n+\t\t</Resource>\n+\t\t<Resource name=\"planBefore\">\n+\t\t\t<![CDATA[\n+LogicalSort(offset=[10], fetch=[10])\n++- LogicalProject(a=[$0], c=[$2])\n+   +- LogicalTableScan(table=[[default_catalog, default_database, LimitTable]])\n+]]>\n+\t\t</Resource>\n+\t\t<Resource name=\"planAfter\">\n+\t\t\t<![CDATA[\n+LogicalProject(a=[$0], c=[$2])\n++- FlinkLogicalSort(offset=[10], fetch=[10])\n+   +- FlinkLogicalTableSourceScan(table=[[default_catalog, default_database, LimitTable, limit=[20]]], fields=[a, b, c])\n+]]>\n+\t\t</Resource>\n+\t</TestCase>\n+\t<TestCase name=\"testCanPushdownFetchWithoutOffset\">\n+\t\t<Resource name=\"sql\">\n+\t\t\t<![CDATA[SELECT a, c FROM LimitTable FETCH FIRST 10 ROWS ONLY]]>\n+\t\t</Resource>\n+\t\t<Resource name=\"planBefore\">\n+\t\t\t<![CDATA[\n+LogicalSort(fetch=[10])\n++- LogicalProject(a=[$0], c=[$2])\n+   +- LogicalTableScan(table=[[default_catalog, default_database, LimitTable]])\n+]]>\n+\t\t</Resource>\n+\t\t<Resource name=\"planAfter\">\n+\t\t\t<![CDATA[\n+LogicalProject(a=[$0], c=[$2])\n++- FlinkLogicalSort(fetch=[10])\n+   +- FlinkLogicalTableSourceScan(table=[[default_catalog, default_database, LimitTable, limit=[10]]], fields=[a, b, c])\n+]]>\n+\t\t</Resource>\n+\t</TestCase>\n+\t<TestCase name=\"testCannotPushDownWithOrderBy\">\n+\t\t<Resource name=\"sql\">\n+\t\t\t<![CDATA[SELECT a, c FROM LimitTable ORDER BY c LIMIT 10]]>\n+\t\t</Resource>\n+\t\t<Resource name=\"planBefore\">\n+\t\t\t<![CDATA[\n+LogicalSort(sort0=[$1], dir0=[ASC-nulls-first], fetch=[10])\n++- LogicalProject(a=[$0], c=[$2])\n+   +- LogicalTableScan(table=[[default_catalog, default_database, LimitTable]])\n+]]>\n+\t\t</Resource>\n+\t\t<Resource name=\"planAfter\">\n+\t\t\t<![CDATA[\n+LogicalProject(a=[$0], c=[$2])\n++- FlinkLogicalSort(sort0=[$2], dir0=[ASC-nulls-first], fetch=[10])\n+   +- FlinkLogicalTableSourceScan(table=[[default_catalog, default_database, LimitTable]], fields=[a, b, c])\n+]]>\n+\t\t</Resource>\n+\t</TestCase>\n+\t<TestCase name=\"testCanPushdownLimitWithOffset\">\n+\t\t<Resource name=\"sql\">\n+\t\t\t<![CDATA[SELECT a, c FROM LimitTable LIMIT 10 OFFSET 1]]>\n+\t\t</Resource>\n+\t\t<Resource name=\"planBefore\">\n+\t\t\t<![CDATA[\n+LogicalSort(offset=[1], fetch=[10])\n++- LogicalProject(a=[$0], c=[$2])\n+   +- LogicalTableScan(table=[[default_catalog, default_database, LimitTable]])\n+]]>\n+\t\t</Resource>\n+\t\t<Resource name=\"planAfter\">\n+\t\t\t<![CDATA[\n+LogicalProject(a=[$0], c=[$2])\n++- FlinkLogicalSort(offset=[1], fetch=[10])\n+   +- FlinkLogicalTableSourceScan(table=[[default_catalog, default_database, LimitTable, limit=[11]]], fields=[a, b, c])\n+]]>\n+\t\t</Resource>\n+\t</TestCase>\n+\t<TestCase name=\"testLimitWithoutOffset\">\n+\t\t<Resource name=\"sql\">\n+\t\t\t<![CDATA[SELECT * FROM LimitTable LIMIT 5]]>\n+\t\t</Resource>\n+\t\t<Resource name=\"planBefore\">\n+\t\t\t<![CDATA[\n+LogicalSort(fetch=[5])\n++- LogicalProject(a=[$0], b=[$1], c=[$2])\n+   +- LogicalTableScan(table=[[default_catalog, default_database, LimitTable]])\n+]]>\n+\t\t</Resource>\n+\t\t<Resource name=\"planAfter\">\n+\t\t\t<![CDATA[\n+LogicalProject(a=[$0], b=[$1], c=[$2])\n++- FlinkLogicalSort(fetch=[5])\n+   +- FlinkLogicalTableSourceScan(table=[[default_catalog, default_database, LimitTable, source: [limit: 5]]], fields=[a, b, c])", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjExMTM4Mw=="}, "originalCommit": {"oid": "141bd8e09efe79e55398adbdd0916592231fdfbf"}, "originalPosition": 129}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4OTU0NjQzOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushLimitIntoTableSourceScanRule.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQwNzo0OTozN1rOG5X5dA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQwNzo0OTozN1rOG5X5dA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjgxMzU1Ng==", "bodyText": "nit:  -> .tableStats(new TableStats(newRowCount))", "url": "https://github.com/apache/flink/pull/12964#discussion_r462813556", "createdAt": "2020-07-30T07:49:37Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushLimitIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsLimitPushDown;\n+import org.apache.flink.table.plan.stats.TableStats;\n+import org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalSort;\n+import org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalTableSourceScan;\n+import org.apache.flink.table.planner.plan.schema.FlinkPreparingTableBase;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Sort;\n+import org.apache.calcite.rex.RexLiteral;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+\n+/**\n+ * Planner rule that tries to push limit into a {@link FlinkLogicalTableSourceScan},\n+ * which table is a {@link TableSourceTable}. And the table source in the table is a {@link SupportsLimitPushDown}.\n+ * The original limit will still be retained.\n+ * The reasons why the limit still be retained:\n+ * 1.If the source is required to return the exact number of limit number, the implementation\n+ * of the source is highly required. The source is required to accurately control the record\n+ * number of split, and the parallelism setting also need to be adjusted accordingly.\n+ * 2.When remove the limit, maybe filter will be pushed down to the source after limit pushed\n+ * down. The source need know it should do limit first and do the filter later, it is hard to\n+ * implement.\n+ * 3.We can support limit with offset, we can push down offset + fetch to table source.\n+ */\n+public class PushLimitIntoTableSourceScanRule extends RelOptRule {\n+\tpublic static final PushLimitIntoTableSourceScanRule INSTANCE = new PushLimitIntoTableSourceScanRule();\n+\n+\tpublic PushLimitIntoTableSourceScanRule() {\n+\t\tsuper(operand(FlinkLogicalSort.class,\n+\t\t\toperand(FlinkLogicalTableSourceScan.class, none())),\n+\t\t\t\"PushLimitIntoTableSourceScanRule\");\n+\t}\n+\n+\t@Override\n+\tpublic boolean matches(RelOptRuleCall call) {\n+\t\tSort sort = call.rel(0);\n+\t\tTableSourceTable tableSourceTable = call.rel(1).getTable().unwrap(TableSourceTable.class);\n+\n+\t\t// a limit can be pushed down only if it satisfies the two conditions: 1) do not have order by keys, 2) have limit.\n+\t\tboolean onlyLimit = sort.getCollation().getFieldCollations().isEmpty() && sort.fetch != null;\n+\t\treturn onlyLimit\n+\t\t\t&& tableSourceTable != null\n+\t\t\t&& tableSourceTable.tableSource() instanceof SupportsLimitPushDown\n+\t\t\t&& Arrays.stream(tableSourceTable.extraDigests()).noneMatch(str -> str.startsWith(\"limit=[\"));\n+\t}\n+\n+\t@Override\n+\tpublic void onMatch(RelOptRuleCall call) {\n+\t\tSort sort = call.rel(0);\n+\t\tFlinkLogicalTableSourceScan scan = call.rel(1);\n+\t\tTableSourceTable tableSourceTable = scan.getTable().unwrap(TableSourceTable.class);\n+\t\tint offset = sort.offset == null ? 0 : RexLiteral.intValue(sort.offset);\n+\t\tint limit = offset + RexLiteral.intValue(sort.fetch);\n+\n+\t\tTableSourceTable newTableSourceTable = applyLimit(limit, tableSourceTable);\n+\n+\t\tFlinkLogicalTableSourceScan newScan = FlinkLogicalTableSourceScan.create(scan.getCluster(), newTableSourceTable);\n+\t\tSort newSort = sort.copy(sort.getTraitSet(), Collections.singletonList(newScan));\n+\t\tcall.transformTo(newSort);\n+\t}\n+\n+\tprivate TableSourceTable applyLimit(\n+\t\tlong limit,\n+\t\tFlinkPreparingTableBase relOptTable) {\n+\t\tTableSourceTable oldTableSourceTable = relOptTable.unwrap(TableSourceTable.class);\n+\t\tDynamicTableSource newTableSource = oldTableSourceTable.tableSource().copy();\n+\t\t((SupportsLimitPushDown) newTableSource).applyLimit(limit);\n+\n+\t\tFlinkStatistic statistic = relOptTable.getStatistic();\n+\t\tlong newRowCount = 0;\n+\t\tif (statistic.getRowCount() != null) {\n+\t\t\tnewRowCount = Math.min(limit, statistic.getRowCount().longValue());\n+\t\t} else {\n+\t\t\tnewRowCount = limit;\n+\t\t}\n+\t\t// update TableStats after limit push down\n+\t\tTableStats newTableStats = new TableStats(newRowCount);\n+\t\tFlinkStatistic newStatistic = FlinkStatistic.builder()\n+\t\t\t.statistic(statistic)\n+\t\t\t.tableStats(newTableStats)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "34dd37d992e9d3c3a0bf92e660575571d83b407c"}, "originalPosition": 106}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4OTU1MDk5OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushLimitIntoTableSourceScanRule.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQwNzo1MDo0OVrOG5X8JA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQwNzo1MDo0OVrOG5X8JA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjgxNDI0NA==", "bodyText": "nit: newExtraDigests is only used once, just move new String[] {\"limit=[\" + limit + \"]\"} into here.", "url": "https://github.com/apache/flink/pull/12964#discussion_r462814244", "createdAt": "2020-07-30T07:50:49Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushLimitIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsLimitPushDown;\n+import org.apache.flink.table.plan.stats.TableStats;\n+import org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalSort;\n+import org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalTableSourceScan;\n+import org.apache.flink.table.planner.plan.schema.FlinkPreparingTableBase;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Sort;\n+import org.apache.calcite.rex.RexLiteral;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+\n+/**\n+ * Planner rule that tries to push limit into a {@link FlinkLogicalTableSourceScan},\n+ * which table is a {@link TableSourceTable}. And the table source in the table is a {@link SupportsLimitPushDown}.\n+ * The original limit will still be retained.\n+ * The reasons why the limit still be retained:\n+ * 1.If the source is required to return the exact number of limit number, the implementation\n+ * of the source is highly required. The source is required to accurately control the record\n+ * number of split, and the parallelism setting also need to be adjusted accordingly.\n+ * 2.When remove the limit, maybe filter will be pushed down to the source after limit pushed\n+ * down. The source need know it should do limit first and do the filter later, it is hard to\n+ * implement.\n+ * 3.We can support limit with offset, we can push down offset + fetch to table source.\n+ */\n+public class PushLimitIntoTableSourceScanRule extends RelOptRule {\n+\tpublic static final PushLimitIntoTableSourceScanRule INSTANCE = new PushLimitIntoTableSourceScanRule();\n+\n+\tpublic PushLimitIntoTableSourceScanRule() {\n+\t\tsuper(operand(FlinkLogicalSort.class,\n+\t\t\toperand(FlinkLogicalTableSourceScan.class, none())),\n+\t\t\t\"PushLimitIntoTableSourceScanRule\");\n+\t}\n+\n+\t@Override\n+\tpublic boolean matches(RelOptRuleCall call) {\n+\t\tSort sort = call.rel(0);\n+\t\tTableSourceTable tableSourceTable = call.rel(1).getTable().unwrap(TableSourceTable.class);\n+\n+\t\t// a limit can be pushed down only if it satisfies the two conditions: 1) do not have order by keys, 2) have limit.\n+\t\tboolean onlyLimit = sort.getCollation().getFieldCollations().isEmpty() && sort.fetch != null;\n+\t\treturn onlyLimit\n+\t\t\t&& tableSourceTable != null\n+\t\t\t&& tableSourceTable.tableSource() instanceof SupportsLimitPushDown\n+\t\t\t&& Arrays.stream(tableSourceTable.extraDigests()).noneMatch(str -> str.startsWith(\"limit=[\"));\n+\t}\n+\n+\t@Override\n+\tpublic void onMatch(RelOptRuleCall call) {\n+\t\tSort sort = call.rel(0);\n+\t\tFlinkLogicalTableSourceScan scan = call.rel(1);\n+\t\tTableSourceTable tableSourceTable = scan.getTable().unwrap(TableSourceTable.class);\n+\t\tint offset = sort.offset == null ? 0 : RexLiteral.intValue(sort.offset);\n+\t\tint limit = offset + RexLiteral.intValue(sort.fetch);\n+\n+\t\tTableSourceTable newTableSourceTable = applyLimit(limit, tableSourceTable);\n+\n+\t\tFlinkLogicalTableSourceScan newScan = FlinkLogicalTableSourceScan.create(scan.getCluster(), newTableSourceTable);\n+\t\tSort newSort = sort.copy(sort.getTraitSet(), Collections.singletonList(newScan));\n+\t\tcall.transformTo(newSort);\n+\t}\n+\n+\tprivate TableSourceTable applyLimit(\n+\t\tlong limit,\n+\t\tFlinkPreparingTableBase relOptTable) {\n+\t\tTableSourceTable oldTableSourceTable = relOptTable.unwrap(TableSourceTable.class);\n+\t\tDynamicTableSource newTableSource = oldTableSourceTable.tableSource().copy();\n+\t\t((SupportsLimitPushDown) newTableSource).applyLimit(limit);\n+\n+\t\tFlinkStatistic statistic = relOptTable.getStatistic();\n+\t\tlong newRowCount = 0;\n+\t\tif (statistic.getRowCount() != null) {\n+\t\t\tnewRowCount = Math.min(limit, statistic.getRowCount().longValue());\n+\t\t} else {\n+\t\t\tnewRowCount = limit;\n+\t\t}\n+\t\t// update TableStats after limit push down\n+\t\tTableStats newTableStats = new TableStats(newRowCount);\n+\t\tFlinkStatistic newStatistic = FlinkStatistic.builder()\n+\t\t\t.statistic(statistic)\n+\t\t\t.tableStats(newTableStats)\n+\t\t\t.build();\n+\n+\t\t// update extraDigests\n+\t\tString[] newExtraDigests = new String[]{\"limit=[\" + limit + \"]\"};\n+\n+\t\treturn oldTableSourceTable.copy(\n+\t\t\tnewTableSource,\n+\t\t\tnewStatistic,\n+\t\t\tnewExtraDigests", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "34dd37d992e9d3c3a0bf92e660575571d83b407c"}, "originalPosition": 115}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4980, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}