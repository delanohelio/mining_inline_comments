{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDY4NjY2OTA2", "number": 13169, "title": "[FLINK-18944][python] Support JDBC connector for Python DataStream API.", "bodyText": "What is the purpose of the change\nSupport JDBC connector for Python DataStream API.\nBrief change log\nAdded a new SinkFunction named JdbcSink.\nVerifying this change\nThis pull request has test case covered by test_jdbc_sink() in FlinkJdbcSinkTest in test_connectors.py.\nDoes this pull request potentially affect one of the following parts:\n\nDependencies (does it add or upgrade a dependency): ( no)\nThe public API, i.e., is any changed class annotated with @Public(Evolving): ( no)\nThe serializers: (no)\nThe runtime per-record code paths (performance sensitive): (no)\nAnything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (no)\nThe S3 file system connector: (no)\n\nDocumentation\n\nDoes this pull request introduce a new feature? ( no)\nIf yes, how is the feature documented? ( not documented)", "createdAt": "2020-08-17T08:07:37Z", "url": "https://github.com/apache/flink/pull/13169", "merged": true, "mergeCommit": {"oid": "f674c7c9c5bf733b77e661e9ecc74c60d7fc906e"}, "closed": true, "closedAt": "2020-08-18T05:12:00Z", "author": {"login": "shuiqiangchen"}, "timelineItems": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABc_vU-1AFqTQ2ODMwNDkzNQ==", "endCursor": "Y3Vyc29yOnYyOpPPAAABc_-iicgFqTQ2ODk2ODE5OQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY4MzA0OTM1", "url": "https://github.com/apache/flink/pull/13169#pullrequestreview-468304935", "createdAt": "2020-08-17T09:24:53Z", "commit": {"oid": "ad222e4a369a1d49d47e6d12497c2f93a21ed749"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QwOToyNDo1NFrOHBhHEA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QwOToyNDo1NFrOHBhHEA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTM1MzEwNA==", "bodyText": "We can make type_info optional too since we can directly get this type from the input datastream. e.g.,\n    def sink(sql: str, jdbc_connection_options: 'JdbcConnectionOptions', type_info: RowTypeInfo = None,\n             jdbc_execution_options: 'JdbcExecutionOptions' = None):", "url": "https://github.com/apache/flink/pull/13169#discussion_r471353104", "createdAt": "2020-08-17T09:24:54Z", "author": {"login": "hequn8128"}, "path": "flink-python/pyflink/datastream/connectors.py", "diffHunk": "@@ -0,0 +1,692 @@\n+################################################################################\n+#  Licensed to the Apache Software Foundation (ASF) under one\n+#  or more contributor license agreements.  See the NOTICE file\n+#  distributed with this work for additional information\n+#  regarding copyright ownership.  The ASF licenses this file\n+#  to you under the Apache License, Version 2.0 (the\n+#  \"License\"); you may not use this file except in compliance\n+#  with the License.  You may obtain a copy of the License at\n+#\n+#      http://www.apache.org/licenses/LICENSE-2.0\n+#\n+#  Unless required by applicable law or agreed to in writing, software\n+#  distributed under the License is distributed on an \"AS IS\" BASIS,\n+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+#  See the License for the specific language governing permissions and\n+# limitations under the License.\n+################################################################################\n+from typing import Dict, List, Union\n+\n+from pyflink.common.serialization_schemas import DeserializationSchema, SerializationSchema\n+from pyflink.common.typeinfo import RowTypeInfo\n+from pyflink.datastream.functions import SourceFunction, SinkFunction\n+from pyflink.java_gateway import get_gateway\n+from pyflink.util.utils import to_jarray\n+\n+\n+class FlinkKafkaConsumerBase(SourceFunction):\n+    \"\"\"\n+    Base class of all Flink Kafka Consumer data sources. This implements the common behavior across\n+    all kafka versions.\n+\n+    The Kafka version specific behavior is defined mainly in the specific subclasses.\n+    \"\"\"\n+\n+    def __init__(self, j_flink_kafka_consumer):\n+        super(FlinkKafkaConsumerBase, self).__init__(source_func=j_flink_kafka_consumer)\n+\n+    def set_commit_offsets_on_checkpoints(self, commit_on_checkpoints: bool):\n+        \"\"\"\n+        Specifies whether or not the consumer should commit offsets back to kafka on checkpoints.\n+        This setting will only have effect if checkpointing is enabled for the job. If checkpointing\n+        isn't enabled, only the \"auto.commit.enable\" (for 0.8) / \"enable.auto.commit\" (for 0.9+)\n+        property settings will be used.\n+        \"\"\"\n+        self._j_function = self._j_function \\\n+            .setCommitOffsetsOnCheckpoints(commit_on_checkpoints)\n+        return self\n+\n+    def set_start_from_earliest(self):\n+        \"\"\"\n+        Specifies the consumer to start reading from the earliest offset for all partitions. This\n+        lets the consumer ignore any committed group offsets in Zookeeper/ Kafka brokers.\n+\n+        This method does not affect where partitions are read from when the consumer is restored\n+        from a checkpoint or savepoint. When the consumer is restored from a checkpoint or\n+        savepoint, only the offsets in the restored state will be used.\n+        \"\"\"\n+        self._j_function = self._j_function.setStartFromEarliest()\n+        return self\n+\n+    def set_start_from_latest(self):\n+        \"\"\"\n+        Specifies the consuer to start reading from the latest offset for all partitions. This lets\n+        the consumer ignore any committed group offsets in Zookeeper / Kafka brokers.\n+\n+        This method does not affect where partitions are read from when the consumer is restored\n+        from a checkpoint or savepoint. When the consumer is restored from a checkpoint or\n+        savepoint, only the offsets in the restored state will be used.\n+        \"\"\"\n+        self._j_function = self._j_function.setStartFromLatest()\n+        return self\n+\n+    def set_start_from_timestamp(self, startup_offsets_timestamp: int):\n+        \"\"\"\n+        Specifies the consumer to start reading partitions from a specified timestamp. The specified\n+        timestamp must be before the current timestamp. This lets the consumer ignore any committed\n+        group offsets in Zookeeper / Kafka brokers.\n+\n+        The consumer will look up the earliest offset whose timestamp is greater than or equal to\n+        the specific timestamp from Kafka. If there's no such offset, the consumer will use the\n+        latest offset to read data from Kafka.\n+\n+        This method does not affect where partitions are read from when the consumer is restored\n+        from a checkpoint or savepoint. When the consumer is restored from a checkpoint or\n+        savepoint, only the offsets in the restored state will be used.\n+\n+        :param startup_offsets_timestamp: timestamp for the startup offsets, as milliseconds for\n+                                          epoch.\n+        \"\"\"\n+        self._j_function = self._j_function.setStartFromTimestamp(\n+            startup_offsets_timestamp)\n+        return self\n+\n+    def set_start_from_group_offsets(self):\n+        \"\"\"\n+        Specifies the consumer to start reading from any committed group offsets found in Zookeeper/\n+        Kafka brokers. The 'group.id' property must be set in the configuration properties. If no\n+        offset can be found for a partition, the behaviour in 'auto.offset.reset' set in the\n+        configuration properties will be used for the partition.\n+\n+        This method does not affect where partitions are read from when the consumer is restored\n+        from a checkpoint or savepoint. When the consumer is restored from a checkpoint or\n+        savepoint, only the offsets in the restored state will be used.\n+        \"\"\"\n+        self._j_function = self._j_function.setStartFromGroupOffsets()\n+        return self\n+\n+    def disable_filter_restored_partitions_with_subscribed_topics(self):\n+        \"\"\"\n+        By default, when restoring from a checkpoint / savepoint, the consumer always ignores\n+        restored partitions that are no longer associated with the current specified topics or topic\n+        pattern to subscribe to.\n+\n+        This method does not affect where partitions are read from when the consumer is restored\n+        from a checkpoint or savepoint. When the consumer is restored from a checkpoint or\n+        savepoint, only the offsets in the restored state will be used.\n+        \"\"\"\n+        self._j_function = self._j_function \\\n+            .disableFilterRestoredPartitionsWithSubscribedTopics()\n+        return self\n+\n+    def get_produced_type(self):\n+        return self._j_function.getProducedType()\n+\n+\n+class FlinkKafkaConsumer010(FlinkKafkaConsumerBase):\n+    \"\"\"\n+    The Flink Kafka Consumer is a streaming data source that pulls a parallel data stream from\n+    Apache Kafka 0.10.x. The consumer can run in multiple parallel instances, each of which will\n+    pull data from one or more Kafka partitions.\n+\n+    The Flink Kafka Consumer participates in checkpointing and guarantees that no data is lost\n+    during a failure, and taht the computation processes elements 'exactly once. (These guarantees\n+    naturally assume that Kafka itself does not loose any data.)\n+\n+    Please note that Flink snapshots the offsets internally as part of its distributed checkpoints.\n+    The offsets committed to Kafka / Zookeeper are only to bring the outside view of progress in\n+    sync with Flink's view of the progress. That way, monitoring and other jobs can get a view of\n+    how far the Flink Kafka consumer has consumed a topic.\n+\n+    Please refer to Kafka's documentation for the available configuration properties:\n+    http://kafka.apache.org/documentation.html#newconsumerconfigs\n+    \"\"\"\n+\n+    def __init__(self, topics: Union[str, List[str]], deserialization_schema: DeserializationSchema,\n+                 properties: Dict):\n+        \"\"\"\n+        Creates a new Kafka streaming source consumer for Kafka 0.10.x.\n+\n+        This constructor allows passing multiple topics to the consumer.\n+\n+        :param topics: The Kafka topics to read from.\n+        :param deserialization_schema: The de-/serializer used to convert between Kafka's byte\n+                                       messages and Flink's objects.\n+        :param properties: The properties that are used to configure both the fetcher and the offset\n+                           handler.\n+        \"\"\"\n+        if not isinstance(topics, list):\n+            topics = [topics]\n+        gateway = get_gateway()\n+        j_properties = gateway.jvm.java.util.Properties()\n+        for key, value in properties.items():\n+            j_properties.setProperty(key, value)\n+\n+        JFlinkKafkaConsumer010 = gateway.jvm \\\n+            .org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer010\n+        j_flink_kafka_consumer_010 = \\\n+            JFlinkKafkaConsumer010(topics,\n+                                   deserialization_schema._j_deserialization_schema,\n+                                   j_properties)\n+        super(FlinkKafkaConsumer010, self).__init__(\n+            j_flink_kafka_consumer=j_flink_kafka_consumer_010)\n+\n+\n+class FlinkKafkaConsumer011(FlinkKafkaConsumerBase):\n+    \"\"\"\n+    The Flink Kafka Consumer is a streaming data source that pulls a parallel data stream from\n+    Apache Kafka 0.10.x. The consumer can run in multiple parallel instances, each of which will\n+    pull data from one or more Kafka partitions.\n+\n+    The Flink Kafka Consumer participates in checkpointing and guarantees that no data is lost\n+    during a failure, and taht the computation processes elements 'exactly once. (These guarantees\n+    naturally assume that Kafka itself does not loose any data.)\n+\n+    Please note that Flink snapshots the offsets internally as part of its distributed checkpoints.\n+    The offsets committed to Kafka / Zookeeper are only to bring the outside view of progress in\n+    sync with Flink's view of the progress. That way, monitoring and other jobs can get a view of\n+    how far the Flink Kafka consumer has consumed a topic.\n+\n+    Please refer to Kafka's documentation for the available configuration properties:\n+    http://kafka.apache.org/documentation.html#newconsumerconfigs\n+    \"\"\"\n+\n+    def __init__(self, topics: Union[str, List[str]], deserialization_schema: DeserializationSchema,\n+                 properties: Dict):\n+        \"\"\"\n+        Creates a new Kafka streaming source consumer for Kafka 0.10.x.\n+\n+        This constructor allows passing multiple topics to the consumer.\n+\n+        :param topics: The Kafka topics to read from.\n+        :param deserialization_schema: The de-/serializer used to convert between Kafka's byte\n+                                       messages and Flink's objects.\n+        :param properties: The properties that are used to configure both the fetcher and the offset\n+                           handler.\n+        \"\"\"\n+        if not isinstance(topics, list):\n+            topics = [topics]\n+        gateway = get_gateway()\n+        j_properties = gateway.jvm.java.util.Properties()\n+        for key, value in properties.items():\n+            j_properties.setProperty(key, value)\n+\n+        JFlinkKafkaConsumer011 = gateway.jvm \\\n+            .org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer011\n+        j_flink_kafka_consumer_011 = \\\n+            JFlinkKafkaConsumer011(topics,\n+                                   deserialization_schema._j_deserialization_schema,\n+                                   j_properties)\n+        super(FlinkKafkaConsumer011, self).__init__(j_flink_kafka_consumer_011)\n+\n+\n+class FlinkKafkaConsumer(FlinkKafkaConsumerBase):\n+    \"\"\"\n+    The Flink Kafka Consumer is a streaming data source that pulls a parallel data stream from\n+    Apache Kafka 0.10.x. The consumer can run in multiple parallel instances, each of which will\n+    pull data from one or more Kafka partitions.\n+\n+    The Flink Kafka Consumer participates in checkpointing and guarantees that no data is lost\n+    during a failure, and taht the computation processes elements 'exactly once. (These guarantees\n+    naturally assume that Kafka itself does not loose any data.)\n+\n+    Please note that Flink snapshots the offsets internally as part of its distributed checkpoints.\n+    The offsets committed to Kafka / Zookeeper are only to bring the outside view of progress in\n+    sync with Flink's view of the progress. That way, monitoring and other jobs can get a view of\n+    how far the Flink Kafka consumer has consumed a topic.\n+\n+    Please refer to Kafka's documentation for the available configuration properties:\n+    http://kafka.apache.org/documentation.html#newconsumerconfigs\n+    \"\"\"\n+\n+    def __init__(self, topics: Union[str, List[str]], deserialization_schema: DeserializationSchema,\n+                 properties: Dict):\n+        \"\"\"\n+        Creates a new Kafka streaming source consumer for Kafka 0.10.x.\n+\n+        This constructor allows passing multiple topics to the consumer.\n+\n+        :param topics: The Kafka topics to read from.\n+        :param deserialization_schema: The de-/serializer used to convert between Kafka's byte\n+                                       messages and Flink's objects.\n+        :param properties: The properties that are used to configure both the fetcher and the offset\n+                           handler.\n+        \"\"\"\n+        if not isinstance(topics, list):\n+            topics = [topics]\n+        gateway = get_gateway()\n+        j_properties = gateway.jvm.java.util.Properties()\n+        for key, value in properties.items():\n+            j_properties.setProperty(key, value)\n+\n+        JFlinkKafkaConsumer = gateway.jvm \\\n+            .org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer\n+        j_flink_kafka_consumer = \\\n+            JFlinkKafkaConsumer(topics,\n+                                deserialization_schema._j_deserialization_schema,\n+                                j_properties)\n+        super(FlinkKafkaConsumer, self).__init__(\n+            j_flink_kafka_consumer=j_flink_kafka_consumer)\n+\n+\n+class FlinkKafkaProducerBase(SinkFunction):\n+    \"\"\"\n+    Flink Sink to produce data into a Kafka topic.\n+\n+    Please note that this producer provides at-least-once reliability guarantees when checkpoints\n+    are enabled and set_flush_on_checkpoint(True) is set. Otherwise, the producer doesn;t provid any\n+    reliability guarantees.\n+    \"\"\"\n+\n+    def __init__(self, j_flink_kafka_producer):\n+        super(FlinkKafkaProducerBase, self).__init__(sink_func=j_flink_kafka_producer)\n+\n+    def set_log_failures_only(self, log_failures_only: bool):\n+        \"\"\"\n+        Defines whether the producer should fail on errors, or only log them. If this is set to\n+        true, then exceptions will be only logged, if set to false, exceptions will be eventually\n+        thrown and cause the streaming program to fail (and enter recovery).\n+\n+        :param log_failures_only: The flag to indicate logging-only on exceptions.\n+        \"\"\"\n+        self._j_function.setLogFailuresOnly(log_failures_only)\n+\n+    def set_flush_on_checkpoint(self, flush_on_checkpoint: bool):\n+        \"\"\"\n+        If set to true, the Flink producer will wait for all outstanding messages in the Kafka\n+        buffers to be acknowledged by the Kafka producer on a checkpoint.\n+\n+        This way, the producer can guarantee that messages in the Kafka buffers are part of the\n+        checkpoint.\n+\n+        :param flush_on_checkpoint: Flag indicating the flush mode (true = flush on checkpoint)\n+        \"\"\"\n+        self._j_function.setFlushOnCheckpoint(flush_on_checkpoint)\n+\n+\n+class FlinkKafkaProducer010(FlinkKafkaProducerBase):\n+    \"\"\"\n+    Flink Sink to produce data into a Kafka topic. This producer is compatible with Kafka 0.10.x.\n+    \"\"\"\n+\n+    def __init__(self, topic: str, serialization_schema: SerializationSchema,\n+                 producer_config: Dict):\n+        \"\"\"\n+        Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to the topic.\n+\n+        Using this constructor, the default FlinkFixedPartitioner will be used as the partitioner.\n+        This default partitioner maps each sink subtask to a single Kafka partition (i.e. all\n+        records received by a sink subtask will end up in the same Kafka partition).\n+\n+        :param topic: ID of the Kafka topic.\n+        :param serialization_schema: User defined key-less serialization schema.\n+        :param producer_config: Properties with the producer configuration.\n+        \"\"\"\n+        gateway = get_gateway()\n+        j_properties = gateway.jvm.java.util.Properties()\n+        for key, value in producer_config.items():\n+            j_properties.setProperty(key, value)\n+\n+        JFlinkKafkaProducer010 = gateway.jvm \\\n+            .org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer010\n+        j_flink_kafka_producer = JFlinkKafkaProducer010(\n+            topic, serialization_schema._j_serialization_schema, j_properties)\n+        super(FlinkKafkaProducer010, self).__init__(j_flink_kafka_producer=j_flink_kafka_producer)\n+\n+    def set_write_timestamp_to_kafka(self, write_timestamp_to_kafka: bool):\n+        \"\"\"\n+        If set to true, Flink will write the (event time) timestamp attached to each record into\n+        Kafka. Timestamps must be positive for Kafka to accept them.\n+\n+        :param write_timestamp_to_kafka: Flag indicating if Flink's internal timestamps are written\n+                                         to Kafka.\n+        \"\"\"\n+        self._j_function.setWriteTimestampToKafka(write_timestamp_to_kafka)\n+\n+\n+class Semantic(object):\n+    \"\"\"\n+    Semantics that can be chosen.\n+    :data: `EXACTLY_ONCE`:\n+    The Flink producer will write all messages in a Kafka transaction that will be committed to\n+    the Kafka on a checkpoint. In this mode FlinkKafkaProducer011 sets up a pool of\n+    FlinkKafkaProducer. Between each checkpoint there is created new Kafka transaction, which is\n+    being committed on FlinkKafkaProducer011.notifyCheckpointComplete(long). If checkpoint\n+    complete notifications are running late, FlinkKafkaProducer011 can run out of\n+    FlinkKafkaProducers in the pool. In that case any subsequent FlinkKafkaProducer011.snapshot-\n+    State() requests will fail and the FlinkKafkaProducer011 will keep using the\n+    FlinkKafkaProducer from previous checkpoint. To decrease chances of failing checkpoints\n+    there are four options:\n+        1. decrease number of max concurrent checkpoints\n+        2. make checkpoints mre reliable (so that they complete faster)\n+        3. increase delay between checkpoints\n+        4. increase size of FlinkKafkaProducers pool\n+\n+    :data: `AT_LEAST_ONCE`:\n+    The Flink producer will wait for all outstanding messages in the Kafka buffers to be\n+    acknowledged by the Kafka producer on a checkpoint.\n+\n+    :data: `NONE`:\n+    Means that nothing will be guaranteed. Messages can be lost and/or duplicated in case of\n+    failure.\n+    \"\"\"\n+\n+    EXACTLY_ONCE = 0,\n+    AT_LEAST_ONCE = 1,\n+    NONE = 2\n+\n+    @staticmethod\n+    def _to_j_semantic(semantic):\n+        gateway = get_gateway()\n+        JSemantic = gateway.jvm \\\n+            .org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer011.Semantic\n+        if semantic == Semantic.EXACTLY_ONCE:\n+            return JSemantic.EXACTLY_ONCE\n+        elif semantic == Semantic.AT_LEAST_ONCE:\n+            return JSemantic.AT_LEAST_ONCE\n+        elif semantic == Semantic.NONE:\n+            return JSemantic.NONE\n+        else:\n+            raise TypeError(\"Unsupported semantic: %s, supported semantics are: \"\n+                            \"Semantic.EXACTLY_ONCE, Semantic.AT_LEAST_ONCE, Semantic.NONE\"\n+                            % semantic)\n+\n+\n+class FlinkKafkaProducer011(SinkFunction):\n+    \"\"\"\n+    Flink Sink to produce data into a Kafka topic. This producer is compatible with Kafka 0.11.x. By\n+    default producer will use AT_LEAST_ONCE sematic. Before using EXACTLY_ONCE please refer to\n+    Flink's Kafka connector documentation.\n+    \"\"\"\n+\n+    def __init__(self, topic: str, serialization_schema: SerializationSchema,\n+                 producer_config: Dict):\n+        \"\"\"\n+        Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to the topic.\n+\n+        Using this constructor, the default FlinkFixedPartitioner will be used as the partitioner.\n+        This default partitioner maps each sink subtask to a single Kafka partition (i.e. all\n+        records received by a sink subtask will end up in the same Kafka partition).\n+\n+        :param topic: ID of the Kafka topic.\n+        :param serialization_schema: User defined key-less serialization schema.\n+        :param producer_config: Properties with the producer configuration.\n+        \"\"\"\n+        gateway = get_gateway()\n+        j_properties = gateway.jvm.java.util.Properties()\n+        for key, value in producer_config.items():\n+            j_properties.setProperty(key, value)\n+\n+        JFlinkKafkaProducer011 = gateway.jvm \\\n+            .org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer011\n+        j_flink_kafka_producer = JFlinkKafkaProducer011(\n+            topic, serialization_schema._j_serialization_schema, j_properties)\n+        super(FlinkKafkaProducer011, self).__init__(sink_func=j_flink_kafka_producer)\n+\n+    def set_write_timestamp_to_kafka(self, write_timestamp_to_kafka: bool):\n+        \"\"\"\n+        If set to true, Flink will write the (event time) timestamp attached to each record into\n+        Kafka. Timestamps must be positive for Kafka to accept them.\n+\n+        :param write_timestamp_to_kafka: Flag indicating if Flink's internal timestamps are written\n+                                         to Kafka.\n+        \"\"\"\n+        self._j_function.setWriteTimestampToKafka(write_timestamp_to_kafka)\n+\n+    def ignore_failures_after_transaction_timeout(self):\n+        \"\"\"\n+        Disables the propagation of exceptions thrown when committing presumable timed out Kafka\n+        transactions during recovery of the job. If a Kafka transaction is timed out, a commit will\n+        never be successful. Hence, use this feature to avoid recovery loops of the job. Exceptions\n+        will still be logged to inform the user that data loss might have occurred.\n+\n+        Note that we use current time millis to track the age of a transaction. Moreover, only\n+        exceptions thrown during the recovery are caught, i.e., the producer will attempt at least\n+        one commit of the transaction before giving up.\n+        \"\"\"\n+        self._j_function = self._j_function.ignoreFailuresAfterTransactionTimeout()\n+        return self\n+\n+    def set_log_failures_only(self, log_failures_only: bool):\n+        \"\"\"\n+        Defines whether the producer should fail on errors, or only log them. If this is set to\n+        true, then exceptions will be only logged, if set to false, exceptions will be eventually\n+        thrown and cause the streaming program to fail (and enter recovery).\n+\n+        :param log_failures_only: The flag to indicate logging-only on exceptions.\n+        \"\"\"\n+        self._j_function.setLogFailuresOnly(log_failures_only)\n+\n+    def set_flush_on_checkpoint(self, flush_on_checkpoint: bool):\n+        \"\"\"\n+        If set to true, the Flink producer will wait for all outstanding messages in the Kafka\n+        buffers to be acknowledged by the Kafka producer on a checkpoint.\n+\n+        This way, the producer can guarantee that messages in the Kafka buffers are part of the\n+        checkpoint.\n+\n+        :param flush_on_checkpoint: Flag indicating the flush mode (true = flush on checkpoint)\n+        \"\"\"\n+        self._j_function.setFlushOnCheckpoint(flush_on_checkpoint)\n+\n+\n+class FlinkKafkaProducer(SinkFunction):\n+    \"\"\"\n+    Flink Sink to produce data into a Kafka topic. This producer is compatible with Kafka 0.11.x. By\n+    default producer will use AT_LEAST_ONCE sematic. Before using EXACTLY_ONCE please refer to\n+    Flink's Kafka connector documentation.\n+    \"\"\"\n+\n+    def __init__(self, topic: str, serialization_schema: SerializationSchema,\n+                 producer_config: Dict):\n+        \"\"\"\n+        Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to the topic.\n+\n+        Using this constructor, the default FlinkFixedPartitioner will be used as the partitioner.\n+        This default partitioner maps each sink subtask to a single Kafka partition (i.e. all\n+        records received by a sink subtask will end up in the same Kafka partition).\n+\n+        :param topic: ID of the Kafka topic.\n+        :param serialization_schema: User defined key-less serialization schema.\n+        :param producer_config: Properties with the producer configuration.\n+        \"\"\"\n+        gateway = get_gateway()\n+        j_properties = gateway.jvm.java.util.Properties()\n+        for key, value in producer_config.items():\n+            j_properties.setProperty(key, value)\n+\n+        JFlinkKafkaProducer = gateway.jvm \\\n+            .org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer\n+        j_flink_kafka_producer = JFlinkKafkaProducer(\n+            topic, serialization_schema._j_serialization_schema, j_properties)\n+        super(FlinkKafkaProducer, self).__init__(sink_func=j_flink_kafka_producer)\n+\n+    def set_write_timestamp_to_kafka(self, write_timestamp_to_kafka: bool):\n+        \"\"\"\n+        If set to true, Flink will write the (event time) timestamp attached to each record into\n+        Kafka. Timestamps must be positive for Kafka to accept them.\n+\n+        :param write_timestamp_to_kafka: Flag indicating if Flink's internal timestamps are written\n+                                         to Kafka.\n+        \"\"\"\n+        self._j_function.setWriteTimestampToKafka(write_timestamp_to_kafka)\n+\n+    def ignore_failures_after_transaction_timeout(self):\n+        \"\"\"\n+        Disables the propagation of exceptions thrown when committing presumable timed out Kafka\n+        transactions during recovery of the job. If a Kafka transaction is timed out, a commit will\n+        never be successful. Hence, use this feature to avoid recovery loops of the job. Exceptions\n+        will still be logged to inform the user that data loss might have occurred.\n+\n+        Note that we use current time millis to track the age of a transaction. Moreover, only\n+        exceptions thrown during the recovery are caught, i.e., the producer will attempt at least\n+        one commit of the transaction before giving up.\n+        \"\"\"\n+        self._j_function = self._j_function.ignoreFailuresAfterTransactionTimeout()\n+        return self\n+\n+    def set_log_failures_only(self, log_failures_only: bool):\n+        \"\"\"\n+        Defines whether the producer should fail on errors, or only log them. If this is set to\n+        true, then exceptions will be only logged, if set to false, exceptions will be eventually\n+        thrown and cause the streaming program to fail (and enter recovery).\n+\n+        :param log_failures_only: The flag to indicate logging-only on exceptions.\n+        \"\"\"\n+        self._j_function.setLogFailuresOnly(log_failures_only)\n+\n+    def set_flush_on_checkpoint(self, flush_on_checkpoint: bool):\n+        \"\"\"\n+        If set to true, the Flink producer will wait for all outstanding messages in the Kafka\n+        buffers to be acknowledged by the Kafka producer on a checkpoint.\n+\n+        This way, the producer can guarantee that messages in the Kafka buffers are part of the\n+        checkpoint.\n+\n+        :param flush_on_checkpoint: Flag indicating the flush mode (true = flush on checkpoint)\n+        \"\"\"\n+        self._j_function.setFlushOnCheckpoint(flush_on_checkpoint)\n+\n+\n+class JdbcSink(SinkFunction):\n+\n+    def __init__(self, j_jdbc_sink):\n+        super(JdbcSink, self).__init__(sink_func=j_jdbc_sink)\n+\n+    @staticmethod\n+    def sink(sql: str, type_info: RowTypeInfo, jdbc_connection_options: 'JdbcConnectionOptions',", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ad222e4a369a1d49d47e6d12497c2f93a21ed749"}, "originalPosition": 557}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6f15edc1979bb83a35dcb56bb0d7c52b23e6ce1f", "author": {"user": {"login": "shuiqiangchen", "name": "Shuiqiang Chen"}}, "url": "https://github.com/apache/flink/commit/6f15edc1979bb83a35dcb56bb0d7c52b23e6ce1f", "committedDate": "2020-08-18T02:03:35Z", "message": "[FLINK-18944][python] Support JDBC connector for Python DataStream API."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "ad222e4a369a1d49d47e6d12497c2f93a21ed749", "author": {"user": {"login": "shuiqiangchen", "name": "Shuiqiang Chen"}}, "url": "https://github.com/apache/flink/commit/ad222e4a369a1d49d47e6d12497c2f93a21ed749", "committedDate": "2020-08-17T08:03:51Z", "message": "[FLINK-18944][python] Support JDBC connector for Python DataStream API."}, "afterCommit": {"oid": "6f15edc1979bb83a35dcb56bb0d7c52b23e6ce1f", "author": {"user": {"login": "shuiqiangchen", "name": "Shuiqiang Chen"}}, "url": "https://github.com/apache/flink/commit/6f15edc1979bb83a35dcb56bb0d7c52b23e6ce1f", "committedDate": "2020-08-18T02:03:35Z", "message": "[FLINK-18944][python] Support JDBC connector for Python DataStream API."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY4OTY4MTk5", "url": "https://github.com/apache/flink/pull/13169#pullrequestreview-468968199", "createdAt": "2020-08-18T03:35:09Z", "commit": {"oid": "6f15edc1979bb83a35dcb56bb0d7c52b23e6ce1f"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4917, "cost": 1, "resetAt": "2021-10-28T16:48:13Z"}}}