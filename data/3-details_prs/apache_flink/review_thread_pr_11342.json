{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzg1MTQ3NDUx", "number": 11342, "reviewThreads": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wOVQwODoxNzoyMVrODmMSSQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wOVQxNTozMTozNVrODmWNxw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxMzczNzY5OnYy", "diffSide": "RIGHT", "path": "flink-python/setup.py", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wOVQwODoxNzoyMVrOFzd7sQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wOVQwODoxNzoyMVrOFzd7sQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTUxMjExMw==", "bodyText": "Remove the blank between the version range? It seems it contains no blank usually.", "url": "https://github.com/apache/flink/pull/11342#discussion_r389512113", "createdAt": "2020-03-09T08:17:21Z", "author": {"login": "hequn8128"}, "path": "flink-python/setup.py", "diffHunk": "@@ -224,7 +224,8 @@ def remove_if_exists(file_path):\n         author_email='dev@flink.apache.org',\n         python_requires='>=3.5',\n         install_requires=['py4j==0.10.8.1', 'python-dateutil==2.8.0', 'apache-beam==2.19.0',\n-                          'cloudpickle==1.2.2', 'avro-python3>=1.8.1,<=1.9.1', 'jsonpickle==1.2'],\n+                          'cloudpickle==1.2.2', 'avro-python3>=1.8.1,<=1.9.1', 'jsonpickle==1.2',\n+                          'pandas>=0.23.4, <=0.25.3', 'pyarrow>=0.15.1, <=0.16.0'],", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "27f93cc605b980ae0ce93b1d5503031ae98f6cd9"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxMzg3MTk5OnYy", "diffSide": "RIGHT", "path": "flink-python/pyflink/table/udf.py", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wOVQwOTowNjoyN1rOFzfMvw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wOVQwOTowNjoyN1rOFzfMvw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTUzMjg2Mw==", "bodyText": "How about adding a PythonFunctionKind class which contains GENERAL and PANDAS? It is consistent with Java and also would be easier for users to add the parameter.", "url": "https://github.com/apache/flink/pull/11342#discussion_r389532863", "createdAt": "2020-03-09T09:06:27Z", "author": {"login": "hequn8128"}, "path": "flink-python/pyflink/table/udf.py", "diffHunk": "@@ -297,15 +308,17 @@ def _get_python_env():\n     return gateway.jvm.org.apache.flink.table.functions.python.PythonEnv(exec_type)\n \n \n-def _create_udf(f, input_types, result_type, deterministic, name):\n-    return UserDefinedScalarFunctionWrapper(f, input_types, result_type, deterministic, name)\n+def _create_udf(f, input_types, result_type, udf_type, deterministic, name):\n+    return UserDefinedScalarFunctionWrapper(\n+        f, input_types, result_type, udf_type, deterministic, name)\n \n \n def _create_udtf(f, input_types, result_types, deterministic, name):\n     return UserDefinedTableFunctionWrapper(f, input_types, result_types, deterministic, name)\n \n \n-def udf(f=None, input_types=None, result_type=None, deterministic=None, name=None):\n+def udf(f=None, input_types=None, result_type=None, deterministic=None, name=None,\n+        udf_type=\"general\"):", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "27f93cc605b980ae0ce93b1d5503031ae98f6cd9"}, "originalPosition": 67}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxNTIxMjM3OnYy", "diffSide": "RIGHT", "path": "flink-python/src/main/java/org/apache/flink/table/runtime/runners/python/scalar/arrow/AbstractArrowPythonScalarFunctionRunner.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wOVQxNDo1Njo0MlrOFzsIIg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wOVQxNDo1Njo0MlrOFzsIIg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTc0NDY3NA==", "bodyText": "Maybe we should not contain this commit in this PR? as it is included in FLINK-16273\uff1fotherwise there will be two commits for addressing the same problem.", "url": "https://github.com/apache/flink/pull/11342#discussion_r389744674", "createdAt": "2020-03-09T14:56:42Z", "author": {"login": "hequn8128"}, "path": "flink-python/src/main/java/org/apache/flink/table/runtime/runners/python/scalar/arrow/AbstractArrowPythonScalarFunctionRunner.java", "diffHunk": "@@ -46,6 +46,12 @@\n \n \tprivate static final String SCHEMA_ARROW_CODER_URN = \"flink:coder:schema:scalar_function:arrow:v1\";\n \n+\tstatic {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "27f93cc605b980ae0ce93b1d5503031ae98f6cd9"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxNTI5NzQ2OnYy", "diffSide": "RIGHT", "path": "flink-python/pyflink/fn_execution/coder_impl.py", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wOVQxNToxNjo0MlrOFzs-Vw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wOVQxNToxNjo0MlrOFzs-Vw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTc1ODU1MQ==", "bodyText": "Why we only get the first batch in the batch list?", "url": "https://github.com/apache/flink/pull/11342#discussion_r389758551", "createdAt": "2020-03-09T15:16:42Z", "author": {"login": "hequn8128"}, "path": "flink-python/pyflink/fn_execution/coder_impl.py", "diffHunk": "@@ -373,3 +376,45 @@ def internal_to_timestamp(self, milliseconds, nanoseconds):\n         second, microsecond = (milliseconds // 1000,\n                                milliseconds % 1000 * 1000 + nanoseconds // 1000)\n         return datetime.datetime.utcfromtimestamp(second).replace(microsecond=microsecond)\n+\n+\n+class ArrowCoderImpl(StreamCoderImpl):\n+\n+    def __init__(self, schema):\n+        self._schema = schema\n+        self._resettable_io = ResettableIO()\n+\n+    def encode_to_stream(self, cols, out_stream, nested):\n+        if not hasattr(self, \"_batch_writer\"):\n+            self._batch_writer = pa.RecordBatchStreamWriter(self._resettable_io, self._schema)\n+\n+        self._resettable_io.set_output_stream(out_stream)\n+        self._batch_writer.write_batch(self._create_batch(cols))\n+\n+    def decode_from_stream(self, in_stream, nested):\n+        if not hasattr(self, \"_batch_reader\"):\n+            def load_from_stream(stream):\n+                reader = pa.ipc.open_stream(stream)\n+                for batch in reader:\n+                    yield batch\n+\n+            self._batch_reader = load_from_stream(self._resettable_io)\n+\n+        self._resettable_io.set_input_bytes(in_stream.read_all())\n+        table = pa.Table.from_batches([next(self._batch_reader)])", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "27f93cc605b980ae0ce93b1d5503031ae98f6cd9"}, "originalPosition": 41}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxNTMyODI1OnYy", "diffSide": "RIGHT", "path": "flink-python/pyflink/fn_execution/coder_impl.py", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wOVQxNToyMzozMlrOFztRnA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wOVQxNToyMzozMlrOFztRnA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTc2MzQ4NA==", "bodyText": "Why we must init the _batch_writer here instead of in __init__?", "url": "https://github.com/apache/flink/pull/11342#discussion_r389763484", "createdAt": "2020-03-09T15:23:32Z", "author": {"login": "hequn8128"}, "path": "flink-python/pyflink/fn_execution/coder_impl.py", "diffHunk": "@@ -373,3 +376,45 @@ def internal_to_timestamp(self, milliseconds, nanoseconds):\n         second, microsecond = (milliseconds // 1000,\n                                milliseconds % 1000 * 1000 + nanoseconds // 1000)\n         return datetime.datetime.utcfromtimestamp(second).replace(microsecond=microsecond)\n+\n+\n+class ArrowCoderImpl(StreamCoderImpl):\n+\n+    def __init__(self, schema):\n+        self._schema = schema\n+        self._resettable_io = ResettableIO()\n+\n+    def encode_to_stream(self, cols, out_stream, nested):\n+        if not hasattr(self, \"_batch_writer\"):", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "27f93cc605b980ae0ce93b1d5503031ae98f6cd9"}, "originalPosition": 25}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxNTM2NDU1OnYy", "diffSide": "RIGHT", "path": "flink-python/pyflink/fn_execution/coder_impl.py", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wOVQxNTozMTozNVrOFztn_w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wOVQxNTozMTozNVrOFztn_w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTc2OTIxNQ==", "bodyText": "I'm wondering if we can avoid these hasattr. Can we use pa.ipc.open_stream read the stream directly and return the batch?", "url": "https://github.com/apache/flink/pull/11342#discussion_r389769215", "createdAt": "2020-03-09T15:31:35Z", "author": {"login": "hequn8128"}, "path": "flink-python/pyflink/fn_execution/coder_impl.py", "diffHunk": "@@ -373,3 +376,45 @@ def internal_to_timestamp(self, milliseconds, nanoseconds):\n         second, microsecond = (milliseconds // 1000,\n                                milliseconds % 1000 * 1000 + nanoseconds // 1000)\n         return datetime.datetime.utcfromtimestamp(second).replace(microsecond=microsecond)\n+\n+\n+class ArrowCoderImpl(StreamCoderImpl):\n+\n+    def __init__(self, schema):\n+        self._schema = schema\n+        self._resettable_io = ResettableIO()\n+\n+    def encode_to_stream(self, cols, out_stream, nested):\n+        if not hasattr(self, \"_batch_writer\"):\n+            self._batch_writer = pa.RecordBatchStreamWriter(self._resettable_io, self._schema)\n+\n+        self._resettable_io.set_output_stream(out_stream)\n+        self._batch_writer.write_batch(self._create_batch(cols))\n+\n+    def decode_from_stream(self, in_stream, nested):\n+        if not hasattr(self, \"_batch_reader\"):", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "27f93cc605b980ae0ce93b1d5503031ae98f6cd9"}, "originalPosition": 32}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 878, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}