{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTAzMjc4MjM4", "number": 13631, "title": "[FLINK-19639][table sql/planner]Support SupportsNestedProjectionPushD\u2026", "bodyText": "\u2026own in planner\n\nWhat is the purpose of the change\nSupport to push nested projection into TableSourceScan. But it may cause name conflicts in some situaion. For example, we create the table with ddl\nCREATE TABLE NestedTable ( nest1 ROW<a INT>, nest2 ROW<a INT> )\nand with query\nSELECT nest1.a, nest2.a from NestedTable\nand we will get 2 a in the new schema when pushing projection.\nHere we use\u00a0'_' to concatenate the names of all levels as the name of the nested fields. In this example, we will get fields nest1_a, nest2_a in the new schema.\nBrief change log\n\nadd RexNodeReWriter#rewriteNestedProjectionWithNewFieldInput that will modify the projection with the mapping\nenable rule PushProjectionIntoTableSourceScanRule to push nested projection\n\nVerifying this change\nThis change added tests and can be verified as follows:\n\nadd tests for RexNodeReWriter#rewriteNestedProjectionWithNewFieldInput\nadd all kinds of tests for rule\n\nDoes this pull request potentially affect one of the following parts:\n\nDependencies (does it add or upgrade a dependency): (yes / no)\nThe public API, i.e., is any changed class annotated with @Public(Evolving): (yes / no)\nThe serializers: (yes / no / don't know)\nThe runtime per-record code paths (performance sensitive): (yes / no / don't know)\nAnything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (yes / no / don't know)\nThe S3 file system connector: (yes / no / don't know)\n\nDocumentation\n\nDoes this pull request introduce a new feature? (yes / no)\nIf yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)", "createdAt": "2020-10-14T10:39:52Z", "url": "https://github.com/apache/flink/pull/13631", "merged": true, "mergeCommit": {"oid": "2145610616b3e2ef9a39e8f1319b0b239f85b695"}, "closed": true, "closedAt": "2020-11-04T07:36:20Z", "author": {"login": "fsk119"}, "timelineItems": {"totalCount": 38, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdSbo5pgBqjM4NzYxMjkyOTI=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdZEKxuAH2gAyNTAzMjc4MjM4OjVmOWQwYWJmYThlYmYyY2EzZjI5ZDFlZWI2OWY4OThhNmI0ZGU5MTU=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "803f22afa05eac973c3c9e7b69722b4840c8173f", "author": {"user": {"login": "fsk119", "name": "Shengkai "}}, "url": "https://github.com/apache/flink/commit/803f22afa05eac973c3c9e7b69722b4840c8173f", "committedDate": "2020-10-14T10:27:46Z", "message": "[FLINK-19693][table sql/planner]Support SupportsNestedProjectionPushDown in planner"}, "afterCommit": {"oid": "cb66dc97754abc1468b03508a41a4f3a34ce9572", "author": {"user": {"login": "fsk119", "name": "Shengkai "}}, "url": "https://github.com/apache/flink/commit/cb66dc97754abc1468b03508a41a4f3a34ce9572", "committedDate": "2020-10-14T11:39:44Z", "message": "[FLINK-19693][table sql/planner]Support SupportsNestedProjectionPushDown in planner"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA5MTM2NTgw", "url": "https://github.com/apache/flink/pull/13631#pullrequestreview-509136580", "createdAt": "2020-10-15T08:26:33Z", "commit": {"oid": "cb66dc97754abc1468b03508a41a4f3a34ce9572"}, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNVQwODoyNjozM1rOHh7Gjw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNVQxMDowMDoyOVrOHiASJg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTMzMzM5MQ==", "bodyText": "what if a field name contains . ?", "url": "https://github.com/apache/flink/pull/13631#discussion_r505333391", "createdAt": "2020-10-15T08:26:33Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/RexNodeExtractor.scala", "diffHunk": "@@ -268,7 +268,8 @@ class RefFieldAccessorVisitor(usedFields: Array[Int]) extends RexVisitorImpl[Uni\n             // access is top-level access => return top-level access\n             case _ :: _ if nestedAccess.equals(\"*\") => List(\"*\")\n             // previous access is not prefix of this access => add access\n-            case head :: _ if !nestedAccess.startsWith(head) =>\n+            // it may cause bug without \".\" as tail if we have references a.b and a.bb\n+            case head :: _ if !nestedAccess.startsWith(head + \".\") =>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cb66dc97754abc1468b03508a41a4f3a34ce9572"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTM5ODY1Mg==", "bodyText": "it's better we can move this method out of its parent method, which could improve code readability", "url": "https://github.com/apache/flink/pull/13631#discussion_r505398652", "createdAt": "2020-10-15T09:33:50Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/RexNodeRewriter.scala", "diffHunk": "@@ -60,3 +79,88 @@ class InputRewriter(fieldMap: Map[Int, Int]) extends RexShuttle {\n     fieldMap.getOrElse(ref.getIndex,\n       throw new IllegalArgumentException(\"input field contains invalid index\"))\n }\n+\n+/**\n+ * A RexShuttle to rewrite field accesses of RexNode with nested projection.\n+ * For `RexInputRef`, it works like `InputReWriter` and use the old input\n+ * ref index to find the new input fields ref.\n+ * For `RexFieldAccess`, it will traverse to the top level of the access and\n+ * find the mapping in field fieldMap first. There are 3 situations we need to consider:\n+ *  1. mapping has the top level access, we should make field access to the reference;\n+ *  2. mapping has the field, we should make an access;\n+ *  3. mapping has no information of the current name, we should keep the full name\n+ *  of the fields and index of mapping for later lookup.\n+ * When the process is back from the recursion, we still have 2 situations need to\n+ * consider:\n+ *  1. we have found the reference of the upper level, we just make an access above the\n+ *  reference we find before;\n+ *  2. we haven't found the reference of the upper level, we concatenate the prefix with\n+ *  the current field name and look up the new prefix in the mapping. If it's in the mapping,\n+ *  we create a reference. Otherwise, we should go to the next level with the new prefix.\n+ */\n+class NestedInputRewriter(\n+  fieldMap: JMap[Integer, JMap[String, Integer]],\n+  rowTypes: JList[RelDataType],\n+  builder: RexBuilder) extends RexShuttle {\n+\n+  override def visitFieldAccess(input: RexFieldAccess): RexNode = {\n+    def traverse(fieldAccess: RexFieldAccess): (Int, String, Option[RexNode]) = {\n+      fieldAccess.getReferenceExpr match {\n+        case ref: RexInputRef =>\n+          val mapping =\n+            fieldMap.getOrElse(ref.getIndex,\n+              throw new IllegalArgumentException(\"input field contains unknown index\"))\n+          if (mapping.contains(\"*\")) {\n+            (ref.getIndex,\n+              \"\",\n+              Option.apply(builder.makeFieldAccess(\n+                new RexInputRef(mapping(\"*\"), rowTypes(mapping(\"*\"))),\n+                fieldAccess.getField.getName,\n+                false))\n+            )\n+          } else if(mapping.contains(fieldAccess.getField.getName)) {\n+            (ref.getIndex,\n+              \"\",\n+              Option.apply(new RexInputRef(mapping(fieldAccess.getField.getName),\n+                rowTypes(mapping(fieldAccess.getField.getName)))))\n+          } else {\n+            (ref.getIndex, fieldAccess.getField.getName, Option.empty)\n+          }\n+        case acc: RexFieldAccess =>\n+          val (i, prefix, node) = traverse(acc)\n+          if (node.isDefined) {\n+            (i,\n+              \"\",\n+              Option.apply(builder.makeFieldAccess(node.get, fieldAccess.getField.getName, false)))\n+          } else {\n+            val newPrefix = s\"$prefix.${fieldAccess.getField.getName}\"\n+            // we have checked before\n+            val mapping = fieldMap(i)\n+            if (mapping.contains(newPrefix)) {\n+              (i,\n+                \"\",\n+                Option.apply(new RexInputRef(mapping(newPrefix), rowTypes(mapping(newPrefix)))))\n+            } else {\n+              (i, newPrefix, Option.empty)\n+            }\n+          }\n+      }\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cb66dc97754abc1468b03508a41a4f3a34ce9572"}, "originalPosition": 107}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTQwMDA4Nw==", "bodyText": "nit: Option.apply() -> Some()", "url": "https://github.com/apache/flink/pull/13631#discussion_r505400087", "createdAt": "2020-10-15T09:35:37Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/RexNodeRewriter.scala", "diffHunk": "@@ -60,3 +79,88 @@ class InputRewriter(fieldMap: Map[Int, Int]) extends RexShuttle {\n     fieldMap.getOrElse(ref.getIndex,\n       throw new IllegalArgumentException(\"input field contains invalid index\"))\n }\n+\n+/**\n+ * A RexShuttle to rewrite field accesses of RexNode with nested projection.\n+ * For `RexInputRef`, it works like `InputReWriter` and use the old input\n+ * ref index to find the new input fields ref.\n+ * For `RexFieldAccess`, it will traverse to the top level of the access and\n+ * find the mapping in field fieldMap first. There are 3 situations we need to consider:\n+ *  1. mapping has the top level access, we should make field access to the reference;\n+ *  2. mapping has the field, we should make an access;\n+ *  3. mapping has no information of the current name, we should keep the full name\n+ *  of the fields and index of mapping for later lookup.\n+ * When the process is back from the recursion, we still have 2 situations need to\n+ * consider:\n+ *  1. we have found the reference of the upper level, we just make an access above the\n+ *  reference we find before;\n+ *  2. we haven't found the reference of the upper level, we concatenate the prefix with\n+ *  the current field name and look up the new prefix in the mapping. If it's in the mapping,\n+ *  we create a reference. Otherwise, we should go to the next level with the new prefix.\n+ */\n+class NestedInputRewriter(\n+  fieldMap: JMap[Integer, JMap[String, Integer]],\n+  rowTypes: JList[RelDataType],\n+  builder: RexBuilder) extends RexShuttle {\n+\n+  override def visitFieldAccess(input: RexFieldAccess): RexNode = {\n+    def traverse(fieldAccess: RexFieldAccess): (Int, String, Option[RexNode]) = {\n+      fieldAccess.getReferenceExpr match {\n+        case ref: RexInputRef =>\n+          val mapping =\n+            fieldMap.getOrElse(ref.getIndex,\n+              throw new IllegalArgumentException(\"input field contains unknown index\"))\n+          if (mapping.contains(\"*\")) {\n+            (ref.getIndex,\n+              \"\",\n+              Option.apply(builder.makeFieldAccess(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cb66dc97754abc1468b03508a41a4f3a34ce9572"}, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTQwMDUyOQ==", "bodyText": "ditto", "url": "https://github.com/apache/flink/pull/13631#discussion_r505400529", "createdAt": "2020-10-15T09:36:13Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/RexNodeRewriter.scala", "diffHunk": "@@ -60,3 +79,88 @@ class InputRewriter(fieldMap: Map[Int, Int]) extends RexShuttle {\n     fieldMap.getOrElse(ref.getIndex,\n       throw new IllegalArgumentException(\"input field contains invalid index\"))\n }\n+\n+/**\n+ * A RexShuttle to rewrite field accesses of RexNode with nested projection.\n+ * For `RexInputRef`, it works like `InputReWriter` and use the old input\n+ * ref index to find the new input fields ref.\n+ * For `RexFieldAccess`, it will traverse to the top level of the access and\n+ * find the mapping in field fieldMap first. There are 3 situations we need to consider:\n+ *  1. mapping has the top level access, we should make field access to the reference;\n+ *  2. mapping has the field, we should make an access;\n+ *  3. mapping has no information of the current name, we should keep the full name\n+ *  of the fields and index of mapping for later lookup.\n+ * When the process is back from the recursion, we still have 2 situations need to\n+ * consider:\n+ *  1. we have found the reference of the upper level, we just make an access above the\n+ *  reference we find before;\n+ *  2. we haven't found the reference of the upper level, we concatenate the prefix with\n+ *  the current field name and look up the new prefix in the mapping. If it's in the mapping,\n+ *  we create a reference. Otherwise, we should go to the next level with the new prefix.\n+ */\n+class NestedInputRewriter(\n+  fieldMap: JMap[Integer, JMap[String, Integer]],\n+  rowTypes: JList[RelDataType],\n+  builder: RexBuilder) extends RexShuttle {\n+\n+  override def visitFieldAccess(input: RexFieldAccess): RexNode = {\n+    def traverse(fieldAccess: RexFieldAccess): (Int, String, Option[RexNode]) = {\n+      fieldAccess.getReferenceExpr match {\n+        case ref: RexInputRef =>\n+          val mapping =\n+            fieldMap.getOrElse(ref.getIndex,\n+              throw new IllegalArgumentException(\"input field contains unknown index\"))\n+          if (mapping.contains(\"*\")) {\n+            (ref.getIndex,\n+              \"\",\n+              Option.apply(builder.makeFieldAccess(\n+                new RexInputRef(mapping(\"*\"), rowTypes(mapping(\"*\"))),\n+                fieldAccess.getField.getName,\n+                false))\n+            )\n+          } else if(mapping.contains(fieldAccess.getField.getName)) {\n+            (ref.getIndex,\n+              \"\",\n+              Option.apply(new RexInputRef(mapping(fieldAccess.getField.getName),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cb66dc97754abc1468b03508a41a4f3a34ce9572"}, "originalPosition": 83}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTQxODI3OA==", "bodyText": "nit: add a test about complex expressions, such as  deepNested.nested1.name + nested.value", "url": "https://github.com/apache/flink/pull/13631#discussion_r505418278", "createdAt": "2020-10-15T10:00:29Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRuleTest.java", "diffHunk": "@@ -107,4 +96,13 @@ private void testNestedProject(boolean nestedProjectionSupported) {\n \t\tutil().verifyPlan(sqlQuery);\n \t}\n \n+\t@Test\n+\tpublic void testComplicatedNestedProject() {\n+\t\tString sqlQuery = \"SELECT id,\" +\n+\t\t\t\t\"    deepNested.nested1.name AS nestedName,\\n\" +\n+\t\t\t\t\"    deepNested.nested2 AS nested2,\\n\" +\n+\t\t\t\t\"    deepNested.nested2.num AS nestedNum\\n\" +\n+\t\t\t\t\"FROM NestedTable\";\n+\t\tutil().verifyPlan(sqlQuery);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cb66dc97754abc1468b03508a41a4f3a34ce9572"}, "originalPosition": 61}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "11b0dfc5a9a5e6aa335f070abfc6781569deb96d", "author": {"user": {"login": "fsk119", "name": "Shengkai "}}, "url": "https://github.com/apache/flink/commit/11b0dfc5a9a5e6aa335f070abfc6781569deb96d", "committedDate": "2020-10-16T03:49:23Z", "message": "fix godfrey's comment:\n1. use qualified name list to get the projectedFields and build new projections;\n2. add more tests"}, "afterCommit": {"oid": "4b3b9898ceeebc31777e2aa96d04b445cf578d87", "author": {"user": {"login": "fsk119", "name": "Shengkai "}}, "url": "https://github.com/apache/flink/commit/4b3b9898ceeebc31777e2aa96d04b445cf578d87", "committedDate": "2020-10-16T03:56:17Z", "message": "fix godfrey's comment:\n1. use qualified name list to get the projectedFields and build new projections;\n2. add more tests"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTEwOTczNDIy", "url": "https://github.com/apache/flink/pull/13631#pullrequestreview-510973422", "createdAt": "2020-10-17T09:48:23Z", "commit": {"oid": "d1b917bb33f837bf04e8351affe7e108911e79b4"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xN1QwOTo0ODoyNFrOHjbpcw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xN1QwOTo1OTowOVrOHjb6yQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjkxNTE4Nw==", "bodyText": "It's better we does not use _1, _2, which makes the code hard to read. we can use use case match with meaningful name, just like:\nright.take(left.length).zip(left).foldLeft(true) {\n       case (ans, (rName, lName)) => {\n          if (ans) {\n            lName.equals(rName)\n          } else {\n            false\n          }\n        }\n      }", "url": "https://github.com/apache/flink/pull/13631#discussion_r506915187", "createdAt": "2020-10-17T09:48:24Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/RexNodeExtractor.scala", "diffHunk": "@@ -256,7 +256,15 @@ class RefFieldAccessorVisitor(usedFields: Array[Int]) extends RexVisitorImpl[Uni\n     if (right.length < left.length) {\n       false\n     } else {\n-      right.take(left.length).equals(left)\n+      right.take(left.length).zip(left).foldLeft(true) {\n+        (ans, fields) => {\n+          if (ans) {\n+            fields._1.equals(fields._2)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d1b917bb33f837bf04e8351affe7e108911e79b4"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjkxNzM5Ng==", "bodyText": "how about resolve the conflicts through adding postfix ?", "url": "https://github.com/apache/flink/pull/13631#discussion_r506917396", "createdAt": "2020-10-17T09:53:50Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-common/src/main/java/org/apache/flink/table/utils/TableSchemaUtils.java", "diffHunk": "@@ -75,10 +79,46 @@ public static TableSchema projectSchema(TableSchema tableSchema, int[][] project\n \t\tcheckArgument(containsPhysicalColumnsOnly(tableSchema), \"Projection is only supported for physical columns.\");\n \t\tTableSchema.Builder schemaBuilder = TableSchema.builder();\n \t\tList<TableColumn> tableColumns = tableSchema.getTableColumns();\n+\t\tMap<String, String> nameDomain = new HashMap<>();\n+\t\tString exceptionTemplate = \"Get name conflicts for origin fields %s and %s with new name `%s`. \" +\n+\t\t\t\t\"When pushing projection into scan, we will concatenate top level names with delimiter '_'. \" +\n+\t\t\t\t\"Please rename the origin field names when creating table.\";\n+\t\tString originFullyQualifiedName;\n+\t\tString newName;\n \t\tfor (int[] fieldPath : projectedFields) {\n-\t\t\tcheckArgument(fieldPath.length == 1, \"Nested projection push down is not supported yet.\");\n-\t\t\tTableColumn column = tableColumns.get(fieldPath[0]);\n-\t\t\tschemaBuilder.field(column.getName(), column.getType());\n+\t\t\tif (fieldPath.length == 1) {\n+\t\t\t\tTableColumn column = tableColumns.get(fieldPath[0]);\n+\t\t\t\tnewName = column.getName();\n+\t\t\t\toriginFullyQualifiedName = String.format(\"`%s`\", column.getName());\n+\t\t\t\tif (nameDomain.containsKey(column.getName())) {\n+\t\t\t\t\tthrow new TableException(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d1b917bb33f837bf04e8351affe7e108911e79b4"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjkxOTYyNQ==", "bodyText": "how about keep the original test, and add a new test case the verify the case which field name contain dot", "url": "https://github.com/apache/flink/pull/13631#discussion_r506919625", "createdAt": "2020-10-17T09:59:09Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRuleTest.java", "diffHunk": "@@ -70,41 +69,40 @@ public void setup() {\n \t\t\t\t\t\t\" 'bounded' = 'true'\\n\" +\n \t\t\t\t\t\t\")\";\n \t\tutil().tableEnv().executeSql(ddl2);\n-\t}\n-\n-\t@Override\n-\tpublic void testNestedProject() {\n-\t\texpectedException().expect(TableException.class);\n-\t\texpectedException().expectMessage(\"Nested projection push down is unsupported now.\");\n-\t\ttestNestedProject(true);\n-\t}\n \n-\t@Test\n-\tpublic void testNestedProjectDisabled() {\n-\t\ttestNestedProject(false);\n-\t}\n-\n-\tprivate void testNestedProject(boolean nestedProjectionSupported) {\n-\t\tString ddl =\n+\t\tString ddl3 =\n \t\t\t\t\"CREATE TABLE NestedTable (\\n\" +\n \t\t\t\t\t\t\"  id int,\\n\" +\n-\t\t\t\t\t\t\"  deepNested row<nested1 row<name string, `value` int>, nested2 row<num int, flag boolean>>,\\n\" +\n-\t\t\t\t\t\t\"  nested row<name string, `value` int>,\\n\" +\n+\t\t\t\t\t\t\"  deepNested row<nested1 row<name string, `value` int>, `nested2.` row<num int, flag boolean>>,\\n\" +\n+\t\t\t\t\t\t\"  nested row<name string, `value.` int>,\\n\" +\n \t\t\t\t\t\t\"  name string\\n\" +\n \t\t\t\t\t\t\") WITH (\\n\" +\n \t\t\t\t\t\t\" 'connector' = 'values',\\n\" +\n-\t\t\t\t\t\t\" 'nested-projection-supported' = '\" + nestedProjectionSupported + \"',\\n\" +\n+\t\t\t\t\t\t\" 'nested-projection-supported' = 'true',\" +\n \t\t\t\t\t\t\"  'bounded' = 'true'\\n\" +\n \t\t\t\t\t\t\")\";\n-\t\tutil().tableEnv().executeSql(ddl);\n+\t\tutil().tableEnv().executeSql(ddl3);\n+\t}\n \n+\t@Override\n+\t@Test\n+\tpublic void testNestedProject() {\n \t\tString sqlQuery = \"SELECT id,\\n\" +\n \t\t\t\t\"    deepNested.nested1.name AS nestedName,\\n\" +\n-\t\t\t\t\"    nested.`value` AS nestedValue,\\n\" +\n-\t\t\t\t\"    deepNested.nested2.flag AS nestedFlag,\\n\" +\n-\t\t\t\t\"    deepNested.nested2.num AS nestedNum\\n\" +\n+\t\t\t\t\"    nested.`value.` AS nestedValue,\\n\" +\n+\t\t\t\t\"    deepNested.`nested2.`.flag AS nestedFlag,\\n\" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d1b917bb33f837bf04e8351affe7e108911e79b4"}, "originalPosition": 55}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "46f99482ad7bea8e467130fe5dd92b61b7d3d444", "author": {"user": {"login": "fsk119", "name": "Shengkai "}}, "url": "https://github.com/apache/flink/commit/46f99482ad7bea8e467130fe5dd92b61b7d3d444", "committedDate": "2020-10-19T09:30:53Z", "message": "use postfix to solve name conflicts."}, "afterCommit": {"oid": "a7fe7dec5d7c869ccbc644bba45df241dd05e953", "author": {"user": {"login": "fsk119", "name": "Shengkai "}}, "url": "https://github.com/apache/flink/commit/a7fe7dec5d7c869ccbc644bba45df241dd05e953", "committedDate": "2020-10-26T02:44:09Z", "message": "1. use postfix \"$%d\" to resolve the name conflicts\n2. rewrite the rule to support metadata push down:\n2.1 we will check the source and extract the physical part of the schema. If the source supports nested projection push down, we use `RexNodeExtractor.extractRefNestedInputFields` to extract data else we add the physical part info into the coordinates info.\n2.2 If the source supports metadata push down, we add the metadata info into the coordinates.\n2.3 with the final coordinates, we write the projection."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "da9bc2d9410e070611621051bcce5461443335a5", "author": {"user": {"login": "fsk119", "name": "Shengkai "}}, "url": "https://github.com/apache/flink/commit/da9bc2d9410e070611621051bcce5461443335a5", "committedDate": "2020-10-26T05:39:10Z", "message": "fix test"}, "afterCommit": {"oid": "1697a20356799dd5e2f8863110de1f4626664f12", "author": {"user": {"login": "fsk119", "name": "Shengkai "}}, "url": "https://github.com/apache/flink/commit/1697a20356799dd5e2f8863110de1f4626664f12", "committedDate": "2020-10-28T13:40:17Z", "message": "fix test"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE5NTkxNTEw", "url": "https://github.com/apache/flink/pull/13631#pullrequestreview-519591510", "createdAt": "2020-10-29T12:00:49Z", "commit": {"oid": "1697a20356799dd5e2f8863110de1f4626664f12"}, "state": "COMMENTED", "comments": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOVQxMjowMDo0OVrOHqYjkA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOVQxMzoxMjowNVrOHqbHQA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDIwNDU2MA==", "bodyText": "nit: how about adding  _ before \"$\"", "url": "https://github.com/apache/flink/pull/13631#discussion_r514204560", "createdAt": "2020-10-29T12:00:49Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-common/src/main/java/org/apache/flink/table/types/utils/DataTypeUtils.java", "diffHunk": "@@ -74,42 +76,46 @@\n \t *\n \t * <p>Note: Index paths allow for arbitrary deep nesting. For example, {@code [[0, 2, 1], ...]}\n \t * specifies to include the 2nd field of the 3rd field of the 1st field in the top-level row.\n+\t * Sometimes, it may get name conflicts when extract fields from the row field. Considering the\n+\t * the path is unique to extract fields, it makes sense to use the path to the fields with\n+\t * delimiter `_` as the new name of the field. For example, the new name of the field `b` in\n+\t * the row `a` is `a_b` rather than `b`. But it may still gets name conflicts in some situation,\n+\t * such as the field `a_b` in the top level schema. In such situation, it will use the postfix\n+\t * in the format '$%d' to resolve the name conflicts.\n \t */\n \tpublic static DataType projectRow(DataType dataType, int[][] indexPaths) {\n \t\tfinal List<RowField> updatedFields = new ArrayList<>();\n \t\tfinal List<DataType> updatedChildren = new ArrayList<>();\n+\t\tSet<String> nameDomain = new HashSet<>();\n+\t\tint duplicateCount = 0;\n \t\tfor (int[] indexPath : indexPaths) {\n-\t\t\tupdatedFields.add(selectChild(dataType.getLogicalType(), indexPath, 0));\n-\t\t\tupdatedChildren.add(selectChild(dataType, indexPath, 0));\n+\t\t\tDataType fieldType = dataType.getChildren().get(indexPath[0]);\n+\t\t\tLogicalType fieldLogicalType = fieldType.getLogicalType();\n+\t\t\tStringBuilder builder =\n+\t\t\t\t\tnew StringBuilder(((RowType) dataType.getLogicalType()).getFieldNames().get(indexPath[0]));\n+\t\t\tfor (int index = 1; index < indexPath.length; index++) {\n+\t\t\t\tPreconditions.checkArgument(\n+\t\t\t\t\t\thasRoot(fieldLogicalType, LogicalTypeRoot.ROW),\n+\t\t\t\t\t\t\"Row data type expected.\");\n+\t\t\t\tRowType rowtype = ((RowType) fieldLogicalType);\n+\t\t\t\tbuilder.append(\"_\").append(rowtype.getFieldNames().get(indexPath[index]));\n+\t\t\t\tfieldLogicalType = rowtype.getFields().get(indexPath[index]).getType();\n+\t\t\t\tfieldType = fieldType.getChildren().get(indexPath[index]);\n+\t\t\t}\n+\t\t\tString path = builder.toString();\n+\t\t\twhile (nameDomain.contains(path)) {\n+\t\t\t\tpath = builder.append(\"$\").append(duplicateCount++).toString();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1697a20356799dd5e2f8863110de1f4626664f12"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDIyMDQ3OQ==", "bodyText": "what if the metadata columns have nested fields ?", "url": "https://github.com/apache/flink/pull/13631#discussion_r514220479", "createdAt": "2020-10-29T12:30:03Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRule.java", "diffHunk": "@@ -109,73 +112,67 @@ public void onMatch(RelOptRuleCall call) {\n \t\t\tusedFields = refFields;\n \t\t}\n \t\t// if no fields can be projected, we keep the original plan.\n-\t\tif (usedFields.length == fieldCount) {\n+\t\tif (!supportsNestedProjection && usedFields.length == fieldCount) {\n \t\t\treturn;\n \t\t}\n \n-\t\tfinal List<String> projectedFieldNames = IntStream.of(usedFields)\n-\t\t\t.mapToObj(fieldNames::get)\n-\t\t\t.collect(Collectors.toList());\n-\n \t\tfinal TableSchema oldSchema = oldTableSourceTable.catalogTable().getSchema();\n \t\tfinal DynamicTableSource oldSource = oldTableSourceTable.tableSource();\n \t\tfinal List<String> metadataKeys = DynamicSourceUtils.createRequiredMetadataKeys(oldSchema, oldSource);\n \t\tfinal int physicalFieldCount = fieldCount - metadataKeys.size();\n \t\tfinal DynamicTableSource newSource = oldSource.copy();\n \n-\t\t// remove metadata columns from the projection push down and store it in a separate list\n-\t\t// the projection push down itself happens purely on physical columns\n-\t\tfinal int[] usedPhysicalFields;\n-\t\tfinal List<String> usedMetadataKeys;\n-\t\tif (newSource instanceof SupportsReadingMetadata) {\n-\t\t\tusedPhysicalFields = IntStream.of(usedFields)\n-\t\t\t\t// select only physical columns\n-\t\t\t\t.filter(i -> i < physicalFieldCount)\n-\t\t\t\t.toArray();\n-\t\t\tfinal List<String> usedMetadataKeysUnordered = IntStream.of(usedFields)\n-\t\t\t\t// select only metadata columns\n-\t\t\t\t.filter(i -> i >= physicalFieldCount)\n-\t\t\t\t// map the indices to keys\n-\t\t\t\t.mapToObj(i -> metadataKeys.get(fieldCount - i - 1))\n-\t\t\t\t.collect(Collectors.toList());\n-\t\t\t// order the keys according to the source's declaration\n-\t\t\tusedMetadataKeys = metadataKeys\n-\t\t\t\t.stream()\n-\t\t\t\t.filter(usedMetadataKeysUnordered::contains)\n-\t\t\t\t.collect(Collectors.toList());\n+\t\tfinal List<List<Integer>> usedFieldsCoordinates = new ArrayList<>();\n+\t\tfinal Map<Integer, Map<List<String>, Integer>> fieldCoordinatesToOrder = new HashMap<>();\n+\n+\t\tif (supportsNestedProjection) {\n+\t\t\tgetCoordinatesAndMappingOfPhysicalColumnWithNestedProjection(\n+\t\t\t\t\tproject, oldSchema, usedFields, physicalFieldCount, usedFieldsCoordinates, fieldCoordinatesToOrder);\n \t\t} else {\n-\t\t\tusedPhysicalFields = usedFields;\n-\t\t\tusedMetadataKeys = Collections.emptyList();\n+\t\t\tfor (int usedField : usedFields) {\n+\t\t\t\t// filter metadata columns", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1697a20356799dd5e2f8863110de1f4626664f12"}, "originalPosition": 122}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDIyNDc3OQ==", "bodyText": "the method name is too long, change to getExpandedFieldsAndOrderMapping ? add some comments the explain the arguments", "url": "https://github.com/apache/flink/pull/13631#discussion_r514224779", "createdAt": "2020-10-29T12:37:15Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRule.java", "diffHunk": "@@ -190,35 +187,82 @@ public void onMatch(RelOptRuleCall call) {\n \t\t}\n \t}\n \n-\tprivate void applyUpdatedMetadata(\n-\t\t\tDynamicTableSource oldSource,\n-\t\t\tTableSchema oldSchema,\n+\tprivate DataType applyUpdateMetadataAndGetNewDataType(\n \t\t\tDynamicTableSource newSource,\n+\t\t\tDataType producedDataType,\n \t\t\tList<String> metadataKeys,\n-\t\t\tList<String> usedMetadataKeys,\n+\t\t\tint[] usedFields,\n \t\t\tint physicalFieldCount,\n-\t\t\tint[][] projectedPhysicalFields) {\n-\t\tif (newSource instanceof SupportsReadingMetadata) {\n-\t\t\tfinal DataType producedDataType = TypeConversions.fromLogicalToDataType(\n-\t\t\t\tDynamicSourceUtils.createProducedType(oldSchema, oldSource));\n+\t\t\tList<List<Integer>> usedFieldsCoordinates,\n+\t\t\tMap<Integer, Map<List<String>, Integer>> fieldCoordinatesToOrder) {\n+\t\tfinal List<String> usedMetadataKeysUnordered = IntStream.of(usedFields)\n+\t\t\t\t// select only metadata columns\n+\t\t\t\t.filter(i -> i >= physicalFieldCount)\n+\t\t\t\t// map the indices to keys\n+\t\t\t\t.mapToObj(i -> metadataKeys.get(i - physicalFieldCount))\n+\t\t\t\t.collect(Collectors.toList());\n+\t\t// order the keys according to the source's declaration\n+\t\tfinal List<String> usedMetadataKeys = metadataKeys\n+\t\t\t\t.stream()\n+\t\t\t\t.filter(usedMetadataKeysUnordered::contains)\n+\t\t\t\t.collect(Collectors.toList());\n \n-\t\t\tfinal int[][] projectedMetadataFields = usedMetadataKeys\n+\t\tfinal List<List<Integer>> projectedMetadataFields = usedMetadataKeys\n \t\t\t\t.stream()\n \t\t\t\t.map(metadataKeys::indexOf)\n-\t\t\t\t.map(i -> new int[]{ physicalFieldCount + i })\n-\t\t\t\t.toArray(int[][]::new);\n+\t\t\t\t.map(i -> {\n+\t\t\t\t\tfieldCoordinatesToOrder.put(physicalFieldCount + i, Collections.singletonMap(Collections.singletonList(\"*\"), fieldCoordinatesToOrder.size()));\n+\t\t\t\t\treturn Collections.singletonList(physicalFieldCount + i); })\n+\t\t\t\t.collect(Collectors.toList());\n+\t\tusedFieldsCoordinates.addAll(projectedMetadataFields);\n \n-\t\t\tfinal int[][] projectedFields = Stream\n-\t\t\t\t.concat(\n-\t\t\t\t\tStream.of(projectedPhysicalFields),\n-\t\t\t\t\tStream.of(projectedMetadataFields)\n-\t\t\t\t)\n+\t\tint[][] allFields = usedFieldsCoordinates\n+\t\t\t\t.stream()\n+\t\t\t\t.map(coordinates -> coordinates.stream().mapToInt(i -> i).toArray())\n \t\t\t\t.toArray(int[][]::new);\n \n-\t\t\t// create a new, final data type that includes all projections\n-\t\t\tfinal DataType newProducedDataType = DataTypeUtils.projectRow(producedDataType, projectedFields);\n+\t\tDataType newProducedDataType = DataTypeUtils.projectRow(producedDataType, allFields);\n \n-\t\t\t((SupportsReadingMetadata) newSource).applyReadableMetadata(usedMetadataKeys, newProducedDataType);\n+\t\t((SupportsReadingMetadata) newSource).applyReadableMetadata(usedMetadataKeys, newProducedDataType);\n+\t\treturn newProducedDataType;\n+\t}\n+\n+\tprivate void getCoordinatesAndMappingOfPhysicalColumnWithNestedProjection(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1697a20356799dd5e2f8863110de1f4626664f12"}, "originalPosition": 246}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDIyOTcxNg==", "bodyText": "nit: It is better to close the position of the defined field to the position in which the field is used.", "url": "https://github.com/apache/flink/pull/13631#discussion_r514229716", "createdAt": "2020-10-29T12:45:34Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRule.java", "diffHunk": "@@ -74,28 +79,26 @@ public boolean matches(RelOptRuleCall call) {\n \t\tif (tableSourceTable == null || !(tableSourceTable.tableSource() instanceof SupportsProjectionPushDown)) {\n \t\t\treturn false;\n \t\t}\n-\t\tSupportsProjectionPushDown pushDownSource = (SupportsProjectionPushDown) tableSourceTable.tableSource();\n-\t\tif (pushDownSource.supportsNestedProjection()) {\n-\t\t\tthrow new TableException(\"Nested projection push down is unsupported now. \\n\" +\n-\t\t\t\t\t\"Please disable nested projection (SupportsProjectionPushDown#supportsNestedProjection returns false), \" +\n-\t\t\t\t\t\"planner will push down the top-level columns.\");\n-\t\t} else {\n-\t\t\treturn true;\n-\t\t}\n+\t\treturn Arrays.stream(tableSourceTable.extraDigests()).noneMatch(digest -> digest.startsWith(\"project=[\"));\n \t}\n \n \t@Override\n \tpublic void onMatch(RelOptRuleCall call) {\n \t\tfinal LogicalProject project = call.rel(0);\n \t\tfinal LogicalTableScan scan = call.rel(1);\n \n+\t\tTableSourceTable oldTableSourceTable = scan.getTable().unwrap(TableSourceTable.class);\n+\n+\t\tfinal boolean supportsNestedProjection =\n+\t\t\t\t((SupportsProjectionPushDown) oldTableSourceTable.tableSource()).supportsNestedProjection();\n+\t\tfinal boolean supportsReadingMetaData = oldTableSourceTable.tableSource() instanceof SupportsReadingMetadata;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1697a20356799dd5e2f8863110de1f4626664f12"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDIzMjU1Mw==", "bodyText": "use for to make it clearer", "url": "https://github.com/apache/flink/pull/13631#discussion_r514232553", "createdAt": "2020-10-29T12:50:16Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRule.java", "diffHunk": "@@ -190,35 +187,82 @@ public void onMatch(RelOptRuleCall call) {\n \t\t}\n \t}\n \n-\tprivate void applyUpdatedMetadata(\n-\t\t\tDynamicTableSource oldSource,\n-\t\t\tTableSchema oldSchema,\n+\tprivate DataType applyUpdateMetadataAndGetNewDataType(\n \t\t\tDynamicTableSource newSource,\n+\t\t\tDataType producedDataType,\n \t\t\tList<String> metadataKeys,\n-\t\t\tList<String> usedMetadataKeys,\n+\t\t\tint[] usedFields,\n \t\t\tint physicalFieldCount,\n-\t\t\tint[][] projectedPhysicalFields) {\n-\t\tif (newSource instanceof SupportsReadingMetadata) {\n-\t\t\tfinal DataType producedDataType = TypeConversions.fromLogicalToDataType(\n-\t\t\t\tDynamicSourceUtils.createProducedType(oldSchema, oldSource));\n+\t\t\tList<List<Integer>> usedFieldsCoordinates,\n+\t\t\tMap<Integer, Map<List<String>, Integer>> fieldCoordinatesToOrder) {\n+\t\tfinal List<String> usedMetadataKeysUnordered = IntStream.of(usedFields)\n+\t\t\t\t// select only metadata columns\n+\t\t\t\t.filter(i -> i >= physicalFieldCount)\n+\t\t\t\t// map the indices to keys\n+\t\t\t\t.mapToObj(i -> metadataKeys.get(i - physicalFieldCount))\n+\t\t\t\t.collect(Collectors.toList());\n+\t\t// order the keys according to the source's declaration\n+\t\tfinal List<String> usedMetadataKeys = metadataKeys\n+\t\t\t\t.stream()\n+\t\t\t\t.filter(usedMetadataKeysUnordered::contains)\n+\t\t\t\t.collect(Collectors.toList());\n \n-\t\t\tfinal int[][] projectedMetadataFields = usedMetadataKeys\n+\t\tfinal List<List<Integer>> projectedMetadataFields = usedMetadataKeys\n \t\t\t\t.stream()\n \t\t\t\t.map(metadataKeys::indexOf)\n-\t\t\t\t.map(i -> new int[]{ physicalFieldCount + i })\n-\t\t\t\t.toArray(int[][]::new);\n+\t\t\t\t.map(i -> {\n+\t\t\t\t\tfieldCoordinatesToOrder.put(physicalFieldCount + i, Collections.singletonMap(Collections.singletonList(\"*\"), fieldCoordinatesToOrder.size()));\n+\t\t\t\t\treturn Collections.singletonList(physicalFieldCount + i); })\n+\t\t\t\t.collect(Collectors.toList());\n+\t\tusedFieldsCoordinates.addAll(projectedMetadataFields);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1697a20356799dd5e2f8863110de1f4626664f12"}, "originalPosition": 225}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDIzMzM5NA==", "bodyText": "allFields -> projectedFields", "url": "https://github.com/apache/flink/pull/13631#discussion_r514233394", "createdAt": "2020-10-29T12:51:37Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRule.java", "diffHunk": "@@ -190,35 +187,82 @@ public void onMatch(RelOptRuleCall call) {\n \t\t}\n \t}\n \n-\tprivate void applyUpdatedMetadata(\n-\t\t\tDynamicTableSource oldSource,\n-\t\t\tTableSchema oldSchema,\n+\tprivate DataType applyUpdateMetadataAndGetNewDataType(\n \t\t\tDynamicTableSource newSource,\n+\t\t\tDataType producedDataType,\n \t\t\tList<String> metadataKeys,\n-\t\t\tList<String> usedMetadataKeys,\n+\t\t\tint[] usedFields,\n \t\t\tint physicalFieldCount,\n-\t\t\tint[][] projectedPhysicalFields) {\n-\t\tif (newSource instanceof SupportsReadingMetadata) {\n-\t\t\tfinal DataType producedDataType = TypeConversions.fromLogicalToDataType(\n-\t\t\t\tDynamicSourceUtils.createProducedType(oldSchema, oldSource));\n+\t\t\tList<List<Integer>> usedFieldsCoordinates,\n+\t\t\tMap<Integer, Map<List<String>, Integer>> fieldCoordinatesToOrder) {\n+\t\tfinal List<String> usedMetadataKeysUnordered = IntStream.of(usedFields)\n+\t\t\t\t// select only metadata columns\n+\t\t\t\t.filter(i -> i >= physicalFieldCount)\n+\t\t\t\t// map the indices to keys\n+\t\t\t\t.mapToObj(i -> metadataKeys.get(i - physicalFieldCount))\n+\t\t\t\t.collect(Collectors.toList());\n+\t\t// order the keys according to the source's declaration\n+\t\tfinal List<String> usedMetadataKeys = metadataKeys\n+\t\t\t\t.stream()\n+\t\t\t\t.filter(usedMetadataKeysUnordered::contains)\n+\t\t\t\t.collect(Collectors.toList());\n \n-\t\t\tfinal int[][] projectedMetadataFields = usedMetadataKeys\n+\t\tfinal List<List<Integer>> projectedMetadataFields = usedMetadataKeys\n \t\t\t\t.stream()\n \t\t\t\t.map(metadataKeys::indexOf)\n-\t\t\t\t.map(i -> new int[]{ physicalFieldCount + i })\n-\t\t\t\t.toArray(int[][]::new);\n+\t\t\t\t.map(i -> {\n+\t\t\t\t\tfieldCoordinatesToOrder.put(physicalFieldCount + i, Collections.singletonMap(Collections.singletonList(\"*\"), fieldCoordinatesToOrder.size()));\n+\t\t\t\t\treturn Collections.singletonList(physicalFieldCount + i); })\n+\t\t\t\t.collect(Collectors.toList());\n+\t\tusedFieldsCoordinates.addAll(projectedMetadataFields);\n \n-\t\t\tfinal int[][] projectedFields = Stream\n-\t\t\t\t.concat(\n-\t\t\t\t\tStream.of(projectedPhysicalFields),\n-\t\t\t\t\tStream.of(projectedMetadataFields)\n-\t\t\t\t)\n+\t\tint[][] allFields = usedFieldsCoordinates", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1697a20356799dd5e2f8863110de1f4626664f12"}, "originalPosition": 232}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDI0MjE0Mw==", "bodyText": "nit: please delete a unused method assertPlannerExpressionArrayEquals", "url": "https://github.com/apache/flink/pull/13631#discussion_r514242143", "createdAt": "2020-10-29T13:05:13Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/utils/RexNodeExtractorTest.scala", "diffHunk": "@@ -79,7 +78,7 @@ class RexNodeExtractorTest extends RexNodeTestBase {\n     val usedFields = RexNodeExtractor.extractRefInputFields(rexProgram)\n     val usedNestedFields = RexNodeExtractor.extractRefNestedInputFields(rexProgram, usedFields)\n \n-    val expected = Array(Array(\"amount\"), Array(\"*\"))\n+    val expected = Array(Array(util.Arrays.asList(\"amount\")), Array(util.Arrays.asList(\"*\")))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1697a20356799dd5e2f8863110de1f4626664f12"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDI0NjQ2NA==", "bodyText": "remove unused imports", "url": "https://github.com/apache/flink/pull/13631#discussion_r514246464", "createdAt": "2020-10-29T13:12:05Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/batch/sql/TableSourceITCase.scala", "diffHunk": "@@ -66,6 +66,26 @@ class TableSourceITCase extends BatchTestBase {\n          |  'bounded' = 'true'\n          |)\n          |\"\"\".stripMargin)\n+    val nestedTableDataId = TestValuesTableFactory.registerData(TestData.deepNestedRow)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1697a20356799dd5e2f8863110de1f4626664f12"}, "originalPosition": 4}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIwNDE4Nzky", "url": "https://github.com/apache/flink/pull/13631#pullrequestreview-520418792", "createdAt": "2020-10-30T07:11:30Z", "commit": {"oid": "a5194ee25cf0c08ecd1d1e484b3f2e335ffb7656"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMFQwNzoxMTozMFrOHrDcyw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMFQwNzoxMTozMFrOHrDcyw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDkwNzMzOQ==", "bodyText": "This line is unnecessary, use usedFieldsCoordinates.add(Collections.singletonList(physicalFieldCount + index)) at line 217", "url": "https://github.com/apache/flink/pull/13631#discussion_r514907339", "createdAt": "2020-10-30T07:11:30Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRule.java", "diffHunk": "@@ -207,27 +208,35 @@ private DataType applyUpdateMetadataAndGetNewDataType(\n \t\t\t\t.filter(usedMetadataKeysUnordered::contains)\n \t\t\t\t.collect(Collectors.toList());\n \n-\t\tfinal List<List<Integer>> projectedMetadataFields = usedMetadataKeys\n-\t\t\t\t.stream()\n-\t\t\t\t.map(metadataKeys::indexOf)\n-\t\t\t\t.map(i -> {\n-\t\t\t\t\tfieldCoordinatesToOrder.put(physicalFieldCount + i, Collections.singletonMap(Collections.singletonList(\"*\"), fieldCoordinatesToOrder.size()));\n-\t\t\t\t\treturn Collections.singletonList(physicalFieldCount + i); })\n-\t\t\t\t.collect(Collectors.toList());\n+\t\tfinal List<List<Integer>> projectedMetadataFields = new ArrayList<>(usedMetadataKeys.size());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a5194ee25cf0c08ecd1d1e484b3f2e335ffb7656"}, "originalPosition": 44}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIwNDc0ODI4", "url": "https://github.com/apache/flink/pull/13631#pullrequestreview-520474828", "createdAt": "2020-10-30T08:55:30Z", "commit": {"oid": "9898b59d09c7483e3e0c034372a4875c46844841"}, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIwNDc2NDUy", "url": "https://github.com/apache/flink/pull/13631#pullrequestreview-520476452", "createdAt": "2020-10-30T08:57:51Z", "commit": {"oid": "9898b59d09c7483e3e0c034372a4875c46844841"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMFQwODo1Nzo1MlrOHrGNRQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMFQwODo1Nzo1MlrOHrGNRQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDk1MjUxNw==", "bodyText": "MyTable does not support nested-projection-supported", "url": "https://github.com/apache/flink/pull/13631#discussion_r514952517", "createdAt": "2020-10-30T08:57:52Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRuleTest.java", "diffHunk": "@@ -70,34 +70,47 @@ public void setup() {\n \t\t\t\t\t\t\" 'bounded' = 'true'\\n\" +\n \t\t\t\t\t\t\")\";\n \t\tutil().tableEnv().executeSql(ddl2);\n-\t}\n-\n-\t@Override\n-\tpublic void testNestedProject() {\n-\t\texpectedException().expect(TableException.class);\n-\t\texpectedException().expectMessage(\"Nested projection push down is unsupported now.\");\n-\t\ttestNestedProject(true);\n-\t}\n-\n-\t@Test\n-\tpublic void testNestedProjectDisabled() {\n-\t\ttestNestedProject(false);\n-\t}\n \n-\tprivate void testNestedProject(boolean nestedProjectionSupported) {\n-\t\tString ddl =\n+\t\tString ddl3 =\n \t\t\t\t\"CREATE TABLE NestedTable (\\n\" +\n \t\t\t\t\t\t\"  id int,\\n\" +\n \t\t\t\t\t\t\"  deepNested row<nested1 row<name string, `value` int>, nested2 row<num int, flag boolean>>,\\n\" +\n \t\t\t\t\t\t\"  nested row<name string, `value` int>,\\n\" +\n+\t\t\t\t\t\t\"  `deepNestedWith.` row<`.value` int, nested row<name string, `.value` int>>,\\n\" +\n \t\t\t\t\t\t\"  name string\\n\" +\n \t\t\t\t\t\t\") WITH (\\n\" +\n \t\t\t\t\t\t\" 'connector' = 'values',\\n\" +\n-\t\t\t\t\t\t\" 'nested-projection-supported' = '\" + nestedProjectionSupported + \"',\\n\" +\n-\t\t\t\t\t\t\"  'bounded' = 'true'\\n\" +\n+\t\t\t\t\t\t\" 'nested-projection-supported' = 'true',\" +\n+\t\t\t\t\t\t\" 'bounded' = 'true'\\n\" +\n+\t\t\t\t\t\t\")\";\n+\t\tutil().tableEnv().executeSql(ddl3);\n+\n+\t\tString ddl4 =\n+\t\t\t\t\"CREATE TABLE MetadataTable(\\n\" +\n+\t\t\t\t\t\t\"  id int,\\n\" +\n+\t\t\t\t\t\t\"  deepNested row<nested1 row<name string, `value` int>, nested2 row<num int, flag boolean>>,\\n\" +\n+\t\t\t\t\t\t\"  metadata_1 int metadata,\\n\" +\n+\t\t\t\t\t\t\"  metadata_2 string metadata\\n\" +\n+\t\t\t\t\t\t\") WITH (\" +\n+\t\t\t\t\t\t\" 'connector' = 'values',\" +\n+\t\t\t\t\t\t\" 'nested-projection-supported' = 'true',\" +\n+\t\t\t\t\t\t\" 'bounded' = 'true',\\n\" +\n+\t\t\t\t\t\t\" 'readable-metadata' = 'metadata_1:INT, metadata_2:STRING, metadata_3:BIGINT'\" +\n \t\t\t\t\t\t\")\";\n-\t\tutil().tableEnv().executeSql(ddl);\n+\t\tutil().tableEnv().executeSql(ddl4);\n+\t}\n \n+\t@Test\n+\tpublic void testProjectWithMapType() {\n+\t\tString sqlQuery =\n+\t\t\t\t\"SELECT a, d['e']\\n\" +\n+\t\t\t\t\t\t\"FROM MyTable\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9898b59d09c7483e3e0c034372a4875c46844841"}, "originalPosition": 74}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "e0da25cfa5ae3e3a5944327f56e392590796e6a0", "author": {"user": {"login": "fsk119", "name": "Shengkai "}}, "url": "https://github.com/apache/flink/commit/e0da25cfa5ae3e3a5944327f56e392590796e6a0", "committedDate": "2020-10-31T16:33:05Z", "message": "introduce the new extractor and rewriter."}, "afterCommit": {"oid": "cb980798a022a35131fccd5c536a5f5519f38cba", "author": {"user": {"login": "fsk119", "name": "Shengkai "}}, "url": "https://github.com/apache/flink/commit/cb980798a022a35131fccd5c536a5f5519f38cba", "committedDate": "2020-11-01T02:14:15Z", "message": "1. introduce the new extractor and rewriter:\n2. fix failed test: the order of fileds in the new schema is determined by the hashmap rather that the order in the projections."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIxMjM1MTQx", "url": "https://github.com/apache/flink/pull/13631#pullrequestreview-521235141", "createdAt": "2020-11-01T15:34:21Z", "commit": {"oid": "aa094733a6678ec72712da928dd457e79f3f81a9"}, "state": "COMMENTED", "comments": {"totalCount": 16, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMVQxNTozNDoyMVrOHrv7zg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMVQxOTowMzoyN1rOHrxULA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTYzNjE3NA==", "bodyText": "indexInOriginalSchema ?", "url": "https://github.com/apache/flink/pull/13631#discussion_r515636174", "createdAt": "2020-11-01T15:34:21Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/RexNodeNestedField.scala", "diffHunk": "@@ -0,0 +1,295 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.utils\n+\n+import org.apache.flink.table.api.TableException\n+\n+import org.apache.calcite.rel.`type`.RelDataType\n+import org.apache.calcite.rex._\n+\n+import java.util\n+import java.util.{List => JList, Map => JMap}\n+\n+import scala.collection.JavaConversions._\n+import scala.collection.JavaConverters._\n+\n+/**\n+ * RexNodeNestedField is a tree node to build the used fields tree.\n+ *\n+ * @param name      The name of the fields in the origin schema\n+ * @param index     The index of the field in the origin schema. It", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "aa094733a6678ec72712da928dd457e79f3f81a9"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTYzNzIwNQ==", "bodyText": "children", "url": "https://github.com/apache/flink/pull/13631#discussion_r515637205", "createdAt": "2020-11-01T15:43:58Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/RexNodeNestedField.scala", "diffHunk": "@@ -0,0 +1,295 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.utils\n+\n+import org.apache.flink.table.api.TableException\n+\n+import org.apache.calcite.rel.`type`.RelDataType\n+import org.apache.calcite.rex._\n+\n+import java.util\n+import java.util.{List => JList, Map => JMap}\n+\n+import scala.collection.JavaConversions._\n+import scala.collection.JavaConverters._\n+\n+/**\n+ * RexNodeNestedField is a tree node to build the used fields tree.\n+ *\n+ * @param name      The name of the fields in the origin schema\n+ * @param index     The index of the field in the origin schema. It\n+ *                  only works for the RowType.\n+ * @param fieldType The type of the field. It is useful when\n+ *                  rewriting the projections. It uses the LinkedHashMap\n+ *                  to keep the insert order. In some cases, it can\n+ *                  reduce the cost of the reorder of the fields in query.\n+ * @param useAll    Mark the field is the leaf node in the tree.\n+ * @param fields    Store the children of the field. It's safe to use", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "aa094733a6678ec72712da928dd457e79f3f81a9"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTYzOTQwMg==", "bodyText": "the usage is ambiguity, what if the middle node ?", "url": "https://github.com/apache/flink/pull/13631#discussion_r515639402", "createdAt": "2020-11-01T16:04:01Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/RexNodeNestedField.scala", "diffHunk": "@@ -0,0 +1,295 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.utils\n+\n+import org.apache.flink.table.api.TableException\n+\n+import org.apache.calcite.rel.`type`.RelDataType\n+import org.apache.calcite.rex._\n+\n+import java.util\n+import java.util.{List => JList, Map => JMap}\n+\n+import scala.collection.JavaConversions._\n+import scala.collection.JavaConverters._\n+\n+/**\n+ * RexNodeNestedField is a tree node to build the used fields tree.\n+ *\n+ * @param name      The name of the fields in the origin schema\n+ * @param index     The index of the field in the origin schema. It\n+ *                  only works for the RowType.\n+ * @param fieldType The type of the field. It is useful when\n+ *                  rewriting the projections. It uses the LinkedHashMap\n+ *                  to keep the insert order. In some cases, it can\n+ *                  reduce the cost of the reorder of the fields in query.\n+ * @param useAll    Mark the field is the leaf node in the tree.\n+ * @param fields    Store the children of the field. It's safe to use\n+ *                  name as the index because name is unique in every\n+ *                  level.\n+ * @param order     For leaf node, the order is used to memorize the", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "aa094733a6678ec72712da928dd457e79f3f81a9"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTYzOTU4Mw==", "bodyText": "use LinkedHashMap ?", "url": "https://github.com/apache/flink/pull/13631#discussion_r515639583", "createdAt": "2020-11-01T16:05:25Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/RexNodeNestedField.scala", "diffHunk": "@@ -0,0 +1,295 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.utils\n+\n+import org.apache.flink.table.api.TableException\n+\n+import org.apache.calcite.rel.`type`.RelDataType\n+import org.apache.calcite.rex._\n+\n+import java.util\n+import java.util.{List => JList, Map => JMap}\n+\n+import scala.collection.JavaConversions._\n+import scala.collection.JavaConverters._\n+\n+/**\n+ * RexNodeNestedField is a tree node to build the used fields tree.\n+ *\n+ * @param name      The name of the fields in the origin schema\n+ * @param index     The index of the field in the origin schema. It\n+ *                  only works for the RowType.\n+ * @param fieldType The type of the field. It is useful when\n+ *                  rewriting the projections. It uses the LinkedHashMap\n+ *                  to keep the insert order. In some cases, it can\n+ *                  reduce the cost of the reorder of the fields in query.\n+ * @param useAll    Mark the field is the leaf node in the tree.\n+ * @param fields    Store the children of the field. It's safe to use\n+ *                  name as the index because name is unique in every\n+ *                  level.\n+ * @param order     For leaf node, the order is used to memorize the\n+ *                  location of the field in the new schema. For root\n+ *                  node, it is used to memorize the number of leaf node.\n+ */\n+class RexNodeNestedField(\n+    val name: String,\n+    val index: Int,\n+    val fieldType: RelDataType,\n+    var useAll: Boolean,\n+    var fields: JMap[String, RexNodeNestedField],", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "aa094733a6678ec72712da928dd457e79f3f81a9"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTYzOTYyOQ==", "bodyText": "addChild", "url": "https://github.com/apache/flink/pull/13631#discussion_r515639629", "createdAt": "2020-11-01T16:05:46Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/RexNodeNestedField.scala", "diffHunk": "@@ -0,0 +1,295 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.utils\n+\n+import org.apache.flink.table.api.TableException\n+\n+import org.apache.calcite.rel.`type`.RelDataType\n+import org.apache.calcite.rex._\n+\n+import java.util\n+import java.util.{List => JList, Map => JMap}\n+\n+import scala.collection.JavaConversions._\n+import scala.collection.JavaConverters._\n+\n+/**\n+ * RexNodeNestedField is a tree node to build the used fields tree.\n+ *\n+ * @param name      The name of the fields in the origin schema\n+ * @param index     The index of the field in the origin schema. It\n+ *                  only works for the RowType.\n+ * @param fieldType The type of the field. It is useful when\n+ *                  rewriting the projections. It uses the LinkedHashMap\n+ *                  to keep the insert order. In some cases, it can\n+ *                  reduce the cost of the reorder of the fields in query.\n+ * @param useAll    Mark the field is the leaf node in the tree.\n+ * @param fields    Store the children of the field. It's safe to use\n+ *                  name as the index because name is unique in every\n+ *                  level.\n+ * @param order     For leaf node, the order is used to memorize the\n+ *                  location of the field in the new schema. For root\n+ *                  node, it is used to memorize the number of leaf node.\n+ */\n+class RexNodeNestedField(\n+    val name: String,\n+    val index: Int,\n+    val fieldType: RelDataType,\n+    var useAll: Boolean,\n+    var fields: JMap[String, RexNodeNestedField],\n+    var order: Int) {\n+\n+  def addField(field: RexNodeNestedField): Unit = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "aa094733a6678ec72712da928dd457e79f3f81a9"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTYzOTY5Mg==", "bodyText": "deleteChild", "url": "https://github.com/apache/flink/pull/13631#discussion_r515639692", "createdAt": "2020-11-01T16:06:15Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/RexNodeNestedField.scala", "diffHunk": "@@ -0,0 +1,295 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.utils\n+\n+import org.apache.flink.table.api.TableException\n+\n+import org.apache.calcite.rel.`type`.RelDataType\n+import org.apache.calcite.rex._\n+\n+import java.util\n+import java.util.{List => JList, Map => JMap}\n+\n+import scala.collection.JavaConversions._\n+import scala.collection.JavaConverters._\n+\n+/**\n+ * RexNodeNestedField is a tree node to build the used fields tree.\n+ *\n+ * @param name      The name of the fields in the origin schema\n+ * @param index     The index of the field in the origin schema. It\n+ *                  only works for the RowType.\n+ * @param fieldType The type of the field. It is useful when\n+ *                  rewriting the projections. It uses the LinkedHashMap\n+ *                  to keep the insert order. In some cases, it can\n+ *                  reduce the cost of the reorder of the fields in query.\n+ * @param useAll    Mark the field is the leaf node in the tree.\n+ * @param fields    Store the children of the field. It's safe to use\n+ *                  name as the index because name is unique in every\n+ *                  level.\n+ * @param order     For leaf node, the order is used to memorize the\n+ *                  location of the field in the new schema. For root\n+ *                  node, it is used to memorize the number of leaf node.\n+ */\n+class RexNodeNestedField(\n+    val name: String,\n+    val index: Int,\n+    val fieldType: RelDataType,\n+    var useAll: Boolean,\n+    var fields: JMap[String, RexNodeNestedField],\n+    var order: Int) {\n+\n+  def addField(field: RexNodeNestedField): Unit = {\n+    useAll = false\n+    fields.put(field.name, field)\n+  }\n+\n+  def deleteField(fieldName: String): Option[RexNodeNestedField] = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "aa094733a6678ec72712da928dd457e79f3f81a9"}, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTYzOTc0MA==", "bodyText": "I think this dummy root field is not necessary, we can create a RexNodeNestedField for each RexNode, and return List< RexNodeNestedField> for this method. Or we can add a new class named RexNodeNestedFields to store all fields", "url": "https://github.com/apache/flink/pull/13631#discussion_r515639740", "createdAt": "2020-11-01T16:06:41Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/RexNodeNestedField.scala", "diffHunk": "@@ -0,0 +1,295 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.utils\n+\n+import org.apache.flink.table.api.TableException\n+\n+import org.apache.calcite.rel.`type`.RelDataType\n+import org.apache.calcite.rex._\n+\n+import java.util\n+import java.util.{List => JList, Map => JMap}\n+\n+import scala.collection.JavaConversions._\n+import scala.collection.JavaConverters._\n+\n+/**\n+ * RexNodeNestedField is a tree node to build the used fields tree.\n+ *\n+ * @param name      The name of the fields in the origin schema\n+ * @param index     The index of the field in the origin schema. It\n+ *                  only works for the RowType.\n+ * @param fieldType The type of the field. It is useful when\n+ *                  rewriting the projections. It uses the LinkedHashMap\n+ *                  to keep the insert order. In some cases, it can\n+ *                  reduce the cost of the reorder of the fields in query.\n+ * @param useAll    Mark the field is the leaf node in the tree.\n+ * @param fields    Store the children of the field. It's safe to use\n+ *                  name as the index because name is unique in every\n+ *                  level.\n+ * @param order     For leaf node, the order is used to memorize the\n+ *                  location of the field in the new schema. For root\n+ *                  node, it is used to memorize the number of leaf node.\n+ */\n+class RexNodeNestedField(\n+    val name: String,\n+    val index: Int,\n+    val fieldType: RelDataType,\n+    var useAll: Boolean,\n+    var fields: JMap[String, RexNodeNestedField],\n+    var order: Int) {\n+\n+  def addField(field: RexNodeNestedField): Unit = {\n+    useAll = false\n+    fields.put(field.name, field)\n+  }\n+\n+  def deleteField(fieldName: String): Option[RexNodeNestedField] = {\n+    if (fields.containsKey(fieldName)) {\n+      Some(fields.remove(fieldName))\n+    } else {\n+      Option.empty\n+    }\n+  }\n+}\n+\n+object RexNodeNestedField {\n+  def build(exprs: JList[RexNode], rowType: RelDataType):\n+      RexNodeNestedField = {\n+    // the order field in the root node is to memorize\n+    // the number of leaf\n+    val root = new RexNodeNestedField(\n+      \"root\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "aa094733a6678ec72712da928dd457e79f3f81a9"}, "originalPosition": 78}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTYzOTg0MQ==", "bodyText": "give an example to explain it more clear and add a Test for this class", "url": "https://github.com/apache/flink/pull/13631#discussion_r515639841", "createdAt": "2020-11-01T16:07:38Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/RexNodeNestedField.scala", "diffHunk": "@@ -0,0 +1,295 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.utils\n+\n+import org.apache.flink.table.api.TableException\n+\n+import org.apache.calcite.rel.`type`.RelDataType\n+import org.apache.calcite.rex._\n+\n+import java.util\n+import java.util.{List => JList, Map => JMap}\n+\n+import scala.collection.JavaConversions._\n+import scala.collection.JavaConverters._\n+\n+/**\n+ * RexNodeNestedField is a tree node to build the used fields tree.\n+ *\n+ * @param name      The name of the fields in the origin schema\n+ * @param index     The index of the field in the origin schema. It\n+ *                  only works for the RowType.\n+ * @param fieldType The type of the field. It is useful when\n+ *                  rewriting the projections. It uses the LinkedHashMap\n+ *                  to keep the insert order. In some cases, it can\n+ *                  reduce the cost of the reorder of the fields in query.\n+ * @param useAll    Mark the field is the leaf node in the tree.\n+ * @param fields    Store the children of the field. It's safe to use\n+ *                  name as the index because name is unique in every\n+ *                  level.\n+ * @param order     For leaf node, the order is used to memorize the\n+ *                  location of the field in the new schema. For root\n+ *                  node, it is used to memorize the number of leaf node.\n+ */\n+class RexNodeNestedField(\n+    val name: String,\n+    val index: Int,\n+    val fieldType: RelDataType,\n+    var useAll: Boolean,\n+    var fields: JMap[String, RexNodeNestedField],\n+    var order: Int) {\n+\n+  def addField(field: RexNodeNestedField): Unit = {\n+    useAll = false\n+    fields.put(field.name, field)\n+  }\n+\n+  def deleteField(fieldName: String): Option[RexNodeNestedField] = {\n+    if (fields.containsKey(fieldName)) {\n+      Some(fields.remove(fieldName))\n+    } else {\n+      Option.empty\n+    }\n+  }\n+}\n+\n+object RexNodeNestedField {\n+  def build(exprs: JList[RexNode], rowType: RelDataType):", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "aa094733a6678ec72712da928dd457e79f3f81a9"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTYzOTk2Mg==", "bodyText": "add a doc for each method to explain the purpose of the method", "url": "https://github.com/apache/flink/pull/13631#discussion_r515639962", "createdAt": "2020-11-01T16:08:54Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/RexNodeNestedField.scala", "diffHunk": "@@ -0,0 +1,295 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.utils\n+\n+import org.apache.flink.table.api.TableException\n+\n+import org.apache.calcite.rel.`type`.RelDataType\n+import org.apache.calcite.rex._\n+\n+import java.util\n+import java.util.{List => JList, Map => JMap}\n+\n+import scala.collection.JavaConversions._\n+import scala.collection.JavaConverters._\n+\n+/**\n+ * RexNodeNestedField is a tree node to build the used fields tree.\n+ *\n+ * @param name      The name of the fields in the origin schema\n+ * @param index     The index of the field in the origin schema. It\n+ *                  only works for the RowType.\n+ * @param fieldType The type of the field. It is useful when\n+ *                  rewriting the projections. It uses the LinkedHashMap\n+ *                  to keep the insert order. In some cases, it can\n+ *                  reduce the cost of the reorder of the fields in query.\n+ * @param useAll    Mark the field is the leaf node in the tree.\n+ * @param fields    Store the children of the field. It's safe to use\n+ *                  name as the index because name is unique in every\n+ *                  level.\n+ * @param order     For leaf node, the order is used to memorize the\n+ *                  location of the field in the new schema. For root\n+ *                  node, it is used to memorize the number of leaf node.\n+ */\n+class RexNodeNestedField(\n+    val name: String,\n+    val index: Int,\n+    val fieldType: RelDataType,\n+    var useAll: Boolean,\n+    var fields: JMap[String, RexNodeNestedField],\n+    var order: Int) {\n+\n+  def addField(field: RexNodeNestedField): Unit = {\n+    useAll = false\n+    fields.put(field.name, field)\n+  }\n+\n+  def deleteField(fieldName: String): Option[RexNodeNestedField] = {\n+    if (fields.containsKey(fieldName)) {\n+      Some(fields.remove(fieldName))\n+    } else {\n+      Option.empty\n+    }\n+  }\n+}\n+\n+object RexNodeNestedField {\n+  def build(exprs: JList[RexNode], rowType: RelDataType):\n+      RexNodeNestedField = {\n+    // the order field in the root node is to memorize\n+    // the number of leaf\n+    val root = new RexNodeNestedField(\n+      \"root\",\n+      0,\n+      rowType,\n+      false,\n+      new util.LinkedHashMap[String, RexNodeNestedField](),\n+      0)\n+    val visitor = new NestedFieldExtractor(root, rowType)\n+    for(expr <- exprs) {\n+      expr.accept(visitor)\n+    }\n+    root\n+  }\n+\n+  def rewrite(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "aa094733a6678ec72712da928dd457e79f3f81a9"}, "originalPosition": 91}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY1MDY0MA==", "bodyText": "change var to val", "url": "https://github.com/apache/flink/pull/13631#discussion_r515650640", "createdAt": "2020-11-01T17:45:35Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/RexNodeNestedField.scala", "diffHunk": "@@ -0,0 +1,295 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.utils\n+\n+import org.apache.flink.table.api.TableException\n+\n+import org.apache.calcite.rel.`type`.RelDataType\n+import org.apache.calcite.rex._\n+\n+import java.util\n+import java.util.{List => JList, Map => JMap}\n+\n+import scala.collection.JavaConversions._\n+import scala.collection.JavaConverters._\n+\n+/**\n+ * RexNodeNestedField is a tree node to build the used fields tree.\n+ *\n+ * @param name      The name of the fields in the origin schema\n+ * @param index     The index of the field in the origin schema. It\n+ *                  only works for the RowType.\n+ * @param fieldType The type of the field. It is useful when\n+ *                  rewriting the projections. It uses the LinkedHashMap\n+ *                  to keep the insert order. In some cases, it can\n+ *                  reduce the cost of the reorder of the fields in query.\n+ * @param useAll    Mark the field is the leaf node in the tree.\n+ * @param fields    Store the children of the field. It's safe to use\n+ *                  name as the index because name is unique in every\n+ *                  level.\n+ * @param order     For leaf node, the order is used to memorize the\n+ *                  location of the field in the new schema. For root\n+ *                  node, it is used to memorize the number of leaf node.\n+ */\n+class RexNodeNestedField(\n+    val name: String,\n+    val index: Int,\n+    val fieldType: RelDataType,\n+    var useAll: Boolean,\n+    var fields: JMap[String, RexNodeNestedField],\n+    var order: Int) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "aa094733a6678ec72712da928dd457e79f3f81a9"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY1MjQ3OQ==", "bodyText": "extract this to a method?", "url": "https://github.com/apache/flink/pull/13631#discussion_r515652479", "createdAt": "2020-11-01T18:03:59Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/RexNodeNestedField.scala", "diffHunk": "@@ -0,0 +1,295 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.utils\n+\n+import org.apache.flink.table.api.TableException\n+\n+import org.apache.calcite.rel.`type`.RelDataType\n+import org.apache.calcite.rex._\n+\n+import java.util\n+import java.util.{List => JList, Map => JMap}\n+\n+import scala.collection.JavaConversions._\n+import scala.collection.JavaConverters._\n+\n+/**\n+ * RexNodeNestedField is a tree node to build the used fields tree.\n+ *\n+ * @param name      The name of the fields in the origin schema\n+ * @param index     The index of the field in the origin schema. It\n+ *                  only works for the RowType.\n+ * @param fieldType The type of the field. It is useful when\n+ *                  rewriting the projections. It uses the LinkedHashMap\n+ *                  to keep the insert order. In some cases, it can\n+ *                  reduce the cost of the reorder of the fields in query.\n+ * @param useAll    Mark the field is the leaf node in the tree.\n+ * @param fields    Store the children of the field. It's safe to use\n+ *                  name as the index because name is unique in every\n+ *                  level.\n+ * @param order     For leaf node, the order is used to memorize the\n+ *                  location of the field in the new schema. For root\n+ *                  node, it is used to memorize the number of leaf node.\n+ */\n+class RexNodeNestedField(\n+    val name: String,\n+    val index: Int,\n+    val fieldType: RelDataType,\n+    var useAll: Boolean,\n+    var fields: JMap[String, RexNodeNestedField],\n+    var order: Int) {\n+\n+  def addField(field: RexNodeNestedField): Unit = {\n+    useAll = false\n+    fields.put(field.name, field)\n+  }\n+\n+  def deleteField(fieldName: String): Option[RexNodeNestedField] = {\n+    if (fields.containsKey(fieldName)) {\n+      Some(fields.remove(fieldName))\n+    } else {\n+      Option.empty\n+    }\n+  }\n+}\n+\n+object RexNodeNestedField {\n+  def build(exprs: JList[RexNode], rowType: RelDataType):\n+      RexNodeNestedField = {\n+    // the order field in the root node is to memorize\n+    // the number of leaf\n+    val root = new RexNodeNestedField(\n+      \"root\",\n+      0,\n+      rowType,\n+      false,\n+      new util.LinkedHashMap[String, RexNodeNestedField](),\n+      0)\n+    val visitor = new NestedFieldExtractor(root, rowType)\n+    for(expr <- exprs) {\n+      expr.accept(visitor)\n+    }\n+    root\n+  }\n+\n+  def rewrite(\n+      exprs: JList[RexNode],\n+      root: RexNodeNestedField,\n+      builder: RexBuilder): JList[RexNode] = {\n+    val writer = new NestedFieldReWriter(root, builder)\n+    exprs.map(_.accept(writer)).toList.asJava\n+  }\n+\n+  def labelAndConvert(root: RexNodeNestedField): Array[Array[Int]] = {\n+    val allPaths = new util.LinkedList[Array[Int]]()\n+    traverse(root, root, new util.LinkedList[Int](), allPaths)\n+    allPaths.toArray(new Array[Array[Int]](0))\n+  }\n+\n+  private def traverse(\n+     parent: RexNodeNestedField,\n+     root: RexNodeNestedField,\n+     path: JList[Int],\n+     allPaths: JList[Array[Int]]): Unit ={\n+    val tail = path.size()\n+    // push self\n+    path.add(parent.index)\n+    if (parent.useAll) {\n+      // leaf node\n+      parent.order = root.order\n+      root.order = root.order + 1\n+      // ignore root node\n+      allPaths.add(path.slice(1, tail + 1).toArray)\n+    } else {\n+      // iterate children\n+      for (child <- parent.fields.values()) {\n+        traverse(child, root, path, allPaths)\n+      }\n+    }\n+    // pop self\n+    path.remove(tail)\n+  }\n+}\n+\n+/**\n+ * A RexShuttle to rewrite field accesses of RexNode with nested projection.\n+ * For `RexInputRef`, it uses the old input ref name to find the new input fields ref\n+ * and use the order to generate the new input ref.\n+ * For `RexFieldAccess`, it will traverse to the top level of the field access and\n+ * then to generate new RexNode. There are 3 situations we need to consider:\n+ *  1. if top level field is marked to use all sub-fields , make field access of the reference\n+ *  and warp the ref as RexFieldAccess with the sub field name;\n+ *  2. if top level field isn't marked to use all sub-fields and its direct field\n+ *  is marked as useall, make field reference of the direct subfield;\n+ *  3. if neither situation above happens, return from the recursion with the updated parent.\n+ * When the process is back from the recursion, it still has 2 situations need to\n+ * consider:\n+ *  1. if the process has found the reference of the upper level, just make an access on the\n+ *  reference founded before;\n+ *  2. if the process hasn't found the first reference, the process continues to search under\n+ *  the current parent.\n+ */\n+class NestedFieldReWriter(\n+    root: RexNodeNestedField,\n+    builder: RexBuilder) extends RexShuttle {\n+  override def visitInputRef(inputRef: RexInputRef): RexNode = {\n+    if (!root.fields.containsKey(inputRef.getName)) {\n+      throw new TableException(\n+        \"Illegal input field access\" + inputRef.getName)\n+    } else {\n+      val field = root.fields.get(inputRef.getName)\n+      new RexInputRef(field.order, field.fieldType)\n+    }\n+  }\n+\n+  override def visitFieldAccess(fieldAccess: RexFieldAccess): RexNode = {\n+    val (node, _) = traverse(fieldAccess)\n+    if (node.isDefined) {\n+      node.get\n+    } else {\n+      throw new TableException(\n+        \"Unknown field \" + fieldAccess + \" when rewrite projection \")\n+    }\n+  }\n+\n+  private def traverse(\n+      fieldAccess: RexFieldAccess): (Option[RexNode], RexNodeNestedField) = {\n+    fieldAccess.getReferenceExpr match {\n+      case ref: RexInputRef =>\n+        val parent = root.fields.get(ref.getName)\n+        if (parent.useAll) {\n+          (\n+            Some(builder.makeFieldAccess(\n+              new RexInputRef(parent.order, parent.fieldType),\n+              fieldAccess.getField.getName,\n+              true)),\n+            root)\n+        } else {\n+          val child = parent.fields.get(fieldAccess.getField.getName)\n+          if (child.useAll) {\n+            (Some(new RexInputRef(child.order, child.fieldType)), root)\n+          } else {\n+            (Option.empty, child)\n+          }\n+        }\n+      case acc: RexFieldAccess =>\n+        val (field, parent) = traverse(acc)\n+        if (field.isDefined) {\n+          (\n+            Some(\n+              builder.makeFieldAccess(\n+                field.get,\n+                fieldAccess.getField.getName,\n+                true)),\n+            parent)\n+        } else {\n+          val child = parent.fields.get(fieldAccess.getField.getName)\n+          if (child.useAll) {\n+            (Some(new RexInputRef(child.order, child.fieldType)), child)\n+          } else {\n+            (Option.empty, child)\n+          }\n+        }\n+    }\n+  }\n+}\n+\n+/**\n+ * An RexVisitor to extract all referenced input fields\n+ */\n+class NestedFieldExtractor(val root: RexNodeNestedField, val rowType: RelDataType)\n+  extends RexVisitorImpl[Unit](true) {\n+\n+  override def visitFieldAccess(fieldAccess: RexFieldAccess): Unit = {\n+    def internalVisit(fieldAccess: RexFieldAccess): (Int, List[String]) = {\n+      fieldAccess.getReferenceExpr match {\n+        case ref: RexInputRef =>\n+          (ref.getIndex, List(ref.getName, fieldAccess.getField.getName))\n+        case fac: RexFieldAccess =>\n+          val (i, n) = internalVisit(fac)\n+          (i, n :+ fieldAccess.getField.getName)\n+      }\n+    }\n+\n+    // extract the info\n+    val (index, names) = internalVisit(fieldAccess)\n+    if (!root.fields.containsKey(names.get(0))) {\n+      root.fields.put(\n+        names.get(0),\n+        new RexNodeNestedField(\n+          names.get(0),\n+          index,\n+          rowType.getFieldList.get(index).getType,\n+          false,\n+          new util.HashMap[String, RexNodeNestedField](),\n+          -1))\n+    }\n+    val (leaf, _) = names.foldLeft(Tuple2(root, rowType)) {\n+      case((parent, fieldType), name) =>\n+        if (parent.useAll) {\n+          return\n+        }\n+        if(!parent.fields.containsKey(name)) {\n+          val index = fieldType.getFieldNames.indexOf(name)\n+          if (index < 0) {\n+            throw new TableException(\"Illegal type\")\n+          }\n+          parent.fields.put(\n+            name,\n+            new RexNodeNestedField(\n+              name,\n+              index,\n+              fieldType.getFieldList.get(index).getType,\n+              false,\n+              new util.LinkedHashMap[String, RexNodeNestedField](),\n+              -1\n+            )\n+          )", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "aa094733a6678ec72712da928dd457e79f3f81a9"}, "originalPosition": 263}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY1MjczMw==", "bodyText": "we need not to override this method", "url": "https://github.com/apache/flink/pull/13631#discussion_r515652733", "createdAt": "2020-11-01T18:06:09Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/RexNodeNestedField.scala", "diffHunk": "@@ -0,0 +1,295 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.utils\n+\n+import org.apache.flink.table.api.TableException\n+\n+import org.apache.calcite.rel.`type`.RelDataType\n+import org.apache.calcite.rex._\n+\n+import java.util\n+import java.util.{List => JList, Map => JMap}\n+\n+import scala.collection.JavaConversions._\n+import scala.collection.JavaConverters._\n+\n+/**\n+ * RexNodeNestedField is a tree node to build the used fields tree.\n+ *\n+ * @param name      The name of the fields in the origin schema\n+ * @param index     The index of the field in the origin schema. It\n+ *                  only works for the RowType.\n+ * @param fieldType The type of the field. It is useful when\n+ *                  rewriting the projections. It uses the LinkedHashMap\n+ *                  to keep the insert order. In some cases, it can\n+ *                  reduce the cost of the reorder of the fields in query.\n+ * @param useAll    Mark the field is the leaf node in the tree.\n+ * @param fields    Store the children of the field. It's safe to use\n+ *                  name as the index because name is unique in every\n+ *                  level.\n+ * @param order     For leaf node, the order is used to memorize the\n+ *                  location of the field in the new schema. For root\n+ *                  node, it is used to memorize the number of leaf node.\n+ */\n+class RexNodeNestedField(\n+    val name: String,\n+    val index: Int,\n+    val fieldType: RelDataType,\n+    var useAll: Boolean,\n+    var fields: JMap[String, RexNodeNestedField],\n+    var order: Int) {\n+\n+  def addField(field: RexNodeNestedField): Unit = {\n+    useAll = false\n+    fields.put(field.name, field)\n+  }\n+\n+  def deleteField(fieldName: String): Option[RexNodeNestedField] = {\n+    if (fields.containsKey(fieldName)) {\n+      Some(fields.remove(fieldName))\n+    } else {\n+      Option.empty\n+    }\n+  }\n+}\n+\n+object RexNodeNestedField {\n+  def build(exprs: JList[RexNode], rowType: RelDataType):\n+      RexNodeNestedField = {\n+    // the order field in the root node is to memorize\n+    // the number of leaf\n+    val root = new RexNodeNestedField(\n+      \"root\",\n+      0,\n+      rowType,\n+      false,\n+      new util.LinkedHashMap[String, RexNodeNestedField](),\n+      0)\n+    val visitor = new NestedFieldExtractor(root, rowType)\n+    for(expr <- exprs) {\n+      expr.accept(visitor)\n+    }\n+    root\n+  }\n+\n+  def rewrite(\n+      exprs: JList[RexNode],\n+      root: RexNodeNestedField,\n+      builder: RexBuilder): JList[RexNode] = {\n+    val writer = new NestedFieldReWriter(root, builder)\n+    exprs.map(_.accept(writer)).toList.asJava\n+  }\n+\n+  def labelAndConvert(root: RexNodeNestedField): Array[Array[Int]] = {\n+    val allPaths = new util.LinkedList[Array[Int]]()\n+    traverse(root, root, new util.LinkedList[Int](), allPaths)\n+    allPaths.toArray(new Array[Array[Int]](0))\n+  }\n+\n+  private def traverse(\n+     parent: RexNodeNestedField,\n+     root: RexNodeNestedField,\n+     path: JList[Int],\n+     allPaths: JList[Array[Int]]): Unit ={\n+    val tail = path.size()\n+    // push self\n+    path.add(parent.index)\n+    if (parent.useAll) {\n+      // leaf node\n+      parent.order = root.order\n+      root.order = root.order + 1\n+      // ignore root node\n+      allPaths.add(path.slice(1, tail + 1).toArray)\n+    } else {\n+      // iterate children\n+      for (child <- parent.fields.values()) {\n+        traverse(child, root, path, allPaths)\n+      }\n+    }\n+    // pop self\n+    path.remove(tail)\n+  }\n+}\n+\n+/**\n+ * A RexShuttle to rewrite field accesses of RexNode with nested projection.\n+ * For `RexInputRef`, it uses the old input ref name to find the new input fields ref\n+ * and use the order to generate the new input ref.\n+ * For `RexFieldAccess`, it will traverse to the top level of the field access and\n+ * then to generate new RexNode. There are 3 situations we need to consider:\n+ *  1. if top level field is marked to use all sub-fields , make field access of the reference\n+ *  and warp the ref as RexFieldAccess with the sub field name;\n+ *  2. if top level field isn't marked to use all sub-fields and its direct field\n+ *  is marked as useall, make field reference of the direct subfield;\n+ *  3. if neither situation above happens, return from the recursion with the updated parent.\n+ * When the process is back from the recursion, it still has 2 situations need to\n+ * consider:\n+ *  1. if the process has found the reference of the upper level, just make an access on the\n+ *  reference founded before;\n+ *  2. if the process hasn't found the first reference, the process continues to search under\n+ *  the current parent.\n+ */\n+class NestedFieldReWriter(\n+    root: RexNodeNestedField,\n+    builder: RexBuilder) extends RexShuttle {\n+  override def visitInputRef(inputRef: RexInputRef): RexNode = {\n+    if (!root.fields.containsKey(inputRef.getName)) {\n+      throw new TableException(\n+        \"Illegal input field access\" + inputRef.getName)\n+    } else {\n+      val field = root.fields.get(inputRef.getName)\n+      new RexInputRef(field.order, field.fieldType)\n+    }\n+  }\n+\n+  override def visitFieldAccess(fieldAccess: RexFieldAccess): RexNode = {\n+    val (node, _) = traverse(fieldAccess)\n+    if (node.isDefined) {\n+      node.get\n+    } else {\n+      throw new TableException(\n+        \"Unknown field \" + fieldAccess + \" when rewrite projection \")\n+    }\n+  }\n+\n+  private def traverse(\n+      fieldAccess: RexFieldAccess): (Option[RexNode], RexNodeNestedField) = {\n+    fieldAccess.getReferenceExpr match {\n+      case ref: RexInputRef =>\n+        val parent = root.fields.get(ref.getName)\n+        if (parent.useAll) {\n+          (\n+            Some(builder.makeFieldAccess(\n+              new RexInputRef(parent.order, parent.fieldType),\n+              fieldAccess.getField.getName,\n+              true)),\n+            root)\n+        } else {\n+          val child = parent.fields.get(fieldAccess.getField.getName)\n+          if (child.useAll) {\n+            (Some(new RexInputRef(child.order, child.fieldType)), root)\n+          } else {\n+            (Option.empty, child)\n+          }\n+        }\n+      case acc: RexFieldAccess =>\n+        val (field, parent) = traverse(acc)\n+        if (field.isDefined) {\n+          (\n+            Some(\n+              builder.makeFieldAccess(\n+                field.get,\n+                fieldAccess.getField.getName,\n+                true)),\n+            parent)\n+        } else {\n+          val child = parent.fields.get(fieldAccess.getField.getName)\n+          if (child.useAll) {\n+            (Some(new RexInputRef(child.order, child.fieldType)), child)\n+          } else {\n+            (Option.empty, child)\n+          }\n+        }\n+    }\n+  }\n+}\n+\n+/**\n+ * An RexVisitor to extract all referenced input fields\n+ */\n+class NestedFieldExtractor(val root: RexNodeNestedField, val rowType: RelDataType)\n+  extends RexVisitorImpl[Unit](true) {\n+\n+  override def visitFieldAccess(fieldAccess: RexFieldAccess): Unit = {\n+    def internalVisit(fieldAccess: RexFieldAccess): (Int, List[String]) = {\n+      fieldAccess.getReferenceExpr match {\n+        case ref: RexInputRef =>\n+          (ref.getIndex, List(ref.getName, fieldAccess.getField.getName))\n+        case fac: RexFieldAccess =>\n+          val (i, n) = internalVisit(fac)\n+          (i, n :+ fieldAccess.getField.getName)\n+      }\n+    }\n+\n+    // extract the info\n+    val (index, names) = internalVisit(fieldAccess)\n+    if (!root.fields.containsKey(names.get(0))) {\n+      root.fields.put(\n+        names.get(0),\n+        new RexNodeNestedField(\n+          names.get(0),\n+          index,\n+          rowType.getFieldList.get(index).getType,\n+          false,\n+          new util.HashMap[String, RexNodeNestedField](),\n+          -1))\n+    }\n+    val (leaf, _) = names.foldLeft(Tuple2(root, rowType)) {\n+      case((parent, fieldType), name) =>\n+        if (parent.useAll) {\n+          return\n+        }\n+        if(!parent.fields.containsKey(name)) {\n+          val index = fieldType.getFieldNames.indexOf(name)\n+          if (index < 0) {\n+            throw new TableException(\"Illegal type\")\n+          }\n+          parent.fields.put(\n+            name,\n+            new RexNodeNestedField(\n+              name,\n+              index,\n+              fieldType.getFieldList.get(index).getType,\n+              false,\n+              new util.LinkedHashMap[String, RexNodeNestedField](),\n+              -1\n+            )\n+          )\n+        }\n+        val son = parent.fields.get(name)\n+        (son,\n+          fieldType.getFieldList.get(son.index).getType)\n+    }\n+    leaf.useAll = true\n+    leaf.fields = util.Collections.emptyMap()\n+  }\n+\n+  override def visitInputRef(inputRef: RexInputRef): Unit = {\n+    val name = inputRef.getName\n+    if (root.fields.containsKey(name)) {\n+      // mark the node as top level node\n+      val child = root.fields.get(name)\n+      child.fields = util.Collections.emptyMap()\n+      child.useAll = true\n+    } else {\n+      val index = inputRef.getIndex\n+      root.fields.put(\n+        name, new RexNodeNestedField(\n+          name,\n+          index,\n+          rowType.getFieldList.get(index).getType,\n+          true,\n+          util.Collections.emptyMap(),\n+          -1))\n+    }\n+  }\n+\n+  override def visitCall(call: RexCall): Unit =\n+    call.operands.foreach(operand => operand.accept(this))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "aa094733a6678ec72712da928dd457e79f3f81a9"}, "originalPosition": 294}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY1MjkzOA==", "bodyText": "if build RexNodeNestedField tree from leaf to root, root is not necessary, and all attributes of RexNodeNestedField is val\nbtw, mark NestedFieldExtractor as private", "url": "https://github.com/apache/flink/pull/13631#discussion_r515652938", "createdAt": "2020-11-01T18:08:31Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/RexNodeNestedField.scala", "diffHunk": "@@ -0,0 +1,295 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.utils\n+\n+import org.apache.flink.table.api.TableException\n+\n+import org.apache.calcite.rel.`type`.RelDataType\n+import org.apache.calcite.rex._\n+\n+import java.util\n+import java.util.{List => JList, Map => JMap}\n+\n+import scala.collection.JavaConversions._\n+import scala.collection.JavaConverters._\n+\n+/**\n+ * RexNodeNestedField is a tree node to build the used fields tree.\n+ *\n+ * @param name      The name of the fields in the origin schema\n+ * @param index     The index of the field in the origin schema. It\n+ *                  only works for the RowType.\n+ * @param fieldType The type of the field. It is useful when\n+ *                  rewriting the projections. It uses the LinkedHashMap\n+ *                  to keep the insert order. In some cases, it can\n+ *                  reduce the cost of the reorder of the fields in query.\n+ * @param useAll    Mark the field is the leaf node in the tree.\n+ * @param fields    Store the children of the field. It's safe to use\n+ *                  name as the index because name is unique in every\n+ *                  level.\n+ * @param order     For leaf node, the order is used to memorize the\n+ *                  location of the field in the new schema. For root\n+ *                  node, it is used to memorize the number of leaf node.\n+ */\n+class RexNodeNestedField(\n+    val name: String,\n+    val index: Int,\n+    val fieldType: RelDataType,\n+    var useAll: Boolean,\n+    var fields: JMap[String, RexNodeNestedField],\n+    var order: Int) {\n+\n+  def addField(field: RexNodeNestedField): Unit = {\n+    useAll = false\n+    fields.put(field.name, field)\n+  }\n+\n+  def deleteField(fieldName: String): Option[RexNodeNestedField] = {\n+    if (fields.containsKey(fieldName)) {\n+      Some(fields.remove(fieldName))\n+    } else {\n+      Option.empty\n+    }\n+  }\n+}\n+\n+object RexNodeNestedField {\n+  def build(exprs: JList[RexNode], rowType: RelDataType):\n+      RexNodeNestedField = {\n+    // the order field in the root node is to memorize\n+    // the number of leaf\n+    val root = new RexNodeNestedField(\n+      \"root\",\n+      0,\n+      rowType,\n+      false,\n+      new util.LinkedHashMap[String, RexNodeNestedField](),\n+      0)\n+    val visitor = new NestedFieldExtractor(root, rowType)\n+    for(expr <- exprs) {\n+      expr.accept(visitor)\n+    }\n+    root\n+  }\n+\n+  def rewrite(\n+      exprs: JList[RexNode],\n+      root: RexNodeNestedField,\n+      builder: RexBuilder): JList[RexNode] = {\n+    val writer = new NestedFieldReWriter(root, builder)\n+    exprs.map(_.accept(writer)).toList.asJava\n+  }\n+\n+  def labelAndConvert(root: RexNodeNestedField): Array[Array[Int]] = {\n+    val allPaths = new util.LinkedList[Array[Int]]()\n+    traverse(root, root, new util.LinkedList[Int](), allPaths)\n+    allPaths.toArray(new Array[Array[Int]](0))\n+  }\n+\n+  private def traverse(\n+     parent: RexNodeNestedField,\n+     root: RexNodeNestedField,\n+     path: JList[Int],\n+     allPaths: JList[Array[Int]]): Unit ={\n+    val tail = path.size()\n+    // push self\n+    path.add(parent.index)\n+    if (parent.useAll) {\n+      // leaf node\n+      parent.order = root.order\n+      root.order = root.order + 1\n+      // ignore root node\n+      allPaths.add(path.slice(1, tail + 1).toArray)\n+    } else {\n+      // iterate children\n+      for (child <- parent.fields.values()) {\n+        traverse(child, root, path, allPaths)\n+      }\n+    }\n+    // pop self\n+    path.remove(tail)\n+  }\n+}\n+\n+/**\n+ * A RexShuttle to rewrite field accesses of RexNode with nested projection.\n+ * For `RexInputRef`, it uses the old input ref name to find the new input fields ref\n+ * and use the order to generate the new input ref.\n+ * For `RexFieldAccess`, it will traverse to the top level of the field access and\n+ * then to generate new RexNode. There are 3 situations we need to consider:\n+ *  1. if top level field is marked to use all sub-fields , make field access of the reference\n+ *  and warp the ref as RexFieldAccess with the sub field name;\n+ *  2. if top level field isn't marked to use all sub-fields and its direct field\n+ *  is marked as useall, make field reference of the direct subfield;\n+ *  3. if neither situation above happens, return from the recursion with the updated parent.\n+ * When the process is back from the recursion, it still has 2 situations need to\n+ * consider:\n+ *  1. if the process has found the reference of the upper level, just make an access on the\n+ *  reference founded before;\n+ *  2. if the process hasn't found the first reference, the process continues to search under\n+ *  the current parent.\n+ */\n+class NestedFieldReWriter(\n+    root: RexNodeNestedField,\n+    builder: RexBuilder) extends RexShuttle {\n+  override def visitInputRef(inputRef: RexInputRef): RexNode = {\n+    if (!root.fields.containsKey(inputRef.getName)) {\n+      throw new TableException(\n+        \"Illegal input field access\" + inputRef.getName)\n+    } else {\n+      val field = root.fields.get(inputRef.getName)\n+      new RexInputRef(field.order, field.fieldType)\n+    }\n+  }\n+\n+  override def visitFieldAccess(fieldAccess: RexFieldAccess): RexNode = {\n+    val (node, _) = traverse(fieldAccess)\n+    if (node.isDefined) {\n+      node.get\n+    } else {\n+      throw new TableException(\n+        \"Unknown field \" + fieldAccess + \" when rewrite projection \")\n+    }\n+  }\n+\n+  private def traverse(\n+      fieldAccess: RexFieldAccess): (Option[RexNode], RexNodeNestedField) = {\n+    fieldAccess.getReferenceExpr match {\n+      case ref: RexInputRef =>\n+        val parent = root.fields.get(ref.getName)\n+        if (parent.useAll) {\n+          (\n+            Some(builder.makeFieldAccess(\n+              new RexInputRef(parent.order, parent.fieldType),\n+              fieldAccess.getField.getName,\n+              true)),\n+            root)\n+        } else {\n+          val child = parent.fields.get(fieldAccess.getField.getName)\n+          if (child.useAll) {\n+            (Some(new RexInputRef(child.order, child.fieldType)), root)\n+          } else {\n+            (Option.empty, child)\n+          }\n+        }\n+      case acc: RexFieldAccess =>\n+        val (field, parent) = traverse(acc)\n+        if (field.isDefined) {\n+          (\n+            Some(\n+              builder.makeFieldAccess(\n+                field.get,\n+                fieldAccess.getField.getName,\n+                true)),\n+            parent)\n+        } else {\n+          val child = parent.fields.get(fieldAccess.getField.getName)\n+          if (child.useAll) {\n+            (Some(new RexInputRef(child.order, child.fieldType)), child)\n+          } else {\n+            (Option.empty, child)\n+          }\n+        }\n+    }\n+  }\n+}\n+\n+/**\n+ * An RexVisitor to extract all referenced input fields\n+ */\n+class NestedFieldExtractor(val root: RexNodeNestedField, val rowType: RelDataType)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "aa094733a6678ec72712da928dd457e79f3f81a9"}, "originalPosition": 216}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY1MzU5MA==", "bodyText": "mark this as private", "url": "https://github.com/apache/flink/pull/13631#discussion_r515653590", "createdAt": "2020-11-01T18:14:43Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/RexNodeNestedField.scala", "diffHunk": "@@ -0,0 +1,295 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.utils\n+\n+import org.apache.flink.table.api.TableException\n+\n+import org.apache.calcite.rel.`type`.RelDataType\n+import org.apache.calcite.rex._\n+\n+import java.util\n+import java.util.{List => JList, Map => JMap}\n+\n+import scala.collection.JavaConversions._\n+import scala.collection.JavaConverters._\n+\n+/**\n+ * RexNodeNestedField is a tree node to build the used fields tree.\n+ *\n+ * @param name      The name of the fields in the origin schema\n+ * @param index     The index of the field in the origin schema. It\n+ *                  only works for the RowType.\n+ * @param fieldType The type of the field. It is useful when\n+ *                  rewriting the projections. It uses the LinkedHashMap\n+ *                  to keep the insert order. In some cases, it can\n+ *                  reduce the cost of the reorder of the fields in query.\n+ * @param useAll    Mark the field is the leaf node in the tree.\n+ * @param fields    Store the children of the field. It's safe to use\n+ *                  name as the index because name is unique in every\n+ *                  level.\n+ * @param order     For leaf node, the order is used to memorize the\n+ *                  location of the field in the new schema. For root\n+ *                  node, it is used to memorize the number of leaf node.\n+ */\n+class RexNodeNestedField(\n+    val name: String,\n+    val index: Int,\n+    val fieldType: RelDataType,\n+    var useAll: Boolean,\n+    var fields: JMap[String, RexNodeNestedField],\n+    var order: Int) {\n+\n+  def addField(field: RexNodeNestedField): Unit = {\n+    useAll = false\n+    fields.put(field.name, field)\n+  }\n+\n+  def deleteField(fieldName: String): Option[RexNodeNestedField] = {\n+    if (fields.containsKey(fieldName)) {\n+      Some(fields.remove(fieldName))\n+    } else {\n+      Option.empty\n+    }\n+  }\n+}\n+\n+object RexNodeNestedField {\n+  def build(exprs: JList[RexNode], rowType: RelDataType):\n+      RexNodeNestedField = {\n+    // the order field in the root node is to memorize\n+    // the number of leaf\n+    val root = new RexNodeNestedField(\n+      \"root\",\n+      0,\n+      rowType,\n+      false,\n+      new util.LinkedHashMap[String, RexNodeNestedField](),\n+      0)\n+    val visitor = new NestedFieldExtractor(root, rowType)\n+    for(expr <- exprs) {\n+      expr.accept(visitor)\n+    }\n+    root\n+  }\n+\n+  def rewrite(\n+      exprs: JList[RexNode],\n+      root: RexNodeNestedField,\n+      builder: RexBuilder): JList[RexNode] = {\n+    val writer = new NestedFieldReWriter(root, builder)\n+    exprs.map(_.accept(writer)).toList.asJava\n+  }\n+\n+  def labelAndConvert(root: RexNodeNestedField): Array[Array[Int]] = {\n+    val allPaths = new util.LinkedList[Array[Int]]()\n+    traverse(root, root, new util.LinkedList[Int](), allPaths)\n+    allPaths.toArray(new Array[Array[Int]](0))\n+  }\n+\n+  private def traverse(\n+     parent: RexNodeNestedField,\n+     root: RexNodeNestedField,\n+     path: JList[Int],\n+     allPaths: JList[Array[Int]]): Unit ={\n+    val tail = path.size()\n+    // push self\n+    path.add(parent.index)\n+    if (parent.useAll) {\n+      // leaf node\n+      parent.order = root.order\n+      root.order = root.order + 1\n+      // ignore root node\n+      allPaths.add(path.slice(1, tail + 1).toArray)\n+    } else {\n+      // iterate children\n+      for (child <- parent.fields.values()) {\n+        traverse(child, root, path, allPaths)\n+      }\n+    }\n+    // pop self\n+    path.remove(tail)\n+  }\n+}\n+\n+/**\n+ * A RexShuttle to rewrite field accesses of RexNode with nested projection.\n+ * For `RexInputRef`, it uses the old input ref name to find the new input fields ref\n+ * and use the order to generate the new input ref.\n+ * For `RexFieldAccess`, it will traverse to the top level of the field access and\n+ * then to generate new RexNode. There are 3 situations we need to consider:\n+ *  1. if top level field is marked to use all sub-fields , make field access of the reference\n+ *  and warp the ref as RexFieldAccess with the sub field name;\n+ *  2. if top level field isn't marked to use all sub-fields and its direct field\n+ *  is marked as useall, make field reference of the direct subfield;\n+ *  3. if neither situation above happens, return from the recursion with the updated parent.\n+ * When the process is back from the recursion, it still has 2 situations need to\n+ * consider:\n+ *  1. if the process has found the reference of the upper level, just make an access on the\n+ *  reference founded before;\n+ *  2. if the process hasn't found the first reference, the process continues to search under\n+ *  the current parent.\n+ */\n+class NestedFieldReWriter(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "aa094733a6678ec72712da928dd457e79f3f81a9"}, "originalPosition": 148}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY1NjM4Mg==", "bodyText": "which plans are affected?", "url": "https://github.com/apache/flink/pull/13631#discussion_r515656382", "createdAt": "2020-11-01T18:40:30Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRule.java", "diffHunk": "@@ -74,108 +76,114 @@ public boolean matches(RelOptRuleCall call) {\n \t\tif (tableSourceTable == null || !(tableSourceTable.tableSource() instanceof SupportsProjectionPushDown)) {\n \t\t\treturn false;\n \t\t}\n-\t\tSupportsProjectionPushDown pushDownSource = (SupportsProjectionPushDown) tableSourceTable.tableSource();\n-\t\tif (pushDownSource.supportsNestedProjection()) {\n-\t\t\tthrow new TableException(\"Nested projection push down is unsupported now. \\n\" +\n-\t\t\t\t\t\"Please disable nested projection (SupportsProjectionPushDown#supportsNestedProjection returns false), \" +\n-\t\t\t\t\t\"planner will push down the top-level columns.\");\n-\t\t} else {\n-\t\t\treturn true;\n-\t\t}\n+\t\treturn Arrays.stream(tableSourceTable.extraDigests()).noneMatch(digest -> digest.startsWith(\"project=[\"));\n \t}\n \n \t@Override\n \tpublic void onMatch(RelOptRuleCall call) {\n \t\tfinal LogicalProject project = call.rel(0);\n \t\tfinal LogicalTableScan scan = call.rel(1);\n \n-\t\tfinal List<String> fieldNames = scan.getRowType().getFieldNames();\n-\t\tfinal int fieldCount = fieldNames.size();\n-\n \t\tfinal int[] refFields = RexNodeExtractor.extractRefInputFields(project.getProjects());\n-\t\tfinal int[] usedFields;\n-\n \t\tTableSourceTable oldTableSourceTable = scan.getTable().unwrap(TableSourceTable.class);\n-\t\tif (isUpsertSource(oldTableSourceTable)) {\n-\t\t\t// primary key fields are needed for upsert source\n-\t\t\tList<String> keyFields = oldTableSourceTable.catalogTable().getSchema()\n-\t\t\t\t.getPrimaryKey().get().getColumns();\n-\t\t\t// we should get source fields from scan node instead of CatalogTable,\n-\t\t\t// because projection may have been pushed down\n-\t\t\tList<String> sourceFields = scan.getRowType().getFieldNames();\n-\t\t\tint[] primaryKey = ScanUtil.getPrimaryKeyIndices(sourceFields, keyFields);\n-\t\t\tusedFields = mergeFields(refFields, primaryKey);\n-\t\t} else {\n-\t\t\tusedFields = refFields;\n-\t\t}\n-\t\t// if no fields can be projected, we keep the original plan.\n-\t\tif (usedFields.length == fieldCount) {\n+\t\tfinal TableSchema oldSchema = oldTableSourceTable.catalogTable().getSchema();\n+\t\tfinal DynamicTableSource oldSource = oldTableSourceTable.tableSource();\n+\n+\t\tfinal boolean supportsNestedProjection =\n+\t\t\t\t((SupportsProjectionPushDown) oldTableSourceTable.tableSource()).supportsNestedProjection();\n+\t\tList<String> fieldNames = scan.getRowType().getFieldNames();\n+\n+\t\tif (!supportsNestedProjection && refFields.length == fieldNames.size()) {\n+\t\t\t// just keep as same as the old plan\n+\t\t\t// TODO: refactor the affected plan", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "aa094733a6678ec72712da928dd457e79f3f81a9"}, "originalPosition": 93}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY1ODc5Ng==", "bodyText": "remove this", "url": "https://github.com/apache/flink/pull/13631#discussion_r515658796", "createdAt": "2020-11-01T19:03:27Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/utils/RexNodeExtractorTest.scala", "diffHunk": "@@ -66,159 +68,120 @@ class RexNodeExtractorTest extends RexNodeTestBase {\n   private val expressionBridge: ExpressionBridge[PlannerExpression] =", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "aa094733a6678ec72712da928dd457e79f3f81a9"}, "originalPosition": 31}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "34b48e349af82ba98ca546d4d3c8798b0521a1e7", "author": {"user": {"login": "fsk119", "name": "Shengkai "}}, "url": "https://github.com/apache/flink/commit/34b48e349af82ba98ca546d4d3c8798b0521a1e7", "committedDate": "2020-11-03T05:43:58Z", "message": "[FLINK-19639][table sql/planner]Support SupportsNestedProjectionPushDown in planner"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1a6af35c7056199c7f925687e7cff8dc525a1071", "author": {"user": {"login": "fsk119", "name": "Shengkai "}}, "url": "https://github.com/apache/flink/commit/1a6af35c7056199c7f925687e7cff8dc525a1071", "committedDate": "2020-11-03T05:43:58Z", "message": "fix godfrey's comment:\n1. use qualified name list to get the projectedFields and build new projections;\n2. add more tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "679a5eb6a3e649aa2dceadef8a4e6c9455b91983", "author": {"user": {"login": "fsk119", "name": "Shengkai "}}, "url": "https://github.com/apache/flink/commit/679a5eb6a3e649aa2dceadef8a4e6c9455b91983", "committedDate": "2020-11-03T05:43:58Z", "message": "minor fix"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c624950f08de80ee7dc1e0c23d85fc832f779ad2", "author": {"user": {"login": "fsk119", "name": "Shengkai "}}, "url": "https://github.com/apache/flink/commit/c624950f08de80ee7dc1e0c23d85fc832f779ad2", "committedDate": "2020-11-03T05:43:58Z", "message": "fix godfrey's comment:\n1. use qualified name as the projected column name\n2. fix suggestions"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "62edbb5ff58d100105d1bb1f85e44ef11d04cf05", "author": {"user": {"login": "fsk119", "name": "Shengkai "}}, "url": "https://github.com/apache/flink/commit/62edbb5ff58d100105d1bb1f85e44ef11d04cf05", "committedDate": "2020-11-03T05:43:58Z", "message": "use postfix to solve name conflicts."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7ac21c27e3ba812bc0fc73941a494638d8cc6e80", "author": {"user": {"login": "fsk119", "name": "Shengkai "}}, "url": "https://github.com/apache/flink/commit/7ac21c27e3ba812bc0fc73941a494638d8cc6e80", "committedDate": "2020-11-03T05:48:43Z", "message": "1. use postfix \"$%d\" to resolve the name conflicts\n2. rewrite the rule to support metadata push down:\n2.1 we will check the source and extract the physical part of the schema. If the source supports nested projection push down, we use `RexNodeExtractor.extractRefNestedInputFields` to extract data else we add the physical part info into the coordinates info.\n2.2 If the source supports metadata push down, we add the metadata info into the coordinates.\n2.3 with the final coordinates, we write the projection."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2e8ef24b27c38b03f0e6917972a4ab3f14947f04", "author": {"user": {"login": "fsk119", "name": "Shengkai "}}, "url": "https://github.com/apache/flink/commit/2e8ef24b27c38b03f0e6917972a4ab3f14947f04", "committedDate": "2020-11-03T05:48:47Z", "message": "fix line too long"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "503bf618e2ea4cb46c1b929f6e5c1bbdcc5ab170", "author": {"user": {"login": "fsk119", "name": "Shengkai "}}, "url": "https://github.com/apache/flink/commit/503bf618e2ea4cb46c1b929f6e5c1bbdcc5ab170", "committedDate": "2020-11-03T05:48:47Z", "message": "fix test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1c148a77f2f5b68a80dc7e7ecb9a6014d9e760ea", "author": {"user": {"login": "fsk119", "name": "Shengkai "}}, "url": "https://github.com/apache/flink/commit/1c148a77f2f5b68a80dc7e7ecb9a6014d9e760ea", "committedDate": "2020-11-03T05:48:47Z", "message": "address feedback:\n1. rename the func name and add comments;\n2. add test: projection push down with map type;\n3. other minor fix;"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e24f5d693ec0ad3f687fcb622d64ec9f78d20f11", "author": {"user": {"login": "fsk119", "name": "Shengkai "}}, "url": "https://github.com/apache/flink/commit/e24f5d693ec0ad3f687fcb622d64ec9f78d20f11", "committedDate": "2020-11-03T05:48:47Z", "message": "delete unused method"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9515a90a341fdb1173d20025f6ad2915364489b6", "author": {"user": {"login": "fsk119", "name": "Shengkai "}}, "url": "https://github.com/apache/flink/commit/9515a90a341fdb1173d20025f6ad2915364489b6", "committedDate": "2020-11-03T05:48:48Z", "message": "1. introduce the new extractor and rewriter:\n2. fix failed test: the order of fileds in the new schema is determined by the hashmap rather that the order in the projections."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "40d2a12d81458dc803fe9cfd6eba45d0f3098a8a", "author": {"user": {"login": "fsk119", "name": "Shengkai "}}, "url": "https://github.com/apache/flink/commit/40d2a12d81458dc803fe9cfd6eba45d0f3098a8a", "committedDate": "2020-11-03T05:48:48Z", "message": "fix failed test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ad207e1b987cee8de52adac5e3dd2c8eee96d185", "author": {"user": {"login": "fsk119", "name": "Shengkai "}}, "url": "https://github.com/apache/flink/commit/ad207e1b987cee8de52adac5e3dd2c8eee96d185", "committedDate": "2020-11-03T05:48:48Z", "message": "use LinkedHashMap to reduce the cost of the reorder and roll back the modification of the test."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "421d5bb5e7f784a38064941dc2bc3fdc6b1f2ed7", "author": {"user": {"login": "fsk119", "name": "Shengkai "}}, "url": "https://github.com/apache/flink/commit/421d5bb5e7f784a38064941dc2bc3fdc6b1f2ed7", "committedDate": "2020-11-03T05:48:48Z", "message": "fix test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "95f0efef8a8dbbd0caf5e90cce810b448fbe5c74", "author": {"user": {"login": "fsk119", "name": "Shengkai "}}, "url": "https://github.com/apache/flink/commit/95f0efef8a8dbbd0caf5e90cce810b448fbe5c74", "committedDate": "2020-11-03T05:48:48Z", "message": "fix test and address the feedbacks."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0707f7489676ee1a7e709479e47c6a3da5dd0e8b", "author": {"user": {"login": "fsk119", "name": "Shengkai "}}, "url": "https://github.com/apache/flink/commit/0707f7489676ee1a7e709479e47c6a3da5dd0e8b", "committedDate": "2020-11-03T05:51:50Z", "message": "address godfrey's comment:\n1. add RexNodeNestedFields that works as tableschema\n2. fix failed test and some small problems"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "ea637a3e8a33df6264ded320755ec0a397342f0b", "author": {"user": {"login": "fsk119", "name": "Shengkai "}}, "url": "https://github.com/apache/flink/commit/ea637a3e8a33df6264ded320755ec0a397342f0b", "committedDate": "2020-11-02T13:19:28Z", "message": "address godfrey's comment:\n1. add RexNodeNestedFields that works as tableschema\n2. fix failed test and some small problems"}, "afterCommit": {"oid": "0707f7489676ee1a7e709479e47c6a3da5dd0e8b", "author": {"user": {"login": "fsk119", "name": "Shengkai "}}, "url": "https://github.com/apache/flink/commit/0707f7489676ee1a7e709479e47c6a3da5dd0e8b", "committedDate": "2020-11-03T05:51:50Z", "message": "address godfrey's comment:\n1. add RexNodeNestedFields that works as tableschema\n2. fix failed test and some small problems"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIyMjE5MzI3", "url": "https://github.com/apache/flink/pull/13631#pullrequestreview-522219327", "createdAt": "2020-11-03T06:46:55Z", "commit": {"oid": "0707f7489676ee1a7e709479e47c6a3da5dd0e8b"}, "state": "COMMENTED", "comments": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QwNjo0Njo1NlrOHshzFw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QwNzozMDoxOFrOHsinUw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjQ1MzE0Mw==", "bodyText": "If indexInNewSchema is only used for leaf node ,  I suggest renamaing indexInNewSchema to indexOfLeafInNewSchema", "url": "https://github.com/apache/flink/pull/13631#discussion_r516453143", "createdAt": "2020-11-03T06:46:56Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/RexNodeNestedField.scala", "diffHunk": "@@ -0,0 +1,332 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.utils\n+\n+import org.apache.flink.table.api.TableException\n+\n+import org.apache.calcite.rel.`type`.RelDataType\n+import org.apache.calcite.rex._\n+\n+import java.util\n+import java.util.{List => JList}\n+\n+import scala.collection.JavaConversions._\n+import scala.collection.JavaConverters._\n+\n+/**\n+ * RexNodeNestedField is a tree node to build the used fields tree.\n+ *\n+ * @param name                 The name of the fields in the origin schema\n+ * @param indexInOriginSchema  The index of the field in the origin schema.\n+ *                             It only works for the RowType.\n+ * @param fieldType            The type of the field. It is useful when\n+ *                             rewriting the projections.\n+ * @param isLeaf               Mark the field is the leaf node in the tree.\n+ * @param children             Store the children of the field. It's safe\n+ *                             to use name as the index because name is\n+ *                             unique in every level. It uses the\n+ *                             LinkedHashMap to keep the insert order.\n+ *                             In some cases, it can reduce the cost of the\n+ *                             reorder of the fields in query.\n+ * @param indexInNewSchema     It is used by the leaf node to memorize the", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0707f7489676ee1a7e709479e47c6a3da5dd0e8b"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjQ1MzkwNw==", "bodyText": "nit: merge them into one line", "url": "https://github.com/apache/flink/pull/13631#discussion_r516453907", "createdAt": "2020-11-03T06:49:48Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/RexNodeNestedField.scala", "diffHunk": "@@ -0,0 +1,332 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.utils\n+\n+import org.apache.flink.table.api.TableException\n+\n+import org.apache.calcite.rel.`type`.RelDataType\n+import org.apache.calcite.rex._\n+\n+import java.util\n+import java.util.{List => JList}\n+\n+import scala.collection.JavaConversions._\n+import scala.collection.JavaConverters._\n+\n+/**\n+ * RexNodeNestedField is a tree node to build the used fields tree.\n+ *\n+ * @param name                 The name of the fields in the origin schema\n+ * @param indexInOriginSchema  The index of the field in the origin schema.\n+ *                             It only works for the RowType.\n+ * @param fieldType            The type of the field. It is useful when\n+ *                             rewriting the projections.\n+ * @param isLeaf               Mark the field is the leaf node in the tree.\n+ * @param children             Store the children of the field. It's safe\n+ *                             to use name as the index because name is\n+ *                             unique in every level. It uses the\n+ *                             LinkedHashMap to keep the insert order.\n+ *                             In some cases, it can reduce the cost of the\n+ *                             reorder of the fields in query.\n+ * @param indexInNewSchema     It is used by the leaf node to memorize the\n+ *                             index in the new schema.\n+ */\n+class RexNodeNestedField(\n+    val name: String,\n+    val indexInOriginSchema: Int,\n+    val fieldType: RelDataType,\n+    val children: util.LinkedHashMap[String, RexNodeNestedField],\n+    var isLeaf: Boolean,\n+    var indexInNewSchema: Int) {\n+\n+  def addChild(field: RexNodeNestedField): Unit = {\n+    if (!children.contains(field.name)) {\n+      isLeaf = false\n+      children.put(field.name, field)\n+    }\n+  }\n+}\n+\n+/**\n+ * RexNodeNestedFields could be regard as a table schema that represents\n+ * a table's structure with field names and data types. It uses a\n+ * LinkedHashMap to store the pairs of name: String and column: RexNodeNestedField.\n+ *\n+ * @param columns  Fields in the origin schema are used by the query.\n+ */\n+class RexNodeNestedFields(\n+    val columns: util.LinkedHashMap[String, RexNodeNestedField]) {\n+\n+}\n+\n+object RexNodeNestedFields {\n+  /**\n+   * It will uses the RexNodes to build a tree of the used fields.\n+   * It uses a visitor to visit the operands of the expression. For\n+   * input ref, it sits on the top level of the schema and it is the\n+   * direct child of the root. For field access, it first decompose\n+   * the field into a list and then create the node for every node in\n+   * the list.\n+   *\n+   * In some situation, it will delete node. For example, the input\n+   * expressions are \"$0.child\" and \"$0\". It will first create the\n+   * intermediate node \"$0\" and leaf node \"child\". When coming to the\n+   * expression \"$0\", it indicate the query will use the whole fields \"$0\"\n+   * rather than the child \"child\" only. In this situation, it will mark\n+   * the node \"$0\" as a leaf node and delete its children.\n+   * */\n+  def build(exprs: JList[RexNode], rowType: RelDataType):\n+      RexNodeNestedFields = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0707f7489676ee1a7e709479e47c6a3da5dd0e8b"}, "originalPosition": 95}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjQ1NDI0NQ==", "bodyText": "rename to NestedColumn ?", "url": "https://github.com/apache/flink/pull/13631#discussion_r516454245", "createdAt": "2020-11-03T06:50:49Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/RexNodeNestedField.scala", "diffHunk": "@@ -0,0 +1,332 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.utils\n+\n+import org.apache.flink.table.api.TableException\n+\n+import org.apache.calcite.rel.`type`.RelDataType\n+import org.apache.calcite.rex._\n+\n+import java.util\n+import java.util.{List => JList}\n+\n+import scala.collection.JavaConversions._\n+import scala.collection.JavaConverters._\n+\n+/**\n+ * RexNodeNestedField is a tree node to build the used fields tree.\n+ *\n+ * @param name                 The name of the fields in the origin schema\n+ * @param indexInOriginSchema  The index of the field in the origin schema.\n+ *                             It only works for the RowType.\n+ * @param fieldType            The type of the field. It is useful when\n+ *                             rewriting the projections.\n+ * @param isLeaf               Mark the field is the leaf node in the tree.\n+ * @param children             Store the children of the field. It's safe\n+ *                             to use name as the index because name is\n+ *                             unique in every level. It uses the\n+ *                             LinkedHashMap to keep the insert order.\n+ *                             In some cases, it can reduce the cost of the\n+ *                             reorder of the fields in query.\n+ * @param indexInNewSchema     It is used by the leaf node to memorize the\n+ *                             index in the new schema.\n+ */\n+class RexNodeNestedField(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0707f7489676ee1a7e709479e47c6a3da5dd0e8b"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjQ1NDU1Nw==", "bodyText": "rename to NestedSchema ? and rename the file to NestedSchema too", "url": "https://github.com/apache/flink/pull/13631#discussion_r516454557", "createdAt": "2020-11-03T06:51:58Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/RexNodeNestedField.scala", "diffHunk": "@@ -0,0 +1,332 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.utils\n+\n+import org.apache.flink.table.api.TableException\n+\n+import org.apache.calcite.rel.`type`.RelDataType\n+import org.apache.calcite.rex._\n+\n+import java.util\n+import java.util.{List => JList}\n+\n+import scala.collection.JavaConversions._\n+import scala.collection.JavaConverters._\n+\n+/**\n+ * RexNodeNestedField is a tree node to build the used fields tree.\n+ *\n+ * @param name                 The name of the fields in the origin schema\n+ * @param indexInOriginSchema  The index of the field in the origin schema.\n+ *                             It only works for the RowType.\n+ * @param fieldType            The type of the field. It is useful when\n+ *                             rewriting the projections.\n+ * @param isLeaf               Mark the field is the leaf node in the tree.\n+ * @param children             Store the children of the field. It's safe\n+ *                             to use name as the index because name is\n+ *                             unique in every level. It uses the\n+ *                             LinkedHashMap to keep the insert order.\n+ *                             In some cases, it can reduce the cost of the\n+ *                             reorder of the fields in query.\n+ * @param indexInNewSchema     It is used by the leaf node to memorize the\n+ *                             index in the new schema.\n+ */\n+class RexNodeNestedField(\n+    val name: String,\n+    val indexInOriginSchema: Int,\n+    val fieldType: RelDataType,\n+    val children: util.LinkedHashMap[String, RexNodeNestedField],\n+    var isLeaf: Boolean,\n+    var indexInNewSchema: Int) {\n+\n+  def addChild(field: RexNodeNestedField): Unit = {\n+    if (!children.contains(field.name)) {\n+      isLeaf = false\n+      children.put(field.name, field)\n+    }\n+  }\n+}\n+\n+/**\n+ * RexNodeNestedFields could be regard as a table schema that represents\n+ * a table's structure with field names and data types. It uses a\n+ * LinkedHashMap to store the pairs of name: String and column: RexNodeNestedField.\n+ *\n+ * @param columns  Fields in the origin schema are used by the query.\n+ */\n+class RexNodeNestedFields(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0707f7489676ee1a7e709479e47c6a3da5dd0e8b"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjQ1NTk4Mw==", "bodyText": "index ?", "url": "https://github.com/apache/flink/pull/13631#discussion_r516455983", "createdAt": "2020-11-03T06:56:46Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/RexNodeNestedField.scala", "diffHunk": "@@ -0,0 +1,332 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.utils\n+\n+import org.apache.flink.table.api.TableException\n+\n+import org.apache.calcite.rel.`type`.RelDataType\n+import org.apache.calcite.rex._\n+\n+import java.util\n+import java.util.{List => JList}\n+\n+import scala.collection.JavaConversions._\n+import scala.collection.JavaConverters._\n+\n+/**\n+ * RexNodeNestedField is a tree node to build the used fields tree.\n+ *\n+ * @param name                 The name of the fields in the origin schema\n+ * @param indexInOriginSchema  The index of the field in the origin schema.\n+ *                             It only works for the RowType.\n+ * @param fieldType            The type of the field. It is useful when\n+ *                             rewriting the projections.\n+ * @param isLeaf               Mark the field is the leaf node in the tree.\n+ * @param children             Store the children of the field. It's safe\n+ *                             to use name as the index because name is\n+ *                             unique in every level. It uses the\n+ *                             LinkedHashMap to keep the insert order.\n+ *                             In some cases, it can reduce the cost of the\n+ *                             reorder of the fields in query.\n+ * @param indexInNewSchema     It is used by the leaf node to memorize the\n+ *                             index in the new schema.\n+ */\n+class RexNodeNestedField(\n+    val name: String,\n+    val indexInOriginSchema: Int,\n+    val fieldType: RelDataType,\n+    val children: util.LinkedHashMap[String, RexNodeNestedField],\n+    var isLeaf: Boolean,\n+    var indexInNewSchema: Int) {\n+\n+  def addChild(field: RexNodeNestedField): Unit = {\n+    if (!children.contains(field.name)) {\n+      isLeaf = false\n+      children.put(field.name, field)\n+    }\n+  }\n+}\n+\n+/**\n+ * RexNodeNestedFields could be regard as a table schema that represents\n+ * a table's structure with field names and data types. It uses a\n+ * LinkedHashMap to store the pairs of name: String and column: RexNodeNestedField.\n+ *\n+ * @param columns  Fields in the origin schema are used by the query.\n+ */\n+class RexNodeNestedFields(\n+    val columns: util.LinkedHashMap[String, RexNodeNestedField]) {\n+\n+}\n+\n+object RexNodeNestedFields {\n+  /**\n+   * It will uses the RexNodes to build a tree of the used fields.\n+   * It uses a visitor to visit the operands of the expression. For\n+   * input ref, it sits on the top level of the schema and it is the\n+   * direct child of the root. For field access, it first decompose\n+   * the field into a list and then create the node for every node in\n+   * the list.\n+   *\n+   * In some situation, it will delete node. For example, the input\n+   * expressions are \"$0.child\" and \"$0\". It will first create the\n+   * intermediate node \"$0\" and leaf node \"child\". When coming to the\n+   * expression \"$0\", it indicate the query will use the whole fields \"$0\"\n+   * rather than the child \"child\" only. In this situation, it will mark\n+   * the node \"$0\" as a leaf node and delete its children.\n+   * */\n+  def build(exprs: JList[RexNode], rowType: RelDataType):\n+      RexNodeNestedFields = {\n+    // the order field in the root node is to memorize\n+    // the number of leaf\n+    val root = new RexNodeNestedField(\n+      \"root\",\n+      -1,\n+      rowType,\n+      new util.LinkedHashMap[String, RexNodeNestedField](),\n+      false,\n+      -1)\n+    val visitor = new NestedFieldExtractor(root, rowType)\n+    for(expr <- exprs) {\n+      expr.accept(visitor)\n+    }\n+    new RexNodeNestedFields(root.children)\n+  }\n+\n+  /**\n+   * After the projection, the used fields location has been changed.\n+   * If the node in the tree has been labeled with the order, it will\n+   * rewrite the location in the old schema with the new location.\n+   *\n+   * It uses a visitor to visit operands of the RexNode. If the type of\n+   * operand is InputRef, it still in the top level of the schema and get\n+   * the location of the fields using map. If the type of the operand is\n+   * FieldAccess, it will first traverse to the top level of the field and\n+   * iterate every level of the field with the name in the RexNode. For more\n+   * details, please refer to NestedFieldReWriter.\n+   */\n+  def rewrite(\n+      exprs: JList[RexNode],\n+      root: RexNodeNestedFields,\n+      builder: RexBuilder): JList[RexNode] = {\n+    val writer = new NestedFieldReWriter(root, builder)\n+    exprs.map(_.accept(writer)).toList.asJava\n+  }\n+\n+  /**\n+   * It will label the order of the leaf node with the insert order rather\n+   * than the natural order of the name and output the path to the every\n+   * leaf node. The paths are useful for interface SupportsProjectionPushDown\n+   * and test(debug).\n+   */\n+  def labelAndConvert(root: RexNodeNestedFields): Array[Array[Int]] = {\n+    val allPaths = new util.LinkedList[Array[Int]]()\n+    val path = new util.LinkedList[Int]()\n+    root.columns.foldLeft(0) {\n+      case (newOrder, (_, column)) =>\n+        traverse(column, newOrder, path, allPaths)\n+    }\n+    allPaths.toArray(new Array[Array[Int]](0))\n+  }\n+\n+  private def traverse(\n+      parent: RexNodeNestedField,\n+      order: Int,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0707f7489676ee1a7e709479e47c6a3da5dd0e8b"}, "originalPosition": 150}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjQ1NjM5MQ==", "bodyText": "use RexNodeNestedFields, and remove vals", "url": "https://github.com/apache/flink/pull/13631#discussion_r516456391", "createdAt": "2020-11-03T06:58:05Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/RexNodeNestedField.scala", "diffHunk": "@@ -0,0 +1,332 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.utils\n+\n+import org.apache.flink.table.api.TableException\n+\n+import org.apache.calcite.rel.`type`.RelDataType\n+import org.apache.calcite.rex._\n+\n+import java.util\n+import java.util.{List => JList}\n+\n+import scala.collection.JavaConversions._\n+import scala.collection.JavaConverters._\n+\n+/**\n+ * RexNodeNestedField is a tree node to build the used fields tree.\n+ *\n+ * @param name                 The name of the fields in the origin schema\n+ * @param indexInOriginSchema  The index of the field in the origin schema.\n+ *                             It only works for the RowType.\n+ * @param fieldType            The type of the field. It is useful when\n+ *                             rewriting the projections.\n+ * @param isLeaf               Mark the field is the leaf node in the tree.\n+ * @param children             Store the children of the field. It's safe\n+ *                             to use name as the index because name is\n+ *                             unique in every level. It uses the\n+ *                             LinkedHashMap to keep the insert order.\n+ *                             In some cases, it can reduce the cost of the\n+ *                             reorder of the fields in query.\n+ * @param indexInNewSchema     It is used by the leaf node to memorize the\n+ *                             index in the new schema.\n+ */\n+class RexNodeNestedField(\n+    val name: String,\n+    val indexInOriginSchema: Int,\n+    val fieldType: RelDataType,\n+    val children: util.LinkedHashMap[String, RexNodeNestedField],\n+    var isLeaf: Boolean,\n+    var indexInNewSchema: Int) {\n+\n+  def addChild(field: RexNodeNestedField): Unit = {\n+    if (!children.contains(field.name)) {\n+      isLeaf = false\n+      children.put(field.name, field)\n+    }\n+  }\n+}\n+\n+/**\n+ * RexNodeNestedFields could be regard as a table schema that represents\n+ * a table's structure with field names and data types. It uses a\n+ * LinkedHashMap to store the pairs of name: String and column: RexNodeNestedField.\n+ *\n+ * @param columns  Fields in the origin schema are used by the query.\n+ */\n+class RexNodeNestedFields(\n+    val columns: util.LinkedHashMap[String, RexNodeNestedField]) {\n+\n+}\n+\n+object RexNodeNestedFields {\n+  /**\n+   * It will uses the RexNodes to build a tree of the used fields.\n+   * It uses a visitor to visit the operands of the expression. For\n+   * input ref, it sits on the top level of the schema and it is the\n+   * direct child of the root. For field access, it first decompose\n+   * the field into a list and then create the node for every node in\n+   * the list.\n+   *\n+   * In some situation, it will delete node. For example, the input\n+   * expressions are \"$0.child\" and \"$0\". It will first create the\n+   * intermediate node \"$0\" and leaf node \"child\". When coming to the\n+   * expression \"$0\", it indicate the query will use the whole fields \"$0\"\n+   * rather than the child \"child\" only. In this situation, it will mark\n+   * the node \"$0\" as a leaf node and delete its children.\n+   * */\n+  def build(exprs: JList[RexNode], rowType: RelDataType):\n+      RexNodeNestedFields = {\n+    // the order field in the root node is to memorize\n+    // the number of leaf\n+    val root = new RexNodeNestedField(\n+      \"root\",\n+      -1,\n+      rowType,\n+      new util.LinkedHashMap[String, RexNodeNestedField](),\n+      false,\n+      -1)\n+    val visitor = new NestedFieldExtractor(root, rowType)\n+    for(expr <- exprs) {\n+      expr.accept(visitor)\n+    }\n+    new RexNodeNestedFields(root.children)\n+  }\n+\n+  /**\n+   * After the projection, the used fields location has been changed.\n+   * If the node in the tree has been labeled with the order, it will\n+   * rewrite the location in the old schema with the new location.\n+   *\n+   * It uses a visitor to visit operands of the RexNode. If the type of\n+   * operand is InputRef, it still in the top level of the schema and get\n+   * the location of the fields using map. If the type of the operand is\n+   * FieldAccess, it will first traverse to the top level of the field and\n+   * iterate every level of the field with the name in the RexNode. For more\n+   * details, please refer to NestedFieldReWriter.\n+   */\n+  def rewrite(\n+      exprs: JList[RexNode],\n+      root: RexNodeNestedFields,\n+      builder: RexBuilder): JList[RexNode] = {\n+    val writer = new NestedFieldReWriter(root, builder)\n+    exprs.map(_.accept(writer)).toList.asJava\n+  }\n+\n+  /**\n+   * It will label the order of the leaf node with the insert order rather\n+   * than the natural order of the name and output the path to the every\n+   * leaf node. The paths are useful for interface SupportsProjectionPushDown\n+   * and test(debug).\n+   */\n+  def labelAndConvert(root: RexNodeNestedFields): Array[Array[Int]] = {\n+    val allPaths = new util.LinkedList[Array[Int]]()\n+    val path = new util.LinkedList[Int]()\n+    root.columns.foldLeft(0) {\n+      case (newOrder, (_, column)) =>\n+        traverse(column, newOrder, path, allPaths)\n+    }\n+    allPaths.toArray(new Array[Array[Int]](0))\n+  }\n+\n+  private def traverse(\n+      parent: RexNodeNestedField,\n+      order: Int,\n+      path: JList[Int],\n+      allPaths: JList[Array[Int]]): Int ={\n+    val tail = path.size()\n+    // push self\n+    path.add(parent.indexInOriginSchema)\n+    val newOrder = if (parent.isLeaf) {\n+      // leaf node\n+      parent.indexInNewSchema = order\n+      // ignore root node\n+      allPaths.add(path.asScala.toArray)\n+      order + 1\n+    } else {\n+      // iterate children\n+      parent.children.values().foldLeft(order) {\n+        case (newOrder, child) =>\n+          traverse(child, newOrder, path, allPaths)\n+      }\n+    }\n+    // pop self\n+    path.remove(tail)\n+    newOrder\n+  }\n+}\n+\n+/**\n+ * A RexShuttle to rewrite field accesses of RexNode with nested projection.\n+ * For `RexInputRef`, it uses the old input ref name to find the new input fields ref\n+ * and use the order to generate the new input ref.\n+ * For `RexFieldAccess`, it will traverse to the top level of the field access and\n+ * then to generate new RexNode. There are 3 situations we need to consider:\n+ *  1. if top level field is marked to use all sub-fields , make field access of the reference\n+ *  and warp the ref as RexFieldAccess with the sub field name;\n+ *  2. if top level field isn't marked to use all sub-fields and its direct field\n+ *  is marked as useall, make field reference of the direct subfield;\n+ *  3. if neither situation above happens, return from the recursion with the updated parent.\n+ * When the process is back from the recursion, it still has 2 situations need to\n+ * consider:\n+ *  1. if the process has found the reference of the upper level, just make an access on the\n+ *  reference founded before;\n+ *  2. if the process hasn't found the first reference, the process continues to search under\n+ *  the current parent.\n+ */\n+private class NestedFieldReWriter(\n+    usedFields: RexNodeNestedFields,\n+    builder: RexBuilder) extends RexShuttle {\n+  override def visitInputRef(inputRef: RexInputRef): RexNode = {\n+    if (!usedFields.columns.containsKey(inputRef.getName)) {\n+      throw new TableException(\n+        \"Illegal input field access\" + inputRef.getName)\n+    } else {\n+      val field = usedFields.columns.get(inputRef.getName)\n+      new RexInputRef(field.indexInNewSchema, field.fieldType)\n+    }\n+  }\n+\n+  override def visitFieldAccess(fieldAccess: RexFieldAccess): RexNode = {\n+    val (node, _) = traverse(fieldAccess)\n+    if (node.isDefined) {\n+      node.get\n+    } else {\n+      throw new TableException(\n+        \"Unknown field \" + fieldAccess + \" when rewrite projection \")\n+    }\n+  }\n+\n+  private def traverse(\n+      fieldAccess: RexFieldAccess): (Option[RexNode], RexNodeNestedField) = {\n+    fieldAccess.getReferenceExpr match {\n+      case ref: RexInputRef =>\n+        val parent = usedFields.columns.get(ref.getName)\n+        if (parent.isLeaf) {\n+          (\n+            Some(builder.makeFieldAccess(\n+              new RexInputRef(parent.indexInNewSchema, parent.fieldType),\n+              fieldAccess.getField.getName,\n+              true)), parent)\n+        } else {\n+          val child = parent.children.get(fieldAccess.getField.getName)\n+          if (child.isLeaf) {\n+            (Some(new RexInputRef(child.indexInNewSchema, child.fieldType)), child)\n+          } else {\n+            (Option.empty, child)\n+          }\n+        }\n+      case acc: RexFieldAccess =>\n+        val (field, parent) = traverse(acc)\n+        if (field.isDefined) {\n+          (\n+            Some(\n+              builder.makeFieldAccess(\n+                field.get,\n+                fieldAccess.getField.getName,\n+                true)),\n+            parent)\n+        } else {\n+          val child = parent.children.get(fieldAccess.getField.getName)\n+          if (child.isLeaf) {\n+            (Some(new RexInputRef(child.indexInNewSchema, child.fieldType)), child)\n+          } else {\n+            (Option.empty, child)\n+          }\n+        }\n+    }\n+  }\n+}\n+\n+/**\n+ * An RexVisitor to extract all referenced input fields\n+ */\n+private class NestedFieldExtractor(val root: RexNodeNestedField, val rowType: RelDataType)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0707f7489676ee1a7e709479e47c6a3da5dd0e8b"}, "originalPosition": 260}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjQ1NzAwMg==", "bodyText": "newIndex", "url": "https://github.com/apache/flink/pull/13631#discussion_r516457002", "createdAt": "2020-11-03T07:00:16Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/RexNodeNestedField.scala", "diffHunk": "@@ -0,0 +1,332 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.utils\n+\n+import org.apache.flink.table.api.TableException\n+\n+import org.apache.calcite.rel.`type`.RelDataType\n+import org.apache.calcite.rex._\n+\n+import java.util\n+import java.util.{List => JList}\n+\n+import scala.collection.JavaConversions._\n+import scala.collection.JavaConverters._\n+\n+/**\n+ * RexNodeNestedField is a tree node to build the used fields tree.\n+ *\n+ * @param name                 The name of the fields in the origin schema\n+ * @param indexInOriginSchema  The index of the field in the origin schema.\n+ *                             It only works for the RowType.\n+ * @param fieldType            The type of the field. It is useful when\n+ *                             rewriting the projections.\n+ * @param isLeaf               Mark the field is the leaf node in the tree.\n+ * @param children             Store the children of the field. It's safe\n+ *                             to use name as the index because name is\n+ *                             unique in every level. It uses the\n+ *                             LinkedHashMap to keep the insert order.\n+ *                             In some cases, it can reduce the cost of the\n+ *                             reorder of the fields in query.\n+ * @param indexInNewSchema     It is used by the leaf node to memorize the\n+ *                             index in the new schema.\n+ */\n+class RexNodeNestedField(\n+    val name: String,\n+    val indexInOriginSchema: Int,\n+    val fieldType: RelDataType,\n+    val children: util.LinkedHashMap[String, RexNodeNestedField],\n+    var isLeaf: Boolean,\n+    var indexInNewSchema: Int) {\n+\n+  def addChild(field: RexNodeNestedField): Unit = {\n+    if (!children.contains(field.name)) {\n+      isLeaf = false\n+      children.put(field.name, field)\n+    }\n+  }\n+}\n+\n+/**\n+ * RexNodeNestedFields could be regard as a table schema that represents\n+ * a table's structure with field names and data types. It uses a\n+ * LinkedHashMap to store the pairs of name: String and column: RexNodeNestedField.\n+ *\n+ * @param columns  Fields in the origin schema are used by the query.\n+ */\n+class RexNodeNestedFields(\n+    val columns: util.LinkedHashMap[String, RexNodeNestedField]) {\n+\n+}\n+\n+object RexNodeNestedFields {\n+  /**\n+   * It will uses the RexNodes to build a tree of the used fields.\n+   * It uses a visitor to visit the operands of the expression. For\n+   * input ref, it sits on the top level of the schema and it is the\n+   * direct child of the root. For field access, it first decompose\n+   * the field into a list and then create the node for every node in\n+   * the list.\n+   *\n+   * In some situation, it will delete node. For example, the input\n+   * expressions are \"$0.child\" and \"$0\". It will first create the\n+   * intermediate node \"$0\" and leaf node \"child\". When coming to the\n+   * expression \"$0\", it indicate the query will use the whole fields \"$0\"\n+   * rather than the child \"child\" only. In this situation, it will mark\n+   * the node \"$0\" as a leaf node and delete its children.\n+   * */\n+  def build(exprs: JList[RexNode], rowType: RelDataType):\n+      RexNodeNestedFields = {\n+    // the order field in the root node is to memorize\n+    // the number of leaf\n+    val root = new RexNodeNestedField(\n+      \"root\",\n+      -1,\n+      rowType,\n+      new util.LinkedHashMap[String, RexNodeNestedField](),\n+      false,\n+      -1)\n+    val visitor = new NestedFieldExtractor(root, rowType)\n+    for(expr <- exprs) {\n+      expr.accept(visitor)\n+    }\n+    new RexNodeNestedFields(root.children)\n+  }\n+\n+  /**\n+   * After the projection, the used fields location has been changed.\n+   * If the node in the tree has been labeled with the order, it will\n+   * rewrite the location in the old schema with the new location.\n+   *\n+   * It uses a visitor to visit operands of the RexNode. If the type of\n+   * operand is InputRef, it still in the top level of the schema and get\n+   * the location of the fields using map. If the type of the operand is\n+   * FieldAccess, it will first traverse to the top level of the field and\n+   * iterate every level of the field with the name in the RexNode. For more\n+   * details, please refer to NestedFieldReWriter.\n+   */\n+  def rewrite(\n+      exprs: JList[RexNode],\n+      root: RexNodeNestedFields,\n+      builder: RexBuilder): JList[RexNode] = {\n+    val writer = new NestedFieldReWriter(root, builder)\n+    exprs.map(_.accept(writer)).toList.asJava\n+  }\n+\n+  /**\n+   * It will label the order of the leaf node with the insert order rather\n+   * than the natural order of the name and output the path to the every\n+   * leaf node. The paths are useful for interface SupportsProjectionPushDown\n+   * and test(debug).\n+   */\n+  def labelAndConvert(root: RexNodeNestedFields): Array[Array[Int]] = {\n+    val allPaths = new util.LinkedList[Array[Int]]()\n+    val path = new util.LinkedList[Int]()\n+    root.columns.foldLeft(0) {\n+      case (newOrder, (_, column)) =>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0707f7489676ee1a7e709479e47c6a3da5dd0e8b"}, "originalPosition": 142}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjQ1ODEwOA==", "bodyText": "extract the similar code into a comment method", "url": "https://github.com/apache/flink/pull/13631#discussion_r516458108", "createdAt": "2020-11-03T07:04:15Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/RexNodeNestedField.scala", "diffHunk": "@@ -0,0 +1,332 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.utils\n+\n+import org.apache.flink.table.api.TableException\n+\n+import org.apache.calcite.rel.`type`.RelDataType\n+import org.apache.calcite.rex._\n+\n+import java.util\n+import java.util.{List => JList}\n+\n+import scala.collection.JavaConversions._\n+import scala.collection.JavaConverters._\n+\n+/**\n+ * RexNodeNestedField is a tree node to build the used fields tree.\n+ *\n+ * @param name                 The name of the fields in the origin schema\n+ * @param indexInOriginSchema  The index of the field in the origin schema.\n+ *                             It only works for the RowType.\n+ * @param fieldType            The type of the field. It is useful when\n+ *                             rewriting the projections.\n+ * @param isLeaf               Mark the field is the leaf node in the tree.\n+ * @param children             Store the children of the field. It's safe\n+ *                             to use name as the index because name is\n+ *                             unique in every level. It uses the\n+ *                             LinkedHashMap to keep the insert order.\n+ *                             In some cases, it can reduce the cost of the\n+ *                             reorder of the fields in query.\n+ * @param indexInNewSchema     It is used by the leaf node to memorize the\n+ *                             index in the new schema.\n+ */\n+class RexNodeNestedField(\n+    val name: String,\n+    val indexInOriginSchema: Int,\n+    val fieldType: RelDataType,\n+    val children: util.LinkedHashMap[String, RexNodeNestedField],\n+    var isLeaf: Boolean,\n+    var indexInNewSchema: Int) {\n+\n+  def addChild(field: RexNodeNestedField): Unit = {\n+    if (!children.contains(field.name)) {\n+      isLeaf = false\n+      children.put(field.name, field)\n+    }\n+  }\n+}\n+\n+/**\n+ * RexNodeNestedFields could be regard as a table schema that represents\n+ * a table's structure with field names and data types. It uses a\n+ * LinkedHashMap to store the pairs of name: String and column: RexNodeNestedField.\n+ *\n+ * @param columns  Fields in the origin schema are used by the query.\n+ */\n+class RexNodeNestedFields(\n+    val columns: util.LinkedHashMap[String, RexNodeNestedField]) {\n+\n+}\n+\n+object RexNodeNestedFields {\n+  /**\n+   * It will uses the RexNodes to build a tree of the used fields.\n+   * It uses a visitor to visit the operands of the expression. For\n+   * input ref, it sits on the top level of the schema and it is the\n+   * direct child of the root. For field access, it first decompose\n+   * the field into a list and then create the node for every node in\n+   * the list.\n+   *\n+   * In some situation, it will delete node. For example, the input\n+   * expressions are \"$0.child\" and \"$0\". It will first create the\n+   * intermediate node \"$0\" and leaf node \"child\". When coming to the\n+   * expression \"$0\", it indicate the query will use the whole fields \"$0\"\n+   * rather than the child \"child\" only. In this situation, it will mark\n+   * the node \"$0\" as a leaf node and delete its children.\n+   * */\n+  def build(exprs: JList[RexNode], rowType: RelDataType):\n+      RexNodeNestedFields = {\n+    // the order field in the root node is to memorize\n+    // the number of leaf\n+    val root = new RexNodeNestedField(\n+      \"root\",\n+      -1,\n+      rowType,\n+      new util.LinkedHashMap[String, RexNodeNestedField](),\n+      false,\n+      -1)\n+    val visitor = new NestedFieldExtractor(root, rowType)\n+    for(expr <- exprs) {\n+      expr.accept(visitor)\n+    }\n+    new RexNodeNestedFields(root.children)\n+  }\n+\n+  /**\n+   * After the projection, the used fields location has been changed.\n+   * If the node in the tree has been labeled with the order, it will\n+   * rewrite the location in the old schema with the new location.\n+   *\n+   * It uses a visitor to visit operands of the RexNode. If the type of\n+   * operand is InputRef, it still in the top level of the schema and get\n+   * the location of the fields using map. If the type of the operand is\n+   * FieldAccess, it will first traverse to the top level of the field and\n+   * iterate every level of the field with the name in the RexNode. For more\n+   * details, please refer to NestedFieldReWriter.\n+   */\n+  def rewrite(\n+      exprs: JList[RexNode],\n+      root: RexNodeNestedFields,\n+      builder: RexBuilder): JList[RexNode] = {\n+    val writer = new NestedFieldReWriter(root, builder)\n+    exprs.map(_.accept(writer)).toList.asJava\n+  }\n+\n+  /**\n+   * It will label the order of the leaf node with the insert order rather\n+   * than the natural order of the name and output the path to the every\n+   * leaf node. The paths are useful for interface SupportsProjectionPushDown\n+   * and test(debug).\n+   */\n+  def labelAndConvert(root: RexNodeNestedFields): Array[Array[Int]] = {\n+    val allPaths = new util.LinkedList[Array[Int]]()\n+    val path = new util.LinkedList[Int]()\n+    root.columns.foldLeft(0) {\n+      case (newOrder, (_, column)) =>\n+        traverse(column, newOrder, path, allPaths)\n+    }\n+    allPaths.toArray(new Array[Array[Int]](0))\n+  }\n+\n+  private def traverse(\n+      parent: RexNodeNestedField,\n+      order: Int,\n+      path: JList[Int],\n+      allPaths: JList[Array[Int]]): Int ={\n+    val tail = path.size()\n+    // push self\n+    path.add(parent.indexInOriginSchema)\n+    val newOrder = if (parent.isLeaf) {\n+      // leaf node\n+      parent.indexInNewSchema = order\n+      // ignore root node\n+      allPaths.add(path.asScala.toArray)\n+      order + 1\n+    } else {\n+      // iterate children\n+      parent.children.values().foldLeft(order) {\n+        case (newOrder, child) =>\n+          traverse(child, newOrder, path, allPaths)\n+      }\n+    }\n+    // pop self\n+    path.remove(tail)\n+    newOrder\n+  }\n+}\n+\n+/**\n+ * A RexShuttle to rewrite field accesses of RexNode with nested projection.\n+ * For `RexInputRef`, it uses the old input ref name to find the new input fields ref\n+ * and use the order to generate the new input ref.\n+ * For `RexFieldAccess`, it will traverse to the top level of the field access and\n+ * then to generate new RexNode. There are 3 situations we need to consider:\n+ *  1. if top level field is marked to use all sub-fields , make field access of the reference\n+ *  and warp the ref as RexFieldAccess with the sub field name;\n+ *  2. if top level field isn't marked to use all sub-fields and its direct field\n+ *  is marked as useall, make field reference of the direct subfield;\n+ *  3. if neither situation above happens, return from the recursion with the updated parent.\n+ * When the process is back from the recursion, it still has 2 situations need to\n+ * consider:\n+ *  1. if the process has found the reference of the upper level, just make an access on the\n+ *  reference founded before;\n+ *  2. if the process hasn't found the first reference, the process continues to search under\n+ *  the current parent.\n+ */\n+private class NestedFieldReWriter(\n+    usedFields: RexNodeNestedFields,\n+    builder: RexBuilder) extends RexShuttle {\n+  override def visitInputRef(inputRef: RexInputRef): RexNode = {\n+    if (!usedFields.columns.containsKey(inputRef.getName)) {\n+      throw new TableException(\n+        \"Illegal input field access\" + inputRef.getName)\n+    } else {\n+      val field = usedFields.columns.get(inputRef.getName)\n+      new RexInputRef(field.indexInNewSchema, field.fieldType)\n+    }\n+  }\n+\n+  override def visitFieldAccess(fieldAccess: RexFieldAccess): RexNode = {\n+    val (node, _) = traverse(fieldAccess)\n+    if (node.isDefined) {\n+      node.get\n+    } else {\n+      throw new TableException(\n+        \"Unknown field \" + fieldAccess + \" when rewrite projection \")\n+    }\n+  }\n+\n+  private def traverse(\n+      fieldAccess: RexFieldAccess): (Option[RexNode], RexNodeNestedField) = {\n+    fieldAccess.getReferenceExpr match {\n+      case ref: RexInputRef =>\n+        val parent = usedFields.columns.get(ref.getName)\n+        if (parent.isLeaf) {\n+          (\n+            Some(builder.makeFieldAccess(\n+              new RexInputRef(parent.indexInNewSchema, parent.fieldType),\n+              fieldAccess.getField.getName,\n+              true)), parent)\n+        } else {\n+          val child = parent.children.get(fieldAccess.getField.getName)\n+          if (child.isLeaf) {\n+            (Some(new RexInputRef(child.indexInNewSchema, child.fieldType)), child)\n+          } else {\n+            (Option.empty, child)\n+          }\n+        }\n+      case acc: RexFieldAccess =>\n+        val (field, parent) = traverse(acc)\n+        if (field.isDefined) {\n+          (\n+            Some(\n+              builder.makeFieldAccess(\n+                field.get,\n+                fieldAccess.getField.getName,\n+                true)),\n+            parent)\n+        } else {\n+          val child = parent.children.get(fieldAccess.getField.getName)\n+          if (child.isLeaf) {\n+            (Some(new RexInputRef(child.indexInNewSchema, child.fieldType)), child)\n+          } else {\n+            (Option.empty, child)\n+          }\n+        }\n+    }\n+  }\n+}\n+\n+/**\n+ * An RexVisitor to extract all referenced input fields\n+ */\n+private class NestedFieldExtractor(val root: RexNodeNestedField, val rowType: RelDataType)\n+  extends RexVisitorImpl[Unit](true) {\n+\n+  override def visitFieldAccess(fieldAccess: RexFieldAccess): Unit = {\n+    def internalVisit(fieldAccess: RexFieldAccess): (Int, List[String]) = {\n+      fieldAccess.getReferenceExpr match {\n+        case ref: RexInputRef =>\n+          (ref.getIndex, List(ref.getName, fieldAccess.getField.getName))\n+        case fac: RexFieldAccess =>\n+          val (i, n) = internalVisit(fac)\n+          (i, n :+ fieldAccess.getField.getName)\n+      }\n+    }\n+\n+    // extract the info\n+    val (index, names) = internalVisit(fieldAccess)\n+    root.addChild(\n+      new RexNodeNestedField(\n+        names.get(0),\n+        index,\n+        rowType.getFieldList.get(index).getType,\n+        new util.LinkedHashMap[String, RexNodeNestedField](),\n+        false,\n+        -1))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0707f7489676ee1a7e709479e47c6a3da5dd0e8b"}, "originalPosition": 283}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjQ2MDI2OA==", "bodyText": "it does not compare the length of expected set and actual array", "url": "https://github.com/apache/flink/pull/13631#discussion_r516460268", "createdAt": "2020-11-03T07:11:05Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/utils/RexNodeNestedFieldTest.scala", "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.utils\n+\n+import org.apache.flink.table.planner.calcite.FlinkRexBuilder\n+\n+import java.util\n+import java.util.{List => JList}\n+\n+import org.junit.Assert.{assertThat, assertTrue}\n+import org.junit.Test\n+\n+import org.hamcrest.{BaseMatcher, Description}\n+\n+import scala.collection.JavaConversions._\n+import scala.collection.JavaConverters._\n+\n+/**\n+ *  Test for RexNodeNestedField.\n+ */\n+class RexNodeNestedFieldTest extends RexNodeTestBase{\n+  private class UnorderedArrayMatcher(expected: util.HashSet[JList[Int]])", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0707f7489676ee1a7e709479e47c6a3da5dd0e8b"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjQ2NjUxNQ==", "bodyText": "the expected is\n1,1\n0,\n0.\nwe can use list or array to store the expected result, and before comparing, we can sort the expected collection and actual collection.", "url": "https://github.com/apache/flink/pull/13631#discussion_r516466515", "createdAt": "2020-11-03T07:30:18Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/utils/RexNodeNestedFieldTest.scala", "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.utils\n+\n+import org.apache.flink.table.planner.calcite.FlinkRexBuilder\n+\n+import java.util\n+import java.util.{List => JList}\n+\n+import org.junit.Assert.{assertThat, assertTrue}\n+import org.junit.Test\n+\n+import org.hamcrest.{BaseMatcher, Description}\n+\n+import scala.collection.JavaConversions._\n+import scala.collection.JavaConverters._\n+\n+/**\n+ *  Test for RexNodeNestedField.\n+ */\n+class RexNodeNestedFieldTest extends RexNodeTestBase{\n+  private class UnorderedArrayMatcher(expected: util.HashSet[JList[Int]])\n+    extends BaseMatcher[Array[Array[Int]]] {\n+    override def matches(item: Any): Boolean = {\n+      for (path <- item.asInstanceOf[Array[Array[Int]]]) {\n+        if (!expected.contains(path.toSeq.asJava)) {\n+          return false\n+        }\n+      }\n+      true\n+    }\n+\n+    override def describeTo(description: Description): Unit = {\n+      description.appendValueList(\"\", \",\", \"\", expected)\n+    }\n+  }\n+\n+  @Test\n+  def testExtractRefInputFields(): Unit = {\n+    val (exprs, rowType) = buildExprs()\n+    val nestedFields = RexNodeNestedFields.build(exprs, rowType)\n+    val actual = RexNodeNestedFields.labelAndConvert(nestedFields)\n+    val expected = new util.HashSet[JList[Int]]\n+    expected.add(util.Arrays.asList(2))\n+    expected.add(util.Arrays.asList(3))\n+    expected.add(util.Arrays.asList(1))\n+\n+    assertThat(actual, new UnorderedArrayMatcher(expected))\n+  }\n+\n+  @Test\n+  def testExtractRefNestedInputFields(): Unit = {\n+    val (rexProgram, rowType) = buildExprsWithNesting()\n+\n+    val nestedFields = RexNodeNestedFields.build(rexProgram, rowType)\n+    val actual = RexNodeNestedFields.labelAndConvert(nestedFields)\n+    val expected = new util.HashSet[JList[Int]]\n+    expected.add(Array(1, 1).toSeq.asJava)\n+    expected.add(Array(0).toSeq.asJava)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0707f7489676ee1a7e709479e47c6a3da5dd0e8b"}, "originalPosition": 75}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4d844d385b7a556ee1f3f53b36b0b1211136e5b3", "author": {"user": {"login": "fsk119", "name": "Shengkai "}}, "url": "https://github.com/apache/flink/commit/4d844d385b7a556ee1f3f53b36b0b1211136e5b3", "committedDate": "2020-11-03T12:19:24Z", "message": "address feedback: refactor the NestedSchema && NestedColumn"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4bc536663a8a89fee69b58494c76b8e6f5e98c25", "author": {"user": {"login": "fsk119", "name": "Shengkai "}}, "url": "https://github.com/apache/flink/commit/4bc536663a8a89fee69b58494c76b8e6f5e98c25", "committedDate": "2020-11-03T12:25:04Z", "message": "minor fix"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "95b1f48e2fa7c49f8193ef35738787e01bdf2fb5", "author": {"user": {"login": "fsk119", "name": "Shengkai "}}, "url": "https://github.com/apache/flink/commit/95b1f48e2fa7c49f8193ef35738787e01bdf2fb5", "committedDate": "2020-11-03T13:18:01Z", "message": "minor fix"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5a3617336c68ff9311adf5d93a07c9eb5ff39dbf", "author": {"user": {"login": "fsk119", "name": "Shengkai "}}, "url": "https://github.com/apache/flink/commit/5a3617336c68ff9311adf5d93a07c9eb5ff39dbf", "committedDate": "2020-11-03T13:43:49Z", "message": "minor fix"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bb84157d91d0f564a199c6af00209db8f51a7dda", "author": {"user": {"login": "fsk119", "name": "Shengkai "}}, "url": "https://github.com/apache/flink/commit/bb84157d91d0f564a199c6af00209db8f51a7dda", "committedDate": "2020-11-03T13:53:45Z", "message": "delete JLong"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fd8961181c3838b57b78d28c22029d581031763e", "author": {"user": {"login": "fsk119", "name": "Shengkai "}}, "url": "https://github.com/apache/flink/commit/fd8961181c3838b57b78d28c22029d581031763e", "committedDate": "2020-11-03T15:29:11Z", "message": "fix tab problems"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIyOTk1ODQx", "url": "https://github.com/apache/flink/pull/13631#pullrequestreview-522995841", "createdAt": "2020-11-04T02:01:22Z", "commit": {"oid": "fd8961181c3838b57b78d28c22029d581031763e"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5f9d0abfa8ebf2ca3f29d1eeb69f898a6b4de915", "author": {"user": {"login": "fsk119", "name": "Shengkai "}}, "url": "https://github.com/apache/flink/commit/5f9d0abfa8ebf2ca3f29d1eeb69f898a6b4de915", "committedDate": "2020-11-04T02:16:44Z", "message": "rename the file and class"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3236, "cost": 1, "resetAt": "2021-10-28T16:48:13Z"}}}