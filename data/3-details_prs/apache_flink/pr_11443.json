{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzkwNDY2MzM1", "number": 11443, "title": "[FLINK-14791][coordination] ResourceManager tracks ClusterPartitions", "bodyText": "Based on #10362 (FLINK-14792).\nWith this PR the ResourceManager tracks cluster partitions hosted on task executors, based on the ClusterPartitionReports that are already submitted via TE heartbeats.\nThe entire partition handling logic is encapsulated in a ResourceManagerPartitionTracker; the RM does not really do anything and just informs the tracker about certain events (arrival of a new report, shutdown of a TM, requested release of a partition via the REST API).\nThe tracker maintains the following mappings\na) data set -> TE\nb) TE -> data set\nc) TE -> partitions\nMapping a) is required for the release of partitions via the REST API.\nMapping b) is required for figuring out which data sets may be affected by a TE shutdown.\nMapping c) is required for detecting when all partitions of a partition are being tracked and whether a TE has lost a single partition.\nThe tracker not only tracks partitions but also searches for data sets that have been corrupted by the loss of partitions (e.g.,  because of a TE shutdown). In this case the tracker will issue release calls for the remaining partitions to the hosting task executors.\nNote that ResourceManagerPartitionTracker#listDataSets and  #releastPartitions are unused at this time, but are required for the upcoming REST API.\nThis PR does NOT handle cases where a partition of a cluster partition is never being tracked, for example because the TE died just after the job finished. In this case we will currently never release partitions of the corresponding data set.\nThis will be tackled in a follow-up, and requires setting up a timeout for the maximum duration between the first and last registration of a partition.", "createdAt": "2020-03-18T14:23:06Z", "url": "https://github.com/apache/flink/pull/11443", "merged": true, "mergeCommit": {"oid": "e63c31ba8a28a609e385eb98a874c2abdf72a28e"}, "closed": true, "closedAt": "2020-04-02T15:14:47Z", "author": {"login": "zentol"}, "timelineItems": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcRYCgLgFqTM4MTA0MzY2OQ==", "endCursor": "Y3Vyc29yOnYyOpPPAAABcTt0o4ABqjMxOTI2ODI0NTQ=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzgxMDQzNjY5", "url": "https://github.com/apache/flink/pull/11443#pullrequestreview-381043669", "createdAt": "2020-03-25T10:59:57Z", "commit": {"oid": "0240c441a2ea285e49af3172c2e5829207be011b"}, "state": "COMMENTED", "comments": {"totalCount": 13, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQxMDo1OTo1N1rOF7V2Bg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNlQwODo0MDowOFrOF78SBA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzc2ODE5OA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            public interface ResourceManagerPartitionTrackerFactory {\n          \n          \n            \n            @FunctionalInterface\n          \n          \n            \n            public interface ResourceManagerPartitionTrackerFactory", "url": "https://github.com/apache/flink/pull/11443#discussion_r397768198", "createdAt": "2020-03-25T10:59:57Z", "author": {"login": "azagrebin"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTrackerFactory.java", "diffHunk": "@@ -0,0 +1,25 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.io.network.partition;\n+\n+/**\n+ * Factory for {@link ResourceManagerPartitionTracker}.\n+ */\n+public interface ResourceManagerPartitionTrackerFactory {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0240c441a2ea285e49af3172c2e5829207be011b"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzc2OTQ5OA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            public interface ClusterPartitionReleaser {\n          \n          \n            \n            @FunctionalInterface\n          \n          \n            \n            public interface ClusterPartitionReleaser {", "url": "https://github.com/apache/flink/pull/11443#discussion_r397769498", "createdAt": "2020-03-25T11:02:22Z", "author": {"login": "azagrebin"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ClusterPartitionReleaser.java", "diffHunk": "@@ -0,0 +1,30 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.io.network.partition;\n+\n+import org.apache.flink.runtime.clusterframework.types.ResourceID;\n+import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;\n+\n+import java.util.Set;\n+\n+/**\n+ * Interface for releasing cluster partitions on a task executor.\n+ */\n+public interface ClusterPartitionReleaser {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0240c441a2ea285e49af3172c2e5829207be011b"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg2NTA3OQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            public interface ClusterPartitionReleaser {\n          \n          \n            \n            public interface TaskExecutorClusterPartitionReleaser {\n          \n      \n    \n    \n  \n\nJust if we have also something to release external partitions over shuffle master/ClusterPartitionShuffleClient.", "url": "https://github.com/apache/flink/pull/11443#discussion_r397865079", "createdAt": "2020-03-25T13:46:28Z", "author": {"login": "azagrebin"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ClusterPartitionReleaser.java", "diffHunk": "@@ -0,0 +1,30 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.io.network.partition;\n+\n+import org.apache.flink.runtime.clusterframework.types.ResourceID;\n+import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;\n+\n+import java.util.Set;\n+\n+/**\n+ * Interface for releasing cluster partitions on a task executor.\n+ */\n+public interface ClusterPartitionReleaser {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0240c441a2ea285e49af3172c2e5829207be011b"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg2ODU1Mw==", "bodyText": "a bit strange that this all resides in network package\nI would expect it to be somewhere in org.apache.flink.runtime.resourcemanager. partition.", "url": "https://github.com/apache/flink/pull/11443#discussion_r397868553", "createdAt": "2020-03-25T13:50:55Z", "author": {"login": "azagrebin"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTracker.java", "diffHunk": "@@ -0,0 +1,67 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.io.network.partition;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0240c441a2ea285e49af3172c2e5829207be011b"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODAyNzUzNA==", "bodyText": "do we want to check whether internalReleasePartitions actually issued any releases?\nto avoid dangling futures in partitionReleaseCompletionFutures if dataSetId is not actually tracked at all", "url": "https://github.com/apache/flink/pull/11443#discussion_r398027534", "createdAt": "2020-03-25T17:13:51Z", "author": {"login": "azagrebin"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTrackerImpl.java", "diffHunk": "@@ -0,0 +1,232 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.io.network.partition;\n+\n+import org.apache.flink.runtime.clusterframework.types.ResourceID;\n+import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;\n+import org.apache.flink.runtime.taskexecutor.partition.ClusterPartitionReport;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Default {@link ResourceManagerPartitionTracker} implementation.\n+ *\n+ * <p>Internal tracking info must only be updated upon reception of a {@link ClusterPartitionReport}, as the task\n+ * executor state is the source of truth.\n+ */\n+public class ResourceManagerPartitionTrackerImpl implements ResourceManagerPartitionTracker {\n+\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(ResourceManagerPartitionTrackerImpl.class);\n+\n+\tprivate final Map<ResourceID, Set<IntermediateDataSetID>> taskExecutorToDataSets = new HashMap<>();\n+\tprivate final Map<IntermediateDataSetID, Map<ResourceID, Set<ResultPartitionID>>> dataSetToTaskExecutors = new HashMap<>();\n+\tprivate final Map<IntermediateDataSetID, DataSetMetaInfo> dataSetMetaInfo = new HashMap<>();\n+\tprivate final Map<IntermediateDataSetID, CompletableFuture<Void>> partitionReleaseCompletionFutures = new HashMap<>();\n+\n+\tprivate final ClusterPartitionReleaser clusterPartitionReleaser;\n+\n+\tpublic ResourceManagerPartitionTrackerImpl(ClusterPartitionReleaser clusterPartitionReleaser) {\n+\t\tthis.clusterPartitionReleaser = clusterPartitionReleaser;\n+\t}\n+\n+\t@Override\n+\tpublic void processTaskExecutorClusterPartitionReport(ResourceID taskExecutorId, ClusterPartitionReport clusterPartitionReport) {\n+\t\tPreconditions.checkNotNull(taskExecutorId);\n+\t\tPreconditions.checkNotNull(clusterPartitionReport);\n+\t\tLOG.debug(\"Processing cluster partition report from task executor {}: {}.\", taskExecutorId, clusterPartitionReport);\n+\n+\t\tinternalProcessClusterPartitionReport(taskExecutorId, clusterPartitionReport);\n+\t}\n+\n+\t@Override\n+\tpublic void processTaskExecutorShutdown(ResourceID taskExecutorId) {\n+\t\tPreconditions.checkNotNull(taskExecutorId);\n+\t\tLOG.debug(\"Processing shutdown of task executor {}.\", taskExecutorId);\n+\n+\t\tinternalProcessClusterPartitionReport(taskExecutorId, new ClusterPartitionReport(Collections.emptyList()));\n+\t}\n+\n+\t@Override\n+\tpublic CompletableFuture<Void> releaseClusterPartitions(IntermediateDataSetID dataSetId) {\n+\t\tPreconditions.checkNotNull(dataSetId);\n+\t\tLOG.debug(\"Releasing cluster partitions for data set {}.\", dataSetId);\n+\n+\t\tCompletableFuture<Void> partitionReleaseCompletionFuture = partitionReleaseCompletionFutures.computeIfAbsent(dataSetId, ignored -> new CompletableFuture<>());\n+\t\tinternalReleasePartitions(Collections.singleton(dataSetId));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0240c441a2ea285e49af3172c2e5829207be011b"}, "originalPosition": 82}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODAyODEyNg==", "bodyText": "do we also want to GC the completed future from partitionReleaseCompletionFutures?", "url": "https://github.com/apache/flink/pull/11443#discussion_r398028126", "createdAt": "2020-03-25T17:14:37Z", "author": {"login": "azagrebin"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTrackerImpl.java", "diffHunk": "@@ -0,0 +1,232 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.io.network.partition;\n+\n+import org.apache.flink.runtime.clusterframework.types.ResourceID;\n+import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;\n+import org.apache.flink.runtime.taskexecutor.partition.ClusterPartitionReport;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Default {@link ResourceManagerPartitionTracker} implementation.\n+ *\n+ * <p>Internal tracking info must only be updated upon reception of a {@link ClusterPartitionReport}, as the task\n+ * executor state is the source of truth.\n+ */\n+public class ResourceManagerPartitionTrackerImpl implements ResourceManagerPartitionTracker {\n+\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(ResourceManagerPartitionTrackerImpl.class);\n+\n+\tprivate final Map<ResourceID, Set<IntermediateDataSetID>> taskExecutorToDataSets = new HashMap<>();\n+\tprivate final Map<IntermediateDataSetID, Map<ResourceID, Set<ResultPartitionID>>> dataSetToTaskExecutors = new HashMap<>();\n+\tprivate final Map<IntermediateDataSetID, DataSetMetaInfo> dataSetMetaInfo = new HashMap<>();\n+\tprivate final Map<IntermediateDataSetID, CompletableFuture<Void>> partitionReleaseCompletionFutures = new HashMap<>();\n+\n+\tprivate final ClusterPartitionReleaser clusterPartitionReleaser;\n+\n+\tpublic ResourceManagerPartitionTrackerImpl(ClusterPartitionReleaser clusterPartitionReleaser) {\n+\t\tthis.clusterPartitionReleaser = clusterPartitionReleaser;\n+\t}\n+\n+\t@Override\n+\tpublic void processTaskExecutorClusterPartitionReport(ResourceID taskExecutorId, ClusterPartitionReport clusterPartitionReport) {\n+\t\tPreconditions.checkNotNull(taskExecutorId);\n+\t\tPreconditions.checkNotNull(clusterPartitionReport);\n+\t\tLOG.debug(\"Processing cluster partition report from task executor {}: {}.\", taskExecutorId, clusterPartitionReport);\n+\n+\t\tinternalProcessClusterPartitionReport(taskExecutorId, clusterPartitionReport);\n+\t}\n+\n+\t@Override\n+\tpublic void processTaskExecutorShutdown(ResourceID taskExecutorId) {\n+\t\tPreconditions.checkNotNull(taskExecutorId);\n+\t\tLOG.debug(\"Processing shutdown of task executor {}.\", taskExecutorId);\n+\n+\t\tinternalProcessClusterPartitionReport(taskExecutorId, new ClusterPartitionReport(Collections.emptyList()));\n+\t}\n+\n+\t@Override\n+\tpublic CompletableFuture<Void> releaseClusterPartitions(IntermediateDataSetID dataSetId) {\n+\t\tPreconditions.checkNotNull(dataSetId);\n+\t\tLOG.debug(\"Releasing cluster partitions for data set {}.\", dataSetId);\n+\n+\t\tCompletableFuture<Void> partitionReleaseCompletionFuture = partitionReleaseCompletionFutures.computeIfAbsent(dataSetId, ignored -> new CompletableFuture<>());\n+\t\tinternalReleasePartitions(Collections.singleton(dataSetId));\n+\t\treturn partitionReleaseCompletionFuture;\n+\t}\n+\n+\tprivate void internalProcessClusterPartitionReport(ResourceID taskExecutorId, ClusterPartitionReport clusterPartitionReport) {\n+\t\tfinal Set<IntermediateDataSetID> dataSetsWithLostPartitions = clusterPartitionReport.getEntries().isEmpty()\n+\t\t\t? processEmptyReport(taskExecutorId)\n+\t\t\t: setHostedDataSetsAndCheckCorruption(taskExecutorId, clusterPartitionReport.getEntries());\n+\n+\t\tupdateDataSetMetaData(clusterPartitionReport);\n+\n+\t\tcheckForFullyLostDatasets(dataSetsWithLostPartitions);\n+\n+\t\tinternalReleasePartitions(dataSetsWithLostPartitions);\n+\t}\n+\n+\tprivate void internalReleasePartitions(Set<IntermediateDataSetID> dataSetsToRelease) {\n+\t\tMap<ResourceID, Set<IntermediateDataSetID>> releaseCalls = prepareReleaseCalls(dataSetsToRelease);\n+\t\treleaseCalls.forEach(clusterPartitionReleaser::releaseClusterPartitions);\n+\t}\n+\n+\tprivate Set<IntermediateDataSetID> processEmptyReport(ResourceID taskExecutorId) {\n+\t\tSet<IntermediateDataSetID> previouslyHostedDatasets = taskExecutorToDataSets.remove(taskExecutorId);\n+\t\tif (previouslyHostedDatasets == null) {\n+\t\t\t// default path for task executors that never have any cluster partitions\n+\t\t\tpreviouslyHostedDatasets = Collections.emptySet();\n+\t\t} else {\n+\t\t\tpreviouslyHostedDatasets.forEach(dataSetId -> removeInnerKey(dataSetId, taskExecutorId, dataSetToTaskExecutors));\n+\t\t}\n+\t\treturn previouslyHostedDatasets;\n+\t}\n+\n+\t/**\n+\t * Updates the data sets for which the given task executor is hosting partitions and returns data sets that were\n+\t * corrupted due to a loss of partitions.\n+\t *\n+\t * @param taskExecutorId ID of the hosting TaskExecutor\n+\t * @param reportEntries  IDs of data sets for which partitions are hosted\n+\t * @return corrupted data sets\n+\t */\n+\tprivate Set<IntermediateDataSetID> setHostedDataSetsAndCheckCorruption(ResourceID taskExecutorId, Collection<ClusterPartitionReport.ClusterPartitionReportEntry> reportEntries) {\n+\t\tfinal Set<IntermediateDataSetID> currentlyHostedDatasets = reportEntries\n+\t\t\t.stream()\n+\t\t\t.map(ClusterPartitionReport.ClusterPartitionReportEntry::getDataSetId)\n+\t\t\t.collect(Collectors.toSet());\n+\n+\t\tfinal Set<IntermediateDataSetID> previouslyHostedDataSets = taskExecutorToDataSets.put(\n+\t\t\ttaskExecutorId,\n+\t\t\tcurrentlyHostedDatasets);\n+\n+\t\t// previously tracked data sets may be corrupted since we may be tracking less partitions than before\n+\t\tfinal Set<IntermediateDataSetID> potentiallyCorruptedDataSets = Optional\n+\t\t\t.ofNullable(previouslyHostedDataSets)\n+\t\t\t.orElse(new HashSet<>(0));\n+\n+\t\t// update data set -> task executor mapping and find datasets for which lost a partition\n+\t\treportEntries.forEach(hostedPartition -> {\n+\t\t\tfinal Map<ResourceID, Set<ResultPartitionID>> taskExecutorHosts = dataSetToTaskExecutors.computeIfAbsent(hostedPartition.getDataSetId(), ignored -> new HashMap<>());\n+\t\t\tfinal Set<ResultPartitionID> previouslyHostedPartitions = taskExecutorHosts.put(taskExecutorId, hostedPartition.getHostedPartitions());\n+\n+\t\t\tfinal boolean noPartitionLost = previouslyHostedPartitions == null || hostedPartition.getHostedPartitions().containsAll(previouslyHostedPartitions);\n+\t\t\tif (noPartitionLost) {\n+\t\t\t\tpotentiallyCorruptedDataSets.remove(hostedPartition.getDataSetId());\n+\t\t\t}\n+\t\t});\n+\n+\t\t// now only contains data sets for which a partition is no longer tracked\n+\t\treturn potentiallyCorruptedDataSets;\n+\t}\n+\n+\tprivate void updateDataSetMetaData(ClusterPartitionReport clusterPartitionReport) {\n+\t\t// add meta info for new data sets\n+\t\tclusterPartitionReport.getEntries().forEach(entry ->\n+\t\t\tdataSetMetaInfo.compute(entry.getDataSetId(), (dataSetID, dataSetMetaInfo) -> {\n+\t\t\t\tif (dataSetMetaInfo == null) {\n+\t\t\t\t\treturn new DataSetMetaInfo(entry.getNumTotalPartitions());\n+\t\t\t\t} else {\n+\t\t\t\t\t// double check that the meta data is consistent\n+\t\t\t\t\tPreconditions.checkState(dataSetMetaInfo.getNumTotalPartitions() == entry.getNumTotalPartitions());\n+\t\t\t\t\treturn dataSetMetaInfo;\n+\t\t\t\t}\n+\t\t\t}));\n+\t}\n+\n+\tprivate void checkForFullyLostDatasets(Set<IntermediateDataSetID> dataSetsWithLostPartitions) {\n+\t\tdataSetsWithLostPartitions.forEach(dataSetId -> {\n+\t\t\tif (getHostingTaskExecutors(dataSetId).isEmpty()) {\n+\t\t\t\tLOG.debug(\"There are no longer partitions being tracked for dataset {}.\", dataSetId);\n+\t\t\t\tdataSetMetaInfo.remove(dataSetId);\n+\t\t\t\tOptional.ofNullable(partitionReleaseCompletionFutures.get(dataSetId)).map(future -> future.complete(null));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0240c441a2ea285e49af3172c2e5829207be011b"}, "originalPosition": 171}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODA2MzcxOQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \t\t\t\tint numTrackedPartitions = 0;\n          \n          \n            \n            \t\t\t\tfor (Set<ResultPartitionID> hostedPartitions : taskExecutorToPartitions.values()) {\n          \n          \n            \n            \t\t\t\t\tnumTrackedPartitions += hostedPartitions.size();\n          \n          \n            \n            \t\t\t\t}\n          \n          \n            \n            \t\t\t\tfinal int numTrackedPartitions = taskExecutorToPartitions.values().stream().mapToInt(Set::size).sum();\n          \n      \n    \n    \n  \n\nnit idea, as you seem to like streams :)", "url": "https://github.com/apache/flink/pull/11443#discussion_r398063719", "createdAt": "2020-03-25T18:05:42Z", "author": {"login": "azagrebin"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTrackerImpl.java", "diffHunk": "@@ -0,0 +1,232 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.io.network.partition;\n+\n+import org.apache.flink.runtime.clusterframework.types.ResourceID;\n+import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;\n+import org.apache.flink.runtime.taskexecutor.partition.ClusterPartitionReport;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Default {@link ResourceManagerPartitionTracker} implementation.\n+ *\n+ * <p>Internal tracking info must only be updated upon reception of a {@link ClusterPartitionReport}, as the task\n+ * executor state is the source of truth.\n+ */\n+public class ResourceManagerPartitionTrackerImpl implements ResourceManagerPartitionTracker {\n+\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(ResourceManagerPartitionTrackerImpl.class);\n+\n+\tprivate final Map<ResourceID, Set<IntermediateDataSetID>> taskExecutorToDataSets = new HashMap<>();\n+\tprivate final Map<IntermediateDataSetID, Map<ResourceID, Set<ResultPartitionID>>> dataSetToTaskExecutors = new HashMap<>();\n+\tprivate final Map<IntermediateDataSetID, DataSetMetaInfo> dataSetMetaInfo = new HashMap<>();\n+\tprivate final Map<IntermediateDataSetID, CompletableFuture<Void>> partitionReleaseCompletionFutures = new HashMap<>();\n+\n+\tprivate final ClusterPartitionReleaser clusterPartitionReleaser;\n+\n+\tpublic ResourceManagerPartitionTrackerImpl(ClusterPartitionReleaser clusterPartitionReleaser) {\n+\t\tthis.clusterPartitionReleaser = clusterPartitionReleaser;\n+\t}\n+\n+\t@Override\n+\tpublic void processTaskExecutorClusterPartitionReport(ResourceID taskExecutorId, ClusterPartitionReport clusterPartitionReport) {\n+\t\tPreconditions.checkNotNull(taskExecutorId);\n+\t\tPreconditions.checkNotNull(clusterPartitionReport);\n+\t\tLOG.debug(\"Processing cluster partition report from task executor {}: {}.\", taskExecutorId, clusterPartitionReport);\n+\n+\t\tinternalProcessClusterPartitionReport(taskExecutorId, clusterPartitionReport);\n+\t}\n+\n+\t@Override\n+\tpublic void processTaskExecutorShutdown(ResourceID taskExecutorId) {\n+\t\tPreconditions.checkNotNull(taskExecutorId);\n+\t\tLOG.debug(\"Processing shutdown of task executor {}.\", taskExecutorId);\n+\n+\t\tinternalProcessClusterPartitionReport(taskExecutorId, new ClusterPartitionReport(Collections.emptyList()));\n+\t}\n+\n+\t@Override\n+\tpublic CompletableFuture<Void> releaseClusterPartitions(IntermediateDataSetID dataSetId) {\n+\t\tPreconditions.checkNotNull(dataSetId);\n+\t\tLOG.debug(\"Releasing cluster partitions for data set {}.\", dataSetId);\n+\n+\t\tCompletableFuture<Void> partitionReleaseCompletionFuture = partitionReleaseCompletionFutures.computeIfAbsent(dataSetId, ignored -> new CompletableFuture<>());\n+\t\tinternalReleasePartitions(Collections.singleton(dataSetId));\n+\t\treturn partitionReleaseCompletionFuture;\n+\t}\n+\n+\tprivate void internalProcessClusterPartitionReport(ResourceID taskExecutorId, ClusterPartitionReport clusterPartitionReport) {\n+\t\tfinal Set<IntermediateDataSetID> dataSetsWithLostPartitions = clusterPartitionReport.getEntries().isEmpty()\n+\t\t\t? processEmptyReport(taskExecutorId)\n+\t\t\t: setHostedDataSetsAndCheckCorruption(taskExecutorId, clusterPartitionReport.getEntries());\n+\n+\t\tupdateDataSetMetaData(clusterPartitionReport);\n+\n+\t\tcheckForFullyLostDatasets(dataSetsWithLostPartitions);\n+\n+\t\tinternalReleasePartitions(dataSetsWithLostPartitions);\n+\t}\n+\n+\tprivate void internalReleasePartitions(Set<IntermediateDataSetID> dataSetsToRelease) {\n+\t\tMap<ResourceID, Set<IntermediateDataSetID>> releaseCalls = prepareReleaseCalls(dataSetsToRelease);\n+\t\treleaseCalls.forEach(clusterPartitionReleaser::releaseClusterPartitions);\n+\t}\n+\n+\tprivate Set<IntermediateDataSetID> processEmptyReport(ResourceID taskExecutorId) {\n+\t\tSet<IntermediateDataSetID> previouslyHostedDatasets = taskExecutorToDataSets.remove(taskExecutorId);\n+\t\tif (previouslyHostedDatasets == null) {\n+\t\t\t// default path for task executors that never have any cluster partitions\n+\t\t\tpreviouslyHostedDatasets = Collections.emptySet();\n+\t\t} else {\n+\t\t\tpreviouslyHostedDatasets.forEach(dataSetId -> removeInnerKey(dataSetId, taskExecutorId, dataSetToTaskExecutors));\n+\t\t}\n+\t\treturn previouslyHostedDatasets;\n+\t}\n+\n+\t/**\n+\t * Updates the data sets for which the given task executor is hosting partitions and returns data sets that were\n+\t * corrupted due to a loss of partitions.\n+\t *\n+\t * @param taskExecutorId ID of the hosting TaskExecutor\n+\t * @param reportEntries  IDs of data sets for which partitions are hosted\n+\t * @return corrupted data sets\n+\t */\n+\tprivate Set<IntermediateDataSetID> setHostedDataSetsAndCheckCorruption(ResourceID taskExecutorId, Collection<ClusterPartitionReport.ClusterPartitionReportEntry> reportEntries) {\n+\t\tfinal Set<IntermediateDataSetID> currentlyHostedDatasets = reportEntries\n+\t\t\t.stream()\n+\t\t\t.map(ClusterPartitionReport.ClusterPartitionReportEntry::getDataSetId)\n+\t\t\t.collect(Collectors.toSet());\n+\n+\t\tfinal Set<IntermediateDataSetID> previouslyHostedDataSets = taskExecutorToDataSets.put(\n+\t\t\ttaskExecutorId,\n+\t\t\tcurrentlyHostedDatasets);\n+\n+\t\t// previously tracked data sets may be corrupted since we may be tracking less partitions than before\n+\t\tfinal Set<IntermediateDataSetID> potentiallyCorruptedDataSets = Optional\n+\t\t\t.ofNullable(previouslyHostedDataSets)\n+\t\t\t.orElse(new HashSet<>(0));\n+\n+\t\t// update data set -> task executor mapping and find datasets for which lost a partition\n+\t\treportEntries.forEach(hostedPartition -> {\n+\t\t\tfinal Map<ResourceID, Set<ResultPartitionID>> taskExecutorHosts = dataSetToTaskExecutors.computeIfAbsent(hostedPartition.getDataSetId(), ignored -> new HashMap<>());\n+\t\t\tfinal Set<ResultPartitionID> previouslyHostedPartitions = taskExecutorHosts.put(taskExecutorId, hostedPartition.getHostedPartitions());\n+\n+\t\t\tfinal boolean noPartitionLost = previouslyHostedPartitions == null || hostedPartition.getHostedPartitions().containsAll(previouslyHostedPartitions);\n+\t\t\tif (noPartitionLost) {\n+\t\t\t\tpotentiallyCorruptedDataSets.remove(hostedPartition.getDataSetId());\n+\t\t\t}\n+\t\t});\n+\n+\t\t// now only contains data sets for which a partition is no longer tracked\n+\t\treturn potentiallyCorruptedDataSets;\n+\t}\n+\n+\tprivate void updateDataSetMetaData(ClusterPartitionReport clusterPartitionReport) {\n+\t\t// add meta info for new data sets\n+\t\tclusterPartitionReport.getEntries().forEach(entry ->\n+\t\t\tdataSetMetaInfo.compute(entry.getDataSetId(), (dataSetID, dataSetMetaInfo) -> {\n+\t\t\t\tif (dataSetMetaInfo == null) {\n+\t\t\t\t\treturn new DataSetMetaInfo(entry.getNumTotalPartitions());\n+\t\t\t\t} else {\n+\t\t\t\t\t// double check that the meta data is consistent\n+\t\t\t\t\tPreconditions.checkState(dataSetMetaInfo.getNumTotalPartitions() == entry.getNumTotalPartitions());\n+\t\t\t\t\treturn dataSetMetaInfo;\n+\t\t\t\t}\n+\t\t\t}));\n+\t}\n+\n+\tprivate void checkForFullyLostDatasets(Set<IntermediateDataSetID> dataSetsWithLostPartitions) {\n+\t\tdataSetsWithLostPartitions.forEach(dataSetId -> {\n+\t\t\tif (getHostingTaskExecutors(dataSetId).isEmpty()) {\n+\t\t\t\tLOG.debug(\"There are no longer partitions being tracked for dataset {}.\", dataSetId);\n+\t\t\t\tdataSetMetaInfo.remove(dataSetId);\n+\t\t\t\tOptional.ofNullable(partitionReleaseCompletionFutures.get(dataSetId)).map(future -> future.complete(null));\n+\t\t\t}\n+\t\t});\n+\t}\n+\n+\tprivate Map<ResourceID, Set<IntermediateDataSetID>> prepareReleaseCalls(Set<IntermediateDataSetID> dataSetsToRelease) {\n+\t\tfinal Map<ResourceID, Set<IntermediateDataSetID>> releaseCalls = new HashMap<>();\n+\t\tdataSetsToRelease.forEach(dataSetToRelease -> {\n+\t\t\tfinal Set<ResourceID> hostingTaskExecutors = getHostingTaskExecutors(dataSetToRelease);\n+\t\t\thostingTaskExecutors.forEach(hostingTaskExecutor -> insert(hostingTaskExecutor, dataSetToRelease, releaseCalls));\n+\t\t});\n+\t\treturn releaseCalls;\n+\t}\n+\n+\tprivate Set<ResourceID> getHostingTaskExecutors(IntermediateDataSetID dataSetId) {\n+\t\tPreconditions.checkNotNull(dataSetId);\n+\n+\t\tMap<ResourceID, Set<ResultPartitionID>> trackedPartitions = dataSetToTaskExecutors.get(dataSetId);\n+\t\tif (trackedPartitions == null) {\n+\t\t\treturn Collections.emptySet();\n+\t\t} else {\n+\t\t\treturn trackedPartitions.keySet();\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic Map<IntermediateDataSetID, DataSetMetaInfo> listDataSets() {\n+\t\treturn dataSetMetaInfo.entrySet().stream()\n+\t\t\t.filter(entry -> {\n+\t\t\t\tfinal Map<ResourceID, Set<ResultPartitionID>> taskExecutorToPartitions = dataSetToTaskExecutors.get(entry.getKey());\n+\t\t\t\tPreconditions.checkState(taskExecutorToPartitions != null, \"Have metadata entry for dataset %s, but no partition is tracked.\", entry.getKey());\n+\n+\t\t\t\tint numTrackedPartitions = 0;\n+\t\t\t\tfor (Set<ResultPartitionID> hostedPartitions : taskExecutorToPartitions.values()) {\n+\t\t\t\t\tnumTrackedPartitions += hostedPartitions.size();\n+\t\t\t\t}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0240c441a2ea285e49af3172c2e5829207be011b"}, "originalPosition": 206}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODM2MjYwNA==", "bodyText": "nit: can be private", "url": "https://github.com/apache/flink/pull/11443#discussion_r398362604", "createdAt": "2020-03-26T07:27:47Z", "author": {"login": "azagrebin"}, "path": "flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTrackerImplTest.java", "diffHunk": "@@ -0,0 +1,241 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.io.network.partition;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.runtime.clusterframework.types.ResourceID;\n+import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;\n+import org.apache.flink.runtime.taskexecutor.partition.ClusterPartitionReport;\n+import org.apache.flink.util.TestLogger;\n+\n+import org.junit.Test;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.contains;\n+import static org.hamcrest.Matchers.containsInAnyOrder;\n+import static org.hamcrest.Matchers.hasKey;\n+import static org.hamcrest.collection.IsEmptyCollection.empty;\n+import static org.junit.Assert.assertEquals;\n+\n+/**\n+ * Test for the {@link ResourceManagerPartitionTrackerImpl}.\n+ */\n+public class ResourceManagerPartitionTrackerImplTest extends TestLogger {\n+\n+\tprivate static final ClusterPartitionReport EMPTY_PARTITION_REPORT = new ClusterPartitionReport(Collections.emptySet());\n+\n+\tprivate static final ResourceID TASK_EXECUTOR_ID_1 = ResourceID.generate();\n+\tprivate static final ResourceID TASK_EXECUTOR_ID_2 = ResourceID.generate();\n+\tprivate static final IntermediateDataSetID DATA_SET_ID = new IntermediateDataSetID();\n+\tprivate static final ResultPartitionID PARTITION_ID_1 = new ResultPartitionID();\n+\tprivate static final ResultPartitionID PARTITION_ID_2 = new ResultPartitionID();\n+\n+\t@Test\n+\tpublic void testProcessEmptyClusterPartitionReport() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(TASK_EXECUTOR_ID_1, EMPTY_PARTITION_REPORT);\n+\t\tassertThat(partitionReleaser.releaseCalls, empty());\n+\t}\n+\n+\t/**\n+\t * Verifies that a task executor hosting multiple partitions of a data set receives a release call if a subset of\n+\t * its partitions is lost.\n+\t */\n+\t@Test\n+\tpublic void testReportProcessingWithPartitionLossOnSameTaskExecutor() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1, PARTITION_ID_2));\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_2));\n+\n+\t\tassertThat(partitionReleaser.releaseCalls, contains(Tuple2.of(TASK_EXECUTOR_ID_1, Collections.singleton(DATA_SET_ID))));\n+\t}\n+\n+\t/**\n+\t * Verifies that a task executor hosting partitions of a data set receives a release call if a partition of the\n+\t * data set is lost on another task executor.\n+\t */\n+\t@Test\n+\tpublic void testReportProcessingWithPartitionLossOnOtherTaskExecutor() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1));\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_2,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_2));\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(TASK_EXECUTOR_ID_1, EMPTY_PARTITION_REPORT);\n+\n+\t\tassertThat(partitionReleaser.releaseCalls, contains(Tuple2.of(TASK_EXECUTOR_ID_2, Collections.singleton(DATA_SET_ID))));\n+\t}\n+\n+\t@Test\n+\tpublic void testListDataSetsBasics() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\tassertEquals(0, tracker.listDataSets().size());\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 1, PARTITION_ID_1));\n+\n+\t\tfinal Map<IntermediateDataSetID, DataSetMetaInfo> listing = tracker.listDataSets();\n+\t\tassertThat(listing, hasKey(DATA_SET_ID));\n+\t\tDataSetMetaInfo metaInfo = listing.get(DATA_SET_ID);\n+\t\tassertEquals(1, metaInfo.getNumTotalPartitions());\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tEMPTY_PARTITION_REPORT);\n+\t\tassertEquals(0, tracker.listDataSets().size());\n+\t}\n+\n+\t@Test\n+\tpublic void testListDataSetsMultiplePartitionsOnSingleTaskExecutor() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\t// data set consists of 2 partitions but only 1 is being tracked -> incomplete and should not be listed (yet)\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1));\n+\t\tassertEquals(0, tracker.listDataSets().size());\n+\n+\t\t// start tracking another partitions, but we lost partition 1 so the data set is still incomplete\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_2));\n+\t\tassertEquals(0, tracker.listDataSets().size());\n+\n+\t\t// dataset is considered complete since all partitions are being tracked\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1, PARTITION_ID_2));\n+\t\tfinal Map<IntermediateDataSetID, DataSetMetaInfo> listing = tracker.listDataSets();\n+\t\tassertThat(listing, hasKey(DATA_SET_ID));\n+\n+\t\t// dataset is no longer considered complete since partition 2 was lost\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1));\n+\t\tassertEquals(0, tracker.listDataSets().size());\n+\t}\n+\n+\t@Test\n+\tpublic void testListDataSetsMultiplePartitionsAcrossTaskExecutors() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1));\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_2,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_2));\n+\t\tfinal Map<IntermediateDataSetID, DataSetMetaInfo> listing = tracker.listDataSets();\n+\t\tassertThat(listing, hasKey(DATA_SET_ID));\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tEMPTY_PARTITION_REPORT);\n+\t\tassertEquals(0, tracker.listDataSets().size());\n+\t}\n+\n+\t@Test\n+\tpublic void testReleasePartition() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1));\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_2,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_2));\n+\n+\t\ttracker.releaseClusterPartitions(DATA_SET_ID);\n+\n+\t\tassertThat(partitionReleaser.releaseCalls, containsInAnyOrder(\n+\t\t\tTuple2.of(TASK_EXECUTOR_ID_1, Collections.singleton(DATA_SET_ID)),\n+\t\t\tTuple2.of(TASK_EXECUTOR_ID_2, Collections.singleton(DATA_SET_ID))));\n+\n+\t\t// the data set should still be tracked, since the partition release was not confirmed yet by the task executors\n+\t\tassertThat(tracker.listDataSets().keySet(), contains(DATA_SET_ID));\n+\t}\n+\n+\t@Test\n+\tpublic void testShutdownProcessing() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\ttracker.processTaskExecutorShutdown(TASK_EXECUTOR_ID_1);\n+\t\tassertThat(partitionReleaser.releaseCalls, empty());\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 3, PARTITION_ID_1, PARTITION_ID_2));\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_2,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 3, new ResultPartitionID()));\n+\n+\t\ttracker.processTaskExecutorShutdown(TASK_EXECUTOR_ID_1);\n+\n+\t\tassertThat(partitionReleaser.releaseCalls, contains(Tuple2.of(TASK_EXECUTOR_ID_2, Collections.singleton(DATA_SET_ID))));\n+\t}\n+\n+\tprivate static ClusterPartitionReport createClusterPartitionReport(IntermediateDataSetID dataSetId, int numTotalPartitions, ResultPartitionID... partitionId) {\n+\t\treturn new ClusterPartitionReport(Collections.singletonList(\n+\t\t\tnew ClusterPartitionReport.ClusterPartitionReportEntry(\n+\t\t\t\tdataSetId,\n+\t\t\t\tnew HashSet<>(Arrays.asList(partitionId)),\n+\t\t\t\tnumTotalPartitions)));\n+\t}\n+\n+\tprivate static class TestClusterPartitionReleaser implements ClusterPartitionReleaser {\n+\n+\t\tfinal List<Tuple2<ResourceID, Set<IntermediateDataSetID>>> releaseCalls = new ArrayList<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0240c441a2ea285e49af3172c2e5829207be011b"}, "originalPosition": 234}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODM3MTg5OA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \t\tassertEquals(0, tracker.listDataSets().size());\n          \n          \n            \n            \t\tassertThat(tracker.listDataSets().size(), is(0));\n          \n      \n    \n    \n  \n\nminor: looks we usually do it like this for readability", "url": "https://github.com/apache/flink/pull/11443#discussion_r398371898", "createdAt": "2020-03-26T07:49:13Z", "author": {"login": "azagrebin"}, "path": "flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTrackerImplTest.java", "diffHunk": "@@ -0,0 +1,241 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.io.network.partition;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.runtime.clusterframework.types.ResourceID;\n+import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;\n+import org.apache.flink.runtime.taskexecutor.partition.ClusterPartitionReport;\n+import org.apache.flink.util.TestLogger;\n+\n+import org.junit.Test;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.contains;\n+import static org.hamcrest.Matchers.containsInAnyOrder;\n+import static org.hamcrest.Matchers.hasKey;\n+import static org.hamcrest.collection.IsEmptyCollection.empty;\n+import static org.junit.Assert.assertEquals;\n+\n+/**\n+ * Test for the {@link ResourceManagerPartitionTrackerImpl}.\n+ */\n+public class ResourceManagerPartitionTrackerImplTest extends TestLogger {\n+\n+\tprivate static final ClusterPartitionReport EMPTY_PARTITION_REPORT = new ClusterPartitionReport(Collections.emptySet());\n+\n+\tprivate static final ResourceID TASK_EXECUTOR_ID_1 = ResourceID.generate();\n+\tprivate static final ResourceID TASK_EXECUTOR_ID_2 = ResourceID.generate();\n+\tprivate static final IntermediateDataSetID DATA_SET_ID = new IntermediateDataSetID();\n+\tprivate static final ResultPartitionID PARTITION_ID_1 = new ResultPartitionID();\n+\tprivate static final ResultPartitionID PARTITION_ID_2 = new ResultPartitionID();\n+\n+\t@Test\n+\tpublic void testProcessEmptyClusterPartitionReport() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(TASK_EXECUTOR_ID_1, EMPTY_PARTITION_REPORT);\n+\t\tassertThat(partitionReleaser.releaseCalls, empty());\n+\t}\n+\n+\t/**\n+\t * Verifies that a task executor hosting multiple partitions of a data set receives a release call if a subset of\n+\t * its partitions is lost.\n+\t */\n+\t@Test\n+\tpublic void testReportProcessingWithPartitionLossOnSameTaskExecutor() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1, PARTITION_ID_2));\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_2));\n+\n+\t\tassertThat(partitionReleaser.releaseCalls, contains(Tuple2.of(TASK_EXECUTOR_ID_1, Collections.singleton(DATA_SET_ID))));\n+\t}\n+\n+\t/**\n+\t * Verifies that a task executor hosting partitions of a data set receives a release call if a partition of the\n+\t * data set is lost on another task executor.\n+\t */\n+\t@Test\n+\tpublic void testReportProcessingWithPartitionLossOnOtherTaskExecutor() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1));\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_2,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_2));\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(TASK_EXECUTOR_ID_1, EMPTY_PARTITION_REPORT);\n+\n+\t\tassertThat(partitionReleaser.releaseCalls, contains(Tuple2.of(TASK_EXECUTOR_ID_2, Collections.singleton(DATA_SET_ID))));\n+\t}\n+\n+\t@Test\n+\tpublic void testListDataSetsBasics() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\tassertEquals(0, tracker.listDataSets().size());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0240c441a2ea285e49af3172c2e5829207be011b"}, "originalPosition": 112}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODM4MTU1Mw==", "bodyText": "For this particular component, I would also suggest to test that the freeing of internal maps is checked because their sizes are somewhat side effect of the component in the system. They could be injected in constructor for testing or checked over some other test API. As mentioned in another comment, not sure that e.g. partitionReleaseCompletionFutures is freed properly and we do not accumulate data there over time.\nI was also thinking to use listDataSets for this check but it seems to be tricky.", "url": "https://github.com/apache/flink/pull/11443#discussion_r398381553", "createdAt": "2020-03-26T08:09:40Z", "author": {"login": "azagrebin"}, "path": "flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTrackerImplTest.java", "diffHunk": "@@ -0,0 +1,241 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.io.network.partition;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.runtime.clusterframework.types.ResourceID;\n+import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;\n+import org.apache.flink.runtime.taskexecutor.partition.ClusterPartitionReport;\n+import org.apache.flink.util.TestLogger;\n+\n+import org.junit.Test;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.contains;\n+import static org.hamcrest.Matchers.containsInAnyOrder;\n+import static org.hamcrest.Matchers.hasKey;\n+import static org.hamcrest.collection.IsEmptyCollection.empty;\n+import static org.junit.Assert.assertEquals;\n+\n+/**\n+ * Test for the {@link ResourceManagerPartitionTrackerImpl}.\n+ */\n+public class ResourceManagerPartitionTrackerImplTest extends TestLogger {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0240c441a2ea285e49af3172c2e5829207be011b"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODM4MzUyNQ==", "bodyText": "minor: this test tests 3 transitions:\nincomplete -> incomplete\nincomplete -> complete\ncomplete -> incomplete\nI would prefer 3 test cases\nsome other tests also test multiple things", "url": "https://github.com/apache/flink/pull/11443#discussion_r398383525", "createdAt": "2020-03-26T08:13:42Z", "author": {"login": "azagrebin"}, "path": "flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTrackerImplTest.java", "diffHunk": "@@ -0,0 +1,241 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.io.network.partition;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.runtime.clusterframework.types.ResourceID;\n+import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;\n+import org.apache.flink.runtime.taskexecutor.partition.ClusterPartitionReport;\n+import org.apache.flink.util.TestLogger;\n+\n+import org.junit.Test;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.contains;\n+import static org.hamcrest.Matchers.containsInAnyOrder;\n+import static org.hamcrest.Matchers.hasKey;\n+import static org.hamcrest.collection.IsEmptyCollection.empty;\n+import static org.junit.Assert.assertEquals;\n+\n+/**\n+ * Test for the {@link ResourceManagerPartitionTrackerImpl}.\n+ */\n+public class ResourceManagerPartitionTrackerImplTest extends TestLogger {\n+\n+\tprivate static final ClusterPartitionReport EMPTY_PARTITION_REPORT = new ClusterPartitionReport(Collections.emptySet());\n+\n+\tprivate static final ResourceID TASK_EXECUTOR_ID_1 = ResourceID.generate();\n+\tprivate static final ResourceID TASK_EXECUTOR_ID_2 = ResourceID.generate();\n+\tprivate static final IntermediateDataSetID DATA_SET_ID = new IntermediateDataSetID();\n+\tprivate static final ResultPartitionID PARTITION_ID_1 = new ResultPartitionID();\n+\tprivate static final ResultPartitionID PARTITION_ID_2 = new ResultPartitionID();\n+\n+\t@Test\n+\tpublic void testProcessEmptyClusterPartitionReport() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(TASK_EXECUTOR_ID_1, EMPTY_PARTITION_REPORT);\n+\t\tassertThat(partitionReleaser.releaseCalls, empty());\n+\t}\n+\n+\t/**\n+\t * Verifies that a task executor hosting multiple partitions of a data set receives a release call if a subset of\n+\t * its partitions is lost.\n+\t */\n+\t@Test\n+\tpublic void testReportProcessingWithPartitionLossOnSameTaskExecutor() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1, PARTITION_ID_2));\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_2));\n+\n+\t\tassertThat(partitionReleaser.releaseCalls, contains(Tuple2.of(TASK_EXECUTOR_ID_1, Collections.singleton(DATA_SET_ID))));\n+\t}\n+\n+\t/**\n+\t * Verifies that a task executor hosting partitions of a data set receives a release call if a partition of the\n+\t * data set is lost on another task executor.\n+\t */\n+\t@Test\n+\tpublic void testReportProcessingWithPartitionLossOnOtherTaskExecutor() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1));\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_2,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_2));\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(TASK_EXECUTOR_ID_1, EMPTY_PARTITION_REPORT);\n+\n+\t\tassertThat(partitionReleaser.releaseCalls, contains(Tuple2.of(TASK_EXECUTOR_ID_2, Collections.singleton(DATA_SET_ID))));\n+\t}\n+\n+\t@Test\n+\tpublic void testListDataSetsBasics() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\tassertEquals(0, tracker.listDataSets().size());\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 1, PARTITION_ID_1));\n+\n+\t\tfinal Map<IntermediateDataSetID, DataSetMetaInfo> listing = tracker.listDataSets();\n+\t\tassertThat(listing, hasKey(DATA_SET_ID));\n+\t\tDataSetMetaInfo metaInfo = listing.get(DATA_SET_ID);\n+\t\tassertEquals(1, metaInfo.getNumTotalPartitions());\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tEMPTY_PARTITION_REPORT);\n+\t\tassertEquals(0, tracker.listDataSets().size());\n+\t}\n+\n+\t@Test\n+\tpublic void testListDataSetsMultiplePartitionsOnSingleTaskExecutor() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0240c441a2ea285e49af3172c2e5829207be011b"}, "originalPosition": 130}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODM4OTg2OA==", "bodyText": "Just an idea, if we init partitionReleaser/tracker in test setup and factor out\ntracker.processTaskExecutorClusterPartitionReport(TASK_EXECUTOR_ID_1, createClusterPartitionReport(..\ninto e.g. report(TASK_EXECUTOR_ID_1, DATA_SET_ID, 2, PARTITION_ID_1, ..)\nthis would reduce code size and simplify writing tests with duplicated setup but testing one thing.", "url": "https://github.com/apache/flink/pull/11443#discussion_r398389868", "createdAt": "2020-03-26T08:25:29Z", "author": {"login": "azagrebin"}, "path": "flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTrackerImplTest.java", "diffHunk": "@@ -0,0 +1,241 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.io.network.partition;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.runtime.clusterframework.types.ResourceID;\n+import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;\n+import org.apache.flink.runtime.taskexecutor.partition.ClusterPartitionReport;\n+import org.apache.flink.util.TestLogger;\n+\n+import org.junit.Test;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.contains;\n+import static org.hamcrest.Matchers.containsInAnyOrder;\n+import static org.hamcrest.Matchers.hasKey;\n+import static org.hamcrest.collection.IsEmptyCollection.empty;\n+import static org.junit.Assert.assertEquals;\n+\n+/**\n+ * Test for the {@link ResourceManagerPartitionTrackerImpl}.\n+ */\n+public class ResourceManagerPartitionTrackerImplTest extends TestLogger {\n+\n+\tprivate static final ClusterPartitionReport EMPTY_PARTITION_REPORT = new ClusterPartitionReport(Collections.emptySet());\n+\n+\tprivate static final ResourceID TASK_EXECUTOR_ID_1 = ResourceID.generate();\n+\tprivate static final ResourceID TASK_EXECUTOR_ID_2 = ResourceID.generate();\n+\tprivate static final IntermediateDataSetID DATA_SET_ID = new IntermediateDataSetID();\n+\tprivate static final ResultPartitionID PARTITION_ID_1 = new ResultPartitionID();\n+\tprivate static final ResultPartitionID PARTITION_ID_2 = new ResultPartitionID();\n+\n+\t@Test\n+\tpublic void testProcessEmptyClusterPartitionReport() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(TASK_EXECUTOR_ID_1, EMPTY_PARTITION_REPORT);\n+\t\tassertThat(partitionReleaser.releaseCalls, empty());\n+\t}\n+\n+\t/**\n+\t * Verifies that a task executor hosting multiple partitions of a data set receives a release call if a subset of\n+\t * its partitions is lost.\n+\t */\n+\t@Test\n+\tpublic void testReportProcessingWithPartitionLossOnSameTaskExecutor() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1, PARTITION_ID_2));\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_2));\n+\n+\t\tassertThat(partitionReleaser.releaseCalls, contains(Tuple2.of(TASK_EXECUTOR_ID_1, Collections.singleton(DATA_SET_ID))));\n+\t}\n+\n+\t/**\n+\t * Verifies that a task executor hosting partitions of a data set receives a release call if a partition of the\n+\t * data set is lost on another task executor.\n+\t */\n+\t@Test\n+\tpublic void testReportProcessingWithPartitionLossOnOtherTaskExecutor() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1));\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_2,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_2));\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(TASK_EXECUTOR_ID_1, EMPTY_PARTITION_REPORT);\n+\n+\t\tassertThat(partitionReleaser.releaseCalls, contains(Tuple2.of(TASK_EXECUTOR_ID_2, Collections.singleton(DATA_SET_ID))));\n+\t}\n+\n+\t@Test\n+\tpublic void testListDataSetsBasics() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\tassertEquals(0, tracker.listDataSets().size());\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 1, PARTITION_ID_1));\n+\n+\t\tfinal Map<IntermediateDataSetID, DataSetMetaInfo> listing = tracker.listDataSets();\n+\t\tassertThat(listing, hasKey(DATA_SET_ID));\n+\t\tDataSetMetaInfo metaInfo = listing.get(DATA_SET_ID);\n+\t\tassertEquals(1, metaInfo.getNumTotalPartitions());\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tEMPTY_PARTITION_REPORT);\n+\t\tassertEquals(0, tracker.listDataSets().size());\n+\t}\n+\n+\t@Test\n+\tpublic void testListDataSetsMultiplePartitionsOnSingleTaskExecutor() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODM4MzUyNQ=="}, "originalCommit": {"oid": "0240c441a2ea285e49af3172c2e5829207be011b"}, "originalPosition": 130}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODM5Nzk1Ng==", "bodyText": "Could we reuse some code from ResourceManagerTest?", "url": "https://github.com/apache/flink/pull/11443#discussion_r398397956", "createdAt": "2020-03-26T08:40:08Z", "author": {"login": "azagrebin"}, "path": "flink-runtime/src/test/java/org/apache/flink/runtime/resourcemanager/ResourceManagerPartitionLifecycleTest.java", "diffHunk": "@@ -0,0 +1,247 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.resourcemanager;\n+\n+import org.apache.flink.api.common.time.Time;\n+import org.apache.flink.runtime.clusterframework.types.ResourceID;\n+import org.apache.flink.runtime.clusterframework.types.ResourceProfile;\n+import org.apache.flink.runtime.heartbeat.HeartbeatServices;\n+import org.apache.flink.runtime.highavailability.TestingHighAvailabilityServices;\n+import org.apache.flink.runtime.instance.HardwareDescription;\n+import org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl;\n+import org.apache.flink.runtime.io.network.partition.ResultPartitionID;\n+import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;\n+import org.apache.flink.runtime.leaderelection.TestingLeaderElectionService;\n+import org.apache.flink.runtime.metrics.groups.UnregisteredMetricGroups;\n+import org.apache.flink.runtime.registration.RegistrationResponse;\n+import org.apache.flink.runtime.resourcemanager.slotmanager.SlotManager;\n+import org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerBuilder;\n+import org.apache.flink.runtime.rpc.RpcUtils;\n+import org.apache.flink.runtime.rpc.TestingRpcService;\n+import org.apache.flink.runtime.taskexecutor.SlotReport;\n+import org.apache.flink.runtime.taskexecutor.TaskExecutorGateway;\n+import org.apache.flink.runtime.taskexecutor.TaskExecutorHeartbeatPayload;\n+import org.apache.flink.runtime.taskexecutor.TestingTaskExecutorGatewayBuilder;\n+import org.apache.flink.runtime.taskexecutor.partition.ClusterPartitionReport;\n+import org.apache.flink.runtime.testingUtils.TestingUtils;\n+import org.apache.flink.runtime.util.TestingFatalErrorHandler;\n+import org.apache.flink.util.TestLogger;\n+\n+import org.junit.After;\n+import org.junit.AfterClass;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.TimeUnit;\n+\n+import static org.hamcrest.Matchers.contains;\n+import static org.hamcrest.Matchers.instanceOf;\n+import static org.junit.Assert.assertThat;\n+\n+/**\n+ * Tests for the partition-lifecycle logic in the {@link ResourceManager}.\n+ */\n+public class ResourceManagerPartitionLifecycleTest extends TestLogger {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0240c441a2ea285e49af3172c2e5829207be011b"}, "originalPosition": 66}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8c735174c9681bdd42a78b5e1df838d4cdc0a06d", "author": {"user": {"login": "zentol", "name": "Chesnay Schepler"}}, "url": "https://github.com/apache/flink/commit/8c735174c9681bdd42a78b5e1df838d4cdc0a06d", "committedDate": "2020-03-30T08:07:18Z", "message": "[hotfix][coordination] Add sanity checks to ClusterPartitionReportEntry"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "0240c441a2ea285e49af3172c2e5829207be011b", "author": {"user": {"login": "zentol", "name": "Chesnay Schepler"}}, "url": "https://github.com/apache/flink/commit/0240c441a2ea285e49af3172c2e5829207be011b", "committedDate": "2020-03-18T13:04:40Z", "message": "[FLINK-14791][coordination] ResourceManager tracks ClusterPartitions"}, "afterCommit": {"oid": "df635bda88e745a98993533481f9a343603bf919", "author": {"user": {"login": "zentol", "name": "Chesnay Schepler"}}, "url": "https://github.com/apache/flink/commit/df635bda88e745a98993533481f9a343603bf919", "committedDate": "2020-03-30T08:07:19Z", "message": "[FLINK-14791][coordination] ResourceManager tracks ClusterPartitions"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "3e149786a3b54c141ecd14b99d98062d31f56bb4", "author": {"user": {"login": "zentol", "name": "Chesnay Schepler"}}, "url": "https://github.com/apache/flink/commit/3e149786a3b54c141ecd14b99d98062d31f56bb4", "committedDate": "2020-03-30T10:11:57Z", "message": "address comments"}, "afterCommit": {"oid": "14b3f8a73814fa648afc42be527a565821a262a8", "author": {"user": {"login": "zentol", "name": "Chesnay Schepler"}}, "url": "https://github.com/apache/flink/commit/14b3f8a73814fa648afc42be527a565821a262a8", "committedDate": "2020-03-30T11:11:04Z", "message": "[TMP] add missing timeout"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "38a8e1762445f4a2e521a0e2bc5459bb9dc3dc72", "author": {"user": {"login": "zentol", "name": "Chesnay Schepler"}}, "url": "https://github.com/apache/flink/commit/38a8e1762445f4a2e521a0e2bc5459bb9dc3dc72", "committedDate": "2020-03-30T11:26:51Z", "message": "simplify reporting"}, "afterCommit": {"oid": "8a86aa89a221085749362a508718bd4959ac8190", "author": {"user": {"login": "zentol", "name": "Chesnay Schepler"}}, "url": "https://github.com/apache/flink/commit/8a86aa89a221085749362a508718bd4959ac8190", "committedDate": "2020-03-30T11:41:17Z", "message": "simplify reporting"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg1MzQ2MTk0", "url": "https://github.com/apache/flink/pull/11443#pullrequestreview-385346194", "createdAt": "2020-04-01T07:58:08Z", "commit": {"oid": "8a86aa89a221085749362a508718bd4959ac8190"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "8a86aa89a221085749362a508718bd4959ac8190", "author": {"user": {"login": "zentol", "name": "Chesnay Schepler"}}, "url": "https://github.com/apache/flink/commit/8a86aa89a221085749362a508718bd4959ac8190", "committedDate": "2020-03-30T11:41:17Z", "message": "simplify reporting"}, "afterCommit": {"oid": "998a1c6d2107addf7bfafd0114b2f696837134aa", "author": {"user": {"login": "zentol", "name": "Chesnay Schepler"}}, "url": "https://github.com/apache/flink/commit/998a1c6d2107addf7bfafd0114b2f696837134aa", "committedDate": "2020-04-02T10:05:25Z", "message": "[FLINK-14791][coordination] ResourceManager tracks ClusterPartitions"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c04e7d39c3d7b3ba8e6678d8bd193f7d7ce5405f", "author": {"user": {"login": "zentol", "name": "Chesnay Schepler"}}, "url": "https://github.com/apache/flink/commit/c04e7d39c3d7b3ba8e6678d8bd193f7d7ce5405f", "committedDate": "2020-04-02T15:13:35Z", "message": "[FLINK-14791][coordination] ResourceManager tracks ClusterPartitions"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "998a1c6d2107addf7bfafd0114b2f696837134aa", "author": {"user": {"login": "zentol", "name": "Chesnay Schepler"}}, "url": "https://github.com/apache/flink/commit/998a1c6d2107addf7bfafd0114b2f696837134aa", "committedDate": "2020-04-02T10:05:25Z", "message": "[FLINK-14791][coordination] ResourceManager tracks ClusterPartitions"}, "afterCommit": {"oid": "c04e7d39c3d7b3ba8e6678d8bd193f7d7ce5405f", "author": {"user": {"login": "zentol", "name": "Chesnay Schepler"}}, "url": "https://github.com/apache/flink/commit/c04e7d39c3d7b3ba8e6678d8bd193f7d7ce5405f", "committedDate": "2020-04-02T15:13:35Z", "message": "[FLINK-14791][coordination] ResourceManager tracks ClusterPartitions"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2873, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}