{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDA1MDMwMDcx", "number": 11791, "reviewThreads": {"totalCount": 15, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQwMzo0MjozNVrOD1kwkg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQwMTo1Mzo0N1rOD2-mtQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU3NTAzMzc4OnYy", "diffSide": "RIGHT", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java", "isResolved": true, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQwMzo0MjozNVrOGLFAew==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQwNzo1OToxNFrOGMArIA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDI2OTU2Mw==", "bodyText": "We usually put the properties key in the connector validator instead of the SqlNode, i think.", "url": "https://github.com/apache/flink/pull/11791#discussion_r414269563", "createdAt": "2020-04-24T03:42:35Z", "author": {"login": "danny0405"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java", "diffHunk": "@@ -247,7 +253,10 @@ public CatalogDatabase getDatabase(String databaseName) throws DatabaseNotExistE\n \n \t\tMap<String, String> properties = hiveDatabase.getParameters();\n \n-\t\tproperties.put(HiveCatalogConfig.DATABASE_LOCATION_URI, hiveDatabase.getLocationUri());\n+\t\tboolean isGeneric = getObjectIsGeneric(properties);\n+\t\tif (!isGeneric) {\n+\t\t\tproperties.put(SqlCreateHiveDatabase.DATABASE_LOCATION_URI, hiveDatabase.getLocationUri());\n+\t\t}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6359340c0b5b01923d7ac2824b9eb1071e0f2f37"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDMxOTU0Ng==", "bodyText": "I put the property keys in SqlNode because the SqlNode has to add these extra properties for extended Hive semantics.", "url": "https://github.com/apache/flink/pull/11791#discussion_r414319546", "createdAt": "2020-04-24T06:10:56Z", "author": {"login": "lirui-apache"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java", "diffHunk": "@@ -247,7 +253,10 @@ public CatalogDatabase getDatabase(String databaseName) throws DatabaseNotExistE\n \n \t\tMap<String, String> properties = hiveDatabase.getParameters();\n \n-\t\tproperties.put(HiveCatalogConfig.DATABASE_LOCATION_URI, hiveDatabase.getLocationUri());\n+\t\tboolean isGeneric = getObjectIsGeneric(properties);\n+\t\tif (!isGeneric) {\n+\t\t\tproperties.put(SqlCreateHiveDatabase.DATABASE_LOCATION_URI, hiveDatabase.getLocationUri());\n+\t\t}", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDI2OTU2Mw=="}, "originalCommit": {"oid": "6359340c0b5b01923d7ac2824b9eb1071e0f2f37"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDk1NjY2MA==", "bodyText": "But generally, the Catalog code should not see the SqlNodes, because the former belongs to the planner, the later belongs to parser.", "url": "https://github.com/apache/flink/pull/11791#discussion_r414956660", "createdAt": "2020-04-25T02:30:17Z", "author": {"login": "danny0405"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java", "diffHunk": "@@ -247,7 +253,10 @@ public CatalogDatabase getDatabase(String databaseName) throws DatabaseNotExistE\n \n \t\tMap<String, String> properties = hiveDatabase.getParameters();\n \n-\t\tproperties.put(HiveCatalogConfig.DATABASE_LOCATION_URI, hiveDatabase.getLocationUri());\n+\t\tboolean isGeneric = getObjectIsGeneric(properties);\n+\t\tif (!isGeneric) {\n+\t\t\tproperties.put(SqlCreateHiveDatabase.DATABASE_LOCATION_URI, hiveDatabase.getLocationUri());\n+\t\t}", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDI2OTU2Mw=="}, "originalCommit": {"oid": "6359340c0b5b01923d7ac2824b9eb1071e0f2f37"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTAyODg1OQ==", "bodyText": "Both SqlNode and HiveCatalog need to access these properties. I can't put them in the hive connector because hive connector depends on sql-parser and not the other way around. So any suggestions where they should go?", "url": "https://github.com/apache/flink/pull/11791#discussion_r415028859", "createdAt": "2020-04-25T10:00:25Z", "author": {"login": "lirui-apache"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java", "diffHunk": "@@ -247,7 +253,10 @@ public CatalogDatabase getDatabase(String databaseName) throws DatabaseNotExistE\n \n \t\tMap<String, String> properties = hiveDatabase.getParameters();\n \n-\t\tproperties.put(HiveCatalogConfig.DATABASE_LOCATION_URI, hiveDatabase.getLocationUri());\n+\t\tboolean isGeneric = getObjectIsGeneric(properties);\n+\t\tif (!isGeneric) {\n+\t\t\tproperties.put(SqlCreateHiveDatabase.DATABASE_LOCATION_URI, hiveDatabase.getLocationUri());\n+\t\t}", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDI2OTU2Mw=="}, "originalCommit": {"oid": "6359340c0b5b01923d7ac2824b9eb1071e0f2f37"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTI0NzEzNg==", "bodyText": "After another think i think it is okey, because we passes around the Hive specific properties through the SqlNode which is the root cause i think.\nWe can have a refactor when we really have a new planner.", "url": "https://github.com/apache/flink/pull/11791#discussion_r415247136", "createdAt": "2020-04-26T07:59:14Z", "author": {"login": "danny0405"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java", "diffHunk": "@@ -247,7 +253,10 @@ public CatalogDatabase getDatabase(String databaseName) throws DatabaseNotExistE\n \n \t\tMap<String, String> properties = hiveDatabase.getParameters();\n \n-\t\tproperties.put(HiveCatalogConfig.DATABASE_LOCATION_URI, hiveDatabase.getLocationUri());\n+\t\tboolean isGeneric = getObjectIsGeneric(properties);\n+\t\tif (!isGeneric) {\n+\t\t\tproperties.put(SqlCreateHiveDatabase.DATABASE_LOCATION_URI, hiveDatabase.getLocationUri());\n+\t\t}", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDI2OTU2Mw=="}, "originalCommit": {"oid": "6359340c0b5b01923d7ac2824b9eb1071e0f2f37"}, "originalPosition": 28}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU3NTI5NzQ1OnYy", "diffSide": "RIGHT", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java", "isResolved": true, "comments": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQwNTozMzoyMlrOGLHMCQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQwODowMDo0MVrOGMAr-w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDMwNTI4OQ==", "bodyText": "How about name it getObjectIsGenericDefaultTrue", "url": "https://github.com/apache/flink/pull/11791#discussion_r414305289", "createdAt": "2020-04-24T05:33:22Z", "author": {"login": "danny0405"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java", "diffHunk": "@@ -1403,4 +1446,29 @@ public CatalogColumnStatistics getPartitionColumnStatistics(ObjectPath tablePath\n \t\t}\n \t}\n \n+\tprivate static boolean createObjectIsGeneric(Map<String, String> properties) {\n+\t\t// When creating an object, a hive object needs explicitly have a key is_generic = false", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6359340c0b5b01923d7ac2824b9eb1071e0f2f37"}, "originalPosition": 166}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDMyMDk0OQ==", "bodyText": "It's not just about the default value. For example, when creating an object and the is_generic key is missing, we'll add is_generic=false to the properties.\nSince the logic is different based on whether we're creating or retrieving an object, I think it's better to reflect that in the method names.", "url": "https://github.com/apache/flink/pull/11791#discussion_r414320949", "createdAt": "2020-04-24T06:14:26Z", "author": {"login": "lirui-apache"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java", "diffHunk": "@@ -1403,4 +1446,29 @@ public CatalogColumnStatistics getPartitionColumnStatistics(ObjectPath tablePath\n \t\t}\n \t}\n \n+\tprivate static boolean createObjectIsGeneric(Map<String, String> properties) {\n+\t\t// When creating an object, a hive object needs explicitly have a key is_generic = false", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDMwNTI4OQ=="}, "originalCommit": {"oid": "6359340c0b5b01923d7ac2824b9eb1071e0f2f37"}, "originalPosition": 166}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDQ1NDAzMQ==", "bodyText": "The implementation is not really create the object but only append a property there.", "url": "https://github.com/apache/flink/pull/11791#discussion_r414454031", "createdAt": "2020-04-24T10:03:00Z", "author": {"login": "danny0405"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java", "diffHunk": "@@ -1403,4 +1446,29 @@ public CatalogColumnStatistics getPartitionColumnStatistics(ObjectPath tablePath\n \t\t}\n \t}\n \n+\tprivate static boolean createObjectIsGeneric(Map<String, String> properties) {\n+\t\t// When creating an object, a hive object needs explicitly have a key is_generic = false", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDMwNTI4OQ=="}, "originalCommit": {"oid": "6359340c0b5b01923d7ac2824b9eb1071e0f2f37"}, "originalPosition": 166}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDU3MDIzOA==", "bodyText": "Sorry to hit the \"resolve\" button by mistake...\nYou're right but my point is we should differentiate whether we're creating or getting an object here. E.g. createObjectIsGeneric should be used in \"create\" methods like createTable or createDatabase, while getObjectIsGeneric should be used in \"get\" methods like getTable or getDatabase. It's clearer if the method names carry such information.", "url": "https://github.com/apache/flink/pull/11791#discussion_r414570238", "createdAt": "2020-04-24T13:20:37Z", "author": {"login": "lirui-apache"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java", "diffHunk": "@@ -1403,4 +1446,29 @@ public CatalogColumnStatistics getPartitionColumnStatistics(ObjectPath tablePath\n \t\t}\n \t}\n \n+\tprivate static boolean createObjectIsGeneric(Map<String, String> properties) {\n+\t\t// When creating an object, a hive object needs explicitly have a key is_generic = false", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDMwNTI4OQ=="}, "originalCommit": {"oid": "6359340c0b5b01923d7ac2824b9eb1071e0f2f37"}, "originalPosition": 166}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDk1Njk2OA==", "bodyText": "I think each method should take care itself, if it only appends a property item, how about name it \"appendIsGenericIfNecessary\" or something like that ?", "url": "https://github.com/apache/flink/pull/11791#discussion_r414956968", "createdAt": "2020-04-25T02:31:56Z", "author": {"login": "danny0405"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java", "diffHunk": "@@ -1403,4 +1446,29 @@ public CatalogColumnStatistics getPartitionColumnStatistics(ObjectPath tablePath\n \t\t}\n \t}\n \n+\tprivate static boolean createObjectIsGeneric(Map<String, String> properties) {\n+\t\t// When creating an object, a hive object needs explicitly have a key is_generic = false", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDMwNTI4OQ=="}, "originalCommit": {"oid": "6359340c0b5b01923d7ac2824b9eb1071e0f2f37"}, "originalPosition": 166}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTAzMDI4MQ==", "bodyText": "Having \"create\" or \"get\" in the method names are more friendly to the method caller. The caller doesn't need to know whether the default value should be true or false, or whether a default value should be appended if missing. Instead, the caller just need to choose one method according to whether it's creating or getting an object.\nPerhaps we can name them as isGenericForCreate and isGenericForGet? So that it's clear the method doesn't really create an object, but just to decide whether an object is generic in different scenarios. Does that sound better to you?", "url": "https://github.com/apache/flink/pull/11791#discussion_r415030281", "createdAt": "2020-04-25T10:09:10Z", "author": {"login": "lirui-apache"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java", "diffHunk": "@@ -1403,4 +1446,29 @@ public CatalogColumnStatistics getPartitionColumnStatistics(ObjectPath tablePath\n \t\t}\n \t}\n \n+\tprivate static boolean createObjectIsGeneric(Map<String, String> properties) {\n+\t\t// When creating an object, a hive object needs explicitly have a key is_generic = false", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDMwNTI4OQ=="}, "originalCommit": {"oid": "6359340c0b5b01923d7ac2824b9eb1071e0f2f37"}, "originalPosition": 166}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTIwMTExMg==", "bodyText": "I am with isGenericForCreate and isGenericForGet. -1 for createObjectIsGeneric and getObjectIsGeneric", "url": "https://github.com/apache/flink/pull/11791#discussion_r415201112", "createdAt": "2020-04-26T03:17:10Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java", "diffHunk": "@@ -1403,4 +1446,29 @@ public CatalogColumnStatistics getPartitionColumnStatistics(ObjectPath tablePath\n \t\t}\n \t}\n \n+\tprivate static boolean createObjectIsGeneric(Map<String, String> properties) {\n+\t\t// When creating an object, a hive object needs explicitly have a key is_generic = false", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDMwNTI4OQ=="}, "originalCommit": {"oid": "6359340c0b5b01923d7ac2824b9eb1071e0f2f37"}, "originalPosition": 166}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTI0NzM1NQ==", "bodyText": "I'm also fine with name isGenericForCreate and isGenericForGet.", "url": "https://github.com/apache/flink/pull/11791#discussion_r415247355", "createdAt": "2020-04-26T08:00:41Z", "author": {"login": "danny0405"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java", "diffHunk": "@@ -1403,4 +1446,29 @@ public CatalogColumnStatistics getPartitionColumnStatistics(ObjectPath tablePath\n \t\t}\n \t}\n \n+\tprivate static boolean createObjectIsGeneric(Map<String, String> properties) {\n+\t\t// When creating an object, a hive object needs explicitly have a key is_generic = false", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDMwNTI4OQ=="}, "originalCommit": {"oid": "6359340c0b5b01923d7ac2824b9eb1071e0f2f37"}, "originalPosition": 166}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU3NTI5ODA3OnYy", "diffSide": "RIGHT", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQwNTozMzozOFrOGLHMYw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQwMzoxNTo1MFrOGL92YA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDMwNTM3OQ==", "bodyText": "How about name it getObjectIsGenericDefaultFalse", "url": "https://github.com/apache/flink/pull/11791#discussion_r414305379", "createdAt": "2020-04-24T05:33:38Z", "author": {"login": "danny0405"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java", "diffHunk": "@@ -1403,4 +1446,29 @@ public CatalogColumnStatistics getPartitionColumnStatistics(ObjectPath tablePath\n \t\t}\n \t}\n \n+\tprivate static boolean createObjectIsGeneric(Map<String, String> properties) {\n+\t\t// When creating an object, a hive object needs explicitly have a key is_generic = false\n+\t\t// otherwise, this is a generic object if 1) the key is missing 2) is_generic = true\n+\t\t// this is opposite to reading an object. See getObjectIsGeneric().\n+\t\tif (properties == null) {\n+\t\t\treturn true;\n+\t\t}\n+\t\tboolean isGeneric;\n+\t\tif (!properties.containsKey(CatalogConfig.IS_GENERIC)) {\n+\t\t\t// must be a generic object\n+\t\t\tisGeneric = true;\n+\t\t\tproperties.put(CatalogConfig.IS_GENERIC, String.valueOf(true));\n+\t\t} else {\n+\t\t\tisGeneric = Boolean.parseBoolean(properties.get(CatalogConfig.IS_GENERIC));\n+\t\t}\n+\t\treturn isGeneric;\n+\t}\n+\n+\tprivate static boolean getObjectIsGeneric(Map<String, String> properties) {\n+\t\t// When retrieving an object, a generic object needs explicitly have a key is_generic = true", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6359340c0b5b01923d7ac2824b9eb1071e0f2f37"}, "originalPosition": 184}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTIwMDg2NA==", "bodyText": "+1\nThere is two default value between getXX and createXX in catalog.", "url": "https://github.com/apache/flink/pull/11791#discussion_r415200864", "createdAt": "2020-04-26T03:15:50Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java", "diffHunk": "@@ -1403,4 +1446,29 @@ public CatalogColumnStatistics getPartitionColumnStatistics(ObjectPath tablePath\n \t\t}\n \t}\n \n+\tprivate static boolean createObjectIsGeneric(Map<String, String> properties) {\n+\t\t// When creating an object, a hive object needs explicitly have a key is_generic = false\n+\t\t// otherwise, this is a generic object if 1) the key is missing 2) is_generic = true\n+\t\t// this is opposite to reading an object. See getObjectIsGeneric().\n+\t\tif (properties == null) {\n+\t\t\treturn true;\n+\t\t}\n+\t\tboolean isGeneric;\n+\t\tif (!properties.containsKey(CatalogConfig.IS_GENERIC)) {\n+\t\t\t// must be a generic object\n+\t\t\tisGeneric = true;\n+\t\t\tproperties.put(CatalogConfig.IS_GENERIC, String.valueOf(true));\n+\t\t} else {\n+\t\t\tisGeneric = Boolean.parseBoolean(properties.get(CatalogConfig.IS_GENERIC));\n+\t\t}\n+\t\treturn isGeneric;\n+\t}\n+\n+\tprivate static boolean getObjectIsGeneric(Map<String, String> properties) {\n+\t\t// When retrieving an object, a generic object needs explicitly have a key is_generic = true", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDMwNTM3OQ=="}, "originalCommit": {"oid": "6359340c0b5b01923d7ac2824b9eb1071e0f2f37"}, "originalPosition": 184}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU3NTMxNzk4OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-sql-parser/src/main/codegen-hive/data/Parser.tdd", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQwNTo0MToyMFrOGLHW4Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQwNTo0MToyMFrOGLHW4Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDMwODA2NQ==", "bodyText": "Remove the useless imports.", "url": "https://github.com/apache/flink/pull/11791#discussion_r414308065", "createdAt": "2020-04-24T05:41:20Z", "author": {"login": "danny0405"}, "path": "flink-table/flink-sql-parser/src/main/codegen-hive/data/Parser.tdd", "diffHunk": "@@ -0,0 +1,547 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+# http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+{\n+  # Generated parser implementation package and class name.\n+  package: \"org.apache.flink.sql.parser.impl\",\n+  class: \"FlinkHiveSqlParserImpl\",\n+\n+  # List of additional classes and packages to import.\n+  # Example. \"org.apache.calcite.sql.*\", \"java.util.List\".\n+  # Please keep the import classes in alphabetical order if new class is added.\n+  imports: [\n+    \"org.apache.flink.sql.parser.ddl.hive.HiveDDLUtils\"\n+    \"org.apache.flink.sql.parser.ddl.hive.SqlAlterHiveDatabaseLocation\"\n+    \"org.apache.flink.sql.parser.ddl.hive.SqlAlterHiveDatabaseOwner\"\n+    \"org.apache.flink.sql.parser.ddl.hive.SqlAlterHiveDatabaseProps\"\n+    \"org.apache.flink.sql.parser.ddl.hive.SqlCreateHiveDatabase\"\n+    \"org.apache.flink.sql.parser.ddl.SqlAlterDatabase\"\n+    \"org.apache.flink.sql.parser.ddl.SqlAlterTable\"\n+    \"org.apache.flink.sql.parser.ddl.SqlAlterTableProperties\"\n+    \"org.apache.flink.sql.parser.ddl.SqlAlterTableRename\"\n+    \"org.apache.flink.sql.parser.ddl.SqlCreateCatalog\"\n+    \"org.apache.flink.sql.parser.ddl.SqlCreateFunction\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6359340c0b5b01923d7ac2824b9eb1071e0f2f37"}, "originalPosition": 36}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU3NTM2MDcyOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-sql-parser/src/main/java/org/apache/flink/sql/parser/ddl/hive/HiveDDLUtils.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQwNTo1Njo0M1rOGLHt1g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNVQwMjozMzo0MVrOGLu-oA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDMxMzk0Mg==", "bodyText": "It seems hacky we put these properties internal through the table options, i think these properties should be kept in each SqlNode but not the property list.", "url": "https://github.com/apache/flink/pull/11791#discussion_r414313942", "createdAt": "2020-04-24T05:56:43Z", "author": {"login": "danny0405"}, "path": "flink-table/flink-sql-parser/src/main/java/org/apache/flink/sql/parser/ddl/hive/HiveDDLUtils.java", "diffHunk": "@@ -0,0 +1,94 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.sql.parser.ddl.hive;\n+\n+import org.apache.flink.sql.parser.ddl.SqlTableOption;\n+import org.apache.flink.sql.parser.impl.ParseException;\n+import org.apache.flink.table.catalog.config.CatalogConfig;\n+\n+import org.apache.calcite.sql.SqlLiteral;\n+import org.apache.calcite.sql.SqlNode;\n+import org.apache.calcite.sql.SqlNodeList;\n+import org.apache.calcite.sql.parser.SqlParserPos;\n+\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.Set;\n+\n+import static org.apache.flink.sql.parser.ddl.hive.SqlAlterHiveDatabase.ALTER_DATABASE_OP;\n+import static org.apache.flink.sql.parser.ddl.hive.SqlCreateHiveDatabase.DATABASE_LOCATION_URI;\n+\n+/**\n+ * Util methods for Hive DDL Sql nodes.\n+ */\n+public class HiveDDLUtils {\n+\n+\tprivate static final Set<String> RESERVED_DB_PROPERTIES = new HashSet<>();\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6359340c0b5b01923d7ac2824b9eb1071e0f2f37"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDM0MDIwMQ==", "bodyText": "I agree it seems hacky. But the problem is some semantics are specific to Hive and may never be adopted in Flink, e.g. the locations of table and database. If we keep them in the SqlNode, the planner needs to be able handle them, and the Catalog object (e.g. CatalogDatabase) needs extra fields for the semantics. To avoid this, we have to encode the Hive-specific semantics as properties, which has been mentioned and discussed in FLIP-123.", "url": "https://github.com/apache/flink/pull/11791#discussion_r414340201", "createdAt": "2020-04-24T06:57:15Z", "author": {"login": "lirui-apache"}, "path": "flink-table/flink-sql-parser/src/main/java/org/apache/flink/sql/parser/ddl/hive/HiveDDLUtils.java", "diffHunk": "@@ -0,0 +1,94 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.sql.parser.ddl.hive;\n+\n+import org.apache.flink.sql.parser.ddl.SqlTableOption;\n+import org.apache.flink.sql.parser.impl.ParseException;\n+import org.apache.flink.table.catalog.config.CatalogConfig;\n+\n+import org.apache.calcite.sql.SqlLiteral;\n+import org.apache.calcite.sql.SqlNode;\n+import org.apache.calcite.sql.SqlNodeList;\n+import org.apache.calcite.sql.parser.SqlParserPos;\n+\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.Set;\n+\n+import static org.apache.flink.sql.parser.ddl.hive.SqlAlterHiveDatabase.ALTER_DATABASE_OP;\n+import static org.apache.flink.sql.parser.ddl.hive.SqlCreateHiveDatabase.DATABASE_LOCATION_URI;\n+\n+/**\n+ * Util methods for Hive DDL Sql nodes.\n+ */\n+public class HiveDDLUtils {\n+\n+\tprivate static final Set<String> RESERVED_DB_PROPERTIES = new HashSet<>();\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDMxMzk0Mg=="}, "originalCommit": {"oid": "6359340c0b5b01923d7ac2824b9eb1071e0f2f37"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDk1NzIxNg==", "bodyText": "As discussed offline, this is by-design intentionally, previously we design the Flink databases DDLs to have a extensible properties for such \"hacky\" extention way to make the planner code clean.\nWhile for the long term, if we also want to make a Hive dialect DML, we may need another planner which is another big story.", "url": "https://github.com/apache/flink/pull/11791#discussion_r414957216", "createdAt": "2020-04-25T02:33:41Z", "author": {"login": "danny0405"}, "path": "flink-table/flink-sql-parser/src/main/java/org/apache/flink/sql/parser/ddl/hive/HiveDDLUtils.java", "diffHunk": "@@ -0,0 +1,94 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.sql.parser.ddl.hive;\n+\n+import org.apache.flink.sql.parser.ddl.SqlTableOption;\n+import org.apache.flink.sql.parser.impl.ParseException;\n+import org.apache.flink.table.catalog.config.CatalogConfig;\n+\n+import org.apache.calcite.sql.SqlLiteral;\n+import org.apache.calcite.sql.SqlNode;\n+import org.apache.calcite.sql.SqlNodeList;\n+import org.apache.calcite.sql.parser.SqlParserPos;\n+\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.Set;\n+\n+import static org.apache.flink.sql.parser.ddl.hive.SqlAlterHiveDatabase.ALTER_DATABASE_OP;\n+import static org.apache.flink.sql.parser.ddl.hive.SqlCreateHiveDatabase.DATABASE_LOCATION_URI;\n+\n+/**\n+ * Util methods for Hive DDL Sql nodes.\n+ */\n+public class HiveDDLUtils {\n+\n+\tprivate static final Set<String> RESERVED_DB_PROPERTIES = new HashSet<>();\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDMxMzk0Mg=="}, "originalCommit": {"oid": "6359340c0b5b01923d7ac2824b9eb1071e0f2f37"}, "originalPosition": 43}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU3NTM2NDczOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-sql-parser/pom.xml", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQwNTo1ODoxN1rOGLHwGA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQwNjo0Mzo1NlrOGLI7ow==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDMxNDUyMA==", "bodyText": "The plugin is a mess, can we just create another module instead put the code in one ? And why we copy the parser file of Flink, it seems there is no any reuse parse code block.", "url": "https://github.com/apache/flink/pull/11791#discussion_r414314520", "createdAt": "2020-04-24T05:58:17Z", "author": {"login": "danny0405"}, "path": "flink-table/flink-sql-parser/pom.xml", "diffHunk": "@@ -298,6 +348,23 @@ under the License.\n \t\t\t\t\t\t\t<outputDirectory>${project.build.directory}/generated-sources/</outputDirectory>\n \t\t\t\t\t\t</configuration>\n \t\t\t\t\t</execution>\n+\t\t\t\t\t<execution>\n+\t\t\t\t\t\t<phase>generate-sources</phase>\n+\t\t\t\t\t\t<id>javacc-hive</id>\n+\t\t\t\t\t\t<goals>\n+\t\t\t\t\t\t\t<goal>javacc</goal>\n+\t\t\t\t\t\t</goals>\n+\t\t\t\t\t\t<configuration>\n+\t\t\t\t\t\t\t<sourceDirectory>${project.build.directory}/generated-sources-hive/</sourceDirectory>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6359340c0b5b01923d7ac2824b9eb1071e0f2f37"}, "originalPosition": 89}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDMzMzg1OQ==", "bodyText": "We're not copying the parser file of Flink. The process of generating the parser for Hive is:\n\nMake a copy of the parser template in calcite-core.jar\nGenerate the fmpp sources from codegen-hive and output to generated-sources-hive.\nCall javacc to build the parser from generated-sources-hive and output to generated-sources.\n\nFinally, we'll have both Flink and Hive parser in generated-sources.\nI agree moving the Hive parser to a separate module may make things a little clearer. But it also brings redundancies. E.g. need extra pom file and duplicated classes like ParseException. What do you think?", "url": "https://github.com/apache/flink/pull/11791#discussion_r414333859", "createdAt": "2020-04-24T06:43:56Z", "author": {"login": "lirui-apache"}, "path": "flink-table/flink-sql-parser/pom.xml", "diffHunk": "@@ -298,6 +348,23 @@ under the License.\n \t\t\t\t\t\t\t<outputDirectory>${project.build.directory}/generated-sources/</outputDirectory>\n \t\t\t\t\t\t</configuration>\n \t\t\t\t\t</execution>\n+\t\t\t\t\t<execution>\n+\t\t\t\t\t\t<phase>generate-sources</phase>\n+\t\t\t\t\t\t<id>javacc-hive</id>\n+\t\t\t\t\t\t<goals>\n+\t\t\t\t\t\t\t<goal>javacc</goal>\n+\t\t\t\t\t\t</goals>\n+\t\t\t\t\t\t<configuration>\n+\t\t\t\t\t\t\t<sourceDirectory>${project.build.directory}/generated-sources-hive/</sourceDirectory>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDMxNDUyMA=="}, "originalCommit": {"oid": "6359340c0b5b01923d7ac2824b9eb1071e0f2f37"}, "originalPosition": 89}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU3OTc0NjY2OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-sql-parser-hive/src/main/java/org/apache/flink/sql/parser/hive/package-info.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNVQwMjoyNzo0OFrOGLu6xw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNVQwMjoyNzo0OFrOGLu6xw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDk1NjIzMQ==", "bodyText": "The comment is not correct now, we actually do not switch parser dialect with the setConformance interface.", "url": "https://github.com/apache/flink/pull/11791#discussion_r414956231", "createdAt": "2020-04-25T02:27:48Z", "author": {"login": "danny0405"}, "path": "flink-table/flink-sql-parser-hive/src/main/java/org/apache/flink/sql/parser/hive/package-info.java", "diffHunk": "@@ -0,0 +1,45 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to you under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+/**\n+ * Flink sql parser for hive dialect.\n+ *\n+ * <p>This module contains the DDLs and some custom DMLs for Apache Flink.\n+ *\n+ * <p>Most of the sql grammars belong to sql standard or Flink's dialect. To support\n+ * a new sql dialect, add a new sql conformance to\n+ * {@link org.apache.flink.sql.parser.validate.FlinkSqlConformance},\n+ * then use this sql conformance to make context aware decisions in parse block. See the usage of\n+ * {@link org.apache.flink.sql.parser.validate.FlinkSqlConformance#HIVE} in {@code parserimpls.ftl}.\n+ *\n+ * <p>To use a specific sql dialect for the parser, config the parser to the specific sql conformance\n+ * with a code snippet like below:\n+ * <blockquote><pre>\n+ *   SqlParser.create(source,\n+ *   \t\tSqlParser.configBuilder()\n+ *   \t\t\t.setParserFactory(parserImplFactory())\n+ * \t\t\t\t.setQuoting(Quoting.DOUBLE_QUOTE)\n+ * \t\t\t\t.setUnquotedCasing(Casing.TO_UPPER)\n+ * \t\t\t\t.setQuotedCasing(Casing.UNCHANGED)\n+ * \t\t\t\t.setConformance(conformance0) // the sql conformance you want use.\n+ * \t\t\t\t.build());\n+ * </pre></blockquote>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3a861a4778cf6b4347788fd7a6cfc6e6c5bd970a"}, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4MTk1MzYxOnYy", "diffSide": "RIGHT", "path": "flink-table/pom.xml", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQwMzoyMjozOVrOGL96mw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQwMzo0ODo1OVrOGL-KSA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTIwMTk0Nw==", "bodyText": "must we create a new module?", "url": "https://github.com/apache/flink/pull/11791#discussion_r415201947", "createdAt": "2020-04-26T03:22:39Z", "author": {"login": "JingsongLi"}, "path": "flink-table/pom.xml", "diffHunk": "@@ -45,6 +45,7 @@ under the License.\n \t\t<module>flink-table-uber-blink</module>\n \t\t<module>flink-sql-client</module>\n \t\t<module>flink-sql-parser</module>\n+\t\t<module>flink-sql-parser-hive</module>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3a861a4778cf6b4347788fd7a6cfc6e6c5bd970a"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTIwNTk2MA==", "bodyText": "A separate module makes things a little cleaner. Otherwise the pom file of sql-parser will become messy and it can get even worse if we add more dialects in the future.", "url": "https://github.com/apache/flink/pull/11791#discussion_r415205960", "createdAt": "2020-04-26T03:48:59Z", "author": {"login": "lirui-apache"}, "path": "flink-table/pom.xml", "diffHunk": "@@ -45,6 +45,7 @@ under the License.\n \t\t<module>flink-table-uber-blink</module>\n \t\t<module>flink-sql-client</module>\n \t\t<module>flink-sql-parser</module>\n+\t\t<module>flink-sql-parser-hive</module>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTIwMTk0Nw=="}, "originalCommit": {"oid": "3a861a4778cf6b4347788fd7a6cfc6e6c5bd970a"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4MTk3NDUzOnYy", "diffSide": "RIGHT", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQwMzozNzozM1rOGL-Dpg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQwMzozNzozM1rOGL-Dpg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTIwNDI2Mg==", "bodyText": "I can see HiveCatalog becomes big and big, can we split database related codes to a separate class?", "url": "https://github.com/apache/flink/pull/11791#discussion_r415204262", "createdAt": "2020-04-26T03:37:33Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java", "diffHunk": "@@ -292,18 +303,62 @@ public void alterDatabase(String databaseName, CatalogDatabase newDatabase, bool\n \t\tcheckNotNull(newDatabase, \"newDatabase cannot be null\");\n \n \t\t// client.alterDatabase doesn't throw any exception if there is no existing database\n-\t\tif (!databaseExists(databaseName)) {\n+\t\tDatabase hiveDB;\n+\t\ttry {\n+\t\t\thiveDB = getHiveDatabase(databaseName);\n+\t\t} catch (DatabaseNotExistException e) {\n \t\t\tif (!ignoreIfNotExists) {\n \t\t\t\tthrow new DatabaseNotExistException(getName(), databaseName);\n \t\t\t}\n \n \t\t\treturn;\n \t\t}\n-\n-\t\tDatabase newHiveDatabase = instantiateHiveDatabase(databaseName, newDatabase);\n+\t\tMap<String, String> params = hiveDB.getParameters();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3a861a4778cf6b4347788fd7a6cfc6e6c5bd970a"}, "originalPosition": 70}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4MjU5ODU5OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-sql-parser-hive/src/test/java/org/apache/flink/sql/parser/hive/FlinkHiveSqlParserImplTest.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQxMDoyMTo0M1rOGMCXYg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQxMToxMzoyNFrOGMC9Pw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTI3NDg1MA==", "bodyText": "Why the unparse SQL is different ?", "url": "https://github.com/apache/flink/pull/11791#discussion_r415274850", "createdAt": "2020-04-26T10:21:43Z", "author": {"login": "danny0405"}, "path": "flink-table/flink-sql-parser-hive/src/test/java/org/apache/flink/sql/parser/hive/FlinkHiveSqlParserImplTest.java", "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.sql.parser.hive;\n+\n+import org.apache.flink.sql.parser.hive.impl.FlinkHiveSqlParserImpl;\n+\n+import org.apache.calcite.sql.parser.SqlParserImplFactory;\n+import org.apache.calcite.sql.parser.SqlParserTest;\n+import org.junit.Test;\n+\n+/**\n+ * Tests for {@link FlinkHiveSqlParserImpl}.\n+ */\n+public class FlinkHiveSqlParserImplTest extends SqlParserTest {\n+\n+\t@Override\n+\tprotected SqlParserImplFactory parserImplFactory() {\n+\t\treturn FlinkHiveSqlParserImpl.FACTORY;\n+\t}\n+\n+\t// overrides test methods that we don't support\n+\t@Override\n+\tpublic void testDescribeStatement() {\n+\t}\n+\n+\t@Override\n+\tpublic void testTableHintsInInsert() {\n+\t}\n+\n+\t@Override\n+\tpublic void testDescribeSchema() {\n+\t}\n+\n+\t@Test\n+\tpublic void testShowDatabases() {\n+\t\tsql(\"show databases\").ok(\"SHOW DATABASES\");\n+\t}\n+\n+\t@Test\n+\tpublic void testUseDatabase() {\n+\t\t// use database\n+\t\tsql(\"use db1\").ok(\"USE `DB1`\");\n+\t}\n+\n+\t@Test\n+\tpublic void testCreateDatabase() {\n+\t\tsql(\"create database db1\")\n+\t\t\t\t.ok(\"CREATE DATABASE `DB1` WITH (\\n\" +\n+\t\t\t\t\t\t\"  'is_generic' = 'false'\\n\" +\n+\t\t\t\t\t\t\")\");\n+\t\tsql(\"create database db1 comment 'comment db1' location '/path/to/db1'\")\n+\t\t\t\t.ok(\"CREATE DATABASE `DB1`\\n\" +\n+\t\t\t\t\t\t\"COMMENT 'comment db1' WITH (\\n\" +\n+\t\t\t\t\t\t\"  'is_generic' = 'false',\\n\" +\n+\t\t\t\t\t\t\"  'database.location_uri' = '/path/to/db1'\\n\" +\n+\t\t\t\t\t\t\")\");\n+\t\tsql(\"create database db1 with dbproperties ('k1'='v1','k2'='v2')\")\n+\t\t\t\t.ok(\"CREATE DATABASE `DB1` WITH (\\n\" +\n+\t\t\t\t\t\t\"  'k1' = 'v1',\\n\" +\n+\t\t\t\t\t\t\"  'k2' = 'v2',\\n\" +\n+\t\t\t\t\t\t\"  'is_generic' = 'false'\\n\" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f5a365a9ea932cb758f2888c119943173f809f57"}, "originalPosition": 77}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTI3OTU1Mg==", "bodyText": "Creating a hive database requires explicit is_generic=false in the properties. But hive users won't write DDL like that, so SqlCreateHiveDatabase will automatically add it.", "url": "https://github.com/apache/flink/pull/11791#discussion_r415279552", "createdAt": "2020-04-26T10:47:50Z", "author": {"login": "lirui-apache"}, "path": "flink-table/flink-sql-parser-hive/src/test/java/org/apache/flink/sql/parser/hive/FlinkHiveSqlParserImplTest.java", "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.sql.parser.hive;\n+\n+import org.apache.flink.sql.parser.hive.impl.FlinkHiveSqlParserImpl;\n+\n+import org.apache.calcite.sql.parser.SqlParserImplFactory;\n+import org.apache.calcite.sql.parser.SqlParserTest;\n+import org.junit.Test;\n+\n+/**\n+ * Tests for {@link FlinkHiveSqlParserImpl}.\n+ */\n+public class FlinkHiveSqlParserImplTest extends SqlParserTest {\n+\n+\t@Override\n+\tprotected SqlParserImplFactory parserImplFactory() {\n+\t\treturn FlinkHiveSqlParserImpl.FACTORY;\n+\t}\n+\n+\t// overrides test methods that we don't support\n+\t@Override\n+\tpublic void testDescribeStatement() {\n+\t}\n+\n+\t@Override\n+\tpublic void testTableHintsInInsert() {\n+\t}\n+\n+\t@Override\n+\tpublic void testDescribeSchema() {\n+\t}\n+\n+\t@Test\n+\tpublic void testShowDatabases() {\n+\t\tsql(\"show databases\").ok(\"SHOW DATABASES\");\n+\t}\n+\n+\t@Test\n+\tpublic void testUseDatabase() {\n+\t\t// use database\n+\t\tsql(\"use db1\").ok(\"USE `DB1`\");\n+\t}\n+\n+\t@Test\n+\tpublic void testCreateDatabase() {\n+\t\tsql(\"create database db1\")\n+\t\t\t\t.ok(\"CREATE DATABASE `DB1` WITH (\\n\" +\n+\t\t\t\t\t\t\"  'is_generic' = 'false'\\n\" +\n+\t\t\t\t\t\t\")\");\n+\t\tsql(\"create database db1 comment 'comment db1' location '/path/to/db1'\")\n+\t\t\t\t.ok(\"CREATE DATABASE `DB1`\\n\" +\n+\t\t\t\t\t\t\"COMMENT 'comment db1' WITH (\\n\" +\n+\t\t\t\t\t\t\"  'is_generic' = 'false',\\n\" +\n+\t\t\t\t\t\t\"  'database.location_uri' = '/path/to/db1'\\n\" +\n+\t\t\t\t\t\t\")\");\n+\t\tsql(\"create database db1 with dbproperties ('k1'='v1','k2'='v2')\")\n+\t\t\t\t.ok(\"CREATE DATABASE `DB1` WITH (\\n\" +\n+\t\t\t\t\t\t\"  'k1' = 'v1',\\n\" +\n+\t\t\t\t\t\t\"  'k2' = 'v2',\\n\" +\n+\t\t\t\t\t\t\"  'is_generic' = 'false'\\n\" +", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTI3NDg1MA=="}, "originalCommit": {"oid": "f5a365a9ea932cb758f2888c119943173f809f57"}, "originalPosition": 77}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTI4NDU0Mw==", "bodyText": "I mean the dbproperties keyword.", "url": "https://github.com/apache/flink/pull/11791#discussion_r415284543", "createdAt": "2020-04-26T11:13:24Z", "author": {"login": "danny0405"}, "path": "flink-table/flink-sql-parser-hive/src/test/java/org/apache/flink/sql/parser/hive/FlinkHiveSqlParserImplTest.java", "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.sql.parser.hive;\n+\n+import org.apache.flink.sql.parser.hive.impl.FlinkHiveSqlParserImpl;\n+\n+import org.apache.calcite.sql.parser.SqlParserImplFactory;\n+import org.apache.calcite.sql.parser.SqlParserTest;\n+import org.junit.Test;\n+\n+/**\n+ * Tests for {@link FlinkHiveSqlParserImpl}.\n+ */\n+public class FlinkHiveSqlParserImplTest extends SqlParserTest {\n+\n+\t@Override\n+\tprotected SqlParserImplFactory parserImplFactory() {\n+\t\treturn FlinkHiveSqlParserImpl.FACTORY;\n+\t}\n+\n+\t// overrides test methods that we don't support\n+\t@Override\n+\tpublic void testDescribeStatement() {\n+\t}\n+\n+\t@Override\n+\tpublic void testTableHintsInInsert() {\n+\t}\n+\n+\t@Override\n+\tpublic void testDescribeSchema() {\n+\t}\n+\n+\t@Test\n+\tpublic void testShowDatabases() {\n+\t\tsql(\"show databases\").ok(\"SHOW DATABASES\");\n+\t}\n+\n+\t@Test\n+\tpublic void testUseDatabase() {\n+\t\t// use database\n+\t\tsql(\"use db1\").ok(\"USE `DB1`\");\n+\t}\n+\n+\t@Test\n+\tpublic void testCreateDatabase() {\n+\t\tsql(\"create database db1\")\n+\t\t\t\t.ok(\"CREATE DATABASE `DB1` WITH (\\n\" +\n+\t\t\t\t\t\t\"  'is_generic' = 'false'\\n\" +\n+\t\t\t\t\t\t\")\");\n+\t\tsql(\"create database db1 comment 'comment db1' location '/path/to/db1'\")\n+\t\t\t\t.ok(\"CREATE DATABASE `DB1`\\n\" +\n+\t\t\t\t\t\t\"COMMENT 'comment db1' WITH (\\n\" +\n+\t\t\t\t\t\t\"  'is_generic' = 'false',\\n\" +\n+\t\t\t\t\t\t\"  'database.location_uri' = '/path/to/db1'\\n\" +\n+\t\t\t\t\t\t\")\");\n+\t\tsql(\"create database db1 with dbproperties ('k1'='v1','k2'='v2')\")\n+\t\t\t\t.ok(\"CREATE DATABASE `DB1` WITH (\\n\" +\n+\t\t\t\t\t\t\"  'k1' = 'v1',\\n\" +\n+\t\t\t\t\t\t\"  'k2' = 'v2',\\n\" +\n+\t\t\t\t\t\t\"  'is_generic' = 'false'\\n\" +", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTI3NDg1MA=="}, "originalCommit": {"oid": "f5a365a9ea932cb758f2888c119943173f809f57"}, "originalPosition": 77}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4MjU5OTE0OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-sql-parser-hive/src/test/java/org/apache/flink/sql/parser/hive/FlinkHiveSqlParserImplTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQxMDoyMjoxM1rOGMCXng==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQxMDo1MTowNFrOGMCsIA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTI3NDkxMA==", "bodyText": "The unparse SQL is different.", "url": "https://github.com/apache/flink/pull/11791#discussion_r415274910", "createdAt": "2020-04-26T10:22:13Z", "author": {"login": "danny0405"}, "path": "flink-table/flink-sql-parser-hive/src/test/java/org/apache/flink/sql/parser/hive/FlinkHiveSqlParserImplTest.java", "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.sql.parser.hive;\n+\n+import org.apache.flink.sql.parser.hive.impl.FlinkHiveSqlParserImpl;\n+\n+import org.apache.calcite.sql.parser.SqlParserImplFactory;\n+import org.apache.calcite.sql.parser.SqlParserTest;\n+import org.junit.Test;\n+\n+/**\n+ * Tests for {@link FlinkHiveSqlParserImpl}.\n+ */\n+public class FlinkHiveSqlParserImplTest extends SqlParserTest {\n+\n+\t@Override\n+\tprotected SqlParserImplFactory parserImplFactory() {\n+\t\treturn FlinkHiveSqlParserImpl.FACTORY;\n+\t}\n+\n+\t// overrides test methods that we don't support\n+\t@Override\n+\tpublic void testDescribeStatement() {\n+\t}\n+\n+\t@Override\n+\tpublic void testTableHintsInInsert() {\n+\t}\n+\n+\t@Override\n+\tpublic void testDescribeSchema() {\n+\t}\n+\n+\t@Test\n+\tpublic void testShowDatabases() {\n+\t\tsql(\"show databases\").ok(\"SHOW DATABASES\");\n+\t}\n+\n+\t@Test\n+\tpublic void testUseDatabase() {\n+\t\t// use database\n+\t\tsql(\"use db1\").ok(\"USE `DB1`\");\n+\t}\n+\n+\t@Test\n+\tpublic void testCreateDatabase() {\n+\t\tsql(\"create database db1\")\n+\t\t\t\t.ok(\"CREATE DATABASE `DB1` WITH (\\n\" +\n+\t\t\t\t\t\t\"  'is_generic' = 'false'\\n\" +\n+\t\t\t\t\t\t\")\");\n+\t\tsql(\"create database db1 comment 'comment db1' location '/path/to/db1'\")\n+\t\t\t\t.ok(\"CREATE DATABASE `DB1`\\n\" +\n+\t\t\t\t\t\t\"COMMENT 'comment db1' WITH (\\n\" +\n+\t\t\t\t\t\t\"  'is_generic' = 'false',\\n\" +\n+\t\t\t\t\t\t\"  'database.location_uri' = '/path/to/db1'\\n\" +\n+\t\t\t\t\t\t\")\");\n+\t\tsql(\"create database db1 with dbproperties ('k1'='v1','k2'='v2')\")\n+\t\t\t\t.ok(\"CREATE DATABASE `DB1` WITH (\\n\" +\n+\t\t\t\t\t\t\"  'k1' = 'v1',\\n\" +\n+\t\t\t\t\t\t\"  'k2' = 'v2',\\n\" +\n+\t\t\t\t\t\t\"  'is_generic' = 'false'\\n\" +\n+\t\t\t\t\t\t\")\");\n+\t}\n+\n+\t@Test\n+\tpublic void testAlterDatabase() {\n+\t\tsql(\"alter database db1 set dbproperties('k1'='v1')\")\n+\t\t\t\t.ok(\"ALTER DATABASE `DB1` SET (\\n\" +\n+\t\t\t\t\t\t\"  'k1' = 'v1',\\n\" +\n+\t\t\t\t\t\t\"  'alter.database.op' = 'CHANGE_PROPS'\\n\" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f5a365a9ea932cb758f2888c119943173f809f57"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTI4MDE2MA==", "bodyText": "This is similar to creating a DB -- the hive-specific semantics have to be encoded as properties. Flink only allows altering DB properties. But Hive supports altering DB location and owner. So we add alter.database.op in the properties to indicate which alter operation we want to perform.", "url": "https://github.com/apache/flink/pull/11791#discussion_r415280160", "createdAt": "2020-04-26T10:51:04Z", "author": {"login": "lirui-apache"}, "path": "flink-table/flink-sql-parser-hive/src/test/java/org/apache/flink/sql/parser/hive/FlinkHiveSqlParserImplTest.java", "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.sql.parser.hive;\n+\n+import org.apache.flink.sql.parser.hive.impl.FlinkHiveSqlParserImpl;\n+\n+import org.apache.calcite.sql.parser.SqlParserImplFactory;\n+import org.apache.calcite.sql.parser.SqlParserTest;\n+import org.junit.Test;\n+\n+/**\n+ * Tests for {@link FlinkHiveSqlParserImpl}.\n+ */\n+public class FlinkHiveSqlParserImplTest extends SqlParserTest {\n+\n+\t@Override\n+\tprotected SqlParserImplFactory parserImplFactory() {\n+\t\treturn FlinkHiveSqlParserImpl.FACTORY;\n+\t}\n+\n+\t// overrides test methods that we don't support\n+\t@Override\n+\tpublic void testDescribeStatement() {\n+\t}\n+\n+\t@Override\n+\tpublic void testTableHintsInInsert() {\n+\t}\n+\n+\t@Override\n+\tpublic void testDescribeSchema() {\n+\t}\n+\n+\t@Test\n+\tpublic void testShowDatabases() {\n+\t\tsql(\"show databases\").ok(\"SHOW DATABASES\");\n+\t}\n+\n+\t@Test\n+\tpublic void testUseDatabase() {\n+\t\t// use database\n+\t\tsql(\"use db1\").ok(\"USE `DB1`\");\n+\t}\n+\n+\t@Test\n+\tpublic void testCreateDatabase() {\n+\t\tsql(\"create database db1\")\n+\t\t\t\t.ok(\"CREATE DATABASE `DB1` WITH (\\n\" +\n+\t\t\t\t\t\t\"  'is_generic' = 'false'\\n\" +\n+\t\t\t\t\t\t\")\");\n+\t\tsql(\"create database db1 comment 'comment db1' location '/path/to/db1'\")\n+\t\t\t\t.ok(\"CREATE DATABASE `DB1`\\n\" +\n+\t\t\t\t\t\t\"COMMENT 'comment db1' WITH (\\n\" +\n+\t\t\t\t\t\t\"  'is_generic' = 'false',\\n\" +\n+\t\t\t\t\t\t\"  'database.location_uri' = '/path/to/db1'\\n\" +\n+\t\t\t\t\t\t\")\");\n+\t\tsql(\"create database db1 with dbproperties ('k1'='v1','k2'='v2')\")\n+\t\t\t\t.ok(\"CREATE DATABASE `DB1` WITH (\\n\" +\n+\t\t\t\t\t\t\"  'k1' = 'v1',\\n\" +\n+\t\t\t\t\t\t\"  'k2' = 'v2',\\n\" +\n+\t\t\t\t\t\t\"  'is_generic' = 'false'\\n\" +\n+\t\t\t\t\t\t\")\");\n+\t}\n+\n+\t@Test\n+\tpublic void testAlterDatabase() {\n+\t\tsql(\"alter database db1 set dbproperties('k1'='v1')\")\n+\t\t\t\t.ok(\"ALTER DATABASE `DB1` SET (\\n\" +\n+\t\t\t\t\t\t\"  'k1' = 'v1',\\n\" +\n+\t\t\t\t\t\t\"  'alter.database.op' = 'CHANGE_PROPS'\\n\" +", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTI3NDkxMA=="}, "originalCommit": {"oid": "f5a365a9ea932cb758f2888c119943173f809f57"}, "originalPosition": 86}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4MjYwMDY4OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/pom.xml", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQxMDoyMzoxM1rOGMCYTQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QwMjo1MzoyMlrOGMOkpQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTI3NTA4NQ==", "bodyText": "Why we append a new dependency instead of override the flink-sql-parser ?", "url": "https://github.com/apache/flink/pull/11791#discussion_r415275085", "createdAt": "2020-04-26T10:23:13Z", "author": {"login": "danny0405"}, "path": "flink-table/flink-table-planner-blink/pom.xml", "diffHunk": "@@ -100,6 +100,18 @@ under the License.\n \t\t\t</exclusions>\n \t\t</dependency>\n \n+\t\t<dependency>\n+\t\t\t<groupId>org.apache.flink</groupId>\n+\t\t\t<artifactId>flink-sql-parser-hive</artifactId>\n+\t\t\t<version>${project.version}</version>\n+\t\t\t<exclusions>\n+\t\t\t\t<exclusion>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f5a365a9ea932cb758f2888c119943173f809f57"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTI4MTczOQ==", "bodyText": "flink-sql-parser is a provided dep of flink-sql-parser-hive, which means flink-sql-parser is not transitive. I suppose this can make the dependencies clearer if we add a 3rd parser in the future. What do you think?", "url": "https://github.com/apache/flink/pull/11791#discussion_r415281739", "createdAt": "2020-04-26T10:59:03Z", "author": {"login": "lirui-apache"}, "path": "flink-table/flink-table-planner-blink/pom.xml", "diffHunk": "@@ -100,6 +100,18 @@ under the License.\n \t\t\t</exclusions>\n \t\t</dependency>\n \n+\t\t<dependency>\n+\t\t\t<groupId>org.apache.flink</groupId>\n+\t\t\t<artifactId>flink-sql-parser-hive</artifactId>\n+\t\t\t<version>${project.version}</version>\n+\t\t\t<exclusions>\n+\t\t\t\t<exclusion>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTI3NTA4NQ=="}, "originalCommit": {"oid": "f5a365a9ea932cb758f2888c119943173f809f57"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQ3NDg1Mw==", "bodyText": "I'm fine with that ~", "url": "https://github.com/apache/flink/pull/11791#discussion_r415474853", "createdAt": "2020-04-27T02:53:22Z", "author": {"login": "danny0405"}, "path": "flink-table/flink-table-planner-blink/pom.xml", "diffHunk": "@@ -100,6 +100,18 @@ under the License.\n \t\t\t</exclusions>\n \t\t</dependency>\n \n+\t\t<dependency>\n+\t\t\t<groupId>org.apache.flink</groupId>\n+\t\t\t<artifactId>flink-sql-parser-hive</artifactId>\n+\t\t\t<version>${project.version}</version>\n+\t\t\t<exclusions>\n+\t\t\t\t<exclusion>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTI3NTA4NQ=="}, "originalCommit": {"oid": "f5a365a9ea932cb758f2888c119943173f809f57"}, "originalPosition": 9}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4MjYwNzc4OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-sql-parser-hive/pom.xml", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQxMDoyNzozMVrOGMCbdg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQxMDoyNzozMVrOGMCbdg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTI3NTg5NA==", "bodyText": "The provided scope is useless when it becomes an in-direct dependency.", "url": "https://github.com/apache/flink/pull/11791#discussion_r415275894", "createdAt": "2020-04-26T10:27:31Z", "author": {"login": "danny0405"}, "path": "flink-table/flink-sql-parser-hive/pom.xml", "diffHunk": "@@ -0,0 +1,217 @@\n+<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+  http://www.apache.org/licenses/LICENSE-2.0\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n+\t\t xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n+\t\t xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n+\n+\t<modelVersion>4.0.0</modelVersion>\n+\n+\t<parent>\n+\t\t<artifactId>flink-table</artifactId>\n+\t\t<groupId>org.apache.flink</groupId>\n+\t\t<version>1.11-SNAPSHOT</version>\n+\t</parent>\n+\n+\t<artifactId>flink-sql-parser-hive</artifactId>\n+\t<name>flink-sql-parser-hive</name>\n+\n+\t<packaging>jar</packaging>\n+\n+\t<properties>\n+\t\t<!-- override parent pom -->\n+\t\t<test.excludedGroups/>\n+\t</properties>\n+\n+\t<dependencies>\n+\t\t<!-- depend on flink-sql-parser and explicitly define its provided deps -->\n+\t\t<dependency>\n+\t\t\t<groupId>org.apache.flink</groupId>\n+\t\t\t<artifactId>flink-sql-parser</artifactId>\n+\t\t\t<version>${project.version}</version>\n+\t\t\t<scope>provided</scope>\n+\t\t</dependency>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f5a365a9ea932cb758f2888c119943173f809f57"}, "originalPosition": 47}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4NDMxMTAzOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/delegation/FlinkSqlParserImplFactory.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QwMjo1NjoxMlrOGMOnvA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QwMzozOTozMVrOGMPXbQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQ3NTY0NA==", "bodyText": "We do not need to extend SqlParserImplFactory , how about just a tool class named FlinkSqlParserFactories and there is a method to return the factory by conformance FlinkSqlParserFactories#createFactory(SqlConformance) ?", "url": "https://github.com/apache/flink/pull/11791#discussion_r415475644", "createdAt": "2020-04-27T02:56:12Z", "author": {"login": "danny0405"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/delegation/FlinkSqlParserImplFactory.java", "diffHunk": "@@ -0,0 +1,50 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.delegation;\n+\n+import org.apache.flink.sql.parser.hive.impl.FlinkHiveSqlParserImpl;\n+import org.apache.flink.sql.parser.impl.FlinkSqlParserImpl;\n+import org.apache.flink.sql.parser.validate.FlinkSqlConformance;\n+\n+import org.apache.calcite.sql.parser.SqlAbstractParserImpl;\n+import org.apache.calcite.sql.parser.SqlParserImplFactory;\n+import org.apache.calcite.sql.validate.SqlConformance;\n+\n+import java.io.Reader;\n+\n+/**\n+ * A SqlParserImplFactory that creates the parser according to SqlConformance.\n+ */\n+public class FlinkSqlParserImplFactory implements SqlParserImplFactory {\n+\n+\tprivate final SqlConformance conformance;\n+\n+\tpublic FlinkSqlParserImplFactory(SqlConformance conformance) {\n+\t\tthis.conformance = conformance;\n+\t}\n+\n+\t@Override\n+\tpublic SqlAbstractParserImpl getParser(Reader stream) {\n+\t\tif (conformance == FlinkSqlConformance.HIVE) {\n+\t\t\treturn FlinkHiveSqlParserImpl.FACTORY.getParser(stream);\n+\t\t} else {\n+\t\t\treturn FlinkSqlParserImpl.FACTORY.getParser(stream);\n+\t\t}\n+\t}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a6a3a9e51fd9ba484d1ab4ccd0646dcacc0692ff"}, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQ4Nzg1Mw==", "bodyText": "OK, sounds good.", "url": "https://github.com/apache/flink/pull/11791#discussion_r415487853", "createdAt": "2020-04-27T03:39:31Z", "author": {"login": "lirui-apache"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/delegation/FlinkSqlParserImplFactory.java", "diffHunk": "@@ -0,0 +1,50 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.delegation;\n+\n+import org.apache.flink.sql.parser.hive.impl.FlinkHiveSqlParserImpl;\n+import org.apache.flink.sql.parser.impl.FlinkSqlParserImpl;\n+import org.apache.flink.sql.parser.validate.FlinkSqlConformance;\n+\n+import org.apache.calcite.sql.parser.SqlAbstractParserImpl;\n+import org.apache.calcite.sql.parser.SqlParserImplFactory;\n+import org.apache.calcite.sql.validate.SqlConformance;\n+\n+import java.io.Reader;\n+\n+/**\n+ * A SqlParserImplFactory that creates the parser according to SqlConformance.\n+ */\n+public class FlinkSqlParserImplFactory implements SqlParserImplFactory {\n+\n+\tprivate final SqlConformance conformance;\n+\n+\tpublic FlinkSqlParserImplFactory(SqlConformance conformance) {\n+\t\tthis.conformance = conformance;\n+\t}\n+\n+\t@Override\n+\tpublic SqlAbstractParserImpl getParser(Reader stream) {\n+\t\tif (conformance == FlinkSqlConformance.HIVE) {\n+\t\t\treturn FlinkHiveSqlParserImpl.FACTORY.getParser(stream);\n+\t\t} else {\n+\t\t\treturn FlinkSqlParserImpl.FACTORY.getParser(stream);\n+\t\t}\n+\t}", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQ3NTY0NA=="}, "originalCommit": {"oid": "a6a3a9e51fd9ba484d1ab4ccd0646dcacc0692ff"}, "originalPosition": 49}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4OTc1NDEzOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/pom.xml", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQwMTo1Mzo0N1rOGM_EWw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQwMzozNTo0NVrOGNBC0g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjI2OTQwMw==", "bodyText": "You maybe need shade flink-sql-parser-hive to the jar too.", "url": "https://github.com/apache/flink/pull/11791#discussion_r416269403", "createdAt": "2020-04-28T01:53:47Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-planner-blink/pom.xml", "diffHunk": "@@ -100,6 +100,18 @@ under the License.\n \t\t\t</exclusions>\n \t\t</dependency>\n \n+\t\t<dependency>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5ce52e3cda62c629df16d95ccb345d830b3a3531"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjMwMTc3OA==", "bodyText": "Nice catch! Shading...", "url": "https://github.com/apache/flink/pull/11791#discussion_r416301778", "createdAt": "2020-04-28T03:35:45Z", "author": {"login": "lirui-apache"}, "path": "flink-table/flink-table-planner-blink/pom.xml", "diffHunk": "@@ -100,6 +100,18 @@ under the License.\n \t\t\t</exclusions>\n \t\t</dependency>\n \n+\t\t<dependency>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjI2OTQwMw=="}, "originalCommit": {"oid": "5ce52e3cda62c629df16d95ccb345d830b3a3531"}, "originalPosition": 4}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1662, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}