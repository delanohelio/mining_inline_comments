{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzkyMzE1ODYy", "number": 11490, "reviewThreads": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yM1QxNDoxNzo1MFrODqZajg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yM1QxNDoyOToxNlrODqZvKA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ1NzgzMTgyOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecSink.scala", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yM1QxNDoxNzo1MFrOF6HdXA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQwMjoyMDo0MVrOF7J5fQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjQ4MzkzMg==", "bodyText": "Do we need to support RetractStreamTableSink?\nI don't see a requirement on this and the title doesn't describe this.", "url": "https://github.com/apache/flink/pull/11490#discussion_r396483932", "createdAt": "2020-03-23T14:17:50Z", "author": {"login": "wuchong"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecSink.scala", "diffHunk": "@@ -81,13 +82,26 @@ class BatchExecSink[T](\n   override protected def translateToPlanInternal(\n       planner: BatchPlanner): Transformation[Any] = {\n     val resultTransformation = sink match {\n-      case _: RetractStreamTableSink[T] | _: UpsertStreamTableSink[T] =>\n-        throw new TableException(\"RetractStreamTableSink and UpsertStreamTableSink is not\" +\n-          \" supported in Batch environment.\")\n-\n       case streamTableSink: StreamTableSink[T] =>\n-        // we can insert the bounded DataStream into a StreamTableSink\n-        val transformation = translateToTransformation(withChangeFlag = false, planner)\n+        val transformation = streamTableSink match {\n+          case _: RetractStreamTableSink[T] =>\n+            translateToTransformation(withChangeFlag = true, planner)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d1a5a4372582555948fb210c33efe3699822089e"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Njg1OTIwNA==", "bodyText": "Like print sink, there is no way to know isBatch or isStreaming now. We can use a unify RetractSink to support.", "url": "https://github.com/apache/flink/pull/11490#discussion_r396859204", "createdAt": "2020-03-24T01:49:25Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecSink.scala", "diffHunk": "@@ -81,13 +82,26 @@ class BatchExecSink[T](\n   override protected def translateToPlanInternal(\n       planner: BatchPlanner): Transformation[Any] = {\n     val resultTransformation = sink match {\n-      case _: RetractStreamTableSink[T] | _: UpsertStreamTableSink[T] =>\n-        throw new TableException(\"RetractStreamTableSink and UpsertStreamTableSink is not\" +\n-          \" supported in Batch environment.\")\n-\n       case streamTableSink: StreamTableSink[T] =>\n-        // we can insert the bounded DataStream into a StreamTableSink\n-        val transformation = translateToTransformation(withChangeFlag = false, planner)\n+        val transformation = streamTableSink match {\n+          case _: RetractStreamTableSink[T] =>\n+            translateToTransformation(withChangeFlag = true, planner)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjQ4MzkzMg=="}, "originalCommit": {"oid": "d1a5a4372582555948fb210c33efe3699822089e"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzU3MjQ3Nw==", "bodyText": "OK", "url": "https://github.com/apache/flink/pull/11490#discussion_r397572477", "createdAt": "2020-03-25T02:20:41Z", "author": {"login": "wuchong"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecSink.scala", "diffHunk": "@@ -81,13 +82,26 @@ class BatchExecSink[T](\n   override protected def translateToPlanInternal(\n       planner: BatchPlanner): Transformation[Any] = {\n     val resultTransformation = sink match {\n-      case _: RetractStreamTableSink[T] | _: UpsertStreamTableSink[T] =>\n-        throw new TableException(\"RetractStreamTableSink and UpsertStreamTableSink is not\" +\n-          \" supported in Batch environment.\")\n-\n       case streamTableSink: StreamTableSink[T] =>\n-        // we can insert the bounded DataStream into a StreamTableSink\n-        val transformation = translateToTransformation(withChangeFlag = false, planner)\n+        val transformation = streamTableSink match {\n+          case _: RetractStreamTableSink[T] =>\n+            translateToTransformation(withChangeFlag = true, planner)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjQ4MzkzMg=="}, "originalCommit": {"oid": "d1a5a4372582555948fb210c33efe3699822089e"}, "originalPosition": 21}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ1Nzg1MjIyOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecSink.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yM1QxNDoyMjowOVrOF6HqSw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNFQwMjozMTo1MFrOF6e_jA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjQ4NzI0Mw==", "bodyText": "I think this should be always in isAppendOnly and may have a key fields. Some connector can have an optimizaton based on this, e.g. MySQL can use INSERT INTO rather than INSERT .. ON DUPLICATE KEY.", "url": "https://github.com/apache/flink/pull/11490#discussion_r396487243", "createdAt": "2020-03-23T14:22:09Z", "author": {"login": "wuchong"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecSink.scala", "diffHunk": "@@ -81,13 +82,26 @@ class BatchExecSink[T](\n   override protected def translateToPlanInternal(\n       planner: BatchPlanner): Transformation[Any] = {\n     val resultTransformation = sink match {\n-      case _: RetractStreamTableSink[T] | _: UpsertStreamTableSink[T] =>\n-        throw new TableException(\"RetractStreamTableSink and UpsertStreamTableSink is not\" +\n-          \" supported in Batch environment.\")\n-\n       case streamTableSink: StreamTableSink[T] =>\n-        // we can insert the bounded DataStream into a StreamTableSink\n-        val transformation = translateToTransformation(withChangeFlag = false, planner)\n+        val transformation = streamTableSink match {\n+          case _: RetractStreamTableSink[T] =>\n+            translateToTransformation(withChangeFlag = true, planner)\n+\n+          case upsertSink: UpsertStreamTableSink[T] =>\n+            UpdatingPlanChecker.getUniqueKeyForUpsertSink(this, planner, upsertSink) match {\n+              case Some(keys) =>\n+                upsertSink.setIsAppendOnly(false)\n+                upsertSink.setKeyFields(keys)\n+              case None =>\n+                upsertSink.setIsAppendOnly(true)\n+                upsertSink.setKeyFields(null)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d1a5a4372582555948fb210c33efe3699822089e"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Njg2OTUxNg==", "bodyText": "setIsAppendOnly(false) should be more safe.\n\nUser maybe use same MySQL table to multiple batch inserts. Update should be safe.\nIn future, maybe primary keys are come from DDL, batch can not ensure that there is no duplicate key.", "url": "https://github.com/apache/flink/pull/11490#discussion_r396869516", "createdAt": "2020-03-24T02:31:50Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecSink.scala", "diffHunk": "@@ -81,13 +82,26 @@ class BatchExecSink[T](\n   override protected def translateToPlanInternal(\n       planner: BatchPlanner): Transformation[Any] = {\n     val resultTransformation = sink match {\n-      case _: RetractStreamTableSink[T] | _: UpsertStreamTableSink[T] =>\n-        throw new TableException(\"RetractStreamTableSink and UpsertStreamTableSink is not\" +\n-          \" supported in Batch environment.\")\n-\n       case streamTableSink: StreamTableSink[T] =>\n-        // we can insert the bounded DataStream into a StreamTableSink\n-        val transformation = translateToTransformation(withChangeFlag = false, planner)\n+        val transformation = streamTableSink match {\n+          case _: RetractStreamTableSink[T] =>\n+            translateToTransformation(withChangeFlag = true, planner)\n+\n+          case upsertSink: UpsertStreamTableSink[T] =>\n+            UpdatingPlanChecker.getUniqueKeyForUpsertSink(this, planner, upsertSink) match {\n+              case Some(keys) =>\n+                upsertSink.setIsAppendOnly(false)\n+                upsertSink.setKeyFields(keys)\n+              case None =>\n+                upsertSink.setIsAppendOnly(true)\n+                upsertSink.setKeyFields(null)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjQ4NzI0Mw=="}, "originalCommit": {"oid": "d1a5a4372582555948fb210c33efe3699822089e"}, "originalPosition": 30}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ1Nzg1ODM5OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/UpdatingPlanChecker.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yM1QxNDoyMzoyOFrOF6HuMw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yM1QxNDoyMzoyOFrOF6HuMw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjQ4ODI0Mw==", "bodyText": "UpdatingPlanChecker.getUniqueKeyFields is only used by this method. Maybe you can merge it into this method.", "url": "https://github.com/apache/flink/pull/11490#discussion_r396488243", "createdAt": "2020-03-23T14:23:28Z", "author": {"login": "wuchong"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/UpdatingPlanChecker.scala", "diffHunk": "@@ -68,4 +70,18 @@ object UpdatingPlanChecker {\n       }\n     }\n   }\n+\n+  def getUniqueKeyForUpsertSink(\n+      sinkNode: Sink,\n+      planner: PlannerBase,\n+      sink: UpsertStreamTableSink[_]): Option[Array[String]] = {\n+    // extract unique key fields\n+    // Now we pick shortest one to sink\n+    // TODO UpsertStreamTableSink setKeyFields interface should be Array[Array[String]]\n+    val sinkFieldNames = sink.getTableSchema.getFieldNames\n+    UpdatingPlanChecker.getUniqueKeyFields(sinkNode.getInput, planner, sinkFieldNames) match {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d1a5a4372582555948fb210c33efe3699822089e"}, "originalPosition": 35}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ1Nzg4NDU2OnYy", "diffSide": "RIGHT", "path": "flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCUpsertTableSinkITCase.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yM1QxNDoyOToxNlrOF6H-7w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yM1QxNDoyOToxNlrOF6H-7w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjQ5MjUyNw==", "bodyText": "We should avoid creating too many testing TableSource. You can use VALUES instead.\nINSERT INTO USER_RESULT\n  SELECT user_name, score\n  FROM (VALUES (1, 'Bob'), (22, 'Tom'), (42, 'Kim'), (42, 'Kim'), (42, 'Kim'), (1, 'Bob'))\n    AS UserCountTable(score, user_name)", "url": "https://github.com/apache/flink/pull/11490#discussion_r396492527", "createdAt": "2020-03-23T14:29:16Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCUpsertTableSinkITCase.java", "diffHunk": "@@ -210,4 +228,76 @@ public void testAppend() throws Exception {\n \t\t\t\tRow.of(20, 6, Timestamp.valueOf(\"1970-01-01 00:00:00.02\"))\n \t\t}, DB_URL, OUTPUT_TABLE2, new String[]{\"id\", \"num\", \"ts\"});\n \t}\n+\n+\t@Test\n+\tpublic void testBatchUpsert() throws Exception {\n+\t\tStreamExecutionEnvironment bsEnv = StreamExecutionEnvironment.getExecutionEnvironment();\n+\t\tEnvironmentSettings bsSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();\n+\t\tStreamTableEnvironment bsTableEnv = StreamTableEnvironment.create(bsEnv, bsSettings);\n+\t\tRowTypeInfo rt = (RowTypeInfo) Types.ROW_NAMED(new String[]{\"NAME\", \"SCORE\"}, Types.STRING, Types.LONG);\n+\t\tTable source = bsTableEnv.fromTableSource(new CollectionTableSource(generateRecords(2), rt));\n+\t\tbsTableEnv.registerTable(\"sourceTable\", source);\n+\t\tbsTableEnv.sqlUpdate(\n+\t\t\t\"CREATE TABLE USER_RESULT(\" +\n+\t\t\t\t\"NAME VARCHAR,\" +\n+\t\t\t\t\"SCORE BIGINT\" +\n+\t\t\t\t\") WITH ( \" +\n+\t\t\t\t\"'connector.type' = 'jdbc',\" +\n+\t\t\t\t\"'connector.url'='\" + DB_URL + \"',\" +\n+\t\t\t\t\"'connector.table' = '\" + OUTPUT_TABLE3 + \"'\" +\n+\t\t\t\t\")\");\n+\n+\t\tbsTableEnv.sqlUpdate(\"insert into USER_RESULT SELECT s.NAME, s.SCORE \" +\n+\t\t\t\"FROM sourceTable as s \");\n+\t\tbsTableEnv.execute(\"test\");\n+\n+\t\tcheck(new Row[] {\n+\t\t\tRow.of(\"a0\", 0L),\n+\t\t\tRow.of(\"a1\", 1L)\n+\t\t}, DB_URL, OUTPUT_TABLE3, new String[]{\"NAME\", \"SCORE\"});\n+\t}\n+\n+\tprivate List<Row> generateRecords(int numRecords) {\n+\t\tint arity = 2;\n+\t\tList<Row> res = new ArrayList<>(numRecords);\n+\t\tfor (long i = 0; i < numRecords; i++) {\n+\t\t\tRow row = new Row(arity);\n+\t\t\trow.setField(0, \"a\" + i);\n+\t\t\trow.setField(1, i);\n+\t\t\tres.add(row);\n+\t\t}\n+\t\treturn res;\n+\t}\n+\n+\tprivate static class CollectionTableSource extends InputFormatTableSource<Row> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d1a5a4372582555948fb210c33efe3699822089e"}, "originalPosition": 105}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 690, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}