{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTEyNzc5MDMy", "number": 13852, "reviewThreads": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwNjozNDozNVrOE0P1XQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQxMTo0MzowOFrOE0V_xg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzMjIwODI5OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/runtime/stream/sql/FileCompactionITCaseBase.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwNjozNDozNVrOHr3tpg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwODoxMzozNVrOHr55Bg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTc2MzYyMg==", "bodyText": "Why do we need to call clear here?", "url": "https://github.com/apache/flink/pull/13852#discussion_r515763622", "createdAt": "2020-11-02T06:34:35Z", "author": {"login": "lirui-apache"}, "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/runtime/stream/sql/FileCompactionITCaseBase.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.runtime.stream.sql;\n+\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeinfo.Types;\n+import org.apache.flink.api.java.typeutils.RowTypeInfo;\n+import org.apache.flink.streaming.api.scala.DataStream;\n+import org.apache.flink.table.planner.runtime.utils.ParallelFiniteTestSource;\n+import org.apache.flink.table.planner.runtime.utils.StreamingTestBase;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.util.CloseableIterator;\n+import org.apache.flink.util.CollectionUtil;\n+import org.apache.flink.util.FileUtils;\n+\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.List;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * Streaming sink File Compaction ITCase base, test checkpoint.\n+ */\n+public abstract class FileCompactionITCaseBase extends StreamingTestBase {\n+\n+\t@Rule\n+\tpublic Timeout timeoutPerTest = Timeout.seconds(60);\n+\n+\tprivate String resultPath;\n+\n+\tprivate List<Row> rows;\n+\n+\t@Before\n+\tpublic void init() throws IOException {\n+\t\tresultPath = tempFolder().newFolder().toURI().toString();\n+\t\tclear();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "949d1689d348b587d0ecb7637f519af311ad1d68"}, "originalPosition": 64}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTc5OTMwMg==", "bodyText": "we don't clear here.", "url": "https://github.com/apache/flink/pull/13852#discussion_r515799302", "createdAt": "2020-11-02T08:13:35Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/runtime/stream/sql/FileCompactionITCaseBase.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.runtime.stream.sql;\n+\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeinfo.Types;\n+import org.apache.flink.api.java.typeutils.RowTypeInfo;\n+import org.apache.flink.streaming.api.scala.DataStream;\n+import org.apache.flink.table.planner.runtime.utils.ParallelFiniteTestSource;\n+import org.apache.flink.table.planner.runtime.utils.StreamingTestBase;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.util.CloseableIterator;\n+import org.apache.flink.util.CollectionUtil;\n+import org.apache.flink.util.FileUtils;\n+\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.List;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * Streaming sink File Compaction ITCase base, test checkpoint.\n+ */\n+public abstract class FileCompactionITCaseBase extends StreamingTestBase {\n+\n+\t@Rule\n+\tpublic Timeout timeoutPerTest = Timeout.seconds(60);\n+\n+\tprivate String resultPath;\n+\n+\tprivate List<Row> rows;\n+\n+\t@Before\n+\tpublic void init() throws IOException {\n+\t\tresultPath = tempFolder().newFolder().toURI().toString();\n+\t\tclear();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTc2MzYyMg=="}, "originalCommit": {"oid": "949d1689d348b587d0ecb7637f519af311ad1d68"}, "originalPosition": 64}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzMjI0MzM5OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/runtime/stream/sql/FileCompactionITCaseBase.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwNjo1MzowMlrOHr4CBQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwNjo1MzowMlrOHr4CBQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTc2ODgzNw==", "bodyText": "Can we reuse the COMPACTED_PREFIX defined in CompactOperator?", "url": "https://github.com/apache/flink/pull/13852#discussion_r515768837", "createdAt": "2020-11-02T06:53:02Z", "author": {"login": "lirui-apache"}, "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/runtime/stream/sql/FileCompactionITCaseBase.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.runtime.stream.sql;\n+\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeinfo.Types;\n+import org.apache.flink.api.java.typeutils.RowTypeInfo;\n+import org.apache.flink.streaming.api.scala.DataStream;\n+import org.apache.flink.table.planner.runtime.utils.ParallelFiniteTestSource;\n+import org.apache.flink.table.planner.runtime.utils.StreamingTestBase;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.util.CloseableIterator;\n+import org.apache.flink.util.CollectionUtil;\n+import org.apache.flink.util.FileUtils;\n+\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.List;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * Streaming sink File Compaction ITCase base, test checkpoint.\n+ */\n+public abstract class FileCompactionITCaseBase extends StreamingTestBase {\n+\n+\t@Rule\n+\tpublic Timeout timeoutPerTest = Timeout.seconds(60);\n+\n+\tprivate String resultPath;\n+\n+\tprivate List<Row> rows;\n+\n+\t@Before\n+\tpublic void init() throws IOException {\n+\t\tresultPath = tempFolder().newFolder().toURI().toString();\n+\t\tclear();\n+\n+\t\tenv().setParallelism(3);\n+\t\tenv().enableCheckpointing(100);\n+\n+\t\trows = new ArrayList<>();\n+\t\tfor (int i = 0; i < 100; i++) {\n+\t\t\trows.add(Row.of(i, String.valueOf(i % 10), String.valueOf(i)));\n+\t\t}\n+\n+\t\tDataStream<Row> stream = new DataStream<>(env().getJavaEnv().addSource(\n+\t\t\t\tnew ParallelFiniteTestSource<>(rows),\n+\t\t\t\tnew RowTypeInfo(\n+\t\t\t\t\t\tnew TypeInformation[] {Types.INT, Types.STRING, Types.STRING},\n+\t\t\t\t\t\tnew String[] {\"a\", \"b\", \"c\"})));\n+\n+\t\ttEnv().createTemporaryView(\"my_table\",  stream);\n+\t}\n+\n+\t@After\n+\tpublic void clear() throws IOException {\n+\t\tFileUtils.deleteDirectory(new File(URI.create(resultPath)));\n+\t}\n+\n+\tprotected abstract String format();\n+\n+\t@Test\n+\tpublic void testNonPartition() throws Exception {\n+\t\ttEnv().executeSql(\"CREATE TABLE sink_table (a int, b string, c string) with (\" + options() + \")\");\n+\t\ttEnv().executeSql(\"insert into sink_table select * from my_table\").await();\n+\n+\t\tList<Row> results = toListAndClose(tEnv().executeSql(\"select * from sink_table\").collect());\n+\t\tresults.sort(Comparator.comparingInt(o -> (Integer) o.getField(0)));\n+\t\tassertEquals(rows, results);\n+\n+\t\tFile[] files = new File(URI.create(resultPath)).listFiles(\n+\t\t\t\t(dir, name) -> name.startsWith(\"compacted-part-\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "949d1689d348b587d0ecb7637f519af311ad1d68"}, "originalPosition": 100}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzMjI0NjAyOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/runtime/stream/sql/FileCompactionITCaseBase.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwNjo1NDoyOFrOHr4DjQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwNjo1NDoyOFrOHr4DjQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTc2OTIyOQ==", "bodyText": "Seems this is redundant because we list files with a filter that only returns files with this prefix", "url": "https://github.com/apache/flink/pull/13852#discussion_r515769229", "createdAt": "2020-11-02T06:54:28Z", "author": {"login": "lirui-apache"}, "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/runtime/stream/sql/FileCompactionITCaseBase.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.runtime.stream.sql;\n+\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeinfo.Types;\n+import org.apache.flink.api.java.typeutils.RowTypeInfo;\n+import org.apache.flink.streaming.api.scala.DataStream;\n+import org.apache.flink.table.planner.runtime.utils.ParallelFiniteTestSource;\n+import org.apache.flink.table.planner.runtime.utils.StreamingTestBase;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.util.CloseableIterator;\n+import org.apache.flink.util.CollectionUtil;\n+import org.apache.flink.util.FileUtils;\n+\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.List;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * Streaming sink File Compaction ITCase base, test checkpoint.\n+ */\n+public abstract class FileCompactionITCaseBase extends StreamingTestBase {\n+\n+\t@Rule\n+\tpublic Timeout timeoutPerTest = Timeout.seconds(60);\n+\n+\tprivate String resultPath;\n+\n+\tprivate List<Row> rows;\n+\n+\t@Before\n+\tpublic void init() throws IOException {\n+\t\tresultPath = tempFolder().newFolder().toURI().toString();\n+\t\tclear();\n+\n+\t\tenv().setParallelism(3);\n+\t\tenv().enableCheckpointing(100);\n+\n+\t\trows = new ArrayList<>();\n+\t\tfor (int i = 0; i < 100; i++) {\n+\t\t\trows.add(Row.of(i, String.valueOf(i % 10), String.valueOf(i)));\n+\t\t}\n+\n+\t\tDataStream<Row> stream = new DataStream<>(env().getJavaEnv().addSource(\n+\t\t\t\tnew ParallelFiniteTestSource<>(rows),\n+\t\t\t\tnew RowTypeInfo(\n+\t\t\t\t\t\tnew TypeInformation[] {Types.INT, Types.STRING, Types.STRING},\n+\t\t\t\t\t\tnew String[] {\"a\", \"b\", \"c\"})));\n+\n+\t\ttEnv().createTemporaryView(\"my_table\",  stream);\n+\t}\n+\n+\t@After\n+\tpublic void clear() throws IOException {\n+\t\tFileUtils.deleteDirectory(new File(URI.create(resultPath)));\n+\t}\n+\n+\tprotected abstract String format();\n+\n+\t@Test\n+\tpublic void testNonPartition() throws Exception {\n+\t\ttEnv().executeSql(\"CREATE TABLE sink_table (a int, b string, c string) with (\" + options() + \")\");\n+\t\ttEnv().executeSql(\"insert into sink_table select * from my_table\").await();\n+\n+\t\tList<Row> results = toListAndClose(tEnv().executeSql(\"select * from sink_table\").collect());\n+\t\tresults.sort(Comparator.comparingInt(o -> (Integer) o.getField(0)));\n+\t\tassertEquals(rows, results);\n+\n+\t\tFile[] files = new File(URI.create(resultPath)).listFiles(\n+\t\t\t\t(dir, name) -> name.startsWith(\"compacted-part-\"));\n+\t\tassertEquals(Arrays.toString(files), 1, files.length);\n+\n+\t\tString fileName = files[0].getName();\n+\t\tassertTrue(fileName, fileName.startsWith(\"compacted-part-\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "949d1689d348b587d0ecb7637f519af311ad1d68"}, "originalPosition": 104}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzMjI0ODQ1OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/runtime/stream/sql/FileCompactionITCaseBase.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwNjo1NTo0NFrOHr4FBA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwOTo1MDoyN1rOHr9Kzg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTc2OTYwNA==", "bodyText": "Can we also verify there's no un-compacted files left?", "url": "https://github.com/apache/flink/pull/13852#discussion_r515769604", "createdAt": "2020-11-02T06:55:44Z", "author": {"login": "lirui-apache"}, "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/runtime/stream/sql/FileCompactionITCaseBase.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.runtime.stream.sql;\n+\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeinfo.Types;\n+import org.apache.flink.api.java.typeutils.RowTypeInfo;\n+import org.apache.flink.streaming.api.scala.DataStream;\n+import org.apache.flink.table.planner.runtime.utils.ParallelFiniteTestSource;\n+import org.apache.flink.table.planner.runtime.utils.StreamingTestBase;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.util.CloseableIterator;\n+import org.apache.flink.util.CollectionUtil;\n+import org.apache.flink.util.FileUtils;\n+\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.List;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * Streaming sink File Compaction ITCase base, test checkpoint.\n+ */\n+public abstract class FileCompactionITCaseBase extends StreamingTestBase {\n+\n+\t@Rule\n+\tpublic Timeout timeoutPerTest = Timeout.seconds(60);\n+\n+\tprivate String resultPath;\n+\n+\tprivate List<Row> rows;\n+\n+\t@Before\n+\tpublic void init() throws IOException {\n+\t\tresultPath = tempFolder().newFolder().toURI().toString();\n+\t\tclear();\n+\n+\t\tenv().setParallelism(3);\n+\t\tenv().enableCheckpointing(100);\n+\n+\t\trows = new ArrayList<>();\n+\t\tfor (int i = 0; i < 100; i++) {\n+\t\t\trows.add(Row.of(i, String.valueOf(i % 10), String.valueOf(i)));\n+\t\t}\n+\n+\t\tDataStream<Row> stream = new DataStream<>(env().getJavaEnv().addSource(\n+\t\t\t\tnew ParallelFiniteTestSource<>(rows),\n+\t\t\t\tnew RowTypeInfo(\n+\t\t\t\t\t\tnew TypeInformation[] {Types.INT, Types.STRING, Types.STRING},\n+\t\t\t\t\t\tnew String[] {\"a\", \"b\", \"c\"})));\n+\n+\t\ttEnv().createTemporaryView(\"my_table\",  stream);\n+\t}\n+\n+\t@After\n+\tpublic void clear() throws IOException {\n+\t\tFileUtils.deleteDirectory(new File(URI.create(resultPath)));\n+\t}\n+\n+\tprotected abstract String format();\n+\n+\t@Test\n+\tpublic void testNonPartition() throws Exception {\n+\t\ttEnv().executeSql(\"CREATE TABLE sink_table (a int, b string, c string) with (\" + options() + \")\");\n+\t\ttEnv().executeSql(\"insert into sink_table select * from my_table\").await();\n+\n+\t\tList<Row> results = toListAndClose(tEnv().executeSql(\"select * from sink_table\").collect());\n+\t\tresults.sort(Comparator.comparingInt(o -> (Integer) o.getField(0)));\n+\t\tassertEquals(rows, results);\n+\n+\t\tFile[] files = new File(URI.create(resultPath)).listFiles(\n+\t\t\t\t(dir, name) -> name.startsWith(\"compacted-part-\"));\n+\t\tassertEquals(Arrays.toString(files), 1, files.length);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "949d1689d348b587d0ecb7637f519af311ad1d68"}, "originalPosition": 101}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTg0ODU2Ng==", "bodyText": "I  think we can", "url": "https://github.com/apache/flink/pull/13852#discussion_r515848566", "createdAt": "2020-11-02T09:43:22Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/runtime/stream/sql/FileCompactionITCaseBase.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.runtime.stream.sql;\n+\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeinfo.Types;\n+import org.apache.flink.api.java.typeutils.RowTypeInfo;\n+import org.apache.flink.streaming.api.scala.DataStream;\n+import org.apache.flink.table.planner.runtime.utils.ParallelFiniteTestSource;\n+import org.apache.flink.table.planner.runtime.utils.StreamingTestBase;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.util.CloseableIterator;\n+import org.apache.flink.util.CollectionUtil;\n+import org.apache.flink.util.FileUtils;\n+\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.List;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * Streaming sink File Compaction ITCase base, test checkpoint.\n+ */\n+public abstract class FileCompactionITCaseBase extends StreamingTestBase {\n+\n+\t@Rule\n+\tpublic Timeout timeoutPerTest = Timeout.seconds(60);\n+\n+\tprivate String resultPath;\n+\n+\tprivate List<Row> rows;\n+\n+\t@Before\n+\tpublic void init() throws IOException {\n+\t\tresultPath = tempFolder().newFolder().toURI().toString();\n+\t\tclear();\n+\n+\t\tenv().setParallelism(3);\n+\t\tenv().enableCheckpointing(100);\n+\n+\t\trows = new ArrayList<>();\n+\t\tfor (int i = 0; i < 100; i++) {\n+\t\t\trows.add(Row.of(i, String.valueOf(i % 10), String.valueOf(i)));\n+\t\t}\n+\n+\t\tDataStream<Row> stream = new DataStream<>(env().getJavaEnv().addSource(\n+\t\t\t\tnew ParallelFiniteTestSource<>(rows),\n+\t\t\t\tnew RowTypeInfo(\n+\t\t\t\t\t\tnew TypeInformation[] {Types.INT, Types.STRING, Types.STRING},\n+\t\t\t\t\t\tnew String[] {\"a\", \"b\", \"c\"})));\n+\n+\t\ttEnv().createTemporaryView(\"my_table\",  stream);\n+\t}\n+\n+\t@After\n+\tpublic void clear() throws IOException {\n+\t\tFileUtils.deleteDirectory(new File(URI.create(resultPath)));\n+\t}\n+\n+\tprotected abstract String format();\n+\n+\t@Test\n+\tpublic void testNonPartition() throws Exception {\n+\t\ttEnv().executeSql(\"CREATE TABLE sink_table (a int, b string, c string) with (\" + options() + \")\");\n+\t\ttEnv().executeSql(\"insert into sink_table select * from my_table\").await();\n+\n+\t\tList<Row> results = toListAndClose(tEnv().executeSql(\"select * from sink_table\").collect());\n+\t\tresults.sort(Comparator.comparingInt(o -> (Integer) o.getField(0)));\n+\t\tassertEquals(rows, results);\n+\n+\t\tFile[] files = new File(URI.create(resultPath)).listFiles(\n+\t\t\t\t(dir, name) -> name.startsWith(\"compacted-part-\"));\n+\t\tassertEquals(Arrays.toString(files), 1, files.length);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTc2OTYwNA=="}, "originalCommit": {"oid": "949d1689d348b587d0ecb7637f519af311ad1d68"}, "originalPosition": 101}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTg0OTA4OA==", "bodyText": "We can not assert just one file, This is because ParallelFiniteTestSource may spread data across multiple checkpoints.", "url": "https://github.com/apache/flink/pull/13852#discussion_r515849088", "createdAt": "2020-11-02T09:44:13Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/runtime/stream/sql/FileCompactionITCaseBase.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.runtime.stream.sql;\n+\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeinfo.Types;\n+import org.apache.flink.api.java.typeutils.RowTypeInfo;\n+import org.apache.flink.streaming.api.scala.DataStream;\n+import org.apache.flink.table.planner.runtime.utils.ParallelFiniteTestSource;\n+import org.apache.flink.table.planner.runtime.utils.StreamingTestBase;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.util.CloseableIterator;\n+import org.apache.flink.util.CollectionUtil;\n+import org.apache.flink.util.FileUtils;\n+\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.List;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * Streaming sink File Compaction ITCase base, test checkpoint.\n+ */\n+public abstract class FileCompactionITCaseBase extends StreamingTestBase {\n+\n+\t@Rule\n+\tpublic Timeout timeoutPerTest = Timeout.seconds(60);\n+\n+\tprivate String resultPath;\n+\n+\tprivate List<Row> rows;\n+\n+\t@Before\n+\tpublic void init() throws IOException {\n+\t\tresultPath = tempFolder().newFolder().toURI().toString();\n+\t\tclear();\n+\n+\t\tenv().setParallelism(3);\n+\t\tenv().enableCheckpointing(100);\n+\n+\t\trows = new ArrayList<>();\n+\t\tfor (int i = 0; i < 100; i++) {\n+\t\t\trows.add(Row.of(i, String.valueOf(i % 10), String.valueOf(i)));\n+\t\t}\n+\n+\t\tDataStream<Row> stream = new DataStream<>(env().getJavaEnv().addSource(\n+\t\t\t\tnew ParallelFiniteTestSource<>(rows),\n+\t\t\t\tnew RowTypeInfo(\n+\t\t\t\t\t\tnew TypeInformation[] {Types.INT, Types.STRING, Types.STRING},\n+\t\t\t\t\t\tnew String[] {\"a\", \"b\", \"c\"})));\n+\n+\t\ttEnv().createTemporaryView(\"my_table\",  stream);\n+\t}\n+\n+\t@After\n+\tpublic void clear() throws IOException {\n+\t\tFileUtils.deleteDirectory(new File(URI.create(resultPath)));\n+\t}\n+\n+\tprotected abstract String format();\n+\n+\t@Test\n+\tpublic void testNonPartition() throws Exception {\n+\t\ttEnv().executeSql(\"CREATE TABLE sink_table (a int, b string, c string) with (\" + options() + \")\");\n+\t\ttEnv().executeSql(\"insert into sink_table select * from my_table\").await();\n+\n+\t\tList<Row> results = toListAndClose(tEnv().executeSql(\"select * from sink_table\").collect());\n+\t\tresults.sort(Comparator.comparingInt(o -> (Integer) o.getField(0)));\n+\t\tassertEquals(rows, results);\n+\n+\t\tFile[] files = new File(URI.create(resultPath)).listFiles(\n+\t\t\t\t(dir, name) -> name.startsWith(\"compacted-part-\"));\n+\t\tassertEquals(Arrays.toString(files), 1, files.length);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTc2OTYwNA=="}, "originalCommit": {"oid": "949d1689d348b587d0ecb7637f519af311ad1d68"}, "originalPosition": 101}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTg1MzAwNg==", "bodyText": "Find a bug in endInput", "url": "https://github.com/apache/flink/pull/13852#discussion_r515853006", "createdAt": "2020-11-02T09:50:27Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/runtime/stream/sql/FileCompactionITCaseBase.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.runtime.stream.sql;\n+\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeinfo.Types;\n+import org.apache.flink.api.java.typeutils.RowTypeInfo;\n+import org.apache.flink.streaming.api.scala.DataStream;\n+import org.apache.flink.table.planner.runtime.utils.ParallelFiniteTestSource;\n+import org.apache.flink.table.planner.runtime.utils.StreamingTestBase;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.util.CloseableIterator;\n+import org.apache.flink.util.CollectionUtil;\n+import org.apache.flink.util.FileUtils;\n+\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.List;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * Streaming sink File Compaction ITCase base, test checkpoint.\n+ */\n+public abstract class FileCompactionITCaseBase extends StreamingTestBase {\n+\n+\t@Rule\n+\tpublic Timeout timeoutPerTest = Timeout.seconds(60);\n+\n+\tprivate String resultPath;\n+\n+\tprivate List<Row> rows;\n+\n+\t@Before\n+\tpublic void init() throws IOException {\n+\t\tresultPath = tempFolder().newFolder().toURI().toString();\n+\t\tclear();\n+\n+\t\tenv().setParallelism(3);\n+\t\tenv().enableCheckpointing(100);\n+\n+\t\trows = new ArrayList<>();\n+\t\tfor (int i = 0; i < 100; i++) {\n+\t\t\trows.add(Row.of(i, String.valueOf(i % 10), String.valueOf(i)));\n+\t\t}\n+\n+\t\tDataStream<Row> stream = new DataStream<>(env().getJavaEnv().addSource(\n+\t\t\t\tnew ParallelFiniteTestSource<>(rows),\n+\t\t\t\tnew RowTypeInfo(\n+\t\t\t\t\t\tnew TypeInformation[] {Types.INT, Types.STRING, Types.STRING},\n+\t\t\t\t\t\tnew String[] {\"a\", \"b\", \"c\"})));\n+\n+\t\ttEnv().createTemporaryView(\"my_table\",  stream);\n+\t}\n+\n+\t@After\n+\tpublic void clear() throws IOException {\n+\t\tFileUtils.deleteDirectory(new File(URI.create(resultPath)));\n+\t}\n+\n+\tprotected abstract String format();\n+\n+\t@Test\n+\tpublic void testNonPartition() throws Exception {\n+\t\ttEnv().executeSql(\"CREATE TABLE sink_table (a int, b string, c string) with (\" + options() + \")\");\n+\t\ttEnv().executeSql(\"insert into sink_table select * from my_table\").await();\n+\n+\t\tList<Row> results = toListAndClose(tEnv().executeSql(\"select * from sink_table\").collect());\n+\t\tresults.sort(Comparator.comparingInt(o -> (Integer) o.getField(0)));\n+\t\tassertEquals(rows, results);\n+\n+\t\tFile[] files = new File(URI.create(resultPath)).listFiles(\n+\t\t\t\t(dir, name) -> name.startsWith(\"compacted-part-\"));\n+\t\tassertEquals(Arrays.toString(files), 1, files.length);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTc2OTYwNA=="}, "originalCommit": {"oid": "949d1689d348b587d0ecb7637f519af311ad1d68"}, "originalPosition": 101}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzMjI2OTY3OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemOptions.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwNzowNjoxMFrOHr4RBg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwNzowNjoxMFrOHr4RBg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTc3MjY3OA==", "bodyText": "Let's have some more high-level explanations here, e.g. what will be compacted, when the compaction happens, whether files are usable before compaction, etc.", "url": "https://github.com/apache/flink/pull/13852#discussion_r515772678", "createdAt": "2020-11-02T07:06:10Z", "author": {"login": "lirui-apache"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemOptions.java", "diffHunk": "@@ -185,4 +185,17 @@\n \t\t\t\t\t.defaultValue(\"_SUCCESS\")\n \t\t\t\t\t.withDescription(\"The file name for success-file partition commit policy,\" +\n \t\t\t\t\t\t\t\" default is '_SUCCESS'.\");\n+\n+\tpublic static final ConfigOption<Boolean> AUTO_COMPACTION =\n+\t\t\tkey(\"auto-compaction\")\n+\t\t\t\t\t.booleanType()\n+\t\t\t\t\t.defaultValue(false)\n+\t\t\t\t\t.withDescription(\"Whether to enable automatic compaction in streaming sink or not.\" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "949d1689d348b587d0ecb7637f519af311ad1d68"}, "originalPosition": 9}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzMjI4MzM5OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/compact/FileInputFormatReader.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwNzoxMjo0NFrOHr4Y1w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwNzoxMjo0NFrOHr4Y1w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTc3NDY3OQ==", "bodyText": "FileInputFormatCompactReader?", "url": "https://github.com/apache/flink/pull/13852#discussion_r515774679", "createdAt": "2020-11-02T07:12:44Z", "author": {"login": "lirui-apache"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/compact/FileInputFormatReader.java", "diffHunk": "@@ -0,0 +1,72 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem.stream.compact;\n+\n+import org.apache.flink.api.common.io.FileInputFormat;\n+import org.apache.flink.core.fs.FileInputSplit;\n+\n+import java.io.IOException;\n+\n+/**\n+ * The {@link CompactReader} to delegate {@link FileInputFormat}.\n+ */\n+public class FileInputFormatReader<T> implements CompactReader<T> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "949d1689d348b587d0ecb7637f519af311ad1d68"}, "originalPosition": 29}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzMjMxMDUxOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSink.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwNzoyNDoyN1rOHr4o3Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwODoyNDoxM1rOHr6NIg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTc3ODc4MQ==", "bodyText": "Can we have a method to create builder from an OutputFileConfig instance? To make sure we won't lose anything here.", "url": "https://github.com/apache/flink/pull/13852#discussion_r515778781", "createdAt": "2020-11-02T07:24:27Z", "author": {"login": "lirui-apache"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSink.java", "diffHunk": "@@ -138,15 +154,28 @@ public SinkRuntimeProvider getSinkRuntimeProvider(Context sinkContext) {\n \t\t\treturn dataStream.writeUsingOutputFormat(builder.build())\n \t\t\t\t\t.setParallelism(dataStream.getParallelism());\n \t\t} else {\n+\t\t\tif (overwrite) {\n+\t\t\t\tthrow new IllegalStateException(\"Streaming mode not support overwrite.\");\n+\t\t\t}\n+\n+\t\t\tboolean autoCompaction = tableOptions.getBoolean(FileSystemOptions.AUTO_COMPACTION);\n \t\t\tObject writer = createWriter(sinkContext);\n+\t\t\tboolean isEncoder = writer instanceof Encoder;\n \t\t\tTableBucketAssigner assigner = new TableBucketAssigner(computer);\n \t\t\tTableRollingPolicy rollingPolicy = new TableRollingPolicy(\n-\t\t\t\t\t!(writer instanceof Encoder),\n+\t\t\t\t\t!isEncoder || autoCompaction,\n \t\t\t\t\ttableOptions.get(SINK_ROLLING_POLICY_FILE_SIZE).getBytes(),\n \t\t\t\t\ttableOptions.get(SINK_ROLLING_POLICY_ROLLOVER_INTERVAL).toMillis());\n \n+\t\t\tif (autoCompaction) {\n+\t\t\t\toutputFileConfig = OutputFileConfig.builder()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "949d1689d348b587d0ecb7637f519af311ad1d68"}, "originalPosition": 101}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTgwNDQ1MA==", "bodyText": "I think I can keep builder as a local field", "url": "https://github.com/apache/flink/pull/13852#discussion_r515804450", "createdAt": "2020-11-02T08:24:13Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSink.java", "diffHunk": "@@ -138,15 +154,28 @@ public SinkRuntimeProvider getSinkRuntimeProvider(Context sinkContext) {\n \t\t\treturn dataStream.writeUsingOutputFormat(builder.build())\n \t\t\t\t\t.setParallelism(dataStream.getParallelism());\n \t\t} else {\n+\t\t\tif (overwrite) {\n+\t\t\t\tthrow new IllegalStateException(\"Streaming mode not support overwrite.\");\n+\t\t\t}\n+\n+\t\t\tboolean autoCompaction = tableOptions.getBoolean(FileSystemOptions.AUTO_COMPACTION);\n \t\t\tObject writer = createWriter(sinkContext);\n+\t\t\tboolean isEncoder = writer instanceof Encoder;\n \t\t\tTableBucketAssigner assigner = new TableBucketAssigner(computer);\n \t\t\tTableRollingPolicy rollingPolicy = new TableRollingPolicy(\n-\t\t\t\t\t!(writer instanceof Encoder),\n+\t\t\t\t\t!isEncoder || autoCompaction,\n \t\t\t\t\ttableOptions.get(SINK_ROLLING_POLICY_FILE_SIZE).getBytes(),\n \t\t\t\t\ttableOptions.get(SINK_ROLLING_POLICY_ROLLOVER_INTERVAL).toMillis());\n \n+\t\t\tif (autoCompaction) {\n+\t\t\t\toutputFileConfig = OutputFileConfig.builder()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTc3ODc4MQ=="}, "originalCommit": {"oid": "949d1689d348b587d0ecb7637f519af311ad1d68"}, "originalPosition": 101}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzMjM5NjU2OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSink.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwNzo1NjoyNlrOHr5bVw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwNzo1NjoyNlrOHr5bVw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTc5MTcwMw==", "bodyText": "createCompactReaderFactory?", "url": "https://github.com/apache/flink/pull/13852#discussion_r515791703", "createdAt": "2020-11-02T07:56:26Z", "author": {"login": "lirui-apache"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSink.java", "diffHunk": "@@ -161,20 +190,120 @@ public SinkRuntimeProvider getSinkRuntimeProvider(Context sinkContext) {\n \t\t\t\t\t\t.withOutputFileConfig(outputFileConfig)\n \t\t\t\t\t\t.withRollingPolicy(rollingPolicy);\n \t\t\t}\n-\t\t\treturn createStreamingSink(\n-\t\t\t\t\ttableOptions,\n+\n+\t\t\tlong bucketCheckInterval = tableOptions.get(SINK_ROLLING_POLICY_CHECK_INTERVAL).toMillis();\n+\n+\t\t\tDataStream<PartitionCommitInfo> writerStream;\n+\t\t\tif (autoCompaction) {\n+\t\t\t\tlong compactionSize = tableOptions\n+\t\t\t\t\t\t.getOptional(FileSystemOptions.COMPACTION_FILE_SIZE)\n+\t\t\t\t\t\t.orElse(tableOptions.get(SINK_ROLLING_POLICY_FILE_SIZE))\n+\t\t\t\t\t\t.getBytes();\n+\n+\t\t\t\tCompactReader.Factory<RowData> reader = createCompactReader(sinkContext).orElseThrow(\n+\t\t\t\t\t\t() -> new TableException(\"Please implement available reader for compaction:\" +\n+\t\t\t\t\t\t\t\t\" BulkFormat, FileInputFormat.\"));\n+\n+\t\t\t\twriterStream = StreamingSink.compactionWriter(\n+\t\t\t\t\t\tdataStream,\n+\t\t\t\t\t\tbucketCheckInterval,\n+\t\t\t\t\t\tbucketsBuilder,\n+\t\t\t\t\t\tfsFactory,\n+\t\t\t\t\t\tpath,\n+\t\t\t\t\t\treader,\n+\t\t\t\t\t\tcompactionSize);\n+\t\t\t} else {\n+\t\t\t\twriterStream = StreamingSink.writer(\n+\t\t\t\t\t\tdataStream, bucketCheckInterval, bucketsBuilder);\n+\t\t\t}\n+\n+\t\t\treturn StreamingSink.sink(\n+\t\t\t\t\twriterStream,\n \t\t\t\t\tpath,\n-\t\t\t\t\tpartitionKeys,\n \t\t\t\t\ttableIdentifier,\n-\t\t\t\t\toverwrite,\n-\t\t\t\t\tdataStream,\n-\t\t\t\t\tbucketsBuilder,\n+\t\t\t\t\tpartitionKeys,\n \t\t\t\t\tmetaStoreFactory,\n \t\t\t\t\tfsFactory,\n-\t\t\t\t\ttableOptions.get(SINK_ROLLING_POLICY_CHECK_INTERVAL).toMillis());\n+\t\t\t\t\ttableOptions);\n+\t\t}\n+\t}\n+\n+\tprivate Optional<CompactReader.Factory<RowData>> createCompactReader(Context context) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "949d1689d348b587d0ecb7637f519af311ad1d68"}, "originalPosition": 162}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzMzIxNzk4OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSink.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQxMTo0MzowOFrOHsBCjA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQxMTo1NDo1NlrOHsBZ5w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTkxNjQyOA==", "bodyText": "Why do we call withPartPrefix twice?", "url": "https://github.com/apache/flink/pull/13852#discussion_r515916428", "createdAt": "2020-11-02T11:43:08Z", "author": {"login": "lirui-apache"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSink.java", "diffHunk": "@@ -127,108 +127,118 @@ public SinkRuntimeProvider getSinkRuntimeProvider(Context sinkContext) {\n \t}\n \n \tprivate DataStreamSink<?> consume(DataStream<RowData> dataStream, Context sinkContext) {\n-\t\tRowDataPartitionComputer computer = new RowDataPartitionComputer(\n+\t\tif (sinkContext.isBounded()) {\n+\t\t\treturn createBatchSink(dataStream, sinkContext);\n+\t\t} else {\n+\t\t\tif (overwrite) {\n+\t\t\t\tthrow new IllegalStateException(\"Streaming mode not support overwrite.\");\n+\t\t\t}\n+\n+\t\t\treturn createStreamingSink(dataStream, sinkContext);\n+\t\t}\n+\t}\n+\n+\tprivate RowDataPartitionComputer partitionComputer() {\n+\t\treturn new RowDataPartitionComputer(\n \t\t\t\tdefaultPartName,\n \t\t\t\tschema.getFieldNames(),\n \t\t\t\tschema.getFieldDataTypes(),\n \t\t\t\tpartitionKeys.toArray(new String[0]));\n+\t}\n \n-\t\tEmptyMetaStoreFactory metaStoreFactory = new EmptyMetaStoreFactory(path);\n-\t\tOutputFileConfig outputFileConfig = OutputFileConfig.builder()\n+\tprivate DataStreamSink<RowData> createBatchSink(\n+\t\t\tDataStream<RowData> inputStream, Context sinkContext) {\n+\t\tFileSystemOutputFormat.Builder<RowData> builder = new FileSystemOutputFormat.Builder<>();\n+\t\tbuilder.setPartitionComputer(partitionComputer());\n+\t\tbuilder.setDynamicGrouped(dynamicGrouping);\n+\t\tbuilder.setPartitionColumns(partitionKeys.toArray(new String[0]));\n+\t\tbuilder.setFormatFactory(createOutputFormatFactory(sinkContext));\n+\t\tbuilder.setMetaStoreFactory(new EmptyMetaStoreFactory(path));\n+\t\tbuilder.setOverwrite(overwrite);\n+\t\tbuilder.setStaticPartitions(staticPartitions);\n+\t\tbuilder.setTempPath(toStagingPath());\n+\t\tbuilder.setOutputFileConfig(OutputFileConfig.builder()\n \t\t\t\t.withPartPrefix(\"part-\" + UUID.randomUUID().toString())\n-\t\t\t\t.build();\n+\t\t\t\t.build());\n+\t\treturn inputStream.writeUsingOutputFormat(builder.build())\n+\t\t\t\t.setParallelism(inputStream.getParallelism());\n+\t}\n+\n+\tprivate DataStreamSink<?> createStreamingSink(\n+\t\t\tDataStream<RowData> dataStream, Context sinkContext) {\n \t\tFileSystemFactory fsFactory = FileSystem::get;\n+\t\tRowDataPartitionComputer computer = partitionComputer();\n \n-\t\tif (sinkContext.isBounded()) {\n-\t\t\tFileSystemOutputFormat.Builder<RowData> builder = new FileSystemOutputFormat.Builder<>();\n-\t\t\tbuilder.setPartitionComputer(computer);\n-\t\t\tbuilder.setDynamicGrouped(dynamicGrouping);\n-\t\t\tbuilder.setPartitionColumns(partitionKeys.toArray(new String[0]));\n-\t\t\tbuilder.setFormatFactory(createOutputFormatFactory(sinkContext));\n-\t\t\tbuilder.setMetaStoreFactory(metaStoreFactory);\n-\t\t\tbuilder.setFileSystemFactory(fsFactory);\n-\t\t\tbuilder.setOverwrite(overwrite);\n-\t\t\tbuilder.setStaticPartitions(staticPartitions);\n-\t\t\tbuilder.setTempPath(toStagingPath());\n-\t\t\tbuilder.setOutputFileConfig(outputFileConfig);\n-\t\t\treturn dataStream.writeUsingOutputFormat(builder.build())\n-\t\t\t\t\t.setParallelism(dataStream.getParallelism());\n+\t\tboolean autoCompaction = tableOptions.getBoolean(FileSystemOptions.AUTO_COMPACTION);\n+\t\tObject writer = createWriter(sinkContext);\n+\t\tboolean isEncoder = writer instanceof Encoder;\n+\t\tTableBucketAssigner assigner = new TableBucketAssigner(computer);\n+\t\tTableRollingPolicy rollingPolicy = new TableRollingPolicy(\n+\t\t\t\t!isEncoder || autoCompaction,\n+\t\t\t\ttableOptions.get(SINK_ROLLING_POLICY_FILE_SIZE).getBytes(),\n+\t\t\t\ttableOptions.get(SINK_ROLLING_POLICY_ROLLOVER_INTERVAL).toMillis());\n+\n+\t\tString randomPrefix = \"part-\" + UUID.randomUUID().toString();\n+\t\tOutputFileConfig.OutputFileConfigBuilder fileNamingBuilder = OutputFileConfig.builder();\n+\t\tfileNamingBuilder = autoCompaction ?\n+\t\t\t\tfileNamingBuilder.withPartPrefix(convertToUncompacted(randomPrefix)) :\n+\t\t\t\tfileNamingBuilder.withPartPrefix(randomPrefix);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b1bffe4fa9f3adbe03d77dd7bbf8502d39619e13"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTkyMjQwNw==", "bodyText": "Never mind... Just noted it's for different cases", "url": "https://github.com/apache/flink/pull/13852#discussion_r515922407", "createdAt": "2020-11-02T11:54:56Z", "author": {"login": "lirui-apache"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSink.java", "diffHunk": "@@ -127,108 +127,118 @@ public SinkRuntimeProvider getSinkRuntimeProvider(Context sinkContext) {\n \t}\n \n \tprivate DataStreamSink<?> consume(DataStream<RowData> dataStream, Context sinkContext) {\n-\t\tRowDataPartitionComputer computer = new RowDataPartitionComputer(\n+\t\tif (sinkContext.isBounded()) {\n+\t\t\treturn createBatchSink(dataStream, sinkContext);\n+\t\t} else {\n+\t\t\tif (overwrite) {\n+\t\t\t\tthrow new IllegalStateException(\"Streaming mode not support overwrite.\");\n+\t\t\t}\n+\n+\t\t\treturn createStreamingSink(dataStream, sinkContext);\n+\t\t}\n+\t}\n+\n+\tprivate RowDataPartitionComputer partitionComputer() {\n+\t\treturn new RowDataPartitionComputer(\n \t\t\t\tdefaultPartName,\n \t\t\t\tschema.getFieldNames(),\n \t\t\t\tschema.getFieldDataTypes(),\n \t\t\t\tpartitionKeys.toArray(new String[0]));\n+\t}\n \n-\t\tEmptyMetaStoreFactory metaStoreFactory = new EmptyMetaStoreFactory(path);\n-\t\tOutputFileConfig outputFileConfig = OutputFileConfig.builder()\n+\tprivate DataStreamSink<RowData> createBatchSink(\n+\t\t\tDataStream<RowData> inputStream, Context sinkContext) {\n+\t\tFileSystemOutputFormat.Builder<RowData> builder = new FileSystemOutputFormat.Builder<>();\n+\t\tbuilder.setPartitionComputer(partitionComputer());\n+\t\tbuilder.setDynamicGrouped(dynamicGrouping);\n+\t\tbuilder.setPartitionColumns(partitionKeys.toArray(new String[0]));\n+\t\tbuilder.setFormatFactory(createOutputFormatFactory(sinkContext));\n+\t\tbuilder.setMetaStoreFactory(new EmptyMetaStoreFactory(path));\n+\t\tbuilder.setOverwrite(overwrite);\n+\t\tbuilder.setStaticPartitions(staticPartitions);\n+\t\tbuilder.setTempPath(toStagingPath());\n+\t\tbuilder.setOutputFileConfig(OutputFileConfig.builder()\n \t\t\t\t.withPartPrefix(\"part-\" + UUID.randomUUID().toString())\n-\t\t\t\t.build();\n+\t\t\t\t.build());\n+\t\treturn inputStream.writeUsingOutputFormat(builder.build())\n+\t\t\t\t.setParallelism(inputStream.getParallelism());\n+\t}\n+\n+\tprivate DataStreamSink<?> createStreamingSink(\n+\t\t\tDataStream<RowData> dataStream, Context sinkContext) {\n \t\tFileSystemFactory fsFactory = FileSystem::get;\n+\t\tRowDataPartitionComputer computer = partitionComputer();\n \n-\t\tif (sinkContext.isBounded()) {\n-\t\t\tFileSystemOutputFormat.Builder<RowData> builder = new FileSystemOutputFormat.Builder<>();\n-\t\t\tbuilder.setPartitionComputer(computer);\n-\t\t\tbuilder.setDynamicGrouped(dynamicGrouping);\n-\t\t\tbuilder.setPartitionColumns(partitionKeys.toArray(new String[0]));\n-\t\t\tbuilder.setFormatFactory(createOutputFormatFactory(sinkContext));\n-\t\t\tbuilder.setMetaStoreFactory(metaStoreFactory);\n-\t\t\tbuilder.setFileSystemFactory(fsFactory);\n-\t\t\tbuilder.setOverwrite(overwrite);\n-\t\t\tbuilder.setStaticPartitions(staticPartitions);\n-\t\t\tbuilder.setTempPath(toStagingPath());\n-\t\t\tbuilder.setOutputFileConfig(outputFileConfig);\n-\t\t\treturn dataStream.writeUsingOutputFormat(builder.build())\n-\t\t\t\t\t.setParallelism(dataStream.getParallelism());\n+\t\tboolean autoCompaction = tableOptions.getBoolean(FileSystemOptions.AUTO_COMPACTION);\n+\t\tObject writer = createWriter(sinkContext);\n+\t\tboolean isEncoder = writer instanceof Encoder;\n+\t\tTableBucketAssigner assigner = new TableBucketAssigner(computer);\n+\t\tTableRollingPolicy rollingPolicy = new TableRollingPolicy(\n+\t\t\t\t!isEncoder || autoCompaction,\n+\t\t\t\ttableOptions.get(SINK_ROLLING_POLICY_FILE_SIZE).getBytes(),\n+\t\t\t\ttableOptions.get(SINK_ROLLING_POLICY_ROLLOVER_INTERVAL).toMillis());\n+\n+\t\tString randomPrefix = \"part-\" + UUID.randomUUID().toString();\n+\t\tOutputFileConfig.OutputFileConfigBuilder fileNamingBuilder = OutputFileConfig.builder();\n+\t\tfileNamingBuilder = autoCompaction ?\n+\t\t\t\tfileNamingBuilder.withPartPrefix(convertToUncompacted(randomPrefix)) :\n+\t\t\t\tfileNamingBuilder.withPartPrefix(randomPrefix);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTkxNjQyOA=="}, "originalCommit": {"oid": "b1bffe4fa9f3adbe03d77dd7bbf8502d39619e13"}, "originalPosition": 86}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4992, "cost": 1, "resetAt": "2021-11-12T11:57:46Z"}}}