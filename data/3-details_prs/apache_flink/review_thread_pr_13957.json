{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTE2NTg5MzQx", "number": 13957, "reviewThreads": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wN1QwNTowMzozNVrOE2TZvQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wN1QwNTozODowOFrOE2TyVw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI1Mzc2NDQ1OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/DeserializationSchemaAdapter.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wN1QwNTowMzozNVrOHvDYvQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wN1QwNjoxMTo1M1rOHvEvIg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTEwMDYwNQ==", "bodyText": "return reader?", "url": "https://github.com/apache/flink/pull/13957#discussion_r519100605", "createdAt": "2020-11-07T05:03:35Z", "author": {"login": "wuchong"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/DeserializationSchemaAdapter.java", "diffHunk": "@@ -0,0 +1,316 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.api.common.io.DelimitedInputFormat;\n+import org.apache.flink.api.common.serialization.DeserializationSchema;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.connector.file.src.FileSourceSplit;\n+import org.apache.flink.connector.file.src.reader.BulkFormat;\n+import org.apache.flink.connector.file.src.util.ArrayResultIterator;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.metrics.MetricGroup;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.utils.PartitionPathUtils;\n+import org.apache.flink.util.Collector;\n+import org.apache.flink.util.InstantiationUtil;\n+import org.apache.flink.util.UserCodeClassLoader;\n+\n+import javax.annotation.Nullable;\n+\n+import java.io.IOException;\n+import java.util.ArrayDeque;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Queue;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.connector.file.src.util.CheckpointedPosition.NO_OFFSET;\n+import static org.apache.flink.table.data.vector.VectorizedColumnBatch.DEFAULT_SIZE;\n+\n+/**\n+ * Adapter to turn a {@link DeserializationSchema} into a {@link BulkFormat}.\n+ */\n+public class DeserializationSchemaAdapter implements BulkFormat<RowData, FileSourceSplit> {\n+\n+\tprivate static final int BATCH_SIZE = 100;\n+\n+\t// NOTE, deserializationSchema produce full format fields with original order\n+\tprivate final DeserializationSchema<RowData> deserializationSchema;\n+\n+\tprivate final String[] fieldNames;\n+\tprivate final DataType[] fieldTypes;\n+\tprivate final int[] projectFields;\n+\tprivate final RowType projectedRowType;\n+\n+\tprivate final List<String> partitionKeys;\n+\tprivate final String defaultPartValue;\n+\n+\tprivate final int[] toProjectedField;\n+\tprivate final RowData.FieldGetter[] formatFieldGetters;\n+\n+\tpublic DeserializationSchemaAdapter(\n+\t\t\tDeserializationSchema<RowData> deserializationSchema,\n+\t\t\tTableSchema schema,\n+\t\t\tint[] projectFields,\n+\t\t\tList<String> partitionKeys,\n+\t\t\tString defaultPartValue) {\n+\t\tthis.deserializationSchema = deserializationSchema;\n+\t\tthis.fieldNames = schema.getFieldNames();\n+\t\tthis.fieldTypes = schema.getFieldDataTypes();\n+\t\tthis.projectFields = projectFields;\n+\t\tthis.partitionKeys = partitionKeys;\n+\t\tthis.defaultPartValue = defaultPartValue;\n+\n+\t\tList<String> projectedNames = Arrays.stream(projectFields)\n+\t\t\t\t.mapToObj(idx -> schema.getFieldNames()[idx])\n+\t\t\t\t.collect(Collectors.toList());\n+\n+\t\tthis.projectedRowType = RowType.of(\n+\t\t\t\tArrays.stream(projectFields).mapToObj(idx ->\n+\t\t\t\t\t\tschema.getFieldDataTypes()[idx].getLogicalType()).toArray(LogicalType[]::new),\n+\t\t\t\tprojectedNames.toArray(new String[0]));\n+\n+\t\tList<String> formatFields = Arrays.stream(schema.getFieldNames())\n+\t\t\t\t.filter(field -> !partitionKeys.contains(field))\n+\t\t\t\t.collect(Collectors.toList());\n+\n+\t\tList<String> formatProjectedFields = projectedNames.stream()\n+\t\t\t\t.filter(field -> !partitionKeys.contains(field))\n+\t\t\t\t.collect(Collectors.toList());\n+\n+\t\tthis.toProjectedField = formatProjectedFields.stream()\n+\t\t\t\t.mapToInt(projectedNames::indexOf)\n+\t\t\t\t.toArray();\n+\n+\t\tthis.formatFieldGetters = new RowData.FieldGetter[formatProjectedFields.size()];\n+\t\tfor (int i = 0; i < formatProjectedFields.size(); i++) {\n+\t\t\tString name = formatProjectedFields.get(i);\n+\t\t\tthis.formatFieldGetters[i] = RowData.createFieldGetter(\n+\t\t\t\t\tschema.getFieldDataType(name).get().getLogicalType(),\n+\t\t\t\t\tformatFields.indexOf(name));\n+\t\t}\n+\t}\n+\n+\tprivate DeserializationSchema<RowData> createDeserialization() throws IOException {\n+\t\ttry {\n+\t\t\tDeserializationSchema<RowData> deserialization = InstantiationUtil.clone(deserializationSchema);\n+\t\t\tdeserialization.open(new DeserializationSchema.InitializationContext() {\n+\t\t\t\t@Override\n+\t\t\t\tpublic MetricGroup getMetricGroup() {\n+\t\t\t\t\tthrow new UnsupportedOperationException(\"MetricGroup is unsupported in BulkFormat.\");\n+\t\t\t\t}\n+\n+\t\t\t\t@Override\n+\t\t\t\tpublic UserCodeClassLoader getUserCodeClassLoader() {\n+\t\t\t\t\treturn (UserCodeClassLoader) Thread.currentThread().getContextClassLoader();\n+\t\t\t\t}\n+\t\t\t});\n+\t\t\treturn deserialization;\n+\t\t} catch (Exception e) {\n+\t\t\tthrow new IOException(e);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic Reader createReader(Configuration config, FileSourceSplit split) throws IOException {\n+\t\treturn new Reader(config, split);\n+\t}\n+\n+\t@Override\n+\tpublic Reader restoreReader(Configuration config, FileSourceSplit split) throws IOException {\n+\t\tReader reader = new Reader(config, split);\n+\t\treader.seek(split.getReaderPosition().get().getRecordsAfterOffset());\n+\t\treturn null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c4e62b055b5cd62d95be004423262ed9ce5eb86"}, "originalPosition": 148}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTExOTM2OA==", "bodyText": "Streaming restore case: no test cover", "url": "https://github.com/apache/flink/pull/13957#discussion_r519119368", "createdAt": "2020-11-07T05:59:18Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/DeserializationSchemaAdapter.java", "diffHunk": "@@ -0,0 +1,316 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.api.common.io.DelimitedInputFormat;\n+import org.apache.flink.api.common.serialization.DeserializationSchema;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.connector.file.src.FileSourceSplit;\n+import org.apache.flink.connector.file.src.reader.BulkFormat;\n+import org.apache.flink.connector.file.src.util.ArrayResultIterator;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.metrics.MetricGroup;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.utils.PartitionPathUtils;\n+import org.apache.flink.util.Collector;\n+import org.apache.flink.util.InstantiationUtil;\n+import org.apache.flink.util.UserCodeClassLoader;\n+\n+import javax.annotation.Nullable;\n+\n+import java.io.IOException;\n+import java.util.ArrayDeque;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Queue;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.connector.file.src.util.CheckpointedPosition.NO_OFFSET;\n+import static org.apache.flink.table.data.vector.VectorizedColumnBatch.DEFAULT_SIZE;\n+\n+/**\n+ * Adapter to turn a {@link DeserializationSchema} into a {@link BulkFormat}.\n+ */\n+public class DeserializationSchemaAdapter implements BulkFormat<RowData, FileSourceSplit> {\n+\n+\tprivate static final int BATCH_SIZE = 100;\n+\n+\t// NOTE, deserializationSchema produce full format fields with original order\n+\tprivate final DeserializationSchema<RowData> deserializationSchema;\n+\n+\tprivate final String[] fieldNames;\n+\tprivate final DataType[] fieldTypes;\n+\tprivate final int[] projectFields;\n+\tprivate final RowType projectedRowType;\n+\n+\tprivate final List<String> partitionKeys;\n+\tprivate final String defaultPartValue;\n+\n+\tprivate final int[] toProjectedField;\n+\tprivate final RowData.FieldGetter[] formatFieldGetters;\n+\n+\tpublic DeserializationSchemaAdapter(\n+\t\t\tDeserializationSchema<RowData> deserializationSchema,\n+\t\t\tTableSchema schema,\n+\t\t\tint[] projectFields,\n+\t\t\tList<String> partitionKeys,\n+\t\t\tString defaultPartValue) {\n+\t\tthis.deserializationSchema = deserializationSchema;\n+\t\tthis.fieldNames = schema.getFieldNames();\n+\t\tthis.fieldTypes = schema.getFieldDataTypes();\n+\t\tthis.projectFields = projectFields;\n+\t\tthis.partitionKeys = partitionKeys;\n+\t\tthis.defaultPartValue = defaultPartValue;\n+\n+\t\tList<String> projectedNames = Arrays.stream(projectFields)\n+\t\t\t\t.mapToObj(idx -> schema.getFieldNames()[idx])\n+\t\t\t\t.collect(Collectors.toList());\n+\n+\t\tthis.projectedRowType = RowType.of(\n+\t\t\t\tArrays.stream(projectFields).mapToObj(idx ->\n+\t\t\t\t\t\tschema.getFieldDataTypes()[idx].getLogicalType()).toArray(LogicalType[]::new),\n+\t\t\t\tprojectedNames.toArray(new String[0]));\n+\n+\t\tList<String> formatFields = Arrays.stream(schema.getFieldNames())\n+\t\t\t\t.filter(field -> !partitionKeys.contains(field))\n+\t\t\t\t.collect(Collectors.toList());\n+\n+\t\tList<String> formatProjectedFields = projectedNames.stream()\n+\t\t\t\t.filter(field -> !partitionKeys.contains(field))\n+\t\t\t\t.collect(Collectors.toList());\n+\n+\t\tthis.toProjectedField = formatProjectedFields.stream()\n+\t\t\t\t.mapToInt(projectedNames::indexOf)\n+\t\t\t\t.toArray();\n+\n+\t\tthis.formatFieldGetters = new RowData.FieldGetter[formatProjectedFields.size()];\n+\t\tfor (int i = 0; i < formatProjectedFields.size(); i++) {\n+\t\t\tString name = formatProjectedFields.get(i);\n+\t\t\tthis.formatFieldGetters[i] = RowData.createFieldGetter(\n+\t\t\t\t\tschema.getFieldDataType(name).get().getLogicalType(),\n+\t\t\t\t\tformatFields.indexOf(name));\n+\t\t}\n+\t}\n+\n+\tprivate DeserializationSchema<RowData> createDeserialization() throws IOException {\n+\t\ttry {\n+\t\t\tDeserializationSchema<RowData> deserialization = InstantiationUtil.clone(deserializationSchema);\n+\t\t\tdeserialization.open(new DeserializationSchema.InitializationContext() {\n+\t\t\t\t@Override\n+\t\t\t\tpublic MetricGroup getMetricGroup() {\n+\t\t\t\t\tthrow new UnsupportedOperationException(\"MetricGroup is unsupported in BulkFormat.\");\n+\t\t\t\t}\n+\n+\t\t\t\t@Override\n+\t\t\t\tpublic UserCodeClassLoader getUserCodeClassLoader() {\n+\t\t\t\t\treturn (UserCodeClassLoader) Thread.currentThread().getContextClassLoader();\n+\t\t\t\t}\n+\t\t\t});\n+\t\t\treturn deserialization;\n+\t\t} catch (Exception e) {\n+\t\t\tthrow new IOException(e);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic Reader createReader(Configuration config, FileSourceSplit split) throws IOException {\n+\t\treturn new Reader(config, split);\n+\t}\n+\n+\t@Override\n+\tpublic Reader restoreReader(Configuration config, FileSourceSplit split) throws IOException {\n+\t\tReader reader = new Reader(config, split);\n+\t\treader.seek(split.getReaderPosition().get().getRecordsAfterOffset());\n+\t\treturn null;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTEwMDYwNQ=="}, "originalCommit": {"oid": "0c4e62b055b5cd62d95be004423262ed9ce5eb86"}, "originalPosition": 148}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTEyMjcyMg==", "bodyText": "But there is no way, because the current file system does not support streaming reading", "url": "https://github.com/apache/flink/pull/13957#discussion_r519122722", "createdAt": "2020-11-07T06:11:53Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/DeserializationSchemaAdapter.java", "diffHunk": "@@ -0,0 +1,316 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.api.common.io.DelimitedInputFormat;\n+import org.apache.flink.api.common.serialization.DeserializationSchema;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.connector.file.src.FileSourceSplit;\n+import org.apache.flink.connector.file.src.reader.BulkFormat;\n+import org.apache.flink.connector.file.src.util.ArrayResultIterator;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.metrics.MetricGroup;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.utils.PartitionPathUtils;\n+import org.apache.flink.util.Collector;\n+import org.apache.flink.util.InstantiationUtil;\n+import org.apache.flink.util.UserCodeClassLoader;\n+\n+import javax.annotation.Nullable;\n+\n+import java.io.IOException;\n+import java.util.ArrayDeque;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Queue;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.connector.file.src.util.CheckpointedPosition.NO_OFFSET;\n+import static org.apache.flink.table.data.vector.VectorizedColumnBatch.DEFAULT_SIZE;\n+\n+/**\n+ * Adapter to turn a {@link DeserializationSchema} into a {@link BulkFormat}.\n+ */\n+public class DeserializationSchemaAdapter implements BulkFormat<RowData, FileSourceSplit> {\n+\n+\tprivate static final int BATCH_SIZE = 100;\n+\n+\t// NOTE, deserializationSchema produce full format fields with original order\n+\tprivate final DeserializationSchema<RowData> deserializationSchema;\n+\n+\tprivate final String[] fieldNames;\n+\tprivate final DataType[] fieldTypes;\n+\tprivate final int[] projectFields;\n+\tprivate final RowType projectedRowType;\n+\n+\tprivate final List<String> partitionKeys;\n+\tprivate final String defaultPartValue;\n+\n+\tprivate final int[] toProjectedField;\n+\tprivate final RowData.FieldGetter[] formatFieldGetters;\n+\n+\tpublic DeserializationSchemaAdapter(\n+\t\t\tDeserializationSchema<RowData> deserializationSchema,\n+\t\t\tTableSchema schema,\n+\t\t\tint[] projectFields,\n+\t\t\tList<String> partitionKeys,\n+\t\t\tString defaultPartValue) {\n+\t\tthis.deserializationSchema = deserializationSchema;\n+\t\tthis.fieldNames = schema.getFieldNames();\n+\t\tthis.fieldTypes = schema.getFieldDataTypes();\n+\t\tthis.projectFields = projectFields;\n+\t\tthis.partitionKeys = partitionKeys;\n+\t\tthis.defaultPartValue = defaultPartValue;\n+\n+\t\tList<String> projectedNames = Arrays.stream(projectFields)\n+\t\t\t\t.mapToObj(idx -> schema.getFieldNames()[idx])\n+\t\t\t\t.collect(Collectors.toList());\n+\n+\t\tthis.projectedRowType = RowType.of(\n+\t\t\t\tArrays.stream(projectFields).mapToObj(idx ->\n+\t\t\t\t\t\tschema.getFieldDataTypes()[idx].getLogicalType()).toArray(LogicalType[]::new),\n+\t\t\t\tprojectedNames.toArray(new String[0]));\n+\n+\t\tList<String> formatFields = Arrays.stream(schema.getFieldNames())\n+\t\t\t\t.filter(field -> !partitionKeys.contains(field))\n+\t\t\t\t.collect(Collectors.toList());\n+\n+\t\tList<String> formatProjectedFields = projectedNames.stream()\n+\t\t\t\t.filter(field -> !partitionKeys.contains(field))\n+\t\t\t\t.collect(Collectors.toList());\n+\n+\t\tthis.toProjectedField = formatProjectedFields.stream()\n+\t\t\t\t.mapToInt(projectedNames::indexOf)\n+\t\t\t\t.toArray();\n+\n+\t\tthis.formatFieldGetters = new RowData.FieldGetter[formatProjectedFields.size()];\n+\t\tfor (int i = 0; i < formatProjectedFields.size(); i++) {\n+\t\t\tString name = formatProjectedFields.get(i);\n+\t\t\tthis.formatFieldGetters[i] = RowData.createFieldGetter(\n+\t\t\t\t\tschema.getFieldDataType(name).get().getLogicalType(),\n+\t\t\t\t\tformatFields.indexOf(name));\n+\t\t}\n+\t}\n+\n+\tprivate DeserializationSchema<RowData> createDeserialization() throws IOException {\n+\t\ttry {\n+\t\t\tDeserializationSchema<RowData> deserialization = InstantiationUtil.clone(deserializationSchema);\n+\t\t\tdeserialization.open(new DeserializationSchema.InitializationContext() {\n+\t\t\t\t@Override\n+\t\t\t\tpublic MetricGroup getMetricGroup() {\n+\t\t\t\t\tthrow new UnsupportedOperationException(\"MetricGroup is unsupported in BulkFormat.\");\n+\t\t\t\t}\n+\n+\t\t\t\t@Override\n+\t\t\t\tpublic UserCodeClassLoader getUserCodeClassLoader() {\n+\t\t\t\t\treturn (UserCodeClassLoader) Thread.currentThread().getContextClassLoader();\n+\t\t\t\t}\n+\t\t\t});\n+\t\t\treturn deserialization;\n+\t\t} catch (Exception e) {\n+\t\t\tthrow new IOException(e);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic Reader createReader(Configuration config, FileSourceSplit split) throws IOException {\n+\t\treturn new Reader(config, split);\n+\t}\n+\n+\t@Override\n+\tpublic Reader restoreReader(Configuration config, FileSourceSplit split) throws IOException {\n+\t\tReader reader = new Reader(config, split);\n+\t\treader.seek(split.getReaderPosition().get().getRecordsAfterOffset());\n+\t\treturn null;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTEwMDYwNQ=="}, "originalCommit": {"oid": "0c4e62b055b5cd62d95be004423262ed9ce5eb86"}, "originalPosition": 148}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI1MzgxMDQ4OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSink.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wN1QwNToyOToyNlrOHvD2xg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wN1QwNjoxMToyNlrOHvEujg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTEwODI5NA==", "bodyText": "IntStream.range?", "url": "https://github.com/apache/flink/pull/13957#discussion_r519108294", "createdAt": "2020-11-07T05:29:26Z", "author": {"login": "wuchong"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSink.java", "diffHunk": "@@ -251,6 +258,14 @@ private RowDataPartitionComputer partitionComputer() {\n \t\t\t\t//noinspection unchecked\n \t\t\t\treturn Optional.of(FileInputFormatCompactReader.factory((FileInputFormat<RowData>) format));\n \t\t\t}\n+\t\t} else if (deserializationFormat != null) {\n+\t\t\t// NOTE, we need pass full format types to deserializationFormat\n+\t\t\tDeserializationSchema<RowData> decoder = deserializationFormat.createRuntimeDecoder(\n+\t\t\t\t\tcreateSourceContext(context), getFormatDataType());\n+\t\t\tint[] projectedFields = IntStream.of(0, schema.getFieldCount()).toArray();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c4e62b055b5cd62d95be004423262ed9ce5eb86"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTExOTQ2MA==", "bodyText": "Streaming sink compaction case: no test cover", "url": "https://github.com/apache/flink/pull/13957#discussion_r519119460", "createdAt": "2020-11-07T05:59:31Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSink.java", "diffHunk": "@@ -251,6 +258,14 @@ private RowDataPartitionComputer partitionComputer() {\n \t\t\t\t//noinspection unchecked\n \t\t\t\treturn Optional.of(FileInputFormatCompactReader.factory((FileInputFormat<RowData>) format));\n \t\t\t}\n+\t\t} else if (deserializationFormat != null) {\n+\t\t\t// NOTE, we need pass full format types to deserializationFormat\n+\t\t\tDeserializationSchema<RowData> decoder = deserializationFormat.createRuntimeDecoder(\n+\t\t\t\t\tcreateSourceContext(context), getFormatDataType());\n+\t\t\tint[] projectedFields = IntStream.of(0, schema.getFieldCount()).toArray();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTEwODI5NA=="}, "originalCommit": {"oid": "0c4e62b055b5cd62d95be004423262ed9ce5eb86"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTEyMjU3NA==", "bodyText": "I'll add Json Compaction test", "url": "https://github.com/apache/flink/pull/13957#discussion_r519122574", "createdAt": "2020-11-07T06:11:26Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSink.java", "diffHunk": "@@ -251,6 +258,14 @@ private RowDataPartitionComputer partitionComputer() {\n \t\t\t\t//noinspection unchecked\n \t\t\t\treturn Optional.of(FileInputFormatCompactReader.factory((FileInputFormat<RowData>) format));\n \t\t\t}\n+\t\t} else if (deserializationFormat != null) {\n+\t\t\t// NOTE, we need pass full format types to deserializationFormat\n+\t\t\tDeserializationSchema<RowData> decoder = deserializationFormat.createRuntimeDecoder(\n+\t\t\t\t\tcreateSourceContext(context), getFormatDataType());\n+\t\t\tint[] projectedFields = IntStream.of(0, schema.getFieldCount()).toArray();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTEwODI5NA=="}, "originalCommit": {"oid": "0c4e62b055b5cd62d95be004423262ed9ce5eb86"}, "originalPosition": 57}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI1MzgxODk4OnYy", "diffSide": "RIGHT", "path": "flink-formats/flink-json/src/test/java/org/apache/flink/formats/json/debezium/DebeziumJsonFileSystemITCase.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wN1QwNTozMzo1MFrOHvD8WA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wN1QwNTozMzo1MFrOHvD8WA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTEwOTcyMA==", "bodyText": "Select some fields to trigger the projection push down? We can write one more column into the sink, e.g UPPER(name), and then select out the upper_name instead of the `name.", "url": "https://github.com/apache/flink/pull/13957#discussion_r519109720", "createdAt": "2020-11-07T05:33:50Z", "author": {"login": "wuchong"}, "path": "flink-formats/flink-json/src/test/java/org/apache/flink/formats/json/debezium/DebeziumJsonFileSystemITCase.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.json.debezium;\n+\n+import org.apache.flink.table.planner.runtime.utils.StreamingTestBase;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.util.CloseableIterator;\n+import org.apache.flink.util.CollectionUtil;\n+\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URL;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import static java.lang.String.format;\n+\n+/**\n+ * Test Filesystem connector with DebeziumJson.\n+ */\n+public class DebeziumJsonFileSystemITCase extends StreamingTestBase {\n+\n+\tprivate static final List<String> EXPECTED = Arrays.asList(\n+\t\t\t\"+I(101,scooter,Small 2-wheel scooter,3.14)\",\n+\t\t\t\"+I(102,car battery,12V car battery,8.1)\",\n+\t\t\t\"+I(103,12-pack drill bits,12-pack of drill bits with sizes ranging from #40 to #3,0.8)\",\n+\t\t\t\"+I(104,hammer,12oz carpenter's hammer,0.75)\",\n+\t\t\t\"+I(105,hammer,14oz carpenter's hammer,0.875)\",\n+\t\t\t\"+I(106,hammer,16oz carpenter's hammer,1.0)\",\n+\t\t\t\"+I(107,rocks,box of assorted rocks,5.3)\",\n+\t\t\t\"+I(108,jacket,water resistent black wind breaker,0.1)\",\n+\t\t\t\"+I(109,spare tire,24 inch spare tire,22.2)\",\n+\t\t\t\"-D(106,hammer,16oz carpenter's hammer,1.0)\", // -U\n+\t\t\t\"+I(106,hammer,18oz carpenter hammer,1.0)\", // +U\n+\t\t\t\"-D(107,rocks,box of assorted rocks,5.3)\", // -U\n+\t\t\t\"+I(107,rocks,box of assorted rocks,5.1)\", // +U\n+\t\t\t\"+I(110,jacket,water resistent white wind breaker,0.2)\",\n+\t\t\t\"+I(111,scooter,Big 2-wheel scooter ,5.18)\",\n+\t\t\t\"-D(110,jacket,water resistent white wind breaker,0.2)\", // -U\n+\t\t\t\"+I(110,jacket,new water resistent white wind breaker,0.5)\", // +U\n+\t\t\t\"-D(111,scooter,Big 2-wheel scooter ,5.18)\", // -U\n+\t\t\t\"+I(111,scooter,Big 2-wheel scooter ,5.17)\", // +U\n+\t\t\t\"-D(111,scooter,Big 2-wheel scooter ,5.17)\"\n+\t);\n+\n+\tprivate File source;\n+\tprivate File sink;\n+\n+\tprivate void prepareTables(boolean isPartition) throws IOException {\n+\t\tbyte[] bytes = readBytes(\"debezium-data-schema-exclude.txt\");\n+\t\tsource = TEMPORARY_FOLDER.newFolder();\n+\t\tFile file;\n+\t\tif (isPartition) {\n+\t\t\tFile partition = new File(source, \"p=1\");\n+\t\t\tpartition.mkdirs();\n+\t\t\tfile = new File(partition, \"my_file\");\n+\t\t} else {\n+\t\t\tfile = new File(source, \"my_file\");\n+\t\t}\n+\t\tfile.createNewFile();\n+\t\tFiles.write(file.toPath(), bytes);\n+\n+\t\tsink = TEMPORARY_FOLDER.newFolder();\n+\n+\t\tenv().setParallelism(1);\n+\t}\n+\n+\tprivate void createTable(String name, String path, boolean isPartition) {\n+\t\ttEnv().executeSql(format(\"create table %s (\", name) +\n+\t\t\t\t\"id int, name string, description string, weight float\" +\n+\t\t\t\t(isPartition ? \", p int) partitioned by (p) \" : \")\") +\n+\t\t\t\t\" with (\" +\n+\t\t\t\t\"'connector'='filesystem',\" +\n+\t\t\t\t\"'format'='debezium-json',\" +\n+\t\t\t\tformat(\"'path'='%s'\", path) +\n+\t\t\t\t\")\");\n+\t}\n+\n+\t@Test\n+\tpublic void testNonPartition() throws Exception {\n+\t\tprepareTables(false);\n+\t\tcreateTable(\"source\", source.toURI().toString(), false);\n+\t\tcreateTable(\"sink\", sink.toURI().toString(), false);\n+\n+\t\ttEnv().executeSql(\"insert into sink select * from source\").await();\n+\t\tCloseableIterator<Row> iter = tEnv().executeSql(\"select * from sink\").collect();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f9c06b641c424a4e3f4e91ecc668b599dd45b6e1"}, "originalPosition": 108}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI1MzgyNzQzOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/data/BinaryRowDataTest.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wN1QwNTozODowOFrOHvECOQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wN1QwNTozODowOFrOHvECOQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTExMTIyNQ==", "bodyText": "remove.", "url": "https://github.com/apache/flink/pull/13957#discussion_r519111225", "createdAt": "2020-11-07T05:38:08Z", "author": {"login": "wuchong"}, "path": "flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/data/BinaryRowDataTest.java", "diffHunk": "@@ -94,6 +94,10 @@\n  */\n public class BinaryRowDataTest {\n \n+\tpublic static void main(String[] args) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f9c06b641c424a4e3f4e91ecc668b599dd45b6e1"}, "originalPosition": 4}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4822, "cost": 1, "resetAt": "2021-11-12T11:57:46Z"}}}