{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzkwNDY2MzM1", "number": 11443, "reviewThreads": {"totalCount": 12, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQxMDo1OTo1N1rODrKTwg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNlQwODo0MDowOFrODripcw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ2NTg0MjU4OnYy", "diffSide": "RIGHT", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTrackerFactory.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQxMDo1OTo1N1rOF7V2Bg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQxMDo1OTo1N1rOF7V2Bg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzc2ODE5OA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            public interface ResourceManagerPartitionTrackerFactory {\n          \n          \n            \n            @FunctionalInterface\n          \n          \n            \n            public interface ResourceManagerPartitionTrackerFactory", "url": "https://github.com/apache/flink/pull/11443#discussion_r397768198", "createdAt": "2020-03-25T10:59:57Z", "author": {"login": "azagrebin"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTrackerFactory.java", "diffHunk": "@@ -0,0 +1,25 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.io.network.partition;\n+\n+/**\n+ * Factory for {@link ResourceManagerPartitionTracker}.\n+ */\n+public interface ResourceManagerPartitionTrackerFactory {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0240c441a2ea285e49af3172c2e5829207be011b"}, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ2NTg1MDY5OnYy", "diffSide": "RIGHT", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ClusterPartitionReleaser.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQxMTowMjoyMlrOF7V7Gg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQxMTowMjoyMlrOF7V7Gg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzc2OTQ5OA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            public interface ClusterPartitionReleaser {\n          \n          \n            \n            @FunctionalInterface\n          \n          \n            \n            public interface ClusterPartitionReleaser {", "url": "https://github.com/apache/flink/pull/11443#discussion_r397769498", "createdAt": "2020-03-25T11:02:22Z", "author": {"login": "azagrebin"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ClusterPartitionReleaser.java", "diffHunk": "@@ -0,0 +1,30 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.io.network.partition;\n+\n+import org.apache.flink.runtime.clusterframework.types.ResourceID;\n+import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;\n+\n+import java.util.Set;\n+\n+/**\n+ * Interface for releasing cluster partitions on a task executor.\n+ */\n+public interface ClusterPartitionReleaser {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0240c441a2ea285e49af3172c2e5829207be011b"}, "originalPosition": 28}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ2NjQ1NjY4OnYy", "diffSide": "RIGHT", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ClusterPartitionReleaser.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQxMzo0NjoyOFrOF7bwdw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQxMzo0NjoyOFrOF7bwdw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg2NTA3OQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            public interface ClusterPartitionReleaser {\n          \n          \n            \n            public interface TaskExecutorClusterPartitionReleaser {\n          \n      \n    \n    \n  \n\nJust if we have also something to release external partitions over shuffle master/ClusterPartitionShuffleClient.", "url": "https://github.com/apache/flink/pull/11443#discussion_r397865079", "createdAt": "2020-03-25T13:46:28Z", "author": {"login": "azagrebin"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ClusterPartitionReleaser.java", "diffHunk": "@@ -0,0 +1,30 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.io.network.partition;\n+\n+import org.apache.flink.runtime.clusterframework.types.ResourceID;\n+import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;\n+\n+import java.util.Set;\n+\n+/**\n+ * Interface for releasing cluster partitions on a task executor.\n+ */\n+public interface ClusterPartitionReleaser {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0240c441a2ea285e49af3172c2e5829207be011b"}, "originalPosition": 28}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ2NjQ3ODc4OnYy", "diffSide": "RIGHT", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTracker.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQxMzo1MDo1NVrOF7b-CQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQwODo1Njo0N1rOF9f4xw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg2ODU1Mw==", "bodyText": "a bit strange that this all resides in network package\nI would expect it to be somewhere in org.apache.flink.runtime.resourcemanager. partition.", "url": "https://github.com/apache/flink/pull/11443#discussion_r397868553", "createdAt": "2020-03-25T13:50:55Z", "author": {"login": "azagrebin"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTracker.java", "diffHunk": "@@ -0,0 +1,67 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.io.network.partition;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0240c441a2ea285e49af3172c2e5829207be011b"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDAyOTg5NQ==", "bodyText": "All partition tracking code is currently in this package. I agree that we may want to move all them elsewhere at some point.", "url": "https://github.com/apache/flink/pull/11443#discussion_r400029895", "createdAt": "2020-03-30T08:56:47Z", "author": {"login": "zentol"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTracker.java", "diffHunk": "@@ -0,0 +1,67 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.io.network.partition;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg2ODU1Mw=="}, "originalCommit": {"oid": "0240c441a2ea285e49af3172c2e5829207be011b"}, "originalPosition": 18}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ2NzQ1MzUxOnYy", "diffSide": "RIGHT", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTrackerImpl.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQxNzoxMzo1MVrOF7lrDg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQwOTowMjo1NlrOF9gH_w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODAyNzUzNA==", "bodyText": "do we want to check whether internalReleasePartitions actually issued any releases?\nto avoid dangling futures in partitionReleaseCompletionFutures if dataSetId is not actually tracked at all", "url": "https://github.com/apache/flink/pull/11443#discussion_r398027534", "createdAt": "2020-03-25T17:13:51Z", "author": {"login": "azagrebin"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTrackerImpl.java", "diffHunk": "@@ -0,0 +1,232 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.io.network.partition;\n+\n+import org.apache.flink.runtime.clusterframework.types.ResourceID;\n+import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;\n+import org.apache.flink.runtime.taskexecutor.partition.ClusterPartitionReport;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Default {@link ResourceManagerPartitionTracker} implementation.\n+ *\n+ * <p>Internal tracking info must only be updated upon reception of a {@link ClusterPartitionReport}, as the task\n+ * executor state is the source of truth.\n+ */\n+public class ResourceManagerPartitionTrackerImpl implements ResourceManagerPartitionTracker {\n+\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(ResourceManagerPartitionTrackerImpl.class);\n+\n+\tprivate final Map<ResourceID, Set<IntermediateDataSetID>> taskExecutorToDataSets = new HashMap<>();\n+\tprivate final Map<IntermediateDataSetID, Map<ResourceID, Set<ResultPartitionID>>> dataSetToTaskExecutors = new HashMap<>();\n+\tprivate final Map<IntermediateDataSetID, DataSetMetaInfo> dataSetMetaInfo = new HashMap<>();\n+\tprivate final Map<IntermediateDataSetID, CompletableFuture<Void>> partitionReleaseCompletionFutures = new HashMap<>();\n+\n+\tprivate final ClusterPartitionReleaser clusterPartitionReleaser;\n+\n+\tpublic ResourceManagerPartitionTrackerImpl(ClusterPartitionReleaser clusterPartitionReleaser) {\n+\t\tthis.clusterPartitionReleaser = clusterPartitionReleaser;\n+\t}\n+\n+\t@Override\n+\tpublic void processTaskExecutorClusterPartitionReport(ResourceID taskExecutorId, ClusterPartitionReport clusterPartitionReport) {\n+\t\tPreconditions.checkNotNull(taskExecutorId);\n+\t\tPreconditions.checkNotNull(clusterPartitionReport);\n+\t\tLOG.debug(\"Processing cluster partition report from task executor {}: {}.\", taskExecutorId, clusterPartitionReport);\n+\n+\t\tinternalProcessClusterPartitionReport(taskExecutorId, clusterPartitionReport);\n+\t}\n+\n+\t@Override\n+\tpublic void processTaskExecutorShutdown(ResourceID taskExecutorId) {\n+\t\tPreconditions.checkNotNull(taskExecutorId);\n+\t\tLOG.debug(\"Processing shutdown of task executor {}.\", taskExecutorId);\n+\n+\t\tinternalProcessClusterPartitionReport(taskExecutorId, new ClusterPartitionReport(Collections.emptyList()));\n+\t}\n+\n+\t@Override\n+\tpublic CompletableFuture<Void> releaseClusterPartitions(IntermediateDataSetID dataSetId) {\n+\t\tPreconditions.checkNotNull(dataSetId);\n+\t\tLOG.debug(\"Releasing cluster partitions for data set {}.\", dataSetId);\n+\n+\t\tCompletableFuture<Void> partitionReleaseCompletionFuture = partitionReleaseCompletionFutures.computeIfAbsent(dataSetId, ignored -> new CompletableFuture<>());\n+\t\tinternalReleasePartitions(Collections.singleton(dataSetId));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0240c441a2ea285e49af3172c2e5829207be011b"}, "originalPosition": 82}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDAzMzc5MQ==", "bodyText": "I will add a check at the start of the method that the partition is being tracked. if it isn't we will log something and return a completed future.", "url": "https://github.com/apache/flink/pull/11443#discussion_r400033791", "createdAt": "2020-03-30T09:02:56Z", "author": {"login": "zentol"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTrackerImpl.java", "diffHunk": "@@ -0,0 +1,232 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.io.network.partition;\n+\n+import org.apache.flink.runtime.clusterframework.types.ResourceID;\n+import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;\n+import org.apache.flink.runtime.taskexecutor.partition.ClusterPartitionReport;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Default {@link ResourceManagerPartitionTracker} implementation.\n+ *\n+ * <p>Internal tracking info must only be updated upon reception of a {@link ClusterPartitionReport}, as the task\n+ * executor state is the source of truth.\n+ */\n+public class ResourceManagerPartitionTrackerImpl implements ResourceManagerPartitionTracker {\n+\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(ResourceManagerPartitionTrackerImpl.class);\n+\n+\tprivate final Map<ResourceID, Set<IntermediateDataSetID>> taskExecutorToDataSets = new HashMap<>();\n+\tprivate final Map<IntermediateDataSetID, Map<ResourceID, Set<ResultPartitionID>>> dataSetToTaskExecutors = new HashMap<>();\n+\tprivate final Map<IntermediateDataSetID, DataSetMetaInfo> dataSetMetaInfo = new HashMap<>();\n+\tprivate final Map<IntermediateDataSetID, CompletableFuture<Void>> partitionReleaseCompletionFutures = new HashMap<>();\n+\n+\tprivate final ClusterPartitionReleaser clusterPartitionReleaser;\n+\n+\tpublic ResourceManagerPartitionTrackerImpl(ClusterPartitionReleaser clusterPartitionReleaser) {\n+\t\tthis.clusterPartitionReleaser = clusterPartitionReleaser;\n+\t}\n+\n+\t@Override\n+\tpublic void processTaskExecutorClusterPartitionReport(ResourceID taskExecutorId, ClusterPartitionReport clusterPartitionReport) {\n+\t\tPreconditions.checkNotNull(taskExecutorId);\n+\t\tPreconditions.checkNotNull(clusterPartitionReport);\n+\t\tLOG.debug(\"Processing cluster partition report from task executor {}: {}.\", taskExecutorId, clusterPartitionReport);\n+\n+\t\tinternalProcessClusterPartitionReport(taskExecutorId, clusterPartitionReport);\n+\t}\n+\n+\t@Override\n+\tpublic void processTaskExecutorShutdown(ResourceID taskExecutorId) {\n+\t\tPreconditions.checkNotNull(taskExecutorId);\n+\t\tLOG.debug(\"Processing shutdown of task executor {}.\", taskExecutorId);\n+\n+\t\tinternalProcessClusterPartitionReport(taskExecutorId, new ClusterPartitionReport(Collections.emptyList()));\n+\t}\n+\n+\t@Override\n+\tpublic CompletableFuture<Void> releaseClusterPartitions(IntermediateDataSetID dataSetId) {\n+\t\tPreconditions.checkNotNull(dataSetId);\n+\t\tLOG.debug(\"Releasing cluster partitions for data set {}.\", dataSetId);\n+\n+\t\tCompletableFuture<Void> partitionReleaseCompletionFuture = partitionReleaseCompletionFutures.computeIfAbsent(dataSetId, ignored -> new CompletableFuture<>());\n+\t\tinternalReleasePartitions(Collections.singleton(dataSetId));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODAyNzUzNA=="}, "originalCommit": {"oid": "0240c441a2ea285e49af3172c2e5829207be011b"}, "originalPosition": 82}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ2NzQ1NzMwOnYy", "diffSide": "RIGHT", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTrackerImpl.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQxNzoxNDozN1rOF7ltXg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQwOTowMzoyOFrOF9gJNA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODAyODEyNg==", "bodyText": "do we also want to GC the completed future from partitionReleaseCompletionFutures?", "url": "https://github.com/apache/flink/pull/11443#discussion_r398028126", "createdAt": "2020-03-25T17:14:37Z", "author": {"login": "azagrebin"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTrackerImpl.java", "diffHunk": "@@ -0,0 +1,232 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.io.network.partition;\n+\n+import org.apache.flink.runtime.clusterframework.types.ResourceID;\n+import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;\n+import org.apache.flink.runtime.taskexecutor.partition.ClusterPartitionReport;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Default {@link ResourceManagerPartitionTracker} implementation.\n+ *\n+ * <p>Internal tracking info must only be updated upon reception of a {@link ClusterPartitionReport}, as the task\n+ * executor state is the source of truth.\n+ */\n+public class ResourceManagerPartitionTrackerImpl implements ResourceManagerPartitionTracker {\n+\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(ResourceManagerPartitionTrackerImpl.class);\n+\n+\tprivate final Map<ResourceID, Set<IntermediateDataSetID>> taskExecutorToDataSets = new HashMap<>();\n+\tprivate final Map<IntermediateDataSetID, Map<ResourceID, Set<ResultPartitionID>>> dataSetToTaskExecutors = new HashMap<>();\n+\tprivate final Map<IntermediateDataSetID, DataSetMetaInfo> dataSetMetaInfo = new HashMap<>();\n+\tprivate final Map<IntermediateDataSetID, CompletableFuture<Void>> partitionReleaseCompletionFutures = new HashMap<>();\n+\n+\tprivate final ClusterPartitionReleaser clusterPartitionReleaser;\n+\n+\tpublic ResourceManagerPartitionTrackerImpl(ClusterPartitionReleaser clusterPartitionReleaser) {\n+\t\tthis.clusterPartitionReleaser = clusterPartitionReleaser;\n+\t}\n+\n+\t@Override\n+\tpublic void processTaskExecutorClusterPartitionReport(ResourceID taskExecutorId, ClusterPartitionReport clusterPartitionReport) {\n+\t\tPreconditions.checkNotNull(taskExecutorId);\n+\t\tPreconditions.checkNotNull(clusterPartitionReport);\n+\t\tLOG.debug(\"Processing cluster partition report from task executor {}: {}.\", taskExecutorId, clusterPartitionReport);\n+\n+\t\tinternalProcessClusterPartitionReport(taskExecutorId, clusterPartitionReport);\n+\t}\n+\n+\t@Override\n+\tpublic void processTaskExecutorShutdown(ResourceID taskExecutorId) {\n+\t\tPreconditions.checkNotNull(taskExecutorId);\n+\t\tLOG.debug(\"Processing shutdown of task executor {}.\", taskExecutorId);\n+\n+\t\tinternalProcessClusterPartitionReport(taskExecutorId, new ClusterPartitionReport(Collections.emptyList()));\n+\t}\n+\n+\t@Override\n+\tpublic CompletableFuture<Void> releaseClusterPartitions(IntermediateDataSetID dataSetId) {\n+\t\tPreconditions.checkNotNull(dataSetId);\n+\t\tLOG.debug(\"Releasing cluster partitions for data set {}.\", dataSetId);\n+\n+\t\tCompletableFuture<Void> partitionReleaseCompletionFuture = partitionReleaseCompletionFutures.computeIfAbsent(dataSetId, ignored -> new CompletableFuture<>());\n+\t\tinternalReleasePartitions(Collections.singleton(dataSetId));\n+\t\treturn partitionReleaseCompletionFuture;\n+\t}\n+\n+\tprivate void internalProcessClusterPartitionReport(ResourceID taskExecutorId, ClusterPartitionReport clusterPartitionReport) {\n+\t\tfinal Set<IntermediateDataSetID> dataSetsWithLostPartitions = clusterPartitionReport.getEntries().isEmpty()\n+\t\t\t? processEmptyReport(taskExecutorId)\n+\t\t\t: setHostedDataSetsAndCheckCorruption(taskExecutorId, clusterPartitionReport.getEntries());\n+\n+\t\tupdateDataSetMetaData(clusterPartitionReport);\n+\n+\t\tcheckForFullyLostDatasets(dataSetsWithLostPartitions);\n+\n+\t\tinternalReleasePartitions(dataSetsWithLostPartitions);\n+\t}\n+\n+\tprivate void internalReleasePartitions(Set<IntermediateDataSetID> dataSetsToRelease) {\n+\t\tMap<ResourceID, Set<IntermediateDataSetID>> releaseCalls = prepareReleaseCalls(dataSetsToRelease);\n+\t\treleaseCalls.forEach(clusterPartitionReleaser::releaseClusterPartitions);\n+\t}\n+\n+\tprivate Set<IntermediateDataSetID> processEmptyReport(ResourceID taskExecutorId) {\n+\t\tSet<IntermediateDataSetID> previouslyHostedDatasets = taskExecutorToDataSets.remove(taskExecutorId);\n+\t\tif (previouslyHostedDatasets == null) {\n+\t\t\t// default path for task executors that never have any cluster partitions\n+\t\t\tpreviouslyHostedDatasets = Collections.emptySet();\n+\t\t} else {\n+\t\t\tpreviouslyHostedDatasets.forEach(dataSetId -> removeInnerKey(dataSetId, taskExecutorId, dataSetToTaskExecutors));\n+\t\t}\n+\t\treturn previouslyHostedDatasets;\n+\t}\n+\n+\t/**\n+\t * Updates the data sets for which the given task executor is hosting partitions and returns data sets that were\n+\t * corrupted due to a loss of partitions.\n+\t *\n+\t * @param taskExecutorId ID of the hosting TaskExecutor\n+\t * @param reportEntries  IDs of data sets for which partitions are hosted\n+\t * @return corrupted data sets\n+\t */\n+\tprivate Set<IntermediateDataSetID> setHostedDataSetsAndCheckCorruption(ResourceID taskExecutorId, Collection<ClusterPartitionReport.ClusterPartitionReportEntry> reportEntries) {\n+\t\tfinal Set<IntermediateDataSetID> currentlyHostedDatasets = reportEntries\n+\t\t\t.stream()\n+\t\t\t.map(ClusterPartitionReport.ClusterPartitionReportEntry::getDataSetId)\n+\t\t\t.collect(Collectors.toSet());\n+\n+\t\tfinal Set<IntermediateDataSetID> previouslyHostedDataSets = taskExecutorToDataSets.put(\n+\t\t\ttaskExecutorId,\n+\t\t\tcurrentlyHostedDatasets);\n+\n+\t\t// previously tracked data sets may be corrupted since we may be tracking less partitions than before\n+\t\tfinal Set<IntermediateDataSetID> potentiallyCorruptedDataSets = Optional\n+\t\t\t.ofNullable(previouslyHostedDataSets)\n+\t\t\t.orElse(new HashSet<>(0));\n+\n+\t\t// update data set -> task executor mapping and find datasets for which lost a partition\n+\t\treportEntries.forEach(hostedPartition -> {\n+\t\t\tfinal Map<ResourceID, Set<ResultPartitionID>> taskExecutorHosts = dataSetToTaskExecutors.computeIfAbsent(hostedPartition.getDataSetId(), ignored -> new HashMap<>());\n+\t\t\tfinal Set<ResultPartitionID> previouslyHostedPartitions = taskExecutorHosts.put(taskExecutorId, hostedPartition.getHostedPartitions());\n+\n+\t\t\tfinal boolean noPartitionLost = previouslyHostedPartitions == null || hostedPartition.getHostedPartitions().containsAll(previouslyHostedPartitions);\n+\t\t\tif (noPartitionLost) {\n+\t\t\t\tpotentiallyCorruptedDataSets.remove(hostedPartition.getDataSetId());\n+\t\t\t}\n+\t\t});\n+\n+\t\t// now only contains data sets for which a partition is no longer tracked\n+\t\treturn potentiallyCorruptedDataSets;\n+\t}\n+\n+\tprivate void updateDataSetMetaData(ClusterPartitionReport clusterPartitionReport) {\n+\t\t// add meta info for new data sets\n+\t\tclusterPartitionReport.getEntries().forEach(entry ->\n+\t\t\tdataSetMetaInfo.compute(entry.getDataSetId(), (dataSetID, dataSetMetaInfo) -> {\n+\t\t\t\tif (dataSetMetaInfo == null) {\n+\t\t\t\t\treturn new DataSetMetaInfo(entry.getNumTotalPartitions());\n+\t\t\t\t} else {\n+\t\t\t\t\t// double check that the meta data is consistent\n+\t\t\t\t\tPreconditions.checkState(dataSetMetaInfo.getNumTotalPartitions() == entry.getNumTotalPartitions());\n+\t\t\t\t\treturn dataSetMetaInfo;\n+\t\t\t\t}\n+\t\t\t}));\n+\t}\n+\n+\tprivate void checkForFullyLostDatasets(Set<IntermediateDataSetID> dataSetsWithLostPartitions) {\n+\t\tdataSetsWithLostPartitions.forEach(dataSetId -> {\n+\t\t\tif (getHostingTaskExecutors(dataSetId).isEmpty()) {\n+\t\t\t\tLOG.debug(\"There are no longer partitions being tracked for dataset {}.\", dataSetId);\n+\t\t\t\tdataSetMetaInfo.remove(dataSetId);\n+\t\t\t\tOptional.ofNullable(partitionReleaseCompletionFutures.get(dataSetId)).map(future -> future.complete(null));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0240c441a2ea285e49af3172c2e5829207be011b"}, "originalPosition": 171}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDAzNDEwMA==", "bodyText": "yes, will change it to remove() the future instead of get().", "url": "https://github.com/apache/flink/pull/11443#discussion_r400034100", "createdAt": "2020-03-30T09:03:28Z", "author": {"login": "zentol"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTrackerImpl.java", "diffHunk": "@@ -0,0 +1,232 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.io.network.partition;\n+\n+import org.apache.flink.runtime.clusterframework.types.ResourceID;\n+import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;\n+import org.apache.flink.runtime.taskexecutor.partition.ClusterPartitionReport;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Default {@link ResourceManagerPartitionTracker} implementation.\n+ *\n+ * <p>Internal tracking info must only be updated upon reception of a {@link ClusterPartitionReport}, as the task\n+ * executor state is the source of truth.\n+ */\n+public class ResourceManagerPartitionTrackerImpl implements ResourceManagerPartitionTracker {\n+\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(ResourceManagerPartitionTrackerImpl.class);\n+\n+\tprivate final Map<ResourceID, Set<IntermediateDataSetID>> taskExecutorToDataSets = new HashMap<>();\n+\tprivate final Map<IntermediateDataSetID, Map<ResourceID, Set<ResultPartitionID>>> dataSetToTaskExecutors = new HashMap<>();\n+\tprivate final Map<IntermediateDataSetID, DataSetMetaInfo> dataSetMetaInfo = new HashMap<>();\n+\tprivate final Map<IntermediateDataSetID, CompletableFuture<Void>> partitionReleaseCompletionFutures = new HashMap<>();\n+\n+\tprivate final ClusterPartitionReleaser clusterPartitionReleaser;\n+\n+\tpublic ResourceManagerPartitionTrackerImpl(ClusterPartitionReleaser clusterPartitionReleaser) {\n+\t\tthis.clusterPartitionReleaser = clusterPartitionReleaser;\n+\t}\n+\n+\t@Override\n+\tpublic void processTaskExecutorClusterPartitionReport(ResourceID taskExecutorId, ClusterPartitionReport clusterPartitionReport) {\n+\t\tPreconditions.checkNotNull(taskExecutorId);\n+\t\tPreconditions.checkNotNull(clusterPartitionReport);\n+\t\tLOG.debug(\"Processing cluster partition report from task executor {}: {}.\", taskExecutorId, clusterPartitionReport);\n+\n+\t\tinternalProcessClusterPartitionReport(taskExecutorId, clusterPartitionReport);\n+\t}\n+\n+\t@Override\n+\tpublic void processTaskExecutorShutdown(ResourceID taskExecutorId) {\n+\t\tPreconditions.checkNotNull(taskExecutorId);\n+\t\tLOG.debug(\"Processing shutdown of task executor {}.\", taskExecutorId);\n+\n+\t\tinternalProcessClusterPartitionReport(taskExecutorId, new ClusterPartitionReport(Collections.emptyList()));\n+\t}\n+\n+\t@Override\n+\tpublic CompletableFuture<Void> releaseClusterPartitions(IntermediateDataSetID dataSetId) {\n+\t\tPreconditions.checkNotNull(dataSetId);\n+\t\tLOG.debug(\"Releasing cluster partitions for data set {}.\", dataSetId);\n+\n+\t\tCompletableFuture<Void> partitionReleaseCompletionFuture = partitionReleaseCompletionFutures.computeIfAbsent(dataSetId, ignored -> new CompletableFuture<>());\n+\t\tinternalReleasePartitions(Collections.singleton(dataSetId));\n+\t\treturn partitionReleaseCompletionFuture;\n+\t}\n+\n+\tprivate void internalProcessClusterPartitionReport(ResourceID taskExecutorId, ClusterPartitionReport clusterPartitionReport) {\n+\t\tfinal Set<IntermediateDataSetID> dataSetsWithLostPartitions = clusterPartitionReport.getEntries().isEmpty()\n+\t\t\t? processEmptyReport(taskExecutorId)\n+\t\t\t: setHostedDataSetsAndCheckCorruption(taskExecutorId, clusterPartitionReport.getEntries());\n+\n+\t\tupdateDataSetMetaData(clusterPartitionReport);\n+\n+\t\tcheckForFullyLostDatasets(dataSetsWithLostPartitions);\n+\n+\t\tinternalReleasePartitions(dataSetsWithLostPartitions);\n+\t}\n+\n+\tprivate void internalReleasePartitions(Set<IntermediateDataSetID> dataSetsToRelease) {\n+\t\tMap<ResourceID, Set<IntermediateDataSetID>> releaseCalls = prepareReleaseCalls(dataSetsToRelease);\n+\t\treleaseCalls.forEach(clusterPartitionReleaser::releaseClusterPartitions);\n+\t}\n+\n+\tprivate Set<IntermediateDataSetID> processEmptyReport(ResourceID taskExecutorId) {\n+\t\tSet<IntermediateDataSetID> previouslyHostedDatasets = taskExecutorToDataSets.remove(taskExecutorId);\n+\t\tif (previouslyHostedDatasets == null) {\n+\t\t\t// default path for task executors that never have any cluster partitions\n+\t\t\tpreviouslyHostedDatasets = Collections.emptySet();\n+\t\t} else {\n+\t\t\tpreviouslyHostedDatasets.forEach(dataSetId -> removeInnerKey(dataSetId, taskExecutorId, dataSetToTaskExecutors));\n+\t\t}\n+\t\treturn previouslyHostedDatasets;\n+\t}\n+\n+\t/**\n+\t * Updates the data sets for which the given task executor is hosting partitions and returns data sets that were\n+\t * corrupted due to a loss of partitions.\n+\t *\n+\t * @param taskExecutorId ID of the hosting TaskExecutor\n+\t * @param reportEntries  IDs of data sets for which partitions are hosted\n+\t * @return corrupted data sets\n+\t */\n+\tprivate Set<IntermediateDataSetID> setHostedDataSetsAndCheckCorruption(ResourceID taskExecutorId, Collection<ClusterPartitionReport.ClusterPartitionReportEntry> reportEntries) {\n+\t\tfinal Set<IntermediateDataSetID> currentlyHostedDatasets = reportEntries\n+\t\t\t.stream()\n+\t\t\t.map(ClusterPartitionReport.ClusterPartitionReportEntry::getDataSetId)\n+\t\t\t.collect(Collectors.toSet());\n+\n+\t\tfinal Set<IntermediateDataSetID> previouslyHostedDataSets = taskExecutorToDataSets.put(\n+\t\t\ttaskExecutorId,\n+\t\t\tcurrentlyHostedDatasets);\n+\n+\t\t// previously tracked data sets may be corrupted since we may be tracking less partitions than before\n+\t\tfinal Set<IntermediateDataSetID> potentiallyCorruptedDataSets = Optional\n+\t\t\t.ofNullable(previouslyHostedDataSets)\n+\t\t\t.orElse(new HashSet<>(0));\n+\n+\t\t// update data set -> task executor mapping and find datasets for which lost a partition\n+\t\treportEntries.forEach(hostedPartition -> {\n+\t\t\tfinal Map<ResourceID, Set<ResultPartitionID>> taskExecutorHosts = dataSetToTaskExecutors.computeIfAbsent(hostedPartition.getDataSetId(), ignored -> new HashMap<>());\n+\t\t\tfinal Set<ResultPartitionID> previouslyHostedPartitions = taskExecutorHosts.put(taskExecutorId, hostedPartition.getHostedPartitions());\n+\n+\t\t\tfinal boolean noPartitionLost = previouslyHostedPartitions == null || hostedPartition.getHostedPartitions().containsAll(previouslyHostedPartitions);\n+\t\t\tif (noPartitionLost) {\n+\t\t\t\tpotentiallyCorruptedDataSets.remove(hostedPartition.getDataSetId());\n+\t\t\t}\n+\t\t});\n+\n+\t\t// now only contains data sets for which a partition is no longer tracked\n+\t\treturn potentiallyCorruptedDataSets;\n+\t}\n+\n+\tprivate void updateDataSetMetaData(ClusterPartitionReport clusterPartitionReport) {\n+\t\t// add meta info for new data sets\n+\t\tclusterPartitionReport.getEntries().forEach(entry ->\n+\t\t\tdataSetMetaInfo.compute(entry.getDataSetId(), (dataSetID, dataSetMetaInfo) -> {\n+\t\t\t\tif (dataSetMetaInfo == null) {\n+\t\t\t\t\treturn new DataSetMetaInfo(entry.getNumTotalPartitions());\n+\t\t\t\t} else {\n+\t\t\t\t\t// double check that the meta data is consistent\n+\t\t\t\t\tPreconditions.checkState(dataSetMetaInfo.getNumTotalPartitions() == entry.getNumTotalPartitions());\n+\t\t\t\t\treturn dataSetMetaInfo;\n+\t\t\t\t}\n+\t\t\t}));\n+\t}\n+\n+\tprivate void checkForFullyLostDatasets(Set<IntermediateDataSetID> dataSetsWithLostPartitions) {\n+\t\tdataSetsWithLostPartitions.forEach(dataSetId -> {\n+\t\t\tif (getHostingTaskExecutors(dataSetId).isEmpty()) {\n+\t\t\t\tLOG.debug(\"There are no longer partitions being tracked for dataset {}.\", dataSetId);\n+\t\t\t\tdataSetMetaInfo.remove(dataSetId);\n+\t\t\t\tOptional.ofNullable(partitionReleaseCompletionFutures.get(dataSetId)).map(future -> future.complete(null));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODAyODEyNg=="}, "originalCommit": {"oid": "0240c441a2ea285e49af3172c2e5829207be011b"}, "originalPosition": 171}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ2NzY3NjUwOnYy", "diffSide": "RIGHT", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTrackerImpl.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQxODowNTo0MlrOF7n4Zw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQwOTowNDoxM1rOF9gLYw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODA2MzcxOQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \t\t\t\tint numTrackedPartitions = 0;\n          \n          \n            \n            \t\t\t\tfor (Set<ResultPartitionID> hostedPartitions : taskExecutorToPartitions.values()) {\n          \n          \n            \n            \t\t\t\t\tnumTrackedPartitions += hostedPartitions.size();\n          \n          \n            \n            \t\t\t\t}\n          \n          \n            \n            \t\t\t\tfinal int numTrackedPartitions = taskExecutorToPartitions.values().stream().mapToInt(Set::size).sum();\n          \n      \n    \n    \n  \n\nnit idea, as you seem to like streams :)", "url": "https://github.com/apache/flink/pull/11443#discussion_r398063719", "createdAt": "2020-03-25T18:05:42Z", "author": {"login": "azagrebin"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTrackerImpl.java", "diffHunk": "@@ -0,0 +1,232 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.io.network.partition;\n+\n+import org.apache.flink.runtime.clusterframework.types.ResourceID;\n+import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;\n+import org.apache.flink.runtime.taskexecutor.partition.ClusterPartitionReport;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Default {@link ResourceManagerPartitionTracker} implementation.\n+ *\n+ * <p>Internal tracking info must only be updated upon reception of a {@link ClusterPartitionReport}, as the task\n+ * executor state is the source of truth.\n+ */\n+public class ResourceManagerPartitionTrackerImpl implements ResourceManagerPartitionTracker {\n+\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(ResourceManagerPartitionTrackerImpl.class);\n+\n+\tprivate final Map<ResourceID, Set<IntermediateDataSetID>> taskExecutorToDataSets = new HashMap<>();\n+\tprivate final Map<IntermediateDataSetID, Map<ResourceID, Set<ResultPartitionID>>> dataSetToTaskExecutors = new HashMap<>();\n+\tprivate final Map<IntermediateDataSetID, DataSetMetaInfo> dataSetMetaInfo = new HashMap<>();\n+\tprivate final Map<IntermediateDataSetID, CompletableFuture<Void>> partitionReleaseCompletionFutures = new HashMap<>();\n+\n+\tprivate final ClusterPartitionReleaser clusterPartitionReleaser;\n+\n+\tpublic ResourceManagerPartitionTrackerImpl(ClusterPartitionReleaser clusterPartitionReleaser) {\n+\t\tthis.clusterPartitionReleaser = clusterPartitionReleaser;\n+\t}\n+\n+\t@Override\n+\tpublic void processTaskExecutorClusterPartitionReport(ResourceID taskExecutorId, ClusterPartitionReport clusterPartitionReport) {\n+\t\tPreconditions.checkNotNull(taskExecutorId);\n+\t\tPreconditions.checkNotNull(clusterPartitionReport);\n+\t\tLOG.debug(\"Processing cluster partition report from task executor {}: {}.\", taskExecutorId, clusterPartitionReport);\n+\n+\t\tinternalProcessClusterPartitionReport(taskExecutorId, clusterPartitionReport);\n+\t}\n+\n+\t@Override\n+\tpublic void processTaskExecutorShutdown(ResourceID taskExecutorId) {\n+\t\tPreconditions.checkNotNull(taskExecutorId);\n+\t\tLOG.debug(\"Processing shutdown of task executor {}.\", taskExecutorId);\n+\n+\t\tinternalProcessClusterPartitionReport(taskExecutorId, new ClusterPartitionReport(Collections.emptyList()));\n+\t}\n+\n+\t@Override\n+\tpublic CompletableFuture<Void> releaseClusterPartitions(IntermediateDataSetID dataSetId) {\n+\t\tPreconditions.checkNotNull(dataSetId);\n+\t\tLOG.debug(\"Releasing cluster partitions for data set {}.\", dataSetId);\n+\n+\t\tCompletableFuture<Void> partitionReleaseCompletionFuture = partitionReleaseCompletionFutures.computeIfAbsent(dataSetId, ignored -> new CompletableFuture<>());\n+\t\tinternalReleasePartitions(Collections.singleton(dataSetId));\n+\t\treturn partitionReleaseCompletionFuture;\n+\t}\n+\n+\tprivate void internalProcessClusterPartitionReport(ResourceID taskExecutorId, ClusterPartitionReport clusterPartitionReport) {\n+\t\tfinal Set<IntermediateDataSetID> dataSetsWithLostPartitions = clusterPartitionReport.getEntries().isEmpty()\n+\t\t\t? processEmptyReport(taskExecutorId)\n+\t\t\t: setHostedDataSetsAndCheckCorruption(taskExecutorId, clusterPartitionReport.getEntries());\n+\n+\t\tupdateDataSetMetaData(clusterPartitionReport);\n+\n+\t\tcheckForFullyLostDatasets(dataSetsWithLostPartitions);\n+\n+\t\tinternalReleasePartitions(dataSetsWithLostPartitions);\n+\t}\n+\n+\tprivate void internalReleasePartitions(Set<IntermediateDataSetID> dataSetsToRelease) {\n+\t\tMap<ResourceID, Set<IntermediateDataSetID>> releaseCalls = prepareReleaseCalls(dataSetsToRelease);\n+\t\treleaseCalls.forEach(clusterPartitionReleaser::releaseClusterPartitions);\n+\t}\n+\n+\tprivate Set<IntermediateDataSetID> processEmptyReport(ResourceID taskExecutorId) {\n+\t\tSet<IntermediateDataSetID> previouslyHostedDatasets = taskExecutorToDataSets.remove(taskExecutorId);\n+\t\tif (previouslyHostedDatasets == null) {\n+\t\t\t// default path for task executors that never have any cluster partitions\n+\t\t\tpreviouslyHostedDatasets = Collections.emptySet();\n+\t\t} else {\n+\t\t\tpreviouslyHostedDatasets.forEach(dataSetId -> removeInnerKey(dataSetId, taskExecutorId, dataSetToTaskExecutors));\n+\t\t}\n+\t\treturn previouslyHostedDatasets;\n+\t}\n+\n+\t/**\n+\t * Updates the data sets for which the given task executor is hosting partitions and returns data sets that were\n+\t * corrupted due to a loss of partitions.\n+\t *\n+\t * @param taskExecutorId ID of the hosting TaskExecutor\n+\t * @param reportEntries  IDs of data sets for which partitions are hosted\n+\t * @return corrupted data sets\n+\t */\n+\tprivate Set<IntermediateDataSetID> setHostedDataSetsAndCheckCorruption(ResourceID taskExecutorId, Collection<ClusterPartitionReport.ClusterPartitionReportEntry> reportEntries) {\n+\t\tfinal Set<IntermediateDataSetID> currentlyHostedDatasets = reportEntries\n+\t\t\t.stream()\n+\t\t\t.map(ClusterPartitionReport.ClusterPartitionReportEntry::getDataSetId)\n+\t\t\t.collect(Collectors.toSet());\n+\n+\t\tfinal Set<IntermediateDataSetID> previouslyHostedDataSets = taskExecutorToDataSets.put(\n+\t\t\ttaskExecutorId,\n+\t\t\tcurrentlyHostedDatasets);\n+\n+\t\t// previously tracked data sets may be corrupted since we may be tracking less partitions than before\n+\t\tfinal Set<IntermediateDataSetID> potentiallyCorruptedDataSets = Optional\n+\t\t\t.ofNullable(previouslyHostedDataSets)\n+\t\t\t.orElse(new HashSet<>(0));\n+\n+\t\t// update data set -> task executor mapping and find datasets for which lost a partition\n+\t\treportEntries.forEach(hostedPartition -> {\n+\t\t\tfinal Map<ResourceID, Set<ResultPartitionID>> taskExecutorHosts = dataSetToTaskExecutors.computeIfAbsent(hostedPartition.getDataSetId(), ignored -> new HashMap<>());\n+\t\t\tfinal Set<ResultPartitionID> previouslyHostedPartitions = taskExecutorHosts.put(taskExecutorId, hostedPartition.getHostedPartitions());\n+\n+\t\t\tfinal boolean noPartitionLost = previouslyHostedPartitions == null || hostedPartition.getHostedPartitions().containsAll(previouslyHostedPartitions);\n+\t\t\tif (noPartitionLost) {\n+\t\t\t\tpotentiallyCorruptedDataSets.remove(hostedPartition.getDataSetId());\n+\t\t\t}\n+\t\t});\n+\n+\t\t// now only contains data sets for which a partition is no longer tracked\n+\t\treturn potentiallyCorruptedDataSets;\n+\t}\n+\n+\tprivate void updateDataSetMetaData(ClusterPartitionReport clusterPartitionReport) {\n+\t\t// add meta info for new data sets\n+\t\tclusterPartitionReport.getEntries().forEach(entry ->\n+\t\t\tdataSetMetaInfo.compute(entry.getDataSetId(), (dataSetID, dataSetMetaInfo) -> {\n+\t\t\t\tif (dataSetMetaInfo == null) {\n+\t\t\t\t\treturn new DataSetMetaInfo(entry.getNumTotalPartitions());\n+\t\t\t\t} else {\n+\t\t\t\t\t// double check that the meta data is consistent\n+\t\t\t\t\tPreconditions.checkState(dataSetMetaInfo.getNumTotalPartitions() == entry.getNumTotalPartitions());\n+\t\t\t\t\treturn dataSetMetaInfo;\n+\t\t\t\t}\n+\t\t\t}));\n+\t}\n+\n+\tprivate void checkForFullyLostDatasets(Set<IntermediateDataSetID> dataSetsWithLostPartitions) {\n+\t\tdataSetsWithLostPartitions.forEach(dataSetId -> {\n+\t\t\tif (getHostingTaskExecutors(dataSetId).isEmpty()) {\n+\t\t\t\tLOG.debug(\"There are no longer partitions being tracked for dataset {}.\", dataSetId);\n+\t\t\t\tdataSetMetaInfo.remove(dataSetId);\n+\t\t\t\tOptional.ofNullable(partitionReleaseCompletionFutures.get(dataSetId)).map(future -> future.complete(null));\n+\t\t\t}\n+\t\t});\n+\t}\n+\n+\tprivate Map<ResourceID, Set<IntermediateDataSetID>> prepareReleaseCalls(Set<IntermediateDataSetID> dataSetsToRelease) {\n+\t\tfinal Map<ResourceID, Set<IntermediateDataSetID>> releaseCalls = new HashMap<>();\n+\t\tdataSetsToRelease.forEach(dataSetToRelease -> {\n+\t\t\tfinal Set<ResourceID> hostingTaskExecutors = getHostingTaskExecutors(dataSetToRelease);\n+\t\t\thostingTaskExecutors.forEach(hostingTaskExecutor -> insert(hostingTaskExecutor, dataSetToRelease, releaseCalls));\n+\t\t});\n+\t\treturn releaseCalls;\n+\t}\n+\n+\tprivate Set<ResourceID> getHostingTaskExecutors(IntermediateDataSetID dataSetId) {\n+\t\tPreconditions.checkNotNull(dataSetId);\n+\n+\t\tMap<ResourceID, Set<ResultPartitionID>> trackedPartitions = dataSetToTaskExecutors.get(dataSetId);\n+\t\tif (trackedPartitions == null) {\n+\t\t\treturn Collections.emptySet();\n+\t\t} else {\n+\t\t\treturn trackedPartitions.keySet();\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic Map<IntermediateDataSetID, DataSetMetaInfo> listDataSets() {\n+\t\treturn dataSetMetaInfo.entrySet().stream()\n+\t\t\t.filter(entry -> {\n+\t\t\t\tfinal Map<ResourceID, Set<ResultPartitionID>> taskExecutorToPartitions = dataSetToTaskExecutors.get(entry.getKey());\n+\t\t\t\tPreconditions.checkState(taskExecutorToPartitions != null, \"Have metadata entry for dataset %s, but no partition is tracked.\", entry.getKey());\n+\n+\t\t\t\tint numTrackedPartitions = 0;\n+\t\t\t\tfor (Set<ResultPartitionID> hostedPartitions : taskExecutorToPartitions.values()) {\n+\t\t\t\t\tnumTrackedPartitions += hostedPartitions.size();\n+\t\t\t\t}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0240c441a2ea285e49af3172c2e5829207be011b"}, "originalPosition": 206}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDAzNDY1OQ==", "bodyText": "I had the streams solution before but it was such an eye-sore that i opted for the simple loop approach instead.", "url": "https://github.com/apache/flink/pull/11443#discussion_r400034659", "createdAt": "2020-03-30T09:04:13Z", "author": {"login": "zentol"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTrackerImpl.java", "diffHunk": "@@ -0,0 +1,232 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.io.network.partition;\n+\n+import org.apache.flink.runtime.clusterframework.types.ResourceID;\n+import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;\n+import org.apache.flink.runtime.taskexecutor.partition.ClusterPartitionReport;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Default {@link ResourceManagerPartitionTracker} implementation.\n+ *\n+ * <p>Internal tracking info must only be updated upon reception of a {@link ClusterPartitionReport}, as the task\n+ * executor state is the source of truth.\n+ */\n+public class ResourceManagerPartitionTrackerImpl implements ResourceManagerPartitionTracker {\n+\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(ResourceManagerPartitionTrackerImpl.class);\n+\n+\tprivate final Map<ResourceID, Set<IntermediateDataSetID>> taskExecutorToDataSets = new HashMap<>();\n+\tprivate final Map<IntermediateDataSetID, Map<ResourceID, Set<ResultPartitionID>>> dataSetToTaskExecutors = new HashMap<>();\n+\tprivate final Map<IntermediateDataSetID, DataSetMetaInfo> dataSetMetaInfo = new HashMap<>();\n+\tprivate final Map<IntermediateDataSetID, CompletableFuture<Void>> partitionReleaseCompletionFutures = new HashMap<>();\n+\n+\tprivate final ClusterPartitionReleaser clusterPartitionReleaser;\n+\n+\tpublic ResourceManagerPartitionTrackerImpl(ClusterPartitionReleaser clusterPartitionReleaser) {\n+\t\tthis.clusterPartitionReleaser = clusterPartitionReleaser;\n+\t}\n+\n+\t@Override\n+\tpublic void processTaskExecutorClusterPartitionReport(ResourceID taskExecutorId, ClusterPartitionReport clusterPartitionReport) {\n+\t\tPreconditions.checkNotNull(taskExecutorId);\n+\t\tPreconditions.checkNotNull(clusterPartitionReport);\n+\t\tLOG.debug(\"Processing cluster partition report from task executor {}: {}.\", taskExecutorId, clusterPartitionReport);\n+\n+\t\tinternalProcessClusterPartitionReport(taskExecutorId, clusterPartitionReport);\n+\t}\n+\n+\t@Override\n+\tpublic void processTaskExecutorShutdown(ResourceID taskExecutorId) {\n+\t\tPreconditions.checkNotNull(taskExecutorId);\n+\t\tLOG.debug(\"Processing shutdown of task executor {}.\", taskExecutorId);\n+\n+\t\tinternalProcessClusterPartitionReport(taskExecutorId, new ClusterPartitionReport(Collections.emptyList()));\n+\t}\n+\n+\t@Override\n+\tpublic CompletableFuture<Void> releaseClusterPartitions(IntermediateDataSetID dataSetId) {\n+\t\tPreconditions.checkNotNull(dataSetId);\n+\t\tLOG.debug(\"Releasing cluster partitions for data set {}.\", dataSetId);\n+\n+\t\tCompletableFuture<Void> partitionReleaseCompletionFuture = partitionReleaseCompletionFutures.computeIfAbsent(dataSetId, ignored -> new CompletableFuture<>());\n+\t\tinternalReleasePartitions(Collections.singleton(dataSetId));\n+\t\treturn partitionReleaseCompletionFuture;\n+\t}\n+\n+\tprivate void internalProcessClusterPartitionReport(ResourceID taskExecutorId, ClusterPartitionReport clusterPartitionReport) {\n+\t\tfinal Set<IntermediateDataSetID> dataSetsWithLostPartitions = clusterPartitionReport.getEntries().isEmpty()\n+\t\t\t? processEmptyReport(taskExecutorId)\n+\t\t\t: setHostedDataSetsAndCheckCorruption(taskExecutorId, clusterPartitionReport.getEntries());\n+\n+\t\tupdateDataSetMetaData(clusterPartitionReport);\n+\n+\t\tcheckForFullyLostDatasets(dataSetsWithLostPartitions);\n+\n+\t\tinternalReleasePartitions(dataSetsWithLostPartitions);\n+\t}\n+\n+\tprivate void internalReleasePartitions(Set<IntermediateDataSetID> dataSetsToRelease) {\n+\t\tMap<ResourceID, Set<IntermediateDataSetID>> releaseCalls = prepareReleaseCalls(dataSetsToRelease);\n+\t\treleaseCalls.forEach(clusterPartitionReleaser::releaseClusterPartitions);\n+\t}\n+\n+\tprivate Set<IntermediateDataSetID> processEmptyReport(ResourceID taskExecutorId) {\n+\t\tSet<IntermediateDataSetID> previouslyHostedDatasets = taskExecutorToDataSets.remove(taskExecutorId);\n+\t\tif (previouslyHostedDatasets == null) {\n+\t\t\t// default path for task executors that never have any cluster partitions\n+\t\t\tpreviouslyHostedDatasets = Collections.emptySet();\n+\t\t} else {\n+\t\t\tpreviouslyHostedDatasets.forEach(dataSetId -> removeInnerKey(dataSetId, taskExecutorId, dataSetToTaskExecutors));\n+\t\t}\n+\t\treturn previouslyHostedDatasets;\n+\t}\n+\n+\t/**\n+\t * Updates the data sets for which the given task executor is hosting partitions and returns data sets that were\n+\t * corrupted due to a loss of partitions.\n+\t *\n+\t * @param taskExecutorId ID of the hosting TaskExecutor\n+\t * @param reportEntries  IDs of data sets for which partitions are hosted\n+\t * @return corrupted data sets\n+\t */\n+\tprivate Set<IntermediateDataSetID> setHostedDataSetsAndCheckCorruption(ResourceID taskExecutorId, Collection<ClusterPartitionReport.ClusterPartitionReportEntry> reportEntries) {\n+\t\tfinal Set<IntermediateDataSetID> currentlyHostedDatasets = reportEntries\n+\t\t\t.stream()\n+\t\t\t.map(ClusterPartitionReport.ClusterPartitionReportEntry::getDataSetId)\n+\t\t\t.collect(Collectors.toSet());\n+\n+\t\tfinal Set<IntermediateDataSetID> previouslyHostedDataSets = taskExecutorToDataSets.put(\n+\t\t\ttaskExecutorId,\n+\t\t\tcurrentlyHostedDatasets);\n+\n+\t\t// previously tracked data sets may be corrupted since we may be tracking less partitions than before\n+\t\tfinal Set<IntermediateDataSetID> potentiallyCorruptedDataSets = Optional\n+\t\t\t.ofNullable(previouslyHostedDataSets)\n+\t\t\t.orElse(new HashSet<>(0));\n+\n+\t\t// update data set -> task executor mapping and find datasets for which lost a partition\n+\t\treportEntries.forEach(hostedPartition -> {\n+\t\t\tfinal Map<ResourceID, Set<ResultPartitionID>> taskExecutorHosts = dataSetToTaskExecutors.computeIfAbsent(hostedPartition.getDataSetId(), ignored -> new HashMap<>());\n+\t\t\tfinal Set<ResultPartitionID> previouslyHostedPartitions = taskExecutorHosts.put(taskExecutorId, hostedPartition.getHostedPartitions());\n+\n+\t\t\tfinal boolean noPartitionLost = previouslyHostedPartitions == null || hostedPartition.getHostedPartitions().containsAll(previouslyHostedPartitions);\n+\t\t\tif (noPartitionLost) {\n+\t\t\t\tpotentiallyCorruptedDataSets.remove(hostedPartition.getDataSetId());\n+\t\t\t}\n+\t\t});\n+\n+\t\t// now only contains data sets for which a partition is no longer tracked\n+\t\treturn potentiallyCorruptedDataSets;\n+\t}\n+\n+\tprivate void updateDataSetMetaData(ClusterPartitionReport clusterPartitionReport) {\n+\t\t// add meta info for new data sets\n+\t\tclusterPartitionReport.getEntries().forEach(entry ->\n+\t\t\tdataSetMetaInfo.compute(entry.getDataSetId(), (dataSetID, dataSetMetaInfo) -> {\n+\t\t\t\tif (dataSetMetaInfo == null) {\n+\t\t\t\t\treturn new DataSetMetaInfo(entry.getNumTotalPartitions());\n+\t\t\t\t} else {\n+\t\t\t\t\t// double check that the meta data is consistent\n+\t\t\t\t\tPreconditions.checkState(dataSetMetaInfo.getNumTotalPartitions() == entry.getNumTotalPartitions());\n+\t\t\t\t\treturn dataSetMetaInfo;\n+\t\t\t\t}\n+\t\t\t}));\n+\t}\n+\n+\tprivate void checkForFullyLostDatasets(Set<IntermediateDataSetID> dataSetsWithLostPartitions) {\n+\t\tdataSetsWithLostPartitions.forEach(dataSetId -> {\n+\t\t\tif (getHostingTaskExecutors(dataSetId).isEmpty()) {\n+\t\t\t\tLOG.debug(\"There are no longer partitions being tracked for dataset {}.\", dataSetId);\n+\t\t\t\tdataSetMetaInfo.remove(dataSetId);\n+\t\t\t\tOptional.ofNullable(partitionReleaseCompletionFutures.get(dataSetId)).map(future -> future.complete(null));\n+\t\t\t}\n+\t\t});\n+\t}\n+\n+\tprivate Map<ResourceID, Set<IntermediateDataSetID>> prepareReleaseCalls(Set<IntermediateDataSetID> dataSetsToRelease) {\n+\t\tfinal Map<ResourceID, Set<IntermediateDataSetID>> releaseCalls = new HashMap<>();\n+\t\tdataSetsToRelease.forEach(dataSetToRelease -> {\n+\t\t\tfinal Set<ResourceID> hostingTaskExecutors = getHostingTaskExecutors(dataSetToRelease);\n+\t\t\thostingTaskExecutors.forEach(hostingTaskExecutor -> insert(hostingTaskExecutor, dataSetToRelease, releaseCalls));\n+\t\t});\n+\t\treturn releaseCalls;\n+\t}\n+\n+\tprivate Set<ResourceID> getHostingTaskExecutors(IntermediateDataSetID dataSetId) {\n+\t\tPreconditions.checkNotNull(dataSetId);\n+\n+\t\tMap<ResourceID, Set<ResultPartitionID>> trackedPartitions = dataSetToTaskExecutors.get(dataSetId);\n+\t\tif (trackedPartitions == null) {\n+\t\t\treturn Collections.emptySet();\n+\t\t} else {\n+\t\t\treturn trackedPartitions.keySet();\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic Map<IntermediateDataSetID, DataSetMetaInfo> listDataSets() {\n+\t\treturn dataSetMetaInfo.entrySet().stream()\n+\t\t\t.filter(entry -> {\n+\t\t\t\tfinal Map<ResourceID, Set<ResultPartitionID>> taskExecutorToPartitions = dataSetToTaskExecutors.get(entry.getKey());\n+\t\t\t\tPreconditions.checkState(taskExecutorToPartitions != null, \"Have metadata entry for dataset %s, but no partition is tracked.\", entry.getKey());\n+\n+\t\t\t\tint numTrackedPartitions = 0;\n+\t\t\t\tfor (Set<ResultPartitionID> hostedPartitions : taskExecutorToPartitions.values()) {\n+\t\t\t\t\tnumTrackedPartitions += hostedPartitions.size();\n+\t\t\t\t}", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODA2MzcxOQ=="}, "originalCommit": {"oid": "0240c441a2ea285e49af3172c2e5829207be011b"}, "originalPosition": 206}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ2OTU5OTkzOnYy", "diffSide": "RIGHT", "path": "flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTrackerImplTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNlQwNzoyNzo0N1rOF76H7A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQwOTowNTo0NVrOF9gO5w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODM2MjYwNA==", "bodyText": "nit: can be private", "url": "https://github.com/apache/flink/pull/11443#discussion_r398362604", "createdAt": "2020-03-26T07:27:47Z", "author": {"login": "azagrebin"}, "path": "flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTrackerImplTest.java", "diffHunk": "@@ -0,0 +1,241 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.io.network.partition;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.runtime.clusterframework.types.ResourceID;\n+import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;\n+import org.apache.flink.runtime.taskexecutor.partition.ClusterPartitionReport;\n+import org.apache.flink.util.TestLogger;\n+\n+import org.junit.Test;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.contains;\n+import static org.hamcrest.Matchers.containsInAnyOrder;\n+import static org.hamcrest.Matchers.hasKey;\n+import static org.hamcrest.collection.IsEmptyCollection.empty;\n+import static org.junit.Assert.assertEquals;\n+\n+/**\n+ * Test for the {@link ResourceManagerPartitionTrackerImpl}.\n+ */\n+public class ResourceManagerPartitionTrackerImplTest extends TestLogger {\n+\n+\tprivate static final ClusterPartitionReport EMPTY_PARTITION_REPORT = new ClusterPartitionReport(Collections.emptySet());\n+\n+\tprivate static final ResourceID TASK_EXECUTOR_ID_1 = ResourceID.generate();\n+\tprivate static final ResourceID TASK_EXECUTOR_ID_2 = ResourceID.generate();\n+\tprivate static final IntermediateDataSetID DATA_SET_ID = new IntermediateDataSetID();\n+\tprivate static final ResultPartitionID PARTITION_ID_1 = new ResultPartitionID();\n+\tprivate static final ResultPartitionID PARTITION_ID_2 = new ResultPartitionID();\n+\n+\t@Test\n+\tpublic void testProcessEmptyClusterPartitionReport() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(TASK_EXECUTOR_ID_1, EMPTY_PARTITION_REPORT);\n+\t\tassertThat(partitionReleaser.releaseCalls, empty());\n+\t}\n+\n+\t/**\n+\t * Verifies that a task executor hosting multiple partitions of a data set receives a release call if a subset of\n+\t * its partitions is lost.\n+\t */\n+\t@Test\n+\tpublic void testReportProcessingWithPartitionLossOnSameTaskExecutor() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1, PARTITION_ID_2));\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_2));\n+\n+\t\tassertThat(partitionReleaser.releaseCalls, contains(Tuple2.of(TASK_EXECUTOR_ID_1, Collections.singleton(DATA_SET_ID))));\n+\t}\n+\n+\t/**\n+\t * Verifies that a task executor hosting partitions of a data set receives a release call if a partition of the\n+\t * data set is lost on another task executor.\n+\t */\n+\t@Test\n+\tpublic void testReportProcessingWithPartitionLossOnOtherTaskExecutor() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1));\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_2,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_2));\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(TASK_EXECUTOR_ID_1, EMPTY_PARTITION_REPORT);\n+\n+\t\tassertThat(partitionReleaser.releaseCalls, contains(Tuple2.of(TASK_EXECUTOR_ID_2, Collections.singleton(DATA_SET_ID))));\n+\t}\n+\n+\t@Test\n+\tpublic void testListDataSetsBasics() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\tassertEquals(0, tracker.listDataSets().size());\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 1, PARTITION_ID_1));\n+\n+\t\tfinal Map<IntermediateDataSetID, DataSetMetaInfo> listing = tracker.listDataSets();\n+\t\tassertThat(listing, hasKey(DATA_SET_ID));\n+\t\tDataSetMetaInfo metaInfo = listing.get(DATA_SET_ID);\n+\t\tassertEquals(1, metaInfo.getNumTotalPartitions());\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tEMPTY_PARTITION_REPORT);\n+\t\tassertEquals(0, tracker.listDataSets().size());\n+\t}\n+\n+\t@Test\n+\tpublic void testListDataSetsMultiplePartitionsOnSingleTaskExecutor() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\t// data set consists of 2 partitions but only 1 is being tracked -> incomplete and should not be listed (yet)\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1));\n+\t\tassertEquals(0, tracker.listDataSets().size());\n+\n+\t\t// start tracking another partitions, but we lost partition 1 so the data set is still incomplete\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_2));\n+\t\tassertEquals(0, tracker.listDataSets().size());\n+\n+\t\t// dataset is considered complete since all partitions are being tracked\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1, PARTITION_ID_2));\n+\t\tfinal Map<IntermediateDataSetID, DataSetMetaInfo> listing = tracker.listDataSets();\n+\t\tassertThat(listing, hasKey(DATA_SET_ID));\n+\n+\t\t// dataset is no longer considered complete since partition 2 was lost\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1));\n+\t\tassertEquals(0, tracker.listDataSets().size());\n+\t}\n+\n+\t@Test\n+\tpublic void testListDataSetsMultiplePartitionsAcrossTaskExecutors() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1));\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_2,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_2));\n+\t\tfinal Map<IntermediateDataSetID, DataSetMetaInfo> listing = tracker.listDataSets();\n+\t\tassertThat(listing, hasKey(DATA_SET_ID));\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tEMPTY_PARTITION_REPORT);\n+\t\tassertEquals(0, tracker.listDataSets().size());\n+\t}\n+\n+\t@Test\n+\tpublic void testReleasePartition() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1));\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_2,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_2));\n+\n+\t\ttracker.releaseClusterPartitions(DATA_SET_ID);\n+\n+\t\tassertThat(partitionReleaser.releaseCalls, containsInAnyOrder(\n+\t\t\tTuple2.of(TASK_EXECUTOR_ID_1, Collections.singleton(DATA_SET_ID)),\n+\t\t\tTuple2.of(TASK_EXECUTOR_ID_2, Collections.singleton(DATA_SET_ID))));\n+\n+\t\t// the data set should still be tracked, since the partition release was not confirmed yet by the task executors\n+\t\tassertThat(tracker.listDataSets().keySet(), contains(DATA_SET_ID));\n+\t}\n+\n+\t@Test\n+\tpublic void testShutdownProcessing() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\ttracker.processTaskExecutorShutdown(TASK_EXECUTOR_ID_1);\n+\t\tassertThat(partitionReleaser.releaseCalls, empty());\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 3, PARTITION_ID_1, PARTITION_ID_2));\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_2,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 3, new ResultPartitionID()));\n+\n+\t\ttracker.processTaskExecutorShutdown(TASK_EXECUTOR_ID_1);\n+\n+\t\tassertThat(partitionReleaser.releaseCalls, contains(Tuple2.of(TASK_EXECUTOR_ID_2, Collections.singleton(DATA_SET_ID))));\n+\t}\n+\n+\tprivate static ClusterPartitionReport createClusterPartitionReport(IntermediateDataSetID dataSetId, int numTotalPartitions, ResultPartitionID... partitionId) {\n+\t\treturn new ClusterPartitionReport(Collections.singletonList(\n+\t\t\tnew ClusterPartitionReport.ClusterPartitionReportEntry(\n+\t\t\t\tdataSetId,\n+\t\t\t\tnew HashSet<>(Arrays.asList(partitionId)),\n+\t\t\t\tnumTotalPartitions)));\n+\t}\n+\n+\tprivate static class TestClusterPartitionReleaser implements ClusterPartitionReleaser {\n+\n+\t\tfinal List<Tuple2<ResourceID, Set<IntermediateDataSetID>>> releaseCalls = new ArrayList<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0240c441a2ea285e49af3172c2e5829207be011b"}, "originalPosition": 234}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDAzNTU1OQ==", "bodyText": "it is a field that is purposefully accessed from the outside and hence shouldn't be private for clarity reasons.", "url": "https://github.com/apache/flink/pull/11443#discussion_r400035559", "createdAt": "2020-03-30T09:05:45Z", "author": {"login": "zentol"}, "path": "flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTrackerImplTest.java", "diffHunk": "@@ -0,0 +1,241 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.io.network.partition;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.runtime.clusterframework.types.ResourceID;\n+import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;\n+import org.apache.flink.runtime.taskexecutor.partition.ClusterPartitionReport;\n+import org.apache.flink.util.TestLogger;\n+\n+import org.junit.Test;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.contains;\n+import static org.hamcrest.Matchers.containsInAnyOrder;\n+import static org.hamcrest.Matchers.hasKey;\n+import static org.hamcrest.collection.IsEmptyCollection.empty;\n+import static org.junit.Assert.assertEquals;\n+\n+/**\n+ * Test for the {@link ResourceManagerPartitionTrackerImpl}.\n+ */\n+public class ResourceManagerPartitionTrackerImplTest extends TestLogger {\n+\n+\tprivate static final ClusterPartitionReport EMPTY_PARTITION_REPORT = new ClusterPartitionReport(Collections.emptySet());\n+\n+\tprivate static final ResourceID TASK_EXECUTOR_ID_1 = ResourceID.generate();\n+\tprivate static final ResourceID TASK_EXECUTOR_ID_2 = ResourceID.generate();\n+\tprivate static final IntermediateDataSetID DATA_SET_ID = new IntermediateDataSetID();\n+\tprivate static final ResultPartitionID PARTITION_ID_1 = new ResultPartitionID();\n+\tprivate static final ResultPartitionID PARTITION_ID_2 = new ResultPartitionID();\n+\n+\t@Test\n+\tpublic void testProcessEmptyClusterPartitionReport() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(TASK_EXECUTOR_ID_1, EMPTY_PARTITION_REPORT);\n+\t\tassertThat(partitionReleaser.releaseCalls, empty());\n+\t}\n+\n+\t/**\n+\t * Verifies that a task executor hosting multiple partitions of a data set receives a release call if a subset of\n+\t * its partitions is lost.\n+\t */\n+\t@Test\n+\tpublic void testReportProcessingWithPartitionLossOnSameTaskExecutor() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1, PARTITION_ID_2));\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_2));\n+\n+\t\tassertThat(partitionReleaser.releaseCalls, contains(Tuple2.of(TASK_EXECUTOR_ID_1, Collections.singleton(DATA_SET_ID))));\n+\t}\n+\n+\t/**\n+\t * Verifies that a task executor hosting partitions of a data set receives a release call if a partition of the\n+\t * data set is lost on another task executor.\n+\t */\n+\t@Test\n+\tpublic void testReportProcessingWithPartitionLossOnOtherTaskExecutor() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1));\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_2,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_2));\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(TASK_EXECUTOR_ID_1, EMPTY_PARTITION_REPORT);\n+\n+\t\tassertThat(partitionReleaser.releaseCalls, contains(Tuple2.of(TASK_EXECUTOR_ID_2, Collections.singleton(DATA_SET_ID))));\n+\t}\n+\n+\t@Test\n+\tpublic void testListDataSetsBasics() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\tassertEquals(0, tracker.listDataSets().size());\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 1, PARTITION_ID_1));\n+\n+\t\tfinal Map<IntermediateDataSetID, DataSetMetaInfo> listing = tracker.listDataSets();\n+\t\tassertThat(listing, hasKey(DATA_SET_ID));\n+\t\tDataSetMetaInfo metaInfo = listing.get(DATA_SET_ID);\n+\t\tassertEquals(1, metaInfo.getNumTotalPartitions());\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tEMPTY_PARTITION_REPORT);\n+\t\tassertEquals(0, tracker.listDataSets().size());\n+\t}\n+\n+\t@Test\n+\tpublic void testListDataSetsMultiplePartitionsOnSingleTaskExecutor() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\t// data set consists of 2 partitions but only 1 is being tracked -> incomplete and should not be listed (yet)\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1));\n+\t\tassertEquals(0, tracker.listDataSets().size());\n+\n+\t\t// start tracking another partitions, but we lost partition 1 so the data set is still incomplete\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_2));\n+\t\tassertEquals(0, tracker.listDataSets().size());\n+\n+\t\t// dataset is considered complete since all partitions are being tracked\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1, PARTITION_ID_2));\n+\t\tfinal Map<IntermediateDataSetID, DataSetMetaInfo> listing = tracker.listDataSets();\n+\t\tassertThat(listing, hasKey(DATA_SET_ID));\n+\n+\t\t// dataset is no longer considered complete since partition 2 was lost\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1));\n+\t\tassertEquals(0, tracker.listDataSets().size());\n+\t}\n+\n+\t@Test\n+\tpublic void testListDataSetsMultiplePartitionsAcrossTaskExecutors() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1));\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_2,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_2));\n+\t\tfinal Map<IntermediateDataSetID, DataSetMetaInfo> listing = tracker.listDataSets();\n+\t\tassertThat(listing, hasKey(DATA_SET_ID));\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tEMPTY_PARTITION_REPORT);\n+\t\tassertEquals(0, tracker.listDataSets().size());\n+\t}\n+\n+\t@Test\n+\tpublic void testReleasePartition() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1));\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_2,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_2));\n+\n+\t\ttracker.releaseClusterPartitions(DATA_SET_ID);\n+\n+\t\tassertThat(partitionReleaser.releaseCalls, containsInAnyOrder(\n+\t\t\tTuple2.of(TASK_EXECUTOR_ID_1, Collections.singleton(DATA_SET_ID)),\n+\t\t\tTuple2.of(TASK_EXECUTOR_ID_2, Collections.singleton(DATA_SET_ID))));\n+\n+\t\t// the data set should still be tracked, since the partition release was not confirmed yet by the task executors\n+\t\tassertThat(tracker.listDataSets().keySet(), contains(DATA_SET_ID));\n+\t}\n+\n+\t@Test\n+\tpublic void testShutdownProcessing() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\ttracker.processTaskExecutorShutdown(TASK_EXECUTOR_ID_1);\n+\t\tassertThat(partitionReleaser.releaseCalls, empty());\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 3, PARTITION_ID_1, PARTITION_ID_2));\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_2,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 3, new ResultPartitionID()));\n+\n+\t\ttracker.processTaskExecutorShutdown(TASK_EXECUTOR_ID_1);\n+\n+\t\tassertThat(partitionReleaser.releaseCalls, contains(Tuple2.of(TASK_EXECUTOR_ID_2, Collections.singleton(DATA_SET_ID))));\n+\t}\n+\n+\tprivate static ClusterPartitionReport createClusterPartitionReport(IntermediateDataSetID dataSetId, int numTotalPartitions, ResultPartitionID... partitionId) {\n+\t\treturn new ClusterPartitionReport(Collections.singletonList(\n+\t\t\tnew ClusterPartitionReport.ClusterPartitionReportEntry(\n+\t\t\t\tdataSetId,\n+\t\t\t\tnew HashSet<>(Arrays.asList(partitionId)),\n+\t\t\t\tnumTotalPartitions)));\n+\t}\n+\n+\tprivate static class TestClusterPartitionReleaser implements ClusterPartitionReleaser {\n+\n+\t\tfinal List<Tuple2<ResourceID, Set<IntermediateDataSetID>>> releaseCalls = new ArrayList<>();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODM2MjYwNA=="}, "originalCommit": {"oid": "0240c441a2ea285e49af3172c2e5829207be011b"}, "originalPosition": 234}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ2OTY2MDc1OnYy", "diffSide": "RIGHT", "path": "flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTrackerImplTest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNlQwNzo0OToxM1rOF76sOg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNlQwNzo0OToxM1rOF76sOg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODM3MTg5OA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \t\tassertEquals(0, tracker.listDataSets().size());\n          \n          \n            \n            \t\tassertThat(tracker.listDataSets().size(), is(0));\n          \n      \n    \n    \n  \n\nminor: looks we usually do it like this for readability", "url": "https://github.com/apache/flink/pull/11443#discussion_r398371898", "createdAt": "2020-03-26T07:49:13Z", "author": {"login": "azagrebin"}, "path": "flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTrackerImplTest.java", "diffHunk": "@@ -0,0 +1,241 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.io.network.partition;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.runtime.clusterframework.types.ResourceID;\n+import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;\n+import org.apache.flink.runtime.taskexecutor.partition.ClusterPartitionReport;\n+import org.apache.flink.util.TestLogger;\n+\n+import org.junit.Test;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.contains;\n+import static org.hamcrest.Matchers.containsInAnyOrder;\n+import static org.hamcrest.Matchers.hasKey;\n+import static org.hamcrest.collection.IsEmptyCollection.empty;\n+import static org.junit.Assert.assertEquals;\n+\n+/**\n+ * Test for the {@link ResourceManagerPartitionTrackerImpl}.\n+ */\n+public class ResourceManagerPartitionTrackerImplTest extends TestLogger {\n+\n+\tprivate static final ClusterPartitionReport EMPTY_PARTITION_REPORT = new ClusterPartitionReport(Collections.emptySet());\n+\n+\tprivate static final ResourceID TASK_EXECUTOR_ID_1 = ResourceID.generate();\n+\tprivate static final ResourceID TASK_EXECUTOR_ID_2 = ResourceID.generate();\n+\tprivate static final IntermediateDataSetID DATA_SET_ID = new IntermediateDataSetID();\n+\tprivate static final ResultPartitionID PARTITION_ID_1 = new ResultPartitionID();\n+\tprivate static final ResultPartitionID PARTITION_ID_2 = new ResultPartitionID();\n+\n+\t@Test\n+\tpublic void testProcessEmptyClusterPartitionReport() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(TASK_EXECUTOR_ID_1, EMPTY_PARTITION_REPORT);\n+\t\tassertThat(partitionReleaser.releaseCalls, empty());\n+\t}\n+\n+\t/**\n+\t * Verifies that a task executor hosting multiple partitions of a data set receives a release call if a subset of\n+\t * its partitions is lost.\n+\t */\n+\t@Test\n+\tpublic void testReportProcessingWithPartitionLossOnSameTaskExecutor() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1, PARTITION_ID_2));\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_2));\n+\n+\t\tassertThat(partitionReleaser.releaseCalls, contains(Tuple2.of(TASK_EXECUTOR_ID_1, Collections.singleton(DATA_SET_ID))));\n+\t}\n+\n+\t/**\n+\t * Verifies that a task executor hosting partitions of a data set receives a release call if a partition of the\n+\t * data set is lost on another task executor.\n+\t */\n+\t@Test\n+\tpublic void testReportProcessingWithPartitionLossOnOtherTaskExecutor() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1));\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_2,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_2));\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(TASK_EXECUTOR_ID_1, EMPTY_PARTITION_REPORT);\n+\n+\t\tassertThat(partitionReleaser.releaseCalls, contains(Tuple2.of(TASK_EXECUTOR_ID_2, Collections.singleton(DATA_SET_ID))));\n+\t}\n+\n+\t@Test\n+\tpublic void testListDataSetsBasics() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\tassertEquals(0, tracker.listDataSets().size());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0240c441a2ea285e49af3172c2e5829207be011b"}, "originalPosition": 112}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ2OTcyNDMzOnYy", "diffSide": "RIGHT", "path": "flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTrackerImplTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNlQwODowOTo0MFrOF77R8Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQwOToyMTowNVrOF9g0Og==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODM4MTU1Mw==", "bodyText": "For this particular component, I would also suggest to test that the freeing of internal maps is checked because their sizes are somewhat side effect of the component in the system. They could be injected in constructor for testing or checked over some other test API. As mentioned in another comment, not sure that e.g. partitionReleaseCompletionFutures is freed properly and we do not accumulate data there over time.\nI was also thinking to use listDataSets for this check but it seems to be tricky.", "url": "https://github.com/apache/flink/pull/11443#discussion_r398381553", "createdAt": "2020-03-26T08:09:40Z", "author": {"login": "azagrebin"}, "path": "flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTrackerImplTest.java", "diffHunk": "@@ -0,0 +1,241 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.io.network.partition;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.runtime.clusterframework.types.ResourceID;\n+import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;\n+import org.apache.flink.runtime.taskexecutor.partition.ClusterPartitionReport;\n+import org.apache.flink.util.TestLogger;\n+\n+import org.junit.Test;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.contains;\n+import static org.hamcrest.Matchers.containsInAnyOrder;\n+import static org.hamcrest.Matchers.hasKey;\n+import static org.hamcrest.collection.IsEmptyCollection.empty;\n+import static org.junit.Assert.assertEquals;\n+\n+/**\n+ * Test for the {@link ResourceManagerPartitionTrackerImpl}.\n+ */\n+public class ResourceManagerPartitionTrackerImplTest extends TestLogger {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0240c441a2ea285e49af3172c2e5829207be011b"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDA0NTExNA==", "bodyText": "I will add a VisibleForTesting method for checking whether all maps are empty.", "url": "https://github.com/apache/flink/pull/11443#discussion_r400045114", "createdAt": "2020-03-30T09:21:05Z", "author": {"login": "zentol"}, "path": "flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTrackerImplTest.java", "diffHunk": "@@ -0,0 +1,241 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.io.network.partition;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.runtime.clusterframework.types.ResourceID;\n+import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;\n+import org.apache.flink.runtime.taskexecutor.partition.ClusterPartitionReport;\n+import org.apache.flink.util.TestLogger;\n+\n+import org.junit.Test;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.contains;\n+import static org.hamcrest.Matchers.containsInAnyOrder;\n+import static org.hamcrest.Matchers.hasKey;\n+import static org.hamcrest.collection.IsEmptyCollection.empty;\n+import static org.junit.Assert.assertEquals;\n+\n+/**\n+ * Test for the {@link ResourceManagerPartitionTrackerImpl}.\n+ */\n+public class ResourceManagerPartitionTrackerImplTest extends TestLogger {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODM4MTU1Mw=="}, "originalCommit": {"oid": "0240c441a2ea285e49af3172c2e5829207be011b"}, "originalPosition": 46}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ2OTczNzMxOnYy", "diffSide": "RIGHT", "path": "flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTrackerImplTest.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNlQwODoxMzo0MlrOF77ZpQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMVQwNzo1Nzo0OFrOF-0-lQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODM4MzUyNQ==", "bodyText": "minor: this test tests 3 transitions:\nincomplete -> incomplete\nincomplete -> complete\ncomplete -> incomplete\nI would prefer 3 test cases\nsome other tests also test multiple things", "url": "https://github.com/apache/flink/pull/11443#discussion_r398383525", "createdAt": "2020-03-26T08:13:42Z", "author": {"login": "azagrebin"}, "path": "flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTrackerImplTest.java", "diffHunk": "@@ -0,0 +1,241 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.io.network.partition;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.runtime.clusterframework.types.ResourceID;\n+import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;\n+import org.apache.flink.runtime.taskexecutor.partition.ClusterPartitionReport;\n+import org.apache.flink.util.TestLogger;\n+\n+import org.junit.Test;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.contains;\n+import static org.hamcrest.Matchers.containsInAnyOrder;\n+import static org.hamcrest.Matchers.hasKey;\n+import static org.hamcrest.collection.IsEmptyCollection.empty;\n+import static org.junit.Assert.assertEquals;\n+\n+/**\n+ * Test for the {@link ResourceManagerPartitionTrackerImpl}.\n+ */\n+public class ResourceManagerPartitionTrackerImplTest extends TestLogger {\n+\n+\tprivate static final ClusterPartitionReport EMPTY_PARTITION_REPORT = new ClusterPartitionReport(Collections.emptySet());\n+\n+\tprivate static final ResourceID TASK_EXECUTOR_ID_1 = ResourceID.generate();\n+\tprivate static final ResourceID TASK_EXECUTOR_ID_2 = ResourceID.generate();\n+\tprivate static final IntermediateDataSetID DATA_SET_ID = new IntermediateDataSetID();\n+\tprivate static final ResultPartitionID PARTITION_ID_1 = new ResultPartitionID();\n+\tprivate static final ResultPartitionID PARTITION_ID_2 = new ResultPartitionID();\n+\n+\t@Test\n+\tpublic void testProcessEmptyClusterPartitionReport() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(TASK_EXECUTOR_ID_1, EMPTY_PARTITION_REPORT);\n+\t\tassertThat(partitionReleaser.releaseCalls, empty());\n+\t}\n+\n+\t/**\n+\t * Verifies that a task executor hosting multiple partitions of a data set receives a release call if a subset of\n+\t * its partitions is lost.\n+\t */\n+\t@Test\n+\tpublic void testReportProcessingWithPartitionLossOnSameTaskExecutor() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1, PARTITION_ID_2));\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_2));\n+\n+\t\tassertThat(partitionReleaser.releaseCalls, contains(Tuple2.of(TASK_EXECUTOR_ID_1, Collections.singleton(DATA_SET_ID))));\n+\t}\n+\n+\t/**\n+\t * Verifies that a task executor hosting partitions of a data set receives a release call if a partition of the\n+\t * data set is lost on another task executor.\n+\t */\n+\t@Test\n+\tpublic void testReportProcessingWithPartitionLossOnOtherTaskExecutor() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1));\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_2,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_2));\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(TASK_EXECUTOR_ID_1, EMPTY_PARTITION_REPORT);\n+\n+\t\tassertThat(partitionReleaser.releaseCalls, contains(Tuple2.of(TASK_EXECUTOR_ID_2, Collections.singleton(DATA_SET_ID))));\n+\t}\n+\n+\t@Test\n+\tpublic void testListDataSetsBasics() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\tassertEquals(0, tracker.listDataSets().size());\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 1, PARTITION_ID_1));\n+\n+\t\tfinal Map<IntermediateDataSetID, DataSetMetaInfo> listing = tracker.listDataSets();\n+\t\tassertThat(listing, hasKey(DATA_SET_ID));\n+\t\tDataSetMetaInfo metaInfo = listing.get(DATA_SET_ID);\n+\t\tassertEquals(1, metaInfo.getNumTotalPartitions());\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tEMPTY_PARTITION_REPORT);\n+\t\tassertEquals(0, tracker.listDataSets().size());\n+\t}\n+\n+\t@Test\n+\tpublic void testListDataSetsMultiplePartitionsOnSingleTaskExecutor() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0240c441a2ea285e49af3172c2e5829207be011b"}, "originalPosition": 130}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODM4OTg2OA==", "bodyText": "Just an idea, if we init partitionReleaser/tracker in test setup and factor out\ntracker.processTaskExecutorClusterPartitionReport(TASK_EXECUTOR_ID_1, createClusterPartitionReport(..\ninto e.g. report(TASK_EXECUTOR_ID_1, DATA_SET_ID, 2, PARTITION_ID_1, ..)\nthis would reduce code size and simplify writing tests with duplicated setup but testing one thing.", "url": "https://github.com/apache/flink/pull/11443#discussion_r398389868", "createdAt": "2020-03-26T08:25:29Z", "author": {"login": "azagrebin"}, "path": "flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTrackerImplTest.java", "diffHunk": "@@ -0,0 +1,241 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.io.network.partition;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.runtime.clusterframework.types.ResourceID;\n+import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;\n+import org.apache.flink.runtime.taskexecutor.partition.ClusterPartitionReport;\n+import org.apache.flink.util.TestLogger;\n+\n+import org.junit.Test;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.contains;\n+import static org.hamcrest.Matchers.containsInAnyOrder;\n+import static org.hamcrest.Matchers.hasKey;\n+import static org.hamcrest.collection.IsEmptyCollection.empty;\n+import static org.junit.Assert.assertEquals;\n+\n+/**\n+ * Test for the {@link ResourceManagerPartitionTrackerImpl}.\n+ */\n+public class ResourceManagerPartitionTrackerImplTest extends TestLogger {\n+\n+\tprivate static final ClusterPartitionReport EMPTY_PARTITION_REPORT = new ClusterPartitionReport(Collections.emptySet());\n+\n+\tprivate static final ResourceID TASK_EXECUTOR_ID_1 = ResourceID.generate();\n+\tprivate static final ResourceID TASK_EXECUTOR_ID_2 = ResourceID.generate();\n+\tprivate static final IntermediateDataSetID DATA_SET_ID = new IntermediateDataSetID();\n+\tprivate static final ResultPartitionID PARTITION_ID_1 = new ResultPartitionID();\n+\tprivate static final ResultPartitionID PARTITION_ID_2 = new ResultPartitionID();\n+\n+\t@Test\n+\tpublic void testProcessEmptyClusterPartitionReport() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(TASK_EXECUTOR_ID_1, EMPTY_PARTITION_REPORT);\n+\t\tassertThat(partitionReleaser.releaseCalls, empty());\n+\t}\n+\n+\t/**\n+\t * Verifies that a task executor hosting multiple partitions of a data set receives a release call if a subset of\n+\t * its partitions is lost.\n+\t */\n+\t@Test\n+\tpublic void testReportProcessingWithPartitionLossOnSameTaskExecutor() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1, PARTITION_ID_2));\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_2));\n+\n+\t\tassertThat(partitionReleaser.releaseCalls, contains(Tuple2.of(TASK_EXECUTOR_ID_1, Collections.singleton(DATA_SET_ID))));\n+\t}\n+\n+\t/**\n+\t * Verifies that a task executor hosting partitions of a data set receives a release call if a partition of the\n+\t * data set is lost on another task executor.\n+\t */\n+\t@Test\n+\tpublic void testReportProcessingWithPartitionLossOnOtherTaskExecutor() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1));\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_2,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_2));\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(TASK_EXECUTOR_ID_1, EMPTY_PARTITION_REPORT);\n+\n+\t\tassertThat(partitionReleaser.releaseCalls, contains(Tuple2.of(TASK_EXECUTOR_ID_2, Collections.singleton(DATA_SET_ID))));\n+\t}\n+\n+\t@Test\n+\tpublic void testListDataSetsBasics() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\tassertEquals(0, tracker.listDataSets().size());\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 1, PARTITION_ID_1));\n+\n+\t\tfinal Map<IntermediateDataSetID, DataSetMetaInfo> listing = tracker.listDataSets();\n+\t\tassertThat(listing, hasKey(DATA_SET_ID));\n+\t\tDataSetMetaInfo metaInfo = listing.get(DATA_SET_ID);\n+\t\tassertEquals(1, metaInfo.getNumTotalPartitions());\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tEMPTY_PARTITION_REPORT);\n+\t\tassertEquals(0, tracker.listDataSets().size());\n+\t}\n+\n+\t@Test\n+\tpublic void testListDataSetsMultiplePartitionsOnSingleTaskExecutor() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODM4MzUyNQ=="}, "originalCommit": {"oid": "0240c441a2ea285e49af3172c2e5829207be011b"}, "originalPosition": 130}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDcyNzgyNg==", "bodyText": "I've addressed your second comment (I think).\nAs for the other one, your state transitions aren't quite correct, or rather overly simplified.\nIt goes from unknown -> partially-complete -> complete -> partially-complete.\nTrying to split these up just means we duplicate code we already have. You can't go partially-complete -> complete without first having to go unknown -> partially-complete, so if you were to write separate tests you're running the same code unknown -> partially-complete twice.\nAnd this is true for all state transitions.\nAssertions are usable enough for us to differentiate between failures at different stages, so I don't buy in to the whole \"test exactly one thing\" mentality.", "url": "https://github.com/apache/flink/pull/11443#discussion_r400727826", "createdAt": "2020-03-31T08:21:34Z", "author": {"login": "zentol"}, "path": "flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTrackerImplTest.java", "diffHunk": "@@ -0,0 +1,241 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.io.network.partition;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.runtime.clusterframework.types.ResourceID;\n+import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;\n+import org.apache.flink.runtime.taskexecutor.partition.ClusterPartitionReport;\n+import org.apache.flink.util.TestLogger;\n+\n+import org.junit.Test;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.contains;\n+import static org.hamcrest.Matchers.containsInAnyOrder;\n+import static org.hamcrest.Matchers.hasKey;\n+import static org.hamcrest.collection.IsEmptyCollection.empty;\n+import static org.junit.Assert.assertEquals;\n+\n+/**\n+ * Test for the {@link ResourceManagerPartitionTrackerImpl}.\n+ */\n+public class ResourceManagerPartitionTrackerImplTest extends TestLogger {\n+\n+\tprivate static final ClusterPartitionReport EMPTY_PARTITION_REPORT = new ClusterPartitionReport(Collections.emptySet());\n+\n+\tprivate static final ResourceID TASK_EXECUTOR_ID_1 = ResourceID.generate();\n+\tprivate static final ResourceID TASK_EXECUTOR_ID_2 = ResourceID.generate();\n+\tprivate static final IntermediateDataSetID DATA_SET_ID = new IntermediateDataSetID();\n+\tprivate static final ResultPartitionID PARTITION_ID_1 = new ResultPartitionID();\n+\tprivate static final ResultPartitionID PARTITION_ID_2 = new ResultPartitionID();\n+\n+\t@Test\n+\tpublic void testProcessEmptyClusterPartitionReport() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(TASK_EXECUTOR_ID_1, EMPTY_PARTITION_REPORT);\n+\t\tassertThat(partitionReleaser.releaseCalls, empty());\n+\t}\n+\n+\t/**\n+\t * Verifies that a task executor hosting multiple partitions of a data set receives a release call if a subset of\n+\t * its partitions is lost.\n+\t */\n+\t@Test\n+\tpublic void testReportProcessingWithPartitionLossOnSameTaskExecutor() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1, PARTITION_ID_2));\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_2));\n+\n+\t\tassertThat(partitionReleaser.releaseCalls, contains(Tuple2.of(TASK_EXECUTOR_ID_1, Collections.singleton(DATA_SET_ID))));\n+\t}\n+\n+\t/**\n+\t * Verifies that a task executor hosting partitions of a data set receives a release call if a partition of the\n+\t * data set is lost on another task executor.\n+\t */\n+\t@Test\n+\tpublic void testReportProcessingWithPartitionLossOnOtherTaskExecutor() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1));\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_2,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_2));\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(TASK_EXECUTOR_ID_1, EMPTY_PARTITION_REPORT);\n+\n+\t\tassertThat(partitionReleaser.releaseCalls, contains(Tuple2.of(TASK_EXECUTOR_ID_2, Collections.singleton(DATA_SET_ID))));\n+\t}\n+\n+\t@Test\n+\tpublic void testListDataSetsBasics() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\tassertEquals(0, tracker.listDataSets().size());\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 1, PARTITION_ID_1));\n+\n+\t\tfinal Map<IntermediateDataSetID, DataSetMetaInfo> listing = tracker.listDataSets();\n+\t\tassertThat(listing, hasKey(DATA_SET_ID));\n+\t\tDataSetMetaInfo metaInfo = listing.get(DATA_SET_ID);\n+\t\tassertEquals(1, metaInfo.getNumTotalPartitions());\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tEMPTY_PARTITION_REPORT);\n+\t\tassertEquals(0, tracker.listDataSets().size());\n+\t}\n+\n+\t@Test\n+\tpublic void testListDataSetsMultiplePartitionsOnSingleTaskExecutor() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODM4MzUyNQ=="}, "originalCommit": {"oid": "0240c441a2ea285e49af3172c2e5829207be011b"}, "originalPosition": 130}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTQyNDAyMQ==", "bodyText": "Indeed, there is going to be some code duplication. The idea was to reduce its size with the report method (which happened) in favour of isolated checks at the end. I am ok to keep it like this.", "url": "https://github.com/apache/flink/pull/11443#discussion_r401424021", "createdAt": "2020-04-01T07:57:48Z", "author": {"login": "azagrebin"}, "path": "flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTrackerImplTest.java", "diffHunk": "@@ -0,0 +1,241 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.io.network.partition;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.runtime.clusterframework.types.ResourceID;\n+import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;\n+import org.apache.flink.runtime.taskexecutor.partition.ClusterPartitionReport;\n+import org.apache.flink.util.TestLogger;\n+\n+import org.junit.Test;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.contains;\n+import static org.hamcrest.Matchers.containsInAnyOrder;\n+import static org.hamcrest.Matchers.hasKey;\n+import static org.hamcrest.collection.IsEmptyCollection.empty;\n+import static org.junit.Assert.assertEquals;\n+\n+/**\n+ * Test for the {@link ResourceManagerPartitionTrackerImpl}.\n+ */\n+public class ResourceManagerPartitionTrackerImplTest extends TestLogger {\n+\n+\tprivate static final ClusterPartitionReport EMPTY_PARTITION_REPORT = new ClusterPartitionReport(Collections.emptySet());\n+\n+\tprivate static final ResourceID TASK_EXECUTOR_ID_1 = ResourceID.generate();\n+\tprivate static final ResourceID TASK_EXECUTOR_ID_2 = ResourceID.generate();\n+\tprivate static final IntermediateDataSetID DATA_SET_ID = new IntermediateDataSetID();\n+\tprivate static final ResultPartitionID PARTITION_ID_1 = new ResultPartitionID();\n+\tprivate static final ResultPartitionID PARTITION_ID_2 = new ResultPartitionID();\n+\n+\t@Test\n+\tpublic void testProcessEmptyClusterPartitionReport() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(TASK_EXECUTOR_ID_1, EMPTY_PARTITION_REPORT);\n+\t\tassertThat(partitionReleaser.releaseCalls, empty());\n+\t}\n+\n+\t/**\n+\t * Verifies that a task executor hosting multiple partitions of a data set receives a release call if a subset of\n+\t * its partitions is lost.\n+\t */\n+\t@Test\n+\tpublic void testReportProcessingWithPartitionLossOnSameTaskExecutor() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1, PARTITION_ID_2));\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_2));\n+\n+\t\tassertThat(partitionReleaser.releaseCalls, contains(Tuple2.of(TASK_EXECUTOR_ID_1, Collections.singleton(DATA_SET_ID))));\n+\t}\n+\n+\t/**\n+\t * Verifies that a task executor hosting partitions of a data set receives a release call if a partition of the\n+\t * data set is lost on another task executor.\n+\t */\n+\t@Test\n+\tpublic void testReportProcessingWithPartitionLossOnOtherTaskExecutor() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1));\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_2,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_2));\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(TASK_EXECUTOR_ID_1, EMPTY_PARTITION_REPORT);\n+\n+\t\tassertThat(partitionReleaser.releaseCalls, contains(Tuple2.of(TASK_EXECUTOR_ID_2, Collections.singleton(DATA_SET_ID))));\n+\t}\n+\n+\t@Test\n+\tpublic void testListDataSetsBasics() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\tassertEquals(0, tracker.listDataSets().size());\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 1, PARTITION_ID_1));\n+\n+\t\tfinal Map<IntermediateDataSetID, DataSetMetaInfo> listing = tracker.listDataSets();\n+\t\tassertThat(listing, hasKey(DATA_SET_ID));\n+\t\tDataSetMetaInfo metaInfo = listing.get(DATA_SET_ID);\n+\t\tassertEquals(1, metaInfo.getNumTotalPartitions());\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tEMPTY_PARTITION_REPORT);\n+\t\tassertEquals(0, tracker.listDataSets().size());\n+\t}\n+\n+\t@Test\n+\tpublic void testListDataSetsMultiplePartitionsOnSingleTaskExecutor() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODM4MzUyNQ=="}, "originalCommit": {"oid": "0240c441a2ea285e49af3172c2e5829207be011b"}, "originalPosition": 130}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ2OTgzMDI3OnYy", "diffSide": "RIGHT", "path": "flink-runtime/src/test/java/org/apache/flink/runtime/resourcemanager/ResourceManagerPartitionLifecycleTest.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNlQwODo0MDowOFrOF78SBA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQxNTowMDowOFrOF9uAvA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODM5Nzk1Ng==", "bodyText": "Could we reuse some code from ResourceManagerTest?", "url": "https://github.com/apache/flink/pull/11443#discussion_r398397956", "createdAt": "2020-03-26T08:40:08Z", "author": {"login": "azagrebin"}, "path": "flink-runtime/src/test/java/org/apache/flink/runtime/resourcemanager/ResourceManagerPartitionLifecycleTest.java", "diffHunk": "@@ -0,0 +1,247 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.resourcemanager;\n+\n+import org.apache.flink.api.common.time.Time;\n+import org.apache.flink.runtime.clusterframework.types.ResourceID;\n+import org.apache.flink.runtime.clusterframework.types.ResourceProfile;\n+import org.apache.flink.runtime.heartbeat.HeartbeatServices;\n+import org.apache.flink.runtime.highavailability.TestingHighAvailabilityServices;\n+import org.apache.flink.runtime.instance.HardwareDescription;\n+import org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl;\n+import org.apache.flink.runtime.io.network.partition.ResultPartitionID;\n+import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;\n+import org.apache.flink.runtime.leaderelection.TestingLeaderElectionService;\n+import org.apache.flink.runtime.metrics.groups.UnregisteredMetricGroups;\n+import org.apache.flink.runtime.registration.RegistrationResponse;\n+import org.apache.flink.runtime.resourcemanager.slotmanager.SlotManager;\n+import org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerBuilder;\n+import org.apache.flink.runtime.rpc.RpcUtils;\n+import org.apache.flink.runtime.rpc.TestingRpcService;\n+import org.apache.flink.runtime.taskexecutor.SlotReport;\n+import org.apache.flink.runtime.taskexecutor.TaskExecutorGateway;\n+import org.apache.flink.runtime.taskexecutor.TaskExecutorHeartbeatPayload;\n+import org.apache.flink.runtime.taskexecutor.TestingTaskExecutorGatewayBuilder;\n+import org.apache.flink.runtime.taskexecutor.partition.ClusterPartitionReport;\n+import org.apache.flink.runtime.testingUtils.TestingUtils;\n+import org.apache.flink.runtime.util.TestingFatalErrorHandler;\n+import org.apache.flink.util.TestLogger;\n+\n+import org.junit.After;\n+import org.junit.AfterClass;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.TimeUnit;\n+\n+import static org.hamcrest.Matchers.contains;\n+import static org.hamcrest.Matchers.instanceOf;\n+import static org.junit.Assert.assertThat;\n+\n+/**\n+ * Tests for the partition-lifecycle logic in the {@link ResourceManager}.\n+ */\n+public class ResourceManagerPartitionLifecycleTest extends TestLogger {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0240c441a2ea285e49af3172c2e5829207be011b"}, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTA3NDQ1Ng==", "bodyText": "Possibly, most of the code was in fact copied over. I'm not a huge fan of making methods from other tests accessible just because they happen to now match the same use-case.\nWhat we really want are some flexible utility methods/resources, but this would be quite an effor.", "url": "https://github.com/apache/flink/pull/11443#discussion_r399074456", "createdAt": "2020-03-27T07:19:06Z", "author": {"login": "zentol"}, "path": "flink-runtime/src/test/java/org/apache/flink/runtime/resourcemanager/ResourceManagerPartitionLifecycleTest.java", "diffHunk": "@@ -0,0 +1,247 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.resourcemanager;\n+\n+import org.apache.flink.api.common.time.Time;\n+import org.apache.flink.runtime.clusterframework.types.ResourceID;\n+import org.apache.flink.runtime.clusterframework.types.ResourceProfile;\n+import org.apache.flink.runtime.heartbeat.HeartbeatServices;\n+import org.apache.flink.runtime.highavailability.TestingHighAvailabilityServices;\n+import org.apache.flink.runtime.instance.HardwareDescription;\n+import org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl;\n+import org.apache.flink.runtime.io.network.partition.ResultPartitionID;\n+import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;\n+import org.apache.flink.runtime.leaderelection.TestingLeaderElectionService;\n+import org.apache.flink.runtime.metrics.groups.UnregisteredMetricGroups;\n+import org.apache.flink.runtime.registration.RegistrationResponse;\n+import org.apache.flink.runtime.resourcemanager.slotmanager.SlotManager;\n+import org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerBuilder;\n+import org.apache.flink.runtime.rpc.RpcUtils;\n+import org.apache.flink.runtime.rpc.TestingRpcService;\n+import org.apache.flink.runtime.taskexecutor.SlotReport;\n+import org.apache.flink.runtime.taskexecutor.TaskExecutorGateway;\n+import org.apache.flink.runtime.taskexecutor.TaskExecutorHeartbeatPayload;\n+import org.apache.flink.runtime.taskexecutor.TestingTaskExecutorGatewayBuilder;\n+import org.apache.flink.runtime.taskexecutor.partition.ClusterPartitionReport;\n+import org.apache.flink.runtime.testingUtils.TestingUtils;\n+import org.apache.flink.runtime.util.TestingFatalErrorHandler;\n+import org.apache.flink.util.TestLogger;\n+\n+import org.junit.After;\n+import org.junit.AfterClass;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.TimeUnit;\n+\n+import static org.hamcrest.Matchers.contains;\n+import static org.hamcrest.Matchers.instanceOf;\n+import static org.junit.Assert.assertThat;\n+\n+/**\n+ * Tests for the partition-lifecycle logic in the {@link ResourceManager}.\n+ */\n+public class ResourceManagerPartitionLifecycleTest extends TestLogger {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODM5Nzk1Ng=="}, "originalCommit": {"oid": "0240c441a2ea285e49af3172c2e5829207be011b"}, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDI2MTMwOA==", "bodyText": "Indeed, we want good utility methods/resources and it is an effort. It looks like we have accumulated some technical debt in tests. At this point, it may be only feasible to approach the ideal state of code stepwise. I leave it up to you then whether we can do here any steps or not, it is indeed time-consuming.", "url": "https://github.com/apache/flink/pull/11443#discussion_r400261308", "createdAt": "2020-03-30T15:00:08Z", "author": {"login": "azagrebin"}, "path": "flink-runtime/src/test/java/org/apache/flink/runtime/resourcemanager/ResourceManagerPartitionLifecycleTest.java", "diffHunk": "@@ -0,0 +1,247 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.resourcemanager;\n+\n+import org.apache.flink.api.common.time.Time;\n+import org.apache.flink.runtime.clusterframework.types.ResourceID;\n+import org.apache.flink.runtime.clusterframework.types.ResourceProfile;\n+import org.apache.flink.runtime.heartbeat.HeartbeatServices;\n+import org.apache.flink.runtime.highavailability.TestingHighAvailabilityServices;\n+import org.apache.flink.runtime.instance.HardwareDescription;\n+import org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl;\n+import org.apache.flink.runtime.io.network.partition.ResultPartitionID;\n+import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;\n+import org.apache.flink.runtime.leaderelection.TestingLeaderElectionService;\n+import org.apache.flink.runtime.metrics.groups.UnregisteredMetricGroups;\n+import org.apache.flink.runtime.registration.RegistrationResponse;\n+import org.apache.flink.runtime.resourcemanager.slotmanager.SlotManager;\n+import org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerBuilder;\n+import org.apache.flink.runtime.rpc.RpcUtils;\n+import org.apache.flink.runtime.rpc.TestingRpcService;\n+import org.apache.flink.runtime.taskexecutor.SlotReport;\n+import org.apache.flink.runtime.taskexecutor.TaskExecutorGateway;\n+import org.apache.flink.runtime.taskexecutor.TaskExecutorHeartbeatPayload;\n+import org.apache.flink.runtime.taskexecutor.TestingTaskExecutorGatewayBuilder;\n+import org.apache.flink.runtime.taskexecutor.partition.ClusterPartitionReport;\n+import org.apache.flink.runtime.testingUtils.TestingUtils;\n+import org.apache.flink.runtime.util.TestingFatalErrorHandler;\n+import org.apache.flink.util.TestLogger;\n+\n+import org.junit.After;\n+import org.junit.AfterClass;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.TimeUnit;\n+\n+import static org.hamcrest.Matchers.contains;\n+import static org.hamcrest.Matchers.instanceOf;\n+import static org.junit.Assert.assertThat;\n+\n+/**\n+ * Tests for the partition-lifecycle logic in the {@link ResourceManager}.\n+ */\n+public class ResourceManagerPartitionLifecycleTest extends TestLogger {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODM5Nzk1Ng=="}, "originalCommit": {"oid": "0240c441a2ea285e49af3172c2e5829207be011b"}, "originalPosition": 66}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 814, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}