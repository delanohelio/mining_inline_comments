{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTA3NDc2MTY4", "number": 13724, "reviewThreads": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQwNjo0NDowOFrOE1FwGQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQwNDowMzo1MFrOE1fYCA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0MTA0MjE3OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/PartitionFieldExtractor.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQwNjo0NDowOFrOHtK2Vw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQwNjo0NDowOFrOHtK2Vw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzEyNTcxOQ==", "bodyText": "This will be introduced in #13919", "url": "https://github.com/apache/flink/pull/13724#discussion_r517125719", "createdAt": "2020-11-04T06:44:08Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/PartitionFieldExtractor.java", "diffHunk": "@@ -0,0 +1,50 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.connector.file.src.FileSourceSplit;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.utils.PartitionPathUtils;\n+\n+import java.io.Serializable;\n+import java.util.LinkedHashMap;\n+\n+/**\n+ * Interface to extract partition field from split.\n+ */\n+@FunctionalInterface\n+public interface PartitionFieldExtractor<T extends FileSourceSplit> extends Serializable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "012beed0dd7796ec1fc67a6e717431540461a7e0"}, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0NTIyODQ0OnYy", "diffSide": "RIGHT", "path": "flink-formats/flink-orc/src/main/java/org/apache/flink/orc/OrcColumnarRowFileInputFormat.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQwMzo1NjoyMFrOHtysEg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQwNjo1NzoyM1rOHt1r4w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzc3ODQ1MA==", "bodyText": "Can we make this consistent with ParquetColumnarRowInputFormat? E.g. take produced RowType and no need for selectedFields?", "url": "https://github.com/apache/flink/pull/13724#discussion_r517778450", "createdAt": "2020-11-05T03:56:20Z", "author": {"login": "lirui-apache"}, "path": "flink-formats/flink-orc/src/main/java/org/apache/flink/orc/OrcColumnarRowFileInputFormat.java", "diffHunk": "@@ -0,0 +1,171 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.orc;\n+\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.connector.file.src.FileSourceSplit;\n+import org.apache.flink.connector.file.src.util.Pool;\n+import org.apache.flink.orc.shim.OrcShim;\n+import org.apache.flink.orc.vector.ColumnBatchFactory;\n+import org.apache.flink.orc.vector.OrcVectorizedBatchWrapper;\n+import org.apache.flink.table.data.ColumnarRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.vector.ColumnVector;\n+import org.apache.flink.table.data.vector.VectorizedColumnBatch;\n+import org.apache.flink.table.filesystem.ColumnarRowIterator;\n+import org.apache.flink.table.filesystem.PartitionFieldExtractor;\n+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.orc.TypeDescription;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.orc.OrcSplitReaderUtil.convertToOrcTypeWithPart;\n+import static org.apache.flink.orc.OrcSplitReaderUtil.getNonPartNames;\n+import static org.apache.flink.orc.OrcSplitReaderUtil.getSelectedOrcFields;\n+import static org.apache.flink.orc.vector.AbstractOrcColumnVector.createFlinkVector;\n+import static org.apache.flink.orc.vector.AbstractOrcColumnVector.createFlinkVectorFromConstant;\n+\n+/**\n+ * An ORC reader that produces a stream of {@link ColumnarRowData} records.\n+ *\n+ * <p>This class can add extra fields through {@link ColumnBatchFactory}, for example,\n+ * add partition fields, which can be extracted from path. Therefore, the {@link #getProducedType()}\n+ * may be different and types of extra fields need to be added.\n+ */\n+public class OrcColumnarRowFileInputFormat<BatchT, SplitT extends FileSourceSplit> extends\n+\t\tAbstractOrcFileInputFormat<RowData, BatchT, SplitT> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate final ColumnBatchFactory<BatchT, SplitT> batchFactory;\n+\tprivate final RowType projectedOutputType;\n+\n+\tpublic OrcColumnarRowFileInputFormat(\n+\t\t\tfinal OrcShim<BatchT> shim,\n+\t\t\tfinal Configuration hadoopConfig,\n+\t\t\tfinal TypeDescription schema,\n+\t\t\tfinal int[] selectedFields,\n+\t\t\tfinal List<OrcFilters.Predicate> conjunctPredicates,\n+\t\t\tfinal int batchSize,\n+\t\t\tfinal ColumnBatchFactory<BatchT, SplitT> batchFactory,\n+\t\t\tfinal RowType projectedOutputType) {\n+\t\tsuper(shim, hadoopConfig, schema, selectedFields, conjunctPredicates, batchSize);\n+\t\tthis.batchFactory = batchFactory;\n+\t\tthis.projectedOutputType = projectedOutputType;\n+\t}\n+\n+\t@Override\n+\tpublic OrcReaderBatch<RowData, BatchT> createReaderBatch(\n+\t\t\tfinal SplitT split,\n+\t\t\tfinal OrcVectorizedBatchWrapper<BatchT> orcBatch,\n+\t\t\tfinal Pool.Recycler<OrcReaderBatch<RowData, BatchT>> recycler,\n+\t\t\tfinal int batchSize) {\n+\n+\t\tfinal VectorizedColumnBatch flinkColumnBatch = batchFactory.create(split, orcBatch.getBatch());\n+\t\treturn new VectorizedColumnReaderBatch<>(orcBatch, flinkColumnBatch, recycler);\n+\t}\n+\n+\t@Override\n+\tpublic TypeInformation<RowData> getProducedType() {\n+\t\treturn InternalTypeInfo.of(projectedOutputType);\n+\t}\n+\n+\t// ------------------------------------------------------------------------\n+\n+\t/**\n+\t * One batch of ORC columnar vectors and Flink column vectors.\n+\t */\n+\tprivate static final class VectorizedColumnReaderBatch<BatchT> extends OrcReaderBatch<RowData, BatchT> {\n+\n+\t\tprivate final VectorizedColumnBatch flinkColumnBatch;\n+\t\tprivate final ColumnarRowIterator result;\n+\n+\t\tVectorizedColumnReaderBatch(\n+\t\t\t\tfinal OrcVectorizedBatchWrapper<BatchT> orcBatch,\n+\t\t\t\tfinal VectorizedColumnBatch flinkColumnBatch,\n+\t\t\t\tfinal Pool.Recycler<OrcReaderBatch<RowData, BatchT>> recycler) {\n+\t\t\tsuper(orcBatch, recycler);\n+\t\t\tthis.flinkColumnBatch = flinkColumnBatch;\n+\t\t\tthis.result = new ColumnarRowIterator(new ColumnarRowData(flinkColumnBatch), this::recycle);\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic RecordIterator<RowData> convertAndGetIterator(\n+\t\t\t\tfinal OrcVectorizedBatchWrapper<BatchT> orcBatch,\n+\t\t\t\tfinal long startingOffset) {\n+\t\t\t// no copying from the ORC column vectors to the Flink columns vectors necessary,\n+\t\t\t// because they point to the same data arrays internally design\n+\t\t\tint batchSize = orcBatch.size();\n+\t\t\tflinkColumnBatch.setNumRows(batchSize);\n+\t\t\tresult.set(batchSize, startingOffset, 0);\n+\t\t\treturn result;\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * Create a partitioned {@link OrcColumnarRowFileInputFormat}, the partition columns can be\n+\t * generated by split.\n+\t */\n+\tpublic static <SplitT extends FileSourceSplit> OrcColumnarRowFileInputFormat<VectorizedRowBatch, SplitT> createPartitionedFormat(\n+\t\t\tOrcShim<VectorizedRowBatch> shim,\n+\t\t\tConfiguration hadoopConfig,\n+\t\t\tRowType tableType,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3d25c91253ce75af6d2d87b0bae62bc7dbd575bb"}, "originalPosition": 135}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzgyMTgyNQ==", "bodyText": "We can not, because orc needs full schema to reader, but parquet does not need.", "url": "https://github.com/apache/flink/pull/13724#discussion_r517821825", "createdAt": "2020-11-05T06:40:41Z", "author": {"login": "JingsongLi"}, "path": "flink-formats/flink-orc/src/main/java/org/apache/flink/orc/OrcColumnarRowFileInputFormat.java", "diffHunk": "@@ -0,0 +1,171 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.orc;\n+\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.connector.file.src.FileSourceSplit;\n+import org.apache.flink.connector.file.src.util.Pool;\n+import org.apache.flink.orc.shim.OrcShim;\n+import org.apache.flink.orc.vector.ColumnBatchFactory;\n+import org.apache.flink.orc.vector.OrcVectorizedBatchWrapper;\n+import org.apache.flink.table.data.ColumnarRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.vector.ColumnVector;\n+import org.apache.flink.table.data.vector.VectorizedColumnBatch;\n+import org.apache.flink.table.filesystem.ColumnarRowIterator;\n+import org.apache.flink.table.filesystem.PartitionFieldExtractor;\n+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.orc.TypeDescription;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.orc.OrcSplitReaderUtil.convertToOrcTypeWithPart;\n+import static org.apache.flink.orc.OrcSplitReaderUtil.getNonPartNames;\n+import static org.apache.flink.orc.OrcSplitReaderUtil.getSelectedOrcFields;\n+import static org.apache.flink.orc.vector.AbstractOrcColumnVector.createFlinkVector;\n+import static org.apache.flink.orc.vector.AbstractOrcColumnVector.createFlinkVectorFromConstant;\n+\n+/**\n+ * An ORC reader that produces a stream of {@link ColumnarRowData} records.\n+ *\n+ * <p>This class can add extra fields through {@link ColumnBatchFactory}, for example,\n+ * add partition fields, which can be extracted from path. Therefore, the {@link #getProducedType()}\n+ * may be different and types of extra fields need to be added.\n+ */\n+public class OrcColumnarRowFileInputFormat<BatchT, SplitT extends FileSourceSplit> extends\n+\t\tAbstractOrcFileInputFormat<RowData, BatchT, SplitT> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate final ColumnBatchFactory<BatchT, SplitT> batchFactory;\n+\tprivate final RowType projectedOutputType;\n+\n+\tpublic OrcColumnarRowFileInputFormat(\n+\t\t\tfinal OrcShim<BatchT> shim,\n+\t\t\tfinal Configuration hadoopConfig,\n+\t\t\tfinal TypeDescription schema,\n+\t\t\tfinal int[] selectedFields,\n+\t\t\tfinal List<OrcFilters.Predicate> conjunctPredicates,\n+\t\t\tfinal int batchSize,\n+\t\t\tfinal ColumnBatchFactory<BatchT, SplitT> batchFactory,\n+\t\t\tfinal RowType projectedOutputType) {\n+\t\tsuper(shim, hadoopConfig, schema, selectedFields, conjunctPredicates, batchSize);\n+\t\tthis.batchFactory = batchFactory;\n+\t\tthis.projectedOutputType = projectedOutputType;\n+\t}\n+\n+\t@Override\n+\tpublic OrcReaderBatch<RowData, BatchT> createReaderBatch(\n+\t\t\tfinal SplitT split,\n+\t\t\tfinal OrcVectorizedBatchWrapper<BatchT> orcBatch,\n+\t\t\tfinal Pool.Recycler<OrcReaderBatch<RowData, BatchT>> recycler,\n+\t\t\tfinal int batchSize) {\n+\n+\t\tfinal VectorizedColumnBatch flinkColumnBatch = batchFactory.create(split, orcBatch.getBatch());\n+\t\treturn new VectorizedColumnReaderBatch<>(orcBatch, flinkColumnBatch, recycler);\n+\t}\n+\n+\t@Override\n+\tpublic TypeInformation<RowData> getProducedType() {\n+\t\treturn InternalTypeInfo.of(projectedOutputType);\n+\t}\n+\n+\t// ------------------------------------------------------------------------\n+\n+\t/**\n+\t * One batch of ORC columnar vectors and Flink column vectors.\n+\t */\n+\tprivate static final class VectorizedColumnReaderBatch<BatchT> extends OrcReaderBatch<RowData, BatchT> {\n+\n+\t\tprivate final VectorizedColumnBatch flinkColumnBatch;\n+\t\tprivate final ColumnarRowIterator result;\n+\n+\t\tVectorizedColumnReaderBatch(\n+\t\t\t\tfinal OrcVectorizedBatchWrapper<BatchT> orcBatch,\n+\t\t\t\tfinal VectorizedColumnBatch flinkColumnBatch,\n+\t\t\t\tfinal Pool.Recycler<OrcReaderBatch<RowData, BatchT>> recycler) {\n+\t\t\tsuper(orcBatch, recycler);\n+\t\t\tthis.flinkColumnBatch = flinkColumnBatch;\n+\t\t\tthis.result = new ColumnarRowIterator(new ColumnarRowData(flinkColumnBatch), this::recycle);\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic RecordIterator<RowData> convertAndGetIterator(\n+\t\t\t\tfinal OrcVectorizedBatchWrapper<BatchT> orcBatch,\n+\t\t\t\tfinal long startingOffset) {\n+\t\t\t// no copying from the ORC column vectors to the Flink columns vectors necessary,\n+\t\t\t// because they point to the same data arrays internally design\n+\t\t\tint batchSize = orcBatch.size();\n+\t\t\tflinkColumnBatch.setNumRows(batchSize);\n+\t\t\tresult.set(batchSize, startingOffset, 0);\n+\t\t\treturn result;\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * Create a partitioned {@link OrcColumnarRowFileInputFormat}, the partition columns can be\n+\t * generated by split.\n+\t */\n+\tpublic static <SplitT extends FileSourceSplit> OrcColumnarRowFileInputFormat<VectorizedRowBatch, SplitT> createPartitionedFormat(\n+\t\t\tOrcShim<VectorizedRowBatch> shim,\n+\t\t\tConfiguration hadoopConfig,\n+\t\t\tRowType tableType,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzc3ODQ1MA=="}, "originalCommit": {"oid": "3d25c91253ce75af6d2d87b0bae62bc7dbd575bb"}, "originalPosition": 135}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzgyMjA3OQ==", "bodyText": "And AbstractOrcFileInputFormat is a table-free class.", "url": "https://github.com/apache/flink/pull/13724#discussion_r517822079", "createdAt": "2020-11-05T06:41:30Z", "author": {"login": "JingsongLi"}, "path": "flink-formats/flink-orc/src/main/java/org/apache/flink/orc/OrcColumnarRowFileInputFormat.java", "diffHunk": "@@ -0,0 +1,171 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.orc;\n+\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.connector.file.src.FileSourceSplit;\n+import org.apache.flink.connector.file.src.util.Pool;\n+import org.apache.flink.orc.shim.OrcShim;\n+import org.apache.flink.orc.vector.ColumnBatchFactory;\n+import org.apache.flink.orc.vector.OrcVectorizedBatchWrapper;\n+import org.apache.flink.table.data.ColumnarRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.vector.ColumnVector;\n+import org.apache.flink.table.data.vector.VectorizedColumnBatch;\n+import org.apache.flink.table.filesystem.ColumnarRowIterator;\n+import org.apache.flink.table.filesystem.PartitionFieldExtractor;\n+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.orc.TypeDescription;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.orc.OrcSplitReaderUtil.convertToOrcTypeWithPart;\n+import static org.apache.flink.orc.OrcSplitReaderUtil.getNonPartNames;\n+import static org.apache.flink.orc.OrcSplitReaderUtil.getSelectedOrcFields;\n+import static org.apache.flink.orc.vector.AbstractOrcColumnVector.createFlinkVector;\n+import static org.apache.flink.orc.vector.AbstractOrcColumnVector.createFlinkVectorFromConstant;\n+\n+/**\n+ * An ORC reader that produces a stream of {@link ColumnarRowData} records.\n+ *\n+ * <p>This class can add extra fields through {@link ColumnBatchFactory}, for example,\n+ * add partition fields, which can be extracted from path. Therefore, the {@link #getProducedType()}\n+ * may be different and types of extra fields need to be added.\n+ */\n+public class OrcColumnarRowFileInputFormat<BatchT, SplitT extends FileSourceSplit> extends\n+\t\tAbstractOrcFileInputFormat<RowData, BatchT, SplitT> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate final ColumnBatchFactory<BatchT, SplitT> batchFactory;\n+\tprivate final RowType projectedOutputType;\n+\n+\tpublic OrcColumnarRowFileInputFormat(\n+\t\t\tfinal OrcShim<BatchT> shim,\n+\t\t\tfinal Configuration hadoopConfig,\n+\t\t\tfinal TypeDescription schema,\n+\t\t\tfinal int[] selectedFields,\n+\t\t\tfinal List<OrcFilters.Predicate> conjunctPredicates,\n+\t\t\tfinal int batchSize,\n+\t\t\tfinal ColumnBatchFactory<BatchT, SplitT> batchFactory,\n+\t\t\tfinal RowType projectedOutputType) {\n+\t\tsuper(shim, hadoopConfig, schema, selectedFields, conjunctPredicates, batchSize);\n+\t\tthis.batchFactory = batchFactory;\n+\t\tthis.projectedOutputType = projectedOutputType;\n+\t}\n+\n+\t@Override\n+\tpublic OrcReaderBatch<RowData, BatchT> createReaderBatch(\n+\t\t\tfinal SplitT split,\n+\t\t\tfinal OrcVectorizedBatchWrapper<BatchT> orcBatch,\n+\t\t\tfinal Pool.Recycler<OrcReaderBatch<RowData, BatchT>> recycler,\n+\t\t\tfinal int batchSize) {\n+\n+\t\tfinal VectorizedColumnBatch flinkColumnBatch = batchFactory.create(split, orcBatch.getBatch());\n+\t\treturn new VectorizedColumnReaderBatch<>(orcBatch, flinkColumnBatch, recycler);\n+\t}\n+\n+\t@Override\n+\tpublic TypeInformation<RowData> getProducedType() {\n+\t\treturn InternalTypeInfo.of(projectedOutputType);\n+\t}\n+\n+\t// ------------------------------------------------------------------------\n+\n+\t/**\n+\t * One batch of ORC columnar vectors and Flink column vectors.\n+\t */\n+\tprivate static final class VectorizedColumnReaderBatch<BatchT> extends OrcReaderBatch<RowData, BatchT> {\n+\n+\t\tprivate final VectorizedColumnBatch flinkColumnBatch;\n+\t\tprivate final ColumnarRowIterator result;\n+\n+\t\tVectorizedColumnReaderBatch(\n+\t\t\t\tfinal OrcVectorizedBatchWrapper<BatchT> orcBatch,\n+\t\t\t\tfinal VectorizedColumnBatch flinkColumnBatch,\n+\t\t\t\tfinal Pool.Recycler<OrcReaderBatch<RowData, BatchT>> recycler) {\n+\t\t\tsuper(orcBatch, recycler);\n+\t\t\tthis.flinkColumnBatch = flinkColumnBatch;\n+\t\t\tthis.result = new ColumnarRowIterator(new ColumnarRowData(flinkColumnBatch), this::recycle);\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic RecordIterator<RowData> convertAndGetIterator(\n+\t\t\t\tfinal OrcVectorizedBatchWrapper<BatchT> orcBatch,\n+\t\t\t\tfinal long startingOffset) {\n+\t\t\t// no copying from the ORC column vectors to the Flink columns vectors necessary,\n+\t\t\t// because they point to the same data arrays internally design\n+\t\t\tint batchSize = orcBatch.size();\n+\t\t\tflinkColumnBatch.setNumRows(batchSize);\n+\t\t\tresult.set(batchSize, startingOffset, 0);\n+\t\t\treturn result;\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * Create a partitioned {@link OrcColumnarRowFileInputFormat}, the partition columns can be\n+\t * generated by split.\n+\t */\n+\tpublic static <SplitT extends FileSourceSplit> OrcColumnarRowFileInputFormat<VectorizedRowBatch, SplitT> createPartitionedFormat(\n+\t\t\tOrcShim<VectorizedRowBatch> shim,\n+\t\t\tConfiguration hadoopConfig,\n+\t\t\tRowType tableType,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzc3ODQ1MA=="}, "originalCommit": {"oid": "3d25c91253ce75af6d2d87b0bae62bc7dbd575bb"}, "originalPosition": 135}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzgyNzU1NQ==", "bodyText": "OK \ud83e\udd23", "url": "https://github.com/apache/flink/pull/13724#discussion_r517827555", "createdAt": "2020-11-05T06:57:23Z", "author": {"login": "lirui-apache"}, "path": "flink-formats/flink-orc/src/main/java/org/apache/flink/orc/OrcColumnarRowFileInputFormat.java", "diffHunk": "@@ -0,0 +1,171 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.orc;\n+\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.connector.file.src.FileSourceSplit;\n+import org.apache.flink.connector.file.src.util.Pool;\n+import org.apache.flink.orc.shim.OrcShim;\n+import org.apache.flink.orc.vector.ColumnBatchFactory;\n+import org.apache.flink.orc.vector.OrcVectorizedBatchWrapper;\n+import org.apache.flink.table.data.ColumnarRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.vector.ColumnVector;\n+import org.apache.flink.table.data.vector.VectorizedColumnBatch;\n+import org.apache.flink.table.filesystem.ColumnarRowIterator;\n+import org.apache.flink.table.filesystem.PartitionFieldExtractor;\n+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.orc.TypeDescription;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.orc.OrcSplitReaderUtil.convertToOrcTypeWithPart;\n+import static org.apache.flink.orc.OrcSplitReaderUtil.getNonPartNames;\n+import static org.apache.flink.orc.OrcSplitReaderUtil.getSelectedOrcFields;\n+import static org.apache.flink.orc.vector.AbstractOrcColumnVector.createFlinkVector;\n+import static org.apache.flink.orc.vector.AbstractOrcColumnVector.createFlinkVectorFromConstant;\n+\n+/**\n+ * An ORC reader that produces a stream of {@link ColumnarRowData} records.\n+ *\n+ * <p>This class can add extra fields through {@link ColumnBatchFactory}, for example,\n+ * add partition fields, which can be extracted from path. Therefore, the {@link #getProducedType()}\n+ * may be different and types of extra fields need to be added.\n+ */\n+public class OrcColumnarRowFileInputFormat<BatchT, SplitT extends FileSourceSplit> extends\n+\t\tAbstractOrcFileInputFormat<RowData, BatchT, SplitT> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate final ColumnBatchFactory<BatchT, SplitT> batchFactory;\n+\tprivate final RowType projectedOutputType;\n+\n+\tpublic OrcColumnarRowFileInputFormat(\n+\t\t\tfinal OrcShim<BatchT> shim,\n+\t\t\tfinal Configuration hadoopConfig,\n+\t\t\tfinal TypeDescription schema,\n+\t\t\tfinal int[] selectedFields,\n+\t\t\tfinal List<OrcFilters.Predicate> conjunctPredicates,\n+\t\t\tfinal int batchSize,\n+\t\t\tfinal ColumnBatchFactory<BatchT, SplitT> batchFactory,\n+\t\t\tfinal RowType projectedOutputType) {\n+\t\tsuper(shim, hadoopConfig, schema, selectedFields, conjunctPredicates, batchSize);\n+\t\tthis.batchFactory = batchFactory;\n+\t\tthis.projectedOutputType = projectedOutputType;\n+\t}\n+\n+\t@Override\n+\tpublic OrcReaderBatch<RowData, BatchT> createReaderBatch(\n+\t\t\tfinal SplitT split,\n+\t\t\tfinal OrcVectorizedBatchWrapper<BatchT> orcBatch,\n+\t\t\tfinal Pool.Recycler<OrcReaderBatch<RowData, BatchT>> recycler,\n+\t\t\tfinal int batchSize) {\n+\n+\t\tfinal VectorizedColumnBatch flinkColumnBatch = batchFactory.create(split, orcBatch.getBatch());\n+\t\treturn new VectorizedColumnReaderBatch<>(orcBatch, flinkColumnBatch, recycler);\n+\t}\n+\n+\t@Override\n+\tpublic TypeInformation<RowData> getProducedType() {\n+\t\treturn InternalTypeInfo.of(projectedOutputType);\n+\t}\n+\n+\t// ------------------------------------------------------------------------\n+\n+\t/**\n+\t * One batch of ORC columnar vectors and Flink column vectors.\n+\t */\n+\tprivate static final class VectorizedColumnReaderBatch<BatchT> extends OrcReaderBatch<RowData, BatchT> {\n+\n+\t\tprivate final VectorizedColumnBatch flinkColumnBatch;\n+\t\tprivate final ColumnarRowIterator result;\n+\n+\t\tVectorizedColumnReaderBatch(\n+\t\t\t\tfinal OrcVectorizedBatchWrapper<BatchT> orcBatch,\n+\t\t\t\tfinal VectorizedColumnBatch flinkColumnBatch,\n+\t\t\t\tfinal Pool.Recycler<OrcReaderBatch<RowData, BatchT>> recycler) {\n+\t\t\tsuper(orcBatch, recycler);\n+\t\t\tthis.flinkColumnBatch = flinkColumnBatch;\n+\t\t\tthis.result = new ColumnarRowIterator(new ColumnarRowData(flinkColumnBatch), this::recycle);\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic RecordIterator<RowData> convertAndGetIterator(\n+\t\t\t\tfinal OrcVectorizedBatchWrapper<BatchT> orcBatch,\n+\t\t\t\tfinal long startingOffset) {\n+\t\t\t// no copying from the ORC column vectors to the Flink columns vectors necessary,\n+\t\t\t// because they point to the same data arrays internally design\n+\t\t\tint batchSize = orcBatch.size();\n+\t\t\tflinkColumnBatch.setNumRows(batchSize);\n+\t\t\tresult.set(batchSize, startingOffset, 0);\n+\t\t\treturn result;\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * Create a partitioned {@link OrcColumnarRowFileInputFormat}, the partition columns can be\n+\t * generated by split.\n+\t */\n+\tpublic static <SplitT extends FileSourceSplit> OrcColumnarRowFileInputFormat<VectorizedRowBatch, SplitT> createPartitionedFormat(\n+\t\t\tOrcShim<VectorizedRowBatch> shim,\n+\t\t\tConfiguration hadoopConfig,\n+\t\t\tRowType tableType,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzc3ODQ1MA=="}, "originalCommit": {"oid": "3d25c91253ce75af6d2d87b0bae62bc7dbd575bb"}, "originalPosition": 135}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0NTI0MDQwOnYy", "diffSide": "RIGHT", "path": "flink-formats/flink-orc/src/main/java/org/apache/flink/orc/OrcFileSystemFormatFactory.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQwNDowMzo1MFrOHtyymQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQwNjo0MjowMlrOHt1XSw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzc4MDEyMQ==", "bodyText": "Rename this class since it's no longer a FileSystemFormatFactory?", "url": "https://github.com/apache/flink/pull/13724#discussion_r517780121", "createdAt": "2020-11-05T04:03:50Z", "author": {"login": "lirui-apache"}, "path": "flink-formats/flink-orc/src/main/java/org/apache/flink/orc/OrcFileSystemFormatFactory.java", "diffHunk": "@@ -18,45 +18,45 @@\n \n package org.apache.flink.orc;\n \n-import org.apache.flink.api.common.io.FileInputFormat;\n-import org.apache.flink.api.common.io.InputFormat;\n import org.apache.flink.api.common.serialization.BulkWriter;\n-import org.apache.flink.api.common.serialization.Encoder;\n import org.apache.flink.configuration.ConfigOption;\n import org.apache.flink.configuration.ReadableConfig;\n-import org.apache.flink.core.fs.FileInputSplit;\n-import org.apache.flink.core.fs.Path;\n+import org.apache.flink.connector.file.src.FileSourceSplit;\n+import org.apache.flink.connector.file.src.reader.BulkFormat;\n+import org.apache.flink.orc.shim.OrcShim;\n import org.apache.flink.orc.vector.RowDataVectorizer;\n import org.apache.flink.orc.writer.OrcBulkWriterFactory;\n+import org.apache.flink.table.connector.ChangelogMode;\n+import org.apache.flink.table.connector.format.BulkDecodingFormat;\n+import org.apache.flink.table.connector.format.EncodingFormat;\n+import org.apache.flink.table.connector.sink.DynamicTableSink;\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.vector.VectorizedColumnBatch;\n import org.apache.flink.table.expressions.Expression;\n-import org.apache.flink.table.factories.FileSystemFormatFactory;\n+import org.apache.flink.table.expressions.ResolvedExpression;\n+import org.apache.flink.table.factories.BulkReaderFormatFactory;\n+import org.apache.flink.table.factories.BulkWriterFormatFactory;\n+import org.apache.flink.table.factories.DynamicTableFactory;\n+import org.apache.flink.table.filesystem.FileSystemOptions;\n+import org.apache.flink.table.filesystem.PartitionFieldExtractor;\n import org.apache.flink.table.types.DataType;\n import org.apache.flink.table.types.logical.LogicalType;\n import org.apache.flink.table.types.logical.RowType;\n-import org.apache.flink.table.utils.PartitionPathUtils;\n \n import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n import org.apache.orc.TypeDescription;\n \n-import java.io.IOException;\n import java.util.ArrayList;\n-import java.util.Arrays;\n import java.util.HashSet;\n-import java.util.LinkedHashMap;\n import java.util.List;\n-import java.util.Optional;\n import java.util.Properties;\n import java.util.Set;\n \n-import static org.apache.flink.table.data.vector.VectorizedColumnBatch.DEFAULT_SIZE;\n-import static org.apache.flink.table.filesystem.RowPartitionComputer.restorePartValueFromType;\n-\n /**\n- * Orc {@link FileSystemFormatFactory} for file system.\n+ * Orc format factory for file system.\n  */\n-public class OrcFileSystemFormatFactory implements FileSystemFormatFactory {\n+public class OrcFileSystemFormatFactory implements BulkReaderFormatFactory, BulkWriterFormatFactory {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3d25c91253ce75af6d2d87b0bae62bc7dbd575bb"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzgyMjI4Mw==", "bodyText": "I think OrcFileFormatFactory is better", "url": "https://github.com/apache/flink/pull/13724#discussion_r517822283", "createdAt": "2020-11-05T06:42:02Z", "author": {"login": "JingsongLi"}, "path": "flink-formats/flink-orc/src/main/java/org/apache/flink/orc/OrcFileSystemFormatFactory.java", "diffHunk": "@@ -18,45 +18,45 @@\n \n package org.apache.flink.orc;\n \n-import org.apache.flink.api.common.io.FileInputFormat;\n-import org.apache.flink.api.common.io.InputFormat;\n import org.apache.flink.api.common.serialization.BulkWriter;\n-import org.apache.flink.api.common.serialization.Encoder;\n import org.apache.flink.configuration.ConfigOption;\n import org.apache.flink.configuration.ReadableConfig;\n-import org.apache.flink.core.fs.FileInputSplit;\n-import org.apache.flink.core.fs.Path;\n+import org.apache.flink.connector.file.src.FileSourceSplit;\n+import org.apache.flink.connector.file.src.reader.BulkFormat;\n+import org.apache.flink.orc.shim.OrcShim;\n import org.apache.flink.orc.vector.RowDataVectorizer;\n import org.apache.flink.orc.writer.OrcBulkWriterFactory;\n+import org.apache.flink.table.connector.ChangelogMode;\n+import org.apache.flink.table.connector.format.BulkDecodingFormat;\n+import org.apache.flink.table.connector.format.EncodingFormat;\n+import org.apache.flink.table.connector.sink.DynamicTableSink;\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.vector.VectorizedColumnBatch;\n import org.apache.flink.table.expressions.Expression;\n-import org.apache.flink.table.factories.FileSystemFormatFactory;\n+import org.apache.flink.table.expressions.ResolvedExpression;\n+import org.apache.flink.table.factories.BulkReaderFormatFactory;\n+import org.apache.flink.table.factories.BulkWriterFormatFactory;\n+import org.apache.flink.table.factories.DynamicTableFactory;\n+import org.apache.flink.table.filesystem.FileSystemOptions;\n+import org.apache.flink.table.filesystem.PartitionFieldExtractor;\n import org.apache.flink.table.types.DataType;\n import org.apache.flink.table.types.logical.LogicalType;\n import org.apache.flink.table.types.logical.RowType;\n-import org.apache.flink.table.utils.PartitionPathUtils;\n \n import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n import org.apache.orc.TypeDescription;\n \n-import java.io.IOException;\n import java.util.ArrayList;\n-import java.util.Arrays;\n import java.util.HashSet;\n-import java.util.LinkedHashMap;\n import java.util.List;\n-import java.util.Optional;\n import java.util.Properties;\n import java.util.Set;\n \n-import static org.apache.flink.table.data.vector.VectorizedColumnBatch.DEFAULT_SIZE;\n-import static org.apache.flink.table.filesystem.RowPartitionComputer.restorePartValueFromType;\n-\n /**\n- * Orc {@link FileSystemFormatFactory} for file system.\n+ * Orc format factory for file system.\n  */\n-public class OrcFileSystemFormatFactory implements FileSystemFormatFactory {\n+public class OrcFileSystemFormatFactory implements BulkReaderFormatFactory, BulkWriterFormatFactory {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzc4MDEyMQ=="}, "originalCommit": {"oid": "3d25c91253ce75af6d2d87b0bae62bc7dbd575bb"}, "originalPosition": 59}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 61, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}