{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTA2NDA1NTAw", "number": 13692, "reviewThreads": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQwNzowODoxM1rOEwGjZg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQwODoxNzozMFrOEwIMpA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4ODc0NDcwOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/InputPriorityConflictResolver.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQwNzowODoxM1rOHldMsA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQwNzowODoxM1rOHldMsA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTAzNzc0NA==", "bodyText": "make this pojo stateless, we can use a map to represent the usage of the tag", "url": "https://github.com/apache/flink/pull/13692#discussion_r509037744", "createdAt": "2020-10-21T07:08:13Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/InputPriorityConflictResolver.java", "diffHunk": "@@ -0,0 +1,353 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.reuse;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.transformations.ShuffleMode;\n+import org.apache.flink.table.planner.plan.nodes.exec.AbstractExecNodeExactlyOnceVisitor;\n+import org.apache.flink.table.planner.plan.nodes.exec.BatchExecNode;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecEdge;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecBoundedStreamScan;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecExchange;\n+import org.apache.flink.table.planner.plan.trait.FlinkRelDistribution;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.calcite.rel.RelNode;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.Set;\n+import java.util.TreeMap;\n+\n+/**\n+ * This class contains algorithm to detect and resolve input priority conflict in an {@link ExecNode} graph.\n+ *\n+ * <p>Some batch operators (for example, hash join and nested loop join) have different priorities for their inputs.\n+ * When some operators are reused, a deadlock may occur due to the conflict in these priorities.\n+ *\n+ * <p>For example, consider the SQL query:\n+ * <pre>\n+ * WITH\n+ *   T1 AS (SELECT a, COUNT(*) AS cnt1 FROM x GROUP BY a),\n+ *   T2 AS (SELECT d, COUNT(*) AS cnt2 FROM y GROUP BY d)\n+ * SELECT * FROM\n+ *   (SELECT cnt1, cnt2 FROM T1 LEFT JOIN T2 ON a = d)\n+ *   UNION ALL\n+ *   (SELECT cnt1, cnt2 FROM T2 LEFT JOIN T1 ON d = a)\n+ * </pre>\n+ *\n+ * <p>When sub-plan reuse are enabled, we'll get the following physical plan:\n+ * <pre>\n+ * Union(all=[true], union=[cnt1, cnt2])\n+ * :- Calc(select=[CAST(cnt1) AS cnt1, cnt2])\n+ * :  +- HashJoin(joinType=[LeftOuterJoin], where=[=(a, d)], select=[a, cnt1, d, cnt2], build=[right])\n+ * :     :- HashAggregate(isMerge=[true], groupBy=[a], select=[a, Final_COUNT(count1$0) AS cnt1], reuse_id=[2])\n+ * :     :  +- Exchange(distribution=[hash[a]])\n+ * :     :     +- LocalHashAggregate(groupBy=[a], select=[a, Partial_COUNT(*) AS count1$0])\n+ * :     :        +- Calc(select=[a])\n+ * :     :           +- LegacyTableSourceScan(table=[[default_catalog, default_database, x, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])\n+ * :     +- HashAggregate(isMerge=[true], groupBy=[d], select=[d, Final_COUNT(count1$0) AS cnt2], reuse_id=[1])\n+ * :        +- Exchange(distribution=[hash[d]])\n+ * :           +- LocalHashAggregate(groupBy=[d], select=[d, Partial_COUNT(*) AS count1$0])\n+ * :              +- Calc(select=[d])\n+ * :                 +- LegacyTableSourceScan(table=[[default_catalog, default_database, y, source: [TestTableSource(d, e, f)]]], fields=[d, e, f])\n+ * +- Calc(select=[cnt1, CAST(cnt2) AS cnt2])\n+ *    +- HashJoin(joinType=[LeftOuterJoin], where=[=(d, a)], select=[d, cnt2, a, cnt1], build=[right])\n+ *       :- Reused(reference_id=[1])\n+ *       +- Reused(reference_id=[2])\n+ * </pre>\n+ *\n+ * <p>Note that the first hash join needs to read all results from the hash aggregate whose reuse id is 1\n+ * before reading the results from the hash aggregate whose reuse id is 2, while the second hash join requires\n+ * the opposite. This physical plan will thus cause a deadlock.\n+ *\n+ * <p>This class maintains a topological graph in which an edge pointing from vertex A to vertex B indicates\n+ * that the results from vertex A need to be read before those from vertex B. A loop in the graph indicates\n+ * a deadlock, and we resolve such deadlock by inserting a {@link BatchExecExchange} with batch shuffle mode.\n+ *\n+ * <p>For a detailed explanation of the algorithm, see appendix of the\n+ * <a href=\"https://docs.google.com/document/d/1qKVohV12qn-bM51cBZ8Hcgp31ntwClxjoiNBUOqVHsI\">design doc</a>.\n+ */\n+@Internal\n+public class InputPriorityConflictResolver {\n+\n+\tprivate final List<ExecNode<?, ?>> sinks;\n+\n+\tprivate TopologyGraph graph;\n+\n+\tpublic InputPriorityConflictResolver(List<ExecNode<?, ?>> sinks) {\n+\t\tPreconditions.checkArgument(\n+\t\t\tsinks.stream().allMatch(sink -> sink instanceof BatchExecNode),\n+\t\t\t\"InputPriorityConflictResolver can only be used for batch jobs.\");\n+\t\tthis.sinks = sinks;\n+\t}\n+\n+\tpublic void detectAndResolve() {\n+\t\t// build an initial topology graph\n+\t\tgraph = new TopologyGraph(sinks);\n+\n+\t\t// check and resolve conflicts about input priorities\n+\t\tAbstractExecNodeExactlyOnceVisitor inputPriorityVisitor = new AbstractExecNodeExactlyOnceVisitor() {\n+\t\t\t@Override\n+\t\t\tprotected void visitNode(ExecNode<?, ?> node) {\n+\t\t\t\tvisitInputs(node);\n+\t\t\t\tcheckInputPriorities(node);\n+\t\t\t}\n+\t\t};\n+\t\tsinks.forEach(n -> n.accept(inputPriorityVisitor));\n+\t}\n+\n+\tprivate void checkInputPriorities(ExecNode<?, ?> node) {\n+\t\t// group inputs by input priorities\n+\t\tTreeMap<Integer, List<Integer>> inputPriorityGroupMap = new TreeMap<>();\n+\t\tPreconditions.checkState(\n+\t\t\tnode.getInputNodes().size() == node.getInputEdges().size(),\n+\t\t\t\"Number of inputs nodes does not equal to number of input edges for node \" +\n+\t\t\t\tnode.getClass().getName() + \". This is a bug.\");\n+\t\tfor (int i = 0; i < node.getInputNodes().size(); i++) {\n+\t\t\tint priority = node.getInputEdges().get(i).getPriority();\n+\t\t\tinputPriorityGroupMap.computeIfAbsent(priority, k -> new ArrayList<>()).add(i);\n+\t\t}\n+\n+\t\t// add edges between neighboring priority groups\n+\t\tList<List<Integer>> inputPriorityGroups = new ArrayList<>(inputPriorityGroupMap.values());\n+\t\tfor (int i = 0; i + 1 < inputPriorityGroups.size(); i++) {\n+\t\t\tList<Integer> higherGroup = inputPriorityGroups.get(i);\n+\t\t\tList<Integer> lowerGroup = inputPriorityGroups.get(i + 1);\n+\n+\t\t\tfor (int higher : higherGroup) {\n+\t\t\t\tfor (int lower : lowerGroup) {\n+\t\t\t\t\taddTopologyEdges(node, higher, lower);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate void addTopologyEdges(ExecNode<?, ?> node, int higherInput, int lowerInput) {\n+\t\tExecNode<?, ?> higherNode = node.getInputNodes().get(higherInput);\n+\t\tExecNode<?, ?> lowerNode = node.getInputNodes().get(lowerInput);\n+\t\tList<ExecNode<?, ?>> lowerAncestors = calculateAncestors(lowerNode);\n+\n+\t\tList<Tuple2<ExecNode<?, ?>, ExecNode<?, ?>>> linkedEdges = new ArrayList<>();\n+\t\tfor (ExecNode<?, ?> ancestor : lowerAncestors) {\n+\t\t\tif (graph.link(higherNode, ancestor)) {\n+\t\t\t\tlinkedEdges.add(Tuple2.of(higherNode, ancestor));\n+\t\t\t} else {\n+\t\t\t\t// a conflict occurs, resolve it by adding a batch exchange\n+\t\t\t\t// and revert all linked edges\n+\t\t\t\tif (lowerNode instanceof BatchExecExchange) {\n+\t\t\t\t\tBatchExecExchange exchange = (BatchExecExchange) lowerNode;\n+\t\t\t\t\texchange.setRequiredShuffleMode(ShuffleMode.BATCH);\n+\t\t\t\t} else {\n+\t\t\t\t\tnode.replaceInputNode(lowerInput, (ExecNode) createExchange(node, lowerInput));\n+\t\t\t\t}\n+\n+\t\t\t\tfor (Tuple2<ExecNode<?, ?>, ExecNode<?, ?>> linkedEdge : linkedEdges) {\n+\t\t\t\t\tgraph.unlink(linkedEdge.f0, linkedEdge.f1);\n+\t\t\t\t}\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * Find the ancestors by going through PIPELINED edges.\n+\t */\n+\t@VisibleForTesting\n+\tList<ExecNode<?, ?>> calculateAncestors(ExecNode<?, ?> node) {\n+\t\tList<ExecNode<?, ?>> ret = new ArrayList<>();\n+\t\tAbstractExecNodeExactlyOnceVisitor ancestorVisitor = new AbstractExecNodeExactlyOnceVisitor() {\n+\t\t\t@Override\n+\t\t\tprotected void visitNode(ExecNode<?, ?> node) {\n+\t\t\t\tList<ExecEdge> inputEdges = node.getInputEdges();\n+\t\t\t\tboolean hasAncestor = false;\n+\n+\t\t\t\tfor (int i = 0; i < inputEdges.size(); i++) {\n+\t\t\t\t\t// we only go through PIPELINED edges\n+\t\t\t\t\tif (inputEdges.get(i).getDamBehavior().stricterOrEqual(ExecEdge.DamBehavior.END_INPUT)) {\n+\t\t\t\t\t\tcontinue;\n+\t\t\t\t\t}\n+\t\t\t\t\thasAncestor = true;\n+\t\t\t\t\tnode.getInputNodes().get(i).accept(this);\n+\t\t\t\t}\n+\n+\t\t\t\tif (!hasAncestor) {\n+\t\t\t\t\tret.add(node);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t};\n+\t\tnode.accept(ancestorVisitor);\n+\t\treturn ret;\n+\t}\n+\n+\tprivate BatchExecExchange createExchange(ExecNode<?, ?> node, int idx) {\n+\t\tRelNode inputRel = (RelNode) node.getInputNodes().get(idx);\n+\n+\t\tFlinkRelDistribution distribution;\n+\t\tExecEdge.RequiredShuffle requiredShuffle = node.getInputEdges().get(idx).getRequiredShuffle();\n+\t\tif (requiredShuffle.getType() == ExecEdge.ShuffleType.HASH) {\n+\t\t\tdistribution = FlinkRelDistribution.hash(requiredShuffle.getKeys(), true);\n+\t\t} else if (requiredShuffle.getType() == ExecEdge.ShuffleType.BROADCAST) {\n+\t\t\t// should not occur\n+\t\t\tthrow new IllegalStateException(\n+\t\t\t\t\"Trying to resolve input priority conflict on broadcast side. This is not expected.\");\n+\t\t} else if (requiredShuffle.getType() == ExecEdge.ShuffleType.SINGLETON) {\n+\t\t\tdistribution = FlinkRelDistribution.SINGLETON();\n+\t\t} else {\n+\t\t\tdistribution = FlinkRelDistribution.ANY();\n+\t\t}\n+\n+\t\tBatchExecExchange exchange = new BatchExecExchange(\n+\t\t\tinputRel.getCluster(),\n+\t\t\tinputRel.getTraitSet().replace(distribution),\n+\t\t\tinputRel,\n+\t\t\tdistribution);\n+\t\texchange.setRequiredShuffleMode(ShuffleMode.BATCH);\n+\t\treturn exchange;\n+\t}\n+\n+\t/**\n+\t * A data structure storing the topological information of an {@link ExecNode} graph.\n+\t */\n+\t@VisibleForTesting\n+\tstatic class TopologyGraph {\n+\t\tprivate final Map<ExecNode<?, ?>, TopologyNode> nodes;\n+\t\tprivate int visitTag;\n+\n+\t\tTopologyGraph(List<ExecNode<?, ?>> sinks) {\n+\t\t\tthis.nodes = new HashMap<>();\n+\t\t\tthis.visitTag = 0;\n+\n+\t\t\t// we first link all edges in the original exec node graph\n+\t\t\tAbstractExecNodeExactlyOnceVisitor visitor = new AbstractExecNodeExactlyOnceVisitor() {\n+\t\t\t\t@Override\n+\t\t\t\tprotected void visitNode(ExecNode<?, ?> node) {\n+\t\t\t\t\tfor (ExecNode<?, ?> input : node.getInputNodes()) {\n+\t\t\t\t\t\tlink(input, node);\n+\t\t\t\t\t}\n+\t\t\t\t\tvisitInputs(node);\n+\t\t\t\t}\n+\t\t\t};\n+\t\t\tsinks.forEach(n -> n.accept(visitor));\n+\t\t}\n+\n+\t\t/**\n+\t\t * Link an edge from `from` node to `to` node if no loop will occur after adding this edge.\n+\t\t * Returns if this edge is successfully added.\n+\t\t */\n+\t\tboolean link(ExecNode<?, ?> from, ExecNode<?, ?> to) {\n+\t\t\tTopologyNode fromNode = getTopologyNode(from);\n+\t\t\tTopologyNode toNode = getTopologyNode(to);\n+\n+\t\t\tif (canReach(toNode, fromNode)) {\n+\t\t\t\t// invalid edge, as `to` is the predecessor of `from`\n+\t\t\t\treturn false;\n+\t\t\t} else {\n+\t\t\t\t// link `from` and `to`\n+\t\t\t\tfromNode.outputs.add(toNode);\n+\t\t\t\ttoNode.inputs.add(fromNode);\n+\t\t\t\treturn true;\n+\t\t\t}\n+\t\t}\n+\n+\t\t/**\n+\t\t * Remove the edge from `from` node to `to` node. If there is no edge between them then do nothing.\n+\t\t */\n+\t\tvoid unlink(ExecNode<?, ?> from, ExecNode<?, ?> to) {\n+\t\t\tTopologyNode fromNode = getTopologyNode(from);\n+\t\t\tTopologyNode toNode = getTopologyNode(to);\n+\n+\t\t\tfromNode.outputs.remove(toNode);\n+\t\t\ttoNode.inputs.remove(fromNode);\n+\t\t}\n+\n+\t\t@VisibleForTesting\n+\t\tboolean canReach(ExecNode<?, ?> from, ExecNode<?, ?> to) {\n+\t\t\tTopologyNode fromNode = getTopologyNode(from);\n+\t\t\tTopologyNode toNode = getTopologyNode(to);\n+\t\t\treturn canReach(fromNode, toNode);\n+\t\t}\n+\n+\t\tprivate boolean canReach(TopologyNode from, TopologyNode to) {\n+\t\t\tQueue<TopologyNode> queue = new LinkedList<>();\n+\t\t\tfrom.tag = --visitTag;\n+\t\t\tqueue.offer(from);\n+\n+\t\t\twhile (!queue.isEmpty()) {\n+\t\t\t\tTopologyNode node = queue.poll();\n+\t\t\t\tif (to.equals(node)) {\n+\t\t\t\t\treturn true;\n+\t\t\t\t}\n+\n+\t\t\t\tfor (TopologyNode next : node.outputs) {\n+\t\t\t\t\tif (next.tag == visitTag) {\n+\t\t\t\t\t\tcontinue;\n+\t\t\t\t\t}\n+\t\t\t\t\tnext.tag = visitTag;\n+\t\t\t\t\tqueue.offer(next);\n+\t\t\t\t}\n+\t\t\t}\n+\n+\t\t\treturn false;\n+\t\t}\n+\n+\t\tprivate TopologyNode getTopologyNode(ExecNode<?, ?> execNode) {\n+\t\t\t// NOTE: We treat different `BatchExecBoundedStreamScan`s with same `DataStream` object as the same\n+\t\t\tif (execNode instanceof BatchExecBoundedStreamScan) {\n+\t\t\t\tDataStream<?> currentStream =\n+\t\t\t\t\t((BatchExecBoundedStreamScan) execNode).boundedStreamTable().dataStream();\n+\t\t\t\tfor (Map.Entry<ExecNode<?, ?>, TopologyNode> entry : nodes.entrySet()) {\n+\t\t\t\t\tExecNode<?, ?> key = entry.getKey();\n+\t\t\t\t\tif (key instanceof BatchExecBoundedStreamScan) {\n+\t\t\t\t\t\tDataStream<?> existingStream =\n+\t\t\t\t\t\t\t((BatchExecBoundedStreamScan) key).boundedStreamTable().dataStream();\n+\t\t\t\t\t\tif (existingStream.equals(currentStream)) {\n+\t\t\t\t\t\t\treturn entry.getValue();\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\n+\t\t\t\tTopologyNode result = new TopologyNode();\n+\t\t\t\tnodes.put(execNode, result);\n+\t\t\t\treturn result;\n+\t\t\t} else {\n+\t\t\t\treturn nodes.computeIfAbsent(execNode, k -> new TopologyNode());\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * A node in the {@link TopologyGraph}.\n+\t */\n+\tprivate static class TopologyNode {\n+\t\tprivate final Set<TopologyNode> inputs = new HashSet<>();\n+\t\tprivate final Set<TopologyNode> outputs = new HashSet<>();\n+\n+\t\tprivate int tag;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "22cf14c8194ecbb1811caf7bddf03641bf82c1d3"}, "originalPosition": 351}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4ODc3NDE0OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/InputPriorityConflictResolver.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQwNzoxNjo0MFrOHldegQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQwNzoxNjo0MFrOHldegQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTA0MjMwNQ==", "bodyText": "nit: use node.getInputEdges().size(), consistent with the body of for", "url": "https://github.com/apache/flink/pull/13692#discussion_r509042305", "createdAt": "2020-10-21T07:16:40Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/InputPriorityConflictResolver.java", "diffHunk": "@@ -0,0 +1,353 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.reuse;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.transformations.ShuffleMode;\n+import org.apache.flink.table.planner.plan.nodes.exec.AbstractExecNodeExactlyOnceVisitor;\n+import org.apache.flink.table.planner.plan.nodes.exec.BatchExecNode;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecEdge;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecBoundedStreamScan;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecExchange;\n+import org.apache.flink.table.planner.plan.trait.FlinkRelDistribution;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.calcite.rel.RelNode;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.Set;\n+import java.util.TreeMap;\n+\n+/**\n+ * This class contains algorithm to detect and resolve input priority conflict in an {@link ExecNode} graph.\n+ *\n+ * <p>Some batch operators (for example, hash join and nested loop join) have different priorities for their inputs.\n+ * When some operators are reused, a deadlock may occur due to the conflict in these priorities.\n+ *\n+ * <p>For example, consider the SQL query:\n+ * <pre>\n+ * WITH\n+ *   T1 AS (SELECT a, COUNT(*) AS cnt1 FROM x GROUP BY a),\n+ *   T2 AS (SELECT d, COUNT(*) AS cnt2 FROM y GROUP BY d)\n+ * SELECT * FROM\n+ *   (SELECT cnt1, cnt2 FROM T1 LEFT JOIN T2 ON a = d)\n+ *   UNION ALL\n+ *   (SELECT cnt1, cnt2 FROM T2 LEFT JOIN T1 ON d = a)\n+ * </pre>\n+ *\n+ * <p>When sub-plan reuse are enabled, we'll get the following physical plan:\n+ * <pre>\n+ * Union(all=[true], union=[cnt1, cnt2])\n+ * :- Calc(select=[CAST(cnt1) AS cnt1, cnt2])\n+ * :  +- HashJoin(joinType=[LeftOuterJoin], where=[=(a, d)], select=[a, cnt1, d, cnt2], build=[right])\n+ * :     :- HashAggregate(isMerge=[true], groupBy=[a], select=[a, Final_COUNT(count1$0) AS cnt1], reuse_id=[2])\n+ * :     :  +- Exchange(distribution=[hash[a]])\n+ * :     :     +- LocalHashAggregate(groupBy=[a], select=[a, Partial_COUNT(*) AS count1$0])\n+ * :     :        +- Calc(select=[a])\n+ * :     :           +- LegacyTableSourceScan(table=[[default_catalog, default_database, x, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])\n+ * :     +- HashAggregate(isMerge=[true], groupBy=[d], select=[d, Final_COUNT(count1$0) AS cnt2], reuse_id=[1])\n+ * :        +- Exchange(distribution=[hash[d]])\n+ * :           +- LocalHashAggregate(groupBy=[d], select=[d, Partial_COUNT(*) AS count1$0])\n+ * :              +- Calc(select=[d])\n+ * :                 +- LegacyTableSourceScan(table=[[default_catalog, default_database, y, source: [TestTableSource(d, e, f)]]], fields=[d, e, f])\n+ * +- Calc(select=[cnt1, CAST(cnt2) AS cnt2])\n+ *    +- HashJoin(joinType=[LeftOuterJoin], where=[=(d, a)], select=[d, cnt2, a, cnt1], build=[right])\n+ *       :- Reused(reference_id=[1])\n+ *       +- Reused(reference_id=[2])\n+ * </pre>\n+ *\n+ * <p>Note that the first hash join needs to read all results from the hash aggregate whose reuse id is 1\n+ * before reading the results from the hash aggregate whose reuse id is 2, while the second hash join requires\n+ * the opposite. This physical plan will thus cause a deadlock.\n+ *\n+ * <p>This class maintains a topological graph in which an edge pointing from vertex A to vertex B indicates\n+ * that the results from vertex A need to be read before those from vertex B. A loop in the graph indicates\n+ * a deadlock, and we resolve such deadlock by inserting a {@link BatchExecExchange} with batch shuffle mode.\n+ *\n+ * <p>For a detailed explanation of the algorithm, see appendix of the\n+ * <a href=\"https://docs.google.com/document/d/1qKVohV12qn-bM51cBZ8Hcgp31ntwClxjoiNBUOqVHsI\">design doc</a>.\n+ */\n+@Internal\n+public class InputPriorityConflictResolver {\n+\n+\tprivate final List<ExecNode<?, ?>> sinks;\n+\n+\tprivate TopologyGraph graph;\n+\n+\tpublic InputPriorityConflictResolver(List<ExecNode<?, ?>> sinks) {\n+\t\tPreconditions.checkArgument(\n+\t\t\tsinks.stream().allMatch(sink -> sink instanceof BatchExecNode),\n+\t\t\t\"InputPriorityConflictResolver can only be used for batch jobs.\");\n+\t\tthis.sinks = sinks;\n+\t}\n+\n+\tpublic void detectAndResolve() {\n+\t\t// build an initial topology graph\n+\t\tgraph = new TopologyGraph(sinks);\n+\n+\t\t// check and resolve conflicts about input priorities\n+\t\tAbstractExecNodeExactlyOnceVisitor inputPriorityVisitor = new AbstractExecNodeExactlyOnceVisitor() {\n+\t\t\t@Override\n+\t\t\tprotected void visitNode(ExecNode<?, ?> node) {\n+\t\t\t\tvisitInputs(node);\n+\t\t\t\tcheckInputPriorities(node);\n+\t\t\t}\n+\t\t};\n+\t\tsinks.forEach(n -> n.accept(inputPriorityVisitor));\n+\t}\n+\n+\tprivate void checkInputPriorities(ExecNode<?, ?> node) {\n+\t\t// group inputs by input priorities\n+\t\tTreeMap<Integer, List<Integer>> inputPriorityGroupMap = new TreeMap<>();\n+\t\tPreconditions.checkState(\n+\t\t\tnode.getInputNodes().size() == node.getInputEdges().size(),\n+\t\t\t\"Number of inputs nodes does not equal to number of input edges for node \" +\n+\t\t\t\tnode.getClass().getName() + \". This is a bug.\");\n+\t\tfor (int i = 0; i < node.getInputNodes().size(); i++) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "22cf14c8194ecbb1811caf7bddf03641bf82c1d3"}, "originalPosition": 132}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4ODk5MjQ4OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/InputPriorityConflictResolver.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQwODoxMjoyOFrOHlfkIw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQwODoxMjoyOFrOHlfkIw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTA3NjUxNQ==", "bodyText": "nit: an ExecNode may be not BatchExecSink here, use roots ?", "url": "https://github.com/apache/flink/pull/13692#discussion_r509076515", "createdAt": "2020-10-21T08:12:28Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/InputPriorityConflictResolver.java", "diffHunk": "@@ -0,0 +1,353 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.reuse;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.transformations.ShuffleMode;\n+import org.apache.flink.table.planner.plan.nodes.exec.AbstractExecNodeExactlyOnceVisitor;\n+import org.apache.flink.table.planner.plan.nodes.exec.BatchExecNode;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecEdge;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecBoundedStreamScan;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecExchange;\n+import org.apache.flink.table.planner.plan.trait.FlinkRelDistribution;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.calcite.rel.RelNode;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.Set;\n+import java.util.TreeMap;\n+\n+/**\n+ * This class contains algorithm to detect and resolve input priority conflict in an {@link ExecNode} graph.\n+ *\n+ * <p>Some batch operators (for example, hash join and nested loop join) have different priorities for their inputs.\n+ * When some operators are reused, a deadlock may occur due to the conflict in these priorities.\n+ *\n+ * <p>For example, consider the SQL query:\n+ * <pre>\n+ * WITH\n+ *   T1 AS (SELECT a, COUNT(*) AS cnt1 FROM x GROUP BY a),\n+ *   T2 AS (SELECT d, COUNT(*) AS cnt2 FROM y GROUP BY d)\n+ * SELECT * FROM\n+ *   (SELECT cnt1, cnt2 FROM T1 LEFT JOIN T2 ON a = d)\n+ *   UNION ALL\n+ *   (SELECT cnt1, cnt2 FROM T2 LEFT JOIN T1 ON d = a)\n+ * </pre>\n+ *\n+ * <p>When sub-plan reuse are enabled, we'll get the following physical plan:\n+ * <pre>\n+ * Union(all=[true], union=[cnt1, cnt2])\n+ * :- Calc(select=[CAST(cnt1) AS cnt1, cnt2])\n+ * :  +- HashJoin(joinType=[LeftOuterJoin], where=[=(a, d)], select=[a, cnt1, d, cnt2], build=[right])\n+ * :     :- HashAggregate(isMerge=[true], groupBy=[a], select=[a, Final_COUNT(count1$0) AS cnt1], reuse_id=[2])\n+ * :     :  +- Exchange(distribution=[hash[a]])\n+ * :     :     +- LocalHashAggregate(groupBy=[a], select=[a, Partial_COUNT(*) AS count1$0])\n+ * :     :        +- Calc(select=[a])\n+ * :     :           +- LegacyTableSourceScan(table=[[default_catalog, default_database, x, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])\n+ * :     +- HashAggregate(isMerge=[true], groupBy=[d], select=[d, Final_COUNT(count1$0) AS cnt2], reuse_id=[1])\n+ * :        +- Exchange(distribution=[hash[d]])\n+ * :           +- LocalHashAggregate(groupBy=[d], select=[d, Partial_COUNT(*) AS count1$0])\n+ * :              +- Calc(select=[d])\n+ * :                 +- LegacyTableSourceScan(table=[[default_catalog, default_database, y, source: [TestTableSource(d, e, f)]]], fields=[d, e, f])\n+ * +- Calc(select=[cnt1, CAST(cnt2) AS cnt2])\n+ *    +- HashJoin(joinType=[LeftOuterJoin], where=[=(d, a)], select=[d, cnt2, a, cnt1], build=[right])\n+ *       :- Reused(reference_id=[1])\n+ *       +- Reused(reference_id=[2])\n+ * </pre>\n+ *\n+ * <p>Note that the first hash join needs to read all results from the hash aggregate whose reuse id is 1\n+ * before reading the results from the hash aggregate whose reuse id is 2, while the second hash join requires\n+ * the opposite. This physical plan will thus cause a deadlock.\n+ *\n+ * <p>This class maintains a topological graph in which an edge pointing from vertex A to vertex B indicates\n+ * that the results from vertex A need to be read before those from vertex B. A loop in the graph indicates\n+ * a deadlock, and we resolve such deadlock by inserting a {@link BatchExecExchange} with batch shuffle mode.\n+ *\n+ * <p>For a detailed explanation of the algorithm, see appendix of the\n+ * <a href=\"https://docs.google.com/document/d/1qKVohV12qn-bM51cBZ8Hcgp31ntwClxjoiNBUOqVHsI\">design doc</a>.\n+ */\n+@Internal\n+public class InputPriorityConflictResolver {\n+\n+\tprivate final List<ExecNode<?, ?>> sinks;\n+\n+\tprivate TopologyGraph graph;\n+\n+\tpublic InputPriorityConflictResolver(List<ExecNode<?, ?>> sinks) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "22cf14c8194ecbb1811caf7bddf03641bf82c1d3"}, "originalPosition": 103}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4OTAwMzM3OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/InputPriorityConflictResolver.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQwODoxNTowNlrOHlfqxw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQxMjozMzozOFrOHlpedw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTA3ODIxNQ==", "bodyText": "could you provide more test cases to cover different branches ?", "url": "https://github.com/apache/flink/pull/13692#discussion_r509078215", "createdAt": "2020-10-21T08:15:06Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/InputPriorityConflictResolver.java", "diffHunk": "@@ -0,0 +1,353 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.reuse;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.transformations.ShuffleMode;\n+import org.apache.flink.table.planner.plan.nodes.exec.AbstractExecNodeExactlyOnceVisitor;\n+import org.apache.flink.table.planner.plan.nodes.exec.BatchExecNode;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecEdge;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecBoundedStreamScan;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecExchange;\n+import org.apache.flink.table.planner.plan.trait.FlinkRelDistribution;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.calcite.rel.RelNode;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.Set;\n+import java.util.TreeMap;\n+\n+/**\n+ * This class contains algorithm to detect and resolve input priority conflict in an {@link ExecNode} graph.\n+ *\n+ * <p>Some batch operators (for example, hash join and nested loop join) have different priorities for their inputs.\n+ * When some operators are reused, a deadlock may occur due to the conflict in these priorities.\n+ *\n+ * <p>For example, consider the SQL query:\n+ * <pre>\n+ * WITH\n+ *   T1 AS (SELECT a, COUNT(*) AS cnt1 FROM x GROUP BY a),\n+ *   T2 AS (SELECT d, COUNT(*) AS cnt2 FROM y GROUP BY d)\n+ * SELECT * FROM\n+ *   (SELECT cnt1, cnt2 FROM T1 LEFT JOIN T2 ON a = d)\n+ *   UNION ALL\n+ *   (SELECT cnt1, cnt2 FROM T2 LEFT JOIN T1 ON d = a)\n+ * </pre>\n+ *\n+ * <p>When sub-plan reuse are enabled, we'll get the following physical plan:\n+ * <pre>\n+ * Union(all=[true], union=[cnt1, cnt2])\n+ * :- Calc(select=[CAST(cnt1) AS cnt1, cnt2])\n+ * :  +- HashJoin(joinType=[LeftOuterJoin], where=[=(a, d)], select=[a, cnt1, d, cnt2], build=[right])\n+ * :     :- HashAggregate(isMerge=[true], groupBy=[a], select=[a, Final_COUNT(count1$0) AS cnt1], reuse_id=[2])\n+ * :     :  +- Exchange(distribution=[hash[a]])\n+ * :     :     +- LocalHashAggregate(groupBy=[a], select=[a, Partial_COUNT(*) AS count1$0])\n+ * :     :        +- Calc(select=[a])\n+ * :     :           +- LegacyTableSourceScan(table=[[default_catalog, default_database, x, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])\n+ * :     +- HashAggregate(isMerge=[true], groupBy=[d], select=[d, Final_COUNT(count1$0) AS cnt2], reuse_id=[1])\n+ * :        +- Exchange(distribution=[hash[d]])\n+ * :           +- LocalHashAggregate(groupBy=[d], select=[d, Partial_COUNT(*) AS count1$0])\n+ * :              +- Calc(select=[d])\n+ * :                 +- LegacyTableSourceScan(table=[[default_catalog, default_database, y, source: [TestTableSource(d, e, f)]]], fields=[d, e, f])\n+ * +- Calc(select=[cnt1, CAST(cnt2) AS cnt2])\n+ *    +- HashJoin(joinType=[LeftOuterJoin], where=[=(d, a)], select=[d, cnt2, a, cnt1], build=[right])\n+ *       :- Reused(reference_id=[1])\n+ *       +- Reused(reference_id=[2])\n+ * </pre>\n+ *\n+ * <p>Note that the first hash join needs to read all results from the hash aggregate whose reuse id is 1\n+ * before reading the results from the hash aggregate whose reuse id is 2, while the second hash join requires\n+ * the opposite. This physical plan will thus cause a deadlock.\n+ *\n+ * <p>This class maintains a topological graph in which an edge pointing from vertex A to vertex B indicates\n+ * that the results from vertex A need to be read before those from vertex B. A loop in the graph indicates\n+ * a deadlock, and we resolve such deadlock by inserting a {@link BatchExecExchange} with batch shuffle mode.\n+ *\n+ * <p>For a detailed explanation of the algorithm, see appendix of the\n+ * <a href=\"https://docs.google.com/document/d/1qKVohV12qn-bM51cBZ8Hcgp31ntwClxjoiNBUOqVHsI\">design doc</a>.\n+ */\n+@Internal\n+public class InputPriorityConflictResolver {\n+\n+\tprivate final List<ExecNode<?, ?>> sinks;\n+\n+\tprivate TopologyGraph graph;\n+\n+\tpublic InputPriorityConflictResolver(List<ExecNode<?, ?>> sinks) {\n+\t\tPreconditions.checkArgument(\n+\t\t\tsinks.stream().allMatch(sink -> sink instanceof BatchExecNode),\n+\t\t\t\"InputPriorityConflictResolver can only be used for batch jobs.\");\n+\t\tthis.sinks = sinks;\n+\t}\n+\n+\tpublic void detectAndResolve() {\n+\t\t// build an initial topology graph\n+\t\tgraph = new TopologyGraph(sinks);\n+\n+\t\t// check and resolve conflicts about input priorities\n+\t\tAbstractExecNodeExactlyOnceVisitor inputPriorityVisitor = new AbstractExecNodeExactlyOnceVisitor() {\n+\t\t\t@Override\n+\t\t\tprotected void visitNode(ExecNode<?, ?> node) {\n+\t\t\t\tvisitInputs(node);\n+\t\t\t\tcheckInputPriorities(node);\n+\t\t\t}\n+\t\t};\n+\t\tsinks.forEach(n -> n.accept(inputPriorityVisitor));\n+\t}\n+\n+\tprivate void checkInputPriorities(ExecNode<?, ?> node) {\n+\t\t// group inputs by input priorities\n+\t\tTreeMap<Integer, List<Integer>> inputPriorityGroupMap = new TreeMap<>();\n+\t\tPreconditions.checkState(\n+\t\t\tnode.getInputNodes().size() == node.getInputEdges().size(),\n+\t\t\t\"Number of inputs nodes does not equal to number of input edges for node \" +\n+\t\t\t\tnode.getClass().getName() + \". This is a bug.\");\n+\t\tfor (int i = 0; i < node.getInputNodes().size(); i++) {\n+\t\t\tint priority = node.getInputEdges().get(i).getPriority();\n+\t\t\tinputPriorityGroupMap.computeIfAbsent(priority, k -> new ArrayList<>()).add(i);\n+\t\t}\n+\n+\t\t// add edges between neighboring priority groups\n+\t\tList<List<Integer>> inputPriorityGroups = new ArrayList<>(inputPriorityGroupMap.values());\n+\t\tfor (int i = 0; i + 1 < inputPriorityGroups.size(); i++) {\n+\t\t\tList<Integer> higherGroup = inputPriorityGroups.get(i);\n+\t\t\tList<Integer> lowerGroup = inputPriorityGroups.get(i + 1);\n+\n+\t\t\tfor (int higher : higherGroup) {\n+\t\t\t\tfor (int lower : lowerGroup) {\n+\t\t\t\t\taddTopologyEdges(node, higher, lower);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate void addTopologyEdges(ExecNode<?, ?> node, int higherInput, int lowerInput) {\n+\t\tExecNode<?, ?> higherNode = node.getInputNodes().get(higherInput);\n+\t\tExecNode<?, ?> lowerNode = node.getInputNodes().get(lowerInput);\n+\t\tList<ExecNode<?, ?>> lowerAncestors = calculateAncestors(lowerNode);\n+\n+\t\tList<Tuple2<ExecNode<?, ?>, ExecNode<?, ?>>> linkedEdges = new ArrayList<>();\n+\t\tfor (ExecNode<?, ?> ancestor : lowerAncestors) {\n+\t\t\tif (graph.link(higherNode, ancestor)) {\n+\t\t\t\tlinkedEdges.add(Tuple2.of(higherNode, ancestor));\n+\t\t\t} else {\n+\t\t\t\t// a conflict occurs, resolve it by adding a batch exchange\n+\t\t\t\t// and revert all linked edges\n+\t\t\t\tif (lowerNode instanceof BatchExecExchange) {\n+\t\t\t\t\tBatchExecExchange exchange = (BatchExecExchange) lowerNode;\n+\t\t\t\t\texchange.setRequiredShuffleMode(ShuffleMode.BATCH);\n+\t\t\t\t} else {\n+\t\t\t\t\tnode.replaceInputNode(lowerInput, (ExecNode) createExchange(node, lowerInput));\n+\t\t\t\t}\n+\n+\t\t\t\tfor (Tuple2<ExecNode<?, ?>, ExecNode<?, ?>> linkedEdge : linkedEdges) {\n+\t\t\t\t\tgraph.unlink(linkedEdge.f0, linkedEdge.f1);\n+\t\t\t\t}\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * Find the ancestors by going through PIPELINED edges.\n+\t */\n+\t@VisibleForTesting\n+\tList<ExecNode<?, ?>> calculateAncestors(ExecNode<?, ?> node) {\n+\t\tList<ExecNode<?, ?>> ret = new ArrayList<>();\n+\t\tAbstractExecNodeExactlyOnceVisitor ancestorVisitor = new AbstractExecNodeExactlyOnceVisitor() {\n+\t\t\t@Override\n+\t\t\tprotected void visitNode(ExecNode<?, ?> node) {\n+\t\t\t\tList<ExecEdge> inputEdges = node.getInputEdges();\n+\t\t\t\tboolean hasAncestor = false;\n+\n+\t\t\t\tfor (int i = 0; i < inputEdges.size(); i++) {\n+\t\t\t\t\t// we only go through PIPELINED edges\n+\t\t\t\t\tif (inputEdges.get(i).getDamBehavior().stricterOrEqual(ExecEdge.DamBehavior.END_INPUT)) {\n+\t\t\t\t\t\tcontinue;\n+\t\t\t\t\t}\n+\t\t\t\t\thasAncestor = true;\n+\t\t\t\t\tnode.getInputNodes().get(i).accept(this);\n+\t\t\t\t}\n+\n+\t\t\t\tif (!hasAncestor) {\n+\t\t\t\t\tret.add(node);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t};\n+\t\tnode.accept(ancestorVisitor);\n+\t\treturn ret;\n+\t}\n+\n+\tprivate BatchExecExchange createExchange(ExecNode<?, ?> node, int idx) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "22cf14c8194ecbb1811caf7bddf03641bf82c1d3"}, "originalPosition": 208}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTIzODkwMw==", "bodyText": "Existing tests in DeadlockBreakupTest already covers the possible branches.", "url": "https://github.com/apache/flink/pull/13692#discussion_r509238903", "createdAt": "2020-10-21T12:33:38Z", "author": {"login": "tsreaper"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/InputPriorityConflictResolver.java", "diffHunk": "@@ -0,0 +1,353 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.reuse;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.transformations.ShuffleMode;\n+import org.apache.flink.table.planner.plan.nodes.exec.AbstractExecNodeExactlyOnceVisitor;\n+import org.apache.flink.table.planner.plan.nodes.exec.BatchExecNode;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecEdge;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecBoundedStreamScan;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecExchange;\n+import org.apache.flink.table.planner.plan.trait.FlinkRelDistribution;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.calcite.rel.RelNode;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.Set;\n+import java.util.TreeMap;\n+\n+/**\n+ * This class contains algorithm to detect and resolve input priority conflict in an {@link ExecNode} graph.\n+ *\n+ * <p>Some batch operators (for example, hash join and nested loop join) have different priorities for their inputs.\n+ * When some operators are reused, a deadlock may occur due to the conflict in these priorities.\n+ *\n+ * <p>For example, consider the SQL query:\n+ * <pre>\n+ * WITH\n+ *   T1 AS (SELECT a, COUNT(*) AS cnt1 FROM x GROUP BY a),\n+ *   T2 AS (SELECT d, COUNT(*) AS cnt2 FROM y GROUP BY d)\n+ * SELECT * FROM\n+ *   (SELECT cnt1, cnt2 FROM T1 LEFT JOIN T2 ON a = d)\n+ *   UNION ALL\n+ *   (SELECT cnt1, cnt2 FROM T2 LEFT JOIN T1 ON d = a)\n+ * </pre>\n+ *\n+ * <p>When sub-plan reuse are enabled, we'll get the following physical plan:\n+ * <pre>\n+ * Union(all=[true], union=[cnt1, cnt2])\n+ * :- Calc(select=[CAST(cnt1) AS cnt1, cnt2])\n+ * :  +- HashJoin(joinType=[LeftOuterJoin], where=[=(a, d)], select=[a, cnt1, d, cnt2], build=[right])\n+ * :     :- HashAggregate(isMerge=[true], groupBy=[a], select=[a, Final_COUNT(count1$0) AS cnt1], reuse_id=[2])\n+ * :     :  +- Exchange(distribution=[hash[a]])\n+ * :     :     +- LocalHashAggregate(groupBy=[a], select=[a, Partial_COUNT(*) AS count1$0])\n+ * :     :        +- Calc(select=[a])\n+ * :     :           +- LegacyTableSourceScan(table=[[default_catalog, default_database, x, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])\n+ * :     +- HashAggregate(isMerge=[true], groupBy=[d], select=[d, Final_COUNT(count1$0) AS cnt2], reuse_id=[1])\n+ * :        +- Exchange(distribution=[hash[d]])\n+ * :           +- LocalHashAggregate(groupBy=[d], select=[d, Partial_COUNT(*) AS count1$0])\n+ * :              +- Calc(select=[d])\n+ * :                 +- LegacyTableSourceScan(table=[[default_catalog, default_database, y, source: [TestTableSource(d, e, f)]]], fields=[d, e, f])\n+ * +- Calc(select=[cnt1, CAST(cnt2) AS cnt2])\n+ *    +- HashJoin(joinType=[LeftOuterJoin], where=[=(d, a)], select=[d, cnt2, a, cnt1], build=[right])\n+ *       :- Reused(reference_id=[1])\n+ *       +- Reused(reference_id=[2])\n+ * </pre>\n+ *\n+ * <p>Note that the first hash join needs to read all results from the hash aggregate whose reuse id is 1\n+ * before reading the results from the hash aggregate whose reuse id is 2, while the second hash join requires\n+ * the opposite. This physical plan will thus cause a deadlock.\n+ *\n+ * <p>This class maintains a topological graph in which an edge pointing from vertex A to vertex B indicates\n+ * that the results from vertex A need to be read before those from vertex B. A loop in the graph indicates\n+ * a deadlock, and we resolve such deadlock by inserting a {@link BatchExecExchange} with batch shuffle mode.\n+ *\n+ * <p>For a detailed explanation of the algorithm, see appendix of the\n+ * <a href=\"https://docs.google.com/document/d/1qKVohV12qn-bM51cBZ8Hcgp31ntwClxjoiNBUOqVHsI\">design doc</a>.\n+ */\n+@Internal\n+public class InputPriorityConflictResolver {\n+\n+\tprivate final List<ExecNode<?, ?>> sinks;\n+\n+\tprivate TopologyGraph graph;\n+\n+\tpublic InputPriorityConflictResolver(List<ExecNode<?, ?>> sinks) {\n+\t\tPreconditions.checkArgument(\n+\t\t\tsinks.stream().allMatch(sink -> sink instanceof BatchExecNode),\n+\t\t\t\"InputPriorityConflictResolver can only be used for batch jobs.\");\n+\t\tthis.sinks = sinks;\n+\t}\n+\n+\tpublic void detectAndResolve() {\n+\t\t// build an initial topology graph\n+\t\tgraph = new TopologyGraph(sinks);\n+\n+\t\t// check and resolve conflicts about input priorities\n+\t\tAbstractExecNodeExactlyOnceVisitor inputPriorityVisitor = new AbstractExecNodeExactlyOnceVisitor() {\n+\t\t\t@Override\n+\t\t\tprotected void visitNode(ExecNode<?, ?> node) {\n+\t\t\t\tvisitInputs(node);\n+\t\t\t\tcheckInputPriorities(node);\n+\t\t\t}\n+\t\t};\n+\t\tsinks.forEach(n -> n.accept(inputPriorityVisitor));\n+\t}\n+\n+\tprivate void checkInputPriorities(ExecNode<?, ?> node) {\n+\t\t// group inputs by input priorities\n+\t\tTreeMap<Integer, List<Integer>> inputPriorityGroupMap = new TreeMap<>();\n+\t\tPreconditions.checkState(\n+\t\t\tnode.getInputNodes().size() == node.getInputEdges().size(),\n+\t\t\t\"Number of inputs nodes does not equal to number of input edges for node \" +\n+\t\t\t\tnode.getClass().getName() + \". This is a bug.\");\n+\t\tfor (int i = 0; i < node.getInputNodes().size(); i++) {\n+\t\t\tint priority = node.getInputEdges().get(i).getPriority();\n+\t\t\tinputPriorityGroupMap.computeIfAbsent(priority, k -> new ArrayList<>()).add(i);\n+\t\t}\n+\n+\t\t// add edges between neighboring priority groups\n+\t\tList<List<Integer>> inputPriorityGroups = new ArrayList<>(inputPriorityGroupMap.values());\n+\t\tfor (int i = 0; i + 1 < inputPriorityGroups.size(); i++) {\n+\t\t\tList<Integer> higherGroup = inputPriorityGroups.get(i);\n+\t\t\tList<Integer> lowerGroup = inputPriorityGroups.get(i + 1);\n+\n+\t\t\tfor (int higher : higherGroup) {\n+\t\t\t\tfor (int lower : lowerGroup) {\n+\t\t\t\t\taddTopologyEdges(node, higher, lower);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate void addTopologyEdges(ExecNode<?, ?> node, int higherInput, int lowerInput) {\n+\t\tExecNode<?, ?> higherNode = node.getInputNodes().get(higherInput);\n+\t\tExecNode<?, ?> lowerNode = node.getInputNodes().get(lowerInput);\n+\t\tList<ExecNode<?, ?>> lowerAncestors = calculateAncestors(lowerNode);\n+\n+\t\tList<Tuple2<ExecNode<?, ?>, ExecNode<?, ?>>> linkedEdges = new ArrayList<>();\n+\t\tfor (ExecNode<?, ?> ancestor : lowerAncestors) {\n+\t\t\tif (graph.link(higherNode, ancestor)) {\n+\t\t\t\tlinkedEdges.add(Tuple2.of(higherNode, ancestor));\n+\t\t\t} else {\n+\t\t\t\t// a conflict occurs, resolve it by adding a batch exchange\n+\t\t\t\t// and revert all linked edges\n+\t\t\t\tif (lowerNode instanceof BatchExecExchange) {\n+\t\t\t\t\tBatchExecExchange exchange = (BatchExecExchange) lowerNode;\n+\t\t\t\t\texchange.setRequiredShuffleMode(ShuffleMode.BATCH);\n+\t\t\t\t} else {\n+\t\t\t\t\tnode.replaceInputNode(lowerInput, (ExecNode) createExchange(node, lowerInput));\n+\t\t\t\t}\n+\n+\t\t\t\tfor (Tuple2<ExecNode<?, ?>, ExecNode<?, ?>> linkedEdge : linkedEdges) {\n+\t\t\t\t\tgraph.unlink(linkedEdge.f0, linkedEdge.f1);\n+\t\t\t\t}\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * Find the ancestors by going through PIPELINED edges.\n+\t */\n+\t@VisibleForTesting\n+\tList<ExecNode<?, ?>> calculateAncestors(ExecNode<?, ?> node) {\n+\t\tList<ExecNode<?, ?>> ret = new ArrayList<>();\n+\t\tAbstractExecNodeExactlyOnceVisitor ancestorVisitor = new AbstractExecNodeExactlyOnceVisitor() {\n+\t\t\t@Override\n+\t\t\tprotected void visitNode(ExecNode<?, ?> node) {\n+\t\t\t\tList<ExecEdge> inputEdges = node.getInputEdges();\n+\t\t\t\tboolean hasAncestor = false;\n+\n+\t\t\t\tfor (int i = 0; i < inputEdges.size(); i++) {\n+\t\t\t\t\t// we only go through PIPELINED edges\n+\t\t\t\t\tif (inputEdges.get(i).getDamBehavior().stricterOrEqual(ExecEdge.DamBehavior.END_INPUT)) {\n+\t\t\t\t\t\tcontinue;\n+\t\t\t\t\t}\n+\t\t\t\t\thasAncestor = true;\n+\t\t\t\t\tnode.getInputNodes().get(i).accept(this);\n+\t\t\t\t}\n+\n+\t\t\t\tif (!hasAncestor) {\n+\t\t\t\t\tret.add(node);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t};\n+\t\tnode.accept(ancestorVisitor);\n+\t\treturn ret;\n+\t}\n+\n+\tprivate BatchExecExchange createExchange(ExecNode<?, ?> node, int idx) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTA3ODIxNQ=="}, "originalCommit": {"oid": "22cf14c8194ecbb1811caf7bddf03641bf82c1d3"}, "originalPosition": 208}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4OTAxMjAyOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/plan/reuse/InputPriorityConflictResolverTest.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQwODoxNjo1OVrOHlfwBw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQwODoxNjo1OVrOHlfwBw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTA3OTU1OQ==", "bodyText": "add some comments to explain Pxx, Exx", "url": "https://github.com/apache/flink/pull/13692#discussion_r509079559", "createdAt": "2020-10-21T08:16:59Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/plan/reuse/InputPriorityConflictResolverTest.java", "diffHunk": "@@ -0,0 +1,180 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.reuse;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecEdge;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecNode;\n+import org.apache.flink.table.planner.plan.nodes.exec.TestingExecNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecExchange;\n+\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+\n+/**\n+ * Tests for {@link InputPriorityConflictResolver}.\n+ */\n+public class InputPriorityConflictResolverTest {\n+\n+\tprivate Tuple2<InputPriorityConflictResolver.TopologyGraph, TestingExecNode[]> buildTopologyGraph() {\n+\t\t// 0 -> 1 -> 2 --------> 5\n+\t\t//       \\-> 3 -> 4 -/\n+\t\t//            \\-> 6 -> 7\n+\t\tTestingExecNode[] nodes = new TestingExecNode[8];\n+\t\tfor (int i = 0; i < nodes.length; i++) {\n+\t\t\tnodes[i] = new TestingExecNode();\n+\t\t}\n+\t\tnodes[1].addInput(nodes[0]);\n+\t\tnodes[2].addInput(nodes[1]);\n+\t\tnodes[3].addInput(nodes[1]);\n+\t\tnodes[4].addInput(nodes[3]);\n+\t\tnodes[5].addInput(nodes[2]);\n+\t\tnodes[5].addInput(nodes[4]);\n+\t\tnodes[6].addInput(nodes[3]);\n+\t\tnodes[7].addInput(nodes[6]);\n+\n+\t\treturn Tuple2.of(\n+\t\t\tnew InputPriorityConflictResolver.TopologyGraph(Arrays.asList(nodes[5], nodes[7])),\n+\t\t\tnodes);\n+\t}\n+\n+\t@Test\n+\tpublic void testTopologyGraphCanReach() {\n+\t\tTuple2<InputPriorityConflictResolver.TopologyGraph, TestingExecNode[]> tuple2 = buildTopologyGraph();\n+\t\tInputPriorityConflictResolver.TopologyGraph graph = tuple2.f0;\n+\t\tTestingExecNode[] nodes = tuple2.f1;\n+\n+\t\tString[] canReach = new String[] {\n+\t\t\t\"11111111\",\n+\t\t\t\"01111111\",\n+\t\t\t\"00100100\",\n+\t\t\t\"00011111\",\n+\t\t\t\"00001100\",\n+\t\t\t\"00000100\",\n+\t\t\t\"00000011\",\n+\t\t\t\"00000001\"};\n+\t\tfor (int i = 0; i < 8; i++) {\n+\t\t\tfor (int j = 0; j < 8; j++) {\n+\t\t\t\tif (canReach[i].charAt(j) == '1') {\n+\t\t\t\t\tAssert.assertTrue(graph.canReach(nodes[i], nodes[j]));\n+\t\t\t\t} else {\n+\t\t\t\t\tAssert.assertFalse(graph.canReach(nodes[i], nodes[j]));\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t@Test\n+\tpublic void testTopologyGraphLink() {\n+\t\tTuple2<InputPriorityConflictResolver.TopologyGraph, TestingExecNode[]> tuple2 = buildTopologyGraph();\n+\t\tInputPriorityConflictResolver.TopologyGraph graph = tuple2.f0;\n+\t\tTestingExecNode[] nodes = tuple2.f1;\n+\n+\t\tAssert.assertTrue(graph.link(nodes[2], nodes[4]));\n+\t\tAssert.assertTrue(graph.link(nodes[3], nodes[5]));\n+\t\tAssert.assertTrue(graph.link(nodes[5], nodes[6]));\n+\t\tAssert.assertFalse(graph.link(nodes[7], nodes[2]));\n+\t\tAssert.assertFalse(graph.link(nodes[7], nodes[4]));\n+\t\tAssert.assertTrue(graph.link(nodes[0], nodes[7]));\n+\t}\n+\n+\t@Test\n+\tpublic void testTopologyGraphUnlink() {\n+\t\tTuple2<InputPriorityConflictResolver.TopologyGraph, TestingExecNode[]> tuple2 = buildTopologyGraph();\n+\t\tInputPriorityConflictResolver.TopologyGraph graph = tuple2.f0;\n+\t\tTestingExecNode[] nodes = tuple2.f1;\n+\n+\t\tgraph.unlink(nodes[2], nodes[5]);\n+\t\tAssert.assertTrue(graph.canReach(nodes[0], nodes[5]));\n+\t\tgraph.unlink(nodes[4], nodes[5]);\n+\t\tAssert.assertFalse(graph.canReach(nodes[0], nodes[5]));\n+\t\tgraph.unlink(nodes[3], nodes[6]);\n+\t\tAssert.assertFalse(graph.canReach(nodes[0], nodes[7]));\n+\t}\n+\n+\t@Test\n+\tpublic void testCalculateAncestors() {\n+\t\t// 0 ------P----> 1 -E--> 2\n+\t\t//   \\-----P----> 3 -P-/\n+\t\t// 4 -E-> 5 -P-/ /\n+\t\t// 6 -----E-----/\n+\t\tTestingExecNode[] nodes = new TestingExecNode[7];\n+\t\tfor (int i = 0; i < nodes.length; i++) {\n+\t\t\tnodes[i] = new TestingExecNode();\n+\t\t}\n+\t\tnodes[1].addInput(nodes[0]);\n+\t\tnodes[2].addInput(nodes[1], ExecEdge.builder().damBehavior(ExecEdge.DamBehavior.END_INPUT).build());\n+\t\tnodes[2].addInput(nodes[3]);\n+\t\tnodes[3].addInput(nodes[0]);\n+\t\tnodes[3].addInput(nodes[5]);\n+\t\tnodes[3].addInput(nodes[6], ExecEdge.builder().damBehavior(ExecEdge.DamBehavior.END_INPUT).build());\n+\t\tnodes[5].addInput(nodes[4], ExecEdge.builder().damBehavior(ExecEdge.DamBehavior.END_INPUT).build());\n+\n+\t\tInputPriorityConflictResolver resolver = new InputPriorityConflictResolver(Collections.singletonList(nodes[2]));\n+\t\tList<ExecNode<?, ?>> ancestors = resolver.calculateAncestors(nodes[2]);\n+\t\tAssert.assertEquals(2, ancestors.size());\n+\t\tAssert.assertTrue(ancestors.contains(nodes[0]));\n+\t\tAssert.assertTrue(ancestors.contains(nodes[5]));\n+\t}\n+\n+\t@Test\n+\tpublic void testDetectAndResolve() {\n+\t\t// 0 --------(P0)----> 1 --(P0)-----------> 7\n+\t\t//  \\                    \\-(P0)-> 2 -(P0)--/\n+\t\t//   \\-------(P0)----> 3 --(P1)-----------/\n+\t\t//    \\------(P0)----> 4 --(P10)---------/\n+\t\t//     \\              /                 /\n+\t\t//      \\    8 -(P0)-<                 /\n+\t\t//       \\            \\               /\n+\t\t//        \\--(E0)----> 5 --(P10)-----/\n+\t\t// 6 ---------(P100)----------------/", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "22cf14c8194ecbb1811caf7bddf03641bf82c1d3"}, "originalPosition": 150}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4OTAxNDEyOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/nodes/exec/TestingExecNode.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQwODoxNzozMFrOHlfxWw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQwODoxNzozMFrOHlfxWw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTA3OTg5OQ==", "bodyText": "rename to TestingBatchExecNode ?\nalso extends from AbstractRelNode, which has implemented many methods.", "url": "https://github.com/apache/flink/pull/13692#discussion_r509079899", "createdAt": "2020-10-21T08:17:30Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/nodes/exec/TestingExecNode.scala", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.nodes.exec\n+\n+import org.apache.flink.api.dag.Transformation\n+import org.apache.flink.table.api.TableConfig\n+import org.apache.flink.table.planner.calcite.{FlinkContextImpl, FlinkRelOptClusterFactory, FlinkRexBuilder, FlinkTypeFactory, FlinkTypeSystem}\n+import org.apache.flink.table.planner.delegation.BatchPlanner\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchPhysicalRel\n+\n+import org.apache.calcite.plan.hep.{HepPlanner, HepProgram}\n+import org.apache.calcite.plan.{Convention, RelOptCluster, RelOptCost, RelOptPlanner, RelOptQuery, RelOptTable, RelTraitSet}\n+import org.apache.calcite.rel.`type`.{RelDataType, RelDataTypeSystem}\n+import org.apache.calcite.rel.core.CorrelationId\n+import org.apache.calcite.rel.{RelCollation, RelNode, RelShuttle, RelVisitor, RelWriter}\n+import org.apache.calcite.rel.metadata.{Metadata, RelMetadataQuery}\n+import org.apache.calcite.rex.{RexNode, RexShuttle}\n+import org.apache.calcite.util.{ImmutableBitSet, Litmus}\n+\n+import java.util\n+\n+/**\n+ * [[BatchExecNode]] for testing purpose.\n+ */\n+class TestingExecNode extends BatchExecNode[BatchPlanner] with BatchPhysicalRel {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "22cf14c8194ecbb1811caf7bddf03641bf82c1d3"}, "originalPosition": 41}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 26, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}