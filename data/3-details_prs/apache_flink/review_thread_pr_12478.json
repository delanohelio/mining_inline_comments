{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDI3NjUwOTI5", "number": 12478, "reviewThreads": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxMzo0NDozM1rOECkVYg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQwNzo1MzozMlrOEELq4Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcxMTI3OTA2OnYy", "diffSide": "RIGHT", "path": "flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/SubtaskCheckpointCoordinatorImpl.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxMzo0NDozM1rOGfGj1w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxMzo0NDozM1rOGfGj1w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTI2NjUxOQ==", "bodyText": "This is just an optimization.", "url": "https://github.com/apache/flink/pull/12478#discussion_r435266519", "createdAt": "2020-06-04T13:44:33Z", "author": {"login": "rkhachatryan"}, "path": "flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/SubtaskCheckpointCoordinatorImpl.java", "diffHunk": "@@ -158,6 +159,16 @@ private ChannelStateWriter openChannelStateWriter() {\n \t@Override\n \tpublic void abortCheckpointOnBarrier(long checkpointId, Throwable cause, OperatorChain<?, ?> operatorChain) throws IOException {\n \t\tLOG.debug(\"Aborting checkpoint via cancel-barrier {} for task {}\", checkpointId, taskName);\n+\t\tlastCheckpointId = Math.max(lastCheckpointId, checkpointId);\n+\t\tIterator<Long> iterator = abortedCheckpointIds.iterator();\n+\t\twhile (iterator.hasNext()) {\n+\t\t\tlong next = iterator.next();\n+\t\t\tif (next < lastCheckpointId) {\n+\t\t\t\titerator.remove();\n+\t\t\t} else {\n+\t\t\t\tbreak;\n+\t\t\t}\n+\t\t}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "be8fbcf506b8dd38e5425cf772a55f033f0962b0"}, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcxMTMyMTM2OnYy", "diffSide": "RIGHT", "path": "flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/SubtaskCheckpointCoordinatorImpl.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxMzo1MTowOFrOGfG-3w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOFQwOTo0NjozNlrOGgWkRw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTI3MzQzOQ==", "bodyText": "This is just an optimization (possible now with abort and cleanup=false).", "url": "https://github.com/apache/flink/pull/12478#discussion_r435273439", "createdAt": "2020-06-04T13:51:08Z", "author": {"login": "rkhachatryan"}, "path": "flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/SubtaskCheckpointCoordinatorImpl.java", "diffHunk": "@@ -281,6 +281,8 @@ public void notifyCheckpointAborted(long checkpointId, OperatorChain<?, ?> opera\n \t\t\t\t}\n \t\t\t}\n \n+\t\t\tchannelStateWriter.abort(checkpointId, new CancellationException(\"checkpoint aborted via notification\"), false);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "420b7b3f645625b0b43f056440509cdd53bbd4ef"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjUwODAzMw==", "bodyText": "I like this optimization.\nWhat happens if some other thread still enqueues data to this checkpoint? Or the writes dismissed?", "url": "https://github.com/apache/flink/pull/12478#discussion_r436508033", "createdAt": "2020-06-08T07:39:00Z", "author": {"login": "AHeise"}, "path": "flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/SubtaskCheckpointCoordinatorImpl.java", "diffHunk": "@@ -281,6 +281,8 @@ public void notifyCheckpointAborted(long checkpointId, OperatorChain<?, ?> opera\n \t\t\t\t}\n \t\t\t}\n \n+\t\t\tchannelStateWriter.abort(checkpointId, new CancellationException(\"checkpoint aborted via notification\"), false);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTI3MzQzOQ=="}, "originalCommit": {"oid": "420b7b3f645625b0b43f056440509cdd53bbd4ef"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjU3NzM1MQ==", "bodyText": "Yes, write requests will be ignored when dequeued by ChannelStateWriteRequestExecutor.", "url": "https://github.com/apache/flink/pull/12478#discussion_r436577351", "createdAt": "2020-06-08T09:46:36Z", "author": {"login": "rkhachatryan"}, "path": "flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/SubtaskCheckpointCoordinatorImpl.java", "diffHunk": "@@ -281,6 +281,8 @@ public void notifyCheckpointAborted(long checkpointId, OperatorChain<?, ?> opera\n \t\t\t\t}\n \t\t\t}\n \n+\t\t\tchannelStateWriter.abort(checkpointId, new CancellationException(\"checkpoint aborted via notification\"), false);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTI3MzQzOQ=="}, "originalCommit": {"oid": "420b7b3f645625b0b43f056440509cdd53bbd4ef"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcxOTI0NDAwOnYy", "diffSide": "RIGHT", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/ChannelStateWriterImpl.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOFQwNzoyNDowMFrOGgR6_Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOFQwNzoyNDowMFrOGgR6_Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjUwMTI0NQ==", "bodyText": "nit: usually we add task name to front, also it would be nice to use %d > %d for the comparison values.", "url": "https://github.com/apache/flink/pull/12478#discussion_r436501245", "createdAt": "2020-06-08T07:24:00Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/ChannelStateWriterImpl.java", "diffHunk": "@@ -102,11 +102,11 @@ public void start(long checkpointId, CheckpointOptions checkpointOptions) {\n \t\tLOG.debug(\"{} starting checkpoint {} ({})\", taskName, checkpointId, checkpointOptions);\n \t\tChannelStateWriteResult result = new ChannelStateWriteResult();\n \t\tChannelStateWriteResult put = results.computeIfAbsent(checkpointId, id -> {\n-\t\t\tPreconditions.checkState(results.size() < maxCheckpoints, \"results.size() > maxCheckpoints\", results.size(), maxCheckpoints);\n+\t\t\tPreconditions.checkState(results.size() < maxCheckpoints, String.format(\"can't start %d, results.size() > maxCheckpoints: %d, %d, %s\", checkpointId, results.size(), maxCheckpoints, taskName));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "72bfc6b84e33e5abcee8d078f6d4d00904f626f2"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcxOTI1MDg3OnYy", "diffSide": "RIGHT", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/ChannelStateWriterImpl.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOFQwNzoyNjoxM1rOGgR-9g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQwODozNTowNVrOGg-jgA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjUwMjI2Mg==", "bodyText": "I also thought about that in FLINK-17218, but ultimately couldn't find a reason why any arbitrary large value offers an advantage over the current value. It makes the failing checkState more unlikely, but didn't eliminate it.\nI also thought about removing the checkState, but it helped me to find some issues previously.\nTL;DR I'm not convinced that this is a proper fix.", "url": "https://github.com/apache/flink/pull/12478#discussion_r436502262", "createdAt": "2020-06-08T07:26:13Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/ChannelStateWriterImpl.java", "diffHunk": "@@ -56,7 +56,7 @@\n public class ChannelStateWriterImpl implements ChannelStateWriter {\n \n \tprivate static final Logger LOG = LoggerFactory.getLogger(ChannelStateWriterImpl.class);\n-\tprivate static final int DEFAULT_MAX_CHECKPOINTS = 5; // currently, only single in-flight checkpoint is supported\n+\tprivate static final int DEFAULT_MAX_CHECKPOINTS = 100; // includes max-concurrent-checkpoints + checkpoints to be aborted (scheduled via mailbox)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4ee338a57d401b7ae9c79d926f47637d4e6431e6"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjU2MzY5OQ==", "bodyText": "The map is already cleared during the regular checkpoint abortion by the task thread but it happens with some delay. I see only two reasons why its size could significantly exceed max-concurrent-checkpoints:\n\nbug in abort procedure\ntask thread is stuck while netty thread continues to receive new barriers fast\n\nTheoretically, 2nd case is unbounded, but in practice, I didn't observe issues with a value of 5 in UnalignedITCase after fixing other issues. I think such a scenario where task thread is not aborting writes would signify some problem and should be investigated.\nBesides that, aborting async parts of previous checkpoints upon receiving a new barrier won't work with max-concurrent-checkpoints > 1.", "url": "https://github.com/apache/flink/pull/12478#discussion_r436563699", "createdAt": "2020-06-08T09:21:47Z", "author": {"login": "rkhachatryan"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/ChannelStateWriterImpl.java", "diffHunk": "@@ -56,7 +56,7 @@\n public class ChannelStateWriterImpl implements ChannelStateWriter {\n \n \tprivate static final Logger LOG = LoggerFactory.getLogger(ChannelStateWriterImpl.class);\n-\tprivate static final int DEFAULT_MAX_CHECKPOINTS = 5; // currently, only single in-flight checkpoint is supported\n+\tprivate static final int DEFAULT_MAX_CHECKPOINTS = 100; // includes max-concurrent-checkpoints + checkpoints to be aborted (scheduled via mailbox)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjUwMjI2Mg=="}, "originalCommit": {"oid": "4ee338a57d401b7ae9c79d926f47637d4e6431e6"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzIzMjUxMg==", "bodyText": "Just thinking loud.\nPreviously, the issue was that sources received new RPC triggers while being stuck, which enqueued a ton of mails.\nWith a checkpointing interval of 100ms, you only need to be stuck for 10s until you enqueue 100 mails and hit the limit.\nBut I guess, the assumption is that now notifyCheckpointAborted is called reliably. And if doesn't, we need to fail probably anyways, since something is broken.\nSo I guess this solution is as good as it gets. I trade-off between quickly identifying bugs in abort and running potentially into some issues with ultra slow tasks and ultra fast checkpoint barriers.", "url": "https://github.com/apache/flink/pull/12478#discussion_r437232512", "createdAt": "2020-06-09T08:35:05Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/ChannelStateWriterImpl.java", "diffHunk": "@@ -56,7 +56,7 @@\n public class ChannelStateWriterImpl implements ChannelStateWriter {\n \n \tprivate static final Logger LOG = LoggerFactory.getLogger(ChannelStateWriterImpl.class);\n-\tprivate static final int DEFAULT_MAX_CHECKPOINTS = 5; // currently, only single in-flight checkpoint is supported\n+\tprivate static final int DEFAULT_MAX_CHECKPOINTS = 100; // includes max-concurrent-checkpoints + checkpoints to be aborted (scheduled via mailbox)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjUwMjI2Mg=="}, "originalCommit": {"oid": "4ee338a57d401b7ae9c79d926f47637d4e6431e6"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcxOTI2Mzk4OnYy", "diffSide": "RIGHT", "path": "flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/SubtaskCheckpointCoordinatorImpl.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOFQwNzozMDozNFrOGgSG3A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOFQwNzozMDozNFrOGgSG3A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjUwNDI4NA==", "bodyText": "Should that be warning? I think on concept level yes.\nHowever, in reality, in current UC, this case happens probably quite frequently (barrier overtakes by cancellation marker doesn't). I wouldn't like seeing the log polluted with WARN.", "url": "https://github.com/apache/flink/pull/12478#discussion_r436504284", "createdAt": "2020-06-08T07:30:34Z", "author": {"login": "AHeise"}, "path": "flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/SubtaskCheckpointCoordinatorImpl.java", "diffHunk": "@@ -196,9 +207,14 @@ public void checkpointState(\n \t\t// We generally try to emit the checkpoint barrier as soon as possible to not affect downstream\n \t\t// checkpoint alignments\n \n+\t\tif (lastCheckpointId >= metadata.getCheckpointId()) {\n+\t\t\tLOG.warn(\"Out of order checkpoint barrier (aborted previously?): {} >= {}\", lastCheckpointId, metadata.getCheckpointId());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "be8fbcf506b8dd38e5425cf772a55f033f0962b0"}, "originalPosition": 43}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcxOTI3Mzc3OnYy", "diffSide": "RIGHT", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/ChannelStateWriter.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOFQwNzozNDoxNFrOGgSM8A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOFQwODozMDoyMFrOGgT7lA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjUwNTg0MA==", "bodyText": "Please rename getWriteResult. I think most developers would expect a getter to be idempotent.\nHow about removeWriteResult? That would be similar to how a Java map works. Or getAndRemoveWriteResult to be completely explicit.", "url": "https://github.com/apache/flink/pull/12478#discussion_r436505840", "createdAt": "2020-06-08T07:34:14Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/ChannelStateWriter.java", "diffHunk": "@@ -141,15 +141,10 @@ boolean isDone() {\n \tvoid abort(long checkpointId, Throwable cause);\n \n \t/**\n-\t * Must be called after {@link #start(long, CheckpointOptions)}.\n+\t * Must be called after {@link #start(long, CheckpointOptions)} once.\n \t */\n \tChannelStateWriteResult getWriteResult(long checkpointId);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9fc0ec3ed824c9864430ed97bd681d510c8ee18e"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjUzNDE2NA==", "bodyText": "Good point, I'll rename it to getAndRemoveWriteResult.", "url": "https://github.com/apache/flink/pull/12478#discussion_r436534164", "createdAt": "2020-06-08T08:30:20Z", "author": {"login": "rkhachatryan"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/ChannelStateWriter.java", "diffHunk": "@@ -141,15 +141,10 @@ boolean isDone() {\n \tvoid abort(long checkpointId, Throwable cause);\n \n \t/**\n-\t * Must be called after {@link #start(long, CheckpointOptions)}.\n+\t * Must be called after {@link #start(long, CheckpointOptions)} once.\n \t */\n \tChannelStateWriteResult getWriteResult(long checkpointId);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjUwNTg0MA=="}, "originalCommit": {"oid": "9fc0ec3ed824c9864430ed97bd681d510c8ee18e"}, "originalPosition": 7}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcxOTI4MjA3OnYy", "diffSide": "RIGHT", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/ChannelStateWriter.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOFQwNzozNzoxOFrOGgSSRQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOFQwOTozMToyN1rOGgWDQQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjUwNzIwNQ==", "bodyText": "I guess the intend of cleanup is to avoid throwing an exception if getWriteResult(long) is called nonetheless, right?\nSo in which cases can that happen? Do we ensure cleanup in these cases (e.g., is abort then called twice?)?", "url": "https://github.com/apache/flink/pull/12478#discussion_r436507205", "createdAt": "2020-06-08T07:37:18Z", "author": {"login": "AHeise"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/ChannelStateWriter.java", "diffHunk": "@@ -137,8 +137,9 @@ boolean isDone() {\n \n \t/**\n \t * Aborts the checkpoint and fails pending result for this checkpoint.\n+\t * @param cleanup true if {@link #getWriteResult(long)} is not supposed to be called afterwards.\n \t */\n-\tvoid abort(long checkpointId, Throwable cause);\n+\tvoid abort(long checkpointId, Throwable cause, boolean cleanup);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dad26a041df3442f65b6592bb10611828cf80d37"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjU2ODg5Nw==", "bodyText": "I guess the intend of cleanup is to avoid throwing an exception if getWriteResult(long) is called nonetheless, right?\n\nYes.\n\nSo in which cases can that happen?\n\nabort with cleanup=false can be called when:\n\nreceiving abort notification via RPC\nCheckpointBarrierUnaligner.allBarriersReceivedFuture is cancelled\n\n\nDo we ensure cleanup in these cases (e.g., is abort then called twice?)?\n\nYes, for each checkpoint either \"regular\" abort or getWriteResult is eventually called.", "url": "https://github.com/apache/flink/pull/12478#discussion_r436568897", "createdAt": "2020-06-08T09:31:27Z", "author": {"login": "rkhachatryan"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/ChannelStateWriter.java", "diffHunk": "@@ -137,8 +137,9 @@ boolean isDone() {\n \n \t/**\n \t * Aborts the checkpoint and fails pending result for this checkpoint.\n+\t * @param cleanup true if {@link #getWriteResult(long)} is not supposed to be called afterwards.\n \t */\n-\tvoid abort(long checkpointId, Throwable cause);\n+\tvoid abort(long checkpointId, Throwable cause, boolean cleanup);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjUwNzIwNQ=="}, "originalCommit": {"oid": "dad26a041df3442f65b6592bb10611828cf80d37"}, "originalPosition": 7}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcyODAwMjcxOnYy", "diffSide": "RIGHT", "path": "flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/SubtaskCheckpointCoordinatorImpl.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQwNjo1MjoxOVrOGhnHZw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQwODowODo0OFrOGhprXg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg5NzA2Mw==", "bodyText": "I am not quite sure whether we still have race condition here. abortCheckpointOnBarrier might be called from CheckpointBarrierUnaligner after triggering checkpoint into mailbox. After aborting, we did not remove the checkpoint action from mailbox, so the checkpoint might still happen afterwards. So how to guarantee that #getAndRemoveWriteResult would never be called after aborting?", "url": "https://github.com/apache/flink/pull/12478#discussion_r437897063", "createdAt": "2020-06-10T06:52:19Z", "author": {"login": "zhijiangW"}, "path": "flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/SubtaskCheckpointCoordinatorImpl.java", "diffHunk": "@@ -186,7 +186,7 @@ public void abortCheckpointOnBarrier(long checkpointId, Throwable cause, Operato\n \n \t\tcheckpointStorage.clearCacheFor(checkpointId);\n \n-\t\tchannelStateWriter.abort(checkpointId, cause);\n+\t\tchannelStateWriter.abort(checkpointId, cause, true);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6eee77db29211ce1fc8c23d7f6e3df6af9fd12c6"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzkxNTc0Mw==", "bodyText": "I got the answer when seeing the following commit \"Ignore out of order checkpoints in SubtaskCheckpointCoordinator\". It updates the lastCheckpointId to prevent the following aborted checkpoint execution.", "url": "https://github.com/apache/flink/pull/12478#discussion_r437915743", "createdAt": "2020-06-10T07:31:26Z", "author": {"login": "zhijiangW"}, "path": "flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/SubtaskCheckpointCoordinatorImpl.java", "diffHunk": "@@ -186,7 +186,7 @@ public void abortCheckpointOnBarrier(long checkpointId, Throwable cause, Operato\n \n \t\tcheckpointStorage.clearCacheFor(checkpointId);\n \n-\t\tchannelStateWriter.abort(checkpointId, cause);\n+\t\tchannelStateWriter.abort(checkpointId, cause, true);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg5NzA2Mw=="}, "originalCommit": {"oid": "6eee77db29211ce1fc8c23d7f6e3df6af9fd12c6"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzkzOTAzOA==", "bodyText": "Right, there are several race conditions that are addressed in different commits.", "url": "https://github.com/apache/flink/pull/12478#discussion_r437939038", "createdAt": "2020-06-10T08:08:48Z", "author": {"login": "rkhachatryan"}, "path": "flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/SubtaskCheckpointCoordinatorImpl.java", "diffHunk": "@@ -186,7 +186,7 @@ public void abortCheckpointOnBarrier(long checkpointId, Throwable cause, Operato\n \n \t\tcheckpointStorage.clearCacheFor(checkpointId);\n \n-\t\tchannelStateWriter.abort(checkpointId, cause);\n+\t\tchannelStateWriter.abort(checkpointId, cause, true);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg5NzA2Mw=="}, "originalCommit": {"oid": "6eee77db29211ce1fc8c23d7f6e3df6af9fd12c6"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcyODA2MDUzOnYy", "diffSide": "RIGHT", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/ChannelStateWriterImpl.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQwNzoxMjo0MFrOGhnrFQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQwOTo0MDo0OVrOGhtIVQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzkwNjE5Nw==", "bodyText": "I guess the current way seems a temporary work-around solution, not an elegant way. The initial purpose for introducing this threshold is for logic validating and avoiding invalid checkpoints retained in writer forever. But if we consider the abort delay into this threshold, it seems somehow lose the initial meaning for the guard, and we are really not sure what is the proper value for this threshold.\nThe proper way might resolve the potential race condition in essence, but it might pay more efforts not feasible ATM. So I think we might leave another debt here in future.", "url": "https://github.com/apache/flink/pull/12478#discussion_r437906197", "createdAt": "2020-06-10T07:12:40Z", "author": {"login": "zhijiangW"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/ChannelStateWriterImpl.java", "diffHunk": "@@ -56,7 +56,7 @@\n public class ChannelStateWriterImpl implements ChannelStateWriter {\n \n \tprivate static final Logger LOG = LoggerFactory.getLogger(ChannelStateWriterImpl.class);\n-\tprivate static final int DEFAULT_MAX_CHECKPOINTS = 5; // currently, only single in-flight checkpoint is supported\n+\tprivate static final int DEFAULT_MAX_CHECKPOINTS = 1000; // includes max-concurrent-checkpoints + checkpoints to be aborted (scheduled via mailbox)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "884371e7dc8a604f5b55b8e39f27a1bffd002173"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzkzMzQxMA==", "bodyText": "Yes, me and @AHeise also had these concerns too and discussed it above (I increased the limit to 1K since then).\nI'm not sure if there is a proper solution because in the general case (max-concurrent-checkpoints > 1) we simply don't know which checkpoints to abort. Another issue is thread-safety, which is probably solvable but with some extra complexity which I think doesn't pay off here.", "url": "https://github.com/apache/flink/pull/12478#discussion_r437933410", "createdAt": "2020-06-10T07:59:06Z", "author": {"login": "rkhachatryan"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/ChannelStateWriterImpl.java", "diffHunk": "@@ -56,7 +56,7 @@\n public class ChannelStateWriterImpl implements ChannelStateWriter {\n \n \tprivate static final Logger LOG = LoggerFactory.getLogger(ChannelStateWriterImpl.class);\n-\tprivate static final int DEFAULT_MAX_CHECKPOINTS = 5; // currently, only single in-flight checkpoint is supported\n+\tprivate static final int DEFAULT_MAX_CHECKPOINTS = 1000; // includes max-concurrent-checkpoints + checkpoints to be aborted (scheduled via mailbox)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzkwNjE5Nw=="}, "originalCommit": {"oid": "884371e7dc8a604f5b55b8e39f27a1bffd002173"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzk5NTYwNQ==", "bodyText": "Ok, let's keep it as now since no better solutions right now. And further improve it if possible future.", "url": "https://github.com/apache/flink/pull/12478#discussion_r437995605", "createdAt": "2020-06-10T09:40:49Z", "author": {"login": "zhijiangW"}, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/ChannelStateWriterImpl.java", "diffHunk": "@@ -56,7 +56,7 @@\n public class ChannelStateWriterImpl implements ChannelStateWriter {\n \n \tprivate static final Logger LOG = LoggerFactory.getLogger(ChannelStateWriterImpl.class);\n-\tprivate static final int DEFAULT_MAX_CHECKPOINTS = 5; // currently, only single in-flight checkpoint is supported\n+\tprivate static final int DEFAULT_MAX_CHECKPOINTS = 1000; // includes max-concurrent-checkpoints + checkpoints to be aborted (scheduled via mailbox)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzkwNjE5Nw=="}, "originalCommit": {"oid": "884371e7dc8a604f5b55b8e39f27a1bffd002173"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcyODIwOTYxOnYy", "diffSide": "RIGHT", "path": "flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/SubtaskCheckpointCoordinatorImpl.java", "isResolved": true, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQwNzo1MzozMlrOGhpIzA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQwOTozOTo0NFrOGhtF2Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzkzMDE4OA==", "bodyText": "For the below logic for checking abort request from RPC:\nif (checkAndClearAbortedStatus(metadata.getCheckpointId())) {\n\t\t\tLOG.info(\"Checkpoint {} has been notified as aborted, would not trigger any checkpoint.\", metadata.getCheckpointId());\n\t\t\treturn;\n}\n\nDo we also need to call channelStateWriter.abort for RPC abort? If so, maybe we can integrate these two cases together for better handling the actions for aborting.", "url": "https://github.com/apache/flink/pull/12478#discussion_r437930188", "createdAt": "2020-06-10T07:53:32Z", "author": {"login": "zhijiangW"}, "path": "flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/SubtaskCheckpointCoordinatorImpl.java", "diffHunk": "@@ -221,9 +232,14 @@ public void checkpointState(\n \t\t// We generally try to emit the checkpoint barrier as soon as possible to not affect downstream\n \t\t// checkpoint alignments\n \n+\t\tif (lastCheckpointId >= metadata.getCheckpointId()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "26343fa05d8e72cf21cea7246e332c7850e05db3"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzkzODMzMA==", "bodyText": "Yes, I added it as a separate commit (it is just an optimization because we'll still get a \"regular\" abort call).", "url": "https://github.com/apache/flink/pull/12478#discussion_r437938330", "createdAt": "2020-06-10T08:07:29Z", "author": {"login": "rkhachatryan"}, "path": "flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/SubtaskCheckpointCoordinatorImpl.java", "diffHunk": "@@ -221,9 +232,14 @@ public void checkpointState(\n \t\t// We generally try to emit the checkpoint barrier as soon as possible to not affect downstream\n \t\t// checkpoint alignments\n \n+\t\tif (lastCheckpointId >= metadata.getCheckpointId()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzkzMDE4OA=="}, "originalCommit": {"oid": "26343fa05d8e72cf21cea7246e332c7850e05db3"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzkzOTgyNw==", "bodyText": "In addition, it is a bit tough to understand the semantic of different cases now. The previous lastCheckpointId only reflects the already happened checkpoint, so we can rely on lastCheckpointId >= metadata.getCheckpointId() to discard all the following smaller checkpoints based on the checkpoint id incremental guarantee.\nNow the lastCheckpointId also reflects the canceled id from CheckpointBarrierHandler, so if the handler notifies to abort checkpoint 10, should we also need to ignore checkpoint 9. If so, it is also different with the abort case from RPC. If RPC notifies to abort checkpoint 10, the checkpoint 9 can still execute afterwards since we store the aborted id from RPC in the abortedCheckpointIds set.", "url": "https://github.com/apache/flink/pull/12478#discussion_r437939827", "createdAt": "2020-06-10T08:10:07Z", "author": {"login": "zhijiangW"}, "path": "flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/SubtaskCheckpointCoordinatorImpl.java", "diffHunk": "@@ -221,9 +232,14 @@ public void checkpointState(\n \t\t// We generally try to emit the checkpoint barrier as soon as possible to not affect downstream\n \t\t// checkpoint alignments\n \n+\t\tif (lastCheckpointId >= metadata.getCheckpointId()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzkzMDE4OA=="}, "originalCommit": {"oid": "26343fa05d8e72cf21cea7246e332c7850e05db3"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzk0NzQ5MA==", "bodyText": "Yes, this is the right semantics and I agree it is more complex now.\nI described it in the comment for the aborted checkpoints field.\nHowever, I think this new complexity comes from the domain (or maybe design), and therefore can't be avoided in implementation.", "url": "https://github.com/apache/flink/pull/12478#discussion_r437947490", "createdAt": "2020-06-10T08:22:08Z", "author": {"login": "rkhachatryan"}, "path": "flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/SubtaskCheckpointCoordinatorImpl.java", "diffHunk": "@@ -221,9 +232,14 @@ public void checkpointState(\n \t\t// We generally try to emit the checkpoint barrier as soon as possible to not affect downstream\n \t\t// checkpoint alignments\n \n+\t\tif (lastCheckpointId >= metadata.getCheckpointId()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzkzMDE4OA=="}, "originalCommit": {"oid": "26343fa05d8e72cf21cea7246e332c7850e05db3"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzk5NDk2OQ==", "bodyText": "Yes, the current complexity somehow comes from coupling abort from handler with abort from RPC together, since abortedCheckpointIds will compare with lastCheckpointId(aborted id from handler) in some processes.\nBut indeed I can not think of an easy solution right now and also have not thought of critical problems ATM. So let's further discuss it if encountering problems future.", "url": "https://github.com/apache/flink/pull/12478#discussion_r437994969", "createdAt": "2020-06-10T09:39:44Z", "author": {"login": "zhijiangW"}, "path": "flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/SubtaskCheckpointCoordinatorImpl.java", "diffHunk": "@@ -221,9 +232,14 @@ public void checkpointState(\n \t\t// We generally try to emit the checkpoint barrier as soon as possible to not affect downstream\n \t\t// checkpoint alignments\n \n+\t\tif (lastCheckpointId >= metadata.getCheckpointId()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzkzMDE4OA=="}, "originalCommit": {"oid": "26343fa05d8e72cf21cea7246e332c7850e05db3"}, "originalPosition": 42}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4425, "cost": 1, "resetAt": "2021-11-12T11:57:46Z"}}}