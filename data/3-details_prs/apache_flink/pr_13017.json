{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDU4MzE4ODMx", "number": 13017, "title": "[FLINK-18258][hive] Implement SHOW PARTITIONS for Hive dialect", "bodyText": "What is the purpose of the change\nCurrently Hive dialect supports ADD PARTITION and DROP PARTITION, lack of support for SHOW PARTITIONS. It's necessary to implement SHOW PARTITIONS for Hive dialect.\nBrief change log\n\nAdd SqlShowPartitions and ShowPartitionsOperation to support SqlToOperationConverter for SHOW PARTITIONS convert.\nMethod executeOperation of TableEnvironmentImpl support execution of ShowPartitionsOperation.\n\nVerifying this change\n\nFlinkHiveSqlParserImplTest add method testShowPartitions to verify the parser of SHOW PARTITIONS dialect.\nMethod testAddDropPartitions of HiveDialectITCase add execution of SHOW PARTITIONS dialect to verify the result of the operation.\n\nDoes this pull request potentially affect one of the following parts:\n\nDependencies (does it add or upgrade a dependency): (yes / no)\nThe public API, i.e., is any changed class annotated with @Public(Evolving): (yes / no)\nThe serializers: (yes / no / don't know)\nThe runtime per-record code paths (performance sensitive): (yes / no / don't know)\nAnything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (yes / no / don't know)\nThe S3 file system connector: (yes / no / don't know)\n\nDocumentation\n\nDoes this pull request introduce a new feature? (yes / no)\nIf yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)", "createdAt": "2020-07-29T09:40:32Z", "url": "https://github.com/apache/flink/pull/13017", "merged": true, "mergeCommit": {"oid": "7574952d31caf1c7f7b114571d2e1d0c7b7f123a"}, "closed": true, "closedAt": "2020-08-13T02:13:12Z", "author": {"login": "SteNicholas"}, "timelineItems": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABc5nn91gH2gAyNDU4MzE4ODMxOjE2Y2Q3ZGZmYmZmNjdmZDhhYTA2MGU5ZGNmZGU0YWY3MDBkYjA4Mzk=", "endCursor": "Y3Vyc29yOnYyOpPPAAABc-WXXOgFqTQ2NjM5ODk1Mw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "16cd7dffbff67fd8aa060e9dcfde4af700db0839", "author": {"user": {"login": "SteNicholas", "name": "SteNicholas"}}, "url": "https://github.com/apache/flink/commit/16cd7dffbff67fd8aa060e9dcfde4af700db0839", "committedDate": "2020-07-29T09:29:43Z", "message": "[FLINK-18258][hive] Implement SHOW PARTITIONS for Hive dialect"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU5NzExNDEy", "url": "https://github.com/apache/flink/pull/13017#pullrequestreview-459711412", "createdAt": "2020-08-03T02:11:42Z", "commit": {"oid": "16cd7dffbff67fd8aa060e9dcfde4af700db0839"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2f2a2d5d7ef165e0c7ce068535bc79e90aae9cca", "author": {"user": {"login": "SteNicholas", "name": "SteNicholas"}}, "url": "https://github.com/apache/flink/commit/2f2a2d5d7ef165e0c7ce068535bc79e90aae9cca", "committedDate": "2020-08-06T03:24:03Z", "message": "[FLINK-18258][hive] Implement SHOW PARTITIONS for Hive dialect"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYyMTU1MzA4", "url": "https://github.com/apache/flink/pull/13017#pullrequestreview-462155308", "createdAt": "2020-08-06T03:04:17Z", "commit": {"oid": "16cd7dffbff67fd8aa060e9dcfde4af700db0839"}, "state": "COMMENTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQwMzowNDoxOFrOG8hkkA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQwMzo1Njo1MFrOG8iX_g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjExNzc3Ng==", "bodyText": "I think it'll be clearer to move these to a separate test case.", "url": "https://github.com/apache/flink/pull/13017#discussion_r466117776", "createdAt": "2020-08-06T03:04:18Z", "author": {"login": "lirui-apache"}, "path": "flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveDialectITCase.java", "diffHunk": "@@ -450,6 +450,18 @@ public void testAddDropPartitions() throws Exception {\n \t\tObjectPath tablePath = new ObjectPath(\"default\", \"tbl\");\n \t\tassertEquals(2, hiveCatalog.listPartitions(tablePath).size());\n \n+\t\tList<Row> partitions = Lists.newArrayList(tableEnv.executeSql(\"show partitions tbl\").collect());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "16cd7dffbff67fd8aa060e9dcfde4af700db0839"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjEyMjc0OA==", "bodyText": "If partitionSpec is not null, I think it must not be empty. We can add a check to verify that.", "url": "https://github.com/apache/flink/pull/13017#discussion_r466122748", "createdAt": "2020-08-06T03:24:44Z", "author": {"login": "lirui-apache"}, "path": "flink-table/flink-sql-parser/src/main/java/org/apache/flink/sql/parser/dql/SqlShowPartitions.java", "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.sql.parser.dql;\n+\n+import org.apache.flink.sql.parser.SqlPartitionUtils;\n+\n+import org.apache.calcite.sql.SqlCall;\n+import org.apache.calcite.sql.SqlIdentifier;\n+import org.apache.calcite.sql.SqlKind;\n+import org.apache.calcite.sql.SqlNode;\n+import org.apache.calcite.sql.SqlNodeList;\n+import org.apache.calcite.sql.SqlOperator;\n+import org.apache.calcite.sql.SqlSpecialOperator;\n+import org.apache.calcite.sql.SqlWriter;\n+import org.apache.calcite.sql.parser.SqlParserPos;\n+\n+import javax.annotation.Nullable;\n+\n+import java.util.ArrayList;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+\n+import static java.util.Objects.requireNonNull;\n+\n+/**\n+ * SHOW PARTITIONS sql call.\n+ */\n+public class SqlShowPartitions extends SqlCall {\n+\n+\tpublic static final SqlSpecialOperator OPERATOR = new SqlSpecialOperator(\"SHOW PARTITIONS\", SqlKind.OTHER);\n+\n+\tprotected final SqlIdentifier tableIdentifier;\n+\tprotected final SqlNodeList partitionSpec;\n+\n+\tpublic SqlShowPartitions(SqlParserPos pos, SqlIdentifier tableName, @Nullable SqlNodeList partitionSpec) {\n+\t\tsuper(pos);\n+\t\tthis.tableIdentifier = requireNonNull(tableName, \"tableName should not be null\");\n+\t\tthis.partitionSpec = partitionSpec;\n+\t}\n+\n+\t@Override\n+\tpublic SqlOperator getOperator() {\n+\t\treturn OPERATOR;\n+\t}\n+\n+\t@Override\n+\tpublic List<SqlNode> getOperandList() {\n+\t\tList<SqlNode> operands = new ArrayList<>();\n+\t\toperands.add(tableIdentifier);\n+\t\toperands.add(partitionSpec);\n+\t\treturn operands;\n+\t}\n+\n+\t@Override\n+\tpublic void unparse(SqlWriter writer, int leftPrec, int rightPrec) {\n+\t\twriter.keyword(\"SHOW PARTITIONS\");\n+\t\ttableIdentifier.unparse(writer, leftPrec, rightPrec);\n+\t\tSqlNodeList partitionSpec = getPartitionSpec();\n+\t\tif (partitionSpec != null && partitionSpec.size() > 0) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "16cd7dffbff67fd8aa060e9dcfde4af700db0839"}, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjEyMzc5Nw==", "bodyText": "Don't we need the table identifier and partition spec here?", "url": "https://github.com/apache/flink/pull/13017#discussion_r466123797", "createdAt": "2020-08-06T03:28:39Z", "author": {"login": "lirui-apache"}, "path": "flink-table/flink-sql-client/src/main/java/org/apache/flink/table/client/cli/CliClient.java", "diffHunk": "@@ -549,6 +552,22 @@ private void callShowModules() {\n \t\tterminal.flush();\n \t}\n \n+\tprivate void callShowPartitions() {\n+\t\tfinal List<String> partitions;\n+\t\ttry {\n+\t\t\tpartitions = getShowResult(\"PARTITIONS\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2f2a2d5d7ef165e0c7ce068535bc79e90aae9cca"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjEyNzA2Mw==", "bodyText": "Why not just call getDDLOpExecuteErrorMsg?", "url": "https://github.com/apache/flink/pull/13017#discussion_r466127063", "createdAt": "2020-08-06T03:41:25Z", "author": {"login": "lirui-apache"}, "path": "flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/internal/TableEnvironmentImpl.java", "diffHunk": "@@ -1021,6 +1022,28 @@ private TableResult executeOperation(Operation operation) {\n \t\t\treturn buildShowResult(\"function name\", listFunctions());\n \t\t} else if (operation instanceof ShowViewsOperation) {\n \t\t\treturn buildShowResult(\"view name\", listViews());\n+\t\t} else if (operation instanceof ShowPartitionsOperation) {\n+\t\t\tString exMsg = getDQLOpExecuteErrorMsg(operation.asSummaryString());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2f2a2d5d7ef165e0c7ce068535bc79e90aae9cca"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjEzMDE3Mw==", "bodyText": "We already have a TestItem::validSql method that takes SQL dialect as a parameter. Can you reuse that?", "url": "https://github.com/apache/flink/pull/13017#discussion_r466130173", "createdAt": "2020-08-06T03:53:53Z", "author": {"login": "lirui-apache"}, "path": "flink-table/flink-sql-client/src/test/java/org/apache/flink/table/client/cli/SqlCommandParserTest.java", "diffHunk": "@@ -389,6 +409,15 @@ public static TestItem validSql(\n \t\t\treturn testItem;\n \t\t}\n \n+\t\tpublic static TestItem validSql(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2f2a2d5d7ef165e0c7ce068535bc79e90aae9cca"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjEzMDk0Mg==", "bodyText": "I don't think this makes sense, unless TestItem::invalidSql also supports HIVE dialect.", "url": "https://github.com/apache/flink/pull/13017#discussion_r466130942", "createdAt": "2020-08-06T03:56:50Z", "author": {"login": "lirui-apache"}, "path": "flink-table/flink-sql-client/src/test/java/org/apache/flink/table/client/cli/SqlCommandParserTest.java", "diffHunk": "@@ -299,6 +299,21 @@ public void testCommands() throws Exception {\n \t\t}\n \t}\n \n+\t@Test\n+\tpublic void testHiveCommands() throws Exception {\n+\t\tSqlParserHelper helper = new SqlParserHelper(SqlDialect.HIVE);\n+\t\tparser = helper.getSqlParser();\n+\t\tList<TestItem> testItems = Arrays.asList(\n+\t\t\t// show partitions\n+\t\t\tTestItem.invalidSql(\"SHOW PARTITIONS \", SqlExecutionException.class, \"Encountered \\\"<EOF>\\\"\"),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2f2a2d5d7ef165e0c7ce068535bc79e90aae9cca"}, "originalPosition": 10}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "898931cb7d94e490f094504c8d8f544d123ad0e5", "author": {"user": {"login": "SteNicholas", "name": "SteNicholas"}}, "url": "https://github.com/apache/flink/commit/898931cb7d94e490f094504c8d8f544d123ad0e5", "committedDate": "2020-08-06T08:11:14Z", "message": "[FLINK-18258][hive] Implement SHOW PARTITIONS for Hive dialect"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYyNDMyNzAx", "url": "https://github.com/apache/flink/pull/13017#pullrequestreview-462432701", "createdAt": "2020-08-06T11:29:10Z", "commit": {"oid": "898931cb7d94e490f094504c8d8f544d123ad0e5"}, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQxMToyOToxMFrOG8virA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQxMTo1NzoxMlrOG8wW3A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM0NjY2OA==", "bodyText": "Also add a test case where the partition spec only contains country", "url": "https://github.com/apache/flink/pull/13017#discussion_r466346668", "createdAt": "2020-08-06T11:29:10Z", "author": {"login": "lirui-apache"}, "path": "flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveDialectITCase.java", "diffHunk": "@@ -465,6 +465,49 @@ public void testAddDropPartitions() throws Exception {\n \t\tassertEquals(1, hiveCatalog.listPartitions(tablePath).size());\n \t}\n \n+\t@Test\n+\tpublic void testShowPartitions() throws Exception {\n+\t\ttableEnv.executeSql(\"create table tbl (x int,y binary) partitioned by (dt date,country string)\");\n+\t\ttableEnv.executeSql(\"alter table tbl add partition (dt='2020-04-30',country='china') partition (dt='2020-04-30',country='us')\");\n+\n+\t\tObjectPath tablePath = new ObjectPath(\"default\", \"tbl\");\n+\t\tassertEquals(2, hiveCatalog.listPartitions(tablePath).size());\n+\n+\t\tList<Row> partitions = Lists.newArrayList(tableEnv.executeSql(\"show partitions tbl\").collect());\n+\t\tassertEquals(2, partitions.size());\n+\t\tassertTrue(partitions.toString().contains(\"dt=2020-04-30/country=china\"));\n+\t\tassertTrue(partitions.toString().contains(\"dt=2020-04-30/country=us\"));\n+\t\tpartitions = Lists.newArrayList(tableEnv.executeSql(\"show partitions tbl partition (dt='2020-04-30')\").collect());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "898931cb7d94e490f094504c8d8f544d123ad0e5"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM1MTI0MA==", "bodyText": "Does a DATE column accept values like '2020-04-30 01:02:03'?", "url": "https://github.com/apache/flink/pull/13017#discussion_r466351240", "createdAt": "2020-08-06T11:38:41Z", "author": {"login": "lirui-apache"}, "path": "flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveDialectITCase.java", "diffHunk": "@@ -465,6 +465,49 @@ public void testAddDropPartitions() throws Exception {\n \t\tassertEquals(1, hiveCatalog.listPartitions(tablePath).size());\n \t}\n \n+\t@Test\n+\tpublic void testShowPartitions() throws Exception {\n+\t\ttableEnv.executeSql(\"create table tbl (x int,y binary) partitioned by (dt date,country string)\");\n+\t\ttableEnv.executeSql(\"alter table tbl add partition (dt='2020-04-30',country='china') partition (dt='2020-04-30',country='us')\");\n+\n+\t\tObjectPath tablePath = new ObjectPath(\"default\", \"tbl\");\n+\t\tassertEquals(2, hiveCatalog.listPartitions(tablePath).size());\n+\n+\t\tList<Row> partitions = Lists.newArrayList(tableEnv.executeSql(\"show partitions tbl\").collect());\n+\t\tassertEquals(2, partitions.size());\n+\t\tassertTrue(partitions.toString().contains(\"dt=2020-04-30/country=china\"));\n+\t\tassertTrue(partitions.toString().contains(\"dt=2020-04-30/country=us\"));\n+\t\tpartitions = Lists.newArrayList(tableEnv.executeSql(\"show partitions tbl partition (dt='2020-04-30')\").collect());\n+\t\tassertEquals(2, partitions.size());\n+\t\tassertTrue(partitions.toString().contains(\"dt=2020-04-30/country=china\"));\n+\t\tassertTrue(partitions.toString().contains(\"dt=2020-04-30/country=us\"));\n+\t\tpartitions = Lists.newArrayList(tableEnv.executeSql(\"show partitions tbl partition (dt='2020-04-30',country='china')\").collect());\n+\t\tassertEquals(1, partitions.size());\n+\t\tassertTrue(partitions.toString().contains(\"dt=2020-04-30/country=china\"));\n+\n+\t\ttableEnv.executeSql(\"alter table tbl drop partition (dt='2020-04-30',country='china'),partition (dt='2020-04-30',country='us')\");\n+\t\tassertEquals(0, hiveCatalog.listPartitions(tablePath).size());\n+\n+\t\ttableEnv.executeSql(\"alter table tbl add partition (dt='2020-04-30 01:02:03',country='china') partition (dt='2020-04-30 04:05:06',country='us')\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "898931cb7d94e490f094504c8d8f544d123ad0e5"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM1MjE0Ng==", "bodyText": "I don't think this is necessary. We're not testing add/drop partitions here.", "url": "https://github.com/apache/flink/pull/13017#discussion_r466352146", "createdAt": "2020-08-06T11:40:31Z", "author": {"login": "lirui-apache"}, "path": "flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveDialectITCase.java", "diffHunk": "@@ -465,6 +465,49 @@ public void testAddDropPartitions() throws Exception {\n \t\tassertEquals(1, hiveCatalog.listPartitions(tablePath).size());\n \t}\n \n+\t@Test\n+\tpublic void testShowPartitions() throws Exception {\n+\t\ttableEnv.executeSql(\"create table tbl (x int,y binary) partitioned by (dt date,country string)\");\n+\t\ttableEnv.executeSql(\"alter table tbl add partition (dt='2020-04-30',country='china') partition (dt='2020-04-30',country='us')\");\n+\n+\t\tObjectPath tablePath = new ObjectPath(\"default\", \"tbl\");\n+\t\tassertEquals(2, hiveCatalog.listPartitions(tablePath).size());\n+\n+\t\tList<Row> partitions = Lists.newArrayList(tableEnv.executeSql(\"show partitions tbl\").collect());\n+\t\tassertEquals(2, partitions.size());\n+\t\tassertTrue(partitions.toString().contains(\"dt=2020-04-30/country=china\"));\n+\t\tassertTrue(partitions.toString().contains(\"dt=2020-04-30/country=us\"));\n+\t\tpartitions = Lists.newArrayList(tableEnv.executeSql(\"show partitions tbl partition (dt='2020-04-30')\").collect());\n+\t\tassertEquals(2, partitions.size());\n+\t\tassertTrue(partitions.toString().contains(\"dt=2020-04-30/country=china\"));\n+\t\tassertTrue(partitions.toString().contains(\"dt=2020-04-30/country=us\"));\n+\t\tpartitions = Lists.newArrayList(tableEnv.executeSql(\"show partitions tbl partition (dt='2020-04-30',country='china')\").collect());\n+\t\tassertEquals(1, partitions.size());\n+\t\tassertTrue(partitions.toString().contains(\"dt=2020-04-30/country=china\"));\n+\n+\t\ttableEnv.executeSql(\"alter table tbl drop partition (dt='2020-04-30',country='china'),partition (dt='2020-04-30',country='us')\");\n+\t\tassertEquals(0, hiveCatalog.listPartitions(tablePath).size());\n+\n+\t\ttableEnv.executeSql(\"alter table tbl add partition (dt='2020-04-30 01:02:03',country='china') partition (dt='2020-04-30 04:05:06',country='us')\");\n+\n+\t\tpartitions = Lists.newArrayList(tableEnv.executeSql(\"show partitions tbl\").collect());\n+\t\tassertEquals(2, partitions.size());\n+\t\tassertTrue(partitions.toString().contains(\"dt=2020-04-30 01:02:03/country=china\"));\n+\t\tassertTrue(partitions.toString().contains(\"dt=2020-04-30 04:05:06/country=us\"));\n+\t\tpartitions = Lists.newArrayList(tableEnv.executeSql(\"show partitions tbl partition (dt='2020-04-30 01:02:03')\").collect());\n+\t\tassertEquals(1, partitions.size());\n+\t\tassertTrue(partitions.toString().contains(\"dt=2020-04-30 01:02:03/country=china\"));\n+\t\tpartitions = Lists.newArrayList(tableEnv.executeSql(\"show partitions tbl partition (dt='2020-04-30 04:05:06')\").collect());\n+\t\tassertEquals(1, partitions.size());\n+\t\tassertTrue(partitions.toString().contains(\"dt=2020-04-30 04:05:06/country=us\"));\n+\t\tpartitions = Lists.newArrayList(tableEnv.executeSql(\"show partitions tbl partition (dt='2020-04-30 01:02:03',country='china')\").collect());\n+\t\tassertEquals(1, partitions.size());\n+\t\tassertTrue(partitions.toString().contains(\"dt=2020-04-30 01:02:03/country=china\"));\n+\n+\t\ttableEnv.executeSql(\"alter table tbl drop partition (dt='2020-04-30 01:02:03',country='china'),partition (dt='2020-04-30 04:05:06',country='us')\");\n+\t\tassertEquals(1, hiveCatalog.listPartitions(tablePath).size());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "898931cb7d94e490f094504c8d8f544d123ad0e5"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM1NzM0Ng==", "bodyText": "I think cmdCall.operands[0] contains the whole SQL statement, no?", "url": "https://github.com/apache/flink/pull/13017#discussion_r466357346", "createdAt": "2020-08-06T11:51:47Z", "author": {"login": "lirui-apache"}, "path": "flink-table/flink-sql-client/src/main/java/org/apache/flink/table/client/cli/CliClient.java", "diffHunk": "@@ -532,6 +535,14 @@ private void callShowFunctions() {\n \t\t\t\t.collect(Collectors.toList());\n \t}\n \n+\tprivate List<String> getShowResult(String objectToShow, SqlCommandCall cmdCall) {\n+\t\tTableResult tableResult = executor.executeSql(sessionId, \"SHOW \" + objectToShow + \" \" + cmdCall.operands[0]);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "898931cb7d94e490f094504c8d8f544d123ad0e5"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM2MDAyOA==", "bodyText": "Do we still need these?", "url": "https://github.com/apache/flink/pull/13017#discussion_r466360028", "createdAt": "2020-08-06T11:57:12Z", "author": {"login": "lirui-apache"}, "path": "flink-table/flink-sql-client/src/test/java/org/apache/flink/table/client/cli/SqlCommandParserTest.java", "diffHunk": "@@ -299,6 +300,20 @@ public void testCommands() throws Exception {\n \t\t}\n \t}\n \n+\t@Test\n+\tpublic void testHiveCommands() throws Exception {\n+\t\tSqlParserHelper helper = new SqlParserHelper(SqlDialect.HIVE);\n+\t\tparser = helper.getSqlParser();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "898931cb7d94e490f094504c8d8f544d123ad0e5"}, "originalPosition": 15}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "79c5ec9370376b93a352d9bf7c67f904aee0c6ed", "author": {"user": {"login": "SteNicholas", "name": "SteNicholas"}}, "url": "https://github.com/apache/flink/commit/79c5ec9370376b93a352d9bf7c67f904aee0c6ed", "committedDate": "2020-08-11T07:15:46Z", "message": "[FLINK-18258][hive] Implement SHOW PARTITIONS for Hive dialect"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "31623f278ff1fab58be814e077669f032711a82b", "author": {"user": {"login": "SteNicholas", "name": "SteNicholas"}}, "url": "https://github.com/apache/flink/commit/31623f278ff1fab58be814e077669f032711a82b", "committedDate": "2020-08-11T13:10:06Z", "message": "[FLINK-18258][hive] Implement SHOW PARTITIONS for Hive dialect"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "60db23d3f621b7817e97750d7bddb86ea1c35b4c", "author": {"user": {"login": "SteNicholas", "name": "SteNicholas"}}, "url": "https://github.com/apache/flink/commit/60db23d3f621b7817e97750d7bddb86ea1c35b4c", "committedDate": "2020-08-12T07:44:54Z", "message": "[FLINK-18258][hive] Implement SHOW PARTITIONS for Hive dialect"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY1NjkzMjU2", "url": "https://github.com/apache/flink/pull/13017#pullrequestreview-465693256", "createdAt": "2020-08-12T08:21:40Z", "commit": {"oid": "60db23d3f621b7817e97750d7bddb86ea1c35b4c"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY2Mzk4OTUz", "url": "https://github.com/apache/flink/pull/13017#pullrequestreview-466398953", "createdAt": "2020-08-13T02:12:49Z", "commit": {"oid": "60db23d3f621b7817e97750d7bddb86ea1c35b4c"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4850, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}