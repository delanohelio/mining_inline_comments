{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDY2MTY3ODY3", "number": 13119, "title": "[FLINK-18885][python] Add partitioning interfaces for Python DataStre\u2026", "bodyText": "What is the purpose of the change\nAdd partitioning interfaces for Python DataStream API so that users can configure the partitioning strategy of DataStream.\nBrief change log\n\nAdd union()/project()/broadcast()/rebalance()/rescale()/shuffle() interfaces for Python DataStream API.\n\nVerifying this change\nThis change has test cases covered in test_data_stream.py\nDoes this pull request potentially affect one of the following parts:\n\nDependencies (does it add or upgrade a dependency): ( no)\nThe public API, i.e., is any changed class annotated with @Public(Evolving): (/ no)\nThe serializers: (no)\nThe runtime per-record code paths (performance sensitive): (no)\nAnything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (no)\nThe S3 file system connector: (no)\n\nDocumentation\n\nDoes this pull request introduce a new feature? (no)\nIf yes, how is the feature documented? ( not documented)", "createdAt": "2020-08-11T15:11:05Z", "url": "https://github.com/apache/flink/pull/13119", "merged": true, "mergeCommit": {"oid": "82b3669d277b145a06bf818432efbd73e5423246"}, "closed": true, "closedAt": "2020-08-12T07:30:49Z", "author": {"login": "shuiqiangchen"}, "timelineItems": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABc952hMABqjM2NDQxNDQzMzg=", "endCursor": "Y3Vyc29yOnYyOpPPAAABc-EuJsgFqTQ2NTYwMzk5Mw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "373206017f34fae205f58bab38290f9bcb7ba361", "author": {"user": {"login": "shuiqiangchen", "name": "Shuiqiang Chen"}}, "url": "https://github.com/apache/flink/commit/373206017f34fae205f58bab38290f9bcb7ba361", "committedDate": "2020-08-11T15:06:13Z", "message": "[FLINK-18885][python] Add partitioning interfaces for Python DataStream API."}, "afterCommit": {"oid": "9b7c269c1f59721e9a2f9accea9fd3d30e18b65a", "author": {"user": {"login": "shuiqiangchen", "name": "Shuiqiang Chen"}}, "url": "https://github.com/apache/flink/commit/9b7c269c1f59721e9a2f9accea9fd3d30e18b65a", "committedDate": "2020-08-11T16:58:51Z", "message": "[FLINK-18885][python] Add partitioning interfaces for Python DataStream API."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ec6e531e07a441ccb328f735cc40d618087798c4", "author": {"user": {"login": "shuiqiangchen", "name": "Shuiqiang Chen"}}, "url": "https://github.com/apache/flink/commit/ec6e531e07a441ccb328f735cc40d618087798c4", "committedDate": "2020-08-12T02:15:53Z", "message": "[FLINK-18885][python] Add partitioning interfaces for Python DataStream API."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "9b7c269c1f59721e9a2f9accea9fd3d30e18b65a", "author": {"user": {"login": "shuiqiangchen", "name": "Shuiqiang Chen"}}, "url": "https://github.com/apache/flink/commit/9b7c269c1f59721e9a2f9accea9fd3d30e18b65a", "committedDate": "2020-08-11T16:58:51Z", "message": "[FLINK-18885][python] Add partitioning interfaces for Python DataStream API."}, "afterCommit": {"oid": "ec6e531e07a441ccb328f735cc40d618087798c4", "author": {"user": {"login": "shuiqiangchen", "name": "Shuiqiang Chen"}}, "url": "https://github.com/apache/flink/commit/ec6e531e07a441ccb328f735cc40d618087798c4", "committedDate": "2020-08-12T02:15:53Z", "message": "[FLINK-18885][python] Add partitioning interfaces for Python DataStream API."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY1NTUzNDQ2", "url": "https://github.com/apache/flink/pull/13119#pullrequestreview-465553446", "createdAt": "2020-08-12T02:40:53Z", "commit": {"oid": "9b7c269c1f59721e9a2f9accea9fd3d30e18b65a"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQwMjo0MTowOVrOG_Px4Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQwMzowMDozMlrOG_QFOA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk3MjAwMQ==", "bodyText": "Cannot override partitioning for KeyedStream.", "url": "https://github.com/apache/flink/pull/13119#discussion_r468972001", "createdAt": "2020-08-12T02:41:09Z", "author": {"login": "hequn8128"}, "path": "flink-python/pyflink/datastream/data_stream.py", "diffHunk": "@@ -563,6 +654,27 @@ def key_by(self, key_selector: Union[Callable, KeySelector],\n                key_type_info: TypeInformation = None) -> 'KeyedStream':\n         return self._origin_stream.key_by(key_selector, key_type_info)\n \n+    def union(self, *streams) -> 'DataStream':\n+        return self._values().union(*streams)\n+\n+    def shuffle(self) -> 'DataStream':\n+        return self._origin_stream.shuffle()\n+\n+    def project(self, *field_indexes) -> 'DataStream':\n+        return self._origin_stream.project(*field_indexes)\n+\n+    def rescale(self) -> 'DataStream':\n+        return self._origin_stream.rescale()\n+\n+    def rebalance(self) -> 'DataStream':\n+        return self._origin_stream.rebalance()\n+\n+    def forward(self) -> 'DataStream':\n+        return self._origin_stream.forward()\n+\n+    def broadcast(self) -> 'DataStream':\n+        return self._origin_stream.broadcast()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ec6e531e07a441ccb328f735cc40d618087798c4"}, "originalPosition": 121}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk3MjI3Mg==", "bodyText": "project on Keyedstream should preserve the hash partitioning, i.e., we can't simply project on its origin_stream.", "url": "https://github.com/apache/flink/pull/13119#discussion_r468972272", "createdAt": "2020-08-12T02:42:15Z", "author": {"login": "hequn8128"}, "path": "flink-python/pyflink/datastream/data_stream.py", "diffHunk": "@@ -563,6 +654,27 @@ def key_by(self, key_selector: Union[Callable, KeySelector],\n                key_type_info: TypeInformation = None) -> 'KeyedStream':\n         return self._origin_stream.key_by(key_selector, key_type_info)\n \n+    def union(self, *streams) -> 'DataStream':\n+        return self._values().union(*streams)\n+\n+    def shuffle(self) -> 'DataStream':\n+        return self._origin_stream.shuffle()\n+\n+    def project(self, *field_indexes) -> 'DataStream':", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ec6e531e07a441ccb328f735cc40d618087798c4"}, "originalPosition": 108}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk3NjcyMQ==", "bodyText": "I find we have bugs when handling parallelism for the keyby. Suppose we perform x -> key_by-> map, the internal java stream graph would be x -> map1 -> keyBy -> map2 -> map3. The parallelism of map1 should equal to x and map2 should equal to map3.", "url": "https://github.com/apache/flink/pull/13119#discussion_r468976721", "createdAt": "2020-08-12T02:59:40Z", "author": {"login": "hequn8128"}, "path": "flink-python/pyflink/datastream/data_stream.py", "diffHunk": "@@ -563,6 +654,27 @@ def key_by(self, key_selector: Union[Callable, KeySelector],\n                key_type_info: TypeInformation = None) -> 'KeyedStream':\n         return self._origin_stream.key_by(key_selector, key_type_info)\n \n+    def union(self, *streams) -> 'DataStream':\n+        return self._values().union(*streams)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ec6e531e07a441ccb328f735cc40d618087798c4"}, "originalPosition": 103}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk3Njk1Mg==", "bodyText": "These tests are not correct. Exceptions should be thrown since we cannot override partitioning for KeyedStream.", "url": "https://github.com/apache/flink/pull/13119#discussion_r468976952", "createdAt": "2020-08-12T03:00:32Z", "author": {"login": "hequn8128"}, "path": "flink-python/pyflink/datastream/tests/test_data_stream.py", "diffHunk": "@@ -242,6 +242,68 @@ def test_print_with_align_output(self):\n         self.assertEqual(3, len(plan['nodes']))\n         self.assertEqual(\"Sink: Print to Std. Out\", plan['nodes'][2]['type'])\n \n+    def test_union_stream(self):\n+        ds_1 = self.env.from_collection([1, 2, 3])\n+        ds_2 = self.env.from_collection([4, 5, 6])\n+        ds_3 = self.env.from_collection([7, 8, 9])\n+\n+        united_stream = ds_3.union(ds_1, ds_2)\n+\n+        united_stream.map(lambda x: x + 1).add_sink(self.test_sink)\n+        exec_plan = eval(self.env.get_execution_plan())\n+        source_ids = []\n+        union_node_pre_ids = []\n+        for node in exec_plan['nodes']:\n+            if node['pact'] == 'Data Source':\n+                source_ids.append(node['id'])\n+            if node['pact'] == 'Operator':\n+                for pre in node['predecessors']:\n+                    union_node_pre_ids.append(pre['id'])\n+\n+        source_ids.sort()\n+        union_node_pre_ids.sort()\n+        self.assertEqual(source_ids, union_node_pre_ids)\n+\n+    def test_project(self):\n+        ds = self.env.from_collection([[1, 2, 3, 4], [5, 6, 7, 8]],\n+                                      type_info=Types.TUPLE(\n+                                          [Types.INT(), Types.INT(), Types.INT(), Types.INT()]))\n+        ds.project(1, 3).map(lambda x: (x[0], x[1] + 1)).add_sink(self.test_sink)\n+        exec_plan = eval(self.env.get_execution_plan())\n+        self.assertEqual(exec_plan['nodes'][1]['type'], 'Projection')\n+\n+    def test_broadcast(self):\n+        ds_1 = self.env.from_collection([1, 2, 3])\n+        ds_1.broadcast().map(lambda x: x + 1).set_parallelism(3).add_sink(self.test_sink)\n+        exec_plan = eval(self.env.get_execution_plan())\n+        broadcast_node = exec_plan['nodes'][1]\n+        pre_ship_strategy = broadcast_node['predecessors'][0]['ship_strategy']\n+        self.assertEqual(pre_ship_strategy, 'BROADCAST')\n+\n+    def test_rebalance(self):\n+        ds_1 = self.env.from_collection([1, 2, 3])\n+        ds_1.rebalance().map(lambda x: x + 1).set_parallelism(3).add_sink(self.test_sink)\n+        exec_plan = eval(self.env.get_execution_plan())\n+        rebalance_node = exec_plan['nodes'][1]\n+        pre_ship_strategy = rebalance_node['predecessors'][0]['ship_strategy']\n+        self.assertEqual(pre_ship_strategy, 'REBALANCE')\n+\n+    def test_rescale(self):\n+        ds_1 = self.env.from_collection([1, 2, 3])\n+        ds_1.rescale().map(lambda x: x + 1).set_parallelism(3).add_sink(self.test_sink)\n+        exec_plan = eval(self.env.get_execution_plan())\n+        rescale_node = exec_plan['nodes'][1]\n+        pre_ship_strategy = rescale_node['predecessors'][0]['ship_strategy']\n+        self.assertEqual(pre_ship_strategy, 'RESCALE')\n+\n+    def test_shuffle(self):\n+        ds_1 = self.env.from_collection([1, 2, 3])\n+        ds_1.shuffle().map(lambda x: x + 1).set_parallelism(3).add_sink(self.test_sink)\n+        exec_plan = eval(self.env.get_execution_plan())\n+        shuffle_node = exec_plan['nodes'][1]\n+        pre_ship_strategy = shuffle_node['predecessors'][0]['ship_strategy']\n+        self.assertEqual(pre_ship_strategy, 'SHUFFLE')", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ec6e531e07a441ccb328f735cc40d618087798c4"}, "originalPosition": 64}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ca9d8e144dc513a34f5bf31523f1db79757a09a0", "author": {"user": {"login": "shuiqiangchen", "name": "Shuiqiang Chen"}}, "url": "https://github.com/apache/flink/commit/ca9d8e144dc513a34f5bf31523f1db79757a09a0", "committedDate": "2020-08-12T05:07:35Z", "message": "[FLINK-18885][python] Correct the partitioning of KeyedStream, add test for KeyedStream partitioning."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY1NjAzOTkz", "url": "https://github.com/apache/flink/pull/13119#pullrequestreview-465603993", "createdAt": "2020-08-12T05:39:25Z", "commit": {"oid": "ca9d8e144dc513a34f5bf31523f1db79757a09a0"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4812, "cost": 1, "resetAt": "2021-10-28T16:48:13Z"}}}