{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTA3NDI1Njg1", "number": 13721, "reviewThreads": {"totalCount": 13, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNFQwNDo1MjowM1rOExeCmQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QwOToxMTo1MVrOEyP9eQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwMzA3ODY1OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecUpsertMaterialize.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNFQwNDo1MjowM1rOHnogdA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNFQwNDo1MjowM1rOHnogdA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTMyMDE4MA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             * Stream physical RelNode which materializes an upsert stream where where each data record\n          \n          \n            \n             * Stream physical RelNode which materializes an upsert stream where each record", "url": "https://github.com/apache/flink/pull/13721#discussion_r511320180", "createdAt": "2020-10-24T04:52:03Z", "author": {"login": "leonardBang"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecUpsertMaterialize.scala", "diffHunk": "@@ -0,0 +1,139 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.nodes.physical.stream\n+\n+import org.apache.calcite.plan.{RelOptCluster, RelTraitSet}\n+import org.apache.calcite.rel.`type`.RelDataType\n+import org.apache.calcite.rel.{RelNode, RelWriter, SingleRel}\n+import org.apache.flink.api.dag.Transformation\n+import org.apache.flink.streaming.api.operators.KeyedProcessOperator\n+import org.apache.flink.streaming.api.transformations.OneInputTransformation\n+import org.apache.flink.table.api.config.ExecutionConfigOptions\n+import org.apache.flink.table.data.RowData\n+import org.apache.flink.table.planner.delegation.StreamPlanner\n+import org.apache.flink.table.planner.plan.nodes.exec.{ExecNode, StreamExecNode}\n+import org.apache.flink.table.planner.plan.utils.{AggregateUtil, ChangelogPlanUtils, KeySelectorUtil}\n+import org.apache.flink.table.runtime.operators.bundle.KeyedMapBundleOperator\n+import org.apache.flink.table.runtime.operators.deduplicate.{DeduplicateKeepLastRowFunction, MiniBatchDeduplicateKeepLastRowFunction}\n+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo\n+\n+import java.util\n+\n+import scala.collection.JavaConversions._\n+\n+/**\n+ * Stream physical RelNode which materializes an upsert stream where where each data record", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b03cd2b69573ff31561807574f0d9d9f2af803b3"}, "originalPosition": 41}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwMzA3OTQxOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecUpsertMaterialize.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNFQwNDo1MzozM1rOHnogzQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNFQwNDo1MzozM1rOHnogzQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTMyMDI2OQ==", "bodyText": "Looks like this conflicts with global TTL setting? I understand the motivation, but it may confuse user.", "url": "https://github.com/apache/flink/pull/13721#discussion_r511320269", "createdAt": "2020-10-24T04:53:33Z", "author": {"login": "leonardBang"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecUpsertMaterialize.scala", "diffHunk": "@@ -0,0 +1,139 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.nodes.physical.stream\n+\n+import org.apache.calcite.plan.{RelOptCluster, RelTraitSet}\n+import org.apache.calcite.rel.`type`.RelDataType\n+import org.apache.calcite.rel.{RelNode, RelWriter, SingleRel}\n+import org.apache.flink.api.dag.Transformation\n+import org.apache.flink.streaming.api.operators.KeyedProcessOperator\n+import org.apache.flink.streaming.api.transformations.OneInputTransformation\n+import org.apache.flink.table.api.config.ExecutionConfigOptions\n+import org.apache.flink.table.data.RowData\n+import org.apache.flink.table.planner.delegation.StreamPlanner\n+import org.apache.flink.table.planner.plan.nodes.exec.{ExecNode, StreamExecNode}\n+import org.apache.flink.table.planner.plan.utils.{AggregateUtil, ChangelogPlanUtils, KeySelectorUtil}\n+import org.apache.flink.table.runtime.operators.bundle.KeyedMapBundleOperator\n+import org.apache.flink.table.runtime.operators.deduplicate.{DeduplicateKeepLastRowFunction, MiniBatchDeduplicateKeepLastRowFunction}\n+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo\n+\n+import java.util\n+\n+import scala.collection.JavaConversions._\n+\n+/**\n+ * Stream physical RelNode which materializes an upsert stream where where each data record\n+ * represents an update or delete event. The materialize node will merge upsert/delete records\n+ * and output normalized changelog stream that contains INSERT/UPDATE_BEFORE/UPDATE_AFTER/DELETE\n+ * records.\n+ */\n+class StreamExecUpsertMaterialize(\n+    cluster: RelOptCluster,\n+    traitSet: RelTraitSet,\n+    input: RelNode,\n+    val uniqueKeys: Array[Int])\n+  extends SingleRel(cluster, traitSet, input)\n+  with StreamPhysicalRel\n+  with StreamExecNode[RowData] {\n+\n+  override def requireWatermark: Boolean = false\n+\n+  override def deriveRowType(): RelDataType = getInput.getRowType\n+\n+  override def copy(traitSet: RelTraitSet, inputs: util.List[RelNode]): RelNode = {\n+    new StreamExecUpsertMaterialize(\n+      cluster,\n+      traitSet,\n+      inputs.get(0),\n+      uniqueKeys)\n+  }\n+\n+  override def explainTerms(pw: RelWriter): RelWriter = {\n+    val fieldNames = getRowType.getFieldNames\n+    super.explainTerms(pw)\n+      .item(\"key\", uniqueKeys.map(fieldNames.get).mkString(\", \"))\n+  }\n+\n+  //~ ExecNode methods -----------------------------------------------------------\n+\n+  override def getInputNodes: util.List[ExecNode[StreamPlanner, _]] = {\n+    List(getInput.asInstanceOf[ExecNode[StreamPlanner, _]])\n+  }\n+\n+  override def replaceInputNode(\n+    ordinalInParent: Int,\n+    newInputNode: ExecNode[StreamPlanner, _]): Unit = {\n+    replaceInput(ordinalInParent, newInputNode.asInstanceOf[RelNode])\n+  }\n+\n+  override protected def translateToPlanInternal(\n+      planner: StreamPlanner): Transformation[RowData] = {\n+\n+    val inputTransform = getInputNodes.get(0).translateToPlan(planner)\n+      .asInstanceOf[Transformation[RowData]]\n+\n+    val rowTypeInfo = inputTransform.getOutputType.asInstanceOf[InternalTypeInfo[RowData]]\n+    val generateUpdateBefore = ChangelogPlanUtils.generateUpdateBefore(this)\n+    val tableConfig = planner.getTableConfig\n+    val isMiniBatchEnabled = tableConfig.getConfiguration.getBoolean(\n+      ExecutionConfigOptions.TABLE_EXEC_MINIBATCH_ENABLED)\n+    val operator = if (isMiniBatchEnabled) {\n+      val exeConfig = planner.getExecEnv.getConfig\n+      val rowSerializer = rowTypeInfo.createSerializer(exeConfig)\n+      val processFunction = new MiniBatchDeduplicateKeepLastRowFunction(\n+        rowTypeInfo,\n+        generateUpdateBefore,\n+        true,   // generateInsert\n+        false,  // inputInsertOnly\n+        rowSerializer,\n+        // disable state ttl, the upsert materialize should keep all state to have data integrity\n+        // we can enable state ttl if this is really needed in some cases", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b03cd2b69573ff31561807574f0d9d9f2af803b3"}, "originalPosition": 106}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwMzA4MTc4OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/batch/sql/TableScanTest.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNFQwNDo1NjoyOVrOHnoh3A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNFQwNDo1NjoyOVrOHnoh3A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTMyMDU0MA==", "bodyText": "The test failed, please update the exception message.", "url": "https://github.com/apache/flink/pull/13721#discussion_r511320540", "createdAt": "2020-10-24T04:56:29Z", "author": {"login": "leonardBang"}, "path": "flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/batch/sql/TableScanTest.scala", "diffHunk": "@@ -130,6 +130,28 @@ class TableScanTest extends TableTestBase {\n     util.verifyPlan(\"SELECT * FROM src WHERE a > 1\")\n   }\n \n+  @Test\n+  def testScanOnUpsertSource(): Unit = {\n+    util.addTable(\n+      \"\"\"\n+        |CREATE TABLE src (\n+        |  id STRING,\n+        |  a INT,\n+        |  b DOUBLE,\n+        |  PRIMARY KEY (id) NOT ENFORCED\n+        |) WITH (\n+        |  'connector' = 'values',\n+        |  'bounded' = 'true',\n+        |  'changelog-mode' = 'UA,D'\n+        |)\n+      \"\"\".stripMargin)\n+    thrown.expect(classOf[UnsupportedOperationException])\n+    thrown.expectMessage(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b03cd2b69573ff31561807574f0d9d9f2af803b3"}, "originalPosition": 20}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwMzA4NzQ1OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/stream/sql/join/SemiAntiJoinTest.xml", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNFQwNTowNToyNFrOHnokbw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQwNDowMjozNFrOHoAQGQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTMyMTE5OQ==", "bodyText": "Why the function change for existed test?", "url": "https://github.com/apache/flink/pull/13721#discussion_r511321199", "createdAt": "2020-10-24T05:05:24Z", "author": {"login": "leonardBang"}, "path": "flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/stream/sql/join/SemiAntiJoinTest.xml", "diffHunk": "@@ -307,7 +307,7 @@ Join(joinType=[LeftSemiJoin], where=[$f0], select=[a, b, c], leftInputSpec=[NoUn\n :  +- LegacyTableSourceScan(table=[[default_catalog, default_database, l, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])\n +- Exchange(distribution=[single])\n    +- Calc(select=[IS NOT NULL(m) AS $f0])\n-      +- GroupAggregate(select=[MIN(i) AS m])\n+      +- GroupAggregate(select=[MIN_RETRACT(i) AS m])", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b03cd2b69573ff31561807574f0d9d9f2af803b3"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTcwOTIwOQ==", "bodyText": "This is a bug in previous FlinkRelMdModifiedMonotonicity, the need retraction inference was not correct.", "url": "https://github.com/apache/flink/pull/13721#discussion_r511709209", "createdAt": "2020-10-26T04:02:34Z", "author": {"login": "wuchong"}, "path": "flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/stream/sql/join/SemiAntiJoinTest.xml", "diffHunk": "@@ -307,7 +307,7 @@ Join(joinType=[LeftSemiJoin], where=[$f0], select=[a, b, c], leftInputSpec=[NoUn\n :  +- LegacyTableSourceScan(table=[[default_catalog, default_database, l, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])\n +- Exchange(distribution=[single])\n    +- Calc(select=[IS NOT NULL(m) AS $f0])\n-      +- GroupAggregate(select=[MIN(i) AS m])\n+      +- GroupAggregate(select=[MIN_RETRACT(i) AS m])", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTMyMTE5OQ=="}, "originalCommit": {"oid": "b03cd2b69573ff31561807574f0d9d9f2af803b3"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNTE1ODA4OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/sources/DynamicSourceUtils.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNVQxNDo1MjowOFrOHn6A0g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNVQxNDo1MjowOFrOHn6A0g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTYwNjk5NA==", "bodyText": "which contains only UPDATE_AFTER, no UPDATE_BEFORE. ?", "url": "https://github.com/apache/flink/pull/13721#discussion_r511606994", "createdAt": "2020-10-25T14:52:08Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/sources/DynamicSourceUtils.java", "diffHunk": "@@ -241,30 +241,31 @@ private static void validateScanSource(\n \t\tvalidateWatermarks(sourceIdentifier, schema);\n \n \t\tif (isStreamingMode) {\n-\t\t\tvalidateScanSourceForStreaming(sourceIdentifier, scanSource, changelogMode);\n+\t\t\tvalidateScanSourceForStreaming(sourceIdentifier, schema, scanSource, changelogMode);\n \t\t} else {\n \t\t\tvalidateScanSourceForBatch(sourceIdentifier, changelogMode, provider);\n \t\t}\n \t}\n \n \tprivate static void validateScanSourceForStreaming(\n \t\t\tObjectIdentifier sourceIdentifier,\n+\t\t\tTableSchema schema,\n \t\t\tScanTableSource scanSource,\n \t\t\tChangelogMode changelogMode) {\n \t\t// sanity check for produced ChangelogMode\n \t\tfinal boolean hasUpdateBefore = changelogMode.contains(RowKind.UPDATE_BEFORE);\n \t\tfinal boolean hasUpdateAfter = changelogMode.contains(RowKind.UPDATE_AFTER);\n \t\tif (!hasUpdateBefore && hasUpdateAfter) {\n \t\t\t// only UPDATE_AFTER\n-\t\t\tthrow new TableException(\n-\t\t\t\tString.format(\n-\t\t\t\t\t\"Unsupported source for table '%s'. Currently, a %s doesn't support a changelog which contains \" +\n-\t\t\t\t\t\t\"UPDATE_AFTER but no UPDATE_BEFORE. Please adapt the implementation of class '%s'.\",\n-\t\t\t\t\tsourceIdentifier.asSummaryString(),\n-\t\t\t\t\tScanTableSource.class.getSimpleName(),\n-\t\t\t\t\tscanSource.getClass().getName()\n-\t\t\t\t)\n-\t\t\t);\n+\t\t\tif (!schema.getPrimaryKey().isPresent()) {\n+\t\t\t\tthrow new TableException(\n+\t\t\t\t\tString.format(\n+\t\t\t\t\t\t\"Table '%s' produces a changelog stream contains UPDATE_AFTER no UPDATE_BEFORE, \" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b03cd2b69573ff31561807574f0d9d9f2af803b3"}, "originalPosition": 33}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNTE2NjQ5OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/rules/physical/stream/StreamExecTableSourceScanRule.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNVQxNTowMToxNlrOHn6FMg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQwMzo1MTo0MlrOHoAIWw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTYwODExNA==", "bodyText": "If the primary key fields are not required, we should add a Calc here to reduce output fields.", "url": "https://github.com/apache/flink/pull/13721#discussion_r511608114", "createdAt": "2020-10-25T15:01:16Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/rules/physical/stream/StreamExecTableSourceScanRule.scala", "diffHunk": "@@ -56,12 +66,57 @@ class StreamExecTableSourceScanRule\n   def convert(rel: RelNode): RelNode = {\n     val scan = rel.asInstanceOf[FlinkLogicalTableSourceScan]\n     val traitSet: RelTraitSet = rel.getTraitSet.replace(FlinkConventions.STREAM_PHYSICAL)\n-\n-    new StreamExecTableSourceScan(\n+    val newScan = new StreamExecTableSourceScan(\n       rel.getCluster,\n       traitSet,\n-      scan.getTable.asInstanceOf[TableSourceTable]\n-    )\n+      scan.getTable.asInstanceOf[TableSourceTable])\n+\n+    val table = scan.getTable.asInstanceOf[TableSourceTable]\n+    val tableSource = table.tableSource.asInstanceOf[ScanTableSource]\n+    val changelogMode = tableSource.getChangelogMode\n+    if (changelogMode.contains(RowKind.UPDATE_AFTER) &&\n+        !changelogMode.contains(RowKind.UPDATE_BEFORE)) {\n+      // generate upsert materialize node for upsert source\n+      val primaryKey = table.catalogTable.getSchema.getPrimaryKey\n+      if (!primaryKey.isPresent) {\n+        throw new TableException(s\"Table '${table.tableIdentifier.asSummaryString()}' produces\" +\n+          \" a changelog stream contains UPDATE_AFTER but no UPDATE_BEFORE,\" +\n+          \" this requires to define primary key on the table.\")\n+      }\n+      val keyFields = primaryKey.get().getColumns\n+      val inputFieldNames = newScan.getRowType.getFieldNames\n+      val primaryKeyIndices = getPrimaryKeyIndices(inputFieldNames, keyFields)\n+      val requiredDistribution = FlinkRelDistribution.hash(primaryKeyIndices, requireStrict = true)\n+      val requiredTraitSet = rel.getCluster.getPlanner.emptyTraitSet()\n+        .replace(requiredDistribution)\n+        .replace(FlinkConventions.STREAM_PHYSICAL)\n+      val newInput: RelNode = RelOptRule.convert(newScan, requiredTraitSet)\n+\n+      new StreamExecUpsertMaterialize(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b03cd2b69573ff31561807574f0d9d9f2af803b3"}, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTcwNzIyNw==", "bodyText": "Why we have to add a Calc here? The transformed tree has the same output row type with the original Scan node. If the primary key fields are never used in the following nodes, there should already be a Calc after the Scan node. So I think we shouldn't add a Calc.", "url": "https://github.com/apache/flink/pull/13721#discussion_r511707227", "createdAt": "2020-10-26T03:51:42Z", "author": {"login": "wuchong"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/rules/physical/stream/StreamExecTableSourceScanRule.scala", "diffHunk": "@@ -56,12 +66,57 @@ class StreamExecTableSourceScanRule\n   def convert(rel: RelNode): RelNode = {\n     val scan = rel.asInstanceOf[FlinkLogicalTableSourceScan]\n     val traitSet: RelTraitSet = rel.getTraitSet.replace(FlinkConventions.STREAM_PHYSICAL)\n-\n-    new StreamExecTableSourceScan(\n+    val newScan = new StreamExecTableSourceScan(\n       rel.getCluster,\n       traitSet,\n-      scan.getTable.asInstanceOf[TableSourceTable]\n-    )\n+      scan.getTable.asInstanceOf[TableSourceTable])\n+\n+    val table = scan.getTable.asInstanceOf[TableSourceTable]\n+    val tableSource = table.tableSource.asInstanceOf[ScanTableSource]\n+    val changelogMode = tableSource.getChangelogMode\n+    if (changelogMode.contains(RowKind.UPDATE_AFTER) &&\n+        !changelogMode.contains(RowKind.UPDATE_BEFORE)) {\n+      // generate upsert materialize node for upsert source\n+      val primaryKey = table.catalogTable.getSchema.getPrimaryKey\n+      if (!primaryKey.isPresent) {\n+        throw new TableException(s\"Table '${table.tableIdentifier.asSummaryString()}' produces\" +\n+          \" a changelog stream contains UPDATE_AFTER but no UPDATE_BEFORE,\" +\n+          \" this requires to define primary key on the table.\")\n+      }\n+      val keyFields = primaryKey.get().getColumns\n+      val inputFieldNames = newScan.getRowType.getFieldNames\n+      val primaryKeyIndices = getPrimaryKeyIndices(inputFieldNames, keyFields)\n+      val requiredDistribution = FlinkRelDistribution.hash(primaryKeyIndices, requireStrict = true)\n+      val requiredTraitSet = rel.getCluster.getPlanner.emptyTraitSet()\n+        .replace(requiredDistribution)\n+        .replace(FlinkConventions.STREAM_PHYSICAL)\n+      val newInput: RelNode = RelOptRule.convert(newScan, requiredTraitSet)\n+\n+      new StreamExecUpsertMaterialize(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTYwODExNA=="}, "originalCommit": {"oid": "b03cd2b69573ff31561807574f0d9d9f2af803b3"}, "originalPosition": 66}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNTE3MjQ2OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/stream/sql/TableScanTest.xml", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNVQxNTowNjozNVrOHn6H-w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQwMzo0ODo1OVrOHoAGJQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTYwODgyNw==", "bodyText": "does UpsertMaterialize produce  UB message ?", "url": "https://github.com/apache/flink/pull/13721#discussion_r511608827", "createdAt": "2020-10-25T15:06:35Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/stream/sql/TableScanTest.xml", "diffHunk": "@@ -87,6 +129,39 @@ LogicalProject(a=[$0], b=[$1], c=[+($0, 1)], d=[TO_TIMESTAMP($1)], e=[my_udf($0)\n       <![CDATA[\n Calc(select=[a, b, +(a, 1) AS c, TO_TIMESTAMP(b) AS d, my_udf(a) AS e])\n +- TableSourceScan(table=[[default_catalog, default_database, t1]], fields=[a, b])\n+]]>\n+    </Resource>\n+  </TestCase>\n+  <TestCase name=\"testTemporalJoinOnUpsertSource\">\n+    <Resource name=\"sql\">\n+      <![CDATA[\n+SELECT o.currency, o.amount, r.rate, o.amount * r.rate\n+FROM orders AS o LEFT JOIN rates_history FOR SYSTEM_TIME AS OF o.proctime AS r\n+ON o.currency = r.currency\n+]]>\n+    </Resource>\n+    <Resource name=\"planBefore\">\n+      <![CDATA[\n+LogicalProject(currency=[$1], amount=[$0], rate=[$4], EXPR$3=[*($0, $4)])\n++- LogicalCorrelate(correlation=[$cor0], joinType=[left], requiredColumns=[{1, 2}])\n+   :- LogicalProject(amount=[$0], currency=[$1], proctime=[PROCTIME()])\n+   :  +- LogicalTableScan(table=[[default_catalog, default_database, orders]])\n+   +- LogicalFilter(condition=[=($cor0.currency, $0)])\n+      +- LogicalSnapshot(period=[$cor0.proctime])\n+         +- LogicalTableScan(table=[[default_catalog, default_database, rates_history]])\n+]]>\n+    </Resource>\n+    <Resource name=\"planAfter\">\n+      <![CDATA[\n+Calc(select=[currency, amount, rate, *(amount, rate) AS EXPR$3], changelogMode=[I])\n++- TemporalJoin(joinType=[LeftOuterJoin], where=[AND(=(currency, currency0), __TEMPORAL_JOIN_CONDITION(proctime, __TEMPORAL_JOIN_LEFT_KEY(currency), __TEMPORAL_JOIN_RIGHT_KEY(currency0)))], select=[amount, currency, proctime, currency0, rate], changelogMode=[I])\n+   :- Exchange(distribution=[hash[currency]], changelogMode=[I])\n+   :  +- Calc(select=[amount, currency, PROCTIME() AS proctime], changelogMode=[I])\n+   :     +- TableSourceScan(table=[[default_catalog, default_database, orders]], fields=[amount, currency], changelogMode=[I])\n+   +- Exchange(distribution=[hash[currency]], changelogMode=[I,UB,UA,D])\n+      +- UpsertMaterialize(key=[currency], changelogMode=[I,UB,UA,D])", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b03cd2b69573ff31561807574f0d9d9f2af803b3"}, "originalPosition": 83}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTcwNjY2MQ==", "bodyText": "It depends on whether the following nodes requires UB messages.\nIn this case, the temporal join currently always root requires UB, therefore UpsertMaterialize has to emit `UB.", "url": "https://github.com/apache/flink/pull/13721#discussion_r511706661", "createdAt": "2020-10-26T03:48:59Z", "author": {"login": "wuchong"}, "path": "flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/stream/sql/TableScanTest.xml", "diffHunk": "@@ -87,6 +129,39 @@ LogicalProject(a=[$0], b=[$1], c=[+($0, 1)], d=[TO_TIMESTAMP($1)], e=[my_udf($0)\n       <![CDATA[\n Calc(select=[a, b, +(a, 1) AS c, TO_TIMESTAMP(b) AS d, my_udf(a) AS e])\n +- TableSourceScan(table=[[default_catalog, default_database, t1]], fields=[a, b])\n+]]>\n+    </Resource>\n+  </TestCase>\n+  <TestCase name=\"testTemporalJoinOnUpsertSource\">\n+    <Resource name=\"sql\">\n+      <![CDATA[\n+SELECT o.currency, o.amount, r.rate, o.amount * r.rate\n+FROM orders AS o LEFT JOIN rates_history FOR SYSTEM_TIME AS OF o.proctime AS r\n+ON o.currency = r.currency\n+]]>\n+    </Resource>\n+    <Resource name=\"planBefore\">\n+      <![CDATA[\n+LogicalProject(currency=[$1], amount=[$0], rate=[$4], EXPR$3=[*($0, $4)])\n++- LogicalCorrelate(correlation=[$cor0], joinType=[left], requiredColumns=[{1, 2}])\n+   :- LogicalProject(amount=[$0], currency=[$1], proctime=[PROCTIME()])\n+   :  +- LogicalTableScan(table=[[default_catalog, default_database, orders]])\n+   +- LogicalFilter(condition=[=($cor0.currency, $0)])\n+      +- LogicalSnapshot(period=[$cor0.proctime])\n+         +- LogicalTableScan(table=[[default_catalog, default_database, rates_history]])\n+]]>\n+    </Resource>\n+    <Resource name=\"planAfter\">\n+      <![CDATA[\n+Calc(select=[currency, amount, rate, *(amount, rate) AS EXPR$3], changelogMode=[I])\n++- TemporalJoin(joinType=[LeftOuterJoin], where=[AND(=(currency, currency0), __TEMPORAL_JOIN_CONDITION(proctime, __TEMPORAL_JOIN_LEFT_KEY(currency), __TEMPORAL_JOIN_RIGHT_KEY(currency0)))], select=[amount, currency, proctime, currency0, rate], changelogMode=[I])\n+   :- Exchange(distribution=[hash[currency]], changelogMode=[I])\n+   :  +- Calc(select=[amount, currency, PROCTIME() AS proctime], changelogMode=[I])\n+   :     +- TableSourceScan(table=[[default_catalog, default_database, orders]], fields=[amount, currency], changelogMode=[I])\n+   +- Exchange(distribution=[hash[currency]], changelogMode=[I,UB,UA,D])\n+      +- UpsertMaterialize(key=[currency], changelogMode=[I,UB,UA,D])", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTYwODgyNw=="}, "originalCommit": {"oid": "b03cd2b69573ff31561807574f0d9d9f2af803b3"}, "originalPosition": 83}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNTE3NTM5OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdColumnUniqueness.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNVQxNTowOToyMlrOHn6Jbg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQwMzo1MjoyM1rOHoAI2Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTYwOTE5OA==", "bodyText": "is the values of rel.uniqueKeys in order ?", "url": "https://github.com/apache/flink/pull/13721#discussion_r511609198", "createdAt": "2020-10-25T15:09:22Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdColumnUniqueness.scala", "diffHunk": "@@ -304,6 +304,14 @@ class FlinkRelMdColumnUniqueness private extends MetadataHandler[BuiltInMetadata\n     columns != null && util.Arrays.equals(columns.toArray, rel.getUniqueKeys)\n   }\n \n+  def areColumnsUnique(\n+      rel: StreamExecUpsertMaterialize,\n+      mq: RelMetadataQuery,\n+      columns: ImmutableBitSet,\n+      ignoreNulls: Boolean): JBoolean = {\n+    columns != null && util.Arrays.equals(columns.toArray, rel.uniqueKeys)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b03cd2b69573ff31561807574f0d9d9f2af803b3"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTcwNzM1Mw==", "bodyText": "Good point!", "url": "https://github.com/apache/flink/pull/13721#discussion_r511707353", "createdAt": "2020-10-26T03:52:23Z", "author": {"login": "wuchong"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdColumnUniqueness.scala", "diffHunk": "@@ -304,6 +304,14 @@ class FlinkRelMdColumnUniqueness private extends MetadataHandler[BuiltInMetadata\n     columns != null && util.Arrays.equals(columns.toArray, rel.getUniqueKeys)\n   }\n \n+  def areColumnsUnique(\n+      rel: StreamExecUpsertMaterialize,\n+      mq: RelMetadataQuery,\n+      columns: ImmutableBitSet,\n+      ignoreNulls: Boolean): JBoolean = {\n+    columns != null && util.Arrays.equals(columns.toArray, rel.uniqueKeys)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTYwOTE5OA=="}, "originalCommit": {"oid": "b03cd2b69573ff31561807574f0d9d9f2af803b3"}, "originalPosition": 9}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIxMTIyOTA5OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdUniqueKeysTest.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QwOTowNDo1MVrOHoxs-Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QwOTowNDo1MVrOHoxs-Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjUxOTQxNw==", "bodyText": "rename to testGetUniqueKeysOnStreamExecChangelogNormalize", "url": "https://github.com/apache/flink/pull/13721#discussion_r512519417", "createdAt": "2020-10-27T09:04:51Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdUniqueKeysTest.scala", "diffHunk": "@@ -155,7 +155,12 @@ class FlinkRelMdUniqueKeysTest extends FlinkRelMdHandlerTestBase {\n   def testGetUniqueKeysOnStreamExecDeduplicate(): Unit = {\n     assertEquals(uniqueKeys(Array(1)), mq.getUniqueKeys(streamDeduplicateFirstRow).toSet)\n     assertEquals(uniqueKeys(Array(1, 2)), mq.getUniqueKeys(streamDeduplicateLastRow).toSet)\n-    assertEquals(uniqueKeys(Array(5)), mq.getUniqueKeys(rowtimeDeduplicate).toSet)\n+    assertEquals(uniqueKeys(Array(1)), mq.getUniqueKeys(rowtimeDeduplicate).toSet)\n+  }\n+\n+  @Test\n+  def testGetUniqueKeysOnStreamExecUpsertMaterialize(): Unit = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "abcd6adcb7094de11bf46b06d9d68d69035293d2"}, "originalPosition": 9}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIxMTIzMTE5OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdHandlerTestBase.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QwOTowNToyNFrOHoxuTA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QwOTowNToyNFrOHoxuTA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjUxOTc1Ng==", "bodyText": "rename to streamChangelogNormalize", "url": "https://github.com/apache/flink/pull/13721#discussion_r512519756", "createdAt": "2020-10-27T09:05:24Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdHandlerTestBase.scala", "diffHunk": "@@ -691,6 +691,18 @@ class FlinkRelMdHandlerTestBase {\n     (calcOfFirstRow, calcOfLastRow)\n   }\n \n+  protected lazy val streamUpsertMaterialize = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "abcd6adcb7094de11bf46b06d9d68d69035293d2"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIxMTIzNTU2OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdModifiedMonotonicityTest.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QwOTowNjoyOFrOHoxw6Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QwOTowNjoyOFrOHoxw6Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjUyMDQyNQ==", "bodyText": "rename to testGetRelMonotonicityOnChangelogNormalize", "url": "https://github.com/apache/flink/pull/13721#discussion_r512520425", "createdAt": "2020-10-27T09:06:28Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdModifiedMonotonicityTest.scala", "diffHunk": "@@ -315,5 +315,30 @@ class FlinkRelMdModifiedMonotonicityTest extends FlinkRelMdHandlerTestBase {\n     assertNull(mq.getRelModifiedMonotonicity(logicalAntiJoinOnUniqueKeys))\n   }\n \n+  @Test\n+  def testGetRelMonotonicityOnDeduplicate(): Unit = {\n+    assertEquals(\n+      new RelModifiedMonotonicity(Array(NOT_MONOTONIC, CONSTANT, NOT_MONOTONIC)),\n+      mq.getRelModifiedMonotonicity(streamDeduplicateFirstRow))\n+\n+    assertEquals(\n+      new RelModifiedMonotonicity(Array(NOT_MONOTONIC, CONSTANT, CONSTANT)),\n+      mq.getRelModifiedMonotonicity(streamDeduplicateLastRow))\n+\n+    assertEquals(\n+      new RelModifiedMonotonicity(Array(\n+        NOT_MONOTONIC, CONSTANT, NOT_MONOTONIC, NOT_MONOTONIC, NOT_MONOTONIC)),\n+      mq.getRelModifiedMonotonicity(rowtimeDeduplicate))\n+  }\n+\n+  @Test\n+  def testGetRelMonotonicityOnUpsertMaterialize(): Unit = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "abcd6adcb7094de11bf46b06d9d68d69035293d2"}, "originalPosition": 21}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIxMTI0MTk1OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdColumnUniquenessTest.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QwOTowNzo1OFrOHox0rQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QwOTowNzo1OFrOHox0rQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjUyMTM4OQ==", "bodyText": "rename to testAreColumnsUniqueCountOnStreamExecChangelogNormalize", "url": "https://github.com/apache/flink/pull/13721#discussion_r512521389", "createdAt": "2020-10-27T09:07:58Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdColumnUniquenessTest.scala", "diffHunk": "@@ -275,6 +275,15 @@ class FlinkRelMdColumnUniquenessTest extends FlinkRelMdHandlerTestBase {\n     assertFalse(mq.areColumnsUnique(streamDeduplicateLastRow, ImmutableBitSet.of(0, 1, 2)))\n   }\n \n+  @Test\n+  def testAreColumnsUniqueCountOnStreamExecUpsertMaterialize(): Unit = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "abcd6adcb7094de11bf46b06d9d68d69035293d2"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIxMTI1NzUzOnYy", "diffSide": "LEFT", "path": "flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/sql/JoinITCase.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QwOToxMTo1MVrOHox-TQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QwOTozMTozM1rOHoywQQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjUyMzg1Mw==", "bodyText": "why this test is removed ?", "url": "https://github.com/apache/flink/pull/13721#discussion_r512523853", "createdAt": "2020-10-27T09:11:51Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/sql/JoinITCase.scala", "diffHunk": "@@ -1140,48 +1140,4 @@ class JoinITCase(state: StateBackendMode) extends StreamingWithStateTestBase(sta\n     val expected = Seq(\"Hi,Hallo\", \"Hello,Hallo Welt\", \"Hello world,Hallo Welt\")\n     assertEquals(expected.sorted, sink.getRetractResults.sorted)\n   }\n-\n-  @Test\n-  def testJoinOnChangelogSource(): Unit = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "abcd6adcb7094de11bf46b06d9d68d69035293d2"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjUzNjY0MQ==", "bodyText": "This has been moved to ChangelogSourceITCase#testRegularJoin to reuse the tests.", "url": "https://github.com/apache/flink/pull/13721#discussion_r512536641", "createdAt": "2020-10-27T09:31:33Z", "author": {"login": "wuchong"}, "path": "flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/sql/JoinITCase.scala", "diffHunk": "@@ -1140,48 +1140,4 @@ class JoinITCase(state: StateBackendMode) extends StreamingWithStateTestBase(sta\n     val expected = Seq(\"Hi,Hallo\", \"Hello,Hallo Welt\", \"Hello world,Hallo Welt\")\n     assertEquals(expected.sorted, sink.getRetractResults.sorted)\n   }\n-\n-  @Test\n-  def testJoinOnChangelogSource(): Unit = {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjUyMzg1Mw=="}, "originalCommit": {"oid": "abcd6adcb7094de11bf46b06d9d68d69035293d2"}, "originalPosition": 6}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 50, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}