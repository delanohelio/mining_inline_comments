{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzkyMzE1ODYy", "number": 11490, "title": "[FLINK-15579][table-planner-blink] UpsertStreamTableSink should work on batch mode", "bodyText": "What is the purpose of the change\nJDBCTableSourceSinkFactory.createStreamTableSink() create JDBCUpsertTableSink. But BatchExecSink can not work with UpsertStreamTableSink.\nBrief change log\nFix BatchExecSink.\nVerifying this change\n\nTests in JDBCUpsertTableSinkITCase\nTests in TableSinkITCase\n\nDoes this pull request potentially affect one of the following parts:\n\nDependencies (does it add or upgrade a dependency): (yes / no)\nThe public API, i.e., is any changed class annotated with @Public(Evolving): (yes / no)\nThe serializers: (yes / no / don't know)\nThe runtime per-record code paths (performance sensitive): (yes / no / don't know)\nAnything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (yes / no / don't know)\nThe S3 file system connector: (yes / no / don't know)\n\nDocumentation\n\nDoes this pull request introduce a new feature? (yes / no)", "createdAt": "2020-03-23T11:06:49Z", "url": "https://github.com/apache/flink/pull/11490", "merged": true, "mergeCommit": {"oid": "d38a010c55ad78f4e421d581ec72a96a79324dfe"}, "closed": true, "closedAt": "2020-03-25T05:09:46Z", "author": {"login": "JingsongLi"}, "timelineItems": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcQcZkAABqjMxNTQ3NDQzNzA=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcQ-DJGgBqjMxNjIyMzUwNzc=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "d93b01c3f96fe9ed8638f7a054ded19e97677d60", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/flink/commit/d93b01c3f96fe9ed8638f7a054ded19e97677d60", "committedDate": "2020-03-23T11:01:43Z", "message": "[FLINK-15579][table-planner-blink] Fix UpsertStreamTableSink support and add tests"}, "afterCommit": {"oid": "d1a5a4372582555948fb210c33efe3699822089e", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/flink/commit/d1a5a4372582555948fb210c33efe3699822089e", "committedDate": "2020-03-23T11:13:52Z", "message": "[FLINK-15579][table-planner-blink] Fix UpsertStreamTableSink support and add tests"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc5NDcxNjcz", "url": "https://github.com/apache/flink/pull/11490#pullrequestreview-379471673", "createdAt": "2020-03-23T14:17:49Z", "commit": {"oid": "d1a5a4372582555948fb210c33efe3699822089e"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yM1QxNDoxNzo1MFrOF6HdXA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yM1QxNDoyOToxNlrOF6H-7w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjQ4MzkzMg==", "bodyText": "Do we need to support RetractStreamTableSink?\nI don't see a requirement on this and the title doesn't describe this.", "url": "https://github.com/apache/flink/pull/11490#discussion_r396483932", "createdAt": "2020-03-23T14:17:50Z", "author": {"login": "wuchong"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecSink.scala", "diffHunk": "@@ -81,13 +82,26 @@ class BatchExecSink[T](\n   override protected def translateToPlanInternal(\n       planner: BatchPlanner): Transformation[Any] = {\n     val resultTransformation = sink match {\n-      case _: RetractStreamTableSink[T] | _: UpsertStreamTableSink[T] =>\n-        throw new TableException(\"RetractStreamTableSink and UpsertStreamTableSink is not\" +\n-          \" supported in Batch environment.\")\n-\n       case streamTableSink: StreamTableSink[T] =>\n-        // we can insert the bounded DataStream into a StreamTableSink\n-        val transformation = translateToTransformation(withChangeFlag = false, planner)\n+        val transformation = streamTableSink match {\n+          case _: RetractStreamTableSink[T] =>\n+            translateToTransformation(withChangeFlag = true, planner)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d1a5a4372582555948fb210c33efe3699822089e"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjQ4NzI0Mw==", "bodyText": "I think this should be always in isAppendOnly and may have a key fields. Some connector can have an optimizaton based on this, e.g. MySQL can use INSERT INTO rather than INSERT .. ON DUPLICATE KEY.", "url": "https://github.com/apache/flink/pull/11490#discussion_r396487243", "createdAt": "2020-03-23T14:22:09Z", "author": {"login": "wuchong"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecSink.scala", "diffHunk": "@@ -81,13 +82,26 @@ class BatchExecSink[T](\n   override protected def translateToPlanInternal(\n       planner: BatchPlanner): Transformation[Any] = {\n     val resultTransformation = sink match {\n-      case _: RetractStreamTableSink[T] | _: UpsertStreamTableSink[T] =>\n-        throw new TableException(\"RetractStreamTableSink and UpsertStreamTableSink is not\" +\n-          \" supported in Batch environment.\")\n-\n       case streamTableSink: StreamTableSink[T] =>\n-        // we can insert the bounded DataStream into a StreamTableSink\n-        val transformation = translateToTransformation(withChangeFlag = false, planner)\n+        val transformation = streamTableSink match {\n+          case _: RetractStreamTableSink[T] =>\n+            translateToTransformation(withChangeFlag = true, planner)\n+\n+          case upsertSink: UpsertStreamTableSink[T] =>\n+            UpdatingPlanChecker.getUniqueKeyForUpsertSink(this, planner, upsertSink) match {\n+              case Some(keys) =>\n+                upsertSink.setIsAppendOnly(false)\n+                upsertSink.setKeyFields(keys)\n+              case None =>\n+                upsertSink.setIsAppendOnly(true)\n+                upsertSink.setKeyFields(null)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d1a5a4372582555948fb210c33efe3699822089e"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjQ4ODI0Mw==", "bodyText": "UpdatingPlanChecker.getUniqueKeyFields is only used by this method. Maybe you can merge it into this method.", "url": "https://github.com/apache/flink/pull/11490#discussion_r396488243", "createdAt": "2020-03-23T14:23:28Z", "author": {"login": "wuchong"}, "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/UpdatingPlanChecker.scala", "diffHunk": "@@ -68,4 +70,18 @@ object UpdatingPlanChecker {\n       }\n     }\n   }\n+\n+  def getUniqueKeyForUpsertSink(\n+      sinkNode: Sink,\n+      planner: PlannerBase,\n+      sink: UpsertStreamTableSink[_]): Option[Array[String]] = {\n+    // extract unique key fields\n+    // Now we pick shortest one to sink\n+    // TODO UpsertStreamTableSink setKeyFields interface should be Array[Array[String]]\n+    val sinkFieldNames = sink.getTableSchema.getFieldNames\n+    UpdatingPlanChecker.getUniqueKeyFields(sinkNode.getInput, planner, sinkFieldNames) match {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d1a5a4372582555948fb210c33efe3699822089e"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjQ5MjUyNw==", "bodyText": "We should avoid creating too many testing TableSource. You can use VALUES instead.\nINSERT INTO USER_RESULT\n  SELECT user_name, score\n  FROM (VALUES (1, 'Bob'), (22, 'Tom'), (42, 'Kim'), (42, 'Kim'), (42, 'Kim'), (1, 'Bob'))\n    AS UserCountTable(score, user_name)", "url": "https://github.com/apache/flink/pull/11490#discussion_r396492527", "createdAt": "2020-03-23T14:29:16Z", "author": {"login": "wuchong"}, "path": "flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCUpsertTableSinkITCase.java", "diffHunk": "@@ -210,4 +228,76 @@ public void testAppend() throws Exception {\n \t\t\t\tRow.of(20, 6, Timestamp.valueOf(\"1970-01-01 00:00:00.02\"))\n \t\t}, DB_URL, OUTPUT_TABLE2, new String[]{\"id\", \"num\", \"ts\"});\n \t}\n+\n+\t@Test\n+\tpublic void testBatchUpsert() throws Exception {\n+\t\tStreamExecutionEnvironment bsEnv = StreamExecutionEnvironment.getExecutionEnvironment();\n+\t\tEnvironmentSettings bsSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();\n+\t\tStreamTableEnvironment bsTableEnv = StreamTableEnvironment.create(bsEnv, bsSettings);\n+\t\tRowTypeInfo rt = (RowTypeInfo) Types.ROW_NAMED(new String[]{\"NAME\", \"SCORE\"}, Types.STRING, Types.LONG);\n+\t\tTable source = bsTableEnv.fromTableSource(new CollectionTableSource(generateRecords(2), rt));\n+\t\tbsTableEnv.registerTable(\"sourceTable\", source);\n+\t\tbsTableEnv.sqlUpdate(\n+\t\t\t\"CREATE TABLE USER_RESULT(\" +\n+\t\t\t\t\"NAME VARCHAR,\" +\n+\t\t\t\t\"SCORE BIGINT\" +\n+\t\t\t\t\") WITH ( \" +\n+\t\t\t\t\"'connector.type' = 'jdbc',\" +\n+\t\t\t\t\"'connector.url'='\" + DB_URL + \"',\" +\n+\t\t\t\t\"'connector.table' = '\" + OUTPUT_TABLE3 + \"'\" +\n+\t\t\t\t\")\");\n+\n+\t\tbsTableEnv.sqlUpdate(\"insert into USER_RESULT SELECT s.NAME, s.SCORE \" +\n+\t\t\t\"FROM sourceTable as s \");\n+\t\tbsTableEnv.execute(\"test\");\n+\n+\t\tcheck(new Row[] {\n+\t\t\tRow.of(\"a0\", 0L),\n+\t\t\tRow.of(\"a1\", 1L)\n+\t\t}, DB_URL, OUTPUT_TABLE3, new String[]{\"NAME\", \"SCORE\"});\n+\t}\n+\n+\tprivate List<Row> generateRecords(int numRecords) {\n+\t\tint arity = 2;\n+\t\tList<Row> res = new ArrayList<>(numRecords);\n+\t\tfor (long i = 0; i < numRecords; i++) {\n+\t\t\tRow row = new Row(arity);\n+\t\t\trow.setField(0, \"a\" + i);\n+\t\t\trow.setField(1, i);\n+\t\t\tres.add(row);\n+\t\t}\n+\t\treturn res;\n+\t}\n+\n+\tprivate static class CollectionTableSource extends InputFormatTableSource<Row> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d1a5a4372582555948fb210c33efe3699822089e"}, "originalPosition": 105}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzgwODE5ODcy", "url": "https://github.com/apache/flink/pull/11490#pullrequestreview-380819872", "createdAt": "2020-03-25T02:22:33Z", "commit": {"oid": "6cb6fbc43e0aaf290e74a101ca38313584d1cfe5"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "38222efd1cde395cc2110d4c799c9227e7f1e12b", "author": {"user": {"login": "nezhazheng", "name": "Shu Li Zheng"}}, "url": "https://github.com/apache/flink/commit/38222efd1cde395cc2110d4c799c9227e7f1e12b", "committedDate": "2020-03-25T02:26:09Z", "message": "[FLINK-15579][table-planner-blink] UpsertStreamTableSink should work on batch mode"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3d3ddb474eace1680157c2ccb826b8e6d8c593d0", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/flink/commit/3d3ddb474eace1680157c2ccb826b8e6d8c593d0", "committedDate": "2020-03-25T02:26:10Z", "message": "[FLINK-15579][table-planner-blink] Fix UpsertStreamTableSink support and add tests"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "6cb6fbc43e0aaf290e74a101ca38313584d1cfe5", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/flink/commit/6cb6fbc43e0aaf290e74a101ca38313584d1cfe5", "committedDate": "2020-03-24T02:53:22Z", "message": "fix"}, "afterCommit": {"oid": "3d3ddb474eace1680157c2ccb826b8e6d8c593d0", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/flink/commit/3d3ddb474eace1680157c2ccb826b8e6d8c593d0", "committedDate": "2020-03-25T02:26:10Z", "message": "[FLINK-15579][table-planner-blink] Fix UpsertStreamTableSink support and add tests"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2456, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}