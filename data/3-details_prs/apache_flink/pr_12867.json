{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDQ3MzI3NDM3", "number": 12867, "title": "[FLINK-18558][streaming] Introduce collect iterator with at least once semantics and exactly once semantics without fault tolerance", "bodyText": "What is the purpose of the change\nCurrently TableResult#collect and DataStreamUtils#collect can only produce results if users explicitly enable checkpoint for infinite streaming jobs. It would be strange to require the users to do so if they just want to take a look at their data.\nThis PR introduces collect iterator with at least once semantics and exactly once semantics without fault tolerance. When calling the collect method, we automatically pick an iterator for the user:\n\nIf the user does not explicitly enable a checkpoint, we use exactly once iterator without fault tolerance. That is to say, the iterator will throw exception once the job restarts.\nIf the user explicitly enables an exactly once checkpoint, we use the current implementation of collect iterator.\nIf the user explicitly enables an at least once checkpoint, we use the at least once iterator. That is to say, the iterator ignores both checkpoint information and job restarts.\n\nBrief change log\n\nRefactor tests for datastream / table collect\nIntroduce collect iterator with at least once semantics and exactly once semantics without fault tolerance\n\nVerifying this change\nThis change is already covered by existing datastream / table collect tests, and this change added tests and can be verified by running those unit test cases.\nDoes this pull request potentially affect one of the following parts:\n\nDependencies (does it add or upgrade a dependency): no\nThe public API, i.e., is any changed class annotated with @Public(Evolving): no\nThe serializers: no\nThe runtime per-record code paths (performance sensitive): no\nAnything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: no\nThe S3 file system connector: no\n\nDocumentation\n\nDoes this pull request introduce a new feature? yes\nIf yes, how is the feature documented? Java doc, documentation", "createdAt": "2020-07-10T09:18:15Z", "url": "https://github.com/apache/flink/pull/12867", "merged": true, "mergeCommit": {"oid": "791e276c8346a49130cb096bafa128d7f1231236"}, "closed": true, "closedAt": "2020-07-23T05:35:35Z", "author": {"login": "tsreaper"}, "timelineItems": {"totalCount": 14, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABc1J2uHgFqTQ0ODc3MjQyMA==", "endCursor": "Y3Vyc29yOnYyOpPPAAABc3oprzgFqTQ1Mzg0NDkyNQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ4NzcyNDIw", "url": "https://github.com/apache/flink/pull/12867#pullrequestreview-448772420", "createdAt": "2020-07-15T09:13:36Z", "commit": {"oid": "c3ac42923fc06b5d0d86c01c814918916ab86790"}, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQwOToxMzozNlrOGx1chA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQxMjowNDozNlrOGx6_Qw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDkwOTA2MA==", "bodyText": "typo: Funtion", "url": "https://github.com/apache/flink/pull/12867#discussion_r454909060", "createdAt": "2020-07-15T09:13:36Z", "author": {"login": "godfreyhe"}, "path": "flink-streaming-java/src/test/java/org/apache/flink/streaming/api/operators/collect/utils/CollectSinkFunctionTestWrapper.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.api.operators.collect.utils;\n+\n+import org.apache.flink.api.common.accumulators.Accumulator;\n+import org.apache.flink.api.common.accumulators.SerializedListAccumulator;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.runtime.io.disk.iomanager.IOManager;\n+import org.apache.flink.runtime.io.disk.iomanager.IOManagerAsync;\n+import org.apache.flink.runtime.memory.MemoryManager;\n+import org.apache.flink.runtime.operators.testutils.MockEnvironment;\n+import org.apache.flink.runtime.operators.testutils.MockEnvironmentBuilder;\n+import org.apache.flink.streaming.api.operators.StreamingRuntimeContext;\n+import org.apache.flink.streaming.api.operators.collect.CollectCoordinationRequest;\n+import org.apache.flink.streaming.api.operators.collect.CollectCoordinationResponse;\n+import org.apache.flink.streaming.api.operators.collect.CollectSinkFunction;\n+import org.apache.flink.streaming.api.operators.collect.CollectSinkOperatorCoordinator;\n+import org.apache.flink.streaming.util.MockStreamingRuntimeContext;\n+\n+import org.junit.Assert;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.concurrent.TimeUnit;\n+\n+/**\n+ * A wrapper class for creating, checkpointing and closing\n+ * {@link org.apache.flink.streaming.api.operators.collect.CollectSinkFunction} for tests.\n+ */\n+public class CollectSinkFunctionTestWrapper<IN> {\n+\n+\tpublic static final String ACCUMULATOR_NAME = \"tableCollectAccumulator\";\n+\n+\tprivate static final int SOCKET_TIMEOUT_MILLIS = 1000;\n+\tprivate static final int FUTURE_TIMEOUT_MILLIS = 10000;\n+\tprivate static final int MAX_RETIRES = 100;\n+\n+\tprivate final TypeSerializer<IN> serializer;\n+\tprivate final int maxBytesPerBatch;\n+\n+\tprivate final IOManager ioManager;\n+\tprivate final StreamingRuntimeContext runtimeContext;\n+\tprivate final MockOperatorEventGateway gateway;\n+\tprivate final CollectSinkOperatorCoordinator coordinator;\n+\tprivate final MockFunctionInitializationContext functionInitializationContext;\n+\n+\tprivate CollectSinkFunction<IN> function;\n+\n+\tpublic CollectSinkFunctionTestWrapper(TypeSerializer<IN> serializer, int maxBytesPerBatch) throws Exception {\n+\t\tthis.serializer = serializer;\n+\t\tthis.maxBytesPerBatch = maxBytesPerBatch;\n+\n+\t\tthis.ioManager = new IOManagerAsync();\n+\t\tMockEnvironment environment = new MockEnvironmentBuilder()\n+\t\t\t.setTaskName(\"mockTask\")\n+\t\t\t.setManagedMemorySize(4 * MemoryManager.DEFAULT_PAGE_SIZE)\n+\t\t\t.setIOManager(ioManager)\n+\t\t\t.build();\n+\t\tthis.runtimeContext = new MockStreamingRuntimeContext(false, 1, 0, environment);\n+\t\tthis.gateway = new MockOperatorEventGateway();\n+\n+\t\tthis.coordinator = new CollectSinkOperatorCoordinator(SOCKET_TIMEOUT_MILLIS);\n+\t\tthis.coordinator.start();\n+\n+\t\tthis.functionInitializationContext = new MockFunctionInitializationContext();\n+\t}\n+\n+\tpublic void closeWrapper() throws Exception {\n+\t\tcoordinator.close();\n+\t\tioManager.close();\n+\t}\n+\n+\tpublic CollectSinkOperatorCoordinator getCoordinator() {\n+\t\treturn coordinator;\n+\t}\n+\n+\tpublic void openFunction() throws Exception {\n+\t\tfunction = new CollectSinkFunction<>(serializer, maxBytesPerBatch, ACCUMULATOR_NAME);\n+\t\tfunction.setRuntimeContext(runtimeContext);\n+\t\tfunction.setOperatorEventGateway(gateway);\n+\t\tfunction.open(new Configuration());\n+\t\tcoordinator.handleEventFromOperator(0, gateway.getNextEvent());\n+\t}\n+\n+\tpublic void openFunctionWithState() throws Exception {\n+\t\tfunctionInitializationContext.getOperatorStateStore().revertToLastSuccessCheckpoint();\n+\t\tfunction = new CollectSinkFunction<>(serializer, maxBytesPerBatch, ACCUMULATOR_NAME);\n+\t\tfunction.setRuntimeContext(runtimeContext);\n+\t\tfunction.setOperatorEventGateway(gateway);\n+\t\tfunction.initializeState(functionInitializationContext);\n+\t\tfunction.open(new Configuration());\n+\t\tcoordinator.handleEventFromOperator(0, gateway.getNextEvent());\n+\t}\n+\n+\tpublic void invoke(IN record) throws Exception {\n+\t\tfunction.invoke(record, null);\n+\t}\n+\n+\tpublic void checkpointFunction(long checkpointId) throws Exception {\n+\t\tfunction.snapshotState(new MockFunctionSnapshotContext(checkpointId));\n+\t\tfunctionInitializationContext.getOperatorStateStore().checkpointBegin(checkpointId);\n+\t}\n+\n+\tpublic void checkpointComplete(long checkpointId) {\n+\t\tfunction.notifyCheckpointComplete(checkpointId);\n+\t\tfunctionInitializationContext.getOperatorStateStore().checkpointSuccess(checkpointId);\n+\t}\n+\n+\tpublic void closeFunctionNormally() throws Exception {\n+\t\t// this is a normal shutdown\n+\t\tfunction.accumulateFinalResults();\n+\t\tfunction.close();\n+\t}\n+\n+\tpublic void closeFuntionAbnormally() throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c3ac42923fc06b5d0d86c01c814918916ab86790"}, "originalPosition": 134}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDkxMTk2NA==", "bodyText": "typo: Accumualtor", "url": "https://github.com/apache/flink/pull/12867#discussion_r454911964", "createdAt": "2020-07-15T09:18:31Z", "author": {"login": "godfreyhe"}, "path": "flink-streaming-java/src/test/java/org/apache/flink/streaming/api/operators/collect/utils/CollectSinkFunctionTestWrapper.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.api.operators.collect.utils;\n+\n+import org.apache.flink.api.common.accumulators.Accumulator;\n+import org.apache.flink.api.common.accumulators.SerializedListAccumulator;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.runtime.io.disk.iomanager.IOManager;\n+import org.apache.flink.runtime.io.disk.iomanager.IOManagerAsync;\n+import org.apache.flink.runtime.memory.MemoryManager;\n+import org.apache.flink.runtime.operators.testutils.MockEnvironment;\n+import org.apache.flink.runtime.operators.testutils.MockEnvironmentBuilder;\n+import org.apache.flink.streaming.api.operators.StreamingRuntimeContext;\n+import org.apache.flink.streaming.api.operators.collect.CollectCoordinationRequest;\n+import org.apache.flink.streaming.api.operators.collect.CollectCoordinationResponse;\n+import org.apache.flink.streaming.api.operators.collect.CollectSinkFunction;\n+import org.apache.flink.streaming.api.operators.collect.CollectSinkOperatorCoordinator;\n+import org.apache.flink.streaming.util.MockStreamingRuntimeContext;\n+\n+import org.junit.Assert;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.concurrent.TimeUnit;\n+\n+/**\n+ * A wrapper class for creating, checkpointing and closing\n+ * {@link org.apache.flink.streaming.api.operators.collect.CollectSinkFunction} for tests.\n+ */\n+public class CollectSinkFunctionTestWrapper<IN> {\n+\n+\tpublic static final String ACCUMULATOR_NAME = \"tableCollectAccumulator\";\n+\n+\tprivate static final int SOCKET_TIMEOUT_MILLIS = 1000;\n+\tprivate static final int FUTURE_TIMEOUT_MILLIS = 10000;\n+\tprivate static final int MAX_RETIRES = 100;\n+\n+\tprivate final TypeSerializer<IN> serializer;\n+\tprivate final int maxBytesPerBatch;\n+\n+\tprivate final IOManager ioManager;\n+\tprivate final StreamingRuntimeContext runtimeContext;\n+\tprivate final MockOperatorEventGateway gateway;\n+\tprivate final CollectSinkOperatorCoordinator coordinator;\n+\tprivate final MockFunctionInitializationContext functionInitializationContext;\n+\n+\tprivate CollectSinkFunction<IN> function;\n+\n+\tpublic CollectSinkFunctionTestWrapper(TypeSerializer<IN> serializer, int maxBytesPerBatch) throws Exception {\n+\t\tthis.serializer = serializer;\n+\t\tthis.maxBytesPerBatch = maxBytesPerBatch;\n+\n+\t\tthis.ioManager = new IOManagerAsync();\n+\t\tMockEnvironment environment = new MockEnvironmentBuilder()\n+\t\t\t.setTaskName(\"mockTask\")\n+\t\t\t.setManagedMemorySize(4 * MemoryManager.DEFAULT_PAGE_SIZE)\n+\t\t\t.setIOManager(ioManager)\n+\t\t\t.build();\n+\t\tthis.runtimeContext = new MockStreamingRuntimeContext(false, 1, 0, environment);\n+\t\tthis.gateway = new MockOperatorEventGateway();\n+\n+\t\tthis.coordinator = new CollectSinkOperatorCoordinator(SOCKET_TIMEOUT_MILLIS);\n+\t\tthis.coordinator.start();\n+\n+\t\tthis.functionInitializationContext = new MockFunctionInitializationContext();\n+\t}\n+\n+\tpublic void closeWrapper() throws Exception {\n+\t\tcoordinator.close();\n+\t\tioManager.close();\n+\t}\n+\n+\tpublic CollectSinkOperatorCoordinator getCoordinator() {\n+\t\treturn coordinator;\n+\t}\n+\n+\tpublic void openFunction() throws Exception {\n+\t\tfunction = new CollectSinkFunction<>(serializer, maxBytesPerBatch, ACCUMULATOR_NAME);\n+\t\tfunction.setRuntimeContext(runtimeContext);\n+\t\tfunction.setOperatorEventGateway(gateway);\n+\t\tfunction.open(new Configuration());\n+\t\tcoordinator.handleEventFromOperator(0, gateway.getNextEvent());\n+\t}\n+\n+\tpublic void openFunctionWithState() throws Exception {\n+\t\tfunctionInitializationContext.getOperatorStateStore().revertToLastSuccessCheckpoint();\n+\t\tfunction = new CollectSinkFunction<>(serializer, maxBytesPerBatch, ACCUMULATOR_NAME);\n+\t\tfunction.setRuntimeContext(runtimeContext);\n+\t\tfunction.setOperatorEventGateway(gateway);\n+\t\tfunction.initializeState(functionInitializationContext);\n+\t\tfunction.open(new Configuration());\n+\t\tcoordinator.handleEventFromOperator(0, gateway.getNextEvent());\n+\t}\n+\n+\tpublic void invoke(IN record) throws Exception {\n+\t\tfunction.invoke(record, null);\n+\t}\n+\n+\tpublic void checkpointFunction(long checkpointId) throws Exception {\n+\t\tfunction.snapshotState(new MockFunctionSnapshotContext(checkpointId));\n+\t\tfunctionInitializationContext.getOperatorStateStore().checkpointBegin(checkpointId);\n+\t}\n+\n+\tpublic void checkpointComplete(long checkpointId) {\n+\t\tfunction.notifyCheckpointComplete(checkpointId);\n+\t\tfunctionInitializationContext.getOperatorStateStore().checkpointSuccess(checkpointId);\n+\t}\n+\n+\tpublic void closeFunctionNormally() throws Exception {\n+\t\t// this is a normal shutdown\n+\t\tfunction.accumulateFinalResults();\n+\t\tfunction.close();\n+\t}\n+\n+\tpublic void closeFuntionAbnormally() throws Exception {\n+\t\t// this is an exceptional shutdown\n+\t\tfunction.close();\n+\t\tcoordinator.subtaskFailed(0, null);\n+\t}\n+\n+\tpublic CollectCoordinationResponse sendRequestAndGetResponse(String version, long offset) throws Exception {\n+\t\tCollectCoordinationResponse response;\n+\t\tfor (int i = 0; i < MAX_RETIRES; i++) {\n+\t\t\tresponse = sendRequest(version, offset);\n+\t\t\tif (response.getLastCheckpointedOffset() >= 0) {\n+\t\t\t\treturn response;\n+\t\t\t}\n+\t\t}\n+\t\tthrow new RuntimeException(\"Too many retries in sendRequestAndGetValidResponse\");\n+\t}\n+\n+\tprivate CollectCoordinationResponse sendRequest(String version, long offset) throws Exception {\n+\t\tCollectCoordinationRequest request = new CollectCoordinationRequest(version, offset);\n+\t\t// we add a timeout to not block the tests if it fails\n+\t\treturn ((CollectCoordinationResponse) coordinator\n+\t\t\t.handleCoordinationRequest(request).get(FUTURE_TIMEOUT_MILLIS, TimeUnit.MILLISECONDS));\n+\t}\n+\n+\tpublic Tuple2<Long, CollectCoordinationResponse> getAccumualtorResults() throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c3ac42923fc06b5d0d86c01c814918916ab86790"}, "originalPosition": 158}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDkzMDcwMQ==", "bodyText": "I think this branch is unnecessary, because it's illegal that checkpoint interval is less than MINIMAL_CHECKPOINT_TIME, many places have such validation, e.g. CheckpointConfig.setCheckpointInterval", "url": "https://github.com/apache/flink/pull/12867#discussion_r454930701", "createdAt": "2020-07-15T09:50:27Z", "author": {"login": "godfreyhe"}, "path": "flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/collect/CollectResultIterator.java", "diffHunk": "@@ -40,18 +43,37 @@\n \tpublic CollectResultIterator(\n \t\t\tCompletableFuture<OperatorID> operatorIdFuture,\n \t\t\tTypeSerializer<T> serializer,\n-\t\t\tString accumulatorName) {\n-\t\tthis.fetcher = new CollectResultFetcher<>(operatorIdFuture, serializer, accumulatorName);\n+\t\t\tString accumulatorName,\n+\t\t\tCheckpointConfig checkpointConfig) {\n+\t\tif (checkpointConfig.getCheckpointingMode() == CheckpointingMode.EXACTLY_ONCE) {\n+\t\t\tif (checkpointConfig.getCheckpointInterval() >= CheckpointCoordinatorConfiguration.MINIMAL_CHECKPOINT_TIME) {\n+\t\t\t\tthis.fetcher = new CollectResultFetcher<>(\n+\t\t\t\t\tnew CheckpointedCollectResultBuffer<>(serializer),\n+\t\t\t\t\toperatorIdFuture,\n+\t\t\t\t\taccumulatorName);\n+\t\t\t} else {\n+\t\t\t\tthis.fetcher = new CollectResultFetcher<>(\n+\t\t\t\t\tnew UncheckpointedCollectResultBuffer<>(serializer, false),\n+\t\t\t\t\toperatorIdFuture,\n+\t\t\t\t\taccumulatorName);\n+\t\t\t}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c3ac42923fc06b5d0d86c01c814918916ab86790"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDk4NzkyNw==", "bodyText": "change addResults  as a utility method ? so that we can handle the variables (e.g. offset) in one method, and make this method more readable.", "url": "https://github.com/apache/flink/pull/12867#discussion_r454987927", "createdAt": "2020-07-15T11:41:11Z", "author": {"login": "godfreyhe"}, "path": "flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/collect/UncheckpointedCollectResultBuffer.java", "diffHunk": "@@ -0,0 +1,58 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.api.operators.collect;\n+\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+\n+import java.io.IOException;\n+\n+/**\n+ * A buffer which encapsulates the logic of dealing with the response from the {@link CollectSinkFunction}.\n+ * It ignores the checkpoint related fields in the response.\n+ * See Java doc of {@link CollectSinkFunction} for explanation of this communication protocol.\n+ */\n+public class UncheckpointedCollectResultBuffer<T> extends AbstractCollectResultBuffer<T> {\n+\n+\tprivate final boolean failureTolerance;\n+\n+\tpublic UncheckpointedCollectResultBuffer(TypeSerializer<T> serializer, boolean failureTolerance) {\n+\t\tsuper(serializer);\n+\t\tthis.failureTolerance = failureTolerance;\n+\t}\n+\n+\t@Override\n+\tpublic void dealWithResponse(CollectCoordinationResponse response, long responseOffset) throws IOException {\n+\t\tString responseVersion = response.getVersion();\n+\n+\t\tif (!version.equals(responseVersion)) {\n+\t\t\tif (!INIT_VERSION.equals(version) && !failureTolerance) {\n+\t\t\t\t// sink restarted but we do not tolerate failure\n+\t\t\t\tthrow new RuntimeException(\"Job restarted\");\n+\t\t\t}\n+\n+\t\t\treset();\n+\t\t\tversion = responseVersion;\n+\t\t}\n+\n+\t\taddResults(response, responseOffset);\n+\t\t// the results are instantly visible by users\n+\t\tuserVisibleTail = offset;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c3ac42923fc06b5d0d86c01c814918916ab86790"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDk5OTg3NQ==", "bodyText": "checkpointedData for  UncheckpointedDataFeeder ?", "url": "https://github.com/apache/flink/pull/12867#discussion_r454999875", "createdAt": "2020-07-15T12:04:36Z", "author": {"login": "godfreyhe"}, "path": "flink-streaming-java/src/test/java/org/apache/flink/streaming/api/operators/collect/CollectSinkFunctionRandomITCase.java", "diffHunk": "@@ -0,0 +1,366 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.api.operators.collect;\n+\n+import org.apache.flink.api.common.JobID;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.common.typeutils.base.IntSerializer;\n+import org.apache.flink.runtime.jobgraph.OperatorID;\n+import org.apache.flink.streaming.api.operators.collect.utils.CollectSinkFunctionTestWrapper;\n+import org.apache.flink.streaming.api.operators.collect.utils.TestJobClient;\n+import org.apache.flink.util.OptionalFailure;\n+import org.apache.flink.util.TestLogger;\n+import org.apache.flink.util.function.RunnableWithException;\n+\n+import org.hamcrest.CoreMatchers;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.ListIterator;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.CompletableFuture;\n+\n+import static org.apache.flink.streaming.api.operators.collect.utils.CollectSinkFunctionTestWrapper.ACCUMULATOR_NAME;\n+\n+/**\n+ * Random IT cases for {@link CollectSinkFunction}.\n+ * It will perform random insert, random checkpoint and random restart.\n+ */\n+public class CollectSinkFunctionRandomITCase extends TestLogger {\n+\n+\tprivate static final int MAX_RESULTS_PER_BATCH = 3;\n+\tprivate static final JobID TEST_JOB_ID = new JobID();\n+\tprivate static final OperatorID TEST_OPERATOR_ID = new OperatorID();\n+\n+\tprivate static final TypeSerializer<Integer> serializer = IntSerializer.INSTANCE;\n+\n+\tprivate CollectSinkFunctionTestWrapper<Integer> functionWrapper;\n+\tprivate boolean jobFinished;\n+\n+\t@Test\n+\tpublic void testUncheckpointedFunction() throws Exception {\n+\t\t// run multiple times for this random test\n+\t\tfor (int testCount = 30; testCount > 0; testCount--) {\n+\t\t\tfunctionWrapper = new CollectSinkFunctionTestWrapper<>(serializer, MAX_RESULTS_PER_BATCH * 4);\n+\t\t\tjobFinished = false;\n+\n+\t\t\tList<Integer> expected = new ArrayList<>();\n+\t\t\tfor (int i = 0; i < 50; i++) {\n+\t\t\t\texpected.add(i);\n+\t\t\t}\n+\t\t\tThread feeder = new ThreadWithException(new UncheckpointedDataFeeder(expected));\n+\n+\t\t\tList<Integer> actual = runFunctionRandomTest(feeder);\n+\t\t\tassertResultsEqualAfterSort(expected, actual);\n+\n+\t\t\tfunctionWrapper.closeWrapper();\n+\t\t}\n+\t}\n+\n+\t@Test\n+\tpublic void testCheckpointedFunction() throws Exception {\n+\t\t// run multiple times for this random test\n+\t\tfor (int testCount = 30; testCount > 0; testCount--) {\n+\t\t\tfunctionWrapper = new CollectSinkFunctionTestWrapper<>(serializer, MAX_RESULTS_PER_BATCH * 4);\n+\t\t\tjobFinished = false;\n+\n+\t\t\tList<Integer> expected = new ArrayList<>();\n+\t\t\tfor (int i = 0; i < 50; i++) {\n+\t\t\t\texpected.add(i);\n+\t\t\t}\n+\t\t\tThread feeder = new ThreadWithException(new CheckpointedDataFeeder(expected));\n+\n+\t\t\tList<Integer> actual = runFunctionRandomTest(feeder);\n+\t\t\tassertResultsEqualAfterSort(expected, actual);\n+\n+\t\t\tfunctionWrapper.closeWrapper();\n+\t\t}\n+\t}\n+\n+\tprivate List<Integer> runFunctionRandomTest(Thread feeder) throws Exception {\n+\t\tCollectClient collectClient = new CollectClient();\n+\t\tThread client = new ThreadWithException(collectClient);\n+\n+\t\tThread.UncaughtExceptionHandler exceptionHandler = (t, e) -> {\n+\t\t\tfeeder.interrupt();\n+\t\t\tclient.interrupt();\n+\t\t\te.printStackTrace();\n+\t\t};\n+\t\tfeeder.setUncaughtExceptionHandler(exceptionHandler);\n+\t\tclient.setUncaughtExceptionHandler(exceptionHandler);\n+\n+\t\tfeeder.start();\n+\t\tclient.start();\n+\t\tfeeder.join();\n+\t\tclient.join();\n+\n+\t\treturn collectClient.results;\n+\t}\n+\n+\tprivate void assertResultsEqualAfterSort(List<Integer> expected, List<Integer> actual) {\n+\t\tCollections.sort(expected);\n+\t\tCollections.sort(actual);\n+\t\tAssert.assertThat(actual, CoreMatchers.is(expected));\n+\t}\n+\n+\t/**\n+\t * A {@link RunnableWithException} feeding data to the function. It will fail when half of the data is fed.\n+\t */\n+\tprivate class UncheckpointedDataFeeder implements RunnableWithException {\n+\n+\t\tprivate LinkedList<Integer> data;\n+\t\tprivate final List<Integer> checkpointedData;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c3ac42923fc06b5d0d86c01c814918916ab86790"}, "originalPosition": 133}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5f4341766d42b1bb60db4b94e028ede58bf5a661", "author": {"user": {"login": "tsreaper", "name": null}}, "url": "https://github.com/apache/flink/commit/5f4341766d42b1bb60db4b94e028ede58bf5a661", "committedDate": "2020-07-17T02:32:23Z", "message": "[FLINK-18559][tests] Refactor tests for datastream / table collect"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8d114eb7c57a869bd094a0fd8164d671f1475dac", "author": {"user": {"login": "tsreaper", "name": null}}, "url": "https://github.com/apache/flink/commit/8d114eb7c57a869bd094a0fd8164d671f1475dac", "committedDate": "2020-07-17T02:32:23Z", "message": "[FLINK-18560][streaming] Introduce collect iterator with at least once semantics and exactly once semantics without fault tolerance"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "965cb49e682f803553e028a13595921cb95f8684", "author": {"user": {"login": "tsreaper", "name": null}}, "url": "https://github.com/apache/flink/commit/965cb49e682f803553e028a13595921cb95f8684", "committedDate": "2020-07-17T02:32:23Z", "message": "[FLINK-18560][fix] Add comments for newly added class and change iterator selection a little"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "454479efb6de5855111bfa45ff10cf55148f4117", "author": {"user": {"login": "tsreaper", "name": null}}, "url": "https://github.com/apache/flink/commit/454479efb6de5855111bfa45ff10cf55148f4117", "committedDate": "2020-07-17T02:32:23Z", "message": "[FLINK-18560][docs] Update documentation for Table API and SQL queries"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "45bcb707df83715483595caf6b3a0c03964abc3d", "author": {"user": {"login": "tsreaper", "name": null}}, "url": "https://github.com/apache/flink/commit/45bcb707df83715483595caf6b3a0c03964abc3d", "committedDate": "2020-07-17T03:02:24Z", "message": "[FLINK-18560][fix] Fix collect result buffer logic which is different with the logic before refactor"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a64ca67c2e50380c4b6aca1748fb71a3c620377a", "author": {"user": {"login": "tsreaper", "name": null}}, "url": "https://github.com/apache/flink/commit/a64ca67c2e50380c4b6aca1748fb71a3c620377a", "committedDate": "2020-07-17T03:11:49Z", "message": "[FLINK-18560][docs] Update java docs for TableResult#collect and #print"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDUwMzIzNjI0", "url": "https://github.com/apache/flink/pull/12867#pullrequestreview-450323624", "createdAt": "2020-07-17T01:50:00Z", "commit": {"oid": "577a355b33c5bdd7176f207201dc1b33d98c665a"}, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QwMTo1MDowMFrOGzCnig==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QwNToyODoxOFrOGzF03w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjE3MzQ1MA==", "bodyText": "users do not know what the the function calls means", "url": "https://github.com/apache/flink/pull/12867#discussion_r456173450", "createdAt": "2020-07-17T01:50:00Z", "author": {"login": "godfreyhe"}, "path": "docs/dev/table/sql/queries.md", "diffHunk": "@@ -145,21 +145,16 @@ A SELECT statement or a VALUES statement can be executed to collect the content\n `TableResult.collect()` method returns a closeable row iterator. The select job will not be finished unless all result data has been collected. We should actively close the job to avoid resource leak through the `CloseableIterator#close()` method. \n We can also print the select result to client console through the `TableResult.print()` method. The result data in `TableResult` can be accessed only once. Thus, `collect()` and `print()` must not be called after each other.\n \n-For streaming job, `TableResult.collect()` method or `TableResult.print` method guarantee end-to-end exactly-once record delivery. This requires the checkpointing mechanism to be enabled. By default, checkpointing is disabled. To enable checkpointing, we can set checkpointing properties (see the <a href=\"{{ site.baseurl }}/ops/config.html#checkpointing\">checkpointing config</a> for details) through `TableConfig`.\n-So a result record can be accessed by client only after its corresponding checkpoint completes.\n-\n-**Notes:** For streaming mode, only append-only query is supported now. \n+`TableResult.collect()` and `TableResult.print()` have slightly different behaviors under different checkpointing settings (to enable checkpointing for a streaming job, see <a href=\"{{ site.baseurl }}/ops/config.html#checkpointing\">checkpointing config</a>).\n+* If the user is running a batch job, or does not enable checkpointing for a streaming job, `TableResult.collect()` and `TableResult.print()` have neither exactly-once nor at-least-once guarantee. Query results are immediately accessible by the clients once they're produced, but the function calls will throw an exception when the job fails and restarts;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "577a355b33c5bdd7176f207201dc1b33d98c665a"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjIyNTQ4Mw==", "bodyText": "ditto", "url": "https://github.com/apache/flink/pull/12867#discussion_r456225483", "createdAt": "2020-07-17T05:26:01Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/TableResult.java", "diffHunk": "@@ -132,10 +132,22 @@\n \t *  }\n \t * }</pre>\n \t *\n-\t * <p>For streaming mode, this method guarantees end-to-end exactly-once record delivery\n-\t * which requires the checkpointing mechanism to be enabled.\n-\t * By default, checkpointing is disabled. To enable checkpointing, set checkpointing properties\n-\t * (see ExecutionCheckpointingOptions) through {@link TableConfig#getConfiguration()}.\n+\t * <p>This method has slightly different behaviors under different checkpointing settings\n+\t * (to enable checkpointing for a streaming job,\n+\t * set checkpointing properties through {@link TableConfig#getConfiguration()}).\n+\t * <ul>\n+\t *     <li>If the user is running a batch job, or does not enable checkpointing for a streaming job,\n+\t *     this method has neither exactly-once nor at-least-once guarantee.\n+\t *     Query results are immediately accessible by the clients once they're produced,\n+\t *     but the function calls will throw an exception when the job fails and restarts.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a64ca67c2e50380c4b6aca1748fb71a3c620377a"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjIyNTc3Ng==", "bodyText": "ditto", "url": "https://github.com/apache/flink/pull/12867#discussion_r456225776", "createdAt": "2020-07-17T05:27:17Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/TableResult.java", "diffHunk": "@@ -146,10 +158,22 @@\n \t/**\n \t * Print the result contents as tableau form to client console.\n \t *\n-\t * <p>For streaming mode, this method guarantees end-to-end exactly-once record delivery\n-\t * which requires the checkpointing mechanism to be enabled.\n-\t * By default, checkpointing is disabled. To enable checkpointing, set checkpointing properties\n-\t * (see ExecutionCheckpointingOptions) through {@link TableConfig#getConfiguration()}.\n+\t * <p>This method has slightly different behaviors under different checkpointing settings\n+\t * (to enable checkpointing for a streaming job,\n+\t * set checkpointing properties through {@link TableConfig#getConfiguration()}).\n+\t * <ul>\n+\t *     <li>If the user is running a batch job, or does not enable checkpointing for a streaming job,\n+\t *     this method has neither exactly-once nor at-least-once guarantee.\n+\t *     Query results are immediately accessible by the clients once they're produced,\n+\t *     but the function calls will throw an exception when the job fails and restarts.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a64ca67c2e50380c4b6aca1748fb71a3c620377a"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjIyNTg5Mg==", "bodyText": "If exactly-once checkpointing is enabled for a streaming job, ?", "url": "https://github.com/apache/flink/pull/12867#discussion_r456225892", "createdAt": "2020-07-17T05:27:45Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/TableResult.java", "diffHunk": "@@ -132,10 +132,22 @@\n \t *  }\n \t * }</pre>\n \t *\n-\t * <p>For streaming mode, this method guarantees end-to-end exactly-once record delivery\n-\t * which requires the checkpointing mechanism to be enabled.\n-\t * By default, checkpointing is disabled. To enable checkpointing, set checkpointing properties\n-\t * (see ExecutionCheckpointingOptions) through {@link TableConfig#getConfiguration()}.\n+\t * <p>This method has slightly different behaviors under different checkpointing settings\n+\t * (to enable checkpointing for a streaming job,\n+\t * set checkpointing properties through {@link TableConfig#getConfiguration()}).\n+\t * <ul>\n+\t *     <li>If the user is running a batch job, or does not enable checkpointing for a streaming job,\n+\t *     this method has neither exactly-once nor at-least-once guarantee.\n+\t *     Query results are immediately accessible by the clients once they're produced,\n+\t *     but the function calls will throw an exception when the job fails and restarts.\n+\t *     <li>If the user enables exactly-once checkpointing for a streaming job,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a64ca67c2e50380c4b6aca1748fb71a3c620377a"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjIyNjAxNQ==", "bodyText": "please update the doc of TableResult in flink-python", "url": "https://github.com/apache/flink/pull/12867#discussion_r456226015", "createdAt": "2020-07-17T05:28:18Z", "author": {"login": "godfreyhe"}, "path": "flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/TableResult.java", "diffHunk": "@@ -146,10 +158,22 @@\n \t/**\n \t * Print the result contents as tableau form to client console.\n \t *\n-\t * <p>For streaming mode, this method guarantees end-to-end exactly-once record delivery\n-\t * which requires the checkpointing mechanism to be enabled.\n-\t * By default, checkpointing is disabled. To enable checkpointing, set checkpointing properties\n-\t * (see ExecutionCheckpointingOptions) through {@link TableConfig#getConfiguration()}.\n+\t * <p>This method has slightly different behaviors under different checkpointing settings", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a64ca67c2e50380c4b6aca1748fb71a3c620377a"}, "originalPosition": 35}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bf49c2297508e5dde1a9e0d8d4dfbafefc5f46f2", "author": {"user": {"login": "tsreaper", "name": null}}, "url": "https://github.com/apache/flink/commit/bf49c2297508e5dde1a9e0d8d4dfbafefc5f46f2", "committedDate": "2020-07-20T02:57:49Z", "message": "[FLINK-18560][docs] Update docs according to godfrey's comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDUxMzg3OTYz", "url": "https://github.com/apache/flink/pull/12867#pullrequestreview-451387963", "createdAt": "2020-07-20T08:13:59Z", "commit": {"oid": "bf49c2297508e5dde1a9e0d8d4dfbafefc5f46f2"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQwODoxNDowMFrOGz-6eA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQwODoxNDowMFrOGz-6eA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzE2MTMzNg==", "bodyText": "Reduce indentation", "url": "https://github.com/apache/flink/pull/12867#discussion_r457161336", "createdAt": "2020-07-20T08:14:00Z", "author": {"login": "godfreyhe"}, "path": "flink-python/pyflink/table/table_result.py", "diffHunk": "@@ -134,10 +134,19 @@ def print(self):\n         \"\"\"\n         Print the result contents as tableau form to client console.\n \n-        For streaming mode, this method guarantees end-to-end exactly-once record delivery\n-        which requires the checkpointing mechanism to be enabled.\n-        By default, checkpointing is disabled. To enable checkpointing, set checkpointing properties\n-        (see ExecutionCheckpointingOptions) through `TableConfig#getConfiguration()`.\n+        This method has slightly different behaviors under different checkpointing settings.\n+\n+            - For batch jobs or streaming jobs without checkpointing,\n+              this method has neither exactly-once nor at-least-once guarantee.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bf49c2297508e5dde1a9e0d8d4dfbafefc5f46f2"}, "originalPosition": 11}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fb16ba0168cae5742f27c87acae3c477dd42150b", "author": {"user": {"login": "tsreaper", "name": null}}, "url": "https://github.com/apache/flink/commit/fb16ba0168cae5742f27c87acae3c477dd42150b", "committedDate": "2020-07-22T08:19:25Z", "message": "[FLINK-18560][fix] Fix indent in python docstring"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDUzNzkxNDE4", "url": "https://github.com/apache/flink/pull/12867#pullrequestreview-453791418", "createdAt": "2020-07-23T01:44:27Z", "commit": {"oid": "fb16ba0168cae5742f27c87acae3c477dd42150b"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a1ea81a9dd67cab4c8b483dfe3eed51d7ab4a9e2", "author": {"user": {"login": "tsreaper", "name": null}}, "url": "https://github.com/apache/flink/commit/a1ea81a9dd67cab4c8b483dfe3eed51d7ab4a9e2", "committedDate": "2020-07-23T02:09:43Z", "message": "[FLINK-18560][fix] Fix line too long"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDUzODQ0OTI1", "url": "https://github.com/apache/flink/pull/12867#pullrequestreview-453844925", "createdAt": "2020-07-23T05:33:39Z", "commit": {"oid": "a1ea81a9dd67cab4c8b483dfe3eed51d7ab4a9e2"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3120, "cost": 1, "resetAt": "2021-10-28T16:48:13Z"}}}