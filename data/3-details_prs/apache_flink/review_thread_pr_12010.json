{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDE0MTkxMTIz", "number": 12010, "reviewThreads": {"totalCount": 31, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wN1QwMjoxOTo1M1rOD6EIAg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMVQwMTo1MzoyNlrOD7DWfQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYyMjExNTg2OnYy", "diffSide": "RIGHT", "path": "flink-formats/flink-json/src/main/java/org/apache/flink/formats/json/JsonRowDataFileInputFormat.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wN1QwMjoxOTo1M1rOGRsBUw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wN1QwMjoxOTo1M1rOGRsBUw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTIwMDIxMQ==", "bodyText": "This class is not for user, we don't need builder.", "url": "https://github.com/apache/flink/pull/12010#discussion_r421200211", "createdAt": "2020-05-07T02:19:53Z", "author": {"login": "JingsongLi"}, "path": "flink-formats/flink-json/src/main/java/org/apache/flink/formats/json/JsonRowDataFileInputFormat.java", "diffHunk": "@@ -0,0 +1,248 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.json;\n+\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.factories.FileSystemFormatFactory;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.utils.PartitionPathUtils;\n+\n+import java.io.BufferedReader;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.nio.charset.Charset;\n+import java.util.Arrays;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.util.Preconditions.checkNotNull;\n+\n+/**\n+ * A {@link JsonRowDataFileInputFormat} is responsible to read {@link RowData} records\n+ * from json format files.\n+ */\n+public class JsonRowDataFileInputFormat extends AbstractJsonFileInputFormat<RowData> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate final List<DataType> fieldTypes;\n+\tprivate final List<String> fieldNames;\n+\tprivate final int[] selectFields;\n+\tprivate final List<String> partitionKeys;\n+\tprivate final String defaultPartValue;\n+\tprivate final long limit;\n+\tprivate final List<String> jsonSelectFieldNames;\n+\tprivate final int[] jsonFieldMapping;\n+\tprivate final JsonRowDataDeserializationSchema deserializationSchema;\n+\n+\tprivate transient boolean end;\n+\tprivate transient long emitted;\n+\t// reuse object for per record\n+\tprivate transient GenericRowData rowData;\n+\tprivate transient InputStreamReader inputStreamReader;\n+\tprivate transient BufferedReader reader;\n+\tprivate transient Charset charset;\n+\n+\tprivate JsonRowDataFileInputFormat(\n+\t\tPath[] filePaths,\n+\t\tRowType nonPartitionRowType,\n+\t\tTypeInformation nonPartitionRowTypeInfo,\n+\t\tList<DataType> dataTypes,\n+\t\tList<String> fieldNames,\n+\t\tint[] selectFields,\n+\t\tList<String> partitionKeys,\n+\t\tString defaultPartValue,\n+\t\tboolean failOnMissingField,\n+\t\tboolean ignoreParseErrors,\n+\t\tlong limit) {\n+\t\tsuper(filePaths);\n+\t\tthis.fieldTypes = dataTypes;\n+\t\tthis.fieldNames = fieldNames;\n+\t\tthis.selectFields = selectFields;\n+\t\tthis.partitionKeys = partitionKeys;\n+\t\tthis.defaultPartValue = defaultPartValue;\n+\t\tthis.limit = limit;\n+\t\t// project field\n+\t\tList<String> selectFieldNames = Arrays.stream(selectFields)\n+\t\t\t.mapToObj(fieldNames::get)\n+\t\t\t.collect(Collectors.toList());\n+\t\tthis.jsonSelectFieldNames = selectFieldNames.stream()\n+\t\t\t.filter(name -> !partitionKeys.contains(name)).collect(Collectors.toList());\n+\t\tthis.jsonFieldMapping = jsonSelectFieldNames.stream().mapToInt(selectFieldNames::indexOf).toArray();\n+\t\tthis.deserializationSchema = new JsonRowDataDeserializationSchema(\n+\t\t\tnonPartitionRowType,\n+\t\t\tnonPartitionRowTypeInfo,\n+\t\t\tfailOnMissingField,\n+\t\t\tignoreParseErrors);\n+\t}\n+\n+\t@Override\n+\tpublic void open(FileInputSplit split) throws IOException {\n+\t\tsuper.open(split);\n+\t\tthis.end = false;\n+\t\tthis.inputStreamReader = new InputStreamReader(jsonInputStream);\n+\t\tthis.reader = new BufferedReader(inputStreamReader);\n+\t\tthis.charset = Charset.forName(\"UTF-8\");\n+\t\tfillPartitionValueForRecord();\n+\t\tthis.emitted = 0L;\n+\t}\n+\n+\tprivate void fillPartitionValueForRecord() {\n+\t\trowData = new GenericRowData(selectFields.length);\n+\t\tPath path = currentSplit.getPath();\n+\t\tLinkedHashMap<String, String> partSpec = PartitionPathUtils.extractPartitionSpecFromPath(path);\n+\t\tfor (int i = 0; i < selectFields.length; i++) {\n+\t\t\tint selectField = selectFields[i];\n+\t\t\tString name = fieldNames.get(selectField);\n+\t\t\tif (partitionKeys.contains(name)) {\n+\t\t\t\tString value = partSpec.get(name);\n+\t\t\t\tvalue = defaultPartValue.equals(value) ? null : value;\n+\t\t\t\trowData.setField(i, PartitionPathUtils.restorePartValueFromType(value, fieldTypes.get(selectField)));\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic boolean reachedEnd() throws IOException {\n+\t\treturn emitted >= limit || end;\n+\t}\n+\n+\t@Override\n+\tpublic RowData nextRecord(RowData reuse) throws IOException {\n+\t\tGenericRowData returnRecord = rowData;\n+\t\tString line = reader.readLine();\n+\t\tif (line == null) {\n+\t\t\tthis.end = true;\n+\t\t\treturn null;\n+\t\t}\n+\t\tGenericRowData jsonRow = (GenericRowData) deserializationSchema.deserialize(line.getBytes(charset));\n+\t\tif (jsonRow != null) {\n+\t\t\tfor (int i = 0; i < jsonSelectFieldNames.size(); i++) {\n+\t\t\t\treturnRecord.setField(jsonFieldMapping[i], jsonRow.getField(i));\n+\t\t\t}\n+\t\t}\n+\t\temitted++;\n+\t\treturn returnRecord;\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws IOException {\n+\t\tsuper.close();\n+\t\tif (reader != null) {\n+\t\t\tthis.reader.close();\n+\t\t\tthis.reader = null;\n+\t\t}\n+\t\tif (inputStreamReader != null) {\n+\t\t\tthis.inputStreamReader.close();\n+\t\t\tthis.inputStreamReader = null;\n+\t\t}\n+\t}\n+\n+\tpublic static Builder builder(FileSystemFormatFactory.ReaderContext context, TableSchema tableSchema, List<String> partitionKeys, Path... filePaths) {\n+\t\treturn new Builder(context, tableSchema, partitionKeys, filePaths);\n+\t}\n+\n+\t/**\n+\t * Builder for {@link JsonRowDataFileInputFormat}.\n+\t */\n+\tpublic static class Builder {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d7445e68ed77a6b1b88fdf0dac8638ff62ac8967"}, "originalPosition": 172}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYyMjExOTMwOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSource.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wN1QwMjoyMTo1NFrOGRsDTA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wN1QxMDozMTo1N1rOGR4fDA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTIwMDcxNg==", "bodyText": "We don't need type info now.", "url": "https://github.com/apache/flink/pull/12010#discussion_r421200716", "createdAt": "2020-05-07T02:21:54Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSource.java", "diffHunk": "@@ -121,6 +127,14 @@ public TableSchema getSchema() {\n \t\t\t\treturn schema;\n \t\t\t}\n \n+\t\t\t@Override\n+\t\t\tpublic TypeInformation<?> createTypeInformation(DataType dataType) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d7445e68ed77a6b1b88fdf0dac8638ff62ac8967"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTIwMjE1Nw==", "bodyText": "JsonRowDataDeserializationSchema need this type info", "url": "https://github.com/apache/flink/pull/12010#discussion_r421202157", "createdAt": "2020-05-07T02:27:51Z", "author": {"login": "leonardBang"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSource.java", "diffHunk": "@@ -121,6 +127,14 @@ public TableSchema getSchema() {\n \t\t\t\treturn schema;\n \t\t\t}\n \n+\t\t\t@Override\n+\t\t\tpublic TypeInformation<?> createTypeInformation(DataType dataType) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTIwMDcxNg=="}, "originalCommit": {"oid": "d7445e68ed77a6b1b88fdf0dac8638ff62ac8967"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTQwNDQyOA==", "bodyText": "we can pass a GenericTypeInformation", "url": "https://github.com/apache/flink/pull/12010#discussion_r421404428", "createdAt": "2020-05-07T10:31:57Z", "author": {"login": "leonardBang"}, "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSource.java", "diffHunk": "@@ -121,6 +127,14 @@ public TableSchema getSchema() {\n \t\t\t\treturn schema;\n \t\t\t}\n \n+\t\t\t@Override\n+\t\t\tpublic TypeInformation<?> createTypeInformation(DataType dataType) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTIwMDcxNg=="}, "originalCommit": {"oid": "d7445e68ed77a6b1b88fdf0dac8638ff62ac8967"}, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYyMjEyMzAyOnYy", "diffSide": "RIGHT", "path": "flink-formats/flink-json/src/main/java/org/apache/flink/formats/json/JsonRowDataFileInputFormat.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wN1QwMjoyNDowOVrOGRsFbQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wN1QxMDozMToxNFrOGR4dYw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTIwMTI2MQ==", "bodyText": "This can be an inner class in FormatFactory just like parquet and orc.", "url": "https://github.com/apache/flink/pull/12010#discussion_r421201261", "createdAt": "2020-05-07T02:24:09Z", "author": {"login": "JingsongLi"}, "path": "flink-formats/flink-json/src/main/java/org/apache/flink/formats/json/JsonRowDataFileInputFormat.java", "diffHunk": "@@ -0,0 +1,248 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.json;\n+\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.factories.FileSystemFormatFactory;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.utils.PartitionPathUtils;\n+\n+import java.io.BufferedReader;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.nio.charset.Charset;\n+import java.util.Arrays;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.util.Preconditions.checkNotNull;\n+\n+/**\n+ * A {@link JsonRowDataFileInputFormat} is responsible to read {@link RowData} records\n+ * from json format files.\n+ */\n+public class JsonRowDataFileInputFormat extends AbstractJsonFileInputFormat<RowData> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d7445e68ed77a6b1b88fdf0dac8638ff62ac8967"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTQwNDAwMw==", "bodyText": "Thanks for contribution.\nWe need introduce an option multiLine, it default value is false now. (We can not support true, but should throw a good unsupported exception)\n\nHi, @JingsongLi multiline option for support read multiline per time? I don't get this point, could you explain more ?", "url": "https://github.com/apache/flink/pull/12010#discussion_r421404003", "createdAt": "2020-05-07T10:31:14Z", "author": {"login": "leonardBang"}, "path": "flink-formats/flink-json/src/main/java/org/apache/flink/formats/json/JsonRowDataFileInputFormat.java", "diffHunk": "@@ -0,0 +1,248 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.json;\n+\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.factories.FileSystemFormatFactory;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.utils.PartitionPathUtils;\n+\n+import java.io.BufferedReader;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.nio.charset.Charset;\n+import java.util.Arrays;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.util.Preconditions.checkNotNull;\n+\n+/**\n+ * A {@link JsonRowDataFileInputFormat} is responsible to read {@link RowData} records\n+ * from json format files.\n+ */\n+public class JsonRowDataFileInputFormat extends AbstractJsonFileInputFormat<RowData> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTIwMTI2MQ=="}, "originalCommit": {"oid": "d7445e68ed77a6b1b88fdf0dac8638ff62ac8967"}, "originalPosition": 48}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYyNzM4NjI0OnYy", "diffSide": "RIGHT", "path": "flink-formats/flink-json/src/main/java/org/apache/flink/formats/json/JsonFileSystemFormatFactory.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOFQwODo0Njo1NVrOGSeWaQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOFQwODo0Njo1NVrOGSeWaQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjAyNDgwOQ==", "bodyText": "Standardxxx.UTF-8", "url": "https://github.com/apache/flink/pull/12010#discussion_r422024809", "createdAt": "2020-05-08T08:46:55Z", "author": {"login": "JingsongLi"}, "path": "flink-formats/flink-json/src/main/java/org/apache/flink/formats/json/JsonFileSystemFormatFactory.java", "diffHunk": "@@ -0,0 +1,293 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.json;\n+\n+import org.apache.flink.api.common.io.DelimitedInputFormat;\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.api.java.typeutils.GenericTypeInfo;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.FileSystemFormatFactory;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.utils.PartitionPathUtils;\n+\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.nio.charset.Charset;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+import static org.apache.flink.table.descriptors.JsonValidator.FORMAT_FAIL_ON_MISSING_FIELD;\n+import static org.apache.flink.table.descriptors.JsonValidator.FORMAT_IGNORE_PARSE_ERRORS;\n+\n+/**\n+ * Factory to build reader/writer to read/write json format file.\n+ */\n+public class JsonFileSystemFormatFactory implements FileSystemFormatFactory {\n+\t@Override\n+\tpublic Map<String, String> requiredContext() {\n+\t\tMap<String, String> context = new HashMap<>();\n+\t\tcontext.put(FORMAT, \"json\");\n+\t\treturn context;\n+\t}\n+\n+\t@Override\n+\tpublic List<String> supportedProperties() {\n+\t\tArrayList<String> properties = new ArrayList<>();\n+\t\tproperties.add(FORMAT_FAIL_ON_MISSING_FIELD);\n+\t\tproperties.add(FORMAT_IGNORE_PARSE_ERRORS);\n+\t\treturn properties;\n+\t}\n+\n+\t@Override\n+\tpublic InputFormat<RowData, ?> createReader(ReaderContext context) {\n+\t\tDescriptorProperties properties = getValidatedProperties(context.getFormatProperties());\n+\t\tboolean failOnMissingField = properties.getOptionalBoolean(FORMAT_FAIL_ON_MISSING_FIELD).orElse(false);\n+\t\tboolean ignoreParseErrors = properties.getOptionalBoolean(FORMAT_IGNORE_PARSE_ERRORS).orElse(false);\n+\n+\t\t// deal partition fields\n+\t\tTableSchema tableSchema = context.getSchema();\n+\t\tList<DataType> fieldTypes = Arrays.asList(tableSchema.getFieldDataTypes());\n+\t\tList<String> fieldNames = Arrays.asList(tableSchema.getFieldNames());\n+\t\tString[] nonPartFieldNames = fieldNames.stream()\n+\t\t\t.filter(name -> !context.getPartitionKeys().contains(name)).toArray(String[]::new);\n+\t\tDataType[] nonPartFieldTypes = Arrays.asList(nonPartFieldNames).stream()\n+\t\t\t.map(name -> fieldTypes.get(fieldNames.indexOf(name))).toArray(DataType[]::new);\n+\t\tRowType nonPartRowType = RowType.of(\n+\t\t\tArrays.asList(nonPartFieldTypes).stream()\n+\t\t\t\t.map(DataType::getLogicalType)\n+\t\t\t\t.toArray(LogicalType[]::new),\n+\t\t\tnonPartFieldNames);\n+\n+\t\t// deal project fields\n+\t\tint[] selectFields = context.getProjectFields();\n+\t\tList<String> partitionKeys = context.getPartitionKeys();\n+\t\tList<String> selectFieldNames = Arrays.stream(selectFields)\n+\t\t\t.mapToObj(fieldNames::get)\n+\t\t\t.collect(Collectors.toList());\n+\t\tList<String> jsonSelectFieldNames = selectFieldNames.stream()\n+\t\t\t.filter(name -> !partitionKeys.contains(name)).collect(Collectors.toList());\n+\t\tint[] jsonFieldMapping =  jsonSelectFieldNames.stream().mapToInt(selectFieldNames::indexOf).toArray();\n+\n+\t\tJsonRowDataDeserializationSchema  deserializationSchema = new JsonRowDataDeserializationSchema(\n+\t\t\tnonPartRowType,\n+\t\t\tnew GenericTypeInfo(GenericRowData.class),\n+\t\t\tfailOnMissingField,\n+\t\t\tignoreParseErrors);\n+\n+\t\treturn new JsonFileSystemFormat(\n+\t\t\tcontext.getPaths(),\n+\t\t\tfieldTypes,\n+\t\t\tfieldNames,\n+\t\t\tselectFields,\n+\t\t\tpartitionKeys,\n+\t\t\tcontext.getDefaultPartName(),\n+\t\t\tcontext.getPushedDownLimit(),\n+\t\t\tjsonSelectFieldNames,\n+\t\t\tjsonFieldMapping,\n+\t\t\tdeserializationSchema);\n+\t}\n+\n+\t@Override\n+\tpublic Optional<Encoder<RowData>> createEncoder(WriterContext context) {\n+\t\tTableSchema nonPartKeySchema = new TableSchema.Builder()\n+\t\t\t.fields(context.getFieldNamesWithoutPartKeys(), context.getFieldTypesWithoutPartKeys())\n+\t\t\t.build();\n+\t\tLogicalType[] logicalTypes = Arrays.asList(nonPartKeySchema.getFieldDataTypes())\n+\t\t\t.stream().map(DataType::getLogicalType).toArray(LogicalType[]::new);\n+\t\tRowType nonPartRowType = RowType.of(logicalTypes, nonPartKeySchema.getFieldNames());\n+\t\treturn Optional.of(new JsonRowDataEncoder(new JsonRowDataSerializationSchema(nonPartRowType)));\n+\t}\n+\n+\t@Override\n+\tpublic Optional<BulkWriter.Factory<RowData>> createBulkWriterFactory(WriterContext context) {\n+\t\treturn Optional.empty();\n+\t}\n+\n+\t@Override\n+\tpublic boolean supportsSchemaDerivation() {\n+\t\treturn true;\n+\t}\n+\n+\tprivate static DescriptorProperties getValidatedProperties(Map<String, String> propertiesMap) {\n+\t\tfinal DescriptorProperties properties = new DescriptorProperties(true);\n+\t\tproperties.putProperties(propertiesMap);\n+\t\tproperties.validateBoolean(FORMAT_FAIL_ON_MISSING_FIELD, true);\n+\t\tproperties.validateBoolean(FORMAT_IGNORE_PARSE_ERRORS, true);\n+\t\treturn properties;\n+\t}\n+\n+\t/**\n+\t * A {@link JsonFileSystemFormat} is responsible to read {@link RowData} records\n+\t * from json format files.\n+\t */\n+\tpublic static class JsonFileSystemFormat extends DelimitedInputFormat<RowData> {\n+\t\t/**\n+\t\t * Code of \\r, used to remove \\r from a line when the line ends with \\r\\n.\n+\t\t */\n+\t\tprivate static final byte CARRIAGE_RETURN = (byte) '\\r';\n+\n+\t\t/**\n+\t\t * Code of \\n, used to identify if \\n is used as delimiter.\n+\t\t */\n+\t\tprivate static final byte NEW_LINE = (byte) '\\n';\n+\n+\t\t/**\n+\t\t * The name of the charset to use for decoding.\n+\t\t */\n+\t\tprivate String charsetName = \"UTF-8\";\n+\n+\t\tprivate final List<DataType> fieldTypes;\n+\t\tprivate final List<String> fieldNames;\n+\t\tprivate final int[] selectFields;\n+\t\tprivate final List<String> partitionKeys;\n+\t\tprivate final String defaultPartValue;\n+\t\tprivate final long limit;\n+\t\tprivate final List<String> jsonSelectFieldNames;\n+\t\tprivate final int[] jsonFieldMapping;\n+\t\tprivate final JsonRowDataDeserializationSchema deserializationSchema;\n+\n+\t\tprivate transient boolean end;\n+\t\tprivate transient long emitted;\n+\t\t// reuse object for per record\n+\t\tprivate transient GenericRowData rowData;\n+\n+\t\tpublic JsonFileSystemFormat(\n+\t\t\t\tPath[] filePaths,\n+\t\t\t\tList<DataType> fieldTypes,\n+\t\t\t\tList<String> fieldNames,\n+\t\t\t\tint[] selectFields,\n+\t\t\t\tList<String> partitionKeys,\n+\t\t\t\tString defaultPartValue,\n+\t\t\t\tlong limit,\n+\t\t\t\tList<String> jsonSelectFieldNames,\n+\t\t\t\tint[] jsonFieldMapping,\n+\t\t\t\tJsonRowDataDeserializationSchema deserializationSchema) {\n+\t\t\tsuper.setFilePaths(filePaths);\n+\t\t\tthis.fieldTypes = fieldTypes;\n+\t\t\tthis.fieldNames = fieldNames;\n+\t\t\tthis.selectFields = selectFields;\n+\t\t\tthis.partitionKeys = partitionKeys;\n+\t\t\tthis.defaultPartValue = defaultPartValue;\n+\t\t\tthis.limit = limit;\n+\t\t\tthis.jsonSelectFieldNames = jsonSelectFieldNames;\n+\t\t\tthis.jsonFieldMapping = jsonFieldMapping;\n+\t\t\tthis.deserializationSchema = deserializationSchema;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void configure(Configuration parameters) {\n+\t\t\tsuper.configure(parameters);\n+\n+\t\t\tif (charsetName == null || !Charset.isSupported(charsetName)) {\n+\t\t\t\tthrow new RuntimeException(\"Unsupported charset: \" + charsetName);\n+\t\t\t}\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic boolean supportsMultiPaths() {\n+\t\t\treturn true;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void open(FileInputSplit split) throws IOException {\n+\t\t\tsuper.open(split);\n+\t\t\tthis.end = false;\n+\t\t\tfillPartitionValueForRecord();\n+\t\t\tthis.emitted = 0L;\n+\t\t}\n+\n+\t\tprivate void fillPartitionValueForRecord() {\n+\t\t\trowData = new GenericRowData(selectFields.length);\n+\t\t\tPath path = currentSplit.getPath();\n+\t\t\tLinkedHashMap<String, String> partSpec = PartitionPathUtils.extractPartitionSpecFromPath(path);\n+\t\t\tfor (int i = 0; i < selectFields.length; i++) {\n+\t\t\t\tint selectField = selectFields[i];\n+\t\t\t\tString name = fieldNames.get(selectField);\n+\t\t\t\tif (partitionKeys.contains(name)) {\n+\t\t\t\t\tString value = partSpec.get(name);\n+\t\t\t\t\tvalue = defaultPartValue.equals(value) ? null : value;\n+\t\t\t\t\trowData.setField(i, PartitionPathUtils.restorePartValueFromType(value, fieldTypes.get(selectField)));\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic boolean reachedEnd() {\n+\t\t\treturn emitted >= limit || end;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic RowData readRecord(RowData reuse, byte[] bytes, int offset, int numBytes) throws IOException {\n+\t\t\tGenericRowData returnRecord = rowData;\n+\n+\t\t\tif (this.getDelimiter() != null && this.getDelimiter().length == 1\n+\t\t\t\t&& this.getDelimiter()[0] == NEW_LINE && offset + numBytes >= 1\n+\t\t\t\t&& bytes[offset + numBytes - 1] == CARRIAGE_RETURN){\n+\t\t\t\tnumBytes -= 1;\n+\t\t\t}\n+\t\t\tbyte[] trimBytes = Arrays.copyOfRange(bytes, offset, offset + numBytes);\n+\t\t\tGenericRowData jsonRow = (GenericRowData) deserializationSchema.deserialize(trimBytes);\n+\t\t\tif (jsonRow != null) {\n+\t\t\t\tfor (int i = 0; i < jsonSelectFieldNames.size(); i++) {\n+\t\t\t\t\treturnRecord.setField(jsonFieldMapping[i], jsonRow.getField(i));\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\temitted++;\n+\t\t\treturn returnRecord;\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * A {@link JsonRowDataEncoder} is responsible to encode a {@link RowData} to {@link java.io.OutputStream}\n+\t * with json format.\n+\t */\n+\tpublic static class JsonRowDataEncoder implements Encoder<RowData> {\n+\n+\t\tprivate static final long serialVersionUID = 1L;\n+\t\tprivate static final String DEFAULT_LINE_DELIMITER = \"\\n\";\n+\t\tprivate final JsonRowDataSerializationSchema serializationSchema;\n+\n+\t\tpublic JsonRowDataEncoder(JsonRowDataSerializationSchema serializationSchema) {\n+\t\t\tthis.serializationSchema = serializationSchema;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void encode(RowData element, OutputStream stream) throws IOException {\n+\t\t\tstream.write(serializationSchema.serialize(element));\n+\t\t\tstream.write(DEFAULT_LINE_DELIMITER.getBytes());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c3958066f46822218c29a10c7bad1d4d699949f8"}, "originalPosition": 290}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYyNzM5MzU5OnYy", "diffSide": "RIGHT", "path": "flink-formats/flink-json/src/main/java/org/apache/flink/formats/json/JsonFileSystemFormatFactory.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOFQwODo0OToxN1rOGSea1g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOFQwODo0OToxN1rOGSea1g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjAyNTk0Mg==", "bodyText": "Can extract a method or class to put it in 'PartitionPathUtils'?", "url": "https://github.com/apache/flink/pull/12010#discussion_r422025942", "createdAt": "2020-05-08T08:49:17Z", "author": {"login": "JingsongLi"}, "path": "flink-formats/flink-json/src/main/java/org/apache/flink/formats/json/JsonFileSystemFormatFactory.java", "diffHunk": "@@ -0,0 +1,293 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.json;\n+\n+import org.apache.flink.api.common.io.DelimitedInputFormat;\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.api.java.typeutils.GenericTypeInfo;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.FileSystemFormatFactory;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.utils.PartitionPathUtils;\n+\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.nio.charset.Charset;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+import static org.apache.flink.table.descriptors.JsonValidator.FORMAT_FAIL_ON_MISSING_FIELD;\n+import static org.apache.flink.table.descriptors.JsonValidator.FORMAT_IGNORE_PARSE_ERRORS;\n+\n+/**\n+ * Factory to build reader/writer to read/write json format file.\n+ */\n+public class JsonFileSystemFormatFactory implements FileSystemFormatFactory {\n+\t@Override\n+\tpublic Map<String, String> requiredContext() {\n+\t\tMap<String, String> context = new HashMap<>();\n+\t\tcontext.put(FORMAT, \"json\");\n+\t\treturn context;\n+\t}\n+\n+\t@Override\n+\tpublic List<String> supportedProperties() {\n+\t\tArrayList<String> properties = new ArrayList<>();\n+\t\tproperties.add(FORMAT_FAIL_ON_MISSING_FIELD);\n+\t\tproperties.add(FORMAT_IGNORE_PARSE_ERRORS);\n+\t\treturn properties;\n+\t}\n+\n+\t@Override\n+\tpublic InputFormat<RowData, ?> createReader(ReaderContext context) {\n+\t\tDescriptorProperties properties = getValidatedProperties(context.getFormatProperties());\n+\t\tboolean failOnMissingField = properties.getOptionalBoolean(FORMAT_FAIL_ON_MISSING_FIELD).orElse(false);\n+\t\tboolean ignoreParseErrors = properties.getOptionalBoolean(FORMAT_IGNORE_PARSE_ERRORS).orElse(false);\n+\n+\t\t// deal partition fields\n+\t\tTableSchema tableSchema = context.getSchema();\n+\t\tList<DataType> fieldTypes = Arrays.asList(tableSchema.getFieldDataTypes());\n+\t\tList<String> fieldNames = Arrays.asList(tableSchema.getFieldNames());\n+\t\tString[] nonPartFieldNames = fieldNames.stream()\n+\t\t\t.filter(name -> !context.getPartitionKeys().contains(name)).toArray(String[]::new);\n+\t\tDataType[] nonPartFieldTypes = Arrays.asList(nonPartFieldNames).stream()\n+\t\t\t.map(name -> fieldTypes.get(fieldNames.indexOf(name))).toArray(DataType[]::new);\n+\t\tRowType nonPartRowType = RowType.of(\n+\t\t\tArrays.asList(nonPartFieldTypes).stream()\n+\t\t\t\t.map(DataType::getLogicalType)\n+\t\t\t\t.toArray(LogicalType[]::new),\n+\t\t\tnonPartFieldNames);\n+\n+\t\t// deal project fields\n+\t\tint[] selectFields = context.getProjectFields();\n+\t\tList<String> partitionKeys = context.getPartitionKeys();\n+\t\tList<String> selectFieldNames = Arrays.stream(selectFields)\n+\t\t\t.mapToObj(fieldNames::get)\n+\t\t\t.collect(Collectors.toList());\n+\t\tList<String> jsonSelectFieldNames = selectFieldNames.stream()\n+\t\t\t.filter(name -> !partitionKeys.contains(name)).collect(Collectors.toList());\n+\t\tint[] jsonFieldMapping =  jsonSelectFieldNames.stream().mapToInt(selectFieldNames::indexOf).toArray();\n+\n+\t\tJsonRowDataDeserializationSchema  deserializationSchema = new JsonRowDataDeserializationSchema(\n+\t\t\tnonPartRowType,\n+\t\t\tnew GenericTypeInfo(GenericRowData.class),\n+\t\t\tfailOnMissingField,\n+\t\t\tignoreParseErrors);\n+\n+\t\treturn new JsonFileSystemFormat(\n+\t\t\tcontext.getPaths(),\n+\t\t\tfieldTypes,\n+\t\t\tfieldNames,\n+\t\t\tselectFields,\n+\t\t\tpartitionKeys,\n+\t\t\tcontext.getDefaultPartName(),\n+\t\t\tcontext.getPushedDownLimit(),\n+\t\t\tjsonSelectFieldNames,\n+\t\t\tjsonFieldMapping,\n+\t\t\tdeserializationSchema);\n+\t}\n+\n+\t@Override\n+\tpublic Optional<Encoder<RowData>> createEncoder(WriterContext context) {\n+\t\tTableSchema nonPartKeySchema = new TableSchema.Builder()\n+\t\t\t.fields(context.getFieldNamesWithoutPartKeys(), context.getFieldTypesWithoutPartKeys())\n+\t\t\t.build();\n+\t\tLogicalType[] logicalTypes = Arrays.asList(nonPartKeySchema.getFieldDataTypes())\n+\t\t\t.stream().map(DataType::getLogicalType).toArray(LogicalType[]::new);\n+\t\tRowType nonPartRowType = RowType.of(logicalTypes, nonPartKeySchema.getFieldNames());\n+\t\treturn Optional.of(new JsonRowDataEncoder(new JsonRowDataSerializationSchema(nonPartRowType)));\n+\t}\n+\n+\t@Override\n+\tpublic Optional<BulkWriter.Factory<RowData>> createBulkWriterFactory(WriterContext context) {\n+\t\treturn Optional.empty();\n+\t}\n+\n+\t@Override\n+\tpublic boolean supportsSchemaDerivation() {\n+\t\treturn true;\n+\t}\n+\n+\tprivate static DescriptorProperties getValidatedProperties(Map<String, String> propertiesMap) {\n+\t\tfinal DescriptorProperties properties = new DescriptorProperties(true);\n+\t\tproperties.putProperties(propertiesMap);\n+\t\tproperties.validateBoolean(FORMAT_FAIL_ON_MISSING_FIELD, true);\n+\t\tproperties.validateBoolean(FORMAT_IGNORE_PARSE_ERRORS, true);\n+\t\treturn properties;\n+\t}\n+\n+\t/**\n+\t * A {@link JsonFileSystemFormat} is responsible to read {@link RowData} records\n+\t * from json format files.\n+\t */\n+\tpublic static class JsonFileSystemFormat extends DelimitedInputFormat<RowData> {\n+\t\t/**\n+\t\t * Code of \\r, used to remove \\r from a line when the line ends with \\r\\n.\n+\t\t */\n+\t\tprivate static final byte CARRIAGE_RETURN = (byte) '\\r';\n+\n+\t\t/**\n+\t\t * Code of \\n, used to identify if \\n is used as delimiter.\n+\t\t */\n+\t\tprivate static final byte NEW_LINE = (byte) '\\n';\n+\n+\t\t/**\n+\t\t * The name of the charset to use for decoding.\n+\t\t */\n+\t\tprivate String charsetName = \"UTF-8\";\n+\n+\t\tprivate final List<DataType> fieldTypes;\n+\t\tprivate final List<String> fieldNames;\n+\t\tprivate final int[] selectFields;\n+\t\tprivate final List<String> partitionKeys;\n+\t\tprivate final String defaultPartValue;\n+\t\tprivate final long limit;\n+\t\tprivate final List<String> jsonSelectFieldNames;\n+\t\tprivate final int[] jsonFieldMapping;\n+\t\tprivate final JsonRowDataDeserializationSchema deserializationSchema;\n+\n+\t\tprivate transient boolean end;\n+\t\tprivate transient long emitted;\n+\t\t// reuse object for per record\n+\t\tprivate transient GenericRowData rowData;\n+\n+\t\tpublic JsonFileSystemFormat(\n+\t\t\t\tPath[] filePaths,\n+\t\t\t\tList<DataType> fieldTypes,\n+\t\t\t\tList<String> fieldNames,\n+\t\t\t\tint[] selectFields,\n+\t\t\t\tList<String> partitionKeys,\n+\t\t\t\tString defaultPartValue,\n+\t\t\t\tlong limit,\n+\t\t\t\tList<String> jsonSelectFieldNames,\n+\t\t\t\tint[] jsonFieldMapping,\n+\t\t\t\tJsonRowDataDeserializationSchema deserializationSchema) {\n+\t\t\tsuper.setFilePaths(filePaths);\n+\t\t\tthis.fieldTypes = fieldTypes;\n+\t\t\tthis.fieldNames = fieldNames;\n+\t\t\tthis.selectFields = selectFields;\n+\t\t\tthis.partitionKeys = partitionKeys;\n+\t\t\tthis.defaultPartValue = defaultPartValue;\n+\t\t\tthis.limit = limit;\n+\t\t\tthis.jsonSelectFieldNames = jsonSelectFieldNames;\n+\t\t\tthis.jsonFieldMapping = jsonFieldMapping;\n+\t\t\tthis.deserializationSchema = deserializationSchema;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void configure(Configuration parameters) {\n+\t\t\tsuper.configure(parameters);\n+\n+\t\t\tif (charsetName == null || !Charset.isSupported(charsetName)) {\n+\t\t\t\tthrow new RuntimeException(\"Unsupported charset: \" + charsetName);\n+\t\t\t}\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic boolean supportsMultiPaths() {\n+\t\t\treturn true;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void open(FileInputSplit split) throws IOException {\n+\t\t\tsuper.open(split);\n+\t\t\tthis.end = false;\n+\t\t\tfillPartitionValueForRecord();\n+\t\t\tthis.emitted = 0L;\n+\t\t}\n+\n+\t\tprivate void fillPartitionValueForRecord() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c3958066f46822218c29a10c7bad1d4d699949f8"}, "originalPosition": 232}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYyNzM5OTgzOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-common/src/main/java/org/apache/flink/table/utils/PartitionPathUtils.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOFQwODo1MTozMFrOGSeekQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOFQwODo1MTozMFrOGSeekQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjAyNjg5Nw==", "bodyText": "convertStringToInternalValue?", "url": "https://github.com/apache/flink/pull/12010#discussion_r422026897", "createdAt": "2020-05-08T08:51:30Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-common/src/main/java/org/apache/flink/table/utils/PartitionPathUtils.java", "diffHunk": "@@ -201,6 +210,50 @@ public static String unescapePathName(String path) {\n \t\treturn ret;\n \t}\n \n+\t/**\n+\t * Restore partition value from string and type.\n+\t *\n+\t * @param valStr string partition value.\n+\t * @param type type of partition field.\n+\t * @return partition value.\n+\t */\n+\tpublic static Object restorePartValueFromType(String valStr, DataType type) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c3958066f46822218c29a10c7bad1d4d699949f8"}, "originalPosition": 45}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYyNzQwMDcwOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-common/src/main/java/org/apache/flink/table/utils/PartitionPathUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOFQwODo1MTo0N1rOGSefEg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOFQxMzoxMTozMFrOGSlA1w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjAyNzAyNg==", "bodyText": "why floatValue ?", "url": "https://github.com/apache/flink/pull/12010#discussion_r422027026", "createdAt": "2020-05-08T08:51:47Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-common/src/main/java/org/apache/flink/table/utils/PartitionPathUtils.java", "diffHunk": "@@ -201,6 +210,50 @@ public static String unescapePathName(String path) {\n \t\treturn ret;\n \t}\n \n+\t/**\n+\t * Restore partition value from string and type.\n+\t *\n+\t * @param valStr string partition value.\n+\t * @param type type of partition field.\n+\t * @return partition value.\n+\t */\n+\tpublic static Object restorePartValueFromType(String valStr, DataType type) {\n+\t\tif (valStr == null) {\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\tLogicalTypeRoot typeRoot = type.getLogicalType().getTypeRoot();\n+\t\tswitch (typeRoot) {\n+\t\t\tcase CHAR:\n+\t\t\tcase VARCHAR:\n+\t\t\t\treturn StringData.fromString(valStr);\n+\t\t\tcase BOOLEAN:\n+\t\t\t\treturn Boolean.parseBoolean(valStr);\n+\t\t\tcase TINYINT:\n+\t\t\t\treturn Byte.parseByte(valStr);\n+\t\t\tcase SMALLINT:\n+\t\t\t\treturn Short.parseShort(valStr);\n+\t\t\tcase INTEGER:\n+\t\t\t\treturn Integer.parseInt(valStr);\n+\t\t\tcase BIGINT:\n+\t\t\t\treturn Long.parseLong(valStr);\n+\t\t\tcase FLOAT:\n+\t\t\t\treturn Float.valueOf(valStr).floatValue();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c3958066f46822218c29a10c7bad1d4d699949f8"}, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjEzMzk3NQ==", "bodyText": "will use Float.parseFloat() too", "url": "https://github.com/apache/flink/pull/12010#discussion_r422133975", "createdAt": "2020-05-08T13:11:30Z", "author": {"login": "leonardBang"}, "path": "flink-table/flink-table-common/src/main/java/org/apache/flink/table/utils/PartitionPathUtils.java", "diffHunk": "@@ -201,6 +210,50 @@ public static String unescapePathName(String path) {\n \t\treturn ret;\n \t}\n \n+\t/**\n+\t * Restore partition value from string and type.\n+\t *\n+\t * @param valStr string partition value.\n+\t * @param type type of partition field.\n+\t * @return partition value.\n+\t */\n+\tpublic static Object restorePartValueFromType(String valStr, DataType type) {\n+\t\tif (valStr == null) {\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\tLogicalTypeRoot typeRoot = type.getLogicalType().getTypeRoot();\n+\t\tswitch (typeRoot) {\n+\t\t\tcase CHAR:\n+\t\t\tcase VARCHAR:\n+\t\t\t\treturn StringData.fromString(valStr);\n+\t\t\tcase BOOLEAN:\n+\t\t\t\treturn Boolean.parseBoolean(valStr);\n+\t\t\tcase TINYINT:\n+\t\t\t\treturn Byte.parseByte(valStr);\n+\t\t\tcase SMALLINT:\n+\t\t\t\treturn Short.parseShort(valStr);\n+\t\t\tcase INTEGER:\n+\t\t\t\treturn Integer.parseInt(valStr);\n+\t\t\tcase BIGINT:\n+\t\t\t\treturn Long.parseLong(valStr);\n+\t\t\tcase FLOAT:\n+\t\t\t\treturn Float.valueOf(valStr).floatValue();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjAyNzAyNg=="}, "originalCommit": {"oid": "c3958066f46822218c29a10c7bad1d4d699949f8"}, "originalPosition": 66}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYyNzQwNTk0OnYy", "diffSide": "RIGHT", "path": "flink-formats/flink-json/src/test/java/org/apache/flink/formats/json/FsBatchJsonSinkITCase.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOFQwODo1Mzo0NVrOGSeinQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOFQwODo1Mzo0NVrOGSeinQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjAyNzkzMw==", "bodyText": "JsonBatchFileSystemITCase?", "url": "https://github.com/apache/flink/pull/12010#discussion_r422027933", "createdAt": "2020-05-08T08:53:45Z", "author": {"login": "JingsongLi"}, "path": "flink-formats/flink-json/src/test/java/org/apache/flink/formats/json/FsBatchJsonSinkITCase.java", "diffHunk": "@@ -0,0 +1,40 @@\n+package org.apache.flink.formats.json;\n+\n+import org.apache.flink.table.planner.runtime.batch.sql.BatchFileSystemITCaseBase;\n+\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.List;\n+\n+/**\n+ * ITCase to test json format for {@link JsonFileSystemFormatFactory}.\n+ */\n+@RunWith(Parameterized.class)\n+public class FsBatchJsonSinkITCase extends BatchFileSystemITCaseBase {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c3958066f46822218c29a10c7bad1d4d699949f8"}, "originalPosition": 17}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYyNzQwNjU3OnYy", "diffSide": "RIGHT", "path": "flink-formats/flink-json/src/test/java/org/apache/flink/formats/json/FsStreamJsonSinkITCase.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOFQwODo1Mzo1OFrOGSei-A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOFQwODo1Mzo1OFrOGSei-A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjAyODAyNA==", "bodyText": "JsonFsStreamSinkITCase", "url": "https://github.com/apache/flink/pull/12010#discussion_r422028024", "createdAt": "2020-05-08T08:53:58Z", "author": {"login": "JingsongLi"}, "path": "flink-formats/flink-json/src/test/java/org/apache/flink/formats/json/FsStreamJsonSinkITCase.java", "diffHunk": "@@ -0,0 +1,39 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.json;\n+\n+import org.apache.flink.table.planner.runtime.stream.FsStreamingSinkITCaseBase;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Test checkpoint for file system table factory with json format.\n+ */\n+public class FsStreamJsonSinkITCase extends FsStreamingSinkITCaseBase {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c3958066f46822218c29a10c7bad1d4d699949f8"}, "originalPosition": 29}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYyNzQwODc1OnYy", "diffSide": "RIGHT", "path": "flink-formats/flink-json/src/main/java/org/apache/flink/formats/json/JsonFileSystemFormatFactory.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOFQwODo1NDo0MVrOGSekSA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOFQxMjo0NjowOVrOGSkTmQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjAyODM2MA==", "bodyText": "What is this for?", "url": "https://github.com/apache/flink/pull/12010#discussion_r422028360", "createdAt": "2020-05-08T08:54:41Z", "author": {"login": "JingsongLi"}, "path": "flink-formats/flink-json/src/main/java/org/apache/flink/formats/json/JsonFileSystemFormatFactory.java", "diffHunk": "@@ -0,0 +1,293 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.json;\n+\n+import org.apache.flink.api.common.io.DelimitedInputFormat;\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.api.java.typeutils.GenericTypeInfo;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.FileSystemFormatFactory;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.utils.PartitionPathUtils;\n+\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.nio.charset.Charset;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+import static org.apache.flink.table.descriptors.JsonValidator.FORMAT_FAIL_ON_MISSING_FIELD;\n+import static org.apache.flink.table.descriptors.JsonValidator.FORMAT_IGNORE_PARSE_ERRORS;\n+\n+/**\n+ * Factory to build reader/writer to read/write json format file.\n+ */\n+public class JsonFileSystemFormatFactory implements FileSystemFormatFactory {\n+\t@Override\n+\tpublic Map<String, String> requiredContext() {\n+\t\tMap<String, String> context = new HashMap<>();\n+\t\tcontext.put(FORMAT, \"json\");\n+\t\treturn context;\n+\t}\n+\n+\t@Override\n+\tpublic List<String> supportedProperties() {\n+\t\tArrayList<String> properties = new ArrayList<>();\n+\t\tproperties.add(FORMAT_FAIL_ON_MISSING_FIELD);\n+\t\tproperties.add(FORMAT_IGNORE_PARSE_ERRORS);\n+\t\treturn properties;\n+\t}\n+\n+\t@Override\n+\tpublic InputFormat<RowData, ?> createReader(ReaderContext context) {\n+\t\tDescriptorProperties properties = getValidatedProperties(context.getFormatProperties());\n+\t\tboolean failOnMissingField = properties.getOptionalBoolean(FORMAT_FAIL_ON_MISSING_FIELD).orElse(false);\n+\t\tboolean ignoreParseErrors = properties.getOptionalBoolean(FORMAT_IGNORE_PARSE_ERRORS).orElse(false);\n+\n+\t\t// deal partition fields\n+\t\tTableSchema tableSchema = context.getSchema();\n+\t\tList<DataType> fieldTypes = Arrays.asList(tableSchema.getFieldDataTypes());\n+\t\tList<String> fieldNames = Arrays.asList(tableSchema.getFieldNames());\n+\t\tString[] nonPartFieldNames = fieldNames.stream()\n+\t\t\t.filter(name -> !context.getPartitionKeys().contains(name)).toArray(String[]::new);\n+\t\tDataType[] nonPartFieldTypes = Arrays.asList(nonPartFieldNames).stream()\n+\t\t\t.map(name -> fieldTypes.get(fieldNames.indexOf(name))).toArray(DataType[]::new);\n+\t\tRowType nonPartRowType = RowType.of(\n+\t\t\tArrays.asList(nonPartFieldTypes).stream()\n+\t\t\t\t.map(DataType::getLogicalType)\n+\t\t\t\t.toArray(LogicalType[]::new),\n+\t\t\tnonPartFieldNames);\n+\n+\t\t// deal project fields\n+\t\tint[] selectFields = context.getProjectFields();\n+\t\tList<String> partitionKeys = context.getPartitionKeys();\n+\t\tList<String> selectFieldNames = Arrays.stream(selectFields)\n+\t\t\t.mapToObj(fieldNames::get)\n+\t\t\t.collect(Collectors.toList());\n+\t\tList<String> jsonSelectFieldNames = selectFieldNames.stream()\n+\t\t\t.filter(name -> !partitionKeys.contains(name)).collect(Collectors.toList());\n+\t\tint[] jsonFieldMapping =  jsonSelectFieldNames.stream().mapToInt(selectFieldNames::indexOf).toArray();\n+\n+\t\tJsonRowDataDeserializationSchema  deserializationSchema = new JsonRowDataDeserializationSchema(\n+\t\t\tnonPartRowType,\n+\t\t\tnew GenericTypeInfo(GenericRowData.class),\n+\t\t\tfailOnMissingField,\n+\t\t\tignoreParseErrors);\n+\n+\t\treturn new JsonFileSystemFormat(\n+\t\t\tcontext.getPaths(),\n+\t\t\tfieldTypes,\n+\t\t\tfieldNames,\n+\t\t\tselectFields,\n+\t\t\tpartitionKeys,\n+\t\t\tcontext.getDefaultPartName(),\n+\t\t\tcontext.getPushedDownLimit(),\n+\t\t\tjsonSelectFieldNames,\n+\t\t\tjsonFieldMapping,\n+\t\t\tdeserializationSchema);\n+\t}\n+\n+\t@Override\n+\tpublic Optional<Encoder<RowData>> createEncoder(WriterContext context) {\n+\t\tTableSchema nonPartKeySchema = new TableSchema.Builder()\n+\t\t\t.fields(context.getFieldNamesWithoutPartKeys(), context.getFieldTypesWithoutPartKeys())\n+\t\t\t.build();\n+\t\tLogicalType[] logicalTypes = Arrays.asList(nonPartKeySchema.getFieldDataTypes())\n+\t\t\t.stream().map(DataType::getLogicalType).toArray(LogicalType[]::new);\n+\t\tRowType nonPartRowType = RowType.of(logicalTypes, nonPartKeySchema.getFieldNames());\n+\t\treturn Optional.of(new JsonRowDataEncoder(new JsonRowDataSerializationSchema(nonPartRowType)));\n+\t}\n+\n+\t@Override\n+\tpublic Optional<BulkWriter.Factory<RowData>> createBulkWriterFactory(WriterContext context) {\n+\t\treturn Optional.empty();\n+\t}\n+\n+\t@Override\n+\tpublic boolean supportsSchemaDerivation() {\n+\t\treturn true;\n+\t}\n+\n+\tprivate static DescriptorProperties getValidatedProperties(Map<String, String> propertiesMap) {\n+\t\tfinal DescriptorProperties properties = new DescriptorProperties(true);\n+\t\tproperties.putProperties(propertiesMap);\n+\t\tproperties.validateBoolean(FORMAT_FAIL_ON_MISSING_FIELD, true);\n+\t\tproperties.validateBoolean(FORMAT_IGNORE_PARSE_ERRORS, true);\n+\t\treturn properties;\n+\t}\n+\n+\t/**\n+\t * A {@link JsonFileSystemFormat} is responsible to read {@link RowData} records\n+\t * from json format files.\n+\t */\n+\tpublic static class JsonFileSystemFormat extends DelimitedInputFormat<RowData> {\n+\t\t/**\n+\t\t * Code of \\r, used to remove \\r from a line when the line ends with \\r\\n.\n+\t\t */\n+\t\tprivate static final byte CARRIAGE_RETURN = (byte) '\\r';\n+\n+\t\t/**\n+\t\t * Code of \\n, used to identify if \\n is used as delimiter.\n+\t\t */\n+\t\tprivate static final byte NEW_LINE = (byte) '\\n';\n+\n+\t\t/**\n+\t\t * The name of the charset to use for decoding.\n+\t\t */\n+\t\tprivate String charsetName = \"UTF-8\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c3958066f46822218c29a10c7bad1d4d699949f8"}, "originalPosition": 170}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjEyMjM5Mw==", "bodyText": "I wanted to construct a transient Charset, but now it don't need any more after use StandardCharsets.UTF_8", "url": "https://github.com/apache/flink/pull/12010#discussion_r422122393", "createdAt": "2020-05-08T12:46:09Z", "author": {"login": "leonardBang"}, "path": "flink-formats/flink-json/src/main/java/org/apache/flink/formats/json/JsonFileSystemFormatFactory.java", "diffHunk": "@@ -0,0 +1,293 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.json;\n+\n+import org.apache.flink.api.common.io.DelimitedInputFormat;\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.api.java.typeutils.GenericTypeInfo;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.FileSystemFormatFactory;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.utils.PartitionPathUtils;\n+\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.nio.charset.Charset;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+import static org.apache.flink.table.descriptors.JsonValidator.FORMAT_FAIL_ON_MISSING_FIELD;\n+import static org.apache.flink.table.descriptors.JsonValidator.FORMAT_IGNORE_PARSE_ERRORS;\n+\n+/**\n+ * Factory to build reader/writer to read/write json format file.\n+ */\n+public class JsonFileSystemFormatFactory implements FileSystemFormatFactory {\n+\t@Override\n+\tpublic Map<String, String> requiredContext() {\n+\t\tMap<String, String> context = new HashMap<>();\n+\t\tcontext.put(FORMAT, \"json\");\n+\t\treturn context;\n+\t}\n+\n+\t@Override\n+\tpublic List<String> supportedProperties() {\n+\t\tArrayList<String> properties = new ArrayList<>();\n+\t\tproperties.add(FORMAT_FAIL_ON_MISSING_FIELD);\n+\t\tproperties.add(FORMAT_IGNORE_PARSE_ERRORS);\n+\t\treturn properties;\n+\t}\n+\n+\t@Override\n+\tpublic InputFormat<RowData, ?> createReader(ReaderContext context) {\n+\t\tDescriptorProperties properties = getValidatedProperties(context.getFormatProperties());\n+\t\tboolean failOnMissingField = properties.getOptionalBoolean(FORMAT_FAIL_ON_MISSING_FIELD).orElse(false);\n+\t\tboolean ignoreParseErrors = properties.getOptionalBoolean(FORMAT_IGNORE_PARSE_ERRORS).orElse(false);\n+\n+\t\t// deal partition fields\n+\t\tTableSchema tableSchema = context.getSchema();\n+\t\tList<DataType> fieldTypes = Arrays.asList(tableSchema.getFieldDataTypes());\n+\t\tList<String> fieldNames = Arrays.asList(tableSchema.getFieldNames());\n+\t\tString[] nonPartFieldNames = fieldNames.stream()\n+\t\t\t.filter(name -> !context.getPartitionKeys().contains(name)).toArray(String[]::new);\n+\t\tDataType[] nonPartFieldTypes = Arrays.asList(nonPartFieldNames).stream()\n+\t\t\t.map(name -> fieldTypes.get(fieldNames.indexOf(name))).toArray(DataType[]::new);\n+\t\tRowType nonPartRowType = RowType.of(\n+\t\t\tArrays.asList(nonPartFieldTypes).stream()\n+\t\t\t\t.map(DataType::getLogicalType)\n+\t\t\t\t.toArray(LogicalType[]::new),\n+\t\t\tnonPartFieldNames);\n+\n+\t\t// deal project fields\n+\t\tint[] selectFields = context.getProjectFields();\n+\t\tList<String> partitionKeys = context.getPartitionKeys();\n+\t\tList<String> selectFieldNames = Arrays.stream(selectFields)\n+\t\t\t.mapToObj(fieldNames::get)\n+\t\t\t.collect(Collectors.toList());\n+\t\tList<String> jsonSelectFieldNames = selectFieldNames.stream()\n+\t\t\t.filter(name -> !partitionKeys.contains(name)).collect(Collectors.toList());\n+\t\tint[] jsonFieldMapping =  jsonSelectFieldNames.stream().mapToInt(selectFieldNames::indexOf).toArray();\n+\n+\t\tJsonRowDataDeserializationSchema  deserializationSchema = new JsonRowDataDeserializationSchema(\n+\t\t\tnonPartRowType,\n+\t\t\tnew GenericTypeInfo(GenericRowData.class),\n+\t\t\tfailOnMissingField,\n+\t\t\tignoreParseErrors);\n+\n+\t\treturn new JsonFileSystemFormat(\n+\t\t\tcontext.getPaths(),\n+\t\t\tfieldTypes,\n+\t\t\tfieldNames,\n+\t\t\tselectFields,\n+\t\t\tpartitionKeys,\n+\t\t\tcontext.getDefaultPartName(),\n+\t\t\tcontext.getPushedDownLimit(),\n+\t\t\tjsonSelectFieldNames,\n+\t\t\tjsonFieldMapping,\n+\t\t\tdeserializationSchema);\n+\t}\n+\n+\t@Override\n+\tpublic Optional<Encoder<RowData>> createEncoder(WriterContext context) {\n+\t\tTableSchema nonPartKeySchema = new TableSchema.Builder()\n+\t\t\t.fields(context.getFieldNamesWithoutPartKeys(), context.getFieldTypesWithoutPartKeys())\n+\t\t\t.build();\n+\t\tLogicalType[] logicalTypes = Arrays.asList(nonPartKeySchema.getFieldDataTypes())\n+\t\t\t.stream().map(DataType::getLogicalType).toArray(LogicalType[]::new);\n+\t\tRowType nonPartRowType = RowType.of(logicalTypes, nonPartKeySchema.getFieldNames());\n+\t\treturn Optional.of(new JsonRowDataEncoder(new JsonRowDataSerializationSchema(nonPartRowType)));\n+\t}\n+\n+\t@Override\n+\tpublic Optional<BulkWriter.Factory<RowData>> createBulkWriterFactory(WriterContext context) {\n+\t\treturn Optional.empty();\n+\t}\n+\n+\t@Override\n+\tpublic boolean supportsSchemaDerivation() {\n+\t\treturn true;\n+\t}\n+\n+\tprivate static DescriptorProperties getValidatedProperties(Map<String, String> propertiesMap) {\n+\t\tfinal DescriptorProperties properties = new DescriptorProperties(true);\n+\t\tproperties.putProperties(propertiesMap);\n+\t\tproperties.validateBoolean(FORMAT_FAIL_ON_MISSING_FIELD, true);\n+\t\tproperties.validateBoolean(FORMAT_IGNORE_PARSE_ERRORS, true);\n+\t\treturn properties;\n+\t}\n+\n+\t/**\n+\t * A {@link JsonFileSystemFormat} is responsible to read {@link RowData} records\n+\t * from json format files.\n+\t */\n+\tpublic static class JsonFileSystemFormat extends DelimitedInputFormat<RowData> {\n+\t\t/**\n+\t\t * Code of \\r, used to remove \\r from a line when the line ends with \\r\\n.\n+\t\t */\n+\t\tprivate static final byte CARRIAGE_RETURN = (byte) '\\r';\n+\n+\t\t/**\n+\t\t * Code of \\n, used to identify if \\n is used as delimiter.\n+\t\t */\n+\t\tprivate static final byte NEW_LINE = (byte) '\\n';\n+\n+\t\t/**\n+\t\t * The name of the charset to use for decoding.\n+\t\t */\n+\t\tprivate String charsetName = \"UTF-8\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjAyODM2MA=="}, "originalCommit": {"oid": "c3958066f46822218c29a10c7bad1d4d699949f8"}, "originalPosition": 170}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYyNzQxMDU5OnYy", "diffSide": "RIGHT", "path": "flink-formats/flink-json/src/main/java/org/apache/flink/formats/json/JsonFileSystemFormatFactory.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOFQwODo1NToxOFrOGSelYQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOFQwODo1NToxOFrOGSelYQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjAyODY0MQ==", "bodyText": "JsonInputFormat", "url": "https://github.com/apache/flink/pull/12010#discussion_r422028641", "createdAt": "2020-05-08T08:55:18Z", "author": {"login": "JingsongLi"}, "path": "flink-formats/flink-json/src/main/java/org/apache/flink/formats/json/JsonFileSystemFormatFactory.java", "diffHunk": "@@ -0,0 +1,293 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.json;\n+\n+import org.apache.flink.api.common.io.DelimitedInputFormat;\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.api.java.typeutils.GenericTypeInfo;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.FileSystemFormatFactory;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.utils.PartitionPathUtils;\n+\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.nio.charset.Charset;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+import static org.apache.flink.table.descriptors.JsonValidator.FORMAT_FAIL_ON_MISSING_FIELD;\n+import static org.apache.flink.table.descriptors.JsonValidator.FORMAT_IGNORE_PARSE_ERRORS;\n+\n+/**\n+ * Factory to build reader/writer to read/write json format file.\n+ */\n+public class JsonFileSystemFormatFactory implements FileSystemFormatFactory {\n+\t@Override\n+\tpublic Map<String, String> requiredContext() {\n+\t\tMap<String, String> context = new HashMap<>();\n+\t\tcontext.put(FORMAT, \"json\");\n+\t\treturn context;\n+\t}\n+\n+\t@Override\n+\tpublic List<String> supportedProperties() {\n+\t\tArrayList<String> properties = new ArrayList<>();\n+\t\tproperties.add(FORMAT_FAIL_ON_MISSING_FIELD);\n+\t\tproperties.add(FORMAT_IGNORE_PARSE_ERRORS);\n+\t\treturn properties;\n+\t}\n+\n+\t@Override\n+\tpublic InputFormat<RowData, ?> createReader(ReaderContext context) {\n+\t\tDescriptorProperties properties = getValidatedProperties(context.getFormatProperties());\n+\t\tboolean failOnMissingField = properties.getOptionalBoolean(FORMAT_FAIL_ON_MISSING_FIELD).orElse(false);\n+\t\tboolean ignoreParseErrors = properties.getOptionalBoolean(FORMAT_IGNORE_PARSE_ERRORS).orElse(false);\n+\n+\t\t// deal partition fields\n+\t\tTableSchema tableSchema = context.getSchema();\n+\t\tList<DataType> fieldTypes = Arrays.asList(tableSchema.getFieldDataTypes());\n+\t\tList<String> fieldNames = Arrays.asList(tableSchema.getFieldNames());\n+\t\tString[] nonPartFieldNames = fieldNames.stream()\n+\t\t\t.filter(name -> !context.getPartitionKeys().contains(name)).toArray(String[]::new);\n+\t\tDataType[] nonPartFieldTypes = Arrays.asList(nonPartFieldNames).stream()\n+\t\t\t.map(name -> fieldTypes.get(fieldNames.indexOf(name))).toArray(DataType[]::new);\n+\t\tRowType nonPartRowType = RowType.of(\n+\t\t\tArrays.asList(nonPartFieldTypes).stream()\n+\t\t\t\t.map(DataType::getLogicalType)\n+\t\t\t\t.toArray(LogicalType[]::new),\n+\t\t\tnonPartFieldNames);\n+\n+\t\t// deal project fields\n+\t\tint[] selectFields = context.getProjectFields();\n+\t\tList<String> partitionKeys = context.getPartitionKeys();\n+\t\tList<String> selectFieldNames = Arrays.stream(selectFields)\n+\t\t\t.mapToObj(fieldNames::get)\n+\t\t\t.collect(Collectors.toList());\n+\t\tList<String> jsonSelectFieldNames = selectFieldNames.stream()\n+\t\t\t.filter(name -> !partitionKeys.contains(name)).collect(Collectors.toList());\n+\t\tint[] jsonFieldMapping =  jsonSelectFieldNames.stream().mapToInt(selectFieldNames::indexOf).toArray();\n+\n+\t\tJsonRowDataDeserializationSchema  deserializationSchema = new JsonRowDataDeserializationSchema(\n+\t\t\tnonPartRowType,\n+\t\t\tnew GenericTypeInfo(GenericRowData.class),\n+\t\t\tfailOnMissingField,\n+\t\t\tignoreParseErrors);\n+\n+\t\treturn new JsonFileSystemFormat(\n+\t\t\tcontext.getPaths(),\n+\t\t\tfieldTypes,\n+\t\t\tfieldNames,\n+\t\t\tselectFields,\n+\t\t\tpartitionKeys,\n+\t\t\tcontext.getDefaultPartName(),\n+\t\t\tcontext.getPushedDownLimit(),\n+\t\t\tjsonSelectFieldNames,\n+\t\t\tjsonFieldMapping,\n+\t\t\tdeserializationSchema);\n+\t}\n+\n+\t@Override\n+\tpublic Optional<Encoder<RowData>> createEncoder(WriterContext context) {\n+\t\tTableSchema nonPartKeySchema = new TableSchema.Builder()\n+\t\t\t.fields(context.getFieldNamesWithoutPartKeys(), context.getFieldTypesWithoutPartKeys())\n+\t\t\t.build();\n+\t\tLogicalType[] logicalTypes = Arrays.asList(nonPartKeySchema.getFieldDataTypes())\n+\t\t\t.stream().map(DataType::getLogicalType).toArray(LogicalType[]::new);\n+\t\tRowType nonPartRowType = RowType.of(logicalTypes, nonPartKeySchema.getFieldNames());\n+\t\treturn Optional.of(new JsonRowDataEncoder(new JsonRowDataSerializationSchema(nonPartRowType)));\n+\t}\n+\n+\t@Override\n+\tpublic Optional<BulkWriter.Factory<RowData>> createBulkWriterFactory(WriterContext context) {\n+\t\treturn Optional.empty();\n+\t}\n+\n+\t@Override\n+\tpublic boolean supportsSchemaDerivation() {\n+\t\treturn true;\n+\t}\n+\n+\tprivate static DescriptorProperties getValidatedProperties(Map<String, String> propertiesMap) {\n+\t\tfinal DescriptorProperties properties = new DescriptorProperties(true);\n+\t\tproperties.putProperties(propertiesMap);\n+\t\tproperties.validateBoolean(FORMAT_FAIL_ON_MISSING_FIELD, true);\n+\t\tproperties.validateBoolean(FORMAT_IGNORE_PARSE_ERRORS, true);\n+\t\treturn properties;\n+\t}\n+\n+\t/**\n+\t * A {@link JsonFileSystemFormat} is responsible to read {@link RowData} records\n+\t * from json format files.\n+\t */\n+\tpublic static class JsonFileSystemFormat extends DelimitedInputFormat<RowData> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c3958066f46822218c29a10c7bad1d4d699949f8"}, "originalPosition": 156}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMDA2NzcwOnYy", "diffSide": "RIGHT", "path": "flink-formats/flink-json/src/main/java/org/apache/flink/formats/json/JsonFileSystemFormatFactory.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQwMTo1MzowOVrOGS3rmA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQwMTo1MzowOVrOGS3rmA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQzOTgzMg==", "bodyText": "It is better to left an empty line in the first of class.", "url": "https://github.com/apache/flink/pull/12010#discussion_r422439832", "createdAt": "2020-05-09T01:53:09Z", "author": {"login": "JingsongLi"}, "path": "flink-formats/flink-json/src/main/java/org/apache/flink/formats/json/JsonFileSystemFormatFactory.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.json;\n+\n+import org.apache.flink.api.common.io.DelimitedInputFormat;\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.api.java.typeutils.GenericTypeInfo;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.FileSystemFormatFactory;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.utils.PartitionPathUtils;\n+\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+import static org.apache.flink.table.descriptors.JsonValidator.FORMAT_FAIL_ON_MISSING_FIELD;\n+import static org.apache.flink.table.descriptors.JsonValidator.FORMAT_IGNORE_PARSE_ERRORS;\n+\n+/**\n+ * Factory to build reader/writer to read/write json format file.\n+ */\n+public class JsonFileSystemFormatFactory implements FileSystemFormatFactory {\n+\t@Override", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e13ca920f02c00cfc883bc55499d7c179b7a8564"}, "originalPosition": 57}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMDA2ODUzOnYy", "diffSide": "RIGHT", "path": "flink-formats/flink-json/src/main/java/org/apache/flink/formats/json/JsonFileSystemFormatFactory.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQwMTo1NDo0N1rOGS3sCQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQwMTo1NDo0N1rOGS3sCQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQzOTk0NQ==", "bodyText": "Don't need a table schema, Just:\nLogicalType[] logicalTypes = Arrays.stream(context.getFieldTypesWithoutPartKeys())\n\t\t\t\t.map(DataType::getLogicalType)\n\t\t\t\t.toArray(LogicalType[]::new);\n\t\tRowType nonPartRowType = RowType.of(logicalTypes, context.getFieldNamesWithoutPartKeys());\n\nAnd please add a method into WriterContext. So we can reuse this in csv.", "url": "https://github.com/apache/flink/pull/12010#discussion_r422439945", "createdAt": "2020-05-09T01:54:47Z", "author": {"login": "JingsongLi"}, "path": "flink-formats/flink-json/src/main/java/org/apache/flink/formats/json/JsonFileSystemFormatFactory.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.json;\n+\n+import org.apache.flink.api.common.io.DelimitedInputFormat;\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.api.java.typeutils.GenericTypeInfo;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.FileSystemFormatFactory;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.utils.PartitionPathUtils;\n+\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+import static org.apache.flink.table.descriptors.JsonValidator.FORMAT_FAIL_ON_MISSING_FIELD;\n+import static org.apache.flink.table.descriptors.JsonValidator.FORMAT_IGNORE_PARSE_ERRORS;\n+\n+/**\n+ * Factory to build reader/writer to read/write json format file.\n+ */\n+public class JsonFileSystemFormatFactory implements FileSystemFormatFactory {\n+\t@Override\n+\tpublic Map<String, String> requiredContext() {\n+\t\tMap<String, String> context = new HashMap<>();\n+\t\tcontext.put(FORMAT, \"json\");\n+\t\treturn context;\n+\t}\n+\n+\t@Override\n+\tpublic List<String> supportedProperties() {\n+\t\tArrayList<String> properties = new ArrayList<>();\n+\t\tproperties.add(FORMAT_FAIL_ON_MISSING_FIELD);\n+\t\tproperties.add(FORMAT_IGNORE_PARSE_ERRORS);\n+\t\treturn properties;\n+\t}\n+\n+\t@Override\n+\tpublic InputFormat<RowData, ?> createReader(ReaderContext context) {\n+\t\tDescriptorProperties properties = getValidatedProperties(context.getFormatProperties());\n+\t\tboolean failOnMissingField = properties.getOptionalBoolean(FORMAT_FAIL_ON_MISSING_FIELD).orElse(false);\n+\t\tboolean ignoreParseErrors = properties.getOptionalBoolean(FORMAT_IGNORE_PARSE_ERRORS).orElse(false);\n+\n+\t\t// deal partition fields\n+\t\tTableSchema tableSchema = context.getSchema();\n+\t\tList<DataType> fieldTypes = Arrays.asList(tableSchema.getFieldDataTypes());\n+\t\tList<String> fieldNames = Arrays.asList(tableSchema.getFieldNames());\n+\t\tString[] nonPartFieldNames = fieldNames.stream()\n+\t\t\t.filter(name -> !context.getPartitionKeys().contains(name)).toArray(String[]::new);\n+\t\tDataType[] nonPartFieldTypes = Arrays.asList(nonPartFieldNames).stream()\n+\t\t\t.map(name -> fieldTypes.get(fieldNames.indexOf(name))).toArray(DataType[]::new);\n+\t\tRowType nonPartRowType = RowType.of(\n+\t\t\tArrays.asList(nonPartFieldTypes).stream()\n+\t\t\t\t.map(DataType::getLogicalType)\n+\t\t\t\t.toArray(LogicalType[]::new),\n+\t\t\tnonPartFieldNames);\n+\n+\t\t// deal project fields\n+\t\tint[] selectFields = context.getProjectFields();\n+\t\tList<String> partitionKeys = context.getPartitionKeys();\n+\t\tList<String> selectFieldNames = Arrays.stream(selectFields)\n+\t\t\t.mapToObj(fieldNames::get)\n+\t\t\t.collect(Collectors.toList());\n+\t\tList<String> jsonSelectFieldNames = selectFieldNames.stream()\n+\t\t\t.filter(name -> !partitionKeys.contains(name)).collect(Collectors.toList());\n+\t\tint[] jsonFieldMapping =  jsonSelectFieldNames.stream().mapToInt(selectFieldNames::indexOf).toArray();\n+\n+\t\tJsonRowDataDeserializationSchema  deserializationSchema = new JsonRowDataDeserializationSchema(\n+\t\t\tnonPartRowType,\n+\t\t\tnew GenericTypeInfo(GenericRowData.class),\n+\t\t\tfailOnMissingField,\n+\t\t\tignoreParseErrors);\n+\n+\t\treturn new JsonInputFormat(\n+\t\t\tcontext.getPaths(),\n+\t\t\tfieldTypes,\n+\t\t\tfieldNames,\n+\t\t\tselectFields,\n+\t\t\tpartitionKeys,\n+\t\t\tcontext.getDefaultPartName(),\n+\t\t\tcontext.getPushedDownLimit(),\n+\t\t\tjsonSelectFieldNames,\n+\t\t\tjsonFieldMapping,\n+\t\t\tdeserializationSchema);\n+\t}\n+\n+\t@Override\n+\tpublic Optional<Encoder<RowData>> createEncoder(WriterContext context) {\n+\t\tTableSchema nonPartKeySchema = new TableSchema.Builder()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e13ca920f02c00cfc883bc55499d7c179b7a8564"}, "originalPosition": 123}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMDA3MDY0OnYy", "diffSide": "RIGHT", "path": "flink-formats/flink-json/src/main/java/org/apache/flink/formats/json/JsonFileSystemFormatFactory.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQwMTo1ODoyNFrOGS3tMg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQwMTo1ODoyNFrOGS3tMg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ0MDI0Mg==", "bodyText": "Notice to use Arrays.stream", "url": "https://github.com/apache/flink/pull/12010#discussion_r422440242", "createdAt": "2020-05-09T01:58:24Z", "author": {"login": "JingsongLi"}, "path": "flink-formats/flink-json/src/main/java/org/apache/flink/formats/json/JsonFileSystemFormatFactory.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.json;\n+\n+import org.apache.flink.api.common.io.DelimitedInputFormat;\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.api.java.typeutils.GenericTypeInfo;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.FileSystemFormatFactory;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.utils.PartitionPathUtils;\n+\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+import static org.apache.flink.table.descriptors.JsonValidator.FORMAT_FAIL_ON_MISSING_FIELD;\n+import static org.apache.flink.table.descriptors.JsonValidator.FORMAT_IGNORE_PARSE_ERRORS;\n+\n+/**\n+ * Factory to build reader/writer to read/write json format file.\n+ */\n+public class JsonFileSystemFormatFactory implements FileSystemFormatFactory {\n+\t@Override\n+\tpublic Map<String, String> requiredContext() {\n+\t\tMap<String, String> context = new HashMap<>();\n+\t\tcontext.put(FORMAT, \"json\");\n+\t\treturn context;\n+\t}\n+\n+\t@Override\n+\tpublic List<String> supportedProperties() {\n+\t\tArrayList<String> properties = new ArrayList<>();\n+\t\tproperties.add(FORMAT_FAIL_ON_MISSING_FIELD);\n+\t\tproperties.add(FORMAT_IGNORE_PARSE_ERRORS);\n+\t\treturn properties;\n+\t}\n+\n+\t@Override\n+\tpublic InputFormat<RowData, ?> createReader(ReaderContext context) {\n+\t\tDescriptorProperties properties = getValidatedProperties(context.getFormatProperties());\n+\t\tboolean failOnMissingField = properties.getOptionalBoolean(FORMAT_FAIL_ON_MISSING_FIELD).orElse(false);\n+\t\tboolean ignoreParseErrors = properties.getOptionalBoolean(FORMAT_IGNORE_PARSE_ERRORS).orElse(false);\n+\n+\t\t// deal partition fields\n+\t\tTableSchema tableSchema = context.getSchema();\n+\t\tList<DataType> fieldTypes = Arrays.asList(tableSchema.getFieldDataTypes());\n+\t\tList<String> fieldNames = Arrays.asList(tableSchema.getFieldNames());\n+\t\tString[] nonPartFieldNames = fieldNames.stream()\n+\t\t\t.filter(name -> !context.getPartitionKeys().contains(name)).toArray(String[]::new);\n+\t\tDataType[] nonPartFieldTypes = Arrays.asList(nonPartFieldNames).stream()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e13ca920f02c00cfc883bc55499d7c179b7a8564"}, "originalPosition": 84}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMDA3NDMxOnYy", "diffSide": "RIGHT", "path": "flink-formats/flink-json/src/main/java/org/apache/flink/formats/json/JsonFileSystemFormatFactory.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQwMjowNDoyMlrOGS3vIw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQwMjowNDoyMlrOGS3vIw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ0MDczOQ==", "bodyText": "Can you also add methods to ReaderContext:\ngetRowTypeWithoutPartKeys", "url": "https://github.com/apache/flink/pull/12010#discussion_r422440739", "createdAt": "2020-05-09T02:04:22Z", "author": {"login": "JingsongLi"}, "path": "flink-formats/flink-json/src/main/java/org/apache/flink/formats/json/JsonFileSystemFormatFactory.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.json;\n+\n+import org.apache.flink.api.common.io.DelimitedInputFormat;\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.api.java.typeutils.GenericTypeInfo;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.FileSystemFormatFactory;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.utils.PartitionPathUtils;\n+\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+import static org.apache.flink.table.descriptors.JsonValidator.FORMAT_FAIL_ON_MISSING_FIELD;\n+import static org.apache.flink.table.descriptors.JsonValidator.FORMAT_IGNORE_PARSE_ERRORS;\n+\n+/**\n+ * Factory to build reader/writer to read/write json format file.\n+ */\n+public class JsonFileSystemFormatFactory implements FileSystemFormatFactory {\n+\t@Override\n+\tpublic Map<String, String> requiredContext() {\n+\t\tMap<String, String> context = new HashMap<>();\n+\t\tcontext.put(FORMAT, \"json\");\n+\t\treturn context;\n+\t}\n+\n+\t@Override\n+\tpublic List<String> supportedProperties() {\n+\t\tArrayList<String> properties = new ArrayList<>();\n+\t\tproperties.add(FORMAT_FAIL_ON_MISSING_FIELD);\n+\t\tproperties.add(FORMAT_IGNORE_PARSE_ERRORS);\n+\t\treturn properties;\n+\t}\n+\n+\t@Override\n+\tpublic InputFormat<RowData, ?> createReader(ReaderContext context) {\n+\t\tDescriptorProperties properties = getValidatedProperties(context.getFormatProperties());\n+\t\tboolean failOnMissingField = properties.getOptionalBoolean(FORMAT_FAIL_ON_MISSING_FIELD).orElse(false);\n+\t\tboolean ignoreParseErrors = properties.getOptionalBoolean(FORMAT_IGNORE_PARSE_ERRORS).orElse(false);\n+\n+\t\t// deal partition fields\n+\t\tTableSchema tableSchema = context.getSchema();\n+\t\tList<DataType> fieldTypes = Arrays.asList(tableSchema.getFieldDataTypes());\n+\t\tList<String> fieldNames = Arrays.asList(tableSchema.getFieldNames());\n+\t\tString[] nonPartFieldNames = fieldNames.stream()\n+\t\t\t.filter(name -> !context.getPartitionKeys().contains(name)).toArray(String[]::new);\n+\t\tDataType[] nonPartFieldTypes = Arrays.asList(nonPartFieldNames).stream()\n+\t\t\t.map(name -> fieldTypes.get(fieldNames.indexOf(name))).toArray(DataType[]::new);\n+\t\tRowType nonPartRowType = RowType.of(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e13ca920f02c00cfc883bc55499d7c179b7a8564"}, "originalPosition": 86}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMDA3NTM1OnYy", "diffSide": "RIGHT", "path": "flink-formats/flink-json/src/main/java/org/apache/flink/formats/json/JsonFileSystemFormatFactory.java", "isResolved": true, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQwMjowNTo0OFrOGS3vrA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQwMzozNjozM1rOGS4MNw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ0MDg3Ng==", "bodyText": "jsonRow.getField? jsonSelectFieldNames useless?\nYou can add tests too.", "url": "https://github.com/apache/flink/pull/12010#discussion_r422440876", "createdAt": "2020-05-09T02:05:48Z", "author": {"login": "JingsongLi"}, "path": "flink-formats/flink-json/src/main/java/org/apache/flink/formats/json/JsonFileSystemFormatFactory.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.json;\n+\n+import org.apache.flink.api.common.io.DelimitedInputFormat;\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.api.java.typeutils.GenericTypeInfo;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.FileSystemFormatFactory;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.utils.PartitionPathUtils;\n+\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+import static org.apache.flink.table.descriptors.JsonValidator.FORMAT_FAIL_ON_MISSING_FIELD;\n+import static org.apache.flink.table.descriptors.JsonValidator.FORMAT_IGNORE_PARSE_ERRORS;\n+\n+/**\n+ * Factory to build reader/writer to read/write json format file.\n+ */\n+public class JsonFileSystemFormatFactory implements FileSystemFormatFactory {\n+\t@Override\n+\tpublic Map<String, String> requiredContext() {\n+\t\tMap<String, String> context = new HashMap<>();\n+\t\tcontext.put(FORMAT, \"json\");\n+\t\treturn context;\n+\t}\n+\n+\t@Override\n+\tpublic List<String> supportedProperties() {\n+\t\tArrayList<String> properties = new ArrayList<>();\n+\t\tproperties.add(FORMAT_FAIL_ON_MISSING_FIELD);\n+\t\tproperties.add(FORMAT_IGNORE_PARSE_ERRORS);\n+\t\treturn properties;\n+\t}\n+\n+\t@Override\n+\tpublic InputFormat<RowData, ?> createReader(ReaderContext context) {\n+\t\tDescriptorProperties properties = getValidatedProperties(context.getFormatProperties());\n+\t\tboolean failOnMissingField = properties.getOptionalBoolean(FORMAT_FAIL_ON_MISSING_FIELD).orElse(false);\n+\t\tboolean ignoreParseErrors = properties.getOptionalBoolean(FORMAT_IGNORE_PARSE_ERRORS).orElse(false);\n+\n+\t\t// deal partition fields\n+\t\tTableSchema tableSchema = context.getSchema();\n+\t\tList<DataType> fieldTypes = Arrays.asList(tableSchema.getFieldDataTypes());\n+\t\tList<String> fieldNames = Arrays.asList(tableSchema.getFieldNames());\n+\t\tString[] nonPartFieldNames = fieldNames.stream()\n+\t\t\t.filter(name -> !context.getPartitionKeys().contains(name)).toArray(String[]::new);\n+\t\tDataType[] nonPartFieldTypes = Arrays.asList(nonPartFieldNames).stream()\n+\t\t\t.map(name -> fieldTypes.get(fieldNames.indexOf(name))).toArray(DataType[]::new);\n+\t\tRowType nonPartRowType = RowType.of(\n+\t\t\tArrays.asList(nonPartFieldTypes).stream()\n+\t\t\t\t.map(DataType::getLogicalType)\n+\t\t\t\t.toArray(LogicalType[]::new),\n+\t\t\tnonPartFieldNames);\n+\n+\t\t// deal project fields\n+\t\tint[] selectFields = context.getProjectFields();\n+\t\tList<String> partitionKeys = context.getPartitionKeys();\n+\t\tList<String> selectFieldNames = Arrays.stream(selectFields)\n+\t\t\t.mapToObj(fieldNames::get)\n+\t\t\t.collect(Collectors.toList());\n+\t\tList<String> jsonSelectFieldNames = selectFieldNames.stream()\n+\t\t\t.filter(name -> !partitionKeys.contains(name)).collect(Collectors.toList());\n+\t\tint[] jsonFieldMapping =  jsonSelectFieldNames.stream().mapToInt(selectFieldNames::indexOf).toArray();\n+\n+\t\tJsonRowDataDeserializationSchema  deserializationSchema = new JsonRowDataDeserializationSchema(\n+\t\t\tnonPartRowType,\n+\t\t\tnew GenericTypeInfo(GenericRowData.class),\n+\t\t\tfailOnMissingField,\n+\t\t\tignoreParseErrors);\n+\n+\t\treturn new JsonInputFormat(\n+\t\t\tcontext.getPaths(),\n+\t\t\tfieldTypes,\n+\t\t\tfieldNames,\n+\t\t\tselectFields,\n+\t\t\tpartitionKeys,\n+\t\t\tcontext.getDefaultPartName(),\n+\t\t\tcontext.getPushedDownLimit(),\n+\t\t\tjsonSelectFieldNames,\n+\t\t\tjsonFieldMapping,\n+\t\t\tdeserializationSchema);\n+\t}\n+\n+\t@Override\n+\tpublic Optional<Encoder<RowData>> createEncoder(WriterContext context) {\n+\t\tTableSchema nonPartKeySchema = new TableSchema.Builder()\n+\t\t\t.fields(context.getFieldNamesWithoutPartKeys(), context.getFieldTypesWithoutPartKeys())\n+\t\t\t.build();\n+\t\tLogicalType[] logicalTypes = Arrays.asList(nonPartKeySchema.getFieldDataTypes())\n+\t\t\t.stream().map(DataType::getLogicalType).toArray(LogicalType[]::new);\n+\t\tRowType nonPartRowType = RowType.of(logicalTypes, nonPartKeySchema.getFieldNames());\n+\t\treturn Optional.of(new JsonRowDataEncoder(new JsonRowDataSerializationSchema(nonPartRowType)));\n+\t}\n+\n+\t@Override\n+\tpublic Optional<BulkWriter.Factory<RowData>> createBulkWriterFactory(WriterContext context) {\n+\t\treturn Optional.empty();\n+\t}\n+\n+\t@Override\n+\tpublic boolean supportsSchemaDerivation() {\n+\t\treturn true;\n+\t}\n+\n+\tprivate static DescriptorProperties getValidatedProperties(Map<String, String> propertiesMap) {\n+\t\tfinal DescriptorProperties properties = new DescriptorProperties(true);\n+\t\tproperties.putProperties(propertiesMap);\n+\t\tproperties.validateBoolean(FORMAT_FAIL_ON_MISSING_FIELD, true);\n+\t\tproperties.validateBoolean(FORMAT_IGNORE_PARSE_ERRORS, true);\n+\t\treturn properties;\n+\t}\n+\n+\t/**\n+\t * A {@link JsonInputFormat} is responsible to read {@link RowData} records\n+\t * from json format files.\n+\t */\n+\tpublic static class JsonInputFormat extends DelimitedInputFormat<RowData> {\n+\t\t/**\n+\t\t * Code of \\r, used to remove \\r from a line when the line ends with \\r\\n.\n+\t\t */\n+\t\tprivate static final byte CARRIAGE_RETURN = (byte) '\\r';\n+\n+\t\t/**\n+\t\t * Code of \\n, used to identify if \\n is used as delimiter.\n+\t\t */\n+\t\tprivate static final byte NEW_LINE = (byte) '\\n';\n+\n+\t\tprivate final List<DataType> fieldTypes;\n+\t\tprivate final List<String> fieldNames;\n+\t\tprivate final int[] selectFields;\n+\t\tprivate final List<String> partitionKeys;\n+\t\tprivate final String defaultPartValue;\n+\t\tprivate final long limit;\n+\t\tprivate final List<String> jsonSelectFieldNames;\n+\t\tprivate final int[] jsonFieldMapping;\n+\t\tprivate final JsonRowDataDeserializationSchema deserializationSchema;\n+\n+\t\tprivate transient boolean end;\n+\t\tprivate transient long emitted;\n+\t\t// reuse object for per record\n+\t\tprivate transient GenericRowData rowData;\n+\n+\t\tpublic JsonInputFormat(\n+\t\t\t\tPath[] filePaths,\n+\t\t\t\tList<DataType> fieldTypes,\n+\t\t\t\tList<String> fieldNames,\n+\t\t\t\tint[] selectFields,\n+\t\t\t\tList<String> partitionKeys,\n+\t\t\t\tString defaultPartValue,\n+\t\t\t\tlong limit,\n+\t\t\t\tList<String> jsonSelectFieldNames,\n+\t\t\t\tint[] jsonFieldMapping,\n+\t\t\t\tJsonRowDataDeserializationSchema deserializationSchema) {\n+\t\t\tsuper.setFilePaths(filePaths);\n+\t\t\tthis.fieldTypes = fieldTypes;\n+\t\t\tthis.fieldNames = fieldNames;\n+\t\t\tthis.selectFields = selectFields;\n+\t\t\tthis.partitionKeys = partitionKeys;\n+\t\t\tthis.defaultPartValue = defaultPartValue;\n+\t\t\tthis.limit = limit;\n+\t\t\tthis.jsonSelectFieldNames = jsonSelectFieldNames;\n+\t\t\tthis.jsonFieldMapping = jsonFieldMapping;\n+\t\t\tthis.deserializationSchema = deserializationSchema;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic boolean supportsMultiPaths() {\n+\t\t\treturn true;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void open(FileInputSplit split) throws IOException {\n+\t\t\tsuper.open(split);\n+\t\t\tthis.end = false;\n+\t\t\tthis.emitted = 0L;\n+\t\t\tthis.rowData = PartitionPathUtils.fillPartitionValueForRecord(fieldNames, fieldTypes, selectFields,\n+\t\t\t\tpartitionKeys, currentSplit.getPath(), defaultPartValue);\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic boolean reachedEnd() {\n+\t\t\treturn emitted >= limit || end;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic RowData readRecord(RowData reuse, byte[] bytes, int offset, int numBytes) throws IOException {\n+\t\t\tGenericRowData returnRecord = rowData;\n+\n+\t\t\tif (this.getDelimiter() != null && this.getDelimiter().length == 1\n+\t\t\t\t&& this.getDelimiter()[0] == NEW_LINE && offset + numBytes >= 1\n+\t\t\t\t&& bytes[offset + numBytes - 1] == CARRIAGE_RETURN){\n+\t\t\t\tnumBytes -= 1;\n+\t\t\t}\n+\t\t\tbyte[] trimBytes = Arrays.copyOfRange(bytes, offset, offset + numBytes);\n+\t\t\tGenericRowData jsonRow = (GenericRowData) deserializationSchema.deserialize(trimBytes);\n+\t\t\tif (jsonRow != null) {\n+\t\t\t\tfor (int i = 0; i < jsonSelectFieldNames.size(); i++) {\n+\t\t\t\t\treturnRecord.setField(jsonFieldMapping[i], jsonRow.getField(i));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e13ca920f02c00cfc883bc55499d7c179b7a8564"}, "originalPosition": 235}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ0NzU1Ng==", "bodyText": "jsonSelectFieldNames length may less than jsonRow.size()?", "url": "https://github.com/apache/flink/pull/12010#discussion_r422447556", "createdAt": "2020-05-09T03:28:09Z", "author": {"login": "leonardBang"}, "path": "flink-formats/flink-json/src/main/java/org/apache/flink/formats/json/JsonFileSystemFormatFactory.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.json;\n+\n+import org.apache.flink.api.common.io.DelimitedInputFormat;\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.api.java.typeutils.GenericTypeInfo;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.FileSystemFormatFactory;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.utils.PartitionPathUtils;\n+\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+import static org.apache.flink.table.descriptors.JsonValidator.FORMAT_FAIL_ON_MISSING_FIELD;\n+import static org.apache.flink.table.descriptors.JsonValidator.FORMAT_IGNORE_PARSE_ERRORS;\n+\n+/**\n+ * Factory to build reader/writer to read/write json format file.\n+ */\n+public class JsonFileSystemFormatFactory implements FileSystemFormatFactory {\n+\t@Override\n+\tpublic Map<String, String> requiredContext() {\n+\t\tMap<String, String> context = new HashMap<>();\n+\t\tcontext.put(FORMAT, \"json\");\n+\t\treturn context;\n+\t}\n+\n+\t@Override\n+\tpublic List<String> supportedProperties() {\n+\t\tArrayList<String> properties = new ArrayList<>();\n+\t\tproperties.add(FORMAT_FAIL_ON_MISSING_FIELD);\n+\t\tproperties.add(FORMAT_IGNORE_PARSE_ERRORS);\n+\t\treturn properties;\n+\t}\n+\n+\t@Override\n+\tpublic InputFormat<RowData, ?> createReader(ReaderContext context) {\n+\t\tDescriptorProperties properties = getValidatedProperties(context.getFormatProperties());\n+\t\tboolean failOnMissingField = properties.getOptionalBoolean(FORMAT_FAIL_ON_MISSING_FIELD).orElse(false);\n+\t\tboolean ignoreParseErrors = properties.getOptionalBoolean(FORMAT_IGNORE_PARSE_ERRORS).orElse(false);\n+\n+\t\t// deal partition fields\n+\t\tTableSchema tableSchema = context.getSchema();\n+\t\tList<DataType> fieldTypes = Arrays.asList(tableSchema.getFieldDataTypes());\n+\t\tList<String> fieldNames = Arrays.asList(tableSchema.getFieldNames());\n+\t\tString[] nonPartFieldNames = fieldNames.stream()\n+\t\t\t.filter(name -> !context.getPartitionKeys().contains(name)).toArray(String[]::new);\n+\t\tDataType[] nonPartFieldTypes = Arrays.asList(nonPartFieldNames).stream()\n+\t\t\t.map(name -> fieldTypes.get(fieldNames.indexOf(name))).toArray(DataType[]::new);\n+\t\tRowType nonPartRowType = RowType.of(\n+\t\t\tArrays.asList(nonPartFieldTypes).stream()\n+\t\t\t\t.map(DataType::getLogicalType)\n+\t\t\t\t.toArray(LogicalType[]::new),\n+\t\t\tnonPartFieldNames);\n+\n+\t\t// deal project fields\n+\t\tint[] selectFields = context.getProjectFields();\n+\t\tList<String> partitionKeys = context.getPartitionKeys();\n+\t\tList<String> selectFieldNames = Arrays.stream(selectFields)\n+\t\t\t.mapToObj(fieldNames::get)\n+\t\t\t.collect(Collectors.toList());\n+\t\tList<String> jsonSelectFieldNames = selectFieldNames.stream()\n+\t\t\t.filter(name -> !partitionKeys.contains(name)).collect(Collectors.toList());\n+\t\tint[] jsonFieldMapping =  jsonSelectFieldNames.stream().mapToInt(selectFieldNames::indexOf).toArray();\n+\n+\t\tJsonRowDataDeserializationSchema  deserializationSchema = new JsonRowDataDeserializationSchema(\n+\t\t\tnonPartRowType,\n+\t\t\tnew GenericTypeInfo(GenericRowData.class),\n+\t\t\tfailOnMissingField,\n+\t\t\tignoreParseErrors);\n+\n+\t\treturn new JsonInputFormat(\n+\t\t\tcontext.getPaths(),\n+\t\t\tfieldTypes,\n+\t\t\tfieldNames,\n+\t\t\tselectFields,\n+\t\t\tpartitionKeys,\n+\t\t\tcontext.getDefaultPartName(),\n+\t\t\tcontext.getPushedDownLimit(),\n+\t\t\tjsonSelectFieldNames,\n+\t\t\tjsonFieldMapping,\n+\t\t\tdeserializationSchema);\n+\t}\n+\n+\t@Override\n+\tpublic Optional<Encoder<RowData>> createEncoder(WriterContext context) {\n+\t\tTableSchema nonPartKeySchema = new TableSchema.Builder()\n+\t\t\t.fields(context.getFieldNamesWithoutPartKeys(), context.getFieldTypesWithoutPartKeys())\n+\t\t\t.build();\n+\t\tLogicalType[] logicalTypes = Arrays.asList(nonPartKeySchema.getFieldDataTypes())\n+\t\t\t.stream().map(DataType::getLogicalType).toArray(LogicalType[]::new);\n+\t\tRowType nonPartRowType = RowType.of(logicalTypes, nonPartKeySchema.getFieldNames());\n+\t\treturn Optional.of(new JsonRowDataEncoder(new JsonRowDataSerializationSchema(nonPartRowType)));\n+\t}\n+\n+\t@Override\n+\tpublic Optional<BulkWriter.Factory<RowData>> createBulkWriterFactory(WriterContext context) {\n+\t\treturn Optional.empty();\n+\t}\n+\n+\t@Override\n+\tpublic boolean supportsSchemaDerivation() {\n+\t\treturn true;\n+\t}\n+\n+\tprivate static DescriptorProperties getValidatedProperties(Map<String, String> propertiesMap) {\n+\t\tfinal DescriptorProperties properties = new DescriptorProperties(true);\n+\t\tproperties.putProperties(propertiesMap);\n+\t\tproperties.validateBoolean(FORMAT_FAIL_ON_MISSING_FIELD, true);\n+\t\tproperties.validateBoolean(FORMAT_IGNORE_PARSE_ERRORS, true);\n+\t\treturn properties;\n+\t}\n+\n+\t/**\n+\t * A {@link JsonInputFormat} is responsible to read {@link RowData} records\n+\t * from json format files.\n+\t */\n+\tpublic static class JsonInputFormat extends DelimitedInputFormat<RowData> {\n+\t\t/**\n+\t\t * Code of \\r, used to remove \\r from a line when the line ends with \\r\\n.\n+\t\t */\n+\t\tprivate static final byte CARRIAGE_RETURN = (byte) '\\r';\n+\n+\t\t/**\n+\t\t * Code of \\n, used to identify if \\n is used as delimiter.\n+\t\t */\n+\t\tprivate static final byte NEW_LINE = (byte) '\\n';\n+\n+\t\tprivate final List<DataType> fieldTypes;\n+\t\tprivate final List<String> fieldNames;\n+\t\tprivate final int[] selectFields;\n+\t\tprivate final List<String> partitionKeys;\n+\t\tprivate final String defaultPartValue;\n+\t\tprivate final long limit;\n+\t\tprivate final List<String> jsonSelectFieldNames;\n+\t\tprivate final int[] jsonFieldMapping;\n+\t\tprivate final JsonRowDataDeserializationSchema deserializationSchema;\n+\n+\t\tprivate transient boolean end;\n+\t\tprivate transient long emitted;\n+\t\t// reuse object for per record\n+\t\tprivate transient GenericRowData rowData;\n+\n+\t\tpublic JsonInputFormat(\n+\t\t\t\tPath[] filePaths,\n+\t\t\t\tList<DataType> fieldTypes,\n+\t\t\t\tList<String> fieldNames,\n+\t\t\t\tint[] selectFields,\n+\t\t\t\tList<String> partitionKeys,\n+\t\t\t\tString defaultPartValue,\n+\t\t\t\tlong limit,\n+\t\t\t\tList<String> jsonSelectFieldNames,\n+\t\t\t\tint[] jsonFieldMapping,\n+\t\t\t\tJsonRowDataDeserializationSchema deserializationSchema) {\n+\t\t\tsuper.setFilePaths(filePaths);\n+\t\t\tthis.fieldTypes = fieldTypes;\n+\t\t\tthis.fieldNames = fieldNames;\n+\t\t\tthis.selectFields = selectFields;\n+\t\t\tthis.partitionKeys = partitionKeys;\n+\t\t\tthis.defaultPartValue = defaultPartValue;\n+\t\t\tthis.limit = limit;\n+\t\t\tthis.jsonSelectFieldNames = jsonSelectFieldNames;\n+\t\t\tthis.jsonFieldMapping = jsonFieldMapping;\n+\t\t\tthis.deserializationSchema = deserializationSchema;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic boolean supportsMultiPaths() {\n+\t\t\treturn true;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void open(FileInputSplit split) throws IOException {\n+\t\t\tsuper.open(split);\n+\t\t\tthis.end = false;\n+\t\t\tthis.emitted = 0L;\n+\t\t\tthis.rowData = PartitionPathUtils.fillPartitionValueForRecord(fieldNames, fieldTypes, selectFields,\n+\t\t\t\tpartitionKeys, currentSplit.getPath(), defaultPartValue);\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic boolean reachedEnd() {\n+\t\t\treturn emitted >= limit || end;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic RowData readRecord(RowData reuse, byte[] bytes, int offset, int numBytes) throws IOException {\n+\t\t\tGenericRowData returnRecord = rowData;\n+\n+\t\t\tif (this.getDelimiter() != null && this.getDelimiter().length == 1\n+\t\t\t\t&& this.getDelimiter()[0] == NEW_LINE && offset + numBytes >= 1\n+\t\t\t\t&& bytes[offset + numBytes - 1] == CARRIAGE_RETURN){\n+\t\t\t\tnumBytes -= 1;\n+\t\t\t}\n+\t\t\tbyte[] trimBytes = Arrays.copyOfRange(bytes, offset, offset + numBytes);\n+\t\t\tGenericRowData jsonRow = (GenericRowData) deserializationSchema.deserialize(trimBytes);\n+\t\t\tif (jsonRow != null) {\n+\t\t\t\tfor (int i = 0; i < jsonSelectFieldNames.size(); i++) {\n+\t\t\t\t\treturnRecord.setField(jsonFieldMapping[i], jsonRow.getField(i));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ0MDg3Ng=="}, "originalCommit": {"oid": "e13ca920f02c00cfc883bc55499d7c179b7a8564"}, "originalPosition": 235}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ0ODE0Mg==", "bodyText": "Do you want the project fields? But only the fields in front?", "url": "https://github.com/apache/flink/pull/12010#discussion_r422448142", "createdAt": "2020-05-09T03:36:00Z", "author": {"login": "JingsongLi"}, "path": "flink-formats/flink-json/src/main/java/org/apache/flink/formats/json/JsonFileSystemFormatFactory.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.json;\n+\n+import org.apache.flink.api.common.io.DelimitedInputFormat;\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.api.java.typeutils.GenericTypeInfo;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.FileSystemFormatFactory;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.utils.PartitionPathUtils;\n+\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+import static org.apache.flink.table.descriptors.JsonValidator.FORMAT_FAIL_ON_MISSING_FIELD;\n+import static org.apache.flink.table.descriptors.JsonValidator.FORMAT_IGNORE_PARSE_ERRORS;\n+\n+/**\n+ * Factory to build reader/writer to read/write json format file.\n+ */\n+public class JsonFileSystemFormatFactory implements FileSystemFormatFactory {\n+\t@Override\n+\tpublic Map<String, String> requiredContext() {\n+\t\tMap<String, String> context = new HashMap<>();\n+\t\tcontext.put(FORMAT, \"json\");\n+\t\treturn context;\n+\t}\n+\n+\t@Override\n+\tpublic List<String> supportedProperties() {\n+\t\tArrayList<String> properties = new ArrayList<>();\n+\t\tproperties.add(FORMAT_FAIL_ON_MISSING_FIELD);\n+\t\tproperties.add(FORMAT_IGNORE_PARSE_ERRORS);\n+\t\treturn properties;\n+\t}\n+\n+\t@Override\n+\tpublic InputFormat<RowData, ?> createReader(ReaderContext context) {\n+\t\tDescriptorProperties properties = getValidatedProperties(context.getFormatProperties());\n+\t\tboolean failOnMissingField = properties.getOptionalBoolean(FORMAT_FAIL_ON_MISSING_FIELD).orElse(false);\n+\t\tboolean ignoreParseErrors = properties.getOptionalBoolean(FORMAT_IGNORE_PARSE_ERRORS).orElse(false);\n+\n+\t\t// deal partition fields\n+\t\tTableSchema tableSchema = context.getSchema();\n+\t\tList<DataType> fieldTypes = Arrays.asList(tableSchema.getFieldDataTypes());\n+\t\tList<String> fieldNames = Arrays.asList(tableSchema.getFieldNames());\n+\t\tString[] nonPartFieldNames = fieldNames.stream()\n+\t\t\t.filter(name -> !context.getPartitionKeys().contains(name)).toArray(String[]::new);\n+\t\tDataType[] nonPartFieldTypes = Arrays.asList(nonPartFieldNames).stream()\n+\t\t\t.map(name -> fieldTypes.get(fieldNames.indexOf(name))).toArray(DataType[]::new);\n+\t\tRowType nonPartRowType = RowType.of(\n+\t\t\tArrays.asList(nonPartFieldTypes).stream()\n+\t\t\t\t.map(DataType::getLogicalType)\n+\t\t\t\t.toArray(LogicalType[]::new),\n+\t\t\tnonPartFieldNames);\n+\n+\t\t// deal project fields\n+\t\tint[] selectFields = context.getProjectFields();\n+\t\tList<String> partitionKeys = context.getPartitionKeys();\n+\t\tList<String> selectFieldNames = Arrays.stream(selectFields)\n+\t\t\t.mapToObj(fieldNames::get)\n+\t\t\t.collect(Collectors.toList());\n+\t\tList<String> jsonSelectFieldNames = selectFieldNames.stream()\n+\t\t\t.filter(name -> !partitionKeys.contains(name)).collect(Collectors.toList());\n+\t\tint[] jsonFieldMapping =  jsonSelectFieldNames.stream().mapToInt(selectFieldNames::indexOf).toArray();\n+\n+\t\tJsonRowDataDeserializationSchema  deserializationSchema = new JsonRowDataDeserializationSchema(\n+\t\t\tnonPartRowType,\n+\t\t\tnew GenericTypeInfo(GenericRowData.class),\n+\t\t\tfailOnMissingField,\n+\t\t\tignoreParseErrors);\n+\n+\t\treturn new JsonInputFormat(\n+\t\t\tcontext.getPaths(),\n+\t\t\tfieldTypes,\n+\t\t\tfieldNames,\n+\t\t\tselectFields,\n+\t\t\tpartitionKeys,\n+\t\t\tcontext.getDefaultPartName(),\n+\t\t\tcontext.getPushedDownLimit(),\n+\t\t\tjsonSelectFieldNames,\n+\t\t\tjsonFieldMapping,\n+\t\t\tdeserializationSchema);\n+\t}\n+\n+\t@Override\n+\tpublic Optional<Encoder<RowData>> createEncoder(WriterContext context) {\n+\t\tTableSchema nonPartKeySchema = new TableSchema.Builder()\n+\t\t\t.fields(context.getFieldNamesWithoutPartKeys(), context.getFieldTypesWithoutPartKeys())\n+\t\t\t.build();\n+\t\tLogicalType[] logicalTypes = Arrays.asList(nonPartKeySchema.getFieldDataTypes())\n+\t\t\t.stream().map(DataType::getLogicalType).toArray(LogicalType[]::new);\n+\t\tRowType nonPartRowType = RowType.of(logicalTypes, nonPartKeySchema.getFieldNames());\n+\t\treturn Optional.of(new JsonRowDataEncoder(new JsonRowDataSerializationSchema(nonPartRowType)));\n+\t}\n+\n+\t@Override\n+\tpublic Optional<BulkWriter.Factory<RowData>> createBulkWriterFactory(WriterContext context) {\n+\t\treturn Optional.empty();\n+\t}\n+\n+\t@Override\n+\tpublic boolean supportsSchemaDerivation() {\n+\t\treturn true;\n+\t}\n+\n+\tprivate static DescriptorProperties getValidatedProperties(Map<String, String> propertiesMap) {\n+\t\tfinal DescriptorProperties properties = new DescriptorProperties(true);\n+\t\tproperties.putProperties(propertiesMap);\n+\t\tproperties.validateBoolean(FORMAT_FAIL_ON_MISSING_FIELD, true);\n+\t\tproperties.validateBoolean(FORMAT_IGNORE_PARSE_ERRORS, true);\n+\t\treturn properties;\n+\t}\n+\n+\t/**\n+\t * A {@link JsonInputFormat} is responsible to read {@link RowData} records\n+\t * from json format files.\n+\t */\n+\tpublic static class JsonInputFormat extends DelimitedInputFormat<RowData> {\n+\t\t/**\n+\t\t * Code of \\r, used to remove \\r from a line when the line ends with \\r\\n.\n+\t\t */\n+\t\tprivate static final byte CARRIAGE_RETURN = (byte) '\\r';\n+\n+\t\t/**\n+\t\t * Code of \\n, used to identify if \\n is used as delimiter.\n+\t\t */\n+\t\tprivate static final byte NEW_LINE = (byte) '\\n';\n+\n+\t\tprivate final List<DataType> fieldTypes;\n+\t\tprivate final List<String> fieldNames;\n+\t\tprivate final int[] selectFields;\n+\t\tprivate final List<String> partitionKeys;\n+\t\tprivate final String defaultPartValue;\n+\t\tprivate final long limit;\n+\t\tprivate final List<String> jsonSelectFieldNames;\n+\t\tprivate final int[] jsonFieldMapping;\n+\t\tprivate final JsonRowDataDeserializationSchema deserializationSchema;\n+\n+\t\tprivate transient boolean end;\n+\t\tprivate transient long emitted;\n+\t\t// reuse object for per record\n+\t\tprivate transient GenericRowData rowData;\n+\n+\t\tpublic JsonInputFormat(\n+\t\t\t\tPath[] filePaths,\n+\t\t\t\tList<DataType> fieldTypes,\n+\t\t\t\tList<String> fieldNames,\n+\t\t\t\tint[] selectFields,\n+\t\t\t\tList<String> partitionKeys,\n+\t\t\t\tString defaultPartValue,\n+\t\t\t\tlong limit,\n+\t\t\t\tList<String> jsonSelectFieldNames,\n+\t\t\t\tint[] jsonFieldMapping,\n+\t\t\t\tJsonRowDataDeserializationSchema deserializationSchema) {\n+\t\t\tsuper.setFilePaths(filePaths);\n+\t\t\tthis.fieldTypes = fieldTypes;\n+\t\t\tthis.fieldNames = fieldNames;\n+\t\t\tthis.selectFields = selectFields;\n+\t\t\tthis.partitionKeys = partitionKeys;\n+\t\t\tthis.defaultPartValue = defaultPartValue;\n+\t\t\tthis.limit = limit;\n+\t\t\tthis.jsonSelectFieldNames = jsonSelectFieldNames;\n+\t\t\tthis.jsonFieldMapping = jsonFieldMapping;\n+\t\t\tthis.deserializationSchema = deserializationSchema;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic boolean supportsMultiPaths() {\n+\t\t\treturn true;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void open(FileInputSplit split) throws IOException {\n+\t\t\tsuper.open(split);\n+\t\t\tthis.end = false;\n+\t\t\tthis.emitted = 0L;\n+\t\t\tthis.rowData = PartitionPathUtils.fillPartitionValueForRecord(fieldNames, fieldTypes, selectFields,\n+\t\t\t\tpartitionKeys, currentSplit.getPath(), defaultPartValue);\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic boolean reachedEnd() {\n+\t\t\treturn emitted >= limit || end;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic RowData readRecord(RowData reuse, byte[] bytes, int offset, int numBytes) throws IOException {\n+\t\t\tGenericRowData returnRecord = rowData;\n+\n+\t\t\tif (this.getDelimiter() != null && this.getDelimiter().length == 1\n+\t\t\t\t&& this.getDelimiter()[0] == NEW_LINE && offset + numBytes >= 1\n+\t\t\t\t&& bytes[offset + numBytes - 1] == CARRIAGE_RETURN){\n+\t\t\t\tnumBytes -= 1;\n+\t\t\t}\n+\t\t\tbyte[] trimBytes = Arrays.copyOfRange(bytes, offset, offset + numBytes);\n+\t\t\tGenericRowData jsonRow = (GenericRowData) deserializationSchema.deserialize(trimBytes);\n+\t\t\tif (jsonRow != null) {\n+\t\t\t\tfor (int i = 0; i < jsonSelectFieldNames.size(); i++) {\n+\t\t\t\t\treturnRecord.setField(jsonFieldMapping[i], jsonRow.getField(i));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ0MDg3Ng=="}, "originalCommit": {"oid": "e13ca920f02c00cfc883bc55499d7c179b7a8564"}, "originalPosition": 235}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ0ODE1Mw==", "bodyText": "got your point\uff0cuse:\nfor (int i = 0; i < jsonFieldMapping.length; i++) {\n\treturnRecord.setField(jsonFieldMapping[i], jsonRow.getField(i));\n}", "url": "https://github.com/apache/flink/pull/12010#discussion_r422448153", "createdAt": "2020-05-09T03:36:07Z", "author": {"login": "leonardBang"}, "path": "flink-formats/flink-json/src/main/java/org/apache/flink/formats/json/JsonFileSystemFormatFactory.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.json;\n+\n+import org.apache.flink.api.common.io.DelimitedInputFormat;\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.api.java.typeutils.GenericTypeInfo;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.FileSystemFormatFactory;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.utils.PartitionPathUtils;\n+\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+import static org.apache.flink.table.descriptors.JsonValidator.FORMAT_FAIL_ON_MISSING_FIELD;\n+import static org.apache.flink.table.descriptors.JsonValidator.FORMAT_IGNORE_PARSE_ERRORS;\n+\n+/**\n+ * Factory to build reader/writer to read/write json format file.\n+ */\n+public class JsonFileSystemFormatFactory implements FileSystemFormatFactory {\n+\t@Override\n+\tpublic Map<String, String> requiredContext() {\n+\t\tMap<String, String> context = new HashMap<>();\n+\t\tcontext.put(FORMAT, \"json\");\n+\t\treturn context;\n+\t}\n+\n+\t@Override\n+\tpublic List<String> supportedProperties() {\n+\t\tArrayList<String> properties = new ArrayList<>();\n+\t\tproperties.add(FORMAT_FAIL_ON_MISSING_FIELD);\n+\t\tproperties.add(FORMAT_IGNORE_PARSE_ERRORS);\n+\t\treturn properties;\n+\t}\n+\n+\t@Override\n+\tpublic InputFormat<RowData, ?> createReader(ReaderContext context) {\n+\t\tDescriptorProperties properties = getValidatedProperties(context.getFormatProperties());\n+\t\tboolean failOnMissingField = properties.getOptionalBoolean(FORMAT_FAIL_ON_MISSING_FIELD).orElse(false);\n+\t\tboolean ignoreParseErrors = properties.getOptionalBoolean(FORMAT_IGNORE_PARSE_ERRORS).orElse(false);\n+\n+\t\t// deal partition fields\n+\t\tTableSchema tableSchema = context.getSchema();\n+\t\tList<DataType> fieldTypes = Arrays.asList(tableSchema.getFieldDataTypes());\n+\t\tList<String> fieldNames = Arrays.asList(tableSchema.getFieldNames());\n+\t\tString[] nonPartFieldNames = fieldNames.stream()\n+\t\t\t.filter(name -> !context.getPartitionKeys().contains(name)).toArray(String[]::new);\n+\t\tDataType[] nonPartFieldTypes = Arrays.asList(nonPartFieldNames).stream()\n+\t\t\t.map(name -> fieldTypes.get(fieldNames.indexOf(name))).toArray(DataType[]::new);\n+\t\tRowType nonPartRowType = RowType.of(\n+\t\t\tArrays.asList(nonPartFieldTypes).stream()\n+\t\t\t\t.map(DataType::getLogicalType)\n+\t\t\t\t.toArray(LogicalType[]::new),\n+\t\t\tnonPartFieldNames);\n+\n+\t\t// deal project fields\n+\t\tint[] selectFields = context.getProjectFields();\n+\t\tList<String> partitionKeys = context.getPartitionKeys();\n+\t\tList<String> selectFieldNames = Arrays.stream(selectFields)\n+\t\t\t.mapToObj(fieldNames::get)\n+\t\t\t.collect(Collectors.toList());\n+\t\tList<String> jsonSelectFieldNames = selectFieldNames.stream()\n+\t\t\t.filter(name -> !partitionKeys.contains(name)).collect(Collectors.toList());\n+\t\tint[] jsonFieldMapping =  jsonSelectFieldNames.stream().mapToInt(selectFieldNames::indexOf).toArray();\n+\n+\t\tJsonRowDataDeserializationSchema  deserializationSchema = new JsonRowDataDeserializationSchema(\n+\t\t\tnonPartRowType,\n+\t\t\tnew GenericTypeInfo(GenericRowData.class),\n+\t\t\tfailOnMissingField,\n+\t\t\tignoreParseErrors);\n+\n+\t\treturn new JsonInputFormat(\n+\t\t\tcontext.getPaths(),\n+\t\t\tfieldTypes,\n+\t\t\tfieldNames,\n+\t\t\tselectFields,\n+\t\t\tpartitionKeys,\n+\t\t\tcontext.getDefaultPartName(),\n+\t\t\tcontext.getPushedDownLimit(),\n+\t\t\tjsonSelectFieldNames,\n+\t\t\tjsonFieldMapping,\n+\t\t\tdeserializationSchema);\n+\t}\n+\n+\t@Override\n+\tpublic Optional<Encoder<RowData>> createEncoder(WriterContext context) {\n+\t\tTableSchema nonPartKeySchema = new TableSchema.Builder()\n+\t\t\t.fields(context.getFieldNamesWithoutPartKeys(), context.getFieldTypesWithoutPartKeys())\n+\t\t\t.build();\n+\t\tLogicalType[] logicalTypes = Arrays.asList(nonPartKeySchema.getFieldDataTypes())\n+\t\t\t.stream().map(DataType::getLogicalType).toArray(LogicalType[]::new);\n+\t\tRowType nonPartRowType = RowType.of(logicalTypes, nonPartKeySchema.getFieldNames());\n+\t\treturn Optional.of(new JsonRowDataEncoder(new JsonRowDataSerializationSchema(nonPartRowType)));\n+\t}\n+\n+\t@Override\n+\tpublic Optional<BulkWriter.Factory<RowData>> createBulkWriterFactory(WriterContext context) {\n+\t\treturn Optional.empty();\n+\t}\n+\n+\t@Override\n+\tpublic boolean supportsSchemaDerivation() {\n+\t\treturn true;\n+\t}\n+\n+\tprivate static DescriptorProperties getValidatedProperties(Map<String, String> propertiesMap) {\n+\t\tfinal DescriptorProperties properties = new DescriptorProperties(true);\n+\t\tproperties.putProperties(propertiesMap);\n+\t\tproperties.validateBoolean(FORMAT_FAIL_ON_MISSING_FIELD, true);\n+\t\tproperties.validateBoolean(FORMAT_IGNORE_PARSE_ERRORS, true);\n+\t\treturn properties;\n+\t}\n+\n+\t/**\n+\t * A {@link JsonInputFormat} is responsible to read {@link RowData} records\n+\t * from json format files.\n+\t */\n+\tpublic static class JsonInputFormat extends DelimitedInputFormat<RowData> {\n+\t\t/**\n+\t\t * Code of \\r, used to remove \\r from a line when the line ends with \\r\\n.\n+\t\t */\n+\t\tprivate static final byte CARRIAGE_RETURN = (byte) '\\r';\n+\n+\t\t/**\n+\t\t * Code of \\n, used to identify if \\n is used as delimiter.\n+\t\t */\n+\t\tprivate static final byte NEW_LINE = (byte) '\\n';\n+\n+\t\tprivate final List<DataType> fieldTypes;\n+\t\tprivate final List<String> fieldNames;\n+\t\tprivate final int[] selectFields;\n+\t\tprivate final List<String> partitionKeys;\n+\t\tprivate final String defaultPartValue;\n+\t\tprivate final long limit;\n+\t\tprivate final List<String> jsonSelectFieldNames;\n+\t\tprivate final int[] jsonFieldMapping;\n+\t\tprivate final JsonRowDataDeserializationSchema deserializationSchema;\n+\n+\t\tprivate transient boolean end;\n+\t\tprivate transient long emitted;\n+\t\t// reuse object for per record\n+\t\tprivate transient GenericRowData rowData;\n+\n+\t\tpublic JsonInputFormat(\n+\t\t\t\tPath[] filePaths,\n+\t\t\t\tList<DataType> fieldTypes,\n+\t\t\t\tList<String> fieldNames,\n+\t\t\t\tint[] selectFields,\n+\t\t\t\tList<String> partitionKeys,\n+\t\t\t\tString defaultPartValue,\n+\t\t\t\tlong limit,\n+\t\t\t\tList<String> jsonSelectFieldNames,\n+\t\t\t\tint[] jsonFieldMapping,\n+\t\t\t\tJsonRowDataDeserializationSchema deserializationSchema) {\n+\t\t\tsuper.setFilePaths(filePaths);\n+\t\t\tthis.fieldTypes = fieldTypes;\n+\t\t\tthis.fieldNames = fieldNames;\n+\t\t\tthis.selectFields = selectFields;\n+\t\t\tthis.partitionKeys = partitionKeys;\n+\t\t\tthis.defaultPartValue = defaultPartValue;\n+\t\t\tthis.limit = limit;\n+\t\t\tthis.jsonSelectFieldNames = jsonSelectFieldNames;\n+\t\t\tthis.jsonFieldMapping = jsonFieldMapping;\n+\t\t\tthis.deserializationSchema = deserializationSchema;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic boolean supportsMultiPaths() {\n+\t\t\treturn true;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void open(FileInputSplit split) throws IOException {\n+\t\t\tsuper.open(split);\n+\t\t\tthis.end = false;\n+\t\t\tthis.emitted = 0L;\n+\t\t\tthis.rowData = PartitionPathUtils.fillPartitionValueForRecord(fieldNames, fieldTypes, selectFields,\n+\t\t\t\tpartitionKeys, currentSplit.getPath(), defaultPartValue);\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic boolean reachedEnd() {\n+\t\t\treturn emitted >= limit || end;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic RowData readRecord(RowData reuse, byte[] bytes, int offset, int numBytes) throws IOException {\n+\t\t\tGenericRowData returnRecord = rowData;\n+\n+\t\t\tif (this.getDelimiter() != null && this.getDelimiter().length == 1\n+\t\t\t\t&& this.getDelimiter()[0] == NEW_LINE && offset + numBytes >= 1\n+\t\t\t\t&& bytes[offset + numBytes - 1] == CARRIAGE_RETURN){\n+\t\t\t\tnumBytes -= 1;\n+\t\t\t}\n+\t\t\tbyte[] trimBytes = Arrays.copyOfRange(bytes, offset, offset + numBytes);\n+\t\t\tGenericRowData jsonRow = (GenericRowData) deserializationSchema.deserialize(trimBytes);\n+\t\t\tif (jsonRow != null) {\n+\t\t\t\tfor (int i = 0; i < jsonSelectFieldNames.size(); i++) {\n+\t\t\t\t\treturnRecord.setField(jsonFieldMapping[i], jsonRow.getField(i));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ0MDg3Ng=="}, "originalCommit": {"oid": "e13ca920f02c00cfc883bc55499d7c179b7a8564"}, "originalPosition": 235}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ0ODE4Mw==", "bodyText": "It looks like totally wrong, you could add some projection tests.", "url": "https://github.com/apache/flink/pull/12010#discussion_r422448183", "createdAt": "2020-05-09T03:36:33Z", "author": {"login": "JingsongLi"}, "path": "flink-formats/flink-json/src/main/java/org/apache/flink/formats/json/JsonFileSystemFormatFactory.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.json;\n+\n+import org.apache.flink.api.common.io.DelimitedInputFormat;\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.api.java.typeutils.GenericTypeInfo;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.FileSystemFormatFactory;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.utils.PartitionPathUtils;\n+\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+import static org.apache.flink.table.descriptors.JsonValidator.FORMAT_FAIL_ON_MISSING_FIELD;\n+import static org.apache.flink.table.descriptors.JsonValidator.FORMAT_IGNORE_PARSE_ERRORS;\n+\n+/**\n+ * Factory to build reader/writer to read/write json format file.\n+ */\n+public class JsonFileSystemFormatFactory implements FileSystemFormatFactory {\n+\t@Override\n+\tpublic Map<String, String> requiredContext() {\n+\t\tMap<String, String> context = new HashMap<>();\n+\t\tcontext.put(FORMAT, \"json\");\n+\t\treturn context;\n+\t}\n+\n+\t@Override\n+\tpublic List<String> supportedProperties() {\n+\t\tArrayList<String> properties = new ArrayList<>();\n+\t\tproperties.add(FORMAT_FAIL_ON_MISSING_FIELD);\n+\t\tproperties.add(FORMAT_IGNORE_PARSE_ERRORS);\n+\t\treturn properties;\n+\t}\n+\n+\t@Override\n+\tpublic InputFormat<RowData, ?> createReader(ReaderContext context) {\n+\t\tDescriptorProperties properties = getValidatedProperties(context.getFormatProperties());\n+\t\tboolean failOnMissingField = properties.getOptionalBoolean(FORMAT_FAIL_ON_MISSING_FIELD).orElse(false);\n+\t\tboolean ignoreParseErrors = properties.getOptionalBoolean(FORMAT_IGNORE_PARSE_ERRORS).orElse(false);\n+\n+\t\t// deal partition fields\n+\t\tTableSchema tableSchema = context.getSchema();\n+\t\tList<DataType> fieldTypes = Arrays.asList(tableSchema.getFieldDataTypes());\n+\t\tList<String> fieldNames = Arrays.asList(tableSchema.getFieldNames());\n+\t\tString[] nonPartFieldNames = fieldNames.stream()\n+\t\t\t.filter(name -> !context.getPartitionKeys().contains(name)).toArray(String[]::new);\n+\t\tDataType[] nonPartFieldTypes = Arrays.asList(nonPartFieldNames).stream()\n+\t\t\t.map(name -> fieldTypes.get(fieldNames.indexOf(name))).toArray(DataType[]::new);\n+\t\tRowType nonPartRowType = RowType.of(\n+\t\t\tArrays.asList(nonPartFieldTypes).stream()\n+\t\t\t\t.map(DataType::getLogicalType)\n+\t\t\t\t.toArray(LogicalType[]::new),\n+\t\t\tnonPartFieldNames);\n+\n+\t\t// deal project fields\n+\t\tint[] selectFields = context.getProjectFields();\n+\t\tList<String> partitionKeys = context.getPartitionKeys();\n+\t\tList<String> selectFieldNames = Arrays.stream(selectFields)\n+\t\t\t.mapToObj(fieldNames::get)\n+\t\t\t.collect(Collectors.toList());\n+\t\tList<String> jsonSelectFieldNames = selectFieldNames.stream()\n+\t\t\t.filter(name -> !partitionKeys.contains(name)).collect(Collectors.toList());\n+\t\tint[] jsonFieldMapping =  jsonSelectFieldNames.stream().mapToInt(selectFieldNames::indexOf).toArray();\n+\n+\t\tJsonRowDataDeserializationSchema  deserializationSchema = new JsonRowDataDeserializationSchema(\n+\t\t\tnonPartRowType,\n+\t\t\tnew GenericTypeInfo(GenericRowData.class),\n+\t\t\tfailOnMissingField,\n+\t\t\tignoreParseErrors);\n+\n+\t\treturn new JsonInputFormat(\n+\t\t\tcontext.getPaths(),\n+\t\t\tfieldTypes,\n+\t\t\tfieldNames,\n+\t\t\tselectFields,\n+\t\t\tpartitionKeys,\n+\t\t\tcontext.getDefaultPartName(),\n+\t\t\tcontext.getPushedDownLimit(),\n+\t\t\tjsonSelectFieldNames,\n+\t\t\tjsonFieldMapping,\n+\t\t\tdeserializationSchema);\n+\t}\n+\n+\t@Override\n+\tpublic Optional<Encoder<RowData>> createEncoder(WriterContext context) {\n+\t\tTableSchema nonPartKeySchema = new TableSchema.Builder()\n+\t\t\t.fields(context.getFieldNamesWithoutPartKeys(), context.getFieldTypesWithoutPartKeys())\n+\t\t\t.build();\n+\t\tLogicalType[] logicalTypes = Arrays.asList(nonPartKeySchema.getFieldDataTypes())\n+\t\t\t.stream().map(DataType::getLogicalType).toArray(LogicalType[]::new);\n+\t\tRowType nonPartRowType = RowType.of(logicalTypes, nonPartKeySchema.getFieldNames());\n+\t\treturn Optional.of(new JsonRowDataEncoder(new JsonRowDataSerializationSchema(nonPartRowType)));\n+\t}\n+\n+\t@Override\n+\tpublic Optional<BulkWriter.Factory<RowData>> createBulkWriterFactory(WriterContext context) {\n+\t\treturn Optional.empty();\n+\t}\n+\n+\t@Override\n+\tpublic boolean supportsSchemaDerivation() {\n+\t\treturn true;\n+\t}\n+\n+\tprivate static DescriptorProperties getValidatedProperties(Map<String, String> propertiesMap) {\n+\t\tfinal DescriptorProperties properties = new DescriptorProperties(true);\n+\t\tproperties.putProperties(propertiesMap);\n+\t\tproperties.validateBoolean(FORMAT_FAIL_ON_MISSING_FIELD, true);\n+\t\tproperties.validateBoolean(FORMAT_IGNORE_PARSE_ERRORS, true);\n+\t\treturn properties;\n+\t}\n+\n+\t/**\n+\t * A {@link JsonInputFormat} is responsible to read {@link RowData} records\n+\t * from json format files.\n+\t */\n+\tpublic static class JsonInputFormat extends DelimitedInputFormat<RowData> {\n+\t\t/**\n+\t\t * Code of \\r, used to remove \\r from a line when the line ends with \\r\\n.\n+\t\t */\n+\t\tprivate static final byte CARRIAGE_RETURN = (byte) '\\r';\n+\n+\t\t/**\n+\t\t * Code of \\n, used to identify if \\n is used as delimiter.\n+\t\t */\n+\t\tprivate static final byte NEW_LINE = (byte) '\\n';\n+\n+\t\tprivate final List<DataType> fieldTypes;\n+\t\tprivate final List<String> fieldNames;\n+\t\tprivate final int[] selectFields;\n+\t\tprivate final List<String> partitionKeys;\n+\t\tprivate final String defaultPartValue;\n+\t\tprivate final long limit;\n+\t\tprivate final List<String> jsonSelectFieldNames;\n+\t\tprivate final int[] jsonFieldMapping;\n+\t\tprivate final JsonRowDataDeserializationSchema deserializationSchema;\n+\n+\t\tprivate transient boolean end;\n+\t\tprivate transient long emitted;\n+\t\t// reuse object for per record\n+\t\tprivate transient GenericRowData rowData;\n+\n+\t\tpublic JsonInputFormat(\n+\t\t\t\tPath[] filePaths,\n+\t\t\t\tList<DataType> fieldTypes,\n+\t\t\t\tList<String> fieldNames,\n+\t\t\t\tint[] selectFields,\n+\t\t\t\tList<String> partitionKeys,\n+\t\t\t\tString defaultPartValue,\n+\t\t\t\tlong limit,\n+\t\t\t\tList<String> jsonSelectFieldNames,\n+\t\t\t\tint[] jsonFieldMapping,\n+\t\t\t\tJsonRowDataDeserializationSchema deserializationSchema) {\n+\t\t\tsuper.setFilePaths(filePaths);\n+\t\t\tthis.fieldTypes = fieldTypes;\n+\t\t\tthis.fieldNames = fieldNames;\n+\t\t\tthis.selectFields = selectFields;\n+\t\t\tthis.partitionKeys = partitionKeys;\n+\t\t\tthis.defaultPartValue = defaultPartValue;\n+\t\t\tthis.limit = limit;\n+\t\t\tthis.jsonSelectFieldNames = jsonSelectFieldNames;\n+\t\t\tthis.jsonFieldMapping = jsonFieldMapping;\n+\t\t\tthis.deserializationSchema = deserializationSchema;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic boolean supportsMultiPaths() {\n+\t\t\treturn true;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void open(FileInputSplit split) throws IOException {\n+\t\t\tsuper.open(split);\n+\t\t\tthis.end = false;\n+\t\t\tthis.emitted = 0L;\n+\t\t\tthis.rowData = PartitionPathUtils.fillPartitionValueForRecord(fieldNames, fieldTypes, selectFields,\n+\t\t\t\tpartitionKeys, currentSplit.getPath(), defaultPartValue);\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic boolean reachedEnd() {\n+\t\t\treturn emitted >= limit || end;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic RowData readRecord(RowData reuse, byte[] bytes, int offset, int numBytes) throws IOException {\n+\t\t\tGenericRowData returnRecord = rowData;\n+\n+\t\t\tif (this.getDelimiter() != null && this.getDelimiter().length == 1\n+\t\t\t\t&& this.getDelimiter()[0] == NEW_LINE && offset + numBytes >= 1\n+\t\t\t\t&& bytes[offset + numBytes - 1] == CARRIAGE_RETURN){\n+\t\t\t\tnumBytes -= 1;\n+\t\t\t}\n+\t\t\tbyte[] trimBytes = Arrays.copyOfRange(bytes, offset, offset + numBytes);\n+\t\t\tGenericRowData jsonRow = (GenericRowData) deserializationSchema.deserialize(trimBytes);\n+\t\t\tif (jsonRow != null) {\n+\t\t\t\tfor (int i = 0; i < jsonSelectFieldNames.size(); i++) {\n+\t\t\t\t\treturnRecord.setField(jsonFieldMapping[i], jsonRow.getField(i));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ0MDg3Ng=="}, "originalCommit": {"oid": "e13ca920f02c00cfc883bc55499d7c179b7a8564"}, "originalPosition": 235}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMDA3NjEzOnYy", "diffSide": "RIGHT", "path": "flink-formats/flink-json/src/main/java/org/apache/flink/formats/json/JsonFileSystemFormatFactory.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQwMjowNjo1NVrOGS3wHg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQwMzozNDoyNFrOGS4LlQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ0MDk5MA==", "bodyText": "jsonRow != null? So what will be happen when jsonRow is null?", "url": "https://github.com/apache/flink/pull/12010#discussion_r422440990", "createdAt": "2020-05-09T02:06:55Z", "author": {"login": "JingsongLi"}, "path": "flink-formats/flink-json/src/main/java/org/apache/flink/formats/json/JsonFileSystemFormatFactory.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.json;\n+\n+import org.apache.flink.api.common.io.DelimitedInputFormat;\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.api.java.typeutils.GenericTypeInfo;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.FileSystemFormatFactory;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.utils.PartitionPathUtils;\n+\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+import static org.apache.flink.table.descriptors.JsonValidator.FORMAT_FAIL_ON_MISSING_FIELD;\n+import static org.apache.flink.table.descriptors.JsonValidator.FORMAT_IGNORE_PARSE_ERRORS;\n+\n+/**\n+ * Factory to build reader/writer to read/write json format file.\n+ */\n+public class JsonFileSystemFormatFactory implements FileSystemFormatFactory {\n+\t@Override\n+\tpublic Map<String, String> requiredContext() {\n+\t\tMap<String, String> context = new HashMap<>();\n+\t\tcontext.put(FORMAT, \"json\");\n+\t\treturn context;\n+\t}\n+\n+\t@Override\n+\tpublic List<String> supportedProperties() {\n+\t\tArrayList<String> properties = new ArrayList<>();\n+\t\tproperties.add(FORMAT_FAIL_ON_MISSING_FIELD);\n+\t\tproperties.add(FORMAT_IGNORE_PARSE_ERRORS);\n+\t\treturn properties;\n+\t}\n+\n+\t@Override\n+\tpublic InputFormat<RowData, ?> createReader(ReaderContext context) {\n+\t\tDescriptorProperties properties = getValidatedProperties(context.getFormatProperties());\n+\t\tboolean failOnMissingField = properties.getOptionalBoolean(FORMAT_FAIL_ON_MISSING_FIELD).orElse(false);\n+\t\tboolean ignoreParseErrors = properties.getOptionalBoolean(FORMAT_IGNORE_PARSE_ERRORS).orElse(false);\n+\n+\t\t// deal partition fields\n+\t\tTableSchema tableSchema = context.getSchema();\n+\t\tList<DataType> fieldTypes = Arrays.asList(tableSchema.getFieldDataTypes());\n+\t\tList<String> fieldNames = Arrays.asList(tableSchema.getFieldNames());\n+\t\tString[] nonPartFieldNames = fieldNames.stream()\n+\t\t\t.filter(name -> !context.getPartitionKeys().contains(name)).toArray(String[]::new);\n+\t\tDataType[] nonPartFieldTypes = Arrays.asList(nonPartFieldNames).stream()\n+\t\t\t.map(name -> fieldTypes.get(fieldNames.indexOf(name))).toArray(DataType[]::new);\n+\t\tRowType nonPartRowType = RowType.of(\n+\t\t\tArrays.asList(nonPartFieldTypes).stream()\n+\t\t\t\t.map(DataType::getLogicalType)\n+\t\t\t\t.toArray(LogicalType[]::new),\n+\t\t\tnonPartFieldNames);\n+\n+\t\t// deal project fields\n+\t\tint[] selectFields = context.getProjectFields();\n+\t\tList<String> partitionKeys = context.getPartitionKeys();\n+\t\tList<String> selectFieldNames = Arrays.stream(selectFields)\n+\t\t\t.mapToObj(fieldNames::get)\n+\t\t\t.collect(Collectors.toList());\n+\t\tList<String> jsonSelectFieldNames = selectFieldNames.stream()\n+\t\t\t.filter(name -> !partitionKeys.contains(name)).collect(Collectors.toList());\n+\t\tint[] jsonFieldMapping =  jsonSelectFieldNames.stream().mapToInt(selectFieldNames::indexOf).toArray();\n+\n+\t\tJsonRowDataDeserializationSchema  deserializationSchema = new JsonRowDataDeserializationSchema(\n+\t\t\tnonPartRowType,\n+\t\t\tnew GenericTypeInfo(GenericRowData.class),\n+\t\t\tfailOnMissingField,\n+\t\t\tignoreParseErrors);\n+\n+\t\treturn new JsonInputFormat(\n+\t\t\tcontext.getPaths(),\n+\t\t\tfieldTypes,\n+\t\t\tfieldNames,\n+\t\t\tselectFields,\n+\t\t\tpartitionKeys,\n+\t\t\tcontext.getDefaultPartName(),\n+\t\t\tcontext.getPushedDownLimit(),\n+\t\t\tjsonSelectFieldNames,\n+\t\t\tjsonFieldMapping,\n+\t\t\tdeserializationSchema);\n+\t}\n+\n+\t@Override\n+\tpublic Optional<Encoder<RowData>> createEncoder(WriterContext context) {\n+\t\tTableSchema nonPartKeySchema = new TableSchema.Builder()\n+\t\t\t.fields(context.getFieldNamesWithoutPartKeys(), context.getFieldTypesWithoutPartKeys())\n+\t\t\t.build();\n+\t\tLogicalType[] logicalTypes = Arrays.asList(nonPartKeySchema.getFieldDataTypes())\n+\t\t\t.stream().map(DataType::getLogicalType).toArray(LogicalType[]::new);\n+\t\tRowType nonPartRowType = RowType.of(logicalTypes, nonPartKeySchema.getFieldNames());\n+\t\treturn Optional.of(new JsonRowDataEncoder(new JsonRowDataSerializationSchema(nonPartRowType)));\n+\t}\n+\n+\t@Override\n+\tpublic Optional<BulkWriter.Factory<RowData>> createBulkWriterFactory(WriterContext context) {\n+\t\treturn Optional.empty();\n+\t}\n+\n+\t@Override\n+\tpublic boolean supportsSchemaDerivation() {\n+\t\treturn true;\n+\t}\n+\n+\tprivate static DescriptorProperties getValidatedProperties(Map<String, String> propertiesMap) {\n+\t\tfinal DescriptorProperties properties = new DescriptorProperties(true);\n+\t\tproperties.putProperties(propertiesMap);\n+\t\tproperties.validateBoolean(FORMAT_FAIL_ON_MISSING_FIELD, true);\n+\t\tproperties.validateBoolean(FORMAT_IGNORE_PARSE_ERRORS, true);\n+\t\treturn properties;\n+\t}\n+\n+\t/**\n+\t * A {@link JsonInputFormat} is responsible to read {@link RowData} records\n+\t * from json format files.\n+\t */\n+\tpublic static class JsonInputFormat extends DelimitedInputFormat<RowData> {\n+\t\t/**\n+\t\t * Code of \\r, used to remove \\r from a line when the line ends with \\r\\n.\n+\t\t */\n+\t\tprivate static final byte CARRIAGE_RETURN = (byte) '\\r';\n+\n+\t\t/**\n+\t\t * Code of \\n, used to identify if \\n is used as delimiter.\n+\t\t */\n+\t\tprivate static final byte NEW_LINE = (byte) '\\n';\n+\n+\t\tprivate final List<DataType> fieldTypes;\n+\t\tprivate final List<String> fieldNames;\n+\t\tprivate final int[] selectFields;\n+\t\tprivate final List<String> partitionKeys;\n+\t\tprivate final String defaultPartValue;\n+\t\tprivate final long limit;\n+\t\tprivate final List<String> jsonSelectFieldNames;\n+\t\tprivate final int[] jsonFieldMapping;\n+\t\tprivate final JsonRowDataDeserializationSchema deserializationSchema;\n+\n+\t\tprivate transient boolean end;\n+\t\tprivate transient long emitted;\n+\t\t// reuse object for per record\n+\t\tprivate transient GenericRowData rowData;\n+\n+\t\tpublic JsonInputFormat(\n+\t\t\t\tPath[] filePaths,\n+\t\t\t\tList<DataType> fieldTypes,\n+\t\t\t\tList<String> fieldNames,\n+\t\t\t\tint[] selectFields,\n+\t\t\t\tList<String> partitionKeys,\n+\t\t\t\tString defaultPartValue,\n+\t\t\t\tlong limit,\n+\t\t\t\tList<String> jsonSelectFieldNames,\n+\t\t\t\tint[] jsonFieldMapping,\n+\t\t\t\tJsonRowDataDeserializationSchema deserializationSchema) {\n+\t\t\tsuper.setFilePaths(filePaths);\n+\t\t\tthis.fieldTypes = fieldTypes;\n+\t\t\tthis.fieldNames = fieldNames;\n+\t\t\tthis.selectFields = selectFields;\n+\t\t\tthis.partitionKeys = partitionKeys;\n+\t\t\tthis.defaultPartValue = defaultPartValue;\n+\t\t\tthis.limit = limit;\n+\t\t\tthis.jsonSelectFieldNames = jsonSelectFieldNames;\n+\t\t\tthis.jsonFieldMapping = jsonFieldMapping;\n+\t\t\tthis.deserializationSchema = deserializationSchema;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic boolean supportsMultiPaths() {\n+\t\t\treturn true;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void open(FileInputSplit split) throws IOException {\n+\t\t\tsuper.open(split);\n+\t\t\tthis.end = false;\n+\t\t\tthis.emitted = 0L;\n+\t\t\tthis.rowData = PartitionPathUtils.fillPartitionValueForRecord(fieldNames, fieldTypes, selectFields,\n+\t\t\t\tpartitionKeys, currentSplit.getPath(), defaultPartValue);\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic boolean reachedEnd() {\n+\t\t\treturn emitted >= limit || end;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic RowData readRecord(RowData reuse, byte[] bytes, int offset, int numBytes) throws IOException {\n+\t\t\tGenericRowData returnRecord = rowData;\n+\n+\t\t\tif (this.getDelimiter() != null && this.getDelimiter().length == 1\n+\t\t\t\t&& this.getDelimiter()[0] == NEW_LINE && offset + numBytes >= 1\n+\t\t\t\t&& bytes[offset + numBytes - 1] == CARRIAGE_RETURN){\n+\t\t\t\tnumBytes -= 1;\n+\t\t\t}\n+\t\t\tbyte[] trimBytes = Arrays.copyOfRange(bytes, offset, offset + numBytes);\n+\t\t\tGenericRowData jsonRow = (GenericRowData) deserializationSchema.deserialize(trimBytes);\n+\t\t\tif (jsonRow != null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e13ca920f02c00cfc883bc55499d7c179b7a8564"}, "originalPosition": 233}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ0NzgwNw==", "bodyText": "return a record that filled with only partition key field, other fields are null", "url": "https://github.com/apache/flink/pull/12010#discussion_r422447807", "createdAt": "2020-05-09T03:31:34Z", "author": {"login": "leonardBang"}, "path": "flink-formats/flink-json/src/main/java/org/apache/flink/formats/json/JsonFileSystemFormatFactory.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.json;\n+\n+import org.apache.flink.api.common.io.DelimitedInputFormat;\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.api.java.typeutils.GenericTypeInfo;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.FileSystemFormatFactory;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.utils.PartitionPathUtils;\n+\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+import static org.apache.flink.table.descriptors.JsonValidator.FORMAT_FAIL_ON_MISSING_FIELD;\n+import static org.apache.flink.table.descriptors.JsonValidator.FORMAT_IGNORE_PARSE_ERRORS;\n+\n+/**\n+ * Factory to build reader/writer to read/write json format file.\n+ */\n+public class JsonFileSystemFormatFactory implements FileSystemFormatFactory {\n+\t@Override\n+\tpublic Map<String, String> requiredContext() {\n+\t\tMap<String, String> context = new HashMap<>();\n+\t\tcontext.put(FORMAT, \"json\");\n+\t\treturn context;\n+\t}\n+\n+\t@Override\n+\tpublic List<String> supportedProperties() {\n+\t\tArrayList<String> properties = new ArrayList<>();\n+\t\tproperties.add(FORMAT_FAIL_ON_MISSING_FIELD);\n+\t\tproperties.add(FORMAT_IGNORE_PARSE_ERRORS);\n+\t\treturn properties;\n+\t}\n+\n+\t@Override\n+\tpublic InputFormat<RowData, ?> createReader(ReaderContext context) {\n+\t\tDescriptorProperties properties = getValidatedProperties(context.getFormatProperties());\n+\t\tboolean failOnMissingField = properties.getOptionalBoolean(FORMAT_FAIL_ON_MISSING_FIELD).orElse(false);\n+\t\tboolean ignoreParseErrors = properties.getOptionalBoolean(FORMAT_IGNORE_PARSE_ERRORS).orElse(false);\n+\n+\t\t// deal partition fields\n+\t\tTableSchema tableSchema = context.getSchema();\n+\t\tList<DataType> fieldTypes = Arrays.asList(tableSchema.getFieldDataTypes());\n+\t\tList<String> fieldNames = Arrays.asList(tableSchema.getFieldNames());\n+\t\tString[] nonPartFieldNames = fieldNames.stream()\n+\t\t\t.filter(name -> !context.getPartitionKeys().contains(name)).toArray(String[]::new);\n+\t\tDataType[] nonPartFieldTypes = Arrays.asList(nonPartFieldNames).stream()\n+\t\t\t.map(name -> fieldTypes.get(fieldNames.indexOf(name))).toArray(DataType[]::new);\n+\t\tRowType nonPartRowType = RowType.of(\n+\t\t\tArrays.asList(nonPartFieldTypes).stream()\n+\t\t\t\t.map(DataType::getLogicalType)\n+\t\t\t\t.toArray(LogicalType[]::new),\n+\t\t\tnonPartFieldNames);\n+\n+\t\t// deal project fields\n+\t\tint[] selectFields = context.getProjectFields();\n+\t\tList<String> partitionKeys = context.getPartitionKeys();\n+\t\tList<String> selectFieldNames = Arrays.stream(selectFields)\n+\t\t\t.mapToObj(fieldNames::get)\n+\t\t\t.collect(Collectors.toList());\n+\t\tList<String> jsonSelectFieldNames = selectFieldNames.stream()\n+\t\t\t.filter(name -> !partitionKeys.contains(name)).collect(Collectors.toList());\n+\t\tint[] jsonFieldMapping =  jsonSelectFieldNames.stream().mapToInt(selectFieldNames::indexOf).toArray();\n+\n+\t\tJsonRowDataDeserializationSchema  deserializationSchema = new JsonRowDataDeserializationSchema(\n+\t\t\tnonPartRowType,\n+\t\t\tnew GenericTypeInfo(GenericRowData.class),\n+\t\t\tfailOnMissingField,\n+\t\t\tignoreParseErrors);\n+\n+\t\treturn new JsonInputFormat(\n+\t\t\tcontext.getPaths(),\n+\t\t\tfieldTypes,\n+\t\t\tfieldNames,\n+\t\t\tselectFields,\n+\t\t\tpartitionKeys,\n+\t\t\tcontext.getDefaultPartName(),\n+\t\t\tcontext.getPushedDownLimit(),\n+\t\t\tjsonSelectFieldNames,\n+\t\t\tjsonFieldMapping,\n+\t\t\tdeserializationSchema);\n+\t}\n+\n+\t@Override\n+\tpublic Optional<Encoder<RowData>> createEncoder(WriterContext context) {\n+\t\tTableSchema nonPartKeySchema = new TableSchema.Builder()\n+\t\t\t.fields(context.getFieldNamesWithoutPartKeys(), context.getFieldTypesWithoutPartKeys())\n+\t\t\t.build();\n+\t\tLogicalType[] logicalTypes = Arrays.asList(nonPartKeySchema.getFieldDataTypes())\n+\t\t\t.stream().map(DataType::getLogicalType).toArray(LogicalType[]::new);\n+\t\tRowType nonPartRowType = RowType.of(logicalTypes, nonPartKeySchema.getFieldNames());\n+\t\treturn Optional.of(new JsonRowDataEncoder(new JsonRowDataSerializationSchema(nonPartRowType)));\n+\t}\n+\n+\t@Override\n+\tpublic Optional<BulkWriter.Factory<RowData>> createBulkWriterFactory(WriterContext context) {\n+\t\treturn Optional.empty();\n+\t}\n+\n+\t@Override\n+\tpublic boolean supportsSchemaDerivation() {\n+\t\treturn true;\n+\t}\n+\n+\tprivate static DescriptorProperties getValidatedProperties(Map<String, String> propertiesMap) {\n+\t\tfinal DescriptorProperties properties = new DescriptorProperties(true);\n+\t\tproperties.putProperties(propertiesMap);\n+\t\tproperties.validateBoolean(FORMAT_FAIL_ON_MISSING_FIELD, true);\n+\t\tproperties.validateBoolean(FORMAT_IGNORE_PARSE_ERRORS, true);\n+\t\treturn properties;\n+\t}\n+\n+\t/**\n+\t * A {@link JsonInputFormat} is responsible to read {@link RowData} records\n+\t * from json format files.\n+\t */\n+\tpublic static class JsonInputFormat extends DelimitedInputFormat<RowData> {\n+\t\t/**\n+\t\t * Code of \\r, used to remove \\r from a line when the line ends with \\r\\n.\n+\t\t */\n+\t\tprivate static final byte CARRIAGE_RETURN = (byte) '\\r';\n+\n+\t\t/**\n+\t\t * Code of \\n, used to identify if \\n is used as delimiter.\n+\t\t */\n+\t\tprivate static final byte NEW_LINE = (byte) '\\n';\n+\n+\t\tprivate final List<DataType> fieldTypes;\n+\t\tprivate final List<String> fieldNames;\n+\t\tprivate final int[] selectFields;\n+\t\tprivate final List<String> partitionKeys;\n+\t\tprivate final String defaultPartValue;\n+\t\tprivate final long limit;\n+\t\tprivate final List<String> jsonSelectFieldNames;\n+\t\tprivate final int[] jsonFieldMapping;\n+\t\tprivate final JsonRowDataDeserializationSchema deserializationSchema;\n+\n+\t\tprivate transient boolean end;\n+\t\tprivate transient long emitted;\n+\t\t// reuse object for per record\n+\t\tprivate transient GenericRowData rowData;\n+\n+\t\tpublic JsonInputFormat(\n+\t\t\t\tPath[] filePaths,\n+\t\t\t\tList<DataType> fieldTypes,\n+\t\t\t\tList<String> fieldNames,\n+\t\t\t\tint[] selectFields,\n+\t\t\t\tList<String> partitionKeys,\n+\t\t\t\tString defaultPartValue,\n+\t\t\t\tlong limit,\n+\t\t\t\tList<String> jsonSelectFieldNames,\n+\t\t\t\tint[] jsonFieldMapping,\n+\t\t\t\tJsonRowDataDeserializationSchema deserializationSchema) {\n+\t\t\tsuper.setFilePaths(filePaths);\n+\t\t\tthis.fieldTypes = fieldTypes;\n+\t\t\tthis.fieldNames = fieldNames;\n+\t\t\tthis.selectFields = selectFields;\n+\t\t\tthis.partitionKeys = partitionKeys;\n+\t\t\tthis.defaultPartValue = defaultPartValue;\n+\t\t\tthis.limit = limit;\n+\t\t\tthis.jsonSelectFieldNames = jsonSelectFieldNames;\n+\t\t\tthis.jsonFieldMapping = jsonFieldMapping;\n+\t\t\tthis.deserializationSchema = deserializationSchema;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic boolean supportsMultiPaths() {\n+\t\t\treturn true;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void open(FileInputSplit split) throws IOException {\n+\t\t\tsuper.open(split);\n+\t\t\tthis.end = false;\n+\t\t\tthis.emitted = 0L;\n+\t\t\tthis.rowData = PartitionPathUtils.fillPartitionValueForRecord(fieldNames, fieldTypes, selectFields,\n+\t\t\t\tpartitionKeys, currentSplit.getPath(), defaultPartValue);\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic boolean reachedEnd() {\n+\t\t\treturn emitted >= limit || end;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic RowData readRecord(RowData reuse, byte[] bytes, int offset, int numBytes) throws IOException {\n+\t\t\tGenericRowData returnRecord = rowData;\n+\n+\t\t\tif (this.getDelimiter() != null && this.getDelimiter().length == 1\n+\t\t\t\t&& this.getDelimiter()[0] == NEW_LINE && offset + numBytes >= 1\n+\t\t\t\t&& bytes[offset + numBytes - 1] == CARRIAGE_RETURN){\n+\t\t\t\tnumBytes -= 1;\n+\t\t\t}\n+\t\t\tbyte[] trimBytes = Arrays.copyOfRange(bytes, offset, offset + numBytes);\n+\t\t\tGenericRowData jsonRow = (GenericRowData) deserializationSchema.deserialize(trimBytes);\n+\t\t\tif (jsonRow != null) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ0MDk5MA=="}, "originalCommit": {"oid": "e13ca920f02c00cfc883bc55499d7c179b7a8564"}, "originalPosition": 233}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ0ODAyMQ==", "bodyText": "I don't think this is a correct behavior, you can take a look to kafka.", "url": "https://github.com/apache/flink/pull/12010#discussion_r422448021", "createdAt": "2020-05-09T03:34:24Z", "author": {"login": "JingsongLi"}, "path": "flink-formats/flink-json/src/main/java/org/apache/flink/formats/json/JsonFileSystemFormatFactory.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.json;\n+\n+import org.apache.flink.api.common.io.DelimitedInputFormat;\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.api.java.typeutils.GenericTypeInfo;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.FileSystemFormatFactory;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.utils.PartitionPathUtils;\n+\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+import static org.apache.flink.table.descriptors.JsonValidator.FORMAT_FAIL_ON_MISSING_FIELD;\n+import static org.apache.flink.table.descriptors.JsonValidator.FORMAT_IGNORE_PARSE_ERRORS;\n+\n+/**\n+ * Factory to build reader/writer to read/write json format file.\n+ */\n+public class JsonFileSystemFormatFactory implements FileSystemFormatFactory {\n+\t@Override\n+\tpublic Map<String, String> requiredContext() {\n+\t\tMap<String, String> context = new HashMap<>();\n+\t\tcontext.put(FORMAT, \"json\");\n+\t\treturn context;\n+\t}\n+\n+\t@Override\n+\tpublic List<String> supportedProperties() {\n+\t\tArrayList<String> properties = new ArrayList<>();\n+\t\tproperties.add(FORMAT_FAIL_ON_MISSING_FIELD);\n+\t\tproperties.add(FORMAT_IGNORE_PARSE_ERRORS);\n+\t\treturn properties;\n+\t}\n+\n+\t@Override\n+\tpublic InputFormat<RowData, ?> createReader(ReaderContext context) {\n+\t\tDescriptorProperties properties = getValidatedProperties(context.getFormatProperties());\n+\t\tboolean failOnMissingField = properties.getOptionalBoolean(FORMAT_FAIL_ON_MISSING_FIELD).orElse(false);\n+\t\tboolean ignoreParseErrors = properties.getOptionalBoolean(FORMAT_IGNORE_PARSE_ERRORS).orElse(false);\n+\n+\t\t// deal partition fields\n+\t\tTableSchema tableSchema = context.getSchema();\n+\t\tList<DataType> fieldTypes = Arrays.asList(tableSchema.getFieldDataTypes());\n+\t\tList<String> fieldNames = Arrays.asList(tableSchema.getFieldNames());\n+\t\tString[] nonPartFieldNames = fieldNames.stream()\n+\t\t\t.filter(name -> !context.getPartitionKeys().contains(name)).toArray(String[]::new);\n+\t\tDataType[] nonPartFieldTypes = Arrays.asList(nonPartFieldNames).stream()\n+\t\t\t.map(name -> fieldTypes.get(fieldNames.indexOf(name))).toArray(DataType[]::new);\n+\t\tRowType nonPartRowType = RowType.of(\n+\t\t\tArrays.asList(nonPartFieldTypes).stream()\n+\t\t\t\t.map(DataType::getLogicalType)\n+\t\t\t\t.toArray(LogicalType[]::new),\n+\t\t\tnonPartFieldNames);\n+\n+\t\t// deal project fields\n+\t\tint[] selectFields = context.getProjectFields();\n+\t\tList<String> partitionKeys = context.getPartitionKeys();\n+\t\tList<String> selectFieldNames = Arrays.stream(selectFields)\n+\t\t\t.mapToObj(fieldNames::get)\n+\t\t\t.collect(Collectors.toList());\n+\t\tList<String> jsonSelectFieldNames = selectFieldNames.stream()\n+\t\t\t.filter(name -> !partitionKeys.contains(name)).collect(Collectors.toList());\n+\t\tint[] jsonFieldMapping =  jsonSelectFieldNames.stream().mapToInt(selectFieldNames::indexOf).toArray();\n+\n+\t\tJsonRowDataDeserializationSchema  deserializationSchema = new JsonRowDataDeserializationSchema(\n+\t\t\tnonPartRowType,\n+\t\t\tnew GenericTypeInfo(GenericRowData.class),\n+\t\t\tfailOnMissingField,\n+\t\t\tignoreParseErrors);\n+\n+\t\treturn new JsonInputFormat(\n+\t\t\tcontext.getPaths(),\n+\t\t\tfieldTypes,\n+\t\t\tfieldNames,\n+\t\t\tselectFields,\n+\t\t\tpartitionKeys,\n+\t\t\tcontext.getDefaultPartName(),\n+\t\t\tcontext.getPushedDownLimit(),\n+\t\t\tjsonSelectFieldNames,\n+\t\t\tjsonFieldMapping,\n+\t\t\tdeserializationSchema);\n+\t}\n+\n+\t@Override\n+\tpublic Optional<Encoder<RowData>> createEncoder(WriterContext context) {\n+\t\tTableSchema nonPartKeySchema = new TableSchema.Builder()\n+\t\t\t.fields(context.getFieldNamesWithoutPartKeys(), context.getFieldTypesWithoutPartKeys())\n+\t\t\t.build();\n+\t\tLogicalType[] logicalTypes = Arrays.asList(nonPartKeySchema.getFieldDataTypes())\n+\t\t\t.stream().map(DataType::getLogicalType).toArray(LogicalType[]::new);\n+\t\tRowType nonPartRowType = RowType.of(logicalTypes, nonPartKeySchema.getFieldNames());\n+\t\treturn Optional.of(new JsonRowDataEncoder(new JsonRowDataSerializationSchema(nonPartRowType)));\n+\t}\n+\n+\t@Override\n+\tpublic Optional<BulkWriter.Factory<RowData>> createBulkWriterFactory(WriterContext context) {\n+\t\treturn Optional.empty();\n+\t}\n+\n+\t@Override\n+\tpublic boolean supportsSchemaDerivation() {\n+\t\treturn true;\n+\t}\n+\n+\tprivate static DescriptorProperties getValidatedProperties(Map<String, String> propertiesMap) {\n+\t\tfinal DescriptorProperties properties = new DescriptorProperties(true);\n+\t\tproperties.putProperties(propertiesMap);\n+\t\tproperties.validateBoolean(FORMAT_FAIL_ON_MISSING_FIELD, true);\n+\t\tproperties.validateBoolean(FORMAT_IGNORE_PARSE_ERRORS, true);\n+\t\treturn properties;\n+\t}\n+\n+\t/**\n+\t * A {@link JsonInputFormat} is responsible to read {@link RowData} records\n+\t * from json format files.\n+\t */\n+\tpublic static class JsonInputFormat extends DelimitedInputFormat<RowData> {\n+\t\t/**\n+\t\t * Code of \\r, used to remove \\r from a line when the line ends with \\r\\n.\n+\t\t */\n+\t\tprivate static final byte CARRIAGE_RETURN = (byte) '\\r';\n+\n+\t\t/**\n+\t\t * Code of \\n, used to identify if \\n is used as delimiter.\n+\t\t */\n+\t\tprivate static final byte NEW_LINE = (byte) '\\n';\n+\n+\t\tprivate final List<DataType> fieldTypes;\n+\t\tprivate final List<String> fieldNames;\n+\t\tprivate final int[] selectFields;\n+\t\tprivate final List<String> partitionKeys;\n+\t\tprivate final String defaultPartValue;\n+\t\tprivate final long limit;\n+\t\tprivate final List<String> jsonSelectFieldNames;\n+\t\tprivate final int[] jsonFieldMapping;\n+\t\tprivate final JsonRowDataDeserializationSchema deserializationSchema;\n+\n+\t\tprivate transient boolean end;\n+\t\tprivate transient long emitted;\n+\t\t// reuse object for per record\n+\t\tprivate transient GenericRowData rowData;\n+\n+\t\tpublic JsonInputFormat(\n+\t\t\t\tPath[] filePaths,\n+\t\t\t\tList<DataType> fieldTypes,\n+\t\t\t\tList<String> fieldNames,\n+\t\t\t\tint[] selectFields,\n+\t\t\t\tList<String> partitionKeys,\n+\t\t\t\tString defaultPartValue,\n+\t\t\t\tlong limit,\n+\t\t\t\tList<String> jsonSelectFieldNames,\n+\t\t\t\tint[] jsonFieldMapping,\n+\t\t\t\tJsonRowDataDeserializationSchema deserializationSchema) {\n+\t\t\tsuper.setFilePaths(filePaths);\n+\t\t\tthis.fieldTypes = fieldTypes;\n+\t\t\tthis.fieldNames = fieldNames;\n+\t\t\tthis.selectFields = selectFields;\n+\t\t\tthis.partitionKeys = partitionKeys;\n+\t\t\tthis.defaultPartValue = defaultPartValue;\n+\t\t\tthis.limit = limit;\n+\t\t\tthis.jsonSelectFieldNames = jsonSelectFieldNames;\n+\t\t\tthis.jsonFieldMapping = jsonFieldMapping;\n+\t\t\tthis.deserializationSchema = deserializationSchema;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic boolean supportsMultiPaths() {\n+\t\t\treturn true;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void open(FileInputSplit split) throws IOException {\n+\t\t\tsuper.open(split);\n+\t\t\tthis.end = false;\n+\t\t\tthis.emitted = 0L;\n+\t\t\tthis.rowData = PartitionPathUtils.fillPartitionValueForRecord(fieldNames, fieldTypes, selectFields,\n+\t\t\t\tpartitionKeys, currentSplit.getPath(), defaultPartValue);\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic boolean reachedEnd() {\n+\t\t\treturn emitted >= limit || end;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic RowData readRecord(RowData reuse, byte[] bytes, int offset, int numBytes) throws IOException {\n+\t\t\tGenericRowData returnRecord = rowData;\n+\n+\t\t\tif (this.getDelimiter() != null && this.getDelimiter().length == 1\n+\t\t\t\t&& this.getDelimiter()[0] == NEW_LINE && offset + numBytes >= 1\n+\t\t\t\t&& bytes[offset + numBytes - 1] == CARRIAGE_RETURN){\n+\t\t\t\tnumBytes -= 1;\n+\t\t\t}\n+\t\t\tbyte[] trimBytes = Arrays.copyOfRange(bytes, offset, offset + numBytes);\n+\t\t\tGenericRowData jsonRow = (GenericRowData) deserializationSchema.deserialize(trimBytes);\n+\t\t\tif (jsonRow != null) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ0MDk5MA=="}, "originalCommit": {"oid": "e13ca920f02c00cfc883bc55499d7c179b7a8564"}, "originalPosition": 233}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMDA3ODczOnYy", "diffSide": "RIGHT", "path": "flink-formats/flink-json/src/main/java/org/apache/flink/formats/json/JsonFileSystemFormatFactory.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQwMjoxMDozNVrOGS3xdQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQwMzozMTo1MFrOGS4K0g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ0MTMzMw==", "bodyText": "Please think about how to reuse codes to csv.", "url": "https://github.com/apache/flink/pull/12010#discussion_r422441333", "createdAt": "2020-05-09T02:10:35Z", "author": {"login": "JingsongLi"}, "path": "flink-formats/flink-json/src/main/java/org/apache/flink/formats/json/JsonFileSystemFormatFactory.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.json;\n+\n+import org.apache.flink.api.common.io.DelimitedInputFormat;\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.api.java.typeutils.GenericTypeInfo;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.FileSystemFormatFactory;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.utils.PartitionPathUtils;\n+\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+import static org.apache.flink.table.descriptors.JsonValidator.FORMAT_FAIL_ON_MISSING_FIELD;\n+import static org.apache.flink.table.descriptors.JsonValidator.FORMAT_IGNORE_PARSE_ERRORS;\n+\n+/**\n+ * Factory to build reader/writer to read/write json format file.\n+ */\n+public class JsonFileSystemFormatFactory implements FileSystemFormatFactory {\n+\t@Override\n+\tpublic Map<String, String> requiredContext() {\n+\t\tMap<String, String> context = new HashMap<>();\n+\t\tcontext.put(FORMAT, \"json\");\n+\t\treturn context;\n+\t}\n+\n+\t@Override\n+\tpublic List<String> supportedProperties() {\n+\t\tArrayList<String> properties = new ArrayList<>();\n+\t\tproperties.add(FORMAT_FAIL_ON_MISSING_FIELD);\n+\t\tproperties.add(FORMAT_IGNORE_PARSE_ERRORS);\n+\t\treturn properties;\n+\t}\n+\n+\t@Override\n+\tpublic InputFormat<RowData, ?> createReader(ReaderContext context) {\n+\t\tDescriptorProperties properties = getValidatedProperties(context.getFormatProperties());\n+\t\tboolean failOnMissingField = properties.getOptionalBoolean(FORMAT_FAIL_ON_MISSING_FIELD).orElse(false);\n+\t\tboolean ignoreParseErrors = properties.getOptionalBoolean(FORMAT_IGNORE_PARSE_ERRORS).orElse(false);\n+\n+\t\t// deal partition fields\n+\t\tTableSchema tableSchema = context.getSchema();\n+\t\tList<DataType> fieldTypes = Arrays.asList(tableSchema.getFieldDataTypes());\n+\t\tList<String> fieldNames = Arrays.asList(tableSchema.getFieldNames());\n+\t\tString[] nonPartFieldNames = fieldNames.stream()\n+\t\t\t.filter(name -> !context.getPartitionKeys().contains(name)).toArray(String[]::new);\n+\t\tDataType[] nonPartFieldTypes = Arrays.asList(nonPartFieldNames).stream()\n+\t\t\t.map(name -> fieldTypes.get(fieldNames.indexOf(name))).toArray(DataType[]::new);\n+\t\tRowType nonPartRowType = RowType.of(\n+\t\t\tArrays.asList(nonPartFieldTypes).stream()\n+\t\t\t\t.map(DataType::getLogicalType)\n+\t\t\t\t.toArray(LogicalType[]::new),\n+\t\t\tnonPartFieldNames);\n+\n+\t\t// deal project fields\n+\t\tint[] selectFields = context.getProjectFields();\n+\t\tList<String> partitionKeys = context.getPartitionKeys();\n+\t\tList<String> selectFieldNames = Arrays.stream(selectFields)\n+\t\t\t.mapToObj(fieldNames::get)\n+\t\t\t.collect(Collectors.toList());\n+\t\tList<String> jsonSelectFieldNames = selectFieldNames.stream()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e13ca920f02c00cfc883bc55499d7c179b7a8564"}, "originalPosition": 98}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ0NzgyNg==", "bodyText": "nice tips", "url": "https://github.com/apache/flink/pull/12010#discussion_r422447826", "createdAt": "2020-05-09T03:31:50Z", "author": {"login": "leonardBang"}, "path": "flink-formats/flink-json/src/main/java/org/apache/flink/formats/json/JsonFileSystemFormatFactory.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.json;\n+\n+import org.apache.flink.api.common.io.DelimitedInputFormat;\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.api.java.typeutils.GenericTypeInfo;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.FileSystemFormatFactory;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.utils.PartitionPathUtils;\n+\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+import static org.apache.flink.table.descriptors.JsonValidator.FORMAT_FAIL_ON_MISSING_FIELD;\n+import static org.apache.flink.table.descriptors.JsonValidator.FORMAT_IGNORE_PARSE_ERRORS;\n+\n+/**\n+ * Factory to build reader/writer to read/write json format file.\n+ */\n+public class JsonFileSystemFormatFactory implements FileSystemFormatFactory {\n+\t@Override\n+\tpublic Map<String, String> requiredContext() {\n+\t\tMap<String, String> context = new HashMap<>();\n+\t\tcontext.put(FORMAT, \"json\");\n+\t\treturn context;\n+\t}\n+\n+\t@Override\n+\tpublic List<String> supportedProperties() {\n+\t\tArrayList<String> properties = new ArrayList<>();\n+\t\tproperties.add(FORMAT_FAIL_ON_MISSING_FIELD);\n+\t\tproperties.add(FORMAT_IGNORE_PARSE_ERRORS);\n+\t\treturn properties;\n+\t}\n+\n+\t@Override\n+\tpublic InputFormat<RowData, ?> createReader(ReaderContext context) {\n+\t\tDescriptorProperties properties = getValidatedProperties(context.getFormatProperties());\n+\t\tboolean failOnMissingField = properties.getOptionalBoolean(FORMAT_FAIL_ON_MISSING_FIELD).orElse(false);\n+\t\tboolean ignoreParseErrors = properties.getOptionalBoolean(FORMAT_IGNORE_PARSE_ERRORS).orElse(false);\n+\n+\t\t// deal partition fields\n+\t\tTableSchema tableSchema = context.getSchema();\n+\t\tList<DataType> fieldTypes = Arrays.asList(tableSchema.getFieldDataTypes());\n+\t\tList<String> fieldNames = Arrays.asList(tableSchema.getFieldNames());\n+\t\tString[] nonPartFieldNames = fieldNames.stream()\n+\t\t\t.filter(name -> !context.getPartitionKeys().contains(name)).toArray(String[]::new);\n+\t\tDataType[] nonPartFieldTypes = Arrays.asList(nonPartFieldNames).stream()\n+\t\t\t.map(name -> fieldTypes.get(fieldNames.indexOf(name))).toArray(DataType[]::new);\n+\t\tRowType nonPartRowType = RowType.of(\n+\t\t\tArrays.asList(nonPartFieldTypes).stream()\n+\t\t\t\t.map(DataType::getLogicalType)\n+\t\t\t\t.toArray(LogicalType[]::new),\n+\t\t\tnonPartFieldNames);\n+\n+\t\t// deal project fields\n+\t\tint[] selectFields = context.getProjectFields();\n+\t\tList<String> partitionKeys = context.getPartitionKeys();\n+\t\tList<String> selectFieldNames = Arrays.stream(selectFields)\n+\t\t\t.mapToObj(fieldNames::get)\n+\t\t\t.collect(Collectors.toList());\n+\t\tList<String> jsonSelectFieldNames = selectFieldNames.stream()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ0MTMzMw=="}, "originalCommit": {"oid": "e13ca920f02c00cfc883bc55499d7c179b7a8564"}, "originalPosition": 98}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMDI2NDg1OnYy", "diffSide": "RIGHT", "path": "flink-formats/flink-json/src/main/java/org/apache/flink/formats/json/JsonFileSystemFormatFactory.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQwNzo1MToxM1rOGS5RYg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQwODowNDoxNFrOGS5WIg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ2NTg5MA==", "bodyText": "Should not return null only, if just return null, this source will been closed.\nYou should override nextRecord.", "url": "https://github.com/apache/flink/pull/12010#discussion_r422465890", "createdAt": "2020-05-09T07:51:13Z", "author": {"login": "JingsongLi"}, "path": "flink-formats/flink-json/src/main/java/org/apache/flink/formats/json/JsonFileSystemFormatFactory.java", "diffHunk": "@@ -0,0 +1,239 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.json;\n+\n+import org.apache.flink.api.common.io.DelimitedInputFormat;\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.api.java.typeutils.GenericTypeInfo;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.FileSystemFormatFactory;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.utils.PartitionPathUtils;\n+\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+import static org.apache.flink.table.descriptors.JsonValidator.FORMAT_FAIL_ON_MISSING_FIELD;\n+import static org.apache.flink.table.descriptors.JsonValidator.FORMAT_IGNORE_PARSE_ERRORS;\n+\n+/**\n+ * Factory to build reader/writer to read/write json format file.\n+ */\n+public class JsonFileSystemFormatFactory implements FileSystemFormatFactory {\n+\n+\t@Override\n+\tpublic Map<String, String> requiredContext() {\n+\t\tMap<String, String> context = new HashMap<>();\n+\t\tcontext.put(FORMAT, \"json\");\n+\t\treturn context;\n+\t}\n+\n+\t@Override\n+\tpublic List<String> supportedProperties() {\n+\t\tArrayList<String> properties = new ArrayList<>();\n+\t\tproperties.add(FORMAT_FAIL_ON_MISSING_FIELD);\n+\t\tproperties.add(FORMAT_IGNORE_PARSE_ERRORS);\n+\t\treturn properties;\n+\t}\n+\n+\t@Override\n+\tpublic InputFormat<RowData, ?> createReader(ReaderContext context) {\n+\t\tDescriptorProperties properties = getValidatedProperties(context.getFormatProperties());\n+\t\tboolean failOnMissingField = properties.getOptionalBoolean(FORMAT_FAIL_ON_MISSING_FIELD).orElse(false);\n+\t\tboolean ignoreParseErrors = properties.getOptionalBoolean(FORMAT_IGNORE_PARSE_ERRORS).orElse(false);\n+\n+\t\tRowType nonPartRowType = context.getRowTypeWithoutPartKeys();\n+\t\tJsonRowDataDeserializationSchema  deserializationSchema = new JsonRowDataDeserializationSchema(\n+\t\t\tnonPartRowType,\n+\t\t\tnew GenericTypeInfo(GenericRowData.class),\n+\t\t\tfailOnMissingField,\n+\t\t\tignoreParseErrors);\n+\n+\t\tint[] jsonSelectFieldToProjectFieldMapping = context.getNonPartFieldProjectMapping();\n+\t\tint[] jsonSelectFieldToJsonFieldMapping = context.getSelectFieldToNonPartFieldMapping();\n+\n+\t\treturn new JsonInputFormat(\n+\t\t\tcontext.getPaths(),\n+\t\t\tcontext.getSchema().getFieldDataTypes(),\n+\t\t\tcontext.getSchema().getFieldNames(),\n+\t\t\tcontext.getProjectFields(),\n+\t\t\tcontext.getPartitionKeys(),\n+\t\t\tcontext.getDefaultPartName(),\n+\t\t\tcontext.getPushedDownLimit(),\n+\t\t\tjsonSelectFieldToProjectFieldMapping,\n+\t\t\tjsonSelectFieldToJsonFieldMapping,\n+\t\t\tdeserializationSchema);\n+\t}\n+\n+\t@Override\n+\tpublic Optional<Encoder<RowData>> createEncoder(WriterContext context) {\n+\t\treturn Optional.of(new JsonRowDataEncoder(new JsonRowDataSerializationSchema(context.getNonPartRowType())));\n+\t}\n+\n+\t@Override\n+\tpublic Optional<BulkWriter.Factory<RowData>> createBulkWriterFactory(WriterContext context) {\n+\t\treturn Optional.empty();\n+\t}\n+\n+\t@Override\n+\tpublic boolean supportsSchemaDerivation() {\n+\t\treturn true;\n+\t}\n+\n+\tprivate static DescriptorProperties getValidatedProperties(Map<String, String> propertiesMap) {\n+\t\tfinal DescriptorProperties properties = new DescriptorProperties(true);\n+\t\tproperties.putProperties(propertiesMap);\n+\t\tproperties.validateBoolean(FORMAT_FAIL_ON_MISSING_FIELD, true);\n+\t\tproperties.validateBoolean(FORMAT_IGNORE_PARSE_ERRORS, true);\n+\t\treturn properties;\n+\t}\n+\n+\t/**\n+\t * A {@link JsonInputFormat} is responsible to read {@link RowData} records\n+\t * from json format files.\n+\t */\n+\tpublic static class JsonInputFormat extends DelimitedInputFormat<RowData> {\n+\t\t/**\n+\t\t * Code of \\r, used to remove \\r from a line when the line ends with \\r\\n.\n+\t\t */\n+\t\tprivate static final byte CARRIAGE_RETURN = (byte) '\\r';\n+\n+\t\t/**\n+\t\t * Code of \\n, used to identify if \\n is used as delimiter.\n+\t\t */\n+\t\tprivate static final byte NEW_LINE = (byte) '\\n';\n+\n+\t\tprivate final DataType[] fieldTypes;\n+\t\tprivate final String[] fieldNames;\n+\t\tprivate final int[] selectFields;\n+\t\tprivate final List<String> partitionKeys;\n+\t\tprivate final String defaultPartValue;\n+\t\tprivate final long limit;\n+\t\tprivate final int[] jsonSelectFieldToProjectFieldMapping;\n+\t\tprivate final int[] jsonSelectFieldToJsonFieldMapping;\n+\t\tprivate final JsonRowDataDeserializationSchema deserializationSchema;\n+\n+\t\tprivate transient boolean end;\n+\t\tprivate transient long emitted;\n+\t\t// reuse object for per record\n+\t\tprivate transient GenericRowData rowData;\n+\n+\t\tpublic JsonInputFormat(\n+\t\t\t\tPath[] filePaths,\n+\t\t\t\tDataType[] fieldTypes,\n+\t\t\t\tString[] fieldNames,\n+\t\t\t\tint[] selectFields,\n+\t\t\t\tList<String> partitionKeys,\n+\t\t\t\tString defaultPartValue,\n+\t\t\t\tlong limit,\n+\t\t\t\tint[] jsonSelectFieldToProjectFieldMapping,\n+\t\t\t\tint[] jsonSelectFieldToJsonFieldMapping,\n+\t\t\t\tJsonRowDataDeserializationSchema deserializationSchema) {\n+\t\t\tsuper.setFilePaths(filePaths);\n+\t\t\tthis.fieldTypes = fieldTypes;\n+\t\t\tthis.fieldNames = fieldNames;\n+\t\t\tthis.selectFields = selectFields;\n+\t\t\tthis.partitionKeys = partitionKeys;\n+\t\t\tthis.defaultPartValue = defaultPartValue;\n+\t\t\tthis.limit = limit;\n+\t\t\tthis.jsonSelectFieldToProjectFieldMapping = jsonSelectFieldToProjectFieldMapping;\n+\t\t\tthis.jsonSelectFieldToJsonFieldMapping = jsonSelectFieldToJsonFieldMapping;\n+\t\t\tthis.deserializationSchema = deserializationSchema;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic boolean supportsMultiPaths() {\n+\t\t\treturn true;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void open(FileInputSplit split) throws IOException {\n+\t\t\tsuper.open(split);\n+\t\t\tthis.end = false;\n+\t\t\tthis.emitted = 0L;\n+\t\t\tthis.rowData = PartitionPathUtils.fillPartitionValueForRecord(fieldNames, fieldTypes, selectFields,\n+\t\t\t\tpartitionKeys, currentSplit.getPath(), defaultPartValue);\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic boolean reachedEnd() {\n+\t\t\treturn emitted >= limit || end;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic RowData readRecord(RowData reuse, byte[] bytes, int offset, int numBytes) throws IOException {\n+\t\t\tGenericRowData returnRecord = rowData;\n+\t\t\t// remove \\r from a line when the line ends with \\r\\n\n+\t\t\tif (this.getDelimiter() != null && this.getDelimiter().length == 1\n+\t\t\t\t&& this.getDelimiter()[0] == NEW_LINE && offset + numBytes >= 1\n+\t\t\t\t&& bytes[offset + numBytes - 1] == CARRIAGE_RETURN){\n+\t\t\t\tnumBytes -= 1;\n+\t\t\t}\n+\t\t\tbyte[] trimBytes = Arrays.copyOfRange(bytes, offset, offset + numBytes);\n+\t\t\tGenericRowData jsonRow = (GenericRowData) deserializationSchema.deserialize(trimBytes);\n+\n+\t\t\tif (jsonRow == null) {\n+\t\t\t\t// if the record is null, simply just skip\n+\t\t\t\treturn null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0a0e23de47a9e838c81a4bf1244e50c98e689508"}, "originalPosition": 208}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ2Njg5Mg==", "bodyText": "So, we need import a loop to read util meet not null value as per loop?", "url": "https://github.com/apache/flink/pull/12010#discussion_r422466892", "createdAt": "2020-05-09T08:01:43Z", "author": {"login": "leonardBang"}, "path": "flink-formats/flink-json/src/main/java/org/apache/flink/formats/json/JsonFileSystemFormatFactory.java", "diffHunk": "@@ -0,0 +1,239 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.json;\n+\n+import org.apache.flink.api.common.io.DelimitedInputFormat;\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.api.java.typeutils.GenericTypeInfo;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.FileSystemFormatFactory;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.utils.PartitionPathUtils;\n+\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+import static org.apache.flink.table.descriptors.JsonValidator.FORMAT_FAIL_ON_MISSING_FIELD;\n+import static org.apache.flink.table.descriptors.JsonValidator.FORMAT_IGNORE_PARSE_ERRORS;\n+\n+/**\n+ * Factory to build reader/writer to read/write json format file.\n+ */\n+public class JsonFileSystemFormatFactory implements FileSystemFormatFactory {\n+\n+\t@Override\n+\tpublic Map<String, String> requiredContext() {\n+\t\tMap<String, String> context = new HashMap<>();\n+\t\tcontext.put(FORMAT, \"json\");\n+\t\treturn context;\n+\t}\n+\n+\t@Override\n+\tpublic List<String> supportedProperties() {\n+\t\tArrayList<String> properties = new ArrayList<>();\n+\t\tproperties.add(FORMAT_FAIL_ON_MISSING_FIELD);\n+\t\tproperties.add(FORMAT_IGNORE_PARSE_ERRORS);\n+\t\treturn properties;\n+\t}\n+\n+\t@Override\n+\tpublic InputFormat<RowData, ?> createReader(ReaderContext context) {\n+\t\tDescriptorProperties properties = getValidatedProperties(context.getFormatProperties());\n+\t\tboolean failOnMissingField = properties.getOptionalBoolean(FORMAT_FAIL_ON_MISSING_FIELD).orElse(false);\n+\t\tboolean ignoreParseErrors = properties.getOptionalBoolean(FORMAT_IGNORE_PARSE_ERRORS).orElse(false);\n+\n+\t\tRowType nonPartRowType = context.getRowTypeWithoutPartKeys();\n+\t\tJsonRowDataDeserializationSchema  deserializationSchema = new JsonRowDataDeserializationSchema(\n+\t\t\tnonPartRowType,\n+\t\t\tnew GenericTypeInfo(GenericRowData.class),\n+\t\t\tfailOnMissingField,\n+\t\t\tignoreParseErrors);\n+\n+\t\tint[] jsonSelectFieldToProjectFieldMapping = context.getNonPartFieldProjectMapping();\n+\t\tint[] jsonSelectFieldToJsonFieldMapping = context.getSelectFieldToNonPartFieldMapping();\n+\n+\t\treturn new JsonInputFormat(\n+\t\t\tcontext.getPaths(),\n+\t\t\tcontext.getSchema().getFieldDataTypes(),\n+\t\t\tcontext.getSchema().getFieldNames(),\n+\t\t\tcontext.getProjectFields(),\n+\t\t\tcontext.getPartitionKeys(),\n+\t\t\tcontext.getDefaultPartName(),\n+\t\t\tcontext.getPushedDownLimit(),\n+\t\t\tjsonSelectFieldToProjectFieldMapping,\n+\t\t\tjsonSelectFieldToJsonFieldMapping,\n+\t\t\tdeserializationSchema);\n+\t}\n+\n+\t@Override\n+\tpublic Optional<Encoder<RowData>> createEncoder(WriterContext context) {\n+\t\treturn Optional.of(new JsonRowDataEncoder(new JsonRowDataSerializationSchema(context.getNonPartRowType())));\n+\t}\n+\n+\t@Override\n+\tpublic Optional<BulkWriter.Factory<RowData>> createBulkWriterFactory(WriterContext context) {\n+\t\treturn Optional.empty();\n+\t}\n+\n+\t@Override\n+\tpublic boolean supportsSchemaDerivation() {\n+\t\treturn true;\n+\t}\n+\n+\tprivate static DescriptorProperties getValidatedProperties(Map<String, String> propertiesMap) {\n+\t\tfinal DescriptorProperties properties = new DescriptorProperties(true);\n+\t\tproperties.putProperties(propertiesMap);\n+\t\tproperties.validateBoolean(FORMAT_FAIL_ON_MISSING_FIELD, true);\n+\t\tproperties.validateBoolean(FORMAT_IGNORE_PARSE_ERRORS, true);\n+\t\treturn properties;\n+\t}\n+\n+\t/**\n+\t * A {@link JsonInputFormat} is responsible to read {@link RowData} records\n+\t * from json format files.\n+\t */\n+\tpublic static class JsonInputFormat extends DelimitedInputFormat<RowData> {\n+\t\t/**\n+\t\t * Code of \\r, used to remove \\r from a line when the line ends with \\r\\n.\n+\t\t */\n+\t\tprivate static final byte CARRIAGE_RETURN = (byte) '\\r';\n+\n+\t\t/**\n+\t\t * Code of \\n, used to identify if \\n is used as delimiter.\n+\t\t */\n+\t\tprivate static final byte NEW_LINE = (byte) '\\n';\n+\n+\t\tprivate final DataType[] fieldTypes;\n+\t\tprivate final String[] fieldNames;\n+\t\tprivate final int[] selectFields;\n+\t\tprivate final List<String> partitionKeys;\n+\t\tprivate final String defaultPartValue;\n+\t\tprivate final long limit;\n+\t\tprivate final int[] jsonSelectFieldToProjectFieldMapping;\n+\t\tprivate final int[] jsonSelectFieldToJsonFieldMapping;\n+\t\tprivate final JsonRowDataDeserializationSchema deserializationSchema;\n+\n+\t\tprivate transient boolean end;\n+\t\tprivate transient long emitted;\n+\t\t// reuse object for per record\n+\t\tprivate transient GenericRowData rowData;\n+\n+\t\tpublic JsonInputFormat(\n+\t\t\t\tPath[] filePaths,\n+\t\t\t\tDataType[] fieldTypes,\n+\t\t\t\tString[] fieldNames,\n+\t\t\t\tint[] selectFields,\n+\t\t\t\tList<String> partitionKeys,\n+\t\t\t\tString defaultPartValue,\n+\t\t\t\tlong limit,\n+\t\t\t\tint[] jsonSelectFieldToProjectFieldMapping,\n+\t\t\t\tint[] jsonSelectFieldToJsonFieldMapping,\n+\t\t\t\tJsonRowDataDeserializationSchema deserializationSchema) {\n+\t\t\tsuper.setFilePaths(filePaths);\n+\t\t\tthis.fieldTypes = fieldTypes;\n+\t\t\tthis.fieldNames = fieldNames;\n+\t\t\tthis.selectFields = selectFields;\n+\t\t\tthis.partitionKeys = partitionKeys;\n+\t\t\tthis.defaultPartValue = defaultPartValue;\n+\t\t\tthis.limit = limit;\n+\t\t\tthis.jsonSelectFieldToProjectFieldMapping = jsonSelectFieldToProjectFieldMapping;\n+\t\t\tthis.jsonSelectFieldToJsonFieldMapping = jsonSelectFieldToJsonFieldMapping;\n+\t\t\tthis.deserializationSchema = deserializationSchema;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic boolean supportsMultiPaths() {\n+\t\t\treturn true;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void open(FileInputSplit split) throws IOException {\n+\t\t\tsuper.open(split);\n+\t\t\tthis.end = false;\n+\t\t\tthis.emitted = 0L;\n+\t\t\tthis.rowData = PartitionPathUtils.fillPartitionValueForRecord(fieldNames, fieldTypes, selectFields,\n+\t\t\t\tpartitionKeys, currentSplit.getPath(), defaultPartValue);\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic boolean reachedEnd() {\n+\t\t\treturn emitted >= limit || end;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic RowData readRecord(RowData reuse, byte[] bytes, int offset, int numBytes) throws IOException {\n+\t\t\tGenericRowData returnRecord = rowData;\n+\t\t\t// remove \\r from a line when the line ends with \\r\\n\n+\t\t\tif (this.getDelimiter() != null && this.getDelimiter().length == 1\n+\t\t\t\t&& this.getDelimiter()[0] == NEW_LINE && offset + numBytes >= 1\n+\t\t\t\t&& bytes[offset + numBytes - 1] == CARRIAGE_RETURN){\n+\t\t\t\tnumBytes -= 1;\n+\t\t\t}\n+\t\t\tbyte[] trimBytes = Arrays.copyOfRange(bytes, offset, offset + numBytes);\n+\t\t\tGenericRowData jsonRow = (GenericRowData) deserializationSchema.deserialize(trimBytes);\n+\n+\t\t\tif (jsonRow == null) {\n+\t\t\t\t// if the record is null, simply just skip\n+\t\t\t\treturn null;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ2NTg5MA=="}, "originalCommit": {"oid": "0a0e23de47a9e838c81a4bf1244e50c98e689508"}, "originalPosition": 208}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ2NzEwNg==", "bodyText": "Yes, I think so.", "url": "https://github.com/apache/flink/pull/12010#discussion_r422467106", "createdAt": "2020-05-09T08:04:14Z", "author": {"login": "JingsongLi"}, "path": "flink-formats/flink-json/src/main/java/org/apache/flink/formats/json/JsonFileSystemFormatFactory.java", "diffHunk": "@@ -0,0 +1,239 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.json;\n+\n+import org.apache.flink.api.common.io.DelimitedInputFormat;\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.api.java.typeutils.GenericTypeInfo;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.FileSystemFormatFactory;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.utils.PartitionPathUtils;\n+\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+import static org.apache.flink.table.descriptors.JsonValidator.FORMAT_FAIL_ON_MISSING_FIELD;\n+import static org.apache.flink.table.descriptors.JsonValidator.FORMAT_IGNORE_PARSE_ERRORS;\n+\n+/**\n+ * Factory to build reader/writer to read/write json format file.\n+ */\n+public class JsonFileSystemFormatFactory implements FileSystemFormatFactory {\n+\n+\t@Override\n+\tpublic Map<String, String> requiredContext() {\n+\t\tMap<String, String> context = new HashMap<>();\n+\t\tcontext.put(FORMAT, \"json\");\n+\t\treturn context;\n+\t}\n+\n+\t@Override\n+\tpublic List<String> supportedProperties() {\n+\t\tArrayList<String> properties = new ArrayList<>();\n+\t\tproperties.add(FORMAT_FAIL_ON_MISSING_FIELD);\n+\t\tproperties.add(FORMAT_IGNORE_PARSE_ERRORS);\n+\t\treturn properties;\n+\t}\n+\n+\t@Override\n+\tpublic InputFormat<RowData, ?> createReader(ReaderContext context) {\n+\t\tDescriptorProperties properties = getValidatedProperties(context.getFormatProperties());\n+\t\tboolean failOnMissingField = properties.getOptionalBoolean(FORMAT_FAIL_ON_MISSING_FIELD).orElse(false);\n+\t\tboolean ignoreParseErrors = properties.getOptionalBoolean(FORMAT_IGNORE_PARSE_ERRORS).orElse(false);\n+\n+\t\tRowType nonPartRowType = context.getRowTypeWithoutPartKeys();\n+\t\tJsonRowDataDeserializationSchema  deserializationSchema = new JsonRowDataDeserializationSchema(\n+\t\t\tnonPartRowType,\n+\t\t\tnew GenericTypeInfo(GenericRowData.class),\n+\t\t\tfailOnMissingField,\n+\t\t\tignoreParseErrors);\n+\n+\t\tint[] jsonSelectFieldToProjectFieldMapping = context.getNonPartFieldProjectMapping();\n+\t\tint[] jsonSelectFieldToJsonFieldMapping = context.getSelectFieldToNonPartFieldMapping();\n+\n+\t\treturn new JsonInputFormat(\n+\t\t\tcontext.getPaths(),\n+\t\t\tcontext.getSchema().getFieldDataTypes(),\n+\t\t\tcontext.getSchema().getFieldNames(),\n+\t\t\tcontext.getProjectFields(),\n+\t\t\tcontext.getPartitionKeys(),\n+\t\t\tcontext.getDefaultPartName(),\n+\t\t\tcontext.getPushedDownLimit(),\n+\t\t\tjsonSelectFieldToProjectFieldMapping,\n+\t\t\tjsonSelectFieldToJsonFieldMapping,\n+\t\t\tdeserializationSchema);\n+\t}\n+\n+\t@Override\n+\tpublic Optional<Encoder<RowData>> createEncoder(WriterContext context) {\n+\t\treturn Optional.of(new JsonRowDataEncoder(new JsonRowDataSerializationSchema(context.getNonPartRowType())));\n+\t}\n+\n+\t@Override\n+\tpublic Optional<BulkWriter.Factory<RowData>> createBulkWriterFactory(WriterContext context) {\n+\t\treturn Optional.empty();\n+\t}\n+\n+\t@Override\n+\tpublic boolean supportsSchemaDerivation() {\n+\t\treturn true;\n+\t}\n+\n+\tprivate static DescriptorProperties getValidatedProperties(Map<String, String> propertiesMap) {\n+\t\tfinal DescriptorProperties properties = new DescriptorProperties(true);\n+\t\tproperties.putProperties(propertiesMap);\n+\t\tproperties.validateBoolean(FORMAT_FAIL_ON_MISSING_FIELD, true);\n+\t\tproperties.validateBoolean(FORMAT_IGNORE_PARSE_ERRORS, true);\n+\t\treturn properties;\n+\t}\n+\n+\t/**\n+\t * A {@link JsonInputFormat} is responsible to read {@link RowData} records\n+\t * from json format files.\n+\t */\n+\tpublic static class JsonInputFormat extends DelimitedInputFormat<RowData> {\n+\t\t/**\n+\t\t * Code of \\r, used to remove \\r from a line when the line ends with \\r\\n.\n+\t\t */\n+\t\tprivate static final byte CARRIAGE_RETURN = (byte) '\\r';\n+\n+\t\t/**\n+\t\t * Code of \\n, used to identify if \\n is used as delimiter.\n+\t\t */\n+\t\tprivate static final byte NEW_LINE = (byte) '\\n';\n+\n+\t\tprivate final DataType[] fieldTypes;\n+\t\tprivate final String[] fieldNames;\n+\t\tprivate final int[] selectFields;\n+\t\tprivate final List<String> partitionKeys;\n+\t\tprivate final String defaultPartValue;\n+\t\tprivate final long limit;\n+\t\tprivate final int[] jsonSelectFieldToProjectFieldMapping;\n+\t\tprivate final int[] jsonSelectFieldToJsonFieldMapping;\n+\t\tprivate final JsonRowDataDeserializationSchema deserializationSchema;\n+\n+\t\tprivate transient boolean end;\n+\t\tprivate transient long emitted;\n+\t\t// reuse object for per record\n+\t\tprivate transient GenericRowData rowData;\n+\n+\t\tpublic JsonInputFormat(\n+\t\t\t\tPath[] filePaths,\n+\t\t\t\tDataType[] fieldTypes,\n+\t\t\t\tString[] fieldNames,\n+\t\t\t\tint[] selectFields,\n+\t\t\t\tList<String> partitionKeys,\n+\t\t\t\tString defaultPartValue,\n+\t\t\t\tlong limit,\n+\t\t\t\tint[] jsonSelectFieldToProjectFieldMapping,\n+\t\t\t\tint[] jsonSelectFieldToJsonFieldMapping,\n+\t\t\t\tJsonRowDataDeserializationSchema deserializationSchema) {\n+\t\t\tsuper.setFilePaths(filePaths);\n+\t\t\tthis.fieldTypes = fieldTypes;\n+\t\t\tthis.fieldNames = fieldNames;\n+\t\t\tthis.selectFields = selectFields;\n+\t\t\tthis.partitionKeys = partitionKeys;\n+\t\t\tthis.defaultPartValue = defaultPartValue;\n+\t\t\tthis.limit = limit;\n+\t\t\tthis.jsonSelectFieldToProjectFieldMapping = jsonSelectFieldToProjectFieldMapping;\n+\t\t\tthis.jsonSelectFieldToJsonFieldMapping = jsonSelectFieldToJsonFieldMapping;\n+\t\t\tthis.deserializationSchema = deserializationSchema;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic boolean supportsMultiPaths() {\n+\t\t\treturn true;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void open(FileInputSplit split) throws IOException {\n+\t\t\tsuper.open(split);\n+\t\t\tthis.end = false;\n+\t\t\tthis.emitted = 0L;\n+\t\t\tthis.rowData = PartitionPathUtils.fillPartitionValueForRecord(fieldNames, fieldTypes, selectFields,\n+\t\t\t\tpartitionKeys, currentSplit.getPath(), defaultPartValue);\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic boolean reachedEnd() {\n+\t\t\treturn emitted >= limit || end;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic RowData readRecord(RowData reuse, byte[] bytes, int offset, int numBytes) throws IOException {\n+\t\t\tGenericRowData returnRecord = rowData;\n+\t\t\t// remove \\r from a line when the line ends with \\r\\n\n+\t\t\tif (this.getDelimiter() != null && this.getDelimiter().length == 1\n+\t\t\t\t&& this.getDelimiter()[0] == NEW_LINE && offset + numBytes >= 1\n+\t\t\t\t&& bytes[offset + numBytes - 1] == CARRIAGE_RETURN){\n+\t\t\t\tnumBytes -= 1;\n+\t\t\t}\n+\t\t\tbyte[] trimBytes = Arrays.copyOfRange(bytes, offset, offset + numBytes);\n+\t\t\tGenericRowData jsonRow = (GenericRowData) deserializationSchema.deserialize(trimBytes);\n+\n+\t\t\tif (jsonRow == null) {\n+\t\t\t\t// if the record is null, simply just skip\n+\t\t\t\treturn null;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ2NTg5MA=="}, "originalCommit": {"oid": "0a0e23de47a9e838c81a4bf1244e50c98e689508"}, "originalPosition": 208}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMDQ0Mjk1OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/FileSystemFormatFactory.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQxMjoyNzoxNFrOGS6uiQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQxMjoyNzoxNFrOGS6uiQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ4OTczNw==", "bodyText": "getFormatFieldNames", "url": "https://github.com/apache/flink/pull/12010#discussion_r422489737", "createdAt": "2020-05-09T12:27:14Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/FileSystemFormatFactory.java", "diffHunk": "@@ -102,6 +105,66 @@\n \t\t * The follow up operator will filter the records again.\n \t\t */\n \t\tList<Expression> getPushedDownFilters();\n+\n+\t\t/**\n+\t\t * Get field names without partition keys.\n+\t\t */\n+\t\tdefault String[] getFieldNamesWithoutPartKeys() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c6f1ab4f02b683d44ac5c987430b9d9eab1e8cd3"}, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMDQ0MzA1OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/FileSystemFormatFactory.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQxMjoyNzoyM1rOGS6ulA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQxMjoyNzoyM1rOGS6ulA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ4OTc0OA==", "bodyText": "getFormatFieldTypes", "url": "https://github.com/apache/flink/pull/12010#discussion_r422489748", "createdAt": "2020-05-09T12:27:23Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/FileSystemFormatFactory.java", "diffHunk": "@@ -102,6 +105,66 @@\n \t\t * The follow up operator will filter the records again.\n \t\t */\n \t\tList<Expression> getPushedDownFilters();\n+\n+\t\t/**\n+\t\t * Get field names without partition keys.\n+\t\t */\n+\t\tdefault String[] getFieldNamesWithoutPartKeys() {\n+\t\t\treturn Arrays.stream(getSchema().getFieldNames())\n+\t\t\t\t.filter(name -> !getPartitionKeys().contains(name))\n+\t\t\t\t.toArray(String[]::new);\n+\t\t}\n+\n+\t\t/**\n+\t\t * Get field types without partition keys.\n+\t\t */\n+\t\tdefault DataType[] getFieldTypesWithoutPartKeys() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c6f1ab4f02b683d44ac5c987430b9d9eab1e8cd3"}, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMDQ0MzI3OnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/FileSystemFormatFactory.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQxMjoyNzozNFrOGS6urg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQxMjoyNzozNFrOGS6urg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ4OTc3NA==", "bodyText": "getFormatRowType", "url": "https://github.com/apache/flink/pull/12010#discussion_r422489774", "createdAt": "2020-05-09T12:27:34Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/FileSystemFormatFactory.java", "diffHunk": "@@ -102,6 +105,66 @@\n \t\t * The follow up operator will filter the records again.\n \t\t */\n \t\tList<Expression> getPushedDownFilters();\n+\n+\t\t/**\n+\t\t * Get field names without partition keys.\n+\t\t */\n+\t\tdefault String[] getFieldNamesWithoutPartKeys() {\n+\t\t\treturn Arrays.stream(getSchema().getFieldNames())\n+\t\t\t\t.filter(name -> !getPartitionKeys().contains(name))\n+\t\t\t\t.toArray(String[]::new);\n+\t\t}\n+\n+\t\t/**\n+\t\t * Get field types without partition keys.\n+\t\t */\n+\t\tdefault DataType[] getFieldTypesWithoutPartKeys() {\n+\t\t\treturn Arrays.stream(getSchema().getFieldNames())\n+\t\t\t\t.filter(name -> !getPartitionKeys().contains(name))\n+\t\t\t\t.map(name -> getSchema().getFieldDataType(name).get())\n+\t\t\t\t.toArray(DataType[]::new);\n+\t\t}\n+\n+\t\t/**\n+\t\t * RowType of table that excludes partition key fields.\n+\t\t */\n+\t\tdefault RowType getNonPartRowType() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c6f1ab4f02b683d44ac5c987430b9d9eab1e8cd3"}, "originalPosition": 42}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMDQ0MzUzOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/FileSystemFormatFactory.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQxMjoyNzo1MFrOGS6u0A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQxMjoyNzo1MFrOGS6u0A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ4OTgwOA==", "bodyText": "These two methods should only one, otherwise it is very hard to understand.\nLeft only one: getFormatProjectFields", "url": "https://github.com/apache/flink/pull/12010#discussion_r422489808", "createdAt": "2020-05-09T12:27:50Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/FileSystemFormatFactory.java", "diffHunk": "@@ -102,6 +105,66 @@\n \t\t * The follow up operator will filter the records again.\n \t\t */\n \t\tList<Expression> getPushedDownFilters();\n+\n+\t\t/**\n+\t\t * Get field names without partition keys.\n+\t\t */\n+\t\tdefault String[] getFieldNamesWithoutPartKeys() {\n+\t\t\treturn Arrays.stream(getSchema().getFieldNames())\n+\t\t\t\t.filter(name -> !getPartitionKeys().contains(name))\n+\t\t\t\t.toArray(String[]::new);\n+\t\t}\n+\n+\t\t/**\n+\t\t * Get field types without partition keys.\n+\t\t */\n+\t\tdefault DataType[] getFieldTypesWithoutPartKeys() {\n+\t\t\treturn Arrays.stream(getSchema().getFieldNames())\n+\t\t\t\t.filter(name -> !getPartitionKeys().contains(name))\n+\t\t\t\t.map(name -> getSchema().getFieldDataType(name).get())\n+\t\t\t\t.toArray(DataType[]::new);\n+\t\t}\n+\n+\t\t/**\n+\t\t * RowType of table that excludes partition key fields.\n+\t\t */\n+\t\tdefault RowType getNonPartRowType() {\n+\t\t\treturn RowType.of(\n+\t\t\t\tArrays.stream(getFieldTypesWithoutPartKeys())\n+\t\t\t\t\t.map(DataType::getLogicalType)\n+\t\t\t\t\t.toArray(LogicalType[]::new),\n+\t\t\t\tgetFieldNamesWithoutPartKeys());\n+\t\t}\n+\n+\t\t/**\n+\t\t * Mapping from non-partition project fields index to all project fields index.\n+\t\t */\n+\t\tdefault int[] getNonPartFieldProjectMapping() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c6f1ab4f02b683d44ac5c987430b9d9eab1e8cd3"}, "originalPosition": 53}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMDQ0NTcyOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/FileSystemFormatFactory.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQxMjozMDoyNlrOGS6v2w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQxMjozMDoyNlrOGS6v2w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ5MDA3NQ==", "bodyText": "getFormatRowType\nAnd please modify getFieldNamesWithoutPartKeys  and getFieldTypesWithoutPartKeys", "url": "https://github.com/apache/flink/pull/12010#discussion_r422490075", "createdAt": "2020-05-09T12:30:26Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/FileSystemFormatFactory.java", "diffHunk": "@@ -142,5 +205,17 @@\n \t\t\t\t\t.map(name -> getSchema().getFieldDataType(name).get())\n \t\t\t\t\t.toArray(DataType[]::new);\n \t\t}\n+\n+\t\t/**\n+\t\t * Get RowType of the table without partition keys.\n+\t\t * @return\n+\t\t */\n+\t\tdefault RowType getNonPartRowType() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c6f1ab4f02b683d44ac5c987430b9d9eab1e8cd3"}, "originalPosition": 91}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMDQ0NjMyOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-common/src/main/java/org/apache/flink/table/utils/PartitionPathUtils.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQxMjozMTowMFrOGS6wIg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQxMjozMTowMFrOGS6wIg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ5MDE0Ng==", "bodyText": "LocalDate.parse?", "url": "https://github.com/apache/flink/pull/12010#discussion_r422490146", "createdAt": "2020-05-09T12:31:00Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-common/src/main/java/org/apache/flink/table/utils/PartitionPathUtils.java", "diffHunk": "@@ -201,6 +211,81 @@ public static String unescapePathName(String path) {\n \t\treturn ret;\n \t}\n \n+\t/**\n+\t * Extract partition value from path and fill to record.\n+\t * @param fieldNames record field names.\n+\t * @param fieldTypes record field types.\n+\t * @param selectFields the selected fields.\n+\t * @param partitionKeys the partition field names.\n+\t * @param path the file path that the partition located.\n+\t * @param defaultPartValue default value of partition field.\n+\t * @return the filled record.\n+\t */\n+\tpublic static GenericRowData fillPartitionValueForRecord(\n+\t\t\tString[] fieldNames,\n+\t\t\tDataType[] fieldTypes,\n+\t\t\tint[] selectFields,\n+\t\t\tList<String> partitionKeys,\n+\t\t\tPath path,\n+\t\t\tString defaultPartValue) {\n+\t\tGenericRowData record = new GenericRowData(selectFields.length);\n+\t\tLinkedHashMap<String, String> partSpec = PartitionPathUtils.extractPartitionSpecFromPath(path);\n+\t\tfor (int i = 0; i < selectFields.length; i++) {\n+\t\t\tint selectField = selectFields[i];\n+\t\t\tString name = fieldNames[selectField];\n+\t\t\tif (partitionKeys.contains(name)) {\n+\t\t\t\tString value = partSpec.get(name);\n+\t\t\t\tvalue = defaultPartValue.equals(value) ? null : value;\n+\t\t\t\trecord.setField(i, PartitionPathUtils.convertStringToInternalValue(value, fieldTypes[selectField]));\n+\t\t\t}\n+\t\t}\n+\t\treturn record;\n+\t}\n+\n+\t/**\n+\t * Restore partition value from string and type.\n+\t *\n+\t * @param valStr string partition value.\n+\t * @param type type of partition field.\n+\t * @return partition value.\n+\t */\n+\tpublic static Object convertStringToInternalValue(String valStr, DataType type) {\n+\t\tif (valStr == null) {\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\tLogicalTypeRoot typeRoot = type.getLogicalType().getTypeRoot();\n+\t\tswitch (typeRoot) {\n+\t\t\tcase CHAR:\n+\t\t\tcase VARCHAR:\n+\t\t\t\treturn StringData.fromString(valStr);\n+\t\t\tcase BOOLEAN:\n+\t\t\t\treturn Boolean.parseBoolean(valStr);\n+\t\t\tcase TINYINT:\n+\t\t\t\treturn Byte.parseByte(valStr);\n+\t\t\tcase SMALLINT:\n+\t\t\t\treturn Short.parseShort(valStr);\n+\t\t\tcase INTEGER:\n+\t\t\t\treturn Integer.parseInt(valStr);\n+\t\t\tcase BIGINT:\n+\t\t\t\treturn Long.parseLong(valStr);\n+\t\t\tcase FLOAT:\n+\t\t\t\treturn Float.parseFloat(valStr);\n+\t\t\tcase DOUBLE:\n+\t\t\t\treturn Double.parseDouble(valStr);\n+\t\t\tcase DATE:\n+\t\t\t\tLocalDate date = ISO_LOCAL_DATE.parse(valStr).query(TemporalQueries.localDate());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c6f1ab4f02b683d44ac5c987430b9d9eab1e8cd3"}, "originalPosition": 102}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMDQ0Njk4OnYy", "diffSide": "RIGHT", "path": "flink-formats/flink-json/src/main/java/org/apache/flink/formats/json/JsonFileSystemFormatFactory.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQxMjozMTo0N1rOGS6wdA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQxMjozMTo0N1rOGS6wdA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ5MDIyOA==", "bodyText": "I don't get this.\nPlease add cases.", "url": "https://github.com/apache/flink/pull/12010#discussion_r422490228", "createdAt": "2020-05-09T12:31:47Z", "author": {"login": "JingsongLi"}, "path": "flink-formats/flink-json/src/main/java/org/apache/flink/formats/json/JsonFileSystemFormatFactory.java", "diffHunk": "@@ -0,0 +1,242 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.json;\n+\n+import org.apache.flink.api.common.io.DelimitedInputFormat;\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.api.java.typeutils.GenericTypeInfo;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.FileSystemFormatFactory;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.utils.PartitionPathUtils;\n+\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+import static org.apache.flink.table.descriptors.JsonValidator.FORMAT_FAIL_ON_MISSING_FIELD;\n+import static org.apache.flink.table.descriptors.JsonValidator.FORMAT_IGNORE_PARSE_ERRORS;\n+\n+/**\n+ * Factory to build reader/writer to read/write json format file.\n+ */\n+public class JsonFileSystemFormatFactory implements FileSystemFormatFactory {\n+\n+\t@Override\n+\tpublic Map<String, String> requiredContext() {\n+\t\tMap<String, String> context = new HashMap<>();\n+\t\tcontext.put(FORMAT, \"json\");\n+\t\treturn context;\n+\t}\n+\n+\t@Override\n+\tpublic List<String> supportedProperties() {\n+\t\tArrayList<String> properties = new ArrayList<>();\n+\t\tproperties.add(FORMAT_FAIL_ON_MISSING_FIELD);\n+\t\tproperties.add(FORMAT_IGNORE_PARSE_ERRORS);\n+\t\treturn properties;\n+\t}\n+\n+\t@Override\n+\tpublic InputFormat<RowData, ?> createReader(ReaderContext context) {\n+\t\tDescriptorProperties properties = getValidatedProperties(context.getFormatProperties());\n+\t\tboolean failOnMissingField = properties.getOptionalBoolean(FORMAT_FAIL_ON_MISSING_FIELD).orElse(false);\n+\t\tboolean ignoreParseErrors = properties.getOptionalBoolean(FORMAT_IGNORE_PARSE_ERRORS).orElse(false);\n+\n+\t\tRowType nonPartRowType = context.getNonPartRowType();\n+\t\tJsonRowDataDeserializationSchema  deserializationSchema = new JsonRowDataDeserializationSchema(\n+\t\t\tnonPartRowType,\n+\t\t\tnew GenericTypeInfo(GenericRowData.class),\n+\t\t\tfailOnMissingField,\n+\t\t\tignoreParseErrors);\n+\n+\t\tint[] jsonSelectFieldToProjectFieldMapping = context.getNonPartFieldProjectMapping();\n+\t\tint[] jsonSelectFieldToJsonFieldMapping = context.getSelectFieldToNonPartFieldMapping();\n+\n+\t\treturn new JsonInputFormat(\n+\t\t\tcontext.getPaths(),\n+\t\t\tcontext.getSchema().getFieldDataTypes(),\n+\t\t\tcontext.getSchema().getFieldNames(),\n+\t\t\tcontext.getProjectFields(),\n+\t\t\tcontext.getPartitionKeys(),\n+\t\t\tcontext.getDefaultPartName(),\n+\t\t\tcontext.getPushedDownLimit(),\n+\t\t\tjsonSelectFieldToProjectFieldMapping,\n+\t\t\tjsonSelectFieldToJsonFieldMapping,\n+\t\t\tdeserializationSchema);\n+\t}\n+\n+\t@Override\n+\tpublic Optional<Encoder<RowData>> createEncoder(WriterContext context) {\n+\t\treturn Optional.of(new JsonRowDataEncoder(new JsonRowDataSerializationSchema(context.getNonPartRowType())));\n+\t}\n+\n+\t@Override\n+\tpublic Optional<BulkWriter.Factory<RowData>> createBulkWriterFactory(WriterContext context) {\n+\t\treturn Optional.empty();\n+\t}\n+\n+\t@Override\n+\tpublic boolean supportsSchemaDerivation() {\n+\t\treturn true;\n+\t}\n+\n+\tprivate static DescriptorProperties getValidatedProperties(Map<String, String> propertiesMap) {\n+\t\tfinal DescriptorProperties properties = new DescriptorProperties(true);\n+\t\tproperties.putProperties(propertiesMap);\n+\t\tproperties.validateBoolean(FORMAT_FAIL_ON_MISSING_FIELD, true);\n+\t\tproperties.validateBoolean(FORMAT_IGNORE_PARSE_ERRORS, true);\n+\t\treturn properties;\n+\t}\n+\n+\t/**\n+\t * A {@link JsonInputFormat} is responsible to read {@link RowData} records\n+\t * from json format files.\n+\t */\n+\tpublic static class JsonInputFormat extends DelimitedInputFormat<RowData> {\n+\t\t/**\n+\t\t * Code of \\r, used to remove \\r from a line when the line ends with \\r\\n.\n+\t\t */\n+\t\tprivate static final byte CARRIAGE_RETURN = (byte) '\\r';\n+\n+\t\t/**\n+\t\t * Code of \\n, used to identify if \\n is used as delimiter.\n+\t\t */\n+\t\tprivate static final byte NEW_LINE = (byte) '\\n';\n+\n+\t\tprivate final DataType[] fieldTypes;\n+\t\tprivate final String[] fieldNames;\n+\t\tprivate final int[] selectFields;\n+\t\tprivate final List<String> partitionKeys;\n+\t\tprivate final String defaultPartValue;\n+\t\tprivate final long limit;\n+\t\tprivate final int[] jsonSelectFieldToProjectFieldMapping;\n+\t\tprivate final int[] jsonSelectFieldToJsonFieldMapping;\n+\t\tprivate final JsonRowDataDeserializationSchema deserializationSchema;\n+\n+\t\tprivate transient boolean end;\n+\t\tprivate transient long emitted;\n+\t\t// reuse object for per record\n+\t\tprivate transient GenericRowData rowData;\n+\n+\t\tpublic JsonInputFormat(\n+\t\t\t\tPath[] filePaths,\n+\t\t\t\tDataType[] fieldTypes,\n+\t\t\t\tString[] fieldNames,\n+\t\t\t\tint[] selectFields,\n+\t\t\t\tList<String> partitionKeys,\n+\t\t\t\tString defaultPartValue,\n+\t\t\t\tlong limit,\n+\t\t\t\tint[] jsonSelectFieldToProjectFieldMapping,\n+\t\t\t\tint[] jsonSelectFieldToJsonFieldMapping,\n+\t\t\t\tJsonRowDataDeserializationSchema deserializationSchema) {\n+\t\t\tsuper.setFilePaths(filePaths);\n+\t\t\tthis.fieldTypes = fieldTypes;\n+\t\t\tthis.fieldNames = fieldNames;\n+\t\t\tthis.selectFields = selectFields;\n+\t\t\tthis.partitionKeys = partitionKeys;\n+\t\t\tthis.defaultPartValue = defaultPartValue;\n+\t\t\tthis.limit = limit;\n+\t\t\tthis.jsonSelectFieldToProjectFieldMapping = jsonSelectFieldToProjectFieldMapping;\n+\t\t\tthis.jsonSelectFieldToJsonFieldMapping = jsonSelectFieldToJsonFieldMapping;\n+\t\t\tthis.deserializationSchema = deserializationSchema;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic boolean supportsMultiPaths() {\n+\t\t\treturn true;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void open(FileInputSplit split) throws IOException {\n+\t\t\tsuper.open(split);\n+\t\t\tthis.end = false;\n+\t\t\tthis.emitted = 0L;\n+\t\t\tthis.rowData = PartitionPathUtils.fillPartitionValueForRecord(fieldNames, fieldTypes, selectFields,\n+\t\t\t\tpartitionKeys, currentSplit.getPath(), defaultPartValue);\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic boolean reachedEnd() {\n+\t\t\treturn emitted >= limit || end;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic RowData readRecord(RowData reuse, byte[] bytes, int offset, int numBytes) throws IOException {\n+\t\t\tGenericRowData returnRecord = null;\n+\t\t\tdo {\n+\t\t\t\t// remove \\r from a line when the line ends with \\r\\n\n+\t\t\t\tif (this.getDelimiter() != null && this.getDelimiter().length == 1\n+\t\t\t\t\t&& this.getDelimiter()[0] == NEW_LINE && offset + numBytes >= 1\n+\t\t\t\t\t&& bytes[offset + numBytes - 1] == CARRIAGE_RETURN){\n+\t\t\t\t\tnumBytes -= 1;\n+\t\t\t\t}\n+\t\t\t\tbyte[] trimBytes = Arrays.copyOfRange(bytes, offset, offset + numBytes);\n+\t\t\t\tGenericRowData jsonRow = (GenericRowData) deserializationSchema.deserialize(trimBytes);\n+\n+\t\t\t\tif (jsonRow != null) {\n+\t\t\t\t\t// if the record is null, simply just skip\n+\t\t\t\t\treturnRecord = rowData;\n+\t\t\t\t\tfor (int i = 0; i < jsonSelectFieldToJsonFieldMapping.length; i++) {\n+\t\t\t\t\t\treturnRecord.setField(jsonSelectFieldToProjectFieldMapping[i],\n+\t\t\t\t\t\t\tjsonRow.getField(jsonSelectFieldToJsonFieldMapping[i]));\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t} while (returnRecord == null && !reachedEnd());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c6f1ab4f02b683d44ac5c987430b9d9eab1e8cd3"}, "originalPosition": 215}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMDQ0OTYzOnYy", "diffSide": "RIGHT", "path": "flink-formats/flink-json/src/main/java/org/apache/flink/formats/json/JsonFileSystemFormatFactory.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQxMjozNDo1MVrOGS6xvg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQxNjowMDowNFrOGS8JLw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ5MDU1OA==", "bodyText": "RowData readRecord() {\n...\n  if (jsonRow == null) {\n    return null;\n  }\n...\n}\n\nRowData nextRecord() {\n   while (true) {\n      if (readLine()) {\n\t\t\tRowData row = readRecord(record, this.currBuffer, this.currOffset, this.currLen);\n             if (row == null) {\n                 continue;\n             } else {\n                return row; \n            }\n\t\t} else {\n\t\t\tthis.end = true;\n\t\t\treturn null;\n\t\t}\n  }\n}", "url": "https://github.com/apache/flink/pull/12010#discussion_r422490558", "createdAt": "2020-05-09T12:34:51Z", "author": {"login": "JingsongLi"}, "path": "flink-formats/flink-json/src/main/java/org/apache/flink/formats/json/JsonFileSystemFormatFactory.java", "diffHunk": "@@ -0,0 +1,242 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.json;\n+\n+import org.apache.flink.api.common.io.DelimitedInputFormat;\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.api.java.typeutils.GenericTypeInfo;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.FileSystemFormatFactory;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.utils.PartitionPathUtils;\n+\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+import static org.apache.flink.table.descriptors.JsonValidator.FORMAT_FAIL_ON_MISSING_FIELD;\n+import static org.apache.flink.table.descriptors.JsonValidator.FORMAT_IGNORE_PARSE_ERRORS;\n+\n+/**\n+ * Factory to build reader/writer to read/write json format file.\n+ */\n+public class JsonFileSystemFormatFactory implements FileSystemFormatFactory {\n+\n+\t@Override\n+\tpublic Map<String, String> requiredContext() {\n+\t\tMap<String, String> context = new HashMap<>();\n+\t\tcontext.put(FORMAT, \"json\");\n+\t\treturn context;\n+\t}\n+\n+\t@Override\n+\tpublic List<String> supportedProperties() {\n+\t\tArrayList<String> properties = new ArrayList<>();\n+\t\tproperties.add(FORMAT_FAIL_ON_MISSING_FIELD);\n+\t\tproperties.add(FORMAT_IGNORE_PARSE_ERRORS);\n+\t\treturn properties;\n+\t}\n+\n+\t@Override\n+\tpublic InputFormat<RowData, ?> createReader(ReaderContext context) {\n+\t\tDescriptorProperties properties = getValidatedProperties(context.getFormatProperties());\n+\t\tboolean failOnMissingField = properties.getOptionalBoolean(FORMAT_FAIL_ON_MISSING_FIELD).orElse(false);\n+\t\tboolean ignoreParseErrors = properties.getOptionalBoolean(FORMAT_IGNORE_PARSE_ERRORS).orElse(false);\n+\n+\t\tRowType nonPartRowType = context.getNonPartRowType();\n+\t\tJsonRowDataDeserializationSchema  deserializationSchema = new JsonRowDataDeserializationSchema(\n+\t\t\tnonPartRowType,\n+\t\t\tnew GenericTypeInfo(GenericRowData.class),\n+\t\t\tfailOnMissingField,\n+\t\t\tignoreParseErrors);\n+\n+\t\tint[] jsonSelectFieldToProjectFieldMapping = context.getNonPartFieldProjectMapping();\n+\t\tint[] jsonSelectFieldToJsonFieldMapping = context.getSelectFieldToNonPartFieldMapping();\n+\n+\t\treturn new JsonInputFormat(\n+\t\t\tcontext.getPaths(),\n+\t\t\tcontext.getSchema().getFieldDataTypes(),\n+\t\t\tcontext.getSchema().getFieldNames(),\n+\t\t\tcontext.getProjectFields(),\n+\t\t\tcontext.getPartitionKeys(),\n+\t\t\tcontext.getDefaultPartName(),\n+\t\t\tcontext.getPushedDownLimit(),\n+\t\t\tjsonSelectFieldToProjectFieldMapping,\n+\t\t\tjsonSelectFieldToJsonFieldMapping,\n+\t\t\tdeserializationSchema);\n+\t}\n+\n+\t@Override\n+\tpublic Optional<Encoder<RowData>> createEncoder(WriterContext context) {\n+\t\treturn Optional.of(new JsonRowDataEncoder(new JsonRowDataSerializationSchema(context.getNonPartRowType())));\n+\t}\n+\n+\t@Override\n+\tpublic Optional<BulkWriter.Factory<RowData>> createBulkWriterFactory(WriterContext context) {\n+\t\treturn Optional.empty();\n+\t}\n+\n+\t@Override\n+\tpublic boolean supportsSchemaDerivation() {\n+\t\treturn true;\n+\t}\n+\n+\tprivate static DescriptorProperties getValidatedProperties(Map<String, String> propertiesMap) {\n+\t\tfinal DescriptorProperties properties = new DescriptorProperties(true);\n+\t\tproperties.putProperties(propertiesMap);\n+\t\tproperties.validateBoolean(FORMAT_FAIL_ON_MISSING_FIELD, true);\n+\t\tproperties.validateBoolean(FORMAT_IGNORE_PARSE_ERRORS, true);\n+\t\treturn properties;\n+\t}\n+\n+\t/**\n+\t * A {@link JsonInputFormat} is responsible to read {@link RowData} records\n+\t * from json format files.\n+\t */\n+\tpublic static class JsonInputFormat extends DelimitedInputFormat<RowData> {\n+\t\t/**\n+\t\t * Code of \\r, used to remove \\r from a line when the line ends with \\r\\n.\n+\t\t */\n+\t\tprivate static final byte CARRIAGE_RETURN = (byte) '\\r';\n+\n+\t\t/**\n+\t\t * Code of \\n, used to identify if \\n is used as delimiter.\n+\t\t */\n+\t\tprivate static final byte NEW_LINE = (byte) '\\n';\n+\n+\t\tprivate final DataType[] fieldTypes;\n+\t\tprivate final String[] fieldNames;\n+\t\tprivate final int[] selectFields;\n+\t\tprivate final List<String> partitionKeys;\n+\t\tprivate final String defaultPartValue;\n+\t\tprivate final long limit;\n+\t\tprivate final int[] jsonSelectFieldToProjectFieldMapping;\n+\t\tprivate final int[] jsonSelectFieldToJsonFieldMapping;\n+\t\tprivate final JsonRowDataDeserializationSchema deserializationSchema;\n+\n+\t\tprivate transient boolean end;\n+\t\tprivate transient long emitted;\n+\t\t// reuse object for per record\n+\t\tprivate transient GenericRowData rowData;\n+\n+\t\tpublic JsonInputFormat(\n+\t\t\t\tPath[] filePaths,\n+\t\t\t\tDataType[] fieldTypes,\n+\t\t\t\tString[] fieldNames,\n+\t\t\t\tint[] selectFields,\n+\t\t\t\tList<String> partitionKeys,\n+\t\t\t\tString defaultPartValue,\n+\t\t\t\tlong limit,\n+\t\t\t\tint[] jsonSelectFieldToProjectFieldMapping,\n+\t\t\t\tint[] jsonSelectFieldToJsonFieldMapping,\n+\t\t\t\tJsonRowDataDeserializationSchema deserializationSchema) {\n+\t\t\tsuper.setFilePaths(filePaths);\n+\t\t\tthis.fieldTypes = fieldTypes;\n+\t\t\tthis.fieldNames = fieldNames;\n+\t\t\tthis.selectFields = selectFields;\n+\t\t\tthis.partitionKeys = partitionKeys;\n+\t\t\tthis.defaultPartValue = defaultPartValue;\n+\t\t\tthis.limit = limit;\n+\t\t\tthis.jsonSelectFieldToProjectFieldMapping = jsonSelectFieldToProjectFieldMapping;\n+\t\t\tthis.jsonSelectFieldToJsonFieldMapping = jsonSelectFieldToJsonFieldMapping;\n+\t\t\tthis.deserializationSchema = deserializationSchema;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic boolean supportsMultiPaths() {\n+\t\t\treturn true;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void open(FileInputSplit split) throws IOException {\n+\t\t\tsuper.open(split);\n+\t\t\tthis.end = false;\n+\t\t\tthis.emitted = 0L;\n+\t\t\tthis.rowData = PartitionPathUtils.fillPartitionValueForRecord(fieldNames, fieldTypes, selectFields,\n+\t\t\t\tpartitionKeys, currentSplit.getPath(), defaultPartValue);\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic boolean reachedEnd() {\n+\t\t\treturn emitted >= limit || end;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic RowData readRecord(RowData reuse, byte[] bytes, int offset, int numBytes) throws IOException {\n+\t\t\tGenericRowData returnRecord = null;\n+\t\t\tdo {\n+\t\t\t\t// remove \\r from a line when the line ends with \\r\\n\n+\t\t\t\tif (this.getDelimiter() != null && this.getDelimiter().length == 1\n+\t\t\t\t\t&& this.getDelimiter()[0] == NEW_LINE && offset + numBytes >= 1\n+\t\t\t\t\t&& bytes[offset + numBytes - 1] == CARRIAGE_RETURN){\n+\t\t\t\t\tnumBytes -= 1;\n+\t\t\t\t}\n+\t\t\t\tbyte[] trimBytes = Arrays.copyOfRange(bytes, offset, offset + numBytes);\n+\t\t\t\tGenericRowData jsonRow = (GenericRowData) deserializationSchema.deserialize(trimBytes);\n+\n+\t\t\t\tif (jsonRow != null) {\n+\t\t\t\t\t// if the record is null, simply just skip\n+\t\t\t\t\treturnRecord = rowData;\n+\t\t\t\t\tfor (int i = 0; i < jsonSelectFieldToJsonFieldMapping.length; i++) {\n+\t\t\t\t\t\treturnRecord.setField(jsonSelectFieldToProjectFieldMapping[i],\n+\t\t\t\t\t\t\tjsonRow.getField(jsonSelectFieldToJsonFieldMapping[i]));\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t} while (returnRecord == null && !reachedEnd());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c6f1ab4f02b683d44ac5c987430b9d9eab1e8cd3"}, "originalPosition": 215}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjUxMjk0Mw==", "bodyText": "so ,by this we can support ignore-parser-error and open by default?\nthat's a good tips", "url": "https://github.com/apache/flink/pull/12010#discussion_r422512943", "createdAt": "2020-05-09T16:00:04Z", "author": {"login": "leonardBang"}, "path": "flink-formats/flink-json/src/main/java/org/apache/flink/formats/json/JsonFileSystemFormatFactory.java", "diffHunk": "@@ -0,0 +1,242 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.json;\n+\n+import org.apache.flink.api.common.io.DelimitedInputFormat;\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.api.java.typeutils.GenericTypeInfo;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.FileSystemFormatFactory;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.utils.PartitionPathUtils;\n+\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+import static org.apache.flink.table.descriptors.JsonValidator.FORMAT_FAIL_ON_MISSING_FIELD;\n+import static org.apache.flink.table.descriptors.JsonValidator.FORMAT_IGNORE_PARSE_ERRORS;\n+\n+/**\n+ * Factory to build reader/writer to read/write json format file.\n+ */\n+public class JsonFileSystemFormatFactory implements FileSystemFormatFactory {\n+\n+\t@Override\n+\tpublic Map<String, String> requiredContext() {\n+\t\tMap<String, String> context = new HashMap<>();\n+\t\tcontext.put(FORMAT, \"json\");\n+\t\treturn context;\n+\t}\n+\n+\t@Override\n+\tpublic List<String> supportedProperties() {\n+\t\tArrayList<String> properties = new ArrayList<>();\n+\t\tproperties.add(FORMAT_FAIL_ON_MISSING_FIELD);\n+\t\tproperties.add(FORMAT_IGNORE_PARSE_ERRORS);\n+\t\treturn properties;\n+\t}\n+\n+\t@Override\n+\tpublic InputFormat<RowData, ?> createReader(ReaderContext context) {\n+\t\tDescriptorProperties properties = getValidatedProperties(context.getFormatProperties());\n+\t\tboolean failOnMissingField = properties.getOptionalBoolean(FORMAT_FAIL_ON_MISSING_FIELD).orElse(false);\n+\t\tboolean ignoreParseErrors = properties.getOptionalBoolean(FORMAT_IGNORE_PARSE_ERRORS).orElse(false);\n+\n+\t\tRowType nonPartRowType = context.getNonPartRowType();\n+\t\tJsonRowDataDeserializationSchema  deserializationSchema = new JsonRowDataDeserializationSchema(\n+\t\t\tnonPartRowType,\n+\t\t\tnew GenericTypeInfo(GenericRowData.class),\n+\t\t\tfailOnMissingField,\n+\t\t\tignoreParseErrors);\n+\n+\t\tint[] jsonSelectFieldToProjectFieldMapping = context.getNonPartFieldProjectMapping();\n+\t\tint[] jsonSelectFieldToJsonFieldMapping = context.getSelectFieldToNonPartFieldMapping();\n+\n+\t\treturn new JsonInputFormat(\n+\t\t\tcontext.getPaths(),\n+\t\t\tcontext.getSchema().getFieldDataTypes(),\n+\t\t\tcontext.getSchema().getFieldNames(),\n+\t\t\tcontext.getProjectFields(),\n+\t\t\tcontext.getPartitionKeys(),\n+\t\t\tcontext.getDefaultPartName(),\n+\t\t\tcontext.getPushedDownLimit(),\n+\t\t\tjsonSelectFieldToProjectFieldMapping,\n+\t\t\tjsonSelectFieldToJsonFieldMapping,\n+\t\t\tdeserializationSchema);\n+\t}\n+\n+\t@Override\n+\tpublic Optional<Encoder<RowData>> createEncoder(WriterContext context) {\n+\t\treturn Optional.of(new JsonRowDataEncoder(new JsonRowDataSerializationSchema(context.getNonPartRowType())));\n+\t}\n+\n+\t@Override\n+\tpublic Optional<BulkWriter.Factory<RowData>> createBulkWriterFactory(WriterContext context) {\n+\t\treturn Optional.empty();\n+\t}\n+\n+\t@Override\n+\tpublic boolean supportsSchemaDerivation() {\n+\t\treturn true;\n+\t}\n+\n+\tprivate static DescriptorProperties getValidatedProperties(Map<String, String> propertiesMap) {\n+\t\tfinal DescriptorProperties properties = new DescriptorProperties(true);\n+\t\tproperties.putProperties(propertiesMap);\n+\t\tproperties.validateBoolean(FORMAT_FAIL_ON_MISSING_FIELD, true);\n+\t\tproperties.validateBoolean(FORMAT_IGNORE_PARSE_ERRORS, true);\n+\t\treturn properties;\n+\t}\n+\n+\t/**\n+\t * A {@link JsonInputFormat} is responsible to read {@link RowData} records\n+\t * from json format files.\n+\t */\n+\tpublic static class JsonInputFormat extends DelimitedInputFormat<RowData> {\n+\t\t/**\n+\t\t * Code of \\r, used to remove \\r from a line when the line ends with \\r\\n.\n+\t\t */\n+\t\tprivate static final byte CARRIAGE_RETURN = (byte) '\\r';\n+\n+\t\t/**\n+\t\t * Code of \\n, used to identify if \\n is used as delimiter.\n+\t\t */\n+\t\tprivate static final byte NEW_LINE = (byte) '\\n';\n+\n+\t\tprivate final DataType[] fieldTypes;\n+\t\tprivate final String[] fieldNames;\n+\t\tprivate final int[] selectFields;\n+\t\tprivate final List<String> partitionKeys;\n+\t\tprivate final String defaultPartValue;\n+\t\tprivate final long limit;\n+\t\tprivate final int[] jsonSelectFieldToProjectFieldMapping;\n+\t\tprivate final int[] jsonSelectFieldToJsonFieldMapping;\n+\t\tprivate final JsonRowDataDeserializationSchema deserializationSchema;\n+\n+\t\tprivate transient boolean end;\n+\t\tprivate transient long emitted;\n+\t\t// reuse object for per record\n+\t\tprivate transient GenericRowData rowData;\n+\n+\t\tpublic JsonInputFormat(\n+\t\t\t\tPath[] filePaths,\n+\t\t\t\tDataType[] fieldTypes,\n+\t\t\t\tString[] fieldNames,\n+\t\t\t\tint[] selectFields,\n+\t\t\t\tList<String> partitionKeys,\n+\t\t\t\tString defaultPartValue,\n+\t\t\t\tlong limit,\n+\t\t\t\tint[] jsonSelectFieldToProjectFieldMapping,\n+\t\t\t\tint[] jsonSelectFieldToJsonFieldMapping,\n+\t\t\t\tJsonRowDataDeserializationSchema deserializationSchema) {\n+\t\t\tsuper.setFilePaths(filePaths);\n+\t\t\tthis.fieldTypes = fieldTypes;\n+\t\t\tthis.fieldNames = fieldNames;\n+\t\t\tthis.selectFields = selectFields;\n+\t\t\tthis.partitionKeys = partitionKeys;\n+\t\t\tthis.defaultPartValue = defaultPartValue;\n+\t\t\tthis.limit = limit;\n+\t\t\tthis.jsonSelectFieldToProjectFieldMapping = jsonSelectFieldToProjectFieldMapping;\n+\t\t\tthis.jsonSelectFieldToJsonFieldMapping = jsonSelectFieldToJsonFieldMapping;\n+\t\t\tthis.deserializationSchema = deserializationSchema;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic boolean supportsMultiPaths() {\n+\t\t\treturn true;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void open(FileInputSplit split) throws IOException {\n+\t\t\tsuper.open(split);\n+\t\t\tthis.end = false;\n+\t\t\tthis.emitted = 0L;\n+\t\t\tthis.rowData = PartitionPathUtils.fillPartitionValueForRecord(fieldNames, fieldTypes, selectFields,\n+\t\t\t\tpartitionKeys, currentSplit.getPath(), defaultPartValue);\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic boolean reachedEnd() {\n+\t\t\treturn emitted >= limit || end;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic RowData readRecord(RowData reuse, byte[] bytes, int offset, int numBytes) throws IOException {\n+\t\t\tGenericRowData returnRecord = null;\n+\t\t\tdo {\n+\t\t\t\t// remove \\r from a line when the line ends with \\r\\n\n+\t\t\t\tif (this.getDelimiter() != null && this.getDelimiter().length == 1\n+\t\t\t\t\t&& this.getDelimiter()[0] == NEW_LINE && offset + numBytes >= 1\n+\t\t\t\t\t&& bytes[offset + numBytes - 1] == CARRIAGE_RETURN){\n+\t\t\t\t\tnumBytes -= 1;\n+\t\t\t\t}\n+\t\t\t\tbyte[] trimBytes = Arrays.copyOfRange(bytes, offset, offset + numBytes);\n+\t\t\t\tGenericRowData jsonRow = (GenericRowData) deserializationSchema.deserialize(trimBytes);\n+\n+\t\t\t\tif (jsonRow != null) {\n+\t\t\t\t\t// if the record is null, simply just skip\n+\t\t\t\t\treturnRecord = rowData;\n+\t\t\t\t\tfor (int i = 0; i < jsonSelectFieldToJsonFieldMapping.length; i++) {\n+\t\t\t\t\t\treturnRecord.setField(jsonSelectFieldToProjectFieldMapping[i],\n+\t\t\t\t\t\t\tjsonRow.getField(jsonSelectFieldToJsonFieldMapping[i]));\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t} while (returnRecord == null && !reachedEnd());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ5MDU1OA=="}, "originalCommit": {"oid": "c6f1ab4f02b683d44ac5c987430b9d9eab1e8cd3"}, "originalPosition": 215}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMDQ4MTY0OnYy", "diffSide": "RIGHT", "path": "flink-formats/flink-json/src/test/java/org/apache/flink/formats/json/JsonBatchFileSystemITCase.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQxMzoxMzoyMVrOGS7BAw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQxMzoxMzoyMVrOGS7BAw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ5NDQ2Nw==", "bodyText": "The default value of fail-on-missing-field is false, should set it to true.", "url": "https://github.com/apache/flink/pull/12010#discussion_r422494467", "createdAt": "2020-05-09T13:13:21Z", "author": {"login": "JingsongLi"}, "path": "flink-formats/flink-json/src/test/java/org/apache/flink/formats/json/JsonBatchFileSystemITCase.java", "diffHunk": "@@ -0,0 +1,58 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.json;\n+\n+import org.apache.flink.table.planner.runtime.batch.sql.BatchFileSystemITCaseBase;\n+\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.List;\n+\n+/**\n+ * ITCase to test json format for {@link JsonFileSystemFormatFactory}.\n+ */\n+@RunWith(Parameterized.class)\n+public class JsonBatchFileSystemITCase extends BatchFileSystemITCaseBase {\n+\n+\tprivate final boolean configure;\n+\n+\t@Parameterized.Parameters(name = \"{0}\")\n+\tpublic static Collection<Boolean> parameters() {\n+\t\treturn Arrays.asList(false, true);\n+\t}\n+\n+\tpublic JsonBatchFileSystemITCase(boolean configure) {\n+\t\tthis.configure = configure;\n+\t}\n+\n+\t@Override\n+\tpublic String[] formatProperties() {\n+\t\tList<String> ret = new ArrayList<>();\n+\t\tret.add(\"'format'='json'\");\n+\t\tif (configure) {\n+\t\t\tret.add(\"'format.fail-on-missing-field'='true'\");\n+\t\t\tret.add(\"'format.ignore-parse-errors'='false'\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c6f1ab4f02b683d44ac5c987430b9d9eab1e8cd3"}, "originalPosition": 54}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMjQ3MjUzOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/FileSystemFormatFactory.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMVQwMTo1MTo1OFrOGTKAaQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMVQwMTo1MTo1OFrOGTKAaQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjc0MDA3Mw==", "bodyText": "getFormatFieldNames", "url": "https://github.com/apache/flink/pull/12010#discussion_r422740073", "createdAt": "2020-05-11T01:51:58Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/FileSystemFormatFactory.java", "diffHunk": "@@ -127,7 +172,7 @@\n \t\t/**\n \t\t * Get field names without partition keys.\n \t\t */\n-\t\tdefault String[] getFieldNamesWithoutPartKeys() {\n+\t\tdefault String[] getFormatFieldTypes() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "410e972bd87875f3ebc7eb8c3b1f39279397473a"}, "originalPosition": 69}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMjQ3MjYyOnYy", "diffSide": "RIGHT", "path": "flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/FileSystemFormatFactory.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMVQwMTo1MjowNFrOGTKAeQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMVQwMTo1MjowNFrOGTKAeQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjc0MDA4OQ==", "bodyText": "getFormatFieldTypes", "url": "https://github.com/apache/flink/pull/12010#discussion_r422740089", "createdAt": "2020-05-11T01:52:04Z", "author": {"login": "JingsongLi"}, "path": "flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/FileSystemFormatFactory.java", "diffHunk": "@@ -136,11 +181,23 @@\n \t\t/**\n \t\t * Get field types without partition keys.\n \t\t */\n-\t\tdefault DataType[] getFieldTypesWithoutPartKeys() {\n+\t\tdefault DataType[] getFormatFieldNames() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "410e972bd87875f3ebc7eb8c3b1f39279397473a"}, "originalPosition": 78}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMjQ3NDg1OnYy", "diffSide": "RIGHT", "path": "flink-formats/flink-json/src/test/java/org/apache/flink/formats/json/JsonBatchFileSystemITCase.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMVQwMTo1MzoyNlrOGTKBsA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMVQwMTo1MzoyNlrOGTKBsA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjc0MDQwMA==", "bodyText": "You could add a check(String sqlQuery, java.util.List<Row> expectedResult) to FileSystemITCaseBase.", "url": "https://github.com/apache/flink/pull/12010#discussion_r422740400", "createdAt": "2020-05-11T01:53:26Z", "author": {"login": "JingsongLi"}, "path": "flink-formats/flink-json/src/test/java/org/apache/flink/formats/json/JsonBatchFileSystemITCase.java", "diffHunk": "@@ -0,0 +1,64 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.json;\n+\n+import org.apache.flink.table.planner.runtime.batch.sql.BatchFileSystemITCaseBase;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.util.FileUtils;\n+\n+import org.junit.Test;\n+\n+import java.io.File;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * ITCase to test json format for {@link JsonFileSystemFormatFactory}.\n+ */\n+public class JsonBatchFileSystemITCase extends BatchFileSystemITCaseBase {\n+\n+\t@Override\n+\tpublic String[] formatProperties() {\n+\t\tList<String> ret = new ArrayList<>();\n+\t\tret.add(\"'format'='json'\");\n+\t\tret.add(\"'format.ignore-parse-errors'='true'\");\n+\t\treturn ret.toArray(new String[0]);\n+\t}\n+\n+\t@Test\n+\tpublic void testParseError() throws Exception {\n+\t\tString path = new URI(resultPath()).getPath();\n+\t\tnew File(path).mkdirs();\n+\t\tFile file = new File(path, \"my_file\");\n+\t\tfile.createNewFile();\n+\t\tFileUtils.writeFileUtf8(file,\n+\t\t\t\"{\\\"x\\\":\\\"x5\\\",\\\"y\\\":5,\\\"a\\\":1,\\\"b\\\":1}\\n\" +\n+\t\t\t\t\"{I am a wrong json.}\\n\" +\n+\t\t\t\t\"{\\\"x\\\":\\\"x5\\\",\\\"y\\\":5,\\\"a\\\":1,\\\"b\\\":1}\");\n+\n+\t\tcheck(\"select * from nonPartitionedTable\",\n+\t\t\tJavaConverters.asScalaIteratorConverter(Arrays.asList(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "410e972bd87875f3ebc7eb8c3b1f39279397473a"}, "originalPosition": 60}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1518, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}