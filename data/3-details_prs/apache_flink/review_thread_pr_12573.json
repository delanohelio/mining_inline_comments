{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDMyMjYwNTE1", "number": 12573, "reviewThreads": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQwNzo1NDo1M1rOEELslg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxMjo1NzozNVrOEESEiw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcyODIxMzk4OnYy", "diffSide": "RIGHT", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveTableFileInputFormat.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQwNzo1NDo1M1rOGhpLng==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQwODozMjoyN1rOGhqlOA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzkzMDkxMA==", "bodyText": "Can we update the doc of FileInputSplit::getLength() to indicate length == -1 means to read all data from the file? I'll feel more comfortable about this change if it's guaranteed by the API contract.", "url": "https://github.com/apache/flink/pull/12573#discussion_r437930910", "createdAt": "2020-06-10T07:54:53Z", "author": {"login": "lirui-apache"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveTableFileInputFormat.java", "diffHunk": "@@ -52,16 +53,26 @@ public HiveTableFileInputFormat(\n \n \t@Override\n \tpublic void open(FileInputSplit fileSplit) throws IOException {\n-\t\tURI uri = fileSplit.getPath().toUri();\n \t\tHiveTableInputSplit split = new HiveTableInputSplit(\n \t\t\t\tfileSplit.getSplitNumber(),\n-\t\t\t\tnew FileSplit(new Path(uri), fileSplit.getStart(), fileSplit.getLength(), (String[]) null),\n+\t\t\t\ttoHadoopFileSplit(fileSplit),\n \t\t\t\tinputFormat.getJobConf(),\n-\t\t\t\thiveTablePartition\n-\t\t);\n+\t\t\t\thiveTablePartition);\n \t\tinputFormat.open(split);\n \t}\n \n+\t@VisibleForTesting\n+\tstatic FileSplit toHadoopFileSplit(FileInputSplit fileSplit) throws IOException {\n+\t\tURI uri = fileSplit.getPath().toUri();\n+\t\tlong length = fileSplit.getLength();\n+\t\t// Hadoop FileSplit should not have -1 length.\n+\t\tif (length == -1) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "afc18c9ef5b1c7a762ba55b020412578adac402f"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzk1Mzg0OA==", "bodyText": "Actually, there are comments in FileInputSplit: the number of bytes in the file to process (-1 is flag for \"read whole file\")", "url": "https://github.com/apache/flink/pull/12573#discussion_r437953848", "createdAt": "2020-06-10T08:32:27Z", "author": {"login": "JingsongLi"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveTableFileInputFormat.java", "diffHunk": "@@ -52,16 +53,26 @@ public HiveTableFileInputFormat(\n \n \t@Override\n \tpublic void open(FileInputSplit fileSplit) throws IOException {\n-\t\tURI uri = fileSplit.getPath().toUri();\n \t\tHiveTableInputSplit split = new HiveTableInputSplit(\n \t\t\t\tfileSplit.getSplitNumber(),\n-\t\t\t\tnew FileSplit(new Path(uri), fileSplit.getStart(), fileSplit.getLength(), (String[]) null),\n+\t\t\t\ttoHadoopFileSplit(fileSplit),\n \t\t\t\tinputFormat.getJobConf(),\n-\t\t\t\thiveTablePartition\n-\t\t);\n+\t\t\t\thiveTablePartition);\n \t\tinputFormat.open(split);\n \t}\n \n+\t@VisibleForTesting\n+\tstatic FileSplit toHadoopFileSplit(FileInputSplit fileSplit) throws IOException {\n+\t\tURI uri = fileSplit.getPath().toUri();\n+\t\tlong length = fileSplit.getLength();\n+\t\t// Hadoop FileSplit should not have -1 length.\n+\t\tif (length == -1) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzkzMDkxMA=="}, "originalCommit": {"oid": "afc18c9ef5b1c7a762ba55b020412578adac402f"}, "originalPosition": 29}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcyOTI1ODM1OnYy", "diffSide": "RIGHT", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/DirectoryMonitorDiscovery.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxMjo1NzozNVrOGhzhHw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxMjo1NzozNVrOGhzhHw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODEwMDI1NQ==", "bodyText": "Add some comments for this method? What is a suitable partition?", "url": "https://github.com/apache/flink/pull/12573#discussion_r438100255", "createdAt": "2020-06-10T12:57:35Z", "author": {"login": "lirui-apache"}, "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/DirectoryMonitorDiscovery.java", "diffHunk": "@@ -41,18 +44,36 @@\n \t\t\tContext context, long previousTimestamp) throws Exception {\n \t\tFileStatus[] statuses = getFileStatusRecurse(\n \t\t\t\tcontext.tableLocation(), context.partitionKeys().size(), context.fileSystem());\n+\t\tList<Tuple2<List<String>, Long>> partValueList = suitablePartitions(context, previousTimestamp, statuses);\n+\n \t\tList<Tuple2<Partition, Long>> partitions = new ArrayList<>();\n+\t\tfor (Tuple2<List<String>, Long> tuple2 : partValueList) {\n+\t\t\tcontext.getPartition(tuple2.f0).ifPresent(\n+\t\t\t\t\tpartition -> partitions.add(new Tuple2<>(partition, tuple2.f1)));\n+\t\t}\n+\t\treturn partitions;\n+\t}\n+\n+\t@VisibleForTesting\n+\tstatic List<Tuple2<List<String>, Long>> suitablePartitions(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a6ef0628e2b21e484bfefc7fb31bf7c1649ffe34"}, "originalPosition": 33}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4368, "cost": 1, "resetAt": "2021-11-12T11:57:46Z"}}}