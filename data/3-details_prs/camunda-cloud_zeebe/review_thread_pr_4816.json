{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDQwMDI1MDYy", "number": 4816, "reviewThreads": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQxNDowMzo0OFrOEIxnRg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQxNDowNzo0N1rOEIxtYQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3NjM2OTM0OnYy", "diffSide": "RIGHT", "path": "docs/src/operations/backpressure.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQxNDowMzo0OFrOGo8PqQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQxNDowMzo0OFrOGo8PqQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTU4MzI3Mw==", "bodyText": "Maybe better \"client requests\" instead of \"user requests\"", "url": "https://github.com/camunda-cloud/zeebe/pull/4816#discussion_r445583273", "createdAt": "2020-06-25T14:03:48Z", "author": {"login": "pihme"}, "path": "docs/src/operations/backpressure.md", "diffHunk": "@@ -0,0 +1,93 @@\n+# Backpressure\n+\n+When a broker receives a user request, it is written to the *event stream* first (see section [Internal Processing](/basics/internal-processing.html) for details), and processed later by the stream processor.\n+If the processing is slow or if there are many user requests in the stream, it might take too long for the processor to start processing the command.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e4c59384de1db30f3042c39fe786e5d509bdd812"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3NjM3MzkyOnYy", "diffSide": "RIGHT", "path": "docs/src/operations/backpressure.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQxNDowNDo1NFrOGo8Ssg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQxNDowNDo1NFrOGo8Ssg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTU4NDA1MA==", "bodyText": "Maybe better \"with a defined latency\" (acceptable implies a value judgement)", "url": "https://github.com/camunda-cloud/zeebe/pull/4816#discussion_r445584050", "createdAt": "2020-06-25T14:04:54Z", "author": {"login": "pihme"}, "path": "docs/src/operations/backpressure.md", "diffHunk": "@@ -0,0 +1,93 @@\n+# Backpressure\n+\n+When a broker receives a user request, it is written to the *event stream* first (see section [Internal Processing](/basics/internal-processing.html) for details), and processed later by the stream processor.\n+If the processing is slow or if there are many user requests in the stream, it might take too long for the processor to start processing the command.\n+If the broker keeps accepting new requests from the user, the back log increases and the processing latency can grow beyond an acceptable time.\n+To avoid such problems, Zeebe employs a backpressure mechanism.\n+When the broker receives more requests than it can process with an acceptable latency, it rejects some requests (see section [Error handling](/reference/grpc.html)).", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e4c59384de1db30f3042c39fe786e5d509bdd812"}, "originalPosition": 7}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3NjM4NDk3OnYy", "diffSide": "RIGHT", "path": "docs/src/operations/backpressure.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQxNDowNzo0N1rOGo8aEw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQxNDowNzo0N1rOGo8aEw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTU4NTkzOQ==", "bodyText": "\"you might consider\"", "url": "https://github.com/camunda-cloud/zeebe/pull/4816#discussion_r445585939", "createdAt": "2020-06-25T14:07:47Z", "author": {"login": "pihme"}, "path": "docs/src/operations/backpressure.md", "diffHunk": "@@ -0,0 +1,93 @@\n+# Backpressure\n+\n+When a broker receives a user request, it is written to the *event stream* first (see section [Internal Processing](/basics/internal-processing.html) for details), and processed later by the stream processor.\n+If the processing is slow or if there are many user requests in the stream, it might take too long for the processor to start processing the command.\n+If the broker keeps accepting new requests from the user, the back log increases and the processing latency can grow beyond an acceptable time.\n+To avoid such problems, Zeebe employs a backpressure mechanism.\n+When the broker receives more requests than it can process with an acceptable latency, it rejects some requests (see section [Error handling](/reference/grpc.html)).\n+\n+### Terminologies\n+* *RTT* - The time between the request is accepted by the broker and the response to the request is sent back to the gateway.\n+* *inflight count* - The number of requests accepted by the broker but the response is not yet sent.\n+* *limit* - maximum number of flight requests. When the inflight count is above the limit, any new incoming request will be rejected.\n+\n+Note that the limit and inflight count are calculated per partition.\n+\n+### Backpressure algorithms\n+\n+Zeebe uses adaptive algorithms from [concurrency-limits](https://github.com/Netflix/concurrency-limits) to dynamically calculate the limit.\n+Zeebe can be configured with one of the following backpressure algorithms.\n+\n+#### Fixed Limit\n+With \u201cfixed limit\u201d one can configure a fixed value of the limit.\n+Zeebe operators are recommended to evaluate the latencies observed with different values for limit.\n+Note that with different cluster configurations, you may have to choose different limit values.\n+\n+#### AIMD\n+AIMD calculates the limit based on the configured *requestTimeout*.\n+When the RTT for a request *requestTimeout*, the limit is increased by 1.\n+When the RTT is longer than *requestTimeout*,\n+the limit will be reduced according to the configured *backoffRatio*.\n+\n+#### Vegas\n+Vegas is an adaptive limit algorithm based on TCP Vegas congestion control algorithm.\n+Vegas estimates a base latency as the minimum observed latency.\n+This base RTT is the expected latency when there is no load.\n+Whenever the RTT deviates from the base RTT, a new limit is calculated based on the vegas algorithm.\n+Vegas allows to configure two parameters - *alpha* and *beta*.\n+The values correspond to a queue size that is estimated by the Vegas algorithm based on the observed RTT, base RTT, and current limit.\n+When the queue size is below *alpha*, the limit is increased.\n+When the queue size is above *beta*, the limit is decreased.\n+\n+### Gradient\n+Gradient is an adaptive limit algorithm that dynamically calculates the limit based on observed RTT.\n+In the gradient algorithm, the limit is adjusted based on the gradient of observed RTT and an observed minimum RTT.\n+If gradient is less than 1, the limit is decreased otherwise the limit is increased.\n+\n+### Gradient2\n+Gradient2 is similar to Gradient, but instead of using observed minimum RTT as the base, it uses and exponentially smoothed average RTT.\n+\n+## Backpressure Tuning\n+\n+The goal of backpressure is to keep the processing latency low.\n+The processing latency is calculated as the time between the command is written to the event stream until it is processed.\n+Hence to see how backpressure behaves you can run a benchmark on your cluster and observe\n+the following metrics.\n+\n+* `zeebe_stream_processor_latency_bucket`\n+* `zeebe_dropped_request_count_total`\n+* `zeebe_received_request_count_total`\n+* `zeebe_backpressure_requests_limit`\n+\n+You may want to run the benchmark with different load\n+1. With low load - where the number of user requests send per second is low.\n+2. With high load - where the number of user requests sent per second is above what zeebe can process within a reasonable latency.\n+\n+If the value of the limit is small, the processing latency will be small but the number of rejected requests may be high.\n+If the value of the limit is large, less requests may be rejected (depending on the request rate),\n+but the processing latency may increase.\n+\n+When using \"fixed limit\", you can run the benchmark with different values for the limit.\n+You can then determine a suitable value for a limit for which the processing latency (`zeebe_stream_processor_latency_bucket`) is within the desired latency.\n+\n+When using \"AIMD\", you can configure a *requestTimeout* which corresponds to a desired latency.\n+Note that during high load \"AIMD\" can lead to a processing latency two times more than the configured *requestTimeout*.\n+It is also recommended to configure a *minLimit* to prevent the limit from aggressively dropping during constant high load.\n+\n+When using \"Vegas\", you cannot configure the backpressure to a desired latency.\n+Instead Vegas tries to keep the RTT as low as possible based on the observed minimum RTT.\n+\n+Similar to \"Vegas\", you cannot configure the desired latency in \"Gradient\" and \"Gradient2\".\n+They calculated the limit based on the gradient of observed RTT from the expected RTT.\n+Higher the value of *rttTolerance*, higher deviations are tolerated that results in higher values for limit.\n+\n+If a lot of requests are rejected due to backpressure, it might indicate that the processing capacity of the cluster is not enough to handle the expected throughput.\n+If this is the expected workload, then you should consider a different configuration for the cluster such as provisioning more resources and, increasing the number of nodes and partitions.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e4c59384de1db30f3042c39fe786e5d509bdd812"}, "originalPosition": 85}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 400, "cost": 1, "resetAt": "2021-11-12T13:16:51Z"}}}