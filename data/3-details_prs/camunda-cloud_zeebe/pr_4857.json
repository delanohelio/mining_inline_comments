{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDQyNDg3MTA2", "number": 4857, "title": "Fix resource managment in ZeebePartition", "bodyText": "@npepinpe @deepthidevaki who ever has time to review this. We can also talk shortly before you're starting with the review.\nDescription\nSome resources were not correctly closed or not removed as listener and some resources have been closed, where the responsibility was not existing. In general made some minor improvements on logging and closing resources in the ZeebePartition.\nQA Tests:\n\nuse all nodes as initial contact pointes -> makes tests more stable -> less flaky\nAdd functionallity to the clustering rule to cause a network partition and to step down from the leadership\nAdded new tests which test whether leader steps down on network partition and new leader is chosen and that replication still works on reconnect.\nAdded regression test for #4810\n\nFixes:\nAdded new interface to close asynchronously a resource. Remove in the ZeebePartition the hybrid handling of resources.\nNow we have just a closable list which is iterated on closing the partition.\n\nSnapshotStore is not closed in ZeebePartition, since the ZeebePartition is not responsible for that and causes an bug #4810.\nStateController is only registered for replication on LeaderPartition, fixes #4844\nMetricsTimer is closed correctly, fixes #4847\n\nThe state controller was previous also registered on follower side, which caused replication on follower side when new snapshot was received this caused #4686 this is fixed now.\nRelated issues\n\ncloses #4810\ncloses #4844\ncloses #4847\ncloses #4686\nBenchmark\nLooks good.\n\n\nPull Request Checklist\n\n All commit messages match our commit message guidelines\n The submitting code follows our code style\n If submitting code, please run mvn clean install -DskipTests locally before committing", "createdAt": "2020-07-01T06:59:55Z", "url": "https://github.com/camunda-cloud/zeebe/pull/4857", "merged": true, "mergeCommit": {"oid": "56dff5a220f891cc44e4b546e3f3d77bec2223cf"}, "closed": true, "closedAt": "2020-07-07T07:12:42Z", "author": {"login": "Zelldon"}, "timelineItems": {"totalCount": 13, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcwlFIEABqjM1MDEwNjIwNDA=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcyeUpcABqjM1MTg1Nzg4ODE=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "f45ae14ca9d1ab41100e359092e26490ed500ff7", "author": {"user": {"login": "Zelldon", "name": "Christopher Zell"}}, "url": "https://github.com/camunda-cloud/zeebe/commit/f45ae14ca9d1ab41100e359092e26490ed500ff7", "committedDate": "2020-07-01T06:06:29Z", "message": "fix(broker): fix closing of resources in ZeebePartition\n\n Some resources were not correctly closed or not removed as listener.\n Added new interface to close asynchronously a resource. Remove in the ZeebePartition the hybrid handling of resources.\n Now we have just a closable list which is iterated on closing the partition.\n\n SnapshotStore is not closed in ZeebePartition, since the ZeebePartition is not responsible for that and causes an bug #4810.\n\n StateController is only registered for replication on LeaderPartition, fixes #4844\n MetricsTimer is closed correctly, fixes #4847\n\n The state controller was previous also registered on follower side, which caused replication on follower side when new snapshot was received this caused #4686 this is fixed now."}, "afterCommit": {"oid": "24d1fee58efbcee01c5c5a866bb95ced579fe581", "author": {"user": {"login": "Zelldon", "name": "Christopher Zell"}}, "url": "https://github.com/camunda-cloud/zeebe/commit/24d1fee58efbcee01c5c5a866bb95ced579fe581", "committedDate": "2020-07-01T07:26:20Z", "message": "fix(broker): fix closing of resources in ZeebePartition\n\n Some resources were not correctly closed or not removed as listener.\n Added new interface to close asynchronously a resource. Remove in the ZeebePartition the hybrid handling of resources.\n Now we have just a closable list which is iterated on closing the partition.\n\n SnapshotStore is not closed in ZeebePartition, since the ZeebePartition is not responsible for that and causes an bug #4810.\n\n StateController is only registered for replication on LeaderPartition, fixes #4844\n MetricsTimer is closed correctly, fixes #4847\n\n The state controller was previous also registered on follower side, which caused replication on follower side when new snapshot was received this caused #4686 this is fixed now."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQwNjUwNDA1", "url": "https://github.com/camunda-cloud/zeebe/pull/4857#pullrequestreview-440650405", "createdAt": "2020-07-01T08:21:48Z", "commit": {"oid": "24d1fee58efbcee01c5c5a866bb95ced579fe581"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 16, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQwODoyMTo0OVrOGrb36w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQwOTowOTozOVrOGrdkrw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE5ODYzNQ==", "bodyText": "Just to understand, what was previously happening here? Were the connections eventually closed (e.g. when they were garbage collected), or...?", "url": "https://github.com/camunda-cloud/zeebe/pull/4857#discussion_r448198635", "createdAt": "2020-07-01T08:21:49Z", "author": {"login": "npepinpe"}, "path": "atomix/cluster/src/main/java/io/atomix/cluster/messaging/impl/NettyMessagingService.java", "diffHunk": "@@ -296,6 +297,13 @@ public boolean isRunning() {\n                 interrupted = true;\n               }\n               timeoutExecutor.shutdown();\n+\n+              for (final var entry : connections.entrySet()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "24d1fee58efbcee01c5c5a866bb95ced579fe581"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE5OTAzNg==", "bodyText": "If we want to enable debug logging for Atomix, wouldn't this create tons of noise?", "url": "https://github.com/camunda-cloud/zeebe/pull/4857#discussion_r448199036", "createdAt": "2020-07-01T08:22:29Z", "author": {"login": "npepinpe"}, "path": "atomix/cluster/src/main/java/io/atomix/raft/roles/AbstractRole.java", "diffHunk": "@@ -59,13 +59,13 @@ protected AbstractRole(final RaftContext raft) {\n \n   /** Logs a request. */\n   protected final <R extends RaftRequest> R logRequest(final R request) {\n-    log.trace(\"Received {}\", request);\n+    log.debug(\"Received {}\", request);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "24d1fee58efbcee01c5c5a866bb95ced579fe581"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODIwMzE3OA==", "bodyText": "Nit: no need to compute it on every loop, and probably more readable if assigned to a variable \ud83e\udd37\u200d\u2642\ufe0f", "url": "https://github.com/camunda-cloud/zeebe/pull/4857#discussion_r448203178", "createdAt": "2020-07-01T08:29:10Z", "author": {"login": "npepinpe"}, "path": "qa/integration-tests/src/test/java/io/zeebe/broker/it/clustering/ClusteringRule.java", "diffHunk": "@@ -189,6 +194,19 @@ protected void before() throws IOException {\n       getBroker(nodeId);\n     }\n \n+    for (int nodeId = 0; nodeId < clusterSize; nodeId++) {\n+      final var brokerCfg = getBrokerCfg(nodeId);\n+\n+      setInitialContactPoints(\n+              brokerCfgs.values().stream()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "24d1fee58efbcee01c5c5a866bb95ced579fe581"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODIwMzc0OA==", "bodyText": "Any reason not to use Awaitility here?", "url": "https://github.com/camunda-cloud/zeebe/pull/4857#discussion_r448203748", "createdAt": "2020-07-01T08:29:59Z", "author": {"login": "npepinpe"}, "path": "qa/integration-tests/src/test/java/io/zeebe/broker/it/clustering/ClusteringRule.java", "diffHunk": "@@ -512,6 +530,52 @@ public long getPartitionLeaderCount() {\n         .count();\n   }\n \n+  public Broker stepDownFromPartition(final int partitionId) {\n+    final int leaderNodeId = getLeaderForPartition(partitionId).getNodeId();\n+    final Broker leader = getBroker(leaderNodeId);\n+    stepDown(leader, partitionId);\n+\n+    final BrokerInfo newLeaderInfo = awaitOtherLeader(partitionId, leaderNodeId);\n+\n+    return getBroker(newLeaderInfo.getNodeId());\n+  }\n+\n+  public BrokerInfo awaitOtherLeader(final int partitionId, final int previousLeader) {\n+    return doRepeatedly(() -> getLeaderForPartition(partitionId))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "24d1fee58efbcee01c5c5a866bb95ced579fe581"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODIwODA2MQ==", "bodyText": "Nit: I find it more readable splitting the conditions in 2 filter calls \ud83e\udd37\u200d\u2642\ufe0f\n.stream()\n.filter(partition -> partition.id().id() == partitionId)\n.filter(partition -> partition.members().contains(nodeId))\n...\nafaik it's equivalent, though maybe less efficient (but here I don't think it matters)", "url": "https://github.com/camunda-cloud/zeebe/pull/4857#discussion_r448208061", "createdAt": "2020-07-01T08:37:22Z", "author": {"login": "npepinpe"}, "path": "qa/integration-tests/src/test/java/io/zeebe/broker/it/clustering/ClusteringRule.java", "diffHunk": "@@ -512,6 +530,52 @@ public long getPartitionLeaderCount() {\n         .count();\n   }\n \n+  public Broker stepDownFromPartition(final int partitionId) {\n+    final int leaderNodeId = getLeaderForPartition(partitionId).getNodeId();\n+    final Broker leader = getBroker(leaderNodeId);\n+    stepDown(leader, partitionId);\n+\n+    final BrokerInfo newLeaderInfo = awaitOtherLeader(partitionId, leaderNodeId);\n+\n+    return getBroker(newLeaderInfo.getNodeId());\n+  }\n+\n+  public BrokerInfo awaitOtherLeader(final int partitionId, final int previousLeader) {\n+    return doRepeatedly(() -> getLeaderForPartition(partitionId))\n+        .until(curLeader -> curLeader.getNodeId() != previousLeader, 1000);\n+  }\n+\n+  public void stepDown(final Broker broker, final int partitionId) {\n+    final var atomix = broker.getAtomix();\n+    final MemberId nodeId = atomix.getMembershipService().getLocalMember().id();\n+\n+    final var raftPartition =\n+        atomix.getPartitionService().getPartitionGroup(AtomixFactory.GROUP_NAME).getPartitions()\n+            .stream()\n+            .filter(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "24d1fee58efbcee01c5c5a866bb95ced579fe581"}, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODIxMTE0MQ==", "bodyText": "Side note, I'd love to test these more with TestContainers actually, since we can simulate a real network partition and disconnect instead of closing the messaging service. That said I'm happy we have these until then \ud83d\ude42", "url": "https://github.com/camunda-cloud/zeebe/pull/4857#discussion_r448211141", "createdAt": "2020-07-01T08:42:54Z", "author": {"login": "npepinpe"}, "path": "qa/integration-tests/src/test/java/io/zeebe/broker/it/clustering/FailOverReplicationTest.java", "diffHunk": "@@ -0,0 +1,247 @@\n+/*\n+ * Copyright Camunda Services GmbH and/or licensed to Camunda Services GmbH under\n+ * one or more contributor license agreements. See the NOTICE file distributed\n+ * with this work for additional information regarding copyright ownership.\n+ * Licensed under the Zeebe Community License 1.0. You may not use this file\n+ * except in compliance with the Zeebe Community License 1.0.\n+ */\n+package io.zeebe.broker.it.clustering;\n+\n+import static org.assertj.core.api.Assertions.assertThat;\n+\n+import io.atomix.raft.snapshot.impl.FileBasedSnapshotMetadata;\n+import io.zeebe.broker.Broker;\n+import io.zeebe.broker.it.util.GrpcClientRule;\n+import io.zeebe.broker.system.configuration.BrokerCfg;\n+import io.zeebe.client.ZeebeClient;\n+import io.zeebe.model.bpmn.Bpmn;\n+import io.zeebe.model.bpmn.BpmnModelInstance;\n+import io.zeebe.test.util.TestUtil;\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.time.Duration;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.RuleChain;\n+import org.springframework.util.unit.DataSize;\n+\n+public class FailOverReplicationTest {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "24d1fee58efbcee01c5c5a866bb95ced579fe581"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODIxMzQxNg==", "bodyText": "Isn't this testing mostly that disconnecting works as expected? Or what are we testing here?", "url": "https://github.com/camunda-cloud/zeebe/pull/4857#discussion_r448213416", "createdAt": "2020-07-01T08:46:57Z", "author": {"login": "npepinpe"}, "path": "qa/integration-tests/src/test/java/io/zeebe/broker/it/clustering/FailOverReplicationTest.java", "diffHunk": "@@ -0,0 +1,247 @@\n+/*\n+ * Copyright Camunda Services GmbH and/or licensed to Camunda Services GmbH under\n+ * one or more contributor license agreements. See the NOTICE file distributed\n+ * with this work for additional information regarding copyright ownership.\n+ * Licensed under the Zeebe Community License 1.0. You may not use this file\n+ * except in compliance with the Zeebe Community License 1.0.\n+ */\n+package io.zeebe.broker.it.clustering;\n+\n+import static org.assertj.core.api.Assertions.assertThat;\n+\n+import io.atomix.raft.snapshot.impl.FileBasedSnapshotMetadata;\n+import io.zeebe.broker.Broker;\n+import io.zeebe.broker.it.util.GrpcClientRule;\n+import io.zeebe.broker.system.configuration.BrokerCfg;\n+import io.zeebe.client.ZeebeClient;\n+import io.zeebe.model.bpmn.Bpmn;\n+import io.zeebe.model.bpmn.BpmnModelInstance;\n+import io.zeebe.test.util.TestUtil;\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.time.Duration;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.RuleChain;\n+import org.springframework.util.unit.DataSize;\n+\n+public class FailOverReplicationTest {\n+\n+  private static final int PARTITION_COUNT = 1;\n+  private static final Duration SNAPSHOT_PERIOD = Duration.ofMinutes(5);\n+  private static final BpmnModelInstance WORKFLOW =\n+      Bpmn.createExecutableProcess(\"process\").startEvent().endEvent().done();\n+  private static final String WORKFLOW_RESOURCE_NAME = \"workflow.bpmn\";\n+\n+  private final ClusteringRule clusteringRule =\n+      new ClusteringRule(PARTITION_COUNT, 3, 3, FailOverReplicationTest::configureBroker);\n+  public final GrpcClientRule clientRule = new GrpcClientRule(clusteringRule);\n+  @Rule public RuleChain ruleChain = RuleChain.outerRule(clusteringRule).around(clientRule);\n+  private ZeebeClient client;\n+\n+  @Before\n+  public void init() {\n+    client = clientRule.getClient();\n+  }\n+\n+  @Test\n+  public void shouldFailOverOnNetworkPartition() {\n+    // given\n+    final var leaderNodeId = clusteringRule.getLeaderForPartition(1).getNodeId();\n+    final var leader = clusteringRule.getBroker(leaderNodeId);\n+\n+    // when\n+    clusteringRule.disconnect(leader);\n+\n+    // then\n+    final var newLeader = clusteringRule.awaitOtherLeader(1, leaderNodeId);\n+    assertThat(newLeader.getNodeId()).isNotEqualTo(leaderNodeId);\n+  }\n+\n+  @Test\n+  public void shouldNotReceiveEntriesOnDisconnect() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "24d1fee58efbcee01c5c5a866bb95ced579fe581"}, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODIxNzAyOA==", "bodyText": "I'm not sure about this test, specially the verification...I get that we want to test that replication occurs on fail over, but not sure if it makes sense to test this as an integration test. From the system pov, what do we want to occur on fail over? That we replicate events, or that if the node were to become leader again, it could rebuild the state properly and execute commands properly (e.g. deployments are present, we can continue workflow instances, etc.)? I feel like just checking if events are replicated belongs more to the Raft tests than Zeebe's integration tests \ud83e\udd14\nNot sure if I'm clear, we can also zoom quickly, might be faster \ud83d\ude05", "url": "https://github.com/camunda-cloud/zeebe/pull/4857#discussion_r448217028", "createdAt": "2020-07-01T08:53:08Z", "author": {"login": "npepinpe"}, "path": "qa/integration-tests/src/test/java/io/zeebe/broker/it/clustering/FailOverReplicationTest.java", "diffHunk": "@@ -0,0 +1,247 @@\n+/*\n+ * Copyright Camunda Services GmbH and/or licensed to Camunda Services GmbH under\n+ * one or more contributor license agreements. See the NOTICE file distributed\n+ * with this work for additional information regarding copyright ownership.\n+ * Licensed under the Zeebe Community License 1.0. You may not use this file\n+ * except in compliance with the Zeebe Community License 1.0.\n+ */\n+package io.zeebe.broker.it.clustering;\n+\n+import static org.assertj.core.api.Assertions.assertThat;\n+\n+import io.atomix.raft.snapshot.impl.FileBasedSnapshotMetadata;\n+import io.zeebe.broker.Broker;\n+import io.zeebe.broker.it.util.GrpcClientRule;\n+import io.zeebe.broker.system.configuration.BrokerCfg;\n+import io.zeebe.client.ZeebeClient;\n+import io.zeebe.model.bpmn.Bpmn;\n+import io.zeebe.model.bpmn.BpmnModelInstance;\n+import io.zeebe.test.util.TestUtil;\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.time.Duration;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.RuleChain;\n+import org.springframework.util.unit.DataSize;\n+\n+public class FailOverReplicationTest {\n+\n+  private static final int PARTITION_COUNT = 1;\n+  private static final Duration SNAPSHOT_PERIOD = Duration.ofMinutes(5);\n+  private static final BpmnModelInstance WORKFLOW =\n+      Bpmn.createExecutableProcess(\"process\").startEvent().endEvent().done();\n+  private static final String WORKFLOW_RESOURCE_NAME = \"workflow.bpmn\";\n+\n+  private final ClusteringRule clusteringRule =\n+      new ClusteringRule(PARTITION_COUNT, 3, 3, FailOverReplicationTest::configureBroker);\n+  public final GrpcClientRule clientRule = new GrpcClientRule(clusteringRule);\n+  @Rule public RuleChain ruleChain = RuleChain.outerRule(clusteringRule).around(clientRule);\n+  private ZeebeClient client;\n+\n+  @Before\n+  public void init() {\n+    client = clientRule.getClient();\n+  }\n+\n+  @Test\n+  public void shouldFailOverOnNetworkPartition() {\n+    // given\n+    final var leaderNodeId = clusteringRule.getLeaderForPartition(1).getNodeId();\n+    final var leader = clusteringRule.getBroker(leaderNodeId);\n+\n+    // when\n+    clusteringRule.disconnect(leader);\n+\n+    // then\n+    final var newLeader = clusteringRule.awaitOtherLeader(1, leaderNodeId);\n+    assertThat(newLeader.getNodeId()).isNotEqualTo(leaderNodeId);\n+  }\n+\n+  @Test\n+  public void shouldNotReceiveEntriesOnDisconnect() {\n+    // given\n+    final var segmentCount = 2;\n+    final var oldLeaderId = clusteringRule.getLeaderForPartition(1).getNodeId();\n+    final var oldLeader = clusteringRule.getBroker(oldLeaderId);\n+    clusteringRule.disconnect(oldLeader);\n+\n+    // when\n+    final List<Broker> others = clusteringRule.getOtherBrokerObjects(oldLeaderId);\n+    fillSegments(others, segmentCount);\n+\n+    // then\n+    assertThat(getSegmentsCount(oldLeader)).isLessThan(segmentCount);\n+  }\n+\n+  @Test\n+  public void shouldReceiveEntriesAfterNetworkPartition() {\n+    // given\n+    final var segmentCount = 2;\n+    final var leaderNodeId = clusteringRule.getLeaderForPartition(1).getNodeId();\n+    final var oldLeader = clusteringRule.getBroker(leaderNodeId);\n+    clusteringRule.disconnect(oldLeader);\n+    final List<Broker> followers = clusteringRule.getOtherBrokerObjects(leaderNodeId);\n+    fillSegments(followers, segmentCount);\n+\n+    // when\n+    clusteringRule.connect(oldLeader);\n+\n+    // then\n+    TestUtil.doRepeatedly(() -> getSegmentsCount(oldLeader)).until(count -> count >= segmentCount);\n+    assertThat(getSegmentsCount(oldLeader)).isGreaterThanOrEqualTo(segmentCount);\n+  }\n+\n+  @Test\n+  public void shouldStillReceiveEntriesAfterStepDown() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "24d1fee58efbcee01c5c5a866bb95ced579fe581"}, "originalPosition": 102}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODIxNzg5Nw==", "bodyText": "Could this test be flaky? Just wondering, because iirc we don't retry snapshot replication to a follower that was partitioned off, so you'd have to wait for the next snapshot, right?", "url": "https://github.com/camunda-cloud/zeebe/pull/4857#discussion_r448217897", "createdAt": "2020-07-01T08:54:41Z", "author": {"login": "npepinpe"}, "path": "qa/integration-tests/src/test/java/io/zeebe/broker/it/clustering/FailOverReplicationTest.java", "diffHunk": "@@ -0,0 +1,247 @@\n+/*\n+ * Copyright Camunda Services GmbH and/or licensed to Camunda Services GmbH under\n+ * one or more contributor license agreements. See the NOTICE file distributed\n+ * with this work for additional information regarding copyright ownership.\n+ * Licensed under the Zeebe Community License 1.0. You may not use this file\n+ * except in compliance with the Zeebe Community License 1.0.\n+ */\n+package io.zeebe.broker.it.clustering;\n+\n+import static org.assertj.core.api.Assertions.assertThat;\n+\n+import io.atomix.raft.snapshot.impl.FileBasedSnapshotMetadata;\n+import io.zeebe.broker.Broker;\n+import io.zeebe.broker.it.util.GrpcClientRule;\n+import io.zeebe.broker.system.configuration.BrokerCfg;\n+import io.zeebe.client.ZeebeClient;\n+import io.zeebe.model.bpmn.Bpmn;\n+import io.zeebe.model.bpmn.BpmnModelInstance;\n+import io.zeebe.test.util.TestUtil;\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.time.Duration;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.RuleChain;\n+import org.springframework.util.unit.DataSize;\n+\n+public class FailOverReplicationTest {\n+\n+  private static final int PARTITION_COUNT = 1;\n+  private static final Duration SNAPSHOT_PERIOD = Duration.ofMinutes(5);\n+  private static final BpmnModelInstance WORKFLOW =\n+      Bpmn.createExecutableProcess(\"process\").startEvent().endEvent().done();\n+  private static final String WORKFLOW_RESOURCE_NAME = \"workflow.bpmn\";\n+\n+  private final ClusteringRule clusteringRule =\n+      new ClusteringRule(PARTITION_COUNT, 3, 3, FailOverReplicationTest::configureBroker);\n+  public final GrpcClientRule clientRule = new GrpcClientRule(clusteringRule);\n+  @Rule public RuleChain ruleChain = RuleChain.outerRule(clusteringRule).around(clientRule);\n+  private ZeebeClient client;\n+\n+  @Before\n+  public void init() {\n+    client = clientRule.getClient();\n+  }\n+\n+  @Test\n+  public void shouldFailOverOnNetworkPartition() {\n+    // given\n+    final var leaderNodeId = clusteringRule.getLeaderForPartition(1).getNodeId();\n+    final var leader = clusteringRule.getBroker(leaderNodeId);\n+\n+    // when\n+    clusteringRule.disconnect(leader);\n+\n+    // then\n+    final var newLeader = clusteringRule.awaitOtherLeader(1, leaderNodeId);\n+    assertThat(newLeader.getNodeId()).isNotEqualTo(leaderNodeId);\n+  }\n+\n+  @Test\n+  public void shouldNotReceiveEntriesOnDisconnect() {\n+    // given\n+    final var segmentCount = 2;\n+    final var oldLeaderId = clusteringRule.getLeaderForPartition(1).getNodeId();\n+    final var oldLeader = clusteringRule.getBroker(oldLeaderId);\n+    clusteringRule.disconnect(oldLeader);\n+\n+    // when\n+    final List<Broker> others = clusteringRule.getOtherBrokerObjects(oldLeaderId);\n+    fillSegments(others, segmentCount);\n+\n+    // then\n+    assertThat(getSegmentsCount(oldLeader)).isLessThan(segmentCount);\n+  }\n+\n+  @Test\n+  public void shouldReceiveEntriesAfterNetworkPartition() {\n+    // given\n+    final var segmentCount = 2;\n+    final var leaderNodeId = clusteringRule.getLeaderForPartition(1).getNodeId();\n+    final var oldLeader = clusteringRule.getBroker(leaderNodeId);\n+    clusteringRule.disconnect(oldLeader);\n+    final List<Broker> followers = clusteringRule.getOtherBrokerObjects(leaderNodeId);\n+    fillSegments(followers, segmentCount);\n+\n+    // when\n+    clusteringRule.connect(oldLeader);\n+\n+    // then\n+    TestUtil.doRepeatedly(() -> getSegmentsCount(oldLeader)).until(count -> count >= segmentCount);\n+    assertThat(getSegmentsCount(oldLeader)).isGreaterThanOrEqualTo(segmentCount);\n+  }\n+\n+  @Test\n+  public void shouldStillReceiveEntriesAfterStepDown() {\n+    // given\n+    final var segmentCount = 2;\n+    final var oldLeaderId = clusteringRule.getLeaderForPartition(1).getNodeId();\n+    final var oldLeader = clusteringRule.getBroker(oldLeaderId);\n+    clusteringRule.stepDownFromPartition(1);\n+    final List<Broker> others = clusteringRule.getOtherBrokerObjects(oldLeaderId);\n+    fillSegments(others, segmentCount);\n+\n+    // when\n+    fillSegments(others, segmentCount);\n+\n+    // then\n+    TestUtil.doRepeatedly(() -> getSegmentsCount(oldLeader)).until(count -> count >= segmentCount);\n+    assertThat(getSegmentsCount(oldLeader)).isGreaterThanOrEqualTo(segmentCount);\n+  }\n+\n+  @Test\n+  public void shouldReceiveSnapshotAfterNetworkPartition() {\n+    // given\n+    final var segmentCount = 2;\n+    final var previousLeaderId = clusteringRule.getLeaderForPartition(1).getNodeId();\n+    final var previousLeader = clusteringRule.getBroker(previousLeaderId);\n+    clusteringRule.disconnect(previousLeader);\n+    final var newLeaderInfo = clusteringRule.awaitOtherLeader(1, previousLeaderId);\n+    final var newLeader = clusteringRule.getBroker(newLeaderInfo.getNodeId());\n+    final List<Broker> followers = clusteringRule.getOtherBrokerObjects(previousLeaderId);\n+    fillSegments(followers, segmentCount);\n+    final var snapshotMetadata = awaitSnapshot(newLeader);\n+\n+    // when\n+    clusteringRule.connect(previousLeader);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "24d1fee58efbcee01c5c5a866bb95ced579fe581"}, "originalPosition": 133}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODIxODE0OQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                // send some commands", "url": "https://github.com/camunda-cloud/zeebe/pull/4857#discussion_r448218149", "createdAt": "2020-07-01T08:55:05Z", "author": {"login": "npepinpe"}, "path": "qa/integration-tests/src/test/java/io/zeebe/broker/it/clustering/FailOverReplicationTest.java", "diffHunk": "@@ -0,0 +1,247 @@\n+/*\n+ * Copyright Camunda Services GmbH and/or licensed to Camunda Services GmbH under\n+ * one or more contributor license agreements. See the NOTICE file distributed\n+ * with this work for additional information regarding copyright ownership.\n+ * Licensed under the Zeebe Community License 1.0. You may not use this file\n+ * except in compliance with the Zeebe Community License 1.0.\n+ */\n+package io.zeebe.broker.it.clustering;\n+\n+import static org.assertj.core.api.Assertions.assertThat;\n+\n+import io.atomix.raft.snapshot.impl.FileBasedSnapshotMetadata;\n+import io.zeebe.broker.Broker;\n+import io.zeebe.broker.it.util.GrpcClientRule;\n+import io.zeebe.broker.system.configuration.BrokerCfg;\n+import io.zeebe.client.ZeebeClient;\n+import io.zeebe.model.bpmn.Bpmn;\n+import io.zeebe.model.bpmn.BpmnModelInstance;\n+import io.zeebe.test.util.TestUtil;\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.time.Duration;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.RuleChain;\n+import org.springframework.util.unit.DataSize;\n+\n+public class FailOverReplicationTest {\n+\n+  private static final int PARTITION_COUNT = 1;\n+  private static final Duration SNAPSHOT_PERIOD = Duration.ofMinutes(5);\n+  private static final BpmnModelInstance WORKFLOW =\n+      Bpmn.createExecutableProcess(\"process\").startEvent().endEvent().done();\n+  private static final String WORKFLOW_RESOURCE_NAME = \"workflow.bpmn\";\n+\n+  private final ClusteringRule clusteringRule =\n+      new ClusteringRule(PARTITION_COUNT, 3, 3, FailOverReplicationTest::configureBroker);\n+  public final GrpcClientRule clientRule = new GrpcClientRule(clusteringRule);\n+  @Rule public RuleChain ruleChain = RuleChain.outerRule(clusteringRule).around(clientRule);\n+  private ZeebeClient client;\n+\n+  @Before\n+  public void init() {\n+    client = clientRule.getClient();\n+  }\n+\n+  @Test\n+  public void shouldFailOverOnNetworkPartition() {\n+    // given\n+    final var leaderNodeId = clusteringRule.getLeaderForPartition(1).getNodeId();\n+    final var leader = clusteringRule.getBroker(leaderNodeId);\n+\n+    // when\n+    clusteringRule.disconnect(leader);\n+\n+    // then\n+    final var newLeader = clusteringRule.awaitOtherLeader(1, leaderNodeId);\n+    assertThat(newLeader.getNodeId()).isNotEqualTo(leaderNodeId);\n+  }\n+\n+  @Test\n+  public void shouldNotReceiveEntriesOnDisconnect() {\n+    // given\n+    final var segmentCount = 2;\n+    final var oldLeaderId = clusteringRule.getLeaderForPartition(1).getNodeId();\n+    final var oldLeader = clusteringRule.getBroker(oldLeaderId);\n+    clusteringRule.disconnect(oldLeader);\n+\n+    // when\n+    final List<Broker> others = clusteringRule.getOtherBrokerObjects(oldLeaderId);\n+    fillSegments(others, segmentCount);\n+\n+    // then\n+    assertThat(getSegmentsCount(oldLeader)).isLessThan(segmentCount);\n+  }\n+\n+  @Test\n+  public void shouldReceiveEntriesAfterNetworkPartition() {\n+    // given\n+    final var segmentCount = 2;\n+    final var leaderNodeId = clusteringRule.getLeaderForPartition(1).getNodeId();\n+    final var oldLeader = clusteringRule.getBroker(leaderNodeId);\n+    clusteringRule.disconnect(oldLeader);\n+    final List<Broker> followers = clusteringRule.getOtherBrokerObjects(leaderNodeId);\n+    fillSegments(followers, segmentCount);\n+\n+    // when\n+    clusteringRule.connect(oldLeader);\n+\n+    // then\n+    TestUtil.doRepeatedly(() -> getSegmentsCount(oldLeader)).until(count -> count >= segmentCount);\n+    assertThat(getSegmentsCount(oldLeader)).isGreaterThanOrEqualTo(segmentCount);\n+  }\n+\n+  @Test\n+  public void shouldStillReceiveEntriesAfterStepDown() {\n+    // given\n+    final var segmentCount = 2;\n+    final var oldLeaderId = clusteringRule.getLeaderForPartition(1).getNodeId();\n+    final var oldLeader = clusteringRule.getBroker(oldLeaderId);\n+    clusteringRule.stepDownFromPartition(1);\n+    final List<Broker> others = clusteringRule.getOtherBrokerObjects(oldLeaderId);\n+    fillSegments(others, segmentCount);\n+\n+    // when\n+    fillSegments(others, segmentCount);\n+\n+    // then\n+    TestUtil.doRepeatedly(() -> getSegmentsCount(oldLeader)).until(count -> count >= segmentCount);\n+    assertThat(getSegmentsCount(oldLeader)).isGreaterThanOrEqualTo(segmentCount);\n+  }\n+\n+  @Test\n+  public void shouldReceiveSnapshotAfterNetworkPartition() {\n+    // given\n+    final var segmentCount = 2;\n+    final var previousLeaderId = clusteringRule.getLeaderForPartition(1).getNodeId();\n+    final var previousLeader = clusteringRule.getBroker(previousLeaderId);\n+    clusteringRule.disconnect(previousLeader);\n+    final var newLeaderInfo = clusteringRule.awaitOtherLeader(1, previousLeaderId);\n+    final var newLeader = clusteringRule.getBroker(newLeaderInfo.getNodeId());\n+    final List<Broker> followers = clusteringRule.getOtherBrokerObjects(previousLeaderId);\n+    fillSegments(followers, segmentCount);\n+    final var snapshotMetadata = awaitSnapshot(newLeader);\n+\n+    // when\n+    clusteringRule.connect(previousLeader);\n+\n+    // then\n+    final var receivedSnapshot = clusteringRule.waitForSnapshotAtBroker(previousLeader);\n+    assertThat(receivedSnapshot).isEqualTo(snapshotMetadata);\n+  }\n+\n+  // regression test for https://github.com/zeebe-io/zeebe/issues/4810\n+  @Test\n+  public void shouldFormClusterEvenWhenMissingEvents() {\n+    // given\n+    final var previousLeaderId = clusteringRule.getLeaderForPartition(1).getNodeId();\n+    final var previousLeader = clusteringRule.getBroker(previousLeaderId);\n+    // send some commands", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "24d1fee58efbcee01c5c5a866bb95ced579fe581"}, "originalPosition": 146}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODIxOTQwNA==", "bodyText": "Even with the comment I'm not sure why it's necessary \ud83d\ude05 So, dumb question, what does BECAUSE IT HAS ONE ON GOING refer to? One append request?", "url": "https://github.com/camunda-cloud/zeebe/pull/4857#discussion_r448219404", "createdAt": "2020-07-01T08:57:09Z", "author": {"login": "npepinpe"}, "path": "qa/integration-tests/src/test/java/io/zeebe/broker/it/clustering/FailOverReplicationTest.java", "diffHunk": "@@ -0,0 +1,247 @@\n+/*\n+ * Copyright Camunda Services GmbH and/or licensed to Camunda Services GmbH under\n+ * one or more contributor license agreements. See the NOTICE file distributed\n+ * with this work for additional information regarding copyright ownership.\n+ * Licensed under the Zeebe Community License 1.0. You may not use this file\n+ * except in compliance with the Zeebe Community License 1.0.\n+ */\n+package io.zeebe.broker.it.clustering;\n+\n+import static org.assertj.core.api.Assertions.assertThat;\n+\n+import io.atomix.raft.snapshot.impl.FileBasedSnapshotMetadata;\n+import io.zeebe.broker.Broker;\n+import io.zeebe.broker.it.util.GrpcClientRule;\n+import io.zeebe.broker.system.configuration.BrokerCfg;\n+import io.zeebe.client.ZeebeClient;\n+import io.zeebe.model.bpmn.Bpmn;\n+import io.zeebe.model.bpmn.BpmnModelInstance;\n+import io.zeebe.test.util.TestUtil;\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.time.Duration;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.RuleChain;\n+import org.springframework.util.unit.DataSize;\n+\n+public class FailOverReplicationTest {\n+\n+  private static final int PARTITION_COUNT = 1;\n+  private static final Duration SNAPSHOT_PERIOD = Duration.ofMinutes(5);\n+  private static final BpmnModelInstance WORKFLOW =\n+      Bpmn.createExecutableProcess(\"process\").startEvent().endEvent().done();\n+  private static final String WORKFLOW_RESOURCE_NAME = \"workflow.bpmn\";\n+\n+  private final ClusteringRule clusteringRule =\n+      new ClusteringRule(PARTITION_COUNT, 3, 3, FailOverReplicationTest::configureBroker);\n+  public final GrpcClientRule clientRule = new GrpcClientRule(clusteringRule);\n+  @Rule public RuleChain ruleChain = RuleChain.outerRule(clusteringRule).around(clientRule);\n+  private ZeebeClient client;\n+\n+  @Before\n+  public void init() {\n+    client = clientRule.getClient();\n+  }\n+\n+  @Test\n+  public void shouldFailOverOnNetworkPartition() {\n+    // given\n+    final var leaderNodeId = clusteringRule.getLeaderForPartition(1).getNodeId();\n+    final var leader = clusteringRule.getBroker(leaderNodeId);\n+\n+    // when\n+    clusteringRule.disconnect(leader);\n+\n+    // then\n+    final var newLeader = clusteringRule.awaitOtherLeader(1, leaderNodeId);\n+    assertThat(newLeader.getNodeId()).isNotEqualTo(leaderNodeId);\n+  }\n+\n+  @Test\n+  public void shouldNotReceiveEntriesOnDisconnect() {\n+    // given\n+    final var segmentCount = 2;\n+    final var oldLeaderId = clusteringRule.getLeaderForPartition(1).getNodeId();\n+    final var oldLeader = clusteringRule.getBroker(oldLeaderId);\n+    clusteringRule.disconnect(oldLeader);\n+\n+    // when\n+    final List<Broker> others = clusteringRule.getOtherBrokerObjects(oldLeaderId);\n+    fillSegments(others, segmentCount);\n+\n+    // then\n+    assertThat(getSegmentsCount(oldLeader)).isLessThan(segmentCount);\n+  }\n+\n+  @Test\n+  public void shouldReceiveEntriesAfterNetworkPartition() {\n+    // given\n+    final var segmentCount = 2;\n+    final var leaderNodeId = clusteringRule.getLeaderForPartition(1).getNodeId();\n+    final var oldLeader = clusteringRule.getBroker(leaderNodeId);\n+    clusteringRule.disconnect(oldLeader);\n+    final List<Broker> followers = clusteringRule.getOtherBrokerObjects(leaderNodeId);\n+    fillSegments(followers, segmentCount);\n+\n+    // when\n+    clusteringRule.connect(oldLeader);\n+\n+    // then\n+    TestUtil.doRepeatedly(() -> getSegmentsCount(oldLeader)).until(count -> count >= segmentCount);\n+    assertThat(getSegmentsCount(oldLeader)).isGreaterThanOrEqualTo(segmentCount);\n+  }\n+\n+  @Test\n+  public void shouldStillReceiveEntriesAfterStepDown() {\n+    // given\n+    final var segmentCount = 2;\n+    final var oldLeaderId = clusteringRule.getLeaderForPartition(1).getNodeId();\n+    final var oldLeader = clusteringRule.getBroker(oldLeaderId);\n+    clusteringRule.stepDownFromPartition(1);\n+    final List<Broker> others = clusteringRule.getOtherBrokerObjects(oldLeaderId);\n+    fillSegments(others, segmentCount);\n+\n+    // when\n+    fillSegments(others, segmentCount);\n+\n+    // then\n+    TestUtil.doRepeatedly(() -> getSegmentsCount(oldLeader)).until(count -> count >= segmentCount);\n+    assertThat(getSegmentsCount(oldLeader)).isGreaterThanOrEqualTo(segmentCount);\n+  }\n+\n+  @Test\n+  public void shouldReceiveSnapshotAfterNetworkPartition() {\n+    // given\n+    final var segmentCount = 2;\n+    final var previousLeaderId = clusteringRule.getLeaderForPartition(1).getNodeId();\n+    final var previousLeader = clusteringRule.getBroker(previousLeaderId);\n+    clusteringRule.disconnect(previousLeader);\n+    final var newLeaderInfo = clusteringRule.awaitOtherLeader(1, previousLeaderId);\n+    final var newLeader = clusteringRule.getBroker(newLeaderInfo.getNodeId());\n+    final List<Broker> followers = clusteringRule.getOtherBrokerObjects(previousLeaderId);\n+    fillSegments(followers, segmentCount);\n+    final var snapshotMetadata = awaitSnapshot(newLeader);\n+\n+    // when\n+    clusteringRule.connect(previousLeader);\n+\n+    // then\n+    final var receivedSnapshot = clusteringRule.waitForSnapshotAtBroker(previousLeader);\n+    assertThat(receivedSnapshot).isEqualTo(snapshotMetadata);\n+  }\n+\n+  // regression test for https://github.com/zeebe-io/zeebe/issues/4810\n+  @Test\n+  public void shouldFormClusterEvenWhenMissingEvents() {\n+    // given\n+    final var previousLeaderId = clusteringRule.getLeaderForPartition(1).getNodeId();\n+    final var previousLeader = clusteringRule.getBroker(previousLeaderId);\n+    // send some commands\n+    client.newDeployCommand().addWorkflowModel(WORKFLOW, WORKFLOW_RESOURCE_NAME).send().join();\n+\n+    // disconnect leader - becomes follower\n+    clusteringRule.disconnect(previousLeader);\n+    // IMPORTANT NOTE:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "24d1fee58efbcee01c5c5a866bb95ced579fe581"}, "originalPosition": 151}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODIyMjEzNA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              private void fillSegments(final List<Broker> brokers, final int segmentCount) {\n          \n          \n            \n              private void sendCommandAndAwaitMinimumSegmentCount(final List<Broker> brokers, final int segmentCount) {", "url": "https://github.com/camunda-cloud/zeebe/pull/4857#discussion_r448222134", "createdAt": "2020-07-01T09:01:56Z", "author": {"login": "npepinpe"}, "path": "qa/integration-tests/src/test/java/io/zeebe/broker/it/clustering/FailOverReplicationTest.java", "diffHunk": "@@ -0,0 +1,247 @@\n+/*\n+ * Copyright Camunda Services GmbH and/or licensed to Camunda Services GmbH under\n+ * one or more contributor license agreements. See the NOTICE file distributed\n+ * with this work for additional information regarding copyright ownership.\n+ * Licensed under the Zeebe Community License 1.0. You may not use this file\n+ * except in compliance with the Zeebe Community License 1.0.\n+ */\n+package io.zeebe.broker.it.clustering;\n+\n+import static org.assertj.core.api.Assertions.assertThat;\n+\n+import io.atomix.raft.snapshot.impl.FileBasedSnapshotMetadata;\n+import io.zeebe.broker.Broker;\n+import io.zeebe.broker.it.util.GrpcClientRule;\n+import io.zeebe.broker.system.configuration.BrokerCfg;\n+import io.zeebe.client.ZeebeClient;\n+import io.zeebe.model.bpmn.Bpmn;\n+import io.zeebe.model.bpmn.BpmnModelInstance;\n+import io.zeebe.test.util.TestUtil;\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.time.Duration;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.RuleChain;\n+import org.springframework.util.unit.DataSize;\n+\n+public class FailOverReplicationTest {\n+\n+  private static final int PARTITION_COUNT = 1;\n+  private static final Duration SNAPSHOT_PERIOD = Duration.ofMinutes(5);\n+  private static final BpmnModelInstance WORKFLOW =\n+      Bpmn.createExecutableProcess(\"process\").startEvent().endEvent().done();\n+  private static final String WORKFLOW_RESOURCE_NAME = \"workflow.bpmn\";\n+\n+  private final ClusteringRule clusteringRule =\n+      new ClusteringRule(PARTITION_COUNT, 3, 3, FailOverReplicationTest::configureBroker);\n+  public final GrpcClientRule clientRule = new GrpcClientRule(clusteringRule);\n+  @Rule public RuleChain ruleChain = RuleChain.outerRule(clusteringRule).around(clientRule);\n+  private ZeebeClient client;\n+\n+  @Before\n+  public void init() {\n+    client = clientRule.getClient();\n+  }\n+\n+  @Test\n+  public void shouldFailOverOnNetworkPartition() {\n+    // given\n+    final var leaderNodeId = clusteringRule.getLeaderForPartition(1).getNodeId();\n+    final var leader = clusteringRule.getBroker(leaderNodeId);\n+\n+    // when\n+    clusteringRule.disconnect(leader);\n+\n+    // then\n+    final var newLeader = clusteringRule.awaitOtherLeader(1, leaderNodeId);\n+    assertThat(newLeader.getNodeId()).isNotEqualTo(leaderNodeId);\n+  }\n+\n+  @Test\n+  public void shouldNotReceiveEntriesOnDisconnect() {\n+    // given\n+    final var segmentCount = 2;\n+    final var oldLeaderId = clusteringRule.getLeaderForPartition(1).getNodeId();\n+    final var oldLeader = clusteringRule.getBroker(oldLeaderId);\n+    clusteringRule.disconnect(oldLeader);\n+\n+    // when\n+    final List<Broker> others = clusteringRule.getOtherBrokerObjects(oldLeaderId);\n+    fillSegments(others, segmentCount);\n+\n+    // then\n+    assertThat(getSegmentsCount(oldLeader)).isLessThan(segmentCount);\n+  }\n+\n+  @Test\n+  public void shouldReceiveEntriesAfterNetworkPartition() {\n+    // given\n+    final var segmentCount = 2;\n+    final var leaderNodeId = clusteringRule.getLeaderForPartition(1).getNodeId();\n+    final var oldLeader = clusteringRule.getBroker(leaderNodeId);\n+    clusteringRule.disconnect(oldLeader);\n+    final List<Broker> followers = clusteringRule.getOtherBrokerObjects(leaderNodeId);\n+    fillSegments(followers, segmentCount);\n+\n+    // when\n+    clusteringRule.connect(oldLeader);\n+\n+    // then\n+    TestUtil.doRepeatedly(() -> getSegmentsCount(oldLeader)).until(count -> count >= segmentCount);\n+    assertThat(getSegmentsCount(oldLeader)).isGreaterThanOrEqualTo(segmentCount);\n+  }\n+\n+  @Test\n+  public void shouldStillReceiveEntriesAfterStepDown() {\n+    // given\n+    final var segmentCount = 2;\n+    final var oldLeaderId = clusteringRule.getLeaderForPartition(1).getNodeId();\n+    final var oldLeader = clusteringRule.getBroker(oldLeaderId);\n+    clusteringRule.stepDownFromPartition(1);\n+    final List<Broker> others = clusteringRule.getOtherBrokerObjects(oldLeaderId);\n+    fillSegments(others, segmentCount);\n+\n+    // when\n+    fillSegments(others, segmentCount);\n+\n+    // then\n+    TestUtil.doRepeatedly(() -> getSegmentsCount(oldLeader)).until(count -> count >= segmentCount);\n+    assertThat(getSegmentsCount(oldLeader)).isGreaterThanOrEqualTo(segmentCount);\n+  }\n+\n+  @Test\n+  public void shouldReceiveSnapshotAfterNetworkPartition() {\n+    // given\n+    final var segmentCount = 2;\n+    final var previousLeaderId = clusteringRule.getLeaderForPartition(1).getNodeId();\n+    final var previousLeader = clusteringRule.getBroker(previousLeaderId);\n+    clusteringRule.disconnect(previousLeader);\n+    final var newLeaderInfo = clusteringRule.awaitOtherLeader(1, previousLeaderId);\n+    final var newLeader = clusteringRule.getBroker(newLeaderInfo.getNodeId());\n+    final List<Broker> followers = clusteringRule.getOtherBrokerObjects(previousLeaderId);\n+    fillSegments(followers, segmentCount);\n+    final var snapshotMetadata = awaitSnapshot(newLeader);\n+\n+    // when\n+    clusteringRule.connect(previousLeader);\n+\n+    // then\n+    final var receivedSnapshot = clusteringRule.waitForSnapshotAtBroker(previousLeader);\n+    assertThat(receivedSnapshot).isEqualTo(snapshotMetadata);\n+  }\n+\n+  // regression test for https://github.com/zeebe-io/zeebe/issues/4810\n+  @Test\n+  public void shouldFormClusterEvenWhenMissingEvents() {\n+    // given\n+    final var previousLeaderId = clusteringRule.getLeaderForPartition(1).getNodeId();\n+    final var previousLeader = clusteringRule.getBroker(previousLeaderId);\n+    // send some commands\n+    client.newDeployCommand().addWorkflowModel(WORKFLOW, WORKFLOW_RESOURCE_NAME).send().join();\n+\n+    // disconnect leader - becomes follower\n+    clusteringRule.disconnect(previousLeader);\n+    // IMPORTANT NOTE:\n+    // BE AWARE THAT WE NEED TO STEP DOWN AFTER DISCONNECT\n+    // IT HAPPENS FROM TIME TO TIME THAT THE LEADER DOESN'T DETECT FAST ENOUGH THE NETWORK\n+    // PARTITION.\n+    // THIS MEANS IT WILL NOT STEP DOWN AND SEND NO NEW APPENDS TO THE FOLLOWER, BECAUSE IT HAS ONE\n+    // ON GOING,\n+    // WHICH SEEM NOT TO TIMEOUT CORRECTLY. SINCE MESSAGING SERVICE IS NOT STOPPED IN PRODUCTION\n+    // ENV.\n+    // WE WILL NOT FIX THIS - INSTEAD USE THIS AS WORK AROUND.\n+    clusteringRule.stepDown(previousLeader, 1);\n+    final var newLeaderInfo = clusteringRule.awaitOtherLeader(1, previousLeaderId);\n+    final var newLeaderId = newLeaderInfo.getNodeId();\n+    assertThat(newLeaderId).isNotEqualTo(previousLeaderId);\n+    final var newLeader = clusteringRule.getBroker(newLeaderId);\n+\n+    final List<Broker> followers = clusteringRule.getOtherBrokerObjects(newLeaderId);\n+    final var followerA =\n+        followers.stream()\n+            .filter(broker -> broker.getConfig().getCluster().getNodeId() != previousLeaderId)\n+            .findFirst()\n+            .orElseThrow();\n+\n+    // Leader and Follower A have new entries\n+    // which Follower B - old leader hasn't\n+    fillSegments(List.of(newLeader, followerA), 2);\n+    awaitSnapshot(newLeader);\n+    clusteringRule.waitForSnapshotAtBroker(followerA);\n+\n+    // when shutdown current leader and connect follower (old leader with old log)\n+    clusteringRule.stopBroker(newLeaderId);\n+    clusteringRule.connect(previousLeader);\n+\n+    // then\n+    // we should be able to form a cluster via replicating snapshot etc.\n+    clusteringRule.waitForSnapshotAtBroker(previousLeader);\n+    final var nextLeader = clusteringRule.awaitOtherLeader(1, newLeaderId);\n+\n+    assertThat(nextLeader.getNodeId()).isEqualTo(getNodeId(followerA));\n+    clusteringRule.waitForSnapshotAtBroker(previousLeader);\n+  }\n+\n+  private int getNodeId(final Broker broker) {\n+    return broker.getConfig().getCluster().getNodeId();\n+  }\n+\n+  private FileBasedSnapshotMetadata awaitSnapshot(final Broker leader) {\n+    triggerSnapshotCreation();\n+    return clusteringRule.waitForSnapshotAtBroker(leader);\n+  }\n+\n+  private int getSegmentsCount(final Broker broker) {\n+    return getSegments(broker).size();\n+  }\n+\n+  private Collection<Path> getSegments(final Broker broker) {\n+    try {\n+      return Files.list(clusteringRule.getSegmentsDirectory(broker))\n+          .filter(path -> path.toString().endsWith(\".log\"))\n+          .collect(Collectors.toList());\n+    } catch (final IOException e) {\n+      throw new UncheckedIOException(e);\n+    }\n+  }\n+\n+  private void fillSegments(final List<Broker> brokers, final int segmentCount) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "24d1fee58efbcee01c5c5a866bb95ced579fe581"}, "originalPosition": 215}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODIyMjkxMg==", "bodyText": "Why are we waiting for a snapshot/the snapshot to be replicated?", "url": "https://github.com/camunda-cloud/zeebe/pull/4857#discussion_r448222912", "createdAt": "2020-07-01T09:03:15Z", "author": {"login": "npepinpe"}, "path": "qa/integration-tests/src/test/java/io/zeebe/broker/it/clustering/FailOverReplicationTest.java", "diffHunk": "@@ -0,0 +1,247 @@\n+/*\n+ * Copyright Camunda Services GmbH and/or licensed to Camunda Services GmbH under\n+ * one or more contributor license agreements. See the NOTICE file distributed\n+ * with this work for additional information regarding copyright ownership.\n+ * Licensed under the Zeebe Community License 1.0. You may not use this file\n+ * except in compliance with the Zeebe Community License 1.0.\n+ */\n+package io.zeebe.broker.it.clustering;\n+\n+import static org.assertj.core.api.Assertions.assertThat;\n+\n+import io.atomix.raft.snapshot.impl.FileBasedSnapshotMetadata;\n+import io.zeebe.broker.Broker;\n+import io.zeebe.broker.it.util.GrpcClientRule;\n+import io.zeebe.broker.system.configuration.BrokerCfg;\n+import io.zeebe.client.ZeebeClient;\n+import io.zeebe.model.bpmn.Bpmn;\n+import io.zeebe.model.bpmn.BpmnModelInstance;\n+import io.zeebe.test.util.TestUtil;\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.time.Duration;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.RuleChain;\n+import org.springframework.util.unit.DataSize;\n+\n+public class FailOverReplicationTest {\n+\n+  private static final int PARTITION_COUNT = 1;\n+  private static final Duration SNAPSHOT_PERIOD = Duration.ofMinutes(5);\n+  private static final BpmnModelInstance WORKFLOW =\n+      Bpmn.createExecutableProcess(\"process\").startEvent().endEvent().done();\n+  private static final String WORKFLOW_RESOURCE_NAME = \"workflow.bpmn\";\n+\n+  private final ClusteringRule clusteringRule =\n+      new ClusteringRule(PARTITION_COUNT, 3, 3, FailOverReplicationTest::configureBroker);\n+  public final GrpcClientRule clientRule = new GrpcClientRule(clusteringRule);\n+  @Rule public RuleChain ruleChain = RuleChain.outerRule(clusteringRule).around(clientRule);\n+  private ZeebeClient client;\n+\n+  @Before\n+  public void init() {\n+    client = clientRule.getClient();\n+  }\n+\n+  @Test\n+  public void shouldFailOverOnNetworkPartition() {\n+    // given\n+    final var leaderNodeId = clusteringRule.getLeaderForPartition(1).getNodeId();\n+    final var leader = clusteringRule.getBroker(leaderNodeId);\n+\n+    // when\n+    clusteringRule.disconnect(leader);\n+\n+    // then\n+    final var newLeader = clusteringRule.awaitOtherLeader(1, leaderNodeId);\n+    assertThat(newLeader.getNodeId()).isNotEqualTo(leaderNodeId);\n+  }\n+\n+  @Test\n+  public void shouldNotReceiveEntriesOnDisconnect() {\n+    // given\n+    final var segmentCount = 2;\n+    final var oldLeaderId = clusteringRule.getLeaderForPartition(1).getNodeId();\n+    final var oldLeader = clusteringRule.getBroker(oldLeaderId);\n+    clusteringRule.disconnect(oldLeader);\n+\n+    // when\n+    final List<Broker> others = clusteringRule.getOtherBrokerObjects(oldLeaderId);\n+    fillSegments(others, segmentCount);\n+\n+    // then\n+    assertThat(getSegmentsCount(oldLeader)).isLessThan(segmentCount);\n+  }\n+\n+  @Test\n+  public void shouldReceiveEntriesAfterNetworkPartition() {\n+    // given\n+    final var segmentCount = 2;\n+    final var leaderNodeId = clusteringRule.getLeaderForPartition(1).getNodeId();\n+    final var oldLeader = clusteringRule.getBroker(leaderNodeId);\n+    clusteringRule.disconnect(oldLeader);\n+    final List<Broker> followers = clusteringRule.getOtherBrokerObjects(leaderNodeId);\n+    fillSegments(followers, segmentCount);\n+\n+    // when\n+    clusteringRule.connect(oldLeader);\n+\n+    // then\n+    TestUtil.doRepeatedly(() -> getSegmentsCount(oldLeader)).until(count -> count >= segmentCount);\n+    assertThat(getSegmentsCount(oldLeader)).isGreaterThanOrEqualTo(segmentCount);\n+  }\n+\n+  @Test\n+  public void shouldStillReceiveEntriesAfterStepDown() {\n+    // given\n+    final var segmentCount = 2;\n+    final var oldLeaderId = clusteringRule.getLeaderForPartition(1).getNodeId();\n+    final var oldLeader = clusteringRule.getBroker(oldLeaderId);\n+    clusteringRule.stepDownFromPartition(1);\n+    final List<Broker> others = clusteringRule.getOtherBrokerObjects(oldLeaderId);\n+    fillSegments(others, segmentCount);\n+\n+    // when\n+    fillSegments(others, segmentCount);\n+\n+    // then\n+    TestUtil.doRepeatedly(() -> getSegmentsCount(oldLeader)).until(count -> count >= segmentCount);\n+    assertThat(getSegmentsCount(oldLeader)).isGreaterThanOrEqualTo(segmentCount);\n+  }\n+\n+  @Test\n+  public void shouldReceiveSnapshotAfterNetworkPartition() {\n+    // given\n+    final var segmentCount = 2;\n+    final var previousLeaderId = clusteringRule.getLeaderForPartition(1).getNodeId();\n+    final var previousLeader = clusteringRule.getBroker(previousLeaderId);\n+    clusteringRule.disconnect(previousLeader);\n+    final var newLeaderInfo = clusteringRule.awaitOtherLeader(1, previousLeaderId);\n+    final var newLeader = clusteringRule.getBroker(newLeaderInfo.getNodeId());\n+    final List<Broker> followers = clusteringRule.getOtherBrokerObjects(previousLeaderId);\n+    fillSegments(followers, segmentCount);\n+    final var snapshotMetadata = awaitSnapshot(newLeader);\n+\n+    // when\n+    clusteringRule.connect(previousLeader);\n+\n+    // then\n+    final var receivedSnapshot = clusteringRule.waitForSnapshotAtBroker(previousLeader);\n+    assertThat(receivedSnapshot).isEqualTo(snapshotMetadata);\n+  }\n+\n+  // regression test for https://github.com/zeebe-io/zeebe/issues/4810\n+  @Test\n+  public void shouldFormClusterEvenWhenMissingEvents() {\n+    // given\n+    final var previousLeaderId = clusteringRule.getLeaderForPartition(1).getNodeId();\n+    final var previousLeader = clusteringRule.getBroker(previousLeaderId);\n+    // send some commands\n+    client.newDeployCommand().addWorkflowModel(WORKFLOW, WORKFLOW_RESOURCE_NAME).send().join();\n+\n+    // disconnect leader - becomes follower\n+    clusteringRule.disconnect(previousLeader);\n+    // IMPORTANT NOTE:\n+    // BE AWARE THAT WE NEED TO STEP DOWN AFTER DISCONNECT\n+    // IT HAPPENS FROM TIME TO TIME THAT THE LEADER DOESN'T DETECT FAST ENOUGH THE NETWORK\n+    // PARTITION.\n+    // THIS MEANS IT WILL NOT STEP DOWN AND SEND NO NEW APPENDS TO THE FOLLOWER, BECAUSE IT HAS ONE\n+    // ON GOING,\n+    // WHICH SEEM NOT TO TIMEOUT CORRECTLY. SINCE MESSAGING SERVICE IS NOT STOPPED IN PRODUCTION\n+    // ENV.\n+    // WE WILL NOT FIX THIS - INSTEAD USE THIS AS WORK AROUND.\n+    clusteringRule.stepDown(previousLeader, 1);\n+    final var newLeaderInfo = clusteringRule.awaitOtherLeader(1, previousLeaderId);\n+    final var newLeaderId = newLeaderInfo.getNodeId();\n+    assertThat(newLeaderId).isNotEqualTo(previousLeaderId);\n+    final var newLeader = clusteringRule.getBroker(newLeaderId);\n+\n+    final List<Broker> followers = clusteringRule.getOtherBrokerObjects(newLeaderId);\n+    final var followerA =\n+        followers.stream()\n+            .filter(broker -> broker.getConfig().getCluster().getNodeId() != previousLeaderId)\n+            .findFirst()\n+            .orElseThrow();\n+\n+    // Leader and Follower A have new entries\n+    // which Follower B - old leader hasn't\n+    fillSegments(List.of(newLeader, followerA), 2);\n+    awaitSnapshot(newLeader);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "24d1fee58efbcee01c5c5a866bb95ced579fe581"}, "originalPosition": 176}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODIyMzI1MQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              private final List<AsyncClosable> asyncClosables = new ArrayList<>();\n          \n          \n            \n              private final List<AsyncClosable> managedResources = new ArrayList<>();", "url": "https://github.com/camunda-cloud/zeebe/pull/4857#discussion_r448223251", "createdAt": "2020-07-01T09:03:53Z", "author": {"login": "npepinpe"}, "path": "broker/src/main/java/io/zeebe/broker/system/partitions/ZeebePartition.java", "diffHunk": "@@ -84,7 +84,7 @@\n   private final TypedRecordProcessorsFactory typedRecordProcessorsFactory;\n   private final CommandApiService commandApiService;\n   private final List<PartitionListener> partitionListeners;\n-  private final List<Actor> closeables = new ArrayList<>();\n+  private final List<AsyncClosable> asyncClosables = new ArrayList<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "24d1fee58efbcee01c5c5a866bb95ced579fe581"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODIyNTIyOQ==", "bodyText": "Nit: can we also extract this to a method?", "url": "https://github.com/camunda-cloud/zeebe/pull/4857#discussion_r448225229", "createdAt": "2020-07-01T09:07:27Z", "author": {"login": "npepinpe"}, "path": "broker/src/main/java/io/zeebe/broker/system/partitions/ZeebePartition.java", "diffHunk": "@@ -399,30 +395,34 @@ private void transitionToLeader(final CompletableActorFuture<Void> transitionCom\n   }\n \n   private ActorFuture<Void> installStorageServices() {\n-    final var pendingDirectory =\n-        atomixRaftPartition.dataDirectory().toPath().resolve(\"pushed-pending\");\n-    try {\n-      FileUtil.ensureDirectoryExists(pendingDirectory);\n-    } catch (final IOException e) {\n-      LOG.error(\"Failed to created snapshot storage pending directory {}\", pendingDirectory, e);\n-      return CompletableActorFuture.completedExceptionally(e);\n-    }\n-\n     persistedSnapshotStore = atomixRaftPartition.getServer().getPersistedSnapshotStore();\n-    snapshotController = createSnapshotController();\n+\n+    final var controller = createSnapshotController();\n+    asyncClosables.add(this::closeSnapshotController);\n+    snapshotController = controller;\n \n     final LogCompactor logCompactor = new AtomixLogCompactor(atomixRaftPartition.getServer());\n     final LogDeletionService deletionService =\n         new LogDeletionService(\n             localBroker.getNodeId(), partitionId, logCompactor, persistedSnapshotStore);\n-    closeables.add(deletionService);\n+    asyncClosables.add(deletionService);\n \n     return scheduler.submitActor(deletionService);\n   }\n \n+  private void registerSnapshotListenerForReplication() {\n+    asyncClosables.add(\n+        () -> {\n+          persistedSnapshotStore.removeSnapshotListener(snapshotController);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "24d1fee58efbcee01c5c5a866bb95ced579fe581"}, "originalPosition": 137}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODIyNjQ3OQ==", "bodyText": "Who closes zeebeDb? ZeebePartition? StateSnapshotController? Just wondering if this null check is enough.\nAlso, a little out of scope but since we're here, should we rather reschedule the next metrics export here instead of using runAtFixedRate? We've ran into issues before where you may build up a lot of tasks with runAtFixedRate, no? Though I guess here our actor is usually not very busy, so \ud83e\udd37\u200d\u2642\ufe0f", "url": "https://github.com/camunda-cloud/zeebe/pull/4857#discussion_r448226479", "createdAt": "2020-07-01T09:09:39Z", "author": {"login": "npepinpe"}, "path": "broker/src/main/java/io/zeebe/broker/system/partitions/ZeebePartition.java", "diffHunk": "@@ -485,6 +486,22 @@ private void installProcessingPartition(final CompletableActorFuture<Void> insta\n             });\n   }\n \n+  private void installRocksDBMetricExporter(final ZeebeRocksDBMetricExporter metricExporter) {\n+    final var metricsTimer =\n+        actor.runAtFixedRate(\n+            Duration.ofSeconds(5),\n+            () -> {\n+              if (zeebeDb != null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "24d1fee58efbcee01c5c5a866bb95ced579fe581"}, "originalPosition": 210}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQwOTU2MTQ4", "url": "https://github.com/camunda-cloud/zeebe/pull/4857#pullrequestreview-440956148", "createdAt": "2020-07-01T15:04:29Z", "commit": {"oid": "6600855f44e9c0f8393dbaa6ebb92e8384d5b74e"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQxNTowNDoyOVrOGrp3_g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQxNTowNDoyOVrOGrp3_g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODQyODAzMA==", "bodyText": "This is a pretty long lambda, can we extract it to a method?", "url": "https://github.com/camunda-cloud/zeebe/pull/4857#discussion_r448428030", "createdAt": "2020-07-01T15:04:29Z", "author": {"login": "npepinpe"}, "path": "qa/integration-tests/src/test/java/io/zeebe/broker/it/clustering/ClusteringRule.java", "diffHunk": "@@ -634,38 +692,28 @@ public FileBasedSnapshotMetadata waitForSnapshotAtBroker(final Broker broker) {\n    * @param broker the broker to check on\n    * @param previousSnapshot the previous expected snapshot\n    * @return the new snapshot metadata\n-   * @throws AssertionError if no new snapshot has been found after enough repetitions (see {@link\n-   *     io.zeebe.test.util.TestUtil#waitUntil(BooleanSupplier)}\n-   * @throws IllegalStateException if no new snapshot has been found but {@link\n-   *     io.zeebe.test.util.TestUtil#waitUntil(BooleanSupplier)} did not fail\n    */\n   FileBasedSnapshotMetadata waitForNewSnapshotAtBroker(\n       final Broker broker, final FileBasedSnapshotMetadata previousSnapshot) {\n-    final var referenceToResult =\n-        new AtomicReference<>(Optional.<FileBasedSnapshotMetadata>empty());\n     final File snapshotsDir = getSnapshotsDirectory(broker);\n-    waitUntil(\n-        () -> {\n-          final File[] files = snapshotsDir.listFiles();\n-          if (files == null || files.length != 1) {\n-            return false;\n-          }\n-\n-          final var snapshotPath = files[0].toPath();\n-          final var latestSnapshot = FileBasedSnapshotMetadata.ofPath(snapshotPath);\n-          if (latestSnapshot.isPresent()\n-              && (previousSnapshot == null\n-                  || latestSnapshot.get().compareTo(previousSnapshot) > 0)) {\n-            referenceToResult.set(latestSnapshot);\n-            return true;\n-          }\n-\n-          return false;\n-        },\n-        1000);\n-\n-    return referenceToResult\n-        .get()\n+\n+    return Awaitility.await()\n+        .pollInterval(Duration.ofMillis(100))\n+        .atMost(Duration.ofMinutes(1))\n+        .until(\n+            () -> {\n+              final var files = snapshotsDir.listFiles();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6600855f44e9c0f8393dbaa6ebb92e8384d5b74e"}, "originalPosition": 226}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQyMjU5OTc3", "url": "https://github.com/camunda-cloud/zeebe/pull/4857#pullrequestreview-442259977", "createdAt": "2020-07-03T08:58:51Z", "commit": {"oid": "87d55b03a4e4cca8412c0251248da7c1321ab786"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4530fcfa1196559001758faa98cdc23ec344b11a", "author": {"user": {"login": "Zelldon", "name": "Christopher Zell"}}, "url": "https://github.com/camunda-cloud/zeebe/commit/4530fcfa1196559001758faa98cdc23ec344b11a", "committedDate": "2020-07-06T15:35:24Z", "message": "chore(atomix): catch NSFE and log on debug level"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ff56619a17258b28b85dedb7e1f8a46e189a06e0", "author": {"user": {"login": "Zelldon", "name": "Christopher Zell"}}, "url": "https://github.com/camunda-cloud/zeebe/commit/ff56619a17258b28b85dedb7e1f8a46e189a06e0", "committedDate": "2020-07-06T15:35:24Z", "message": "chore(atomix): logRequest and Response on debug"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8fb40a34ed8389885e5838d386f092be999475cd", "author": {"user": {"login": "Zelldon", "name": "Christopher Zell"}}, "url": "https://github.com/camunda-cloud/zeebe/commit/8fb40a34ed8389885e5838d386f092be999475cd", "committedDate": "2020-07-06T15:35:24Z", "message": "chore(atomix): close connections on stop"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "87d55b03a4e4cca8412c0251248da7c1321ab786", "author": {"user": {"login": "Zelldon", "name": "Christopher Zell"}}, "url": "https://github.com/camunda-cloud/zeebe/commit/87d55b03a4e4cca8412c0251248da7c1321ab786", "committedDate": "2020-07-03T05:28:02Z", "message": "chore(broker): add missing snapshot listener"}, "afterCommit": {"oid": "dbbb1f32f6ff1a77a93f92fa85149cedec9c128f", "author": {"user": {"login": "Zelldon", "name": "Christopher Zell"}}, "url": "https://github.com/camunda-cloud/zeebe/commit/dbbb1f32f6ff1a77a93f92fa85149cedec9c128f", "committedDate": "2020-07-06T15:41:25Z", "message": "chore(broker): add missing snapshot listener"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "caf1e2c27f3326e7d20b7e9a31778698a53e8666", "author": {"user": {"login": "Zelldon", "name": "Christopher Zell"}}, "url": "https://github.com/camunda-cloud/zeebe/commit/caf1e2c27f3326e7d20b7e9a31778698a53e8666", "committedDate": "2020-07-07T04:32:00Z", "message": "chore(qa): use all nodes as initial contact points\n\n makes qa tests less flaky"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9303a37b5edc044164ef104a6b21ff28acf75bb3", "author": {"user": {"login": "Zelldon", "name": "Christopher Zell"}}, "url": "https://github.com/camunda-cloud/zeebe/commit/9303a37b5edc044164ef104a6b21ff28acf75bb3", "committedDate": "2020-07-07T04:41:40Z", "message": "chore(qa): migrate ClusteringRule to Awaitility"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bdde8e022c6193f1580016ec51fe12a1e5de2b0d", "author": {"user": {"login": "Zelldon", "name": "Christopher Zell"}}, "url": "https://github.com/camunda-cloud/zeebe/commit/bdde8e022c6193f1580016ec51fe12a1e5de2b0d", "committedDate": "2020-07-07T04:41:40Z", "message": "chore(qa): add new fail over tests\n\n Add functionallity to the clustering rule to cause a network partition and to step down from the leadership\n Added new tests which test wether leader steps down on network partition and new leader is choosen and that replication still works on reconnect.\n Added regression test for #4810"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "583ca9a9042bb0e6abbb4e58e326698a03de14c2", "author": {"user": {"login": "Zelldon", "name": "Christopher Zell"}}, "url": "https://github.com/camunda-cloud/zeebe/commit/583ca9a9042bb0e6abbb4e58e326698a03de14c2", "committedDate": "2020-07-07T04:41:40Z", "message": "fix(broker): fix closing of resources in ZeebePartition\n\n Some resources were not correctly closed or not removed as listener.\n Added new interface to close asynchronously a resource. Remove in the ZeebePartition the hybrid handling of resources.\n Now we have just a closable list which is iterated on closing the partition.\n\n SnapshotStore is not closed in ZeebePartition, since the ZeebePartition is not responsible for that and causes an bug #4810.\n\n StateController is only registered for replication on LeaderPartition, fixes #4844\n MetricsTimer is closed correctly, fixes #4847\n\n The state controller was previous also registered on follower side, which caused replication on follower side when new snapshot was received this caused #4686 this is fixed now."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "6d678b3779cfe7462b689903a372e649ba51a11d", "author": {"user": {"login": "Zelldon", "name": "Christopher Zell"}}, "url": "https://github.com/camunda-cloud/zeebe/commit/6d678b3779cfe7462b689903a372e649ba51a11d", "committedDate": "2020-07-06T19:08:55Z", "message": "chore(qa): fix flakes"}, "afterCommit": {"oid": "583ca9a9042bb0e6abbb4e58e326698a03de14c2", "author": {"user": {"login": "Zelldon", "name": "Christopher Zell"}}, "url": "https://github.com/camunda-cloud/zeebe/commit/583ca9a9042bb0e6abbb4e58e326698a03de14c2", "committedDate": "2020-07-07T04:41:40Z", "message": "fix(broker): fix closing of resources in ZeebePartition\n\n Some resources were not correctly closed or not removed as listener.\n Added new interface to close asynchronously a resource. Remove in the ZeebePartition the hybrid handling of resources.\n Now we have just a closable list which is iterated on closing the partition.\n\n SnapshotStore is not closed in ZeebePartition, since the ZeebePartition is not responsible for that and causes an bug #4810.\n\n StateController is only registered for replication on LeaderPartition, fixes #4844\n MetricsTimer is closed correctly, fixes #4847\n\n The state controller was previous also registered on follower side, which caused replication on follower side when new snapshot was received this caused #4686 this is fixed now."}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2663, "cost": 1, "resetAt": "2021-11-01T13:07:16Z"}}}