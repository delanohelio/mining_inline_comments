{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzg5NzQ1Mzc0", "number": 4063, "reviewThreads": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xN1QxMTo1NjowOFrODosIcg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xN1QyMDoxODowOFrODo32Kg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQzOTkyNjkwOnYy", "diffSide": "RIGHT", "path": "qa/integration-tests/src/test/java/io/zeebe/broker/it/clustering/ClusteringRule.java", "isResolved": true, "comments": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xN1QxMTo1NjowOFrOF3Y_jQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOFQwOToxNjoxMlrOF38H2w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzYyNTQ4NQ==", "bodyText": "I think this would work also if the latest is the same as the previous right? It then returns and I think it is a bit misleading that it says waitForNew.. ?", "url": "https://github.com/camunda-cloud/zeebe/pull/4063#discussion_r393625485", "createdAt": "2020-03-17T11:56:08Z", "author": {"login": "Zelldon"}, "path": "qa/integration-tests/src/test/java/io/zeebe/broker/it/clustering/ClusteringRule.java", "diffHunk": "@@ -606,16 +610,40 @@ public File getSnapshotsDirectory(final Broker broker) {\n     return new File(dataDir, RAFT_PARTITION_PATH + \"/snapshots\");\n   }\n \n-  public void waitForValidSnapshotAtBroker(final Broker broker) {\n-    waitForValidSnapshotAtBroker(broker, 1);\n+  public DbSnapshotMetadata waitForSnapshotAtBroker(final Broker broker) {\n+    return waitForNewSnapshotAtBroker(broker, null);\n   }\n \n-  void waitForValidSnapshotAtBroker(final Broker broker, final int snapshotCount) {\n+  DbSnapshotMetadata waitForNewSnapshotAtBroker(\n+      final Broker broker, final DbSnapshotMetadata previousSnapshot) {\n+    final var referenceToResult = new AtomicReference<>(Optional.<DbSnapshotMetadata>empty());\n     final File snapshotsDir = getSnapshotsDirectory(broker);\n     waitUntil(\n-        () ->\n-            Optional.ofNullable(snapshotsDir.listFiles()).map(f -> f.length).orElse(0)\n-                >= snapshotCount);\n+        () -> {\n+          final File[] files = snapshotsDir.listFiles();\n+          if (files == null || files.length == 0) {\n+            return false;\n+          }\n+          final Optional<DbSnapshotMetadata> latestSnapshot =\n+              Arrays.stream(files)\n+                  .map(File::toPath)\n+                  .map(DbSnapshotMetadata::ofPath)\n+                  .flatMap(Optional::stream)\n+                  .max(Comparator.naturalOrder());\n+          if (previousSnapshot != null", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8b02113cf038b85108829ddc80379c7beea796d5"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzY2MDI2NQ==", "bodyText": "No that is not correct. Either previousSnapshot is null, in which case it is enough to just find a snapshot. Or previousSnapshot is not null, in which case latestSnapshot.map(previousSnapshot::compareTo).orElse(0) needs to be -1 for the latest to actually be newer than the previous. The compareTo will return 0 if the previousSnapshot and the latestSnapshot are the same.\nI've tried an early return (with exceptional cases first) approach to this method. So only in the last return return true it will have actually found something and stop waiting.", "url": "https://github.com/camunda-cloud/zeebe/pull/4063#discussion_r393660265", "createdAt": "2020-03-17T13:00:17Z", "author": {"login": "korthout"}, "path": "qa/integration-tests/src/test/java/io/zeebe/broker/it/clustering/ClusteringRule.java", "diffHunk": "@@ -606,16 +610,40 @@ public File getSnapshotsDirectory(final Broker broker) {\n     return new File(dataDir, RAFT_PARTITION_PATH + \"/snapshots\");\n   }\n \n-  public void waitForValidSnapshotAtBroker(final Broker broker) {\n-    waitForValidSnapshotAtBroker(broker, 1);\n+  public DbSnapshotMetadata waitForSnapshotAtBroker(final Broker broker) {\n+    return waitForNewSnapshotAtBroker(broker, null);\n   }\n \n-  void waitForValidSnapshotAtBroker(final Broker broker, final int snapshotCount) {\n+  DbSnapshotMetadata waitForNewSnapshotAtBroker(\n+      final Broker broker, final DbSnapshotMetadata previousSnapshot) {\n+    final var referenceToResult = new AtomicReference<>(Optional.<DbSnapshotMetadata>empty());\n     final File snapshotsDir = getSnapshotsDirectory(broker);\n     waitUntil(\n-        () ->\n-            Optional.ofNullable(snapshotsDir.listFiles()).map(f -> f.length).orElse(0)\n-                >= snapshotCount);\n+        () -> {\n+          final File[] files = snapshotsDir.listFiles();\n+          if (files == null || files.length == 0) {\n+            return false;\n+          }\n+          final Optional<DbSnapshotMetadata> latestSnapshot =\n+              Arrays.stream(files)\n+                  .map(File::toPath)\n+                  .map(DbSnapshotMetadata::ofPath)\n+                  .flatMap(Optional::stream)\n+                  .max(Comparator.naturalOrder());\n+          if (previousSnapshot != null", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzYyNTQ4NQ=="}, "originalCommit": {"oid": "8b02113cf038b85108829ddc80379c7beea796d5"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzY2NTc3OA==", "bodyText": "latestSnapshot.map(previousSnapshot::compareTo).orElse(0)\n\nIs translated to:\nif (latestSnapshot != null)\n{\n\n  return latestsnapshot.compareTo(previousSnapshot); // return 1 when latest bigger, 0 when equal, -1 when smaller\n}\nreturn 0;\n\nOr is it previousSnapshot.compareTo(latestSnapshot) ? \ud83d\ude05", "url": "https://github.com/camunda-cloud/zeebe/pull/4063#discussion_r393665778", "createdAt": "2020-03-17T13:09:30Z", "author": {"login": "Zelldon"}, "path": "qa/integration-tests/src/test/java/io/zeebe/broker/it/clustering/ClusteringRule.java", "diffHunk": "@@ -606,16 +610,40 @@ public File getSnapshotsDirectory(final Broker broker) {\n     return new File(dataDir, RAFT_PARTITION_PATH + \"/snapshots\");\n   }\n \n-  public void waitForValidSnapshotAtBroker(final Broker broker) {\n-    waitForValidSnapshotAtBroker(broker, 1);\n+  public DbSnapshotMetadata waitForSnapshotAtBroker(final Broker broker) {\n+    return waitForNewSnapshotAtBroker(broker, null);\n   }\n \n-  void waitForValidSnapshotAtBroker(final Broker broker, final int snapshotCount) {\n+  DbSnapshotMetadata waitForNewSnapshotAtBroker(\n+      final Broker broker, final DbSnapshotMetadata previousSnapshot) {\n+    final var referenceToResult = new AtomicReference<>(Optional.<DbSnapshotMetadata>empty());\n     final File snapshotsDir = getSnapshotsDirectory(broker);\n     waitUntil(\n-        () ->\n-            Optional.ofNullable(snapshotsDir.listFiles()).map(f -> f.length).orElse(0)\n-                >= snapshotCount);\n+        () -> {\n+          final File[] files = snapshotsDir.listFiles();\n+          if (files == null || files.length == 0) {\n+            return false;\n+          }\n+          final Optional<DbSnapshotMetadata> latestSnapshot =\n+              Arrays.stream(files)\n+                  .map(File::toPath)\n+                  .map(DbSnapshotMetadata::ofPath)\n+                  .flatMap(Optional::stream)\n+                  .max(Comparator.naturalOrder());\n+          if (previousSnapshot != null", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzYyNTQ4NQ=="}, "originalCommit": {"oid": "8b02113cf038b85108829ddc80379c7beea796d5"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzY3MTQxNg==", "bodyText": "It's actually:\nlatestSnapshot.map(latestSnapshot -> previousSnapshot.compareTo(latestSnapshot)).orElse(0)", "url": "https://github.com/camunda-cloud/zeebe/pull/4063#discussion_r393671416", "createdAt": "2020-03-17T13:18:29Z", "author": {"login": "korthout"}, "path": "qa/integration-tests/src/test/java/io/zeebe/broker/it/clustering/ClusteringRule.java", "diffHunk": "@@ -606,16 +610,40 @@ public File getSnapshotsDirectory(final Broker broker) {\n     return new File(dataDir, RAFT_PARTITION_PATH + \"/snapshots\");\n   }\n \n-  public void waitForValidSnapshotAtBroker(final Broker broker) {\n-    waitForValidSnapshotAtBroker(broker, 1);\n+  public DbSnapshotMetadata waitForSnapshotAtBroker(final Broker broker) {\n+    return waitForNewSnapshotAtBroker(broker, null);\n   }\n \n-  void waitForValidSnapshotAtBroker(final Broker broker, final int snapshotCount) {\n+  DbSnapshotMetadata waitForNewSnapshotAtBroker(\n+      final Broker broker, final DbSnapshotMetadata previousSnapshot) {\n+    final var referenceToResult = new AtomicReference<>(Optional.<DbSnapshotMetadata>empty());\n     final File snapshotsDir = getSnapshotsDirectory(broker);\n     waitUntil(\n-        () ->\n-            Optional.ofNullable(snapshotsDir.listFiles()).map(f -> f.length).orElse(0)\n-                >= snapshotCount);\n+        () -> {\n+          final File[] files = snapshotsDir.listFiles();\n+          if (files == null || files.length == 0) {\n+            return false;\n+          }\n+          final Optional<DbSnapshotMetadata> latestSnapshot =\n+              Arrays.stream(files)\n+                  .map(File::toPath)\n+                  .map(DbSnapshotMetadata::ofPath)\n+                  .flatMap(Optional::stream)\n+                  .max(Comparator.naturalOrder());\n+          if (previousSnapshot != null", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzYyNTQ4NQ=="}, "originalCommit": {"oid": "8b02113cf038b85108829ddc80379c7beea796d5"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzY3MjczOQ==", "bodyText": "So,\nif latest is empty, the result 0.\nIf previous is before latest, the result is -1.\nIf previous is the same as latest, the result is 0.\nIf previous is after latest, the result is 1.", "url": "https://github.com/camunda-cloud/zeebe/pull/4063#discussion_r393672739", "createdAt": "2020-03-17T13:20:31Z", "author": {"login": "korthout"}, "path": "qa/integration-tests/src/test/java/io/zeebe/broker/it/clustering/ClusteringRule.java", "diffHunk": "@@ -606,16 +610,40 @@ public File getSnapshotsDirectory(final Broker broker) {\n     return new File(dataDir, RAFT_PARTITION_PATH + \"/snapshots\");\n   }\n \n-  public void waitForValidSnapshotAtBroker(final Broker broker) {\n-    waitForValidSnapshotAtBroker(broker, 1);\n+  public DbSnapshotMetadata waitForSnapshotAtBroker(final Broker broker) {\n+    return waitForNewSnapshotAtBroker(broker, null);\n   }\n \n-  void waitForValidSnapshotAtBroker(final Broker broker, final int snapshotCount) {\n+  DbSnapshotMetadata waitForNewSnapshotAtBroker(\n+      final Broker broker, final DbSnapshotMetadata previousSnapshot) {\n+    final var referenceToResult = new AtomicReference<>(Optional.<DbSnapshotMetadata>empty());\n     final File snapshotsDir = getSnapshotsDirectory(broker);\n     waitUntil(\n-        () ->\n-            Optional.ofNullable(snapshotsDir.listFiles()).map(f -> f.length).orElse(0)\n-                >= snapshotCount);\n+        () -> {\n+          final File[] files = snapshotsDir.listFiles();\n+          if (files == null || files.length == 0) {\n+            return false;\n+          }\n+          final Optional<DbSnapshotMetadata> latestSnapshot =\n+              Arrays.stream(files)\n+                  .map(File::toPath)\n+                  .map(DbSnapshotMetadata::ofPath)\n+                  .flatMap(Optional::stream)\n+                  .max(Comparator.naturalOrder());\n+          if (previousSnapshot != null", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzYyNTQ4NQ=="}, "originalCommit": {"oid": "8b02113cf038b85108829ddc80379c7beea796d5"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzY3MzM4Mw==", "bodyText": "Only the case where previous is before the latest (or in other words, the latest is actually the newest) it stops waiting", "url": "https://github.com/camunda-cloud/zeebe/pull/4063#discussion_r393673383", "createdAt": "2020-03-17T13:21:30Z", "author": {"login": "korthout"}, "path": "qa/integration-tests/src/test/java/io/zeebe/broker/it/clustering/ClusteringRule.java", "diffHunk": "@@ -606,16 +610,40 @@ public File getSnapshotsDirectory(final Broker broker) {\n     return new File(dataDir, RAFT_PARTITION_PATH + \"/snapshots\");\n   }\n \n-  public void waitForValidSnapshotAtBroker(final Broker broker) {\n-    waitForValidSnapshotAtBroker(broker, 1);\n+  public DbSnapshotMetadata waitForSnapshotAtBroker(final Broker broker) {\n+    return waitForNewSnapshotAtBroker(broker, null);\n   }\n \n-  void waitForValidSnapshotAtBroker(final Broker broker, final int snapshotCount) {\n+  DbSnapshotMetadata waitForNewSnapshotAtBroker(\n+      final Broker broker, final DbSnapshotMetadata previousSnapshot) {\n+    final var referenceToResult = new AtomicReference<>(Optional.<DbSnapshotMetadata>empty());\n     final File snapshotsDir = getSnapshotsDirectory(broker);\n     waitUntil(\n-        () ->\n-            Optional.ofNullable(snapshotsDir.listFiles()).map(f -> f.length).orElse(0)\n-                >= snapshotCount);\n+        () -> {\n+          final File[] files = snapshotsDir.listFiles();\n+          if (files == null || files.length == 0) {\n+            return false;\n+          }\n+          final Optional<DbSnapshotMetadata> latestSnapshot =\n+              Arrays.stream(files)\n+                  .map(File::toPath)\n+                  .map(DbSnapshotMetadata::ofPath)\n+                  .flatMap(Optional::stream)\n+                  .max(Comparator.naturalOrder());\n+          if (previousSnapshot != null", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzYyNTQ4NQ=="}, "originalCommit": {"oid": "8b02113cf038b85108829ddc80379c7beea796d5"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzY4ODAxMA==", "bodyText": "Ok sorry I get it now. As you can see it was for me not easy to understand \ud83d\ude05  I always find this streaming api and compare a bit confusing \ud83d\ude05 But might be just me. If it is actually previousSnapshot.compareTo(latestSnapshot) then yes it makes sense otherwise it would not \ud83d\ude05", "url": "https://github.com/camunda-cloud/zeebe/pull/4063#discussion_r393688010", "createdAt": "2020-03-17T13:43:10Z", "author": {"login": "Zelldon"}, "path": "qa/integration-tests/src/test/java/io/zeebe/broker/it/clustering/ClusteringRule.java", "diffHunk": "@@ -606,16 +610,40 @@ public File getSnapshotsDirectory(final Broker broker) {\n     return new File(dataDir, RAFT_PARTITION_PATH + \"/snapshots\");\n   }\n \n-  public void waitForValidSnapshotAtBroker(final Broker broker) {\n-    waitForValidSnapshotAtBroker(broker, 1);\n+  public DbSnapshotMetadata waitForSnapshotAtBroker(final Broker broker) {\n+    return waitForNewSnapshotAtBroker(broker, null);\n   }\n \n-  void waitForValidSnapshotAtBroker(final Broker broker, final int snapshotCount) {\n+  DbSnapshotMetadata waitForNewSnapshotAtBroker(\n+      final Broker broker, final DbSnapshotMetadata previousSnapshot) {\n+    final var referenceToResult = new AtomicReference<>(Optional.<DbSnapshotMetadata>empty());\n     final File snapshotsDir = getSnapshotsDirectory(broker);\n     waitUntil(\n-        () ->\n-            Optional.ofNullable(snapshotsDir.listFiles()).map(f -> f.length).orElse(0)\n-                >= snapshotCount);\n+        () -> {\n+          final File[] files = snapshotsDir.listFiles();\n+          if (files == null || files.length == 0) {\n+            return false;\n+          }\n+          final Optional<DbSnapshotMetadata> latestSnapshot =\n+              Arrays.stream(files)\n+                  .map(File::toPath)\n+                  .map(DbSnapshotMetadata::ofPath)\n+                  .flatMap(Optional::stream)\n+                  .max(Comparator.naturalOrder());\n+          if (previousSnapshot != null", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzYyNTQ4NQ=="}, "originalCommit": {"oid": "8b02113cf038b85108829ddc80379c7beea796d5"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzcyMjA5NA==", "bodyText": "Yeah, streaming api can be confusing (just remember that it is a method reference, so in this case it will apply the provided method with the values in the Optional/Stream as argument).\nI have to add that I also wrote it a little confusing, but I wanted to maintain a return early approach. Perhaps I can have another look at it, to make it less confusing.", "url": "https://github.com/camunda-cloud/zeebe/pull/4063#discussion_r393722094", "createdAt": "2020-03-17T14:30:51Z", "author": {"login": "korthout"}, "path": "qa/integration-tests/src/test/java/io/zeebe/broker/it/clustering/ClusteringRule.java", "diffHunk": "@@ -606,16 +610,40 @@ public File getSnapshotsDirectory(final Broker broker) {\n     return new File(dataDir, RAFT_PARTITION_PATH + \"/snapshots\");\n   }\n \n-  public void waitForValidSnapshotAtBroker(final Broker broker) {\n-    waitForValidSnapshotAtBroker(broker, 1);\n+  public DbSnapshotMetadata waitForSnapshotAtBroker(final Broker broker) {\n+    return waitForNewSnapshotAtBroker(broker, null);\n   }\n \n-  void waitForValidSnapshotAtBroker(final Broker broker, final int snapshotCount) {\n+  DbSnapshotMetadata waitForNewSnapshotAtBroker(\n+      final Broker broker, final DbSnapshotMetadata previousSnapshot) {\n+    final var referenceToResult = new AtomicReference<>(Optional.<DbSnapshotMetadata>empty());\n     final File snapshotsDir = getSnapshotsDirectory(broker);\n     waitUntil(\n-        () ->\n-            Optional.ofNullable(snapshotsDir.listFiles()).map(f -> f.length).orElse(0)\n-                >= snapshotCount);\n+        () -> {\n+          final File[] files = snapshotsDir.listFiles();\n+          if (files == null || files.length == 0) {\n+            return false;\n+          }\n+          final Optional<DbSnapshotMetadata> latestSnapshot =\n+              Arrays.stream(files)\n+                  .map(File::toPath)\n+                  .map(DbSnapshotMetadata::ofPath)\n+                  .flatMap(Optional::stream)\n+                  .max(Comparator.naturalOrder());\n+          if (previousSnapshot != null", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzYyNTQ4NQ=="}, "originalCommit": {"oid": "8b02113cf038b85108829ddc80379c7beea796d5"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzc0NTU4Ng==", "bodyText": "@Zelldon Would this be more readable?\nfinal Optional<DbSnapshotMetadata> latestSnapshot = ...\nif (latestSnapshot.isEmpty()) {\n  return false;\n}\nif (previousSnapshot != null && previousSnapshot.compareTo(latestSnapshot.get()) > -1) {\n  return false;\n}\n// latestSnapshot is newer than previousSnapshot (or previous is null)\nreferenceToResult.set(latestSnapshot);\nreturn true;\n\nInstead of\nfinal Optional<DbSnapshotMetadata> latestSnapshot = ...\nif (previousSnapshot != null \n    && latestSnapshot.map(previousSnapshot::compareTo).orElse(0) > -1) {\n  return false;\n}\n// latestSnapshot is newer than previousSnapshot (or previous is null)\nreferenceToResult.set(latestSnapshot);\nreturn true;", "url": "https://github.com/camunda-cloud/zeebe/pull/4063#discussion_r393745586", "createdAt": "2020-03-17T15:02:48Z", "author": {"login": "korthout"}, "path": "qa/integration-tests/src/test/java/io/zeebe/broker/it/clustering/ClusteringRule.java", "diffHunk": "@@ -606,16 +610,40 @@ public File getSnapshotsDirectory(final Broker broker) {\n     return new File(dataDir, RAFT_PARTITION_PATH + \"/snapshots\");\n   }\n \n-  public void waitForValidSnapshotAtBroker(final Broker broker) {\n-    waitForValidSnapshotAtBroker(broker, 1);\n+  public DbSnapshotMetadata waitForSnapshotAtBroker(final Broker broker) {\n+    return waitForNewSnapshotAtBroker(broker, null);\n   }\n \n-  void waitForValidSnapshotAtBroker(final Broker broker, final int snapshotCount) {\n+  DbSnapshotMetadata waitForNewSnapshotAtBroker(\n+      final Broker broker, final DbSnapshotMetadata previousSnapshot) {\n+    final var referenceToResult = new AtomicReference<>(Optional.<DbSnapshotMetadata>empty());\n     final File snapshotsDir = getSnapshotsDirectory(broker);\n     waitUntil(\n-        () ->\n-            Optional.ofNullable(snapshotsDir.listFiles()).map(f -> f.length).orElse(0)\n-                >= snapshotCount);\n+        () -> {\n+          final File[] files = snapshotsDir.listFiles();\n+          if (files == null || files.length == 0) {\n+            return false;\n+          }\n+          final Optional<DbSnapshotMetadata> latestSnapshot =\n+              Arrays.stream(files)\n+                  .map(File::toPath)\n+                  .map(DbSnapshotMetadata::ofPath)\n+                  .flatMap(Optional::stream)\n+                  .max(Comparator.naturalOrder());\n+          if (previousSnapshot != null", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzYyNTQ4NQ=="}, "originalCommit": {"oid": "8b02113cf038b85108829ddc80379c7beea796d5"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzg5MjQyOA==", "bodyText": "Yes I think it is easier from my point of view", "url": "https://github.com/camunda-cloud/zeebe/pull/4063#discussion_r393892428", "createdAt": "2020-03-17T18:39:12Z", "author": {"login": "Zelldon"}, "path": "qa/integration-tests/src/test/java/io/zeebe/broker/it/clustering/ClusteringRule.java", "diffHunk": "@@ -606,16 +610,40 @@ public File getSnapshotsDirectory(final Broker broker) {\n     return new File(dataDir, RAFT_PARTITION_PATH + \"/snapshots\");\n   }\n \n-  public void waitForValidSnapshotAtBroker(final Broker broker) {\n-    waitForValidSnapshotAtBroker(broker, 1);\n+  public DbSnapshotMetadata waitForSnapshotAtBroker(final Broker broker) {\n+    return waitForNewSnapshotAtBroker(broker, null);\n   }\n \n-  void waitForValidSnapshotAtBroker(final Broker broker, final int snapshotCount) {\n+  DbSnapshotMetadata waitForNewSnapshotAtBroker(\n+      final Broker broker, final DbSnapshotMetadata previousSnapshot) {\n+    final var referenceToResult = new AtomicReference<>(Optional.<DbSnapshotMetadata>empty());\n     final File snapshotsDir = getSnapshotsDirectory(broker);\n     waitUntil(\n-        () ->\n-            Optional.ofNullable(snapshotsDir.listFiles()).map(f -> f.length).orElse(0)\n-                >= snapshotCount);\n+        () -> {\n+          final File[] files = snapshotsDir.listFiles();\n+          if (files == null || files.length == 0) {\n+            return false;\n+          }\n+          final Optional<DbSnapshotMetadata> latestSnapshot =\n+              Arrays.stream(files)\n+                  .map(File::toPath)\n+                  .map(DbSnapshotMetadata::ofPath)\n+                  .flatMap(Optional::stream)\n+                  .max(Comparator.naturalOrder());\n+          if (previousSnapshot != null", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzYyNTQ4NQ=="}, "originalCommit": {"oid": "8b02113cf038b85108829ddc80379c7beea796d5"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDIwMTA1MQ==", "bodyText": "Ok, changed it.", "url": "https://github.com/camunda-cloud/zeebe/pull/4063#discussion_r394201051", "createdAt": "2020-03-18T09:16:12Z", "author": {"login": "korthout"}, "path": "qa/integration-tests/src/test/java/io/zeebe/broker/it/clustering/ClusteringRule.java", "diffHunk": "@@ -606,16 +610,40 @@ public File getSnapshotsDirectory(final Broker broker) {\n     return new File(dataDir, RAFT_PARTITION_PATH + \"/snapshots\");\n   }\n \n-  public void waitForValidSnapshotAtBroker(final Broker broker) {\n-    waitForValidSnapshotAtBroker(broker, 1);\n+  public DbSnapshotMetadata waitForSnapshotAtBroker(final Broker broker) {\n+    return waitForNewSnapshotAtBroker(broker, null);\n   }\n \n-  void waitForValidSnapshotAtBroker(final Broker broker, final int snapshotCount) {\n+  DbSnapshotMetadata waitForNewSnapshotAtBroker(\n+      final Broker broker, final DbSnapshotMetadata previousSnapshot) {\n+    final var referenceToResult = new AtomicReference<>(Optional.<DbSnapshotMetadata>empty());\n     final File snapshotsDir = getSnapshotsDirectory(broker);\n     waitUntil(\n-        () ->\n-            Optional.ofNullable(snapshotsDir.listFiles()).map(f -> f.length).orElse(0)\n-                >= snapshotCount);\n+        () -> {\n+          final File[] files = snapshotsDir.listFiles();\n+          if (files == null || files.length == 0) {\n+            return false;\n+          }\n+          final Optional<DbSnapshotMetadata> latestSnapshot =\n+              Arrays.stream(files)\n+                  .map(File::toPath)\n+                  .map(DbSnapshotMetadata::ofPath)\n+                  .flatMap(Optional::stream)\n+                  .max(Comparator.naturalOrder());\n+          if (previousSnapshot != null", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzYyNTQ4NQ=="}, "originalCommit": {"oid": "8b02113cf038b85108829ddc80379c7beea796d5"}, "originalPosition": 56}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQzOTkzNTA5OnYy", "diffSide": "RIGHT", "path": "broker/src/main/java/io/zeebe/broker/clustering/atomix/storage/snapshot/AtomixSnapshotStorage.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xN1QxMTo1ODozOFrOF3ZEtw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xN1QxMjo1NToyOFrOF3a71A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzYyNjgwNw==", "bodyText": "Please create a separate issue for this todo and the best would be also to reference it in the todo", "url": "https://github.com/camunda-cloud/zeebe/pull/4063#discussion_r393626807", "createdAt": "2020-03-17T11:58:38Z", "author": {"login": "Zelldon"}, "path": "broker/src/main/java/io/zeebe/broker/clustering/atomix/storage/snapshot/AtomixSnapshotStorage.java", "diffHunk": "@@ -146,6 +143,8 @@ public void onNewSnapshot(\n     metrics.incrementSnapshotCount();\n     observeSnapshotSize(snapshot);\n \n+    // todo: simplify now that maxSnapshotCount can only be 1", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8b02113cf038b85108829ddc80379c7beea796d5"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzY1NzMwMA==", "bodyText": "Will do \ud83d\udc4d", "url": "https://github.com/camunda-cloud/zeebe/pull/4063#discussion_r393657300", "createdAt": "2020-03-17T12:55:28Z", "author": {"login": "korthout"}, "path": "broker/src/main/java/io/zeebe/broker/clustering/atomix/storage/snapshot/AtomixSnapshotStorage.java", "diffHunk": "@@ -146,6 +143,8 @@ public void onNewSnapshot(\n     metrics.incrementSnapshotCount();\n     observeSnapshotSize(snapshot);\n \n+    // todo: simplify now that maxSnapshotCount can only be 1", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzYyNjgwNw=="}, "originalCommit": {"oid": "8b02113cf038b85108829ddc80379c7beea796d5"}, "originalPosition": 25}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ0MTA0ODA5OnYy", "diffSide": "RIGHT", "path": "broker/src/main/java/io/zeebe/broker/clustering/atomix/storage/snapshot/AtomixSnapshotStorage.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xN1QxNjozMDo0OFrOF3kSng==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xN1QxODozOToyNVrOF3pSsQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzgxMDU5MA==", "bodyText": "Created #4067 for this", "url": "https://github.com/camunda-cloud/zeebe/pull/4063#discussion_r393810590", "createdAt": "2020-03-17T16:30:48Z", "author": {"login": "korthout"}, "path": "broker/src/main/java/io/zeebe/broker/clustering/atomix/storage/snapshot/AtomixSnapshotStorage.java", "diffHunk": "@@ -142,31 +139,17 @@ public SnapshotMetrics getMetrics() {\n   public void onNewSnapshot(\n       final io.atomix.protocols.raft.storage.snapshot.Snapshot snapshot,\n       final SnapshotStore store) {\n-    final var snapshots = store.getSnapshots();\n     metrics.incrementSnapshotCount();\n     observeSnapshotSize(snapshot);\n \n-    if (snapshots.size() >= maxSnapshotCount) {\n-      // by the condition it's guaranteed there be a snapshot after skipping maxSnapshotCount - 1\n-      @SuppressWarnings(\"squid:S3655\")\n-      final var oldest =\n-          snapshots.stream()\n-              .sorted(Comparator.reverseOrder())\n-              .skip(maxSnapshotCount - 1L)\n-              .findFirst()\n-              .get();\n-\n-      LOGGER.info(\n-          \"Max snapshot count reached ({}), purging snapshots older than {}\",\n-          snapshots.size(),\n-          oldest);\n-      store.purgeSnapshots(oldest);\n-\n-      final var optionalConverted = toSnapshot(oldest.getPath());\n-      if (optionalConverted.isPresent()) {\n-        final var converted = optionalConverted.get();\n-        deletionListeners.forEach(listener -> listener.onSnapshotsDeleted(converted));\n-      }\n+    LOGGER.info(\"Purging snapshots older than {}\", snapshot);\n+    store.purgeSnapshots(snapshot);\n+\n+    final var optionalConverted = toSnapshot(snapshot.getPath());\n+    if (optionalConverted.isPresent()) {\n+      final var converted = optionalConverted.get();\n+      // TODO #4067(@korthout): rename onSnapshotsDeleted, because it doesn't always delete", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b05a729df45ef847f9103ca36a10ddd73fde0f4c"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzg5MjUyOQ==", "bodyText": "nice thanks", "url": "https://github.com/camunda-cloud/zeebe/pull/4063#discussion_r393892529", "createdAt": "2020-03-17T18:39:25Z", "author": {"login": "Zelldon"}, "path": "broker/src/main/java/io/zeebe/broker/clustering/atomix/storage/snapshot/AtomixSnapshotStorage.java", "diffHunk": "@@ -142,31 +139,17 @@ public SnapshotMetrics getMetrics() {\n   public void onNewSnapshot(\n       final io.atomix.protocols.raft.storage.snapshot.Snapshot snapshot,\n       final SnapshotStore store) {\n-    final var snapshots = store.getSnapshots();\n     metrics.incrementSnapshotCount();\n     observeSnapshotSize(snapshot);\n \n-    if (snapshots.size() >= maxSnapshotCount) {\n-      // by the condition it's guaranteed there be a snapshot after skipping maxSnapshotCount - 1\n-      @SuppressWarnings(\"squid:S3655\")\n-      final var oldest =\n-          snapshots.stream()\n-              .sorted(Comparator.reverseOrder())\n-              .skip(maxSnapshotCount - 1L)\n-              .findFirst()\n-              .get();\n-\n-      LOGGER.info(\n-          \"Max snapshot count reached ({}), purging snapshots older than {}\",\n-          snapshots.size(),\n-          oldest);\n-      store.purgeSnapshots(oldest);\n-\n-      final var optionalConverted = toSnapshot(oldest.getPath());\n-      if (optionalConverted.isPresent()) {\n-        final var converted = optionalConverted.get();\n-        deletionListeners.forEach(listener -> listener.onSnapshotsDeleted(converted));\n-      }\n+    LOGGER.info(\"Purging snapshots older than {}\", snapshot);\n+    store.purgeSnapshots(snapshot);\n+\n+    final var optionalConverted = toSnapshot(snapshot.getPath());\n+    if (optionalConverted.isPresent()) {\n+      final var converted = optionalConverted.get();\n+      // TODO #4067(@korthout): rename onSnapshotsDeleted, because it doesn't always delete", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzgxMDU5MA=="}, "originalCommit": {"oid": "b05a729df45ef847f9103ca36a10ddd73fde0f4c"}, "originalPosition": 70}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ0MTUzNzgyOnYy", "diffSide": "RIGHT", "path": "broker/src/main/java/io/zeebe/broker/clustering/atomix/storage/snapshot/AtomixSnapshotStorage.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xN1QxODo0MDowNFrOF3pUKw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOFQwOToyMDozMVrOF38Rzw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzg5MjkwNw==", "bodyText": "Probably debug level is enough here", "url": "https://github.com/camunda-cloud/zeebe/pull/4063#discussion_r393892907", "createdAt": "2020-03-17T18:40:04Z", "author": {"login": "Zelldon"}, "path": "broker/src/main/java/io/zeebe/broker/clustering/atomix/storage/snapshot/AtomixSnapshotStorage.java", "diffHunk": "@@ -142,31 +139,17 @@ public SnapshotMetrics getMetrics() {\n   public void onNewSnapshot(\n       final io.atomix.protocols.raft.storage.snapshot.Snapshot snapshot,\n       final SnapshotStore store) {\n-    final var snapshots = store.getSnapshots();\n     metrics.incrementSnapshotCount();\n     observeSnapshotSize(snapshot);\n \n-    if (snapshots.size() >= maxSnapshotCount) {\n-      // by the condition it's guaranteed there be a snapshot after skipping maxSnapshotCount - 1\n-      @SuppressWarnings(\"squid:S3655\")\n-      final var oldest =\n-          snapshots.stream()\n-              .sorted(Comparator.reverseOrder())\n-              .skip(maxSnapshotCount - 1L)\n-              .findFirst()\n-              .get();\n-\n-      LOGGER.info(\n-          \"Max snapshot count reached ({}), purging snapshots older than {}\",\n-          snapshots.size(),\n-          oldest);\n-      store.purgeSnapshots(oldest);\n-\n-      final var optionalConverted = toSnapshot(oldest.getPath());\n-      if (optionalConverted.isPresent()) {\n-        final var converted = optionalConverted.get();\n-        deletionListeners.forEach(listener -> listener.onSnapshotsDeleted(converted));\n-      }\n+    LOGGER.info(\"Purging snapshots older than {}\", snapshot);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b05a729df45ef847f9103ca36a10ddd73fde0f4c"}, "originalPosition": 64}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzkwODc1MQ==", "bodyText": "Where would we normally draw the line between info and debug?", "url": "https://github.com/camunda-cloud/zeebe/pull/4063#discussion_r393908751", "createdAt": "2020-03-17T19:09:04Z", "author": {"login": "korthout"}, "path": "broker/src/main/java/io/zeebe/broker/clustering/atomix/storage/snapshot/AtomixSnapshotStorage.java", "diffHunk": "@@ -142,31 +139,17 @@ public SnapshotMetrics getMetrics() {\n   public void onNewSnapshot(\n       final io.atomix.protocols.raft.storage.snapshot.Snapshot snapshot,\n       final SnapshotStore store) {\n-    final var snapshots = store.getSnapshots();\n     metrics.incrementSnapshotCount();\n     observeSnapshotSize(snapshot);\n \n-    if (snapshots.size() >= maxSnapshotCount) {\n-      // by the condition it's guaranteed there be a snapshot after skipping maxSnapshotCount - 1\n-      @SuppressWarnings(\"squid:S3655\")\n-      final var oldest =\n-          snapshots.stream()\n-              .sorted(Comparator.reverseOrder())\n-              .skip(maxSnapshotCount - 1L)\n-              .findFirst()\n-              .get();\n-\n-      LOGGER.info(\n-          \"Max snapshot count reached ({}), purging snapshots older than {}\",\n-          snapshots.size(),\n-          oldest);\n-      store.purgeSnapshots(oldest);\n-\n-      final var optionalConverted = toSnapshot(oldest.getPath());\n-      if (optionalConverted.isPresent()) {\n-        final var converted = optionalConverted.get();\n-        deletionListeners.forEach(listener -> listener.onSnapshotsDeleted(converted));\n-      }\n+    LOGGER.info(\"Purging snapshots older than {}\", snapshot);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzg5MjkwNw=="}, "originalCommit": {"oid": "b05a729df45ef847f9103ca36a10ddd73fde0f4c"}, "originalPosition": 64}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzk0ODI5MA==", "bodyText": "I think it is often not easy to decide. But maybe it helps to think from the user point of view. Does he needs to know it? What is the benefit or what can he do with this knowledge? Maybe also if we keep it on info the more context would be interesting. which dirs have been deleted", "url": "https://github.com/camunda-cloud/zeebe/pull/4063#discussion_r393948290", "createdAt": "2020-03-17T20:25:41Z", "author": {"login": "Zelldon"}, "path": "broker/src/main/java/io/zeebe/broker/clustering/atomix/storage/snapshot/AtomixSnapshotStorage.java", "diffHunk": "@@ -142,31 +139,17 @@ public SnapshotMetrics getMetrics() {\n   public void onNewSnapshot(\n       final io.atomix.protocols.raft.storage.snapshot.Snapshot snapshot,\n       final SnapshotStore store) {\n-    final var snapshots = store.getSnapshots();\n     metrics.incrementSnapshotCount();\n     observeSnapshotSize(snapshot);\n \n-    if (snapshots.size() >= maxSnapshotCount) {\n-      // by the condition it's guaranteed there be a snapshot after skipping maxSnapshotCount - 1\n-      @SuppressWarnings(\"squid:S3655\")\n-      final var oldest =\n-          snapshots.stream()\n-              .sorted(Comparator.reverseOrder())\n-              .skip(maxSnapshotCount - 1L)\n-              .findFirst()\n-              .get();\n-\n-      LOGGER.info(\n-          \"Max snapshot count reached ({}), purging snapshots older than {}\",\n-          snapshots.size(),\n-          oldest);\n-      store.purgeSnapshots(oldest);\n-\n-      final var optionalConverted = toSnapshot(oldest.getPath());\n-      if (optionalConverted.isPresent()) {\n-        final var converted = optionalConverted.get();\n-        deletionListeners.forEach(listener -> listener.onSnapshotsDeleted(converted));\n-      }\n+    LOGGER.info(\"Purging snapshots older than {}\", snapshot);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzg5MjkwNw=="}, "originalCommit": {"oid": "b05a729df45ef847f9103ca36a10ddd73fde0f4c"}, "originalPosition": 64}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDIwMzU5OQ==", "bodyText": "Agreed. At this location it is not that interesting because purging does not necessarily mean that a snapshot was deleted. This is to be tackled in #4067. I've changed it to debug level now.", "url": "https://github.com/camunda-cloud/zeebe/pull/4063#discussion_r394203599", "createdAt": "2020-03-18T09:20:31Z", "author": {"login": "korthout"}, "path": "broker/src/main/java/io/zeebe/broker/clustering/atomix/storage/snapshot/AtomixSnapshotStorage.java", "diffHunk": "@@ -142,31 +139,17 @@ public SnapshotMetrics getMetrics() {\n   public void onNewSnapshot(\n       final io.atomix.protocols.raft.storage.snapshot.Snapshot snapshot,\n       final SnapshotStore store) {\n-    final var snapshots = store.getSnapshots();\n     metrics.incrementSnapshotCount();\n     observeSnapshotSize(snapshot);\n \n-    if (snapshots.size() >= maxSnapshotCount) {\n-      // by the condition it's guaranteed there be a snapshot after skipping maxSnapshotCount - 1\n-      @SuppressWarnings(\"squid:S3655\")\n-      final var oldest =\n-          snapshots.stream()\n-              .sorted(Comparator.reverseOrder())\n-              .skip(maxSnapshotCount - 1L)\n-              .findFirst()\n-              .get();\n-\n-      LOGGER.info(\n-          \"Max snapshot count reached ({}), purging snapshots older than {}\",\n-          snapshots.size(),\n-          oldest);\n-      store.purgeSnapshots(oldest);\n-\n-      final var optionalConverted = toSnapshot(oldest.getPath());\n-      if (optionalConverted.isPresent()) {\n-        final var converted = optionalConverted.get();\n-        deletionListeners.forEach(listener -> listener.onSnapshotsDeleted(converted));\n-      }\n+    LOGGER.info(\"Purging snapshots older than {}\", snapshot);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzg5MjkwNw=="}, "originalCommit": {"oid": "b05a729df45ef847f9103ca36a10ddd73fde0f4c"}, "originalPosition": 64}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ0MTU1MTAzOnYy", "diffSide": "RIGHT", "path": "docs/src/operations/resource-planning.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xN1QxODo0NDowMVrOF3pc9g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xN1QxODo0NDowMVrOF3pc9g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzg5NTE1OA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            When the broker has a (new) snapshot, it deletes all data on the log which was written before the snapshot.\n          \n          \n            \n            When the broker has a (new) snapshot, it tries to deletes all data on the log which was written before the snapshot.", "url": "https://github.com/camunda-cloud/zeebe/pull/4063#discussion_r393895158", "createdAt": "2020-03-17T18:44:01Z", "author": {"login": "Zelldon"}, "path": "docs/src/operations/resource-planning.md", "diffHunk": "@@ -9,58 +9,57 @@ While we cannot tell you exactly what you need - beyond _it depends_ - we can ex\n All Brokers in a partition use disk space to store:\n \n - The event log for each partition they participate in. By default, this is a minimum of 512MB for each partition, incrementing in 512MB segments. The event log is truncated on a given broker when data has been processed and successfully exported by all loaded exporters.\n-- One or more periodic snapshots of the running state (inflight data) of each partition (unbounded, based on in-flight work). The number of snapshots to retain, and their frequency is configurable. By default it is set to three snapshots retained, and a snapshot [every 15 minutes](https://github.com/zeebe-io/zeebe/blob/cca5aeda4bd9d7e7e83f68d221bbf1a3e4d0f000/dist/src/main/config/zeebe.cfg.toml#L145).\n+- One or more periodic snapshots of the running state (inflight data) of each partition (unbounded, based on in-flight work). The frequency of snapshots to retain is configurable. By default it is set to a snapshot [every 15 minutes](https://github.com/zeebe-io/zeebe/blob/cca5aeda4bd9d7e7e83f68d221bbf1a3e4d0f000/dist/src/main/config/zeebe.cfg.toml#L145).\n \n Additionally, the leader of a partition also uses disk space to store:\n - A projection of the running state of the partition in RocksDB. (unbounded, based on in-flight work)\n \n-By default this data is stored in \n+By default this data is stored in\n \n-- `segments` - the data of the log split into segments. The log is only appended - until it is truncated by reaching the max snapshot count.\n+- `segments` - the data of the log split into segments. The log is only appended - its data can be truncated when it becomes part of a new snapshot.\n - `state` - the active state. Deployed workflows, active workflow instances, etc. Completed workflow instances or jobs are removed.\n-- `snapshot` - the data of a state at a certain point in time\n+- `snapshot` - a state at a certain point in time\n \n If you want a recipe to explode your disk space usage, here are a few ways to do it:\n \n-- Create a high number of snapshots with a long period between them.\n - Load an exporter, such as the Debug Exporter, that does not advance its record position.\n \n ## Event Log\n \n-The event log for each partition is segmented. By default, the segment size is 512MB. This can be changed in the zeebe.cfg.toml file with the setting [logSegmentSize](https://github.com/zeebe-io/zeebe/blob/0.20.0/dist/src/main/config/zeebe.cfg.toml#L148). \n+The event log for each partition is segmented. By default, the segment size is 512MB. This can be changed in the zeebe.cfg.toml file with the setting [logSegmentSize](https://github.com/zeebe-io/zeebe/blob/0.20.0/dist/src/main/config/zeebe.cfg.toml#L148).\n \n An event log segment can be deleted once all the events it contains have been processed by exporters, replicated to other brokers, and processed. Three things can cause the event log to not be truncated:\n \n-- A cluster loses its quorum, in which case events are queued but not processed. \n+- A cluster loses its quorum, in which case events are queued but not processed.\n - An exporter does not advance its read position in the event log.\n-- The max number of snapshots has not been written.\n+- There is no snapshot of it yet.\n \n-An event log segment is not deleted until all the events in it have been exported by all configured exporters. This means that exporters that rely on side-effects, perform intensive computation, or experience back pressure from external storage will cause disk usage to grow, as they delay the deletion of event log segments. \n+An event log segment is not deleted until all the events in it have been exported by all configured exporters. This means that exporters that rely on side-effects, perform intensive computation, or experience back pressure from external storage will cause disk usage to grow, as they delay the deletion of event log segments.\n \n Exporting is only performed on the partition leader, but the followers of the partition do not delete segments in their replica of the partition until the leader marks all events in it as unneeded by exporters.\n \n-No event log segments are deleted until the maximum number of snapshots has been reached. When the maximum number of snapshots has been reached, the event log is truncated up to the oldest snapshot.\n+No event log segments are deleted until a snapshot has been taken that includes that log segment. When a snapshot has been taken, the event log is truncated up to that point.\n \n ## Snapshots\n \n-The running state of the partition is captured periodically on the leader in a snapshot. By default, this period is [every 15 minutes](https://github.com/zeebe-io/zeebe/blob/0.20.0/dist/src/main/config/zeebe.cfg.toml#L151). This can be changed in the zeebe.cfg.toml file. The number of valid snapshots to retain is configured in zeebe.cfg.toml. By default, the leader and followers retain only the latest valid snapshot.\n+The running state of the partition is captured periodically on the leader in a snapshot. By default, this period is [every 15 minutes](https://github.com/zeebe-io/zeebe/blob/0.20.0/dist/src/main/config/zeebe.cfg.toml#L151). This can be changed in the zeebe.cfg.toml file.\n \n A snapshot is a projection of all events that represent the current running state of the workflows running on the partition.  It contains all active data, for example, deployed workflows, active workflow instances, and not yet completed jobs.\n \n-When the broker has as many snapshots as configured by the parameter maxSnapshots, it deletes all data on the log which was written before the oldest snapshot.\n+When the broker has a (new) snapshot, it deletes all data on the log which was written before the snapshot.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b05a729df45ef847f9103ca36a10ddd73fde0f4c"}, "originalPosition": 53}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ0MTU1ODE4OnYy", "diffSide": "RIGHT", "path": "docs/src/operations/resource-planning.md", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xN1QxODo0NjowMVrOF3phng==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOFQxMjozMzo1MFrOF4C5cA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzg5NjM1MA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            - One or more periodic snapshots of the running state (inflight data) of each partition (unbounded, based on in-flight work). The frequency of snapshots to retain is configurable. By default it is set to a snapshot [every 15 minutes](https://github.com/zeebe-io/zeebe/blob/cca5aeda4bd9d7e7e83f68d221bbf1a3e4d0f000/dist/src/main/config/zeebe.cfg.toml#L145).\n          \n          \n            \n            - One snapshots of the running state (inflight data) of each partition (unbounded, based on in-flight work). The frequency of snapshots to retain is configurable. By default it is set to a snapshot [every 15 minutes](https://github.com/zeebe-io/zeebe/blob/cca5aeda4bd9d7e7e83f68d221bbf1a3e4d0f000/dist/src/main/config/zeebe.cfg.toml#L145).", "url": "https://github.com/camunda-cloud/zeebe/pull/4063#discussion_r393896350", "createdAt": "2020-03-17T18:46:01Z", "author": {"login": "Zelldon"}, "path": "docs/src/operations/resource-planning.md", "diffHunk": "@@ -9,58 +9,57 @@ While we cannot tell you exactly what you need - beyond _it depends_ - we can ex\n All Brokers in a partition use disk space to store:\n \n - The event log for each partition they participate in. By default, this is a minimum of 512MB for each partition, incrementing in 512MB segments. The event log is truncated on a given broker when data has been processed and successfully exported by all loaded exporters.\n-- One or more periodic snapshots of the running state (inflight data) of each partition (unbounded, based on in-flight work). The number of snapshots to retain, and their frequency is configurable. By default it is set to three snapshots retained, and a snapshot [every 15 minutes](https://github.com/zeebe-io/zeebe/blob/cca5aeda4bd9d7e7e83f68d221bbf1a3e4d0f000/dist/src/main/config/zeebe.cfg.toml#L145).\n+- One or more periodic snapshots of the running state (inflight data) of each partition (unbounded, based on in-flight work). The frequency of snapshots to retain is configurable. By default it is set to a snapshot [every 15 minutes](https://github.com/zeebe-io/zeebe/blob/cca5aeda4bd9d7e7e83f68d221bbf1a3e4d0f000/dist/src/main/config/zeebe.cfg.toml#L145).", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b05a729df45ef847f9103ca36a10ddd73fde0f4c"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDIwNTEwMg==", "bodyText": "@Zelldon what about this?\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            - One or more periodic snapshots of the running state (inflight data) of each partition (unbounded, based on in-flight work). The frequency of snapshots to retain is configurable. By default it is set to a snapshot [every 15 minutes](https://github.com/zeebe-io/zeebe/blob/cca5aeda4bd9d7e7e83f68d221bbf1a3e4d0f000/dist/src/main/config/zeebe.cfg.toml#L145).\n          \n          \n            \n            - A snapshot of the running state (inflight data) of each partition (unbounded, based on in-flight work). The frequency of snapshots to retain is configurable. By default it is set to a snapshot [every 15 minutes](https://github.com/zeebe-io/zeebe/blob/cca5aeda4bd9d7e7e83f68d221bbf1a3e4d0f000/dist/src/main/config/zeebe.cfg.toml#L145).", "url": "https://github.com/camunda-cloud/zeebe/pull/4063#discussion_r394205102", "createdAt": "2020-03-18T09:23:06Z", "author": {"login": "korthout"}, "path": "docs/src/operations/resource-planning.md", "diffHunk": "@@ -9,58 +9,57 @@ While we cannot tell you exactly what you need - beyond _it depends_ - we can ex\n All Brokers in a partition use disk space to store:\n \n - The event log for each partition they participate in. By default, this is a minimum of 512MB for each partition, incrementing in 512MB segments. The event log is truncated on a given broker when data has been processed and successfully exported by all loaded exporters.\n-- One or more periodic snapshots of the running state (inflight data) of each partition (unbounded, based on in-flight work). The number of snapshots to retain, and their frequency is configurable. By default it is set to three snapshots retained, and a snapshot [every 15 minutes](https://github.com/zeebe-io/zeebe/blob/cca5aeda4bd9d7e7e83f68d221bbf1a3e4d0f000/dist/src/main/config/zeebe.cfg.toml#L145).\n+- One or more periodic snapshots of the running state (inflight data) of each partition (unbounded, based on in-flight work). The frequency of snapshots to retain is configurable. By default it is set to a snapshot [every 15 minutes](https://github.com/zeebe-io/zeebe/blob/cca5aeda4bd9d7e7e83f68d221bbf1a3e4d0f000/dist/src/main/config/zeebe.cfg.toml#L145).", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzg5NjM1MA=="}, "originalCommit": {"oid": "b05a729df45ef847f9103ca36a10ddd73fde0f4c"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDMxMjA0OA==", "bodyText": "yes", "url": "https://github.com/camunda-cloud/zeebe/pull/4063#discussion_r394312048", "createdAt": "2020-03-18T12:33:50Z", "author": {"login": "Zelldon"}, "path": "docs/src/operations/resource-planning.md", "diffHunk": "@@ -9,58 +9,57 @@ While we cannot tell you exactly what you need - beyond _it depends_ - we can ex\n All Brokers in a partition use disk space to store:\n \n - The event log for each partition they participate in. By default, this is a minimum of 512MB for each partition, incrementing in 512MB segments. The event log is truncated on a given broker when data has been processed and successfully exported by all loaded exporters.\n-- One or more periodic snapshots of the running state (inflight data) of each partition (unbounded, based on in-flight work). The number of snapshots to retain, and their frequency is configurable. By default it is set to three snapshots retained, and a snapshot [every 15 minutes](https://github.com/zeebe-io/zeebe/blob/cca5aeda4bd9d7e7e83f68d221bbf1a3e4d0f000/dist/src/main/config/zeebe.cfg.toml#L145).\n+- One or more periodic snapshots of the running state (inflight data) of each partition (unbounded, based on in-flight work). The frequency of snapshots to retain is configurable. By default it is set to a snapshot [every 15 minutes](https://github.com/zeebe-io/zeebe/blob/cca5aeda4bd9d7e7e83f68d221bbf1a3e4d0f000/dist/src/main/config/zeebe.cfg.toml#L145).", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzg5NjM1MA=="}, "originalCommit": {"oid": "b05a729df45ef847f9103ca36a10ddd73fde0f4c"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ0MTU2MTkzOnYy", "diffSide": "RIGHT", "path": "docs/src/operations/resource-planning.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xN1QxODo0NzowNlrOF3pkHQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xN1QxODo0ODoxOVrOF3pmsA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzg5Njk4OQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            - `segments` - the data of the log split into segments. The log is only appended - its data can be truncated when it becomes part of a new snapshot.\n          \n          \n            \n            - `segments` - the data of the log split into segments. The log is only appended - its data can be deleted when it becomes part of a new snapshot.", "url": "https://github.com/camunda-cloud/zeebe/pull/4063#discussion_r393896989", "createdAt": "2020-03-17T18:47:06Z", "author": {"login": "Zelldon"}, "path": "docs/src/operations/resource-planning.md", "diffHunk": "@@ -9,58 +9,57 @@ While we cannot tell you exactly what you need - beyond _it depends_ - we can ex\n All Brokers in a partition use disk space to store:\n \n - The event log for each partition they participate in. By default, this is a minimum of 512MB for each partition, incrementing in 512MB segments. The event log is truncated on a given broker when data has been processed and successfully exported by all loaded exporters.\n-- One or more periodic snapshots of the running state (inflight data) of each partition (unbounded, based on in-flight work). The number of snapshots to retain, and their frequency is configurable. By default it is set to three snapshots retained, and a snapshot [every 15 minutes](https://github.com/zeebe-io/zeebe/blob/cca5aeda4bd9d7e7e83f68d221bbf1a3e4d0f000/dist/src/main/config/zeebe.cfg.toml#L145).\n+- One or more periodic snapshots of the running state (inflight data) of each partition (unbounded, based on in-flight work). The frequency of snapshots to retain is configurable. By default it is set to a snapshot [every 15 minutes](https://github.com/zeebe-io/zeebe/blob/cca5aeda4bd9d7e7e83f68d221bbf1a3e4d0f000/dist/src/main/config/zeebe.cfg.toml#L145).\n \n Additionally, the leader of a partition also uses disk space to store:\n - A projection of the running state of the partition in RocksDB. (unbounded, based on in-flight work)\n \n-By default this data is stored in \n+By default this data is stored in\n \n-- `segments` - the data of the log split into segments. The log is only appended - until it is truncated by reaching the max snapshot count.\n+- `segments` - the data of the log split into segments. The log is only appended - its data can be truncated when it becomes part of a new snapshot.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b05a729df45ef847f9103ca36a10ddd73fde0f4c"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzg5NzY0OA==", "bodyText": "Truncated is here the wrong wording, since we truncate when leaders step down. This means we delete uncommitted data at the end of the log. If we delete data then we delete from the beginning of the log.", "url": "https://github.com/camunda-cloud/zeebe/pull/4063#discussion_r393897648", "createdAt": "2020-03-17T18:48:19Z", "author": {"login": "Zelldon"}, "path": "docs/src/operations/resource-planning.md", "diffHunk": "@@ -9,58 +9,57 @@ While we cannot tell you exactly what you need - beyond _it depends_ - we can ex\n All Brokers in a partition use disk space to store:\n \n - The event log for each partition they participate in. By default, this is a minimum of 512MB for each partition, incrementing in 512MB segments. The event log is truncated on a given broker when data has been processed and successfully exported by all loaded exporters.\n-- One or more periodic snapshots of the running state (inflight data) of each partition (unbounded, based on in-flight work). The number of snapshots to retain, and their frequency is configurable. By default it is set to three snapshots retained, and a snapshot [every 15 minutes](https://github.com/zeebe-io/zeebe/blob/cca5aeda4bd9d7e7e83f68d221bbf1a3e4d0f000/dist/src/main/config/zeebe.cfg.toml#L145).\n+- One or more periodic snapshots of the running state (inflight data) of each partition (unbounded, based on in-flight work). The frequency of snapshots to retain is configurable. By default it is set to a snapshot [every 15 minutes](https://github.com/zeebe-io/zeebe/blob/cca5aeda4bd9d7e7e83f68d221bbf1a3e4d0f000/dist/src/main/config/zeebe.cfg.toml#L145).\n \n Additionally, the leader of a partition also uses disk space to store:\n - A projection of the running state of the partition in RocksDB. (unbounded, based on in-flight work)\n \n-By default this data is stored in \n+By default this data is stored in\n \n-- `segments` - the data of the log split into segments. The log is only appended - until it is truncated by reaching the max snapshot count.\n+- `segments` - the data of the log split into segments. The log is only appended - its data can be truncated when it becomes part of a new snapshot.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzg5Njk4OQ=="}, "originalCommit": {"oid": "b05a729df45ef847f9103ca36a10ddd73fde0f4c"}, "originalPosition": 14}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ0MTU2OTgxOnYy", "diffSide": "RIGHT", "path": "docs/src/operations/resource-planning.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xN1QxODo0OTozNlrOF3ppgw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xN1QxODo0OTozNlrOF3ppgw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzg5ODM3MQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            An event log segment can be deleted once all the events it contains have been processed by exporters, replicated to other brokers, and processed. Three things can cause the event log to not be truncated:\n          \n          \n            \n            An event log segment can be deleted once all the events it contains have been processed by exporters, replicated to other brokers, and processed. Three things can cause the event log to not be deleted:", "url": "https://github.com/camunda-cloud/zeebe/pull/4063#discussion_r393898371", "createdAt": "2020-03-17T18:49:36Z", "author": {"login": "Zelldon"}, "path": "docs/src/operations/resource-planning.md", "diffHunk": "@@ -9,58 +9,57 @@ While we cannot tell you exactly what you need - beyond _it depends_ - we can ex\n All Brokers in a partition use disk space to store:\n \n - The event log for each partition they participate in. By default, this is a minimum of 512MB for each partition, incrementing in 512MB segments. The event log is truncated on a given broker when data has been processed and successfully exported by all loaded exporters.\n-- One or more periodic snapshots of the running state (inflight data) of each partition (unbounded, based on in-flight work). The number of snapshots to retain, and their frequency is configurable. By default it is set to three snapshots retained, and a snapshot [every 15 minutes](https://github.com/zeebe-io/zeebe/blob/cca5aeda4bd9d7e7e83f68d221bbf1a3e4d0f000/dist/src/main/config/zeebe.cfg.toml#L145).\n+- One or more periodic snapshots of the running state (inflight data) of each partition (unbounded, based on in-flight work). The frequency of snapshots to retain is configurable. By default it is set to a snapshot [every 15 minutes](https://github.com/zeebe-io/zeebe/blob/cca5aeda4bd9d7e7e83f68d221bbf1a3e4d0f000/dist/src/main/config/zeebe.cfg.toml#L145).\n \n Additionally, the leader of a partition also uses disk space to store:\n - A projection of the running state of the partition in RocksDB. (unbounded, based on in-flight work)\n \n-By default this data is stored in \n+By default this data is stored in\n \n-- `segments` - the data of the log split into segments. The log is only appended - until it is truncated by reaching the max snapshot count.\n+- `segments` - the data of the log split into segments. The log is only appended - its data can be truncated when it becomes part of a new snapshot.\n - `state` - the active state. Deployed workflows, active workflow instances, etc. Completed workflow instances or jobs are removed.\n-- `snapshot` - the data of a state at a certain point in time\n+- `snapshot` - a state at a certain point in time\n \n If you want a recipe to explode your disk space usage, here are a few ways to do it:\n \n-- Create a high number of snapshots with a long period between them.\n - Load an exporter, such as the Debug Exporter, that does not advance its record position.\n \n ## Event Log\n \n-The event log for each partition is segmented. By default, the segment size is 512MB. This can be changed in the zeebe.cfg.toml file with the setting [logSegmentSize](https://github.com/zeebe-io/zeebe/blob/0.20.0/dist/src/main/config/zeebe.cfg.toml#L148). \n+The event log for each partition is segmented. By default, the segment size is 512MB. This can be changed in the zeebe.cfg.toml file with the setting [logSegmentSize](https://github.com/zeebe-io/zeebe/blob/0.20.0/dist/src/main/config/zeebe.cfg.toml#L148).\n \n An event log segment can be deleted once all the events it contains have been processed by exporters, replicated to other brokers, and processed. Three things can cause the event log to not be truncated:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b05a729df45ef847f9103ca36a10ddd73fde0f4c"}, "originalPosition": 29}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ0MTg0NjE4OnYy", "diffSide": "RIGHT", "path": "docs/src/operations/resource-planning.md", "isResolved": true, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xN1QyMDoxODowOFrOF3sdVg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOFQxMjoyODowM1rOF4CtAA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzk0NDQwNg==", "bodyText": "No event log segments are deleted until a snapshot has been taken that includes that log segment\n\nsounds a bit weird", "url": "https://github.com/camunda-cloud/zeebe/pull/4063#discussion_r393944406", "createdAt": "2020-03-17T20:18:08Z", "author": {"login": "Zelldon"}, "path": "docs/src/operations/resource-planning.md", "diffHunk": "@@ -9,58 +9,57 @@ While we cannot tell you exactly what you need - beyond _it depends_ - we can ex\n All Brokers in a partition use disk space to store:\n \n - The event log for each partition they participate in. By default, this is a minimum of 512MB for each partition, incrementing in 512MB segments. The event log is truncated on a given broker when data has been processed and successfully exported by all loaded exporters.\n-- One or more periodic snapshots of the running state (inflight data) of each partition (unbounded, based on in-flight work). The number of snapshots to retain, and their frequency is configurable. By default it is set to three snapshots retained, and a snapshot [every 15 minutes](https://github.com/zeebe-io/zeebe/blob/cca5aeda4bd9d7e7e83f68d221bbf1a3e4d0f000/dist/src/main/config/zeebe.cfg.toml#L145).\n+- One or more periodic snapshots of the running state (inflight data) of each partition (unbounded, based on in-flight work). The frequency of snapshots to retain is configurable. By default it is set to a snapshot [every 15 minutes](https://github.com/zeebe-io/zeebe/blob/cca5aeda4bd9d7e7e83f68d221bbf1a3e4d0f000/dist/src/main/config/zeebe.cfg.toml#L145).\n \n Additionally, the leader of a partition also uses disk space to store:\n - A projection of the running state of the partition in RocksDB. (unbounded, based on in-flight work)\n \n-By default this data is stored in \n+By default this data is stored in\n \n-- `segments` - the data of the log split into segments. The log is only appended - until it is truncated by reaching the max snapshot count.\n+- `segments` - the data of the log split into segments. The log is only appended - its data can be truncated when it becomes part of a new snapshot.\n - `state` - the active state. Deployed workflows, active workflow instances, etc. Completed workflow instances or jobs are removed.\n-- `snapshot` - the data of a state at a certain point in time\n+- `snapshot` - a state at a certain point in time\n \n If you want a recipe to explode your disk space usage, here are a few ways to do it:\n \n-- Create a high number of snapshots with a long period between them.\n - Load an exporter, such as the Debug Exporter, that does not advance its record position.\n \n ## Event Log\n \n-The event log for each partition is segmented. By default, the segment size is 512MB. This can be changed in the zeebe.cfg.toml file with the setting [logSegmentSize](https://github.com/zeebe-io/zeebe/blob/0.20.0/dist/src/main/config/zeebe.cfg.toml#L148). \n+The event log for each partition is segmented. By default, the segment size is 512MB. This can be changed in the zeebe.cfg.toml file with the setting [logSegmentSize](https://github.com/zeebe-io/zeebe/blob/0.20.0/dist/src/main/config/zeebe.cfg.toml#L148).\n \n An event log segment can be deleted once all the events it contains have been processed by exporters, replicated to other brokers, and processed. Three things can cause the event log to not be truncated:\n \n-- A cluster loses its quorum, in which case events are queued but not processed. \n+- A cluster loses its quorum, in which case events are queued but not processed.\n - An exporter does not advance its read position in the event log.\n-- The max number of snapshots has not been written.\n+- There is no snapshot of it yet.\n \n-An event log segment is not deleted until all the events in it have been exported by all configured exporters. This means that exporters that rely on side-effects, perform intensive computation, or experience back pressure from external storage will cause disk usage to grow, as they delay the deletion of event log segments. \n+An event log segment is not deleted until all the events in it have been exported by all configured exporters. This means that exporters that rely on side-effects, perform intensive computation, or experience back pressure from external storage will cause disk usage to grow, as they delay the deletion of event log segments.\n \n Exporting is only performed on the partition leader, but the followers of the partition do not delete segments in their replica of the partition until the leader marks all events in it as unneeded by exporters.\n \n-No event log segments are deleted until the maximum number of snapshots has been reached. When the maximum number of snapshots has been reached, the event log is truncated up to the oldest snapshot.\n+No event log segments are deleted until a snapshot has been taken that includes that log segment. When a snapshot has been taken, the event log is truncated up to that point.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b05a729df45ef847f9103ca36a10ddd73fde0f4c"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDIxMjQ5NA==", "bodyText": "The original line reads to me as 'don't worry about your data being deleted too early'.  I've tried to capture that again. Perhaps it should just start like:\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            No event log segments are deleted until a snapshot has been taken that includes that log segment. When a snapshot has been taken, the event log is truncated up to that point.\n          \n          \n            \n            We make sure that event log segments are not deleted too early. No event log segments are deleted until a snapshot has been taken that includes that segment. When a snapshot has been taken, the event log is only deleted up to that point.", "url": "https://github.com/camunda-cloud/zeebe/pull/4063#discussion_r394212494", "createdAt": "2020-03-18T09:35:06Z", "author": {"login": "korthout"}, "path": "docs/src/operations/resource-planning.md", "diffHunk": "@@ -9,58 +9,57 @@ While we cannot tell you exactly what you need - beyond _it depends_ - we can ex\n All Brokers in a partition use disk space to store:\n \n - The event log for each partition they participate in. By default, this is a minimum of 512MB for each partition, incrementing in 512MB segments. The event log is truncated on a given broker when data has been processed and successfully exported by all loaded exporters.\n-- One or more periodic snapshots of the running state (inflight data) of each partition (unbounded, based on in-flight work). The number of snapshots to retain, and their frequency is configurable. By default it is set to three snapshots retained, and a snapshot [every 15 minutes](https://github.com/zeebe-io/zeebe/blob/cca5aeda4bd9d7e7e83f68d221bbf1a3e4d0f000/dist/src/main/config/zeebe.cfg.toml#L145).\n+- One or more periodic snapshots of the running state (inflight data) of each partition (unbounded, based on in-flight work). The frequency of snapshots to retain is configurable. By default it is set to a snapshot [every 15 minutes](https://github.com/zeebe-io/zeebe/blob/cca5aeda4bd9d7e7e83f68d221bbf1a3e4d0f000/dist/src/main/config/zeebe.cfg.toml#L145).\n \n Additionally, the leader of a partition also uses disk space to store:\n - A projection of the running state of the partition in RocksDB. (unbounded, based on in-flight work)\n \n-By default this data is stored in \n+By default this data is stored in\n \n-- `segments` - the data of the log split into segments. The log is only appended - until it is truncated by reaching the max snapshot count.\n+- `segments` - the data of the log split into segments. The log is only appended - its data can be truncated when it becomes part of a new snapshot.\n - `state` - the active state. Deployed workflows, active workflow instances, etc. Completed workflow instances or jobs are removed.\n-- `snapshot` - the data of a state at a certain point in time\n+- `snapshot` - a state at a certain point in time\n \n If you want a recipe to explode your disk space usage, here are a few ways to do it:\n \n-- Create a high number of snapshots with a long period between them.\n - Load an exporter, such as the Debug Exporter, that does not advance its record position.\n \n ## Event Log\n \n-The event log for each partition is segmented. By default, the segment size is 512MB. This can be changed in the zeebe.cfg.toml file with the setting [logSegmentSize](https://github.com/zeebe-io/zeebe/blob/0.20.0/dist/src/main/config/zeebe.cfg.toml#L148). \n+The event log for each partition is segmented. By default, the segment size is 512MB. This can be changed in the zeebe.cfg.toml file with the setting [logSegmentSize](https://github.com/zeebe-io/zeebe/blob/0.20.0/dist/src/main/config/zeebe.cfg.toml#L148).\n \n An event log segment can be deleted once all the events it contains have been processed by exporters, replicated to other brokers, and processed. Three things can cause the event log to not be truncated:\n \n-- A cluster loses its quorum, in which case events are queued but not processed. \n+- A cluster loses its quorum, in which case events are queued but not processed.\n - An exporter does not advance its read position in the event log.\n-- The max number of snapshots has not been written.\n+- There is no snapshot of it yet.\n \n-An event log segment is not deleted until all the events in it have been exported by all configured exporters. This means that exporters that rely on side-effects, perform intensive computation, or experience back pressure from external storage will cause disk usage to grow, as they delay the deletion of event log segments. \n+An event log segment is not deleted until all the events in it have been exported by all configured exporters. This means that exporters that rely on side-effects, perform intensive computation, or experience back pressure from external storage will cause disk usage to grow, as they delay the deletion of event log segments.\n \n Exporting is only performed on the partition leader, but the followers of the partition do not delete segments in their replica of the partition until the leader marks all events in it as unneeded by exporters.\n \n-No event log segments are deleted until the maximum number of snapshots has been reached. When the maximum number of snapshots has been reached, the event log is truncated up to the oldest snapshot.\n+No event log segments are deleted until a snapshot has been taken that includes that log segment. When a snapshot has been taken, the event log is truncated up to that point.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzk0NDQwNg=="}, "originalCommit": {"oid": "b05a729df45ef847f9103ca36a10ddd73fde0f4c"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDMwNjA1NA==", "bodyText": "No event log segments are deleted until a snapshot has been taken that includes that segment\n\nMaybe:\nNo event log segment is deleted until a snapshot has been taken that includes that segment.", "url": "https://github.com/camunda-cloud/zeebe/pull/4063#discussion_r394306054", "createdAt": "2020-03-18T12:23:03Z", "author": {"login": "Zelldon"}, "path": "docs/src/operations/resource-planning.md", "diffHunk": "@@ -9,58 +9,57 @@ While we cannot tell you exactly what you need - beyond _it depends_ - we can ex\n All Brokers in a partition use disk space to store:\n \n - The event log for each partition they participate in. By default, this is a minimum of 512MB for each partition, incrementing in 512MB segments. The event log is truncated on a given broker when data has been processed and successfully exported by all loaded exporters.\n-- One or more periodic snapshots of the running state (inflight data) of each partition (unbounded, based on in-flight work). The number of snapshots to retain, and their frequency is configurable. By default it is set to three snapshots retained, and a snapshot [every 15 minutes](https://github.com/zeebe-io/zeebe/blob/cca5aeda4bd9d7e7e83f68d221bbf1a3e4d0f000/dist/src/main/config/zeebe.cfg.toml#L145).\n+- One or more periodic snapshots of the running state (inflight data) of each partition (unbounded, based on in-flight work). The frequency of snapshots to retain is configurable. By default it is set to a snapshot [every 15 minutes](https://github.com/zeebe-io/zeebe/blob/cca5aeda4bd9d7e7e83f68d221bbf1a3e4d0f000/dist/src/main/config/zeebe.cfg.toml#L145).\n \n Additionally, the leader of a partition also uses disk space to store:\n - A projection of the running state of the partition in RocksDB. (unbounded, based on in-flight work)\n \n-By default this data is stored in \n+By default this data is stored in\n \n-- `segments` - the data of the log split into segments. The log is only appended - until it is truncated by reaching the max snapshot count.\n+- `segments` - the data of the log split into segments. The log is only appended - its data can be truncated when it becomes part of a new snapshot.\n - `state` - the active state. Deployed workflows, active workflow instances, etc. Completed workflow instances or jobs are removed.\n-- `snapshot` - the data of a state at a certain point in time\n+- `snapshot` - a state at a certain point in time\n \n If you want a recipe to explode your disk space usage, here are a few ways to do it:\n \n-- Create a high number of snapshots with a long period between them.\n - Load an exporter, such as the Debug Exporter, that does not advance its record position.\n \n ## Event Log\n \n-The event log for each partition is segmented. By default, the segment size is 512MB. This can be changed in the zeebe.cfg.toml file with the setting [logSegmentSize](https://github.com/zeebe-io/zeebe/blob/0.20.0/dist/src/main/config/zeebe.cfg.toml#L148). \n+The event log for each partition is segmented. By default, the segment size is 512MB. This can be changed in the zeebe.cfg.toml file with the setting [logSegmentSize](https://github.com/zeebe-io/zeebe/blob/0.20.0/dist/src/main/config/zeebe.cfg.toml#L148).\n \n An event log segment can be deleted once all the events it contains have been processed by exporters, replicated to other brokers, and processed. Three things can cause the event log to not be truncated:\n \n-- A cluster loses its quorum, in which case events are queued but not processed. \n+- A cluster loses its quorum, in which case events are queued but not processed.\n - An exporter does not advance its read position in the event log.\n-- The max number of snapshots has not been written.\n+- There is no snapshot of it yet.\n \n-An event log segment is not deleted until all the events in it have been exported by all configured exporters. This means that exporters that rely on side-effects, perform intensive computation, or experience back pressure from external storage will cause disk usage to grow, as they delay the deletion of event log segments. \n+An event log segment is not deleted until all the events in it have been exported by all configured exporters. This means that exporters that rely on side-effects, perform intensive computation, or experience back pressure from external storage will cause disk usage to grow, as they delay the deletion of event log segments.\n \n Exporting is only performed on the partition leader, but the followers of the partition do not delete segments in their replica of the partition until the leader marks all events in it as unneeded by exporters.\n \n-No event log segments are deleted until the maximum number of snapshots has been reached. When the maximum number of snapshots has been reached, the event log is truncated up to the oldest snapshot.\n+No event log segments are deleted until a snapshot has been taken that includes that log segment. When a snapshot has been taken, the event log is truncated up to that point.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzk0NDQwNg=="}, "originalCommit": {"oid": "b05a729df45ef847f9103ca36a10ddd73fde0f4c"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDMwNzI4Mw==", "bodyText": "That's better. So like:\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            No event log segments are deleted until a snapshot has been taken that includes that log segment. When a snapshot has been taken, the event log is truncated up to that point.\n          \n          \n            \n            We make sure that event log segments are not deleted too early. No event log segment is deleted until a snapshot has been taken that includes that segment. When a snapshot has been taken, the event log is only deleted up to that point.", "url": "https://github.com/camunda-cloud/zeebe/pull/4063#discussion_r394307283", "createdAt": "2020-03-18T12:25:14Z", "author": {"login": "korthout"}, "path": "docs/src/operations/resource-planning.md", "diffHunk": "@@ -9,58 +9,57 @@ While we cannot tell you exactly what you need - beyond _it depends_ - we can ex\n All Brokers in a partition use disk space to store:\n \n - The event log for each partition they participate in. By default, this is a minimum of 512MB for each partition, incrementing in 512MB segments. The event log is truncated on a given broker when data has been processed and successfully exported by all loaded exporters.\n-- One or more periodic snapshots of the running state (inflight data) of each partition (unbounded, based on in-flight work). The number of snapshots to retain, and their frequency is configurable. By default it is set to three snapshots retained, and a snapshot [every 15 minutes](https://github.com/zeebe-io/zeebe/blob/cca5aeda4bd9d7e7e83f68d221bbf1a3e4d0f000/dist/src/main/config/zeebe.cfg.toml#L145).\n+- One or more periodic snapshots of the running state (inflight data) of each partition (unbounded, based on in-flight work). The frequency of snapshots to retain is configurable. By default it is set to a snapshot [every 15 minutes](https://github.com/zeebe-io/zeebe/blob/cca5aeda4bd9d7e7e83f68d221bbf1a3e4d0f000/dist/src/main/config/zeebe.cfg.toml#L145).\n \n Additionally, the leader of a partition also uses disk space to store:\n - A projection of the running state of the partition in RocksDB. (unbounded, based on in-flight work)\n \n-By default this data is stored in \n+By default this data is stored in\n \n-- `segments` - the data of the log split into segments. The log is only appended - until it is truncated by reaching the max snapshot count.\n+- `segments` - the data of the log split into segments. The log is only appended - its data can be truncated when it becomes part of a new snapshot.\n - `state` - the active state. Deployed workflows, active workflow instances, etc. Completed workflow instances or jobs are removed.\n-- `snapshot` - the data of a state at a certain point in time\n+- `snapshot` - a state at a certain point in time\n \n If you want a recipe to explode your disk space usage, here are a few ways to do it:\n \n-- Create a high number of snapshots with a long period between them.\n - Load an exporter, such as the Debug Exporter, that does not advance its record position.\n \n ## Event Log\n \n-The event log for each partition is segmented. By default, the segment size is 512MB. This can be changed in the zeebe.cfg.toml file with the setting [logSegmentSize](https://github.com/zeebe-io/zeebe/blob/0.20.0/dist/src/main/config/zeebe.cfg.toml#L148). \n+The event log for each partition is segmented. By default, the segment size is 512MB. This can be changed in the zeebe.cfg.toml file with the setting [logSegmentSize](https://github.com/zeebe-io/zeebe/blob/0.20.0/dist/src/main/config/zeebe.cfg.toml#L148).\n \n An event log segment can be deleted once all the events it contains have been processed by exporters, replicated to other brokers, and processed. Three things can cause the event log to not be truncated:\n \n-- A cluster loses its quorum, in which case events are queued but not processed. \n+- A cluster loses its quorum, in which case events are queued but not processed.\n - An exporter does not advance its read position in the event log.\n-- The max number of snapshots has not been written.\n+- There is no snapshot of it yet.\n \n-An event log segment is not deleted until all the events in it have been exported by all configured exporters. This means that exporters that rely on side-effects, perform intensive computation, or experience back pressure from external storage will cause disk usage to grow, as they delay the deletion of event log segments. \n+An event log segment is not deleted until all the events in it have been exported by all configured exporters. This means that exporters that rely on side-effects, perform intensive computation, or experience back pressure from external storage will cause disk usage to grow, as they delay the deletion of event log segments.\n \n Exporting is only performed on the partition leader, but the followers of the partition do not delete segments in their replica of the partition until the leader marks all events in it as unneeded by exporters.\n \n-No event log segments are deleted until the maximum number of snapshots has been reached. When the maximum number of snapshots has been reached, the event log is truncated up to the oldest snapshot.\n+No event log segments are deleted until a snapshot has been taken that includes that log segment. When a snapshot has been taken, the event log is truncated up to that point.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzk0NDQwNg=="}, "originalCommit": {"oid": "b05a729df45ef847f9103ca36a10ddd73fde0f4c"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDMwODg2NA==", "bodyText": "I think this is better thanks", "url": "https://github.com/camunda-cloud/zeebe/pull/4063#discussion_r394308864", "createdAt": "2020-03-18T12:28:03Z", "author": {"login": "Zelldon"}, "path": "docs/src/operations/resource-planning.md", "diffHunk": "@@ -9,58 +9,57 @@ While we cannot tell you exactly what you need - beyond _it depends_ - we can ex\n All Brokers in a partition use disk space to store:\n \n - The event log for each partition they participate in. By default, this is a minimum of 512MB for each partition, incrementing in 512MB segments. The event log is truncated on a given broker when data has been processed and successfully exported by all loaded exporters.\n-- One or more periodic snapshots of the running state (inflight data) of each partition (unbounded, based on in-flight work). The number of snapshots to retain, and their frequency is configurable. By default it is set to three snapshots retained, and a snapshot [every 15 minutes](https://github.com/zeebe-io/zeebe/blob/cca5aeda4bd9d7e7e83f68d221bbf1a3e4d0f000/dist/src/main/config/zeebe.cfg.toml#L145).\n+- One or more periodic snapshots of the running state (inflight data) of each partition (unbounded, based on in-flight work). The frequency of snapshots to retain is configurable. By default it is set to a snapshot [every 15 minutes](https://github.com/zeebe-io/zeebe/blob/cca5aeda4bd9d7e7e83f68d221bbf1a3e4d0f000/dist/src/main/config/zeebe.cfg.toml#L145).\n \n Additionally, the leader of a partition also uses disk space to store:\n - A projection of the running state of the partition in RocksDB. (unbounded, based on in-flight work)\n \n-By default this data is stored in \n+By default this data is stored in\n \n-- `segments` - the data of the log split into segments. The log is only appended - until it is truncated by reaching the max snapshot count.\n+- `segments` - the data of the log split into segments. The log is only appended - its data can be truncated when it becomes part of a new snapshot.\n - `state` - the active state. Deployed workflows, active workflow instances, etc. Completed workflow instances or jobs are removed.\n-- `snapshot` - the data of a state at a certain point in time\n+- `snapshot` - a state at a certain point in time\n \n If you want a recipe to explode your disk space usage, here are a few ways to do it:\n \n-- Create a high number of snapshots with a long period between them.\n - Load an exporter, such as the Debug Exporter, that does not advance its record position.\n \n ## Event Log\n \n-The event log for each partition is segmented. By default, the segment size is 512MB. This can be changed in the zeebe.cfg.toml file with the setting [logSegmentSize](https://github.com/zeebe-io/zeebe/blob/0.20.0/dist/src/main/config/zeebe.cfg.toml#L148). \n+The event log for each partition is segmented. By default, the segment size is 512MB. This can be changed in the zeebe.cfg.toml file with the setting [logSegmentSize](https://github.com/zeebe-io/zeebe/blob/0.20.0/dist/src/main/config/zeebe.cfg.toml#L148).\n \n An event log segment can be deleted once all the events it contains have been processed by exporters, replicated to other brokers, and processed. Three things can cause the event log to not be truncated:\n \n-- A cluster loses its quorum, in which case events are queued but not processed. \n+- A cluster loses its quorum, in which case events are queued but not processed.\n - An exporter does not advance its read position in the event log.\n-- The max number of snapshots has not been written.\n+- There is no snapshot of it yet.\n \n-An event log segment is not deleted until all the events in it have been exported by all configured exporters. This means that exporters that rely on side-effects, perform intensive computation, or experience back pressure from external storage will cause disk usage to grow, as they delay the deletion of event log segments. \n+An event log segment is not deleted until all the events in it have been exported by all configured exporters. This means that exporters that rely on side-effects, perform intensive computation, or experience back pressure from external storage will cause disk usage to grow, as they delay the deletion of event log segments.\n \n Exporting is only performed on the partition leader, but the followers of the partition do not delete segments in their replica of the partition until the leader marks all events in it as unneeded by exporters.\n \n-No event log segments are deleted until the maximum number of snapshots has been reached. When the maximum number of snapshots has been reached, the event log is truncated up to the oldest snapshot.\n+No event log segments are deleted until a snapshot has been taken that includes that log segment. When a snapshot has been taken, the event log is truncated up to that point.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzk0NDQwNg=="}, "originalCommit": {"oid": "b05a729df45ef847f9103ca36a10ddd73fde0f4c"}, "originalPosition": 43}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4918, "cost": 1, "resetAt": "2021-11-12T18:49:56Z"}}}