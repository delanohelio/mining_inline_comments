{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDM0NTA2Njg3", "number": 4730, "reviewThreads": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yMlQwOToxMDoyN1rOEHfVwg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yMlQwOTozMzowOVrOEHf1Rg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2Mjg4OTYyOnYy", "diffSide": "RIGHT", "path": "qa/integration-tests/src/test/java/io/zeebe/broker/it/clustering/SingleBrokerDataDeletionTest.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yMlQwOToxMDoyN1rOGm4Qww==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QwNjo0ODozMlrOGnbeJw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzQyMDg2Nw==", "bodyText": "Before this we should also verify that the compaction is completed, right? waitUntil(getSegmentsCount() < segmentsBeforeSnapshot . Otherwise we may not be verifying the state after compaction.", "url": "https://github.com/camunda-cloud/zeebe/pull/4730#discussion_r443420867", "createdAt": "2020-06-22T09:10:27Z", "author": {"login": "deepthidevaki"}, "path": "qa/integration-tests/src/test/java/io/zeebe/broker/it/clustering/SingleBrokerDataDeletionTest.java", "diffHunk": "@@ -58,46 +65,62 @@ public void shouldNotCompactNotExportedEvents() {\n     final var logstream = clusteringRule.getLogStream(1);\n     final var reader = logstream.newLogStreamReader().join();\n \n-    while (getSegmentsCount(broker) <= 2) {\n-      writeToLog();\n-    }\n+    // - write records and update the exporter position\n+    ControllableExporter.updatePosition(true);\n+    fillSegments(broker, SEGMENT_COUNT);\n \n-    // when\n+    // - write more records but don't update the exporter position\n     ControllableExporter.updatePosition(false);\n+    fillSegments(broker, SEGMENT_COUNT * 2);\n \n-    // write more events\n-    while (getSegmentsCount(broker) <= 3) {\n-      writeToLog();\n-    }\n-    // write one more to make sure last processed position in segment 3\n-    writeToLog();\n-\n-    // increase snapshot interval and wait\n+    // - trigger a snapshot creation\n     clusteringRule.getClock().addTime(SNAPSHOT_PERIOD);\n     final var firstSnapshot = clusteringRule.waitForSnapshotAtBroker(broker);\n \n-    // then\n+    // then verify that the log still contains the records that are not exported", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0f4ee484d1a6b46e42a1ad9f7b68adcdab1ec9b9"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzUyMTc1Nw==", "bodyText": "\ud83d\udc4d\n\nwe may not be verifying the state after compaction\n\nAlso with checking the segment count, we don't know when the compaction is done. It is hard to be exact \ud83e\udd14", "url": "https://github.com/camunda-cloud/zeebe/pull/4730#discussion_r443521757", "createdAt": "2020-06-22T12:27:29Z", "author": {"login": "saig0"}, "path": "qa/integration-tests/src/test/java/io/zeebe/broker/it/clustering/SingleBrokerDataDeletionTest.java", "diffHunk": "@@ -58,46 +65,62 @@ public void shouldNotCompactNotExportedEvents() {\n     final var logstream = clusteringRule.getLogStream(1);\n     final var reader = logstream.newLogStreamReader().join();\n \n-    while (getSegmentsCount(broker) <= 2) {\n-      writeToLog();\n-    }\n+    // - write records and update the exporter position\n+    ControllableExporter.updatePosition(true);\n+    fillSegments(broker, SEGMENT_COUNT);\n \n-    // when\n+    // - write more records but don't update the exporter position\n     ControllableExporter.updatePosition(false);\n+    fillSegments(broker, SEGMENT_COUNT * 2);\n \n-    // write more events\n-    while (getSegmentsCount(broker) <= 3) {\n-      writeToLog();\n-    }\n-    // write one more to make sure last processed position in segment 3\n-    writeToLog();\n-\n-    // increase snapshot interval and wait\n+    // - trigger a snapshot creation\n     clusteringRule.getClock().addTime(SNAPSHOT_PERIOD);\n     final var firstSnapshot = clusteringRule.waitForSnapshotAtBroker(broker);\n \n-    // then\n+    // then verify that the log still contains the records that are not exported", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzQyMDg2Nw=="}, "originalCommit": {"oid": "0f4ee484d1a6b46e42a1ad9f7b68adcdab1ec9b9"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk5NzczNQ==", "bodyText": "Ya. You are right. But I guess there is no other way to wait for compaction.", "url": "https://github.com/camunda-cloud/zeebe/pull/4730#discussion_r443997735", "createdAt": "2020-06-23T06:48:32Z", "author": {"login": "deepthidevaki"}, "path": "qa/integration-tests/src/test/java/io/zeebe/broker/it/clustering/SingleBrokerDataDeletionTest.java", "diffHunk": "@@ -58,46 +65,62 @@ public void shouldNotCompactNotExportedEvents() {\n     final var logstream = clusteringRule.getLogStream(1);\n     final var reader = logstream.newLogStreamReader().join();\n \n-    while (getSegmentsCount(broker) <= 2) {\n-      writeToLog();\n-    }\n+    // - write records and update the exporter position\n+    ControllableExporter.updatePosition(true);\n+    fillSegments(broker, SEGMENT_COUNT);\n \n-    // when\n+    // - write more records but don't update the exporter position\n     ControllableExporter.updatePosition(false);\n+    fillSegments(broker, SEGMENT_COUNT * 2);\n \n-    // write more events\n-    while (getSegmentsCount(broker) <= 3) {\n-      writeToLog();\n-    }\n-    // write one more to make sure last processed position in segment 3\n-    writeToLog();\n-\n-    // increase snapshot interval and wait\n+    // - trigger a snapshot creation\n     clusteringRule.getClock().addTime(SNAPSHOT_PERIOD);\n     final var firstSnapshot = clusteringRule.waitForSnapshotAtBroker(broker);\n \n-    // then\n+    // then verify that the log still contains the records that are not exported", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzQyMDg2Nw=="}, "originalCommit": {"oid": "0f4ee484d1a6b46e42a1ad9f7b68adcdab1ec9b9"}, "originalPosition": 71}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2Mjk3MDMwOnYy", "diffSide": "RIGHT", "path": "qa/integration-tests/src/test/java/io/zeebe/broker/it/clustering/ClusteredDataDeletionTest.java", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yMlQwOTozMzowOVrOGm5DxQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QwNjoyODozMlrOGna-cQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzQzMzkyNQ==", "bodyText": "Nit:\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                waitUntil(\n          \n          \n            \n              Awaitility.await()\n          \n          \n            \n                    .untilAsserted(\n          \n          \n            \n                        () ->\n          \n          \n            \n                            assertThat(getSegmentsCount(leader))\n          \n          \n            \n                                .isLessThan(segmentCountBeforeSnapshot.get(leaderNodeId)));", "url": "https://github.com/camunda-cloud/zeebe/pull/4730#discussion_r443433925", "createdAt": "2020-06-22T09:33:09Z", "author": {"login": "deepthidevaki"}, "path": "qa/integration-tests/src/test/java/io/zeebe/broker/it/clustering/ClusteredDataDeletionTest.java", "diffHunk": "@@ -90,22 +94,19 @@ public void shouldDeleteDataOnLeader() {\n     final int leaderNodeId = clusteringRule.getLeaderForPartition(1).getNodeId();\n     final Broker leader = clusteringRule.getBroker(leaderNodeId);\n \n-    while (getSegmentsCount(leader) <= 2) {\n-      clusteringRule\n-          .getClient()\n-          .newPublishMessageCommand()\n-          .messageName(\"msg\")\n-          .correlationKey(\"key\")\n-          .send()\n-          .join();\n-    }\n+    fillSegments(List.of(leader), SEGMENT_COUNT);\n \n     // when\n-    final var segmentCount =\n+    final var segmentCountBeforeSnapshot =\n         takeSnapshotAndWaitForReplication(Collections.singletonList(leader), clusteringRule);\n \n     // then\n-    TestUtil.waitUntil(() -> getSegments(leader).size() < segmentCount.get(leaderNodeId));\n+    waitUntil(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0f4ee484d1a6b46e42a1ad9f7b68adcdab1ec9b9"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzUyMzgzMQ==", "bodyText": "Do you suggest to replace our TestUtil? \ud83d\ude05", "url": "https://github.com/camunda-cloud/zeebe/pull/4730#discussion_r443523831", "createdAt": "2020-06-22T12:31:23Z", "author": {"login": "saig0"}, "path": "qa/integration-tests/src/test/java/io/zeebe/broker/it/clustering/ClusteredDataDeletionTest.java", "diffHunk": "@@ -90,22 +94,19 @@ public void shouldDeleteDataOnLeader() {\n     final int leaderNodeId = clusteringRule.getLeaderForPartition(1).getNodeId();\n     final Broker leader = clusteringRule.getBroker(leaderNodeId);\n \n-    while (getSegmentsCount(leader) <= 2) {\n-      clusteringRule\n-          .getClient()\n-          .newPublishMessageCommand()\n-          .messageName(\"msg\")\n-          .correlationKey(\"key\")\n-          .send()\n-          .join();\n-    }\n+    fillSegments(List.of(leader), SEGMENT_COUNT);\n \n     // when\n-    final var segmentCount =\n+    final var segmentCountBeforeSnapshot =\n         takeSnapshotAndWaitForReplication(Collections.singletonList(leader), clusteringRule);\n \n     // then\n-    TestUtil.waitUntil(() -> getSegments(leader).size() < segmentCount.get(leaderNodeId));\n+    waitUntil(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzQzMzkyNQ=="}, "originalCommit": {"oid": "0f4ee484d1a6b46e42a1ad9f7b68adcdab1ec9b9"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzUyNzg0OA==", "bodyText": ":D We had some discussions before about starting to use Awaitility instead of our own waitUntil. This looks like a good place to use it.", "url": "https://github.com/camunda-cloud/zeebe/pull/4730#discussion_r443527848", "createdAt": "2020-06-22T12:38:50Z", "author": {"login": "deepthidevaki"}, "path": "qa/integration-tests/src/test/java/io/zeebe/broker/it/clustering/ClusteredDataDeletionTest.java", "diffHunk": "@@ -90,22 +94,19 @@ public void shouldDeleteDataOnLeader() {\n     final int leaderNodeId = clusteringRule.getLeaderForPartition(1).getNodeId();\n     final Broker leader = clusteringRule.getBroker(leaderNodeId);\n \n-    while (getSegmentsCount(leader) <= 2) {\n-      clusteringRule\n-          .getClient()\n-          .newPublishMessageCommand()\n-          .messageName(\"msg\")\n-          .correlationKey(\"key\")\n-          .send()\n-          .join();\n-    }\n+    fillSegments(List.of(leader), SEGMENT_COUNT);\n \n     // when\n-    final var segmentCount =\n+    final var segmentCountBeforeSnapshot =\n         takeSnapshotAndWaitForReplication(Collections.singletonList(leader), clusteringRule);\n \n     // then\n-    TestUtil.waitUntil(() -> getSegments(leader).size() < segmentCount.get(leaderNodeId));\n+    waitUntil(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzQzMzkyNQ=="}, "originalCommit": {"oid": "0f4ee484d1a6b46e42a1ad9f7b68adcdab1ec9b9"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzUyOTc3OQ==", "bodyText": "We already use it for example here https://github.com/zeebe-io/zeebe/blob/379afe7174267f168d967b09d295e5b5ab0fd6c4/atomix/cluster/src/test/java/io/atomix/cluster/protocol/SwimProtocolTest.java#L385-L387", "url": "https://github.com/camunda-cloud/zeebe/pull/4730#discussion_r443529779", "createdAt": "2020-06-22T12:42:22Z", "author": {"login": "Zelldon"}, "path": "qa/integration-tests/src/test/java/io/zeebe/broker/it/clustering/ClusteredDataDeletionTest.java", "diffHunk": "@@ -90,22 +94,19 @@ public void shouldDeleteDataOnLeader() {\n     final int leaderNodeId = clusteringRule.getLeaderForPartition(1).getNodeId();\n     final Broker leader = clusteringRule.getBroker(leaderNodeId);\n \n-    while (getSegmentsCount(leader) <= 2) {\n-      clusteringRule\n-          .getClient()\n-          .newPublishMessageCommand()\n-          .messageName(\"msg\")\n-          .correlationKey(\"key\")\n-          .send()\n-          .join();\n-    }\n+    fillSegments(List.of(leader), SEGMENT_COUNT);\n \n     // when\n-    final var segmentCount =\n+    final var segmentCountBeforeSnapshot =\n         takeSnapshotAndWaitForReplication(Collections.singletonList(leader), clusteringRule);\n \n     // then\n-    TestUtil.waitUntil(() -> getSegments(leader).size() < segmentCount.get(leaderNodeId));\n+    waitUntil(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzQzMzkyNQ=="}, "originalCommit": {"oid": "0f4ee484d1a6b46e42a1ad9f7b68adcdab1ec9b9"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk4OTYxNw==", "bodyText": "Ok \ud83d\udc4d If we plan to replace the TestUtil then we should create an issue and communicate it to the team.", "url": "https://github.com/camunda-cloud/zeebe/pull/4730#discussion_r443989617", "createdAt": "2020-06-23T06:28:32Z", "author": {"login": "saig0"}, "path": "qa/integration-tests/src/test/java/io/zeebe/broker/it/clustering/ClusteredDataDeletionTest.java", "diffHunk": "@@ -90,22 +94,19 @@ public void shouldDeleteDataOnLeader() {\n     final int leaderNodeId = clusteringRule.getLeaderForPartition(1).getNodeId();\n     final Broker leader = clusteringRule.getBroker(leaderNodeId);\n \n-    while (getSegmentsCount(leader) <= 2) {\n-      clusteringRule\n-          .getClient()\n-          .newPublishMessageCommand()\n-          .messageName(\"msg\")\n-          .correlationKey(\"key\")\n-          .send()\n-          .join();\n-    }\n+    fillSegments(List.of(leader), SEGMENT_COUNT);\n \n     // when\n-    final var segmentCount =\n+    final var segmentCountBeforeSnapshot =\n         takeSnapshotAndWaitForReplication(Collections.singletonList(leader), clusteringRule);\n \n     // then\n-    TestUtil.waitUntil(() -> getSegments(leader).size() < segmentCount.get(leaderNodeId));\n+    waitUntil(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzQzMzkyNQ=="}, "originalCommit": {"oid": "0f4ee484d1a6b46e42a1ad9f7b68adcdab1ec9b9"}, "originalPosition": 55}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 523, "cost": 1, "resetAt": "2021-11-12T13:16:51Z"}}}