{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDQwMDI1MDYy", "number": 4816, "title": "docs(operations): update documentation of backpressure", "bodyText": "Description\nAdded documentation for backpressure\nRelated issues\ncloses #4646\nPull Request Checklist\n\n All commit messages match our commit message guidelines\n The submitting code follows our code style\n If submitting code, please run mvn clean install -DskipTests locally before committing", "createdAt": "2020-06-25T13:35:09Z", "url": "https://github.com/camunda-cloud/zeebe/pull/4816", "merged": true, "mergeCommit": {"oid": "159aff081f83568644a1ec9a266c78a2691c1ef9"}, "closed": true, "closedAt": "2020-06-26T07:32:30Z", "author": {"login": "deepthidevaki"}, "timelineItems": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcuvPziAFqTQzNzUyNjY1OA==", "endCursor": "Y3Vyc29yOnYyOpPPAAABcu9lEnABqjM0ODUzMjk4MDM=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM3NTI2NjU4", "url": "https://github.com/camunda-cloud/zeebe/pull/4816#pullrequestreview-437526658", "createdAt": "2020-06-25T14:03:48Z", "commit": {"oid": "e4c59384de1db30f3042c39fe786e5d509bdd812"}, "state": "APPROVED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQxNDowMzo0OFrOGo8PqQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQxNDowNzo0N1rOGo8aEw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTU4MzI3Mw==", "bodyText": "Maybe better \"client requests\" instead of \"user requests\"", "url": "https://github.com/camunda-cloud/zeebe/pull/4816#discussion_r445583273", "createdAt": "2020-06-25T14:03:48Z", "author": {"login": "pihme"}, "path": "docs/src/operations/backpressure.md", "diffHunk": "@@ -0,0 +1,93 @@\n+# Backpressure\n+\n+When a broker receives a user request, it is written to the *event stream* first (see section [Internal Processing](/basics/internal-processing.html) for details), and processed later by the stream processor.\n+If the processing is slow or if there are many user requests in the stream, it might take too long for the processor to start processing the command.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e4c59384de1db30f3042c39fe786e5d509bdd812"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTU4NDA1MA==", "bodyText": "Maybe better \"with a defined latency\" (acceptable implies a value judgement)", "url": "https://github.com/camunda-cloud/zeebe/pull/4816#discussion_r445584050", "createdAt": "2020-06-25T14:04:54Z", "author": {"login": "pihme"}, "path": "docs/src/operations/backpressure.md", "diffHunk": "@@ -0,0 +1,93 @@\n+# Backpressure\n+\n+When a broker receives a user request, it is written to the *event stream* first (see section [Internal Processing](/basics/internal-processing.html) for details), and processed later by the stream processor.\n+If the processing is slow or if there are many user requests in the stream, it might take too long for the processor to start processing the command.\n+If the broker keeps accepting new requests from the user, the back log increases and the processing latency can grow beyond an acceptable time.\n+To avoid such problems, Zeebe employs a backpressure mechanism.\n+When the broker receives more requests than it can process with an acceptable latency, it rejects some requests (see section [Error handling](/reference/grpc.html)).", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e4c59384de1db30f3042c39fe786e5d509bdd812"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTU4NTkzOQ==", "bodyText": "\"you might consider\"", "url": "https://github.com/camunda-cloud/zeebe/pull/4816#discussion_r445585939", "createdAt": "2020-06-25T14:07:47Z", "author": {"login": "pihme"}, "path": "docs/src/operations/backpressure.md", "diffHunk": "@@ -0,0 +1,93 @@\n+# Backpressure\n+\n+When a broker receives a user request, it is written to the *event stream* first (see section [Internal Processing](/basics/internal-processing.html) for details), and processed later by the stream processor.\n+If the processing is slow or if there are many user requests in the stream, it might take too long for the processor to start processing the command.\n+If the broker keeps accepting new requests from the user, the back log increases and the processing latency can grow beyond an acceptable time.\n+To avoid such problems, Zeebe employs a backpressure mechanism.\n+When the broker receives more requests than it can process with an acceptable latency, it rejects some requests (see section [Error handling](/reference/grpc.html)).\n+\n+### Terminologies\n+* *RTT* - The time between the request is accepted by the broker and the response to the request is sent back to the gateway.\n+* *inflight count* - The number of requests accepted by the broker but the response is not yet sent.\n+* *limit* - maximum number of flight requests. When the inflight count is above the limit, any new incoming request will be rejected.\n+\n+Note that the limit and inflight count are calculated per partition.\n+\n+### Backpressure algorithms\n+\n+Zeebe uses adaptive algorithms from [concurrency-limits](https://github.com/Netflix/concurrency-limits) to dynamically calculate the limit.\n+Zeebe can be configured with one of the following backpressure algorithms.\n+\n+#### Fixed Limit\n+With \u201cfixed limit\u201d one can configure a fixed value of the limit.\n+Zeebe operators are recommended to evaluate the latencies observed with different values for limit.\n+Note that with different cluster configurations, you may have to choose different limit values.\n+\n+#### AIMD\n+AIMD calculates the limit based on the configured *requestTimeout*.\n+When the RTT for a request *requestTimeout*, the limit is increased by 1.\n+When the RTT is longer than *requestTimeout*,\n+the limit will be reduced according to the configured *backoffRatio*.\n+\n+#### Vegas\n+Vegas is an adaptive limit algorithm based on TCP Vegas congestion control algorithm.\n+Vegas estimates a base latency as the minimum observed latency.\n+This base RTT is the expected latency when there is no load.\n+Whenever the RTT deviates from the base RTT, a new limit is calculated based on the vegas algorithm.\n+Vegas allows to configure two parameters - *alpha* and *beta*.\n+The values correspond to a queue size that is estimated by the Vegas algorithm based on the observed RTT, base RTT, and current limit.\n+When the queue size is below *alpha*, the limit is increased.\n+When the queue size is above *beta*, the limit is decreased.\n+\n+### Gradient\n+Gradient is an adaptive limit algorithm that dynamically calculates the limit based on observed RTT.\n+In the gradient algorithm, the limit is adjusted based on the gradient of observed RTT and an observed minimum RTT.\n+If gradient is less than 1, the limit is decreased otherwise the limit is increased.\n+\n+### Gradient2\n+Gradient2 is similar to Gradient, but instead of using observed minimum RTT as the base, it uses and exponentially smoothed average RTT.\n+\n+## Backpressure Tuning\n+\n+The goal of backpressure is to keep the processing latency low.\n+The processing latency is calculated as the time between the command is written to the event stream until it is processed.\n+Hence to see how backpressure behaves you can run a benchmark on your cluster and observe\n+the following metrics.\n+\n+* `zeebe_stream_processor_latency_bucket`\n+* `zeebe_dropped_request_count_total`\n+* `zeebe_received_request_count_total`\n+* `zeebe_backpressure_requests_limit`\n+\n+You may want to run the benchmark with different load\n+1. With low load - where the number of user requests send per second is low.\n+2. With high load - where the number of user requests sent per second is above what zeebe can process within a reasonable latency.\n+\n+If the value of the limit is small, the processing latency will be small but the number of rejected requests may be high.\n+If the value of the limit is large, less requests may be rejected (depending on the request rate),\n+but the processing latency may increase.\n+\n+When using \"fixed limit\", you can run the benchmark with different values for the limit.\n+You can then determine a suitable value for a limit for which the processing latency (`zeebe_stream_processor_latency_bucket`) is within the desired latency.\n+\n+When using \"AIMD\", you can configure a *requestTimeout* which corresponds to a desired latency.\n+Note that during high load \"AIMD\" can lead to a processing latency two times more than the configured *requestTimeout*.\n+It is also recommended to configure a *minLimit* to prevent the limit from aggressively dropping during constant high load.\n+\n+When using \"Vegas\", you cannot configure the backpressure to a desired latency.\n+Instead Vegas tries to keep the RTT as low as possible based on the observed minimum RTT.\n+\n+Similar to \"Vegas\", you cannot configure the desired latency in \"Gradient\" and \"Gradient2\".\n+They calculated the limit based on the gradient of observed RTT from the expected RTT.\n+Higher the value of *rttTolerance*, higher deviations are tolerated that results in higher values for limit.\n+\n+If a lot of requests are rejected due to backpressure, it might indicate that the processing capacity of the cluster is not enough to handle the expected throughput.\n+If this is the expected workload, then you should consider a different configuration for the cluster such as provisioning more resources and, increasing the number of nodes and partitions.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e4c59384de1db30f3042c39fe786e5d509bdd812"}, "originalPosition": 85}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8ab2d1c0563fdca462f7e8596d07d8c7baa2560c", "author": {"user": {"login": "deepthidevaki", "name": "Deepthi Devaki Akkoorath"}}, "url": "https://github.com/camunda-cloud/zeebe/commit/8ab2d1c0563fdca462f7e8596d07d8c7baa2560c", "committedDate": "2020-06-26T06:51:07Z", "message": "docs(operations): update documentation of backpressure"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "e4c59384de1db30f3042c39fe786e5d509bdd812", "author": {"user": {"login": "deepthidevaki", "name": "Deepthi Devaki Akkoorath"}}, "url": "https://github.com/camunda-cloud/zeebe/commit/e4c59384de1db30f3042c39fe786e5d509bdd812", "committedDate": "2020-06-25T13:33:09Z", "message": "docs(operations): update documentation of backpressure"}, "afterCommit": {"oid": "8ab2d1c0563fdca462f7e8596d07d8c7baa2560c", "author": {"user": {"login": "deepthidevaki", "name": "Deepthi Devaki Akkoorath"}}, "url": "https://github.com/camunda-cloud/zeebe/commit/8ab2d1c0563fdca462f7e8596d07d8c7baa2560c", "committedDate": "2020-06-26T06:51:07Z", "message": "docs(operations): update documentation of backpressure"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2646, "cost": 1, "resetAt": "2021-11-01T13:07:16Z"}}}