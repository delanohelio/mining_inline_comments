{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDkzMTY0NzA2", "number": 5433, "reviewThreads": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQwOToyOToyM1rOEn_1Ew==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQwOTozMTo1MFrOEn_44Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEwMzc1Njk5OnYy", "diffSide": "RIGHT", "path": "qa/integration-tests/src/test/java/io/zeebe/broker/it/clustering/SingleBrokerDataDeletionTest.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQwOToyOToyM1rOHY1qPQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQxMTo1ODoyOFrOHY6YNg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTgwNzAzNw==", "bodyText": "Fix the rawtypes warning.\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              @SuppressWarnings(\"rawtypes\")\n          \n          \n            \n              private Stream<Record> newRecordStream(final LogStreamReader reader) {\n          \n          \n            \n                final Spliterator<LoggedEvent> spliterator =\n          \n          \n            \n                    Spliterators.spliteratorUnknownSize(reader, Spliterator.ORDERED);\n          \n          \n            \n                return StreamSupport.stream(spliterator, false)\n          \n          \n            \n                    .map(event -> CopiedRecords.createCopiedRecord(1, event));\n          \n          \n            \n              }\n          \n          \n            \n              private Stream<Record<?>> newRecordStream(final LogStreamReader reader) {\n          \n          \n            \n                final Spliterator<LoggedEvent> spliterator =\n          \n          \n            \n                    Spliterators.spliteratorUnknownSize(reader, Spliterator.ORDERED);\n          \n          \n            \n                return StreamSupport.stream(spliterator, false)\n          \n          \n            \n                    .map(event -> (CopiedRecord<?>) CopiedRecords.createCopiedRecord(1, event));\n          \n          \n            \n              }", "url": "https://github.com/camunda-cloud/zeebe/pull/5433#discussion_r495807037", "createdAt": "2020-09-28T09:29:23Z", "author": {"login": "saig0"}, "path": "qa/integration-tests/src/test/java/io/zeebe/broker/it/clustering/SingleBrokerDataDeletionTest.java", "diffHunk": "@@ -47,284 +47,247 @@\n public class SingleBrokerDataDeletionTest {\n \n   private static final Duration SNAPSHOT_PERIOD = Duration.ofMinutes(5);\n-  private static final int SEGMENT_COUNT = 5;\n+  private static final DataSize LOG_SEGMENT_SIZE = DataSize.ofKilobytes(8);\n+  private static final DataSize MAX_MESSAGE_SIZE = DataSize.ofKilobytes(4);\n+  // variable has to be a bit smaller than max message size otherwise it will be rejected\n+  private static final int LARGE_VARIABLE_SIZE = (int) MAX_MESSAGE_SIZE.toBytes() / 2;\n+  private static final String MAX_MESSAGE_SIZE_VARIABLE = \"x\".repeat(LARGE_VARIABLE_SIZE);\n \n   @Rule\n-  public final ClusteringRule clusteringRule =\n-      new ClusteringRule(1, 1, 1, this::configureCustomExporter);\n+  public final ClusteringRule clusteringRule = new ClusteringRule(1, 1, 1, this::configureBroker);\n \n-  private final AtomicLong writtenRecords = new AtomicLong(0);\n-\n-  private void configureCustomExporter(final BrokerCfg brokerCfg) {\n-    final DataCfg data = brokerCfg.getData();\n-    data.setSnapshotPeriod(SNAPSHOT_PERIOD);\n-    data.setLogSegmentSize(DataSize.ofKilobytes(8));\n-    data.setLogIndexDensity(5);\n-    brokerCfg.getNetwork().setMaxMessageSize(DataSize.ofKilobytes(8));\n-\n-    final ExporterCfg exporterCfg = new ExporterCfg();\n-    exporterCfg.setClassName(ControllableExporter.class.getName());\n-    brokerCfg.setExporters(Collections.singletonMap(\"snapshot-test-exporter\", exporterCfg));\n+  @After\n+  public void cleanUp() {\n+    ControllableExporter.updatePosition(true);\n+    ControllableExporter.EXPORTED_RECORDS.set(0);\n+    ControllableExporter.RECORD_TYPE_FILTER.set(r -> true);\n+    ControllableExporter.VALUE_TYPE_FILTER.set(r -> true);\n   }\n \n   @Test\n-  public void shouldCompactEvenIfSkippingAllRecords() {\n-    // given\n-    final Broker broker = clusteringRule.getBroker(0);\n-\n-    // when\n+  public void shouldCompactEvenIfSkippingAllRecordsInitially() {\n+    // given - an exporter which does not update its own position and filters everything but\n+    // deployment commands\n+    final LogStream logStream = clusteringRule.getLogStream(1);\n+    final LogStreamReader reader = logStream.newLogStreamReader().join();\n     ControllableExporter.updatePosition(false);\n-    ControllableExporter.RECORD_TYPE_FILTER.set(r -> r == RecordType.COMMAND);\n-    ControllableExporter.VALUE_TYPE_FILTER.set(r -> r == ValueType.DEPLOYMENT);\n-    writeSegments(broker, 2);\n-    clusteringRule\n-        .getClient()\n-        .newDeployCommand()\n-        .addWorkflowModel(\n-            Bpmn.createExecutableProcess(\"process\").startEvent().done(), \"process.bpmn\")\n-        .send()\n-        .join();\n+    ControllableExporter.RECORD_TYPE_FILTER.set(t -> t == RecordType.COMMAND);\n+    ControllableExporter.VALUE_TYPE_FILTER.set(t -> t == ValueType.DEPLOYMENT);\n+\n+    // when - filling up the log with messages and NO deployments\n+    publishEnoughMessagesForCompaction();\n+    deployDummyProcess();\n     await(\"until at least one record is exported\")\n         .atMost(Duration.ofSeconds(5))\n         .untilAsserted(\n             () -> assertThat(ControllableExporter.EXPORTED_RECORDS).hasValueGreaterThan(0));\n \n-    // enforce compaction\n-    final var segmentsBeforeSnapshot = getSegmentsCount(broker);\n-    clusteringRule.getClock().addTime(SNAPSHOT_PERIOD);\n+    // memorize first position pre compaction to compare later on\n+    reader.seekToFirstEvent();\n+    final long firstPositionPreCompaction = reader.getPosition();\n \n-    // then\n-    assertThat(clusteringRule.waitForSnapshotAtBroker(broker)).isNotNull();\n-    await()\n+    // then - enforce compaction and make sure we have less records than we previously did\n+    clusteringRule.getClock().addTime(SNAPSHOT_PERIOD);\n+    clusteringRule.waitForSnapshotAtBroker(clusteringRule.getBroker(0));\n+    await(\"until some data before the deployment command was compacted\")\n         .untilAsserted(\n-            () ->\n-                assertThat(getSegmentsCount(broker))\n-                    .describedAs(\"Expected less segments after a snapshot is taken\")\n-                    .isLessThan(segmentsBeforeSnapshot));\n+            () -> {\n+              reader.seekToFirstEvent();\n+              final long firstPositionPostCompaction = reader.getPosition();\n+              assertThat(firstPositionPostCompaction).isGreaterThan(firstPositionPreCompaction);\n+              assertContainsDeploymentCommand(reader);\n+            });\n   }\n \n   @Test\n   public void shouldNotCompactUnacknowledgedEventsEvenIfSkipping() {\n-    // given\n-    final RecordMetadata metadata = new RecordMetadata();\n-    final Broker broker = clusteringRule.getBroker(0);\n-    final var logstream = clusteringRule.getLogStream(1);\n-    final var reader = logstream.newLogStreamReader().join();\n-\n-    // when\n+    // given - an exporter which does not update its own position and only accepts deployment\n+    // commands\n+    final LogStream logStream = clusteringRule.getLogStream(1);\n+    final LogStreamReader reader = logStream.newLogStreamReader().join();\n     ControllableExporter.updatePosition(false);\n-    ControllableExporter.RECORD_TYPE_FILTER.set(r -> r == RecordType.COMMAND);\n-    ControllableExporter.VALUE_TYPE_FILTER.set(r -> r == ValueType.DEPLOYMENT);\n-    writeSegments(broker, 2);\n-    clusteringRule\n-        .getClient()\n-        .newDeployCommand()\n-        .addWorkflowModel(\n-            Bpmn.createExecutableProcess(\"process\").startEvent().done(), \"process.bpmn\")\n-        .send()\n-        .join();\n-    writeSegments(broker, 2);\n+    ControllableExporter.RECORD_TYPE_FILTER.set(t -> t == RecordType.COMMAND);\n+    ControllableExporter.VALUE_TYPE_FILTER.set(t -> t == ValueType.DEPLOYMENT);\n+\n+    // when - filling up the log with messages and a single deployment\n+    publishEnoughMessagesForCompaction();\n+    deployDummyProcess();\n+    publishEnoughMessagesForCompaction();\n     await(\"until at least one record is exported\")\n         .atMost(Duration.ofSeconds(5))\n         .untilAsserted(\n             () -> assertThat(ControllableExporter.EXPORTED_RECORDS).hasValueGreaterThan(0));\n \n-    // grab the first log position, and the position of the last unacknowledged event\n+    // memorize first position pre compaction to compare later on\n     reader.seekToFirstEvent();\n-    final long firstPosition = reader.getPosition();\n-    long lastUnacknowledgedPosition = -1;\n-    while (reader.hasNext()) {\n-      final LoggedEvent event = reader.next();\n-      event.readMetadata(metadata);\n-      if (metadata.getValueType() == ValueType.DEPLOYMENT) {\n-        lastUnacknowledgedPosition = event.getPosition();\n-        break;\n-      }\n-    }\n+    final long firstPositionPreCompaction = reader.getPosition();\n \n-    // enforce compaction\n-    final var segmentsBeforeSnapshot = getSegmentsCount(broker);\n+    // then - enforce compaction and ensure the accepted deployment is still present on the log\n+    // after compaction\n     clusteringRule.getClock().addTime(SNAPSHOT_PERIOD);\n-\n-    // then\n-    assertThat(lastUnacknowledgedPosition).isGreaterThan(-1L);\n-    assertThat(clusteringRule.waitForSnapshotAtBroker(broker)).isNotNull();\n-    await()\n+    clusteringRule.waitForSnapshotAtBroker(clusteringRule.getBroker(0));\n+    await(\"until some data before the deployment command was compacted\")\n+        .atMost(Duration.ofSeconds(5))\n         .untilAsserted(\n-            () ->\n-                assertThat(getSegmentsCount(broker))\n-                    .describedAs(\"Expected less segments after a snapshot is taken\")\n-                    .isLessThan(segmentsBeforeSnapshot));\n-    reader.seekToFirstEvent();\n-    assertThat(reader.getPosition())\n-        .isGreaterThan(firstPosition)\n-        .isLessThanOrEqualTo(lastUnacknowledgedPosition);\n+            () -> {\n+              reader.seekToFirstEvent();\n+              final long firstPositionPostCompaction = reader.getPosition();\n+              assertThat(firstPositionPostCompaction).isGreaterThan(firstPositionPreCompaction);\n+              assertContainsDeploymentCommand(reader);\n+            });\n   }\n \n   @Test\n   public void shouldNotCompactNotExportedEvents() {\n     // given\n+    final LogStream logStream = clusteringRule.getLogStream(1);\n+    final LogStreamReader reader = logStream.newLogStreamReader().join();\n     final Broker broker = clusteringRule.getBroker(0);\n-\n-    final var logstream = clusteringRule.getLogStream(1);\n-    final var reader = logstream.newLogStreamReader().join();\n-\n-    // - write records and update the exporter position\n     ControllableExporter.updatePosition(true);\n-    fillSegments(broker, SEGMENT_COUNT);\n \n-    // - write more records but don't update the exporter position\n+    // when - filling the log with messages (updating the position), then a single deployment\n+    // command, and more messages (all of which do not update the position)\n+    publishEnoughMessagesForCompaction();\n     ControllableExporter.updatePosition(false);\n+    deployDummyProcess();\n+    publishEnoughMessagesForCompaction();\n \n-    final var filledSegmentCount = SEGMENT_COUNT * 2;\n-    fillSegments(broker, filledSegmentCount);\n-\n-    // - trigger a snapshot creation\n+    // then - force compaction and ensure we compacted only things before our sentinel command\n+    reader.seekToFirstEvent();\n+    long firstPositionPreCompaction = reader.getPosition();\n     clusteringRule.getClock().addTime(SNAPSHOT_PERIOD);\n-    final var firstSnapshot = clusteringRule.waitForSnapshotAtBroker(broker);\n-\n-    await()\n-        .untilAsserted(\n-            () ->\n-                assertThat(getSegmentsCount(broker))\n-                    .describedAs(\"Expected less segments after a snapshot is taken\")\n-                    .isLessThan(filledSegmentCount));\n-\n-    // then verify that the log still contains the records that are not exported\n-    final var firstNonExportedPosition =\n-        ControllableExporter.NOT_EXPORTED_RECORDS.get(0).getPosition();\n-\n-    assertThat(hasRecordWithPosition(reader, firstNonExportedPosition))\n-        .describedAs(\"Expected first non-exported record to be present in the log but not found.\")\n-        .isTrue();\n-\n-    // - write more records and update the exporter position again\n-    final var segmentsBeforeSnapshot = getSegmentsCount(broker);\n+    final FileBasedSnapshotMetadata firstSnapshot = clusteringRule.waitForSnapshotAtBroker(broker);\n+    awaitUntilCompaction(reader, firstPositionPreCompaction);\n+    assertContainsDeploymentCommand(reader);\n \n+    // when - re-enabling updating the position\n     ControllableExporter.updatePosition(true);\n-    fillSegments(broker, segmentsBeforeSnapshot + 1);\n+    publishEnoughMessagesForCompaction();\n \n-    // - trigger the next snapshot creation\n+    // then - ensure we can still compact\n+    reader.seekToFirstEvent();\n+    firstPositionPreCompaction = reader.getPosition();\n     clusteringRule.getClock().addTime(SNAPSHOT_PERIOD);\n     clusteringRule.waitForNewSnapshotAtBroker(broker, firstSnapshot);\n-\n-    // then verify that the log is now compacted after the exporter position was updated\n-    await()\n-        .untilAsserted(\n-            () ->\n-                assertThat(getSegmentsCount(broker))\n-                    .describedAs(\"Expected less segments after a snapshot is taken\")\n-                    .isLessThan(segmentsBeforeSnapshot));\n+    awaitUntilCompaction(reader, firstPositionPreCompaction);\n   }\n \n   @Test\n   public void shouldCompactWhenExporterHasBeenRemoved() {\n-    // given\n-    final Broker broker = clusteringRule.getBroker(0);\n+    // given - an exporter which updates its position and accepts all records\n+    final int nodeId = 0;\n+    LogStreamReader reader = clusteringRule.getLogStream(1).newLogStreamReader().join();\n+    final Broker broker = clusteringRule.getBroker(nodeId);\n     ControllableExporter.updatePosition(true);\n-    fillSegments(broker, SEGMENT_COUNT);\n-    clusteringRule.getClock().addTime(SNAPSHOT_PERIOD);\n-    // create first snapshot with exporter positions\n-    final var firstSnapshot = clusteringRule.waitForSnapshotAtBroker(broker);\n-\n-    // restart with no exporter\n-    final var brokerCfg = clusteringRule.getBrokerCfg(0);\n-    brokerCfg.setExporters(Map.of());\n-    clusteringRule.stopBroker(0);\n-    clusteringRule.startBroker(0);\n \n-    final var filledSegmentCount = SEGMENT_COUNT * 2;\n-    writeSegments(broker, filledSegmentCount);\n+    // when - filling the log with messages, and a single deployment command for which we will not\n+    // update the position\n+    publishEnoughMessagesForCompaction();\n+    ControllableExporter.updatePosition(false);\n+    deployDummyProcess();\n \n-    // when triggering new snapshot creation\n-    final var segmentsCount = getSegmentsCount(broker);\n+    // then - force compaction and ensure we compacted only things before our sentinel command\n+    reader.seekToFirstEvent();\n+    final long firstPositionPreCompaction = reader.getPosition();\n+    clusteringRule.getClock().addTime(SNAPSHOT_PERIOD);\n+    final FileBasedSnapshotMetadata firstSnapshot = clusteringRule.waitForSnapshotAtBroker(broker);\n+    awaitUntilCompaction(reader, firstPositionPreCompaction);\n+    assertContainsDeploymentCommand(reader);\n+\n+    // when - restarting without the exporter\n+    final var brokerCfg = clusteringRule.getBrokerCfg(nodeId);\n+    brokerCfg.setExporters(Collections.emptyMap());\n+    clusteringRule.stopBroker(nodeId);\n+    clusteringRule.startBroker(nodeId);\n+    publishEnoughMessagesForCompaction();\n+\n+    // then - force compaction, and expect the deployment command to have been removed\n+    reader = clusteringRule.getLogStream(1).newLogStreamReader().join();\n+    final long newFirstPositionPreCompaction = reader.getPosition();\n     clusteringRule.getClock().addTime(SNAPSHOT_PERIOD);\n-    final var secondSnapshot = clusteringRule.waitForNewSnapshotAtBroker(broker, firstSnapshot);\n+    clusteringRule.waitForNewSnapshotAtBroker(broker, firstSnapshot);\n+    assertThat(newFirstPositionPreCompaction).isGreaterThan(firstPositionPreCompaction);\n+    awaitUntilCompaction(reader, newFirstPositionPreCompaction);\n+    assertDoesNotContainDeploymentCommand(reader);\n+  }\n \n-    // then\n-    assertThat(firstSnapshot).isNotEqualTo(secondSnapshot);\n-    await()\n+  private void awaitUntilCompaction(\n+      final LogStreamReader reader, final long firstPositionPreCompaction) {\n+    await(\"until some data was compacted\")\n+        .atMost(Duration.ofSeconds(5))\n         .untilAsserted(\n-            () ->\n-                assertThat(getSegmentsCount(broker))\n-                    .describedAs(\"Expected less segments after a snapshot is taken\")\n-                    .isLessThan(segmentsCount));\n+            () -> {\n+              reader.seekToFirstEvent();\n+              final long firstPositionPostCompaction = reader.getPosition();\n+              assertThat(firstPositionPostCompaction).isGreaterThan(firstPositionPreCompaction);\n+            });\n   }\n \n-  private void fillSegments(final Broker broker, final int segmentCount) {\n-    writeSegments(broker, segmentCount);\n-\n-    await()\n-        .untilAsserted(\n-            () ->\n-                assertThat(ControllableExporter.EXPORTED_RECORDS.get())\n-                    .describedAs(\"Expected all written records to be exported\")\n-                    .isGreaterThanOrEqualTo(writtenRecords.get()));\n+  private void deployDummyProcess() {\n+    clusteringRule\n+        .getClient()\n+        .newDeployCommand()\n+        .addWorkflowModel(\n+            Bpmn.createExecutableProcess(\"process\").startEvent().done(), \"process.bpmn\")\n+        .send()\n+        .join();\n   }\n \n-  private void writeSegments(final Broker broker, final int segmentCount) {\n-    while (getSegmentsCount(broker) <= segmentCount) {\n-      writeToLog();\n-      writtenRecords.incrementAndGet();\n-    }\n+  private void configureBroker(final BrokerCfg brokerCfg) {\n+    final DataCfg data = brokerCfg.getData();\n+    data.setSnapshotPeriod(SNAPSHOT_PERIOD);\n+    data.setLogSegmentSize(LOG_SEGMENT_SIZE);\n+    data.setLogIndexDensity(5);\n+    brokerCfg.getNetwork().setMaxMessageSize(MAX_MESSAGE_SIZE);\n+\n+    final ExporterCfg exporterCfg = new ExporterCfg();\n+    exporterCfg.setClassName(ControllableExporter.class.getName());\n+    brokerCfg.setExporters(Collections.singletonMap(\"snapshot-test-exporter\", exporterCfg));\n   }\n \n-  private void writeToLog() {\n+  private void publishEnoughMessagesForCompaction() {\n+    final int requiredMessageCount = (int) LOG_SEGMENT_SIZE.toBytes() / LARGE_VARIABLE_SIZE;\n+    IntStream.range(0, requiredMessageCount + 1).forEach(this::publishMaxMessageSizeMessage);\n+  }\n \n+  private void publishMaxMessageSizeMessage(final int key) {\n     clusteringRule\n         .getClient()\n         .newPublishMessageCommand()\n         .messageName(\"msg\")\n-        .correlationKey(\"key\")\n+        .correlationKey(\"msg-\" + key)\n+        .variables(Map.of(\"foo\", MAX_MESSAGE_SIZE_VARIABLE))\n         .send()\n         .join();\n   }\n \n-  private int getSegmentsCount(final Broker broker) {\n-    return getSegments(broker).size();\n+  @SuppressWarnings(\"rawtypes\")\n+  private Stream<Record> newRecordStream(final LogStreamReader reader) {\n+    final Spliterator<LoggedEvent> spliterator =\n+        Spliterators.spliteratorUnknownSize(reader, Spliterator.ORDERED);\n+    return StreamSupport.stream(spliterator, false)\n+        .map(event -> CopiedRecords.createCopiedRecord(1, event));\n   }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2549378f6d9803bdffbca6f9c5ceffd364782774"}, "originalPosition": 431}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTg4MTA0Mg==", "bodyText": "Did it make sense why I'm using CopiedRecord here? Wasn't sure if it would be obvious - so that we can use RecordAssert and get nicer error messages mostly. I was worried it would seem over the top for someone reading it later, but the errors are really much nicer.", "url": "https://github.com/camunda-cloud/zeebe/pull/5433#discussion_r495881042", "createdAt": "2020-09-28T11:51:48Z", "author": {"login": "npepinpe"}, "path": "qa/integration-tests/src/test/java/io/zeebe/broker/it/clustering/SingleBrokerDataDeletionTest.java", "diffHunk": "@@ -47,284 +47,247 @@\n public class SingleBrokerDataDeletionTest {\n \n   private static final Duration SNAPSHOT_PERIOD = Duration.ofMinutes(5);\n-  private static final int SEGMENT_COUNT = 5;\n+  private static final DataSize LOG_SEGMENT_SIZE = DataSize.ofKilobytes(8);\n+  private static final DataSize MAX_MESSAGE_SIZE = DataSize.ofKilobytes(4);\n+  // variable has to be a bit smaller than max message size otherwise it will be rejected\n+  private static final int LARGE_VARIABLE_SIZE = (int) MAX_MESSAGE_SIZE.toBytes() / 2;\n+  private static final String MAX_MESSAGE_SIZE_VARIABLE = \"x\".repeat(LARGE_VARIABLE_SIZE);\n \n   @Rule\n-  public final ClusteringRule clusteringRule =\n-      new ClusteringRule(1, 1, 1, this::configureCustomExporter);\n+  public final ClusteringRule clusteringRule = new ClusteringRule(1, 1, 1, this::configureBroker);\n \n-  private final AtomicLong writtenRecords = new AtomicLong(0);\n-\n-  private void configureCustomExporter(final BrokerCfg brokerCfg) {\n-    final DataCfg data = brokerCfg.getData();\n-    data.setSnapshotPeriod(SNAPSHOT_PERIOD);\n-    data.setLogSegmentSize(DataSize.ofKilobytes(8));\n-    data.setLogIndexDensity(5);\n-    brokerCfg.getNetwork().setMaxMessageSize(DataSize.ofKilobytes(8));\n-\n-    final ExporterCfg exporterCfg = new ExporterCfg();\n-    exporterCfg.setClassName(ControllableExporter.class.getName());\n-    brokerCfg.setExporters(Collections.singletonMap(\"snapshot-test-exporter\", exporterCfg));\n+  @After\n+  public void cleanUp() {\n+    ControllableExporter.updatePosition(true);\n+    ControllableExporter.EXPORTED_RECORDS.set(0);\n+    ControllableExporter.RECORD_TYPE_FILTER.set(r -> true);\n+    ControllableExporter.VALUE_TYPE_FILTER.set(r -> true);\n   }\n \n   @Test\n-  public void shouldCompactEvenIfSkippingAllRecords() {\n-    // given\n-    final Broker broker = clusteringRule.getBroker(0);\n-\n-    // when\n+  public void shouldCompactEvenIfSkippingAllRecordsInitially() {\n+    // given - an exporter which does not update its own position and filters everything but\n+    // deployment commands\n+    final LogStream logStream = clusteringRule.getLogStream(1);\n+    final LogStreamReader reader = logStream.newLogStreamReader().join();\n     ControllableExporter.updatePosition(false);\n-    ControllableExporter.RECORD_TYPE_FILTER.set(r -> r == RecordType.COMMAND);\n-    ControllableExporter.VALUE_TYPE_FILTER.set(r -> r == ValueType.DEPLOYMENT);\n-    writeSegments(broker, 2);\n-    clusteringRule\n-        .getClient()\n-        .newDeployCommand()\n-        .addWorkflowModel(\n-            Bpmn.createExecutableProcess(\"process\").startEvent().done(), \"process.bpmn\")\n-        .send()\n-        .join();\n+    ControllableExporter.RECORD_TYPE_FILTER.set(t -> t == RecordType.COMMAND);\n+    ControllableExporter.VALUE_TYPE_FILTER.set(t -> t == ValueType.DEPLOYMENT);\n+\n+    // when - filling up the log with messages and NO deployments\n+    publishEnoughMessagesForCompaction();\n+    deployDummyProcess();\n     await(\"until at least one record is exported\")\n         .atMost(Duration.ofSeconds(5))\n         .untilAsserted(\n             () -> assertThat(ControllableExporter.EXPORTED_RECORDS).hasValueGreaterThan(0));\n \n-    // enforce compaction\n-    final var segmentsBeforeSnapshot = getSegmentsCount(broker);\n-    clusteringRule.getClock().addTime(SNAPSHOT_PERIOD);\n+    // memorize first position pre compaction to compare later on\n+    reader.seekToFirstEvent();\n+    final long firstPositionPreCompaction = reader.getPosition();\n \n-    // then\n-    assertThat(clusteringRule.waitForSnapshotAtBroker(broker)).isNotNull();\n-    await()\n+    // then - enforce compaction and make sure we have less records than we previously did\n+    clusteringRule.getClock().addTime(SNAPSHOT_PERIOD);\n+    clusteringRule.waitForSnapshotAtBroker(clusteringRule.getBroker(0));\n+    await(\"until some data before the deployment command was compacted\")\n         .untilAsserted(\n-            () ->\n-                assertThat(getSegmentsCount(broker))\n-                    .describedAs(\"Expected less segments after a snapshot is taken\")\n-                    .isLessThan(segmentsBeforeSnapshot));\n+            () -> {\n+              reader.seekToFirstEvent();\n+              final long firstPositionPostCompaction = reader.getPosition();\n+              assertThat(firstPositionPostCompaction).isGreaterThan(firstPositionPreCompaction);\n+              assertContainsDeploymentCommand(reader);\n+            });\n   }\n \n   @Test\n   public void shouldNotCompactUnacknowledgedEventsEvenIfSkipping() {\n-    // given\n-    final RecordMetadata metadata = new RecordMetadata();\n-    final Broker broker = clusteringRule.getBroker(0);\n-    final var logstream = clusteringRule.getLogStream(1);\n-    final var reader = logstream.newLogStreamReader().join();\n-\n-    // when\n+    // given - an exporter which does not update its own position and only accepts deployment\n+    // commands\n+    final LogStream logStream = clusteringRule.getLogStream(1);\n+    final LogStreamReader reader = logStream.newLogStreamReader().join();\n     ControllableExporter.updatePosition(false);\n-    ControllableExporter.RECORD_TYPE_FILTER.set(r -> r == RecordType.COMMAND);\n-    ControllableExporter.VALUE_TYPE_FILTER.set(r -> r == ValueType.DEPLOYMENT);\n-    writeSegments(broker, 2);\n-    clusteringRule\n-        .getClient()\n-        .newDeployCommand()\n-        .addWorkflowModel(\n-            Bpmn.createExecutableProcess(\"process\").startEvent().done(), \"process.bpmn\")\n-        .send()\n-        .join();\n-    writeSegments(broker, 2);\n+    ControllableExporter.RECORD_TYPE_FILTER.set(t -> t == RecordType.COMMAND);\n+    ControllableExporter.VALUE_TYPE_FILTER.set(t -> t == ValueType.DEPLOYMENT);\n+\n+    // when - filling up the log with messages and a single deployment\n+    publishEnoughMessagesForCompaction();\n+    deployDummyProcess();\n+    publishEnoughMessagesForCompaction();\n     await(\"until at least one record is exported\")\n         .atMost(Duration.ofSeconds(5))\n         .untilAsserted(\n             () -> assertThat(ControllableExporter.EXPORTED_RECORDS).hasValueGreaterThan(0));\n \n-    // grab the first log position, and the position of the last unacknowledged event\n+    // memorize first position pre compaction to compare later on\n     reader.seekToFirstEvent();\n-    final long firstPosition = reader.getPosition();\n-    long lastUnacknowledgedPosition = -1;\n-    while (reader.hasNext()) {\n-      final LoggedEvent event = reader.next();\n-      event.readMetadata(metadata);\n-      if (metadata.getValueType() == ValueType.DEPLOYMENT) {\n-        lastUnacknowledgedPosition = event.getPosition();\n-        break;\n-      }\n-    }\n+    final long firstPositionPreCompaction = reader.getPosition();\n \n-    // enforce compaction\n-    final var segmentsBeforeSnapshot = getSegmentsCount(broker);\n+    // then - enforce compaction and ensure the accepted deployment is still present on the log\n+    // after compaction\n     clusteringRule.getClock().addTime(SNAPSHOT_PERIOD);\n-\n-    // then\n-    assertThat(lastUnacknowledgedPosition).isGreaterThan(-1L);\n-    assertThat(clusteringRule.waitForSnapshotAtBroker(broker)).isNotNull();\n-    await()\n+    clusteringRule.waitForSnapshotAtBroker(clusteringRule.getBroker(0));\n+    await(\"until some data before the deployment command was compacted\")\n+        .atMost(Duration.ofSeconds(5))\n         .untilAsserted(\n-            () ->\n-                assertThat(getSegmentsCount(broker))\n-                    .describedAs(\"Expected less segments after a snapshot is taken\")\n-                    .isLessThan(segmentsBeforeSnapshot));\n-    reader.seekToFirstEvent();\n-    assertThat(reader.getPosition())\n-        .isGreaterThan(firstPosition)\n-        .isLessThanOrEqualTo(lastUnacknowledgedPosition);\n+            () -> {\n+              reader.seekToFirstEvent();\n+              final long firstPositionPostCompaction = reader.getPosition();\n+              assertThat(firstPositionPostCompaction).isGreaterThan(firstPositionPreCompaction);\n+              assertContainsDeploymentCommand(reader);\n+            });\n   }\n \n   @Test\n   public void shouldNotCompactNotExportedEvents() {\n     // given\n+    final LogStream logStream = clusteringRule.getLogStream(1);\n+    final LogStreamReader reader = logStream.newLogStreamReader().join();\n     final Broker broker = clusteringRule.getBroker(0);\n-\n-    final var logstream = clusteringRule.getLogStream(1);\n-    final var reader = logstream.newLogStreamReader().join();\n-\n-    // - write records and update the exporter position\n     ControllableExporter.updatePosition(true);\n-    fillSegments(broker, SEGMENT_COUNT);\n \n-    // - write more records but don't update the exporter position\n+    // when - filling the log with messages (updating the position), then a single deployment\n+    // command, and more messages (all of which do not update the position)\n+    publishEnoughMessagesForCompaction();\n     ControllableExporter.updatePosition(false);\n+    deployDummyProcess();\n+    publishEnoughMessagesForCompaction();\n \n-    final var filledSegmentCount = SEGMENT_COUNT * 2;\n-    fillSegments(broker, filledSegmentCount);\n-\n-    // - trigger a snapshot creation\n+    // then - force compaction and ensure we compacted only things before our sentinel command\n+    reader.seekToFirstEvent();\n+    long firstPositionPreCompaction = reader.getPosition();\n     clusteringRule.getClock().addTime(SNAPSHOT_PERIOD);\n-    final var firstSnapshot = clusteringRule.waitForSnapshotAtBroker(broker);\n-\n-    await()\n-        .untilAsserted(\n-            () ->\n-                assertThat(getSegmentsCount(broker))\n-                    .describedAs(\"Expected less segments after a snapshot is taken\")\n-                    .isLessThan(filledSegmentCount));\n-\n-    // then verify that the log still contains the records that are not exported\n-    final var firstNonExportedPosition =\n-        ControllableExporter.NOT_EXPORTED_RECORDS.get(0).getPosition();\n-\n-    assertThat(hasRecordWithPosition(reader, firstNonExportedPosition))\n-        .describedAs(\"Expected first non-exported record to be present in the log but not found.\")\n-        .isTrue();\n-\n-    // - write more records and update the exporter position again\n-    final var segmentsBeforeSnapshot = getSegmentsCount(broker);\n+    final FileBasedSnapshotMetadata firstSnapshot = clusteringRule.waitForSnapshotAtBroker(broker);\n+    awaitUntilCompaction(reader, firstPositionPreCompaction);\n+    assertContainsDeploymentCommand(reader);\n \n+    // when - re-enabling updating the position\n     ControllableExporter.updatePosition(true);\n-    fillSegments(broker, segmentsBeforeSnapshot + 1);\n+    publishEnoughMessagesForCompaction();\n \n-    // - trigger the next snapshot creation\n+    // then - ensure we can still compact\n+    reader.seekToFirstEvent();\n+    firstPositionPreCompaction = reader.getPosition();\n     clusteringRule.getClock().addTime(SNAPSHOT_PERIOD);\n     clusteringRule.waitForNewSnapshotAtBroker(broker, firstSnapshot);\n-\n-    // then verify that the log is now compacted after the exporter position was updated\n-    await()\n-        .untilAsserted(\n-            () ->\n-                assertThat(getSegmentsCount(broker))\n-                    .describedAs(\"Expected less segments after a snapshot is taken\")\n-                    .isLessThan(segmentsBeforeSnapshot));\n+    awaitUntilCompaction(reader, firstPositionPreCompaction);\n   }\n \n   @Test\n   public void shouldCompactWhenExporterHasBeenRemoved() {\n-    // given\n-    final Broker broker = clusteringRule.getBroker(0);\n+    // given - an exporter which updates its position and accepts all records\n+    final int nodeId = 0;\n+    LogStreamReader reader = clusteringRule.getLogStream(1).newLogStreamReader().join();\n+    final Broker broker = clusteringRule.getBroker(nodeId);\n     ControllableExporter.updatePosition(true);\n-    fillSegments(broker, SEGMENT_COUNT);\n-    clusteringRule.getClock().addTime(SNAPSHOT_PERIOD);\n-    // create first snapshot with exporter positions\n-    final var firstSnapshot = clusteringRule.waitForSnapshotAtBroker(broker);\n-\n-    // restart with no exporter\n-    final var brokerCfg = clusteringRule.getBrokerCfg(0);\n-    brokerCfg.setExporters(Map.of());\n-    clusteringRule.stopBroker(0);\n-    clusteringRule.startBroker(0);\n \n-    final var filledSegmentCount = SEGMENT_COUNT * 2;\n-    writeSegments(broker, filledSegmentCount);\n+    // when - filling the log with messages, and a single deployment command for which we will not\n+    // update the position\n+    publishEnoughMessagesForCompaction();\n+    ControllableExporter.updatePosition(false);\n+    deployDummyProcess();\n \n-    // when triggering new snapshot creation\n-    final var segmentsCount = getSegmentsCount(broker);\n+    // then - force compaction and ensure we compacted only things before our sentinel command\n+    reader.seekToFirstEvent();\n+    final long firstPositionPreCompaction = reader.getPosition();\n+    clusteringRule.getClock().addTime(SNAPSHOT_PERIOD);\n+    final FileBasedSnapshotMetadata firstSnapshot = clusteringRule.waitForSnapshotAtBroker(broker);\n+    awaitUntilCompaction(reader, firstPositionPreCompaction);\n+    assertContainsDeploymentCommand(reader);\n+\n+    // when - restarting without the exporter\n+    final var brokerCfg = clusteringRule.getBrokerCfg(nodeId);\n+    brokerCfg.setExporters(Collections.emptyMap());\n+    clusteringRule.stopBroker(nodeId);\n+    clusteringRule.startBroker(nodeId);\n+    publishEnoughMessagesForCompaction();\n+\n+    // then - force compaction, and expect the deployment command to have been removed\n+    reader = clusteringRule.getLogStream(1).newLogStreamReader().join();\n+    final long newFirstPositionPreCompaction = reader.getPosition();\n     clusteringRule.getClock().addTime(SNAPSHOT_PERIOD);\n-    final var secondSnapshot = clusteringRule.waitForNewSnapshotAtBroker(broker, firstSnapshot);\n+    clusteringRule.waitForNewSnapshotAtBroker(broker, firstSnapshot);\n+    assertThat(newFirstPositionPreCompaction).isGreaterThan(firstPositionPreCompaction);\n+    awaitUntilCompaction(reader, newFirstPositionPreCompaction);\n+    assertDoesNotContainDeploymentCommand(reader);\n+  }\n \n-    // then\n-    assertThat(firstSnapshot).isNotEqualTo(secondSnapshot);\n-    await()\n+  private void awaitUntilCompaction(\n+      final LogStreamReader reader, final long firstPositionPreCompaction) {\n+    await(\"until some data was compacted\")\n+        .atMost(Duration.ofSeconds(5))\n         .untilAsserted(\n-            () ->\n-                assertThat(getSegmentsCount(broker))\n-                    .describedAs(\"Expected less segments after a snapshot is taken\")\n-                    .isLessThan(segmentsCount));\n+            () -> {\n+              reader.seekToFirstEvent();\n+              final long firstPositionPostCompaction = reader.getPosition();\n+              assertThat(firstPositionPostCompaction).isGreaterThan(firstPositionPreCompaction);\n+            });\n   }\n \n-  private void fillSegments(final Broker broker, final int segmentCount) {\n-    writeSegments(broker, segmentCount);\n-\n-    await()\n-        .untilAsserted(\n-            () ->\n-                assertThat(ControllableExporter.EXPORTED_RECORDS.get())\n-                    .describedAs(\"Expected all written records to be exported\")\n-                    .isGreaterThanOrEqualTo(writtenRecords.get()));\n+  private void deployDummyProcess() {\n+    clusteringRule\n+        .getClient()\n+        .newDeployCommand()\n+        .addWorkflowModel(\n+            Bpmn.createExecutableProcess(\"process\").startEvent().done(), \"process.bpmn\")\n+        .send()\n+        .join();\n   }\n \n-  private void writeSegments(final Broker broker, final int segmentCount) {\n-    while (getSegmentsCount(broker) <= segmentCount) {\n-      writeToLog();\n-      writtenRecords.incrementAndGet();\n-    }\n+  private void configureBroker(final BrokerCfg brokerCfg) {\n+    final DataCfg data = brokerCfg.getData();\n+    data.setSnapshotPeriod(SNAPSHOT_PERIOD);\n+    data.setLogSegmentSize(LOG_SEGMENT_SIZE);\n+    data.setLogIndexDensity(5);\n+    brokerCfg.getNetwork().setMaxMessageSize(MAX_MESSAGE_SIZE);\n+\n+    final ExporterCfg exporterCfg = new ExporterCfg();\n+    exporterCfg.setClassName(ControllableExporter.class.getName());\n+    brokerCfg.setExporters(Collections.singletonMap(\"snapshot-test-exporter\", exporterCfg));\n   }\n \n-  private void writeToLog() {\n+  private void publishEnoughMessagesForCompaction() {\n+    final int requiredMessageCount = (int) LOG_SEGMENT_SIZE.toBytes() / LARGE_VARIABLE_SIZE;\n+    IntStream.range(0, requiredMessageCount + 1).forEach(this::publishMaxMessageSizeMessage);\n+  }\n \n+  private void publishMaxMessageSizeMessage(final int key) {\n     clusteringRule\n         .getClient()\n         .newPublishMessageCommand()\n         .messageName(\"msg\")\n-        .correlationKey(\"key\")\n+        .correlationKey(\"msg-\" + key)\n+        .variables(Map.of(\"foo\", MAX_MESSAGE_SIZE_VARIABLE))\n         .send()\n         .join();\n   }\n \n-  private int getSegmentsCount(final Broker broker) {\n-    return getSegments(broker).size();\n+  @SuppressWarnings(\"rawtypes\")\n+  private Stream<Record> newRecordStream(final LogStreamReader reader) {\n+    final Spliterator<LoggedEvent> spliterator =\n+        Spliterators.spliteratorUnknownSize(reader, Spliterator.ORDERED);\n+    return StreamSupport.stream(spliterator, false)\n+        .map(event -> CopiedRecords.createCopiedRecord(1, event));\n   }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTgwNzAzNw=="}, "originalCommit": {"oid": "2549378f6d9803bdffbca6f9c5ceffd364782774"}, "originalPosition": 431}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTg4NDM0Mg==", "bodyText": "Make sense \ud83d\udc4d", "url": "https://github.com/camunda-cloud/zeebe/pull/5433#discussion_r495884342", "createdAt": "2020-09-28T11:58:28Z", "author": {"login": "saig0"}, "path": "qa/integration-tests/src/test/java/io/zeebe/broker/it/clustering/SingleBrokerDataDeletionTest.java", "diffHunk": "@@ -47,284 +47,247 @@\n public class SingleBrokerDataDeletionTest {\n \n   private static final Duration SNAPSHOT_PERIOD = Duration.ofMinutes(5);\n-  private static final int SEGMENT_COUNT = 5;\n+  private static final DataSize LOG_SEGMENT_SIZE = DataSize.ofKilobytes(8);\n+  private static final DataSize MAX_MESSAGE_SIZE = DataSize.ofKilobytes(4);\n+  // variable has to be a bit smaller than max message size otherwise it will be rejected\n+  private static final int LARGE_VARIABLE_SIZE = (int) MAX_MESSAGE_SIZE.toBytes() / 2;\n+  private static final String MAX_MESSAGE_SIZE_VARIABLE = \"x\".repeat(LARGE_VARIABLE_SIZE);\n \n   @Rule\n-  public final ClusteringRule clusteringRule =\n-      new ClusteringRule(1, 1, 1, this::configureCustomExporter);\n+  public final ClusteringRule clusteringRule = new ClusteringRule(1, 1, 1, this::configureBroker);\n \n-  private final AtomicLong writtenRecords = new AtomicLong(0);\n-\n-  private void configureCustomExporter(final BrokerCfg brokerCfg) {\n-    final DataCfg data = brokerCfg.getData();\n-    data.setSnapshotPeriod(SNAPSHOT_PERIOD);\n-    data.setLogSegmentSize(DataSize.ofKilobytes(8));\n-    data.setLogIndexDensity(5);\n-    brokerCfg.getNetwork().setMaxMessageSize(DataSize.ofKilobytes(8));\n-\n-    final ExporterCfg exporterCfg = new ExporterCfg();\n-    exporterCfg.setClassName(ControllableExporter.class.getName());\n-    brokerCfg.setExporters(Collections.singletonMap(\"snapshot-test-exporter\", exporterCfg));\n+  @After\n+  public void cleanUp() {\n+    ControllableExporter.updatePosition(true);\n+    ControllableExporter.EXPORTED_RECORDS.set(0);\n+    ControllableExporter.RECORD_TYPE_FILTER.set(r -> true);\n+    ControllableExporter.VALUE_TYPE_FILTER.set(r -> true);\n   }\n \n   @Test\n-  public void shouldCompactEvenIfSkippingAllRecords() {\n-    // given\n-    final Broker broker = clusteringRule.getBroker(0);\n-\n-    // when\n+  public void shouldCompactEvenIfSkippingAllRecordsInitially() {\n+    // given - an exporter which does not update its own position and filters everything but\n+    // deployment commands\n+    final LogStream logStream = clusteringRule.getLogStream(1);\n+    final LogStreamReader reader = logStream.newLogStreamReader().join();\n     ControllableExporter.updatePosition(false);\n-    ControllableExporter.RECORD_TYPE_FILTER.set(r -> r == RecordType.COMMAND);\n-    ControllableExporter.VALUE_TYPE_FILTER.set(r -> r == ValueType.DEPLOYMENT);\n-    writeSegments(broker, 2);\n-    clusteringRule\n-        .getClient()\n-        .newDeployCommand()\n-        .addWorkflowModel(\n-            Bpmn.createExecutableProcess(\"process\").startEvent().done(), \"process.bpmn\")\n-        .send()\n-        .join();\n+    ControllableExporter.RECORD_TYPE_FILTER.set(t -> t == RecordType.COMMAND);\n+    ControllableExporter.VALUE_TYPE_FILTER.set(t -> t == ValueType.DEPLOYMENT);\n+\n+    // when - filling up the log with messages and NO deployments\n+    publishEnoughMessagesForCompaction();\n+    deployDummyProcess();\n     await(\"until at least one record is exported\")\n         .atMost(Duration.ofSeconds(5))\n         .untilAsserted(\n             () -> assertThat(ControllableExporter.EXPORTED_RECORDS).hasValueGreaterThan(0));\n \n-    // enforce compaction\n-    final var segmentsBeforeSnapshot = getSegmentsCount(broker);\n-    clusteringRule.getClock().addTime(SNAPSHOT_PERIOD);\n+    // memorize first position pre compaction to compare later on\n+    reader.seekToFirstEvent();\n+    final long firstPositionPreCompaction = reader.getPosition();\n \n-    // then\n-    assertThat(clusteringRule.waitForSnapshotAtBroker(broker)).isNotNull();\n-    await()\n+    // then - enforce compaction and make sure we have less records than we previously did\n+    clusteringRule.getClock().addTime(SNAPSHOT_PERIOD);\n+    clusteringRule.waitForSnapshotAtBroker(clusteringRule.getBroker(0));\n+    await(\"until some data before the deployment command was compacted\")\n         .untilAsserted(\n-            () ->\n-                assertThat(getSegmentsCount(broker))\n-                    .describedAs(\"Expected less segments after a snapshot is taken\")\n-                    .isLessThan(segmentsBeforeSnapshot));\n+            () -> {\n+              reader.seekToFirstEvent();\n+              final long firstPositionPostCompaction = reader.getPosition();\n+              assertThat(firstPositionPostCompaction).isGreaterThan(firstPositionPreCompaction);\n+              assertContainsDeploymentCommand(reader);\n+            });\n   }\n \n   @Test\n   public void shouldNotCompactUnacknowledgedEventsEvenIfSkipping() {\n-    // given\n-    final RecordMetadata metadata = new RecordMetadata();\n-    final Broker broker = clusteringRule.getBroker(0);\n-    final var logstream = clusteringRule.getLogStream(1);\n-    final var reader = logstream.newLogStreamReader().join();\n-\n-    // when\n+    // given - an exporter which does not update its own position and only accepts deployment\n+    // commands\n+    final LogStream logStream = clusteringRule.getLogStream(1);\n+    final LogStreamReader reader = logStream.newLogStreamReader().join();\n     ControllableExporter.updatePosition(false);\n-    ControllableExporter.RECORD_TYPE_FILTER.set(r -> r == RecordType.COMMAND);\n-    ControllableExporter.VALUE_TYPE_FILTER.set(r -> r == ValueType.DEPLOYMENT);\n-    writeSegments(broker, 2);\n-    clusteringRule\n-        .getClient()\n-        .newDeployCommand()\n-        .addWorkflowModel(\n-            Bpmn.createExecutableProcess(\"process\").startEvent().done(), \"process.bpmn\")\n-        .send()\n-        .join();\n-    writeSegments(broker, 2);\n+    ControllableExporter.RECORD_TYPE_FILTER.set(t -> t == RecordType.COMMAND);\n+    ControllableExporter.VALUE_TYPE_FILTER.set(t -> t == ValueType.DEPLOYMENT);\n+\n+    // when - filling up the log with messages and a single deployment\n+    publishEnoughMessagesForCompaction();\n+    deployDummyProcess();\n+    publishEnoughMessagesForCompaction();\n     await(\"until at least one record is exported\")\n         .atMost(Duration.ofSeconds(5))\n         .untilAsserted(\n             () -> assertThat(ControllableExporter.EXPORTED_RECORDS).hasValueGreaterThan(0));\n \n-    // grab the first log position, and the position of the last unacknowledged event\n+    // memorize first position pre compaction to compare later on\n     reader.seekToFirstEvent();\n-    final long firstPosition = reader.getPosition();\n-    long lastUnacknowledgedPosition = -1;\n-    while (reader.hasNext()) {\n-      final LoggedEvent event = reader.next();\n-      event.readMetadata(metadata);\n-      if (metadata.getValueType() == ValueType.DEPLOYMENT) {\n-        lastUnacknowledgedPosition = event.getPosition();\n-        break;\n-      }\n-    }\n+    final long firstPositionPreCompaction = reader.getPosition();\n \n-    // enforce compaction\n-    final var segmentsBeforeSnapshot = getSegmentsCount(broker);\n+    // then - enforce compaction and ensure the accepted deployment is still present on the log\n+    // after compaction\n     clusteringRule.getClock().addTime(SNAPSHOT_PERIOD);\n-\n-    // then\n-    assertThat(lastUnacknowledgedPosition).isGreaterThan(-1L);\n-    assertThat(clusteringRule.waitForSnapshotAtBroker(broker)).isNotNull();\n-    await()\n+    clusteringRule.waitForSnapshotAtBroker(clusteringRule.getBroker(0));\n+    await(\"until some data before the deployment command was compacted\")\n+        .atMost(Duration.ofSeconds(5))\n         .untilAsserted(\n-            () ->\n-                assertThat(getSegmentsCount(broker))\n-                    .describedAs(\"Expected less segments after a snapshot is taken\")\n-                    .isLessThan(segmentsBeforeSnapshot));\n-    reader.seekToFirstEvent();\n-    assertThat(reader.getPosition())\n-        .isGreaterThan(firstPosition)\n-        .isLessThanOrEqualTo(lastUnacknowledgedPosition);\n+            () -> {\n+              reader.seekToFirstEvent();\n+              final long firstPositionPostCompaction = reader.getPosition();\n+              assertThat(firstPositionPostCompaction).isGreaterThan(firstPositionPreCompaction);\n+              assertContainsDeploymentCommand(reader);\n+            });\n   }\n \n   @Test\n   public void shouldNotCompactNotExportedEvents() {\n     // given\n+    final LogStream logStream = clusteringRule.getLogStream(1);\n+    final LogStreamReader reader = logStream.newLogStreamReader().join();\n     final Broker broker = clusteringRule.getBroker(0);\n-\n-    final var logstream = clusteringRule.getLogStream(1);\n-    final var reader = logstream.newLogStreamReader().join();\n-\n-    // - write records and update the exporter position\n     ControllableExporter.updatePosition(true);\n-    fillSegments(broker, SEGMENT_COUNT);\n \n-    // - write more records but don't update the exporter position\n+    // when - filling the log with messages (updating the position), then a single deployment\n+    // command, and more messages (all of which do not update the position)\n+    publishEnoughMessagesForCompaction();\n     ControllableExporter.updatePosition(false);\n+    deployDummyProcess();\n+    publishEnoughMessagesForCompaction();\n \n-    final var filledSegmentCount = SEGMENT_COUNT * 2;\n-    fillSegments(broker, filledSegmentCount);\n-\n-    // - trigger a snapshot creation\n+    // then - force compaction and ensure we compacted only things before our sentinel command\n+    reader.seekToFirstEvent();\n+    long firstPositionPreCompaction = reader.getPosition();\n     clusteringRule.getClock().addTime(SNAPSHOT_PERIOD);\n-    final var firstSnapshot = clusteringRule.waitForSnapshotAtBroker(broker);\n-\n-    await()\n-        .untilAsserted(\n-            () ->\n-                assertThat(getSegmentsCount(broker))\n-                    .describedAs(\"Expected less segments after a snapshot is taken\")\n-                    .isLessThan(filledSegmentCount));\n-\n-    // then verify that the log still contains the records that are not exported\n-    final var firstNonExportedPosition =\n-        ControllableExporter.NOT_EXPORTED_RECORDS.get(0).getPosition();\n-\n-    assertThat(hasRecordWithPosition(reader, firstNonExportedPosition))\n-        .describedAs(\"Expected first non-exported record to be present in the log but not found.\")\n-        .isTrue();\n-\n-    // - write more records and update the exporter position again\n-    final var segmentsBeforeSnapshot = getSegmentsCount(broker);\n+    final FileBasedSnapshotMetadata firstSnapshot = clusteringRule.waitForSnapshotAtBroker(broker);\n+    awaitUntilCompaction(reader, firstPositionPreCompaction);\n+    assertContainsDeploymentCommand(reader);\n \n+    // when - re-enabling updating the position\n     ControllableExporter.updatePosition(true);\n-    fillSegments(broker, segmentsBeforeSnapshot + 1);\n+    publishEnoughMessagesForCompaction();\n \n-    // - trigger the next snapshot creation\n+    // then - ensure we can still compact\n+    reader.seekToFirstEvent();\n+    firstPositionPreCompaction = reader.getPosition();\n     clusteringRule.getClock().addTime(SNAPSHOT_PERIOD);\n     clusteringRule.waitForNewSnapshotAtBroker(broker, firstSnapshot);\n-\n-    // then verify that the log is now compacted after the exporter position was updated\n-    await()\n-        .untilAsserted(\n-            () ->\n-                assertThat(getSegmentsCount(broker))\n-                    .describedAs(\"Expected less segments after a snapshot is taken\")\n-                    .isLessThan(segmentsBeforeSnapshot));\n+    awaitUntilCompaction(reader, firstPositionPreCompaction);\n   }\n \n   @Test\n   public void shouldCompactWhenExporterHasBeenRemoved() {\n-    // given\n-    final Broker broker = clusteringRule.getBroker(0);\n+    // given - an exporter which updates its position and accepts all records\n+    final int nodeId = 0;\n+    LogStreamReader reader = clusteringRule.getLogStream(1).newLogStreamReader().join();\n+    final Broker broker = clusteringRule.getBroker(nodeId);\n     ControllableExporter.updatePosition(true);\n-    fillSegments(broker, SEGMENT_COUNT);\n-    clusteringRule.getClock().addTime(SNAPSHOT_PERIOD);\n-    // create first snapshot with exporter positions\n-    final var firstSnapshot = clusteringRule.waitForSnapshotAtBroker(broker);\n-\n-    // restart with no exporter\n-    final var brokerCfg = clusteringRule.getBrokerCfg(0);\n-    brokerCfg.setExporters(Map.of());\n-    clusteringRule.stopBroker(0);\n-    clusteringRule.startBroker(0);\n \n-    final var filledSegmentCount = SEGMENT_COUNT * 2;\n-    writeSegments(broker, filledSegmentCount);\n+    // when - filling the log with messages, and a single deployment command for which we will not\n+    // update the position\n+    publishEnoughMessagesForCompaction();\n+    ControllableExporter.updatePosition(false);\n+    deployDummyProcess();\n \n-    // when triggering new snapshot creation\n-    final var segmentsCount = getSegmentsCount(broker);\n+    // then - force compaction and ensure we compacted only things before our sentinel command\n+    reader.seekToFirstEvent();\n+    final long firstPositionPreCompaction = reader.getPosition();\n+    clusteringRule.getClock().addTime(SNAPSHOT_PERIOD);\n+    final FileBasedSnapshotMetadata firstSnapshot = clusteringRule.waitForSnapshotAtBroker(broker);\n+    awaitUntilCompaction(reader, firstPositionPreCompaction);\n+    assertContainsDeploymentCommand(reader);\n+\n+    // when - restarting without the exporter\n+    final var brokerCfg = clusteringRule.getBrokerCfg(nodeId);\n+    brokerCfg.setExporters(Collections.emptyMap());\n+    clusteringRule.stopBroker(nodeId);\n+    clusteringRule.startBroker(nodeId);\n+    publishEnoughMessagesForCompaction();\n+\n+    // then - force compaction, and expect the deployment command to have been removed\n+    reader = clusteringRule.getLogStream(1).newLogStreamReader().join();\n+    final long newFirstPositionPreCompaction = reader.getPosition();\n     clusteringRule.getClock().addTime(SNAPSHOT_PERIOD);\n-    final var secondSnapshot = clusteringRule.waitForNewSnapshotAtBroker(broker, firstSnapshot);\n+    clusteringRule.waitForNewSnapshotAtBroker(broker, firstSnapshot);\n+    assertThat(newFirstPositionPreCompaction).isGreaterThan(firstPositionPreCompaction);\n+    awaitUntilCompaction(reader, newFirstPositionPreCompaction);\n+    assertDoesNotContainDeploymentCommand(reader);\n+  }\n \n-    // then\n-    assertThat(firstSnapshot).isNotEqualTo(secondSnapshot);\n-    await()\n+  private void awaitUntilCompaction(\n+      final LogStreamReader reader, final long firstPositionPreCompaction) {\n+    await(\"until some data was compacted\")\n+        .atMost(Duration.ofSeconds(5))\n         .untilAsserted(\n-            () ->\n-                assertThat(getSegmentsCount(broker))\n-                    .describedAs(\"Expected less segments after a snapshot is taken\")\n-                    .isLessThan(segmentsCount));\n+            () -> {\n+              reader.seekToFirstEvent();\n+              final long firstPositionPostCompaction = reader.getPosition();\n+              assertThat(firstPositionPostCompaction).isGreaterThan(firstPositionPreCompaction);\n+            });\n   }\n \n-  private void fillSegments(final Broker broker, final int segmentCount) {\n-    writeSegments(broker, segmentCount);\n-\n-    await()\n-        .untilAsserted(\n-            () ->\n-                assertThat(ControllableExporter.EXPORTED_RECORDS.get())\n-                    .describedAs(\"Expected all written records to be exported\")\n-                    .isGreaterThanOrEqualTo(writtenRecords.get()));\n+  private void deployDummyProcess() {\n+    clusteringRule\n+        .getClient()\n+        .newDeployCommand()\n+        .addWorkflowModel(\n+            Bpmn.createExecutableProcess(\"process\").startEvent().done(), \"process.bpmn\")\n+        .send()\n+        .join();\n   }\n \n-  private void writeSegments(final Broker broker, final int segmentCount) {\n-    while (getSegmentsCount(broker) <= segmentCount) {\n-      writeToLog();\n-      writtenRecords.incrementAndGet();\n-    }\n+  private void configureBroker(final BrokerCfg brokerCfg) {\n+    final DataCfg data = brokerCfg.getData();\n+    data.setSnapshotPeriod(SNAPSHOT_PERIOD);\n+    data.setLogSegmentSize(LOG_SEGMENT_SIZE);\n+    data.setLogIndexDensity(5);\n+    brokerCfg.getNetwork().setMaxMessageSize(MAX_MESSAGE_SIZE);\n+\n+    final ExporterCfg exporterCfg = new ExporterCfg();\n+    exporterCfg.setClassName(ControllableExporter.class.getName());\n+    brokerCfg.setExporters(Collections.singletonMap(\"snapshot-test-exporter\", exporterCfg));\n   }\n \n-  private void writeToLog() {\n+  private void publishEnoughMessagesForCompaction() {\n+    final int requiredMessageCount = (int) LOG_SEGMENT_SIZE.toBytes() / LARGE_VARIABLE_SIZE;\n+    IntStream.range(0, requiredMessageCount + 1).forEach(this::publishMaxMessageSizeMessage);\n+  }\n \n+  private void publishMaxMessageSizeMessage(final int key) {\n     clusteringRule\n         .getClient()\n         .newPublishMessageCommand()\n         .messageName(\"msg\")\n-        .correlationKey(\"key\")\n+        .correlationKey(\"msg-\" + key)\n+        .variables(Map.of(\"foo\", MAX_MESSAGE_SIZE_VARIABLE))\n         .send()\n         .join();\n   }\n \n-  private int getSegmentsCount(final Broker broker) {\n-    return getSegments(broker).size();\n+  @SuppressWarnings(\"rawtypes\")\n+  private Stream<Record> newRecordStream(final LogStreamReader reader) {\n+    final Spliterator<LoggedEvent> spliterator =\n+        Spliterators.spliteratorUnknownSize(reader, Spliterator.ORDERED);\n+    return StreamSupport.stream(spliterator, false)\n+        .map(event -> CopiedRecords.createCopiedRecord(1, event));\n   }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTgwNzAzNw=="}, "originalCommit": {"oid": "2549378f6d9803bdffbca6f9c5ceffd364782774"}, "originalPosition": 431}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEwMzc2NjczOnYy", "diffSide": "RIGHT", "path": "qa/integration-tests/src/test/java/io/zeebe/broker/it/clustering/SingleBrokerDataDeletionTest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQwOTozMTo1MFrOHY1wBA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQwOTozMTo1MFrOHY1wBA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTgwODUxNg==", "bodyText": "Replace the magic number with a constant. Or, extract clusteringRule.getLogStream(1).newLogStreamReader().join() into a new method.\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                final LogStream logStream = clusteringRule.getLogStream(1);\n          \n          \n            \n                final LogStream logStream = clusteringRule.getLogStream(PARTITION_ID);", "url": "https://github.com/camunda-cloud/zeebe/pull/5433#discussion_r495808516", "createdAt": "2020-09-28T09:31:50Z", "author": {"login": "saig0"}, "path": "qa/integration-tests/src/test/java/io/zeebe/broker/it/clustering/SingleBrokerDataDeletionTest.java", "diffHunk": "@@ -47,284 +47,247 @@\n public class SingleBrokerDataDeletionTest {\n \n   private static final Duration SNAPSHOT_PERIOD = Duration.ofMinutes(5);\n-  private static final int SEGMENT_COUNT = 5;\n+  private static final DataSize LOG_SEGMENT_SIZE = DataSize.ofKilobytes(8);\n+  private static final DataSize MAX_MESSAGE_SIZE = DataSize.ofKilobytes(4);\n+  // variable has to be a bit smaller than max message size otherwise it will be rejected\n+  private static final int LARGE_VARIABLE_SIZE = (int) MAX_MESSAGE_SIZE.toBytes() / 2;\n+  private static final String MAX_MESSAGE_SIZE_VARIABLE = \"x\".repeat(LARGE_VARIABLE_SIZE);\n \n   @Rule\n-  public final ClusteringRule clusteringRule =\n-      new ClusteringRule(1, 1, 1, this::configureCustomExporter);\n+  public final ClusteringRule clusteringRule = new ClusteringRule(1, 1, 1, this::configureBroker);\n \n-  private final AtomicLong writtenRecords = new AtomicLong(0);\n-\n-  private void configureCustomExporter(final BrokerCfg brokerCfg) {\n-    final DataCfg data = brokerCfg.getData();\n-    data.setSnapshotPeriod(SNAPSHOT_PERIOD);\n-    data.setLogSegmentSize(DataSize.ofKilobytes(8));\n-    data.setLogIndexDensity(5);\n-    brokerCfg.getNetwork().setMaxMessageSize(DataSize.ofKilobytes(8));\n-\n-    final ExporterCfg exporterCfg = new ExporterCfg();\n-    exporterCfg.setClassName(ControllableExporter.class.getName());\n-    brokerCfg.setExporters(Collections.singletonMap(\"snapshot-test-exporter\", exporterCfg));\n+  @After\n+  public void cleanUp() {\n+    ControllableExporter.updatePosition(true);\n+    ControllableExporter.EXPORTED_RECORDS.set(0);\n+    ControllableExporter.RECORD_TYPE_FILTER.set(r -> true);\n+    ControllableExporter.VALUE_TYPE_FILTER.set(r -> true);\n   }\n \n   @Test\n-  public void shouldCompactEvenIfSkippingAllRecords() {\n-    // given\n-    final Broker broker = clusteringRule.getBroker(0);\n-\n-    // when\n+  public void shouldCompactEvenIfSkippingAllRecordsInitially() {\n+    // given - an exporter which does not update its own position and filters everything but\n+    // deployment commands\n+    final LogStream logStream = clusteringRule.getLogStream(1);\n+    final LogStreamReader reader = logStream.newLogStreamReader().join();\n     ControllableExporter.updatePosition(false);\n-    ControllableExporter.RECORD_TYPE_FILTER.set(r -> r == RecordType.COMMAND);\n-    ControllableExporter.VALUE_TYPE_FILTER.set(r -> r == ValueType.DEPLOYMENT);\n-    writeSegments(broker, 2);\n-    clusteringRule\n-        .getClient()\n-        .newDeployCommand()\n-        .addWorkflowModel(\n-            Bpmn.createExecutableProcess(\"process\").startEvent().done(), \"process.bpmn\")\n-        .send()\n-        .join();\n+    ControllableExporter.RECORD_TYPE_FILTER.set(t -> t == RecordType.COMMAND);\n+    ControllableExporter.VALUE_TYPE_FILTER.set(t -> t == ValueType.DEPLOYMENT);\n+\n+    // when - filling up the log with messages and NO deployments\n+    publishEnoughMessagesForCompaction();\n+    deployDummyProcess();\n     await(\"until at least one record is exported\")\n         .atMost(Duration.ofSeconds(5))\n         .untilAsserted(\n             () -> assertThat(ControllableExporter.EXPORTED_RECORDS).hasValueGreaterThan(0));\n \n-    // enforce compaction\n-    final var segmentsBeforeSnapshot = getSegmentsCount(broker);\n-    clusteringRule.getClock().addTime(SNAPSHOT_PERIOD);\n+    // memorize first position pre compaction to compare later on\n+    reader.seekToFirstEvent();\n+    final long firstPositionPreCompaction = reader.getPosition();\n \n-    // then\n-    assertThat(clusteringRule.waitForSnapshotAtBroker(broker)).isNotNull();\n-    await()\n+    // then - enforce compaction and make sure we have less records than we previously did\n+    clusteringRule.getClock().addTime(SNAPSHOT_PERIOD);\n+    clusteringRule.waitForSnapshotAtBroker(clusteringRule.getBroker(0));\n+    await(\"until some data before the deployment command was compacted\")\n         .untilAsserted(\n-            () ->\n-                assertThat(getSegmentsCount(broker))\n-                    .describedAs(\"Expected less segments after a snapshot is taken\")\n-                    .isLessThan(segmentsBeforeSnapshot));\n+            () -> {\n+              reader.seekToFirstEvent();\n+              final long firstPositionPostCompaction = reader.getPosition();\n+              assertThat(firstPositionPostCompaction).isGreaterThan(firstPositionPreCompaction);\n+              assertContainsDeploymentCommand(reader);\n+            });\n   }\n \n   @Test\n   public void shouldNotCompactUnacknowledgedEventsEvenIfSkipping() {\n-    // given\n-    final RecordMetadata metadata = new RecordMetadata();\n-    final Broker broker = clusteringRule.getBroker(0);\n-    final var logstream = clusteringRule.getLogStream(1);\n-    final var reader = logstream.newLogStreamReader().join();\n-\n-    // when\n+    // given - an exporter which does not update its own position and only accepts deployment\n+    // commands\n+    final LogStream logStream = clusteringRule.getLogStream(1);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2549378f6d9803bdffbca6f9c5ceffd364782774"}, "originalPosition": 148}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 277, "cost": 1, "resetAt": "2021-11-12T13:16:51Z"}}}