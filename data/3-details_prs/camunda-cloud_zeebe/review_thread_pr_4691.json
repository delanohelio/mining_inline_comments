{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDMxNzkwMjAx", "number": 4691, "reviewThreads": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQxMjowNTowN1rOEFCPJg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQxMjowNTowN1rOEFCPJg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczNzE0OTgyOnYy", "diffSide": "RIGHT", "path": "exporters/elasticsearch-exporter/src/main/java/io/zeebe/exporter/ElasticsearchClient.java", "isResolved": false, "comments": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQxMjowNTowN1rOGjBjOw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNVQxMzo1NzowN1rOGjzWPQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTM3ODc0Nw==", "bodyText": "@saig0 I don't think it is a good idea to log every failed item, the reason is that we have thousands of records in a bulk request in a high load scenario. As far as I remember initial we had a similar approach in the first implementation which we saw as a potential problem with log pollution, see #1344 (comment).\n/cc @npepinpe wdyt?", "url": "https://github.com/camunda-cloud/zeebe/pull/4691#discussion_r439378747", "createdAt": "2020-06-12T12:05:07Z", "author": {"login": "menski"}, "path": "exporters/elasticsearch-exporter/src/main/java/io/zeebe/exporter/ElasticsearchClient.java", "diffHunk": "@@ -136,21 +140,36 @@ public boolean flush() {\n         bulkRequest = new ArrayList<>();\n       }\n     }\n-\n     return success;\n   }\n \n-  private boolean exportBulk() throws IOException {\n+  private boolean checkBulkResponse(final BulkResponse bulkResponse) {\n+    final var hasErrors = bulkResponse.hasErrors();\n+    if (hasErrors) {\n+      bulkResponse.getItems().stream()\n+          .map(BulkItem::getIndex)\n+          .map(BulkItemIndex::getError)\n+          .forEach(\n+              error ->\n+                  log.warn(\n+                      \"Failed to flush item of bulk request [type: {}, reason: {}]\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07813afe030084accd15d5f24688cfe89211591c"}, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTM4MjA4OQ==", "bodyText": "This is again a case where we would benefit from having a burst filter for errors, a way to log an error max X times over Y seconds; we have several other places where we could benefit from this.\nAs it is I agree, if ES is down this will create tons of logs, and it just creates noise. Suppressing it however hides an issue, so...maybe we can find a middle ground? Log the error every X errors?", "url": "https://github.com/camunda-cloud/zeebe/pull/4691#discussion_r439382089", "createdAt": "2020-06-12T12:13:36Z", "author": {"login": "npepinpe"}, "path": "exporters/elasticsearch-exporter/src/main/java/io/zeebe/exporter/ElasticsearchClient.java", "diffHunk": "@@ -136,21 +140,36 @@ public boolean flush() {\n         bulkRequest = new ArrayList<>();\n       }\n     }\n-\n     return success;\n   }\n \n-  private boolean exportBulk() throws IOException {\n+  private boolean checkBulkResponse(final BulkResponse bulkResponse) {\n+    final var hasErrors = bulkResponse.hasErrors();\n+    if (hasErrors) {\n+      bulkResponse.getItems().stream()\n+          .map(BulkItem::getIndex)\n+          .map(BulkItemIndex::getError)\n+          .forEach(\n+              error ->\n+                  log.warn(\n+                      \"Failed to flush item of bulk request [type: {}, reason: {}]\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTM3ODc0Nw=="}, "originalCommit": {"oid": "07813afe030084accd15d5f24688cfe89211591c"}, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTM5NDE1MQ==", "bodyText": "As a side note we also had the topic yesterday in the cloud product meeting that Operate is logging to much in case elastic is unavailable, this is also which would trigger an immense log burst with this code.\nI would highly recommend to not release this as it is right now as this will immediately lead to problems in cloud.", "url": "https://github.com/camunda-cloud/zeebe/pull/4691#discussion_r439394151", "createdAt": "2020-06-12T12:41:09Z", "author": {"login": "menski"}, "path": "exporters/elasticsearch-exporter/src/main/java/io/zeebe/exporter/ElasticsearchClient.java", "diffHunk": "@@ -136,21 +140,36 @@ public boolean flush() {\n         bulkRequest = new ArrayList<>();\n       }\n     }\n-\n     return success;\n   }\n \n-  private boolean exportBulk() throws IOException {\n+  private boolean checkBulkResponse(final BulkResponse bulkResponse) {\n+    final var hasErrors = bulkResponse.hasErrors();\n+    if (hasErrors) {\n+      bulkResponse.getItems().stream()\n+          .map(BulkItem::getIndex)\n+          .map(BulkItemIndex::getError)\n+          .forEach(\n+              error ->\n+                  log.warn(\n+                      \"Failed to flush item of bulk request [type: {}, reason: {}]\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTM3ODc0Nw=="}, "originalCommit": {"oid": "07813afe030084accd15d5f24688cfe89211591c"}, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTM5NjU4MA==", "bodyText": "Agree here with @menski maybe we can have a metric which count flash fails or something similar. And yes something like a burst filter would be nice.", "url": "https://github.com/camunda-cloud/zeebe/pull/4691#discussion_r439396580", "createdAt": "2020-06-12T12:45:54Z", "author": {"login": "Zelldon"}, "path": "exporters/elasticsearch-exporter/src/main/java/io/zeebe/exporter/ElasticsearchClient.java", "diffHunk": "@@ -136,21 +140,36 @@ public boolean flush() {\n         bulkRequest = new ArrayList<>();\n       }\n     }\n-\n     return success;\n   }\n \n-  private boolean exportBulk() throws IOException {\n+  private boolean checkBulkResponse(final BulkResponse bulkResponse) {\n+    final var hasErrors = bulkResponse.hasErrors();\n+    if (hasErrors) {\n+      bulkResponse.getItems().stream()\n+          .map(BulkItem::getIndex)\n+          .map(BulkItemIndex::getError)\n+          .forEach(\n+              error ->\n+                  log.warn(\n+                      \"Failed to flush item of bulk request [type: {}, reason: {}]\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTM3ODc0Nw=="}, "originalCommit": {"oid": "07813afe030084accd15d5f24688cfe89211591c"}, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTM5Nzk4Mg==", "bodyText": "Agrona has a counters buffer which keeps track of errors and how often they happen.I guess this works well if each error gets a specific code or something, not sure how they deal with context. That said, I also don't think it plays well with logging systems; my point here is that it's definitely a common problem, so I'm sure there's a solution out there", "url": "https://github.com/camunda-cloud/zeebe/pull/4691#discussion_r439397982", "createdAt": "2020-06-12T12:48:52Z", "author": {"login": "npepinpe"}, "path": "exporters/elasticsearch-exporter/src/main/java/io/zeebe/exporter/ElasticsearchClient.java", "diffHunk": "@@ -136,21 +140,36 @@ public boolean flush() {\n         bulkRequest = new ArrayList<>();\n       }\n     }\n-\n     return success;\n   }\n \n-  private boolean exportBulk() throws IOException {\n+  private boolean checkBulkResponse(final BulkResponse bulkResponse) {\n+    final var hasErrors = bulkResponse.hasErrors();\n+    if (hasErrors) {\n+      bulkResponse.getItems().stream()\n+          .map(BulkItem::getIndex)\n+          .map(BulkItemIndex::getError)\n+          .forEach(\n+              error ->\n+                  log.warn(\n+                      \"Failed to flush item of bulk request [type: {}, reason: {}]\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTM3ODc0Nw=="}, "originalCommit": {"oid": "07813afe030084accd15d5f24688cfe89211591c"}, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTk4ODI1MQ==", "bodyText": "If ES is not reachable then the log statement is not printed. Instead, an exception is thrown and the exporter gets back-off.\nTo avoid too much log messages, I could do the following:\n\nlog message only once per type for one bulk flush operation\nthrow an exception if the flush has errors to get back-off\n\nWhat do you think about these changes?", "url": "https://github.com/camunda-cloud/zeebe/pull/4691#discussion_r439988251", "createdAt": "2020-06-15T07:46:32Z", "author": {"login": "saig0"}, "path": "exporters/elasticsearch-exporter/src/main/java/io/zeebe/exporter/ElasticsearchClient.java", "diffHunk": "@@ -136,21 +140,36 @@ public boolean flush() {\n         bulkRequest = new ArrayList<>();\n       }\n     }\n-\n     return success;\n   }\n \n-  private boolean exportBulk() throws IOException {\n+  private boolean checkBulkResponse(final BulkResponse bulkResponse) {\n+    final var hasErrors = bulkResponse.hasErrors();\n+    if (hasErrors) {\n+      bulkResponse.getItems().stream()\n+          .map(BulkItem::getIndex)\n+          .map(BulkItemIndex::getError)\n+          .forEach(\n+              error ->\n+                  log.warn(\n+                      \"Failed to flush item of bulk request [type: {}, reason: {}]\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTM3ODc0Nw=="}, "originalCommit": {"oid": "07813afe030084accd15d5f24688cfe89211591c"}, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTk5NDk5Mw==", "bodyText": "Let's go for that \ud83d\udc4d", "url": "https://github.com/camunda-cloud/zeebe/pull/4691#discussion_r439994993", "createdAt": "2020-06-15T07:58:56Z", "author": {"login": "npepinpe"}, "path": "exporters/elasticsearch-exporter/src/main/java/io/zeebe/exporter/ElasticsearchClient.java", "diffHunk": "@@ -136,21 +140,36 @@ public boolean flush() {\n         bulkRequest = new ArrayList<>();\n       }\n     }\n-\n     return success;\n   }\n \n-  private boolean exportBulk() throws IOException {\n+  private boolean checkBulkResponse(final BulkResponse bulkResponse) {\n+    final var hasErrors = bulkResponse.hasErrors();\n+    if (hasErrors) {\n+      bulkResponse.getItems().stream()\n+          .map(BulkItem::getIndex)\n+          .map(BulkItemIndex::getError)\n+          .forEach(\n+              error ->\n+                  log.warn(\n+                      \"Failed to flush item of bulk request [type: {}, reason: {}]\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTM3ODc0Nw=="}, "originalCommit": {"oid": "07813afe030084accd15d5f24688cfe89211591c"}, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDE5NDYyMQ==", "bodyText": "Will be fixed by #4732", "url": "https://github.com/camunda-cloud/zeebe/pull/4691#discussion_r440194621", "createdAt": "2020-06-15T13:57:07Z", "author": {"login": "saig0"}, "path": "exporters/elasticsearch-exporter/src/main/java/io/zeebe/exporter/ElasticsearchClient.java", "diffHunk": "@@ -136,21 +140,36 @@ public boolean flush() {\n         bulkRequest = new ArrayList<>();\n       }\n     }\n-\n     return success;\n   }\n \n-  private boolean exportBulk() throws IOException {\n+  private boolean checkBulkResponse(final BulkResponse bulkResponse) {\n+    final var hasErrors = bulkResponse.hasErrors();\n+    if (hasErrors) {\n+      bulkResponse.getItems().stream()\n+          .map(BulkItem::getIndex)\n+          .map(BulkItemIndex::getError)\n+          .forEach(\n+              error ->\n+                  log.warn(\n+                      \"Failed to flush item of bulk request [type: {}, reason: {}]\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTM3ODc0Nw=="}, "originalCommit": {"oid": "07813afe030084accd15d5f24688cfe89211591c"}, "originalPosition": 63}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 501, "cost": 1, "resetAt": "2021-11-12T13:16:51Z"}}}