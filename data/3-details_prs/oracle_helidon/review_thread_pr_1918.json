{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDI2MzgwMjgy", "number": 1918, "reviewThreads": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQxMzoyNzowNVrOEErhtw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQxMzoyODozMlrOEErkPA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczMzQyOTAzOnYy", "diffSide": "RIGHT", "path": "tests/integration/kafka/src/test/java/io/helidon/messaging/connectors/kafka/KafkaSeTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQxMzoyNzowNlrOGic9Ug==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQxMjoxOToyOVrOGjB51w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODc3OTIxOA==", "bodyText": "This config seems not to be used anywhere", "url": "https://github.com/oracle/helidon/pull/1918#discussion_r438779218", "createdAt": "2020-06-11T13:27:06Z", "author": {"login": "danielkec"}, "path": "tests/integration/kafka/src/test/java/io/helidon/messaging/connectors/kafka/KafkaSeTest.java", "diffHunk": "@@ -334,4 +349,107 @@ void kafkaHeaderTest() throws InterruptedException {\n             messaging.stop();\n         }\n     }\n+\n+    @Test\n+    void someEventsNoAckWithOnePartition() {\n+        LOGGER.fine(() -> \"==========> test someEventsNoAckWithOnePartition()\");\n+        final String GROUP = \"group_1\";\n+        final String TOPIC = TEST_SE_TOPIC_6;\n+        Channel<String> fromKafka = Channel.<String>builder()\n+                .name(\"from-kafka\")\n+                .publisherConfig(KafkaConnector.configBuilder()\n+                        .bootstrapServers(KAFKA_SERVER)\n+                        .groupId(GROUP)\n+                        .topic(TOPIC)\n+                        .autoOffsetReset(KafkaConfigBuilder.AutoOffsetReset.EARLIEST)\n+                        .enableAutoCommit(false)\n+                        .keyDeserializer(LongDeserializer.class)\n+                        .valueDeserializer(StringDeserializer.class)\n+                        .build()\n+                )\n+                .build();\n+        List<String> uncommit = new ArrayList<>();\n+        Channel6 kafkaConsumingBean = new Channel6();\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(\"bootstrap.servers\", KAFKA_SERVER);\n+        config.put(\"key.serializer\", LongSerializer.class.getName());\n+        config.put(\"value.serializer\", StringSerializer.class.getName());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bc08aced21d655809d4fc2fa95f914f930d27159"}, "originalPosition": 94}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTM4NDUzNQ==", "bodyText": "I forgot to remove it. Now that config is in AbstractKafkaTest.produceAndCheck", "url": "https://github.com/oracle/helidon/pull/1918#discussion_r439384535", "createdAt": "2020-06-12T12:19:29Z", "author": {"login": "jbescos"}, "path": "tests/integration/kafka/src/test/java/io/helidon/messaging/connectors/kafka/KafkaSeTest.java", "diffHunk": "@@ -334,4 +349,107 @@ void kafkaHeaderTest() throws InterruptedException {\n             messaging.stop();\n         }\n     }\n+\n+    @Test\n+    void someEventsNoAckWithOnePartition() {\n+        LOGGER.fine(() -> \"==========> test someEventsNoAckWithOnePartition()\");\n+        final String GROUP = \"group_1\";\n+        final String TOPIC = TEST_SE_TOPIC_6;\n+        Channel<String> fromKafka = Channel.<String>builder()\n+                .name(\"from-kafka\")\n+                .publisherConfig(KafkaConnector.configBuilder()\n+                        .bootstrapServers(KAFKA_SERVER)\n+                        .groupId(GROUP)\n+                        .topic(TOPIC)\n+                        .autoOffsetReset(KafkaConfigBuilder.AutoOffsetReset.EARLIEST)\n+                        .enableAutoCommit(false)\n+                        .keyDeserializer(LongDeserializer.class)\n+                        .valueDeserializer(StringDeserializer.class)\n+                        .build()\n+                )\n+                .build();\n+        List<String> uncommit = new ArrayList<>();\n+        Channel6 kafkaConsumingBean = new Channel6();\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(\"bootstrap.servers\", KAFKA_SERVER);\n+        config.put(\"key.serializer\", LongSerializer.class.getName());\n+        config.put(\"value.serializer\", StringSerializer.class.getName());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODc3OTIxOA=="}, "originalCommit": {"oid": "bc08aced21d655809d4fc2fa95f914f930d27159"}, "originalPosition": 94}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczMzQzNTQ4OnYy", "diffSide": "RIGHT", "path": "tests/integration/kafka/src/test/java/io/helidon/messaging/connectors/kafka/KafkaSeTest.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQxMzoyODozMlrOGidBPQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQxMzoyODozMlrOGidBPQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODc4MDIyMQ==", "bodyText": "here too", "url": "https://github.com/oracle/helidon/pull/1918#discussion_r438780221", "createdAt": "2020-06-11T13:28:32Z", "author": {"login": "danielkec"}, "path": "tests/integration/kafka/src/test/java/io/helidon/messaging/connectors/kafka/KafkaSeTest.java", "diffHunk": "@@ -334,4 +349,107 @@ void kafkaHeaderTest() throws InterruptedException {\n             messaging.stop();\n         }\n     }\n+\n+    @Test\n+    void someEventsNoAckWithOnePartition() {\n+        LOGGER.fine(() -> \"==========> test someEventsNoAckWithOnePartition()\");\n+        final String GROUP = \"group_1\";\n+        final String TOPIC = TEST_SE_TOPIC_6;\n+        Channel<String> fromKafka = Channel.<String>builder()\n+                .name(\"from-kafka\")\n+                .publisherConfig(KafkaConnector.configBuilder()\n+                        .bootstrapServers(KAFKA_SERVER)\n+                        .groupId(GROUP)\n+                        .topic(TOPIC)\n+                        .autoOffsetReset(KafkaConfigBuilder.AutoOffsetReset.EARLIEST)\n+                        .enableAutoCommit(false)\n+                        .keyDeserializer(LongDeserializer.class)\n+                        .valueDeserializer(StringDeserializer.class)\n+                        .build()\n+                )\n+                .build();\n+        List<String> uncommit = new ArrayList<>();\n+        Channel6 kafkaConsumingBean = new Channel6();\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(\"bootstrap.servers\", KAFKA_SERVER);\n+        config.put(\"key.serializer\", LongSerializer.class.getName());\n+        config.put(\"value.serializer\", StringSerializer.class.getName());\n+        Messaging messaging = Messaging.builder().connector(KafkaConnector.create())\n+                .subscriber(fromKafka, ReactiveStreams.<KafkaMessage<Long, String>>builder()\n+                        .forEach(msg -> kafkaConsumingBean.onMsg(msg)))\n+                .build();\n+        try {\n+            messaging.start();\n+            // Push some messages that will ACK\n+            List<String> testData = IntStream.range(0, 20).mapToObj(i -> Integer.toString(i)).collect(Collectors.toList());\n+            produceAndCheck(kafkaConsumingBean, testData, TOPIC, testData);\n+            kafkaConsumingBean.restart();\n+            // Next message will not ACK\n+            testData = Arrays.asList(Channel6.NO_ACK);\n+            uncommit.addAll(testData);\n+            produceAndCheck(kafkaConsumingBean, testData, TOPIC, testData);\n+            kafkaConsumingBean.restart();\n+            // As this topic only have one partition, next messages will not ACK because previous message wasn't\n+            testData = IntStream.range(100, 120).mapToObj(i -> Integer.toString(i)).collect(Collectors.toList());\n+            uncommit.addAll(testData);\n+            produceAndCheck(kafkaConsumingBean, testData, TOPIC, testData);\n+        } finally {\n+            messaging.stop();\n+        }\n+        // We receive uncommitted messages again\n+        List<String> events = readTopic(TOPIC, uncommit.size(), GROUP);\n+        Collections.sort(events);\n+        Collections.sort(uncommit);\n+        assertEquals(uncommit, events);\n+    }\n+\n+    @Test\n+    void someEventsNoAckWithDifferentPartitions() {\n+        LOGGER.fine(() -> \"==========> test someEventsNoAckWithDifferentPartitions()\");\n+        final long FROM = 2000;\n+        final long TO = FROM + Channel8.LIMIT;\n+        final String GROUP = \"group_2\";\n+        final String TOPIC = TEST_SE_TOPIC_7;\n+        Channel<String> fromKafka = Channel.<String>builder()\n+                .name(\"from-kafka\")\n+                .publisherConfig(KafkaConnector.configBuilder()\n+                        .bootstrapServers(KAFKA_SERVER)\n+                        .groupId(GROUP)\n+                        .topic(TOPIC)\n+                        .autoOffsetReset(KafkaConfigBuilder.AutoOffsetReset.EARLIEST)\n+                        .enableAutoCommit(false)\n+                        .keyDeserializer(LongDeserializer.class)\n+                        .valueDeserializer(StringDeserializer.class)\n+                        .build()\n+                )\n+                .build();\n+        // Send the message that will not ACK. This will make in one partition to not commit any new message\n+        Channel8 kafkaConsumingBean = new Channel8();\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(\"bootstrap.servers\", KAFKA_SERVER);\n+        config.put(\"key.serializer\", LongSerializer.class.getName());\n+        config.put(\"value.serializer\", StringSerializer.class.getName());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bc08aced21d655809d4fc2fa95f914f930d27159"}, "originalPosition": 149}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 596, "cost": 1, "resetAt": "2021-11-13T12:10:21Z"}}}