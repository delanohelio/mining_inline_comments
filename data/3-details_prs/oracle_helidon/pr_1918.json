{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDI2MzgwMjgy", "number": 1918, "title": "Moving 2 tests from KafkaMP to KafkaSE", "bodyText": "This should solve the issues of that disabled 2 tests, because there is no other consumer in the moment we get the uncommitted events from Kafka.", "createdAt": "2020-06-02T06:35:52Z", "url": "https://github.com/oracle/helidon/pull/1918", "merged": true, "mergeCommit": {"oid": "fbf5984d9d80b5ad27fd493cdf7f68edd39338df"}, "closed": true, "closedAt": "2020-06-13T13:51:13Z", "author": {"login": "jbescos"}, "timelineItems": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcnCW4iAH2gAyNDI2MzgwMjgyOmJjMDhhY2VkMjFkNjU1ODA5ZDRmYzJmYTk1ZjkxNGY5MzBkMjcxNTk=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcq3yU5gFqTQzMDE0MDgxOQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "bc08aced21d655809d4fc2fa95f914f930d27159", "author": {"user": {"login": "jbescos", "name": "jbescos"}}, "url": "https://github.com/oracle/helidon/commit/bc08aced21d655809d4fc2fa95f914f930d27159", "committedDate": "2020-06-01T15:53:56Z", "message": "Moving 2 tests from KafkaMP to KafkaSE\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI4OTA1Mjk4", "url": "https://github.com/oracle/helidon/pull/1918#pullrequestreview-428905298", "createdAt": "2020-06-11T13:27:05Z", "commit": {"oid": "bc08aced21d655809d4fc2fa95f914f930d27159"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQxMzoyNzowNlrOGic9Ug==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQxMzoyNzowNlrOGic9Ug==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODc3OTIxOA==", "bodyText": "This config seems not to be used anywhere", "url": "https://github.com/oracle/helidon/pull/1918#discussion_r438779218", "createdAt": "2020-06-11T13:27:06Z", "author": {"login": "danielkec"}, "path": "tests/integration/kafka/src/test/java/io/helidon/messaging/connectors/kafka/KafkaSeTest.java", "diffHunk": "@@ -334,4 +349,107 @@ void kafkaHeaderTest() throws InterruptedException {\n             messaging.stop();\n         }\n     }\n+\n+    @Test\n+    void someEventsNoAckWithOnePartition() {\n+        LOGGER.fine(() -> \"==========> test someEventsNoAckWithOnePartition()\");\n+        final String GROUP = \"group_1\";\n+        final String TOPIC = TEST_SE_TOPIC_6;\n+        Channel<String> fromKafka = Channel.<String>builder()\n+                .name(\"from-kafka\")\n+                .publisherConfig(KafkaConnector.configBuilder()\n+                        .bootstrapServers(KAFKA_SERVER)\n+                        .groupId(GROUP)\n+                        .topic(TOPIC)\n+                        .autoOffsetReset(KafkaConfigBuilder.AutoOffsetReset.EARLIEST)\n+                        .enableAutoCommit(false)\n+                        .keyDeserializer(LongDeserializer.class)\n+                        .valueDeserializer(StringDeserializer.class)\n+                        .build()\n+                )\n+                .build();\n+        List<String> uncommit = new ArrayList<>();\n+        Channel6 kafkaConsumingBean = new Channel6();\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(\"bootstrap.servers\", KAFKA_SERVER);\n+        config.put(\"key.serializer\", LongSerializer.class.getName());\n+        config.put(\"value.serializer\", StringSerializer.class.getName());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bc08aced21d655809d4fc2fa95f914f930d27159"}, "originalPosition": 94}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI4OTA3NjU1", "url": "https://github.com/oracle/helidon/pull/1918#pullrequestreview-428907655", "createdAt": "2020-06-11T13:28:32Z", "commit": {"oid": "bc08aced21d655809d4fc2fa95f914f930d27159"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQxMzoyODozMlrOGidBPQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQxMzoyODozMlrOGidBPQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODc4MDIyMQ==", "bodyText": "here too", "url": "https://github.com/oracle/helidon/pull/1918#discussion_r438780221", "createdAt": "2020-06-11T13:28:32Z", "author": {"login": "danielkec"}, "path": "tests/integration/kafka/src/test/java/io/helidon/messaging/connectors/kafka/KafkaSeTest.java", "diffHunk": "@@ -334,4 +349,107 @@ void kafkaHeaderTest() throws InterruptedException {\n             messaging.stop();\n         }\n     }\n+\n+    @Test\n+    void someEventsNoAckWithOnePartition() {\n+        LOGGER.fine(() -> \"==========> test someEventsNoAckWithOnePartition()\");\n+        final String GROUP = \"group_1\";\n+        final String TOPIC = TEST_SE_TOPIC_6;\n+        Channel<String> fromKafka = Channel.<String>builder()\n+                .name(\"from-kafka\")\n+                .publisherConfig(KafkaConnector.configBuilder()\n+                        .bootstrapServers(KAFKA_SERVER)\n+                        .groupId(GROUP)\n+                        .topic(TOPIC)\n+                        .autoOffsetReset(KafkaConfigBuilder.AutoOffsetReset.EARLIEST)\n+                        .enableAutoCommit(false)\n+                        .keyDeserializer(LongDeserializer.class)\n+                        .valueDeserializer(StringDeserializer.class)\n+                        .build()\n+                )\n+                .build();\n+        List<String> uncommit = new ArrayList<>();\n+        Channel6 kafkaConsumingBean = new Channel6();\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(\"bootstrap.servers\", KAFKA_SERVER);\n+        config.put(\"key.serializer\", LongSerializer.class.getName());\n+        config.put(\"value.serializer\", StringSerializer.class.getName());\n+        Messaging messaging = Messaging.builder().connector(KafkaConnector.create())\n+                .subscriber(fromKafka, ReactiveStreams.<KafkaMessage<Long, String>>builder()\n+                        .forEach(msg -> kafkaConsumingBean.onMsg(msg)))\n+                .build();\n+        try {\n+            messaging.start();\n+            // Push some messages that will ACK\n+            List<String> testData = IntStream.range(0, 20).mapToObj(i -> Integer.toString(i)).collect(Collectors.toList());\n+            produceAndCheck(kafkaConsumingBean, testData, TOPIC, testData);\n+            kafkaConsumingBean.restart();\n+            // Next message will not ACK\n+            testData = Arrays.asList(Channel6.NO_ACK);\n+            uncommit.addAll(testData);\n+            produceAndCheck(kafkaConsumingBean, testData, TOPIC, testData);\n+            kafkaConsumingBean.restart();\n+            // As this topic only have one partition, next messages will not ACK because previous message wasn't\n+            testData = IntStream.range(100, 120).mapToObj(i -> Integer.toString(i)).collect(Collectors.toList());\n+            uncommit.addAll(testData);\n+            produceAndCheck(kafkaConsumingBean, testData, TOPIC, testData);\n+        } finally {\n+            messaging.stop();\n+        }\n+        // We receive uncommitted messages again\n+        List<String> events = readTopic(TOPIC, uncommit.size(), GROUP);\n+        Collections.sort(events);\n+        Collections.sort(uncommit);\n+        assertEquals(uncommit, events);\n+    }\n+\n+    @Test\n+    void someEventsNoAckWithDifferentPartitions() {\n+        LOGGER.fine(() -> \"==========> test someEventsNoAckWithDifferentPartitions()\");\n+        final long FROM = 2000;\n+        final long TO = FROM + Channel8.LIMIT;\n+        final String GROUP = \"group_2\";\n+        final String TOPIC = TEST_SE_TOPIC_7;\n+        Channel<String> fromKafka = Channel.<String>builder()\n+                .name(\"from-kafka\")\n+                .publisherConfig(KafkaConnector.configBuilder()\n+                        .bootstrapServers(KAFKA_SERVER)\n+                        .groupId(GROUP)\n+                        .topic(TOPIC)\n+                        .autoOffsetReset(KafkaConfigBuilder.AutoOffsetReset.EARLIEST)\n+                        .enableAutoCommit(false)\n+                        .keyDeserializer(LongDeserializer.class)\n+                        .valueDeserializer(StringDeserializer.class)\n+                        .build()\n+                )\n+                .build();\n+        // Send the message that will not ACK. This will make in one partition to not commit any new message\n+        Channel8 kafkaConsumingBean = new Channel8();\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(\"bootstrap.servers\", KAFKA_SERVER);\n+        config.put(\"key.serializer\", LongSerializer.class.getName());\n+        config.put(\"value.serializer\", StringSerializer.class.getName());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bc08aced21d655809d4fc2fa95f914f930d27159"}, "originalPosition": 149}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "13a481e3ee88cbe091868eae00113b989adad54f", "author": {"user": {"login": "jbescos", "name": "jbescos"}}, "url": "https://github.com/oracle/helidon/commit/13a481e3ee88cbe091868eae00113b989adad54f", "committedDate": "2020-06-12T12:18:01Z", "message": "Removed unused fields\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMwMTQwODE5", "url": "https://github.com/oracle/helidon/pull/1918#pullrequestreview-430140819", "createdAt": "2020-06-13T13:50:39Z", "commit": {"oid": "13a481e3ee88cbe091868eae00113b989adad54f"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 670, "cost": 1, "resetAt": "2021-11-01T15:33:45Z"}}}