{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDE0Mzk4NTUz", "number": 1672, "reviewThreads": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQwMDoxMjoxMFrOD7cjaw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQwMDoxNTo0MlrOD7cl6g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzNjYwMzk1OnYy", "diffSide": "RIGHT", "path": "test-harness/src/test/groovy/com/netflix/counductor/integration/test/KafkaTaskBasedWorkflowSpec.groovy", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQwMDoxMjoxMFrOGTxrKg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xM1QxOToyMjoxN1rOGVAL6g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzM4OTk5NA==", "bodyText": "leftovers?", "url": "https://github.com/Netflix/conductor/pull/1672#discussion_r423389994", "createdAt": "2020-05-12T00:12:10Z", "author": {"login": "kishorebanala"}, "path": "test-harness/src/test/groovy/com/netflix/counductor/integration/test/KafkaTaskBasedWorkflowSpec.groovy", "diffHunk": "@@ -0,0 +1,201 @@\n+package com.netflix.counductor.integration.test\n+\n+import com.fasterxml.jackson.databind.ObjectMapper\n+import com.netflix.archaius.guice.ArchaiusModule\n+import com.netflix.conductor.common.metadata.tasks.TaskDef\n+import com.netflix.conductor.common.metadata.tasks.TaskResult\n+import com.netflix.conductor.common.metadata.workflow.TaskType\n+import com.netflix.conductor.common.metadata.workflow.WorkflowDef\n+import com.netflix.conductor.common.metadata.workflow.WorkflowTask\n+import com.netflix.conductor.common.run.Workflow\n+import com.netflix.conductor.common.utils.JsonMapperProvider\n+import com.netflix.conductor.core.execution.WorkflowExecutor\n+import com.netflix.conductor.service.ExecutionService\n+import com.netflix.conductor.service.MetadataService\n+import com.netflix.conductor.tests.utils.TestModule\n+import com.netflix.conductor.tests.utils.WorkflowCleanUpUtil\n+import com.netflix.governator.guice.test.ModulesForTesting\n+import spock.lang.Shared\n+import spock.lang.Specification\n+\n+import javax.inject.Inject\n+\n+@ModulesForTesting([TestModule.class, ArchaiusModule.class])\n+class KafkaTaskBasedWorkflowSpec extends Specification {\n+\n+    @Inject\n+    ExecutionService workflowExecutionService\n+\n+    @Inject\n+    MetadataService metadataService\n+\n+    @Inject\n+    WorkflowExecutor workflowExecutor\n+\n+    @Inject\n+    WorkflowCleanUpUtil cleanUpUtil\n+\n+    @Shared\n+    ObjectMapper objectMapper = new JsonMapperProvider().get()\n+\n+    @Shared\n+    def isWorkflowRegistered = false\n+\n+    def kafkaInput = ['requestDetails': ['key1': 'value1', 'key2': 42],\n+                      'path1'         : 'file://path1',\n+                      'path2'         : 'file://path2',\n+                      'outputPath'    : 's3://bucket/outputPath'\n+    ]\n+\n+\n+    def expectedTaskInput = \"{\\\"kafka_request\\\":{\\\"topic\\\":\\\"test_kafka_topic\\\",\\\"bootStrapServers\\\":\\\"localhost:9092\\\",\\\"value\\\":{\\\"requestDetails\\\":{\\\"key1\\\":\\\"value1\\\",\\\"key2\\\":42},\\\"outputPath\\\":\\\"s3://bucket/outputPath\\\",\\\"inputPaths\\\":[\\\"file://path1\\\",\\\"file://path2\\\"]}}}\"\n+\n+    def cleanup() {\n+        cleanUpUtil.clearWorkflows()\n+    }\n+\n+    def setup() {\n+        if(!isWorkflowRegistered) {\n+            registerKafkaWorkflow()\n+            isWorkflowRegistered = true\n+        }\n+    }\n+\n+    def \"Test the kafka template usage failure case\"() {\n+\n+        given:\"Start a workflow based on the registered workflow\"\n+        def workflowInstanceId = workflowExecutor.startWorkflow(\"template_kafka_workflow\", 1,\n+                \"testTaskDefTemplate\", kafkaInput,\n+                null, null, null)\n+\n+        and:\"Get the workflow based on the Id that is being executed\"\n+        def workflow = workflowExecutionService.getExecutionStatus(workflowInstanceId, true)\n+        def task = workflow.tasks.get(0)\n+        def taskInput = task.inputData\n+\n+        when:\"Ensure that the task is pollable and fail the task\"\n+        def polledTask = workflowExecutionService.poll('KAFKA_PUBLISH', 'test')\n+        workflowExecutionService.ackTaskReceived(polledTask.taskId)\n+        def taskResult = new TaskResult(polledTask)\n+        taskResult.status = TaskResult.Status.FAILED\n+        taskResult.reasonForIncompletion = 'NON TRANSIENT ERROR OCCURRED: An integration point required to complete the task is down'\n+        taskResult.addOutputData(\"TERMINAL_ERROR\", \"Integration endpoint down: FOOBAR\")\n+        taskResult.addOutputData(\"ErrorMessage\", \"There was a terminal error\")\n+        workflowExecutionService.updateTask(taskResult)\n+\n+        and:\"Then run a decide to move the workflow forward\"\n+        workflowExecutor.decide(workflowInstanceId)\n+\n+        and:\"Get the updated workflow after the task result has been updated\"\n+        def updatedWorkflow = workflowExecutionService.getExecutionStatus(workflowInstanceId, true)\n+\n+        then:\"Check that the workflow is created and is not terminal\"\n+        workflowInstanceId\n+        workflow\n+        !workflow.getStatus().isTerminal()\n+        !workflow.getReasonForIncompletion()\n+\n+        and:\"Check if the input of the next task to be polled is as expected for a kafka task\"\n+        taskInput\n+        taskInput.containsKey('kafka_request')\n+        taskInput['kafka_request'] instanceof Map\n+        objectMapper.writeValueAsString(taskInput) == expectedTaskInput\n+\n+        and:\"Polled task is not null and the workflowInstanceId of the task is same as the workflow created initially\"\n+        polledTask\n+        polledTask.workflowInstanceId == workflowInstanceId\n+\n+        and:\"The updated workflow is in a failed state\"\n+        updatedWorkflow\n+        updatedWorkflow.status == Workflow.WorkflowStatus.FAILED\n+\n+\n+    }\n+\n+\n+    def \"Test the kafka template usage success case\"() {\n+\n+        given:\"Start a workflow based on the registered kafka workflow\"\n+        def workflowInstanceId = workflowExecutor.startWorkflow(\"template_kafka_workflow\", 1,\n+                \"testTaskDefTemplate\", kafkaInput,\n+                null, null, null)\n+\n+        and:\"Get the workflow based on the Id that is being executed\"\n+        def workflow = workflowExecutionService.getExecutionStatus(workflowInstanceId, true)\n+        def task = workflow.tasks.get(0)\n+        def taskInput = task.inputData\n+\n+        when:\"Ensure that the task is pollable and complete the task\"\n+        def polledTask = workflowExecutionService.poll('KAFKA_PUBLISH', 'test')\n+        workflowExecutionService.ackTaskReceived(polledTask.taskId)\n+        def taskResult = new TaskResult(polledTask)\n+        taskResult.setStatus(TaskResult.Status.COMPLETED)\n+        workflowExecutionService.updateTask(taskResult)\n+\n+        and:\"Then run a decide to move the workflow forward\"\n+        workflowExecutor.decide(workflowInstanceId)\n+\n+        and:\"Get the updated workflow after the task result has been updated\"\n+        def updatedWorkflow = workflowExecutionService.getExecutionStatus(workflowInstanceId, true)\n+\n+        then:\"Check that the workflow is created and is not terminal\"\n+        workflowInstanceId\n+        workflow\n+        !workflow.getStatus().isTerminal()\n+        !workflow.getReasonForIncompletion()\n+\n+        and:\"Check if the input of the next task to be polled is as expected for a kafka task\"\n+        taskInput\n+        taskInput.containsKey('kafka_request')\n+        taskInput['kafka_request'] instanceof Map\n+        objectMapper.writeValueAsString(taskInput) == expectedTaskInput\n+\n+        and:\"Polled task is not null and the workflowInstanceId of the task is same as the workflow created initially\"\n+        polledTask\n+        polledTask.workflowInstanceId == workflowInstanceId\n+\n+        and:\"The updated workflow is complete\"\n+        updatedWorkflow\n+        updatedWorkflow.status == Workflow.WorkflowStatus.COMPLETED\n+\n+    }\n+\n+\n+    def registerKafkaWorkflow() {\n+        System.setProperty(\"STACK_KAFKA\", \"test_kafka_topic\")\n+        TaskDef templatedTask = new TaskDef()\n+        templatedTask.name = \"templated_kafka_task\"\n+        templatedTask.retryCount = 0\n+\n+        def kafkaRequest = new HashMap<>()\n+        kafkaRequest[\"topic\"] = '${STACK_KAFKA}'\n+        kafkaRequest[\"bootStrapServers\"] = \"localhost:9092\"\n+\n+        def value = new HashMap<>()\n+        value[\"inputPaths\"] = ['${workflow.input.path1}', '${workflow.input.path2}']\n+        value[\"requestDetails\"] = '${workflow.input.requestDetails}'\n+        value[\"outputPath\"] = '${workflow.input.outputPath}'\n+        kafkaRequest[\"value\"] = value\n+\n+        templatedTask.inputTemplate[\"kafka_request\"] = kafkaRequest\n+        metadataService.registerTaskDef([templatedTask])\n+\n+        WorkflowDef templateWf = new WorkflowDef()\n+        templateWf.name = \"template_kafka_workflow\"\n+        WorkflowTask wft = new WorkflowTask()\n+        wft.name = templatedTask.name\n+        wft.workflowTaskType = TaskType.KAFKA_PUBLISH\n+        wft.taskReferenceName = \"t0\"\n+        templateWf.tasks.add(wft)\n+        templateWf.schemaVersion = 2\n+        metadataService.registerWorkflowDef(templateWf)\n+    }\n+\n+\n+\n+    /*def value = ['inputPaths'    : ['${workflow.input.path1}', '${workflow.input.path2}'],", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9e7223b2a4af6c138e4febe06f2cdb3d4b85370f"}, "originalPosition": 196}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDY3NjMzMA==", "bodyText": "Cleaned up", "url": "https://github.com/Netflix/conductor/pull/1672#discussion_r424676330", "createdAt": "2020-05-13T19:22:17Z", "author": {"login": "pctreddy"}, "path": "test-harness/src/test/groovy/com/netflix/counductor/integration/test/KafkaTaskBasedWorkflowSpec.groovy", "diffHunk": "@@ -0,0 +1,201 @@\n+package com.netflix.counductor.integration.test\n+\n+import com.fasterxml.jackson.databind.ObjectMapper\n+import com.netflix.archaius.guice.ArchaiusModule\n+import com.netflix.conductor.common.metadata.tasks.TaskDef\n+import com.netflix.conductor.common.metadata.tasks.TaskResult\n+import com.netflix.conductor.common.metadata.workflow.TaskType\n+import com.netflix.conductor.common.metadata.workflow.WorkflowDef\n+import com.netflix.conductor.common.metadata.workflow.WorkflowTask\n+import com.netflix.conductor.common.run.Workflow\n+import com.netflix.conductor.common.utils.JsonMapperProvider\n+import com.netflix.conductor.core.execution.WorkflowExecutor\n+import com.netflix.conductor.service.ExecutionService\n+import com.netflix.conductor.service.MetadataService\n+import com.netflix.conductor.tests.utils.TestModule\n+import com.netflix.conductor.tests.utils.WorkflowCleanUpUtil\n+import com.netflix.governator.guice.test.ModulesForTesting\n+import spock.lang.Shared\n+import spock.lang.Specification\n+\n+import javax.inject.Inject\n+\n+@ModulesForTesting([TestModule.class, ArchaiusModule.class])\n+class KafkaTaskBasedWorkflowSpec extends Specification {\n+\n+    @Inject\n+    ExecutionService workflowExecutionService\n+\n+    @Inject\n+    MetadataService metadataService\n+\n+    @Inject\n+    WorkflowExecutor workflowExecutor\n+\n+    @Inject\n+    WorkflowCleanUpUtil cleanUpUtil\n+\n+    @Shared\n+    ObjectMapper objectMapper = new JsonMapperProvider().get()\n+\n+    @Shared\n+    def isWorkflowRegistered = false\n+\n+    def kafkaInput = ['requestDetails': ['key1': 'value1', 'key2': 42],\n+                      'path1'         : 'file://path1',\n+                      'path2'         : 'file://path2',\n+                      'outputPath'    : 's3://bucket/outputPath'\n+    ]\n+\n+\n+    def expectedTaskInput = \"{\\\"kafka_request\\\":{\\\"topic\\\":\\\"test_kafka_topic\\\",\\\"bootStrapServers\\\":\\\"localhost:9092\\\",\\\"value\\\":{\\\"requestDetails\\\":{\\\"key1\\\":\\\"value1\\\",\\\"key2\\\":42},\\\"outputPath\\\":\\\"s3://bucket/outputPath\\\",\\\"inputPaths\\\":[\\\"file://path1\\\",\\\"file://path2\\\"]}}}\"\n+\n+    def cleanup() {\n+        cleanUpUtil.clearWorkflows()\n+    }\n+\n+    def setup() {\n+        if(!isWorkflowRegistered) {\n+            registerKafkaWorkflow()\n+            isWorkflowRegistered = true\n+        }\n+    }\n+\n+    def \"Test the kafka template usage failure case\"() {\n+\n+        given:\"Start a workflow based on the registered workflow\"\n+        def workflowInstanceId = workflowExecutor.startWorkflow(\"template_kafka_workflow\", 1,\n+                \"testTaskDefTemplate\", kafkaInput,\n+                null, null, null)\n+\n+        and:\"Get the workflow based on the Id that is being executed\"\n+        def workflow = workflowExecutionService.getExecutionStatus(workflowInstanceId, true)\n+        def task = workflow.tasks.get(0)\n+        def taskInput = task.inputData\n+\n+        when:\"Ensure that the task is pollable and fail the task\"\n+        def polledTask = workflowExecutionService.poll('KAFKA_PUBLISH', 'test')\n+        workflowExecutionService.ackTaskReceived(polledTask.taskId)\n+        def taskResult = new TaskResult(polledTask)\n+        taskResult.status = TaskResult.Status.FAILED\n+        taskResult.reasonForIncompletion = 'NON TRANSIENT ERROR OCCURRED: An integration point required to complete the task is down'\n+        taskResult.addOutputData(\"TERMINAL_ERROR\", \"Integration endpoint down: FOOBAR\")\n+        taskResult.addOutputData(\"ErrorMessage\", \"There was a terminal error\")\n+        workflowExecutionService.updateTask(taskResult)\n+\n+        and:\"Then run a decide to move the workflow forward\"\n+        workflowExecutor.decide(workflowInstanceId)\n+\n+        and:\"Get the updated workflow after the task result has been updated\"\n+        def updatedWorkflow = workflowExecutionService.getExecutionStatus(workflowInstanceId, true)\n+\n+        then:\"Check that the workflow is created and is not terminal\"\n+        workflowInstanceId\n+        workflow\n+        !workflow.getStatus().isTerminal()\n+        !workflow.getReasonForIncompletion()\n+\n+        and:\"Check if the input of the next task to be polled is as expected for a kafka task\"\n+        taskInput\n+        taskInput.containsKey('kafka_request')\n+        taskInput['kafka_request'] instanceof Map\n+        objectMapper.writeValueAsString(taskInput) == expectedTaskInput\n+\n+        and:\"Polled task is not null and the workflowInstanceId of the task is same as the workflow created initially\"\n+        polledTask\n+        polledTask.workflowInstanceId == workflowInstanceId\n+\n+        and:\"The updated workflow is in a failed state\"\n+        updatedWorkflow\n+        updatedWorkflow.status == Workflow.WorkflowStatus.FAILED\n+\n+\n+    }\n+\n+\n+    def \"Test the kafka template usage success case\"() {\n+\n+        given:\"Start a workflow based on the registered kafka workflow\"\n+        def workflowInstanceId = workflowExecutor.startWorkflow(\"template_kafka_workflow\", 1,\n+                \"testTaskDefTemplate\", kafkaInput,\n+                null, null, null)\n+\n+        and:\"Get the workflow based on the Id that is being executed\"\n+        def workflow = workflowExecutionService.getExecutionStatus(workflowInstanceId, true)\n+        def task = workflow.tasks.get(0)\n+        def taskInput = task.inputData\n+\n+        when:\"Ensure that the task is pollable and complete the task\"\n+        def polledTask = workflowExecutionService.poll('KAFKA_PUBLISH', 'test')\n+        workflowExecutionService.ackTaskReceived(polledTask.taskId)\n+        def taskResult = new TaskResult(polledTask)\n+        taskResult.setStatus(TaskResult.Status.COMPLETED)\n+        workflowExecutionService.updateTask(taskResult)\n+\n+        and:\"Then run a decide to move the workflow forward\"\n+        workflowExecutor.decide(workflowInstanceId)\n+\n+        and:\"Get the updated workflow after the task result has been updated\"\n+        def updatedWorkflow = workflowExecutionService.getExecutionStatus(workflowInstanceId, true)\n+\n+        then:\"Check that the workflow is created and is not terminal\"\n+        workflowInstanceId\n+        workflow\n+        !workflow.getStatus().isTerminal()\n+        !workflow.getReasonForIncompletion()\n+\n+        and:\"Check if the input of the next task to be polled is as expected for a kafka task\"\n+        taskInput\n+        taskInput.containsKey('kafka_request')\n+        taskInput['kafka_request'] instanceof Map\n+        objectMapper.writeValueAsString(taskInput) == expectedTaskInput\n+\n+        and:\"Polled task is not null and the workflowInstanceId of the task is same as the workflow created initially\"\n+        polledTask\n+        polledTask.workflowInstanceId == workflowInstanceId\n+\n+        and:\"The updated workflow is complete\"\n+        updatedWorkflow\n+        updatedWorkflow.status == Workflow.WorkflowStatus.COMPLETED\n+\n+    }\n+\n+\n+    def registerKafkaWorkflow() {\n+        System.setProperty(\"STACK_KAFKA\", \"test_kafka_topic\")\n+        TaskDef templatedTask = new TaskDef()\n+        templatedTask.name = \"templated_kafka_task\"\n+        templatedTask.retryCount = 0\n+\n+        def kafkaRequest = new HashMap<>()\n+        kafkaRequest[\"topic\"] = '${STACK_KAFKA}'\n+        kafkaRequest[\"bootStrapServers\"] = \"localhost:9092\"\n+\n+        def value = new HashMap<>()\n+        value[\"inputPaths\"] = ['${workflow.input.path1}', '${workflow.input.path2}']\n+        value[\"requestDetails\"] = '${workflow.input.requestDetails}'\n+        value[\"outputPath\"] = '${workflow.input.outputPath}'\n+        kafkaRequest[\"value\"] = value\n+\n+        templatedTask.inputTemplate[\"kafka_request\"] = kafkaRequest\n+        metadataService.registerTaskDef([templatedTask])\n+\n+        WorkflowDef templateWf = new WorkflowDef()\n+        templateWf.name = \"template_kafka_workflow\"\n+        WorkflowTask wft = new WorkflowTask()\n+        wft.name = templatedTask.name\n+        wft.workflowTaskType = TaskType.KAFKA_PUBLISH\n+        wft.taskReferenceName = \"t0\"\n+        templateWf.tasks.add(wft)\n+        templateWf.schemaVersion = 2\n+        metadataService.registerWorkflowDef(templateWf)\n+    }\n+\n+\n+\n+    /*def value = ['inputPaths'    : ['${workflow.input.path1}', '${workflow.input.path2}'],", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzM4OTk5NA=="}, "originalCommit": {"oid": "9e7223b2a4af6c138e4febe06f2cdb3d4b85370f"}, "originalPosition": 196}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzNjYxMDM0OnYy", "diffSide": "RIGHT", "path": "test-harness/src/test/java/com/netflix/conductor/tests/utils/WorkflowCleanUpUtil.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQwMDoxNTo0MlrOGTxvHw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xM1QxOTo1ODoyM1rOGVBXpA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzM5MTAwNw==", "bodyText": "Nit: Trying to understand what this class does. JavaDoc?", "url": "https://github.com/Netflix/conductor/pull/1672#discussion_r423391007", "createdAt": "2020-05-12T00:15:42Z", "author": {"login": "kishorebanala"}, "path": "test-harness/src/test/java/com/netflix/conductor/tests/utils/WorkflowCleanUpUtil.java", "diffHunk": "@@ -0,0 +1,54 @@\n+package com.netflix.conductor.tests.utils;\n+\n+import com.netflix.conductor.common.run.Workflow;\n+import com.netflix.conductor.core.execution.WorkflowExecutor;\n+import com.netflix.conductor.dao.QueueDAO;\n+import com.netflix.conductor.service.ExecutionService;\n+import com.netflix.conductor.service.MetadataService;\n+import org.apache.commons.lang.StringUtils;\n+\n+import javax.inject.Inject;\n+import javax.inject.Singleton;\n+import java.io.FileOutputStream;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+@Singleton\n+public class WorkflowCleanUpUtil {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9e7223b2a4af6c138e4febe06f2cdb3d4b85370f"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDY5NTcxNg==", "bodyText": "Moved to another utility class : WorkflowTestUtil and added documentation", "url": "https://github.com/Netflix/conductor/pull/1672#discussion_r424695716", "createdAt": "2020-05-13T19:58:23Z", "author": {"login": "pctreddy"}, "path": "test-harness/src/test/java/com/netflix/conductor/tests/utils/WorkflowCleanUpUtil.java", "diffHunk": "@@ -0,0 +1,54 @@\n+package com.netflix.conductor.tests.utils;\n+\n+import com.netflix.conductor.common.run.Workflow;\n+import com.netflix.conductor.core.execution.WorkflowExecutor;\n+import com.netflix.conductor.dao.QueueDAO;\n+import com.netflix.conductor.service.ExecutionService;\n+import com.netflix.conductor.service.MetadataService;\n+import org.apache.commons.lang.StringUtils;\n+\n+import javax.inject.Inject;\n+import javax.inject.Singleton;\n+import java.io.FileOutputStream;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+@Singleton\n+public class WorkflowCleanUpUtil {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzM5MTAwNw=="}, "originalCommit": {"oid": "9e7223b2a4af6c138e4febe06f2cdb3d4b85370f"}, "originalPosition": 17}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4184, "cost": 1, "resetAt": "2021-11-12T20:28:25Z"}}}