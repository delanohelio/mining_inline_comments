{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDIwMjY3Mjc1", "number": 1691, "title": "Added multiple features in a specification ForkJoinSpec to verify the\u2026", "bodyText": "\u2026 fork join behavior", "createdAt": "2020-05-19T17:40:33Z", "url": "https://github.com/Netflix/conductor/pull/1691", "merged": true, "mergeCommit": {"oid": "a8c81c2beb35c1e7b3e0b035428a5e5592dc24b3"}, "closed": true, "closedAt": "2020-05-20T17:31:59Z", "author": {"login": "pctreddy"}, "timelineItems": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABci4D9dgH2gAyNDIwMjY3Mjc1OjE0NzdjMzJiYjUxMmUyODM3NjMwNmY3NzQ5NTFkODlmOWRhNzBiNjk=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcjMNb1AH2gAyNDIwMjY3Mjc1Ojg3YjljZTRjMDU4NDRhYjc4ZTkyNTljOTZhMjVlZDUxZjcyMzQ2MTA=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "1477c32bb512e28376306f774951d89f9da70b69", "author": {"user": null}, "url": "https://github.com/Netflix/conductor/commit/1477c32bb512e28376306f774951d89f9da70b69", "committedDate": "2020-05-19T17:38:31Z", "message": "Added multiple features in a specification ForkJoinSpec to verify the fork join behavior"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE1MDMwNDc4", "url": "https://github.com/Netflix/conductor/pull/1691#pullrequestreview-415030478", "createdAt": "2020-05-20T06:37:06Z", "commit": {"oid": "1477c32bb512e28376306f774951d89f9da70b69"}, "state": "APPROVED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQwNjozNzowNlrOGX9K8g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQwNjo0MzoyNFrOGX9VQQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzc3MjY1OA==", "bodyText": "Please add license header.", "url": "https://github.com/Netflix/conductor/pull/1691#discussion_r427772658", "createdAt": "2020-05-20T06:37:06Z", "author": {"login": "apanicker-nflx"}, "path": "test-harness/src/test/groovy/com/netflix/counductor/integration/test/ForkJoinSpec.groovy", "diffHunk": "@@ -0,0 +1,874 @@\n+package com.netflix.counductor.integration.test", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1477c32bb512e28376306f774951d89f9da70b69"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzc3MzU3Ng==", "bodyText": "I believe this was done to maintain consistency with the existing JUnit tests, but this spec does not use/need modification of retry counts, so this could be removed.", "url": "https://github.com/Netflix/conductor/pull/1691#discussion_r427773576", "createdAt": "2020-05-20T06:39:23Z", "author": {"login": "apanicker-nflx"}, "path": "test-harness/src/test/groovy/com/netflix/counductor/integration/test/ForkJoinSpec.groovy", "diffHunk": "@@ -0,0 +1,874 @@\n+package com.netflix.counductor.integration.test\n+\n+import com.netflix.archaius.guice.ArchaiusModule\n+import com.netflix.conductor.common.metadata.tasks.Task\n+import com.netflix.conductor.common.metadata.tasks.TaskDef\n+import com.netflix.conductor.common.run.Workflow\n+import com.netflix.conductor.core.execution.WorkflowExecutor\n+import com.netflix.conductor.core.execution.tasks.SubWorkflow\n+import com.netflix.conductor.service.ExecutionService\n+import com.netflix.conductor.service.MetadataService\n+import com.netflix.conductor.test.util.WorkflowTestUtil\n+import com.netflix.conductor.tests.utils.TestModule\n+import com.netflix.governator.guice.test.ModulesForTesting\n+import spock.lang.Shared\n+import spock.lang.Specification\n+\n+import javax.inject.Inject\n+\n+@ModulesForTesting([TestModule.class, ArchaiusModule.class])\n+class ForkJoinSpec extends Specification {\n+\n+    @Inject\n+    ExecutionService workflowExecutionService\n+\n+    @Inject\n+    MetadataService metadataService\n+\n+    @Inject\n+    WorkflowExecutor workflowExecutor\n+\n+    @Inject\n+    WorkflowTestUtil workflowTestUtil\n+\n+    @Shared\n+    def FORK_JOIN_WF = 'FanInOutTest'\n+\n+    @Shared\n+    def FORK_JOIN_NESTED_WF = 'FanInOutNestedTest'\n+\n+    @Shared\n+    def FORK_JOIN_NESTED_SUB_WF = 'FanInOutNestedSubWorkflowTest'\n+\n+    @Shared\n+    def WORKFLOW_FORK_JOIN_OPTIONAL_SW = \"integration_test_fork_join_optional_sw\"\n+\n+    def cleanup() {\n+        workflowTestUtil.clearWorkflows()\n+    }\n+\n+    def setup() {\n+        workflowTestUtil.registerWorkflows('fork_join_integration_test.json',\n+                'fork_join_with_no_task_retry_integration_test.json',\n+                'nested_fork_join_integration_test.json',\n+                'simple_workflow_1_integration_test.json',\n+                'nested_fork_join_with_sub_workflow_integration_test.json',\n+                'simple_one_task_sub_workflow_integration_test.json',\n+                'fork_join_with_optional_sub_workflow_forks_integration_test.json'\n+        )\n+    }\n+\n+    /**\n+     *               start\n+     *                 |\n+     *               fork\n+     *              /    \\\n+     *         task1     task2\n+     *          \\        /\n+     *          task3   /\n+     *           \\     /\n+     *            \\  /\n+     *            join\n+     *              |\n+     *             task4\n+     *              |\n+     *             End\n+     */\n+    def \"Test a simple workflow with fork join success flow\"() {\n+        setup: \"Ensure that all the tasks involved in the workflow have a retry count of 0\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1477c32bb512e28376306f774951d89f9da70b69"}, "originalPosition": 78}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzc3NDI1Mg==", "bodyText": "restarting -> retrying", "url": "https://github.com/Netflix/conductor/pull/1691#discussion_r427774252", "createdAt": "2020-05-20T06:40:56Z", "author": {"login": "apanicker-nflx"}, "path": "test-harness/src/test/groovy/com/netflix/counductor/integration/test/ForkJoinSpec.groovy", "diffHunk": "@@ -0,0 +1,874 @@\n+package com.netflix.counductor.integration.test\n+\n+import com.netflix.archaius.guice.ArchaiusModule\n+import com.netflix.conductor.common.metadata.tasks.Task\n+import com.netflix.conductor.common.metadata.tasks.TaskDef\n+import com.netflix.conductor.common.run.Workflow\n+import com.netflix.conductor.core.execution.WorkflowExecutor\n+import com.netflix.conductor.core.execution.tasks.SubWorkflow\n+import com.netflix.conductor.service.ExecutionService\n+import com.netflix.conductor.service.MetadataService\n+import com.netflix.conductor.test.util.WorkflowTestUtil\n+import com.netflix.conductor.tests.utils.TestModule\n+import com.netflix.governator.guice.test.ModulesForTesting\n+import spock.lang.Shared\n+import spock.lang.Specification\n+\n+import javax.inject.Inject\n+\n+@ModulesForTesting([TestModule.class, ArchaiusModule.class])\n+class ForkJoinSpec extends Specification {\n+\n+    @Inject\n+    ExecutionService workflowExecutionService\n+\n+    @Inject\n+    MetadataService metadataService\n+\n+    @Inject\n+    WorkflowExecutor workflowExecutor\n+\n+    @Inject\n+    WorkflowTestUtil workflowTestUtil\n+\n+    @Shared\n+    def FORK_JOIN_WF = 'FanInOutTest'\n+\n+    @Shared\n+    def FORK_JOIN_NESTED_WF = 'FanInOutNestedTest'\n+\n+    @Shared\n+    def FORK_JOIN_NESTED_SUB_WF = 'FanInOutNestedSubWorkflowTest'\n+\n+    @Shared\n+    def WORKFLOW_FORK_JOIN_OPTIONAL_SW = \"integration_test_fork_join_optional_sw\"\n+\n+    def cleanup() {\n+        workflowTestUtil.clearWorkflows()\n+    }\n+\n+    def setup() {\n+        workflowTestUtil.registerWorkflows('fork_join_integration_test.json',\n+                'fork_join_with_no_task_retry_integration_test.json',\n+                'nested_fork_join_integration_test.json',\n+                'simple_workflow_1_integration_test.json',\n+                'nested_fork_join_with_sub_workflow_integration_test.json',\n+                'simple_one_task_sub_workflow_integration_test.json',\n+                'fork_join_with_optional_sub_workflow_forks_integration_test.json'\n+        )\n+    }\n+\n+    /**\n+     *               start\n+     *                 |\n+     *               fork\n+     *              /    \\\n+     *         task1     task2\n+     *          \\        /\n+     *          task3   /\n+     *           \\     /\n+     *            \\  /\n+     *            join\n+     *              |\n+     *             task4\n+     *              |\n+     *             End\n+     */\n+    def \"Test a simple workflow with fork join success flow\"() {\n+        setup: \"Ensure that all the tasks involved in the workflow have a retry count of 0\"\n+        def persistedIntegrationTask1Definition = workflowTestUtil.getPersistedTaskDefinition('integration_task_1').get()\n+        def modifiedIntegrationTask1Definition = new TaskDef(persistedIntegrationTask1Definition.name,\n+                persistedIntegrationTask1Definition.description, 0, 0)\n+        def persistedIntegrationTask2Definition = workflowTestUtil.getPersistedTaskDefinition('integration_task_2').get()\n+        def modifiedIntegrationTask2Definition = new TaskDef(persistedIntegrationTask2Definition.name,\n+                persistedIntegrationTask2Definition.description, 0, 0)\n+        def persistedIntegrationTask3Definition = workflowTestUtil.getPersistedTaskDefinition('integration_task_3').get()\n+        def modifiedIntegrationTask3Definition = new TaskDef(persistedIntegrationTask3Definition.name,\n+                persistedIntegrationTask3Definition.description, 0, 0)\n+        def persistedIntegrationTask4Definition = workflowTestUtil.getPersistedTaskDefinition('integration_task_4').get()\n+        def modifiedIntegrationTask4Definition = new TaskDef(persistedIntegrationTask4Definition.name,\n+                persistedIntegrationTask4Definition.description, 0, 0)\n+\n+        metadataService.updateTaskDef(modifiedIntegrationTask1Definition)\n+        metadataService.updateTaskDef(modifiedIntegrationTask2Definition)\n+        metadataService.updateTaskDef(modifiedIntegrationTask3Definition)\n+        metadataService.updateTaskDef(modifiedIntegrationTask4Definition)\n+\n+        when: \"A fork join workflow is started\"\n+        def workflowInstanceId = workflowExecutor.startWorkflow(FORK_JOIN_WF, 1,\n+                'fanoutTest', [:],\n+                null, null, null)\n+\n+        then: \"verify that the workflow has started and the starting nodes of the each fork are in scheduled state\"\n+        workflowInstanceId\n+        with(workflowExecutionService.getExecutionStatus(workflowInstanceId, true)) {\n+            status == Workflow.WorkflowStatus.RUNNING\n+            tasks.size() == 4\n+            tasks[0].status == Task.Status.COMPLETED\n+            tasks[0].taskType == 'FORK'\n+            tasks[1].status == Task.Status.SCHEDULED\n+            tasks[1].taskType == 'integration_task_1'\n+            tasks[2].status == Task.Status.SCHEDULED\n+            tasks[2].taskType == 'integration_task_2'\n+            tasks[3].status == Task.Status.IN_PROGRESS\n+            tasks[3].taskType == 'JOIN'\n+        }\n+\n+        when: \"The first task of the fork is polled and completed\"\n+        def polledAndAckTask1Try1 = workflowTestUtil.pollAndCompleteTask('integration_task_1', 'task1.worker', [:], 0)\n+\n+        then: \"verify that the 'integration_task_1' was polled and acknowledged\"\n+        workflowTestUtil.verifyPolledAndAcknowledgedTask([:], polledAndAckTask1Try1)\n+\n+        and: \"The workflow has been updated and has all the required tasks in the right status to move forward\"\n+        with(workflowExecutionService.getExecutionStatus(workflowInstanceId, true)) {\n+            status == Workflow.WorkflowStatus.RUNNING\n+            tasks.size() == 5\n+            tasks[1].status == Task.Status.COMPLETED\n+            tasks[1].taskType == 'integration_task_1'\n+            tasks[2].status == Task.Status.SCHEDULED\n+            tasks[2].taskType == 'integration_task_2'\n+            tasks[3].status == Task.Status.IN_PROGRESS\n+            tasks[3].taskType == 'JOIN'\n+            tasks[4].status == Task.Status.SCHEDULED\n+            tasks[4].taskType == 'integration_task_3'\n+        }\n+\n+        when: \"The 'integration_task_3' is polled and completed\"\n+        def polledAndAckTask3Try1 = workflowTestUtil.pollAndCompleteTask('integration_task_3',\n+                'task1.worker', [:], 0)\n+\n+        then: \"verify that the 'integration_task_3' was polled and acknowledged\"\n+        workflowTestUtil.verifyPolledAndAcknowledgedTask([:], polledAndAckTask3Try1)\n+\n+        and: \"The workflow has been updated with the task status and task list\"\n+        with(workflowExecutionService.getExecutionStatus(workflowInstanceId, true)) {\n+            status == Workflow.WorkflowStatus.RUNNING\n+            tasks.size() == 5\n+            tasks[2].status == Task.Status.SCHEDULED\n+            tasks[2].taskType == 'integration_task_2'\n+            tasks[3].status == Task.Status.IN_PROGRESS\n+            tasks[3].taskType == 'JOIN'\n+            tasks[4].status == Task.Status.COMPLETED\n+            tasks[4].taskType == 'integration_task_3'\n+        }\n+\n+        when: \"The other node of the fork is completed by completing 'integration_task_2'\"\n+        def polledAndAckTask2Try1 = workflowTestUtil.pollAndCompleteTask('integration_task_2',\n+                'task1.worker', [:], 0)\n+\n+        then: \"verify that the 'integration_task_2' was polled and acknowledged\"\n+        workflowTestUtil.verifyPolledAndAcknowledgedTask([:], polledAndAckTask2Try1)\n+\n+        and: \"The workflow has been updated with the task status and task list\"\n+        with(workflowExecutionService.getExecutionStatus(workflowInstanceId, true)) {\n+            status == Workflow.WorkflowStatus.RUNNING\n+            tasks.size() == 6\n+            tasks[2].status == Task.Status.COMPLETED\n+            tasks[2].taskType == 'integration_task_2'\n+            tasks[3].status == Task.Status.COMPLETED\n+            tasks[3].taskType == 'JOIN'\n+            tasks[5].status == Task.Status.SCHEDULED\n+            tasks[5].taskType == 'integration_task_4'\n+        }\n+\n+        when: \"The last task of the workflow is then polled and completed integration_task_4'\"\n+        def polledAndAckTask4Try1 = workflowTestUtil.pollAndCompleteTask('integration_task_4',\n+                'task1.worker', [:], 0)\n+\n+        then: \"verify that the 'integration_task_4' was polled and acknowledged\"\n+        workflowTestUtil.verifyPolledAndAcknowledgedTask([:], polledAndAckTask4Try1)\n+\n+        and: \"Then verify that the workflow is completed\"\n+        with(workflowExecutionService.getExecutionStatus(workflowInstanceId, true)) {\n+            status == Workflow.WorkflowStatus.COMPLETED\n+            tasks.size() == 6\n+            tasks[5].status == Task.Status.COMPLETED\n+            tasks[5].taskType == 'integration_task_4'\n+        }\n+\n+        cleanup: \"Restore the task definitions that were modified as part of this feature testing\"\n+        metadataService.updateTaskDef(persistedIntegrationTask1Definition)\n+        metadataService.updateTaskDef(persistedIntegrationTask2Definition)\n+        metadataService.updateTaskDef(persistedIntegrationTask3Definition)\n+        metadataService.updateTaskDef(persistedIntegrationTask4Definition)\n+    }\n+\n+\n+    def \"Test a simple workflow with fork join failure flow\"() {\n+        setup: \"Ensure that 'integration_task_2' has a retry count of 0\"\n+        def persistedIntegrationTask2Definition = workflowTestUtil.getPersistedTaskDefinition('integration_task_2').get()\n+        def modifiedIntegrationTask2Definition = new TaskDef(persistedIntegrationTask2Definition.name,\n+                persistedIntegrationTask2Definition.description, 0, 0)\n+        metadataService.updateTaskDef(modifiedIntegrationTask2Definition)\n+\n+        when: \"A fork join workflow is started\"\n+        def workflowInstanceId = workflowExecutor.startWorkflow(FORK_JOIN_WF, 1,\n+                'fanoutTest', [:],\n+                null, null, null)\n+\n+        then: \"verify that the workflow has started and the starting nodes of the each fork are in scheduled state\"\n+        workflowInstanceId\n+        with(workflowExecutionService.getExecutionStatus(workflowInstanceId, true)) {\n+            status == Workflow.WorkflowStatus.RUNNING\n+            tasks.size() == 4\n+            tasks[0].status == Task.Status.COMPLETED\n+            tasks[0].taskType == 'FORK'\n+            tasks[1].status == Task.Status.SCHEDULED\n+            tasks[1].taskType == 'integration_task_1'\n+            tasks[2].status == Task.Status.SCHEDULED\n+            tasks[2].taskType == 'integration_task_2'\n+            tasks[3].status == Task.Status.IN_PROGRESS\n+            tasks[3].taskType == 'JOIN'\n+        }\n+\n+        when: \"The first task of the fork is polled and completed\"\n+        def polledAndAckTask1Try1 = workflowTestUtil.pollAndCompleteTask('integration_task_1', 'task1.worker', [:], 0)\n+\n+        then: \"verify that the 'integration_task_1' was polled and acknowledged\"\n+        workflowTestUtil.verifyPolledAndAcknowledgedTask([:], polledAndAckTask1Try1)\n+\n+        and: \"The workflow has been updated and has all the required tasks in the right status to move forward\"\n+        with(workflowExecutionService.getExecutionStatus(workflowInstanceId, true)) {\n+            status == Workflow.WorkflowStatus.RUNNING\n+            tasks.size() == 5\n+            tasks[1].status == Task.Status.COMPLETED\n+            tasks[1].taskType == 'integration_task_1'\n+            tasks[2].status == Task.Status.SCHEDULED\n+            tasks[2].taskType == 'integration_task_2'\n+            tasks[3].status == Task.Status.IN_PROGRESS\n+            tasks[3].taskType == 'JOIN'\n+            tasks[4].status == Task.Status.SCHEDULED\n+            tasks[4].taskType == 'integration_task_3'\n+        }\n+\n+        when: \"The other node of the fork is completed by completing 'integration_task_2'\"\n+        def polledAndAckTask2Try1 = workflowTestUtil.pollAndFailTask('integration_task_2',\n+                'task1.worker', 'Failed....', 0)\n+\n+        then: \"verify that the 'integration_task_2' was polled and acknowledged\"\n+        workflowTestUtil.verifyPolledAndAcknowledgedTask([:], polledAndAckTask2Try1)\n+\n+        and: \"the workflow is in the failed state\"\n+        with(workflowExecutionService.getExecutionStatus(workflowInstanceId, true)) {\n+            status == Workflow.WorkflowStatus.FAILED\n+            tasks.size() == 5\n+            tasks[1].status == Task.Status.COMPLETED\n+            tasks[1].taskType == 'integration_task_1'\n+            tasks[2].status == Task.Status.FAILED\n+            tasks[2].taskType == 'integration_task_2'\n+            tasks[3].status == Task.Status.CANCELED\n+            tasks[3].taskType == 'JOIN'\n+            tasks[4].status == Task.Status.CANCELED\n+            tasks[4].taskType == 'integration_task_3'\n+        }\n+\n+        cleanup: \"Restore the task definitions that were modified as part of this feature testing\"\n+        metadataService.updateTaskDef(persistedIntegrationTask2Definition)\n+    }\n+\n+    def \"Test restarting a failed fork join workflow\"() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1477c32bb512e28376306f774951d89f9da70b69"}, "originalPosition": 270}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzc3NTI5Nw==", "bodyText": "restarted -> retried", "url": "https://github.com/Netflix/conductor/pull/1691#discussion_r427775297", "createdAt": "2020-05-20T06:43:24Z", "author": {"login": "apanicker-nflx"}, "path": "test-harness/src/test/groovy/com/netflix/counductor/integration/test/ForkJoinSpec.groovy", "diffHunk": "@@ -0,0 +1,874 @@\n+package com.netflix.counductor.integration.test\n+\n+import com.netflix.archaius.guice.ArchaiusModule\n+import com.netflix.conductor.common.metadata.tasks.Task\n+import com.netflix.conductor.common.metadata.tasks.TaskDef\n+import com.netflix.conductor.common.run.Workflow\n+import com.netflix.conductor.core.execution.WorkflowExecutor\n+import com.netflix.conductor.core.execution.tasks.SubWorkflow\n+import com.netflix.conductor.service.ExecutionService\n+import com.netflix.conductor.service.MetadataService\n+import com.netflix.conductor.test.util.WorkflowTestUtil\n+import com.netflix.conductor.tests.utils.TestModule\n+import com.netflix.governator.guice.test.ModulesForTesting\n+import spock.lang.Shared\n+import spock.lang.Specification\n+\n+import javax.inject.Inject\n+\n+@ModulesForTesting([TestModule.class, ArchaiusModule.class])\n+class ForkJoinSpec extends Specification {\n+\n+    @Inject\n+    ExecutionService workflowExecutionService\n+\n+    @Inject\n+    MetadataService metadataService\n+\n+    @Inject\n+    WorkflowExecutor workflowExecutor\n+\n+    @Inject\n+    WorkflowTestUtil workflowTestUtil\n+\n+    @Shared\n+    def FORK_JOIN_WF = 'FanInOutTest'\n+\n+    @Shared\n+    def FORK_JOIN_NESTED_WF = 'FanInOutNestedTest'\n+\n+    @Shared\n+    def FORK_JOIN_NESTED_SUB_WF = 'FanInOutNestedSubWorkflowTest'\n+\n+    @Shared\n+    def WORKFLOW_FORK_JOIN_OPTIONAL_SW = \"integration_test_fork_join_optional_sw\"\n+\n+    def cleanup() {\n+        workflowTestUtil.clearWorkflows()\n+    }\n+\n+    def setup() {\n+        workflowTestUtil.registerWorkflows('fork_join_integration_test.json',\n+                'fork_join_with_no_task_retry_integration_test.json',\n+                'nested_fork_join_integration_test.json',\n+                'simple_workflow_1_integration_test.json',\n+                'nested_fork_join_with_sub_workflow_integration_test.json',\n+                'simple_one_task_sub_workflow_integration_test.json',\n+                'fork_join_with_optional_sub_workflow_forks_integration_test.json'\n+        )\n+    }\n+\n+    /**\n+     *               start\n+     *                 |\n+     *               fork\n+     *              /    \\\n+     *         task1     task2\n+     *          \\        /\n+     *          task3   /\n+     *           \\     /\n+     *            \\  /\n+     *            join\n+     *              |\n+     *             task4\n+     *              |\n+     *             End\n+     */\n+    def \"Test a simple workflow with fork join success flow\"() {\n+        setup: \"Ensure that all the tasks involved in the workflow have a retry count of 0\"\n+        def persistedIntegrationTask1Definition = workflowTestUtil.getPersistedTaskDefinition('integration_task_1').get()\n+        def modifiedIntegrationTask1Definition = new TaskDef(persistedIntegrationTask1Definition.name,\n+                persistedIntegrationTask1Definition.description, 0, 0)\n+        def persistedIntegrationTask2Definition = workflowTestUtil.getPersistedTaskDefinition('integration_task_2').get()\n+        def modifiedIntegrationTask2Definition = new TaskDef(persistedIntegrationTask2Definition.name,\n+                persistedIntegrationTask2Definition.description, 0, 0)\n+        def persistedIntegrationTask3Definition = workflowTestUtil.getPersistedTaskDefinition('integration_task_3').get()\n+        def modifiedIntegrationTask3Definition = new TaskDef(persistedIntegrationTask3Definition.name,\n+                persistedIntegrationTask3Definition.description, 0, 0)\n+        def persistedIntegrationTask4Definition = workflowTestUtil.getPersistedTaskDefinition('integration_task_4').get()\n+        def modifiedIntegrationTask4Definition = new TaskDef(persistedIntegrationTask4Definition.name,\n+                persistedIntegrationTask4Definition.description, 0, 0)\n+\n+        metadataService.updateTaskDef(modifiedIntegrationTask1Definition)\n+        metadataService.updateTaskDef(modifiedIntegrationTask2Definition)\n+        metadataService.updateTaskDef(modifiedIntegrationTask3Definition)\n+        metadataService.updateTaskDef(modifiedIntegrationTask4Definition)\n+\n+        when: \"A fork join workflow is started\"\n+        def workflowInstanceId = workflowExecutor.startWorkflow(FORK_JOIN_WF, 1,\n+                'fanoutTest', [:],\n+                null, null, null)\n+\n+        then: \"verify that the workflow has started and the starting nodes of the each fork are in scheduled state\"\n+        workflowInstanceId\n+        with(workflowExecutionService.getExecutionStatus(workflowInstanceId, true)) {\n+            status == Workflow.WorkflowStatus.RUNNING\n+            tasks.size() == 4\n+            tasks[0].status == Task.Status.COMPLETED\n+            tasks[0].taskType == 'FORK'\n+            tasks[1].status == Task.Status.SCHEDULED\n+            tasks[1].taskType == 'integration_task_1'\n+            tasks[2].status == Task.Status.SCHEDULED\n+            tasks[2].taskType == 'integration_task_2'\n+            tasks[3].status == Task.Status.IN_PROGRESS\n+            tasks[3].taskType == 'JOIN'\n+        }\n+\n+        when: \"The first task of the fork is polled and completed\"\n+        def polledAndAckTask1Try1 = workflowTestUtil.pollAndCompleteTask('integration_task_1', 'task1.worker', [:], 0)\n+\n+        then: \"verify that the 'integration_task_1' was polled and acknowledged\"\n+        workflowTestUtil.verifyPolledAndAcknowledgedTask([:], polledAndAckTask1Try1)\n+\n+        and: \"The workflow has been updated and has all the required tasks in the right status to move forward\"\n+        with(workflowExecutionService.getExecutionStatus(workflowInstanceId, true)) {\n+            status == Workflow.WorkflowStatus.RUNNING\n+            tasks.size() == 5\n+            tasks[1].status == Task.Status.COMPLETED\n+            tasks[1].taskType == 'integration_task_1'\n+            tasks[2].status == Task.Status.SCHEDULED\n+            tasks[2].taskType == 'integration_task_2'\n+            tasks[3].status == Task.Status.IN_PROGRESS\n+            tasks[3].taskType == 'JOIN'\n+            tasks[4].status == Task.Status.SCHEDULED\n+            tasks[4].taskType == 'integration_task_3'\n+        }\n+\n+        when: \"The 'integration_task_3' is polled and completed\"\n+        def polledAndAckTask3Try1 = workflowTestUtil.pollAndCompleteTask('integration_task_3',\n+                'task1.worker', [:], 0)\n+\n+        then: \"verify that the 'integration_task_3' was polled and acknowledged\"\n+        workflowTestUtil.verifyPolledAndAcknowledgedTask([:], polledAndAckTask3Try1)\n+\n+        and: \"The workflow has been updated with the task status and task list\"\n+        with(workflowExecutionService.getExecutionStatus(workflowInstanceId, true)) {\n+            status == Workflow.WorkflowStatus.RUNNING\n+            tasks.size() == 5\n+            tasks[2].status == Task.Status.SCHEDULED\n+            tasks[2].taskType == 'integration_task_2'\n+            tasks[3].status == Task.Status.IN_PROGRESS\n+            tasks[3].taskType == 'JOIN'\n+            tasks[4].status == Task.Status.COMPLETED\n+            tasks[4].taskType == 'integration_task_3'\n+        }\n+\n+        when: \"The other node of the fork is completed by completing 'integration_task_2'\"\n+        def polledAndAckTask2Try1 = workflowTestUtil.pollAndCompleteTask('integration_task_2',\n+                'task1.worker', [:], 0)\n+\n+        then: \"verify that the 'integration_task_2' was polled and acknowledged\"\n+        workflowTestUtil.verifyPolledAndAcknowledgedTask([:], polledAndAckTask2Try1)\n+\n+        and: \"The workflow has been updated with the task status and task list\"\n+        with(workflowExecutionService.getExecutionStatus(workflowInstanceId, true)) {\n+            status == Workflow.WorkflowStatus.RUNNING\n+            tasks.size() == 6\n+            tasks[2].status == Task.Status.COMPLETED\n+            tasks[2].taskType == 'integration_task_2'\n+            tasks[3].status == Task.Status.COMPLETED\n+            tasks[3].taskType == 'JOIN'\n+            tasks[5].status == Task.Status.SCHEDULED\n+            tasks[5].taskType == 'integration_task_4'\n+        }\n+\n+        when: \"The last task of the workflow is then polled and completed integration_task_4'\"\n+        def polledAndAckTask4Try1 = workflowTestUtil.pollAndCompleteTask('integration_task_4',\n+                'task1.worker', [:], 0)\n+\n+        then: \"verify that the 'integration_task_4' was polled and acknowledged\"\n+        workflowTestUtil.verifyPolledAndAcknowledgedTask([:], polledAndAckTask4Try1)\n+\n+        and: \"Then verify that the workflow is completed\"\n+        with(workflowExecutionService.getExecutionStatus(workflowInstanceId, true)) {\n+            status == Workflow.WorkflowStatus.COMPLETED\n+            tasks.size() == 6\n+            tasks[5].status == Task.Status.COMPLETED\n+            tasks[5].taskType == 'integration_task_4'\n+        }\n+\n+        cleanup: \"Restore the task definitions that were modified as part of this feature testing\"\n+        metadataService.updateTaskDef(persistedIntegrationTask1Definition)\n+        metadataService.updateTaskDef(persistedIntegrationTask2Definition)\n+        metadataService.updateTaskDef(persistedIntegrationTask3Definition)\n+        metadataService.updateTaskDef(persistedIntegrationTask4Definition)\n+    }\n+\n+\n+    def \"Test a simple workflow with fork join failure flow\"() {\n+        setup: \"Ensure that 'integration_task_2' has a retry count of 0\"\n+        def persistedIntegrationTask2Definition = workflowTestUtil.getPersistedTaskDefinition('integration_task_2').get()\n+        def modifiedIntegrationTask2Definition = new TaskDef(persistedIntegrationTask2Definition.name,\n+                persistedIntegrationTask2Definition.description, 0, 0)\n+        metadataService.updateTaskDef(modifiedIntegrationTask2Definition)\n+\n+        when: \"A fork join workflow is started\"\n+        def workflowInstanceId = workflowExecutor.startWorkflow(FORK_JOIN_WF, 1,\n+                'fanoutTest', [:],\n+                null, null, null)\n+\n+        then: \"verify that the workflow has started and the starting nodes of the each fork are in scheduled state\"\n+        workflowInstanceId\n+        with(workflowExecutionService.getExecutionStatus(workflowInstanceId, true)) {\n+            status == Workflow.WorkflowStatus.RUNNING\n+            tasks.size() == 4\n+            tasks[0].status == Task.Status.COMPLETED\n+            tasks[0].taskType == 'FORK'\n+            tasks[1].status == Task.Status.SCHEDULED\n+            tasks[1].taskType == 'integration_task_1'\n+            tasks[2].status == Task.Status.SCHEDULED\n+            tasks[2].taskType == 'integration_task_2'\n+            tasks[3].status == Task.Status.IN_PROGRESS\n+            tasks[3].taskType == 'JOIN'\n+        }\n+\n+        when: \"The first task of the fork is polled and completed\"\n+        def polledAndAckTask1Try1 = workflowTestUtil.pollAndCompleteTask('integration_task_1', 'task1.worker', [:], 0)\n+\n+        then: \"verify that the 'integration_task_1' was polled and acknowledged\"\n+        workflowTestUtil.verifyPolledAndAcknowledgedTask([:], polledAndAckTask1Try1)\n+\n+        and: \"The workflow has been updated and has all the required tasks in the right status to move forward\"\n+        with(workflowExecutionService.getExecutionStatus(workflowInstanceId, true)) {\n+            status == Workflow.WorkflowStatus.RUNNING\n+            tasks.size() == 5\n+            tasks[1].status == Task.Status.COMPLETED\n+            tasks[1].taskType == 'integration_task_1'\n+            tasks[2].status == Task.Status.SCHEDULED\n+            tasks[2].taskType == 'integration_task_2'\n+            tasks[3].status == Task.Status.IN_PROGRESS\n+            tasks[3].taskType == 'JOIN'\n+            tasks[4].status == Task.Status.SCHEDULED\n+            tasks[4].taskType == 'integration_task_3'\n+        }\n+\n+        when: \"The other node of the fork is completed by completing 'integration_task_2'\"\n+        def polledAndAckTask2Try1 = workflowTestUtil.pollAndFailTask('integration_task_2',\n+                'task1.worker', 'Failed....', 0)\n+\n+        then: \"verify that the 'integration_task_2' was polled and acknowledged\"\n+        workflowTestUtil.verifyPolledAndAcknowledgedTask([:], polledAndAckTask2Try1)\n+\n+        and: \"the workflow is in the failed state\"\n+        with(workflowExecutionService.getExecutionStatus(workflowInstanceId, true)) {\n+            status == Workflow.WorkflowStatus.FAILED\n+            tasks.size() == 5\n+            tasks[1].status == Task.Status.COMPLETED\n+            tasks[1].taskType == 'integration_task_1'\n+            tasks[2].status == Task.Status.FAILED\n+            tasks[2].taskType == 'integration_task_2'\n+            tasks[3].status == Task.Status.CANCELED\n+            tasks[3].taskType == 'JOIN'\n+            tasks[4].status == Task.Status.CANCELED\n+            tasks[4].taskType == 'integration_task_3'\n+        }\n+\n+        cleanup: \"Restore the task definitions that were modified as part of this feature testing\"\n+        metadataService.updateTaskDef(persistedIntegrationTask2Definition)\n+    }\n+\n+    def \"Test restarting a failed fork join workflow\"() {\n+\n+        when: \"A fork join workflow is started\"\n+        def workflowInstanceId = workflowExecutor.startWorkflow(FORK_JOIN_WF + '_2', 1,\n+                'fanoutTest', [:],\n+                null, null, null)\n+\n+        then: \"verify that the workflow has started and the starting nodes of the each fork are in scheduled state\"\n+        workflowInstanceId\n+        with(workflowExecutionService.getExecutionStatus(workflowInstanceId, true)) {\n+            status == Workflow.WorkflowStatus.RUNNING\n+            tasks.size() == 4\n+            tasks[0].status == Task.Status.COMPLETED\n+            tasks[0].taskType == 'FORK'\n+            tasks[1].status == Task.Status.SCHEDULED\n+            tasks[1].taskType == 'integration_task_0_RT_1'\n+            tasks[2].status == Task.Status.SCHEDULED\n+            tasks[2].taskType == 'integration_task_0_RT_2'\n+            tasks[3].status == Task.Status.IN_PROGRESS\n+            tasks[3].taskType == 'JOIN'\n+        }\n+\n+        when: \"The first task of the fork is polled and completed\"\n+        def polledAndAckTask1Try1 = workflowTestUtil.pollAndCompleteTask('integration_task_0_RT_1', 'task1.worker', [:], 0)\n+\n+        then: \"verify that the 'integration_task_0_RT_1' was polled and acknowledged\"\n+        workflowTestUtil.verifyPolledAndAcknowledgedTask([:], polledAndAckTask1Try1)\n+\n+        and: \"The workflow has been updated and has all the required tasks in the right status to move forward\"\n+        with(workflowExecutionService.getExecutionStatus(workflowInstanceId, true)) {\n+            status == Workflow.WorkflowStatus.RUNNING\n+            tasks.size() == 5\n+            tasks[1].status == Task.Status.COMPLETED\n+            tasks[1].taskType == 'integration_task_0_RT_1'\n+            tasks[2].status == Task.Status.SCHEDULED\n+            tasks[2].taskType == 'integration_task_0_RT_2'\n+            tasks[3].status == Task.Status.IN_PROGRESS\n+            tasks[3].taskType == 'JOIN'\n+            tasks[4].status == Task.Status.SCHEDULED\n+            tasks[4].taskType == 'integration_task_0_RT_3'\n+        }\n+\n+        when: \"The other node of the fork is completed by completing 'integration_task_0_RT_2'\"\n+        def polledAndAckTask2Try1 = workflowTestUtil.pollAndFailTask('integration_task_0_RT_2',\n+                'task1.worker', 'Failed....', 0)\n+\n+        then: \"verify that the 'integration_task_0_RT_1' was polled and acknowledged\"\n+        workflowTestUtil.verifyPolledAndAcknowledgedTask([:], polledAndAckTask2Try1)\n+\n+        and: \"the workflow is in the failed state\"\n+        with(workflowExecutionService.getExecutionStatus(workflowInstanceId, true)) {\n+            status == Workflow.WorkflowStatus.FAILED\n+            tasks.size() == 5\n+            tasks[1].status == Task.Status.COMPLETED\n+            tasks[1].taskType == 'integration_task_0_RT_1'\n+            tasks[2].status == Task.Status.FAILED\n+            tasks[2].taskType == 'integration_task_0_RT_2'\n+            tasks[3].status == Task.Status.CANCELED\n+            tasks[3].taskType == 'JOIN'\n+            tasks[4].status == Task.Status.CANCELED\n+            tasks[4].taskType == 'integration_task_0_RT_3'\n+        }\n+\n+        when: \"The workflow is retried\"\n+        workflowExecutor.retry(workflowInstanceId)\n+\n+        then: \"verify that all the workflow is restarted and new tasks are added in place of the failed tasks\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1477c32bb512e28376306f774951d89f9da70b69"}, "originalPosition": 336}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "87b9ce4c05844ab78e9259c96a25ed51f7234610", "author": {"user": null}, "url": "https://github.com/Netflix/conductor/commit/87b9ce4c05844ab78e9259c96a25ed51f7234610", "committedDate": "2020-05-20T17:06:58Z", "message": "Code review clean up"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 788, "cost": 1, "resetAt": "2021-11-01T15:33:45Z"}}}