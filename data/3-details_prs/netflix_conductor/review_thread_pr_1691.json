{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDIwMjY3Mjc1", "number": 1691, "reviewThreads": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQwNjozNzowNlrOD-E7eg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQwNjo0MzoyNFrOD-FCJQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2NDE5MDY2OnYy", "diffSide": "RIGHT", "path": "test-harness/src/test/groovy/com/netflix/counductor/integration/test/ForkJoinSpec.groovy", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQwNjozNzowNlrOGX9K8g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQwNjozNzowNlrOGX9K8g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzc3MjY1OA==", "bodyText": "Please add license header.", "url": "https://github.com/Netflix/conductor/pull/1691#discussion_r427772658", "createdAt": "2020-05-20T06:37:06Z", "author": {"login": "apanicker-nflx"}, "path": "test-harness/src/test/groovy/com/netflix/counductor/integration/test/ForkJoinSpec.groovy", "diffHunk": "@@ -0,0 +1,874 @@\n+package com.netflix.counductor.integration.test", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1477c32bb512e28376306f774951d89f9da70b69"}, "originalPosition": 1}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2NDE5NjYxOnYy", "diffSide": "RIGHT", "path": "test-harness/src/test/groovy/com/netflix/counductor/integration/test/ForkJoinSpec.groovy", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQwNjozOToyM1rOGX9OiA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxNjozMDozOVrOGYUFlA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzc3MzU3Ng==", "bodyText": "I believe this was done to maintain consistency with the existing JUnit tests, but this spec does not use/need modification of retry counts, so this could be removed.", "url": "https://github.com/Netflix/conductor/pull/1691#discussion_r427773576", "createdAt": "2020-05-20T06:39:23Z", "author": {"login": "apanicker-nflx"}, "path": "test-harness/src/test/groovy/com/netflix/counductor/integration/test/ForkJoinSpec.groovy", "diffHunk": "@@ -0,0 +1,874 @@\n+package com.netflix.counductor.integration.test\n+\n+import com.netflix.archaius.guice.ArchaiusModule\n+import com.netflix.conductor.common.metadata.tasks.Task\n+import com.netflix.conductor.common.metadata.tasks.TaskDef\n+import com.netflix.conductor.common.run.Workflow\n+import com.netflix.conductor.core.execution.WorkflowExecutor\n+import com.netflix.conductor.core.execution.tasks.SubWorkflow\n+import com.netflix.conductor.service.ExecutionService\n+import com.netflix.conductor.service.MetadataService\n+import com.netflix.conductor.test.util.WorkflowTestUtil\n+import com.netflix.conductor.tests.utils.TestModule\n+import com.netflix.governator.guice.test.ModulesForTesting\n+import spock.lang.Shared\n+import spock.lang.Specification\n+\n+import javax.inject.Inject\n+\n+@ModulesForTesting([TestModule.class, ArchaiusModule.class])\n+class ForkJoinSpec extends Specification {\n+\n+    @Inject\n+    ExecutionService workflowExecutionService\n+\n+    @Inject\n+    MetadataService metadataService\n+\n+    @Inject\n+    WorkflowExecutor workflowExecutor\n+\n+    @Inject\n+    WorkflowTestUtil workflowTestUtil\n+\n+    @Shared\n+    def FORK_JOIN_WF = 'FanInOutTest'\n+\n+    @Shared\n+    def FORK_JOIN_NESTED_WF = 'FanInOutNestedTest'\n+\n+    @Shared\n+    def FORK_JOIN_NESTED_SUB_WF = 'FanInOutNestedSubWorkflowTest'\n+\n+    @Shared\n+    def WORKFLOW_FORK_JOIN_OPTIONAL_SW = \"integration_test_fork_join_optional_sw\"\n+\n+    def cleanup() {\n+        workflowTestUtil.clearWorkflows()\n+    }\n+\n+    def setup() {\n+        workflowTestUtil.registerWorkflows('fork_join_integration_test.json',\n+                'fork_join_with_no_task_retry_integration_test.json',\n+                'nested_fork_join_integration_test.json',\n+                'simple_workflow_1_integration_test.json',\n+                'nested_fork_join_with_sub_workflow_integration_test.json',\n+                'simple_one_task_sub_workflow_integration_test.json',\n+                'fork_join_with_optional_sub_workflow_forks_integration_test.json'\n+        )\n+    }\n+\n+    /**\n+     *               start\n+     *                 |\n+     *               fork\n+     *              /    \\\n+     *         task1     task2\n+     *          \\        /\n+     *          task3   /\n+     *           \\     /\n+     *            \\  /\n+     *            join\n+     *              |\n+     *             task4\n+     *              |\n+     *             End\n+     */\n+    def \"Test a simple workflow with fork join success flow\"() {\n+        setup: \"Ensure that all the tasks involved in the workflow have a retry count of 0\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1477c32bb512e28376306f774951d89f9da70b69"}, "originalPosition": 78}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODE0ODExNg==", "bodyText": "This was done before using the fork_join_workflow_without_retryable_tasks , this test can be changed to use a different workflow and the setup and cleanup are not needed.", "url": "https://github.com/Netflix/conductor/pull/1691#discussion_r428148116", "createdAt": "2020-05-20T16:30:39Z", "author": {"login": "pctreddy"}, "path": "test-harness/src/test/groovy/com/netflix/counductor/integration/test/ForkJoinSpec.groovy", "diffHunk": "@@ -0,0 +1,874 @@\n+package com.netflix.counductor.integration.test\n+\n+import com.netflix.archaius.guice.ArchaiusModule\n+import com.netflix.conductor.common.metadata.tasks.Task\n+import com.netflix.conductor.common.metadata.tasks.TaskDef\n+import com.netflix.conductor.common.run.Workflow\n+import com.netflix.conductor.core.execution.WorkflowExecutor\n+import com.netflix.conductor.core.execution.tasks.SubWorkflow\n+import com.netflix.conductor.service.ExecutionService\n+import com.netflix.conductor.service.MetadataService\n+import com.netflix.conductor.test.util.WorkflowTestUtil\n+import com.netflix.conductor.tests.utils.TestModule\n+import com.netflix.governator.guice.test.ModulesForTesting\n+import spock.lang.Shared\n+import spock.lang.Specification\n+\n+import javax.inject.Inject\n+\n+@ModulesForTesting([TestModule.class, ArchaiusModule.class])\n+class ForkJoinSpec extends Specification {\n+\n+    @Inject\n+    ExecutionService workflowExecutionService\n+\n+    @Inject\n+    MetadataService metadataService\n+\n+    @Inject\n+    WorkflowExecutor workflowExecutor\n+\n+    @Inject\n+    WorkflowTestUtil workflowTestUtil\n+\n+    @Shared\n+    def FORK_JOIN_WF = 'FanInOutTest'\n+\n+    @Shared\n+    def FORK_JOIN_NESTED_WF = 'FanInOutNestedTest'\n+\n+    @Shared\n+    def FORK_JOIN_NESTED_SUB_WF = 'FanInOutNestedSubWorkflowTest'\n+\n+    @Shared\n+    def WORKFLOW_FORK_JOIN_OPTIONAL_SW = \"integration_test_fork_join_optional_sw\"\n+\n+    def cleanup() {\n+        workflowTestUtil.clearWorkflows()\n+    }\n+\n+    def setup() {\n+        workflowTestUtil.registerWorkflows('fork_join_integration_test.json',\n+                'fork_join_with_no_task_retry_integration_test.json',\n+                'nested_fork_join_integration_test.json',\n+                'simple_workflow_1_integration_test.json',\n+                'nested_fork_join_with_sub_workflow_integration_test.json',\n+                'simple_one_task_sub_workflow_integration_test.json',\n+                'fork_join_with_optional_sub_workflow_forks_integration_test.json'\n+        )\n+    }\n+\n+    /**\n+     *               start\n+     *                 |\n+     *               fork\n+     *              /    \\\n+     *         task1     task2\n+     *          \\        /\n+     *          task3   /\n+     *           \\     /\n+     *            \\  /\n+     *            join\n+     *              |\n+     *             task4\n+     *              |\n+     *             End\n+     */\n+    def \"Test a simple workflow with fork join success flow\"() {\n+        setup: \"Ensure that all the tasks involved in the workflow have a retry count of 0\"", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzc3MzU3Ng=="}, "originalCommit": {"oid": "1477c32bb512e28376306f774951d89f9da70b69"}, "originalPosition": 78}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2NDIwMDk5OnYy", "diffSide": "RIGHT", "path": "test-harness/src/test/groovy/com/netflix/counductor/integration/test/ForkJoinSpec.groovy", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQwNjo0MDo1NlrOGX9RLA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQwNjo0MDo1NlrOGX9RLA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzc3NDI1Mg==", "bodyText": "restarting -> retrying", "url": "https://github.com/Netflix/conductor/pull/1691#discussion_r427774252", "createdAt": "2020-05-20T06:40:56Z", "author": {"login": "apanicker-nflx"}, "path": "test-harness/src/test/groovy/com/netflix/counductor/integration/test/ForkJoinSpec.groovy", "diffHunk": "@@ -0,0 +1,874 @@\n+package com.netflix.counductor.integration.test\n+\n+import com.netflix.archaius.guice.ArchaiusModule\n+import com.netflix.conductor.common.metadata.tasks.Task\n+import com.netflix.conductor.common.metadata.tasks.TaskDef\n+import com.netflix.conductor.common.run.Workflow\n+import com.netflix.conductor.core.execution.WorkflowExecutor\n+import com.netflix.conductor.core.execution.tasks.SubWorkflow\n+import com.netflix.conductor.service.ExecutionService\n+import com.netflix.conductor.service.MetadataService\n+import com.netflix.conductor.test.util.WorkflowTestUtil\n+import com.netflix.conductor.tests.utils.TestModule\n+import com.netflix.governator.guice.test.ModulesForTesting\n+import spock.lang.Shared\n+import spock.lang.Specification\n+\n+import javax.inject.Inject\n+\n+@ModulesForTesting([TestModule.class, ArchaiusModule.class])\n+class ForkJoinSpec extends Specification {\n+\n+    @Inject\n+    ExecutionService workflowExecutionService\n+\n+    @Inject\n+    MetadataService metadataService\n+\n+    @Inject\n+    WorkflowExecutor workflowExecutor\n+\n+    @Inject\n+    WorkflowTestUtil workflowTestUtil\n+\n+    @Shared\n+    def FORK_JOIN_WF = 'FanInOutTest'\n+\n+    @Shared\n+    def FORK_JOIN_NESTED_WF = 'FanInOutNestedTest'\n+\n+    @Shared\n+    def FORK_JOIN_NESTED_SUB_WF = 'FanInOutNestedSubWorkflowTest'\n+\n+    @Shared\n+    def WORKFLOW_FORK_JOIN_OPTIONAL_SW = \"integration_test_fork_join_optional_sw\"\n+\n+    def cleanup() {\n+        workflowTestUtil.clearWorkflows()\n+    }\n+\n+    def setup() {\n+        workflowTestUtil.registerWorkflows('fork_join_integration_test.json',\n+                'fork_join_with_no_task_retry_integration_test.json',\n+                'nested_fork_join_integration_test.json',\n+                'simple_workflow_1_integration_test.json',\n+                'nested_fork_join_with_sub_workflow_integration_test.json',\n+                'simple_one_task_sub_workflow_integration_test.json',\n+                'fork_join_with_optional_sub_workflow_forks_integration_test.json'\n+        )\n+    }\n+\n+    /**\n+     *               start\n+     *                 |\n+     *               fork\n+     *              /    \\\n+     *         task1     task2\n+     *          \\        /\n+     *          task3   /\n+     *           \\     /\n+     *            \\  /\n+     *            join\n+     *              |\n+     *             task4\n+     *              |\n+     *             End\n+     */\n+    def \"Test a simple workflow with fork join success flow\"() {\n+        setup: \"Ensure that all the tasks involved in the workflow have a retry count of 0\"\n+        def persistedIntegrationTask1Definition = workflowTestUtil.getPersistedTaskDefinition('integration_task_1').get()\n+        def modifiedIntegrationTask1Definition = new TaskDef(persistedIntegrationTask1Definition.name,\n+                persistedIntegrationTask1Definition.description, 0, 0)\n+        def persistedIntegrationTask2Definition = workflowTestUtil.getPersistedTaskDefinition('integration_task_2').get()\n+        def modifiedIntegrationTask2Definition = new TaskDef(persistedIntegrationTask2Definition.name,\n+                persistedIntegrationTask2Definition.description, 0, 0)\n+        def persistedIntegrationTask3Definition = workflowTestUtil.getPersistedTaskDefinition('integration_task_3').get()\n+        def modifiedIntegrationTask3Definition = new TaskDef(persistedIntegrationTask3Definition.name,\n+                persistedIntegrationTask3Definition.description, 0, 0)\n+        def persistedIntegrationTask4Definition = workflowTestUtil.getPersistedTaskDefinition('integration_task_4').get()\n+        def modifiedIntegrationTask4Definition = new TaskDef(persistedIntegrationTask4Definition.name,\n+                persistedIntegrationTask4Definition.description, 0, 0)\n+\n+        metadataService.updateTaskDef(modifiedIntegrationTask1Definition)\n+        metadataService.updateTaskDef(modifiedIntegrationTask2Definition)\n+        metadataService.updateTaskDef(modifiedIntegrationTask3Definition)\n+        metadataService.updateTaskDef(modifiedIntegrationTask4Definition)\n+\n+        when: \"A fork join workflow is started\"\n+        def workflowInstanceId = workflowExecutor.startWorkflow(FORK_JOIN_WF, 1,\n+                'fanoutTest', [:],\n+                null, null, null)\n+\n+        then: \"verify that the workflow has started and the starting nodes of the each fork are in scheduled state\"\n+        workflowInstanceId\n+        with(workflowExecutionService.getExecutionStatus(workflowInstanceId, true)) {\n+            status == Workflow.WorkflowStatus.RUNNING\n+            tasks.size() == 4\n+            tasks[0].status == Task.Status.COMPLETED\n+            tasks[0].taskType == 'FORK'\n+            tasks[1].status == Task.Status.SCHEDULED\n+            tasks[1].taskType == 'integration_task_1'\n+            tasks[2].status == Task.Status.SCHEDULED\n+            tasks[2].taskType == 'integration_task_2'\n+            tasks[3].status == Task.Status.IN_PROGRESS\n+            tasks[3].taskType == 'JOIN'\n+        }\n+\n+        when: \"The first task of the fork is polled and completed\"\n+        def polledAndAckTask1Try1 = workflowTestUtil.pollAndCompleteTask('integration_task_1', 'task1.worker', [:], 0)\n+\n+        then: \"verify that the 'integration_task_1' was polled and acknowledged\"\n+        workflowTestUtil.verifyPolledAndAcknowledgedTask([:], polledAndAckTask1Try1)\n+\n+        and: \"The workflow has been updated and has all the required tasks in the right status to move forward\"\n+        with(workflowExecutionService.getExecutionStatus(workflowInstanceId, true)) {\n+            status == Workflow.WorkflowStatus.RUNNING\n+            tasks.size() == 5\n+            tasks[1].status == Task.Status.COMPLETED\n+            tasks[1].taskType == 'integration_task_1'\n+            tasks[2].status == Task.Status.SCHEDULED\n+            tasks[2].taskType == 'integration_task_2'\n+            tasks[3].status == Task.Status.IN_PROGRESS\n+            tasks[3].taskType == 'JOIN'\n+            tasks[4].status == Task.Status.SCHEDULED\n+            tasks[4].taskType == 'integration_task_3'\n+        }\n+\n+        when: \"The 'integration_task_3' is polled and completed\"\n+        def polledAndAckTask3Try1 = workflowTestUtil.pollAndCompleteTask('integration_task_3',\n+                'task1.worker', [:], 0)\n+\n+        then: \"verify that the 'integration_task_3' was polled and acknowledged\"\n+        workflowTestUtil.verifyPolledAndAcknowledgedTask([:], polledAndAckTask3Try1)\n+\n+        and: \"The workflow has been updated with the task status and task list\"\n+        with(workflowExecutionService.getExecutionStatus(workflowInstanceId, true)) {\n+            status == Workflow.WorkflowStatus.RUNNING\n+            tasks.size() == 5\n+            tasks[2].status == Task.Status.SCHEDULED\n+            tasks[2].taskType == 'integration_task_2'\n+            tasks[3].status == Task.Status.IN_PROGRESS\n+            tasks[3].taskType == 'JOIN'\n+            tasks[4].status == Task.Status.COMPLETED\n+            tasks[4].taskType == 'integration_task_3'\n+        }\n+\n+        when: \"The other node of the fork is completed by completing 'integration_task_2'\"\n+        def polledAndAckTask2Try1 = workflowTestUtil.pollAndCompleteTask('integration_task_2',\n+                'task1.worker', [:], 0)\n+\n+        then: \"verify that the 'integration_task_2' was polled and acknowledged\"\n+        workflowTestUtil.verifyPolledAndAcknowledgedTask([:], polledAndAckTask2Try1)\n+\n+        and: \"The workflow has been updated with the task status and task list\"\n+        with(workflowExecutionService.getExecutionStatus(workflowInstanceId, true)) {\n+            status == Workflow.WorkflowStatus.RUNNING\n+            tasks.size() == 6\n+            tasks[2].status == Task.Status.COMPLETED\n+            tasks[2].taskType == 'integration_task_2'\n+            tasks[3].status == Task.Status.COMPLETED\n+            tasks[3].taskType == 'JOIN'\n+            tasks[5].status == Task.Status.SCHEDULED\n+            tasks[5].taskType == 'integration_task_4'\n+        }\n+\n+        when: \"The last task of the workflow is then polled and completed integration_task_4'\"\n+        def polledAndAckTask4Try1 = workflowTestUtil.pollAndCompleteTask('integration_task_4',\n+                'task1.worker', [:], 0)\n+\n+        then: \"verify that the 'integration_task_4' was polled and acknowledged\"\n+        workflowTestUtil.verifyPolledAndAcknowledgedTask([:], polledAndAckTask4Try1)\n+\n+        and: \"Then verify that the workflow is completed\"\n+        with(workflowExecutionService.getExecutionStatus(workflowInstanceId, true)) {\n+            status == Workflow.WorkflowStatus.COMPLETED\n+            tasks.size() == 6\n+            tasks[5].status == Task.Status.COMPLETED\n+            tasks[5].taskType == 'integration_task_4'\n+        }\n+\n+        cleanup: \"Restore the task definitions that were modified as part of this feature testing\"\n+        metadataService.updateTaskDef(persistedIntegrationTask1Definition)\n+        metadataService.updateTaskDef(persistedIntegrationTask2Definition)\n+        metadataService.updateTaskDef(persistedIntegrationTask3Definition)\n+        metadataService.updateTaskDef(persistedIntegrationTask4Definition)\n+    }\n+\n+\n+    def \"Test a simple workflow with fork join failure flow\"() {\n+        setup: \"Ensure that 'integration_task_2' has a retry count of 0\"\n+        def persistedIntegrationTask2Definition = workflowTestUtil.getPersistedTaskDefinition('integration_task_2').get()\n+        def modifiedIntegrationTask2Definition = new TaskDef(persistedIntegrationTask2Definition.name,\n+                persistedIntegrationTask2Definition.description, 0, 0)\n+        metadataService.updateTaskDef(modifiedIntegrationTask2Definition)\n+\n+        when: \"A fork join workflow is started\"\n+        def workflowInstanceId = workflowExecutor.startWorkflow(FORK_JOIN_WF, 1,\n+                'fanoutTest', [:],\n+                null, null, null)\n+\n+        then: \"verify that the workflow has started and the starting nodes of the each fork are in scheduled state\"\n+        workflowInstanceId\n+        with(workflowExecutionService.getExecutionStatus(workflowInstanceId, true)) {\n+            status == Workflow.WorkflowStatus.RUNNING\n+            tasks.size() == 4\n+            tasks[0].status == Task.Status.COMPLETED\n+            tasks[0].taskType == 'FORK'\n+            tasks[1].status == Task.Status.SCHEDULED\n+            tasks[1].taskType == 'integration_task_1'\n+            tasks[2].status == Task.Status.SCHEDULED\n+            tasks[2].taskType == 'integration_task_2'\n+            tasks[3].status == Task.Status.IN_PROGRESS\n+            tasks[3].taskType == 'JOIN'\n+        }\n+\n+        when: \"The first task of the fork is polled and completed\"\n+        def polledAndAckTask1Try1 = workflowTestUtil.pollAndCompleteTask('integration_task_1', 'task1.worker', [:], 0)\n+\n+        then: \"verify that the 'integration_task_1' was polled and acknowledged\"\n+        workflowTestUtil.verifyPolledAndAcknowledgedTask([:], polledAndAckTask1Try1)\n+\n+        and: \"The workflow has been updated and has all the required tasks in the right status to move forward\"\n+        with(workflowExecutionService.getExecutionStatus(workflowInstanceId, true)) {\n+            status == Workflow.WorkflowStatus.RUNNING\n+            tasks.size() == 5\n+            tasks[1].status == Task.Status.COMPLETED\n+            tasks[1].taskType == 'integration_task_1'\n+            tasks[2].status == Task.Status.SCHEDULED\n+            tasks[2].taskType == 'integration_task_2'\n+            tasks[3].status == Task.Status.IN_PROGRESS\n+            tasks[3].taskType == 'JOIN'\n+            tasks[4].status == Task.Status.SCHEDULED\n+            tasks[4].taskType == 'integration_task_3'\n+        }\n+\n+        when: \"The other node of the fork is completed by completing 'integration_task_2'\"\n+        def polledAndAckTask2Try1 = workflowTestUtil.pollAndFailTask('integration_task_2',\n+                'task1.worker', 'Failed....', 0)\n+\n+        then: \"verify that the 'integration_task_2' was polled and acknowledged\"\n+        workflowTestUtil.verifyPolledAndAcknowledgedTask([:], polledAndAckTask2Try1)\n+\n+        and: \"the workflow is in the failed state\"\n+        with(workflowExecutionService.getExecutionStatus(workflowInstanceId, true)) {\n+            status == Workflow.WorkflowStatus.FAILED\n+            tasks.size() == 5\n+            tasks[1].status == Task.Status.COMPLETED\n+            tasks[1].taskType == 'integration_task_1'\n+            tasks[2].status == Task.Status.FAILED\n+            tasks[2].taskType == 'integration_task_2'\n+            tasks[3].status == Task.Status.CANCELED\n+            tasks[3].taskType == 'JOIN'\n+            tasks[4].status == Task.Status.CANCELED\n+            tasks[4].taskType == 'integration_task_3'\n+        }\n+\n+        cleanup: \"Restore the task definitions that were modified as part of this feature testing\"\n+        metadataService.updateTaskDef(persistedIntegrationTask2Definition)\n+    }\n+\n+    def \"Test restarting a failed fork join workflow\"() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1477c32bb512e28376306f774951d89f9da70b69"}, "originalPosition": 270}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2NDIwNzczOnYy", "diffSide": "RIGHT", "path": "test-harness/src/test/groovy/com/netflix/counductor/integration/test/ForkJoinSpec.groovy", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQwNjo0MzoyNFrOGX9VQQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQwNjo0MzoyNFrOGX9VQQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzc3NTI5Nw==", "bodyText": "restarted -> retried", "url": "https://github.com/Netflix/conductor/pull/1691#discussion_r427775297", "createdAt": "2020-05-20T06:43:24Z", "author": {"login": "apanicker-nflx"}, "path": "test-harness/src/test/groovy/com/netflix/counductor/integration/test/ForkJoinSpec.groovy", "diffHunk": "@@ -0,0 +1,874 @@\n+package com.netflix.counductor.integration.test\n+\n+import com.netflix.archaius.guice.ArchaiusModule\n+import com.netflix.conductor.common.metadata.tasks.Task\n+import com.netflix.conductor.common.metadata.tasks.TaskDef\n+import com.netflix.conductor.common.run.Workflow\n+import com.netflix.conductor.core.execution.WorkflowExecutor\n+import com.netflix.conductor.core.execution.tasks.SubWorkflow\n+import com.netflix.conductor.service.ExecutionService\n+import com.netflix.conductor.service.MetadataService\n+import com.netflix.conductor.test.util.WorkflowTestUtil\n+import com.netflix.conductor.tests.utils.TestModule\n+import com.netflix.governator.guice.test.ModulesForTesting\n+import spock.lang.Shared\n+import spock.lang.Specification\n+\n+import javax.inject.Inject\n+\n+@ModulesForTesting([TestModule.class, ArchaiusModule.class])\n+class ForkJoinSpec extends Specification {\n+\n+    @Inject\n+    ExecutionService workflowExecutionService\n+\n+    @Inject\n+    MetadataService metadataService\n+\n+    @Inject\n+    WorkflowExecutor workflowExecutor\n+\n+    @Inject\n+    WorkflowTestUtil workflowTestUtil\n+\n+    @Shared\n+    def FORK_JOIN_WF = 'FanInOutTest'\n+\n+    @Shared\n+    def FORK_JOIN_NESTED_WF = 'FanInOutNestedTest'\n+\n+    @Shared\n+    def FORK_JOIN_NESTED_SUB_WF = 'FanInOutNestedSubWorkflowTest'\n+\n+    @Shared\n+    def WORKFLOW_FORK_JOIN_OPTIONAL_SW = \"integration_test_fork_join_optional_sw\"\n+\n+    def cleanup() {\n+        workflowTestUtil.clearWorkflows()\n+    }\n+\n+    def setup() {\n+        workflowTestUtil.registerWorkflows('fork_join_integration_test.json',\n+                'fork_join_with_no_task_retry_integration_test.json',\n+                'nested_fork_join_integration_test.json',\n+                'simple_workflow_1_integration_test.json',\n+                'nested_fork_join_with_sub_workflow_integration_test.json',\n+                'simple_one_task_sub_workflow_integration_test.json',\n+                'fork_join_with_optional_sub_workflow_forks_integration_test.json'\n+        )\n+    }\n+\n+    /**\n+     *               start\n+     *                 |\n+     *               fork\n+     *              /    \\\n+     *         task1     task2\n+     *          \\        /\n+     *          task3   /\n+     *           \\     /\n+     *            \\  /\n+     *            join\n+     *              |\n+     *             task4\n+     *              |\n+     *             End\n+     */\n+    def \"Test a simple workflow with fork join success flow\"() {\n+        setup: \"Ensure that all the tasks involved in the workflow have a retry count of 0\"\n+        def persistedIntegrationTask1Definition = workflowTestUtil.getPersistedTaskDefinition('integration_task_1').get()\n+        def modifiedIntegrationTask1Definition = new TaskDef(persistedIntegrationTask1Definition.name,\n+                persistedIntegrationTask1Definition.description, 0, 0)\n+        def persistedIntegrationTask2Definition = workflowTestUtil.getPersistedTaskDefinition('integration_task_2').get()\n+        def modifiedIntegrationTask2Definition = new TaskDef(persistedIntegrationTask2Definition.name,\n+                persistedIntegrationTask2Definition.description, 0, 0)\n+        def persistedIntegrationTask3Definition = workflowTestUtil.getPersistedTaskDefinition('integration_task_3').get()\n+        def modifiedIntegrationTask3Definition = new TaskDef(persistedIntegrationTask3Definition.name,\n+                persistedIntegrationTask3Definition.description, 0, 0)\n+        def persistedIntegrationTask4Definition = workflowTestUtil.getPersistedTaskDefinition('integration_task_4').get()\n+        def modifiedIntegrationTask4Definition = new TaskDef(persistedIntegrationTask4Definition.name,\n+                persistedIntegrationTask4Definition.description, 0, 0)\n+\n+        metadataService.updateTaskDef(modifiedIntegrationTask1Definition)\n+        metadataService.updateTaskDef(modifiedIntegrationTask2Definition)\n+        metadataService.updateTaskDef(modifiedIntegrationTask3Definition)\n+        metadataService.updateTaskDef(modifiedIntegrationTask4Definition)\n+\n+        when: \"A fork join workflow is started\"\n+        def workflowInstanceId = workflowExecutor.startWorkflow(FORK_JOIN_WF, 1,\n+                'fanoutTest', [:],\n+                null, null, null)\n+\n+        then: \"verify that the workflow has started and the starting nodes of the each fork are in scheduled state\"\n+        workflowInstanceId\n+        with(workflowExecutionService.getExecutionStatus(workflowInstanceId, true)) {\n+            status == Workflow.WorkflowStatus.RUNNING\n+            tasks.size() == 4\n+            tasks[0].status == Task.Status.COMPLETED\n+            tasks[0].taskType == 'FORK'\n+            tasks[1].status == Task.Status.SCHEDULED\n+            tasks[1].taskType == 'integration_task_1'\n+            tasks[2].status == Task.Status.SCHEDULED\n+            tasks[2].taskType == 'integration_task_2'\n+            tasks[3].status == Task.Status.IN_PROGRESS\n+            tasks[3].taskType == 'JOIN'\n+        }\n+\n+        when: \"The first task of the fork is polled and completed\"\n+        def polledAndAckTask1Try1 = workflowTestUtil.pollAndCompleteTask('integration_task_1', 'task1.worker', [:], 0)\n+\n+        then: \"verify that the 'integration_task_1' was polled and acknowledged\"\n+        workflowTestUtil.verifyPolledAndAcknowledgedTask([:], polledAndAckTask1Try1)\n+\n+        and: \"The workflow has been updated and has all the required tasks in the right status to move forward\"\n+        with(workflowExecutionService.getExecutionStatus(workflowInstanceId, true)) {\n+            status == Workflow.WorkflowStatus.RUNNING\n+            tasks.size() == 5\n+            tasks[1].status == Task.Status.COMPLETED\n+            tasks[1].taskType == 'integration_task_1'\n+            tasks[2].status == Task.Status.SCHEDULED\n+            tasks[2].taskType == 'integration_task_2'\n+            tasks[3].status == Task.Status.IN_PROGRESS\n+            tasks[3].taskType == 'JOIN'\n+            tasks[4].status == Task.Status.SCHEDULED\n+            tasks[4].taskType == 'integration_task_3'\n+        }\n+\n+        when: \"The 'integration_task_3' is polled and completed\"\n+        def polledAndAckTask3Try1 = workflowTestUtil.pollAndCompleteTask('integration_task_3',\n+                'task1.worker', [:], 0)\n+\n+        then: \"verify that the 'integration_task_3' was polled and acknowledged\"\n+        workflowTestUtil.verifyPolledAndAcknowledgedTask([:], polledAndAckTask3Try1)\n+\n+        and: \"The workflow has been updated with the task status and task list\"\n+        with(workflowExecutionService.getExecutionStatus(workflowInstanceId, true)) {\n+            status == Workflow.WorkflowStatus.RUNNING\n+            tasks.size() == 5\n+            tasks[2].status == Task.Status.SCHEDULED\n+            tasks[2].taskType == 'integration_task_2'\n+            tasks[3].status == Task.Status.IN_PROGRESS\n+            tasks[3].taskType == 'JOIN'\n+            tasks[4].status == Task.Status.COMPLETED\n+            tasks[4].taskType == 'integration_task_3'\n+        }\n+\n+        when: \"The other node of the fork is completed by completing 'integration_task_2'\"\n+        def polledAndAckTask2Try1 = workflowTestUtil.pollAndCompleteTask('integration_task_2',\n+                'task1.worker', [:], 0)\n+\n+        then: \"verify that the 'integration_task_2' was polled and acknowledged\"\n+        workflowTestUtil.verifyPolledAndAcknowledgedTask([:], polledAndAckTask2Try1)\n+\n+        and: \"The workflow has been updated with the task status and task list\"\n+        with(workflowExecutionService.getExecutionStatus(workflowInstanceId, true)) {\n+            status == Workflow.WorkflowStatus.RUNNING\n+            tasks.size() == 6\n+            tasks[2].status == Task.Status.COMPLETED\n+            tasks[2].taskType == 'integration_task_2'\n+            tasks[3].status == Task.Status.COMPLETED\n+            tasks[3].taskType == 'JOIN'\n+            tasks[5].status == Task.Status.SCHEDULED\n+            tasks[5].taskType == 'integration_task_4'\n+        }\n+\n+        when: \"The last task of the workflow is then polled and completed integration_task_4'\"\n+        def polledAndAckTask4Try1 = workflowTestUtil.pollAndCompleteTask('integration_task_4',\n+                'task1.worker', [:], 0)\n+\n+        then: \"verify that the 'integration_task_4' was polled and acknowledged\"\n+        workflowTestUtil.verifyPolledAndAcknowledgedTask([:], polledAndAckTask4Try1)\n+\n+        and: \"Then verify that the workflow is completed\"\n+        with(workflowExecutionService.getExecutionStatus(workflowInstanceId, true)) {\n+            status == Workflow.WorkflowStatus.COMPLETED\n+            tasks.size() == 6\n+            tasks[5].status == Task.Status.COMPLETED\n+            tasks[5].taskType == 'integration_task_4'\n+        }\n+\n+        cleanup: \"Restore the task definitions that were modified as part of this feature testing\"\n+        metadataService.updateTaskDef(persistedIntegrationTask1Definition)\n+        metadataService.updateTaskDef(persistedIntegrationTask2Definition)\n+        metadataService.updateTaskDef(persistedIntegrationTask3Definition)\n+        metadataService.updateTaskDef(persistedIntegrationTask4Definition)\n+    }\n+\n+\n+    def \"Test a simple workflow with fork join failure flow\"() {\n+        setup: \"Ensure that 'integration_task_2' has a retry count of 0\"\n+        def persistedIntegrationTask2Definition = workflowTestUtil.getPersistedTaskDefinition('integration_task_2').get()\n+        def modifiedIntegrationTask2Definition = new TaskDef(persistedIntegrationTask2Definition.name,\n+                persistedIntegrationTask2Definition.description, 0, 0)\n+        metadataService.updateTaskDef(modifiedIntegrationTask2Definition)\n+\n+        when: \"A fork join workflow is started\"\n+        def workflowInstanceId = workflowExecutor.startWorkflow(FORK_JOIN_WF, 1,\n+                'fanoutTest', [:],\n+                null, null, null)\n+\n+        then: \"verify that the workflow has started and the starting nodes of the each fork are in scheduled state\"\n+        workflowInstanceId\n+        with(workflowExecutionService.getExecutionStatus(workflowInstanceId, true)) {\n+            status == Workflow.WorkflowStatus.RUNNING\n+            tasks.size() == 4\n+            tasks[0].status == Task.Status.COMPLETED\n+            tasks[0].taskType == 'FORK'\n+            tasks[1].status == Task.Status.SCHEDULED\n+            tasks[1].taskType == 'integration_task_1'\n+            tasks[2].status == Task.Status.SCHEDULED\n+            tasks[2].taskType == 'integration_task_2'\n+            tasks[3].status == Task.Status.IN_PROGRESS\n+            tasks[3].taskType == 'JOIN'\n+        }\n+\n+        when: \"The first task of the fork is polled and completed\"\n+        def polledAndAckTask1Try1 = workflowTestUtil.pollAndCompleteTask('integration_task_1', 'task1.worker', [:], 0)\n+\n+        then: \"verify that the 'integration_task_1' was polled and acknowledged\"\n+        workflowTestUtil.verifyPolledAndAcknowledgedTask([:], polledAndAckTask1Try1)\n+\n+        and: \"The workflow has been updated and has all the required tasks in the right status to move forward\"\n+        with(workflowExecutionService.getExecutionStatus(workflowInstanceId, true)) {\n+            status == Workflow.WorkflowStatus.RUNNING\n+            tasks.size() == 5\n+            tasks[1].status == Task.Status.COMPLETED\n+            tasks[1].taskType == 'integration_task_1'\n+            tasks[2].status == Task.Status.SCHEDULED\n+            tasks[2].taskType == 'integration_task_2'\n+            tasks[3].status == Task.Status.IN_PROGRESS\n+            tasks[3].taskType == 'JOIN'\n+            tasks[4].status == Task.Status.SCHEDULED\n+            tasks[4].taskType == 'integration_task_3'\n+        }\n+\n+        when: \"The other node of the fork is completed by completing 'integration_task_2'\"\n+        def polledAndAckTask2Try1 = workflowTestUtil.pollAndFailTask('integration_task_2',\n+                'task1.worker', 'Failed....', 0)\n+\n+        then: \"verify that the 'integration_task_2' was polled and acknowledged\"\n+        workflowTestUtil.verifyPolledAndAcknowledgedTask([:], polledAndAckTask2Try1)\n+\n+        and: \"the workflow is in the failed state\"\n+        with(workflowExecutionService.getExecutionStatus(workflowInstanceId, true)) {\n+            status == Workflow.WorkflowStatus.FAILED\n+            tasks.size() == 5\n+            tasks[1].status == Task.Status.COMPLETED\n+            tasks[1].taskType == 'integration_task_1'\n+            tasks[2].status == Task.Status.FAILED\n+            tasks[2].taskType == 'integration_task_2'\n+            tasks[3].status == Task.Status.CANCELED\n+            tasks[3].taskType == 'JOIN'\n+            tasks[4].status == Task.Status.CANCELED\n+            tasks[4].taskType == 'integration_task_3'\n+        }\n+\n+        cleanup: \"Restore the task definitions that were modified as part of this feature testing\"\n+        metadataService.updateTaskDef(persistedIntegrationTask2Definition)\n+    }\n+\n+    def \"Test restarting a failed fork join workflow\"() {\n+\n+        when: \"A fork join workflow is started\"\n+        def workflowInstanceId = workflowExecutor.startWorkflow(FORK_JOIN_WF + '_2', 1,\n+                'fanoutTest', [:],\n+                null, null, null)\n+\n+        then: \"verify that the workflow has started and the starting nodes of the each fork are in scheduled state\"\n+        workflowInstanceId\n+        with(workflowExecutionService.getExecutionStatus(workflowInstanceId, true)) {\n+            status == Workflow.WorkflowStatus.RUNNING\n+            tasks.size() == 4\n+            tasks[0].status == Task.Status.COMPLETED\n+            tasks[0].taskType == 'FORK'\n+            tasks[1].status == Task.Status.SCHEDULED\n+            tasks[1].taskType == 'integration_task_0_RT_1'\n+            tasks[2].status == Task.Status.SCHEDULED\n+            tasks[2].taskType == 'integration_task_0_RT_2'\n+            tasks[3].status == Task.Status.IN_PROGRESS\n+            tasks[3].taskType == 'JOIN'\n+        }\n+\n+        when: \"The first task of the fork is polled and completed\"\n+        def polledAndAckTask1Try1 = workflowTestUtil.pollAndCompleteTask('integration_task_0_RT_1', 'task1.worker', [:], 0)\n+\n+        then: \"verify that the 'integration_task_0_RT_1' was polled and acknowledged\"\n+        workflowTestUtil.verifyPolledAndAcknowledgedTask([:], polledAndAckTask1Try1)\n+\n+        and: \"The workflow has been updated and has all the required tasks in the right status to move forward\"\n+        with(workflowExecutionService.getExecutionStatus(workflowInstanceId, true)) {\n+            status == Workflow.WorkflowStatus.RUNNING\n+            tasks.size() == 5\n+            tasks[1].status == Task.Status.COMPLETED\n+            tasks[1].taskType == 'integration_task_0_RT_1'\n+            tasks[2].status == Task.Status.SCHEDULED\n+            tasks[2].taskType == 'integration_task_0_RT_2'\n+            tasks[3].status == Task.Status.IN_PROGRESS\n+            tasks[3].taskType == 'JOIN'\n+            tasks[4].status == Task.Status.SCHEDULED\n+            tasks[4].taskType == 'integration_task_0_RT_3'\n+        }\n+\n+        when: \"The other node of the fork is completed by completing 'integration_task_0_RT_2'\"\n+        def polledAndAckTask2Try1 = workflowTestUtil.pollAndFailTask('integration_task_0_RT_2',\n+                'task1.worker', 'Failed....', 0)\n+\n+        then: \"verify that the 'integration_task_0_RT_1' was polled and acknowledged\"\n+        workflowTestUtil.verifyPolledAndAcknowledgedTask([:], polledAndAckTask2Try1)\n+\n+        and: \"the workflow is in the failed state\"\n+        with(workflowExecutionService.getExecutionStatus(workflowInstanceId, true)) {\n+            status == Workflow.WorkflowStatus.FAILED\n+            tasks.size() == 5\n+            tasks[1].status == Task.Status.COMPLETED\n+            tasks[1].taskType == 'integration_task_0_RT_1'\n+            tasks[2].status == Task.Status.FAILED\n+            tasks[2].taskType == 'integration_task_0_RT_2'\n+            tasks[3].status == Task.Status.CANCELED\n+            tasks[3].taskType == 'JOIN'\n+            tasks[4].status == Task.Status.CANCELED\n+            tasks[4].taskType == 'integration_task_0_RT_3'\n+        }\n+\n+        when: \"The workflow is retried\"\n+        workflowExecutor.retry(workflowInstanceId)\n+\n+        then: \"verify that all the workflow is restarted and new tasks are added in place of the failed tasks\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1477c32bb512e28376306f774951d89f9da70b69"}, "originalPosition": 336}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4194, "cost": 1, "resetAt": "2021-11-12T20:28:25Z"}}}