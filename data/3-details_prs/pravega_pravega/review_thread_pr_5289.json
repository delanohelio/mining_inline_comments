{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTEyODMwNDA2", "number": 5289, "reviewThreads": {"totalCount": 18, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwNjozNjozNVrOE0P2_A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQxOTozMjoxNFrOE6u2LQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzMjIxMjQ0OnYy", "diffSide": "RIGHT", "path": "controller/src/main/java/io/pravega/controller/server/rest/generated/model/ConsumptionLimits.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwNjozNjozNVrOHr3wBQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QwNjoxNTozN1rOHshUAg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTc2NDIyOQ==", "bodyText": "TIME_MINUTES enum mapping to TIME_MILLS String value... looks like a typo to me.", "url": "https://github.com/pravega/pravega/pull/5289#discussion_r515764229", "createdAt": "2020-11-02T06:36:35Z", "author": {"login": "pbelgundi"}, "path": "controller/src/main/java/io/pravega/controller/server/rest/generated/model/ConsumptionLimits.java", "diffHunk": "@@ -0,0 +1,162 @@\n+/*\n+ * Pravega Controller APIs\n+ * List of admin REST APIs for the pravega controller service.\n+ *\n+ * OpenAPI spec version: 0.0.1\n+ *\n+ *\n+ * NOTE: This class is auto generated by the swagger code generator program.\n+ * https://github.com/swagger-api/swagger-codegen.git\n+ * Do not edit the class manually.\n+ */\n+\n+\n+\n+package io.pravega.controller.server.rest.generated.model;\n+\n+import java.util.Objects;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonValue;\n+import io.swagger.annotations.ApiModel;\n+import io.swagger.annotations.ApiModelProperty;\n+\n+\n+\n+\n+/**\n+ * ConsumptionLimits\n+ */\n+\n+public class ConsumptionLimits   {\n+  /**\n+   * Indicates if consumption limits is by space or time.\n+   */\n+  public enum TypeEnum {\n+    SIZE_MB(\"SIZE_MB\"),\n+    \n+    TIME_MINUTES(\"TIME_MILLIS\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "97717a9384ff8c460ad2c3b83774587ebbdc27b9"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjQ0NTE4Ng==", "bodyText": "its a generated class. i wonder how it got changed.. must have been some replace all that i did on some string match.", "url": "https://github.com/pravega/pravega/pull/5289#discussion_r516445186", "createdAt": "2020-11-03T06:15:37Z", "author": {"login": "shiveshr"}, "path": "controller/src/main/java/io/pravega/controller/server/rest/generated/model/ConsumptionLimits.java", "diffHunk": "@@ -0,0 +1,162 @@\n+/*\n+ * Pravega Controller APIs\n+ * List of admin REST APIs for the pravega controller service.\n+ *\n+ * OpenAPI spec version: 0.0.1\n+ *\n+ *\n+ * NOTE: This class is auto generated by the swagger code generator program.\n+ * https://github.com/swagger-api/swagger-codegen.git\n+ * Do not edit the class manually.\n+ */\n+\n+\n+\n+package io.pravega.controller.server.rest.generated.model;\n+\n+import java.util.Objects;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonValue;\n+import io.swagger.annotations.ApiModel;\n+import io.swagger.annotations.ApiModelProperty;\n+\n+\n+\n+\n+/**\n+ * ConsumptionLimits\n+ */\n+\n+public class ConsumptionLimits   {\n+  /**\n+   * Indicates if consumption limits is by space or time.\n+   */\n+  public enum TypeEnum {\n+    SIZE_MB(\"SIZE_MB\"),\n+    \n+    TIME_MINUTES(\"TIME_MILLIS\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTc2NDIyOQ=="}, "originalCommit": {"oid": "97717a9384ff8c460ad2c3b83774587ebbdc27b9"}, "originalPosition": 38}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzMjIyNjIzOnYy", "diffSide": "RIGHT", "path": "controller/src/main/java/io/pravega/controller/store/stream/PersistentStreamBase.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwNjo0NDowNFrOHr34NA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwNjo0NDowNFrOHr34NA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTc2NjMyNA==", "bodyText": "These 4 lines are repeated in 3 different methods in this class, perhaps we could encapsulate them in a private method?", "url": "https://github.com/pravega/pravega/pull/5289#discussion_r515766324", "createdAt": "2020-11-02T06:44:04Z", "author": {"login": "pbelgundi"}, "path": "controller/src/main/java/io/pravega/controller/store/stream/PersistentStreamBase.java", "diffHunk": "@@ -164,20 +166,24 @@ public String getScopeName() {\n                 .thenCompose(existing -> {\n                     Preconditions.checkNotNull(existing);\n                     Preconditions.checkArgument(!existing.getObject().isUpdating());\n-                    return isStreamCutValid(streamCut)\n-                        .thenCompose(isValid -> {\n+                    long mostRecent = streamCut.keySet().stream().max(Comparator.naturalOrder()).get();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "97717a9384ff8c460ad2c3b83774587ebbdc27b9"}, "originalPosition": 31}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzMjI2MzgyOnYy", "diffSide": "RIGHT", "path": "controller/src/main/java/io/pravega/controller/store/stream/PersistentStreamBase.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwNzowMzoyNFrOHr4Nqw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwNzowMzoyNFrOHr4Nqw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTc3MTgxOQ==", "bodyText": "span2 will never be equal to CompletableFuture.completedFuture(ImmutableMap.of()) becuase if previousStreamCut.isEmpty=true, that case is already handled in the if block.", "url": "https://github.com/pravega/pravega/pull/5289#discussion_r515771819", "createdAt": "2020-11-02T07:03:24Z", "author": {"login": "pbelgundi"}, "path": "controller/src/main/java/io/pravega/controller/store/stream/PersistentStreamBase.java", "diffHunk": "@@ -634,75 +734,183 @@ private long findSegmentSplitsMerges(List<StreamSegmentRecord> referenceSegments\n         int epochHigh = NameUtils.getEpoch(mostRecent);\n \n         return fetchEpochs(epochLow, epochHigh, true).thenApply(epochs ->  {\n-            List<Long> toFind = new ArrayList<>(streamCut.keySet());\n-            ImmutableMap.Builder<StreamSegmentRecord, Integer> resultSet = ImmutableMap.builder();\n-            for (int i = epochHigh - epochLow; i >= 0; i--) {\n-                if (toFind.isEmpty()) {\n-                    break;\n-                }\n-                EpochRecord epochRecord = epochs.get(i);\n-                Set<Long> epochSegments = epochRecord.getSegmentIds();\n-                List<Long> found = toFind.stream().filter(epochSegments::contains).collect(Collectors.toList());\n-                resultSet.putAll(found.stream().collect(\n-                        Collectors.toMap(x -> epochRecord.getSegments().stream().filter(z -> z.segmentId() == x).findFirst().get(),\n-                                x -> epochRecord.getEpoch())));\n-\n-                toFind.removeAll(epochSegments);\n-            }\n-            return resultSet.build();\n+            return computeStreamCutSpanInternal(streamCut, epochLow, epochHigh, epochs);\n         });\n     }\n \n+    private ImmutableMap<StreamSegmentRecord, Integer> computeStreamCutSpanInternal(Map<Long, Long> streamCut, int epochLow, \n+                                                                                    int epochHigh, List<EpochRecord> epochs) {\n+        List<Long> toFind = new ArrayList<>(streamCut.keySet());\n+        ImmutableMap.Builder<StreamSegmentRecord, Integer> resultSet = ImmutableMap.builder();\n+        for (int i = epochHigh - epochLow; i >= 0; i--) {\n+            if (toFind.isEmpty()) {\n+                break;\n+            }\n+            EpochRecord epochRecord = epochs.get(i);\n+            Set<Long> epochSegments = epochRecord.getSegmentIds();\n+            List<Long> found = toFind.stream().filter(epochSegments::contains).collect(Collectors.toList());\n+            resultSet.putAll(found.stream().collect(\n+                    Collectors.toMap(x -> epochRecord.getSegments().stream().filter(z -> z.segmentId() == x).findFirst().get(),\n+                            x -> epochRecord.getEpoch())));\n+\n+            toFind.removeAll(epochSegments);\n+        }\n+        return resultSet.build();\n+    }\n+\n     @Override\n     public CompletableFuture<Boolean> isStreamCutValid(Map<Long, Long> streamCut) {\n-        Map<Integer, List<Long>> groupByEpoch = streamCut.keySet().stream().collect(groupingBy(NameUtils::getEpoch));\n-\n-        CompletableFuture<List<List<Map.Entry<Double, Double>>>> segmentRangesByEpoch = Futures.allOfWithResults(groupByEpoch.entrySet().stream().map(epochGroup -> {\n-            return getEpochRecord(epochGroup.getKey())\n-                    .thenApply(epochRecord -> {\n-                        return epochGroup.getValue().stream().map(segmentId -> {\n-                            StreamSegmentRecord segment = epochRecord.getSegment(segmentId);\n-                            return (Map.Entry<Double, Double>) new SimpleEntry<>(segment.getKeyStart(), segment.getKeyEnd());\n-                        }).collect(Collectors.toList());\n-                    });\n-        }).collect(Collectors.toList()));\n+        long mostRecent = streamCut.keySet().stream().max(Comparator.naturalOrder()).get();\n+        long oldest = streamCut.keySet().stream().min(Comparator.naturalOrder()).get();\n+        int epochLow = NameUtils.getEpoch(oldest);\n+        int epochHigh = NameUtils.getEpoch(mostRecent);\n \n-        CompletableFuture<List<Map.Entry<Double, Double>>> segmentRangesFlattened = segmentRangesByEpoch\n-                .thenApply(listOfList -> listOfList.stream().flatMap(Collection::stream).collect(Collectors.toList()));\n-        \n-        return segmentRangesFlattened\n-                      .thenAccept(x -> RecordHelper.validateStreamCut(new ArrayList<>(x)))\n-                      .handle((r, e) -> {\n-                          if (e != null) {\n-                              if (Exceptions.unwrap(e) instanceof IllegalArgumentException) {\n-                                  return false;\n-                              } else {\n-                                  log.warn(\"Exception while trying to validate a stream cut for stream {}/{}\", scope, name);\n-                                  throw Exceptions.sneakyThrow(e);\n-                              }\n-                          } else {\n-                              return true;\n-                          }\n-                      });\n+        return fetchEpochs(epochLow, epochHigh, true).thenApply(epochs -> isStreamCutValidInternal(streamCut, epochLow, epochs));\n+    }\n+\n+    private boolean isStreamCutValidInternal(Map<Long, Long> streamCut, int epochLow, List<EpochRecord> epochs) {\n+        Set<StreamSegmentRecord> segmentsInStreamCut = new HashSet<>();\n+        Set<StreamSegmentRecord> futureSegment = new HashSet<>();\n+\n+        boolean isValid = true;\n+        // for each segment get its epoch and the segment record\n+        streamCut.forEach((key, value) -> {\n+            int epoch = NameUtils.getEpoch(key);\n+            int index = epoch - epochLow;\n+            EpochRecord epochRecord = epochs.get(index);\n+            StreamSegmentRecord segmentRecord = epochRecord.getSegment(key);\n+            if (value < 0) {\n+                futureSegment.add(segmentRecord);\n+            } else {\n+                segmentsInStreamCut.add(segmentRecord);\n+            }\n+        });\n+\n+        isValid = futureSegment.stream().allMatch(x -> \n+                segmentsInStreamCut.stream().filter(y -> y.overlaps(x)).allMatch(y -> y.segmentId() < x.segmentId()));\n+\n+        if (isValid) {\n+            List<StreamSegmentRecord> sorted = segmentsInStreamCut\n+                    .stream().sorted(Comparator.comparingDouble(StreamSegmentRecord::getKeyStart)).collect(Collectors.toList());\n+\n+            // all future segments should have a predecessor and all missing ranges should be covered by a future segment. \n+            Map<Double, Double> missingRanges = new HashMap<>();\n+\n+            StreamSegmentRecord previous = sorted.get(0);\n+            BiFunction<Double, Double, Boolean> validate = (start, end) -> futureSegment.stream().anyMatch(x -> x.overlaps(start, end));\n+\n+            if (previous.getKeyStart() > 0.0) {\n+                double start = 0.0;\n+                double end = previous.getKeyStart();\n+                missingRanges.put(start, end);\n+                // missing range should be covered by a future segment\n+                isValid = validate.apply(start, end);\n+            }\n+\n+            for (int i = 1; i < sorted.size(); i++) {\n+                StreamSegmentRecord next = sorted.get(i);\n+                if (previous.getKeyEnd() < next.getKeyStart()) {\n+                    double start = previous.getKeyEnd();\n+                    double end = next.getKeyStart();\n+                    missingRanges.put(start, end);\n+                    //  missing range should be covered by a future segment\n+                    isValid = validate.apply(start, end);\n+                    if (!isValid) {\n+                        break;\n+                    }\n+                } else if (previous.getKeyEnd() > next.getKeyStart()) {\n+                    isValid = false;\n+                    break;\n+                }\n+                previous = next;\n+            }\n+\n+            if (previous.getKeyEnd() < 1.0) {\n+                double start = previous.getKeyEnd();\n+                double end = 1.0;\n+                missingRanges.put(start, end);\n+                isValid = validate.apply(start, end);\n+            }\n+\n+            if (isValid) {\n+                List<StreamSegmentRecord> toCheck = new ArrayList<>();\n+                Set<StreamSegmentRecord> fullyReadFrom = new HashSet<>();\n+\n+                // now traverse the stream for missing ranges and verify that we can reach those future segments \n+                // in logically consistent fashion for the missing ranges. \n+                missingRanges.entrySet().forEach(x -> toCheck.addAll(findSegmentsForMissingRange(epochs.get(0), x)));\n+                while (!toCheck.isEmpty()) {\n+                    StreamSegmentRecord segmentRecord = toCheck.get(0);\n+                    if (!(fullyReadFrom.contains(segmentRecord) || segmentsInStreamCut.contains(segmentRecord) ||\n+                            futureSegment.contains(segmentRecord))) {\n+                        for (StreamSegmentRecord s : segmentsInStreamCut) {\n+                            if (s.overlaps(segmentRecord)) {\n+                                if (s.segmentId() < segmentRecord.segmentId()) {\n+                                    // if segment record has a predecessor, then it should have been in future segment.  \n+                                    if (!futureSegment.contains(segmentRecord)) {\n+                                        return false;\n+                                    } else {\n+                                        // segment record is a predecessor of a previous segment. \n+                                        fullyReadFrom.add(segmentRecord);\n+                                    }\n+                                } else {\n+                                    // if segment is predecessor of another segment in the stream cut then it has to be \n+                                    // fully read \n+                                    fullyReadFrom.add(segmentRecord);\n+                                    // find successors of segmentRecord and add it to tocheck list\n+                                    int segmentEpoch = NameUtils.getEpoch(segmentRecord.segmentId());\n+                                    int index = segmentEpoch - epochLow;\n+                                    for (int i = index; i < epochs.size(); i++) {\n+                                        if (!epochs.get(i).containsSegment(segmentRecord.segmentId())) {\n+                                            epochs.get(i).getSegments().forEach(x -> {\n+                                                if (x.overlaps(segmentRecord)) {\n+                                                    toCheck.add(x);\n+                                                }\n+                                            });\n+                                            break;\n+                                        }\n+                                    }\n+                                }\n+                            }\n+                        }\n+                    }\n+\n+                    toCheck.remove(segmentRecord);\n+                }\n+            }\n+        }\n+        return isValid;\n+    }\n+    \n+    private List<StreamSegmentRecord> findSegmentsForMissingRange(EpochRecord epochRecord, Map.Entry<Double, Double> missingRange) {\n+        return epochRecord.getSegments().stream().filter(x -> x.overlaps(missingRange.getKey(), missingRange.getValue()))\n+                          .collect(Collectors.toList());\n     }\n \n     private CompletableFuture<Boolean> isStreamCutValidForTruncation(Map<Long, Long> previousStreamCut, final Map<Long, Long> streamCut) {\n         if (previousStreamCut.isEmpty()) {\n             return isStreamCutValid(streamCut);\n         } else {\n             return isStreamCutValid(streamCut)\n-                    .thenCompose( isValidStreamCut -> {\n+                    .thenCompose(isValidStreamCut -> {\n                         if (isValidStreamCut) {\n-                            return computeStreamCutSpan(streamCut)\n-                                    .thenCompose(span -> computeStreamCutSpan(previousStreamCut)\n-                                            .thenApply(previousSpan -> greaterThan(streamCut, span, previousStreamCut, previousSpan)));\n+                            CompletableFuture<ImmutableMap<StreamSegmentRecord, Integer>> span1 = computeStreamCutSpan(streamCut);\n+                            CompletableFuture<ImmutableMap<StreamSegmentRecord, Integer>> span2 = previousStreamCut.isEmpty() ?\n+                                    CompletableFuture.completedFuture(ImmutableMap.of()) : computeStreamCutSpan(previousStreamCut);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "97717a9384ff8c460ad2c3b83774587ebbdc27b9"}, "originalPosition": 402}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzMjI2NTkwOnYy", "diffSide": "RIGHT", "path": "controller/src/main/java/io/pravega/controller/store/stream/Stream.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwNzowNDoyM1rOHr4O2g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwNzowNDoyM1rOHr4O2g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTc3MjEyMg==", "bodyText": "method name should be isStreamCutStrictlyGreaterThan since it returns a boolean.", "url": "https://github.com/pravega/pravega/pull/5289#discussion_r515772122", "createdAt": "2020-11-02T07:04:23Z", "author": {"login": "pbelgundi"}, "path": "controller/src/main/java/io/pravega/controller/store/stream/Stream.java", "diffHunk": "@@ -680,4 +680,22 @@\n      * @return Completable future that, upon completion, holds the epoch in which the segment was sealed.\n      */\n     CompletableFuture<Integer> getSegmentSealedEpoch(long segmentId);\n+\n+    /**\n+     * Method to compare streamcuts to check if streamcut1 is strictly ahead of streamcut2. \n+     * @param cut1 streamcut to check\n+     * @param cut2 streamcut to check against. \n+     *\n+     * @return CompletableFuture which, upon completion, will indicate if the streamcut1 is strictly ahead of streamcut2.\n+     */\n+    CompletableFuture<Boolean> streamCutStrictlyGreaterThan(Map<Long, Long> cut1, Map<Long, Long> cut2);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "97717a9384ff8c460ad2c3b83774587ebbdc27b9"}, "originalPosition": 21}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzMjI3MjgwOnYy", "diffSide": "RIGHT", "path": "controller/src/main/java/io/pravega/controller/store/stream/StreamMetadataStore.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwNzowNzo0OVrOHr4S2Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QwNjoyMjo1OFrOHshafA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTc3MzE0NQ==", "bodyText": "What is meant by \"strictly\" here? Would be good to explain that in the javadoc", "url": "https://github.com/pravega/pravega/pull/5289#discussion_r515773145", "createdAt": "2020-11-02T07:07:49Z", "author": {"login": "pbelgundi"}, "path": "controller/src/main/java/io/pravega/controller/store/stream/StreamMetadataStore.java", "diffHunk": "@@ -1276,4 +1276,40 @@\n     CompletableFuture<Integer> getSegmentSealedEpoch(final String scope, final String streamName, final long segmentId,\n                                                      final OperationContext context, final Executor executor);\n \n+    /**\n+     * Compares two Stream cuts and returns true if streamcut1 is strictly ahead of streamcut2 else returns false. \n+     * This method will return false for both strictly less than and overlapping streamcuts.\n+     * A streamcut is considered greater than if for all key ranges the segment/offset in one streamcut is ahead of \n+     * second streamcut. \n+     *\n+     * @param scope      stream scope.\n+     * @param streamName stream name.\n+     * @param streamCut1 Stream cut to check\n+     * @param streamCut2 Streamcut to check against\n+     * @param context    operation context.\n+     * @param executor   callers executor.\n+     *                   \n+     * @return A completable future which when completed will hold a boolean which will indicate if streamcut1 is strictly\n+     * ahead of streamcut2. \n+     */\n+    CompletableFuture<Boolean> streamCutStrictlyGreaterThan(final String scope, final String streamName,\n+                                                            Map<Long, Long> streamCut1, Map<Long, Long> streamCut2,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "97717a9384ff8c460ad2c3b83774587ebbdc27b9"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjQ0Njg0NA==", "bodyText": "isnt this already explained in the javadoc here.\n     * Compares two Stream cuts and returns true if streamcut1 is strictly ahead of streamcut2 else returns false. \n     * This method will return false for both strictly less than and overlapping streamcuts.\n     * A streamcut is considered greater than if for all key ranges the segment/offset in one streamcut is ahead of \n     * second streamcut.", "url": "https://github.com/pravega/pravega/pull/5289#discussion_r516446844", "createdAt": "2020-11-03T06:22:58Z", "author": {"login": "shiveshr"}, "path": "controller/src/main/java/io/pravega/controller/store/stream/StreamMetadataStore.java", "diffHunk": "@@ -1276,4 +1276,40 @@\n     CompletableFuture<Integer> getSegmentSealedEpoch(final String scope, final String streamName, final long segmentId,\n                                                      final OperationContext context, final Executor executor);\n \n+    /**\n+     * Compares two Stream cuts and returns true if streamcut1 is strictly ahead of streamcut2 else returns false. \n+     * This method will return false for both strictly less than and overlapping streamcuts.\n+     * A streamcut is considered greater than if for all key ranges the segment/offset in one streamcut is ahead of \n+     * second streamcut. \n+     *\n+     * @param scope      stream scope.\n+     * @param streamName stream name.\n+     * @param streamCut1 Stream cut to check\n+     * @param streamCut2 Streamcut to check against\n+     * @param context    operation context.\n+     * @param executor   callers executor.\n+     *                   \n+     * @return A completable future which when completed will hold a boolean which will indicate if streamcut1 is strictly\n+     * ahead of streamcut2. \n+     */\n+    CompletableFuture<Boolean> streamCutStrictlyGreaterThan(final String scope, final String streamName,\n+                                                            Map<Long, Long> streamCut1, Map<Long, Long> streamCut2,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTc3MzE0NQ=="}, "originalCommit": {"oid": "97717a9384ff8c460ad2c3b83774587ebbdc27b9"}, "originalPosition": 21}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzMjI4MDc2OnYy", "diffSide": "RIGHT", "path": "shared/controller-api/src/main/proto/Controller.proto", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwNzoxMToyOFrOHr4XPA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMFQxMToxOToxNlrOHwX8nA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTc3NDI2OA==", "bodyText": "Here, UNKNOWN perhaps means we don;t set a limit for CBR. Could we call it NONE instead of UNKNOWN?", "url": "https://github.com/pravega/pravega/pull/5289#discussion_r515774268", "createdAt": "2020-11-02T07:11:28Z", "author": {"login": "pbelgundi"}, "path": "shared/controller-api/src/main/proto/Controller.proto", "diffHunk": "@@ -298,9 +298,22 @@ message RetentionPolicy {\n         UNKNOWN = 0;\n         TIME = 1;\n         SIZE = 2;\n+        CONSUMPTION = 3;\n     }\n     RetentionPolicyType retentionType = 1;\n     int64 retentionParam = 2;\n+    ConsumptionLimits consumptionLimits = 3;\n+}\n+\n+message ConsumptionLimits {\n+    enum ConsumptionLimitType {\n+        UNKNOWN = 0;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "97717a9384ff8c460ad2c3b83774587ebbdc27b9"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjQ0ODA3MQ==", "bodyText": "UNKNOWN = 0 in protobuf allows future safety of enums evolution .. when reading a protobuf message using an older enum schema, any enum that is not understood is set to unknown.\nif we want to support NONE, that should be a separate enum value. if you think we want to support \"none\" i could add that. but logically none is equivalent to providing limits as null.", "url": "https://github.com/pravega/pravega/pull/5289#discussion_r516448071", "createdAt": "2020-11-03T06:28:09Z", "author": {"login": "shiveshr"}, "path": "shared/controller-api/src/main/proto/Controller.proto", "diffHunk": "@@ -298,9 +298,22 @@ message RetentionPolicy {\n         UNKNOWN = 0;\n         TIME = 1;\n         SIZE = 2;\n+        CONSUMPTION = 3;\n     }\n     RetentionPolicyType retentionType = 1;\n     int64 retentionParam = 2;\n+    ConsumptionLimits consumptionLimits = 3;\n+}\n+\n+message ConsumptionLimits {\n+    enum ConsumptionLimitType {\n+        UNKNOWN = 0;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTc3NDI2OA=="}, "originalCommit": {"oid": "97717a9384ff8c460ad2c3b83774587ebbdc27b9"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDQ4NjA0NA==", "bodyText": "UNKNOWN = 0 in protobuf allows future safety of enums evolution .. when reading a protobuf message using an older enum schema, any enum that is not understood is set to unknown.\nif we want to support NONE, that should be a separate enum value. if you think we want to support \"none\" i could add that. but logically none is equivalent to providing limits as null.\n\nOkay.", "url": "https://github.com/pravega/pravega/pull/5289#discussion_r520486044", "createdAt": "2020-11-10T11:19:16Z", "author": {"login": "pbelgundi"}, "path": "shared/controller-api/src/main/proto/Controller.proto", "diffHunk": "@@ -298,9 +298,22 @@ message RetentionPolicy {\n         UNKNOWN = 0;\n         TIME = 1;\n         SIZE = 2;\n+        CONSUMPTION = 3;\n     }\n     RetentionPolicyType retentionType = 1;\n     int64 retentionParam = 2;\n+    ConsumptionLimits consumptionLimits = 3;\n+}\n+\n+message ConsumptionLimits {\n+    enum ConsumptionLimitType {\n+        UNKNOWN = 0;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTc3NDI2OA=="}, "originalCommit": {"oid": "97717a9384ff8c460ad2c3b83774587ebbdc27b9"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzMjI5MjM2OnYy", "diffSide": "RIGHT", "path": "controller/src/main/java/io/pravega/controller/server/rest/ModelHelper.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwNzoxNjozM1rOHr4eGw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMFQxMTozODo1MlrOHwYo0w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTc3NjAyNw==", "bodyText": "ConsumptionLimit.UNKNOWN is not covered here. That is a valid usecase and should not throw \"NotImplementedException\"", "url": "https://github.com/pravega/pravega/pull/5289#discussion_r515776027", "createdAt": "2020-11-02T07:16:33Z", "author": {"login": "pbelgundi"}, "path": "controller/src/main/java/io/pravega/controller/server/rest/ModelHelper.java", "diffHunk": "@@ -70,6 +80,24 @@ public static final StreamConfiguration getCreateStreamConfig(final CreateStream\n                 .build();\n     }\n \n+    private static RetentionPolicy getConsumptionRetentionPolicy(ConsumptionLimits consumptionLimits) {\n+        RetentionPolicy.ConsumptionLimits.Type type;\n+        int multiplier;\n+        switch (consumptionLimits.getType()) {\n+            case SIZE_MB:\n+                type = RetentionPolicy.ConsumptionLimits.Type.SIZE_BYTES;\n+                multiplier = BYTES_TO_MB;\n+                break;\n+            case TIME_MINUTES:\n+                type = RetentionPolicy.ConsumptionLimits.Type.TIME_MILLIS;\n+                multiplier = MILLIS_TO_MINUTES;\n+                break;\n+            default:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "97717a9384ff8c460ad2c3b83774587ebbdc27b9"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjQ0OTYxNQ==", "bodyText": "unknown, as explained, is not a valid use case. it is a protobuf enum evolution safety measure to allow for new values to be mapped to unknown.\nif we want NONE, that should be added separately. However, i think a limit with null on min or max or both is sufficient to provide that and we may not need a separate enum type NONE", "url": "https://github.com/pravega/pravega/pull/5289#discussion_r516449615", "createdAt": "2020-11-03T06:34:11Z", "author": {"login": "shiveshr"}, "path": "controller/src/main/java/io/pravega/controller/server/rest/ModelHelper.java", "diffHunk": "@@ -70,6 +80,24 @@ public static final StreamConfiguration getCreateStreamConfig(final CreateStream\n                 .build();\n     }\n \n+    private static RetentionPolicy getConsumptionRetentionPolicy(ConsumptionLimits consumptionLimits) {\n+        RetentionPolicy.ConsumptionLimits.Type type;\n+        int multiplier;\n+        switch (consumptionLimits.getType()) {\n+            case SIZE_MB:\n+                type = RetentionPolicy.ConsumptionLimits.Type.SIZE_BYTES;\n+                multiplier = BYTES_TO_MB;\n+                break;\n+            case TIME_MINUTES:\n+                type = RetentionPolicy.ConsumptionLimits.Type.TIME_MILLIS;\n+                multiplier = MILLIS_TO_MINUTES;\n+                break;\n+            default:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTc3NjAyNw=="}, "originalCommit": {"oid": "97717a9384ff8c460ad2c3b83774587ebbdc27b9"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDQ5NzM2Mw==", "bodyText": "Yes NONE is same as min=null and max=null so NONE can be skipped.", "url": "https://github.com/pravega/pravega/pull/5289#discussion_r520497363", "createdAt": "2020-11-10T11:38:52Z", "author": {"login": "pbelgundi"}, "path": "controller/src/main/java/io/pravega/controller/server/rest/ModelHelper.java", "diffHunk": "@@ -70,6 +80,24 @@ public static final StreamConfiguration getCreateStreamConfig(final CreateStream\n                 .build();\n     }\n \n+    private static RetentionPolicy getConsumptionRetentionPolicy(ConsumptionLimits consumptionLimits) {\n+        RetentionPolicy.ConsumptionLimits.Type type;\n+        int multiplier;\n+        switch (consumptionLimits.getType()) {\n+            case SIZE_MB:\n+                type = RetentionPolicy.ConsumptionLimits.Type.SIZE_BYTES;\n+                multiplier = BYTES_TO_MB;\n+                break;\n+            case TIME_MINUTES:\n+                type = RetentionPolicy.ConsumptionLimits.Type.TIME_MILLIS;\n+                multiplier = MILLIS_TO_MINUTES;\n+                break;\n+            default:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTc3NjAyNw=="}, "originalCommit": {"oid": "97717a9384ff8c460ad2c3b83774587ebbdc27b9"}, "originalPosition": 54}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2MzA2NzE2OnYy", "diffSide": "RIGHT", "path": "controller/src/main/java/io/pravega/controller/task/Stream/StreamMetadataTasks.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMFQxMTo0OTowMFrOHwY-Mw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QwOToyMjowNVrOHymtlg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDUwMjgzNQ==", "bodyText": "Using updateStream() API when a user tries to switch the Retention Policy for an existing Stream( Stream created prior to 0.9) from non-CBR to CBR, we need to invoke createSubscribersRecordIfAbsent to make sure the SubscribersSet Record is created in the Stream metadata table. Without this add/createSubscriber would fail for existing Streams.", "url": "https://github.com/pravega/pravega/pull/5289#discussion_r520502835", "createdAt": "2020-11-10T11:49:00Z", "author": {"login": "pbelgundi"}, "path": "controller/src/main/java/io/pravega/controller/task/Stream/StreamMetadataTasks.java", "diffHunk": "@@ -78,18 +79,23 @@\n import io.pravega.shared.protocol.netty.WireCommands;\n import java.io.Serializable;\n import java.time.Duration;\n+import java.util.AbstractMap;\n import java.util.ArrayList;\n import java.util.Comparator;\n+import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n import java.util.Optional;\n import java.util.Set;\n+import java.util.TreeMap;\n import java.util.UUID;\n import java.util.concurrent.CompletableFuture;\n import java.util.concurrent.CompletionException;\n import java.util.concurrent.ScheduledExecutorService;\n import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7ee349a4a1b3d67ab0224c4f33e3a8c958adf7c2"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjgyNTExMA==", "bodyText": "fixed", "url": "https://github.com/pravega/pravega/pull/5289#discussion_r522825110", "createdAt": "2020-11-13T09:22:05Z", "author": {"login": "shiveshr"}, "path": "controller/src/main/java/io/pravega/controller/task/Stream/StreamMetadataTasks.java", "diffHunk": "@@ -78,18 +79,23 @@\n import io.pravega.shared.protocol.netty.WireCommands;\n import java.io.Serializable;\n import java.time.Duration;\n+import java.util.AbstractMap;\n import java.util.ArrayList;\n import java.util.Comparator;\n+import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n import java.util.Optional;\n import java.util.Set;\n+import java.util.TreeMap;\n import java.util.UUID;\n import java.util.concurrent.CompletableFuture;\n import java.util.concurrent.CompletionException;\n import java.util.concurrent.ScheduledExecutorService;\n import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDUwMjgzNQ=="}, "originalCommit": {"oid": "7ee349a4a1b3d67ab0224c4f33e3a8c958adf7c2"}, "originalPosition": 26}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2MzE0OTU3OnYy", "diffSide": "RIGHT", "path": "controller/src/main/java/io/pravega/controller/task/Stream/StreamMetadataTasks.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMFQxMjoxMjo0MlrOHwZwcA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMFQxMjoxMjo0MlrOHwZwcA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDUxNTY5Ng==", "bodyText": "incomplete comment.", "url": "https://github.com/pravega/pravega/pull/5289#discussion_r520515696", "createdAt": "2020-11-10T12:12:42Z", "author": {"login": "pbelgundi"}, "path": "controller/src/main/java/io/pravega/controller/task/Stream/StreamMetadataTasks.java", "diffHunk": "@@ -507,32 +514,283 @@ public void initializeStreamWriters(final EventStreamClientFactory clientFactory\n \n     private CompletableFuture<Void> truncate(String scope, String stream, RetentionPolicy policy, OperationContext context,\n                                              RetentionSet retentionSet, StreamCutRecord newRecord, long recordingTime, long requestId) {\n-        Optional<StreamCutReferenceRecord> truncationRecord = findTruncationRecord(policy, retentionSet, newRecord, recordingTime);\n-        if (!truncationRecord.isPresent()) {\n-            log.info(\"No suitable truncation record found, per retention policy for stream {}/{}\", scope, stream);\n-            return CompletableFuture.completedFuture(null);\n+        if (policy.getRetentionType().equals(RetentionPolicy.RetentionType.CONSUMPTION)) {\n+            return subscriberBasedTruncation(scope, stream, context, policy, retentionSet, newRecord, requestId);\n+        } else {\n+            Optional<StreamCutReferenceRecord> truncationRecord = findTruncationRecord(policy, retentionSet, newRecord, recordingTime);\n+            if (!truncationRecord.isPresent()) {\n+                log.info(\"No suitable truncation record found, per retention policy for stream {}/{}\", scope, stream);\n+                return CompletableFuture.completedFuture(null);\n+            }\n+            log.info(\"Found truncation record for stream {}/{} truncationRecord time/size: {}/{}\", scope, stream,\n+                    truncationRecord.get().getRecordingTime(), truncationRecord.get().getRecordingSize());\n+\n+            return truncate(scope, stream, context, requestId, truncationRecord.get());\n         }\n+    }\n+\n+    private CompletableFuture<Void> truncate(String scope, String stream, OperationContext context, long requestId,\n+                                                   StreamCutReferenceRecord truncationRecord) {\n         log.info(\"Found truncation record for stream {}/{} truncationRecord time/size: {}/{}\", scope, stream,\n-                truncationRecord.get().getRecordingTime(), truncationRecord.get().getRecordingSize());\n-        return streamMetadataStore.getStreamCutRecord(scope, stream, truncationRecord.get(), context, executor)\n-                   .thenCompose(streamCutRecord -> startTruncation(scope, stream, streamCutRecord.getStreamCut(), context, requestId))\n-                   .thenCompose(started -> {\n-                       if (started) {\n-                                return streamMetadataStore.deleteStreamCutBefore(scope, stream, truncationRecord.get(), context, executor);\n+                truncationRecord.getRecordingTime(), truncationRecord.getRecordingSize());\n+        return streamMetadataStore.getStreamCutRecord(scope, stream, truncationRecord, context, executor)\n+                                  .thenCompose(streamCutRecord -> startTruncation(scope, stream, streamCutRecord.getStreamCut(), context, requestId))\n+                                  .thenCompose(started -> {\n+                                      if (started) {\n+                                          return streamMetadataStore.deleteStreamCutBefore(scope, stream, truncationRecord, context, executor);\n+                                      } else {\n+                                          throw new RuntimeException(\"Could not start truncation\");\n+                                      }\n+                                  })\n+                                  .exceptionally(e -> {\n+                                      if (Exceptions.unwrap(e) instanceof IllegalArgumentException) {\n+                                          // This is ignorable exception. Throwing this will cause unnecessary retries and exceptions logged.\n+                                          log.debug(requestId, \"Cannot truncate at given \" +\n+                                                  \"streamCut because it intersects with existing truncation point\");\n+                                          return null;\n+                                      } else {\n+                                          throw new CompletionException(e);\n+                                      }\n+                                  });\n+    }\n+\n+    private CompletableFuture<Void> subscriberBasedTruncation(String scope, String stream, OperationContext context,\n+                                                              RetentionPolicy policy, RetentionSet retentionSet, \n+                                                              StreamCutRecord newRecord, long requestId) {\n+        return streamMetadataStore.listSubscribers(scope, stream, context, executor)\n+                           .thenCompose(list -> Futures.allOfWithResults(list.stream().map(x -> \n+                                   streamMetadataStore.getSubscriber(scope, stream, x, context, executor)).collect(Collectors.toList())))\n+                           .thenCompose(subscribers -> {\n+                               // if the range for which we are looking at is ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7ee349a4a1b3d67ab0224c4f33e3a8c958adf7c2"}, "originalPosition": 110}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2NzY3NTc0OnYy", "diffSide": "RIGHT", "path": "controller/src/test/java/io/pravega/controller/store/stream/StreamTestBase.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQwOTo0MzozOVrOHxFsNw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQwOTo0MzozOVrOHxFsNw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTIzNTUxMQ==", "bodyText": "uncomment the timeout.", "url": "https://github.com/pravega/pravega/pull/5289#discussion_r521235511", "createdAt": "2020-11-11T09:43:39Z", "author": {"login": "shrids"}, "path": "controller/src/test/java/io/pravega/controller/store/stream/StreamTestBase.java", "diffHunk": "@@ -688,7 +688,7 @@ public void segmentQueriesDuringRollingTxn() {\n         stream.completeCommittingTransactions(ctr).join();\n     }\n \n-    @Test(timeout = 30000L)\n+    @Test//(timeout = 30000L)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7ee349a4a1b3d67ab0224c4f33e3a8c958adf7c2"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2Nzg5OTM5OnYy", "diffSide": "RIGHT", "path": "controller/src/main/java/io/pravega/controller/task/Stream/StreamMetadataTasks.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQxMDo0NToxOFrOHxH2Dg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QwOToyMzowOFrOHymv2A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTI3MDc5OA==", "bodyText": "Q: We can use the getCreationEpoch() here right?", "url": "https://github.com/pravega/pravega/pull/5289#discussion_r521270798", "createdAt": "2020-11-11T10:45:18Z", "author": {"login": "shrids"}, "path": "controller/src/main/java/io/pravega/controller/task/Stream/StreamMetadataTasks.java", "diffHunk": "@@ -507,32 +514,283 @@ public void initializeStreamWriters(final EventStreamClientFactory clientFactory\n \n     private CompletableFuture<Void> truncate(String scope, String stream, RetentionPolicy policy, OperationContext context,\n                                              RetentionSet retentionSet, StreamCutRecord newRecord, long recordingTime, long requestId) {\n-        Optional<StreamCutReferenceRecord> truncationRecord = findTruncationRecord(policy, retentionSet, newRecord, recordingTime);\n-        if (!truncationRecord.isPresent()) {\n-            log.info(\"No suitable truncation record found, per retention policy for stream {}/{}\", scope, stream);\n-            return CompletableFuture.completedFuture(null);\n+        if (policy.getRetentionType().equals(RetentionPolicy.RetentionType.CONSUMPTION)) {\n+            return subscriberBasedTruncation(scope, stream, context, policy, retentionSet, newRecord, requestId);\n+        } else {\n+            Optional<StreamCutReferenceRecord> truncationRecord = findTruncationRecord(policy, retentionSet, newRecord, recordingTime);\n+            if (!truncationRecord.isPresent()) {\n+                log.info(\"No suitable truncation record found, per retention policy for stream {}/{}\", scope, stream);\n+                return CompletableFuture.completedFuture(null);\n+            }\n+            log.info(\"Found truncation record for stream {}/{} truncationRecord time/size: {}/{}\", scope, stream,\n+                    truncationRecord.get().getRecordingTime(), truncationRecord.get().getRecordingSize());\n+\n+            return truncate(scope, stream, context, requestId, truncationRecord.get());\n         }\n+    }\n+\n+    private CompletableFuture<Void> truncate(String scope, String stream, OperationContext context, long requestId,\n+                                                   StreamCutReferenceRecord truncationRecord) {\n         log.info(\"Found truncation record for stream {}/{} truncationRecord time/size: {}/{}\", scope, stream,\n-                truncationRecord.get().getRecordingTime(), truncationRecord.get().getRecordingSize());\n-        return streamMetadataStore.getStreamCutRecord(scope, stream, truncationRecord.get(), context, executor)\n-                   .thenCompose(streamCutRecord -> startTruncation(scope, stream, streamCutRecord.getStreamCut(), context, requestId))\n-                   .thenCompose(started -> {\n-                       if (started) {\n-                                return streamMetadataStore.deleteStreamCutBefore(scope, stream, truncationRecord.get(), context, executor);\n+                truncationRecord.getRecordingTime(), truncationRecord.getRecordingSize());\n+        return streamMetadataStore.getStreamCutRecord(scope, stream, truncationRecord, context, executor)\n+                                  .thenCompose(streamCutRecord -> startTruncation(scope, stream, streamCutRecord.getStreamCut(), context, requestId))\n+                                  .thenCompose(started -> {\n+                                      if (started) {\n+                                          return streamMetadataStore.deleteStreamCutBefore(scope, stream, truncationRecord, context, executor);\n+                                      } else {\n+                                          throw new RuntimeException(\"Could not start truncation\");\n+                                      }\n+                                  })\n+                                  .exceptionally(e -> {\n+                                      if (Exceptions.unwrap(e) instanceof IllegalArgumentException) {\n+                                          // This is ignorable exception. Throwing this will cause unnecessary retries and exceptions logged.\n+                                          log.debug(requestId, \"Cannot truncate at given \" +\n+                                                  \"streamCut because it intersects with existing truncation point\");\n+                                          return null;\n+                                      } else {\n+                                          throw new CompletionException(e);\n+                                      }\n+                                  });\n+    }\n+\n+    private CompletableFuture<Void> subscriberBasedTruncation(String scope, String stream, OperationContext context,\n+                                                              RetentionPolicy policy, RetentionSet retentionSet, \n+                                                              StreamCutRecord newRecord, long requestId) {\n+        return streamMetadataStore.listSubscribers(scope, stream, context, executor)\n+                           .thenCompose(list -> Futures.allOfWithResults(list.stream().map(x -> \n+                                   streamMetadataStore.getSubscriber(scope, stream, x, context, executor)).collect(Collectors.toList())))\n+                           .thenCompose(subscribers -> {\n+                               // if the range for which we are looking at is \n+                               return Futures.allOfWithResults(subscribers.stream().map(x -> {\n+                                   ImmutableSet<Map.Entry<Long, Long>> entries = x.getObject().getTruncationStreamCut().entrySet();\n+\n+                                   return Futures.keysAllOfWithResults(entries.stream().collect(Collectors.toMap(\n+                                           y -> streamMetadataStore.getSegment(scope, stream, y.getKey(), context, executor), Map.Entry::getValue)));\n+                               }).collect(Collectors.toList()));\n+                           })\n+                           .thenApply(this::computeSubscribersLowerBound)\n+                .thenCompose(lowerBound -> {\n+                    CompletableFuture<Map<Long, Long>> toTruncateAt; \n+                    if (policy.getConsumptionLimits().getType().equals(RetentionPolicy.ConsumptionLimits.Type.SIZE_BYTES)) {\n+                        toTruncateAt = getTruncationStreamCutBySizeLimit(scope, stream, context, policy, retentionSet, lowerBound, newRecord);\n+                    } else {\n+                        toTruncateAt = getTruncationStreamCutByTimeLimit(scope, stream, context, policy, retentionSet, lowerBound, newRecord);\n+                    }\n+                    return toTruncateAt.thenCompose(truncationStreamCut -> {\n+                        if (truncationStreamCut == null || truncationStreamCut.isEmpty()) {\n+                            log.debug(\"no truncation record could be compute that satisfied retention policy\");\n+                            return CompletableFuture.completedFuture(null);\n+                        } \n+                        return startTruncation(scope, stream, truncationStreamCut, context, requestId)\n+                                .thenCompose(started -> {\n+                                    if (started) {\n+                                        return streamMetadataStore.findStreamCutReferenceRecordBefore(scope, stream, truncationStreamCut, retentionSet, context, executor)\n+                                                                  .thenCompose(ref -> {\n+                                                                      if (ref != null) {\n+                                                                          return streamMetadataStore.deleteStreamCutBefore(scope, stream, ref, context, executor);\n+                                                                      } else {\n+                                                                          return CompletableFuture.completedFuture(null);\n+                                                                      }\n+                                                                  });\n+                                    } else {\n+                                        throw new RuntimeException(\"Could not start truncation\");\n+                                    }\n+                                });\n+                    });\n+                });\n+    }\n+\n+    private CompletableFuture<Map<Long, Long>> getTruncationStreamCutBySizeLimit(String scope, String stream, OperationContext context, RetentionPolicy policy,\n+                                                                                 RetentionSet retentionSet, Map<Long, Long> lowerBound, StreamCutRecord newRecord) {\n+        // if the lowerbound on subscribers streamcuts satisfies the policy size bound, then return it. \n+        // else return the stream cut that satisfies maximum bound on size. \n+        long currentSize = newRecord != null ? newRecord.getRecordingSize() : retentionSet.getLatest().getRecordingSize();\n+        return streamMetadataStore.getSizeTillStreamCut(scope, stream, lowerBound, Optional.empty(), context, executor)\n+                          .thenCompose(sizeTill -> {\n+                               long retainedSize = currentSize - sizeTill;\n+                               Supplier<Optional<StreamCutReferenceRecord>> maxBound = () -> retentionSet\n+                                      .getRetentionRecords().stream()\n+                                      .filter(x -> currentSize - x.getRecordingSize() > policy.getConsumptionLimits().getMaxValue())\n+                                      .max(Comparator.comparingLong(StreamCutReferenceRecord::getRecordingTime));\n+\n+                               // if retainedSize is less than min size then do not truncate the stream. \n+                               if (retainedSize < policy.getConsumptionLimits().getMinValue()) {\n+                                   return maxBound.get().map(x -> streamMetadataStore.getStreamCutRecord(scope, stream, x, context, executor)\n+                                                                               .thenApply(StreamCutRecord::getStreamCut))\n+                                                  .orElse(CompletableFuture.completedFuture(null));\n+                               } else {\n+                                   // if retained size is less than max allowed, then truncate the stream at subscriber lower bound. \n+                                   if (retainedSize < policy.getConsumptionLimits().getMaxValue()) {\n+                                       return CompletableFuture.completedFuture(lowerBound);\n+                                   } else {\n+                                       // if retained size is greater than max allowed, then truncate the stream at streamcut\n+                                       // from retention set that matches the retention policy size bound. \n+                                       return maxBound.get().map(x -> streamMetadataStore.getStreamCutRecord(scope, stream, x, context, executor)\n+                                                   .thenApply(StreamCutRecord::getStreamCut)\n+                                                   .thenCompose(maxRecord -> {\n+                                                       // if max record is strictly greater than lowerbound then we can truncate at max record\n+                                                       return streamMetadataStore.streamCutStrictlyGreaterThan(\n+                                                               scope, stream, maxRecord, lowerBound, context, executor)\n+                                                                                 .thenApply(gt -> {\n+                                                                                     if (gt) {\n+                                                                                         return maxRecord;\n+                                                                                     } else {\n+                                                                                         return lowerBound;\n+                                                                                     }\n+                                                                                 });\n+                                                   })).orElse(CompletableFuture.completedFuture(null));   \n+                                   }\n+                               }\n+                           });\n+    }\n+\n+    private CompletableFuture<Map<Long, Long>> getTruncationStreamCutByTimeLimit(String scope, String stream, OperationContext context,\n+                                                                                 RetentionPolicy policy, RetentionSet retentionSet,\n+                                                                                 Map<Long, Long> lowerBound, StreamCutRecord latest) {\n+        Map.Entry<StreamCutReferenceRecord, StreamCutReferenceRecord> limits =\n+                getBoundStreamCuts(policy.getConsumptionLimits(), retentionSet, latest);\n+        // if subscriber lowerbound is ahead of streamcut corresponding to the max time and is behind stream cut for min time \n+        // from the retention set then we can safely truncate at lowerbound. Else we will truncate at the max time bound if it\n+        // exists\n+        CompletableFuture<StreamCutRecord> limitMaxFuture = limits.getKey() == null ? CompletableFuture.completedFuture(null) :\n+                streamMetadataStore.getStreamCutRecord(scope, stream, limits.getKey(), context, executor);\n+        CompletableFuture<StreamCutRecord> limitMinFuture = limits.getValue() == null ? CompletableFuture.completedFuture(null) :\n+                streamMetadataStore.getStreamCutRecord(scope, stream, limits.getValue(), context, executor);\n+        return CompletableFuture.allOf(limitMaxFuture, limitMinFuture)\n+                         .thenCompose(v -> {\n+                             StreamCutRecord limitMax = limitMaxFuture.join();\n+                             StreamCutRecord limitMin = limitMinFuture.join();\n+                             if (limitMin != null) {\n+                                 return streamMetadataStore.streamCutStrictlyGreaterThan(scope, stream, limitMin.getStreamCut(), lowerBound, context, executor)\n+                                                    .thenCompose(gtMin -> {\n+                                                        if (gtMin) {\n+                                                            if (limitMax == null) {\n+                                                                return CompletableFuture.completedFuture(lowerBound);\n+                                                            } else {\n+                                                                return streamMetadataStore.streamCutStrictlyGreaterThan(scope, stream, limitMax.getStreamCut(), lowerBound, context, executor)\n+                                                                                          .thenApply(gtMax -> gtMax ? limitMax.getStreamCut() : lowerBound);\n+                                                            }\n+                                                        } else {\n+                                                            return streamMetadataStore.streamCutStrictlyGreaterThan(scope, stream, lowerBound, limitMin.getStreamCut(), context, executor)\n+                                                                               .thenApply(gt -> gt ? limitMin.getStreamCut() : null);\n+                                                        }\n+                                                    });\n+                             } else {\n+                                return CompletableFuture.completedFuture(null);  \n+                             }\n+                         });\n+    }\n+\n+    private Map<Long, Long> computeSubscribersLowerBound(List<Map<StreamSegmentRecord, Long>> subscribers) {\n+        // loop over all streamcuts and for each segment in new streamcut: \n+        // if new segment is predecessor of any segment in the bound then replace the successor segment with\n+        // this segment. \n+        Map<StreamSegmentRecord, Long> lowerBound = new HashMap<>();\n+        subscribers.forEach(streamCut -> streamCut.forEach((segment, offset) -> {\n+            if (lowerBound.containsKey(segment)) {\n+                if (lowerBound.get(segment) > offset) {\n+                    lowerBound.put(segment, offset);\n+                }\n+            } else {\n+                Map<StreamSegmentRecord, Long> predecessors = new HashMap<>();\n+                Map<StreamSegmentRecord, Long> successors = new HashMap<>();\n+                lowerBound.forEach((s, o) -> {\n+                    if (s.overlaps(segment)) {\n+                        if (s.segmentId() < segment.segmentId()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7ee349a4a1b3d67ab0224c4f33e3a8c958adf7c2"}, "originalPosition": 246}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjgyNTY4OA==", "bodyText": "creation epoch is extracted from the segment id itself. we create segmentid as \"32byte epoch + 32 byte segment number\".", "url": "https://github.com/pravega/pravega/pull/5289#discussion_r522825688", "createdAt": "2020-11-13T09:23:08Z", "author": {"login": "shiveshr"}, "path": "controller/src/main/java/io/pravega/controller/task/Stream/StreamMetadataTasks.java", "diffHunk": "@@ -507,32 +514,283 @@ public void initializeStreamWriters(final EventStreamClientFactory clientFactory\n \n     private CompletableFuture<Void> truncate(String scope, String stream, RetentionPolicy policy, OperationContext context,\n                                              RetentionSet retentionSet, StreamCutRecord newRecord, long recordingTime, long requestId) {\n-        Optional<StreamCutReferenceRecord> truncationRecord = findTruncationRecord(policy, retentionSet, newRecord, recordingTime);\n-        if (!truncationRecord.isPresent()) {\n-            log.info(\"No suitable truncation record found, per retention policy for stream {}/{}\", scope, stream);\n-            return CompletableFuture.completedFuture(null);\n+        if (policy.getRetentionType().equals(RetentionPolicy.RetentionType.CONSUMPTION)) {\n+            return subscriberBasedTruncation(scope, stream, context, policy, retentionSet, newRecord, requestId);\n+        } else {\n+            Optional<StreamCutReferenceRecord> truncationRecord = findTruncationRecord(policy, retentionSet, newRecord, recordingTime);\n+            if (!truncationRecord.isPresent()) {\n+                log.info(\"No suitable truncation record found, per retention policy for stream {}/{}\", scope, stream);\n+                return CompletableFuture.completedFuture(null);\n+            }\n+            log.info(\"Found truncation record for stream {}/{} truncationRecord time/size: {}/{}\", scope, stream,\n+                    truncationRecord.get().getRecordingTime(), truncationRecord.get().getRecordingSize());\n+\n+            return truncate(scope, stream, context, requestId, truncationRecord.get());\n         }\n+    }\n+\n+    private CompletableFuture<Void> truncate(String scope, String stream, OperationContext context, long requestId,\n+                                                   StreamCutReferenceRecord truncationRecord) {\n         log.info(\"Found truncation record for stream {}/{} truncationRecord time/size: {}/{}\", scope, stream,\n-                truncationRecord.get().getRecordingTime(), truncationRecord.get().getRecordingSize());\n-        return streamMetadataStore.getStreamCutRecord(scope, stream, truncationRecord.get(), context, executor)\n-                   .thenCompose(streamCutRecord -> startTruncation(scope, stream, streamCutRecord.getStreamCut(), context, requestId))\n-                   .thenCompose(started -> {\n-                       if (started) {\n-                                return streamMetadataStore.deleteStreamCutBefore(scope, stream, truncationRecord.get(), context, executor);\n+                truncationRecord.getRecordingTime(), truncationRecord.getRecordingSize());\n+        return streamMetadataStore.getStreamCutRecord(scope, stream, truncationRecord, context, executor)\n+                                  .thenCompose(streamCutRecord -> startTruncation(scope, stream, streamCutRecord.getStreamCut(), context, requestId))\n+                                  .thenCompose(started -> {\n+                                      if (started) {\n+                                          return streamMetadataStore.deleteStreamCutBefore(scope, stream, truncationRecord, context, executor);\n+                                      } else {\n+                                          throw new RuntimeException(\"Could not start truncation\");\n+                                      }\n+                                  })\n+                                  .exceptionally(e -> {\n+                                      if (Exceptions.unwrap(e) instanceof IllegalArgumentException) {\n+                                          // This is ignorable exception. Throwing this will cause unnecessary retries and exceptions logged.\n+                                          log.debug(requestId, \"Cannot truncate at given \" +\n+                                                  \"streamCut because it intersects with existing truncation point\");\n+                                          return null;\n+                                      } else {\n+                                          throw new CompletionException(e);\n+                                      }\n+                                  });\n+    }\n+\n+    private CompletableFuture<Void> subscriberBasedTruncation(String scope, String stream, OperationContext context,\n+                                                              RetentionPolicy policy, RetentionSet retentionSet, \n+                                                              StreamCutRecord newRecord, long requestId) {\n+        return streamMetadataStore.listSubscribers(scope, stream, context, executor)\n+                           .thenCompose(list -> Futures.allOfWithResults(list.stream().map(x -> \n+                                   streamMetadataStore.getSubscriber(scope, stream, x, context, executor)).collect(Collectors.toList())))\n+                           .thenCompose(subscribers -> {\n+                               // if the range for which we are looking at is \n+                               return Futures.allOfWithResults(subscribers.stream().map(x -> {\n+                                   ImmutableSet<Map.Entry<Long, Long>> entries = x.getObject().getTruncationStreamCut().entrySet();\n+\n+                                   return Futures.keysAllOfWithResults(entries.stream().collect(Collectors.toMap(\n+                                           y -> streamMetadataStore.getSegment(scope, stream, y.getKey(), context, executor), Map.Entry::getValue)));\n+                               }).collect(Collectors.toList()));\n+                           })\n+                           .thenApply(this::computeSubscribersLowerBound)\n+                .thenCompose(lowerBound -> {\n+                    CompletableFuture<Map<Long, Long>> toTruncateAt; \n+                    if (policy.getConsumptionLimits().getType().equals(RetentionPolicy.ConsumptionLimits.Type.SIZE_BYTES)) {\n+                        toTruncateAt = getTruncationStreamCutBySizeLimit(scope, stream, context, policy, retentionSet, lowerBound, newRecord);\n+                    } else {\n+                        toTruncateAt = getTruncationStreamCutByTimeLimit(scope, stream, context, policy, retentionSet, lowerBound, newRecord);\n+                    }\n+                    return toTruncateAt.thenCompose(truncationStreamCut -> {\n+                        if (truncationStreamCut == null || truncationStreamCut.isEmpty()) {\n+                            log.debug(\"no truncation record could be compute that satisfied retention policy\");\n+                            return CompletableFuture.completedFuture(null);\n+                        } \n+                        return startTruncation(scope, stream, truncationStreamCut, context, requestId)\n+                                .thenCompose(started -> {\n+                                    if (started) {\n+                                        return streamMetadataStore.findStreamCutReferenceRecordBefore(scope, stream, truncationStreamCut, retentionSet, context, executor)\n+                                                                  .thenCompose(ref -> {\n+                                                                      if (ref != null) {\n+                                                                          return streamMetadataStore.deleteStreamCutBefore(scope, stream, ref, context, executor);\n+                                                                      } else {\n+                                                                          return CompletableFuture.completedFuture(null);\n+                                                                      }\n+                                                                  });\n+                                    } else {\n+                                        throw new RuntimeException(\"Could not start truncation\");\n+                                    }\n+                                });\n+                    });\n+                });\n+    }\n+\n+    private CompletableFuture<Map<Long, Long>> getTruncationStreamCutBySizeLimit(String scope, String stream, OperationContext context, RetentionPolicy policy,\n+                                                                                 RetentionSet retentionSet, Map<Long, Long> lowerBound, StreamCutRecord newRecord) {\n+        // if the lowerbound on subscribers streamcuts satisfies the policy size bound, then return it. \n+        // else return the stream cut that satisfies maximum bound on size. \n+        long currentSize = newRecord != null ? newRecord.getRecordingSize() : retentionSet.getLatest().getRecordingSize();\n+        return streamMetadataStore.getSizeTillStreamCut(scope, stream, lowerBound, Optional.empty(), context, executor)\n+                          .thenCompose(sizeTill -> {\n+                               long retainedSize = currentSize - sizeTill;\n+                               Supplier<Optional<StreamCutReferenceRecord>> maxBound = () -> retentionSet\n+                                      .getRetentionRecords().stream()\n+                                      .filter(x -> currentSize - x.getRecordingSize() > policy.getConsumptionLimits().getMaxValue())\n+                                      .max(Comparator.comparingLong(StreamCutReferenceRecord::getRecordingTime));\n+\n+                               // if retainedSize is less than min size then do not truncate the stream. \n+                               if (retainedSize < policy.getConsumptionLimits().getMinValue()) {\n+                                   return maxBound.get().map(x -> streamMetadataStore.getStreamCutRecord(scope, stream, x, context, executor)\n+                                                                               .thenApply(StreamCutRecord::getStreamCut))\n+                                                  .orElse(CompletableFuture.completedFuture(null));\n+                               } else {\n+                                   // if retained size is less than max allowed, then truncate the stream at subscriber lower bound. \n+                                   if (retainedSize < policy.getConsumptionLimits().getMaxValue()) {\n+                                       return CompletableFuture.completedFuture(lowerBound);\n+                                   } else {\n+                                       // if retained size is greater than max allowed, then truncate the stream at streamcut\n+                                       // from retention set that matches the retention policy size bound. \n+                                       return maxBound.get().map(x -> streamMetadataStore.getStreamCutRecord(scope, stream, x, context, executor)\n+                                                   .thenApply(StreamCutRecord::getStreamCut)\n+                                                   .thenCompose(maxRecord -> {\n+                                                       // if max record is strictly greater than lowerbound then we can truncate at max record\n+                                                       return streamMetadataStore.streamCutStrictlyGreaterThan(\n+                                                               scope, stream, maxRecord, lowerBound, context, executor)\n+                                                                                 .thenApply(gt -> {\n+                                                                                     if (gt) {\n+                                                                                         return maxRecord;\n+                                                                                     } else {\n+                                                                                         return lowerBound;\n+                                                                                     }\n+                                                                                 });\n+                                                   })).orElse(CompletableFuture.completedFuture(null));   \n+                                   }\n+                               }\n+                           });\n+    }\n+\n+    private CompletableFuture<Map<Long, Long>> getTruncationStreamCutByTimeLimit(String scope, String stream, OperationContext context,\n+                                                                                 RetentionPolicy policy, RetentionSet retentionSet,\n+                                                                                 Map<Long, Long> lowerBound, StreamCutRecord latest) {\n+        Map.Entry<StreamCutReferenceRecord, StreamCutReferenceRecord> limits =\n+                getBoundStreamCuts(policy.getConsumptionLimits(), retentionSet, latest);\n+        // if subscriber lowerbound is ahead of streamcut corresponding to the max time and is behind stream cut for min time \n+        // from the retention set then we can safely truncate at lowerbound. Else we will truncate at the max time bound if it\n+        // exists\n+        CompletableFuture<StreamCutRecord> limitMaxFuture = limits.getKey() == null ? CompletableFuture.completedFuture(null) :\n+                streamMetadataStore.getStreamCutRecord(scope, stream, limits.getKey(), context, executor);\n+        CompletableFuture<StreamCutRecord> limitMinFuture = limits.getValue() == null ? CompletableFuture.completedFuture(null) :\n+                streamMetadataStore.getStreamCutRecord(scope, stream, limits.getValue(), context, executor);\n+        return CompletableFuture.allOf(limitMaxFuture, limitMinFuture)\n+                         .thenCompose(v -> {\n+                             StreamCutRecord limitMax = limitMaxFuture.join();\n+                             StreamCutRecord limitMin = limitMinFuture.join();\n+                             if (limitMin != null) {\n+                                 return streamMetadataStore.streamCutStrictlyGreaterThan(scope, stream, limitMin.getStreamCut(), lowerBound, context, executor)\n+                                                    .thenCompose(gtMin -> {\n+                                                        if (gtMin) {\n+                                                            if (limitMax == null) {\n+                                                                return CompletableFuture.completedFuture(lowerBound);\n+                                                            } else {\n+                                                                return streamMetadataStore.streamCutStrictlyGreaterThan(scope, stream, limitMax.getStreamCut(), lowerBound, context, executor)\n+                                                                                          .thenApply(gtMax -> gtMax ? limitMax.getStreamCut() : lowerBound);\n+                                                            }\n+                                                        } else {\n+                                                            return streamMetadataStore.streamCutStrictlyGreaterThan(scope, stream, lowerBound, limitMin.getStreamCut(), context, executor)\n+                                                                               .thenApply(gt -> gt ? limitMin.getStreamCut() : null);\n+                                                        }\n+                                                    });\n+                             } else {\n+                                return CompletableFuture.completedFuture(null);  \n+                             }\n+                         });\n+    }\n+\n+    private Map<Long, Long> computeSubscribersLowerBound(List<Map<StreamSegmentRecord, Long>> subscribers) {\n+        // loop over all streamcuts and for each segment in new streamcut: \n+        // if new segment is predecessor of any segment in the bound then replace the successor segment with\n+        // this segment. \n+        Map<StreamSegmentRecord, Long> lowerBound = new HashMap<>();\n+        subscribers.forEach(streamCut -> streamCut.forEach((segment, offset) -> {\n+            if (lowerBound.containsKey(segment)) {\n+                if (lowerBound.get(segment) > offset) {\n+                    lowerBound.put(segment, offset);\n+                }\n+            } else {\n+                Map<StreamSegmentRecord, Long> predecessors = new HashMap<>();\n+                Map<StreamSegmentRecord, Long> successors = new HashMap<>();\n+                lowerBound.forEach((s, o) -> {\n+                    if (s.overlaps(segment)) {\n+                        if (s.segmentId() < segment.segmentId()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTI3MDc5OA=="}, "originalCommit": {"oid": "7ee349a4a1b3d67ab0224c4f33e3a8c958adf7c2"}, "originalPosition": 246}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI3MzAwMDY0OnYy", "diffSide": "RIGHT", "path": "controller/src/main/java/io/pravega/controller/store/stream/PersistentStreamBase.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQxMjoyMzoyOVrOHx4aAA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QwOToyNDozOFrOHymzSw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjA2NjQzMg==", "bodyText": "With this the SubscribersSet record gets added only when updateStream() is invoked.\nHowever,  if the client tries to addSubscriber() to a Stream, without changing the retention policy using updateStream, it will lead to an exception.\nPerhaps it may be good to add createSubscribersRecordIfAbsent() here:\nhttps://github.com/pravega/pravega/blob/master/controller/src/main/java/io/pravega/controller/store/stream/AbstractStreamMetadataStore.java#L528", "url": "https://github.com/pravega/pravega/pull/5289#discussion_r522066432", "createdAt": "2020-11-12T12:23:29Z", "author": {"login": "pbelgundi"}, "path": "controller/src/main/java/io/pravega/controller/store/stream/PersistentStreamBase.java", "diffHunk": "@@ -250,7 +351,8 @@ private boolean greaterThan(Map<Long, Long> cut1, Map<StreamSegmentRecord, Integ\n      */\n     @Override\n     public CompletableFuture<Void> startUpdateConfiguration(final StreamConfiguration newConfiguration) {\n-        return getVersionedConfigurationRecord()\n+        return createSubscribersRecordIfAbsent()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bd7badf9bd8d1dd457baf8014f4716b0b468f61c"}, "originalPosition": 170}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjgyNjU3MQ==", "bodyText": "thats a good suggestion. it keeps the change in context. i will also add the comment that this is only for streams prior to 0.9 being changed to CBR policy", "url": "https://github.com/pravega/pravega/pull/5289#discussion_r522826571", "createdAt": "2020-11-13T09:24:38Z", "author": {"login": "shiveshr"}, "path": "controller/src/main/java/io/pravega/controller/store/stream/PersistentStreamBase.java", "diffHunk": "@@ -250,7 +351,8 @@ private boolean greaterThan(Map<Long, Long> cut1, Map<StreamSegmentRecord, Integ\n      */\n     @Override\n     public CompletableFuture<Void> startUpdateConfiguration(final StreamConfiguration newConfiguration) {\n-        return getVersionedConfigurationRecord()\n+        return createSubscribersRecordIfAbsent()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjA2NjQzMg=="}, "originalCommit": {"oid": "bd7badf9bd8d1dd457baf8014f4716b0b468f61c"}, "originalPosition": 170}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI3MzA1MjIyOnYy", "diffSide": "RIGHT", "path": "controller/src/main/java/io/pravega/controller/store/stream/PersistentStreamBase.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQxMjozODowMFrOHx45sQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QwOToyNTowNFrOHym0Tg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjA3NDU0NQ==", "bodyText": "Instead of re-implementing binarySearch could we use Collections.binarySearch() ?", "url": "https://github.com/pravega/pravega/pull/5289#discussion_r522074545", "createdAt": "2020-11-12T12:38:00Z", "author": {"login": "pbelgundi"}, "path": "controller/src/main/java/io/pravega/controller/store/stream/PersistentStreamBase.java", "diffHunk": "@@ -242,6 +248,101 @@ private boolean greaterThan(Map<Long, Long> cut1, Map<StreamSegmentRecord, Integ\n                 });\n     }\n \n+    @Override\n+    public CompletableFuture<Boolean> isStreamCutStrictlyGreaterThan(Map<Long, Long> streamcut1, Map<Long, Long> streamcut2) {\n+        CompletableFuture<ImmutableMap<StreamSegmentRecord, Integer>> span1Future = computeStreamCutSpan(streamcut1);\n+        CompletableFuture<ImmutableMap<StreamSegmentRecord, Integer>> span2Future = computeStreamCutSpan(streamcut2);\n+        return CompletableFuture.allOf(span1Future, span2Future)\n+                    .thenApply(v -> {\n+                        ImmutableMap<StreamSegmentRecord, Integer> span1 = span1Future.join();\n+                        ImmutableMap<StreamSegmentRecord, Integer> span2 = span2Future.join();\n+                        return greaterThan(streamcut1, span1, streamcut2, span2);\n+                    });\n+    }\n+\n+    @Override\n+    public CompletableFuture<StreamCutReferenceRecord> findStreamCutReferenceRecordBefore(Map<Long, Long> streamCut, RetentionSet retentionSet) {\n+        Map<Set<Long>, ImmutableMap<StreamSegmentRecord, Integer>> fetched = new HashMap<>();\n+        int size = retentionSet.getRetentionRecords().size();\n+\n+        if (retentionSet.getRetentionRecords().isEmpty()) {\n+            return CompletableFuture.completedFuture(null);\n+        }\n+        return computeStreamCutSpan(streamCut)\n+                .thenCompose(span1 -> {\n+                    fetched.put(streamCut.keySet(), span1);\n+                    BiFunction<StreamCutReferenceRecord, Boolean, CompletableFuture<Integer>> fn = (refRecord, comparison) -> {\n+                        return getStreamCutRecord(refRecord)\n+                                .thenCompose(record -> {\n+                                    if (record.getStreamCut().equals(streamCut)) {\n+                                        return CompletableFuture.completedFuture(0);\n+                                    }\n+                                    ImmutableMap<StreamSegmentRecord, Integer> sc = fetched.get(record.getStreamCut().keySet());\n+                                    CompletableFuture<ImmutableMap<StreamSegmentRecord, Integer>> future;\n+                                    if (sc != null) {\n+                                        future = CompletableFuture.completedFuture(sc);\n+                                    } else {\n+                                        future = computeStreamCutSpan(record.getStreamCut())\n+                                                .thenApply(span2 -> {\n+                                                    fetched.put(record.getStreamCut().keySet(), span2);\n+                                                    return span2;\n+                                                });\n+                                    }\n+                                    return future.thenApply(span2 -> {\n+                                        boolean compare = comparison ? greaterThan(streamCut, span1, record.getStreamCut(), span2) :\n+                                                greaterThan(record.getStreamCut(), span2, streamCut, span1);\n+                                        if (compare) {\n+                                            return 1;\n+                                        } else {\n+                                            return -1;\n+                                        }\n+                                    });\n+                                });\n+                    };\n+\n+                    // binary search retention set. \n+                    return binarySearch(0, size, index -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bd7badf9bd8d1dd457baf8014f4716b0b468f61c"}, "originalPosition": 120}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjgyNjgzMA==", "bodyText": "this is an asynchronous binary search implementation. its working with futures :)", "url": "https://github.com/pravega/pravega/pull/5289#discussion_r522826830", "createdAt": "2020-11-13T09:25:04Z", "author": {"login": "shiveshr"}, "path": "controller/src/main/java/io/pravega/controller/store/stream/PersistentStreamBase.java", "diffHunk": "@@ -242,6 +248,101 @@ private boolean greaterThan(Map<Long, Long> cut1, Map<StreamSegmentRecord, Integ\n                 });\n     }\n \n+    @Override\n+    public CompletableFuture<Boolean> isStreamCutStrictlyGreaterThan(Map<Long, Long> streamcut1, Map<Long, Long> streamcut2) {\n+        CompletableFuture<ImmutableMap<StreamSegmentRecord, Integer>> span1Future = computeStreamCutSpan(streamcut1);\n+        CompletableFuture<ImmutableMap<StreamSegmentRecord, Integer>> span2Future = computeStreamCutSpan(streamcut2);\n+        return CompletableFuture.allOf(span1Future, span2Future)\n+                    .thenApply(v -> {\n+                        ImmutableMap<StreamSegmentRecord, Integer> span1 = span1Future.join();\n+                        ImmutableMap<StreamSegmentRecord, Integer> span2 = span2Future.join();\n+                        return greaterThan(streamcut1, span1, streamcut2, span2);\n+                    });\n+    }\n+\n+    @Override\n+    public CompletableFuture<StreamCutReferenceRecord> findStreamCutReferenceRecordBefore(Map<Long, Long> streamCut, RetentionSet retentionSet) {\n+        Map<Set<Long>, ImmutableMap<StreamSegmentRecord, Integer>> fetched = new HashMap<>();\n+        int size = retentionSet.getRetentionRecords().size();\n+\n+        if (retentionSet.getRetentionRecords().isEmpty()) {\n+            return CompletableFuture.completedFuture(null);\n+        }\n+        return computeStreamCutSpan(streamCut)\n+                .thenCompose(span1 -> {\n+                    fetched.put(streamCut.keySet(), span1);\n+                    BiFunction<StreamCutReferenceRecord, Boolean, CompletableFuture<Integer>> fn = (refRecord, comparison) -> {\n+                        return getStreamCutRecord(refRecord)\n+                                .thenCompose(record -> {\n+                                    if (record.getStreamCut().equals(streamCut)) {\n+                                        return CompletableFuture.completedFuture(0);\n+                                    }\n+                                    ImmutableMap<StreamSegmentRecord, Integer> sc = fetched.get(record.getStreamCut().keySet());\n+                                    CompletableFuture<ImmutableMap<StreamSegmentRecord, Integer>> future;\n+                                    if (sc != null) {\n+                                        future = CompletableFuture.completedFuture(sc);\n+                                    } else {\n+                                        future = computeStreamCutSpan(record.getStreamCut())\n+                                                .thenApply(span2 -> {\n+                                                    fetched.put(record.getStreamCut().keySet(), span2);\n+                                                    return span2;\n+                                                });\n+                                    }\n+                                    return future.thenApply(span2 -> {\n+                                        boolean compare = comparison ? greaterThan(streamCut, span1, record.getStreamCut(), span2) :\n+                                                greaterThan(record.getStreamCut(), span2, streamCut, span1);\n+                                        if (compare) {\n+                                            return 1;\n+                                        } else {\n+                                            return -1;\n+                                        }\n+                                    });\n+                                });\n+                    };\n+\n+                    // binary search retention set. \n+                    return binarySearch(0, size, index -> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjA3NDU0NQ=="}, "originalCommit": {"oid": "bd7badf9bd8d1dd457baf8014f4716b0b468f61c"}, "originalPosition": 120}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI3MzA3Njg5OnYy", "diffSide": "RIGHT", "path": "controller/src/test/java/io/pravega/controller/task/Stream/StreamMetadataTasksTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQxMjo0NDozN1rOHx5Iwg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QxMTozMDozNFrOHyq4mQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjA3ODQwMg==", "bodyText": "Do these tests cover all cases listed here:\n\n  \n    \n      pravega/controller/src/main/java/io/pravega/controller/task/Stream/StreamMetadataTasks.java\n    \n    \n         Line 608\n      in\n      bd7badf\n    \n    \n    \n    \n\n        \n          \n           // 1. if lowerbound.size < max and lowerbound.size > min truncate at lowerbound", "url": "https://github.com/pravega/pravega/pull/5289#discussion_r522078402", "createdAt": "2020-11-12T12:44:37Z", "author": {"login": "pbelgundi"}, "path": "controller/src/test/java/io/pravega/controller/task/Stream/StreamMetadataTasksTest.java", "diffHunk": "@@ -1020,6 +1021,446 @@ public void sizeBasedRetentionStreamTest() throws Exception {\n         // endregion\n         // endregion\n     }\n+    \n+    @Test(timeout = 30000)\n+    public void consumptionBasedRetentionSizeLimitTest() throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bd7badf9bd8d1dd457baf8014f4716b0b468f61c"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjg5MzQ2NQ==", "bodyText": "yes", "url": "https://github.com/pravega/pravega/pull/5289#discussion_r522893465", "createdAt": "2020-11-13T11:30:34Z", "author": {"login": "shiveshr"}, "path": "controller/src/test/java/io/pravega/controller/task/Stream/StreamMetadataTasksTest.java", "diffHunk": "@@ -1020,6 +1021,446 @@ public void sizeBasedRetentionStreamTest() throws Exception {\n         // endregion\n         // endregion\n     }\n+    \n+    @Test(timeout = 30000)\n+    public void consumptionBasedRetentionSizeLimitTest() throws Exception {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjA3ODQwMg=="}, "originalCommit": {"oid": "bd7badf9bd8d1dd457baf8014f4716b0b468f61c"}, "originalPosition": 14}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI3MzA3OTcwOnYy", "diffSide": "RIGHT", "path": "controller/src/test/java/io/pravega/controller/task/Stream/StreamMetadataTasksTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQxMjo0NToxOVrOHx5KdA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QxMTozMDoyOVrOHyq4cA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjA3ODgzNg==", "bodyText": "Do these tests cover all cases listed here:\n\n  \n    \n      pravega/controller/src/main/java/io/pravega/controller/task/Stream/StreamMetadataTasks.java\n    \n    \n         Line 659\n      in\n      bd7badf\n    \n    \n    \n    \n\n        \n          \n           // 1. if LB > min => truncate at min", "url": "https://github.com/pravega/pravega/pull/5289#discussion_r522078836", "createdAt": "2020-11-12T12:45:19Z", "author": {"login": "pbelgundi"}, "path": "controller/src/test/java/io/pravega/controller/task/Stream/StreamMetadataTasksTest.java", "diffHunk": "@@ -1020,6 +1021,446 @@ public void sizeBasedRetentionStreamTest() throws Exception {\n         // endregion\n         // endregion\n     }\n+    \n+    @Test(timeout = 30000)\n+    public void consumptionBasedRetentionSizeLimitTest() throws Exception {\n+        final ScalingPolicy policy = ScalingPolicy.fixed(2);\n+        final RetentionPolicy retentionPolicy = RetentionPolicy.byConsumption(RetentionPolicy.ConsumptionLimits.Type.SIZE_BYTES, 2L, 10L);\n+\n+        String stream1 = \"consumptionSize\";\n+        final StreamConfiguration configuration = StreamConfiguration.builder().scalingPolicy(policy)\n+                .retentionPolicy(retentionPolicy).build();\n+\n+        streamStorePartialMock.createStream(SCOPE, stream1, configuration, System.currentTimeMillis(), null, executor).get();\n+        streamStorePartialMock.setState(SCOPE, stream1, State.ACTIVE, null, executor).get();\n+\n+        assertNotEquals(0, consumer.getCurrentSegments(SCOPE, stream1).get().size());\n+        WriterMock requestEventWriter = new WriterMock(streamMetadataTasks, executor);\n+        streamMetadataTasks.setRequestEventWriter(requestEventWriter);\n+        streamMetadataTasks.setRetentionFrequencyMillis(1L);\n+        // region case 1: basic retention\n+        // add subscriber 1\n+        // add subscriber 2\n+        String subscriber1 = \"subscriber1\";\n+        streamMetadataTasks.addSubscriber(SCOPE, stream1, subscriber1, null).join();\n+\n+        String subscriber2 = \"subscriber2\";\n+        streamMetadataTasks.addSubscriber(SCOPE, stream1, subscriber2, null).join();\n+        \n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber1, ImmutableMap.of(0L, 2L, 1L, 1L), null).join();\n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber2, ImmutableMap.of(0L, 1L, 1L, 2L), null).join();\n+\n+        Map<Long, Long> map1 = new HashMap<>();\n+        map1.put(0L, 2L);\n+        map1.put(1L, 2L);\n+        long size = streamStorePartialMock.getSizeTillStreamCut(SCOPE, stream1, map1, Optional.empty(), null, executor).join();\n+        doReturn(CompletableFuture.completedFuture(new StreamCutRecord(1L, size, ImmutableMap.copyOf(map1))))\n+                .when(streamMetadataTasks).generateStreamCut(anyString(), anyString(), any(), any(), any());\n+\n+        // call retention and verify that retention policy applies\n+        streamMetadataTasks.retention(SCOPE, stream1, retentionPolicy, 1L, null, \"\").join();\n+        // now retention set has one stream cut 0/2, 1/2\n+        // subscriber lowerbound is 0/1, 1/1.. trucation should happen at lowerbound\n+\n+        VersionedMetadata<StreamTruncationRecord> truncationRecord = streamStorePartialMock.getTruncationRecord(SCOPE, stream1, null, executor).join();\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(0L).longValue(), 1L);\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(1L).longValue(), 1L);\n+        assertTrue(truncationRecord.getObject().isUpdating());\n+        streamStorePartialMock.completeTruncation(SCOPE, stream1, truncationRecord, null, executor).join();\n+        // endregion\n+        \n+        // region case 2 min policy check\n+        // we will update the new streamcut to 0/10, 1/10\n+        map1.put(0L, 2L);\n+        map1.put(1L, 2L);\n+        size = streamStorePartialMock.getSizeTillStreamCut(SCOPE, stream1, map1, Optional.empty(), null, executor).join();\n+        doReturn(CompletableFuture.completedFuture(new StreamCutRecord(20L, size, ImmutableMap.copyOf(map1))))\n+                .when(streamMetadataTasks).generateStreamCut(anyString(), anyString(), any(), any(), any());\n+\n+        // update both readers to make sure they have read till the latest position. we have set the min limit to 2.  \n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber1, ImmutableMap.of(0L, 2L, 1L, 2L), null).join();\n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber2, ImmutableMap.of(0L, 2L, 1L, 2L), null).join();\n+\n+        // no new truncation should happen. \n+        // verify that truncation record has not changed. \n+        streamMetadataTasks.retention(SCOPE, stream1, retentionPolicy, 20L, null, \"\").join();\n+        // now retention set has two stream cut 0/2, 1/2...0/2, 1/2\n+        // subscriber lowerbound is 0/2, 1/2.. does not meet min bound criteria. we also do not have a max that satisfies the limit. no truncation should happen. \n+        // no change:\n+        truncationRecord = streamStorePartialMock.getTruncationRecord(SCOPE, stream1, null, executor).join();\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(0L).longValue(), 1L);\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(1L).longValue(), 1L);\n+        assertFalse(truncationRecord.getObject().isUpdating());\n+        // endregion\n+        \n+        // region case 3: min criteria not met on lower bound. truncate at max. \n+        map1.put(0L, 10L);\n+        map1.put(1L, 10L);\n+        size = streamStorePartialMock.getSizeTillStreamCut(SCOPE, stream1, map1, Optional.empty(), null, executor).join();\n+        doReturn(CompletableFuture.completedFuture(new StreamCutRecord(30L, size, ImmutableMap.copyOf(map1))))\n+                .when(streamMetadataTasks).generateStreamCut(anyString(), anyString(), any(), any(), any());\n+\n+        // update both readers to make sure they have read till the latest position - 1. we have set the min limit to 2.  \n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber1, ImmutableMap.of(0L, 10L, 1L, 9L), null).join();\n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber2, ImmutableMap.of(0L, 10L, 1L, 9L), null).join();\n+\n+        streamMetadataTasks.retention(SCOPE, stream1, retentionPolicy, 30L, null, \"\").join();\n+        // now retention set has three stream cut 0/2, 1/2...0/2, 1/2... 0/10, 1/10\n+        // subscriber lowerbound is 0/10, 1/9.. does not meet min bound criteria. but we have max bound on truncation record\n+        // truncation should happen at 0/2, 1/2\n+        truncationRecord = streamStorePartialMock.getTruncationRecord(SCOPE, stream1, null, executor).join();\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(0L).longValue(), 2L);\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(1L).longValue(), 2L);\n+        assertTrue(truncationRecord.getObject().isUpdating());\n+        streamStorePartialMock.completeTruncation(SCOPE, stream1, truncationRecord, null, executor).join();\n+        // endregion\n+        \n+        // region case 4: lowerbound behind max\n+        // now move the stream further ahead so that max truncation limit is crossed but lowerbound is behind max. \n+        map1.put(0L, 20L);\n+        map1.put(1L, 20L);\n+        size = streamStorePartialMock.getSizeTillStreamCut(SCOPE, stream1, map1, Optional.empty(), null, executor).join();\n+        doReturn(CompletableFuture.completedFuture(new StreamCutRecord(40L, size, ImmutableMap.copyOf(map1))))\n+                .when(streamMetadataTasks).generateStreamCut(anyString(), anyString(), any(), any(), any());\n+        \n+        streamMetadataTasks.retention(SCOPE, stream1, retentionPolicy, 40L, null, \"\").join();\n+        // now retention set has three stream cut 0/2, 1/2...0/2, 1/2... 0/10, 1/10.. 0/20, 1/20\n+        // subscriber lowerbound is 0/10, 1/9.. meets min bound criteria. but we have max bound on truncation record\n+        // truncation should happen at 0/10, 1/10\n+        truncationRecord = streamStorePartialMock.getTruncationRecord(SCOPE, stream1, null, executor).join();\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(0L).longValue(), 10L);\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(1L).longValue(), 10L);\n+        assertTrue(truncationRecord.getObject().isUpdating());\n+        streamStorePartialMock.completeTruncation(SCOPE, stream1, truncationRecord, null, executor).join();\n+        // endregion\n+\n+        // region case 5: lowerbound overlaps with max\n+        map1.put(0L, 30L);\n+        map1.put(1L, 30L);\n+        size = streamStorePartialMock.getSizeTillStreamCut(SCOPE, stream1, map1, Optional.empty(), null, executor).join();\n+        doReturn(CompletableFuture.completedFuture(new StreamCutRecord(50L, size, ImmutableMap.copyOf(map1))))\n+                .when(streamMetadataTasks).generateStreamCut(anyString(), anyString(), any(), any(), any());\n+\n+        // update both readers to make sure they have read till the latest position - 1. we have set the min limit to 2.  \n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber1, ImmutableMap.of(0L, 21L, 1L, 19L), null).join();\n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber2, ImmutableMap.of(0L, 21L, 1L, 19L), null).join();\n+\n+        streamMetadataTasks.retention(SCOPE, stream1, retentionPolicy, 50L, null, \"\").join();\n+        // now retention set has three stream cut 0/2, 1/2...0/2, 1/2... 0/10, 1/10.. 0/20, 1/20.. 0/30, 1/30\n+        // subscriber lowerbound is 0/21, 1/19.. meets min bound criteria. and its also greater than max bound. but it overlaps with max bound. \n+        // truncation should happen at 0/21, 1/19\n+        truncationRecord = streamStorePartialMock.getTruncationRecord(SCOPE, stream1, null, executor).join();\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(0L).longValue(), 21L);\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(1L).longValue(), 19L);\n+        assertTrue(truncationRecord.getObject().isUpdating());\n+        streamStorePartialMock.completeTruncation(SCOPE, stream1, truncationRecord, null, executor).join();\n+        // endregion\n+    }\n+    \n+    @Test(timeout = 30000)\n+    public void consumptionBasedRetentionTimeLimitTest() throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bd7badf9bd8d1dd457baf8014f4716b0b468f61c"}, "originalPosition": 149}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjg5MzQyNA==", "bodyText": "yes", "url": "https://github.com/pravega/pravega/pull/5289#discussion_r522893424", "createdAt": "2020-11-13T11:30:29Z", "author": {"login": "shiveshr"}, "path": "controller/src/test/java/io/pravega/controller/task/Stream/StreamMetadataTasksTest.java", "diffHunk": "@@ -1020,6 +1021,446 @@ public void sizeBasedRetentionStreamTest() throws Exception {\n         // endregion\n         // endregion\n     }\n+    \n+    @Test(timeout = 30000)\n+    public void consumptionBasedRetentionSizeLimitTest() throws Exception {\n+        final ScalingPolicy policy = ScalingPolicy.fixed(2);\n+        final RetentionPolicy retentionPolicy = RetentionPolicy.byConsumption(RetentionPolicy.ConsumptionLimits.Type.SIZE_BYTES, 2L, 10L);\n+\n+        String stream1 = \"consumptionSize\";\n+        final StreamConfiguration configuration = StreamConfiguration.builder().scalingPolicy(policy)\n+                .retentionPolicy(retentionPolicy).build();\n+\n+        streamStorePartialMock.createStream(SCOPE, stream1, configuration, System.currentTimeMillis(), null, executor).get();\n+        streamStorePartialMock.setState(SCOPE, stream1, State.ACTIVE, null, executor).get();\n+\n+        assertNotEquals(0, consumer.getCurrentSegments(SCOPE, stream1).get().size());\n+        WriterMock requestEventWriter = new WriterMock(streamMetadataTasks, executor);\n+        streamMetadataTasks.setRequestEventWriter(requestEventWriter);\n+        streamMetadataTasks.setRetentionFrequencyMillis(1L);\n+        // region case 1: basic retention\n+        // add subscriber 1\n+        // add subscriber 2\n+        String subscriber1 = \"subscriber1\";\n+        streamMetadataTasks.addSubscriber(SCOPE, stream1, subscriber1, null).join();\n+\n+        String subscriber2 = \"subscriber2\";\n+        streamMetadataTasks.addSubscriber(SCOPE, stream1, subscriber2, null).join();\n+        \n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber1, ImmutableMap.of(0L, 2L, 1L, 1L), null).join();\n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber2, ImmutableMap.of(0L, 1L, 1L, 2L), null).join();\n+\n+        Map<Long, Long> map1 = new HashMap<>();\n+        map1.put(0L, 2L);\n+        map1.put(1L, 2L);\n+        long size = streamStorePartialMock.getSizeTillStreamCut(SCOPE, stream1, map1, Optional.empty(), null, executor).join();\n+        doReturn(CompletableFuture.completedFuture(new StreamCutRecord(1L, size, ImmutableMap.copyOf(map1))))\n+                .when(streamMetadataTasks).generateStreamCut(anyString(), anyString(), any(), any(), any());\n+\n+        // call retention and verify that retention policy applies\n+        streamMetadataTasks.retention(SCOPE, stream1, retentionPolicy, 1L, null, \"\").join();\n+        // now retention set has one stream cut 0/2, 1/2\n+        // subscriber lowerbound is 0/1, 1/1.. trucation should happen at lowerbound\n+\n+        VersionedMetadata<StreamTruncationRecord> truncationRecord = streamStorePartialMock.getTruncationRecord(SCOPE, stream1, null, executor).join();\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(0L).longValue(), 1L);\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(1L).longValue(), 1L);\n+        assertTrue(truncationRecord.getObject().isUpdating());\n+        streamStorePartialMock.completeTruncation(SCOPE, stream1, truncationRecord, null, executor).join();\n+        // endregion\n+        \n+        // region case 2 min policy check\n+        // we will update the new streamcut to 0/10, 1/10\n+        map1.put(0L, 2L);\n+        map1.put(1L, 2L);\n+        size = streamStorePartialMock.getSizeTillStreamCut(SCOPE, stream1, map1, Optional.empty(), null, executor).join();\n+        doReturn(CompletableFuture.completedFuture(new StreamCutRecord(20L, size, ImmutableMap.copyOf(map1))))\n+                .when(streamMetadataTasks).generateStreamCut(anyString(), anyString(), any(), any(), any());\n+\n+        // update both readers to make sure they have read till the latest position. we have set the min limit to 2.  \n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber1, ImmutableMap.of(0L, 2L, 1L, 2L), null).join();\n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber2, ImmutableMap.of(0L, 2L, 1L, 2L), null).join();\n+\n+        // no new truncation should happen. \n+        // verify that truncation record has not changed. \n+        streamMetadataTasks.retention(SCOPE, stream1, retentionPolicy, 20L, null, \"\").join();\n+        // now retention set has two stream cut 0/2, 1/2...0/2, 1/2\n+        // subscriber lowerbound is 0/2, 1/2.. does not meet min bound criteria. we also do not have a max that satisfies the limit. no truncation should happen. \n+        // no change:\n+        truncationRecord = streamStorePartialMock.getTruncationRecord(SCOPE, stream1, null, executor).join();\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(0L).longValue(), 1L);\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(1L).longValue(), 1L);\n+        assertFalse(truncationRecord.getObject().isUpdating());\n+        // endregion\n+        \n+        // region case 3: min criteria not met on lower bound. truncate at max. \n+        map1.put(0L, 10L);\n+        map1.put(1L, 10L);\n+        size = streamStorePartialMock.getSizeTillStreamCut(SCOPE, stream1, map1, Optional.empty(), null, executor).join();\n+        doReturn(CompletableFuture.completedFuture(new StreamCutRecord(30L, size, ImmutableMap.copyOf(map1))))\n+                .when(streamMetadataTasks).generateStreamCut(anyString(), anyString(), any(), any(), any());\n+\n+        // update both readers to make sure they have read till the latest position - 1. we have set the min limit to 2.  \n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber1, ImmutableMap.of(0L, 10L, 1L, 9L), null).join();\n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber2, ImmutableMap.of(0L, 10L, 1L, 9L), null).join();\n+\n+        streamMetadataTasks.retention(SCOPE, stream1, retentionPolicy, 30L, null, \"\").join();\n+        // now retention set has three stream cut 0/2, 1/2...0/2, 1/2... 0/10, 1/10\n+        // subscriber lowerbound is 0/10, 1/9.. does not meet min bound criteria. but we have max bound on truncation record\n+        // truncation should happen at 0/2, 1/2\n+        truncationRecord = streamStorePartialMock.getTruncationRecord(SCOPE, stream1, null, executor).join();\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(0L).longValue(), 2L);\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(1L).longValue(), 2L);\n+        assertTrue(truncationRecord.getObject().isUpdating());\n+        streamStorePartialMock.completeTruncation(SCOPE, stream1, truncationRecord, null, executor).join();\n+        // endregion\n+        \n+        // region case 4: lowerbound behind max\n+        // now move the stream further ahead so that max truncation limit is crossed but lowerbound is behind max. \n+        map1.put(0L, 20L);\n+        map1.put(1L, 20L);\n+        size = streamStorePartialMock.getSizeTillStreamCut(SCOPE, stream1, map1, Optional.empty(), null, executor).join();\n+        doReturn(CompletableFuture.completedFuture(new StreamCutRecord(40L, size, ImmutableMap.copyOf(map1))))\n+                .when(streamMetadataTasks).generateStreamCut(anyString(), anyString(), any(), any(), any());\n+        \n+        streamMetadataTasks.retention(SCOPE, stream1, retentionPolicy, 40L, null, \"\").join();\n+        // now retention set has three stream cut 0/2, 1/2...0/2, 1/2... 0/10, 1/10.. 0/20, 1/20\n+        // subscriber lowerbound is 0/10, 1/9.. meets min bound criteria. but we have max bound on truncation record\n+        // truncation should happen at 0/10, 1/10\n+        truncationRecord = streamStorePartialMock.getTruncationRecord(SCOPE, stream1, null, executor).join();\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(0L).longValue(), 10L);\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(1L).longValue(), 10L);\n+        assertTrue(truncationRecord.getObject().isUpdating());\n+        streamStorePartialMock.completeTruncation(SCOPE, stream1, truncationRecord, null, executor).join();\n+        // endregion\n+\n+        // region case 5: lowerbound overlaps with max\n+        map1.put(0L, 30L);\n+        map1.put(1L, 30L);\n+        size = streamStorePartialMock.getSizeTillStreamCut(SCOPE, stream1, map1, Optional.empty(), null, executor).join();\n+        doReturn(CompletableFuture.completedFuture(new StreamCutRecord(50L, size, ImmutableMap.copyOf(map1))))\n+                .when(streamMetadataTasks).generateStreamCut(anyString(), anyString(), any(), any(), any());\n+\n+        // update both readers to make sure they have read till the latest position - 1. we have set the min limit to 2.  \n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber1, ImmutableMap.of(0L, 21L, 1L, 19L), null).join();\n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber2, ImmutableMap.of(0L, 21L, 1L, 19L), null).join();\n+\n+        streamMetadataTasks.retention(SCOPE, stream1, retentionPolicy, 50L, null, \"\").join();\n+        // now retention set has three stream cut 0/2, 1/2...0/2, 1/2... 0/10, 1/10.. 0/20, 1/20.. 0/30, 1/30\n+        // subscriber lowerbound is 0/21, 1/19.. meets min bound criteria. and its also greater than max bound. but it overlaps with max bound. \n+        // truncation should happen at 0/21, 1/19\n+        truncationRecord = streamStorePartialMock.getTruncationRecord(SCOPE, stream1, null, executor).join();\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(0L).longValue(), 21L);\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(1L).longValue(), 19L);\n+        assertTrue(truncationRecord.getObject().isUpdating());\n+        streamStorePartialMock.completeTruncation(SCOPE, stream1, truncationRecord, null, executor).join();\n+        // endregion\n+    }\n+    \n+    @Test(timeout = 30000)\n+    public void consumptionBasedRetentionTimeLimitTest() throws Exception {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjA3ODgzNg=="}, "originalCommit": {"oid": "bd7badf9bd8d1dd457baf8014f4716b0b468f61c"}, "originalPosition": 149}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI3MzA4MzEzOnYy", "diffSide": "RIGHT", "path": "controller/src/test/java/io/pravega/controller/task/Stream/StreamMetadataTasksTest.java", "isResolved": true, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQxMjo0NjoxNFrOHx5MjA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QxMDozOTowNVrOHypR3g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjA3OTM3Mg==", "bodyText": "We need to add tests to cover the following corner cases:\n\nCBR Retention Policy does not specify any max/min limit (so they are at their default values)\nStream does not have any subscribers (# of subscribers = 0 and/or no subscriber has a valid StreamCut... there have been addSubscriber() calls but no updateSubscriberStreamCut() calls so far)\nStream has few subscribers with empty StreamCuts (StreamCuts have not been updated from client so far)", "url": "https://github.com/pravega/pravega/pull/5289#discussion_r522079372", "createdAt": "2020-11-12T12:46:14Z", "author": {"login": "pbelgundi"}, "path": "controller/src/test/java/io/pravega/controller/task/Stream/StreamMetadataTasksTest.java", "diffHunk": "@@ -1020,6 +1021,446 @@ public void sizeBasedRetentionStreamTest() throws Exception {\n         // endregion\n         // endregion\n     }\n+    \n+    @Test(timeout = 30000)\n+    public void consumptionBasedRetentionSizeLimitTest() throws Exception {\n+        final ScalingPolicy policy = ScalingPolicy.fixed(2);\n+        final RetentionPolicy retentionPolicy = RetentionPolicy.byConsumption(RetentionPolicy.ConsumptionLimits.Type.SIZE_BYTES, 2L, 10L);\n+\n+        String stream1 = \"consumptionSize\";\n+        final StreamConfiguration configuration = StreamConfiguration.builder().scalingPolicy(policy)\n+                .retentionPolicy(retentionPolicy).build();\n+\n+        streamStorePartialMock.createStream(SCOPE, stream1, configuration, System.currentTimeMillis(), null, executor).get();\n+        streamStorePartialMock.setState(SCOPE, stream1, State.ACTIVE, null, executor).get();\n+\n+        assertNotEquals(0, consumer.getCurrentSegments(SCOPE, stream1).get().size());\n+        WriterMock requestEventWriter = new WriterMock(streamMetadataTasks, executor);\n+        streamMetadataTasks.setRequestEventWriter(requestEventWriter);\n+        streamMetadataTasks.setRetentionFrequencyMillis(1L);\n+        // region case 1: basic retention\n+        // add subscriber 1\n+        // add subscriber 2\n+        String subscriber1 = \"subscriber1\";\n+        streamMetadataTasks.addSubscriber(SCOPE, stream1, subscriber1, null).join();\n+\n+        String subscriber2 = \"subscriber2\";\n+        streamMetadataTasks.addSubscriber(SCOPE, stream1, subscriber2, null).join();\n+        \n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber1, ImmutableMap.of(0L, 2L, 1L, 1L), null).join();\n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber2, ImmutableMap.of(0L, 1L, 1L, 2L), null).join();\n+\n+        Map<Long, Long> map1 = new HashMap<>();\n+        map1.put(0L, 2L);\n+        map1.put(1L, 2L);\n+        long size = streamStorePartialMock.getSizeTillStreamCut(SCOPE, stream1, map1, Optional.empty(), null, executor).join();\n+        doReturn(CompletableFuture.completedFuture(new StreamCutRecord(1L, size, ImmutableMap.copyOf(map1))))\n+                .when(streamMetadataTasks).generateStreamCut(anyString(), anyString(), any(), any(), any());\n+\n+        // call retention and verify that retention policy applies\n+        streamMetadataTasks.retention(SCOPE, stream1, retentionPolicy, 1L, null, \"\").join();\n+        // now retention set has one stream cut 0/2, 1/2\n+        // subscriber lowerbound is 0/1, 1/1.. trucation should happen at lowerbound\n+\n+        VersionedMetadata<StreamTruncationRecord> truncationRecord = streamStorePartialMock.getTruncationRecord(SCOPE, stream1, null, executor).join();\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(0L).longValue(), 1L);\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(1L).longValue(), 1L);\n+        assertTrue(truncationRecord.getObject().isUpdating());\n+        streamStorePartialMock.completeTruncation(SCOPE, stream1, truncationRecord, null, executor).join();\n+        // endregion\n+        \n+        // region case 2 min policy check\n+        // we will update the new streamcut to 0/10, 1/10\n+        map1.put(0L, 2L);\n+        map1.put(1L, 2L);\n+        size = streamStorePartialMock.getSizeTillStreamCut(SCOPE, stream1, map1, Optional.empty(), null, executor).join();\n+        doReturn(CompletableFuture.completedFuture(new StreamCutRecord(20L, size, ImmutableMap.copyOf(map1))))\n+                .when(streamMetadataTasks).generateStreamCut(anyString(), anyString(), any(), any(), any());\n+\n+        // update both readers to make sure they have read till the latest position. we have set the min limit to 2.  \n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber1, ImmutableMap.of(0L, 2L, 1L, 2L), null).join();\n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber2, ImmutableMap.of(0L, 2L, 1L, 2L), null).join();\n+\n+        // no new truncation should happen. \n+        // verify that truncation record has not changed. \n+        streamMetadataTasks.retention(SCOPE, stream1, retentionPolicy, 20L, null, \"\").join();\n+        // now retention set has two stream cut 0/2, 1/2...0/2, 1/2\n+        // subscriber lowerbound is 0/2, 1/2.. does not meet min bound criteria. we also do not have a max that satisfies the limit. no truncation should happen. \n+        // no change:\n+        truncationRecord = streamStorePartialMock.getTruncationRecord(SCOPE, stream1, null, executor).join();\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(0L).longValue(), 1L);\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(1L).longValue(), 1L);\n+        assertFalse(truncationRecord.getObject().isUpdating());\n+        // endregion\n+        \n+        // region case 3: min criteria not met on lower bound. truncate at max. \n+        map1.put(0L, 10L);\n+        map1.put(1L, 10L);\n+        size = streamStorePartialMock.getSizeTillStreamCut(SCOPE, stream1, map1, Optional.empty(), null, executor).join();\n+        doReturn(CompletableFuture.completedFuture(new StreamCutRecord(30L, size, ImmutableMap.copyOf(map1))))\n+                .when(streamMetadataTasks).generateStreamCut(anyString(), anyString(), any(), any(), any());\n+\n+        // update both readers to make sure they have read till the latest position - 1. we have set the min limit to 2.  \n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber1, ImmutableMap.of(0L, 10L, 1L, 9L), null).join();\n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber2, ImmutableMap.of(0L, 10L, 1L, 9L), null).join();\n+\n+        streamMetadataTasks.retention(SCOPE, stream1, retentionPolicy, 30L, null, \"\").join();\n+        // now retention set has three stream cut 0/2, 1/2...0/2, 1/2... 0/10, 1/10\n+        // subscriber lowerbound is 0/10, 1/9.. does not meet min bound criteria. but we have max bound on truncation record\n+        // truncation should happen at 0/2, 1/2\n+        truncationRecord = streamStorePartialMock.getTruncationRecord(SCOPE, stream1, null, executor).join();\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(0L).longValue(), 2L);\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(1L).longValue(), 2L);\n+        assertTrue(truncationRecord.getObject().isUpdating());\n+        streamStorePartialMock.completeTruncation(SCOPE, stream1, truncationRecord, null, executor).join();\n+        // endregion\n+        \n+        // region case 4: lowerbound behind max\n+        // now move the stream further ahead so that max truncation limit is crossed but lowerbound is behind max. \n+        map1.put(0L, 20L);\n+        map1.put(1L, 20L);\n+        size = streamStorePartialMock.getSizeTillStreamCut(SCOPE, stream1, map1, Optional.empty(), null, executor).join();\n+        doReturn(CompletableFuture.completedFuture(new StreamCutRecord(40L, size, ImmutableMap.copyOf(map1))))\n+                .when(streamMetadataTasks).generateStreamCut(anyString(), anyString(), any(), any(), any());\n+        \n+        streamMetadataTasks.retention(SCOPE, stream1, retentionPolicy, 40L, null, \"\").join();\n+        // now retention set has three stream cut 0/2, 1/2...0/2, 1/2... 0/10, 1/10.. 0/20, 1/20\n+        // subscriber lowerbound is 0/10, 1/9.. meets min bound criteria. but we have max bound on truncation record\n+        // truncation should happen at 0/10, 1/10\n+        truncationRecord = streamStorePartialMock.getTruncationRecord(SCOPE, stream1, null, executor).join();\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(0L).longValue(), 10L);\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(1L).longValue(), 10L);\n+        assertTrue(truncationRecord.getObject().isUpdating());\n+        streamStorePartialMock.completeTruncation(SCOPE, stream1, truncationRecord, null, executor).join();\n+        // endregion\n+\n+        // region case 5: lowerbound overlaps with max\n+        map1.put(0L, 30L);\n+        map1.put(1L, 30L);\n+        size = streamStorePartialMock.getSizeTillStreamCut(SCOPE, stream1, map1, Optional.empty(), null, executor).join();\n+        doReturn(CompletableFuture.completedFuture(new StreamCutRecord(50L, size, ImmutableMap.copyOf(map1))))\n+                .when(streamMetadataTasks).generateStreamCut(anyString(), anyString(), any(), any(), any());\n+\n+        // update both readers to make sure they have read till the latest position - 1. we have set the min limit to 2.  \n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber1, ImmutableMap.of(0L, 21L, 1L, 19L), null).join();\n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber2, ImmutableMap.of(0L, 21L, 1L, 19L), null).join();\n+\n+        streamMetadataTasks.retention(SCOPE, stream1, retentionPolicy, 50L, null, \"\").join();\n+        // now retention set has three stream cut 0/2, 1/2...0/2, 1/2... 0/10, 1/10.. 0/20, 1/20.. 0/30, 1/30\n+        // subscriber lowerbound is 0/21, 1/19.. meets min bound criteria. and its also greater than max bound. but it overlaps with max bound. \n+        // truncation should happen at 0/21, 1/19\n+        truncationRecord = streamStorePartialMock.getTruncationRecord(SCOPE, stream1, null, executor).join();\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(0L).longValue(), 21L);\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(1L).longValue(), 19L);\n+        assertTrue(truncationRecord.getObject().isUpdating());\n+        streamStorePartialMock.completeTruncation(SCOPE, stream1, truncationRecord, null, executor).join();\n+        // endregion\n+    }\n+    \n+    @Test(timeout = 30000)\n+    public void consumptionBasedRetentionTimeLimitTest() throws Exception {\n+        final ScalingPolicy policy = ScalingPolicy.fixed(2);\n+        final RetentionPolicy retentionPolicy = RetentionPolicy.byConsumption(RetentionPolicy.ConsumptionLimits.Type.TIME_MILLIS, 1L, 10L);\n+\n+        String stream1 = \"consumptionTime\";\n+        final StreamConfiguration configuration = StreamConfiguration.builder().scalingPolicy(policy)\n+                .retentionPolicy(retentionPolicy).build();\n+\n+        streamStorePartialMock.createStream(SCOPE, stream1, configuration, System.currentTimeMillis(), null, executor).get();\n+        streamStorePartialMock.setState(SCOPE, stream1, State.ACTIVE, null, executor).get();\n+\n+        assertNotEquals(0, consumer.getCurrentSegments(SCOPE, stream1).get().size());\n+        WriterMock requestEventWriter = new WriterMock(streamMetadataTasks, executor);\n+        streamMetadataTasks.setRequestEventWriter(requestEventWriter);\n+        streamMetadataTasks.setRetentionFrequencyMillis(1L);\n+        AtomicLong time = new AtomicLong(0L);\n+        streamMetadataTasks.setRetentionClock(time::get);\n+        // region case 1: basic retention\n+        // add subscriber 1\n+        // add subscriber 2\n+        String subscriber1 = \"subscriber1\";\n+        streamMetadataTasks.addSubscriber(SCOPE, stream1, subscriber1, null).join();\n+\n+        String subscriber2 = \"subscriber2\";\n+        streamMetadataTasks.addSubscriber(SCOPE, stream1, subscriber2, null).join();\n+        \n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber1, ImmutableMap.of(0L, 2L, 1L, 1L), null).join();\n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber2, ImmutableMap.of(0L, 1L, 1L, 2L), null).join();\n+\n+        Map<Long, Long> map1 = new HashMap<>();\n+        map1.put(0L, 2L);\n+        map1.put(1L, 2L);\n+        long size = streamStorePartialMock.getSizeTillStreamCut(SCOPE, stream1, map1, Optional.empty(), null, executor).join();\n+        doReturn(CompletableFuture.completedFuture(new StreamCutRecord(time.get(), size, ImmutableMap.copyOf(map1))))\n+                .when(streamMetadataTasks).generateStreamCut(anyString(), anyString(), any(), any(), any());\n+\n+        // call retention and verify that retention policy applies\n+        streamMetadataTasks.retention(SCOPE, stream1, retentionPolicy, time.get(), null, \"\").join();\n+        // now retention set has one stream cut 0/2, 1/2, recording time 1L\n+        // subscriber lowerbound is 0/1, 1/1.. trucation should not happen as this lowerbound is ahead of min retention streamcut.\n+        VersionedMetadata<StreamTruncationRecord> truncationRecord = streamStorePartialMock.getTruncationRecord(SCOPE, stream1, null, executor).join();\n+        assertFalse(truncationRecord.getObject().isUpdating());\n+        // endregion\n+        \n+        // region case 2 min policy check\n+        // subscriber streamcut > min time streamcut while\n+        streamStorePartialMock.addStreamCutToRetentionSet(SCOPE, stream1,\n+                new StreamCutRecord(2L, 4L, ImmutableMap.of(0L, 2L, 1L, 2L)), null, executor).join();\n+\n+        time.set(10L);\n+        streamStorePartialMock.addStreamCutToRetentionSet(SCOPE, stream1,\n+                new StreamCutRecord(time.get(), 20L, ImmutableMap.of(0L, 10L, 1L, 10L)), null, executor).join();\n+\n+        time.set(11L);\n+        streamStorePartialMock.addStreamCutToRetentionSet(SCOPE, stream1,\n+                new StreamCutRecord(time.get(), 20L, ImmutableMap.of(0L, 10L, 1L, 10L)), null, executor).join();\n+\n+        // retentionset: 0L: 0L/2L, 1L/2L... 2L: 0L/2L, 1L/2L... 10L: 0/10, 1/10....11L: 0/10, 1/10. \n+        // update both readers to 0/3, 1/3.  \n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber1, ImmutableMap.of(0L, 3L, 1L, 3L), null).join();\n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber2, ImmutableMap.of(0L, 3L, 1L, 3L), null).join();\n+\n+        // new truncation should happen at subscriber lowerbound.\n+        streamMetadataTasks.retention(SCOPE, stream1, retentionPolicy, time.get(), null, \"\").join();\n+\n+        truncationRecord = streamStorePartialMock.getTruncationRecord(SCOPE, stream1, null, executor).join();\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(0L).longValue(), 3L);\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(1L).longValue(), 3L);\n+        assertTrue(truncationRecord.getObject().isUpdating());\n+        streamStorePartialMock.completeTruncation(SCOPE, stream1, truncationRecord, null, executor).join();\n+        // endregion\n+        \n+        // region case 3: min criteria not met on lower bound. truncate at max.\n+        time.set(20L);\n+        streamStorePartialMock.addStreamCutToRetentionSet(SCOPE, stream1,\n+                new StreamCutRecord(time.get(), 22L, ImmutableMap.of(0L, 11L, 1L, 11L)), null, executor).join();\n+\n+        // update both readers to make sure they have read till the latest position - 1. we have set the min limit to 2.  \n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber1, ImmutableMap.of(0L, 11L, 1L, 11L), null).join();\n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber2, ImmutableMap.of(0L, 11L, 1L, 11L), null).join();\n+\n+        streamMetadataTasks.retention(SCOPE, stream1, retentionPolicy, time.get(), null, \"\").join();\n+        // retentionset: 0L: 0L/2L, 1L/2L... 2L: 0L/2L, 1L/2L... 10L: 0/10, 1/10....11L: 0/10, 1/10... 20: 0/11, 1/11\n+        // subscriber lowerbound is 0/11, 1/11 \n+        truncationRecord = streamStorePartialMock.getTruncationRecord(SCOPE, stream1, null, executor).join();\n+        // truncate at limit min\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(0L).longValue(), 10L);\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(1L).longValue(), 10L);\n+        assertTrue(truncationRecord.getObject().isUpdating());\n+        streamStorePartialMock.completeTruncation(SCOPE, stream1, truncationRecord, null, executor).join();\n+        // endregion\n+        \n+        // region case 4: lowerbound behind max\n+        streamStorePartialMock.addStreamCutToRetentionSet(SCOPE, stream1,\n+                new StreamCutRecord(30L, 40L, ImmutableMap.of(0L, 20L, 1L, 20L)), null, executor).join();\n+        time.set(40L);\n+        streamStorePartialMock.addStreamCutToRetentionSet(SCOPE, stream1,\n+                new StreamCutRecord(time.get(), 42L, ImmutableMap.of(0L, 21L, 1L, 21L)), null, executor).join();\n+\n+        // update both readers to make sure they have read till the latest position - 1. we have set the min limit to 2.  \n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber1, ImmutableMap.of(0L, 11L, 1L, 11L), null).join();\n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber2, ImmutableMap.of(0L, 11L, 1L, 11L), null).join();\n+\n+        streamMetadataTasks.retention(SCOPE, stream1, retentionPolicy, time.get(), null, \"\").join();\n+        // now retention set has five stream cuts 1: 0/2, 1/2...10: 0/10, 1/10... 20: 0/11, 1/11.. 30: 0/20, 1/20.. 40L: 0/21, 1/21\n+        // subscriber lowerbound is 0/11, 1/11 \n+        // max = 30. truncate at max\n+        truncationRecord = streamStorePartialMock.getTruncationRecord(SCOPE, stream1, null, executor).join();\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(0L).longValue(), 20L);\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(1L).longValue(), 20L);\n+        assertTrue(truncationRecord.getObject().isUpdating());\n+        streamStorePartialMock.completeTruncation(SCOPE, stream1, truncationRecord, null, executor).join();\n+        // endregion\n+\n+        // region case 5: lowerbound overlaps with max\n+        streamStorePartialMock.addStreamCutToRetentionSet(SCOPE, stream1,\n+                new StreamCutRecord(50L, 43L, ImmutableMap.of(0L, 21L, 1L, 22L)), null, executor).join();\n+        time.set(59L);\n+        streamStorePartialMock.addStreamCutToRetentionSet(SCOPE, stream1,\n+                new StreamCutRecord(time.get(), 60L, ImmutableMap.of(0L, 30L, 1L, 30L)), null, executor).join();\n+        time.set(60L);\n+        streamStorePartialMock.addStreamCutToRetentionSet(SCOPE, stream1,\n+                new StreamCutRecord(time.get(), 60L, ImmutableMap.of(0L, 30L, 1L, 30L)), null, executor).join();\n+\n+        // update both readers to make sure they have read till the latest position - 1. we have set the min limit to 2.  \n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber1, ImmutableMap.of(0L, 22L, 1L, 21L), null).join();\n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber2, ImmutableMap.of(0L, 22L, 1L, 21L), null).join();\n+\n+        streamMetadataTasks.retention(SCOPE, stream1, retentionPolicy, time.get(), null, \"\").join();\n+        // now retention set has five stream cuts 1: 0/2, 1/2...10: 0/10, 1/10... 20: 0/11, 1/11.. 30: 0/20, 1/20.. 40L: 0/21, 1/21\n+        // 50: 0/21, 1/22 ... 59: 0/30, 1/30.. 60: 0/30, 1/30\n+        // subscriber lowerbound is 0/22, 1/21 \n+        // this overlaps with max. so truncate at streamcut\n+        truncationRecord = streamStorePartialMock.getTruncationRecord(SCOPE, stream1, null, executor).join();\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(0L).longValue(), 22L);\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(1L).longValue(), 21L);\n+        assertTrue(truncationRecord.getObject().isUpdating());\n+        streamStorePartialMock.completeTruncation(SCOPE, stream1, truncationRecord, null, executor).join();\n+        // endregion\n+    }\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bd7badf9bd8d1dd457baf8014f4716b0b468f61c"}, "originalPosition": 289}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjgyODI2OQ==", "bodyText": "is handled via the CBR policy constructor itself. if nothing is specified the max is long.max and min is defaulted to 0.\n\n\nand 3.\nif one or more subscribers have not updated their streamcut then they are effectively at the mercy of \"min\".\nshould we wait for all subscribers to have reported at least one streamcut? if that is preferred i will make the change.", "url": "https://github.com/pravega/pravega/pull/5289#discussion_r522828269", "createdAt": "2020-11-13T09:27:36Z", "author": {"login": "shiveshr"}, "path": "controller/src/test/java/io/pravega/controller/task/Stream/StreamMetadataTasksTest.java", "diffHunk": "@@ -1020,6 +1021,446 @@ public void sizeBasedRetentionStreamTest() throws Exception {\n         // endregion\n         // endregion\n     }\n+    \n+    @Test(timeout = 30000)\n+    public void consumptionBasedRetentionSizeLimitTest() throws Exception {\n+        final ScalingPolicy policy = ScalingPolicy.fixed(2);\n+        final RetentionPolicy retentionPolicy = RetentionPolicy.byConsumption(RetentionPolicy.ConsumptionLimits.Type.SIZE_BYTES, 2L, 10L);\n+\n+        String stream1 = \"consumptionSize\";\n+        final StreamConfiguration configuration = StreamConfiguration.builder().scalingPolicy(policy)\n+                .retentionPolicy(retentionPolicy).build();\n+\n+        streamStorePartialMock.createStream(SCOPE, stream1, configuration, System.currentTimeMillis(), null, executor).get();\n+        streamStorePartialMock.setState(SCOPE, stream1, State.ACTIVE, null, executor).get();\n+\n+        assertNotEquals(0, consumer.getCurrentSegments(SCOPE, stream1).get().size());\n+        WriterMock requestEventWriter = new WriterMock(streamMetadataTasks, executor);\n+        streamMetadataTasks.setRequestEventWriter(requestEventWriter);\n+        streamMetadataTasks.setRetentionFrequencyMillis(1L);\n+        // region case 1: basic retention\n+        // add subscriber 1\n+        // add subscriber 2\n+        String subscriber1 = \"subscriber1\";\n+        streamMetadataTasks.addSubscriber(SCOPE, stream1, subscriber1, null).join();\n+\n+        String subscriber2 = \"subscriber2\";\n+        streamMetadataTasks.addSubscriber(SCOPE, stream1, subscriber2, null).join();\n+        \n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber1, ImmutableMap.of(0L, 2L, 1L, 1L), null).join();\n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber2, ImmutableMap.of(0L, 1L, 1L, 2L), null).join();\n+\n+        Map<Long, Long> map1 = new HashMap<>();\n+        map1.put(0L, 2L);\n+        map1.put(1L, 2L);\n+        long size = streamStorePartialMock.getSizeTillStreamCut(SCOPE, stream1, map1, Optional.empty(), null, executor).join();\n+        doReturn(CompletableFuture.completedFuture(new StreamCutRecord(1L, size, ImmutableMap.copyOf(map1))))\n+                .when(streamMetadataTasks).generateStreamCut(anyString(), anyString(), any(), any(), any());\n+\n+        // call retention and verify that retention policy applies\n+        streamMetadataTasks.retention(SCOPE, stream1, retentionPolicy, 1L, null, \"\").join();\n+        // now retention set has one stream cut 0/2, 1/2\n+        // subscriber lowerbound is 0/1, 1/1.. trucation should happen at lowerbound\n+\n+        VersionedMetadata<StreamTruncationRecord> truncationRecord = streamStorePartialMock.getTruncationRecord(SCOPE, stream1, null, executor).join();\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(0L).longValue(), 1L);\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(1L).longValue(), 1L);\n+        assertTrue(truncationRecord.getObject().isUpdating());\n+        streamStorePartialMock.completeTruncation(SCOPE, stream1, truncationRecord, null, executor).join();\n+        // endregion\n+        \n+        // region case 2 min policy check\n+        // we will update the new streamcut to 0/10, 1/10\n+        map1.put(0L, 2L);\n+        map1.put(1L, 2L);\n+        size = streamStorePartialMock.getSizeTillStreamCut(SCOPE, stream1, map1, Optional.empty(), null, executor).join();\n+        doReturn(CompletableFuture.completedFuture(new StreamCutRecord(20L, size, ImmutableMap.copyOf(map1))))\n+                .when(streamMetadataTasks).generateStreamCut(anyString(), anyString(), any(), any(), any());\n+\n+        // update both readers to make sure they have read till the latest position. we have set the min limit to 2.  \n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber1, ImmutableMap.of(0L, 2L, 1L, 2L), null).join();\n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber2, ImmutableMap.of(0L, 2L, 1L, 2L), null).join();\n+\n+        // no new truncation should happen. \n+        // verify that truncation record has not changed. \n+        streamMetadataTasks.retention(SCOPE, stream1, retentionPolicy, 20L, null, \"\").join();\n+        // now retention set has two stream cut 0/2, 1/2...0/2, 1/2\n+        // subscriber lowerbound is 0/2, 1/2.. does not meet min bound criteria. we also do not have a max that satisfies the limit. no truncation should happen. \n+        // no change:\n+        truncationRecord = streamStorePartialMock.getTruncationRecord(SCOPE, stream1, null, executor).join();\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(0L).longValue(), 1L);\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(1L).longValue(), 1L);\n+        assertFalse(truncationRecord.getObject().isUpdating());\n+        // endregion\n+        \n+        // region case 3: min criteria not met on lower bound. truncate at max. \n+        map1.put(0L, 10L);\n+        map1.put(1L, 10L);\n+        size = streamStorePartialMock.getSizeTillStreamCut(SCOPE, stream1, map1, Optional.empty(), null, executor).join();\n+        doReturn(CompletableFuture.completedFuture(new StreamCutRecord(30L, size, ImmutableMap.copyOf(map1))))\n+                .when(streamMetadataTasks).generateStreamCut(anyString(), anyString(), any(), any(), any());\n+\n+        // update both readers to make sure they have read till the latest position - 1. we have set the min limit to 2.  \n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber1, ImmutableMap.of(0L, 10L, 1L, 9L), null).join();\n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber2, ImmutableMap.of(0L, 10L, 1L, 9L), null).join();\n+\n+        streamMetadataTasks.retention(SCOPE, stream1, retentionPolicy, 30L, null, \"\").join();\n+        // now retention set has three stream cut 0/2, 1/2...0/2, 1/2... 0/10, 1/10\n+        // subscriber lowerbound is 0/10, 1/9.. does not meet min bound criteria. but we have max bound on truncation record\n+        // truncation should happen at 0/2, 1/2\n+        truncationRecord = streamStorePartialMock.getTruncationRecord(SCOPE, stream1, null, executor).join();\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(0L).longValue(), 2L);\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(1L).longValue(), 2L);\n+        assertTrue(truncationRecord.getObject().isUpdating());\n+        streamStorePartialMock.completeTruncation(SCOPE, stream1, truncationRecord, null, executor).join();\n+        // endregion\n+        \n+        // region case 4: lowerbound behind max\n+        // now move the stream further ahead so that max truncation limit is crossed but lowerbound is behind max. \n+        map1.put(0L, 20L);\n+        map1.put(1L, 20L);\n+        size = streamStorePartialMock.getSizeTillStreamCut(SCOPE, stream1, map1, Optional.empty(), null, executor).join();\n+        doReturn(CompletableFuture.completedFuture(new StreamCutRecord(40L, size, ImmutableMap.copyOf(map1))))\n+                .when(streamMetadataTasks).generateStreamCut(anyString(), anyString(), any(), any(), any());\n+        \n+        streamMetadataTasks.retention(SCOPE, stream1, retentionPolicy, 40L, null, \"\").join();\n+        // now retention set has three stream cut 0/2, 1/2...0/2, 1/2... 0/10, 1/10.. 0/20, 1/20\n+        // subscriber lowerbound is 0/10, 1/9.. meets min bound criteria. but we have max bound on truncation record\n+        // truncation should happen at 0/10, 1/10\n+        truncationRecord = streamStorePartialMock.getTruncationRecord(SCOPE, stream1, null, executor).join();\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(0L).longValue(), 10L);\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(1L).longValue(), 10L);\n+        assertTrue(truncationRecord.getObject().isUpdating());\n+        streamStorePartialMock.completeTruncation(SCOPE, stream1, truncationRecord, null, executor).join();\n+        // endregion\n+\n+        // region case 5: lowerbound overlaps with max\n+        map1.put(0L, 30L);\n+        map1.put(1L, 30L);\n+        size = streamStorePartialMock.getSizeTillStreamCut(SCOPE, stream1, map1, Optional.empty(), null, executor).join();\n+        doReturn(CompletableFuture.completedFuture(new StreamCutRecord(50L, size, ImmutableMap.copyOf(map1))))\n+                .when(streamMetadataTasks).generateStreamCut(anyString(), anyString(), any(), any(), any());\n+\n+        // update both readers to make sure they have read till the latest position - 1. we have set the min limit to 2.  \n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber1, ImmutableMap.of(0L, 21L, 1L, 19L), null).join();\n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber2, ImmutableMap.of(0L, 21L, 1L, 19L), null).join();\n+\n+        streamMetadataTasks.retention(SCOPE, stream1, retentionPolicy, 50L, null, \"\").join();\n+        // now retention set has three stream cut 0/2, 1/2...0/2, 1/2... 0/10, 1/10.. 0/20, 1/20.. 0/30, 1/30\n+        // subscriber lowerbound is 0/21, 1/19.. meets min bound criteria. and its also greater than max bound. but it overlaps with max bound. \n+        // truncation should happen at 0/21, 1/19\n+        truncationRecord = streamStorePartialMock.getTruncationRecord(SCOPE, stream1, null, executor).join();\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(0L).longValue(), 21L);\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(1L).longValue(), 19L);\n+        assertTrue(truncationRecord.getObject().isUpdating());\n+        streamStorePartialMock.completeTruncation(SCOPE, stream1, truncationRecord, null, executor).join();\n+        // endregion\n+    }\n+    \n+    @Test(timeout = 30000)\n+    public void consumptionBasedRetentionTimeLimitTest() throws Exception {\n+        final ScalingPolicy policy = ScalingPolicy.fixed(2);\n+        final RetentionPolicy retentionPolicy = RetentionPolicy.byConsumption(RetentionPolicy.ConsumptionLimits.Type.TIME_MILLIS, 1L, 10L);\n+\n+        String stream1 = \"consumptionTime\";\n+        final StreamConfiguration configuration = StreamConfiguration.builder().scalingPolicy(policy)\n+                .retentionPolicy(retentionPolicy).build();\n+\n+        streamStorePartialMock.createStream(SCOPE, stream1, configuration, System.currentTimeMillis(), null, executor).get();\n+        streamStorePartialMock.setState(SCOPE, stream1, State.ACTIVE, null, executor).get();\n+\n+        assertNotEquals(0, consumer.getCurrentSegments(SCOPE, stream1).get().size());\n+        WriterMock requestEventWriter = new WriterMock(streamMetadataTasks, executor);\n+        streamMetadataTasks.setRequestEventWriter(requestEventWriter);\n+        streamMetadataTasks.setRetentionFrequencyMillis(1L);\n+        AtomicLong time = new AtomicLong(0L);\n+        streamMetadataTasks.setRetentionClock(time::get);\n+        // region case 1: basic retention\n+        // add subscriber 1\n+        // add subscriber 2\n+        String subscriber1 = \"subscriber1\";\n+        streamMetadataTasks.addSubscriber(SCOPE, stream1, subscriber1, null).join();\n+\n+        String subscriber2 = \"subscriber2\";\n+        streamMetadataTasks.addSubscriber(SCOPE, stream1, subscriber2, null).join();\n+        \n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber1, ImmutableMap.of(0L, 2L, 1L, 1L), null).join();\n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber2, ImmutableMap.of(0L, 1L, 1L, 2L), null).join();\n+\n+        Map<Long, Long> map1 = new HashMap<>();\n+        map1.put(0L, 2L);\n+        map1.put(1L, 2L);\n+        long size = streamStorePartialMock.getSizeTillStreamCut(SCOPE, stream1, map1, Optional.empty(), null, executor).join();\n+        doReturn(CompletableFuture.completedFuture(new StreamCutRecord(time.get(), size, ImmutableMap.copyOf(map1))))\n+                .when(streamMetadataTasks).generateStreamCut(anyString(), anyString(), any(), any(), any());\n+\n+        // call retention and verify that retention policy applies\n+        streamMetadataTasks.retention(SCOPE, stream1, retentionPolicy, time.get(), null, \"\").join();\n+        // now retention set has one stream cut 0/2, 1/2, recording time 1L\n+        // subscriber lowerbound is 0/1, 1/1.. trucation should not happen as this lowerbound is ahead of min retention streamcut.\n+        VersionedMetadata<StreamTruncationRecord> truncationRecord = streamStorePartialMock.getTruncationRecord(SCOPE, stream1, null, executor).join();\n+        assertFalse(truncationRecord.getObject().isUpdating());\n+        // endregion\n+        \n+        // region case 2 min policy check\n+        // subscriber streamcut > min time streamcut while\n+        streamStorePartialMock.addStreamCutToRetentionSet(SCOPE, stream1,\n+                new StreamCutRecord(2L, 4L, ImmutableMap.of(0L, 2L, 1L, 2L)), null, executor).join();\n+\n+        time.set(10L);\n+        streamStorePartialMock.addStreamCutToRetentionSet(SCOPE, stream1,\n+                new StreamCutRecord(time.get(), 20L, ImmutableMap.of(0L, 10L, 1L, 10L)), null, executor).join();\n+\n+        time.set(11L);\n+        streamStorePartialMock.addStreamCutToRetentionSet(SCOPE, stream1,\n+                new StreamCutRecord(time.get(), 20L, ImmutableMap.of(0L, 10L, 1L, 10L)), null, executor).join();\n+\n+        // retentionset: 0L: 0L/2L, 1L/2L... 2L: 0L/2L, 1L/2L... 10L: 0/10, 1/10....11L: 0/10, 1/10. \n+        // update both readers to 0/3, 1/3.  \n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber1, ImmutableMap.of(0L, 3L, 1L, 3L), null).join();\n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber2, ImmutableMap.of(0L, 3L, 1L, 3L), null).join();\n+\n+        // new truncation should happen at subscriber lowerbound.\n+        streamMetadataTasks.retention(SCOPE, stream1, retentionPolicy, time.get(), null, \"\").join();\n+\n+        truncationRecord = streamStorePartialMock.getTruncationRecord(SCOPE, stream1, null, executor).join();\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(0L).longValue(), 3L);\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(1L).longValue(), 3L);\n+        assertTrue(truncationRecord.getObject().isUpdating());\n+        streamStorePartialMock.completeTruncation(SCOPE, stream1, truncationRecord, null, executor).join();\n+        // endregion\n+        \n+        // region case 3: min criteria not met on lower bound. truncate at max.\n+        time.set(20L);\n+        streamStorePartialMock.addStreamCutToRetentionSet(SCOPE, stream1,\n+                new StreamCutRecord(time.get(), 22L, ImmutableMap.of(0L, 11L, 1L, 11L)), null, executor).join();\n+\n+        // update both readers to make sure they have read till the latest position - 1. we have set the min limit to 2.  \n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber1, ImmutableMap.of(0L, 11L, 1L, 11L), null).join();\n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber2, ImmutableMap.of(0L, 11L, 1L, 11L), null).join();\n+\n+        streamMetadataTasks.retention(SCOPE, stream1, retentionPolicy, time.get(), null, \"\").join();\n+        // retentionset: 0L: 0L/2L, 1L/2L... 2L: 0L/2L, 1L/2L... 10L: 0/10, 1/10....11L: 0/10, 1/10... 20: 0/11, 1/11\n+        // subscriber lowerbound is 0/11, 1/11 \n+        truncationRecord = streamStorePartialMock.getTruncationRecord(SCOPE, stream1, null, executor).join();\n+        // truncate at limit min\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(0L).longValue(), 10L);\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(1L).longValue(), 10L);\n+        assertTrue(truncationRecord.getObject().isUpdating());\n+        streamStorePartialMock.completeTruncation(SCOPE, stream1, truncationRecord, null, executor).join();\n+        // endregion\n+        \n+        // region case 4: lowerbound behind max\n+        streamStorePartialMock.addStreamCutToRetentionSet(SCOPE, stream1,\n+                new StreamCutRecord(30L, 40L, ImmutableMap.of(0L, 20L, 1L, 20L)), null, executor).join();\n+        time.set(40L);\n+        streamStorePartialMock.addStreamCutToRetentionSet(SCOPE, stream1,\n+                new StreamCutRecord(time.get(), 42L, ImmutableMap.of(0L, 21L, 1L, 21L)), null, executor).join();\n+\n+        // update both readers to make sure they have read till the latest position - 1. we have set the min limit to 2.  \n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber1, ImmutableMap.of(0L, 11L, 1L, 11L), null).join();\n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber2, ImmutableMap.of(0L, 11L, 1L, 11L), null).join();\n+\n+        streamMetadataTasks.retention(SCOPE, stream1, retentionPolicy, time.get(), null, \"\").join();\n+        // now retention set has five stream cuts 1: 0/2, 1/2...10: 0/10, 1/10... 20: 0/11, 1/11.. 30: 0/20, 1/20.. 40L: 0/21, 1/21\n+        // subscriber lowerbound is 0/11, 1/11 \n+        // max = 30. truncate at max\n+        truncationRecord = streamStorePartialMock.getTruncationRecord(SCOPE, stream1, null, executor).join();\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(0L).longValue(), 20L);\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(1L).longValue(), 20L);\n+        assertTrue(truncationRecord.getObject().isUpdating());\n+        streamStorePartialMock.completeTruncation(SCOPE, stream1, truncationRecord, null, executor).join();\n+        // endregion\n+\n+        // region case 5: lowerbound overlaps with max\n+        streamStorePartialMock.addStreamCutToRetentionSet(SCOPE, stream1,\n+                new StreamCutRecord(50L, 43L, ImmutableMap.of(0L, 21L, 1L, 22L)), null, executor).join();\n+        time.set(59L);\n+        streamStorePartialMock.addStreamCutToRetentionSet(SCOPE, stream1,\n+                new StreamCutRecord(time.get(), 60L, ImmutableMap.of(0L, 30L, 1L, 30L)), null, executor).join();\n+        time.set(60L);\n+        streamStorePartialMock.addStreamCutToRetentionSet(SCOPE, stream1,\n+                new StreamCutRecord(time.get(), 60L, ImmutableMap.of(0L, 30L, 1L, 30L)), null, executor).join();\n+\n+        // update both readers to make sure they have read till the latest position - 1. we have set the min limit to 2.  \n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber1, ImmutableMap.of(0L, 22L, 1L, 21L), null).join();\n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber2, ImmutableMap.of(0L, 22L, 1L, 21L), null).join();\n+\n+        streamMetadataTasks.retention(SCOPE, stream1, retentionPolicy, time.get(), null, \"\").join();\n+        // now retention set has five stream cuts 1: 0/2, 1/2...10: 0/10, 1/10... 20: 0/11, 1/11.. 30: 0/20, 1/20.. 40L: 0/21, 1/21\n+        // 50: 0/21, 1/22 ... 59: 0/30, 1/30.. 60: 0/30, 1/30\n+        // subscriber lowerbound is 0/22, 1/21 \n+        // this overlaps with max. so truncate at streamcut\n+        truncationRecord = streamStorePartialMock.getTruncationRecord(SCOPE, stream1, null, executor).join();\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(0L).longValue(), 22L);\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(1L).longValue(), 21L);\n+        assertTrue(truncationRecord.getObject().isUpdating());\n+        streamStorePartialMock.completeTruncation(SCOPE, stream1, truncationRecord, null, executor).join();\n+        // endregion\n+    }\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjA3OTM3Mg=="}, "originalCommit": {"oid": "bd7badf9bd8d1dd457baf8014f4716b0b468f61c"}, "originalPosition": 289}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjgzNDkzMw==", "bodyText": "I understand that default max==LONG_MAX and default min==0, I'm suggesting having a test case that invokes CBR Stream truncation without these values being explicitly set, just to cover the scenario where customers may not set these limits. I know it should work, but having a test case to validate would be good.\n\n\nif one or more subscribers have not updated their streamcut then they are effectively at the mercy of \"min\".\nYes this is by design.", "url": "https://github.com/pravega/pravega/pull/5289#discussion_r522834933", "createdAt": "2020-11-13T09:40:04Z", "author": {"login": "pbelgundi"}, "path": "controller/src/test/java/io/pravega/controller/task/Stream/StreamMetadataTasksTest.java", "diffHunk": "@@ -1020,6 +1021,446 @@ public void sizeBasedRetentionStreamTest() throws Exception {\n         // endregion\n         // endregion\n     }\n+    \n+    @Test(timeout = 30000)\n+    public void consumptionBasedRetentionSizeLimitTest() throws Exception {\n+        final ScalingPolicy policy = ScalingPolicy.fixed(2);\n+        final RetentionPolicy retentionPolicy = RetentionPolicy.byConsumption(RetentionPolicy.ConsumptionLimits.Type.SIZE_BYTES, 2L, 10L);\n+\n+        String stream1 = \"consumptionSize\";\n+        final StreamConfiguration configuration = StreamConfiguration.builder().scalingPolicy(policy)\n+                .retentionPolicy(retentionPolicy).build();\n+\n+        streamStorePartialMock.createStream(SCOPE, stream1, configuration, System.currentTimeMillis(), null, executor).get();\n+        streamStorePartialMock.setState(SCOPE, stream1, State.ACTIVE, null, executor).get();\n+\n+        assertNotEquals(0, consumer.getCurrentSegments(SCOPE, stream1).get().size());\n+        WriterMock requestEventWriter = new WriterMock(streamMetadataTasks, executor);\n+        streamMetadataTasks.setRequestEventWriter(requestEventWriter);\n+        streamMetadataTasks.setRetentionFrequencyMillis(1L);\n+        // region case 1: basic retention\n+        // add subscriber 1\n+        // add subscriber 2\n+        String subscriber1 = \"subscriber1\";\n+        streamMetadataTasks.addSubscriber(SCOPE, stream1, subscriber1, null).join();\n+\n+        String subscriber2 = \"subscriber2\";\n+        streamMetadataTasks.addSubscriber(SCOPE, stream1, subscriber2, null).join();\n+        \n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber1, ImmutableMap.of(0L, 2L, 1L, 1L), null).join();\n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber2, ImmutableMap.of(0L, 1L, 1L, 2L), null).join();\n+\n+        Map<Long, Long> map1 = new HashMap<>();\n+        map1.put(0L, 2L);\n+        map1.put(1L, 2L);\n+        long size = streamStorePartialMock.getSizeTillStreamCut(SCOPE, stream1, map1, Optional.empty(), null, executor).join();\n+        doReturn(CompletableFuture.completedFuture(new StreamCutRecord(1L, size, ImmutableMap.copyOf(map1))))\n+                .when(streamMetadataTasks).generateStreamCut(anyString(), anyString(), any(), any(), any());\n+\n+        // call retention and verify that retention policy applies\n+        streamMetadataTasks.retention(SCOPE, stream1, retentionPolicy, 1L, null, \"\").join();\n+        // now retention set has one stream cut 0/2, 1/2\n+        // subscriber lowerbound is 0/1, 1/1.. trucation should happen at lowerbound\n+\n+        VersionedMetadata<StreamTruncationRecord> truncationRecord = streamStorePartialMock.getTruncationRecord(SCOPE, stream1, null, executor).join();\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(0L).longValue(), 1L);\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(1L).longValue(), 1L);\n+        assertTrue(truncationRecord.getObject().isUpdating());\n+        streamStorePartialMock.completeTruncation(SCOPE, stream1, truncationRecord, null, executor).join();\n+        // endregion\n+        \n+        // region case 2 min policy check\n+        // we will update the new streamcut to 0/10, 1/10\n+        map1.put(0L, 2L);\n+        map1.put(1L, 2L);\n+        size = streamStorePartialMock.getSizeTillStreamCut(SCOPE, stream1, map1, Optional.empty(), null, executor).join();\n+        doReturn(CompletableFuture.completedFuture(new StreamCutRecord(20L, size, ImmutableMap.copyOf(map1))))\n+                .when(streamMetadataTasks).generateStreamCut(anyString(), anyString(), any(), any(), any());\n+\n+        // update both readers to make sure they have read till the latest position. we have set the min limit to 2.  \n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber1, ImmutableMap.of(0L, 2L, 1L, 2L), null).join();\n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber2, ImmutableMap.of(0L, 2L, 1L, 2L), null).join();\n+\n+        // no new truncation should happen. \n+        // verify that truncation record has not changed. \n+        streamMetadataTasks.retention(SCOPE, stream1, retentionPolicy, 20L, null, \"\").join();\n+        // now retention set has two stream cut 0/2, 1/2...0/2, 1/2\n+        // subscriber lowerbound is 0/2, 1/2.. does not meet min bound criteria. we also do not have a max that satisfies the limit. no truncation should happen. \n+        // no change:\n+        truncationRecord = streamStorePartialMock.getTruncationRecord(SCOPE, stream1, null, executor).join();\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(0L).longValue(), 1L);\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(1L).longValue(), 1L);\n+        assertFalse(truncationRecord.getObject().isUpdating());\n+        // endregion\n+        \n+        // region case 3: min criteria not met on lower bound. truncate at max. \n+        map1.put(0L, 10L);\n+        map1.put(1L, 10L);\n+        size = streamStorePartialMock.getSizeTillStreamCut(SCOPE, stream1, map1, Optional.empty(), null, executor).join();\n+        doReturn(CompletableFuture.completedFuture(new StreamCutRecord(30L, size, ImmutableMap.copyOf(map1))))\n+                .when(streamMetadataTasks).generateStreamCut(anyString(), anyString(), any(), any(), any());\n+\n+        // update both readers to make sure they have read till the latest position - 1. we have set the min limit to 2.  \n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber1, ImmutableMap.of(0L, 10L, 1L, 9L), null).join();\n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber2, ImmutableMap.of(0L, 10L, 1L, 9L), null).join();\n+\n+        streamMetadataTasks.retention(SCOPE, stream1, retentionPolicy, 30L, null, \"\").join();\n+        // now retention set has three stream cut 0/2, 1/2...0/2, 1/2... 0/10, 1/10\n+        // subscriber lowerbound is 0/10, 1/9.. does not meet min bound criteria. but we have max bound on truncation record\n+        // truncation should happen at 0/2, 1/2\n+        truncationRecord = streamStorePartialMock.getTruncationRecord(SCOPE, stream1, null, executor).join();\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(0L).longValue(), 2L);\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(1L).longValue(), 2L);\n+        assertTrue(truncationRecord.getObject().isUpdating());\n+        streamStorePartialMock.completeTruncation(SCOPE, stream1, truncationRecord, null, executor).join();\n+        // endregion\n+        \n+        // region case 4: lowerbound behind max\n+        // now move the stream further ahead so that max truncation limit is crossed but lowerbound is behind max. \n+        map1.put(0L, 20L);\n+        map1.put(1L, 20L);\n+        size = streamStorePartialMock.getSizeTillStreamCut(SCOPE, stream1, map1, Optional.empty(), null, executor).join();\n+        doReturn(CompletableFuture.completedFuture(new StreamCutRecord(40L, size, ImmutableMap.copyOf(map1))))\n+                .when(streamMetadataTasks).generateStreamCut(anyString(), anyString(), any(), any(), any());\n+        \n+        streamMetadataTasks.retention(SCOPE, stream1, retentionPolicy, 40L, null, \"\").join();\n+        // now retention set has three stream cut 0/2, 1/2...0/2, 1/2... 0/10, 1/10.. 0/20, 1/20\n+        // subscriber lowerbound is 0/10, 1/9.. meets min bound criteria. but we have max bound on truncation record\n+        // truncation should happen at 0/10, 1/10\n+        truncationRecord = streamStorePartialMock.getTruncationRecord(SCOPE, stream1, null, executor).join();\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(0L).longValue(), 10L);\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(1L).longValue(), 10L);\n+        assertTrue(truncationRecord.getObject().isUpdating());\n+        streamStorePartialMock.completeTruncation(SCOPE, stream1, truncationRecord, null, executor).join();\n+        // endregion\n+\n+        // region case 5: lowerbound overlaps with max\n+        map1.put(0L, 30L);\n+        map1.put(1L, 30L);\n+        size = streamStorePartialMock.getSizeTillStreamCut(SCOPE, stream1, map1, Optional.empty(), null, executor).join();\n+        doReturn(CompletableFuture.completedFuture(new StreamCutRecord(50L, size, ImmutableMap.copyOf(map1))))\n+                .when(streamMetadataTasks).generateStreamCut(anyString(), anyString(), any(), any(), any());\n+\n+        // update both readers to make sure they have read till the latest position - 1. we have set the min limit to 2.  \n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber1, ImmutableMap.of(0L, 21L, 1L, 19L), null).join();\n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber2, ImmutableMap.of(0L, 21L, 1L, 19L), null).join();\n+\n+        streamMetadataTasks.retention(SCOPE, stream1, retentionPolicy, 50L, null, \"\").join();\n+        // now retention set has three stream cut 0/2, 1/2...0/2, 1/2... 0/10, 1/10.. 0/20, 1/20.. 0/30, 1/30\n+        // subscriber lowerbound is 0/21, 1/19.. meets min bound criteria. and its also greater than max bound. but it overlaps with max bound. \n+        // truncation should happen at 0/21, 1/19\n+        truncationRecord = streamStorePartialMock.getTruncationRecord(SCOPE, stream1, null, executor).join();\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(0L).longValue(), 21L);\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(1L).longValue(), 19L);\n+        assertTrue(truncationRecord.getObject().isUpdating());\n+        streamStorePartialMock.completeTruncation(SCOPE, stream1, truncationRecord, null, executor).join();\n+        // endregion\n+    }\n+    \n+    @Test(timeout = 30000)\n+    public void consumptionBasedRetentionTimeLimitTest() throws Exception {\n+        final ScalingPolicy policy = ScalingPolicy.fixed(2);\n+        final RetentionPolicy retentionPolicy = RetentionPolicy.byConsumption(RetentionPolicy.ConsumptionLimits.Type.TIME_MILLIS, 1L, 10L);\n+\n+        String stream1 = \"consumptionTime\";\n+        final StreamConfiguration configuration = StreamConfiguration.builder().scalingPolicy(policy)\n+                .retentionPolicy(retentionPolicy).build();\n+\n+        streamStorePartialMock.createStream(SCOPE, stream1, configuration, System.currentTimeMillis(), null, executor).get();\n+        streamStorePartialMock.setState(SCOPE, stream1, State.ACTIVE, null, executor).get();\n+\n+        assertNotEquals(0, consumer.getCurrentSegments(SCOPE, stream1).get().size());\n+        WriterMock requestEventWriter = new WriterMock(streamMetadataTasks, executor);\n+        streamMetadataTasks.setRequestEventWriter(requestEventWriter);\n+        streamMetadataTasks.setRetentionFrequencyMillis(1L);\n+        AtomicLong time = new AtomicLong(0L);\n+        streamMetadataTasks.setRetentionClock(time::get);\n+        // region case 1: basic retention\n+        // add subscriber 1\n+        // add subscriber 2\n+        String subscriber1 = \"subscriber1\";\n+        streamMetadataTasks.addSubscriber(SCOPE, stream1, subscriber1, null).join();\n+\n+        String subscriber2 = \"subscriber2\";\n+        streamMetadataTasks.addSubscriber(SCOPE, stream1, subscriber2, null).join();\n+        \n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber1, ImmutableMap.of(0L, 2L, 1L, 1L), null).join();\n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber2, ImmutableMap.of(0L, 1L, 1L, 2L), null).join();\n+\n+        Map<Long, Long> map1 = new HashMap<>();\n+        map1.put(0L, 2L);\n+        map1.put(1L, 2L);\n+        long size = streamStorePartialMock.getSizeTillStreamCut(SCOPE, stream1, map1, Optional.empty(), null, executor).join();\n+        doReturn(CompletableFuture.completedFuture(new StreamCutRecord(time.get(), size, ImmutableMap.copyOf(map1))))\n+                .when(streamMetadataTasks).generateStreamCut(anyString(), anyString(), any(), any(), any());\n+\n+        // call retention and verify that retention policy applies\n+        streamMetadataTasks.retention(SCOPE, stream1, retentionPolicy, time.get(), null, \"\").join();\n+        // now retention set has one stream cut 0/2, 1/2, recording time 1L\n+        // subscriber lowerbound is 0/1, 1/1.. trucation should not happen as this lowerbound is ahead of min retention streamcut.\n+        VersionedMetadata<StreamTruncationRecord> truncationRecord = streamStorePartialMock.getTruncationRecord(SCOPE, stream1, null, executor).join();\n+        assertFalse(truncationRecord.getObject().isUpdating());\n+        // endregion\n+        \n+        // region case 2 min policy check\n+        // subscriber streamcut > min time streamcut while\n+        streamStorePartialMock.addStreamCutToRetentionSet(SCOPE, stream1,\n+                new StreamCutRecord(2L, 4L, ImmutableMap.of(0L, 2L, 1L, 2L)), null, executor).join();\n+\n+        time.set(10L);\n+        streamStorePartialMock.addStreamCutToRetentionSet(SCOPE, stream1,\n+                new StreamCutRecord(time.get(), 20L, ImmutableMap.of(0L, 10L, 1L, 10L)), null, executor).join();\n+\n+        time.set(11L);\n+        streamStorePartialMock.addStreamCutToRetentionSet(SCOPE, stream1,\n+                new StreamCutRecord(time.get(), 20L, ImmutableMap.of(0L, 10L, 1L, 10L)), null, executor).join();\n+\n+        // retentionset: 0L: 0L/2L, 1L/2L... 2L: 0L/2L, 1L/2L... 10L: 0/10, 1/10....11L: 0/10, 1/10. \n+        // update both readers to 0/3, 1/3.  \n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber1, ImmutableMap.of(0L, 3L, 1L, 3L), null).join();\n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber2, ImmutableMap.of(0L, 3L, 1L, 3L), null).join();\n+\n+        // new truncation should happen at subscriber lowerbound.\n+        streamMetadataTasks.retention(SCOPE, stream1, retentionPolicy, time.get(), null, \"\").join();\n+\n+        truncationRecord = streamStorePartialMock.getTruncationRecord(SCOPE, stream1, null, executor).join();\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(0L).longValue(), 3L);\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(1L).longValue(), 3L);\n+        assertTrue(truncationRecord.getObject().isUpdating());\n+        streamStorePartialMock.completeTruncation(SCOPE, stream1, truncationRecord, null, executor).join();\n+        // endregion\n+        \n+        // region case 3: min criteria not met on lower bound. truncate at max.\n+        time.set(20L);\n+        streamStorePartialMock.addStreamCutToRetentionSet(SCOPE, stream1,\n+                new StreamCutRecord(time.get(), 22L, ImmutableMap.of(0L, 11L, 1L, 11L)), null, executor).join();\n+\n+        // update both readers to make sure they have read till the latest position - 1. we have set the min limit to 2.  \n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber1, ImmutableMap.of(0L, 11L, 1L, 11L), null).join();\n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber2, ImmutableMap.of(0L, 11L, 1L, 11L), null).join();\n+\n+        streamMetadataTasks.retention(SCOPE, stream1, retentionPolicy, time.get(), null, \"\").join();\n+        // retentionset: 0L: 0L/2L, 1L/2L... 2L: 0L/2L, 1L/2L... 10L: 0/10, 1/10....11L: 0/10, 1/10... 20: 0/11, 1/11\n+        // subscriber lowerbound is 0/11, 1/11 \n+        truncationRecord = streamStorePartialMock.getTruncationRecord(SCOPE, stream1, null, executor).join();\n+        // truncate at limit min\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(0L).longValue(), 10L);\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(1L).longValue(), 10L);\n+        assertTrue(truncationRecord.getObject().isUpdating());\n+        streamStorePartialMock.completeTruncation(SCOPE, stream1, truncationRecord, null, executor).join();\n+        // endregion\n+        \n+        // region case 4: lowerbound behind max\n+        streamStorePartialMock.addStreamCutToRetentionSet(SCOPE, stream1,\n+                new StreamCutRecord(30L, 40L, ImmutableMap.of(0L, 20L, 1L, 20L)), null, executor).join();\n+        time.set(40L);\n+        streamStorePartialMock.addStreamCutToRetentionSet(SCOPE, stream1,\n+                new StreamCutRecord(time.get(), 42L, ImmutableMap.of(0L, 21L, 1L, 21L)), null, executor).join();\n+\n+        // update both readers to make sure they have read till the latest position - 1. we have set the min limit to 2.  \n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber1, ImmutableMap.of(0L, 11L, 1L, 11L), null).join();\n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber2, ImmutableMap.of(0L, 11L, 1L, 11L), null).join();\n+\n+        streamMetadataTasks.retention(SCOPE, stream1, retentionPolicy, time.get(), null, \"\").join();\n+        // now retention set has five stream cuts 1: 0/2, 1/2...10: 0/10, 1/10... 20: 0/11, 1/11.. 30: 0/20, 1/20.. 40L: 0/21, 1/21\n+        // subscriber lowerbound is 0/11, 1/11 \n+        // max = 30. truncate at max\n+        truncationRecord = streamStorePartialMock.getTruncationRecord(SCOPE, stream1, null, executor).join();\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(0L).longValue(), 20L);\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(1L).longValue(), 20L);\n+        assertTrue(truncationRecord.getObject().isUpdating());\n+        streamStorePartialMock.completeTruncation(SCOPE, stream1, truncationRecord, null, executor).join();\n+        // endregion\n+\n+        // region case 5: lowerbound overlaps with max\n+        streamStorePartialMock.addStreamCutToRetentionSet(SCOPE, stream1,\n+                new StreamCutRecord(50L, 43L, ImmutableMap.of(0L, 21L, 1L, 22L)), null, executor).join();\n+        time.set(59L);\n+        streamStorePartialMock.addStreamCutToRetentionSet(SCOPE, stream1,\n+                new StreamCutRecord(time.get(), 60L, ImmutableMap.of(0L, 30L, 1L, 30L)), null, executor).join();\n+        time.set(60L);\n+        streamStorePartialMock.addStreamCutToRetentionSet(SCOPE, stream1,\n+                new StreamCutRecord(time.get(), 60L, ImmutableMap.of(0L, 30L, 1L, 30L)), null, executor).join();\n+\n+        // update both readers to make sure they have read till the latest position - 1. we have set the min limit to 2.  \n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber1, ImmutableMap.of(0L, 22L, 1L, 21L), null).join();\n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber2, ImmutableMap.of(0L, 22L, 1L, 21L), null).join();\n+\n+        streamMetadataTasks.retention(SCOPE, stream1, retentionPolicy, time.get(), null, \"\").join();\n+        // now retention set has five stream cuts 1: 0/2, 1/2...10: 0/10, 1/10... 20: 0/11, 1/11.. 30: 0/20, 1/20.. 40L: 0/21, 1/21\n+        // 50: 0/21, 1/22 ... 59: 0/30, 1/30.. 60: 0/30, 1/30\n+        // subscriber lowerbound is 0/22, 1/21 \n+        // this overlaps with max. so truncate at streamcut\n+        truncationRecord = streamStorePartialMock.getTruncationRecord(SCOPE, stream1, null, executor).join();\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(0L).longValue(), 22L);\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(1L).longValue(), 21L);\n+        assertTrue(truncationRecord.getObject().isUpdating());\n+        streamStorePartialMock.completeTruncation(SCOPE, stream1, truncationRecord, null, executor).join();\n+        // endregion\n+    }\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjA3OTM3Mg=="}, "originalCommit": {"oid": "bd7badf9bd8d1dd457baf8014f4716b0b468f61c"}, "originalPosition": 289}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjgzODQ1Mw==", "bodyText": "should we wait for all subscribers to have reported at least one streamcut? if that is preferred i will make the change.\n\nNo we should not wait for any subscriber to update their Stream cut. Truncation can happen with 0 subscribers/valid-streamcuts ....its just that it will happen at the max value instead of the lowerBound.\nWe just need to make sure these corner cases are tested", "url": "https://github.com/pravega/pravega/pull/5289#discussion_r522838453", "createdAt": "2020-11-13T09:46:13Z", "author": {"login": "pbelgundi"}, "path": "controller/src/test/java/io/pravega/controller/task/Stream/StreamMetadataTasksTest.java", "diffHunk": "@@ -1020,6 +1021,446 @@ public void sizeBasedRetentionStreamTest() throws Exception {\n         // endregion\n         // endregion\n     }\n+    \n+    @Test(timeout = 30000)\n+    public void consumptionBasedRetentionSizeLimitTest() throws Exception {\n+        final ScalingPolicy policy = ScalingPolicy.fixed(2);\n+        final RetentionPolicy retentionPolicy = RetentionPolicy.byConsumption(RetentionPolicy.ConsumptionLimits.Type.SIZE_BYTES, 2L, 10L);\n+\n+        String stream1 = \"consumptionSize\";\n+        final StreamConfiguration configuration = StreamConfiguration.builder().scalingPolicy(policy)\n+                .retentionPolicy(retentionPolicy).build();\n+\n+        streamStorePartialMock.createStream(SCOPE, stream1, configuration, System.currentTimeMillis(), null, executor).get();\n+        streamStorePartialMock.setState(SCOPE, stream1, State.ACTIVE, null, executor).get();\n+\n+        assertNotEquals(0, consumer.getCurrentSegments(SCOPE, stream1).get().size());\n+        WriterMock requestEventWriter = new WriterMock(streamMetadataTasks, executor);\n+        streamMetadataTasks.setRequestEventWriter(requestEventWriter);\n+        streamMetadataTasks.setRetentionFrequencyMillis(1L);\n+        // region case 1: basic retention\n+        // add subscriber 1\n+        // add subscriber 2\n+        String subscriber1 = \"subscriber1\";\n+        streamMetadataTasks.addSubscriber(SCOPE, stream1, subscriber1, null).join();\n+\n+        String subscriber2 = \"subscriber2\";\n+        streamMetadataTasks.addSubscriber(SCOPE, stream1, subscriber2, null).join();\n+        \n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber1, ImmutableMap.of(0L, 2L, 1L, 1L), null).join();\n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber2, ImmutableMap.of(0L, 1L, 1L, 2L), null).join();\n+\n+        Map<Long, Long> map1 = new HashMap<>();\n+        map1.put(0L, 2L);\n+        map1.put(1L, 2L);\n+        long size = streamStorePartialMock.getSizeTillStreamCut(SCOPE, stream1, map1, Optional.empty(), null, executor).join();\n+        doReturn(CompletableFuture.completedFuture(new StreamCutRecord(1L, size, ImmutableMap.copyOf(map1))))\n+                .when(streamMetadataTasks).generateStreamCut(anyString(), anyString(), any(), any(), any());\n+\n+        // call retention and verify that retention policy applies\n+        streamMetadataTasks.retention(SCOPE, stream1, retentionPolicy, 1L, null, \"\").join();\n+        // now retention set has one stream cut 0/2, 1/2\n+        // subscriber lowerbound is 0/1, 1/1.. trucation should happen at lowerbound\n+\n+        VersionedMetadata<StreamTruncationRecord> truncationRecord = streamStorePartialMock.getTruncationRecord(SCOPE, stream1, null, executor).join();\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(0L).longValue(), 1L);\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(1L).longValue(), 1L);\n+        assertTrue(truncationRecord.getObject().isUpdating());\n+        streamStorePartialMock.completeTruncation(SCOPE, stream1, truncationRecord, null, executor).join();\n+        // endregion\n+        \n+        // region case 2 min policy check\n+        // we will update the new streamcut to 0/10, 1/10\n+        map1.put(0L, 2L);\n+        map1.put(1L, 2L);\n+        size = streamStorePartialMock.getSizeTillStreamCut(SCOPE, stream1, map1, Optional.empty(), null, executor).join();\n+        doReturn(CompletableFuture.completedFuture(new StreamCutRecord(20L, size, ImmutableMap.copyOf(map1))))\n+                .when(streamMetadataTasks).generateStreamCut(anyString(), anyString(), any(), any(), any());\n+\n+        // update both readers to make sure they have read till the latest position. we have set the min limit to 2.  \n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber1, ImmutableMap.of(0L, 2L, 1L, 2L), null).join();\n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber2, ImmutableMap.of(0L, 2L, 1L, 2L), null).join();\n+\n+        // no new truncation should happen. \n+        // verify that truncation record has not changed. \n+        streamMetadataTasks.retention(SCOPE, stream1, retentionPolicy, 20L, null, \"\").join();\n+        // now retention set has two stream cut 0/2, 1/2...0/2, 1/2\n+        // subscriber lowerbound is 0/2, 1/2.. does not meet min bound criteria. we also do not have a max that satisfies the limit. no truncation should happen. \n+        // no change:\n+        truncationRecord = streamStorePartialMock.getTruncationRecord(SCOPE, stream1, null, executor).join();\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(0L).longValue(), 1L);\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(1L).longValue(), 1L);\n+        assertFalse(truncationRecord.getObject().isUpdating());\n+        // endregion\n+        \n+        // region case 3: min criteria not met on lower bound. truncate at max. \n+        map1.put(0L, 10L);\n+        map1.put(1L, 10L);\n+        size = streamStorePartialMock.getSizeTillStreamCut(SCOPE, stream1, map1, Optional.empty(), null, executor).join();\n+        doReturn(CompletableFuture.completedFuture(new StreamCutRecord(30L, size, ImmutableMap.copyOf(map1))))\n+                .when(streamMetadataTasks).generateStreamCut(anyString(), anyString(), any(), any(), any());\n+\n+        // update both readers to make sure they have read till the latest position - 1. we have set the min limit to 2.  \n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber1, ImmutableMap.of(0L, 10L, 1L, 9L), null).join();\n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber2, ImmutableMap.of(0L, 10L, 1L, 9L), null).join();\n+\n+        streamMetadataTasks.retention(SCOPE, stream1, retentionPolicy, 30L, null, \"\").join();\n+        // now retention set has three stream cut 0/2, 1/2...0/2, 1/2... 0/10, 1/10\n+        // subscriber lowerbound is 0/10, 1/9.. does not meet min bound criteria. but we have max bound on truncation record\n+        // truncation should happen at 0/2, 1/2\n+        truncationRecord = streamStorePartialMock.getTruncationRecord(SCOPE, stream1, null, executor).join();\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(0L).longValue(), 2L);\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(1L).longValue(), 2L);\n+        assertTrue(truncationRecord.getObject().isUpdating());\n+        streamStorePartialMock.completeTruncation(SCOPE, stream1, truncationRecord, null, executor).join();\n+        // endregion\n+        \n+        // region case 4: lowerbound behind max\n+        // now move the stream further ahead so that max truncation limit is crossed but lowerbound is behind max. \n+        map1.put(0L, 20L);\n+        map1.put(1L, 20L);\n+        size = streamStorePartialMock.getSizeTillStreamCut(SCOPE, stream1, map1, Optional.empty(), null, executor).join();\n+        doReturn(CompletableFuture.completedFuture(new StreamCutRecord(40L, size, ImmutableMap.copyOf(map1))))\n+                .when(streamMetadataTasks).generateStreamCut(anyString(), anyString(), any(), any(), any());\n+        \n+        streamMetadataTasks.retention(SCOPE, stream1, retentionPolicy, 40L, null, \"\").join();\n+        // now retention set has three stream cut 0/2, 1/2...0/2, 1/2... 0/10, 1/10.. 0/20, 1/20\n+        // subscriber lowerbound is 0/10, 1/9.. meets min bound criteria. but we have max bound on truncation record\n+        // truncation should happen at 0/10, 1/10\n+        truncationRecord = streamStorePartialMock.getTruncationRecord(SCOPE, stream1, null, executor).join();\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(0L).longValue(), 10L);\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(1L).longValue(), 10L);\n+        assertTrue(truncationRecord.getObject().isUpdating());\n+        streamStorePartialMock.completeTruncation(SCOPE, stream1, truncationRecord, null, executor).join();\n+        // endregion\n+\n+        // region case 5: lowerbound overlaps with max\n+        map1.put(0L, 30L);\n+        map1.put(1L, 30L);\n+        size = streamStorePartialMock.getSizeTillStreamCut(SCOPE, stream1, map1, Optional.empty(), null, executor).join();\n+        doReturn(CompletableFuture.completedFuture(new StreamCutRecord(50L, size, ImmutableMap.copyOf(map1))))\n+                .when(streamMetadataTasks).generateStreamCut(anyString(), anyString(), any(), any(), any());\n+\n+        // update both readers to make sure they have read till the latest position - 1. we have set the min limit to 2.  \n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber1, ImmutableMap.of(0L, 21L, 1L, 19L), null).join();\n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber2, ImmutableMap.of(0L, 21L, 1L, 19L), null).join();\n+\n+        streamMetadataTasks.retention(SCOPE, stream1, retentionPolicy, 50L, null, \"\").join();\n+        // now retention set has three stream cut 0/2, 1/2...0/2, 1/2... 0/10, 1/10.. 0/20, 1/20.. 0/30, 1/30\n+        // subscriber lowerbound is 0/21, 1/19.. meets min bound criteria. and its also greater than max bound. but it overlaps with max bound. \n+        // truncation should happen at 0/21, 1/19\n+        truncationRecord = streamStorePartialMock.getTruncationRecord(SCOPE, stream1, null, executor).join();\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(0L).longValue(), 21L);\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(1L).longValue(), 19L);\n+        assertTrue(truncationRecord.getObject().isUpdating());\n+        streamStorePartialMock.completeTruncation(SCOPE, stream1, truncationRecord, null, executor).join();\n+        // endregion\n+    }\n+    \n+    @Test(timeout = 30000)\n+    public void consumptionBasedRetentionTimeLimitTest() throws Exception {\n+        final ScalingPolicy policy = ScalingPolicy.fixed(2);\n+        final RetentionPolicy retentionPolicy = RetentionPolicy.byConsumption(RetentionPolicy.ConsumptionLimits.Type.TIME_MILLIS, 1L, 10L);\n+\n+        String stream1 = \"consumptionTime\";\n+        final StreamConfiguration configuration = StreamConfiguration.builder().scalingPolicy(policy)\n+                .retentionPolicy(retentionPolicy).build();\n+\n+        streamStorePartialMock.createStream(SCOPE, stream1, configuration, System.currentTimeMillis(), null, executor).get();\n+        streamStorePartialMock.setState(SCOPE, stream1, State.ACTIVE, null, executor).get();\n+\n+        assertNotEquals(0, consumer.getCurrentSegments(SCOPE, stream1).get().size());\n+        WriterMock requestEventWriter = new WriterMock(streamMetadataTasks, executor);\n+        streamMetadataTasks.setRequestEventWriter(requestEventWriter);\n+        streamMetadataTasks.setRetentionFrequencyMillis(1L);\n+        AtomicLong time = new AtomicLong(0L);\n+        streamMetadataTasks.setRetentionClock(time::get);\n+        // region case 1: basic retention\n+        // add subscriber 1\n+        // add subscriber 2\n+        String subscriber1 = \"subscriber1\";\n+        streamMetadataTasks.addSubscriber(SCOPE, stream1, subscriber1, null).join();\n+\n+        String subscriber2 = \"subscriber2\";\n+        streamMetadataTasks.addSubscriber(SCOPE, stream1, subscriber2, null).join();\n+        \n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber1, ImmutableMap.of(0L, 2L, 1L, 1L), null).join();\n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber2, ImmutableMap.of(0L, 1L, 1L, 2L), null).join();\n+\n+        Map<Long, Long> map1 = new HashMap<>();\n+        map1.put(0L, 2L);\n+        map1.put(1L, 2L);\n+        long size = streamStorePartialMock.getSizeTillStreamCut(SCOPE, stream1, map1, Optional.empty(), null, executor).join();\n+        doReturn(CompletableFuture.completedFuture(new StreamCutRecord(time.get(), size, ImmutableMap.copyOf(map1))))\n+                .when(streamMetadataTasks).generateStreamCut(anyString(), anyString(), any(), any(), any());\n+\n+        // call retention and verify that retention policy applies\n+        streamMetadataTasks.retention(SCOPE, stream1, retentionPolicy, time.get(), null, \"\").join();\n+        // now retention set has one stream cut 0/2, 1/2, recording time 1L\n+        // subscriber lowerbound is 0/1, 1/1.. trucation should not happen as this lowerbound is ahead of min retention streamcut.\n+        VersionedMetadata<StreamTruncationRecord> truncationRecord = streamStorePartialMock.getTruncationRecord(SCOPE, stream1, null, executor).join();\n+        assertFalse(truncationRecord.getObject().isUpdating());\n+        // endregion\n+        \n+        // region case 2 min policy check\n+        // subscriber streamcut > min time streamcut while\n+        streamStorePartialMock.addStreamCutToRetentionSet(SCOPE, stream1,\n+                new StreamCutRecord(2L, 4L, ImmutableMap.of(0L, 2L, 1L, 2L)), null, executor).join();\n+\n+        time.set(10L);\n+        streamStorePartialMock.addStreamCutToRetentionSet(SCOPE, stream1,\n+                new StreamCutRecord(time.get(), 20L, ImmutableMap.of(0L, 10L, 1L, 10L)), null, executor).join();\n+\n+        time.set(11L);\n+        streamStorePartialMock.addStreamCutToRetentionSet(SCOPE, stream1,\n+                new StreamCutRecord(time.get(), 20L, ImmutableMap.of(0L, 10L, 1L, 10L)), null, executor).join();\n+\n+        // retentionset: 0L: 0L/2L, 1L/2L... 2L: 0L/2L, 1L/2L... 10L: 0/10, 1/10....11L: 0/10, 1/10. \n+        // update both readers to 0/3, 1/3.  \n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber1, ImmutableMap.of(0L, 3L, 1L, 3L), null).join();\n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber2, ImmutableMap.of(0L, 3L, 1L, 3L), null).join();\n+\n+        // new truncation should happen at subscriber lowerbound.\n+        streamMetadataTasks.retention(SCOPE, stream1, retentionPolicy, time.get(), null, \"\").join();\n+\n+        truncationRecord = streamStorePartialMock.getTruncationRecord(SCOPE, stream1, null, executor).join();\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(0L).longValue(), 3L);\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(1L).longValue(), 3L);\n+        assertTrue(truncationRecord.getObject().isUpdating());\n+        streamStorePartialMock.completeTruncation(SCOPE, stream1, truncationRecord, null, executor).join();\n+        // endregion\n+        \n+        // region case 3: min criteria not met on lower bound. truncate at max.\n+        time.set(20L);\n+        streamStorePartialMock.addStreamCutToRetentionSet(SCOPE, stream1,\n+                new StreamCutRecord(time.get(), 22L, ImmutableMap.of(0L, 11L, 1L, 11L)), null, executor).join();\n+\n+        // update both readers to make sure they have read till the latest position - 1. we have set the min limit to 2.  \n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber1, ImmutableMap.of(0L, 11L, 1L, 11L), null).join();\n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber2, ImmutableMap.of(0L, 11L, 1L, 11L), null).join();\n+\n+        streamMetadataTasks.retention(SCOPE, stream1, retentionPolicy, time.get(), null, \"\").join();\n+        // retentionset: 0L: 0L/2L, 1L/2L... 2L: 0L/2L, 1L/2L... 10L: 0/10, 1/10....11L: 0/10, 1/10... 20: 0/11, 1/11\n+        // subscriber lowerbound is 0/11, 1/11 \n+        truncationRecord = streamStorePartialMock.getTruncationRecord(SCOPE, stream1, null, executor).join();\n+        // truncate at limit min\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(0L).longValue(), 10L);\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(1L).longValue(), 10L);\n+        assertTrue(truncationRecord.getObject().isUpdating());\n+        streamStorePartialMock.completeTruncation(SCOPE, stream1, truncationRecord, null, executor).join();\n+        // endregion\n+        \n+        // region case 4: lowerbound behind max\n+        streamStorePartialMock.addStreamCutToRetentionSet(SCOPE, stream1,\n+                new StreamCutRecord(30L, 40L, ImmutableMap.of(0L, 20L, 1L, 20L)), null, executor).join();\n+        time.set(40L);\n+        streamStorePartialMock.addStreamCutToRetentionSet(SCOPE, stream1,\n+                new StreamCutRecord(time.get(), 42L, ImmutableMap.of(0L, 21L, 1L, 21L)), null, executor).join();\n+\n+        // update both readers to make sure they have read till the latest position - 1. we have set the min limit to 2.  \n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber1, ImmutableMap.of(0L, 11L, 1L, 11L), null).join();\n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber2, ImmutableMap.of(0L, 11L, 1L, 11L), null).join();\n+\n+        streamMetadataTasks.retention(SCOPE, stream1, retentionPolicy, time.get(), null, \"\").join();\n+        // now retention set has five stream cuts 1: 0/2, 1/2...10: 0/10, 1/10... 20: 0/11, 1/11.. 30: 0/20, 1/20.. 40L: 0/21, 1/21\n+        // subscriber lowerbound is 0/11, 1/11 \n+        // max = 30. truncate at max\n+        truncationRecord = streamStorePartialMock.getTruncationRecord(SCOPE, stream1, null, executor).join();\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(0L).longValue(), 20L);\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(1L).longValue(), 20L);\n+        assertTrue(truncationRecord.getObject().isUpdating());\n+        streamStorePartialMock.completeTruncation(SCOPE, stream1, truncationRecord, null, executor).join();\n+        // endregion\n+\n+        // region case 5: lowerbound overlaps with max\n+        streamStorePartialMock.addStreamCutToRetentionSet(SCOPE, stream1,\n+                new StreamCutRecord(50L, 43L, ImmutableMap.of(0L, 21L, 1L, 22L)), null, executor).join();\n+        time.set(59L);\n+        streamStorePartialMock.addStreamCutToRetentionSet(SCOPE, stream1,\n+                new StreamCutRecord(time.get(), 60L, ImmutableMap.of(0L, 30L, 1L, 30L)), null, executor).join();\n+        time.set(60L);\n+        streamStorePartialMock.addStreamCutToRetentionSet(SCOPE, stream1,\n+                new StreamCutRecord(time.get(), 60L, ImmutableMap.of(0L, 30L, 1L, 30L)), null, executor).join();\n+\n+        // update both readers to make sure they have read till the latest position - 1. we have set the min limit to 2.  \n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber1, ImmutableMap.of(0L, 22L, 1L, 21L), null).join();\n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber2, ImmutableMap.of(0L, 22L, 1L, 21L), null).join();\n+\n+        streamMetadataTasks.retention(SCOPE, stream1, retentionPolicy, time.get(), null, \"\").join();\n+        // now retention set has five stream cuts 1: 0/2, 1/2...10: 0/10, 1/10... 20: 0/11, 1/11.. 30: 0/20, 1/20.. 40L: 0/21, 1/21\n+        // 50: 0/21, 1/22 ... 59: 0/30, 1/30.. 60: 0/30, 1/30\n+        // subscriber lowerbound is 0/22, 1/21 \n+        // this overlaps with max. so truncate at streamcut\n+        truncationRecord = streamStorePartialMock.getTruncationRecord(SCOPE, stream1, null, executor).join();\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(0L).longValue(), 22L);\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(1L).longValue(), 21L);\n+        assertTrue(truncationRecord.getObject().isUpdating());\n+        streamStorePartialMock.completeTruncation(SCOPE, stream1, truncationRecord, null, executor).join();\n+        // endregion\n+    }\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjA3OTM3Mg=="}, "originalCommit": {"oid": "bd7badf9bd8d1dd457baf8014f4716b0b468f61c"}, "originalPosition": 289}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjg2NzE2Ng==", "bodyText": "i will add test to cover Long.Max and 0 as max and min respectively.", "url": "https://github.com/pravega/pravega/pull/5289#discussion_r522867166", "createdAt": "2020-11-13T10:39:05Z", "author": {"login": "shiveshr"}, "path": "controller/src/test/java/io/pravega/controller/task/Stream/StreamMetadataTasksTest.java", "diffHunk": "@@ -1020,6 +1021,446 @@ public void sizeBasedRetentionStreamTest() throws Exception {\n         // endregion\n         // endregion\n     }\n+    \n+    @Test(timeout = 30000)\n+    public void consumptionBasedRetentionSizeLimitTest() throws Exception {\n+        final ScalingPolicy policy = ScalingPolicy.fixed(2);\n+        final RetentionPolicy retentionPolicy = RetentionPolicy.byConsumption(RetentionPolicy.ConsumptionLimits.Type.SIZE_BYTES, 2L, 10L);\n+\n+        String stream1 = \"consumptionSize\";\n+        final StreamConfiguration configuration = StreamConfiguration.builder().scalingPolicy(policy)\n+                .retentionPolicy(retentionPolicy).build();\n+\n+        streamStorePartialMock.createStream(SCOPE, stream1, configuration, System.currentTimeMillis(), null, executor).get();\n+        streamStorePartialMock.setState(SCOPE, stream1, State.ACTIVE, null, executor).get();\n+\n+        assertNotEquals(0, consumer.getCurrentSegments(SCOPE, stream1).get().size());\n+        WriterMock requestEventWriter = new WriterMock(streamMetadataTasks, executor);\n+        streamMetadataTasks.setRequestEventWriter(requestEventWriter);\n+        streamMetadataTasks.setRetentionFrequencyMillis(1L);\n+        // region case 1: basic retention\n+        // add subscriber 1\n+        // add subscriber 2\n+        String subscriber1 = \"subscriber1\";\n+        streamMetadataTasks.addSubscriber(SCOPE, stream1, subscriber1, null).join();\n+\n+        String subscriber2 = \"subscriber2\";\n+        streamMetadataTasks.addSubscriber(SCOPE, stream1, subscriber2, null).join();\n+        \n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber1, ImmutableMap.of(0L, 2L, 1L, 1L), null).join();\n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber2, ImmutableMap.of(0L, 1L, 1L, 2L), null).join();\n+\n+        Map<Long, Long> map1 = new HashMap<>();\n+        map1.put(0L, 2L);\n+        map1.put(1L, 2L);\n+        long size = streamStorePartialMock.getSizeTillStreamCut(SCOPE, stream1, map1, Optional.empty(), null, executor).join();\n+        doReturn(CompletableFuture.completedFuture(new StreamCutRecord(1L, size, ImmutableMap.copyOf(map1))))\n+                .when(streamMetadataTasks).generateStreamCut(anyString(), anyString(), any(), any(), any());\n+\n+        // call retention and verify that retention policy applies\n+        streamMetadataTasks.retention(SCOPE, stream1, retentionPolicy, 1L, null, \"\").join();\n+        // now retention set has one stream cut 0/2, 1/2\n+        // subscriber lowerbound is 0/1, 1/1.. trucation should happen at lowerbound\n+\n+        VersionedMetadata<StreamTruncationRecord> truncationRecord = streamStorePartialMock.getTruncationRecord(SCOPE, stream1, null, executor).join();\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(0L).longValue(), 1L);\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(1L).longValue(), 1L);\n+        assertTrue(truncationRecord.getObject().isUpdating());\n+        streamStorePartialMock.completeTruncation(SCOPE, stream1, truncationRecord, null, executor).join();\n+        // endregion\n+        \n+        // region case 2 min policy check\n+        // we will update the new streamcut to 0/10, 1/10\n+        map1.put(0L, 2L);\n+        map1.put(1L, 2L);\n+        size = streamStorePartialMock.getSizeTillStreamCut(SCOPE, stream1, map1, Optional.empty(), null, executor).join();\n+        doReturn(CompletableFuture.completedFuture(new StreamCutRecord(20L, size, ImmutableMap.copyOf(map1))))\n+                .when(streamMetadataTasks).generateStreamCut(anyString(), anyString(), any(), any(), any());\n+\n+        // update both readers to make sure they have read till the latest position. we have set the min limit to 2.  \n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber1, ImmutableMap.of(0L, 2L, 1L, 2L), null).join();\n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber2, ImmutableMap.of(0L, 2L, 1L, 2L), null).join();\n+\n+        // no new truncation should happen. \n+        // verify that truncation record has not changed. \n+        streamMetadataTasks.retention(SCOPE, stream1, retentionPolicy, 20L, null, \"\").join();\n+        // now retention set has two stream cut 0/2, 1/2...0/2, 1/2\n+        // subscriber lowerbound is 0/2, 1/2.. does not meet min bound criteria. we also do not have a max that satisfies the limit. no truncation should happen. \n+        // no change:\n+        truncationRecord = streamStorePartialMock.getTruncationRecord(SCOPE, stream1, null, executor).join();\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(0L).longValue(), 1L);\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(1L).longValue(), 1L);\n+        assertFalse(truncationRecord.getObject().isUpdating());\n+        // endregion\n+        \n+        // region case 3: min criteria not met on lower bound. truncate at max. \n+        map1.put(0L, 10L);\n+        map1.put(1L, 10L);\n+        size = streamStorePartialMock.getSizeTillStreamCut(SCOPE, stream1, map1, Optional.empty(), null, executor).join();\n+        doReturn(CompletableFuture.completedFuture(new StreamCutRecord(30L, size, ImmutableMap.copyOf(map1))))\n+                .when(streamMetadataTasks).generateStreamCut(anyString(), anyString(), any(), any(), any());\n+\n+        // update both readers to make sure they have read till the latest position - 1. we have set the min limit to 2.  \n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber1, ImmutableMap.of(0L, 10L, 1L, 9L), null).join();\n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber2, ImmutableMap.of(0L, 10L, 1L, 9L), null).join();\n+\n+        streamMetadataTasks.retention(SCOPE, stream1, retentionPolicy, 30L, null, \"\").join();\n+        // now retention set has three stream cut 0/2, 1/2...0/2, 1/2... 0/10, 1/10\n+        // subscriber lowerbound is 0/10, 1/9.. does not meet min bound criteria. but we have max bound on truncation record\n+        // truncation should happen at 0/2, 1/2\n+        truncationRecord = streamStorePartialMock.getTruncationRecord(SCOPE, stream1, null, executor).join();\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(0L).longValue(), 2L);\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(1L).longValue(), 2L);\n+        assertTrue(truncationRecord.getObject().isUpdating());\n+        streamStorePartialMock.completeTruncation(SCOPE, stream1, truncationRecord, null, executor).join();\n+        // endregion\n+        \n+        // region case 4: lowerbound behind max\n+        // now move the stream further ahead so that max truncation limit is crossed but lowerbound is behind max. \n+        map1.put(0L, 20L);\n+        map1.put(1L, 20L);\n+        size = streamStorePartialMock.getSizeTillStreamCut(SCOPE, stream1, map1, Optional.empty(), null, executor).join();\n+        doReturn(CompletableFuture.completedFuture(new StreamCutRecord(40L, size, ImmutableMap.copyOf(map1))))\n+                .when(streamMetadataTasks).generateStreamCut(anyString(), anyString(), any(), any(), any());\n+        \n+        streamMetadataTasks.retention(SCOPE, stream1, retentionPolicy, 40L, null, \"\").join();\n+        // now retention set has three stream cut 0/2, 1/2...0/2, 1/2... 0/10, 1/10.. 0/20, 1/20\n+        // subscriber lowerbound is 0/10, 1/9.. meets min bound criteria. but we have max bound on truncation record\n+        // truncation should happen at 0/10, 1/10\n+        truncationRecord = streamStorePartialMock.getTruncationRecord(SCOPE, stream1, null, executor).join();\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(0L).longValue(), 10L);\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(1L).longValue(), 10L);\n+        assertTrue(truncationRecord.getObject().isUpdating());\n+        streamStorePartialMock.completeTruncation(SCOPE, stream1, truncationRecord, null, executor).join();\n+        // endregion\n+\n+        // region case 5: lowerbound overlaps with max\n+        map1.put(0L, 30L);\n+        map1.put(1L, 30L);\n+        size = streamStorePartialMock.getSizeTillStreamCut(SCOPE, stream1, map1, Optional.empty(), null, executor).join();\n+        doReturn(CompletableFuture.completedFuture(new StreamCutRecord(50L, size, ImmutableMap.copyOf(map1))))\n+                .when(streamMetadataTasks).generateStreamCut(anyString(), anyString(), any(), any(), any());\n+\n+        // update both readers to make sure they have read till the latest position - 1. we have set the min limit to 2.  \n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber1, ImmutableMap.of(0L, 21L, 1L, 19L), null).join();\n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber2, ImmutableMap.of(0L, 21L, 1L, 19L), null).join();\n+\n+        streamMetadataTasks.retention(SCOPE, stream1, retentionPolicy, 50L, null, \"\").join();\n+        // now retention set has three stream cut 0/2, 1/2...0/2, 1/2... 0/10, 1/10.. 0/20, 1/20.. 0/30, 1/30\n+        // subscriber lowerbound is 0/21, 1/19.. meets min bound criteria. and its also greater than max bound. but it overlaps with max bound. \n+        // truncation should happen at 0/21, 1/19\n+        truncationRecord = streamStorePartialMock.getTruncationRecord(SCOPE, stream1, null, executor).join();\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(0L).longValue(), 21L);\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(1L).longValue(), 19L);\n+        assertTrue(truncationRecord.getObject().isUpdating());\n+        streamStorePartialMock.completeTruncation(SCOPE, stream1, truncationRecord, null, executor).join();\n+        // endregion\n+    }\n+    \n+    @Test(timeout = 30000)\n+    public void consumptionBasedRetentionTimeLimitTest() throws Exception {\n+        final ScalingPolicy policy = ScalingPolicy.fixed(2);\n+        final RetentionPolicy retentionPolicy = RetentionPolicy.byConsumption(RetentionPolicy.ConsumptionLimits.Type.TIME_MILLIS, 1L, 10L);\n+\n+        String stream1 = \"consumptionTime\";\n+        final StreamConfiguration configuration = StreamConfiguration.builder().scalingPolicy(policy)\n+                .retentionPolicy(retentionPolicy).build();\n+\n+        streamStorePartialMock.createStream(SCOPE, stream1, configuration, System.currentTimeMillis(), null, executor).get();\n+        streamStorePartialMock.setState(SCOPE, stream1, State.ACTIVE, null, executor).get();\n+\n+        assertNotEquals(0, consumer.getCurrentSegments(SCOPE, stream1).get().size());\n+        WriterMock requestEventWriter = new WriterMock(streamMetadataTasks, executor);\n+        streamMetadataTasks.setRequestEventWriter(requestEventWriter);\n+        streamMetadataTasks.setRetentionFrequencyMillis(1L);\n+        AtomicLong time = new AtomicLong(0L);\n+        streamMetadataTasks.setRetentionClock(time::get);\n+        // region case 1: basic retention\n+        // add subscriber 1\n+        // add subscriber 2\n+        String subscriber1 = \"subscriber1\";\n+        streamMetadataTasks.addSubscriber(SCOPE, stream1, subscriber1, null).join();\n+\n+        String subscriber2 = \"subscriber2\";\n+        streamMetadataTasks.addSubscriber(SCOPE, stream1, subscriber2, null).join();\n+        \n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber1, ImmutableMap.of(0L, 2L, 1L, 1L), null).join();\n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber2, ImmutableMap.of(0L, 1L, 1L, 2L), null).join();\n+\n+        Map<Long, Long> map1 = new HashMap<>();\n+        map1.put(0L, 2L);\n+        map1.put(1L, 2L);\n+        long size = streamStorePartialMock.getSizeTillStreamCut(SCOPE, stream1, map1, Optional.empty(), null, executor).join();\n+        doReturn(CompletableFuture.completedFuture(new StreamCutRecord(time.get(), size, ImmutableMap.copyOf(map1))))\n+                .when(streamMetadataTasks).generateStreamCut(anyString(), anyString(), any(), any(), any());\n+\n+        // call retention and verify that retention policy applies\n+        streamMetadataTasks.retention(SCOPE, stream1, retentionPolicy, time.get(), null, \"\").join();\n+        // now retention set has one stream cut 0/2, 1/2, recording time 1L\n+        // subscriber lowerbound is 0/1, 1/1.. trucation should not happen as this lowerbound is ahead of min retention streamcut.\n+        VersionedMetadata<StreamTruncationRecord> truncationRecord = streamStorePartialMock.getTruncationRecord(SCOPE, stream1, null, executor).join();\n+        assertFalse(truncationRecord.getObject().isUpdating());\n+        // endregion\n+        \n+        // region case 2 min policy check\n+        // subscriber streamcut > min time streamcut while\n+        streamStorePartialMock.addStreamCutToRetentionSet(SCOPE, stream1,\n+                new StreamCutRecord(2L, 4L, ImmutableMap.of(0L, 2L, 1L, 2L)), null, executor).join();\n+\n+        time.set(10L);\n+        streamStorePartialMock.addStreamCutToRetentionSet(SCOPE, stream1,\n+                new StreamCutRecord(time.get(), 20L, ImmutableMap.of(0L, 10L, 1L, 10L)), null, executor).join();\n+\n+        time.set(11L);\n+        streamStorePartialMock.addStreamCutToRetentionSet(SCOPE, stream1,\n+                new StreamCutRecord(time.get(), 20L, ImmutableMap.of(0L, 10L, 1L, 10L)), null, executor).join();\n+\n+        // retentionset: 0L: 0L/2L, 1L/2L... 2L: 0L/2L, 1L/2L... 10L: 0/10, 1/10....11L: 0/10, 1/10. \n+        // update both readers to 0/3, 1/3.  \n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber1, ImmutableMap.of(0L, 3L, 1L, 3L), null).join();\n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber2, ImmutableMap.of(0L, 3L, 1L, 3L), null).join();\n+\n+        // new truncation should happen at subscriber lowerbound.\n+        streamMetadataTasks.retention(SCOPE, stream1, retentionPolicy, time.get(), null, \"\").join();\n+\n+        truncationRecord = streamStorePartialMock.getTruncationRecord(SCOPE, stream1, null, executor).join();\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(0L).longValue(), 3L);\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(1L).longValue(), 3L);\n+        assertTrue(truncationRecord.getObject().isUpdating());\n+        streamStorePartialMock.completeTruncation(SCOPE, stream1, truncationRecord, null, executor).join();\n+        // endregion\n+        \n+        // region case 3: min criteria not met on lower bound. truncate at max.\n+        time.set(20L);\n+        streamStorePartialMock.addStreamCutToRetentionSet(SCOPE, stream1,\n+                new StreamCutRecord(time.get(), 22L, ImmutableMap.of(0L, 11L, 1L, 11L)), null, executor).join();\n+\n+        // update both readers to make sure they have read till the latest position - 1. we have set the min limit to 2.  \n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber1, ImmutableMap.of(0L, 11L, 1L, 11L), null).join();\n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber2, ImmutableMap.of(0L, 11L, 1L, 11L), null).join();\n+\n+        streamMetadataTasks.retention(SCOPE, stream1, retentionPolicy, time.get(), null, \"\").join();\n+        // retentionset: 0L: 0L/2L, 1L/2L... 2L: 0L/2L, 1L/2L... 10L: 0/10, 1/10....11L: 0/10, 1/10... 20: 0/11, 1/11\n+        // subscriber lowerbound is 0/11, 1/11 \n+        truncationRecord = streamStorePartialMock.getTruncationRecord(SCOPE, stream1, null, executor).join();\n+        // truncate at limit min\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(0L).longValue(), 10L);\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(1L).longValue(), 10L);\n+        assertTrue(truncationRecord.getObject().isUpdating());\n+        streamStorePartialMock.completeTruncation(SCOPE, stream1, truncationRecord, null, executor).join();\n+        // endregion\n+        \n+        // region case 4: lowerbound behind max\n+        streamStorePartialMock.addStreamCutToRetentionSet(SCOPE, stream1,\n+                new StreamCutRecord(30L, 40L, ImmutableMap.of(0L, 20L, 1L, 20L)), null, executor).join();\n+        time.set(40L);\n+        streamStorePartialMock.addStreamCutToRetentionSet(SCOPE, stream1,\n+                new StreamCutRecord(time.get(), 42L, ImmutableMap.of(0L, 21L, 1L, 21L)), null, executor).join();\n+\n+        // update both readers to make sure they have read till the latest position - 1. we have set the min limit to 2.  \n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber1, ImmutableMap.of(0L, 11L, 1L, 11L), null).join();\n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber2, ImmutableMap.of(0L, 11L, 1L, 11L), null).join();\n+\n+        streamMetadataTasks.retention(SCOPE, stream1, retentionPolicy, time.get(), null, \"\").join();\n+        // now retention set has five stream cuts 1: 0/2, 1/2...10: 0/10, 1/10... 20: 0/11, 1/11.. 30: 0/20, 1/20.. 40L: 0/21, 1/21\n+        // subscriber lowerbound is 0/11, 1/11 \n+        // max = 30. truncate at max\n+        truncationRecord = streamStorePartialMock.getTruncationRecord(SCOPE, stream1, null, executor).join();\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(0L).longValue(), 20L);\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(1L).longValue(), 20L);\n+        assertTrue(truncationRecord.getObject().isUpdating());\n+        streamStorePartialMock.completeTruncation(SCOPE, stream1, truncationRecord, null, executor).join();\n+        // endregion\n+\n+        // region case 5: lowerbound overlaps with max\n+        streamStorePartialMock.addStreamCutToRetentionSet(SCOPE, stream1,\n+                new StreamCutRecord(50L, 43L, ImmutableMap.of(0L, 21L, 1L, 22L)), null, executor).join();\n+        time.set(59L);\n+        streamStorePartialMock.addStreamCutToRetentionSet(SCOPE, stream1,\n+                new StreamCutRecord(time.get(), 60L, ImmutableMap.of(0L, 30L, 1L, 30L)), null, executor).join();\n+        time.set(60L);\n+        streamStorePartialMock.addStreamCutToRetentionSet(SCOPE, stream1,\n+                new StreamCutRecord(time.get(), 60L, ImmutableMap.of(0L, 30L, 1L, 30L)), null, executor).join();\n+\n+        // update both readers to make sure they have read till the latest position - 1. we have set the min limit to 2.  \n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber1, ImmutableMap.of(0L, 22L, 1L, 21L), null).join();\n+        streamMetadataTasks.updateSubscriberStreamCut(SCOPE, stream1, subscriber2, ImmutableMap.of(0L, 22L, 1L, 21L), null).join();\n+\n+        streamMetadataTasks.retention(SCOPE, stream1, retentionPolicy, time.get(), null, \"\").join();\n+        // now retention set has five stream cuts 1: 0/2, 1/2...10: 0/10, 1/10... 20: 0/11, 1/11.. 30: 0/20, 1/20.. 40L: 0/21, 1/21\n+        // 50: 0/21, 1/22 ... 59: 0/30, 1/30.. 60: 0/30, 1/30\n+        // subscriber lowerbound is 0/22, 1/21 \n+        // this overlaps with max. so truncate at streamcut\n+        truncationRecord = streamStorePartialMock.getTruncationRecord(SCOPE, stream1, null, executor).join();\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(0L).longValue(), 22L);\n+        assertEquals(truncationRecord.getObject().getStreamCut().get(1L).longValue(), 21L);\n+        assertTrue(truncationRecord.getObject().isUpdating());\n+        streamStorePartialMock.completeTruncation(SCOPE, stream1, truncationRecord, null, executor).join();\n+        // endregion\n+    }\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjA3OTM3Mg=="}, "originalCommit": {"oid": "bd7badf9bd8d1dd457baf8014f4716b0b468f61c"}, "originalPosition": 289}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI5MTEzOTY2OnYy", "diffSide": "RIGHT", "path": "controller/src/main/java/io/pravega/controller/task/Stream/StreamMetadataTasks.java", "isResolved": true, "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QwNjoyOTo1NFrOH0mDig==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQwNToyNzo1NFrOH1dt5A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDkxMTQ5OA==", "bodyText": "In the case where a Stream has only a single Subscriber, there is no need to compute a \"lower bound\" and we could short circuit this computation and directly return the Stream-Cut  corresponding to that Subscriber.", "url": "https://github.com/pravega/pravega/pull/5289#discussion_r524911498", "createdAt": "2020-11-17T06:29:54Z", "author": {"login": "pbelgundi"}, "path": "controller/src/main/java/io/pravega/controller/task/Stream/StreamMetadataTasks.java", "diffHunk": "@@ -507,32 +514,296 @@ public void initializeStreamWriters(final EventStreamClientFactory clientFactory\n \n     private CompletableFuture<Void> truncate(String scope, String stream, RetentionPolicy policy, OperationContext context,\n                                              RetentionSet retentionSet, StreamCutRecord newRecord, long recordingTime, long requestId) {\n-        Optional<StreamCutReferenceRecord> truncationRecord = findTruncationRecord(policy, retentionSet, newRecord, recordingTime);\n-        if (!truncationRecord.isPresent()) {\n-            log.info(\"No suitable truncation record found, per retention policy for stream {}/{}\", scope, stream);\n-            return CompletableFuture.completedFuture(null);\n+        if (policy.getRetentionType().equals(RetentionPolicy.RetentionType.CONSUMPTION)) {\n+            return subscriberBasedTruncation(scope, stream, context, policy, retentionSet, newRecord, requestId);\n+        } else {\n+            Optional<StreamCutReferenceRecord> truncationRecord = findTruncationRecord(policy, retentionSet, newRecord, recordingTime);\n+            if (!truncationRecord.isPresent()) {\n+                log.info(\"No suitable truncation record found, per retention policy for stream {}/{}\", scope, stream);\n+                return CompletableFuture.completedFuture(null);\n+            }\n+            log.info(\"Found truncation record for stream {}/{} truncationRecord time/size: {}/{}\", scope, stream,\n+                    truncationRecord.get().getRecordingTime(), truncationRecord.get().getRecordingSize());\n+\n+            return truncate(scope, stream, context, requestId, truncationRecord.get());\n         }\n+    }\n+\n+    private CompletableFuture<Void> truncate(String scope, String stream, OperationContext context, long requestId,\n+                                                   StreamCutReferenceRecord truncationRecord) {\n         log.info(\"Found truncation record for stream {}/{} truncationRecord time/size: {}/{}\", scope, stream,\n-                truncationRecord.get().getRecordingTime(), truncationRecord.get().getRecordingSize());\n-        return streamMetadataStore.getStreamCutRecord(scope, stream, truncationRecord.get(), context, executor)\n-                   .thenCompose(streamCutRecord -> startTruncation(scope, stream, streamCutRecord.getStreamCut(), context, requestId))\n-                   .thenCompose(started -> {\n-                       if (started) {\n-                                return streamMetadataStore.deleteStreamCutBefore(scope, stream, truncationRecord.get(), context, executor);\n+                truncationRecord.getRecordingTime(), truncationRecord.getRecordingSize());\n+        return streamMetadataStore.getStreamCutRecord(scope, stream, truncationRecord, context, executor)\n+                                  .thenCompose(streamCutRecord -> startTruncation(scope, stream, streamCutRecord.getStreamCut(), context, requestId))\n+                                  .thenCompose(started -> {\n+                                      if (started) {\n+                                          return streamMetadataStore.deleteStreamCutBefore(scope, stream, truncationRecord, context, executor);\n+                                      } else {\n+                                          throw new RuntimeException(\"Could not start truncation\");\n+                                      }\n+                                  })\n+                                  .exceptionally(e -> {\n+                                      if (Exceptions.unwrap(e) instanceof IllegalArgumentException) {\n+                                          // This is ignorable exception. Throwing this will cause unnecessary retries and exceptions logged.\n+                                          log.debug(requestId, \"Cannot truncate at given \" +\n+                                                  \"streamCut because it intersects with existing truncation point\");\n+                                          return null;\n+                                      } else {\n+                                          throw new CompletionException(e);\n+                                      }\n+                                  });\n+    }\n+\n+    private CompletableFuture<Void> subscriberBasedTruncation(String scope, String stream, OperationContext context,\n+                                                              RetentionPolicy policy, RetentionSet retentionSet, \n+                                                              StreamCutRecord newRecord, long requestId) {\n+        return streamMetadataStore.listSubscribers(scope, stream, context, executor)\n+                           .thenCompose(list -> Futures.allOfWithResults(list.stream().map(x -> \n+                                   streamMetadataStore.getSubscriber(scope, stream, x, context, executor)).collect(Collectors.toList())))\n+                           .thenCompose(subscribers -> {\n+                               // convert all streamcuts to include the segment range\n+                               return Futures.allOfWithResults(subscribers.stream().map(x -> {\n+                                   ImmutableSet<Map.Entry<Long, Long>> entries = x.getObject().getTruncationStreamCut().entrySet();\n+\n+                                   return Futures.keysAllOfWithResults(entries.stream().collect(Collectors.toMap(\n+                                           y -> streamMetadataStore.getSegment(scope, stream, y.getKey(), context, executor), Map.Entry::getValue)));\n+                               }).collect(Collectors.toList()));\n+                           })\n+                           .thenApply(this::computeSubscribersLowerBound)\n+                .thenCompose(lowerBound -> {\n+                    CompletableFuture<Map<Long, Long>> toTruncateAt; \n+                    if (policy.getConsumptionLimits().getType().equals(RetentionPolicy.ConsumptionLimits.Type.SIZE_BYTES)) {\n+                        toTruncateAt = getTruncationStreamCutBySizeLimit(scope, stream, context, policy, retentionSet, lowerBound, newRecord);\n+                    } else {\n+                        toTruncateAt = getTruncationStreamCutByTimeLimit(scope, stream, context, policy, retentionSet, lowerBound, newRecord);\n+                    }\n+                    return toTruncateAt.thenCompose(truncationStreamCut -> {\n+                        if (truncationStreamCut == null || truncationStreamCut.isEmpty()) {\n+                            log.debug(\"no truncation record could be compute that satisfied retention policy\");\n+                            return CompletableFuture.completedFuture(null);\n+                        } \n+                        return startTruncation(scope, stream, truncationStreamCut, context, requestId)\n+                                .thenCompose(started -> {\n+                                    if (started) {\n+                                        return streamMetadataStore.findStreamCutReferenceRecordBefore(scope, stream, truncationStreamCut, retentionSet, context, executor)\n+                                                                  .thenCompose(ref -> {\n+                                                                      if (ref != null) {\n+                                                                          return streamMetadataStore.deleteStreamCutBefore(scope, stream, ref, context, executor);\n+                                                                      } else {\n+                                                                          return CompletableFuture.completedFuture(null);\n+                                                                      }\n+                                                                  });\n+                                    } else {\n+                                        throw new RuntimeException(\"Could not start truncation\");\n+                                    }\n+                                });\n+                    });\n+                });\n+    }\n+\n+    private CompletableFuture<Map<Long, Long>> getTruncationStreamCutBySizeLimit(String scope, String stream, OperationContext context, RetentionPolicy policy,\n+                                                                                 RetentionSet retentionSet, Map<Long, Long> lowerBound, StreamCutRecord newRecord) {\n+        // if the lowerbound on subscribers streamcuts satisfies the policy size bound, then return it. \n+        // else return the stream cut that satisfies maximum bound on size. \n+        // 1. if lowerbound.size < max and lowerbound.size > min truncate at lowerbound\n+        // 2. if lowerbound.size < min, truncate at max irrespective of if lowerbound overlaps with max or not. \n+        // 3. if lowerbound.size > max, truncate at max\n+        long currentSize = newRecord != null ? newRecord.getRecordingSize() : retentionSet.getLatest().getRecordingSize();\n+        return streamMetadataStore.getSizeTillStreamCut(scope, stream, lowerBound, Optional.empty(), context, executor)\n+                          .thenCompose(sizeTill -> {\n+                               long retainedSize = currentSize - sizeTill;\n+                               Supplier<Optional<StreamCutReferenceRecord>> maxBound = () -> retentionSet\n+                                      .getRetentionRecords().stream()\n+                                      .filter(x -> currentSize - x.getRecordingSize() > policy.getConsumptionLimits().getMaxValue())\n+                                      .max(Comparator.comparingLong(StreamCutReferenceRecord::getRecordingTime));\n+\n+                               // if retainedSize is less than min size then do not truncate the stream. \n+                               if (retainedSize < policy.getConsumptionLimits().getMinValue()) {\n+                                   return maxBound.get().map(x -> streamMetadataStore.getStreamCutRecord(scope, stream, x, context, executor)\n+                                                                               .thenApply(StreamCutRecord::getStreamCut))\n+                                                  .orElse(CompletableFuture.completedFuture(null));\n+                               } else {\n+                                   // if retained size is less than max allowed, then truncate the stream at subscriber lower bound. \n+                                   if (retainedSize < policy.getConsumptionLimits().getMaxValue()) {\n+                                       return CompletableFuture.completedFuture(lowerBound);\n+                                   } else {\n+                                       // if retained size is greater than max allowed, then truncate the stream at streamcut\n+                                       // from retention set that matches the retention policy size bound. \n+                                       return maxBound.get().map(x -> streamMetadataStore.getStreamCutRecord(scope, stream, x, context, executor)\n+                                                   .thenApply(StreamCutRecord::getStreamCut)\n+                                                   .thenCompose(maxRecord -> {\n+                                                       // if max record is strictly greater than lowerbound then we can truncate at max record\n+                                                       return streamMetadataStore.streamCutStrictlyGreaterThan(\n+                                                               scope, stream, maxRecord, lowerBound, context, executor)\n+                                                                                 .thenApply(gt -> {\n+                                                                                     if (gt) {\n+                                                                                         return maxRecord;\n+                                                                                     } else {\n+                                                                                         return lowerBound;\n+                                                                                     }\n+                                                                                 });\n+                                                   })).orElse(CompletableFuture.completedFuture(null));   \n+                                   }\n+                               }\n+                           });\n+    }\n+\n+    private CompletableFuture<Map<Long, Long>> getTruncationStreamCutByTimeLimit(String scope, String stream, OperationContext context,\n+                                                                                 RetentionPolicy policy, RetentionSet retentionSet,\n+                                                                                 Map<Long, Long> lowerBound, StreamCutRecord latest) {\n+        Map.Entry<StreamCutReferenceRecord, StreamCutReferenceRecord> limits =\n+                getBoundStreamCuts(policy.getConsumptionLimits(), retentionSet);\n+        // if subscriber lowerbound is ahead of streamcut corresponding to the max time and is behind stream cut for min time \n+        // from the retention set then we can safely truncate at lowerbound. Else we will truncate at the max time bound if it\n+        // exists\n+        // 1. if LB > min => truncate at min\n+        // 2. if LB < max => truncate at max\n+        // 3. if LB < min && LB > max => truncate at LB\n+        // 4. if LB < min && overlaps max => truncate at LB\n+        CompletableFuture<StreamCutRecord> limitMaxFuture = limits.getKey() == null ? CompletableFuture.completedFuture(null) :\n+                streamMetadataStore.getStreamCutRecord(scope, stream, limits.getKey(), context, executor);\n+        CompletableFuture<StreamCutRecord> limitMinFuture = limits.getValue() == null ? CompletableFuture.completedFuture(null) :\n+                streamMetadataStore.getStreamCutRecord(scope, stream, limits.getValue(), context, executor);\n+        return CompletableFuture.allOf(limitMaxFuture, limitMinFuture)\n+                         .thenCompose(v -> {\n+                             StreamCutRecord limitMax = limitMaxFuture.join();\n+                             StreamCutRecord limitMin = limitMinFuture.join();\n+                             if (limitMin != null) {\n+                                 return streamMetadataStore.streamCutStrictlyGreaterThan(scope, stream, limitMin.getStreamCut(), lowerBound, context, executor)\n+                                                    .thenCompose(gtMin -> {\n+                                                        if (gtMin) {\n+                                                            if (limitMax == null) {\n+                                                                return CompletableFuture.completedFuture(lowerBound);\n+                                                            } else {\n+                                                                return streamMetadataStore.streamCutStrictlyGreaterThan(scope, stream, limitMax.getStreamCut(), lowerBound, context, executor)\n+                                                                                          .thenApply(gtMax -> gtMax ? limitMax.getStreamCut() : lowerBound);\n+                                                            }\n+                                                        } else {\n+                                                            return streamMetadataStore.streamCutStrictlyGreaterThan(scope, stream, lowerBound, limitMin.getStreamCut(), context, executor)\n+                                                                               .thenApply(gt -> gt ? limitMin.getStreamCut() : null);\n+                                                        }\n+                                                    });\n+                             } else {\n+                                 // if min limit is 0 then latest effectively becomes the min. and we truncate at the lowerbound.\n+                                 if (latest != null && policy.getConsumptionLimits().getMinValue() == 0L) {\n+                                     // truncate at the lower bound \n+                                     return CompletableFuture.completedFuture(lowerBound);\n+                                 } else {\n+                                     return CompletableFuture.completedFuture(null);\n+                                 }\n+                             }\n+                         });\n+    }\n+\n+    private Map<Long, Long> computeSubscribersLowerBound(List<Map<StreamSegmentRecord, Long>> subscribers) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07c2838e170b5ac7b040ca02680ca3ab32983ab5"}, "originalPosition": 244}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDkxMjI5MA==", "bodyText": "It may be good to add a test case covering this scenario.", "url": "https://github.com/pravega/pravega/pull/5289#discussion_r524912290", "createdAt": "2020-11-17T06:32:10Z", "author": {"login": "pbelgundi"}, "path": "controller/src/main/java/io/pravega/controller/task/Stream/StreamMetadataTasks.java", "diffHunk": "@@ -507,32 +514,296 @@ public void initializeStreamWriters(final EventStreamClientFactory clientFactory\n \n     private CompletableFuture<Void> truncate(String scope, String stream, RetentionPolicy policy, OperationContext context,\n                                              RetentionSet retentionSet, StreamCutRecord newRecord, long recordingTime, long requestId) {\n-        Optional<StreamCutReferenceRecord> truncationRecord = findTruncationRecord(policy, retentionSet, newRecord, recordingTime);\n-        if (!truncationRecord.isPresent()) {\n-            log.info(\"No suitable truncation record found, per retention policy for stream {}/{}\", scope, stream);\n-            return CompletableFuture.completedFuture(null);\n+        if (policy.getRetentionType().equals(RetentionPolicy.RetentionType.CONSUMPTION)) {\n+            return subscriberBasedTruncation(scope, stream, context, policy, retentionSet, newRecord, requestId);\n+        } else {\n+            Optional<StreamCutReferenceRecord> truncationRecord = findTruncationRecord(policy, retentionSet, newRecord, recordingTime);\n+            if (!truncationRecord.isPresent()) {\n+                log.info(\"No suitable truncation record found, per retention policy for stream {}/{}\", scope, stream);\n+                return CompletableFuture.completedFuture(null);\n+            }\n+            log.info(\"Found truncation record for stream {}/{} truncationRecord time/size: {}/{}\", scope, stream,\n+                    truncationRecord.get().getRecordingTime(), truncationRecord.get().getRecordingSize());\n+\n+            return truncate(scope, stream, context, requestId, truncationRecord.get());\n         }\n+    }\n+\n+    private CompletableFuture<Void> truncate(String scope, String stream, OperationContext context, long requestId,\n+                                                   StreamCutReferenceRecord truncationRecord) {\n         log.info(\"Found truncation record for stream {}/{} truncationRecord time/size: {}/{}\", scope, stream,\n-                truncationRecord.get().getRecordingTime(), truncationRecord.get().getRecordingSize());\n-        return streamMetadataStore.getStreamCutRecord(scope, stream, truncationRecord.get(), context, executor)\n-                   .thenCompose(streamCutRecord -> startTruncation(scope, stream, streamCutRecord.getStreamCut(), context, requestId))\n-                   .thenCompose(started -> {\n-                       if (started) {\n-                                return streamMetadataStore.deleteStreamCutBefore(scope, stream, truncationRecord.get(), context, executor);\n+                truncationRecord.getRecordingTime(), truncationRecord.getRecordingSize());\n+        return streamMetadataStore.getStreamCutRecord(scope, stream, truncationRecord, context, executor)\n+                                  .thenCompose(streamCutRecord -> startTruncation(scope, stream, streamCutRecord.getStreamCut(), context, requestId))\n+                                  .thenCompose(started -> {\n+                                      if (started) {\n+                                          return streamMetadataStore.deleteStreamCutBefore(scope, stream, truncationRecord, context, executor);\n+                                      } else {\n+                                          throw new RuntimeException(\"Could not start truncation\");\n+                                      }\n+                                  })\n+                                  .exceptionally(e -> {\n+                                      if (Exceptions.unwrap(e) instanceof IllegalArgumentException) {\n+                                          // This is ignorable exception. Throwing this will cause unnecessary retries and exceptions logged.\n+                                          log.debug(requestId, \"Cannot truncate at given \" +\n+                                                  \"streamCut because it intersects with existing truncation point\");\n+                                          return null;\n+                                      } else {\n+                                          throw new CompletionException(e);\n+                                      }\n+                                  });\n+    }\n+\n+    private CompletableFuture<Void> subscriberBasedTruncation(String scope, String stream, OperationContext context,\n+                                                              RetentionPolicy policy, RetentionSet retentionSet, \n+                                                              StreamCutRecord newRecord, long requestId) {\n+        return streamMetadataStore.listSubscribers(scope, stream, context, executor)\n+                           .thenCompose(list -> Futures.allOfWithResults(list.stream().map(x -> \n+                                   streamMetadataStore.getSubscriber(scope, stream, x, context, executor)).collect(Collectors.toList())))\n+                           .thenCompose(subscribers -> {\n+                               // convert all streamcuts to include the segment range\n+                               return Futures.allOfWithResults(subscribers.stream().map(x -> {\n+                                   ImmutableSet<Map.Entry<Long, Long>> entries = x.getObject().getTruncationStreamCut().entrySet();\n+\n+                                   return Futures.keysAllOfWithResults(entries.stream().collect(Collectors.toMap(\n+                                           y -> streamMetadataStore.getSegment(scope, stream, y.getKey(), context, executor), Map.Entry::getValue)));\n+                               }).collect(Collectors.toList()));\n+                           })\n+                           .thenApply(this::computeSubscribersLowerBound)\n+                .thenCompose(lowerBound -> {\n+                    CompletableFuture<Map<Long, Long>> toTruncateAt; \n+                    if (policy.getConsumptionLimits().getType().equals(RetentionPolicy.ConsumptionLimits.Type.SIZE_BYTES)) {\n+                        toTruncateAt = getTruncationStreamCutBySizeLimit(scope, stream, context, policy, retentionSet, lowerBound, newRecord);\n+                    } else {\n+                        toTruncateAt = getTruncationStreamCutByTimeLimit(scope, stream, context, policy, retentionSet, lowerBound, newRecord);\n+                    }\n+                    return toTruncateAt.thenCompose(truncationStreamCut -> {\n+                        if (truncationStreamCut == null || truncationStreamCut.isEmpty()) {\n+                            log.debug(\"no truncation record could be compute that satisfied retention policy\");\n+                            return CompletableFuture.completedFuture(null);\n+                        } \n+                        return startTruncation(scope, stream, truncationStreamCut, context, requestId)\n+                                .thenCompose(started -> {\n+                                    if (started) {\n+                                        return streamMetadataStore.findStreamCutReferenceRecordBefore(scope, stream, truncationStreamCut, retentionSet, context, executor)\n+                                                                  .thenCompose(ref -> {\n+                                                                      if (ref != null) {\n+                                                                          return streamMetadataStore.deleteStreamCutBefore(scope, stream, ref, context, executor);\n+                                                                      } else {\n+                                                                          return CompletableFuture.completedFuture(null);\n+                                                                      }\n+                                                                  });\n+                                    } else {\n+                                        throw new RuntimeException(\"Could not start truncation\");\n+                                    }\n+                                });\n+                    });\n+                });\n+    }\n+\n+    private CompletableFuture<Map<Long, Long>> getTruncationStreamCutBySizeLimit(String scope, String stream, OperationContext context, RetentionPolicy policy,\n+                                                                                 RetentionSet retentionSet, Map<Long, Long> lowerBound, StreamCutRecord newRecord) {\n+        // if the lowerbound on subscribers streamcuts satisfies the policy size bound, then return it. \n+        // else return the stream cut that satisfies maximum bound on size. \n+        // 1. if lowerbound.size < max and lowerbound.size > min truncate at lowerbound\n+        // 2. if lowerbound.size < min, truncate at max irrespective of if lowerbound overlaps with max or not. \n+        // 3. if lowerbound.size > max, truncate at max\n+        long currentSize = newRecord != null ? newRecord.getRecordingSize() : retentionSet.getLatest().getRecordingSize();\n+        return streamMetadataStore.getSizeTillStreamCut(scope, stream, lowerBound, Optional.empty(), context, executor)\n+                          .thenCompose(sizeTill -> {\n+                               long retainedSize = currentSize - sizeTill;\n+                               Supplier<Optional<StreamCutReferenceRecord>> maxBound = () -> retentionSet\n+                                      .getRetentionRecords().stream()\n+                                      .filter(x -> currentSize - x.getRecordingSize() > policy.getConsumptionLimits().getMaxValue())\n+                                      .max(Comparator.comparingLong(StreamCutReferenceRecord::getRecordingTime));\n+\n+                               // if retainedSize is less than min size then do not truncate the stream. \n+                               if (retainedSize < policy.getConsumptionLimits().getMinValue()) {\n+                                   return maxBound.get().map(x -> streamMetadataStore.getStreamCutRecord(scope, stream, x, context, executor)\n+                                                                               .thenApply(StreamCutRecord::getStreamCut))\n+                                                  .orElse(CompletableFuture.completedFuture(null));\n+                               } else {\n+                                   // if retained size is less than max allowed, then truncate the stream at subscriber lower bound. \n+                                   if (retainedSize < policy.getConsumptionLimits().getMaxValue()) {\n+                                       return CompletableFuture.completedFuture(lowerBound);\n+                                   } else {\n+                                       // if retained size is greater than max allowed, then truncate the stream at streamcut\n+                                       // from retention set that matches the retention policy size bound. \n+                                       return maxBound.get().map(x -> streamMetadataStore.getStreamCutRecord(scope, stream, x, context, executor)\n+                                                   .thenApply(StreamCutRecord::getStreamCut)\n+                                                   .thenCompose(maxRecord -> {\n+                                                       // if max record is strictly greater than lowerbound then we can truncate at max record\n+                                                       return streamMetadataStore.streamCutStrictlyGreaterThan(\n+                                                               scope, stream, maxRecord, lowerBound, context, executor)\n+                                                                                 .thenApply(gt -> {\n+                                                                                     if (gt) {\n+                                                                                         return maxRecord;\n+                                                                                     } else {\n+                                                                                         return lowerBound;\n+                                                                                     }\n+                                                                                 });\n+                                                   })).orElse(CompletableFuture.completedFuture(null));   \n+                                   }\n+                               }\n+                           });\n+    }\n+\n+    private CompletableFuture<Map<Long, Long>> getTruncationStreamCutByTimeLimit(String scope, String stream, OperationContext context,\n+                                                                                 RetentionPolicy policy, RetentionSet retentionSet,\n+                                                                                 Map<Long, Long> lowerBound, StreamCutRecord latest) {\n+        Map.Entry<StreamCutReferenceRecord, StreamCutReferenceRecord> limits =\n+                getBoundStreamCuts(policy.getConsumptionLimits(), retentionSet);\n+        // if subscriber lowerbound is ahead of streamcut corresponding to the max time and is behind stream cut for min time \n+        // from the retention set then we can safely truncate at lowerbound. Else we will truncate at the max time bound if it\n+        // exists\n+        // 1. if LB > min => truncate at min\n+        // 2. if LB < max => truncate at max\n+        // 3. if LB < min && LB > max => truncate at LB\n+        // 4. if LB < min && overlaps max => truncate at LB\n+        CompletableFuture<StreamCutRecord> limitMaxFuture = limits.getKey() == null ? CompletableFuture.completedFuture(null) :\n+                streamMetadataStore.getStreamCutRecord(scope, stream, limits.getKey(), context, executor);\n+        CompletableFuture<StreamCutRecord> limitMinFuture = limits.getValue() == null ? CompletableFuture.completedFuture(null) :\n+                streamMetadataStore.getStreamCutRecord(scope, stream, limits.getValue(), context, executor);\n+        return CompletableFuture.allOf(limitMaxFuture, limitMinFuture)\n+                         .thenCompose(v -> {\n+                             StreamCutRecord limitMax = limitMaxFuture.join();\n+                             StreamCutRecord limitMin = limitMinFuture.join();\n+                             if (limitMin != null) {\n+                                 return streamMetadataStore.streamCutStrictlyGreaterThan(scope, stream, limitMin.getStreamCut(), lowerBound, context, executor)\n+                                                    .thenCompose(gtMin -> {\n+                                                        if (gtMin) {\n+                                                            if (limitMax == null) {\n+                                                                return CompletableFuture.completedFuture(lowerBound);\n+                                                            } else {\n+                                                                return streamMetadataStore.streamCutStrictlyGreaterThan(scope, stream, limitMax.getStreamCut(), lowerBound, context, executor)\n+                                                                                          .thenApply(gtMax -> gtMax ? limitMax.getStreamCut() : lowerBound);\n+                                                            }\n+                                                        } else {\n+                                                            return streamMetadataStore.streamCutStrictlyGreaterThan(scope, stream, lowerBound, limitMin.getStreamCut(), context, executor)\n+                                                                               .thenApply(gt -> gt ? limitMin.getStreamCut() : null);\n+                                                        }\n+                                                    });\n+                             } else {\n+                                 // if min limit is 0 then latest effectively becomes the min. and we truncate at the lowerbound.\n+                                 if (latest != null && policy.getConsumptionLimits().getMinValue() == 0L) {\n+                                     // truncate at the lower bound \n+                                     return CompletableFuture.completedFuture(lowerBound);\n+                                 } else {\n+                                     return CompletableFuture.completedFuture(null);\n+                                 }\n+                             }\n+                         });\n+    }\n+\n+    private Map<Long, Long> computeSubscribersLowerBound(List<Map<StreamSegmentRecord, Long>> subscribers) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDkxMTQ5OA=="}, "originalCommit": {"oid": "07c2838e170b5ac7b040ca02680ca3ab32983ab5"}, "originalPosition": 244}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTY0ODg4OQ==", "bodyText": "created an issue to tackle it later. #5349", "url": "https://github.com/pravega/pravega/pull/5289#discussion_r525648889", "createdAt": "2020-11-18T02:10:43Z", "author": {"login": "shiveshr"}, "path": "controller/src/main/java/io/pravega/controller/task/Stream/StreamMetadataTasks.java", "diffHunk": "@@ -507,32 +514,296 @@ public void initializeStreamWriters(final EventStreamClientFactory clientFactory\n \n     private CompletableFuture<Void> truncate(String scope, String stream, RetentionPolicy policy, OperationContext context,\n                                              RetentionSet retentionSet, StreamCutRecord newRecord, long recordingTime, long requestId) {\n-        Optional<StreamCutReferenceRecord> truncationRecord = findTruncationRecord(policy, retentionSet, newRecord, recordingTime);\n-        if (!truncationRecord.isPresent()) {\n-            log.info(\"No suitable truncation record found, per retention policy for stream {}/{}\", scope, stream);\n-            return CompletableFuture.completedFuture(null);\n+        if (policy.getRetentionType().equals(RetentionPolicy.RetentionType.CONSUMPTION)) {\n+            return subscriberBasedTruncation(scope, stream, context, policy, retentionSet, newRecord, requestId);\n+        } else {\n+            Optional<StreamCutReferenceRecord> truncationRecord = findTruncationRecord(policy, retentionSet, newRecord, recordingTime);\n+            if (!truncationRecord.isPresent()) {\n+                log.info(\"No suitable truncation record found, per retention policy for stream {}/{}\", scope, stream);\n+                return CompletableFuture.completedFuture(null);\n+            }\n+            log.info(\"Found truncation record for stream {}/{} truncationRecord time/size: {}/{}\", scope, stream,\n+                    truncationRecord.get().getRecordingTime(), truncationRecord.get().getRecordingSize());\n+\n+            return truncate(scope, stream, context, requestId, truncationRecord.get());\n         }\n+    }\n+\n+    private CompletableFuture<Void> truncate(String scope, String stream, OperationContext context, long requestId,\n+                                                   StreamCutReferenceRecord truncationRecord) {\n         log.info(\"Found truncation record for stream {}/{} truncationRecord time/size: {}/{}\", scope, stream,\n-                truncationRecord.get().getRecordingTime(), truncationRecord.get().getRecordingSize());\n-        return streamMetadataStore.getStreamCutRecord(scope, stream, truncationRecord.get(), context, executor)\n-                   .thenCompose(streamCutRecord -> startTruncation(scope, stream, streamCutRecord.getStreamCut(), context, requestId))\n-                   .thenCompose(started -> {\n-                       if (started) {\n-                                return streamMetadataStore.deleteStreamCutBefore(scope, stream, truncationRecord.get(), context, executor);\n+                truncationRecord.getRecordingTime(), truncationRecord.getRecordingSize());\n+        return streamMetadataStore.getStreamCutRecord(scope, stream, truncationRecord, context, executor)\n+                                  .thenCompose(streamCutRecord -> startTruncation(scope, stream, streamCutRecord.getStreamCut(), context, requestId))\n+                                  .thenCompose(started -> {\n+                                      if (started) {\n+                                          return streamMetadataStore.deleteStreamCutBefore(scope, stream, truncationRecord, context, executor);\n+                                      } else {\n+                                          throw new RuntimeException(\"Could not start truncation\");\n+                                      }\n+                                  })\n+                                  .exceptionally(e -> {\n+                                      if (Exceptions.unwrap(e) instanceof IllegalArgumentException) {\n+                                          // This is ignorable exception. Throwing this will cause unnecessary retries and exceptions logged.\n+                                          log.debug(requestId, \"Cannot truncate at given \" +\n+                                                  \"streamCut because it intersects with existing truncation point\");\n+                                          return null;\n+                                      } else {\n+                                          throw new CompletionException(e);\n+                                      }\n+                                  });\n+    }\n+\n+    private CompletableFuture<Void> subscriberBasedTruncation(String scope, String stream, OperationContext context,\n+                                                              RetentionPolicy policy, RetentionSet retentionSet, \n+                                                              StreamCutRecord newRecord, long requestId) {\n+        return streamMetadataStore.listSubscribers(scope, stream, context, executor)\n+                           .thenCompose(list -> Futures.allOfWithResults(list.stream().map(x -> \n+                                   streamMetadataStore.getSubscriber(scope, stream, x, context, executor)).collect(Collectors.toList())))\n+                           .thenCompose(subscribers -> {\n+                               // convert all streamcuts to include the segment range\n+                               return Futures.allOfWithResults(subscribers.stream().map(x -> {\n+                                   ImmutableSet<Map.Entry<Long, Long>> entries = x.getObject().getTruncationStreamCut().entrySet();\n+\n+                                   return Futures.keysAllOfWithResults(entries.stream().collect(Collectors.toMap(\n+                                           y -> streamMetadataStore.getSegment(scope, stream, y.getKey(), context, executor), Map.Entry::getValue)));\n+                               }).collect(Collectors.toList()));\n+                           })\n+                           .thenApply(this::computeSubscribersLowerBound)\n+                .thenCompose(lowerBound -> {\n+                    CompletableFuture<Map<Long, Long>> toTruncateAt; \n+                    if (policy.getConsumptionLimits().getType().equals(RetentionPolicy.ConsumptionLimits.Type.SIZE_BYTES)) {\n+                        toTruncateAt = getTruncationStreamCutBySizeLimit(scope, stream, context, policy, retentionSet, lowerBound, newRecord);\n+                    } else {\n+                        toTruncateAt = getTruncationStreamCutByTimeLimit(scope, stream, context, policy, retentionSet, lowerBound, newRecord);\n+                    }\n+                    return toTruncateAt.thenCompose(truncationStreamCut -> {\n+                        if (truncationStreamCut == null || truncationStreamCut.isEmpty()) {\n+                            log.debug(\"no truncation record could be compute that satisfied retention policy\");\n+                            return CompletableFuture.completedFuture(null);\n+                        } \n+                        return startTruncation(scope, stream, truncationStreamCut, context, requestId)\n+                                .thenCompose(started -> {\n+                                    if (started) {\n+                                        return streamMetadataStore.findStreamCutReferenceRecordBefore(scope, stream, truncationStreamCut, retentionSet, context, executor)\n+                                                                  .thenCompose(ref -> {\n+                                                                      if (ref != null) {\n+                                                                          return streamMetadataStore.deleteStreamCutBefore(scope, stream, ref, context, executor);\n+                                                                      } else {\n+                                                                          return CompletableFuture.completedFuture(null);\n+                                                                      }\n+                                                                  });\n+                                    } else {\n+                                        throw new RuntimeException(\"Could not start truncation\");\n+                                    }\n+                                });\n+                    });\n+                });\n+    }\n+\n+    private CompletableFuture<Map<Long, Long>> getTruncationStreamCutBySizeLimit(String scope, String stream, OperationContext context, RetentionPolicy policy,\n+                                                                                 RetentionSet retentionSet, Map<Long, Long> lowerBound, StreamCutRecord newRecord) {\n+        // if the lowerbound on subscribers streamcuts satisfies the policy size bound, then return it. \n+        // else return the stream cut that satisfies maximum bound on size. \n+        // 1. if lowerbound.size < max and lowerbound.size > min truncate at lowerbound\n+        // 2. if lowerbound.size < min, truncate at max irrespective of if lowerbound overlaps with max or not. \n+        // 3. if lowerbound.size > max, truncate at max\n+        long currentSize = newRecord != null ? newRecord.getRecordingSize() : retentionSet.getLatest().getRecordingSize();\n+        return streamMetadataStore.getSizeTillStreamCut(scope, stream, lowerBound, Optional.empty(), context, executor)\n+                          .thenCompose(sizeTill -> {\n+                               long retainedSize = currentSize - sizeTill;\n+                               Supplier<Optional<StreamCutReferenceRecord>> maxBound = () -> retentionSet\n+                                      .getRetentionRecords().stream()\n+                                      .filter(x -> currentSize - x.getRecordingSize() > policy.getConsumptionLimits().getMaxValue())\n+                                      .max(Comparator.comparingLong(StreamCutReferenceRecord::getRecordingTime));\n+\n+                               // if retainedSize is less than min size then do not truncate the stream. \n+                               if (retainedSize < policy.getConsumptionLimits().getMinValue()) {\n+                                   return maxBound.get().map(x -> streamMetadataStore.getStreamCutRecord(scope, stream, x, context, executor)\n+                                                                               .thenApply(StreamCutRecord::getStreamCut))\n+                                                  .orElse(CompletableFuture.completedFuture(null));\n+                               } else {\n+                                   // if retained size is less than max allowed, then truncate the stream at subscriber lower bound. \n+                                   if (retainedSize < policy.getConsumptionLimits().getMaxValue()) {\n+                                       return CompletableFuture.completedFuture(lowerBound);\n+                                   } else {\n+                                       // if retained size is greater than max allowed, then truncate the stream at streamcut\n+                                       // from retention set that matches the retention policy size bound. \n+                                       return maxBound.get().map(x -> streamMetadataStore.getStreamCutRecord(scope, stream, x, context, executor)\n+                                                   .thenApply(StreamCutRecord::getStreamCut)\n+                                                   .thenCompose(maxRecord -> {\n+                                                       // if max record is strictly greater than lowerbound then we can truncate at max record\n+                                                       return streamMetadataStore.streamCutStrictlyGreaterThan(\n+                                                               scope, stream, maxRecord, lowerBound, context, executor)\n+                                                                                 .thenApply(gt -> {\n+                                                                                     if (gt) {\n+                                                                                         return maxRecord;\n+                                                                                     } else {\n+                                                                                         return lowerBound;\n+                                                                                     }\n+                                                                                 });\n+                                                   })).orElse(CompletableFuture.completedFuture(null));   \n+                                   }\n+                               }\n+                           });\n+    }\n+\n+    private CompletableFuture<Map<Long, Long>> getTruncationStreamCutByTimeLimit(String scope, String stream, OperationContext context,\n+                                                                                 RetentionPolicy policy, RetentionSet retentionSet,\n+                                                                                 Map<Long, Long> lowerBound, StreamCutRecord latest) {\n+        Map.Entry<StreamCutReferenceRecord, StreamCutReferenceRecord> limits =\n+                getBoundStreamCuts(policy.getConsumptionLimits(), retentionSet);\n+        // if subscriber lowerbound is ahead of streamcut corresponding to the max time and is behind stream cut for min time \n+        // from the retention set then we can safely truncate at lowerbound. Else we will truncate at the max time bound if it\n+        // exists\n+        // 1. if LB > min => truncate at min\n+        // 2. if LB < max => truncate at max\n+        // 3. if LB < min && LB > max => truncate at LB\n+        // 4. if LB < min && overlaps max => truncate at LB\n+        CompletableFuture<StreamCutRecord> limitMaxFuture = limits.getKey() == null ? CompletableFuture.completedFuture(null) :\n+                streamMetadataStore.getStreamCutRecord(scope, stream, limits.getKey(), context, executor);\n+        CompletableFuture<StreamCutRecord> limitMinFuture = limits.getValue() == null ? CompletableFuture.completedFuture(null) :\n+                streamMetadataStore.getStreamCutRecord(scope, stream, limits.getValue(), context, executor);\n+        return CompletableFuture.allOf(limitMaxFuture, limitMinFuture)\n+                         .thenCompose(v -> {\n+                             StreamCutRecord limitMax = limitMaxFuture.join();\n+                             StreamCutRecord limitMin = limitMinFuture.join();\n+                             if (limitMin != null) {\n+                                 return streamMetadataStore.streamCutStrictlyGreaterThan(scope, stream, limitMin.getStreamCut(), lowerBound, context, executor)\n+                                                    .thenCompose(gtMin -> {\n+                                                        if (gtMin) {\n+                                                            if (limitMax == null) {\n+                                                                return CompletableFuture.completedFuture(lowerBound);\n+                                                            } else {\n+                                                                return streamMetadataStore.streamCutStrictlyGreaterThan(scope, stream, limitMax.getStreamCut(), lowerBound, context, executor)\n+                                                                                          .thenApply(gtMax -> gtMax ? limitMax.getStreamCut() : lowerBound);\n+                                                            }\n+                                                        } else {\n+                                                            return streamMetadataStore.streamCutStrictlyGreaterThan(scope, stream, lowerBound, limitMin.getStreamCut(), context, executor)\n+                                                                               .thenApply(gt -> gt ? limitMin.getStreamCut() : null);\n+                                                        }\n+                                                    });\n+                             } else {\n+                                 // if min limit is 0 then latest effectively becomes the min. and we truncate at the lowerbound.\n+                                 if (latest != null && policy.getConsumptionLimits().getMinValue() == 0L) {\n+                                     // truncate at the lower bound \n+                                     return CompletableFuture.completedFuture(lowerBound);\n+                                 } else {\n+                                     return CompletableFuture.completedFuture(null);\n+                                 }\n+                             }\n+                         });\n+    }\n+\n+    private Map<Long, Long> computeSubscribersLowerBound(List<Map<StreamSegmentRecord, Long>> subscribers) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDkxMTQ5OA=="}, "originalCommit": {"oid": "07c2838e170b5ac7b040ca02680ca3ab32983ab5"}, "originalPosition": 244}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTgwMzE5Nw==", "bodyText": "It should be a small change. I think we should do it as part of this PR itself.", "url": "https://github.com/pravega/pravega/pull/5289#discussion_r525803197", "createdAt": "2020-11-18T04:43:36Z", "author": {"login": "pbelgundi"}, "path": "controller/src/main/java/io/pravega/controller/task/Stream/StreamMetadataTasks.java", "diffHunk": "@@ -507,32 +514,296 @@ public void initializeStreamWriters(final EventStreamClientFactory clientFactory\n \n     private CompletableFuture<Void> truncate(String scope, String stream, RetentionPolicy policy, OperationContext context,\n                                              RetentionSet retentionSet, StreamCutRecord newRecord, long recordingTime, long requestId) {\n-        Optional<StreamCutReferenceRecord> truncationRecord = findTruncationRecord(policy, retentionSet, newRecord, recordingTime);\n-        if (!truncationRecord.isPresent()) {\n-            log.info(\"No suitable truncation record found, per retention policy for stream {}/{}\", scope, stream);\n-            return CompletableFuture.completedFuture(null);\n+        if (policy.getRetentionType().equals(RetentionPolicy.RetentionType.CONSUMPTION)) {\n+            return subscriberBasedTruncation(scope, stream, context, policy, retentionSet, newRecord, requestId);\n+        } else {\n+            Optional<StreamCutReferenceRecord> truncationRecord = findTruncationRecord(policy, retentionSet, newRecord, recordingTime);\n+            if (!truncationRecord.isPresent()) {\n+                log.info(\"No suitable truncation record found, per retention policy for stream {}/{}\", scope, stream);\n+                return CompletableFuture.completedFuture(null);\n+            }\n+            log.info(\"Found truncation record for stream {}/{} truncationRecord time/size: {}/{}\", scope, stream,\n+                    truncationRecord.get().getRecordingTime(), truncationRecord.get().getRecordingSize());\n+\n+            return truncate(scope, stream, context, requestId, truncationRecord.get());\n         }\n+    }\n+\n+    private CompletableFuture<Void> truncate(String scope, String stream, OperationContext context, long requestId,\n+                                                   StreamCutReferenceRecord truncationRecord) {\n         log.info(\"Found truncation record for stream {}/{} truncationRecord time/size: {}/{}\", scope, stream,\n-                truncationRecord.get().getRecordingTime(), truncationRecord.get().getRecordingSize());\n-        return streamMetadataStore.getStreamCutRecord(scope, stream, truncationRecord.get(), context, executor)\n-                   .thenCompose(streamCutRecord -> startTruncation(scope, stream, streamCutRecord.getStreamCut(), context, requestId))\n-                   .thenCompose(started -> {\n-                       if (started) {\n-                                return streamMetadataStore.deleteStreamCutBefore(scope, stream, truncationRecord.get(), context, executor);\n+                truncationRecord.getRecordingTime(), truncationRecord.getRecordingSize());\n+        return streamMetadataStore.getStreamCutRecord(scope, stream, truncationRecord, context, executor)\n+                                  .thenCompose(streamCutRecord -> startTruncation(scope, stream, streamCutRecord.getStreamCut(), context, requestId))\n+                                  .thenCompose(started -> {\n+                                      if (started) {\n+                                          return streamMetadataStore.deleteStreamCutBefore(scope, stream, truncationRecord, context, executor);\n+                                      } else {\n+                                          throw new RuntimeException(\"Could not start truncation\");\n+                                      }\n+                                  })\n+                                  .exceptionally(e -> {\n+                                      if (Exceptions.unwrap(e) instanceof IllegalArgumentException) {\n+                                          // This is ignorable exception. Throwing this will cause unnecessary retries and exceptions logged.\n+                                          log.debug(requestId, \"Cannot truncate at given \" +\n+                                                  \"streamCut because it intersects with existing truncation point\");\n+                                          return null;\n+                                      } else {\n+                                          throw new CompletionException(e);\n+                                      }\n+                                  });\n+    }\n+\n+    private CompletableFuture<Void> subscriberBasedTruncation(String scope, String stream, OperationContext context,\n+                                                              RetentionPolicy policy, RetentionSet retentionSet, \n+                                                              StreamCutRecord newRecord, long requestId) {\n+        return streamMetadataStore.listSubscribers(scope, stream, context, executor)\n+                           .thenCompose(list -> Futures.allOfWithResults(list.stream().map(x -> \n+                                   streamMetadataStore.getSubscriber(scope, stream, x, context, executor)).collect(Collectors.toList())))\n+                           .thenCompose(subscribers -> {\n+                               // convert all streamcuts to include the segment range\n+                               return Futures.allOfWithResults(subscribers.stream().map(x -> {\n+                                   ImmutableSet<Map.Entry<Long, Long>> entries = x.getObject().getTruncationStreamCut().entrySet();\n+\n+                                   return Futures.keysAllOfWithResults(entries.stream().collect(Collectors.toMap(\n+                                           y -> streamMetadataStore.getSegment(scope, stream, y.getKey(), context, executor), Map.Entry::getValue)));\n+                               }).collect(Collectors.toList()));\n+                           })\n+                           .thenApply(this::computeSubscribersLowerBound)\n+                .thenCompose(lowerBound -> {\n+                    CompletableFuture<Map<Long, Long>> toTruncateAt; \n+                    if (policy.getConsumptionLimits().getType().equals(RetentionPolicy.ConsumptionLimits.Type.SIZE_BYTES)) {\n+                        toTruncateAt = getTruncationStreamCutBySizeLimit(scope, stream, context, policy, retentionSet, lowerBound, newRecord);\n+                    } else {\n+                        toTruncateAt = getTruncationStreamCutByTimeLimit(scope, stream, context, policy, retentionSet, lowerBound, newRecord);\n+                    }\n+                    return toTruncateAt.thenCompose(truncationStreamCut -> {\n+                        if (truncationStreamCut == null || truncationStreamCut.isEmpty()) {\n+                            log.debug(\"no truncation record could be compute that satisfied retention policy\");\n+                            return CompletableFuture.completedFuture(null);\n+                        } \n+                        return startTruncation(scope, stream, truncationStreamCut, context, requestId)\n+                                .thenCompose(started -> {\n+                                    if (started) {\n+                                        return streamMetadataStore.findStreamCutReferenceRecordBefore(scope, stream, truncationStreamCut, retentionSet, context, executor)\n+                                                                  .thenCompose(ref -> {\n+                                                                      if (ref != null) {\n+                                                                          return streamMetadataStore.deleteStreamCutBefore(scope, stream, ref, context, executor);\n+                                                                      } else {\n+                                                                          return CompletableFuture.completedFuture(null);\n+                                                                      }\n+                                                                  });\n+                                    } else {\n+                                        throw new RuntimeException(\"Could not start truncation\");\n+                                    }\n+                                });\n+                    });\n+                });\n+    }\n+\n+    private CompletableFuture<Map<Long, Long>> getTruncationStreamCutBySizeLimit(String scope, String stream, OperationContext context, RetentionPolicy policy,\n+                                                                                 RetentionSet retentionSet, Map<Long, Long> lowerBound, StreamCutRecord newRecord) {\n+        // if the lowerbound on subscribers streamcuts satisfies the policy size bound, then return it. \n+        // else return the stream cut that satisfies maximum bound on size. \n+        // 1. if lowerbound.size < max and lowerbound.size > min truncate at lowerbound\n+        // 2. if lowerbound.size < min, truncate at max irrespective of if lowerbound overlaps with max or not. \n+        // 3. if lowerbound.size > max, truncate at max\n+        long currentSize = newRecord != null ? newRecord.getRecordingSize() : retentionSet.getLatest().getRecordingSize();\n+        return streamMetadataStore.getSizeTillStreamCut(scope, stream, lowerBound, Optional.empty(), context, executor)\n+                          .thenCompose(sizeTill -> {\n+                               long retainedSize = currentSize - sizeTill;\n+                               Supplier<Optional<StreamCutReferenceRecord>> maxBound = () -> retentionSet\n+                                      .getRetentionRecords().stream()\n+                                      .filter(x -> currentSize - x.getRecordingSize() > policy.getConsumptionLimits().getMaxValue())\n+                                      .max(Comparator.comparingLong(StreamCutReferenceRecord::getRecordingTime));\n+\n+                               // if retainedSize is less than min size then do not truncate the stream. \n+                               if (retainedSize < policy.getConsumptionLimits().getMinValue()) {\n+                                   return maxBound.get().map(x -> streamMetadataStore.getStreamCutRecord(scope, stream, x, context, executor)\n+                                                                               .thenApply(StreamCutRecord::getStreamCut))\n+                                                  .orElse(CompletableFuture.completedFuture(null));\n+                               } else {\n+                                   // if retained size is less than max allowed, then truncate the stream at subscriber lower bound. \n+                                   if (retainedSize < policy.getConsumptionLimits().getMaxValue()) {\n+                                       return CompletableFuture.completedFuture(lowerBound);\n+                                   } else {\n+                                       // if retained size is greater than max allowed, then truncate the stream at streamcut\n+                                       // from retention set that matches the retention policy size bound. \n+                                       return maxBound.get().map(x -> streamMetadataStore.getStreamCutRecord(scope, stream, x, context, executor)\n+                                                   .thenApply(StreamCutRecord::getStreamCut)\n+                                                   .thenCompose(maxRecord -> {\n+                                                       // if max record is strictly greater than lowerbound then we can truncate at max record\n+                                                       return streamMetadataStore.streamCutStrictlyGreaterThan(\n+                                                               scope, stream, maxRecord, lowerBound, context, executor)\n+                                                                                 .thenApply(gt -> {\n+                                                                                     if (gt) {\n+                                                                                         return maxRecord;\n+                                                                                     } else {\n+                                                                                         return lowerBound;\n+                                                                                     }\n+                                                                                 });\n+                                                   })).orElse(CompletableFuture.completedFuture(null));   \n+                                   }\n+                               }\n+                           });\n+    }\n+\n+    private CompletableFuture<Map<Long, Long>> getTruncationStreamCutByTimeLimit(String scope, String stream, OperationContext context,\n+                                                                                 RetentionPolicy policy, RetentionSet retentionSet,\n+                                                                                 Map<Long, Long> lowerBound, StreamCutRecord latest) {\n+        Map.Entry<StreamCutReferenceRecord, StreamCutReferenceRecord> limits =\n+                getBoundStreamCuts(policy.getConsumptionLimits(), retentionSet);\n+        // if subscriber lowerbound is ahead of streamcut corresponding to the max time and is behind stream cut for min time \n+        // from the retention set then we can safely truncate at lowerbound. Else we will truncate at the max time bound if it\n+        // exists\n+        // 1. if LB > min => truncate at min\n+        // 2. if LB < max => truncate at max\n+        // 3. if LB < min && LB > max => truncate at LB\n+        // 4. if LB < min && overlaps max => truncate at LB\n+        CompletableFuture<StreamCutRecord> limitMaxFuture = limits.getKey() == null ? CompletableFuture.completedFuture(null) :\n+                streamMetadataStore.getStreamCutRecord(scope, stream, limits.getKey(), context, executor);\n+        CompletableFuture<StreamCutRecord> limitMinFuture = limits.getValue() == null ? CompletableFuture.completedFuture(null) :\n+                streamMetadataStore.getStreamCutRecord(scope, stream, limits.getValue(), context, executor);\n+        return CompletableFuture.allOf(limitMaxFuture, limitMinFuture)\n+                         .thenCompose(v -> {\n+                             StreamCutRecord limitMax = limitMaxFuture.join();\n+                             StreamCutRecord limitMin = limitMinFuture.join();\n+                             if (limitMin != null) {\n+                                 return streamMetadataStore.streamCutStrictlyGreaterThan(scope, stream, limitMin.getStreamCut(), lowerBound, context, executor)\n+                                                    .thenCompose(gtMin -> {\n+                                                        if (gtMin) {\n+                                                            if (limitMax == null) {\n+                                                                return CompletableFuture.completedFuture(lowerBound);\n+                                                            } else {\n+                                                                return streamMetadataStore.streamCutStrictlyGreaterThan(scope, stream, limitMax.getStreamCut(), lowerBound, context, executor)\n+                                                                                          .thenApply(gtMax -> gtMax ? limitMax.getStreamCut() : lowerBound);\n+                                                            }\n+                                                        } else {\n+                                                            return streamMetadataStore.streamCutStrictlyGreaterThan(scope, stream, lowerBound, limitMin.getStreamCut(), context, executor)\n+                                                                               .thenApply(gt -> gt ? limitMin.getStreamCut() : null);\n+                                                        }\n+                                                    });\n+                             } else {\n+                                 // if min limit is 0 then latest effectively becomes the min. and we truncate at the lowerbound.\n+                                 if (latest != null && policy.getConsumptionLimits().getMinValue() == 0L) {\n+                                     // truncate at the lower bound \n+                                     return CompletableFuture.completedFuture(lowerBound);\n+                                 } else {\n+                                     return CompletableFuture.completedFuture(null);\n+                                 }\n+                             }\n+                         });\n+    }\n+\n+    private Map<Long, Long> computeSubscribersLowerBound(List<Map<StreamSegmentRecord, Long>> subscribers) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDkxMTQ5OA=="}, "originalCommit": {"oid": "07c2838e170b5ac7b040ca02680ca3ab32983ab5"}, "originalPosition": 244}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTgxNzk4Mw==", "bodyText": "the reason is \"lowerbound\" is Map<Long, Long> while subscriber's streamcut is Map<StreamSegmentRecord, Long> so there is one pass needed to convert even the single subscriber to Map<Long, Long>.\nso the suggested optimization will not yield anything.\nwhat we can do instead is optimize the code a bit to not have to deal with these different types of maps but unify it. hence i will address this and other optimization in a separate PR.", "url": "https://github.com/pravega/pravega/pull/5289#discussion_r525817983", "createdAt": "2020-11-18T05:08:19Z", "author": {"login": "shiveshr"}, "path": "controller/src/main/java/io/pravega/controller/task/Stream/StreamMetadataTasks.java", "diffHunk": "@@ -507,32 +514,296 @@ public void initializeStreamWriters(final EventStreamClientFactory clientFactory\n \n     private CompletableFuture<Void> truncate(String scope, String stream, RetentionPolicy policy, OperationContext context,\n                                              RetentionSet retentionSet, StreamCutRecord newRecord, long recordingTime, long requestId) {\n-        Optional<StreamCutReferenceRecord> truncationRecord = findTruncationRecord(policy, retentionSet, newRecord, recordingTime);\n-        if (!truncationRecord.isPresent()) {\n-            log.info(\"No suitable truncation record found, per retention policy for stream {}/{}\", scope, stream);\n-            return CompletableFuture.completedFuture(null);\n+        if (policy.getRetentionType().equals(RetentionPolicy.RetentionType.CONSUMPTION)) {\n+            return subscriberBasedTruncation(scope, stream, context, policy, retentionSet, newRecord, requestId);\n+        } else {\n+            Optional<StreamCutReferenceRecord> truncationRecord = findTruncationRecord(policy, retentionSet, newRecord, recordingTime);\n+            if (!truncationRecord.isPresent()) {\n+                log.info(\"No suitable truncation record found, per retention policy for stream {}/{}\", scope, stream);\n+                return CompletableFuture.completedFuture(null);\n+            }\n+            log.info(\"Found truncation record for stream {}/{} truncationRecord time/size: {}/{}\", scope, stream,\n+                    truncationRecord.get().getRecordingTime(), truncationRecord.get().getRecordingSize());\n+\n+            return truncate(scope, stream, context, requestId, truncationRecord.get());\n         }\n+    }\n+\n+    private CompletableFuture<Void> truncate(String scope, String stream, OperationContext context, long requestId,\n+                                                   StreamCutReferenceRecord truncationRecord) {\n         log.info(\"Found truncation record for stream {}/{} truncationRecord time/size: {}/{}\", scope, stream,\n-                truncationRecord.get().getRecordingTime(), truncationRecord.get().getRecordingSize());\n-        return streamMetadataStore.getStreamCutRecord(scope, stream, truncationRecord.get(), context, executor)\n-                   .thenCompose(streamCutRecord -> startTruncation(scope, stream, streamCutRecord.getStreamCut(), context, requestId))\n-                   .thenCompose(started -> {\n-                       if (started) {\n-                                return streamMetadataStore.deleteStreamCutBefore(scope, stream, truncationRecord.get(), context, executor);\n+                truncationRecord.getRecordingTime(), truncationRecord.getRecordingSize());\n+        return streamMetadataStore.getStreamCutRecord(scope, stream, truncationRecord, context, executor)\n+                                  .thenCompose(streamCutRecord -> startTruncation(scope, stream, streamCutRecord.getStreamCut(), context, requestId))\n+                                  .thenCompose(started -> {\n+                                      if (started) {\n+                                          return streamMetadataStore.deleteStreamCutBefore(scope, stream, truncationRecord, context, executor);\n+                                      } else {\n+                                          throw new RuntimeException(\"Could not start truncation\");\n+                                      }\n+                                  })\n+                                  .exceptionally(e -> {\n+                                      if (Exceptions.unwrap(e) instanceof IllegalArgumentException) {\n+                                          // This is ignorable exception. Throwing this will cause unnecessary retries and exceptions logged.\n+                                          log.debug(requestId, \"Cannot truncate at given \" +\n+                                                  \"streamCut because it intersects with existing truncation point\");\n+                                          return null;\n+                                      } else {\n+                                          throw new CompletionException(e);\n+                                      }\n+                                  });\n+    }\n+\n+    private CompletableFuture<Void> subscriberBasedTruncation(String scope, String stream, OperationContext context,\n+                                                              RetentionPolicy policy, RetentionSet retentionSet, \n+                                                              StreamCutRecord newRecord, long requestId) {\n+        return streamMetadataStore.listSubscribers(scope, stream, context, executor)\n+                           .thenCompose(list -> Futures.allOfWithResults(list.stream().map(x -> \n+                                   streamMetadataStore.getSubscriber(scope, stream, x, context, executor)).collect(Collectors.toList())))\n+                           .thenCompose(subscribers -> {\n+                               // convert all streamcuts to include the segment range\n+                               return Futures.allOfWithResults(subscribers.stream().map(x -> {\n+                                   ImmutableSet<Map.Entry<Long, Long>> entries = x.getObject().getTruncationStreamCut().entrySet();\n+\n+                                   return Futures.keysAllOfWithResults(entries.stream().collect(Collectors.toMap(\n+                                           y -> streamMetadataStore.getSegment(scope, stream, y.getKey(), context, executor), Map.Entry::getValue)));\n+                               }).collect(Collectors.toList()));\n+                           })\n+                           .thenApply(this::computeSubscribersLowerBound)\n+                .thenCompose(lowerBound -> {\n+                    CompletableFuture<Map<Long, Long>> toTruncateAt; \n+                    if (policy.getConsumptionLimits().getType().equals(RetentionPolicy.ConsumptionLimits.Type.SIZE_BYTES)) {\n+                        toTruncateAt = getTruncationStreamCutBySizeLimit(scope, stream, context, policy, retentionSet, lowerBound, newRecord);\n+                    } else {\n+                        toTruncateAt = getTruncationStreamCutByTimeLimit(scope, stream, context, policy, retentionSet, lowerBound, newRecord);\n+                    }\n+                    return toTruncateAt.thenCompose(truncationStreamCut -> {\n+                        if (truncationStreamCut == null || truncationStreamCut.isEmpty()) {\n+                            log.debug(\"no truncation record could be compute that satisfied retention policy\");\n+                            return CompletableFuture.completedFuture(null);\n+                        } \n+                        return startTruncation(scope, stream, truncationStreamCut, context, requestId)\n+                                .thenCompose(started -> {\n+                                    if (started) {\n+                                        return streamMetadataStore.findStreamCutReferenceRecordBefore(scope, stream, truncationStreamCut, retentionSet, context, executor)\n+                                                                  .thenCompose(ref -> {\n+                                                                      if (ref != null) {\n+                                                                          return streamMetadataStore.deleteStreamCutBefore(scope, stream, ref, context, executor);\n+                                                                      } else {\n+                                                                          return CompletableFuture.completedFuture(null);\n+                                                                      }\n+                                                                  });\n+                                    } else {\n+                                        throw new RuntimeException(\"Could not start truncation\");\n+                                    }\n+                                });\n+                    });\n+                });\n+    }\n+\n+    private CompletableFuture<Map<Long, Long>> getTruncationStreamCutBySizeLimit(String scope, String stream, OperationContext context, RetentionPolicy policy,\n+                                                                                 RetentionSet retentionSet, Map<Long, Long> lowerBound, StreamCutRecord newRecord) {\n+        // if the lowerbound on subscribers streamcuts satisfies the policy size bound, then return it. \n+        // else return the stream cut that satisfies maximum bound on size. \n+        // 1. if lowerbound.size < max and lowerbound.size > min truncate at lowerbound\n+        // 2. if lowerbound.size < min, truncate at max irrespective of if lowerbound overlaps with max or not. \n+        // 3. if lowerbound.size > max, truncate at max\n+        long currentSize = newRecord != null ? newRecord.getRecordingSize() : retentionSet.getLatest().getRecordingSize();\n+        return streamMetadataStore.getSizeTillStreamCut(scope, stream, lowerBound, Optional.empty(), context, executor)\n+                          .thenCompose(sizeTill -> {\n+                               long retainedSize = currentSize - sizeTill;\n+                               Supplier<Optional<StreamCutReferenceRecord>> maxBound = () -> retentionSet\n+                                      .getRetentionRecords().stream()\n+                                      .filter(x -> currentSize - x.getRecordingSize() > policy.getConsumptionLimits().getMaxValue())\n+                                      .max(Comparator.comparingLong(StreamCutReferenceRecord::getRecordingTime));\n+\n+                               // if retainedSize is less than min size then do not truncate the stream. \n+                               if (retainedSize < policy.getConsumptionLimits().getMinValue()) {\n+                                   return maxBound.get().map(x -> streamMetadataStore.getStreamCutRecord(scope, stream, x, context, executor)\n+                                                                               .thenApply(StreamCutRecord::getStreamCut))\n+                                                  .orElse(CompletableFuture.completedFuture(null));\n+                               } else {\n+                                   // if retained size is less than max allowed, then truncate the stream at subscriber lower bound. \n+                                   if (retainedSize < policy.getConsumptionLimits().getMaxValue()) {\n+                                       return CompletableFuture.completedFuture(lowerBound);\n+                                   } else {\n+                                       // if retained size is greater than max allowed, then truncate the stream at streamcut\n+                                       // from retention set that matches the retention policy size bound. \n+                                       return maxBound.get().map(x -> streamMetadataStore.getStreamCutRecord(scope, stream, x, context, executor)\n+                                                   .thenApply(StreamCutRecord::getStreamCut)\n+                                                   .thenCompose(maxRecord -> {\n+                                                       // if max record is strictly greater than lowerbound then we can truncate at max record\n+                                                       return streamMetadataStore.streamCutStrictlyGreaterThan(\n+                                                               scope, stream, maxRecord, lowerBound, context, executor)\n+                                                                                 .thenApply(gt -> {\n+                                                                                     if (gt) {\n+                                                                                         return maxRecord;\n+                                                                                     } else {\n+                                                                                         return lowerBound;\n+                                                                                     }\n+                                                                                 });\n+                                                   })).orElse(CompletableFuture.completedFuture(null));   \n+                                   }\n+                               }\n+                           });\n+    }\n+\n+    private CompletableFuture<Map<Long, Long>> getTruncationStreamCutByTimeLimit(String scope, String stream, OperationContext context,\n+                                                                                 RetentionPolicy policy, RetentionSet retentionSet,\n+                                                                                 Map<Long, Long> lowerBound, StreamCutRecord latest) {\n+        Map.Entry<StreamCutReferenceRecord, StreamCutReferenceRecord> limits =\n+                getBoundStreamCuts(policy.getConsumptionLimits(), retentionSet);\n+        // if subscriber lowerbound is ahead of streamcut corresponding to the max time and is behind stream cut for min time \n+        // from the retention set then we can safely truncate at lowerbound. Else we will truncate at the max time bound if it\n+        // exists\n+        // 1. if LB > min => truncate at min\n+        // 2. if LB < max => truncate at max\n+        // 3. if LB < min && LB > max => truncate at LB\n+        // 4. if LB < min && overlaps max => truncate at LB\n+        CompletableFuture<StreamCutRecord> limitMaxFuture = limits.getKey() == null ? CompletableFuture.completedFuture(null) :\n+                streamMetadataStore.getStreamCutRecord(scope, stream, limits.getKey(), context, executor);\n+        CompletableFuture<StreamCutRecord> limitMinFuture = limits.getValue() == null ? CompletableFuture.completedFuture(null) :\n+                streamMetadataStore.getStreamCutRecord(scope, stream, limits.getValue(), context, executor);\n+        return CompletableFuture.allOf(limitMaxFuture, limitMinFuture)\n+                         .thenCompose(v -> {\n+                             StreamCutRecord limitMax = limitMaxFuture.join();\n+                             StreamCutRecord limitMin = limitMinFuture.join();\n+                             if (limitMin != null) {\n+                                 return streamMetadataStore.streamCutStrictlyGreaterThan(scope, stream, limitMin.getStreamCut(), lowerBound, context, executor)\n+                                                    .thenCompose(gtMin -> {\n+                                                        if (gtMin) {\n+                                                            if (limitMax == null) {\n+                                                                return CompletableFuture.completedFuture(lowerBound);\n+                                                            } else {\n+                                                                return streamMetadataStore.streamCutStrictlyGreaterThan(scope, stream, limitMax.getStreamCut(), lowerBound, context, executor)\n+                                                                                          .thenApply(gtMax -> gtMax ? limitMax.getStreamCut() : lowerBound);\n+                                                            }\n+                                                        } else {\n+                                                            return streamMetadataStore.streamCutStrictlyGreaterThan(scope, stream, lowerBound, limitMin.getStreamCut(), context, executor)\n+                                                                               .thenApply(gt -> gt ? limitMin.getStreamCut() : null);\n+                                                        }\n+                                                    });\n+                             } else {\n+                                 // if min limit is 0 then latest effectively becomes the min. and we truncate at the lowerbound.\n+                                 if (latest != null && policy.getConsumptionLimits().getMinValue() == 0L) {\n+                                     // truncate at the lower bound \n+                                     return CompletableFuture.completedFuture(lowerBound);\n+                                 } else {\n+                                     return CompletableFuture.completedFuture(null);\n+                                 }\n+                             }\n+                         });\n+    }\n+\n+    private Map<Long, Long> computeSubscribersLowerBound(List<Map<StreamSegmentRecord, Long>> subscribers) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDkxMTQ5OA=="}, "originalCommit": {"oid": "07c2838e170b5ac7b040ca02680ca3ab32983ab5"}, "originalPosition": 244}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTgyMzQ2MA==", "bodyText": "Ok. Got it.", "url": "https://github.com/pravega/pravega/pull/5289#discussion_r525823460", "createdAt": "2020-11-18T05:27:54Z", "author": {"login": "pbelgundi"}, "path": "controller/src/main/java/io/pravega/controller/task/Stream/StreamMetadataTasks.java", "diffHunk": "@@ -507,32 +514,296 @@ public void initializeStreamWriters(final EventStreamClientFactory clientFactory\n \n     private CompletableFuture<Void> truncate(String scope, String stream, RetentionPolicy policy, OperationContext context,\n                                              RetentionSet retentionSet, StreamCutRecord newRecord, long recordingTime, long requestId) {\n-        Optional<StreamCutReferenceRecord> truncationRecord = findTruncationRecord(policy, retentionSet, newRecord, recordingTime);\n-        if (!truncationRecord.isPresent()) {\n-            log.info(\"No suitable truncation record found, per retention policy for stream {}/{}\", scope, stream);\n-            return CompletableFuture.completedFuture(null);\n+        if (policy.getRetentionType().equals(RetentionPolicy.RetentionType.CONSUMPTION)) {\n+            return subscriberBasedTruncation(scope, stream, context, policy, retentionSet, newRecord, requestId);\n+        } else {\n+            Optional<StreamCutReferenceRecord> truncationRecord = findTruncationRecord(policy, retentionSet, newRecord, recordingTime);\n+            if (!truncationRecord.isPresent()) {\n+                log.info(\"No suitable truncation record found, per retention policy for stream {}/{}\", scope, stream);\n+                return CompletableFuture.completedFuture(null);\n+            }\n+            log.info(\"Found truncation record for stream {}/{} truncationRecord time/size: {}/{}\", scope, stream,\n+                    truncationRecord.get().getRecordingTime(), truncationRecord.get().getRecordingSize());\n+\n+            return truncate(scope, stream, context, requestId, truncationRecord.get());\n         }\n+    }\n+\n+    private CompletableFuture<Void> truncate(String scope, String stream, OperationContext context, long requestId,\n+                                                   StreamCutReferenceRecord truncationRecord) {\n         log.info(\"Found truncation record for stream {}/{} truncationRecord time/size: {}/{}\", scope, stream,\n-                truncationRecord.get().getRecordingTime(), truncationRecord.get().getRecordingSize());\n-        return streamMetadataStore.getStreamCutRecord(scope, stream, truncationRecord.get(), context, executor)\n-                   .thenCompose(streamCutRecord -> startTruncation(scope, stream, streamCutRecord.getStreamCut(), context, requestId))\n-                   .thenCompose(started -> {\n-                       if (started) {\n-                                return streamMetadataStore.deleteStreamCutBefore(scope, stream, truncationRecord.get(), context, executor);\n+                truncationRecord.getRecordingTime(), truncationRecord.getRecordingSize());\n+        return streamMetadataStore.getStreamCutRecord(scope, stream, truncationRecord, context, executor)\n+                                  .thenCompose(streamCutRecord -> startTruncation(scope, stream, streamCutRecord.getStreamCut(), context, requestId))\n+                                  .thenCompose(started -> {\n+                                      if (started) {\n+                                          return streamMetadataStore.deleteStreamCutBefore(scope, stream, truncationRecord, context, executor);\n+                                      } else {\n+                                          throw new RuntimeException(\"Could not start truncation\");\n+                                      }\n+                                  })\n+                                  .exceptionally(e -> {\n+                                      if (Exceptions.unwrap(e) instanceof IllegalArgumentException) {\n+                                          // This is ignorable exception. Throwing this will cause unnecessary retries and exceptions logged.\n+                                          log.debug(requestId, \"Cannot truncate at given \" +\n+                                                  \"streamCut because it intersects with existing truncation point\");\n+                                          return null;\n+                                      } else {\n+                                          throw new CompletionException(e);\n+                                      }\n+                                  });\n+    }\n+\n+    private CompletableFuture<Void> subscriberBasedTruncation(String scope, String stream, OperationContext context,\n+                                                              RetentionPolicy policy, RetentionSet retentionSet, \n+                                                              StreamCutRecord newRecord, long requestId) {\n+        return streamMetadataStore.listSubscribers(scope, stream, context, executor)\n+                           .thenCompose(list -> Futures.allOfWithResults(list.stream().map(x -> \n+                                   streamMetadataStore.getSubscriber(scope, stream, x, context, executor)).collect(Collectors.toList())))\n+                           .thenCompose(subscribers -> {\n+                               // convert all streamcuts to include the segment range\n+                               return Futures.allOfWithResults(subscribers.stream().map(x -> {\n+                                   ImmutableSet<Map.Entry<Long, Long>> entries = x.getObject().getTruncationStreamCut().entrySet();\n+\n+                                   return Futures.keysAllOfWithResults(entries.stream().collect(Collectors.toMap(\n+                                           y -> streamMetadataStore.getSegment(scope, stream, y.getKey(), context, executor), Map.Entry::getValue)));\n+                               }).collect(Collectors.toList()));\n+                           })\n+                           .thenApply(this::computeSubscribersLowerBound)\n+                .thenCompose(lowerBound -> {\n+                    CompletableFuture<Map<Long, Long>> toTruncateAt; \n+                    if (policy.getConsumptionLimits().getType().equals(RetentionPolicy.ConsumptionLimits.Type.SIZE_BYTES)) {\n+                        toTruncateAt = getTruncationStreamCutBySizeLimit(scope, stream, context, policy, retentionSet, lowerBound, newRecord);\n+                    } else {\n+                        toTruncateAt = getTruncationStreamCutByTimeLimit(scope, stream, context, policy, retentionSet, lowerBound, newRecord);\n+                    }\n+                    return toTruncateAt.thenCompose(truncationStreamCut -> {\n+                        if (truncationStreamCut == null || truncationStreamCut.isEmpty()) {\n+                            log.debug(\"no truncation record could be compute that satisfied retention policy\");\n+                            return CompletableFuture.completedFuture(null);\n+                        } \n+                        return startTruncation(scope, stream, truncationStreamCut, context, requestId)\n+                                .thenCompose(started -> {\n+                                    if (started) {\n+                                        return streamMetadataStore.findStreamCutReferenceRecordBefore(scope, stream, truncationStreamCut, retentionSet, context, executor)\n+                                                                  .thenCompose(ref -> {\n+                                                                      if (ref != null) {\n+                                                                          return streamMetadataStore.deleteStreamCutBefore(scope, stream, ref, context, executor);\n+                                                                      } else {\n+                                                                          return CompletableFuture.completedFuture(null);\n+                                                                      }\n+                                                                  });\n+                                    } else {\n+                                        throw new RuntimeException(\"Could not start truncation\");\n+                                    }\n+                                });\n+                    });\n+                });\n+    }\n+\n+    private CompletableFuture<Map<Long, Long>> getTruncationStreamCutBySizeLimit(String scope, String stream, OperationContext context, RetentionPolicy policy,\n+                                                                                 RetentionSet retentionSet, Map<Long, Long> lowerBound, StreamCutRecord newRecord) {\n+        // if the lowerbound on subscribers streamcuts satisfies the policy size bound, then return it. \n+        // else return the stream cut that satisfies maximum bound on size. \n+        // 1. if lowerbound.size < max and lowerbound.size > min truncate at lowerbound\n+        // 2. if lowerbound.size < min, truncate at max irrespective of if lowerbound overlaps with max or not. \n+        // 3. if lowerbound.size > max, truncate at max\n+        long currentSize = newRecord != null ? newRecord.getRecordingSize() : retentionSet.getLatest().getRecordingSize();\n+        return streamMetadataStore.getSizeTillStreamCut(scope, stream, lowerBound, Optional.empty(), context, executor)\n+                          .thenCompose(sizeTill -> {\n+                               long retainedSize = currentSize - sizeTill;\n+                               Supplier<Optional<StreamCutReferenceRecord>> maxBound = () -> retentionSet\n+                                      .getRetentionRecords().stream()\n+                                      .filter(x -> currentSize - x.getRecordingSize() > policy.getConsumptionLimits().getMaxValue())\n+                                      .max(Comparator.comparingLong(StreamCutReferenceRecord::getRecordingTime));\n+\n+                               // if retainedSize is less than min size then do not truncate the stream. \n+                               if (retainedSize < policy.getConsumptionLimits().getMinValue()) {\n+                                   return maxBound.get().map(x -> streamMetadataStore.getStreamCutRecord(scope, stream, x, context, executor)\n+                                                                               .thenApply(StreamCutRecord::getStreamCut))\n+                                                  .orElse(CompletableFuture.completedFuture(null));\n+                               } else {\n+                                   // if retained size is less than max allowed, then truncate the stream at subscriber lower bound. \n+                                   if (retainedSize < policy.getConsumptionLimits().getMaxValue()) {\n+                                       return CompletableFuture.completedFuture(lowerBound);\n+                                   } else {\n+                                       // if retained size is greater than max allowed, then truncate the stream at streamcut\n+                                       // from retention set that matches the retention policy size bound. \n+                                       return maxBound.get().map(x -> streamMetadataStore.getStreamCutRecord(scope, stream, x, context, executor)\n+                                                   .thenApply(StreamCutRecord::getStreamCut)\n+                                                   .thenCompose(maxRecord -> {\n+                                                       // if max record is strictly greater than lowerbound then we can truncate at max record\n+                                                       return streamMetadataStore.streamCutStrictlyGreaterThan(\n+                                                               scope, stream, maxRecord, lowerBound, context, executor)\n+                                                                                 .thenApply(gt -> {\n+                                                                                     if (gt) {\n+                                                                                         return maxRecord;\n+                                                                                     } else {\n+                                                                                         return lowerBound;\n+                                                                                     }\n+                                                                                 });\n+                                                   })).orElse(CompletableFuture.completedFuture(null));   \n+                                   }\n+                               }\n+                           });\n+    }\n+\n+    private CompletableFuture<Map<Long, Long>> getTruncationStreamCutByTimeLimit(String scope, String stream, OperationContext context,\n+                                                                                 RetentionPolicy policy, RetentionSet retentionSet,\n+                                                                                 Map<Long, Long> lowerBound, StreamCutRecord latest) {\n+        Map.Entry<StreamCutReferenceRecord, StreamCutReferenceRecord> limits =\n+                getBoundStreamCuts(policy.getConsumptionLimits(), retentionSet);\n+        // if subscriber lowerbound is ahead of streamcut corresponding to the max time and is behind stream cut for min time \n+        // from the retention set then we can safely truncate at lowerbound. Else we will truncate at the max time bound if it\n+        // exists\n+        // 1. if LB > min => truncate at min\n+        // 2. if LB < max => truncate at max\n+        // 3. if LB < min && LB > max => truncate at LB\n+        // 4. if LB < min && overlaps max => truncate at LB\n+        CompletableFuture<StreamCutRecord> limitMaxFuture = limits.getKey() == null ? CompletableFuture.completedFuture(null) :\n+                streamMetadataStore.getStreamCutRecord(scope, stream, limits.getKey(), context, executor);\n+        CompletableFuture<StreamCutRecord> limitMinFuture = limits.getValue() == null ? CompletableFuture.completedFuture(null) :\n+                streamMetadataStore.getStreamCutRecord(scope, stream, limits.getValue(), context, executor);\n+        return CompletableFuture.allOf(limitMaxFuture, limitMinFuture)\n+                         .thenCompose(v -> {\n+                             StreamCutRecord limitMax = limitMaxFuture.join();\n+                             StreamCutRecord limitMin = limitMinFuture.join();\n+                             if (limitMin != null) {\n+                                 return streamMetadataStore.streamCutStrictlyGreaterThan(scope, stream, limitMin.getStreamCut(), lowerBound, context, executor)\n+                                                    .thenCompose(gtMin -> {\n+                                                        if (gtMin) {\n+                                                            if (limitMax == null) {\n+                                                                return CompletableFuture.completedFuture(lowerBound);\n+                                                            } else {\n+                                                                return streamMetadataStore.streamCutStrictlyGreaterThan(scope, stream, limitMax.getStreamCut(), lowerBound, context, executor)\n+                                                                                          .thenApply(gtMax -> gtMax ? limitMax.getStreamCut() : lowerBound);\n+                                                            }\n+                                                        } else {\n+                                                            return streamMetadataStore.streamCutStrictlyGreaterThan(scope, stream, lowerBound, limitMin.getStreamCut(), context, executor)\n+                                                                               .thenApply(gt -> gt ? limitMin.getStreamCut() : null);\n+                                                        }\n+                                                    });\n+                             } else {\n+                                 // if min limit is 0 then latest effectively becomes the min. and we truncate at the lowerbound.\n+                                 if (latest != null && policy.getConsumptionLimits().getMinValue() == 0L) {\n+                                     // truncate at the lower bound \n+                                     return CompletableFuture.completedFuture(lowerBound);\n+                                 } else {\n+                                     return CompletableFuture.completedFuture(null);\n+                                 }\n+                             }\n+                         });\n+    }\n+\n+    private Map<Long, Long> computeSubscribersLowerBound(List<Map<StreamSegmentRecord, Long>> subscribers) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDkxMTQ5OA=="}, "originalCommit": {"oid": "07c2838e170b5ac7b040ca02680ca3ab32983ab5"}, "originalPosition": 244}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMwMDIwMzk3OnYy", "diffSide": "RIGHT", "path": "client/src/main/java/io/pravega/client/stream/RetentionPolicy.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQxOTozMjoxNFrOH1-wcQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQxOTozMjoxNFrOH1-wcQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjM2NDc4NQ==", "bodyText": "subscribed reader groups. is not a concept that has be introduced.", "url": "https://github.com/pravega/pravega/pull/5289#discussion_r526364785", "createdAt": "2020-11-18T19:32:14Z", "author": {"login": "tkaitchuck"}, "path": "client/src/main/java/io/pravega/client/stream/RetentionPolicy.java", "diffHunk": "@@ -57,6 +63,45 @@ public static RetentionPolicy byTime(Duration duration) {\n      * @return Retention policy object.\n      */\n     public static RetentionPolicy bySizeBytes(long size) {\n-        return new RetentionPolicy(RetentionType.SIZE, size);\n+        return RetentionPolicy.builder().retentionType(RetentionType.SIZE).retentionParam(size).build();\n+    }\n+    \n+    /**\n+     * Create a retention policy to configure a stream to truncate a stream\n+     * according to positions of subscribed reader groups.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43ed4c9779a615ec60ff036dbabfd6cb411c6394"}, "originalPosition": 47}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4499, "cost": 1, "resetAt": "2021-11-13T12:26:42Z"}}}